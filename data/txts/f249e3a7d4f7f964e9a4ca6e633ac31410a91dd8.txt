Pushing the Limits of Low-Resource Morphological Inﬂection
Antonios Anastasopoulos and Graham Neubig Language Technologies Institute, Carnegie Mellon University
{aanastas,gneubig}@cs.cmu.edu

arXiv:1908.05838v2 [cs.CL] 20 Aug 2019

Abstract
Recent years have seen exceptional strides in the task of automatic morphological inﬂection generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of improvements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inﬂection decoder. In addition, we investigate the eﬀects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macroaveraged accuracy of our models outperforms the state-of-the-art by 15 percentage points.1 Also, we identify the crucial factors for success with cross-lingual transfer for morphological inﬂection: typological similarity and a common representation across languages.
1 Introduction
The majority of the world’s languages are categorized as synthetic, meaning that they have rich morphology, be it fusional, agglutinative, polysynthetic, or a mixture thereof. As Natural Language Processing (NLP) keeps expanding its frontiers to encompass more and more languages, modeling of the grammatical functions that guide language generation is of utmost importance.
In the case of morphologically-rich languages, explicit modeling of the inﬂection processes has signiﬁcant potential to alleviate issues created by data scarcity and the resulting lack of vocabulary coverage. Especially on low-resource, underrepresented languages and dialects, the potential for impact is much higher. For example, speech
1Our code is available at https://github.com/ antonisa/inflection.

aguà

P(y1 · · · yK)

s1 · · · sK

softmax s1 · · · sK

ct1 · · · ctK attention

decoder

c1x · · · cKx attention

ht1 · · · htM

h1x · · · hNx

encoder t1 · · · tM

encoder x1 · · · xN

V PRS 2 PL IND a g u a r

Figure 1: Visualization of our proposed two-step attention architecture. The decoder ﬁrst attends over the tag sequence T and then uses the updated decoder state s to attend over the character sequence X in order to produce the inﬂected form Y. (Example from Asturian.)

recognition (Foley et al., 2018) and predictive keyboards (Breiner et al., 2019) for under-represented languages, if they exist, largely still rely on unigram lexicons with performance inferior to the sophisticated language models used in high-resource ones. Good inﬂection models would be invaluable for predictive text technology in morphologicallyrich languages as they could eﬀectively enable proper handling of the huge vocabulary.
Additionally, they could be very useful for building educational applications for languages of under-represented communities (along with their inverse, morphological analyzers). Encouraging examples are the Yupik morphological analyzer (Schwartz et al., 2019) and the Inuktitut educational tools from the respective Native Peoples communities.2 The social impact of such applications can be enormous, eﬀectively raising the status of the languages slightly closer to the level of
2http://www.inuktitutcomputing.ca/

the dominant regional language. Morphological inﬂection has been thoroughly
studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges (Cotterell et al., 2016, 2017, 2018). Lowresource settings, in contrast, are relatively underexplored. One promising direction and the main focus of the SIGMORPHON 2019 challenge (McCarthy et al., 2019b) is cross-lingual training, which has been successfully applied in other lowresource tasks such as Machine Translation (MT) or parsing.
In this work we focus in this cross-lingual setting for low-resource morphological inﬂection and propose several simple, yet eﬀective approaches to mitigating problems caused by extreme lack of data which, put together, improve accuracy by 15 percentage points over a state-of-the-art baseline. This is achieved through the combination of a novel decoder architecture, a training regime that alleviates the need for costly structural biases that force attention monotonicity, and a data hallucination technique. We also present thorough ablations and identify the crucial factors for success with our approach.
Our system was the best performing system in the 2019 SIGMORPHON shared task on morphological inﬂection when evaluated on accuracy, while it ranked third when evaluated with average Levenshtein distance.
2 Task Deﬁnition and Approach
Morphological inﬂection is the process that creates grammatical forms (typically guided by sentence structure) of a lexeme/lemma. As a computational task it is framed as mapping from the lemma and a set of morphological tags to the desired form, which simpliﬁes the task by removing the necessity to infer the form from context. For an example from Asturian, given the lemma aguar and tags V;PRS;2;PL;IND, the task is to create the indicative voice, present tense, 2nd person plural form aguà.
Let X = x1 . . . xN be a character sequence of the lemma, T = t1 . . . tM a set of morphological tags, and Y = y1 . . . yK be an inﬂection target character sequence. The goal is to model P(Y | X, T).
Our approach consists of three major components. First, we propose a novel two-step attention decoder architecture (§2.1). Second, we augment the low-resource datasets with a data halluci-

nation technique (§2.2). Third, we devise a training schedule (§2.3) that substitutes structural biases for attention monotonicity.

2.1 Model Architecture

Our models are based on a sequence-to-sequence

model with attention (Bahdanau et al., 2015). In

broad terms, the model is composed of three parts:

an encoder, the attention, and a recurrent decoder.

In the setting of the inﬂection task, there is an ad-

ditional input provided (the set of morphological

tags) which requires an additional encoder.

A visualization of our model is shown in Fig-

ure 1. First, an encoder transforms the input char-
acter sequence x1 . . . xN into a sequence of intermediate representations h1x . . . hNx . A second encoder transforms the set of morphological tags t1 . . . tM into another sequence of states ht1 . . . htM:
hnx = encx(hnx−1, xn) and htm = enct(T).

In our implementation, we use a single layer bi-

directional recurrent encoder for the lemma, and

a self-attention encoder (Vaswani et al., 2017) for

encoding the tags as there is no inherent order

(e.g. left-to-right) in their presentation. In prelim-

inary experiments, a self-attention lemma encoder

proved hard to train, while a recurrent tag encoder

yielded quite competitive results.

Next, we have attention mechanisms that trans-

form the two sequences of input states into two

sequences of context vectors via two matrices of

attention weights (k is the current decoder time

step):

ckx = n αkxnhnx

ctk = m αtkmhtm .

Finally, the recurrent decoder computes a se-

quence of output states in a two-step process, from

which a probability distribution over output char-

acters can be computed:

sk = sk−1 + ctk sk = dec(sk−1, ckx, yk−1)

P(yk) = softmax(sk).

The attention mechanisms produce their weights as in Luong et al. (2015), with vx, vt, Wαs x, Wαs t , Whαx, and Whαt being parameters to be learned:

αtkm = softmax(vt tanh( Wαs t sk−1; Whαt htm ))

αkxn = softmax(vx tanh( Wαs x sk; Whαx hnx )).

The two-step attention process essentially ﬁrst

uses the decoder’s previous state sk−1 as the query for attending over the tags. Then, it creates a tag-

informed state sk by adding the tag context ctk to the previous state. The tag-informed state is then used as the query for attending over the source characters and produce the context ckx. The last step is then to update the recurrent state and produce the output character. Ultimately, we desire that the provided tag set guides the generation, which also means inﬂuencing the attention over the characters of the lemma.
Additional Structural Biases for Attention Incorporating structural biases in the model’s architecture or in the training objective can lead to improvements in performance, especially for tasks where the attention mechanism is expected to behave similarly to an alignment model, like MT. This idea has been successfully applied to the inﬂection task and is at the core of the state-of-theart model (Wu and Cotterell, 2019).
One bias we deem important is coverage of all input characters and tags from the attentions. Intuitively, this entails encouraging the model to “look at” the whole input. We take the approach of Cohn et al. (2016) and add two regularizers over the ﬁnal attention matrices, encouraging them to also sum to one column-wise:
−λ Σ jatjm − I 2 −λ Σ jaxjn − I 2
Another bias that we incorporate encourages the Markov assumption over attention/alignments. Brieﬂy, this means that if the i-th source character/word is aligned to the j-th target one (i ← j), then alignments i+1← j+1 or i← j+1 are also quite likely. In a neural architecture this can be approximated by providing the attention weight vector from the previous timestep as input to the function that computes the attention weights. We refer the reader to Cohn et al. (2016) for exact details.
Adversarial Language Discriminator When training multilingual systems, encouraging the encoder to learn language-invariant representations can often lead to improvements (Xie et al., 2017; Chen et al., 2018), as it forces the model to truly work in a multilingual setting. We achieve that by introducing a language discriminator (Ganin et al., 2016). This additional component receives the last output of the (bi-directional) intermediate lemma representations hNx and outputs a prediction yl of the source language such that yl=softmax(MLP(hNx )).
The discriminator is trained to predict the language by minimizing a standard cross-entropy

Original triple lemma

stem

stem

παρακάμπτω

+V;2;SG;IPFV;PST
Hallucinated lemma
+V;2;SG;IPFV;PST

παρέκαμπτες
πξρακάμοτω πξρέκαμοτες

Figure 2: Example of our hallucination process (Greek). The lemma and inﬂected forms are aligned at the character level. The inside of stem-considered parts (highlighted) are substituted with random characters, creating hallucinated triples (bottom).

loss Ll similar to Lample et al. (2018). However, in order to encourage the encoder to learn language-invariant representations, we reverse the gradients ﬂowing from that component into the encoder during back-propagation.
2.2 Data Hallucination
Low-resource language datasets are usually too small to allow for proper learning with neural networks. A major issue in our case is label bias. Put simply, the character decoder will overly prefer outputting common character sequences. However, with just 50 examples to learn from, the output character n-gram distribution will hardly match the real one, because the majority of the ngrams will have zero probability. In order to mitigate this issue, we augment our training sets with hallucinated data.
In most languages morphological reinﬂection is realized by adding, deleting, or modifying preﬁxes or suﬃxes over a stem that is mainly unchanged. Though we do not have prior information regarding the stem or aﬃxes of our data we can use character alignment to approximately compute them. We use the alignment method from the SIGMORPHON 2016 baseline (Cotterell et al., 2016) to align the lemmata and the inﬂected forms. For each example, we consider as part of the stem any sequence of three or more3 consecutive lemma characters that are aligned to the exact same characters in the inﬂected form.
Now, for each such region considered as a “stem”, we randomly substitute its inside (not start or end) characters with other characters from the
3This heuristic number is largely arbitrary and could potentially be tuned to diﬀerent values for each language.

language’s alphabet. Note that we do not change the length of the region, though allowing for such variation could possibly lead to further improvement. The substitution characters are sampled uniformly for the alphabet, rather than attempting to sample from a more informed distribution, which has potential for further improvements. Overall, we hallucinated 10,000 examples for each lowresource language, creating an additional hallucinated dataset H.
A visualized example of our hallucination process is outlined in Figure 2. Out of the three regions with matching aligned characters (thick lines), we identify two with length equal to three or more. In the hallucinated example (bottom of the ﬁgure), we sample random characters for the inside of such regions.
Silfverberg et al. (2017) have proposed a data hallucination method conceptually quite similar to ours, which treats the single longest common continuous substring between lemma and form as the stem. Their approach would be eﬀectively similar to ours for languages with aﬃxal steminvariant morphology, but it would likely fail in more complicated morphological phenomena like apophony, stem alternation (conceptually similar to inﬁx morphology) or in the root-and-pattern morphology of semitic languages. Consider the apophony example from the past participle form gschwommen of the lemma schwimmen in Swiss German: our approach would treat both the schw and the mmen regions as a stem, as opposed to only considering one.4 Note that neither of the two approaches are suitable for phenomena such as suppletion (e.g. the inﬂection of the Spanish verb ir ‘to go’ into fue ‘went’) or phenomena like the reduplication pattern of Indonesian noun plurals as in kuda ‘horse’, kuda-kuda ‘horses’ (Sneddon et al., 2012).
2.3 Training Schedule
Our training schedule attempts to balance two desires: helping the model to (1) learn to copy, and (2) learn cross-lingually with a particular focus on the low-resource language. To achieve this, we split training into three phases: warm-up, crosslingual training, and low-resource ﬁne-tuning.
Phase 1: Warm-Up As several previous works have noted, learning to copy is crucial. Unlike other proposed models, though, our model does
4Most likely, the desired parts are schw and mm.

Model

Accuracy

Wu and Cotterell (2019) 48.5

this work

48.8

+H

60.1

+H + Ll

60.8

+multi-language transfer 63.8

oracle

68.2

Median
45.5 67.0 66.6 66.0 64.0
74.0

Table 1: Macro-averaged accuracy over 100 language test pairs. Our best model outperforms the baseline by 15 percentage points.

not include any structural biases that encourage copying. Instead, we rely on an additional copying task in a warm up period.
We transform each training triple [X, T, Y] into two additional triples that encourage copying: [X, COPY, X] and [Y, T, Y]. Using both the input and output sequences for the copying task allows the use of slightly more diverse data. Note that we only use the correct tags when copying the inﬂected form. For copying the lemma we use a specialized tag (COPY). Additional improvements could be achieved if one knew the exact tags that match the lemmata. But this could vary by language: an English verb’s lemma is its inﬁnitive and has a V;NFIN tag, while a verb lemma in Modern Greek is its V;1;SG;IPFV;PRS form.
In this stage we use a relatively large batch size (10) in order to encourage more coarse updates. In most cases, the model achieves extremely high copying accuracy after a couple of warm-up epochs, so we stop the warm up stage when copying accuracy exceeds 75%. At this point the attention mechanism over the source characters has learned to be monotonic. In contrast, the model of Wu and Cotterell (2019) requires a dynamic programming method that forces strict monotonicity, with an additional (non-parallelizable) computational cost O(|x|2) throughout training.
Phase 2: Cross-lingual training In the main training phase we use both high- and low-resource language data (including any hallucinated data). If not using hallucinated data, we up-sample the low-resource data in order to match the size of the high-resource ones. Furthermore, with probability 0.30 we also sample copying tasks to intersperse throughout the training epoch. This ensures the source-character attention keeps being monotonic.

Model
Wu and Cotterell (2019): 0th-order soft attention 0st-order hard attention 0th-order monotonic attention 1st-order monotonic attention
two-step attention (this work) − warm-up, copy task + warm-up, copy task + structural biases + scheduled sampling + minibatch schedule
with hallucinated data: +H +H + Ll +ensemble (three models)

Dev Accuracy
(macro-averaged)
29.6 32.3 37.2 40.3
31.2 48.0 48.7 49.1 49.4
82.6† 85.4† 85.6†

Table 2: Our proposed two-step attention architecture outperforms the baselines. Additional biases and scheduled sampling contribute further improvements. †: results with hallucinated data are not directly comparable, as dev data were used for hallucination.

Phase 3: Fine-tuning The last phase is inspired by ﬁne-tuning, or continued training, as applied to cross-lingual MT e.g. by Neubig and Hu (2018) or to domain adaptation for MT e.g. by Luong and Manning (2015). The setting is nearly identical to the second phase, except we only use the low-resource language data for training and do not use the copying task. Furthermore, we substitute teacher forcing for scheduled sampling (Bengio et al., 2015), where with probability 50% the input to the next step of the decoder is not the gold one, but its previous prediction. This technique allows the model to become more robust to its own mistakes, eﬀectively limiting the eﬀect of exposure bias. It is worth noting that at this point, the learning rate is typically quite small, so we reduce the batch size to a single instance. In most cases, though, the improvements on development set accuracy are marginal (1 to 2%).
2.4 Inference and Implementation Details
The performance of inﬂection systems is typically evaluated with exact-match token-level accuracy, as well as character-level Levenshtein distance.5
5Due to space constraints, we do not provide Levenshtein distance results. They generally inversely correlate

Hence, during training we continuously evaluate the model’s performance on the development set with both metrics. Consequently we store three checkpoints: the one that achieved the highest accuracy, the one that reached the lowest Levenshtein distance, and the one that improved on both metrics over previous dev evaluations. In a few cases these three checkpoints coincide, but this is rather rare. We ensemble these three models with equal weights for producing our ﬁnal predictions.
All our models are implemented in DyNet (Neubig et al., 2017). Each model is trained on a single CPU, as each training run requires less than 1GB of RAM and typically concludes within 3 to 4 hours. We provide additional hyperparameter details in the Appendix.
3 Empirical Results
Our experiments are conducted on the SIGMORPHON 2019 challenge datasets (McCarthy et al., 2019a). The dataset consists of 100 pairs between (mostly related) high-resource transfer languages and 43 low-resource test languages with examples taken from the Unimorph database (Kirov et al., 2018). The low-resource languages typically have about 100 training examples, plus 50–100 (or in very few cases, 1000) examples in the development set. In contrast, most high-resource language training sets include 10,000 instances.
Test Accuracy Our main results are summarized in Table 1. Our models signiﬁcantly outperform the baseline, evaluated with macro-averaged accuracy over the 100 language pairs. Our novel architecture performs slightly better than the baseline without any additional data. Importantly, including the hallucinated datasets boosts total accuracy by 10 percentage points, while transfer from multiple languages further improves to a state-ofthe-art accuracy of 63.8% – higher than any system submitted to the shared task.
It is worth noting that all of these improvements are not uniform across languages/pairs: without hallucinated training data, our model’s median is notably larger than its average, implying that for a few language pairs our model is under-trained and under-performs (in particular, pairs with Yiddish, Votic, and Ingrian as test languages). Nonetheless, if one had access to an oracle that would allow them to select the best performing model out of all
with token-level accuracy.

L1

L2

latin bengali sorani italian latvian english italian
urdu slovene russian swahili portuguese kurmanji
zulu kannada

czech greek irish ladin lithuanian murrinhpatha neapolitan old english old saxon portuguese quechua russian sorani swahili telugu

Average

Genetic dist.
0.86 0.88 1.0 0.30 0.25 N/A 0.10 0.88 0.89 0.93 1.0 0.93 N/A 1.0 0.75
0.75

L1+L2
15.0 22.4 20.3 48 17.1 36 70 23.8 10.7 34.5 14.2 25.6 16.2 46 76
31.72

+Ll
26.0 16.4 18.6 54 23.2
2 70 18.5 14.7 22.2 12.8 17.5 13.6 52 72
28.90

+H
71.4 70.5 66.3 74 48.4
6 83 43.4 52.3 88.8 92.1 76.3 69.0 81 94
67.77

+Ll + H
68.0 70.6 64.6 74 48.4 20 83 40.4 50.5 88.4 91.6 74.6 64.3 80 90
67.23

H
77.4 71.6 65.6 74 50.5 20 84 44.3 50.5 87.7 91.6 74.3 66.7 76 94
68.55

Table 3: Results with a single transfer language. Monolingual data hallucination is crucial due to the distance of the languages. In some cases cross lingual transfer should be avoided in favor of a purely monolingual setting (H).

the diﬀerent settings, they would achieve an oracle accuracy of 68.2%.
Architecture Ablations We focus on the architecture of the model and the structural biases we introduced, using the development set for evaluation. Table 2 presents the macro-averaged accuracy for the baseline (Wu and Cotterell, 2019) and the diﬀerent versions of our model.
The ﬁrst thing to note is the importance of the warm-up period in our training schedule and the use of the copying task. Our model neither handles copying in any explicit way nor encourages the attention to be monotonic. Without the warm-up period and the additional copying tasks, our model’s development accuracy is in the same ballpark as the baseline 0th-order soft/hard attention models.
On the other hand, our two-step attention trained with the additional copying task already improves upon the baseline without any of the additional biases, with a dev accuracy of 48 compared to 40.3 for the best baseline model. We attribute this to two factors, the ﬁrst being our novel architecture. Compared to a single attention over the concatenated tag and lemma sequences, our two-step attention has the advantage of two distinct attention mechanisms, which capture the inherently diﬀerent properties of the tag and the lemma character sequences. Another advantage is the two-step process that guides the lemma attention with the tags. Disentangling the two attentions and ordering them in an intuitive way makes it easier for them to learn their respective tasks.
The second factor is, we suspect, a slightly

better choice of hyperparameters: our models are smaller than the baseline ones. Although we did not tune our hyperparameters extensively, a few experiments on a couple of language pairs showed that increasing the model size hurt performance under these extremely data-scarce conditions.
Each of the additional techniques we tested further contributed a few accuracy points. The attention biases add 0.7% points and scheduled sampling in the third training phase further adds 0.4% points. The diﬀerent batching schedule with large batch sizes in the beginning but smaller towards the end of training helps a little more in terms of accuracy, but most importantly it signiﬁcantly speeds up training. Table 2 also reports the development set accuracy when using hallucinated data. Although the large improvements are also proportionally reﬂected in the test set, these numbers are not directly comparable to the rest since the development set data were used in the hallucination process.
4 Analysis
We analyze the results over various groupings of languages to elucidate the properties of our models over the 100 quite diverse language pairs. We use typological information from the URIEL database (Littell et al., 2017) in this analysis.
Single Language Transfer We ﬁrst focus on the test languages for which a single transfer language was provided, with detailed results presented in Table 3. Generally, the average genetic distance between the transfer and the task language is quite

L1

L2

turkish

persian

bashkir

azeri

uzbek

all

urdu sanskrit
hindi greek
all

bengali

turkish bashkir uzbek
all

crimean tatar

ﬁnnish hungarian estonian
all

ingrian

ﬁnnish hungarian estonian
all

karelian

basque slovak czech polish
all

kashubian

bashkir uzbek turkish
all

khakas

estonian hungarian
ﬁnnish all

livonian

italian arabic hebrew
all

maltese

danish dutch german
all

middle high
german

danish dutch english
all

north frisian

asturian spanish french
all

occitan

russian polish bulgarian
all

old church slavonic

sanskrit persian
all

pashto

welsh irish latvian all

scottish gaelic

bashkir

uzbek

tatar

turkish

all

Average (over all pairs)

L1+L2
81 55 57 47 84
42 44 49 42 49
87 59 60 82
38 28 32 38
54 42 42 50
46 58 54 66 70
82 84 90 84
27 28 30 26
34 18 20 29
68 70 72 70
18 28 22 23
58 47 41 52
40 41 38 41
20 36 29
46 58 58 60
67 55 81 81
49.00

+Ll
77 63 59 55 71
32 38 52 46 50
80 60 60 81
36 32 32 44
50 46 42 46
40 62 64 78 72
78 80 90 92
27 30 26 25
25 26 21 28
58 62 68 70
25 22 26 25
49 55 52 61
39 41 42 42
16 28 30
34 62 36 64
64 45 79 79
48.48

+H
80 74 66 74 83
66 66 67 65 64
85 70 72 88
34 32 32 36
58 58 54 54
70 74 78 78 78
80 72 80 86
35 35 34 36
48 41 47 40
78 74 86 84
44 47 47 43
77 78 80 78
39 59 44 64
46 39 46
64 68 58 62
73 67 82 84
59.36

+Ll + H
81 69 67 70 87
67 65 65 67 62
89 69 67 80
40 38 38 36
56 52 58 64
76 76 66 80 80
84 76 82 84
34 33 35 36
42 38 45 46
80 82 82 78
46 46 42 46
74 76 80 80
64 58 56 56
44 48 49
64 66 66 66
69 72 75 83
60.18

H 66.7±0.9
63.7±4.0 71.3±1.1 34.6±2.3 52.6±1.1 74.0±2.3 72.6±6.4
33±1 45.6±5.6 76.7±6.0
43±1 78.3±3.5 57.3±1.5 46.5±3.5
58±2 73.0±4.6
57.9

Table 4: Results with multiple transfer languages (sample). The best performing system per target (L2) language is highlighted. We repeated the hallucinateddata-only experiments as many times as potential transfer (L1) languages, hence the H column reports average accuracy ± standard deviation.

high (0.75) for these pairs. It is easy to observe that the larger the typological distance, the larger

the improvement from adding hallucinated data. In fact, excluding the languages with no typological information available, there is strong correlation (ρ=0.6) between genetic distance and improvement from hallucinated data.
For cross-lingual transfer to be useful, the languages need to be at least somewhat related and share similar characteristics. A prime example is transfer from Italian for Neapolitan, which achieves a 70% accuracy without any additional synthesized data. In the same vein, the same condition is necessary for the adversarial language discriminator to have impact, as using it on extremely distant language pairs leads to worse performance (e.g. Russian-Portuguese, BengaliGreek, or Urdu-Old English). This is expected, as forcing language invariant representations across vastly diﬀerent languages is analogous to representing a bimodal distribution with its mean.
The results on Kurmanji-Sorani (Northern Kurdish-Central Kurdish) seem to be a valid counter-example to the above statement, i.e. the two languages are related,6 but cross-lingual transfer without hallucinated data performs poorly, achieving a mere 16.2 accuracy. The reason for this discrepancy lies in the characters: Kurmanji is written in the Latin alphabet, while Sorani uses the Arabic one.7 The lack of any similar representations across the languages is too hard to overcome even with the adversarial language discriminator.8
Multiple Transfer Languages For most lowresource languages and especially dialects, there exist several possible candidate transfer languages that can be related enough to satisfy the similarity constraint. We present extensive ablations on such cases in Table 4 with results on the rest of the SIGMORPHON language pairs.9
We again observe positive correlations between the language genetic proximity and the performance of cross-lingual transfer, even with all the transfer and test languages being related. For example, transfer from (distant) Basque to Kashubian performs about 10 percentage points worse
6A reviewer pointed out that Kurmanji and Sorani might actually be more distant than the politically-motivated term “Kurdish dialects" might imply.
7Kurmanji is also sometimes written with the Cyrillic or the Nashk Arabic scripts, but our data are Latin-only.
8We did not attempt to address this issue, by e.g. increasing the loss weight, but we leave this for future work.
9We present a sample due to space constraints. The discussion pertains to all language pairs. The full table is available in the Appendix.

than transfer from (related) Slovak or Czech, which in turn perform worse than transfer from Polish (more closely related to Kashubian than the others). Transfer for North Frisian using Danish is also 10 percentage points worse than transfer from the more closely related Dutch. It is worth noting that using the hallucinated data reduces the eﬀect of the genetic distance between languages, with the standard deviation across the results within the same test language becoming much smaller.
A very interesting case of cross-lingual transfer is Maltese, which is a semitic language and hence genetically close to Hebrew and Arabic. Surprisingly, we obtain better results when transferring from Italian. Again, a script discrepancy could be the main reason, also considering that the root and pattern morphology is only partially expressed in the scripts of Hebrew and Arabic, whereas it is fully expressed (by writing all the vowels) in Maltese. We should also point out that genetic similarity might not be enlightening enough. As Hoberman and Aronoﬀ (2003) point out, the productive verbal morphology in Maltese has become aﬃxal due to borrowings from Romance languages. To further exacerbate the situation, all provided train, dev, and test examples are verbs (no nouns or adjectives), providing an explanation to this seemingly counter-intuitive result.
Another interesting case is that of cross-lingual transfer for Bengali, with the potential languages varying from very related (Sanskrit, Hindi) to even only distantly related (Greek). Nevertheless, there is notably little variance in the performance of the systems. We believe that again the culprit is the diﬀerence in writing systems between all selected transfer and test language, which does not allow our system to leverage cross-lingual information: our Bengali data use the Bengali script, the Urdu dataset is in the Arabic one, Hindi and Sanskrit use Devanagari, and Greek uses the Greek alphabet.
We also present analytic results with crosslingual transfer from all transfer languages from the suggested SIGMORPHON pairs.10 In 14 out of the 29 test languages, our best-performing model is trained on multiple transfer languages. For instance, using Turkish, Persian, Bashkir, and Uzbek data for transfer to Azeri leads to a 6 point improvement over any single-languagetransfer result. A potential explanation is that a di-
10We do not use all languages for transfer in a test language. For instance, when testing on Occitan ‘all’ stands for training on Asturian, Spanish, and French.

alect/language has indeed been inﬂuenced by multiple languages. Another reason could lie in the increased amount of data and potential regularization eﬀects. We suspect the truth lies in the union of those factors, but nonetheless we conclude that whenever available, transfer from multiple related languages can further improve accuracy.
In our experiments we used all transfer languages that the SIGMORPHON organizers proposed in the 2019 challenge. On the one hand, a more sophisticated data selection process could likely yield improvements. For example, Yiddish is primarily based in High German and it has elements from Hebrew, Aramaic, as well as Slavic languages, but we did not test how transfer from these languages might perform. Moreover, in order to remain faithful to the SIGMORPHON challenge, we used some distant languages that probably worsen the results (e.g. Greek for Bengali).
Also, alphabet divergence issues still need to be addressed. For instance, we suspect that our accuracy in Yiddish (which uses the Hebrew alphabet with Yiddish orthography) could be greatly improved by ﬁnding some type of mapping between its orthography and the Latin script that most of its related languages use. The same could hold for transfer for the central Asian languages (Tatar, Turkmen, Azeri among others) which use a variety of the Latin, Cyrillic, or Arabic scripts.
Lastly, we experimented with a completely monolingual setting, using just the low-resource and hallucinated language data (columns H in Tables 3 and 4). For fairer comparison to crosslingual transfer, we repeated the hallucination process as many times as candidate transfer languages and we report the mean and standard deviation of the test set accuracy. This baseline is extremely competitive, lagging only a few points behind the L + H combination. Encouragingly, this entails that hallucination is a viable option for entire language families without a single high-resource representative or low-resource isolates.
Interpretability We ﬁnd that the attention matrices can help understand our model’s predictions. A visualization of two examples is shown in Figure 3, showcasing the interpretability advantage of the disentangled two-step attentions. In the Kazakh example the tag attention clearly identiﬁes the suﬃxes да (that marks plural) and рды (that marks the accusative case). The Greek example is a great one of how the two-step process allows the

Kazakh

Modern Greek

а с п а н N ACC PL

π ρ ο σ α ρ τ ώ V 3 Pl Pfv Sbjv

а

ν

с

α

п

π

а

ρ ο

н

σ

д

α ρ

а

τ ή

р

σ

ο

д

υ

ы

μ ε

ax

at

ax

at

Figure 3: Attention visualization examples. The in-

ﬂected form is generated from top to bottom.

tags to guide the lemma attention. Due to the SBJV tag, the model does not use the lemma until the necessary particle να has been generated. Consequently, the lemma attention properly copies the stem, and then the tag attention attends ﬁrst over PFV and then over PL and 3 in order to construct the correct suﬃx for perfective and 3rd person plural.
5 Related Work
The inﬂection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity.
Data augmentation for inﬂection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses.
Kann et al. (2017) have identiﬁed typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance.
Our novel two-step process decoder architecture bares similarities with multi-source models (Anastasopoulos and Chiang, 2018; Zoph and Knight, 2016) which provide two contexts from two encoded sources to the decoder. A similar dis-

entangled encoding was also used by Ács (2018) for their SIGMORPHON 2018 submission. We in fact experimented with this architecture but preliminary results on the development sets showed that our two-step architecture achieved better performance. Interestingly, the second-best performing system (Peters and Martins, 2019) at SIGMORPHON 2019, which also ranked ﬁrst in terms of Levenshtein distance, also uses decoupled encoders to separately encode the lemma and the tags; this further cosolidates our belief that such an approach is superior to using a single encoder for the concatentated sequence of the tags and lemma. The main diﬀerence to our model is that they do not use our two-step decoder process, while they substitute all softmax operations with sparsemax (Martins and Astudillo, 2016), yielding interpretable attention matrices very similar to ours. The use of sparsemax in conjunction with our twostep decoder process, as well as along our data hallucination technique, presents a promising direction towards even better results in the future.
6 Conclusion
With this work we advance the state-of-the-art for morphological inﬂection on low-resource languages by 15 points, through a novel architecture, data hallucination, and a variety of training techniques. Our two-step attention decoder follows an intuitive order, also enhancing interpretability. We also suggest that complicated methods for copying and forcing monotonicity are unnecessary. We identify language genetic similarity as a major success factor for cross-lingual training, and show that using many related languages leads to even better performance. Despite this signiﬁcant stride, the problem is far from solved. Language-speciﬁc or language-family-speciﬁc improvements (i.e. proper dealing with diﬀerent alphabets, or using an adversarial language discriminator) could potentially further boost performance.
Acknowledgements
The authors are grateful to the anonymous reviewers for their exceptionally constructive and insightful comments, to Arya McCarthy for discussions on morphological inﬂection, as well as to Gabriela Weigel for her invaluable help with editing and proofreading the paper. This material is based upon work generously supported by the National Science Foundation under grant 1761548.

References
Judit Ács. 2018. BME-HAS system for CoNLL– SIGMORPHON 2018 shared task: Universal morphological reinﬂection. In Proc. CoNLL– SIGMORPHON, pages 121–126, Brussels. Association for Computational Linguistics.
Roee Aharoni and Yoav Goldberg. 2017. Morphological inﬂection generation with hard monotonic attention. In Proc. ACL.
Roee Aharoni, Yoav Goldberg, and Yonatan Belinkov. 2016. Improving sequence to sequence learning for morphological inﬂection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinﬂection. In Proc. SIGMORPHON.
Antonios Anastasopoulos and David Chiang. 2018. Leveraging translations for speech transcription in low-resource settings. In Proc. INTERSPEECH.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. ICLR.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Proc. NIPS.
Toms Bergmanis, Katharina Kann, Hinrich Schütze, and Sharon Goldwater. 2017. Training data augmentation for low-resource morphological inﬂection. Proc. SIGMORPHON.
Theresa Breiner, Chieu Nguyen, Daan van Esch, and Jeremy O’Brien. 2019. Automatic keyboard layout design for low-resource Latin-script languages. arXiv:1901.06039.
Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. 2018. Adversarial deep averaging networks for cross-lingual sentiment classiﬁcation. Transactions of the Association for Computational Linguistics, 6:557–570.
Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haﬀari. 2016. Incorporating structural alignment biases into an attentional neural translation model. In Proc. NAACL-HLT.
Ryan Cotterell, Christo Kirov, John Sylak-Glassman, Géraldine Walther, Ekaterina Vylomova, Arya D. McCarthy, Katharina Kann, Sebastian Mielke, Garrett Nicolai, Miikka Silfverberg, David Yarowsky, Jason Eisner, and Mans Hulden. 2018. The CoNLL–SIGMORPHON 2018 shared task: Universal morphological reinﬂection. In Proc. CoNLL– SIGMORPHON.
Ryan Cotterell, Christo Kirov, John Sylak-Glassman, Géraldine Walther, Ekaterina Vylomova, Patrick Xia, Manaal Faruqui, Sandra Kübler, David

Yarowsky, Jason Eisner, and Mans Hulden. 2017. CoNLL-SIGMORPHON 2017 shared task: Universal morphological reinﬂection in 52 languages. In Proc. CoNLL SIGMORPHON.
Ryan Cotterell, Christo Kirov, John Sylak-Glassman, David Yarowsky, Jason Eisner, and Mans Hulden. 2016. The SIGMORPHON 2016 shared task— morphological reinﬂection. In Proc. SIGMORPHON.
Ben Foley, Josh Arnold, Rolando Coto-Solano, Gautier Durantin, T Mark Ellison, Daan van Esch, Scott Heath, František Kratochvíl, Zara Maxwell-Smith, David Nash, et al. 2018. Building speech recognition systems for language documentation: The CoEDL endangered language pipeline and inference system (Elpis). In Proc. SLTU.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016. Domain-adversarial training of neural networks. JMLR.
Robert D Hoberman and Mark Aronoﬀ. 2003. The verbal morphology of Maltese. Language Acquisition and Language Disorders, 28:61–78.
Katharina Kann, Ryan Cotterell, and Hinrich Schütze. 2017. One-shot neural cross-lingual transfer for paradigm completion. In Proc. ACL.
Christo Kirov, Ryan Cotterell, John Sylak-Glassman, Géraldine Walther, Ekaterina Vylomova, Patrick Xia, Manaal Faruqui, Sebastian J. Mielke, Arya McCarthy, Sandra Kübler, David Yarowsky, Jason Eisner, and Mans Hulden. 2018. UniMorph 2.0: Universal Morphology. In Proc. LREC.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2018. Unsupervised machine translation using monolingual corpora only. In Proc. ICLR.
Patrick Littell, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. 2017. Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. In Proc. EACL.
Minh-Thang Luong and Christopher D Manning. 2015. Stanford neural machine translation systems for spoken language domains. In Proc. IWSLT.
Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Eﬀective approaches to attention-based neural machine translation. In Proc. EMNLP.
Andre Martins and Ramon Astudillo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classiﬁcation. In Proc. ICML, pages 1614–1623.

Arya D. McCarthy, Ekaterina Vylomova, Shijie Wu, Chaitanya Malaviya, Lawrence Wolf-Sonkin, Garrett Nicolai, Christo Kirov, Miikka Silfverberg, Sebastian Mielke, Jeﬀrey Heinz, Ryan Cotterell, and Mans Hulden. 2019a. The SIGMORPHON 2019 shared task: Crosslinguality and context in morphology. In Proc. SIGMORPHON.
Arya D. McCarthy, Ekaterina Vylomova, Shijie Wu, Chaitanya Malaviya, Lawrence Wolf-Sonkin, Garrett Nicolai, Miikka Silfverberg, Sebastian J. Mielke, Jeﬀrey Heinz, Ryan Cotterell, and Mans Hulden. 2019b. The SIGMORPHON 2019 shared task: Morphological analysis in context and crosslingual transfer for inﬂection. In Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 229–244, Florence, Italy.
Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, et al. 2017. DyNet: The dynamic neural network toolkit. arXiv:1701.03980.
Graham Neubig and Junjie Hu. 2018. Rapid adaptation of neural machine translation to new languages. In Proc. EMNLP.
Ben Peters and André F. T. Martins. 2019. IT–IST at the SIGMORPHON 2019 shared task: Sparse twoheaded models for inﬂection. In Proc. SIGMORPHON, Florence, Italy.
Lane Schwartz, Emily Chen, Benjamin Hunt, and Sylvia LR Schreiner. 2019. Bootstrapping a neural morphological analyzer for St. Lawrence Island Yupik from a ﬁnite-state transducer. In Proc. Comput-EL3.
Miikka Silfverberg, Adam Wiemerslage, Ling Liu, and Lingshuang Jack Mao. 2017. Data augmentation for morphological reinﬂection. Proc. SIGMORPHON.
James Neil Sneddon, K Alexander Adelaar, Dwi N Djenar, and Michael Ewing. 2012. Indonesian: A comprehensive grammar. Routledge.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS.
Shijie Wu and Ryan Cotterell. 2019. Exact hard monotonic attention for character-level transduction. arXiv:1905.06319.
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. 2017. Controllable invariance through adversarial feature learning. In Proc. NeurIPS, pages 585–596.
Chunting Zhou and Graham Neubig. 2017. Morphological inﬂection generation with multi-space variational encoder-decoders. In Proc. SIGMORPHON.
Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proc. NAACL-HLT.

A Hyperparameters
Here we list all the hyperparameters of our models:
• Encoder/Decoder number of layers: 1 • Character/Tag Embedding size: 32 • Recurrent State Size: 100 • LSTM Type: CoupledLSTM • Attention Size: 100 • Target and Source Character Embeddings are
Tied • Tag Self-Attention size: 100 • Tag Self-Attention heads: 1
Here we list the optimizer settings:
• Optimizer: SimpleSGD • Starting learning rate: 0.1 • Learning Rate Decay: 0.5 • Learning Rate Decay Patience: 6 epochs • Maximum number of epochs: 20, 40, 40 (for
each training phase) • Minibatch Size: 10, 10, 2 (for each training
phase).

L1

L2

latin bengali sorani italian latvian english italian
urdu slovene russian swahili portuguese kurmanji
zulu kannada

czech greek irish ladin lithuanian murrinhpatha neapolitan old english old saxon portuguese quechua russian sorani swahili telugu

Average

Genetic dist.
0.86 0.88 1.0 0.30 0.25 N/A 0.10 0.88 0.89 0.93 1.0 0.93 N/A 1.0 0.75
0.75

L1+L2
15.0 22.4 20.3 48 17.1 36 70 23.8 10.7 34.5 14.2 25.6 16.2 46 76
31.72

+Ll
26.0 16.4 18.6 54 23.2
2 70 18.5 14.7 22.2 12.8 17.5 13.6 52 72
28.90

+H
71.4 70.5 66.3 74 48.4
6 83 43.4 52.3 88.8 92.1 76.3 69.0 81 94
67.77

+Ll + H
68.0 70.6 64.6 74 48.4 20 83 40.4 50.5 88.4 91.6 74.6 64.3 80 90
67.23

H
77.4 71.6 65.6 74 50.5 20 84 44.3 50.5 87.7 91.6 74.3 66.7 76 94
68.55

Table 5: Results with a single transfer language. Monolingual data hallucination is crucial due to the distance of the languages. In some cases cross lingual transfer should be avoided in favor of a purely monolingual setting (H).

B Complete Result Tables
Table 5 lists all results for test languages with a single candidate transfer language. Results on test languages with multiple candidate languages (both single-language-transfer, and transfer with all candidate suggested languages) are listed in Tables 6 and 7.

L1

L2

L1+L2 +Ll +H +Ll + H

H

turkish

persian

bashkir

azeri

uzbek

all

81

77

80

55

63

74

57

59

66

47

55

74

84

71

83

81

69

67

66.7±0.9

70

87

urdu sanskrit
hindi greek
all

bengali

42

32

66

44

38

66

49

52

67

42

46

65

49

50

64

67

65

65

63.7±4.0

67

62

welsh irish albanian all

breton

54

55

83

48

52

80

53

59

78

62

55

81

86

88

78.2±2.5

81

82

hebrew

86

85

95

95

arabic

classical

83

82

92

95

all

syriac

87

85

95

96

94±0

irish

18

24

24

20

welsh

cornish

26

28

28

24

all

22

16

22

24

24±0

turkish

87

80

85

bashkir

crimean

59

60

70

uzbek

tatar

60

60

72

all

82

81

88

89

69

71.3±1.1

67

80

spanish italian
all

friulian

55

55

81

56

55

78

64

58

78

83

77

79.0±2.6

83

ﬁnnish hungarian estonian
all

ingrian

38

36

34

28

32

32

32

32

32

38

44

36

40

38

34.6±2.3

38

36

adyghe

92

armenian kabardian

78

all

95

92

93

77

86

91

93

96

80

90.6±4.0

90

ﬁnnish

54

50

58

hungarian karelian

42

46

58

estonian

42

42

54

all

50

46

54

56

52

52.6±1.1

58

64

basque

46

slovak

58

czech

kashubian

54

polish

66

all

70

40

70

62

74

64

78

78

78

72

78

76

76

66

74.0±2.3

80

80

turkish bashkir uzbek
all

kazakh

86

80

74

68

84

74

66

60

78

76

78

80

66

70

73.3±3.0

70

78

bashkir uzbek turkish
all

khakas

82

78

80

84

80

72

90

90

80

84

92

86

84

76

72.6±6.4

82

84

czech

romanian

latin

all

5.4

5.0 20.6

42.0

7.9

9.0 18.8

41.3

41.9±1.4

3.6

7.7 40.1

44.1

estonian

27

27

35

34

hungarian livonian

28

30

35

33

ﬁnnish

30

26

34

35

all

26

25

36

36

33±1

Table 6: Multiple transfer language results (part 1).

L1

L2

italian arabic hebrew
all

maltese

danish dutch german
all

middle high german

dutch german danish
all

middle
low german

danish dutch english
all

north frisian

asturian spanish french
all

occitan

russian polish bulgarian
all

old church slavonic

irish belarusian
welsh all

old irish

sanskrit persian
all

pashto

welsh irish latvian all

scottish gaelic

bashkir

uzbek

tatar

turkish

all

bashkir arabic turkish uzbek
all

turkmen

estonian

ﬁnnish

votic

hungarian

all

dutch english danish
all

west frisian

danish dutch german
all

yiddish

Average (over all pairs)

L1+L2
34 18 20 29
68 70 72 70
36 46 30 32
18 28 22 23
58 47 41 52
40 41 38 41
2 10 4 4
20 36 29
46 58 58 60
67 55 81 81
82 64 82 66 92
12 24 10 17
36 36 33 40
50 49 53 55
49.00

+Ll
25 26 21 28
58 62 68 70
42 26 24 42
25 22 26 25
49 55 52 61
39 41 42 42
4 10 6 4
16 28 30
34 62 36 64
64 45 79 79
76 66 88 74 90
17 20 10 20
39 25 33 38
54 50 54 55
48.48

+H
48 41 47 40
78 74 86 84
32 32 30 36
44 47 47 43
77 78 80 78
39 59 44 64
12 10 10 8
46 39 46
64 68 58 62
73 67 82 84
82 84 90 86 84
23 26 24 28
44 45 42 43
55 56 57 55
59.36

+Ll + H
42 38 45 46
80 82 82 78
36 38 30 38
46 46 42 46
74 76 80 80
64 58 56 56
2 6 6 6
44 48 49
64 66 66 66
69 72 75 83
88 80 92 78 88
27 28 30 29
49 43 43 45
56 55 54 54
60.18

H 45.6±5.6 76.7±6.0 36.6±3.0
43±1 78.3±3.5 57.3±1.5
6±2 46.5±3.5
58±2 73.0±4.6
80±4 26.3±2.1
47±1 55.3±0.6
57.9

Table 7: Multiple transfer language results (part 2).

