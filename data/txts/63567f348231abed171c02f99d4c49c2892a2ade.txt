Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy

arXiv:2009.06389v3 [cs.LG] 3 Oct 2020

Tom Farrand
tom@seldon.io Seldon
OpenMined
Sahib Singh
sahibsin@alumni.cmu.edu Ford R&A OpenMined
ABSTRACT
Deployment of deep learning in different fields and industries is growing day by day due to its performance, which relies on the availability of data and compute. Data is often crowd-sourced and contains sensitive information about its contributors, which leaks into models that are trained on it. To achieve rigorous privacy guarantees, differentially private training mechanisms are used. However, it has recently been shown that differential privacy can exacerbate existing biases in the data and have disparate impacts on the accuracy of different subgroups of data. In this paper, we aim to study these effects within differentially private deep learning. Specifically, we aim to study how different levels of imbalance in the data affect the accuracy and the fairness of the decisions made by the model, given different levels of privacy. We demonstrate that even small imbalances and loose privacy guarantees can cause disparate impacts.
CCS CONCEPTS
• Security and privacy → Social aspects of security and privacy; Privacy protections; Usability in security and privacy; Privacy-preserving protocols; • Computing methodologies → Neural networks.
KEYWORDS
Fairness, Bias, Differential Privacy, Data Imbalance, Deep Learning
ACM Reference Format: Tom Farrand, Fatemehsadat Mireshghallah, Sahib Singh, and Andrew Trask. 2020. Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy. In Proceedings of 2020 Workshop on PrivacyPreserving Machine Learning in Practice (PPMLP’20), November 9, 2020, Virtual Event, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3411501.3419419
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 2020 Workshop on Privacy-Preserving Machine Learning in Practice (PPMLP’20), November 9, 2020, Virtual Event, USA, 15.00 © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8088-1/20/11. . . $15.00 https://doi.org/10.1145/3411501.3419419

Fatemehsadat Mireshghallah
fmireshg@eng.ucsd.edu University of California San Diego
OpenMined
Andrew Trask
andrew@openmined.org University of Oxford OpenMined
1 INTRODUCTION
Given the high performance of deep learning algorithms in a variety of tasks, they are widely deployed. These algorithms rely on large data sets and high performance compute to perform well. This data is often crowd-sourced and likely contains sensitive information about its contributors [30]. It has been shown widely in the literature that machine learning models, and more specifically DNNs, tend to memorize information from the training set [4, 9, 25]. There are a plethora of attacks that exploit the vulnerabilities in DNNs and extract sensitive information (e.g. gender, ethnicity, genetic markers) from models in both black and white box settings [13, 28].
To mitigate this, Differential Privacy (DP) is used. DP can protect against attempts to infer the contribution of a given individual to the training set by adding noise to the computations [1, 10]. DP is used in many contexts, including in medical settings [29] and is also used to protect 2020 US Census data [2]. The most prominent DP algorithm in deep learning is DP-Stochastic Gradient Descent (DP-SGD) [1] wherein noise is added to clipped gradients during training. In this work, Abadi et al. introduce the moments accountant, which is a method that keeps track of the exhausted privacy budget over time and amplifies privacy during training. The moments accountant returns the overall budget used and the privacy leakage, which are demonstrated with 𝜖 and 𝛿, respectively. Although DP-SGD offers rigorous privacy guarantees, it degrades the accuracy of the resulting model. Furthermore, it has recently been shown that this degradation is disparate; minority subgroups of data suffer more utility loss compared to others [5, 19].
Bagdasaryan et al. [5] empirically show that if DP-SGD is used on data which is highly imbalanced, as in contains subgroups with extremely small populations, the less represented groups which already have lower accuracy end up losing more utility: "the poor become poorer". They also show that as stricter privacy guarantees are imposed, this gap gets wider. This gap can have huge societal and economic implications. For instance, [19] shows that if DP was used in the decision making of a fund allocation problem (based on US Census data), smaller districts would end up with comparatively more funding than what they would receive without DP, and larger districts would get less funding.
To achieve a deeper insight into the effects of differential privacy, we set out to study in detail the behavior of deep learning models trained using DP-SGD. To be more precise, we set out to study a

wider range of imbalance than what was explored by Bagdasaryan et al.. They studied datasets in which the minority subgroup formed 0.01% − 5.00% of the entire data. We, on the other hand, study less significant imbalances as well. We cover a range of 0.1%−30% for the population of the minority subgroup (30% might not be considered a minority, however it is a common imbalance to have in datasets). We also explore a wider range of privacy budget. Bagdasaryan et al. study 𝜖 in range of 3 to 10, whereas we also consider 𝜖 = 1.15 and 𝜖 = 16.2. Our results show that if there are two subgroups in the data and the minority group comprises 30% the data (CelebA data and the minority group is male), which is actually a large portion and is the case for many datasets, using differential privacy with 𝜖 = 16.2 and 𝜖 = 4.98 yields 5.1% and 5.2% more disparity in accuracy between the two subgroups (female vs. male). We show that increasing the imbalance (i.e. decreasing the population of the minority subgroup to 0.1%) increases this disparity to 19% and 52% respectively. This result shows that even when differential privacy is applied with loose guarantees, on datasets with negligible imbalance, it can have a huge impact on the minority subgroups. We also study the fairness metrics demographic parity and equality of opportunity, finding that as datasets become more imbalanced and stricter privacy guarantees are used the fairness, with respect to these two metrics, of models is reduced.
2 RELATED WORK
While there have been several papers related to fairness and differential privacy, works most relevant to ours are mentioned below.
Bagdasaryan et al. [5] empirically demonstrates a disproportionately large reduction in accuracy for subgroups when modeled using neural networks trained with DP-SGD. For example, a gender classification model trained using DP-SGD on the Flickr-based Diversity in Faces (DiF) dataset [24] and the UTKFace dataset [33] dataset has a much lower accuracy when classifying black faces compared to white faces. Similar results were also found when performing sentiment analysis on a corpus of African-American English tweets [6, 7], species classification on iNaturalist nature images [32], and federated learning of a language model on a public Reddit comments dataset [22]. As discussed in the introduction, our work is different from the one by Bagdasaryan et al. in that we study a wider range of privacy budgets and also a wider range of data imbalances. Apart from that, we also use other notions of privacy, such as equality of opportunity and demographic parity.
The work by Kuppam et al. [19] proposes novel measures of fairness in the context of randomized DP algorithms and identifies causes of outcome disparities.
Our paper builds on these above works by experimenting with varied data imbalance ratios and seeks to measure the difference in equality of opportunity and demographic parity, common metrics of fairness. Other seminal papers addressing fair decision making include [16, 17, 20, 31]
3 BACKGROUND
In this section, we discuss the fundamental privacy and fairness concepts used throughout the paper.

3.1 Differentially Private SGD (DP-SGD)
Differential Privacy [11, 12] provides a strong privacy guarantee for algorithms on aggregate databases. For 𝜖 ≥ 0, an algorithm 𝐴 is understood to satisfy Differential Privacy if and only if for any pair of datasets that differ in only one element, the following statement holds true.

′

𝑃 [𝐴(𝐷)

=

𝑡]

≤

𝜖
𝑒

𝑃 [𝐴(𝐷

)

=

𝑡]

∀𝑡

Where 𝐷 and 𝐷 ′ are differing datasets by at most one element, and 𝑃 [𝐴(𝐷) = 𝑡] denotes the probability that 𝑡 is the output of 𝐴. This setting approximates the effect of individual opt-outs by minimising the inclusion effect of an individual’s data.
Stochastic Gradient Descent (SGD) is an iterative method for optimising differentiable objective functions. It updates weights and biases by calculating the gradient of a loss function on small batches of data. DP-SGD [1] is a modification of the stochastic gradient descent algorithm which provides provable privacy guarantees. It is different from SGD because it bounds the sensitivity of each gradient and is paired with a moments accountant algorithm to amplify and track the privacy loss across weight updates. The moments accountant accumulates and tracks the privacy expenditure in the training process of deep neural networks. Moments accountant significantly improves on earlier privacy analysis of SGD and allows for meaningful privacy guarantees for deep learning trained on realistically sized datasets [8].
In order to ensure SGD is differentially private (i.e. DP-SGD), there are two modifications to be made to the original SGD algorithm. First, the sensitivity of each gradient must be bounded. This is done by clipping the gradient in the 𝑙2 norm. Second, one applies random noise to the earlier clipped gradient, multiplying its sum by the learning rate, and then using it to update the model parameters.

3.2 Fairness & Bias In Machine Learning
Fairness in machine learning is a measure of the degree to which there is a disparate treatment for different groups which should have been treated equally (e.g. female vs. male). Hence an algorithm whose decisions skew towards a particular group of people can be said to be unfair.
The definitions of fairness can fall under individual fairness, group fairness and subgroup fairness [23]. Individual fairness is where similar predictions are provided for similar individuals. Group fairness refers to equal treatment of various groups. Subgroup fairness obtains the best properties of the former groups by picking a group fairness constraint and asking if it holds over a large collection of subgroups [18].
Like people, algorithms are also susceptible to biases. There are many different types of bias as discussed in Mehrabi et al. [23]. Historical Bias refers to the bias which already exists due to prior historical biases or socio-technical issues in society. For example, a 2018 image search result involving female CEOs showed fewer images since only 5% of Fortune 500 CEOs were women. Representation Bias can result from the way we define and sample a population (e.g. lack of geographical diversity in ImageNet). Algorithmic bias is caused when a bias is generated by the algorithm without being present in the input data.

Table 1: Overall accuracy results and the delta when compared to the non-DP baseline for the 90% female imbalanced dataset.

Privacy level High (𝜖 = 1.15) Medium (𝜖 = 4.98) Low (𝜖 = 16.2) Non-DP

Accuracy (%) 71.05 85.61 87.51 91.65

Δ (%) -20.6 -6.04 -4.14 0

4 EXPERIMENTAL RESULTS
Our experiments implement a ResNet18 model [15], pre-trained on ImageNet, to perform smile classification on a subset of 30,000 images from the CelebA dataset [21]. During the course of training, we experiment with both data imbalance ratios, as well as with the hyperparameter settings used by DP-SGD- specifically the amount of noise applied (𝜎) and the gradient clipping level (𝑆). Following Bagdasaryan et al. [5] we compute privacy budget 𝜖 for each training run using the Rényi DP [26] implementation.
We implement our experiments using PyTorch [27] and ran them on four Nvidia K80 GPUs. After hyperparameter optimisation using grid search; we use a batch size of 32, a learning rate of 0.00005, and train for 60 epochs. Adam optimiser is used to minimise our loss function. A full training run of 60 epochs with differential privacy applied takes 20 hours to converge in this setup.
The gender balance of the CelebA subset was manipulated to ensure that females were always the majority. The gender imbalances considered were: 99.9% (29,970) female to 0.1% male (30), 99% (29,700) to 1% (300) male, 90% (27,000) female to 10% (3,000) male, 80% (24,000) female to 20% (6,000) male, 70% (21,000) female to 30% (9,000) male. The primary attribute of smiling was held constantly balanced with each of the manipulated datasets containing 15,000 smiling and 15,000 non-smiling examples.
Results were then collected across each of the imbalance levels at varying levels of noise while holding all other privacy-related hyperparameters constant (𝛿 = 10−6, 𝑆 = 5). The noise multiplier, 𝑧 = 𝜎𝑆, was varied to ensure a ratio between noise and gradient clipping was enforced. This led to the consideration of three different differential privacy levels across each of the imbalance datasets: high (𝜖 = 1.15, 𝛿 = 10−6, 𝑧 = 1.5) medium (𝜖 = 4.98, 𝛿 = 10−6, 𝑧 = 0.7) and low (𝜖 = 16.2, 𝛿 = 10−6, 𝑧 = 0.5). A baseline without any differential privacy applied (Non-DP) was also recorded.
Furthermore, gradient clipping was varied (𝑆 = 1, 𝑆 = 5, 𝑆 = 10) and studied using the 70% female to 30% male dataset, across the three different privacy levels.
4.1 Impact of DP on Accuracy at Test Time
Empirically we find that model utility has an inverse relationship with information leakage- as greater privacy guarantees are placed on the model, the overall accuracy of the model decreases. An example of our results are shown in Table 1. This observation agrees with the work of Alvim et al. [3].
Figure 1 shows that data imbalance appears to have little effect on the difference (Δ) between subgroup accuracies (i.e. female accuracy minus male accuracy) until the split between the subgroups is

50

40

Δ Accuracy

30

Privacy Level

High Privacy (ε=1.15)

20

Med Privacy (ε=4.98)

Low Privacy (ε=16.2)

Non-DP 10

0

−10 70%

80%

90% Imbalance

99%

99.9%

Figure 1: Smile detection accuracy delta between subgroups (female - male) vs. dataset imbalance at varying levels of DP.

Accuracy

0.85 0.80 0.75 0.70 0.65

Privacy Level High Privacy (ε=1.15, z=1.5) Med Privacy (ε=4.98, z=0.7) Low Privacy (ε=16.2, z=0.5)
Group Overall Female Male

1

5

10

Clipping Bound

Figure 2: Smile detection accuracy across groups vs. gradient clipping at varying levels of DP.

extreme at 99.9% vs 0.1%. This is demonstrated by the similar accuracy deltas across all privacy levels up until the most extreme level of data imbalance at which point accuracy deltas diverge. Interestingly, up to the most extreme level of imbalance, it is the highest privacy level (𝜖 = 1.15) which most closely matches the non-DP baseline. We hypothesise that at this level of differential privacy the noise and clipping techniques applied during the training process obfuscate the gradient signal to such an extent whereby all samples have a similar training effect. Therefore, while the overall model utility reduces, due to noisier gradients, the utility difference between subgroups actually improves. This phenomenon is seen again in experiments with different clipping levels, as shown in Figure 2, where the strictest privacy guarantee has the smallest accuracy difference between subgroups (4.24%). This is an important takeaway; increasing the privacy does not necessarily make the utility disparity worse. There is a point in which the utility is diminished so much, that the classifier is almost random, and at that point, it actually becomes less disparate.
4.2 Impact of DP on Accuracy During Training
From Figure 3 it is clear that the training accuracy is highest without any privacy guarantee (Non-DP) and the accuracy across all groups (overall, female, male) at all time steps reduces as stricter privacy guarantees are used. We see that even very weak privacy guarantees

Accuracy Accuracy

0.9

0.8

Differential Privacy

High DP (ε=1.15)

Med DP (ε=4.98)

0.7

Low DP (ε=16.2)

Non-DP

Group

Overall

Female

0.6

Male

0.5 0

500

1000

1500

2000

Step

Figure 3: Smile detection accuracy for differing groups (overall, female, male) vs. training step at varying levels of DP on the 70% female dataset.

(𝜖 = 16.2) significantly impact the rate of convergence in training, as well as the final utility of the trained model.
Moreover, Figure 3 shows that when conducting training efforts without any privacy guarantees (Non-DP) the difference in accuracy disparity (Δ accuracy) between female and male subgroups remains relatively constant. However, for all differentially private training runs the accuracy delta between female and male subgroups diverges throughout training, with lower levels of privacy (𝜖 = 4.98, 𝜖 = 16.2) suffering from greater divergence than stricter measures.
We hypothesise that the divergence observed during training with DP-SGD is due to the gradient clipping which bounds the influence of outliers. There are fewer examples of the minority group in each training mini-batch thus their gradients are higher and therefore more likely to be clipped. This hampers the model’s ability to learn effectively from the minority group meaning that the bulk of the accuracy gain of the model is obtained from performance increases on the majority group. This effect then compounds over the course of model training; as the model becomes more proficient at accurately detecting the majority group, an increasing proportion of the minority group training examples become outliers, with clipped gradients, leading to a diverging accuracy gap between the subgroups.
Figure 4 demonstrates that more imbalanced datasets cause a larger divergence in accuracy between subgroups during the course of training. This reinforces the above hypothesis; datasets with smaller minority groups are more quickly impacted by the effects of gradient clipping leading to more glaring divergence over time.
4.3 Impact of DP on Fairness Metrics
To measure the effects of DP on fairness, we have chosen two metrics that are commonly used in the literature: difference in Demographic Parity (Δ𝐷𝑒𝑚𝑃 ) and the difference in Equality of Opportunity (Δ𝐸𝑂 ) [14]. Figure 5 shows the results. In a classification task, demographic parity requires the conditional probability of the classifier predicting output class 𝑌ˆ = 𝑦 given sensitive variable 𝑆 = 0 to be the same as predicting class 𝑌ˆ = 𝑦 given 𝑆 = 1. Here the sensitive feature is gender. Δ𝐷𝑒𝑚𝑃 is the difference between these two probabilities and the lower this difference, the more fair the

0.85

0.80

0.75 0.70 0.65 0.60 0.55

Dataset Imbalance 80% Female 90% Female 99% Female
Group Overall Female Male

0.50 0

500

1000

1500

2000

Step

Figure 4: Smile detection accuracy for differing groups (over-
all, female, male) vs. training step at varying levels of dataset imbalance. Trained with (𝜖 = 4.98, 𝛿 = 10−6)-DP.

classifier. This metric is not suitable for the cases where the target and sensitive variables are correlated. This does not concern our case, however, since smiling and gender are unrelated.
Equality of opportunity is another fairness measure, which requires the conditional probability of the classifier predicting class 𝑌ˆ = 1 given sensitive variable 𝑆 = 0 and ground truth class 𝑌 = 1 be equal to the same conditional probability but with 𝑆 = 1. Similar to the demographic parity case, we also measure the difference in these conditional probabilities. The overall trend that we observe in Figure 5 is that when we move to high levels of data imbalance, both the fairness metrics get worse, across all levels of privacy. The trend is similar to that of Figure 1, especially for the Δ𝐸𝑂 . We can see that there are some small differences between the two fairness metrics, especially in the high privacy model, and especially for the highly imbalanced case. We assume that the fact that the high privacy model is doing better in terms of demographic parity and worse in terms of equal opportunity is due to the precision of the high privacy model actually getting worse.
5 CONCLUSION AND FUTURE DIRECTIONS
In this work, we study how different levels of imbalance in the data can have disparate effects on the model accuracy as well as the fairness of the decisions. Our studies yield the following important results: a) The disparate impact of differential privacy on model accuracy is not limited to highly imbalanced data and can occur in situations where the classes are slightly imbalanced. b) The disparate impacts are not limited to high privacy levels- even for loose guarantees, DP has disparate impacts on model accuracy. c) By increasing the privacy level, we do not always see an increase in disparate impacts, since tighter privacy guarantees degrade the utility so much that the model becomes more random and therefore more fair. For future work, we plan to delve deeper and explore the effects of imbalanced data for tasks related to natural language processing. Furthermore, we wish to propose a mitigation mechanism that would help discover bias and decrease it, using recourse or semi-supervised learning. We encourage future research direction in this space with different data modalities.

Δ Demographic Parity

0.200 0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000
70%
0.35

Privacy Level High Privacy (ε=1.15) Med Privacy (ε=4.98) Low Privacy (ε=16.2) Non-DP

80%

90% Imbalance

99%

99.9%
(a) Δ𝐷𝑒𝑚𝑃

0.30

Δ Equality of Opportunity

0.25

0.20 0.15 0.10

Privacy Level High Privacy (ε=1.15) Med Privacy (ε=4.98) Low Privacy (ε=16.2) Non-DP

0.05

0.00 70%

80%

90% Imbalance

99%

99.9%

(b) Δ𝐸𝑂
Figure 5: (a) Δ𝐷𝑒𝑚𝑃 and (b) Δ𝐸𝑂 vs. dataset imbalance at varying levels of DP.

REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 308–318.
[2] John M Abowd. 2018. The US Census Bureau adopts differential privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2867–2867.
[3] Mário S Alvim, Miguel E Andrés, Konstantinos Chatzikokolakis, Pierpaolo Degano, and Catuscia Palamidessi. 2011. Differential privacy: on the tradeoff between utility and information leakage. In International Workshop on Formal Aspects in Security and Trust. Springer, 39–54.
[4] Devansh Arpit, Stanisław Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and et al. 2017. A Closer Look at Memorization in Deep Networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICML’17). JMLR.org, 233–242.
[5] Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. 2019. Differential privacy has disparate impact on model accuracy. In Advances in Neural Information Processing Systems. 15479–15488.
[6] Su Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. arXiv preprint arXiv:1608.08868 (2016).
[7] Su Lin Blodgett, Johnny Wei, and Brendan O’Connor. 2018. Twitter universal dependency parsing for African-American and mainstream American English. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1415–1425.
[8] Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. 2019. Deep learning with Gaussian differential privacy. arXiv preprint arXiv:1911.11607 (2019).
[9] Nicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlingsson, and Dawn Song. 2018. The Secret Sharer: Measuring Unintended Neural Network Memorization & Extracting Secrets. CoRR abs/1802.08232 (2018). arXiv:1802.08232 http://arxiv. org/abs/1802.08232
[10] Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. 2009. Differentially Private Empirical Risk Minimization. arXiv preprint arXiv:0912.0071 (2009). arXiv:0912.0071 [cs.LG]

[11] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and

Moni Naor. 2006. Our data, ourselves: Privacy via distributed noise generation. In

Annual International Conference on the Theory and Applications of Cryptographic

Techniques. Springer, 486–503.

[12] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-

brating noise to sensitivity in private data analysis. In Theory of cryptography

conference. Springer, 265–284.

[13] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas

Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case Study of

Personalized Warfarin Dosing. In Proceedings of the 23rd USENIX Conference on

Security Symposium (San Diego, CA) (SEC’14). USENIX Association, USA, 17–32.

[14] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in

supervised learning. In Advances in neural information processing systems. 3315–

3323.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual

Learning for Image Recognition. CoRR abs/1512.03385 (2015). arXiv:1512.03385

http://arxiv.org/abs/1512.03385

[16] Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed

Sharifi-Malvajerdi, and Jonathan Ullman. 2018. Differentially private fair learning.

arXiv preprint arXiv:1812.02696 (2018).

[17] Peter Kairouz, Jiachun Liao, Chong Huang, and Lalitha Sankar. 2019. Censored

and Fair Universal Representations using Generative Adversarial Models. arXiv

(2019), arXiv–1910.

[18] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2019. An empiri-

cal study of rich subgroup fairness for machine learning. In Proceedings of the

Conference on Fairness, Accountability, and Transparency. 100–109.

[19] Satya Kuppam, Ryan McKenna, David Pujol, Michael Hay, Ashwin Machanava-

jjhala, and Gerome Miklau. 2019. Fair decision making using privacy-protected

data. arXiv preprint arXiv:1905.12744 (2019).

[20] Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. 2018. Does

mitigating ML’s impact disparity require treatment disparity?. In Advances in

Neural Information Processing Systems. 8125–8135.

[21] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face

Attributes in the Wild. In Proceedings of International Conference on Computer

Vision (ICCV).

[22] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. 2016.

Communication-efficient learning of deep networks from decentralized data.

arXiv preprint arXiv:1602.05629 (2016).

[23] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram

Galstyan. 2019. A survey on bias and fairness in machine learning. arXiv preprint

arXiv:1908.09635 (2019).

[24] Michele Merler, Nalini Ratha, Rogerio S Feris, and John R Smith. 2019. Diversity

in faces. arXiv preprint arXiv:1901.10436 (2019).

[25] Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma,

Abhishek Singh, Ramesh Raskar, and Hadi Esmaeilzadeh. 2020. Privacy in Deep

Learning: A Survey. In ArXiv, Vol. abs/2004.12254.

[26] Ilya Mironov. 2017. Renyi Differential Privacy. CoRR abs/1702.07476 (2017).

arXiv:1702.07476 http://arxiv.org/abs/1702.07476

[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory

Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-

maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan

Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith

Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning

Library. In Advances in Neural Information Processing Systems 32, H. Wallach,

H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett (Eds.).

Curran Associates, Inc., 8024–8035. https://github.com/pytorch/

[28] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership Inference

Attacks Against Machine Learning Models. In IEEE Symposium on Security and

Privacy (S&P).

[29] Sahib Singh, Harshvardhan Sikka, Sasikanth Kotti, and Andrew Trask. 2020.

Benchmarking Differentially Private Residual Networks for Medical Imagery.

arXiv preprint arXiv:2005.13099 (2020).

[30] Stuart A. Thompson and Charlie Warzel. 2019. The Privacy Project: Twelve

Million Phones, One Dataset, Zero Privacy.

online accessed Febru-

ary 2020 https://www.nytimes.com/interactive/2019/12/19/opinion/location-

tracking- cell- phone.html.

[31] Berk Ustun, Alexander Spangher, and Yang Liu. 2019. Actionable recourse in

linear classification. In Proceedings of the Conference on Fairness, Accountability,

and Transparency. 10–19.

[32] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,

Hartwig Adam, Pietro Perona, and Serge Belongie. 2018. The inaturalist species

classification and detection dataset. In Proceedings of the IEEE conference on

computer vision and pattern recognition. 8769–8778.

[33] Zhifei Zhang, Yang Song, and Hairong Qi. 2017. Age progression/regression

by conditional adversarial autoencoder. In Proceedings of the IEEE conference on

computer vision and pattern recognition. 5810–5818.

