1

arXiv:2109.12970v1 [cs.LG] 27 Sep 2021

Deep Learning Based Resource Assignment for Wireless Networks
Minseok Kim, Hoon Lee*, Member, IEEE, Hongju Lee, and Inkyu Lee, Fellow, IEEE

Abstract—This paper studies a deep learning approach for binary assignment problems in wireless networks, which identiﬁes binary variables for permutation matrices. This poses challenges in designing a structure of a neural network and its training strategies for generating feasible assignment solutions. To this end, this paper develop a new Sinkhorn neural network which learns a non-convex projection task onto a set of permutation matrices. An unsupervised training algorithm is proposed where the Sinkhorn neural network can be applied to network assignment problems. Numerical results demonstrate the effectiveness of the proposed method in various network scenarios.
Index Terms—Deep learning, Sinkhorn operator, assignment problem
I. INTRODUCTION
Assignment problems which determine matching between two different quantities have been prevailed in various networking scenarios. Popular examples are subcarrier allocation [1], user-cell association [2], and task ofﬂoading [3]. Several algorithms have been proposed for handling such assignment problems. In particular, the Hungarian algorithm [4] was introduced as a globally optimal solver for linear sum assignment problems (LSAPs). Mixed integer programs (MIPs) solvers, e.g., Mosek and Gurobi [5], can address convex assignment problems. However, the technique typically invokes special properties for objective functions such as linearity and convexity, and involves high computational complexity for executing iterative calculations.
Recently, deep learning (DL) based optimization methods have been adopted as promising tools in wireless networks for reducing the computational burden of traditional iterative algorithms [6]. A supervised learning approach which trains deep neural network (DNNs) to memorize solutions of existing optimization techniques has been investigated for power control problems [6]. The DNNs achieve near-optimal performance with reduced execution time. However, the supervised learning invokes high complexity for a data collection step for securing numerous known optimal solutions for training. Therefore, it can be applicable to simple networking scenarios where efﬁcient optimization algorithms are available. To resolve this issue, an unsupervised learning concept [7] was proposed where DNNs are designed to model optimal solution computation rules. The effectiveness of the unsupervised DL strategy has been veriﬁed for optimizing non-convex problems in various wireless systems [8]. Although the unsupervised DL does not require the information of the optimal computation strategy.
There have been lots of efforts on the development of low-complexity DL methods for assignment problems. In
This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT) under Grant 2017R1A2B3012316 (Corresponding authors : Inkyu Lee and Hoon Lee)
M. Kim, H. Lee, and I. Lee are with the School of Electrical Engineering, Korea University, Seoul, Korea (e-mail: {msk1005, honglee2335, inkyu}@korea.ac.kr).
*H. Lee is with the Department of Smart Robot Convergence and Application Engineering and the Department of Information and Communications Engineering, Pukyong National University, Busan 48513, Korea (e-mail: hlee@pknu.ac.kr).

particular, the LSAP which formulates a minimization task of the linear network cost function has been recently solved via the supervised DL approach [9]. An assignment problem was decomposed into several sub-assignment problems, which are regarded as classiﬁcation tasks that ﬁnd one-way matching of a certain item. Individual DNNs are dedicated to solving each subproblem. The optimum solution generated by the Hungarian algorithm facilitates supervised learning of multiple DNNs simultaneously. The combinatorial nature of the assignment problems calls for a feasible solution to be structured in a permutation matrix format. However, such a strict feasibility condition would not be guaranteed by the conventional supervised DL technique [9]. To handle this issue, a post-processing method can be adopted to recover permutation matrices, but it results in performance loss of the trained DNN. This challenge poses in existing unsupervised DL-based optimization approaches since they are conﬁned to the optimization of continuous-valued variables, but not permutation matrices. Therefore, it is necessary to develop a new learning structure that directly identiﬁes feasible assignment solution as an output of a DNN.
This paper proposes an unsupervised DL framework for generic non-convex assignment problems. Existing convex MIP solvers can only address a certain instance of assignment tasks. On the contrary, this paper aim at identifying an efﬁcient mapping, i.e., a DNN, that generates assignment solutions for arbitrary problem instances. As a result, it can be applied to general non-convex assignment problems. The major challenge is to construct a DNN which always generates feasible solutions for arbitrary network assignment tasks. To overcome this difﬁculty, this paper introduce a novel Sinkhorn neural network (SNN). The output layer of the SNN is designed to solve nonconvex projection problems onto the set of permutation matrices, thereby ensuring the feasibility as assignment problem solvers. The SNN is trained in an unsupervised manner without knowing the optimum solutions. Numerical results validate the efﬁcacy of the proposed DL methods in practical network assignment scenarios. It is veriﬁed that the proposed SNN approach achieves almost identical performance to existing algorithms with reduced complexity.
II. PROBLEM DESCRIPTION
Consider network assignment problems which determine optimal matching policies between two distinct wireless nodes, in particular, bipartite matching from M nodes in M = {1, · · · , M } to N nodes N = {1, · · · , N }. This can be interpreted as associations among base stations (BSs), user equipments (UEs), and resource blocks. The balanced case M = N is discussed ﬁrst and it will be extended to a general unbalanced case of N > M later. The target of the balanced assignment problem with N = M is to identify an one-to-one allocation strategy among nodes.
Let xij ∈ {0, 1} as a binary variable indicating the assignment state of nodes i ∈ M and j ∈ N , i.e., xij = 1 if entity i

2

is assigned to entity j and xij = 0 otherwise. Each node can connect to only one node, which imposes the constraints as

N

N

i=1 xij = 1, ∀j, and j=1 xij = 1, ∀i, (1)

where xij collectively form a binary assignment matrix X {xij, ∀i, j} ∈ {0, 1}N×N . It is inferred from (1) that a feasible assignment solution X should be a permutation matrix.
Let H {hij, ∀i, j} ∈ RN×N be the input of the assignment problem describing wireless propagation environments.
In particular, hij represents the connection status between nodes i and j, e.g. channel coefﬁcients, which determine the
cost of the assignment xij . A generic network assignment problem can be described as a minimization of the network cost function f (X, H), possibly non-convex, subject to the constraint on X being a permutation matrix. It is written by

min f (X, H),

(2)

X∈PN

where PN is the set of all N -by-N permutation matrices.

The assignment problem (2) prevails in a design of wireless

networks. Existing convex MIP solvers such as the branch-

and-bound algorithm [10] cannot handle the non-convex cost f (X, H). The Hungarian algorithm has been known as the
optimum algorithm for the LSAP, which is a special case of (2) with the afﬁne cost function f (X, H) = tr(HT X), but it
cannot address generic non-convex cost functions. The dual

function of (2) would not be available due to the non-convex

cost function, thereby making the Lagrange duality method

intractable.

In this paper, we propose a DL approach to solve the generic

non-convex assignment problem (2) that can be applicable

to various networking setups. A key idea is to exploit the

“learning to optimize” concept [6] which replaces unknown

optimization processes with properly trained DNNs. The op-

timization procedure of (2) can be viewed as an identiﬁcation task of a mapping from the network states H to the permutation matrix X. Such a mapping is implemented by a R-layer fully-connected DNN F (·; θ) with a parameter set θ.

The input and the output of the DNN are modeled h vec(H) and x vec(X), respectively, where vec(·) represents vectorization of a matrix. Denoting Kr as the dimension of the r-th layer, the computation of the DNN is expressed as

F (h; θ) = σR(WR × · · · × σ1(W1h + b1) + · · · + bR) (3)

where an element-wise function σr(·) is an activation function at layer r (r = 1, · · · , R) and Wr ∈ RKr×Kr−1 and br ∈ RKr indicate the weight matrices and the bias vectors, respectively. Here Wr and br collectively form the trainable parameter set θ {Wr, br, ∀r}. In the training step, the parameter θ is identiﬁed such that the DNN output x = F (h; θ) minimizes the cost function f (X, H) in (2) for a given network state H while satisfying the permutation constraint X ∈ PN . However, since conventional training algorithms were developed for unconstrained formulations, the
feasibility of the DNN output cannot be guaranteed.
The recent work [9] presented a DL method for tackling the
LSAPs. To ensure the feasibility, a supervised learning strategy
was adopted which forces the DNN to yield the optimal
permutation matrix obtained by the Hungarian algorithm. Nev-
ertheless, the DNN would fail to generate feasible assignment

matrices for unseen test samples. This leads to unintended collisions in matching between jobs and workers. Thus, a post-processing was added in the test step which reassigns conﬂicting jobs to a worker with the lowest cost value in a heuristic way. Although the feasibility may be secured, it might incur a loss of the optimality since the post-processing was not involved in the training step. Furthermore, due to the supervised learning concept, the method in [9] can only be applicable to simple assignment problems having efﬁcient solvers. Therefore, it is necessary to develop a new DNN structure which is able to capture the non-convex constraint of generic assignment tasks with arbitrary network cost functions.

III. PROPOSED DEEP LEARNING APPROACH

This section proposes a SNN for solving the assignment

problem (2). It is desired to determine the output activation

σR(·) of the DNN F (·; θ) which always generates proper

permutation matrices for any given inputs H. To this end,

a novel activation function is developed to carry out non-

convex projections onto the set of permutation matrices PN . Let a ∈ RN2 be the output vector of the DNN with given

input H. Then, a is the input vector to the output activation

σR(·) at the end of the DNN.

Firstly, a is reshaped into an N -by-N matrix A, and then

pass to the output activation X = σR(A) which solves the

non-convex projection problem as

σR(A) arg max tr(AT X).

(4)

X∈PN

Problem (4) determines a permutation matrix that maximizes

the afﬁnity between the output feature A. The output activation

(4) can always satisfy the conditions (1) for an arbitrary

input H. However, the combinatorial nature of (4) invokes

a selection process which nulliﬁes the gradient with respect to

A, posing challenges for gradient-based training algorithms.

To address this issue, a soft approximation of the non-

convex projection (4) is introduced. The key idea is to exploit

the concept of the Sinkhorn operation [11] which has been

originally designed for obtaining a doubly stochastic matrix

(DSM). The DSM D ∈ [0, 1]N×N is deﬁned as a square

matrix whose (i, j)-th elements dij satisfy the constraint in

(1) as

N i=1

dij

=

1

and

N j=1

dij

=

1

with

0

≤

dij

≤

1.

Thus, the DSM can be regarded as a continuous relaxation

of a permutation matrix. The Sinkhorn operator denoted by

S(A) calculates a projection of A into a convex set containing

DSMs. The corresponding problem can be written as

S(A) = arg max tr(AT D),

(5)

D∈DN

where DN stands for the set of N -by-N DSMs.

A solution of (5) can be found by iteratively normaliz-

ing rows and columns of A. The row-wise normalization

R(A) {aij, ∀i, j} and the column-wise normalization

C(A) {aij, ∀i, j} are respectively given with

aij = aij and aij = aij ,

(6)

N k=1

aik

N k=1

akj

where aij is the (i, j)-th element of A.

Then, the computation of the Sinkhorn operator at the m-th iteration Sm(A) is written by

Sm(A) = C(R(Sm−1(A))),

(7)

3

where S0(A) exp(A) denotes an initial Sinkhorn operator with exp(·) being an element-wise exponential function. It has
been revealed in [11] that iterating (7) converges to the optimal point of (5), i.e., S(A) limm→∞ Sm(A). In the following proposition, the relationship between the Sinkhorn operator
S(A) and permutation matrices is provided. Proposition 1. Suppose that elements of A ∈ RN×N are

independent and their distributions are absolutely continuous in the Lebesgue measure. Then, limτ→∞ S(τ A) almost surely provides a permutation matrix. Proof. The Birkhoff theorem [12] states that any DSM is

given as a convex combination of permutation matrices, i.e., the convex hull of PN becomes a polytope DN generated by DSMs. Therefore, vertices of DN form permutation matrices. It has been veriﬁed from the Lagrange duality method that in the extreme case τ → ∞, S(τ A) converges to a vertex of a feasible space DN , i.e., a permutation matrix. Please refer to [11, Theorem 1] for the detailed proof.

Proposition 1 implies that with a sufﬁciently large τ , the Sinkhorn operator S(τ A) can identify a permutation matrix
nearest to an arbitrary square matrix A. With the initialization S0(τ A) = exp(τ A), the normalization in (6) is interpreted as

a scaled softmax function deﬁned as

softmax(i, zj, τ ) = exp(τ zi) .

(8)

j=i exp(τ zj )

As τ gets larger, (8) approaches the argmax function producing

an one-hot vector, which is an all-zero vector except for

the maximum index being replaced by one. The Sinkhorn

operator repeatedly applies the scaled softmax (8) to each row

and column of A so that the output becomes a permutation

matrix whose rows and columns have a single one with zeros elsewhere. Consequently, the Sinkhorn operator S(τ A) with a large τ solves the non-convex problem (4) efﬁciently.

Based on these observations, the output activation function σR(·) of the DNN F (·; θ) is designed as the Sinkhorn operator σR(A) = S(τ A). Thanks to the continuous-valued computations (6), the Sinkhorn operator S(τ A) has valid gradients,

meaning that existing gradient-based training algorithms can be applied to optimize the SNN parameter θ. The parameter τ controls the quality of the approximation S(τ A) ≃ X ∈ PN . A large τ leads to a high approximation accuracy. However, the gradient of S(τ A) may explode as τ gets larger, thereby prohibiting an efﬁcient training of the DNN via gradient-

based algorithms. We thus need to choose τ carefully through

a validation process to achieve a good tradeoff between an

approximation accuracy and training performance.

A. Training and Implementation

A training strategy is presented for the proposed SNN. By substituting X = F (h; θ) into (2), it follows

min f (F (h; θ), H)

(9)

θ

where the constraint F (h; θ) ∈ PN can be ignored since the SNN always generates permutation matrices. Compared

to the original formulation (2), the optimization variable X

now turn out to be the SNN parameter θ. Hence, (9) becomes

a training task which determines an efﬁcient SNN for handling

assignment problems with an arbitrary network observation H.

To solve (9), a training dataset T

{H} containing

numerous realizations of H is ﬁrst prepared. Then, the SNN

10

10

8

8

Affinity Affinity

6

6

4

4

2

2

0 0 4 8 12 16 20 24 28 32 36 40

0 0 4 8 12 16 20 24 28 32 36 40

Iterations

Iterations

(a) τ = 20

(b) τ = 100

Fig. 1. Convergence behavior of the output activation for various K and τ .

is trained to minimize the cost function averaged over the

training dataset. This can be solved by gradient-based learning

algorithms, e.g., the mini-batch stochastic gradient descent

(SGD) [13], which iteratively updates θ for minimizing the

cost function evaluated over a sample dataset called the minibatch set. The SNN parameter θ[t] at the t-th training epoch

of the SGD algorithm is calculated as

θ[t] = θ[t−1] − η

∇f (F (h; θ), H)

(10)

|B| H∈B

where η stands for the learning rate, B ⊂ T is the mini-batch set, and ∇ indicates the gradient operator.
The training algorithm in (10) can be implemented in an unsupervised manner without the knowledge of the optimal solution of the original assignment problem (2). Notice that the conventional supervised DL approach [9] needs to collect the optimal assignment matrices, and thus it can only be applied to the LSAP where efﬁcient algorithms for (2) are available. In contrast, the proposed unsupervised DL framework (10) enables the SNN to learn arbitrary assignment tasks. The trained SNN parameter θ is realized in any computational unit, e.g., BSs. Then, a solution to an unseen input H can be obtained by linear calculations (3). The complexity is given by O( Rr=1KrKr−1) where K0 = KR = N 2 is the length of the input h and the output x. When the hidden dimensions are independent of N , the corresponding complexity becomes O(N 2). Assuming L iterations of the Sinkhorn operations (7), the overall complexity equals O(LN 2), which is lower than that of the Hungarian algorithm given by O(N 3).
In practice, the Sinkhorn operator invokes a large number of iterations to ﬁnd an exact permutation matrix. To improve the convergence speed, K consecutive Sinkhorn operators are applied for constructing the output activation σR(·), where each Sinkhorn operator lasts KL iterations, resulting in total L iterations. Fig. 1 exhibits the convergence trends of the output activation for various K and τ with N = 6 and 10. The evaluation metric is deﬁned as the afﬁnity to the nearest permutation matrix maxX∈PN tr(S(τ A)T X). Achieving the upperbound value N indicates that an exact permutation matrix is found. A single Sinkhorn operator cannot provide an exact permutation matrix. On the contrary, a cascaded integration of K = 4 Sinkhorn operators solves the nonconvex projection (4) within 20 iterations. This implies that the output activation with multiple Sinkhorn operators signiﬁcantly enhance the approximation accuracy of the non-convex projection problem (4). Increasing K ﬁrst helps the output activation converge quickly, but adopting too many Sinkhorn operators may degrade the convergence speed. From the ﬁgure,

4

it is concluded that K = 4 is an efﬁcient choice for all simulated N . A large τ can improve the feasibility for the constraint X ∈ PN . However it incurs the exploding gradient issue, thereby leading to slow convergence. It has been found that τ = 20 achieves a good tradeoff between the feasibility and the convergence speed.

B. Extension to the Unbalanced Case

Now, the proposed SNN approach is extended for the
unbalanced assignment problems with N > M . Job j (j = 1, · · · , M ) should be allocated to one of N workers, and worker i (i = 1, · · · , N ) can handle at most one job. Since
N > M , some workers may not have a job. The assignment matrix X {xij , ∀i, j} ∈ {0, 1}N×M and the cost matrix H {hij, ∀i, j} ∈ RN×M now become non-square matrices.
In this conﬁguration, the constraints in (1) are reﬁned as

N

M

i=1 xij = 1, ∀j, and j=1 xij ≤ 1, ∀i. (11)

The proposed SNN method can tackle the unbalanced assignment constraints (11) with simple modiﬁcations. The SNN takes a non-square cost matrix as an input feature and produces a square output matrix denoted by X˜ ∈ {0, 1}N×N. Since each column of the permutation matrix X˜ have a single one, removing arbitrary N − M columns of X˜ makes the resultant non-square matrix of size N -by-M feasible for (11). For simplicity, the last N −M columns of X˜ are discarded, and the corresponding matrix acts as the non-square assignment solution X. During the training, the cost function f (H, X) is evaluated with the modiﬁed non-square assignment matrix X. The associated backpropagation procedure becomes inactive for the discarded variables. As a result, the proposed SNN method can be readily applied to solve the unbalanced cases.

IV. NUMERICAL RESULTS
This section examines the effectiveness of the proposed DL
method in various networking scenarios. The rectiﬁed linear
unit (ReLU) activation is employed for all hidden layers. The hyperparameters are given as τ = 20, K = 4, and L = 20. The learning rate and the mini-batch size are ﬁxed as η = 10−3 and |B| = 2000, respectively. The number of training iterations is set to 106, resulting in total 2 × 109 training samples. The SNN parameter with the minimum cost over 104 validation
samples is chosen as the best model. Finally, the performance of the trained SNN is examined over 104 test samples.

A. Linear Sum Assignment Problem The LSAP is considered ﬁrst which can be optimally solved
via the Hungarian algorithm [4]. In this example, we examine the performance the proposed SNN with the optimal solution and the conventional DL method [9] in the balanced and unbalanced cases. The SNN is constructed with 3 hidden layers each of which has the dimension 288, 144 and 80, respectively. Elements of the cost matrix H are uniformly distributed within [1, 100].
Table I presents the average degradation of the cost value compared to the optimal Hungarian algorithm in the balanced and unbalanced cases. The system with matching tasks from

TABLE I

Average degradation of cost function.

(M, N )

(4,4) (8,8) (2,4)

Proposed Supervised [9]

0.27% 1.71%

0.85% 22.65%

0.17% 1.78%

(4,8)
0.62% 23.23%

Fig. 2. DNN structure for cell association problem.

M nodes to N nodes is denoted by (M, N ). For comparison, the performance of the supervised DL approach [9] is also evaluated. A performance loss of the proposed SNN is less then 1 % for all conﬁgurations, whereas the supervised DL exhibits a high loss especially for a large N . This stems from the heuristic post-processing steps in [9] that recover the structure of permutation matrices. The unbalanced case shows similar performance to that of the balanced one with the same N . This is because the unbalanced cases are tackled by producing a square output matrix of size N -by-N . The number of ﬂoating point operations of the proposed SNN is obtained as 816N 2 − 20N + 105984, which is typically lower than that of the learning method in [9] which is given by 64N 3 + 512N 2 + 18432N . In addition, at N = 8, the CPU execution time of the proposed scheme is 65 times faster than that of the Hungarian algorithm. Thus, it is concluded that the proposed SNN-based unsupervised DL framework is more suitable for handling the LSAP both in terms of the performance and the computational complexity.

B. Cell association problem
Consider a more practical scenario where N single antenna BSs communicate with N single antenna UEs, and the BSs are assumed to support only one UE. A joint optimization of the transmit power at BSs and cell association among BSs and UEs is addressed. The association state between BS i and UE j is denoted by a binary variable xij ∈ {0, 1}, whereas the transmit power of BS i is expressed as a continuous variable pi ∈ [0, Pi] with Pi being the maximum power budget. Let hij be the channel gain from BS i to UE j. When UE j is supported by BS i, the achievable rate is written by

rij (p) = log 1 +

pihij

,

(12)

σ2 + k=i pkhkj

where p {pj, ∀j} and σ2 accounts for the noise power. The

sum rate maximization problem is then formulated as

N

N

max

xij rij (p), s.t. pi ∈ [0, Pi], ∀i. (13)

X∈PN ,p j=1 i=1

The non-convex MIP in (13) jointly optimizes continuousvalued power variables pi and binary assignment variables xij .
As illustrated in Fig. 2, a DNN for tackling (13) is constructed with three different neural networks. The channel matrix H {hij, ∀i, j} is ﬁrst processed by a shared network, and its output is fed to a SNN and another DNN for power control. The SNN identiﬁes a cell association matrix X, while the power control network calculates the power allocation solution p. The sigmoid activation is adopted at the output layer for satisfying the power budget constraint.
For evaluation purposes, a two-tier heterogeneous network is considered where BS 1 is regarded as a macro-cell BS

5

20

18

Average sum rate [nats/sec/Hz]

16

14

12

10

8

6

4

MM algorithm

2

Proposed

Hungarian

0

3

4

5

6

Fig. 3. Average sum-rate performance with respect to N .
located at the center cell of radius 1 km. The remaining N − 1 BSs act as small-cell BSs that encircle the macroBS with radius 500 m. UEs are randomly deployed with the minimum distance 10 m from BSs. We adopt the path-loss model 120.9+37.6 log10d dB with d being the distance from a BS and an UE. The Rayleigh fading is considered for the small-scale channel gains. The standard deviation of the lognormal shadowing is ﬁxed as 8 dB, and the noise power is set to σ2 = −114 dBm. The shared network consists of two layers each with 576 and 432 dimensions. Three hidden layers are employed for the SNN whose output dimensions are given as 360, 216 and 144. The power control network has two hidden layers having 288 and 144 dimensions.
For comparison, a local optimum solution of (13) is obtained by the majorization minimization (MM) algorithm [14] which addresses the non-convex objective function via a sequence of convex approximations. At each iteration, an approximated convex MIP is tackled by the Gurobi solver [5]. As a result, both the association and power control solutions are jointly computed. The Hungarian scheme carries out an alternating optimization between the association and the power control. The power control solution is optimized using the weighted minimum-mean-square-error (WMMSE) algorithm [15] which yields a local optimum solution for a given association obtained with the Hungarian algorithm.
Fig. 3 shows the average sum rate performance of various schemes with respect to the number of UEs N . Three different scenarios are considered according to the level of the transmit power constraints. In the ﬁgure, (PM , PS) denotes the system with power budgets of PM & PS at the macro-BS and smallcell BSs, respectively. The performance gain of the proposed SNN approach over the Hungarian algorithm gets larger as the network size N and the power budget (PM , PS) increase. This implies that the joint optimization of transmit power levels and cell associations becomes signiﬁcant for a larger MIP with more optimization variables and enlarged feasible spaces. At all simulated (PM , PS) and N , the proposed SNN achieves the almost identical performance to that of the MM algorithm. This veriﬁes the effectiveness of the SNN as a non-convex MIP solver for arbitrary problem size.
Table II presents the time complexity of various schemes by evaluating the CPU running time. For fair comparison, all schemes are realized by Python with Numpy library. Note that the batch operations are not involved for the implementation of the SNN. It is seen that the proposed SNN approach can signiﬁcantly reduce the execution time compared to the

(PM , PS )
MM Hungarian Proposed

TABLE II Average CPU execution

N =3

(20, 10)

(43, 33)

4.37 × 103 7.27 × 10

4.89 × 103 1.17 × 102

5.19

time [sec]

N =6

(20, 10)

(43, 33)

1.43 × 104 1.21 × 102

1.94 × 104 2.50 × 102

6.11

baseline method while achieving the almost identical per-
formance to the MM algorithm. The execution time of the baseline schemes quickly increases with N and (PM , PS). In general, more repetitions are required as the size of a
problem N and feasible set speciﬁed by PM and PS grow. In contrast, the time complexity of the proposed method is
independent of the power constraints since the computations
of the trained SNN is dominated by its structure, e.g., the
number of layers. As discussed before, the SNN would need
slightly more calculations as N gets larger. From these results,
it is concluded that the proposed approach is a promising non-
convex MIP solver which achieves a good trade-off between
the performance and the complexity.
V. CONCLUSIONS
This work has studied a DL framework for handling non-
convex assignment problems in wireless networks. To address
combinatorial binary constraints, an SNN architecture has
been proposed which carries out non-convex projections onto
permutation matrix spaces. The viability of the proposed
DL approach has been demonstrated in various networking
scenarios. REFERENCES
[1] H. Kim, H. Lee, M. Ahn, H. Kong, and I. Lee, “Joint subcarrier and power allocation methods in full duplex wireless powered communication networks for OFDM systems,” IEEE Trans. Wireless Commun., vol. 15, pp. 4745–4753, Jul. 2016.
[2] S. H. Lee, M. Kim, H. Shin, and I. Lee, “Belief propagation for energy efﬁciency maximization in wireless heterogeneous networks,” IEEE Trans. Wireless Commun., vol. 20, pp. 56–68, Jan. 2021.
[3] N. Kiran, C. Pan, S. Wang, and C. Yin, “Joint resource allocation and computation ofﬂoading in mobile edge computing for SDN based wireless networks,” J. Commun. Netw., vol. 22, pp. 1–11, Feb. 2020.
[4] H. W. Kuhn, “The Hungarian method for the assignment problem,” Naval Res. Logist., vol. 2, pp. 83–97, Mar. 1955.
[5] Gurobi Optimization, Inc, “Gurobi optimizer reference manual,” 2021. [6] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos,
“Learning to optimize: Training deep neural networks for interference management,” IEEE Trans. Signal Process., vol. 66, pp. 5438–5453, Oct. 2018. [7] W. Lee, M. Kim, and D. Cho, “Deep power control: Transmit power control scheme based on convolutional neural network,” IEEE Commun. Lett., vol. 22, pp. 1276–1279, Jun. 2018. [8] H. Lee, S. H. Lee, and T. Q. S. Quek, “Deep learning for distributed optimization: applications to wireless resource management,” IEEE J. Sel. Areas Commun., vol. 37, pp. 2251–2266, Oct. 2019. [9] M. Lee, Y. Xiong, G. Yu, and G. Y. Li, “Deep neural networks for linear sum assignment problems,” IEEE Wireless Commun. Lett., vol. 7, pp. 962–965, Dec. 2018. [10] P. M. Narendra and K. Fukunaga, “A branch and bound algorithm for feature subset selection,” IEEE Trans. Comput., vol. 26, pp. 917–922, Sep. 1977. [11] G. Mena, D. Belanger, S. Linderman, and J. Snoek, “Learning latent permutations with Gumbel-Sinkhorn networks,” in Proc. Int. Conf. Learn Represent. (ICLR), 2018. [12] G. Birkhoff, “Three observations on linear algebra,” Univ. Nac. Tucuma´n. Revista A., vol. 5, pp. 147–151, 1946. [13] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, pp. 436–444, May 2015. [14] H. Lee, C. Song, J. Moon, and I. Lee, “Precoder designs for MIMO Gaussian multiple access wiretap channels,” IEEE Trans. Veh. Technol., vol. 66, pp. 8563–8568, Sep. 2017. [15] Q. Shi, M. Razaviyayn, Z. Luo, and C. He, “An iteratively weighted MMSE approach to distributed sum-utility maximization for a MIMO interfering broadcast channel,” IEEE Trans. Signal Process., vol. 59, pp. 4331–4340, Jul. 2011.

