arXiv:2007.12085v3 [cs.SD] 30 Oct 2020

Augmentation adversarial training for self-supervised speaker recognition
Jaesung Huh1, Hee Soo Heo2, Jingu Kang2, Shinji Watanabe3, Joon Son Chung2 1Visual Geometry Group, University of Oxford, UK 2Naver Corporation, South Korea 3Johns Hopkins University, Baltimore, USA
Abstract
The goal of this work is to train robust speaker recognition models without speaker labels. Recent works on unsupervised speaker representations are based on contrastive learning in which they encourage within-utterance embeddings to be similar and across-utterance embeddings to be dissimilar. However, since the withinutterance segments share the same acoustic characteristics, it is diﬃcult to separate the speaker information from the channel information. To this end, we propose an augmentation adversarial training strategy that trains the network to be discriminative for the speaker information, while invariant to the augmentation applied. Since the augmentation simulates the acoustic characteristics, training the network to be invariant to augmentation also encourages the network to be invariant to the channel information in general. Extensive experiments on the VoxCeleb and VOiCES datasets show signiﬁcant improvements over previous works using self-supervision, and the performance of our self-supervised models far exceeds that of humans.
1 Introduction
Speaker recognition is the ability to identify or verify a speaker’s identity based on their voice. It has gained popularity in biometric authentication due to its easy accessibility and non-invasive nature.
Although there is a large body of recent literature on speaker recognition using deep neural network models [21, 17, 44, 41, 9], the overwhelming majority of these are based on the supervised learning framework. The availability of new large-scale datasets [27, 11, 24] combined with powerful neural network models have facilitated fast progress on many popular tasks within speaker recognition, but there are many challenges to extending this strategy to every application. For instance, the cost of annotating a new dataset can be prohibitively expensive and the handling of sensitive biometric data can lead to privacy issues. The task of speaker veriﬁcation is also very diﬃcult for humans, resulting in inaccurate annotations in the absence of visual information.
On the other hand, there are many resources that can be used to learn representations, but have not been used due to the lack of annotations. For these reasons, unsupervised and self-supervised learning have recently received a growing amount of attention in order to leverage the abundant data available.
Existing literature on self-supervised learning of representations can be divided into two strands: generative or discriminative. Generative approaches learn representations by reconstructing the input data [18] or predicting withheld parts of the data, such as inpainting missing part of images [29] and colourising RGB images from only grey-scale images [45]. However, the element-wise generation is computationally expensive and is not necessary for representation learning.
Of relevance to our work is the second strand that learns discriminative representations directly, often using metric learning-based objectives. In particular, approaches based on contrastive learning in the
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

latent space have shown to learn eﬀective representations by taking within-class inputs from multiple views [38, 25, 4, 8] or modalities [12, 3, 34, 26, 13] of the same input data.
These strategies have been applied to speech signals in order to enable unsupervised learning of speaker representations. [31] samples two speech segments from same utterance and trains the network to maximise the mutual information between them. A key diﬀerence between supervised metric learning and the proposed contrastive learning framework is that segments from a single utterance have the same noise and reverberation characteristics. This eﬀect has been partially mitigated using data augmentation in [19], which mimics the strategy of [8] that has shown promising performance in vision tasks.
A key challenge in speaker recognition is to learn embeddings that are speaker-discriminative, but invariant to all other spurious variations. Inspired by the work on domain adaptation using adversarial training [16, 39], recent works have used this framework to improve generalisation between languages [33, 5, 6] and between datasets [5, 43]. In particular, [9] and [22] have proposed channel invariant training for speaker recognition by introducing a confusion loss between same speaker segments from across and within an utterance.
Within the contrastive learning framework, it is diﬃcult to obtain same speaker segments from across diﬀerent utterances, but one can simulate diﬀerent environments using data augmentation. To this end, we propose Augmentation Adversarial Training (AAT) to explicitly train speaker-discriminative and environment-invariant embeddings without speaker labels. Since data augmentation simulates the channel environment, training the network to be invariant to augmentation also encourages the network to be invariant to the channel information in general. Our experiments using the contrastive learning framework demonstrate the eﬀectiveness of the proposed strategy. The proposed model outperforms all existing self-supervised methods on the VoxCeleb1 test set by a large margin, and we also show that the speaker veriﬁcation performance of our model far exceeds that of humans.
2 Augmentation Adversarial Training
This section describes the proposed self-supervised training strategy. We describe the batch formation for training, then introduce the contrastive learning framework which samples two non-overlapping speech segments from each utterance and applies data augmentation. We then propose Augmentation Adversarial Training (AAT), which exploits an augmentation classiﬁer in addition to speaker embedding extractor. Training is performed in turns to remove channel information from the speaker representation.
2.1 Batch formation
Each mini-batch  contains randomly selected utterances 1, 2, ..., out of set. For each utterance , we sample two non-overlapping speech segments, ,1 and ,2, both of which are time-domain
signals. Under the assumption that every utterance contains only one person’s speech, ,1 and ,2 are from same identity.
2.2 Contrastive training
Since ,1 and ,2 are sampled from the same utterance, the channel characteristics of the two segments are likely to be identical. As a result, using the standard metric learning methods, speaker embedding extractor might learn the similarity of the environment between the two segments, not only the speaker characteristics. Therefore, data augmentation such as additive noise or room impulse response (RIR) is added to simulate diﬀerent channel characteristics.
Speciﬁcally, for each two non-overlapping segments ,1 and ,2 (1 ≤ ≤ ), -dimensional speaker embeddings , , are computed as follows:

, , = ( , ∗ , + , ) ( , ) ∈ {(1, 1), (2, 2)}

(1)

where , and , are randomly selected from RIR ﬁlters and noise dataset. (⋅) is the speaker embedding extractor and is trained with speaker loss functions. ∗ is the notation for convolution.
Therefore, , , refers to the embedding of -th segment of -th utterance, with augmentation type .

2

Prototypical loss. Prototypical network has been introduced for few-shot learning and has been shown to perform well in speaker veriﬁcation [42, 2, 10]. In our case, ,1,1 is a query and ,2,2 is a prototype of size 1 support set. We compute the negative of the L2 distance as follows:
( , ) = −‖ − ‖22 (2)
In the angular variant of the prototypical loss (AP) [10], the distance function is replaced by a cosine similarity (⋅, ⋅) combined with learnable weight > 0 and bias :

( , )= × ( , )+

(3)

where cosine similarity between and is deﬁned as an inner product of normalised vectors:

⋅

( , )=

(4)

‖ ‖‖ ‖

Cross entropy loss with a log-softmax function is used to minimise the distance between segments from same utterance and maximise the distance between diﬀerent utterances.

spk = − 1 ∑ log exp( ( ,1,1, ,2,2)) (5) =1 ∑ ′=1 exp( ( ,1,1, ′,2,2))
Contrast to supervised metric learning, it is not guaranteed that all are from diﬀerent speakers. If the batch size is small relative to the total number of speakers and well-shuﬄed, it can be expected that most of the utterances in a batch are from diﬀerent speakers.

2.3 Augmentation Adversarial Training
Data augmentation methods help the learnt embeddings to be more robust to channel variance, however do not explicitly remove the information from the embeddings. Since the augmentation methods simulate diﬀerent channel environments, training the embeddings to be invariant to the augmentation also encourages the embeddings to be channel-invariant. Here, we propose Augmentation Adversarial Training (AAT) that penalises the ability to predict the augmentation in order to prevent the speaker embedding extractor from learning the channel information. The overview of this training method is in Figure 1.
In addition to speaker representations ,1,1 and ,2,2, the third representation is extracted. The third representation ,2,1 comes from the second segment ,2. We apply same RIR ﬁlter ,1 and additive noise ,1 as the ﬁrst, which is illustrated in left ﬁgure of Figure 1.

, , = ( , ∗ , + , ) ( , ) ∈ {(1, 1), (2, 1), (2, 2)}

(6)

Then, discriminator training phase and embedding training phase are performed alternately, as explained below.

Discriminator training. In this step, we train the augmentation classiﬁer . The assumption is that
,1,1 and ,2,1 share the same channel characteristic, while ,1,1 and ,2,2 have diﬀerent characteristics. We generate two types of input per each mini-batch, ,1,1 ⧺ ,2,1 and ,1,1 ⧺ ,2,2, where ⧺ indicates concatenation of vectors. Since , , is -dimensional vector, both of the vectors’ dimensions are 2 . The resultant batch size for training is 2 , inputs of ,1,1 ⧺ ,2,1 and another inputs of
,1,1 ⧺ ,2,2. The network is trained to classify whether two inputs are from the same channel by using binary cross entropy loss. In this step, the gradient does not ﬂow to the speaker embedding
extractor. The loss function dis can be formulated as below where (⋅) is a sigmoid function.

dis = − 21 ∑ log( ( ( ,1,1 ⧺ ,2,1))) + log(1 − ( ( ,1,1 ⧺ ,2,2))) (7)
=1
3

...
...
Same segment Same augmentation
...

Batch

Utt 1 Utt 2

1,1,1

1,2,1

1,2,2

2,1,1

2,2,1

2,2,2

Discriminator training
Binary cross-entropy
Augmentation classifier
Gradient reversal layer

Embedding training

Prototypical Loss

1,1,1

1,2,2

2,1,1

2,2,2

Binary cross-entropy
Augmentation classifier

N,1,1

N,2,2

Gradient reversal layer

Utt N

N,1,1 N,2,1 N,2,2

Update weights Not update weights

Speaker Embedding
Embedding extractor

Concat pairs
1,1,1 ⧺ 1,2,1 1,1,1 ⧺ 1,2,2 N,1,1 ⧺ N,2,2

Speaker Embedding
Embedding extractor

Concat pairs
1,1,1 ⧺ 1,2,1 1,1,1 ⧺ 1,2,2 N,1,1 ⧺ N,2,2

Figure 1: Overview of the training strategy. The index notation for the inputs and the embeddings are consistent with the equations, i.e. , , refer to -th segment of -th utterance, with augmentation type . Best seen in colour.

Embedding training. In this step, we update the weights of the speaker embedding extractor . While training with ,1,1 and ,2,2 similar to Section 2.2, we also apply Augmentation Adversarial Training loss (AAT loss) to encourage speaker embedding extractor to learn channel-invariant embedding. The weights of augmentation classiﬁer are ﬁxed during this step. Learning objective related to this strategy is described below.
AAT loss. AAT loss is applied to remove the channel information from speaker embeddings. After training the augmentation classiﬁer to distinguish channel similarities, we apply binary cross entropy loss which is same as dis. One diﬀerence is, a gradient reversal layer is placed between embedding extractor and augmentation classiﬁer, thereby penalising the ability to correctly predict whether the pair of segments share the same channel characteristics. It can be formulated as Equation 8 where minus indicates the use of gradient reversal layer.

aat = − dis

(8)

The overall loss is summation of the speaker loss and the AAT loss with a weight . spk can be either prototypical or angular prototypical loss function. The ablation study has been done regarding the
value of and we report the results in Section 3.6. We also attach the pseudo-code of AAT algorithm
in supplementary material.

overall = spk + aat

(9)

3 Experiments
3.1 Input representations and model architecture
Since the utterances in VoxCeleb are always longer than 4 seconds, two 1.8-second segments are randomly sampled from each utterance during batch formation to construct two non-overlapping speech segments ,1 and ,2 introduced in Section 2.1. The duration of the segments are slightly shorter than half of the shortest utterance in order to allow for small temporal perturbation. 40 dimensional log-mel spectrogram is extracted with window length 25 ms and hop length 10 ms. Instance normalisation [40] is performed as a mean variance normalisation to the input. We do not use voice activity detection (VAD) since the dataset mostly consists of continuous speech.

4

The network architecture of the speaker embedding extractor closely follows the Fast ResNet-34 architecture in [10]. It is a lightweight version of original ResNet-34 with the same architecture but the channel sizes are reduced to a quarter. Self-attentive pooling is performed on the output of residual blocks along the time axis, followed by a fully connected layer. The dimension of the speaker embedding is 512.
The augmentation classiﬁer consists of a gradient reversal layer followed by two fully connected layers with hidden size 512. ReLU activation and one-dimensional batch normalisation are performed between these layers. The size of last fully connected layer is 2 since the network is a binary classiﬁer.
3.2 Data augmentation
Data augmentation plays a crucial role in contrastive learning, as reported by previous literature in speaker recognition [19] and other domains [38, 25, 4, 8]. We exploit two popular augmentation methods in speech processing – additive noise and RIR simulation. For additive noise, we use the MUSAN corpus [36]; for room impulse responses, we use 1,000 pre-computed RIR ﬁlters. Both noise and RIR ﬁlters are randomly selected during training. The types of augmentation and the SNR range for each type are the same as those used by the original x-vector paper – see Section 3.3 of [37] for details. In order to verify the eﬀects of the diﬀerent augmentation methods, we perform a number of experiments, (1) without any augmentation, (2) applying only noise addition, (3) applying either noise addition or reverberation and (4) applying both noise addition and reverberation. We also compare the results of only augmenting one of the speech segment (i.e. ,1,1 = ( ,1), ,2,1 = ( ,2), and
,2,2 = ( 1,2 ∗ 1,2 + 1,2)) and augmenting all of the speech segments.
3.3 Training Details
Our implementation is based on the PyTorch framework [28]. The models are trained using a NVIDIA V100 GPU with 32GB memory for 150 epochs. We use the Adam optimiser with an initial learning rate of 0.001 decreasing by 5% every 5 epochs. 200 utterances are randomly selected for each minibatch formation. All experiments are repeated independently three times in order to minimise the eﬀect of random initialisation. Mean and standard deviation of the experiments are reported in Table 1.
3.4 Dataset
VoxCeleb. VoxCeleb is an audio-visual dataset consisting of short clips of human speech, extracted from celebrity interview videos uploaded to YouTube. The models are trained on the development set of VoxCeleb2 [11], which consists of over 1 million utterances from 5,994 speakers. Speaker labels in VoxCeleb2 are not used in our method. The original test set of VoxCeleb1 [27] containing 40 speakers is used for evaluation.
VOiCES. The Voices Obscured in Complex Environmental Settings (VOiCES) [32] corpus contains speech recorded by far-ﬁeld microphones in noisy room conditions. Evaluation on this dataset is performed to provide out-of-domain trial for the models trained on the VoxCeleb2 dataset. In particular, we use the evaluation list provided in the development data for the 2019 VOiCES challenge, which contains 4 million pairs from 15,904 utterances. Note that the speaker models are not trained or ﬁne-tuned on this dataset, in order to verify that the models trained on the VoxCeleb dataset generalises to out-of-domain data.
3.5 Baselines
We compare the results of our methods with a range of baselines in Table 1.
Previous works using self-supervision. [26] and [13] use cross-modal self-supervision to learn the joint representation of face images and speech segments. [19] proposes audio-only self-supervised learning with data augmentation using additive noise and RIR ﬁlters, which is of closest relevance to our work since they use the same network inputs as well as the training and the test data.
I-vectors. I-vectors [14] have been used widely in speaker recognition before the emergence of deep learning. Although the i-vectors are often used in conjunction with probabilistic linear discriminant
5

analysis (PLDA) back-end to improve performance [20, 7, 23], training of i-vectors and scoring with cosine similarity as proposed by the original paper [14] do not require any supervision.
60-dimensional frame-level features (19 Mel-frequency cepstral coeﬃcients + energy + Δ + ΔΔ) are extracted from audio signal using a 25 ms window with 10 ms shifts, then mean and variance normalisation (MVN) is applied. A gender-independent universal background model, containing 2,048 Gaussian components, and a total variability matrix with dimensionality 400 are trained, both with 10 iterations. Our implementation of the i-vector system is based on the popular Kaldi [30] toolkit.
Human benchmark. Humans do not learn how to recognise the speaker identity through supervised training as computer do. Therefore, it is interesting to compare the human performance on speaker veriﬁcation as a self-supervised counterpart of our model. We conduct experiments with two groups of annotators – crowdworkers on Amazon Mechanical Turk and experts who have dealt with speaker recognition for several years. Details of these experiments are described in the supplementary material.
3.6 Results

Evaluation protocol. We report two performance metrics: (i) the Equal Error Rate (EER) which is

the rate at which both acceptance and rejection errors are equal; and (ii) the minimum detection cost

of the function used by the NIST SRE [1] and the VoxSRC1 evaluations. For computing EER, we

sample 10 segments for each utterance and compute the mean of 10 × 10 = 100 distances from all

possible combinations per each trial pair in the evaluation set. This protocol is in line with that used

by [9, 10]. The parameters

= 1, = 1 and

= 0.05 are used for the cost function.

Discussion. Table 1 reports the experimental results. Data augmentation is a key to the performance of self-supervised speaker models. More aggressive augmentation schemes (e.g. noise and RIR) improve the performance of the models. This implies that data augmentation helps to train the noise-robust network and is essential to apply diverse channel eﬀects.
AAT reduces the veriﬁcation errors across a range of augmentation settings and objective functions. The best performing model training with angular prototypical loss and AAT achieves an equal error rate of 8.65%, outperforming all comparable works by a signiﬁcant margin. Similar trend is observed in VOiCES dataset results, on which the models trained with AAT outperforms the counterparts without. This demonstrates that the models trained using AAT generalise better to unseen domains, as well as the dataset that the models have been trained on.
Speaker recognition performance for various values of the AAT loss weight is reported in Table 2. The augmentation process and the learning objective are ﬁxed in these experiments. Applying the AAT improves the performance in both datasets. = 3 shows the best performance for VoxCeleb, and = 10 for VOiCES.

4 Conclusion
In this paper, we proposed an augmentation adversarial training strategy to train eﬀective speaker embeddings with self-supervision. The method exploits an augmentation classiﬁer and gradient reversal layer to prevent the speaker embedding extractor from learning the channel information. The experiments on the VoxCeleb and VOiCES datasets demonstrate state-of-the-art performance in self-supervised speaker recognition.

Broader Impact
In this paper, we introduce a novel self-supervised method for speaker recognition. We assess the expected impact of our research into two perspectives.
Voice authentication. Voices can be used as biometric information like ﬁngerprints or facial features. Therefore, voice authentication [35] is one of the important applications of speaker recognition that
1http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html

6

Table 1: Speaker veriﬁcation performance. All experiments are repeated three times and we report the mean and standard deviation. † uses the i-vector together with cosine similarity, as described in Section 3.5. ‡ computed on a subset of 2,000 pairs, see supplementary material for details. P: Prototypical loss, AP: Angular Prototypical loss, AAT: Augmentation Adversarial Training

Loss
Disent. [26] CDDL [13] GCL [19] I-vector †
AMT Expert
Prototypical Angular Prototypical
P P + AAT P P + AAT P P + AAT AP AP + AAT AP AP + AAT AP AP + AAT
P P + AAT P P + AAT P P + AAT AP AP + AAT AP AP + AAT AP AP + AAT

Aug.

VoxCeleb

EER (%)

MinDCF

Self-supervised baselines

Noise or RIR -

22.09 17.52 15.26 15.28

0.627

Human benchmark ‡

-

26.51

-

-

15.77

-

No augmentation

-

27.30 ± 0.15 0.788 ± 0.002

-

25.37 ± 0.15 0.788 ± 0.004

Augment one segment

Noise Noise Noise or RIR Noise or RIR Noise + RIR Noise + RIR Noise Noise Noise or RIR Noise or RIR Noise and RIR Noise and RIR

20.58 ± 0.30 17.08 ± 0.55 18.22 ± 0.42 12.77 ± 0.60 13.03 ± 0.05
9.96 ± 0.33 18.63 ± 0.37 14.47 ± 0.06 16.43 ± 0.25 11.35 ± 0.18 11.43 ± 0.20
8.86 ± 0.18

0.738 ± 0.003 0.685 ± 0.016 0.719 ± 0.003 0.634 ± 0.016 0.610 ± 0.005 0.522 ± 0.019 0.731 ± 0.004 0.666 ± 0.004 0.710 ± 0.006 0.612 ± 0.008 0.592 ± 0.013 0.490 ± 0.009

Augment both segments

Noise Noise Noise or RIR Noise or RIR Noise + RIR Noise + RIR Noise Noise Noise or RIR Noise or RIR Noise and RIR Noise and RIR

16.00 ± 0.05 15.22 ± 0.24 12.42 ± 0.15 10.54 ± 0.06 10.16 ± 0.16
9.36 ± 0.07 14.73 ± 0.19 13.56 ± 0.18 11.60 ± 0.14
9.03 ± 0.07 9.56 ± 0.18 8.65 ± 0.14

0.667 ± 0.002 0.640 ± 0.004 0.623 ± 0.006 0.544 ± 0.002 0.524 ± 0.009 0.482 ± 0.004 0.665 ± 0.006 0.632 ± 0.008 0.620 ± 0.004 0.512 ± 0.011 0.511 ± 0.011 0.454 ± 0.013

VOiCES

EER (%)

MinDCF

17.49

0.817

-

-

-

-

29.69 ± 1.45 0.992 ± 0.004 32.21 ± 0.89 0.994 ± 0.002

22.04 ± 0.53 18.98 ± 0.33 17.27 ± 0.39 12.96 ± 0.43 11.94 ± 0.01
9.05 ± 0.96 21.99 ± 0.68 20.64 ± 1.25 15.90 ± 0.46 12.25 ± 0.48 10.52 ± 0.58
7.95 ± 0.12

0.944 ± 0.002 0.913 ± 0.012 0.894 ± 0.006 0.760 ± 0.011 0.713 ± 0.012 0.583 ± 0.057 0.939 ± 0.008 0.908 ± 0.007 0.850 ± 0.017 0.753 ± 0.022 0.662 ± 0.034 0.528 ± 0.010

19.15 ± 1.71 17.31 ± 1.73 11.31 ± 0.75
9.17 ± 0.23 5.82 ± 0.11 5.26 ± 0.03 18.82 ± 1.13 18.75 ± 1.61 10.93 ± 0.28 9.06 ± 0.58 5.65 ± 0.42 4.96 ± 0.12

0.877 ± 0.017 0.863 ± 0.011 0.684 ± 0.033 0.594 ± 0.007 0.407 ± 0.003 0.378 ± 0.009 0.895 ± 0.012 0.886 ± 0.022 0.687 ± 0.015 0.608 ± 0.013 0.401 ± 0.024 0.356 ± 0.007

can beneﬁt people’s lives. However, considering that it is normally used in telephone speech, there is a potential risk of spooﬁng, such as using synthesized speech or voice imitation [15]. Since speech signal is also sensitive to the environment or channel characteristics, voice authentication performs poorly when used in diﬀerent environments. Our proposed method can mitigate this problem by training a speaker-discriminative, environment-invariant speaker network.
7

Table 2: The eﬀect of the value of on speaker veriﬁcation performance, using Noise and RIR augmentation.

Loss
Angular Prototypical Angular Prototypical + AAT Angular Prototypical + AAT Angular Prototypical + AAT

VoxCeleb1

EER (%)

MinDCF

Augment both segments
0 9.56 ± 0.18 0.511 ± 0.011 1 8.89 ± 0.09 0.476 ± 0.006 3 8.65 ± 0.14 0.469 ± 0.008 10 8.72 ± 0.12 0.454 ± 0.013

VOiCES

EER (%)

MinDCF

5.65 ± 0.42 5.32 ± 0.19 5.05 ± 0.10 4.96 ± 0.12

0.401 ± 0.024 0.361 ± 0.014 0.367 ± 0.012 0.356 ± 0.007

Reduce bias with low resource. One of the major advantages of self-supervised learning is that we can train the model without explicit labels. It makes it possible to learn with a large amount of data without putting eﬀort into annotations. It also helps to reduce dataset bias from the human supervision and to obtain general representation from the model. However, there is a potential risk that we rely too much on the output of algorithms without human supervision. Moreover, we are aware that there are a number of companies that oﬀer labeling services, and we are concerned that the development of this technology will hinder their growth.
Acknowledgements
Jaesung Huh is funded by the Global Korea Scholarship. We would like to thank Soyeon Choe, Icksang Han and Bong-Jin Lee for helpful comments.
References
[1] NIST 2018 Speaker Recognition Evaluation Plan, 2018 (accessed 31 July 2020). https://www.nist.gov/system/files/documents/2018/08/17/sre18_eval_plan_ 2018-05-31_v6.pdf, See Section 3.1.
[2] Prashant Anand, Ajeet Kumar Singh, Siddharth Srivastava, and Brejesh Lall. Few shot speaker recognition using deep neural networks. arXiv preprint arXiv:1904.08775, 2019.
[3] Relja Arandjelović. and Andrew Zisserman. Look, listen and learn. In Proc. ICCV, 2017. [4] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maxi-
mizing mutual information across views. In NIPS, pages 15535–15545, 2019. [5] Gautam Bhattacharya, Jahangir Alam, and Patrick Kenny. Adapting end-to-end neural speaker
veriﬁcation to new languages and recording conditions with adversarial training. In Proc. ICASSP, pages 6041–6045. IEEE, 2019. [6] Gautam Bhattacharya, Joao Monteiro, Jahangir Alam, and Patrick Kenny. Generative adversarial speaker embedding networks for domain robust end-to-end speaker veriﬁcation. In Proc. ICASSP, pages 6226–6230. IEEE, 2019. [7] Lukáš Burget, Oldřich Plchot, Sandro Cumani, Ondřej Glembek, Pavel Matějka, and Niko Brümmer. Discriminatively trained probabilistic linear discriminant analysis for speaker veriﬁcation. In Proc. ICASSP, pages 4832–4835. IEEE, 2011. [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive learning of visual representations. In Proc. ICML, 2020. [9] Joon Son Chung, Jaesung Huh, and Seongkyu Mun. Delving into VoxCeleb: environment invariant speaker recognition. In Speaker Odyssey, 2020. [10] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Lee, Hee Soo Heo, Soyeon Choe, Chiheon Ham, Sunghwan Jung, Bong-Jin Lee, and Icksang Han. In defence of metric learning for speaker recognition. In INTERSPEECH, 2020. [11] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition. In INTERSPEECH, 2018.
8

[12] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Workshop on Multi-view Lip-reading, ACCV, 2016.
[13] Soo-Whan Chung, Hong Goo Kang, and Joon Son Chung. Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision. In INTERSPEECH, 2020.
[14] Najim Dehak, Patrick J Kenny, Réda Dehak, Pierre Dumouchel, and Pierre Ouellet. Front-end factor analysis for speaker veriﬁcation. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):788–798, 2011.
[15] Nicholas Evans, Tomi Kinnunen, Junichi Yamagishi, Zhizheng Wu, Federico Alegre, and Phillip De Leon. Speaker recognition anti-spooﬁng. In Handbook of biometric anti-spooﬁng, pages 125–146. Springer, 2014.
[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016.
[17] Mahdi Hajibabaei and Dengxin Dai. Uniﬁed hypersphere embedding for speaker recognition. arXiv preprint arXiv:1807.08312, 2018.
[18] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.
[19] Nakamasa Inoue and Keita Goto. Semi-supervised contrastive learning with generalized contrastive loss and its application to speaker recognition. arXiv preprint arXiv:2006.04326, 2020.
[20] Patrick Kenny. Bayesian speaker veriﬁcation with heavy-tailed priors. In Speaker Odyssey, volume 14, 2010.
[21] Yutian Li, Feng Gao, Zhijian Ou, and Jiasong Sun. Angular softmax loss for end-to-end speaker veriﬁcation. In International Symposium on Chinese Spoken Language Processing, pages 190–194. IEEE, 2018.
[22] Chau Luu, Peter Bell, and Steve Renals. Channel adversarial training for speaker veriﬁcation and diarization. In Proc. ICASSP, pages 7094–7098. IEEE, 2020.
[23] Pavel Matějka, Ondřej Glembek, Fabio Castaldo, Md Jahangir Alam, Oldřich Plchot, Patrick Kenny, Lukáš Burget, and Jan Černocky. Full-covariance ubm and heavy-tailed plda in i-vector speaker veriﬁcation. In Proc. ICASSP, pages 4828–4831. IEEE, 2011.
[24] Mitchell McLaren, Luciana Ferrer, Diego Castan, and Aaron Lawson. The speakers in the wild (SITW) speaker recognition database. In INTERSPEECH, 2016.
[25] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proc. CVPR, pages 6707–6717, 2020.
[26] Arsha Nagrani, Joon Son Chung, Samuel Albanie, and Andrew Zisserman. Disentangled speech embeddings using cross-modal self-supervision. In Proc. ICASSP, pages 6829–6833. IEEE, 2020.
[27] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: a large-scale speaker identiﬁcation dataset. In INTERSPEECH, 2017.
[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NIPS, pages 8024–8035, 2019.
[29] Deepak Pathak, Philipp Krahenbuhl, Jeﬀ Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proc. CVPR, pages 2536–2544, 2016.
[30] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, 2011.
[31] Mirco Ravanelli and Yoshua Bengio. Learning speaker representations with mutual information. In INTERSPEECH, 2019.
[32] Colleen Richey, Maria A Barrios, Zeb Armstrong, Chris Bartels, Horacio Franco, Martin Graciarena, Aaron Lawson, Mahesh Kumar Nandwana, Allen Stauﬀer, Julien van Hout, et al. Voices obscured in complex environmental settings (voices) corpus. In INTERSPEECH, 2018.
9

[33] Johan Rohdin, Themos Stafylakis, Anna Silnova, Hossein Zeinali, Lukáš Burget, and Oldřich Plchot. Speaker veriﬁcation using end-to-end adversarial language adaptation. In Proc. ICASSP, pages 6006–6010. IEEE, 2019.
[34] Andrew Rouditchenko, Hang Zhao, Chuang Gan, Josh McDermott, and Antonio Torralba. Self-supervised audio-visual co-segmentation. In Proc. ICASSP, pages 2357–2361. IEEE, 2019.
[35] Nilu Singh, RA Khan, and Raj Shree. Applications of speaker recognition. Procedia engineering, 38:3122–3126, 2012.
[36] David Snyder, Guoguo Chen, and Daniel Povey. Musan: A music, speech, and noise corpus. arXiv preprint arXiv:1510.08484, 2015.
[37] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. X-vectors: Robust dnn embeddings for speaker recognition. In Proc. ICASSP, pages 5329–5333. IEEE, 2018.
[38] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.
[39] Eric Tzeng, Judy Hoﬀman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proc. ICCV, pages 4068–4076, 2015.
[40] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
[41] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. Generalized end-to-end loss for speaker veriﬁcation. In Proc. ICASSP, pages 4879–4883. IEEE, 2018.
[42] Jixuan Wang, Kuan-Chieh Wang, Marc T Law, Frank Rudzicz, and Michael Brudno. Centroidbased deep metric learning for speaker recognition. In Proc. ICASSP, pages 3652–3656. IEEE, 2019.
[43] Qing Wang, Wei Rao, Sining Sun, Leib Xie, Eng Siong Chng, and Haizhou Li. Unsupervised domain adaptation via domain adversarial training for speaker recognition. In Proc. ICASSP, pages 4889–4893. IEEE, 2018.
[44] Xu Xiang, Shuai Wang, Houjun Huang, Yanmin Qian, and Kai Yu. Margin matters: Towards more discriminative deep neural network embeddings for speaker recognition. In Asia-Paciﬁc Signal and Information Processing Association Annual Summit and Conference, pages 1652– 1656. IEEE, 2019.
[45] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In Proc. ECCV, pages 649–666. Springer, 2016.
10

Appendices
A Human benchmark
This material provides detailed descriptions of the human experiments introduced in Section 3.5. The purpose of this task is to determine how well automated speaker recognition systems perform compared to human ability.
A.1 Experimental settings
Two groups of annotators – Amazon Mechanical Turk and experts – are asked to annotate random subsets of the VoxCeleb test set. The evaluation protocols for these experiments mimic the VoxCeleb evaluation for automatic speaker recognition – the annotators are given utterance pairs, and they are asked whether they believe that the two utterances are spoken by the same speaker.
The annotators are given a pair of utterances to listen to, and are asked to choose between one of the following options. The annotators are discouraged from using the score of 3 (borderline). They are given up to 30 seconds for the task.
1 - Deﬁnitely diﬀerent, 2 - Probably diﬀerent, 3 - Borderline, 4 - Probably the same, 5 - Deﬁnitely the same.
AMT. Amazon Mechanical Turk is a crowdsourcing marketplace to hire remotely located crowdworkers to perform discrete microtasks such as data annotation or surveys.
2,000 randomly sampled pairs from the VoxCeleb test set are given to the annotators through this platform, who are rewarded on a per-sample basis. The tasks are only made available to the most experienced and highly rated workers, however the annotators do not necessarily have previous experience in speaker recognition.
The annotators are told that the approximately half of the pairs are from the speaker, and are given some example pairs to listen to before working on the task.
Experts. The samples are also annotated by the authors of this paper, who have several years of experience in speaker recognition. The authors are very familiar with the VoxCeleb dataset, including the statistics of the test set.
The same 2,000 pairs used by the Mechanical Turk are divided into 4 subsets of 500, each of which is annotated by a diﬀerent author. These subsets are referred to as Sets A, B, C and D in Table 3 and Figure 2.
A.2 Evaluation
Metrics. We report three metrics for the human benchmark – Equal Error Rates (EER), Area Under Receiver Operating Characteristic curve (AUROC), and binary classiﬁcation accuracy. EER and AUROC are obtained by interpolating the ROC curve between the points for the 5 discrete scores. Binary classiﬁcation accuracy is the most intuitive and fair metric for humans, since binary decision from each pair is exactly the same task that they have been asked to perform. The score of 3 (borderline) is assigned to the positive class for both AMT and experts, since this gives a better accuracy. In reality, the annotators only used the borderline option very few times. To compute the binary classiﬁcation accuracy of our unsupervised automatic speaker veriﬁcation model (U-ASV), we set the threshold tuned on the validation set that does not overlap with the test set in the table.
Discussion. Table 3 shows the speaker veriﬁcation performance of the human annotators. It can be seen that the annotations of the experts are far more accurate compared to the crowdworkers on AMT. It is also notable that the variance between the performance of the four expert annotators is
11

Table 3: Speaker veriﬁcation performance of diﬀerent methods on various subsets of the VoxCeleb1 test set. U-ASV: Unsupervised Automatic Speaker Veriﬁcation model trained using the AP + AAT loss.

Test Set # Pairs Veriﬁcation EER (%)

AUROC (%)

Binary Classiﬁcation Acc. (%)

AMT Experts U-ASV AMT Experts U-ASV AMT Experts

U-ASV

Set A Set B Set C Set D

500 25.75 500 25.63 500 22.59 500 25.91

16.53 15.70 17.78 13.98

8.10 79.46 8.75 82.33 9.01 81.45 8.76 78.34

89.28 90.96 88.61 89.78

97.08 97.73 97.14 97.30

73.80 74.40 75.40 72.60

82.20 86.00 82.20 86.40

91.40 92.00 90.40 91.40

All

2,000 26.51 15.77 8.50 79.60 89.65 97.32 74.10 84.20

91.30

relatively small. We observe that our U-ASV model outperforms the human benchmark. It is diﬃcult for humans to match the performance of the deep learning models on the pairwise veriﬁcation task. We also report four ROC curves based on the annotation done by the experts in Figure 2.

1.0

Receiver Operating Characteristic

1.0

Receiver Operating Characteristic

0.8

0.8

True Positive Rate

True Positive Rate

0.6

0.6

0.4

0.4

0.2

Expert 1

AMT

U-ASV

0.00.0

0.2

0.4

0.6

0.8

1.0

False Positive Rate

(a) Set A

1.0

Receiver Operating Characteristic

0.2

Expert 2

AMT

U-ASV

0.00.0

0.2

0.4

0.6

0.8

1.0

False Positive Rate

(b) Set B

1.0

Receiver Operating Characteristic

0.8

0.8

True Positive Rate

True Positive Rate

0.6

0.6

0.4

0.4

0.2

Expert 3

AMT

U-ASV

0.00.0

0.2

0.4

0.6

0.8

1.0

False Positive Rate

0.2

Expert 4

AMT

U-ASV

0.00.0

0.2

0.4

0.6

0.8

1.0

False Positive Rate

(c) Set C

(d) Set D

Figure 2: Receiver Operating Characteristic curves for the diﬀerent subsets of the VoxCeleb1 test set. Set A, B, C and D are annotated by diﬀerent experts.

12

B Algorithm
The PyTorch [28] style pseudocode for the proposed augmentation adversarial training strategy is described in Listing 1.

1 ## Let batch is (N,3,T) dimensional tensor. N is batch_size , T is length of utterance ,

and D is size of embedding.

2 ## batch[i, 0] and batch[i, 1] : different segments with same augmentation

3 ## batch[i, 1] and batch[i, 2] : same segment with different augmentation

4 ## netspk is speaker embedding extractor and netaug is augmentation classifier.

5

6 spk_optimizer = optim.Adam(netspk.parameters())

7 aug_optimizer = optim.Adam(netaug.parameters())

8

9 for batch in loader:

10

feat = netspk.forward(batch) # feat size : (N,3,D)

11

12

# Discriminator Training

13

aug_optimizer . zero_grad ()

14

out_a , out_s , out_p = feat[:,0,:].detach(), feat[:,1,:].detach(), feat[:,2,:].detach

()

15

conf_input = torch.cat((torch.cat((out_a ,out_s),dim=1),torch.cat((out_a ,out_p),dim=1)

) ,dim =0)

16

conf_output = netaug.forward(conf_input)

17

conf_labels = torch.LongTensor ([1] * N + [0] * N)

18

19

aug_loss = torch.nn.BCELoss(conf_output , conf_labels)

20

aug_loss . backward ()

21

aug_optimizer . step ()

22

23

# Embedding training

24

spk_optimizer . zero_grad ()

25

conf_input = torch.cat((torch.cat((feat[:,0,:],feat[:,1,:]),dim=1),torch.cat((feat

[: ,0 ,:] , feat [: ,2 ,:]) ,dim =1) ) ,dim =0)

26

conf_input = RevGrad(conf_input) # reversing gradients

27

conf_output = netaug.forward(conf_input)

28

aat_loss = torch.nn.BCELoss(conf_output , conf_labels)

29

spk_loss = SpkCriterion(feat[:, [0,2], :]) # prototypical or angular prototypical

loss

30

loss = spk_loss + lambda * aat_loss

31

loss . backward ()

32

spk_optimizer . step ()

Listing 1: PyTorch-style pseudocode of AAT

13

