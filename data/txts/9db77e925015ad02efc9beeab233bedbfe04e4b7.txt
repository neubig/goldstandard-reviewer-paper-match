Accelerating Reinforcement Learning with a Directional-Gaussian-Smoothing Evolution Strategy

arXiv:2002.09077v1 [cs.LG] 21 Feb 2020

Jiaxin Zhang∗ Hoang Tran† Guannan Zhang‡

Abstract
Evolution strategy (ES) has been shown great promise in many challenging reinforcement learning (RL) tasks, rivaling other state-of-theart deep RL methods. Yet, there are two limitations in the current ES practice that may hinder its otherwise further capabilities. First, most current methods rely on Monte Carlo type gradient estimators to suggest search direction, where the policy parameter is, in general, randomly sampled. Due to the low accuracy of such estimators, the RL training may suffer from slow convergence and require more iterations to reach optimal solution. Secondly, the landscape of reward functions can be deceptive and contains many local maxima, causing ES algorithms to prematurely converge and be unable to explore other parts of the parameter space with potentially greater rewards. In this work, we employ a Directional Gaussian Smoothing Evolutionary Strategy (DGS-ES) to accelerate RL training, which is well-suited to address these two challenges with its ability to i) provide gradient estimates with high accuracy, and ii) ﬁnd nonlocal search direction which lays stress on large-scale variation of the reward function and disregards local ﬂuctuation. Through several benchmark RL tasks demonstrated herein, we show that DGS-ES is highly scalable, possesses superior wall-clock time, and achieves competitive reward scores to other popular policy gradient and ES approaches.
∗ Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN 37831.
† Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN 37831.
‡ Corresponding author, Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN 37831. Email: zhangg@ornl.gov.

1 INTRODUCTION
Reinforcement learning is a class of problems which aim to ﬁnd, through trial and error, a feedback policy that prescribes how an agent should act in an uncertain, complex environment to maximize some notion of cumulative reward (Sutton & Barto, 1998). Traditionally, RL algorithms have mainly been employed for small input and action spaces, and suffered difﬁculties when scaling to high-dimensional problems. With the recent emergence of deep learning, powerful non-linear function approximators such as deep neural networks (DNN) can be integrated into RL and extend the capability of RL in a variety of challenging tasks which would otherwise be infeasible, ranging from playing Atari from pixels (Mnih et al., 2015, 2016), playing expert-level Go (Silver et al., 2016) to robotic control (Andrychowicz et al., 2017; Lillicrap et al., 2016).
Among the most popular current deep RL algorithms are Q-learning methods, policy gradient methods, and evolution strategies. Deep Q-learning algorithms (Mnih et al., 2015) use a DNN to approximate the optimal Q function, yielding policies that, for a given state, choose the action that maximizes the Q-value. Policy gradient methods (Sehnke et al., 2010) improve the policies with a gradient estimator obtained from sample trajectories in action space, examples of which are A3C (Mnih et al., 2016), TRPO (Schulman et al., 2015) and PPO (Schulman et al., 2017).
This work concerns RL techniques based on evolution strategies. ES refers to a family of blackbox optimization algorithms inspired by ideas of natural evolution, often used to optimize functions when gradient information is inaccessible. This is exactly the prominent challenge in a typical RL problem, where the environment and policy are usually nonsmooth or can only be accessed via noisy sampling. It is not totally surprising ES has become a convincing competitor to Q-learning and policy gradient in deep RL in recent years. Unlike policy gradient, ES

perturbs and performs policy search directly in the parameter space to ﬁnd an effective policy, which is now generally considered to be superior to action perturbation (Sigaud & Stulp, 2013). The policy search can be guided by a surrogate gradient (Salimans et al., 2017), completely population-based and gradient-free (Such et al., 2017), and hybridized with other exploration strategies such as novelty search and quality diversity (Conti et al., 2018). It has been shown in those works that ES is easy to parallelize, and requires low communication overhead in a distributed setting. More importantly, being able to achieve competitive performance on many RL tasks, these methods are advantageous over other RL approaches in their high scalability and substantially lower wall-clock time. Given wide availability of distributed computing resources, all environment simulations at each iteration of training can be conducted totally in parallel. Thus, a more reasonable metric for the performance of a training algorithm is the number of non-parallelizable iterations, as opposed to the sample complexity. In this metric, ES is truly an appealing choice.
Nevertheless, there are several challenges that need to be addressed in order to further improve the performance of ES in training complex policies. First, most ES methods cope with the non-smoothness of the objective function by considering a Gaussian-smoothed version of the expected total reward. The gradient of this function is intractable and must be estimated to provide the policy parameter update. In the pioneering work (Salimans et al., 2017), a gradient estimator is proposed based on random parameter sampling. Developing efﬁcient sampling strategies for gradient estimates has become an interest in ES research since then, and several improvements have been proposed, based on imposing structures on parameter perturbation (Choromanski et al., 2018, 2019a), or reusing past evaluations, (Choromanski et al., 2019b; Maheswaranathan et al., 2019; Meier et al., 2019). Yet, most of these gradient estimators are of Monte Carlo type, therefore arguably affected by the low accuracy of Monte Carlo methods. For faster convergence of training (i.e., reducing the number of iterations), more accurate gradient estimators are desired, particularly in RL tasks where the policy has a large number of parameters to learn. Another prominent challenge is that the landscape of the objective function is complex and possesses plentiful local maxima. There is a risk for any optimization algorithm to get trapped in some of those points and unable to explore the parameter space effectively. The Gaussian smoothing, with its ability to smooth out function and damps out small, insigniﬁcant ﬂuctuations, is a strong candidate in this very challenge. Speciﬁcally, with a moderately large smoothing parameter (i.e., strong smoothing effect), we can expect the gradient of the smoothed objective function will be able to look

outside unimportant variation in the adjacent area and detect the general trend of the function from a distance, therefore an efﬁcient nonlocal search direction. This potential of Gaussian smoothing, however, has not been explored in reinforcement learning.
In this paper, we propose a new strategy to accelerate the time-to-solution of reinforcement learning by exploiting the Directional Gaussian Smoothing Evolution Strategy (DGS-ES), recently developed in (Zhang et al., 2020). The DGS-ES method introduced a new directional Gaussian smoothing (DGS) gradient operator, that smooths the original objective function only along d-orthogonal directions in the parameter space. In other words, the DGS gradient requires d one-dimensional Gaussian convolutions, instead of one d-dimensional convolution in the existing ES methods. There are several advantages of using the DGS-gradient operator in reinforcement learning. First, each component of the DGS-gradient, represented as a one-dimensional integral, can be accurately approximated with various classic numerical integration techniques. When having Gaussian kernels, we use GaussHermite quadrature rule which can provide spectral accuracy (see Abramowitz & Stegun (1972)) in the DGS gradient approximation. Second, the use of Gauss-Hermite quadrature also features embarrassing parallelism as the random sampling used in existing ES methods. Since the communication cost between computing processors/cores is neglectable, the total computing time for each iteration of training does not increase with the number of environment simulations given sufﬁcient computing resources. Third, the directional smoothing approach enables nonlocal exploration which takes into account large variation of the objective function and disregards local ﬂuctuations. This property will greatly help skipping local optima or saddle points during the training. It is demonstrated in §4 that the proposed strategy can signiﬁcantly reduce the number of iterations in training several benchmark reinforcement learning problems, compared to the stateof-the-art baselines.
1.1 RELATED WORKS
ES belongs to the family of blackbox optimization that employs random search techniques to optimize an objective function (Rechenberg & Eigen, 1973; Schwefel, 1977). The search can be guided via either covariance matrix adaptation (Hansen, 2016; Hansen & Ostermeier, 2001), or search gradients to be fed to ﬁrst order optimization schemes, (Wierstra et al., 2014). The DGS-ES method is in line with the second approach. Gaussian smoothed as a method for approximating search direction has been introduced in (Flaxman et al., 2005; Nesterov & Spokoiny, 2015). In recent years, ES has been revived as a popular strategy in machine learning. In addition

to those mentioned above, applications of ES to RL can also be found in Fazel et al. (2018); Ha & Schmidhuber (2018); Houthooft et al. (2018); Liu et al. (2019); Müller & Glasmachers (2018); Pourchot & Sigaud (2019). Applications of ES beyond RL include meta-learning (Metz et al., 2018), optimizing neural network architecture (Cui et al., 2018; Miikkulainen et al., 2017; Real et al., 2017), and direct training of neural network (Morse & Stanley, 2016).
Effective exploration is also critical for deep RL on high dimensional action and state spaces, and a wide variety of exploration strategies have been developed in recent years, to name a few, count-based exploration (Ostrovski et al., 2017; Tang et al., 2017), intrinsic motivation (Bellemare et al., 2016), curiosity (Pathak et al., 2017) and variational information maximization (Houthooft et al., 2016). For exploration techniques which add noise directly to the parameter space of policy, see (Fortunato et al., 2017; Plappert et al., 2017). Also, exploration can be combine with deep RL methods to improve sample efﬁciency (Colas et al., 2018). Finally, for a recent survey on policy search, we refer to (Sigaud & Stulp, 2019).
2 BACKGROUND
We introduce the background of reinforcement learning and discuss the application of ES methods in reinforcement learning.
2.1 REINFORCEMENT LEARNING
Reinforcement learning considers agents that are able to take a sequence of actions in an environment, denoted by E, over a number of discrete time steps t ∈ {1, . . . , T }. At each time step t, the agent receives a state st ∈ S and produces a follow-up action at ∈ A. The agent will then observe another state st+1 and a scalar reward rt. The goal of the agents is to learn a policy π(at|st) that maximizes the objective function, i.e., the expected cumulative return of the form
T
J (π) := E(st,at) γtr(st, at) , (1)
t=0
where 0 ≤ γ ≤ 1 is a discount rate, at is drawn from the policy π(at|st), and st+1 = E(st, at) is generated by running the environment dynamics.
In policy-based reinforcement learning approaches, the policy π(a|s; θ) is parameterized by θ ∈ Rd, where the vector θ := (θ1, . . . , θd) represents the parameters of the policy, e.g., weights of a neural network. Then, the task of learning a good policy π becomes iterative updating the parameter θ to solve the following optimization

problem

max J(θ),

(2)

θ∈Rd

where we denote J(θ) := J(π(a|s; θ)) by an abuse of notation. In reinforcement learning, it is usually true that the gradient of the environment S is inaccessible, so automatic differentiation cannot be used to obtain the gradient of J(θ). Thus, much of the innovation in reinforcement learning algorithms is focused on addressing the lack of access to or the existence of gradients of the environment/policy. In this work, we focus on the evolution strategy and other types of training algorithms for reinforcement learning are summarized in §1.

2.2 EVOLUTION STRATEGY

We brieﬂy recall the evolution strategy methods, e.g., (Hansen & Ostermeier, 2001; Salimans et al., 2017), which use a multivariate Gaussian distribution to generate the population around the current parameter value θt at the t-iteration. When the Gaussian distribution can be factorized to d independent one-dimensional Gaussian distributions, the standard ES method can be mathematically interpreted based on the Gaussian smoothing technique (Flaxman et al., 2005; Nesterov & Spokoiny, 2015). Speciﬁcally, a smoothed version of J(θ) in Eq. (1), denoted by Jσ(θ), is deﬁned by

1 Jσ(θ) :=

J

(θ

+

σ

◦

u)

e−

1 2

u

2
2 du

(3)

(2

π

)

d 2

Rd

= Eu∼N (0,Id) [J (θ + σ ◦ u)] ,

where N (0, Id) is a d-dimensional standard Gaussian distribution, the notation “◦” represents the element-wise product, and the vector σ = (σ1, . . . , σd) controls the smoothing effect. It is well known that Jσ is always differentiable even if J is not. In addition, most of the characteristics of the original objective function J(θ), e.g., convexity, the Lipschitz constant, are inherited by Jσ(θ). When J(θ) is differentiable, the difference ∇J − ∇Jθ can be bounded by its Lipschitz constant (see (Nesterov & Spokoiny, 2015), Lemma 3 for details). Thus, the original optimization problem in Eq. (2) can be replaced by a smoothed version, i.e.,

max Jσ(θ),

(4)

θ∈Rd

where the gradient of Jσ(θ) is given by

∇Jσ(θ) =

1 d/2 Eu∼N (0,Id) [J (θ + σ ◦ u) u] . (5) σ2

The standard ES method (Salimans et al., 2017) uses Monte Carlo sampling to estimate the gradient ∇Jσ(θ)

and update the state θ from iteration n to n + 1 by
λM θn+1 = θn − M σ J (θn + σ ◦ um)um, (6)
m=1
where λ is the learning rate, um are sampled from the Gaussian distribution N (0, Id).
One drawback of the ES method and its variants is the slow convergence of the training process, due to the low accuracy of the MC-based gradient estimator (see (Berahas et al., 2019)), also (Zhang et al., 2020), for extended discussions on the accuracy of gradient approximations using Eq. (6) and related methods). On the other hand, the evaluations of J(θn + σ ◦ um) for m = 1, . . . , M at the n-th iteration can be generated totally in parallel, which makes it well suited to be scaled up to a large number of parallel workers on modern supercomputers. Therefore, the motivation of this work is to develop a new gradient operator to replace the one in Eq. (5), such that the new gradient can be approximated in a much more accurate way and the embarrassing parallelism feature can be retained.

We can assemble a new gradient, i.e., the DGS gradient, by putting together the derivatives in Eq. (9) along d orthogonal directions, i.e.,

D [Gσ1 (0 | θ, ξ1)] ∇σ,Ξ[J](θ) = Ξ  ...  , (10)
D [Gσd (0 | θ, ξd)]

where Ξ := (ξ1, . . . , ξd) represents the matrix consisting of d orthonormal vectors. It is important to notice that
∇Jσ(θ) = ∇σ,Ξ[J ](θ)
for any σ > 0, because of the directional Gaussian smoothing used in Eq. (10). However, there is consistency between the two quantities as σ → 0, i.e.,

lim |∇Jσ(θ) − ∇σ,Ξ[J ](θ)| = 0,

(11)

σ→0

for ﬁxed θ and Ξ. If ∇J(θ) exists, then ∇σ,Ξ[J](θ) will also converge to ∇J(θ) as σ → 0. Such consistency naturally led to the idea of replacing ∇Jσ(θ) with ∇σ,Ξ[J](θ) in the ES framework.

3 THE DGS-ES METHOD

This section introduces our main framework. We start by introducing in §3.1 the DGS gradient operator and its approximation using the Gauss-Hermite quadrature rule. In §3.2, we describe how to incorporate the DGS gradient operator into the ES for reinforcement learning.

3.1 THE DGS GRADIENT OPEARTOR

For a given direction ξ ∈ Rd, the restriction of the objective function J(θ) along ξ can be represented by

G(y | θ, ξ) = J(θ + y ξ), y ∈ R,

(7)

where θ is the current state of the agent’s parameters. Then, we can deﬁne the one-dimensional Gaussian smoothing of G(y), denoted by Gσ(y), by

Gσ(y | θ, ξ) := Ev∼N (0,1) [G(y + σv | θ, ξ)] , (8)

which is also the Gaussian smoothing of J(θ) along ξ in the neighbourhood of θ. The derivative of Gσ(y|θ, ξ) at y = 0 is given by
1 D [Gσ(0 | θ, ξ)] = Ev∼N (0,1) [G(σv | θ, ξ) v] , (9)
σ
where D denotes the differential operator. It is easy to see that D[Gσ(0 | x, ξ)] only involves the directionally smoothed objective function given in Eq. (8).

3.2 THE DGS-ES ALGORITHM FOR REINFORCEMENT LEARNING

Since each component of ∇σ,Ξ[J](θ) in Eq. (10) only involves a one-dimensional integral, we can use Gaussian quadrature rules (Quarteroni et al., 2007) to obtain spectral convergence. In the case of Gaussian smoothing, a natural choice is the Gauss-Hermite (GH) rule, which is used to approximate integrals of the form g(x)e−x2 dx.
R
By doing a simple change of variable in Eq. (9), the GH rule can be directly used to obtain the following estimator:

DM [Gσ(0 | θ, ξ)]

1M

√

√

(12)

:= √ πσ

wm G( 2σvm | θ, ξ) 2vm

m=1

where wm are the GH quadrature weights deﬁned by

2M +1 M

√ !π

wm = [HM (vm)]2 , m = 1, . . . , M, (13)

vm are the roots of the Hermite polynomial of degree M

M v2 dM −v2

HM (v) = (−1) e dvM (e ),

(14)

and M is the number of function evaluations, i.e., environ-
ment simulations, needed to compute the quadrature in Eq. (12). The weights {wm}M m=1 and the roots {vm}M m=1 can be found in Abramowitz & Stegun (1972). The ap-
proximation error of the GH formula can be bounded by

D M [Gσ] is √
D M [Gσ] − D [Gσ] ≤ C0 M ! π σ2M−1, (15) 2M (2M ) !
where M ! is the factorial of M , and the constant C0 > 0 is independent of M and σ.
Applying the GH quadrature rule DM to each component of ∇σ,Ξ[J](θ) in Eq. (10), we deﬁne the following estimator:
D M [Gσ1 (0 | θ, ξ1)] ∇M σ,Ξ[J ](θ) := Ξ  ...  , (16)
D M [Gσd (0 | θ, ξd)]
which requires a total of M ×d parallelizable environment simulations at each iteration of training. The error bound in Eq. (15) indicates that ∇M σ,Ξ[J](θ) is an accurate estimator of the DGS gradient for a small M , regardless of the value of σ. This enables the use of relatively big values of σ in Eq. (16) to exploit the nonlocal features of the landscape of J(θ) in the training process. The nonlocal exploitation ability of ∇M σ,Ξ[J] is demonstrated in §4 to be effective in reducing the necessary number of iterations to achieve a prescribed reward score.
On the other hand, as the quadrature weights wm and vm deﬁned in Eq. (13) and Eq. (14), are deterministic values, the DGS estimator ∇M σ,Ξ[J](x) in Eq. (16) is also a deterministic for ﬁxed Ξ and σ. To introduce random exploration ability to our approach, we add random perturbations to both Ξ and σ. For the orthonormal matrix Ξ, we add a small random rotation, denoted by ∆Ξ, to the current matrix Ξ. The matrix ∆Ξ is generated as a random skew-symmetric matrix, of which the magnitude of each entry is smaller than a prescribed threshold α > 0. The perturbation of σ is conducted by drawing random samples from a uniform distribution U (r − β, r + β) with two hyper-parameters r and β with r − β > 0. The random perturbation of Ξ and σ can be triggered by various types of indicators e.g., the magnitude of the DGS-gradient, the number of iteration done since last perturbation.
4 EXPERIMENTS
To evaluate the DGS-ES algorithm, we tested its performance on two classes of reinforcement learning environments: three classical control theory problems from OpenAI Gym (https://github.com/openai/gym) (Brockman et al., 2016) and three continuous control tasks simulated using PyBullet (2.6.5) (https: //pybullet.org/) (Coumans & Bai, 2016) which is an open-source library. Within OpenAI Gym, we demonstrate the proposed approach on three benchmark exam-

Algorithm 1: The DGS-ES for reinforcement learning

1: Hyper-parameters: M : the order of GH quadrature rule α: the scaling factor for controlling the norm of ∆Ξ r, β: the mean and perturbation scale for sampling σ γ: the tolerance for triggering random perturbation
2: Input: θ0: the initial parameter value, L: the number of parallel workers
3: Output: the ﬁnal parameter value θN 4: Initialize the policy π with θ0 5: Set Ξ = Id, and σi = r for i = 1, . . . , d 6: Broadcast L copies of π(a|s; θ0) to the L workers 7: Divide the total GH quadrature points into L subsets,

and send each subset to a worker. 8: for n = 0, . . . N − 1 do 9: Each worker runs M d/L environment simulations

at their assigned quadrature points

10: Each worker sends M d/L scores to the master

11: for i = 1, . . . , d do

12:

Compute DM [Gσi (0 | θn, ξi)] in Eq. (12)

13: end for

14: Assemble ∇M σ,Ξ[J ](θn) in Eq. (16) 15: Update θn to θn+1 using Adam

16: if ∇M σ,Ξ[J ](θn) 2 < γ then

17:

Generate ∆Ξ and update Ξ = Id + ∆Ξ

18:

Generate σ from U (r − β, r + β)

19: end if

20: Broadcast θn+1 to the L workers

21: Each worker updates the policy to π(a|s; θn+1)

22: end for

ples: CartPole-v0 (discrete), MountainCarContinuousv0 (continuous), Pendulum-v0 (continuous). The maximum time steps for these examples are 200, 999 and 200, respectively. More details about the environment and reward settings can be found in Brockman et al. (2016). We also examined the DGS-ES algorithm on the challenging continuous robotic control problems in PyBullet library, namely HopperBulletEnv-v0, InvertedPendulumBulletEnv-v0 and ReacherBulletEnvv0. In these three tasks, the maximum time steps are 1000, 1000 and 150, respectively. For the purpose of reproducible comparison, we employed the original environment settings from the OpenAI Gym and the PyBullet library without modifying the rewards or the environments.
For our implementation of DGS-ES, we deﬁned our policies as a two-layer feed-forward neural network with 16

hidden nodes and tanh activation functions. For gradientbased optimization, we used Adam to adaptively update the network parameters with a learning rate of r = 0.1. We chose the hyper-parameters used in Algorithm 1 as follows: M = 7, α = 2.0, r = 1.0, β = 0.2, γ = 0.01. In practice one can tune the critical hyper-parameters (M, r and r) given the following suggested range: M ∈ [7, 9], r ∈ [0.5, 1.0] and r ∈ [0.01, 0.1]. For each task, our results were performed over 5 repeated independent trials (different random seeds) of the Gym/PyBullet simulators and the network policy initialization.
The DGS-ES algorithm is speciﬁcally amenable to parallelization since it only needs to communicate scalars, allowing it to scale to over a large number of parallel workers. We implemented a distributed version of Algorithm 1 to the reinforcement learning tasks. The distributed DGS-ES is implemented using PyTorch (Paszke et al., 2017) combined with Ray (Moritz et al., 2018) (https: //github.com/ray-project/ray), which does not rely on special networking setup and was tested on large-scale high performance computing facilities with thousands of computing nodes/workers.
Comparison metric: As the motivation of this work is to accelerate time-to-solution of reinforcement training under the assumption that sufﬁcient distributed computing resource is available, we used a different metric to evaluate the performance of DGS-ES and the baselines. Speciﬁcally, we are interested in the average return E[J] versus the number of iterations, i.e., N in Algorithm 1, because those iterations cannot be parallelized.
4.1 BASELINE METHODS
We compared Algorithm 1 against several RL baselines, including ES, PPO and TRPO, as well as the state-of-theart algorithms such as ASEBO, DDPG, and TD3. Below is the information of the packages used in this effort.
• ES: The Evolution Strategy proposed in Salimans et al. (2017). We used the implementation of ES from the open-source code https://github. com/hardmaru/estool.
• ASEBO: Adaptive ES-Active Subspaces for Blackbox Optimization, which was recently developed by Choromanski et al. (2019a). We used the implementation released by the authors at https: //github.com/jparkerholder/ASEBO.
• PPO: Proximal Policy Optimization in Schulman et al. (2017), which is available in OpenAI’s baselines repository at https://github.com/ openai/baselines (Dhariwal et al., 2017).

• TRPO: Trust Region Policy Optimization, developed by Schulman et al. (2015). We also used the OpenAI’s baselines implementation (Dhariwal et al., 2017).
• DDPG: Deep Deterministic Policy Gradient, proposed by Lillicrap et al. (2016). We used the implementation from https://github.com/ georgesung/TD3 where the benchmark DDPG in PyBullet is provided.
• TD3: Twin Delayed Deep Deterministic Policy Gradient (Fujimoto et al., 2018), which was built upon the DDPG. The original results were reported for the MuJoCo version environments using the implementation from https://github.com/sfujim/ TD3, but we used the PyBullet implementation from https://github.com/georgesung/TD3.
The hyper-parameters for all algorithms above were set to match the original papers without further tuning to improve performance on the testing benchmark examples.
4.2 COMPARATIVE EVALUATION
Figure 1 shows the comparison results of CartPole, Pendulum and MountainCar problems from the OpenAI Gym. We compared the DGS-ES with classical ES and the improved ASEBO method. In general, the DGS-ES method features faster convergence than the baselines. For the simplest CartPole problem, the three methods perform equally well. Discrepancy appears in the Pendulum test, where the DGS-ES method not only converges faster than the baselines, but also achieves a higher average return. There is a much bigger discrepancy between the DGS-ES and the baselines appear in the MountainCar test. According to the guideline provided in the OpenAI Gym, the success threshold is to achieve an average return of 90. The DGS-ES method achieves the threshold within 500 iterations, while the average returns of the ES and ASEBO methods are around zero. It is well known that the challenge of this problem is that the surface of the objective function J(θ) is very ﬂat at most locations in the parameter space, which makes it difﬁcult to capture the peak of J(θ). This test is a good demonstration of the nonlocal exploration ability of the DGS-ES method. Since the mean of the smoothing factor σ is set to 1.0, DGS-ES can capture the peak of J(θ) much faster than the baselines. In fact, we can see that it took around 50 iterations for the DGS-ES to ﬁnd the peak region, while ES and ASEBO needed more than 3000 iterations (not plotted) to move out of the ﬂat region.
Figure 2 shows the comparison results of Hopper-v0, InvertedPendulum-v0 and Reacher-v0 problems from the

Figure 1: Comparison between the DGS-ES and two baselines, i.e., ES and ASEBO, for solving the three problems from OpenAI Gym. The colored curves are the average return over 5 repeated runs with different random seeds, and the corresponding shade represents the interval between the maximum and minimum return.

Figure 2: Comparison between the DGS-ES method and the baselines, i.e., ES, ASEBO, PPO, TRPO, DDPG and TD3, for solving the three problems from the PyBullet library. The colored curves are the average return over 5 runs with different random seeds, and the corresponding shade represents the interval between the maximum and minimum return.

PyBullet library. We compared the DGS-ES with six baselines including ES, ASEBO, PPO, TRPO, DDPG and TD3. As expected, the DGS-ES method shows better performance in terms of the convergence speed. For the Hopper-v0 problem, the DGS-ES achieves the highest return of all the methods within 400 iterations. It is worth pointing out that some of the baselines will eventually catch up with the DGS-ES given a sufﬁciently large number of iterations. For example, DDPG and TD3 do not provide much improvement within 400 iterations, but DDPG could reach 1650 average return with over 3000 iterations, and TD3 could reach even higher, around 2200 average return, with over 6000 iterations, according to the baselines provided by OpenAI (Dhariwal et al., 2017) and Fujimoto et al. (2018). This phenomenon illustrates the fast convergence feature of DGS-ES. For the InvertedPendulum-v0 problem, DGS-ES can achieve the maximum return 1000 (default value in the PyBellet library) around 30 iterations. In comparison, ES and ASEBO can reach the maximum return but with more iterations than DGS-ES. According to the benchmark results for PyBullet environments in Fujimoto et al. (2018), DDPG and TD3 cannot converge to the maximum return even with a large number of iterations. For the Reacher-v0 problem, DGS-ES and ASEBO are still the top performers, and the advantage of DGS-ES is, again, faster convergence.
Figure 3 illustrates the effect of the radius σ of ∇M σ,Ξ[J](θ) on the performance of the DGS-ES method in solving the Reacher-v0 problem. All the simulations were done using the same initialization. We set the mean of , i.e., the hyper-parameter r in Algorithm 1, to 0.5, 0.05, 0.01, and 0.005. It is easy to see that the performance of DGS-ES deteriorates with the decrease of σ. Since it is evident that the surface of J(θ) is not convex and may have many local maxima, a relatively big radius σ is necessary to help DGS-ES skip the local maxima. As σ becomes smaller, the DGS gradient ∇M σ,Ξ[J](θ) converges to the local gradient ∇J(θ), which may get the optimizer trapped in a local maximum.
5 CONCLUSION
Despite the successful demonstration shown in §4, there are several limitations with the DGS-ES method for reinforcement learning. First, it requires a powerful enough cluster or distributed computing resources to show superior performance. Even though more and more parallel environments for complex RL tasks, those parallel codes might not compatible with a cluster/supercomputer with a speciﬁc architecture. This will need some extra effort to modify environment codes, in order to exploit the advantage of the DGS-ES approach. Second, asynchronization between different environment simulations may drag

Figure 3: Illustration of the effect of the radius σ of ∇M σ,Ξ[J](θ) on the performance of the DGS-ES method in solving the Reacher-v0 problem.
down the total performance. In a distributed computing system, all the parallel workers receive the same number of environment simulations. However, as different parameter values may lead to different termination times of the environment simulations, there will be a potential waste of computing resources due to such asynchronization. Thus, a better scheduling algorithm is needed to further improve the performance of DGS-ES in RL. Third, even though the performance of the DGS-ES is not very sensitive to the hyper-parameters, especially the radius σ in the experiments conducted in this work, optimal or even viable hyper-parameters of the DGS-ES method are still problem dependent, which means hyper-parameter tuning may be needed when applying the method to another RL problem.
Acknowledgement
This material was based upon work supported by the U.S. Department of Energy, Ofﬁce of Science, Ofﬁce of Advanced Scientiﬁc Computing Research, Applied Mathematics program under contract and award numbers ERKJ352, by the DOE SciDac FastMath project, and by the Artiﬁcial Intelligence Initiative at the Oak Ridge National Laboratory (ORNL). ORNL is operated by UTBattelle, LLC., for the U.S. Department of Energy under Contract DE-AC05-00OR22725.
References
Abramowitz, M. and Stegun, I. (eds.). Handbook of Mathematical Functions. Dover, New York, 1972.
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J.,

Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and Zaremba, W. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048–5058, 2017.
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-based exploration and intrinsic motivation. Advances in Neural Information Processing Systems, pp. 1471–1479, 2016.
Berahas, A. S., Cao, L., Choromanskiv, K., and Scheinberg, K. A theoretical and empirical comparison of gradient approximations in derivative-free optimization. arXiv:1905.01332, 2019.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Choromanski, K., Rowland, M., Sindhwani, V., Turner, R. E., and Weller, A. Structured evolution with compact architectures for scalable policy optimization. International Conference on Machine Learning, pp. 969–977, 2018.
Choromanski, K., Pacchiano, A., Parker-Holder, J., , and Tang, Y. From complexity to simplicity: Adaptive esactive subspaces for blackbox optimization. NeurIPS, 2019a.
Choromanski, K., Pacchiano, A., Parker-Holder, J., , and Tang, Y. Provably robust blackbox optimization for reinforcement learning. arXiv:1903.02993, 2019b.
Colas, C., Sigaud, O., and Oudeyer, P. Y. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. arXiv preprint arXiv: 1802.05054., 2018.
Conti, E., Madhavan, V., Such, F. P., Lehman, J., Stanley, K. O., and Clune, J. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. NIPS, 2018.
Coumans, E. and Bai, Y. Pybullet, a python module for physics simulation for games, robotics and machine learning. GitHub repository, 2016.
Cui, X., Zhang, W., Tüske, Z., and Picheny, M. Evolutionary stochastic gradient descent for optimization of deep neural networks. NeurIPS, 2018.
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P. Openai baselines. https://github. com/openai/baselines, 2017.
Fazel, M., Ge, R., Kakade, S. M., and Mesbahi, M. Global convergence of policy gradient methods for the linear quadratic regulator. Proceedings of the 35th International Conference on Machine Learning, 80:1467– 01476, 2018.

Flaxman, A. D., Kalai, A. T., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: gradient descent without a gradient. Proceedings of the 16th Annual ACM-SIAM symposium on Discrete Algorithms, pp. 385–394, 2005.
Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Has-sabis, D., Pietquin, O., and et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.
Fujimoto, S., Van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. Advances in Neural Information Processing Systems, pp. 2450–2462, 2018.
Hansen, N. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.
Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159–195, 2001.
Houthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F. D., and Abbeel, P. Vime: Variational information maximizing exploration. Advances in Neural Information Processing Systems, pp. 1109–1117, 2016.
Houthooft, R., Chen, Y., Isola, P., Stadie, B., Wolski, F., Ho, O. J., and Abbeel, P. Evolved policy gradients. Advances in Neural Information Processing Systems, pp. 5400–5409, 2018.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. ICLR, 2016.
Liu, G., Zhao, L., Yang, F., Bian, J., Qin, T., Yu, N., and Liu, T.-Y. Trust region evolution strategies. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 4352–4359, 2019.
Maheswaranathan, N., Metz, L., Tucker, G., Choi, D., and Sohl-Dickstein, J. Guided evolutionary strategies: Augmenting random search with surrogate gradients. Proceedings of the 36th International Conference on Machine Learning, 2019.
Meier, F., Mujika, A., Gauy, M. M., and Steger, A. Improving gradient estimation in evolutionary strategies with past descent directions. Optimization Foundations for Reinforcement Learning Workshop at NeurIPS, 2019.
Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., and Sohl-dickstein, J. Learned optimizers that outperform sgd on wall-clock and test loss. 2nd Workshop on Meta-Learning at NeurIPS, 2018.

Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B., Shahrzad, H., Navruzyan, A., Nuffy, N., and Hodjat, B. Evolving deep neural networks. arXiv preprint arXiv:1703.00548, 2017.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., and et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. ICML, pp. 1928–1937, 2016.
Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., et al. Ray: A distributed framework for emerging {AI} applications. In 13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18), pp. 561–577, 2018.
Morse, G. and Stanley, K. O. Simple evolutionary optimization can rival stochastic gradient descent in neural networks. Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2016), pp. 477–484, 2016.
Müller, N. and Glasmachers, T. Challenges in highdimensional reinforcement learning with evolution strategies. In International Conference on Parallel Problem Solving from Nature, pp. 411–423. Springer, 2018.
Nesterov, Y. and Spokoiny, V. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, pp. 1–40, 2015.
Ostrovski, G., Bellemare, M. G., v. d. Oord, A., and Munos, R. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in pytorch. 2017.
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self- supervised prediction. International Conference on Machine Learning (ICML), 2017.
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychowicz, M. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.
Pourchot, A. and Sigaud, O. Cem-rl: Combining evolutionary and gradient-based methods for policy search. ICLR, 2019.

Quarteroni, A., Sacco, R., and Saleri, F. Numerical Mathematics, volume 332. Springer Science Business Media &, 2007.
Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y. L., Tan, J., Le, Q. V., and Kurakin, A. Large-scale evolution of image classiﬁers. International Conference on Machine Learning (ICML), pp. 2902–2911, 2017.
Rechenberg, I. and Eigen, M. Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzipien der Biologischen Evolution. Frommann-Holzboog Stuttgart, 1973.
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. From complexity to simplicity as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and Moritz, P. Trust region policy optimization. ICML, pp. 1889–1897, 2015.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Schwefel, H.-P. Numerische optimierung von computermodellen mittels der evolutionsstrategie. 1977.
Sehnke, F., Osendorfer, C., Ruckstieb, T., Graves, A., Peters, J., and Schmidhuber, J. Parameter-exploring policy gradients. Neural Networks, 2010.
Sigaud, O. and Stulp, F. Robot skill learning: From reinforcement learning to evolution strategies. Paladyn Journal of Behavioral Robotics, 4(1):49–61, 2013.
Sigaud, O. and Stulp, F. Policy search in continuous action domains: An overview. Neural Networks, 113: 28–40, 2019.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Driessche, G. V. D., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., and M. Lanctot, e. a. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.
Such, F. P., Madhavan, V., Conti, E., Lehman, J., Stanley, K. O., and Clune, J. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.
Sutton, R. and Barto, A. G. (eds.). Reinforcement learning: An introduction, volume 1. MIT press Cambridge, New York, 1998.
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O. X., Duan, Y., Schulman, J., DeTurck, F., and Abbeel, P. # exploration: A study of count-based exploration for deep reinforcement learning. Advances in Neural Information Processing Systems, pp. 2750–2759, 2017.

Wierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and Schmidhuber, J. Natural evolution strategies. Journal of Machine Learning Research, 15:949–980, 2014.
Zhang, J., Tran, H., Lu, D., and Zhang, G. A Scalable Evolution Strategy with Directional Gaussian Smoothing for Blackbox Optimization. arXiv e-prints, art. arXiv:2002.03001, Feb 2020.

