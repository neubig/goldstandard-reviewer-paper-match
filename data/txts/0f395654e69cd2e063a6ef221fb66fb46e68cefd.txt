arXiv:2012.10203v2 [cs.LG] 14 Jan 2021

Classiﬁcation with Strategically Withheld Data
Anilesh K. Krishnaswamy1, Haoming Li2, David Rein1, Hanrui Zhang1, and Vincent Conitzer1
1Duke University 2University of Southern California
January 15, 2021
Abstract Machine learning techniques can be useful in applications such as credit approval and college admission. However, to be classiﬁed more favorably in such contexts, an agent may decide to strategically withhold some of her features, such as bad test scores. This is a missing data problem with a twist: which data is missing depends on the chosen classiﬁer, because the speciﬁc classiﬁer is what may create the incentive to withhold certain feature values. We address the problem of training classiﬁers that are robust to this behavior. We design three classiﬁcation methods: MINCUT, HILL-CLIMBING (HC) and Incentive-Compatible Logistic Regression (IC-LR). We show that MINCUT is optimal when the true distribution of data is fully known. However, it can produce complex decision boundaries, and hence be prone to overﬁtting in some cases. Based on a characterization of truthful classiﬁers (i.e., those that give no incentive to strategically hide features), we devise a simpler alternative called HC which consists of a hierarchical ensemble of outof-the-box classiﬁers, trained using a specialized hill-climbing procedure which we show to be convergent. For several reasons, MINCUT and HC are not effective in utilizing a large number of complementarily informative features. To this end, we present IC-LR, a modiﬁcation of Logistic Regression that removes the incentive to strategically drop features. We also show that our algorithms perform well in experiments on real-world data sets, and present insights into their relative performance in different settings.
1 Introduction
Applicants to most colleges in the US are required to submit their scores for at least one of the SAT and the ACT. Both tests are more or less equally popular, with close to two million taking each in 2016 [Adams, 2017]. Applicants usually take one of these two tests – whichever works to their advantage.1 However, given the growing competitiveness of college admissions, many applicants now take both tests and then strategically decide whether to drop one of the scores (if they think it will hurt their application) or report both.2 The key issue here is that it is impossible to distinguish between an applicant who takes both tests but reports only one, and an applicant that takes only one test—for example because the applicant simply
1 https://www.princetonreview.com/college/sat- act 2 https://blog.collegevine.com/should- you- submit- your- sat- act- scores/
1

took the one required by her school, the dates for the other test did not work with her schedule, or for other reasons that are not strategic in nature.3
Say a college wants to take a principled machine learning approach to making admission decisions based on the scores from these two tests. For simplicity, assume no other information is available. Assume that the college has enough historical examples that contain the scores of individuals (on whichever tests are taken, truthfully reported) along with the corresponding ideal (binary) admission decisions.4 Based on this data, the college has to choose a decision function that determines which future applicants are accepted. If this function is known to the applicants, they are bound to strategize and use their knowledge of the decision function to decide the scores they report.4 How can the classiﬁer be trained to handle strategic reporting of scores at prediction time?
To see the intricacies of this problem, let us consider a simple example.
Example 1. Say the scores for each of the two tests (SAT and ACT) take one of two values: h (for high) or l (for low). Let ∗ denote a missing value. Then there are eight possible inputs (excluding (∗, ∗) since at least one score is required): (h, h), (h, l), (l, h), (l, l), (h, ∗), (∗, h), (l, ∗) and (∗, l). Assume the natural distribution (without any withholding) over these inputs is known, and so are the conditional probabilities of the label Y ∈ {0, 1}, as shown below:

Table 1: True distribution of inputs and targets:

X

(h, h) (h, l) (l, h) (l, l) (h, ∗) (∗, h) (l, ∗) (∗, l)

P r(X)

1/8 1/8 1/8 1/8 1/8 1/8 1/8 1/8

P r(Y = 1 | X) 0.9 0.7 0.3 0.1 0.6 0.6 0.2 0.2

P r(Y = 0 | X) 0.1 0.3 0.7 0.9 0.4 0.4 0.8 0.8

Assume Y = 1 is the more desirable ”accept” decision. Then, ideally, we would like to predict Y = 1 whenever X ∈ {(h, h), (h, l), (h, ∗), (∗, h)}. However, the strategic reporting of scores at prediction time
effectively means, for example, that an input (∗, h) cannot be assigned the accept decision of Y = 1 unless the same is done for (l, h) as well; otherwise, someone with (l, h) would simply not report the ﬁrst test, thereby misreporting (∗, h) and being accepted. Taking this into account, the classiﬁer with minimum error
is given by Y = 1 whenever X ∈ {(h, h), (h, l), (h, ∗)}.
There are many other settings where a similar problem arises. Many law schools now allow applicants to choose between the GRE and the traditional LSAT.5 Recently, as a result of the COVID-19 pandemic, universities have implemented optional pass/fail policies, where students can choose to take some or all of their courses for pass/fail credit, as opposed to a standard letter grade that inﬂuences their GPA. They are often able to decide the status after already knowing their performance in the course. For credit scoring, some individuals might not report some of their information, especially if it is not mandatory by law [FlorezLopez, 2010].
The ability of strategic agents to withhold some of their features at prediction time poses a challenge only when the data used to train the classiﬁer has some naturally missing components to begin with. For if not, the principal – e.g., the entity deciding on admissions – can reject all agents that withhold any of their features, thereby forcing them to reveal all features. We focus on how a principal can best train classiﬁers that are robust even when there is strategic withholding of data by agents. Our methods produce classiﬁers that eliminate the incentive for agents to withhold data.
3 https://blog.prepscholar.com/do- you- need- to- take- both- the- act- and- sat 4We make these assumptions more generally throughout the paper. 5https://www.ets.org/gre/revised general/about/law/

2

Our contributions We now describe the key questions we are facing, and how we answer them. Our model is described formally in Section 2. All proofs are in the Supplement.
If the true input distribution is known, can we compute the optimal classiﬁer? (Section 3) We answer this question in the afﬁrmative by showing that the problem of computing the optimal classiﬁer (Theorem 1) in this setting reduces to the classical Min-cut problem [Cormen et al., 2009]. This analysis gives us the MINCUT classiﬁer, which can be computed on the empirical distribution, estimated using whatever data is available. However, since it can potentially give complex decision boundaries, it might not generalize well.
Are there simpler classiﬁers that are robust to strategic withholding of features? (Section 4) We ﬁrst characterize the structure of classiﬁers that are “truthful”, i.e., give no incentive to strategically hide features at prediction time (Theorem 2). Using this characterization, we devise a hill-climbing procedure (HC) to train a hierarchical ensemble of out-of-the-box classiﬁers and show that the procedure converges (Theorem 4) as long as we have black-box access to an agnostic learning oracle. We also analytically bound the generalization error of HC (Theorem 3). The ensemble of HC can be populated with any of the commonly used classiﬁers such as logistic regression, ANNs, etc.
Another truthful classiﬁer we present is a modiﬁcation of Logistic Regression. This method, called ICLR (Incentive Compatible Logistic Regression), works by encoding all features with positive values, and using positive regression coefﬁcients – whereby it is in every agent’s best interest to report all features truthfully. IC-LR uses Projected Gradient Descent for its training. The advantage of this method is that it can be directly to a large number of features.
How do our methods perform on real data sets? (Section 6) We conduct experiments on several realworld data sets to test the performance of our methods, comparing them to each other, as well as to other methods that handle missing data but ignore the strategic aspect of the problem. We see that our methods perform well overall, and uncover some interesting insights on their relative performance:
1. When the number of features is small, HC is the most reliable across the board.
2. When the number of features is small, and many of them are discrete/categorical (or suitably discretized), MINCUT and IC-LR perform better.
3. If a large number of features must be used, IC-LR gives the best performance, although HC performs reasonably well with some simple feature selection techniques.
Related work Our work falls broadly in the area of strategic machine learning, wherein a common assumption is that strategic agents can modify their features (i.e., misreport) in certain ways (normally at some cost), either to improve outcomes based on the classiﬁer chosen by the principal [Hardt et al., 2016] or to inﬂuence which classiﬁer is chosen in the ﬁrst place [Dekel et al., 2010a]. The main challenge in strategic machine learning, as in this paper, is the potential misalignment between the interests of the agents and the principal. Existing results in this line of work [Chen et al., 2018, Kleinberg and Raghavan, 2019, Haghtalab et al., 2020], often mainly theoretical, consider classiﬁers of a speciﬁc form, say linear, and ways of misreporting or modifying features in that context. Our results are different in that we focus on a speciﬁc type of strategic misreporting, i.e., withholding parts of the data, and devise general methods that are robust to this behavior that, in addition to having theoretical guarantees, can be tested practically. Some experimental results [Hardt et al., 2016] do exist – but our work is quite different; for instance, we do not need to invent a cost function (as in Hardt et al. [2016]). Another major difference is that we consider generalization in the presence of strategic behavior, while most previous work does not (except for a concurrent paper [Zhang and Conitzer, 2021]), which studies the sample complexity of PAC learning in the presence of strategic behavior).
3

Our problem can also be viewed as an instance of automated mechanism design with partial veriﬁcation [Green and Laffont, 1986, Yu, 2011, Kephart and Conitzer, 2015, 2016] where it is typically assumed that the feature space (usually called type space in mechanism design) is discrete and has reasonably small cardinality, and a prior distribution is known over the feature space. In contrast, the feature spaces considered in this paper consist of all possible combinations of potentially continuous feature values. Moreover, the population distribution can only be accessed by observing examples. Thus, common methodologies in automated mechanism design do not sufﬁce for our setting.
A set of closely related (in particular, to Theorem 1) theoretical results are those of Zhang et al. [2019b,a, 2021b] on the problem of distinguishing “good” agents from “bad” (where each produces a different distribution over a sample space, and the agent can misreport the set of n samples that she has drawn). However, our work is different in that we consider the standard classiﬁcation problem, we focus more on practical aspects, and we do not rely on the full knowledge of the input distribution.
Our work also ﬁnds a happy intersection between strategic machine learning and the literature on classiﬁcation with missing data [Marlin, 2008]. The problem we study is also connected to adversarial classiﬁcation [Dalvi et al., 2004, Dekel et al., 2010b]. We discuss these connections in more detail in the Supplement.

2 Preliminaries
We now describe our model and the requisite notation.

Model with strategically withheld features: We have an input space X , a label space Y = {0, 1}, and a distribution D over X × Y which models the population. A classiﬁer f : X → Y maps a combination of features to a label. Let F = [k] = {1, . . . , k} be the set of features, each of which a data point may or may not have. For x ∈ X , let xi denote the value of its i-th feature (xi = ∗ if x does not have feature i ∈ [k]). For any S ⊆ [k], deﬁne x|S to be the projection of x onto S (i.e., retain features in S and drop those not in S):

(x|S)i = xi, if i ∈ S ∗, otherwise.

We assume that data can be strategically manipulated at prediction (test) time in the following way: an
agent whose true data point is x can report any other data point x such that x|S = x for some S ⊆ [k]. We use → to denote the relation between any such pair x, x (x → x ⇐⇒ ∃S ⊆ [k] : x|S = x ). Note that → is transitive, i.e., for any x1, x2, x3 ∈ X , x1 → x2 and x2 → x3 =⇒ x1 → x3.
We assume agents prefer label 1 to 0: in response to a classiﬁer f , an agent with data point x will always withhold 6 features to receive label 1 if possible, i.e., the agent will report x ∈ argmaxx :x→x f (x ). Incorporating such strategic behavior into the loss of a classiﬁer f , we get

D(f ) = Pr y = max f (x ) .

(x,y)∼D

x :x→x

Truthful classiﬁers We will also be interested in truthful classiﬁers, which provably eliminate incentives for such strategic manipulation. A classiﬁer f is truthful if for any x, x ∈ X where x → x , f (x) ≥ f (x ).
6In practice, f might not be perfectly known, and agents might not be able to best respond. This problem does not arise for our methods, since they are truthul. For other classiﬁers, their accuracy may go up or down if agents fail to best-respond; but the assumption that agents best-respond is common in many such contexts.

4

In other words, not withholding any features is always an optimal way to respond to a truthful classiﬁer. As a result, the loss of any truthful classiﬁer f in the presence of strategically withheld features has the standard form: D(f ) = Pr(x,y)∼D[f (x) = y].
Note that the so-called Revelation Principle – which states that in the presence of strategic behavior, any classiﬁer f is equivalent to a truthful classiﬁer f – holds in this case because the reporting structure is transitive.7 In other words, we are guaranteed that, for any classiﬁer f , there exists a truthful classiﬁer f , such that for any x ∈ X , maxx :x→x f (x ) = f (x). Therefore, we focus on truthful classiﬁers in our model, without loss of generality.
3 The MINCUT Classiﬁer
We ﬁrst present a method for computing an optimal classiﬁer when the input distribution is fully known.8 Assuming X is ﬁnite, our goal is to characterize a classiﬁer f ∗ which minimizes the loss D(.), for a known input distribution D. As shorthand, deﬁne, for all x ∈ X ,
Deﬁnition 1. D+(x) := Pr(x ,y )∼D[x = x ∧ y = 1], D−(x) := Pr(x ,y )∼D[x = x ∧ y = 0].
The basic idea here is simple: to partition X into two sides, one labeled 1 and the other 0, where the error accrued for each x ∈ X is given by D−(x) or D+(x), according as x is labeled 1 or 0. Such a partition should crucially respect the constraints imposed by the strategic behavior of agents : if x → x , then either x is labeled 1 or x is labeled 0.
Deﬁnition 2. Given X and D, let G(D, X ) be a directed capacitated graph with vertices V = X ∪ {s, t}, where the edges E and edge capacities u are deﬁned as follows:
• For each x ∈ X , there are edges (s, x) and (x, t) in E, with capacities u(s, x) = D−(x) and u(x, t) = D+(x).
• For all pairs x, x ∈ X such that x → x , there is an edge (x, x ) ∈ E with capacity u(x, x ) = ∞.
In terms of the graph deﬁned above, computing the optimal classiﬁer f ∗ we seek is equivalent to ﬁnding a minimum s-t cut on G(D, X ). The intuition is that the edges from s and to t reﬂect the value gained from labeling an example 0 or 1, respectively; one of the edges must be cut, reﬂecting the loss of not assigning it to the corresponding side. Moreover, if x → x , then the corresponding edge with inﬁnite capacity prevents the assigning of 0 to x and 1 to x .
Theorem 1. If (S, S¯) is a minimum s-t cut of G(D, X ) (where S is on the same side as s), then for the
classiﬁer f ∗(x) := 1(x ∈ S¯), we have D(f ∗) = minf D(f ).
We note that, consequently, the optimal classiﬁer can be computed in poly(|X |) time. In practice, it is natural to expect that we do not know D exactly, but have a ﬁnite number of samples from it. A more practical option is to apply Theorem 1 to the empirical distribution induced by the samples observed, and hope for the classiﬁer computed from that to generalize to the true population distribution D.
7More details, including a formal proof, are in the Supplement. 8A theoretical companion paper Zhang et al. [2021a] contains a more general version of the mincut-based algorithm. There, the goal is to compute an optimal classiﬁer with possibly more than 2 outcomes given perfect knowledge of the entire population distribution. In this paper, we investigate the special case with only 2 outcomes (i.e., accept and reject), but do not assume prior knowledge about the population distribution.
5

Implementing MINCUT Given a set X of m i.i.d. samples from D, let D be the corresponding empirical distribution over X , and X¯ := X ∪ {x : x → x, ∃x ∈ X }. The MINCUT classiﬁer is then obtained by applying Theorem 1 to G(D, X ), and extending it to X¯ as and when required. Here, note that MINCUT runs in time poly(m) (and not poly(|X |)), since G(D, X ) has m nodes, and checking if a test point is in X¯ takes poly(m) time.
In light of traditional wisdom, the smaller m is relative to X , the larger the generalization error of MINCUT will be. We do not attempt a theoretical analysis in this regard, but note that when X is large, the generalization error can be extremely large (see Example 2 in the Supplement). The reason for this is two-fold:
1. MINCUT can give complicated decision boundaries.
2. MINCUT is indecisive on samples not in X¯.9
Therefore, a suitable discretization of features is sometimes useful (see Section 6). Note that MINCUT is truthful, by virtue of the inﬁnite capacity edges in Deﬁnition 2.
4 Truthful classiﬁers and HILL-CLIMBING
The other drawback of MINCUT, related to the issue of generalization just discussed, is that it can be hard to interpret meaningfully in a practical setting. In this section, we devise a simpler alternative called HILLCLIMBING. To help introduce this algorithm, we ﬁrst present a characterization of truthful classiﬁers in our setting, since we can limit our focus to them without loss of generality (as discussed in Section 2). For shorthand, we use the following deﬁnition:
Deﬁnition 3 (F -classiﬁer). For a subset of features F ⊆ F , a classiﬁer f is said to be an F -classiﬁer if for all x ∈ X , we have f (x) = f (x|F ), and if there exists i ∈ F such that xi = ∗, then f (x) = 0.
In other words, an F -classiﬁer depends only on the values of the features in F , rejecting any x where any of these is empty. We can collect many such classiﬁers into an ensemble as follows:
Deﬁnition 4 (MAX Ensemble). For a collection of classiﬁers C = {fj}, its MAX Ensemble classiﬁer is given by MAXC(.) := maxj fj(.).
This is equivalent to getting each agent to pick the most favorable classiﬁer from among those in {fj}. Now using the above deﬁnitions we have the following characterization of truthful classiﬁers:
Theorem 2. A classiﬁer f is truthful iff f (.) = MAXC(.) for a collection of classiﬁers C = {fj} such that, for some {Fj} ⊆ 2F , each fj is an Fj-classiﬁer .
Now, for any truthful classiﬁer f , we can bound the gap between its population loss D(f ) and its empirical loss on a set of samples X denoted by X (f ) := m1 i∈[m] |f (xi) − yi|. Before stating a theorem to this end, we deﬁne the following entities: Let H be a base hypothesis space over X , and n ∈ {1, . . . , 2k} be a parameter. Deﬁne d := dVC(H) to be the VC dimension of H. Deﬁne H¯ as the set of all classiﬁers that can be written as the MAX Ensemble of n classiﬁers in H. Theorem 3. Let X = {(xi, yi)}i∈[m] be m i.i.d. samples from D. For any f ∈ H¯, for any δ > 0, with probability at least 1 − δ, we have D(f ) ≤ X (f ) + O dn·log dn·lomg m+log(1/δ) .
9This is more likely to happen when using a large number of features.
6

It is easy to see that for any of the commonly used hypothesis spaces – say H consists of linear hypotheses – if a truthful classiﬁer f is in H, then so are the components of the MAX Ensemble version of f as in Theorem 2. We have, however, stated Theorem 3 in slightly more general terms.

The HILL-CLIMBING classiﬁer: In what follows, we present a hill-climbing approach with provable convergence and generalization guarantees. The HILL-CLIMBING classiﬁer, henceforth referred to as HC, is of the same form as given by the characterization of truthful classiﬁers in Theorem 2.10 Intuitively, the approach works by considering a hierarchy of classiﬁers, organized by the features involved. For example, consider a setting with k = 3 features. We make a choice as to what classiﬁers we use — say f1 for input of the form (x1, ∗, ∗), f2 for input of the form (x1, x2, ∗), and f3 for input of the form (x1, x2, x3). Any agent with features 1 and 2 (but not 3), for example, should be able to report both features so as to be classiﬁed by f2, or feature 2 to be classiﬁed by f1 instead. So in effect, assuming full knowledge of the classiﬁers, each agent can check all of the classiﬁers and choose the most favorable one. Without loss of generality, we assume that when a data point does not have all the features required by a classiﬁer, it is automatically rejected.

Algorithm 1 HILL-CLIMBING (HC) Classiﬁer

Input: data set X = {(xi, yi)}i∈[m], n subsets F1, F2, . . . , Fn of F. Initialize: t ← 0, {f10, . . . , fn0}.

while ∆ > 0 do

for i = 1, 2, . . . , n do

Si

←

{(x, y)

∈

X

:

f

t j

(x

|

Fj

)

=

0, ∀j

=

i}.

fit+1 = argminf∈H (x,y)∈Si |f (x|Fi ) − y|.

end for

f ∗ ← MAX{f t+1,...,f t+1}; t = X (f ∗)

1

n

∆ ← t − t−1; t ← t + 1

end while

Return: f ∗.

In short, HC (deﬁned formally in Algorithm 1) works as follows: ﬁrst choose a hypothesis space H, in order for Theorem 3 to apply. Then select n subsets of F (where n is a parameter), say F1, F2, . . . , Fn. For each Fj, we learn a Fj-classiﬁer, say fj, from among those in H. Start by initializing these classiﬁers to any suitable {f10, . . . , fn0}. In each iterative step, each of the subclassiﬁers is updated to minimize the empirical loss on the samples that are rejected by all other classiﬁers. We next show that such an update procedure always converges. To do so, as far as our theoretical analysis goes, we assume we have black-box access to an agnostic learning oracle (Line 6 in Algorithm 1). After convergence, the HC classiﬁer is obtained as the MAX Ensemble of these classiﬁers. The generalization guarantee of Theorem 3 applies directly to the HC classiﬁer.
Theorem 4. Algorithm 1 converges.

Connection with MINCUT: The HC formulation given above can be thought of as a less complicated version of MINCUT: some of the edge constraints are ignored with respect to learning the individual classiﬁers, and are instead factored in via the MAX function. Say F1 ⊂ F2. For some x, it is possible that f1(x|F1 ) = 1 and f2(x|F1 ) = 0. In other words, the individual classiﬁers could violate the MINCUT constraints, in order to
10And, therefore, is truthful, and inherits Theorem 3.

7

learn classiﬁcation functions that generalize well individually, and also collectively thanks to the combined HC training procedure.
Implementing HC: In practice, the classiﬁers {f1, f2, . . . , fn} in HC can be populated with any standard out-of-the-box methods such as logistic regression or neural networks, the choice of which can inﬂuence the performance of f . In Section 6, we test HC with a few such options. The assumption of having access to an agnostic learning oracle does not play a crucial role in practice, with standard training methods performing well enough to ensure convergence. Also, HC will converge in at most m (number of training examples) iterations, because in each iteration the number of correctly classiﬁed examples increases by at least one. (An iteration may need to train n individual classiﬁers.) This also means there is no difference between checking whether ∆ > 0 or ∆ ≥ 1/m. In our experiments, we run HC using ∆ ≥ 10−4, and convergence is achieved pretty quickly (see the Supplement for exact details).
Choosing subsets: Note that we are free to choose any F1, F2, . . . , Fn to deﬁne HC. Its generalization (via Theorem 3), will depend on the choice of n. As more and more subsets of features are included (and further binning them based on their values), HC starts behaving more and more like MINCUT. In addition, using a large number of subsets increases the computational complexity of HC. In practice, therefore, the number of subsets must be limited somehow – we ﬁnd that some simple strategies like the following work reasonably well: (a) selecting a few valuable features and taking all subsets of those features, (b) taking all subsets of size smaller than a ﬁxed number k, say k = 2. In many practical situations, a few features (possibly putting their values in just a few bins) are often enough to get close to optimal accuracy, also improving interpretability (e.g., see Wang and Rudin [2015] or Jung et al. [2017]) The question of devising a more nuanced algorithm for selecting these subsets merits a separate investigation, and we leave this to future work.
5 Incentive-Compatible Logistic Regression
As we just mentioned, it is challenging to directly apply HC and MINCUT to a large number of features. As we will see, we can address this challenge in various ways to still get very strong performance with HC. Moreover, HC enjoys remarkable generality, generalization and convergence guarantees. Nevertheless, we would like to have an algorithm that tries to make use of all the available features, while still being truthful. In this section, we present such an approach, which, as we show later in Section 6, indeed performs comparably to – and in some cases better than – MINCUT and HC.
Below we present a simple and truthful learning algorithm, Incentive-Compatible Logistic Regression (IC-LR), which is a truthful variant of classical gradient-based algorithms for logistic regression. Recall that in logistic regression, the goal is to learn a set of coefﬁcients {βi}, one for each feature i ∈ F , as well as an intercept β0, such that for each data point (x, y), the predicted label yˆ given by
yˆ = 1 σ(β0 + xi · βi) ≥ 0.5
i∈F
ﬁts y as well as possible, where σ(t) = 1/(1 + e−t) is the logistic function. Roughly speaking, IC-LR. (formally deﬁned in Algorithm 2) works by restricting the coefﬁcients {βi} in such a way that dropping a feature (i.e., setting xi to 0) can never make the predicted label larger. If, without loss of generality, all
8

feature values xi are nonnegative11, then this is equivalent to: for each feature i ∈ F , the coefﬁcient βi ≥ 0. IC-LR. enforces this nonnegativity constraint throughout the training procedure, by requiring a projection step after each gradient step, which projects the coefﬁcients to the feasible nonnegative region by setting any negative coefﬁcient to 0 (equivalently, an 1 projection).

Algorithm 2 Incentive-Compatible Logistic Regression

Input: data set X = {(x, y)}, learning rate {ηt}, δ ≥ 0.

Initialize: t ← 0, {β0, β1, . . . , βk}.

while ∆ > δ do

gi ← 0 for all i ∈ {0, 1, . . . , k}

for (x, y) ∈ X do

g0 ← g0 + σ β0 + for i ∈ F do

i∈F xi · βi − y

gi ← gi + (σ β0 + end for

i∈F xi · βi − y) · xi

end for

∀i ∈ {0, 1, . . . , k}, βi ← max{βi − ηt · gi, 0}
f ∗(x) := 1 σ β0 + i∈F βi · xi ≥ 0.5
t = X (f ∗); ∆ ← t − t−1; t ← t + 1 end while
Return: f ∗.

One potential issue with IC-LR. is the following: if a certain feature xi ≥ 0 is negatively correlated with the positive classiﬁcation label, then IC-LR is forced to ignore it (since it is constrained to use positive coefﬁcients). To make good use of this feature, we can include an inverted copy xi = λ − xi (where λ is chosen such that xi ≥ 0). We could also choose an apt discretization of such features (using cross-validation) and translate the discretized bins into separate binary variables. Such a discretization can account for more
complex forms of correlation, e.g., a certain feature’s being too high or too low me makes the positive
label likelier. In practice, we ﬁnd that the latter method does better. If such transformations are undesirable,
perhaps for reasons of complexity or interpretability, HC methods are a safer bet.

6 Evaluation
In this section, we show that, when strategic withholding is at play, MINCUT, HC and IC-LR perform well and provide a signiﬁcant advantage over several out-of-the-box counterparts (that do not account for strategic behavior).
Datasets Four credit approval datasets are obtained from the UCI repository [Dua and Graff, 2017], one each from Australia, Germany, Poland and Taiwan. As is common for credit approval datasets, they are imbalanced to various degrees. In order to demonstrate the performance of classiﬁers in a standard, controlled setting, we balance them by random undersampling. There is a dedicated community [Chawla et al., 2004] that looks at the issue of imbalanced learning. We do not delve into these issues in our paper, and evaluate our methods on both balanced and imbalanced datasets (see the Supplement for the latter). In addition, to
11If not, they can be suitably translated.

9

Table 2: Data set summary statistics (num. = numerical, cat. = categorical)

Data set
Australia Germany Poland Taiwan

Size
690 1000 5910 30,000

Total # of features
15 20 64 23

Size after balancing
614 600 820 13,272

Features after restriction
2 num., 2 cat. 1 num., 3 cat.
4 num. 4 ordinal

demonstrate the challenge of high-dimensional data imposed on some of the classiﬁcation methods, the experiments are run on the datasets (a) restricted to 4 features,12 and (b) with all available features. The basic characteristics of the datasets are summarized in Table 2 – note that there is enough variation in terms of the types of features present. We then randomly remove a fraction = 0, 0.1, . . . , 0.5 of all feature values in each dataset to simulate data that is missing “naturally” – i.e., not due to strategic withholding.
Testing We test all methods under two ways of reporting: “truthful”, i.e., all features are reported as is, and “strategic”, i.e., some features might be withheld if it leads to a better outcome. We measure the test accuracy of each classiﬁer, averaged over N=100 runs, with randomness over the undersampling and the data that is randomly chosen to be missing, to simulate data missing for non-strategic reasons. Other metrics, and details about implementing and training the classiﬁers, are discussed in the Supplement. It is important to note that for testing any method, we have to, in effect, compute the best response of each data point toward the classiﬁer. Since the methods we propose are truthful, this is a trivial task. But for other methods, this might not be easy, thereby limiting what baselines can be used.
Classiﬁers We evaluate our proposed methods, MINCUT, HC with logistic regression (HC (LR)) and neural networks (HC (ANN)) as subclassiﬁers, and incentive-compatible logistic regression (IC-LR), against several baseline methods.
First, they will be compared against three out-of-the-box baseline classiﬁers: logistic regression (LR), neural networks (ANN) and random forest (RF). We select LR for its popularity in credit approval applications; we select ANN for it being the best-performing individual classiﬁer on some credit approval datasets [Lessmann et al., 2015]; we select RF for it being the best-performing homogeneous ensemble on some credit approval datasets [Lessmann et al., 2015], as HC can be viewed as a homogeneous ensemble method. For the sake of exposition, we present numbers just for baselines based on LR, as they perform relatively better.
Second, for the purposes of comparison, we include MAJ – predict the majority label if examples with the exact same feature values appeared in the training set, and reject if not – which can be thought of as a non-strategic counterpart of MINCUT. We also include k-nearest neighbors (KNN) as a baseline, since it is closely related to MAJ.
These out-of-the-box classiﬁers need help dealing with missing data, whether they are missing naturally at training and test time or strategically at test time, and to this end, we employ (a) IMP: mean/mode imputation [Lessmann et al., 2015], and (b) R-F: reduced-feature modeling [Saar-Tsechansky and Provost, 2007], for each of them.
12According to ANOVA F-value evaluated before dropping any feature values.

10

Table 3: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, balanced datasets, 4 features

Classiﬁer
HC(LR) MINCUT IC-LR IMP(LR) R-F(LR)

Australia
Tru. Str.
.792 .792 .770 .770 .788 .788 .796 .791 .808 .545

Germany
Tru. Str.
.639 .639 .580 .580 .654 .654 .663 .580 .631 .508

Poland
Tru. Str.
.659 .659 .501 .501 .639 .639 .714 .660 .670 .511

Taiwan
Tru. Str.
.648 .648 .652 .652 .499 .499 .670 .618 .665 .590

When the dataset has a large number of features, MINCUT and IC-LR can be directly applied. For HC, we assist it in two ways: (a) by selecting 4 features based on the training data, denoted by FS (feature selection),13 and (b) by choosing a limited number of small subsets (30 with 1 feature and 30 with 2 features), denoted by APP (approximation). Note that since our proposed methods are truthful, we can assume that features are reported as is. However, for all out-of-the-box classiﬁers, except IMP(LR), it is infeasible to simulate strategic withholding of feature values, due to the enormous number of combinations of features.
Last but not least, we test all methods with the discretization of continuous features (into categorical ones) [Fayyad and Irani, 1993], for reasons given in earlier sections.
6.1 Results
For the sake of exposition, we report results only for = 0.2. We also limit our exposition of HC, IMP and R-F methods to those based on logistic regression, as these perform better than their ANN/RF/KNN counterparts. For a comprehensive compilation of all results, along with standard deviation numbers, please refer to the Supplement.
With a small number of features (Table 3): As expected, the out-of-the-box baselines perform well under truthful reporting, but not with strategic reporting. Our methods are robust to strategic withholding, and in line with the earlier discussion on the potential issues faced by MINCUT and IC-LR (in Sections 3 and 5), we see that (a) HC(LR) performs most consistently, and (b) in some cases, MINCUT (e.g., Poland) and IC-LR (e.g., Taiwan) do not do well.
With discretization (Table 4): As expected, discretization of numerical features into binary categories improves the performance of MINCUT and IC-LR, for reasons explained in Sections 3 and 5 respectively. We also see some beneﬁt from discretization for HC(LR) when the features are mostly continuous (e.g., Poland), and less so when they are already discrete (e.g., Taiwan).
With a large number of features (Table 5): We see broadly similar trends here, except that in the case with discretization, IC-LR performs much better than before (e.g., Poland). The reason for this is that ICLR is able to use all the available features once they are discretized into binary categories. However, without discretization, HC methods are more reliable (e.g., Poland and Taiwan).
13Such a technique can be applied to other methods too – the results (see the Supplement) are not very different from those in Tables 4.

11

Table 4: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, balanced datasets, 4 features (“w/ disc.” stands for “with discretization of features”)

Classiﬁer

Australia Germany Poland Taiwan Tru. Str. Tru. Str. Tru. Str. Tru. Str.

HC(LR) w/ disc. .794 .794 .641 .641 .692 .692 .650 .650 MINCUT w/ disc. .789 .789 .629 .629 .692 .692 .649 .649 IC-LR w/ disc. .800 .800 .651 .651 .698 .698 .646 .646 IMP(LR) w/ disc. .799 .762 .652 .577 .719 .631 .686 .541 R-F(LR) w/ disc. .796 .542 .633 .516 .708 .522 .684 .587

Table 5: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, balanced datasets, all features

Classiﬁer

Australia Germany Poland Taiwan Tru. Str. Tru. Str. Tru. Str. Tru. Str.

HCFS(LR)

.795 .795 .625 .625 .678 .678 .648 .648

HCAPP(LR)

.777 .777 .617 .617 .658 .658 .638 .638

MINCUT

.496 .496 .499 .499 .499 .499 .499 .499

IC-LR

.798 .798 .654 .654 .607 .607 .588 .588

HCFS(LR) w/ disc. .794 .794 .632 .632 .694 .694 .649 .649

HCAPP(LR) w/ disc. .782 .782 .620 .620 .724 .724 .644 .644

MINCUT w/ disc. .534 .534 .503 .503 .499 .499 .550 .550

IC-LR w/ disc.

.805 .805 .653 .653 .773 .773 .667 .667

IMP(LR)

.802 .701 .663 .523 .729 .507 .657 .501

IMP(LR) w/ disc. .809 .723 .659 .554 .783 .503 .697 .501

On the out-of-the-box baselines: • Imputation-based methods are sensitive vis-a´-vis the mean/mode values used. There is incentive to drop a certain feature if the imputed value is a positive signal. If there are many such features, then these methods perform poorly, as seen in Table 5 (cf. Table 3, Australia). If the imputed values do not give a clear signal (e.g., when the distribution of each feature value is not skewed), there is a high variance in the performance of these methods (see the Supplement). In some cases, the benchmarks perform as well as, or slightly better than, our incentive-compatible classiﬁers. For example, in Table 3, for the Australia and Poland data sets, the accuracy of IMP(LR) and that of HC(LR) are within 0.001 of each other. This happens because the imputed values are, in these cases (but not in most of our other cases), negative indicators of the positive label, and therefore there is generally no incentive to strategically drop features. • Reduced-Feature modeling, despite performing well under truthful reporting, allows too many examples to be accepted under strategic reporting, which hurts its performance. This is true especially for smaller , as each subclassiﬁer has fewer examples to train on, giving several viable options for strategic withholding.
We note here that the variance (in the accuracy achieved) produced by our methods, since they are robust to strategic withholding, is much smaller than that of the baseline methods (exact numbers are deferred to the Supplement).
7 Conclusion
In this paper, we studied the problem of classiﬁcation when each agent at prediction time can strategically withhold some of its features to obtain a more favorable outcome. We devised classiﬁcation methods (MIN-

12

CUT, HC and IC-LR) that are robust to this behavior, and in addition, characterized the space of all possible truthful classiﬁers in our setting. We tested our methods on real-world data sets, showing that they outperform out-of-the-box methods that do not account for the aforementioned strategic behavior.
An immediate question that follows is relaxing the assumption of having access to truthful training data – for example, one could ask what the best incentive-compatible classiﬁer is given that the training data consists of best responses to a known classiﬁer f ; or, one could consider an online learning model where the goal is to bound the overall loss over time. A much broader question for future work is to develop a more general theory of robustness to missing data that naturally includes the case of strategic withholding.
13

Acknowledgements
We are thankful for support from NSF under award IIS-1814056.
Ethics Statement
The methods presented in this paper are geared towards preventing the strategic withholding of data when machine learning methods are used in real-world applications. This will increase the robustness of ML techniques in these contexts: without taking this issue into account, deployment of these techniques will generally result in a rapid change in the distribution of submitted data due to the new incentives faced, causing techniques to work much more poorly than expected at training time. Thus, there is an AI safety [Amodei et al., 2016] beneﬁt to our work. The lack of strategic withholding also enables the collection of (truthful) quality data. Of course, there can be a downside to this as well if the data is not used responsibly, which could be the case especially if the features that (without our techniques) would have been withheld are sensitive or private in nature.
The other issues to consider in our context are those of transparency and fairness. We assume that the classiﬁer is public knowledge, and therefore, agents can appropriately best-respond. In practice, this might not be the case; however, agents may learn how to best-respond over time if similar decisions are made repeatedly (e.g., in the case of college admissions or loan applications). While US college admission is often a black box, it need not be; many countries have transparent public criteria for university admissions (e.g., the Indian IIT admission system), and the same is true in many other contexts (e.g., Canadian immigration). Of course, transparency goes hand in hand with interpretability, i.e., the classiﬁer must be easily explainable as well, and there could be a trade-off, in principle, between how easy the classiﬁer is to interpret and the accuracy it can achieve. It is also possible that our methods hurt the chances of those with more missing data (similarly to how immigrants without credit history may not be able to get a credit card). This is to some extent inevitable, because if one can get in without any feature, everyone could get in by dropping all features. Therefore, the issue of fairness might arise in the case where some groups systematically tend to have more missing data.
References
Caralee J. Adams. In Race for Test-Takers, ACT Outscores SAT—for Now, 2017. URL https://www.edweek. org/ew/articles/2017/05/24/in-race-for-test-takers-act-outscores-sat--for.html.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Mane´. Concrete problems in ai safety. ArXiv, abs/1606.06565, 2016.
N Chawla, N Japkowicz, and A Kolcz. Icml’2003 workshop on learning from imbalanced data sets (ii). In Proceedings available at http://www. site. uottawa. ca/nat/Workshop2003/workshop2003. html, 2003.
Nitesh V Chawla, Nathalie Japkowicz, and Aleksander Kotcz. Special issue on learning from imbalanced data sets. ACM SIGKDD explorations newsletter, 6(1):1–6, 2004.
Yiling Chen, Chara Podimata, Ariel D Procaccia, and Nisarg Shah. Strategyproof linear regression in high dimensions. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 9–26, 2018.
14

Bryan Conroy, Larry Eshelman, Cristhian Potes, and Minnan Xu-Wilson. A dynamic ensemble approach to robust classiﬁcation in the presence of missing data. Machine Learning, 102(3):443–463, 2016.
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to algorithms. MIT press, 2009.
Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, and Deepak Verma. Adversarial classiﬁcation. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 99–108, 2004.
Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass learnability and the erm principle. The Journal of Machine Learning Research, 16(1):2377–2404, 2015.
Ofer Dekel, Felix Fischer, and Ariel D Procaccia. Incentive compatible regression learning. Journal of Computer and System Sciences, 76(8):759–777, 2010a.
Ofer Dekel, Ohad Shamir, and Lin Xiao. Learning to classify with missing and corrupted features. Machine learning, 81(2):149–178, 2010b.
Thomas G Dietterich. Approximate statistical tests for comparing supervised classiﬁcation learning algorithms. Neural computation, 10(7):1895–1923, 1998.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
Charles Elkan. The foundations of cost-sensitive learning. In International joint conference on artiﬁcial intelligence, volume 17, pages 973–978. Lawrence Erlbaum Associates Ltd, 2001.
Usama M. Fayyad and Keki B. Irani. Multi-interval discretization of continuous-valued attributes for classiﬁcation learning. In Ruzena Bajcsy, editor, Proceedings of the 13th International Joint Conference on Artiﬁcial Intelligence. Chambe´ry, France, August 28 - September 3, 1993, pages 1022–1029. Morgan Kaufmann, 1993. URL http://ijcai.org/Proceedings/93-2/Papers/022.pdf.
Raquel Florez-Lopez. Effects of missing data in credit risk scoring. a comparative analysis of methods to achieve robustness in the absence of sufﬁcient data. The Journal of the Operational Research Society, 61 (3):486–501, 2010.
Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Proceedings of the 23rd international conference on Machine learning, pages 353–360, 2006.
Jerry R Green and Jean-Jacques Laffont. Partially veriﬁable information and mechanism design. The Review of Economic Studies, 53(3):447–456, 1986.
Nika Haghtalab, Nicole Immorlica, Brendan Lucier, and Jack Wang. Maximizing welfare with incentiveaware evaluation mechanisms. In 29th International Joint Conference on Artiﬁcial Intelligence, 2020.
David J Hand and Christoforos Anagnostopoulos. When is the area under the receiver operating characteristic curve an appropriate measure of classiﬁer performance? Pattern Recognition Letters, 34(5):492–495, 2013.
Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classiﬁcation. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111–122, 2016.
15

N Japkowicz. Aaai’2000 workshop on learning from imbalanced data sets. AAAI Tech Report, No. WS-0005, 2000.
Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, and Daniel G Goldstein. Simple rules for complex decisions. Available at SSRN 2919024, 2017.
Andrew Kephart and Vincent Conitzer. Complexity of mechanism design with signaling costs. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 357–365, 2015.
Andrew Kephart and Vincent Conitzer. The revelation principle for mechanism design with reporting costs. In Proceedings of the 2016 ACM Conference on Economics and Computation, pages 85–102, 2016.
Jon Kleinberg and Manish Raghavan. How do classiﬁers induce agents to invest effort strategically? In Proceedings of the 2019 ACM Conference on Economics and Computation, pages 825–844, 2019.
Stefan Lessmann, Bart Baesens, Hsin-Vonn Seow, and Lyn C Thomas. Benchmarking state-of-the-art classiﬁcation algorithms for credit scoring: An update of research. European Journal of Operational Research, 247(1):124–136, 2015.
Benjamin Marlin. Missing data problems in machine learning. PhD thesis, Citeseer, 2008.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
Maytal Saar-Tsechansky and Foster Provost. Handling missing values when applying classiﬁcation models. Journal of machine learning research, 8(Jul):1623–1657, 2007.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
Umar Syed and Ben Taskar. Semi-supervised learning with adversarially missing label information. In Advances in Neural Information Processing Systems, pages 2244–2252, 2010.
Yevgeniy Vorobeychik and Murat Kantarcioglu. Adversarial machine learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, 12(3), 2018.
Fulton Wang and Cynthia Rudin. Falling rule lists. In Artiﬁcial Intelligence and Statistics, pages 1013–1022, 2015.
Lan Yu. Mechanism design with partial veriﬁcation and revelation principle. Autonomous Agents and MultiAgent Systems, 22(1):217–223, 2011.
Hanrui Zhang and Vincent Conitzer. Incentive-aware PAC learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021.
Hanrui Zhang, Yu Cheng, and Vincent Conitzer. Distinguishing distributions when samples are strategically transformed. In Advances in Neural Information Processing Systems, pages 3187–3195, 2019a.
Hanrui Zhang, Yu Cheng, and Vincent Conitzer. When samples are strategically selected. In International Conference on Machine Learning, pages 7345–7353, 2019b.
16

Hanrui Zhang, Yu Cheng, and Vincent Conitzer. Automated mechanism design for classiﬁcation with partial veriﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021a.
Hanrui Zhang, Yu Cheng, and Vincent Conitzer. Classiﬁcation with few tests through self-selection. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021b.
17

A Further related work
Our work can, in a way, be thought of as studying an adversarial classiﬁcation [see, e.g., Vorobeychik and Kantarcioglu, 2018] problem – in particular, a decision-time, white-box, targeted attack on binary classiﬁers, assuming that the only strategy available to the attacker is to remove feature values, and the attacker’s goal is to maximize the number of instances classiﬁed as positive. In this regard, what we study is similar in spirit to some of the existing literature [Globerson and Roweis, 2006, Syed and Taskar, 2010, Dekel et al., 2010b] on adversarial classiﬁcation.
For example, Globerson and Roweis [2006] consider a problem where, at test time, the attacker can set up to a certain number of features (say pixels in an image) to zero for each instance individually in a way that is most harmful to the classiﬁer chosen. To be robust to such attacks, they devise convex programming based methods that avoid depending on small sets of features to learn the class structure. Our work is different in that we take a more game-theoretic approach to designing classiﬁers (including ensemble-based ones) that are fully resistant to the strategic withholding of features by agents (that prefer being labeled positively). Moreover, we make no assumptions on the actual structure of the feature space.
Our work is also related to the literature on classiﬁcation withmissing data (Batista and Monard 2003; Marlin 2008). We devise methods that can deal with the strategic withholding by agents of some of their features, against a backdrop of missing data caused by natural reasons (e.g., nonstationary distributions of which features are present, or input sensor failures). The HC method can be viewed as an ensemble method for missing data [Conroy et al., 2016] that is strategy-proof against the aforementioned strategic behavior. We also study the performance of other standard, non-strategic classiﬁcation methods for missing data in the strategic setting, including predictive value imputation and reduced-feature modeling [Saar-Tsechansky and Provost, 2007].

B Proofs
Theorem 1. If (S, S¯) is a minimum s-t cut of G(D, X ) (where S is on the same side as s), then for the
classiﬁer f ∗(x) := 1(x ∈ S¯), we have D(f ∗) = minf D(f ).
Proof. First observe that any classiﬁer f can be viewed equivalently as a subset of X , given by

{x ∈ X | f (x) = 1}.

Below, we use these interpretations, i.e., as a function or a subset, of a classiﬁer interchangeably. The loss of a truthful classiﬁer f can then be written as

D(f ) = Pr [(x ∈ f ∧ y = 0) ∨ (x ∈/ f ∧ y = 1)]
(x,y)∼D

= Pr [x ∈ f ∧ y = 0] + Pr [x ∈/ f ∧ y = 1]

(x,y)∼D

(x,y)∼D

=

Pr [x = x ∧ y = 0] +

Pr [x = x ∧ y = 1]

(x ,y)∼D x∈f

(x ,y)∼D x∈/f

= D−(x) + D+(x),

x∈f

x∈/f

where the last line follows from Deﬁnition 1.

18

Therefore, our goal is to solve the following optimization problem:

min
f ⊆X

D−(x) + D+(x)

x∈f

x∈/f

s.t. x ∈ f =⇒ x ∈ f

∀x, x ∈ X where x → x .

Consider the following min-cut formulation (using Deﬁnition 2): Let G(D, X ) be a directed capacitated graph, with vertices V = X ∪ {s, t}, with edges E and edge capacities u deﬁned as follows:
• For each x ∈ X , there is an edge (s, x) ∈ E with capacity D−(x), and an edge (x, t) ∈ E with capacity D+(x).

• For each pair x, x ∈ X where x → x , there is an edge (x , x) ∈ E with capacity ∞.
Observe that each ﬁnite-capacity s-t cut (S, S¯) corresponds bijectively to a truthful classiﬁer f := 1(x ∈
S¯ \ {t}). Moreover, the capacity of the cut is given precisely by

D−(x) +

D+(x) = D−(x) + D+(x) = D(f ).

x∈S¯∩X

x∈S∩X

x∈f

x∈/f

Therefore, any s-t min-cut corresponds to an optimal classiﬁer f ∗, which can be computed “efﬁciently” (i.e., in time poly(|X |)) using any efﬁcient max-ﬂow algorithm given complete knowledge of D.

Theorem 2. A classiﬁer f is truthful iff f (.) = MAXC(.) for a collection of classiﬁers C = {fj} such that, for some {Fj} ⊆ 2F , each fj is an Fj-classiﬁer .
Proof. Recall that the revelation principle holds in our setting (as mentioned in Section Section 2, also see Proposition 1). It therefore sufﬁces to characterize all direct revelation classiﬁers. For any (not necessarily truthful) classiﬁer f , consider its direct revelation implementation f , which maps feature values x to the most desirable label the data point can get by dropping features, i.e.,

f (x) = max f (x ).
x :x→x

We argue below that f has the desired form. Observe that depending on which features a data point x has, f can be decomposed into 2k subclassiﬁers,
denoted {fF }F ⊆[k]. The label of x is then determined in the following way: let Fx be the set of features possessed by x, i.e.,
Fx = {i ∈ [k] | xi = ∗}.

Then

f (x) = fFx (x).

Moreover, observe that (1) fF effectively depends only on x|F (i.e., fF (x) = fF (x|F )), since fF only acts on those data points where all features not in F are missing, and (2) without loss of generality, fF rejects any data point with a missing feature i ∈ F , since fF never acts on a data point where such a feature i ∈ F is missing. Now consider how f works on a data point x. For any F ⊆ Fx, by dropping all features not in F , x can report x|F . Moreover, for any such F ⊆ Fx, f (x|F ) = fF (x|F ). f outputs 1 for x, iff there exists F ⊆ Fx, such that fF (x|F ) = 1. One can therefore write f in the following way: for any x ∈ X ,

f (x) = max fF (x),
F ⊆[k]

as desired.

19

Theorem 3. Let X = {(xi, yi)}i∈[m] be m i.i.d. samples from D. For any f ∈ H¯, for any δ > 0, with probability at least 1 − δ, we have D(f ) ≤ X (f ) + O dn·log dn·lomg m+log(1/δ) .

Proof. Recall that H¯ is deﬁned as the set of all classiﬁers that can be written as the MAX Ensemble of n
classiﬁers in H. Given the classical VC inequality [e.g., Shalev-Shwartz and Ben-David, 2014, Theorem 6.11], we only need to bound the VC dimension of H¯, and show that

dVC(H¯) = O(dn · log dn),

where d is the VC dimension of H. To this end, observe that each f ∈ H¯ is essentially a decision tree with n + 1 leaves, where each leaf is associated with a binary label, and each internal node corresponds to a classiﬁer in H. To be precise, f can be computed in the following way: for any x ∈ X , if f1(x) = 1, then f (x) = 1; otherwise, if f2(x) = 1, then f (x) = 1, etc. It is known [see, e.g., Daniely et al., 2015, Section 5.2] that the class of all such decision trees with n + 1 leaves, which is a superset of H¯, has VC dimension O(dn log dn). As a result, dVC(H) = O(dn log dn), and the theorem follows.

Theorem 4. Algorithm 1 converges.

Proof. Given f = MAX{ft,ft,...,ft }, consider a single update step for, say, f1t. As in Algorithm 1, deﬁne:

12

n

S1 = {(x, y) ∈ X : fjt(x|Fj ) = 0, ∀j = 1}, S−1 = S \ S1.

Then we perform the update as follows:

f1t+1 = argminh∈H

|h(x|F1 ) − y|.

(x,y)∈S1

Let f = MAX{ft+1,ft,...,ft }. Now, the loss calculated for f is

1

2

n

1 X (f ) = m (|S1| ·
1 = m (|S1| ·
1 = m (|S1| ·
1 ≤ m (|S1| ·
= X (f ).

S1 (f ) + |S−1| · S−1 (f )) S1 (f1t+1) + |S−1| · S−1 (f )) S1 (f1t+1) + |S−1| · S−1 (f )) S1 (f1t) + |S−1| · S−1 (f ))

The inequality in the above sequence of steps follows from the fact that fjt+1 accrues a lower loss on S1 than fjt by deﬁnition, and that the classiﬁcation outcomes for any (x, y) ∈ S−1 is the same for f and f .
If we treat X (f ) as a potential function, we can see that it can only decrease with each step, and therefore, the algorithm has to converge at some point.

20

C Revelation Principle

There are many results in the literature on partial veriﬁcation as to the validity of the revelation principle in various settings [Green and Laffont, 1986, Yu, 2011, Kephart and Conitzer, 2015, 2016]. For our purposes, as mentioned in Section 2, when the reporting structure is given by a partial order (i.e., it is transitive, meaning for any x1, x2, x3, x1 → x2 and x2 → x3 =⇒ x1 → x3), the revelation principle holds. Below we give a quick proof for why this is the case in our setting.
Proposition 1. For any classiﬁer f : X → {0, 1}, there is a truthful classiﬁer f such that after misreporting, f and f output the same label for all x ∈ X , i.e.,

f (x) = max f (x ).
x :x→x

Proof. Below we explicitly construct f . Let f be such that for x ∈ X ,

f (x) = max f (x).
x :x→x

Clearly f and f output the same label after strategic manipulation. We only need to show f is truthful, i.e., for any x1, x2 ∈ X where x1 → x2, f (x1) ≥ f (x2). Let X1 = {x : x1 → x } and X2 = {x : x2 → x }. Recall that → is transitive and x1 → x2, so X1 ⊇ X2. Now we have

f (x1) = max f (x) ≥ max f (x) = f (x2).

x∈X1

x∈X2

Note that the above proof crucially depends on transitivity of the reporting structure. In fact, if the reporting structure → is not transitive, then the revelation principle in general does not hold. To see why this is the case, suppose → is not transitive, and let x1, x2, x3 be such that x1 → x2, x2 → x3, and x1 → x3. Suppose we want to assign label 0 to x1, and label 1 to x2 and x3, then the only way to achieve that is to implement a classiﬁer f where f (x1) = f (x2) = 0 and f (x3) = 1. However, this classiﬁer is not truthful, since x2 always misreports as x3 in order to be accepted.

D Other observations
D.1 Regarding MINCUT
Naturally, the test error of MINCUT depends on X and m. For example, If X is discrete and small, one would expect that MINCUT is almost optimal given enough samples. However, when X is large or even inﬁnite, the generalization gap can be extremely large. To see why this is true, consider the following example:
Example 2. Say we are given a feature space with two features, each of which can take any real value between 0 and 1. Let the marginal distribution of D on X be the uniform distribution over X = {(x, y), (x, ∗), (∗, y) | x, y ∈ [0, 1]}. When we see a new data point (x, y), unless we already have (x, y), (x, ∗) or (∗, y) in the set of samples (which happens with probability 0), we know absolutely nothing about the label of (x, y), and therefore by no means we can expect f to predict the label of (x, y) correctly — in fact, f will always assign label 0 to such a data point.

21

D.2 On truthful classiﬁers and hill-climbing
Below, we make a few remarks regarding the generalization bound (Theorem 3) for HC.
• Observe that the generalization gap depends polynomially on the number of subclassiﬁers n. Without additional restrictions, n can be as large as 2k leading to a gap which is exponential in k. This suggests that in practice, to achieve any meaningful generalization guarantee, one has to restrict the number of subclassiﬁers used. In fact, we do run our algorithm on a small set of features in Section 6.
• Recall that the class of linear classiﬁers in the k-dimensional Euclidean space has VC dimension k +1. So, if we restrict all subclassiﬁers to be linear, and require that the number of subclassiﬁers n to be constant, then Theorem 3 implies that with high probability, the generalization gap is

k

O

,

m

where k is the number of features, m is the number of samples, and O hides a logarithmic factor. Our algorithms are practicable in this kind of regime.

E Experiments
E.1 Implementation details
In our implementation, we use Python’s Scikit-learn (0.22.1) package [Pedregosa et al., 2011] of classiﬁers and other machine learning packages whenever possible. The categorical features in the datasets are onehot encoded. To help ensure the convergence of gradient-based classiﬁers, we then standardize features by removing the mean and scaling to unit variance.
For imputation-based classiﬁers, we use mean/mode imputation: mean for numerical and ordinal features, and mode for categorical features. For reduced-feature-based classiﬁers, we default to reject if the test data point’s set of available features was unseen in the training process. For classiﬁcation methods involving Fayyad and Irani’s MDLP discretization algorithm, we use a modiﬁed version of Discretization-MDLPC, licensed under GPL-314.
The performance of each classiﬁer under each setting is evaluated with Nx2-fold cross-validation [Dietterich, 1998]: training on 50% of the data and testing on the remaining 50%; repeat N = 100 times. To tune the parameters for the classiﬁers, we perform grid search over a subset of the parameter space considered by Lessmann et al. [2015], in a 5-fold cross-validation on every training set of the (outer) Nx2-cross-validation loop.
E.2 Additional experimental results
We evaluate our methods on datasets with 1) 4 selected features, same for all runs (Table 6 to 29), 2) 4 selected features, based on the training data at each run (Table 30 to 53), and 3) all available features (Table 54 to 77). In addition, we evaluate our methods both with and without balancing the datasets through random undersampling. This is denoted by “balanced datasets” when we undersample before the experiment, and “unbalanced datasets” when we do not. We vary from 0 to .5 and report classiﬁer accuracy and AUC (when applicable).
14Discretization-MDLPC codebase: https://github.com/navicto/Discretization-MDLPC.

22

Comparing Figure 1 and 3, it appears that there is no signiﬁcant difference in relative accuracy across the various methods when applied to balanced and imbalanced datasets. However, comparing, for example, Table 8 and 20, we observe that when the dataset is highly unbalanced, classiﬁers based on imputation and reduced-feature modeling, when faced with strategic reporting, tend to accept everything and yield a considerably high accuracy. Many other general issues regarding the use of accuracy as a metric on unbalanced datasets are known [Japkowicz, 2000, Chawla et al., 2003, 2004]. In practice, thresholding methods are sometimes used to determine a proper threshold for binary prediction in such cases [Lessmann et al., 2015, Elkan, 2001].
Therefore, in addition to accuracy, we evaluate our approach with area under the receiver operating characteristic curve (AUC). AUC becomes a useful metric when doing imbalanced classiﬁcation because often a balance of false-positive and false-negative rates is desired, and it is invariant to the threshold used in binary classiﬁcation.15 For MINCUT, its receiver operating characteristic curve is undeﬁned because it does not output probabilistic predictions; for HILL-CLIMBING, we take the maximum of the probabilistic predictions across all applicable classiﬁers to be the HILL-CLIMBING classiﬁer’s probabilistic prediction for a data point, and obtain AUC from that. From Table 24 to 29, we observe that our three proposed methods generally yield a AUC as good as, if not higher than, imputation- and reduced-feature-based classiﬁers on imbalanced datasets (also Figure 4). The same holds for balanced datasets too (Figure 2).
For completeness, we also include the performance of classiﬁers based on imputation and reducedfeature modeling, with discretization. As expected, common classiﬁers are generally less prone to overﬁtting than MINCUT, and discretizing the feature space only limits their performance.
The number of iteration the training of HILL-CLIMBING classiﬁers takes to converge varies by dataset, but usually in no more than than 5 passes through all the subclassiﬁers. For the balanced Australia dataset, for example, HC(LR) takes an average of 4.28 iterations to converge (median 3); HC(LR) w/ disc. takes an average of 2.45 (median 2); HC(ANN): takes an average of 3.56 (median 3); HC(ANN) w/ disc. takes an average of 2.62 (median 2).
15... although AUC’s global perspective assumes implicitly that all thresholds are equally probable. This is often criticized as not plausible in credit scoring [Hand and Anagnostopoulos, 2013]
23

Figure 1: Selected classiﬁer accuracy w/ strategic behavior, balanced datasets, pre-selected 4 features Figure 2: Selected classiﬁer AUC w/ strategic behavior, balanced datasets, pre-selected 4 features 24

Figure 3: Selected classiﬁer accuracy w/ strategic behavior, unbalanced datasets, pre-selected 4 features Figure 4: Selected classiﬁer AUC w/ strategic behavior, unbalanced datasets, pre-selected 4 features 25

Figure 5: Selected classiﬁer accuracy w/ strategic behavior, balanced datasets, each classiﬁer selects 4 best features
Figure 6: Selected classiﬁer AUC w/ strategic behavior, balanced datasets, each classiﬁer selects 4 best features
26

Figure 7: Selected classiﬁer accuracy w/ strategic behavior, unbalanced datasets, each classiﬁer selects 4 best features
Figure 8: Selected classiﬁer AUC w/ strategic behavior, unbalanced datasets, each classiﬁer selects 4 best features
27

Figure 9: Selected classiﬁer accuracy w/ strategic behavior, balanced datasets, all features Figure 10: Selected classiﬁer AUC w/ strategic behavior, balanced datasets, all features
28

Figure 11: Selected classiﬁer accuracy w/ strategic behavior, unbalanced datasets, all features Figure 12: Selected classiﬁer AUC w/ strategic behavior, unbalanced datasets, all features 29

Table 6: Our methods vs. the rest: mean classiﬁer accuracy for = 0.0, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.859 (.016) .855 (.016) .860 (.017) .852 (.030) .859 (.015) .849 (.015) .743 (.021) .852 (.018) .862 (.015) .858 (.015) .861 (.013) .745 (.021) .854 (.018) .861 (.014) .859 (.016) .859 (.015) .849 (.016) .820 (.018) .856 (.017) .852 (.017) .850 (.023) .861 (.014) .859 (.016) .859 (.015) .849 (.016) .820 (.019) .855 (.017) .852 (.017) .848 (.027)

.859 (.016) .855 (.016) .860 (.017) .852 (.030) .859 (.015) .849 (.015) .743 (.021) .852 (.018) .862 (.015) .858 (.015) .861 (.013) .745 (.021) .854 (.018) .862 (.015) .855 (.025) .859 (.019) .828 (.038) .698 (.108) .849 (.029) .800 (.085) .815 (.080) .861 (.014) .859 (.016) .859 (.015) .849 (.016) .820 (.018) .855 (.017) .852 (.018) .849 (.023)

.686 (.024) .679 (.024) .685 (.028) .672 (.030) .691 (.023) .683 (.023) .577 (.028) .658 (.027) .677 (.025) .680 (.023) .691 (.024) .579 (.027) .653 (.026) .692 (.022) .682 (.025) .693 (.022) .683 (.027) .651 (.026) .668 (.026) .672 (.027) .670 (.032) .692 (.022) .682 (.025) .693 (.022) .684 (.024) .651 (.026) .668 (.026) .672 (.026) .668 (.034)

.686 (.024) .679 (.024) .685 (.028) .672 (.030) .691 (.023) .683 (.023) .577 (.028) .658 (.027) .677 (.025) .680 (.023) .691 (.024) .579 (.027) .653 (.026) .601 (.097) .593 (.093) .602 (.098) .596 (.093) .562 (.079) .591 (.090) .585 (.090) .581 (.086) .692 (.022) .682 (.025) .693 (.022) .684 (.024) .651 (.026) .668 (.026) .673 (.027) .669 (.033)

.739 (.022) .753 (.019) .733 (.023) .745 (.019) .729 (.025) .754 (.019) .501 (.018) .748 (.020) .650 (.036) .754 (.018) .660 (.038) .501 (.018) .749 (.020) .738 (.021) .756 (.019) .729 (.024) .756 (.019) .747 (.018) .750 (.020) .726 (.023) .741 (.032) .738 (.021) .756 (.019) .728 (.026) .755 (.019) .747 (.019) .750 (.020) .727 (.023) .737 (.042)

.739 (.022) .753 (.019) .733 (.023) .745 (.019) .729 (.025) .754 (.019) .501 (.018) .748 (.020) .650 (.036) .754 (.018) .660 (.038) .501 (.018) .749 (.020) .676 (.039) .742 (.034) .664 (.039) .739 (.036) .736 (.023) .734 (.039) .647 (.048) .724 (.051) .738 (.021) .756 (.019) .729 (.024) .756 (.019) .747 (.018) .750 (.020) .726 (.023) .739 (.038)

.685 (.006) .695 (.005) .649 (.018) .637 (.009) .688 (.006) .696 (.006) .700 (.005) .701 (.005) .499 (.004) .695 (.004) .687 (.005) .701 (.005) .701 (.005) .689 (.006) .695 (.005) .699 (.005) .697 (.005) .701 (.005) .702 (.005) .674 (.039) .673 (.062) .689 (.006) .695 (.005) .699 (.005) .697 (.005) .701 (.005) .702 (.005) .674 (.040) .671 (.065)

.685 (.006) .695 (.005) .649 (.018) .637 (.009) .688 (.006) .696 (.006) .700 (.005) .701 (.005) .499 (.004) .695 (.004) .687 (.005) .701 (.005) .701 (.005) .637 (.051) .500 (.004) .576 (.027) .503 (.016) .531 (.030) .520 (.032) .515 (.024) .497 (.048) .689 (.006) .695 (.005) .699 (.005) .697 (.006) .701 (.005) .702 (.005) .675 (.038) .674 (.061)

30

Table 7: Our methods vs. the rest: mean classiﬁer accuracy for = 0.1, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.814 (.020) .818 (.020) .817 (.020) .817 (.022) .803 (.024) .796 (.026) .769 (.033) .811 (.021) .823 (.018) .827 (.018) .816 (.017) .689 (.025) .816 (.021) .821 (.019) .822 (.017) .821 (.019) .823 (.019) .794 (.022) .821 (.018) .817 (.021) .812 (.028) .827 (.019) .823 (.020) .825 (.020) .813 (.022) .793 (.021) .821 (.019) .810 (.021) .808 (.024)

.814 (.020) .818 (.020) .817 (.020) .817 (.022) .803 (.024) .796 (.026) .769 (.033) .811 (.021) .823 (.018) .827 (.018) .816 (.017) .726 (.078) .682 (.096) .821 (.026) .795 (.048) .819 (.027) .774 (.066) .658 (.116) .779 (.062) .753 (.086) .755 (.098) .543 (.073) .535 (.069) .544 (.072) .542 (.075) .537 (.060) .543 (.074) .528 (.060) .526 (.060)

.659 (.027) .657 (.028) .669 (.026) .660 (.028) .647 (.029) .634 (.033) .584 (.030) .642 (.029) .664 (.027) .665 (.025) .680 (.021) .548 (.026) .620 (.029) .677 (.024) .664 (.025) .678 (.023) .667 (.027) .636 (.029) .659 (.027) .653 (.029) .649 (.036) .651 (.026) .650 (.028) .649 (.028) .642 (.041) .624 (.028) .642 (.030) .624 (.030) .631 (.034)

.659 (.027) .657 (.028) .669 (.026) .660 (.028) .647 (.029) .634 (.033) .584 (.030) .642 (.029) .664 (.027) .665 (.025) .680 (.021) .536 (.048) .545 (.051) .594 (.089) .594 (.085) .596 (.089) .596 (.086) .560 (.068) .593 (.085) .579 (.079) .581 (.079) .507 (.026) .511 (.032) .507 (.026) .513 (.033) .506 (.024) .513 (.031) .505 (.026) .511 (.032)

.681 (.028) .718 (.021) .694 (.027) .722 (.023) .664 (.028) .705 (.026) .502 (.017) .715 (.022) .645 (.037) .725 (.021) .661 (.035) .502 (.017) .728 (.023) .726 (.024) .735 (.021) .718 (.026) .735 (.022) .735 (.021) .732 (.022) .712 (.025) .721 (.031) .696 (.027) .730 (.022) .696 (.027) .727 (.024) .719 (.020) .724 (.025) .683 (.025) .710 (.030)

.681 (.028) .718 (.021) .694 (.027) .722 (.023) .664 (.028) .705 (.026) .502 (.017) .715 (.022) .645 (.037) .725 (.021) .661 (.035) .502 (.017) .604 (.059) .668 (.037) .676 (.077) .655 (.039) .679 (.072) .655 (.057) .666 (.078) .636 (.047) .656 (.084) .506 (.024) .515 (.036) .505 (.021) .515 (.031) .510 (.027) .516 (.032) .505 (.024) .513 (.035)

.664 (.005) .671 (.005) .664 (.007) .669 (.009) .665 (.006) .671 (.005) .672 (.005) .671 (.005) .500 (.004) .669 (.005) .676 (.007) .686 (.005) .693 (.005) .676 (.006) .690 (.005) .692 (.005) .693 (.006) .687 (.005) .695 (.006) .662 (.029) .650 (.072) .674 (.008) .689 (.005) .680 (.006) .689 (.006) .687 (.005) .693 (.006) .661 (.030) .659 (.047)

.664 (.005) .671 (.005) .664 (.007) .669 (.009) .665 (.006) .671 (.005) .672 (.005) .671 (.005) .500 (.004) .669 (.005) .676 (.007) .565 (.033) .564 (.037) .627 (.047) .549 (.030) .557 (.006) .559 (.019) .541 (.031) .544 (.031) .552 (.033) .539 (.057) .528 (.039) .528 (.040) .520 (.035) .523 (.038) .527 (.034) .523 (.033) .523 (.035) .520 (.033)

31

Table 8: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.792 (.022) .795 (.023) .794 (.021) .795 (.024) .775 (.028) .770 (.032) .770 (.031) .790 (.024) .788 (.019) .800 (.021) .793 (.024) .664 (.025) .795 (.023) .797 (.021) .800 (.021) .799 (.021) .800 (.021) .772 (.023) .797 (.023) .789 (.023) .786 (.028) .808 (.020) .797 (.021) .805 (.022) .793 (.021) .777 (.024) .799 (.022) .780 (.024) .775 (.024)

.792 (.022) .795 (.023) .794 (.021) .795 (.024) .775 (.028) .770 (.032) .770 (.031) .790 (.024) .788 (.019) .800 (.021) .793 (.024) .674 (.099) .625 (.094) .791 (.034) .763 (.066) .789 (.037) .747 (.085) .630 (.101) .745 (.088) .722 (.085) .705 (.119) .545 (.063) .541 (.067) .542 (.061) .542 (.062) .537 (.050) .545 (.063) .535 (.055) .537 (.063)

.639 (.029) .641 (.027) .652 (.027) .649 (.027) .624 (.029) .614 (.036) .580 (.031) .630 (.030) .653 (.028) .651 (.029) .661 (.027) .536 (.028) .603 (.031) .663 (.026) .652 (.025) .665 (.025) .652 (.029) .623 (.031) .643 (.028) .636 (.029) .628 (.041) .630 (.028) .633 (.027) .628 (.028) .630 (.031) .608 (.031) .626 (.028) .602 (.031) .617 (.032)

.639 (.029) .641 (.027) .652 (.027) .649 (.027) .624 (.029) .614 (.036) .580 (.031) .630 (.030) .653 (.028) .651 (.029) .661 (.027) .526 (.043) .532 (.049) .582 (.082) .580 (.083) .583 (.083) .580 (.084) .560 (.065) .578 (.080) .570 (.070) .574 (.072) .508 (.026) .516 (.037) .509 (.026) .514 (.035) .508 (.025) .515 (.036) .505 (.025) .513 (.036)

.660 (.029) .692 (.025) .670 (.027) .694 (.024) .640 (.035) .680 (.027) .501 (.016) .692 (.025) .639 (.039) .698 (.025) .652 (.035) .501 (.017) .714 (.024) .714 (.027) .720 (.024) .705 (.031) .719 (.024) .721 (.021) .716 (.025) .695 (.026) .696 (.047) .670 (.030) .709 (.022) .664 (.037) .708 (.025) .699 (.022) .707 (.024) .658 (.027) .685 (.033)

.660 (.029) .692 (.025) .670 (.027) .694 (.024) .640 (.035) .680 (.027) .501 (.016) .692 (.025) .639 (.039) .698 (.025) .652 (.035) .501 (.018) .564 (.052) .660 (.036) .630 (.088) .644 (.043) .636 (.084) .601 (.065) .622 (.086) .623 (.047) .608 (.087) .511 (.026) .522 (.041) .509 (.026) .520 (.039) .519 (.029) .525 (.043) .512 (.027) .519 (.036)

.648 (.006) .650 (.005) .646 (.005) .648 (.005) .647 (.006) .648 (.006) .651 (.005) .649 (.006) .499 (.004) .646 (.006) .671 (.009) .678 (.005) .688 (.005) .670 (.008) .686 (.006) .686 (.005) .688 (.005) .679 (.005) .689 (.005) .659 (.021) .656 (.044) .665 (.009) .684 (.005) .668 (.008) .683 (.006) .679 (.005) .687 (.005) .659 (.018) .650 (.037)

.648 (.006) .650 (.005) .646 (.005) .648 (.005) .647 (.006) .648 (.006) .651 (.005) .649 (.006) .499 (.004) .646 (.006) .671 (.009) .560 (.034) .566 (.034) .618 (.047) .536 (.026) .536 (.005) .543 (.024) .549 (.031) .542 (.027) .559 (.036) .563 (.050) .590 (.038) .588 (.029) .548 (.047) .542 (.041) .575 (.027) .586 (.026) .585 (.029) .587 (.027)

32

Table 9: Our methods vs. the rest: mean classiﬁer accuracy for = 0.3, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.771 (.023) .769 (.024) .773 (.019) .772 (.021) .753 (.028) .748 (.031) .752 (.028) .770 (.024) .759 (.023) .775 (.021) .771 (.022) .650 (.027) .775 (.026) .774 (.023) .770 (.025) .775 (.023) .770 (.023) .749 (.025) .771 (.024) .766 (.025) .760 (.032) .788 (.023) .777 (.028) .784 (.024) .776 (.027) .760 (.024) .783 (.028) .768 (.026) .759 (.032)

.771 (.023) .769 (.024) .773 (.019) .772 (.021) .753 (.028) .748 (.031) .752 (.028) .770 (.024) .759 (.023) .775 (.021) .771 (.022) .653 (.099) .615 (.094) .758 (.048) .721 (.087) .752 (.050) .708 (.098) .625 (.098) .709 (.097) .663 (.106) .666 (.119) .584 (.072) .577 (.077) .578 (.070) .595 (.081) .580 (.059) .610 (.083) .575 (.069) .586 (.079)

.625 (.029) .629 (.030) .634 (.028) .631 (.031) .610 (.030) .598 (.035) .580 (.032) .620 (.031) .636 (.035) .638 (.031) .640 (.029) .532 (.027) .592 (.033) .651 (.027) .636 (.030) .652 (.027) .635 (.034) .609 (.031) .628 (.032) .623 (.032) .608 (.041) .608 (.028) .615 (.030) .606 (.029) .611 (.032) .592 (.028) .609 (.031) .582 (.031) .598 (.032)

.625 (.029) .629 (.030) .634 (.028) .631 (.031) .610 (.030) .598 (.035) .580 (.032) .620 (.031) .636 (.035) .638 (.031) .640 (.029) .524 (.035) .534 (.047) .580 (.076) .587 (.074) .582 (.077) .586 (.076) .556 (.063) .582 (.073) .565 (.066) .567 (.065) .512 (.029) .520 (.038) .512 (.028) .516 (.036) .512 (.028) .522 (.038) .509 (.029) .519 (.039)

.642 (.029) .670 (.027) .645 (.027) .672 (.027) .621 (.034) .659 (.028) .500 (.018) .670 (.027) .624 (.041) .673 (.030) .645 (.041) .500 (.019) .701 (.024) .701 (.026) .707 (.027) .692 (.032) .705 (.027) .706 (.022) .705 (.025) .681 (.025) .685 (.046) .651 (.027) .697 (.024) .646 (.033) .695 (.025) .683 (.020) .701 (.024) .647 (.025) .681 (.027)

.642 (.029) .670 (.027) .645 (.027) .672 (.027) .621 (.034) .659 (.028) .500 (.018) .670 (.027) .624 (.041) .673 (.030) .645 (.041) .498 (.019) .559 (.057) .654 (.038) .604 (.087) .640 (.048) .612 (.086) .567 (.060) .599 (.089) .614 (.052) .587 (.081) .524 (.031) .550 (.051) .515 (.026) .547 (.051) .543 (.035) .558 (.052) .532 (.032) .547 (.048)

.636 (.006) .639 (.006) .636 (.006) .638 (.006) .636 (.005) .638 (.006) .638 (.006) .638 (.006) .497 (.004) .634 (.007) .660 (.013) .668 (.006) .680 (.005) .659 (.011) .681 (.005) .677 (.005) .682 (.005) .669 (.006) .681 (.005) .650 (.015) .640 (.039) .654 (.010) .678 (.005) .656 (.009) .676 (.006) .669 (.006) .680 (.005) .652 (.013) .653 (.019)

.636 (.006) .639 (.006) .636 (.006) .638 (.006) .636 (.005) .638 (.006) .638 (.006) .638 (.006) .497 (.004) .634 (.007) .660 (.013) .559 (.041) .561 (.041) .603 (.054) .519 (.021) .521 (.005) .527 (.023) .554 (.039) .538 (.038) .559 (.042) .560 (.049) .613 (.017) .601 (.010) .570 (.044) .558 (.044) .592 (.011) .599 (.010) .598 (.020) .600 (.017)

33

Table 10: Our methods vs. the rest: mean classiﬁer accuracy for = 0.4, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.751 (.024) .748 (.025) .752 (.023) .746 (.023) .731 (.028) .726 (.030) .727 (.028) .746 (.027) .731 (.027) .747 (.022) .753 (.026) .651 (.027) .760 (.026) .757 (.024) .741 (.025) .759 (.025) .742 (.026) .729 (.027) .742 (.024) .744 (.026) .726 (.034) .771 (.025) .761 (.026) .766 (.025) .758 (.030) .747 (.026) .767 (.025) .754 (.027) .747 (.029)

.751 (.024) .748 (.025) .752 (.023) .746 (.023) .731 (.028) .726 (.030) .727 (.028) .746 (.027) .731 (.027) .747 (.022) .753 (.026) .634 (.104) .612 (.097) .733 (.052) .690 (.094) .728 (.055) .679 (.103) .636 (.095) .675 (.105) .633 (.108) .637 (.115) .631 (.069) .645 (.077) .619 (.074) .637 (.075) .625 (.056) .670 (.070) .628 (.063) .646 (.075)

.610 (.030) .617 (.030) .618 (.029) .620 (.032) .593 (.031) .586 (.037) .575 (.033) .610 (.033) .623 (.035) .621 (.036) .620 (.040) .532 (.028) .582 (.033) .635 (.028) .622 (.031) .635 (.028) .623 (.033) .603 (.032) .614 (.032) .606 (.033) .591 (.042) .590 (.032) .599 (.032) .587 (.031) .587 (.033) .582 (.031) .593 (.032) .570 (.034) .585 (.033)

.610 (.030) .617 (.030) .618 (.029) .620 (.032) .593 (.031) .586 (.037) .575 (.033) .610 (.033) .623 (.035) .621 (.036) .620 (.040) .524 (.037) .534 (.049) .579 (.071) .577 (.070) .581 (.073) .578 (.070) .561 (.061) .574 (.069) .559 (.061) .561 (.060) .523 (.032) .527 (.042) .522 (.030) .520 (.037) .523 (.030) .527 (.042) .518 (.032) .526 (.038)

.626 (.030) .650 (.026) .629 (.029) .657 (.024) .601 (.035) .640 (.029) .499 (.017) .648 (.028) .614 (.037) .651 (.027) .632 (.046) .499 (.018) .689 (.025) .688 (.025) .679 (.029) .676 (.036) .677 (.031) .685 (.021) .678 (.028) .662 (.025) .657 (.048) .627 (.034) .683 (.025) .619 (.038) .677 (.028) .661 (.023) .685 (.024) .629 (.028) .666 (.028)

.626 (.030) .650 (.026) .629 (.029) .657 (.024) .601 (.035) .640 (.029) .499 (.017) .648 (.028) .614 (.037) .651 (.027) .632 (.046) .500 (.020) .563 (.060) .645 (.036) .576 (.079) .625 (.052) .584 (.078) .560 (.055) .573 (.080) .578 (.062) .572 (.076) .540 (.037) .580 (.055) .523 (.033) .561 (.055) .560 (.031) .581 (.054) .559 (.035) .577 (.055)

.628 (.006) .628 (.006) .627 (.006) .628 (.006) .628 (.007) .627 (.006) .626 (.007) .626 (.007) .497 (.003) .617 (.006) .649 (.013) .659 (.006) .671 (.005) .648 (.012) .671 (.006) .668 (.005) .672 (.005) .660 (.006) .671 (.006) .641 (.012) .641 (.026) .642 (.010) .669 (.006) .643 (.010) .668 (.007) .660 (.006) .670 (.006) .640 (.012) .637 (.020)

.628 (.006) .628 (.006) .627 (.006) .628 (.006) .628 (.007) .627 (.006) .626 (.007) .626 (.007) .497 (.003) .617 (.006) .649 (.013) .555 (.045) .557 (.046) .602 (.049) .512 (.021) .512 (.004) .518 (.026) .553 (.045) .518 (.026) .555 (.045) .550 (.048) .615 (.011) .602 (.006) .603 (.027) .596 (.028) .598 (.007) .602 (.006) .601 (.016) .601 (.016)

34

Table 11: Our methods vs. the rest: mean classiﬁer accuracy for = 0.5, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.726 (.025) .719 (.025) .727 (.026) .720 (.027) .709 (.028) .701 (.032) .706 (.028) .719 (.024) .708 (.031) .717 (.028) .716 (.028) .649 (.028) .734 (.027) .733 (.025) .707 (.030) .734 (.025) .708 (.030) .705 (.026) .707 (.030) .719 (.028) .692 (.036) .744 (.026) .737 (.028) .735 (.028) .730 (.029) .722 (.026) .743 (.025) .731 (.029) .722 (.029)

.726 (.025) .719 (.025) .727 (.026) .720 (.027) .709 (.028) .701 (.032) .706 (.028) .719 (.024) .708 (.031) .717 (.028) .716 (.028) .618 (.106) .607 (.102) .705 (.051) .643 (.099) .699 (.062) .633 (.104) .635 (.090) .633 (.105) .626 (.102) .609 (.103) .660 (.050) .668 (.069) .641 (.060) .650 (.071) .650 (.041) .683 (.062) .659 (.051) .668 (.064)

.596 (.029) .601 (.030) .600 (.028) .603 (.032) .578 (.032) .570 (.036) .572 (.033) .596 (.031) .603 (.038) .603 (.038) .600 (.034) .534 (.029) .571 (.030) .616 (.029) .603 (.033) .617 (.029) .600 (.040) .588 (.032) .595 (.034) .588 (.032) .573 (.043) .572 (.030) .584 (.032) .569 (.031) .569 (.032) .566 (.030) .583 (.031) .555 (.033) .572 (.033)

.596 (.029) .601 (.030) .600 (.028) .603 (.032) .578 (.032) .570 (.036) .572 (.033) .596 (.031) .603 (.038) .603 (.038) .600 (.034) .526 (.040) .534 (.047) .571 (.064) .564 (.065) .571 (.065) .563 (.066) .555 (.055) .562 (.063) .552 (.054) .544 (.056) .532 (.034) .525 (.041) .530 (.034) .515 (.035) .533 (.035) .524 (.041) .528 (.034) .526 (.040)

.612 (.030) .638 (.028) .617 (.029) .636 (.026) .588 (.036) .626 (.030) .500 (.018) .632 (.029) .604 (.039) .625 (.028) .619 (.045) .500 (.018) .667 (.025) .670 (.026) .647 (.036) .660 (.035) .645 (.036) .660 (.023) .648 (.034) .646 (.026) .624 (.050) .613 (.033) .665 (.025) .598 (.036) .656 (.028) .636 (.025) .666 (.025) .618 (.028) .651 (.028)

.612 (.030) .638 (.028) .617 (.029) .636 (.026) .588 (.036) .626 (.030) .500 (.018) .632 (.029) .604 (.039) .625 (.028) .619 (.045) .497 (.019) .567 (.068) .631 (.037) .560 (.069) .606 (.060) .563 (.070) .551 (.055) .558 (.070) .568 (.061) .547 (.066) .559 (.035) .587 (.057) .531 (.037) .566 (.054) .569 (.028) .584 (.056) .577 (.031) .585 (.048)

.616 (.006) .616 (.005) .616 (.007) .617 (.005) .615 (.006) .615 (.006) .613 (.008) .613 (.008) .497 (.003) .606 (.008) .640 (.014) .646 (.006) .657 (.005) .636 (.014) .658 (.005) .655 (.005) .659 (.005) .647 (.006) .658 (.005) .626 (.013) .622 (.027) .627 (.010) .657 (.005) .625 (.011) .654 (.009) .647 (.006) .657 (.005) .627 (.012) .626 (.015)

.616 (.006) .616 (.005) .616 (.007) .617 (.005) .615 (.006) .615 (.006) .613 (.008) .613 (.008) .497 (.003) .606 (.008) .640 (.014) .551 (.049) .552 (.050) .581 (.054) .505 (.015) .505 (.004) .511 (.026) .549 (.049) .509 (.022) .546 (.047) .548 (.050) .611 (.008) .604 (.005) .605 (.015) .595 (.021) .600 (.006) .604 (.005) .599 (.014) .599 (.014)

35

Table 12: Our methods vs. the rest: mean classiﬁer AUC for = 0.0, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.906 (.013) .911 (.013) .910 (.013) .911 (.019) .908 (.013) .909 (.013) .901 (.013) .911 (.013) .904 (.012) .906 (.013) .910 (.013) .907 (.013) .910 (.013) .885 (.015) .911 (.014) .904 (.015) .901 (.019) .906 (.013) .910 (.013) .907 (.013) .910 (.013) .885 (.015) .910 (.014) .904 (.015) .902 (.017)

.906 (.013) .911 (.013) .910 (.013) .911 (.019) .908 (.013) .909 (.013) .901 (.013) .911 (.013) .904 (.012) .905 (.014) .897 (.016) .905 (.014) .898 (.016) .875 (.024) .899 (.017) .874 (.018) .885 (.023) .906 (.013) .910 (.013) .907 (.013) .910 (.013) .885 (.015) .911 (.014) .904 (.015) .901 (.019)

.739 (.026) .732 (.024) .732 (.027) .720 (.027) .753 (.023) .736 (.024) .739 (.023) .739 (.022) .762 (.020) .756 (.022) .739 (.024) .757 (.022) .742 (.023) .709 (.025) .712 (.025) .717 (.026) .715 (.029) .756 (.022) .739 (.024) .757 (.022) .742 (.023) .709 (.025) .712 (.026) .717 (.026) .714 (.031)

.739 (.026) .732 (.024) .732 (.027) .720 (.027) .753 (.023) .736 (.024) .739 (.023) .739 (.022) .762 (.020) .717 (.042) .688 (.053) .717 (.041) .692 (.052) .660 (.084) .667 (.063) .663 (.078) .651 (.075) .756 (.022) .739 (.024) .757 (.022) .742 (.023) .709 (.025) .712 (.026) .717 (.026) .714 (.030)

.807 (.022) .813 (.019) .795 (.023) .786 (.021) .800 (.023) .815 (.018) .714 (.040) .818 (.019) .728 (.044) .806 (.022) .819 (.020) .801 (.022) .818 (.019) .819 (.017) .811 (.020) .794 (.021) .800 (.026) .806 (.022) .819 (.020) .801 (.023) .818 (.019) .819 (.017) .811 (.020) .794 (.021) .802 (.024)

.807 (.022) .813 (.019) .795 (.023) .786 (.021) .800 (.023) .815 (.018) .714 (.040) .818 (.019) .728 (.044) .776 (.028) .794 (.031) .771 (.024) .793 (.030) .810 (.018) .789 (.031) .771 (.027) .778 (.038) .806 (.022) .819 (.020) .801 (.022) .818 (.019) .819 (.017) .811 (.021) .794 (.021) .802 (.024)

.696 (.007) .729 (.006) .649 (.014) .646 (.010) .701 (.010) .731 (.005) .500 (.000) .731 (.004) .704 (.005) .704 (.006) .731 (.005) .732 (.009) .731 (.005) .734 (.005) .731 (.005) .716 (.017) .712 (.024) .704 (.006) .731 (.005) .731 (.009) .731 (.005) .734 (.005) .731 (.005) .716 (.017) .712 (.024)

.696 (.007) .729 (.006) .649 (.014) .646 (.010) .701 (.010) .731 (.005) .500 (.000) .731 (.004) .704 (.005) .667 (.009) .579 (.025) .683 (.013) .587 (.003) .582 (.022) .527 (.075) .636 (.041) .520 (.082) .704 (.006) .731 (.005) .732 (.009) .731 (.005) .734 (.005) .731 (.005) .715 (.017) .713 (.025)

36

Table 13: Our methods vs. the rest: mean classiﬁer AUC for = 0.1, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.889 (.018) .888 (.018) .893 (.016) .891 (.017) .886 (.018) .876 (.019) .878 (.016) .893 (.015) .891 (.014) .893 (.015) .894 (.016) .893 (.015) .892 (.016) .866 (.018) .894 (.016) .885 (.018) .884 (.021) .880 (.018) .884 (.019) .878 (.019) .876 (.021) .860 (.019) .880 (.019) .871 (.019) .869 (.022)

.889 (.018) .888 (.018) .893 (.016) .891 (.017) .886 (.018) .876 (.019) .878 (.016) .893 (.015) .891 (.014) .888 (.017) .859 (.029) .888 (.016) .859 (.029) .850 (.031) .857 (.044) .860 (.027) .845 (.042) .798 (.067) .789 (.070) .817 (.056) .815 (.043) .799 (.056) .803 (.057) .787 (.056) .772 (.070)

.704 (.028) .702 (.027) .713 (.030) .703 (.028) .707 (.028) .681 (.032) .718 (.025) .720 (.025) .740 (.021) .738 (.025) .720 (.026) .739 (.025) .724 (.026) .688 (.028) .695 (.029) .699 (.029) .691 (.033) .697 (.028) .701 (.028) .688 (.030) .687 (.036) .674 (.029) .681 (.029) .664 (.031) .676 (.033)

.704 (.028) .702 (.027) .713 (.030) .703 (.028) .707 (.028) .681 (.032) .718 (.025) .720 (.025) .740 (.021) .701 (.042) .676 (.050) .702 (.041) .679 (.050) .657 (.064) .660 (.057) .658 (.065) .638 (.070) .609 (.055) .617 (.061) .624 (.051) .626 (.059) .611 (.055) .629 (.055) .598 (.044) .610 (.047)

.751 (.029) .774 (.022) .753 (.028) .771 (.023) .730 (.032) .765 (.023) .709 (.040) .790 (.020) .725 (.040) .792 (.025) .800 (.020) .786 (.026) .799 (.020) .805 (.020) .791 (.021) .776 (.024) .779 (.026) .758 (.029) .790 (.021) .758 (.029) .786 (.021) .788 (.021) .777 (.023) .742 (.026) .767 (.027)

.751 (.029) .774 (.022) .753 (.028) .771 (.023) .730 (.032) .765 (.023) .709 (.040) .790 (.020) .725 (.040) .760 (.029) .757 (.038) .753 (.026) .758 (.038) .773 (.024) .741 (.051) .747 (.025) .722 (.063) .591 (.099) .654 (.100) .632 (.084) .695 (.061) .664 (.085) .681 (.068) .640 (.056) .671 (.064)

.674 (.006) .704 (.006) .674 (.007) .703 (.006) .682 (.008) .706 (.006) .500 (.000) .710 (.005) .691 (.007) .694 (.007) .725 (.005) .721 (.008) .726 (.005) .717 (.005) .725 (.005) .708 (.012) .700 (.027) .693 (.006) .723 (.006) .706 (.008) .724 (.006) .717 (.005) .722 (.005) .706 (.013) .701 (.022)

.674 (.006) .704 (.006) .674 (.007) .703 (.006) .682 (.008) .706 (.006) .500 (.000) .710 (.005) .691 (.007) .661 (.007) .677 (.027) .678 (.011) .677 (.026) .634 (.039) .662 (.044) .663 (.028) .596 (.083) .546 (.101) .570 (.121) .588 (.088) .633 (.076) .592 (.072) .574 (.094) .644 (.037) .636 (.057)

37

Table 14: Our methods vs. the rest: mean classiﬁer AUC for = 0.2, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.875 (.018) .870 (.019) .876 (.018) .871 (.020) .865 (.021) .850 (.026) .853 (.019) .875 (.018) .884 (.011) .879 (.017) .877 (.014) .880 (.016) .875 (.014) .850 (.019) .876 (.016) .868 (.019) .861 (.024) .866 (.019) .864 (.018) .863 (.020) .858 (.018) .846 (.021) .858 (.019) .845 (.020) .839 (.022)

.875 (.018) .870 (.019) .876 (.018) .871 (.020) .865 (.021) .850 (.026) .853 (.019) .875 (.018) .884 (.011) .868 (.019) .832 (.029) .868 (.019) .832 (.029) .821 (.040) .831 (.031) .842 (.031) .815 (.036) .778 (.066) .746 (.078) .801 (.050) .793 (.048) .775 (.057) .785 (.049) .742 (.055) .724 (.068)

.678 (.031) .679 (.030) .691 (.029) .684 (.028) .675 (.032) .655 (.035) .700 (.027) .700 (.027) .726 (.024) .720 (.026) .701 (.026) .722 (.026) .705 (.027) .668 (.033) .679 (.028) .681 (.030) .669 (.038) .668 (.032) .678 (.028) .662 (.032) .669 (.030) .650 (.034) .661 (.029) .635 (.033) .655 (.033)

.678 (.031) .679 (.030) .691 (.029) .684 (.028) .675 (.032) .655 (.035) .700 (.027) .700 (.027) .726 (.024) .681 (.042) .658 (.047) .683 (.042) .659 (.048) .641 (.053) .645 (.051) .640 (.057) .628 (.060) .599 (.051) .613 (.058) .607 (.050) .621 (.053) .593 (.055) .627 (.057) .567 (.042) .593 (.052)

.727 (.028) .748 (.025) .731 (.027) .746 (.024) .702 (.036) .735 (.027) .706 (.040) .764 (.023) .718 (.042) .782 (.026) .780 (.024) .775 (.027) .778 (.024) .789 (.022) .772 (.024) .757 (.026) .754 (.033) .726 (.032) .766 (.023) .724 (.035) .761 (.024) .762 (.023) .756 (.024) .707 (.029) .739 (.031)

.727 (.028) .748 (.025) .731 (.027) .746 (.024) .702 (.036) .735 (.027) .706 (.040) .764 (.023) .718 (.042) .746 (.026) .710 (.059) .739 (.024) .711 (.059) .733 (.031) .701 (.061) .725 (.028) .673 (.076) .589 (.091) .633 (.089) .638 (.071) .676 (.048) .647 (.070) .661 (.065) .620 (.044) .647 (.053)

.662 (.007) .693 (.007) .661 (.007) .691 (.006) .670 (.008) .691 (.007) .500 (.000) .694 (.006) .688 (.005) .690 (.006) .721 (.005) .714 (.007) .721 (.005) .709 (.006) .720 (.005) .702 (.010) .698 (.018) .687 (.007) .717 (.005) .694 (.007) .716 (.005) .708 (.006) .716 (.005) .700 (.009) .695 (.018)

.662 (.007) .693 (.007) .661 (.007) .691 (.006) .670 (.008) .691 (.007) .500 (.000) .694 (.006) .688 (.005) .657 (.006) .682 (.011) .673 (.009) .682 (.011) .638 (.033) .671 (.024) .655 (.025) .618 (.057) .629 (.031) .666 (.031) .635 (.028) .666 (.023) .645 (.020) .658 (.033) .658 (.023) .659 (.023)

38

Table 15: Our methods vs. the rest: mean classiﬁer AUC for = 0.3, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.855 (.020) .848 (.021) .854 (.018) .847 (.018) .843 (.023) .826 (.031) .822 (.019) .853 (.017) .865 (.017) .862 (.019) .855 (.017) .862 (.019) .853 (.017) .829 (.021) .854 (.018) .848 (.023) .838 (.027) .848 (.020) .843 (.023) .844 (.021) .839 (.024) .828 (.021) .842 (.023) .825 (.023) .819 (.026)

.855 (.020) .848 (.021) .854 (.018) .847 (.018) .843 (.023) .826 (.031) .822 (.019) .853 (.017) .865 (.017) .842 (.027) .792 (.049) .842 (.026) .792 (.048) .794 (.040) .791 (.048) .806 (.047) .769 (.067) .775 (.068) .751 (.074) .790 (.050) .785 (.045) .757 (.063) .779 (.048) .740 (.049) .729 (.057)

.658 (.032) .659 (.032) .667 (.031) .661 (.030) .651 (.032) .628 (.037) .676 (.028) .677 (.029) .700 (.029) .703 (.029) .682 (.030) .706 (.028) .684 (.031) .650 (.033) .662 (.032) .663 (.033) .646 (.040) .643 (.030) .658 (.032) .640 (.032) .650 (.033) .629 (.030) .647 (.032) .610 (.032) .632 (.034)

.658 (.032) .659 (.032) .667 (.031) .661 (.030) .651 (.032) .628 (.037) .676 (.028) .677 (.029) .700 (.029) .667 (.041) .646 (.045) .668 (.040) .647 (.045) .629 (.049) .635 (.048) .623 (.057) .615 (.056) .595 (.047) .609 (.051) .599 (.046) .611 (.046) .588 (.053) .621 (.046) .562 (.043) .594 (.050)

.704 (.027) .724 (.029) .702 (.030) .721 (.027) .674 (.039) .709 (.030) .697 (.038) .737 (.027) .716 (.041) .769 (.025) .764 (.026) .761 (.027) .761 (.025) .770 (.022) .759 (.024) .740 (.026) .735 (.041) .704 (.030) .752 (.023) .705 (.035) .748 (.024) .738 (.023) .745 (.023) .691 (.026) .726 (.027)

.704 (.027) .724 (.029) .702 (.030) .721 (.027) .674 (.039) .709 (.030) .697 (.038) .737 (.027) .716 (.041) .728 (.026) .681 (.055) .721 (.024) .681 (.055) .695 (.044) .678 (.054) .701 (.031) .649 (.067) .611 (.072) .653 (.075) .636 (.053) .673 (.043) .648 (.060) .665 (.057) .627 (.043) .650 (.045)

.653 (.006) .676 (.008) .653 (.006) .677 (.006) .656 (.006) .679 (.006) .500 (.000) .681 (.006) .685 (.005) .685 (.005) .716 (.006) .705 (.006) .716 (.006) .700 (.006) .714 (.006) .692 (.008) .685 (.017) .679 (.005) .709 (.006) .681 (.006) .709 (.006) .697 (.006) .709 (.006) .690 (.008) .689 (.009)

.653 (.006) .676 (.008) .653 (.006) .677 (.006) .656 (.006) .679 (.006) .500 (.000) .681 (.006) .685 (.005) .649 (.005) .674 (.009) .663 (.009) .675 (.008) .652 (.012) .671 (.011) .641 (.026) .616 (.044) .636 (.013) .672 (.012) .634 (.022) .668 (.011) .652 (.012) .671 (.012) .649 (.016) .651 (.017)

39

Table 16: Our methods vs. the rest: mean classiﬁer AUC for = 0.4, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.828 (.023) .820 (.024) .827 (.021) .817 (.022) .810 (.028) .794 (.034) .788 (.021) .825 (.020) .848 (.017) .844 (.019) .829 (.020) .845 (.019) .828 (.020) .809 (.024) .829 (.020) .826 (.022) .812 (.033) .826 (.023) .821 (.024) .821 (.024) .818 (.024) .805 (.025) .821 (.023) .805 (.024) .802 (.027)

.828 (.023) .820 (.024) .827 (.021) .817 (.022) .810 (.028) .794 (.034) .788 (.021) .825 (.020) .848 (.017) .814 (.028) .762 (.038) .814 (.028) .762 (.038) .768 (.040) .762 (.038) .775 (.045) .738 (.059) .782 (.051) .768 (.052) .781 (.040) .779 (.035) .753 (.049) .784 (.034) .746 (.043) .746 (.040)

.639 (.034) .641 (.032) .645 (.033) .642 (.032) .626 (.037) .604 (.037) .654 (.029) .656 (.028) .689 (.030) .686 (.029) .665 (.032) .688 (.029) .666 (.032) .640 (.033) .646 (.034) .642 (.035) .623 (.044) .622 (.034) .638 (.036) .618 (.034) .630 (.037) .612 (.034) .629 (.035) .591 (.037) .616 (.036)

.639 (.034) .641 (.032) .645 (.033) .642 (.032) .626 (.037) .604 (.037) .654 (.029) .656 (.028) .689 (.030) .652 (.040) .629 (.042) .653 (.040) .629 (.042) .619 (.044) .619 (.043) .609 (.050) .598 (.053) .595 (.041) .611 (.038) .594 (.039) .602 (.039) .592 (.044) .616 (.038) .570 (.041) .590 (.043)

.681 (.030) .695 (.027) .682 (.030) .699 (.026) .646 (.040) .683 (.028) .690 (.033) .712 (.025) .700 (.039) .753 (.027) .736 (.028) .745 (.030) .733 (.028) .746 (.022) .731 (.028) .718 (.027) .717 (.033) .676 (.038) .731 (.026) .675 (.040) .728 (.026) .705 (.026) .728 (.027) .666 (.030) .708 (.028)

.681 (.030) .695 (.027) .682 (.030) .699 (.026) .646 (.040) .683 (.028) .690 (.033) .712 (.025) .700 (.039) .709 (.026) .644 (.058) .704 (.026) .644 (.057) .679 (.028) .642 (.057) .675 (.033) .622 (.065) .623 (.053) .669 (.048) .623 (.047) .668 (.040) .646 (.040) .673 (.044) .633 (.036) .656 (.039)

.644 (.007) .663 (.006) .644 (.006) .663 (.006) .647 (.007) .664 (.007) .500 (.000) .668 (.006) .678 (.007) .679 (.005) .707 (.006) .696 (.007) .707 (.006) .692 (.007) .705 (.006) .682 (.008) .678 (.015) .667 (.006) .694 (.007) .667 (.007) .693 (.007) .684 (.007) .693 (.007) .676 (.008) .674 (.009)

.644 (.007) .663 (.006) .644 (.006) .663 (.006) .647 (.007) .664 (.007) .500 (.000) .668 (.006) .678 (.007) .641 (.006) .660 (.009) .652 (.009) .660 (.009) .650 (.009) .657 (.011) .625 (.026) .607 (.045) .631 (.009) .662 (.008) .633 (.012) .661 (.009) .648 (.009) .661 (.008) .639 (.015) .637 (.016)

40

Table 17: Our methods vs. the rest: mean classiﬁer AUC for = 0.5, balanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.788 (.023) .779 (.024) .787 (.025) .777 (.027) .773 (.029) .761 (.031) .747 (.023) .790 (.024) .816 (.022) .819 (.021) .791 (.021) .818 (.021) .790 (.021) .782 (.024) .791 (.021) .796 (.026) .772 (.035) .787 (.025) .779 (.031) .781 (.025) .778 (.031) .768 (.024) .787 (.029) .770 (.026) .766 (.029)

.788 (.023) .779 (.024) .787 (.025) .777 (.027) .773 (.029) .761 (.031) .747 (.023) .790 (.024) .816 (.022) .778 (.030) .710 (.052) .778 (.030) .710 (.052) .735 (.037) .710 (.052) .741 (.045) .691 (.067) .765 (.032) .750 (.036) .754 (.034) .751 (.036) .730 (.035) .758 (.033) .737 (.035) .736 (.039)

.616 (.033) .618 (.031) .620 (.032) .620 (.031) .600 (.035) .581 (.036) .629 (.029) .631 (.029) .660 (.028) .663 (.032) .641 (.032) .665 (.032) .639 (.035) .622 (.034) .627 (.033) .620 (.036) .604 (.046) .594 (.034) .622 (.036) .590 (.034) .606 (.037) .588 (.033) .618 (.035) .570 (.036) .597 (.037)

.616 (.033) .618 (.031) .620 (.032) .620 (.031) .600 (.035) .581 (.036) .629 (.029) .631 (.029) .660 (.028) .628 (.040) .607 (.039) .629 (.040) .605 (.041) .602 (.040) .600 (.040) .592 (.047) .576 (.049) .585 (.037) .597 (.037) .580 (.036) .578 (.037) .581 (.038) .598 (.036) .567 (.042) .580 (.040)

.657 (.030) .670 (.029) .661 (.031) .670 (.027) .624 (.041) .661 (.032) .686 (.032) .686 (.027) .708 (.035) .736 (.027) .698 (.036) .729 (.030) .694 (.036) .720 (.025) .696 (.036) .703 (.029) .674 (.041) .652 (.037) .706 (.028) .646 (.040) .702 (.028) .670 (.027) .706 (.028) .646 (.031) .687 (.031)

.657 (.030) .670 (.029) .661 (.031) .670 (.027) .624 (.041) .661 (.032) .686 (.032) .686 (.027) .708 (.035) .688 (.026) .611 (.052) .683 (.028) .610 (.053) .655 (.027) .610 (.051) .650 (.043) .584 (.061) .621 (.039) .658 (.031) .607 (.043) .652 (.032) .635 (.032) .660 (.031) .631 (.031) .636 (.043)

.630 (.006) .645 (.006) .631 (.007) .646 (.006) .632 (.006) .646 (.006) .500 (.000) .653 (.006) .672 (.006) .672 (.006) .695 (.005) .684 (.007) .695 (.005) .681 (.006) .693 (.005) .668 (.010) .662 (.020) .649 (.006) .672 (.006) .647 (.006) .670 (.007) .663 (.006) .672 (.007) .656 (.008) .654 (.009)

.630 (.006) .645 (.006) .631 (.007) .646 (.006) .632 (.006) .646 (.006) .500 (.000) .653 (.006) .672 (.006) .630 (.006) .642 (.011) .637 (.008) .643 (.011) .640 (.008) .642 (.011) .607 (.028) .591 (.043) .621 (.007) .644 (.008) .622 (.011) .642 (.008) .636 (.008) .644 (.008) .624 (.012) .623 (.014)

41

Table 18: Our methods vs. the rest: mean classiﬁer accuracy for = 0.0, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.848 (.015) .848 (.013) .849 (.014) .834 (.036) .850 (.013) .848 (.014) .739 (.020) .845 (.014) .855 (.012) .848 (.012) .855 (.012) .738 (.020) .847 (.014) .853 (.013) .850 (.013) .849 (.013) .848 (.014) .818 (.015) .847 (.015) .845 (.016) .841 (.022) .853 (.013) .850 (.013) .849 (.013) .848 (.014) .818 (.015) .846 (.015) .845 (.016) .842 (.019)

.848 (.015) .848 (.013) .849 (.014) .834 (.036) .850 (.013) .848 (.014) .739 (.020) .845 (.014) .855 (.012) .848 (.012) .855 (.012) .738 (.020) .847 (.014) .822 (.097) .798 (.102) .816 (.096) .776 (.096) .637 (.095) .792 (.103) .747 (.111) .749 (.120) .853 (.013) .850 (.013) .849 (.013) .848 (.014) .818 (.016) .847 (.015) .845 (.017) .841 (.023)

.736 (.017) .730 (.015) .720 (.017) .718 (.015) .740 (.018) .730 (.017) .568 (.018) .696 (.019) .725 (.015) .727 (.015) .740 (.015) .553 (.019) .686 (.020) .744 (.017) .731 (.015) .744 (.016) .729 (.016) .705 (.017) .719 (.016) .727 (.018) .717 (.020) .744 (.017) .731 (.015) .745 (.016) .729 (.015) .705 (.016) .719 (.016) .727 (.018) .717 (.020)

.736 (.017) .730 (.015) .720 (.017) .718 (.015) .740 (.018) .730 (.017) .568 (.018) .696 (.019) .725 (.015) .727 (.015) .740 (.015) .553 (.019) .686 (.020) .700 (.014) .700 (.014) .700 (.014) .700 (.014) .700 (.014) .700 (.014) .700 (.014) .699 (.016) .744 (.017) .731 (.015) .744 (.017) .730 (.015) .706 (.016) .719 (.016) .728 (.018) .717 (.021)

.931 (.003) .931 (.004) .931 (.003) .932 (.003) .931 (.003) .931 (.003) .085 (.004) .929 (.004) .931 (.003) .931 (.003) .931 (.003) .085 (.004) .928 (.004) .930 (.003) .932 (.003) .930 (.003) .932 (.004) .929 (.003) .930 (.004) .931 (.003) .929 (.004) .930 (.003) .932 (.003) .930 (.003) .931 (.004) .929 (.003) .930 (.004) .931 (.003) .929 (.005)

.931 (.003) .931 (.004) .931 (.003) .932 (.003) .931 (.003) .931 (.003) .085 (.004) .929 (.004) .931 (.003) .931 (.003) .931 (.003) .085 (.004) .928 (.004) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .926 (.057) .930 (.003) .932 (.003) .930 (.003) .931 (.003) .929 (.003) .930 (.004) .931 (.003) .929 (.004)

.808 (.003) .819 (.002) .808 (.013) .819 (.002) .819 (.002) .819 (.002) .818 (.002) .819 (.002) .779 (.002) .819 (.002) .807 (.003) .818 (.002) .819 (.002) .808 (.003) .820 (.002) .819 (.002) .819 (.002) .818 (.002) .819 (.002) .811 (.007) .811 (.010) .808 (.003) .820 (.002) .819 (.002) .820 (.002) .818 (.002) .819 (.002) .811 (.007) .810 (.012)

.808 (.003) .819 (.002) .808 (.013) .819 (.002) .819 (.002) .819 (.002) .818 (.002) .819 (.002) .779 (.002) .819 (.002) .807 (.003) .818 (.002) .819 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .774 (.036) .771 (.061) .808 (.003) .820 (.002) .819 (.002) .819 (.002) .818 (.002) .819 (.002) .810 (.007) .807 (.042)

42

Table 19: Our methods vs. the rest: mean classiﬁer accuracy for = 0.1, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.814 (.019) .818 (.020) .807 (.014) .815 (.020) .799 (.022) .797 (.025) .770 (.032) .811 (.021) .809 (.018) .819 (.019) .814 (.020) .677 (.024) .814 (.019) .818 (.019) .825 (.018) .821 (.018) .826 (.016) .794 (.020) .822 (.019) .813 (.019) .813 (.023) .825 (.017) .818 (.019) .823 (.017) .817 (.018) .795 (.019) .817 (.018) .807 (.020) .805 (.024)

.814 (.019) .818 (.020) .807 (.014) .815 (.020) .799 (.022) .797 (.025) .770 (.032) .811 (.021) .809 (.018) .819 (.019) .814 (.020) .734 (.073) .703 (.080) .775 (.098) .744 (.105) .768 (.099) .735 (.107) .638 (.094) .732 (.108) .703 (.100) .698 (.122) .570 (.037) .571 (.046) .572 (.039) .576 (.053) .570 (.037) .576 (.052) .563 (.034) .565 (.035)

.708 (.017) .701 (.018) .710 (.013) .702 (.018) .700 (.020) .693 (.017) .606 (.028) .686 (.021) .706 (.013) .710 (.014) .728 (.015) .489 (.021) .643 (.028) .735 (.016) .723 (.016) .735 (.016) .722 (.016) .696 (.019) .708 (.018) .719 (.019) .707 (.023) .715 (.017) .713 (.018) .715 (.017) .711 (.018) .687 (.019) .698 (.019) .701 (.018) .699 (.021)

.708 (.017) .701 (.018) .710 (.013) .702 (.018) .700 (.020) .693 (.017) .606 (.028) .686 (.021) .706 (.013) .710 (.014) .728 (.015) .637 (.030) .686 (.019) .700 (.014) .699 (.015) .700 (.014) .699 (.015) .700 (.014) .699 (.015) .700 (.014) .695 (.022) .697 (.014) .696 (.015) .697 (.014) .697 (.015) .697 (.014) .696 (.015) .696 (.015) .696 (.015)

.930 (.004) .929 (.004) .930 (.003) .930 (.003) .930 (.004) .930 (.004) .080 (.004) .928 (.004) .931 (.003) .930 (.003) .931 (.003) .077 (.004) .922 (.005) .930 (.004) .931 (.003) .930 (.004) .930 (.003) .928 (.004) .929 (.004) .930 (.004) .927 (.006) .930 (.004) .930 (.004) .930 (.004) .930 (.003) .927 (.004) .927 (.004) .930 (.004) .928 (.004)

.930 (.004) .929 (.004) .930 (.003) .930 (.003) .930 (.004) .930 (.004) .080 (.004) .928 (.004) .931 (.003) .930 (.003) .931 (.003) .142 (.120) .929 (.004) .930 (.004) .930 (.003) .930 (.004) .930 (.003) .930 (.004) .930 (.003) .930 (.004) .930 (.003) .930 (.004) .930 (.003) .930 (.004) .930 (.003) .930 (.004) .930 (.003) .930 (.004) .930 (.003)

.803 (.003) .804 (.003) .795 (.010) .804 (.003) .803 (.003) .802 (.003) .802 (.003) .804 (.002) .778 (.002) .804 (.002) .804 (.003) .811 (.003) .815 (.003) .805 (.003) .816 (.003) .815 (.003) .817 (.003) .813 (.003) .817 (.002) .808 (.007) .807 (.010) .806 (.003) .817 (.002) .811 (.003) .814 (.002) .813 (.003) .816 (.002) .808 (.006) .809 (.006)

.803 (.003) .804 (.003) .795 (.010) .804 (.003) .803 (.003) .802 (.003) .802 (.003) .804 (.002) .778 (.002) .804 (.002) .804 (.003) .782 (.006) .782 (.007) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.003) .779 (.002) .779 (.002) .776 (.018) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.003) .779 (.003) .779 (.002) .779 (.003)

43

Table 20: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.796 (.019) .794 (.020) .795 (.019) .789 (.022) .780 (.026) .771 (.027) .769 (.026) .794 (.020) .788 (.021) .796 (.017) .797 (.018) .652 (.024) .798 (.022) .803 (.020) .799 (.019) .806 (.018) .798 (.019) .775 (.021) .797 (.019) .792 (.020) .789 (.023) .807 (.019) .798 (.020) .804 (.019) .795 (.020) .780 (.020) .799 (.020) .781 (.024) .780 (.024)

.796 (.019) .794 (.020) .795 (.019) .789 (.022) .780 (.026) .771 (.027) .769 (.026) .794 (.020) .788 (.021) .796 (.017) .797 (.018) .698 (.086) .647 (.076) .740 (.095) .709 (.105) .729 (.093) .700 (.107) .623 (.081) .691 (.109) .675 (.095) .670 (.112) .577 (.039) .568 (.032) .576 (.039) .576 (.045) .577 (.040) .576 (.043) .567 (.032) .567 (.036)

.694 (.019) .688 (.020) .692 (.016) .690 (.019) .682 (.021) .686 (.021) .621 (.028) .678 (.022) .696 (.019) .701 (.018) .717 (.017) .471 (.022) .632 (.035) .728 (.016) .718 (.017) .727 (.016) .716 (.017) .690 (.019) .706 (.018) .711 (.020) .698 (.028) .700 (.019) .704 (.018) .698 (.020) .701 (.019) .675 (.019) .689 (.019) .691 (.020) .692 (.020)

.694 (.019) .688 (.020) .692 (.016) .690 (.019) .682 (.021) .686 (.021) .621 (.028) .678 (.022) .696 (.019) .701 (.018) .717 (.017) .678 (.026) .694 (.017) .700 (.014) .700 (.015) .700 (.014) .700 (.015) .700 (.014) .700 (.015) .698 (.016) .694 (.029) .696 (.014) .697 (.015) .696 (.014) .698 (.015) .696 (.015) .698 (.015) .695 (.014) .697 (.015)

.929 (.003) .929 (.003) .929 (.004) .928 (.003) .929 (.003) .929 (.003) .078 (.004) .929 (.004) .931 (.004) .931 (.004) .931 (.004) .074 (.003) .922 (.005) .930 (.003) .930 (.004) .931 (.003) .930 (.004) .928 (.003) .929 (.003) .930 (.004) .928 (.004) .929 (.003) .928 (.004) .929 (.003) .929 (.003) .924 (.003) .925 (.004) .929 (.004) .927 (.005)

.929 (.003) .929 (.003) .929 (.004) .928 (.003) .929 (.003) .929 (.003) .078 (.004) .929 (.004) .931 (.004) .931 (.004) .931 (.004) .568 (.091) .931 (.003) .931 (.003) .930 (.003) .931 (.003) .930 (.003) .931 (.003) .930 (.003) .931 (.003) .930 (.003) .929 (.003) .929 (.003) .929 (.003) .929 (.003) .928 (.003) .929 (.003) .929 (.003) .929 (.003)

.800 (.002) .800 (.002) .789 (.011) .800 (.002) .800 (.003) .800 (.002) .798 (.004) .800 (.002) .779 (.002) .800 (.002) .800 (.003) .809 (.003) .814 (.002) .802 (.003) .813 (.003) .813 (.003) .813 (.002) .811 (.003) .814 (.003) .807 (.005) .803 (.027) .803 (.003) .813 (.003) .805 (.003) .809 (.003) .810 (.003) .813 (.003) .806 (.004) .806 (.004)

.800 (.002) .800 (.002) .789 (.011) .800 (.002) .800 (.003) .800 (.002) .798 (.004) .800 (.002) .779 (.002) .800 (.002) .800 (.003) .780 (.003) .781 (.004) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .780 (.003) .779 (.003) .779 (.003) .778 (.005) .779 (.003) .780 (.003) .778 (.002) .778 (.002) .782 (.005) .784 (.007) .781 (.004) .782 (.005)

44

Table 21: Our methods vs. the rest: mean classiﬁer accuracy for = 0.3, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.775 (.021) .771 (.023) .776 (.021) .771 (.026) .755 (.026) .747 (.031) .746 (.024) .772 (.023) .762 (.030) .773 (.020) .770 (.024) .642 (.026) .783 (.022) .783 (.021) .773 (.022) .784 (.021) .772 (.021) .755 (.024) .773 (.020) .770 (.024) .763 (.030) .791 (.022) .783 (.022) .788 (.023) .780 (.024) .765 (.023) .785 (.023) .770 (.025) .767 (.026)

.775 (.021) .771 (.023) .776 (.021) .771 (.026) .755 (.026) .747 (.031) .746 (.024) .772 (.023) .762 (.030) .773 (.020) .770 (.024) .659 (.084) .630 (.076) .709 (.090) .668 (.104) .695 (.093) .657 (.105) .615 (.078) .655 (.105) .635 (.090) .644 (.102) .604 (.050) .596 (.057) .601 (.051) .613 (.061) .609 (.048) .631 (.068) .597 (.049) .600 (.057)

.685 (.019) .682 (.021) .690 (.017) .694 (.018) .673 (.022) .692 (.020) .634 (.027) .677 (.022) .699 (.014) .700 (.014) .717 (.016) .477 (.023) .631 (.036) .723 (.017) .711 (.018) .723 (.017) .710 (.017) .688 (.021) .702 (.018) .704 (.022) .693 (.030) .688 (.020) .697 (.019) .689 (.020) .696 (.019) .666 (.021) .683 (.022) .686 (.020) .686 (.022)

.685 (.019) .682 (.021) .690 (.017) .694 (.018) .673 (.022) .692 (.020) .634 (.027) .677 (.022) .699 (.014) .700 (.014) .717 (.016) .693 (.020) .698 (.015) .701 (.014) .699 (.015) .701 (.014) .699 (.015) .701 (.014) .699 (.015) .697 (.019) .689 (.038) .693 (.015) .696 (.016) .693 (.015) .696 (.016) .692 (.015) .696 (.016) .693 (.015) .696 (.016)

.923 (.004) .923 (.004) .923 (.003) .922 (.003) .923 (.004) .923 (.004) .079 (.004) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .079 (.004) .923 (.005) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .926 (.004) .928 (.003) .930 (.004) .928 (.005) .923 (.004) .922 (.004) .923 (.004) .923 (.004) .915 (.004) .919 (.004) .923 (.004) .921 (.004)

.923 (.004) .923 (.004) .923 (.003) .922 (.003) .923 (.004) .923 (.004) .079 (.004) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .729 (.008) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .928 (.021) .923 (.004) .923 (.004) .923 (.004) .923 (.004) .920 (.004) .923 (.004) .923 (.004) .923 (.004)

.796 (.003) .796 (.003) .789 (.010) .796 (.003) .796 (.003) .796 (.003) .792 (.006) .796 (.003) .778 (.003) .782 (.007) .797 (.003) .807 (.003) .811 (.002) .799 (.003) .808 (.003) .808 (.003) .812 (.002) .809 (.003) .811 (.003) .803 (.005) .802 (.009) .797 (.003) .807 (.002) .797 (.003) .802 (.003) .804 (.003) .807 (.003) .800 (.004) .798 (.010)

.796 (.003) .796 (.003) .789 (.010) .796 (.003) .796 (.003) .796 (.003) .792 (.006) .796 (.003) .778 (.003) .782 (.007) .797 (.003) .780 (.003) .781 (.004) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .780 (.003) .779 (.003) .779 (.003) .775 (.020) .776 (.003) .781 (.007) .774 (.003) .774 (.003) .782 (.007) .783 (.008) .780 (.005) .779 (.006)

45

Table 22: Our methods vs. the rest: mean classiﬁer accuracy for = 0.4, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.756 (.023) .749 (.025) .759 (.023) .749 (.025) .733 (.027) .724 (.033) .725 (.029) .749 (.025) .740 (.027) .744 (.023) .746 (.026) .644 (.027) .767 (.026) .763 (.023) .748 (.025) .763 (.022) .748 (.024) .736 (.024) .749 (.023) .748 (.025) .734 (.038) .775 (.024) .764 (.024) .769 (.025) .762 (.025) .749 (.025) .771 (.024) .755 (.027) .755 (.029)

.756 (.023) .749 (.025) .759 (.023) .749 (.025) .733 (.027) .724 (.033) .725 (.029) .749 (.025) .740 (.027) .744 (.023) .746 (.026) .634 (.083) .620 (.077) .678 (.089) .637 (.093) .659 (.093) .627 (.093) .604 (.074) .627 (.093) .615 (.080) .626 (.089) .645 (.055) .634 (.064) .630 (.056) .640 (.063) .649 (.047) .670 (.065) .649 (.056) .652 (.064)

.673 (.021) .686 (.023) .675 (.018) .692 (.017) .669 (.022) .694 (.019) .633 (.028) .685 (.024) .700 (.013) .699 (.014) .708 (.014) .501 (.024) .640 (.032) .716 (.017) .708 (.016) .715 (.017) .706 (.016) .685 (.020) .700 (.018) .696 (.026) .686 (.040) .675 (.021) .694 (.018) .675 (.021) .694 (.018) .654 (.022) .682 (.021) .674 (.020) .681 (.023)

.673 (.021) .686 (.023) .675 (.018) .692 (.017) .669 (.022) .694 (.019) .633 (.028) .685 (.024) .700 (.013) .699 (.014) .708 (.014) .697 (.018) .699 (.015) .700 (.014) .701 (.014) .700 (.014) .701 (.014) .700 (.014) .701 (.014) .692 (.028) .682 (.054) .686 (.016) .697 (.015) .686 (.016) .697 (.015) .682 (.016) .696 (.015) .684 (.017) .695 (.016)

.908 (.005) .908 (.005) .908 (.004) .908 (.004) .908 (.004) .909 (.005) .079 (.004) .931 (.004) .931 (.003) .930 (.003) .931 (.003) .093 (.004) .924 (.004) .931 (.003) .929 (.004) .931 (.003) .930 (.003) .924 (.004) .929 (.004) .930 (.004) .927 (.007) .908 (.004) .907 (.005) .908 (.004) .908 (.005) .896 (.005) .905 (.005) .908 (.004) .907 (.005)

.908 (.005) .908 (.005) .908 (.004) .908 (.004) .908 (.004) .909 (.005) .079 (.004) .931 (.004) .931 (.003) .930 (.003) .931 (.003) .823 (.007) .931 (.003) .931 (.003) .930 (.003) .931 (.003) .930 (.003) .931 (.003) .930 (.003) .931 (.004) .927 (.030) .908 (.005) .908 (.004) .908 (.004) .908 (.004) .901 (.005) .908 (.004) .908 (.004) .908 (.005)

.786 (.002) .786 (.002) .779 (.010) .786 (.002) .786 (.002) .786 (.003) .777 (.013) .785 (.006) .779 (.002) .779 (.002) .795 (.002) .804 (.003) .808 (.002) .796 (.003) .806 (.003) .805 (.002) .809 (.002) .805 (.002) .808 (.002) .801 (.006) .800 (.013) .784 (.003) .795 (.003) .782 (.003) .790 (.003) .791 (.003) .794 (.003) .787 (.006) .787 (.008)

.786 (.002) .786 (.002) .779 (.010) .786 (.002) .786 (.002) .786 (.003) .777 (.013) .785 (.006) .779 (.002) .779 (.002) .795 (.002) .779 (.002) .780 (.003) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .778 (.003) .764 (.064) .767 (.003) .776 (.007) .765 (.002) .766 (.003) .775 (.005) .777 (.008) .771 (.006) .772 (.008)

46

Table 23: Our methods vs. the rest: mean classiﬁer accuracy for = 0.5, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.729 (.026) .724 (.025) .734 (.021) .730 (.023) .714 (.030) .706 (.032) .714 (.031) .724 (.025) .709 (.026) .715 (.022) .714 (.029) .643 (.027) .745 (.026) .736 (.023) .719 (.029) .734 (.022) .720 (.028) .711 (.024) .721 (.027) .723 (.029) .702 (.040) .749 (.024) .744 (.027) .740 (.026) .735 (.029) .729 (.024) .748 (.025) .732 (.027) .730 (.029)

.729 (.026) .724 (.025) .734 (.021) .730 (.023) .714 (.030) .706 (.032) .714 (.031) .724 (.025) .709 (.026) .715 (.022) .714 (.029) .615 (.079) .608 (.076) .628 (.084) .600 (.073) .613 (.081) .594 (.071) .593 (.064) .596 (.071) .602 (.071) .595 (.071) .667 (.042) .667 (.061) .647 (.047) .649 (.060) .668 (.035) .688 (.057) .669 (.043) .677 (.058)

.660 (.021) .692 (.023) .666 (.021) .692 (.023) .663 (.021) .694 (.019) .647 (.039) .696 (.020) .700 (.015) .700 (.015) .706 (.017) .532 (.023) .649 (.028) .710 (.016) .704 (.017) .709 (.016) .703 (.016) .684 (.019) .698 (.017) .688 (.030) .677 (.052) .657 (.020) .689 (.021) .657 (.020) .691 (.022) .639 (.021) .679 (.024) .654 (.021) .673 (.026)

.660 (.021) .692 (.023) .666 (.021) .692 (.023) .663 (.021) .694 (.019) .647 (.039) .696 (.020) .700 (.015) .700 (.015) .706 (.017) .699 (.015) .699 (.014) .700 (.014) .701 (.014) .700 (.014) .701 (.014) .700 (.014) .701 (.014) .689 (.035) .680 (.060) .670 (.018) .696 (.019) .670 (.018) .696 (.019) .665 (.018) .695 (.020) .666 (.019) .689 (.025)

.877 (.005) .880 (.013) .877 (.005) .881 (.014) .877 (.005) .880 (.013) .101 (.130) .930 (.006) .931 (.003) .931 (.003) .931 (.003) .125 (.005) .924 (.005) .930 (.003) .930 (.004) .930 (.003) .931 (.003) .920 (.004) .930 (.004) .929 (.004) .928 (.005) .876 (.005) .878 (.011) .877 (.005) .879 (.011) .861 (.006) .876 (.011) .876 (.005) .877 (.011)

.877 (.005) .880 (.013) .877 (.005) .881 (.014) .877 (.005) .880 (.013) .101 (.130) .930 (.006) .931 (.003) .931 (.003) .931 (.003) .878 (.005) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .930 (.008) .931 (.003) .877 (.005) .879 (.011) .877 (.005) .879 (.011) .865 (.006) .878 (.011) .877 (.005) .878 (.011)

.764 (.003) .765 (.003) .759 (.008) .764 (.002) .765 (.003) .765 (.003) .752 (.025) .771 (.021) .779 (.002) .779 (.002) .792 (.002) .800 (.003) .805 (.003) .793 (.003) .804 (.003) .801 (.003) .803 (.003) .802 (.003) .804 (.003) .798 (.007) .795 (.021) .761 (.003) .770 (.003) .756 (.004) .765 (.003) .767 (.003) .769 (.003) .764 (.005) .764 (.007)

.764 (.003) .765 (.003) .759 (.008) .764 (.002) .765 (.003) .765 (.003) .752 (.025) .771 (.021) .779 (.002) .779 (.002) .792 (.002) .779 (.002) .780 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .777 (.007) .774 (.026) .747 (.003) .758 (.006) .745 (.003) .748 (.004) .757 (.005) .759 (.006) .751 (.007) .751 (.011)

47

Table 24: Our methods vs. the rest: mean classiﬁer AUC for = 0.0, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.907 (.012) .911 (.012) .910 (.012) .904 (.030) .908 (.012) .910 (.012) .901 (.011) .910 (.011) .905 (.012) .907 (.012) .910 (.012) .908 (.012) .910 (.012) .886 (.013) .911 (.013) .906 (.013) .903 (.016) .907 (.012) .910 (.012) .908 (.012) .910 (.012) .886 (.013) .911 (.013) .905 (.013) .903 (.018)

.907 (.012) .911 (.012) .910 (.012) .904 (.030) .908 (.012) .910 (.012) .901 (.011) .910 (.011) .905 (.012) .895 (.036) .868 (.070) .895 (.035) .868 (.070) .841 (.106) .826 (.189) .833 (.122) .827 (.136) .907 (.012) .910 (.012) .908 (.012) .910 (.012) .886 (.013) .911 (.013) .905 (.013) .903 (.016)

.723 (.033) .738 (.020) .694 (.034) .685 (.031) .745 (.026) .747 (.019) .744 (.015) .746 (.015) .765 (.014) .761 (.017) .744 (.016) .762 (.017) .747 (.016) .711 (.017) .717 (.018) .724 (.020) .709 (.026) .761 (.017) .744 (.016) .763 (.017) .747 (.016) .711 (.017) .717 (.018) .724 (.021) .707 (.027)

.723 (.033) .738 (.020) .694 (.034) .685 (.031) .745 (.026) .747 (.019) .744 (.015) .746 (.015) .765 (.014) .680 (.020) .644 (.019) .681 (.020) .647 (.019) .578 (.037) .604 (.034) .562 (.051) .562 (.048) .761 (.017) .744 (.016) .762 (.017) .747 (.016) .711 (.017) .717 (.018) .724 (.020) .709 (.025)

.729 (.087) .797 (.048) .729 (.067) .781 (.019) .795 (.050) .818 (.040) .693 (.027) .824 (.013) .697 (.033) .766 (.026) .825 (.013) .809 (.016) .827 (.014) .813 (.012) .808 (.019) .755 (.020) .752 (.042) .766 (.026) .825 (.013) .809 (.016) .827 (.014) .813 (.012) .808 (.019) .756 (.020) .758 (.038)

.729 (.087) .797 (.048) .729 (.067) .781 (.019) .795 (.050) .818 (.040) .693 (.027) .824 (.013) .697 (.033) .579 (.062) .608 (.070) .728 (.014) .625 (.062) .689 (.055) .556 (.085) .523 (.017) .495 (.031) .766 (.026) .825 (.013) .809 (.016) .827 (.014) .813 (.012) .809 (.019) .755 (.020) .754 (.039)

.705 (.004) .730 (.007) .674 (.026) .717 (.004) .729 (.014) .730 (.007) .500 (.000) .730 (.004) .704 (.004) .705 (.004) .731 (.004) .735 (.009) .732 (.004) .736 (.004) .731 (.004) .709 (.020) .706 (.026) .705 (.004) .731 (.004) .735 (.008) .732 (.004) .736 (.004) .732 (.004) .709 (.020) .705 (.028)

.705 (.004) .730 (.007) .674 (.026) .717 (.004) .729 (.014) .730 (.007) .500 (.000) .730 (.004) .704 (.004) .625 (.027) .573 (.032) .653 (.018) .587 (.003) .542 (.041) .521 (.077) .528 (.056) .487 (.078) .705 (.004) .731 (.004) .734 (.009) .732 (.004) .736 (.004) .732 (.004) .710 (.020) .707 (.026)

48

Table 25: Our methods vs. the rest: mean classiﬁer AUC for = 0.1, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.890 (.018) .890 (.016) .887 (.014) .886 (.014) .886 (.016) .876 (.018) .874 (.015) .891 (.013) .893 (.012) .892 (.015) .893 (.015) .893 (.014) .892 (.015) .865 (.016) .892 (.016) .885 (.016) .881 (.021) .884 (.014) .886 (.016) .883 (.015) .883 (.016) .863 (.015) .880 (.017) .873 (.017) .871 (.021)

.890 (.018) .890 (.016) .887 (.014) .886 (.014) .886 (.016) .876 (.018) .874 (.015) .891 (.013) .893 (.012) .873 (.043) .832 (.075) .873 (.041) .833 (.075) .826 (.073) .817 (.112) .824 (.103) .798 (.124) .798 (.069) .787 (.070) .822 (.059) .822 (.045) .795 (.061) .801 (.060) .785 (.060) .774 (.077)

.682 (.033) .687 (.031) .684 (.039) .665 (.039) .690 (.031) .670 (.046) .723 (.016) .724 (.015) .746 (.015) .741 (.018) .723 (.019) .743 (.018) .726 (.019) .685 (.021) .696 (.022) .703 (.022) .682 (.030) .709 (.021) .711 (.020) .707 (.022) .704 (.020) .679 (.022) .685 (.023) .678 (.024) .681 (.026)

.682 (.033) .687 (.031) .684 (.039) .665 (.039) .690 (.031) .670 (.046) .723 (.016) .724 (.015) .746 (.015) .666 (.020) .633 (.019) .667 (.020) .635 (.018) .592 (.048) .605 (.030) .582 (.047) .571 (.045) .616 (.049) .638 (.048) .627 (.046) .637 (.050) .621 (.049) .637 (.050) .611 (.043) .618 (.049)

.653 (.072) .715 (.041) .646 (.081) .685 (.093) .682 (.063) .738 (.028) .687 (.028) .790 (.013) .699 (.033) .760 (.028) .807 (.014) .801 (.017) .810 (.014) .799 (.014) .795 (.017) .740 (.021) .743 (.043) .748 (.024) .799 (.015) .770 (.019) .788 (.015) .782 (.013) .758 (.021) .725 (.021) .734 (.031)

.653 (.072) .715 (.041) .646 (.081) .685 (.093) .682 (.063) .738 (.028) .687 (.028) .790 (.013) .699 (.033) .658 (.065) .634 (.070) .722 (.015) .640 (.064) .646 (.059) .595 (.075) .532 (.024) .506 (.045) .587 (.090) .703 (.045) .683 (.060) .728 (.039) .551 (.053) .599 (.043) .524 (.030) .523 (.035)

.655 (.013) .681 (.020) .637 (.025) .682 (.016) .673 (.010) .688 (.017) .500 (.000) .709 (.004) .690 (.003) .694 (.006) .732 (.004) .724 (.006) .733 (.004) .720 (.005) .733 (.004) .704 (.014) .702 (.018) .696 (.005) .725 (.004) .714 (.006) .725 (.004) .720 (.005) .724 (.004) .703 (.013) .701 (.018)

.655 (.013) .681 (.020) .637 (.025) .682 (.016) .673 (.010) .688 (.017) .500 (.000) .709 (.004) .690 (.003) .593 (.014) .675 (.022) .599 (.013) .677 (.020) .624 (.041) .672 (.028) .624 (.042) .565 (.065) .538 (.101) .560 (.135) .603 (.087) .667 (.051) .561 (.077) .533 (.117) .624 (.054) .610 (.063)

49

Table 26: Our methods vs. the rest: mean classiﬁer AUC for = 0.2, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.876 (.017) .872 (.017) .875 (.016) .867 (.021) .866 (.021) .853 (.024) .850 (.017) .873 (.014) .879 (.013) .879 (.016) .874 (.015) .880 (.016) .873 (.015) .850 (.017) .873 (.016) .869 (.018) .861 (.022) .870 (.017) .867 (.017) .867 (.018) .861 (.018) .850 (.017) .861 (.018) .846 (.019) .844 (.020)

.876 (.017) .872 (.017) .875 (.016) .867 (.021) .866 (.021) .853 (.024) .850 (.017) .873 (.014) .879 (.013) .853 (.045) .797 (.081) .853 (.044) .797 (.080) .794 (.077) .795 (.084) .809 (.087) .765 (.115) .782 (.065) .761 (.070) .808 (.048) .799 (.040) .771 (.064) .782 (.046) .735 (.060) .724 (.080)

.661 (.031) .668 (.035) .652 (.043) .646 (.040) .654 (.037) .637 (.043) .701 (.016) .702 (.017) .725 (.018) .722 (.021) .705 (.021) .724 (.021) .708 (.021) .665 (.023) .681 (.023) .681 (.026) .660 (.036) .680 (.024) .690 (.023) .674 (.026) .678 (.026) .656 (.024) .667 (.024) .649 (.027) .660 (.028)

.661 (.031) .668 (.035) .652 (.043) .646 (.040) .654 (.037) .637 (.043) .701 (.016) .702 (.017) .725 (.018) .653 (.022) .621 (.019) .654 (.022) .622 (.019) .604 (.032) .604 (.027) .584 (.042) .570 (.043) .601 (.045) .619 (.049) .611 (.043) .618 (.048) .595 (.049) .622 (.052) .574 (.041) .593 (.052)

.626 (.080) .682 (.071) .608 (.075) .657 (.094) .674 (.053) .701 (.036) .679 (.021) .760 (.016) .688 (.027) .757 (.030) .796 (.014) .790 (.018) .798 (.014) .781 (.015) .786 (.016) .726 (.018) .729 (.038) .739 (.021) .783 (.016) .751 (.023) .773 (.018) .756 (.017) .738 (.021) .710 (.020) .720 (.027)

.626 (.080) .682 (.071) .608 (.075) .657 (.094) .674 (.053) .701 (.036) .679 (.021) .760 (.016) .688 (.027) .681 (.043) .647 (.062) .712 (.017) .648 (.059) .591 (.056) .624 (.064) .533 (.029) .513 (.043) .611 (.074) .662 (.078) .674 (.050) .693 (.048) .524 (.032) .583 (.056) .530 (.031) .516 (.026)

.642 (.011) .676 (.014) .630 (.021) .678 (.014) .654 (.007) .679 (.012) .500 (.000) .694 (.004) .683 (.005) .684 (.005) .725 (.004) .709 (.007) .726 (.004) .711 (.005) .726 (.004) .698 (.010) .692 (.021) .690 (.005) .718 (.004) .699 (.005) .718 (.004) .710 (.005) .718 (.004) .697 (.009) .692 (.016)

.642 (.011) .676 (.014) .630 (.021) .678 (.014) .654 (.007) .679 (.012) .500 (.000) .694 (.004) .683 (.005) .573 (.014) .668 (.013) .578 (.020) .670 (.011) .640 (.027) .666 (.010) .626 (.039) .564 (.059) .633 (.030) .674 (.022) .656 (.014) .686 (.008) .643 (.021) .664 (.024) .639 (.030) .634 (.026)

50

Table 27: Our methods vs. the rest: mean classiﬁer AUC for = 0.3, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.855 (.019) .849 (.020) .854 (.016) .846 (.019) .841 (.024) .830 (.026) .822 (.013) .853 (.015) .867 (.014) .863 (.018) .853 (.017) .862 (.018) .852 (.017) .828 (.022) .853 (.018) .847 (.022) .838 (.026) .854 (.019) .849 (.020) .849 (.020) .844 (.020) .831 (.021) .843 (.020) .826 (.023) .825 (.022)

.855 (.019) .849 (.020) .854 (.016) .846 (.019) .841 (.024) .830 (.026) .822 (.013) .853 (.015) .867 (.014) .824 (.050) .764 (.081) .824 (.050) .764 (.081) .757 (.088) .761 (.087) .766 (.091) .734 (.099) .779 (.068) .753 (.077) .796 (.047) .791 (.038) .755 (.060) .776 (.052) .730 (.061) .729 (.055)

.648 (.034) .656 (.032) .644 (.040) .646 (.043) .635 (.037) .622 (.039) .680 (.021) .683 (.022) .709 (.019) .705 (.022) .682 (.023) .707 (.022) .684 (.023) .649 (.027) .663 (.025) .660 (.027) .634 (.037) .659 (.026) .670 (.024) .655 (.025) .653 (.028) .636 (.026) .654 (.025) .627 (.027) .643 (.028)

.648 (.034) .656 (.032) .644 (.040) .646 (.043) .635 (.037) .622 (.039) .680 (.021) .683 (.022) .709 (.019) .638 (.022) .607 (.020) .638 (.021) .608 (.020) .594 (.030) .594 (.025) .578 (.043) .559 (.040) .603 (.040) .617 (.044) .605 (.039) .606 (.042) .592 (.045) .617 (.040) .571 (.037) .589 (.048)

.634 (.067) .649 (.076) .602 (.093) .621 (.094) .660 (.054) .666 (.037) .683 (.023) .733 (.015) .694 (.029) .749 (.028) .778 (.016) .780 (.018) .780 (.016) .762 (.018) .768 (.017) .715 (.019) .711 (.041) .721 (.022) .759 (.017) .728 (.023) .740 (.017) .732 (.019) .719 (.026) .693 (.023) .705 (.026)

.634 (.067) .649 (.076) .602 (.093) .621 (.094) .660 (.054) .666 (.037) .683 (.023) .733 (.015) .694 (.029) .685 (.031) .640 (.056) .703 (.017) .640 (.055) .626 (.047) .625 (.056) .545 (.039) .523 (.054) .618 (.076) .635 (.086) .664 (.051) .659 (.048) .545 (.030) .553 (.088) .550 (.032) .549 (.041)

.641 (.008) .669 (.012) .628 (.023) .669 (.012) .645 (.007) .668 (.012) .500 (.000) .681 (.006) .674 (.004) .675 (.005) .717 (.004) .695 (.006) .718 (.004) .703 (.005) .717 (.004) .688 (.008) .686 (.015) .682 (.005) .710 (.004) .685 (.005) .710 (.004) .700 (.005) .710 (.004) .685 (.008) .682 (.011)

.641 (.008) .669 (.012) .628 (.023) .669 (.012) .645 (.007) .668 (.012) .500 (.000) .681 (.006) .674 (.004) .563 (.013) .665 (.012) .565 (.015) .665 (.012) .651 (.012) .660 (.012) .619 (.036) .577 (.050) .638 (.010) .673 (.013) .649 (.008) .676 (.008) .650 (.012) .672 (.012) .637 (.018) .632 (.024)

51

Table 28: Our methods vs. the rest: mean classiﬁer AUC for = 0.4, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.827 (.020) .820 (.022) .829 (.018) .820 (.020) .809 (.026) .796 (.029) .785 (.018) .826 (.016) .846 (.015) .842 (.019) .827 (.020) .842 (.019) .826 (.020) .806 (.022) .826 (.022) .822 (.023) .811 (.027) .830 (.021) .824 (.023) .826 (.022) .820 (.022) .806 (.022) .822 (.022) .806 (.023) .805 (.025)

.827 (.020) .820 (.022) .829 (.018) .820 (.020) .809 (.026) .796 (.029) .785 (.018) .826 (.016) .846 (.015) .794 (.050) .728 (.078) .795 (.049) .727 (.078) .737 (.070) .726 (.080) .744 (.076) .703 (.094) .786 (.052) .768 (.050) .787 (.035) .779 (.034) .752 (.047) .779 (.039) .747 (.046) .745 (.048)

.627 (.036) .636 (.030) .623 (.042) .634 (.026) .612 (.038) .606 (.033) .656 (.016) .659 (.017) .688 (.016) .683 (.023) .663 (.024) .685 (.023) .664 (.024) .634 (.028) .645 (.025) .638 (.029) .615 (.039) .633 (.026) .652 (.026) .630 (.027) .635 (.029) .618 (.027) .640 (.027) .606 (.028) .627 (.029)

.627 (.036) .636 (.030) .623 (.042) .634 (.026) .612 (.038) .606 (.033) .656 (.016) .659 (.017) .688 (.016) .620 (.024) .592 (.019) .621 (.024) .592 (.019) .584 (.029) .582 (.023) .568 (.038) .548 (.036) .597 (.036) .615 (.035) .596 (.034) .596 (.034) .592 (.039) .613 (.030) .573 (.038) .588 (.043)

.647 (.043) .663 (.044) .640 (.054) .665 (.054) .658 (.032) .642 (.034) .675 (.022) .709 (.017) .690 (.027) .736 (.026) .756 (.021) .764 (.018) .759 (.022) .735 (.019) .748 (.022) .700 (.023) .695 (.036) .704 (.023) .736 (.020) .704 (.025) .707 (.024) .700 (.019) .704 (.024) .676 (.023) .687 (.024)

.647 (.043) .663 (.044) .640 (.054) .665 (.054) .658 (.032) .642 (.034) .675 (.022) .709 (.017) .690 (.027) .675 (.024) .622 (.054) .688 (.016) .622 (.053) .625 (.025) .615 (.053) .546 (.045) .517 (.047) .639 (.050) .667 (.040) .659 (.033) .643 (.036) .565 (.029) .608 (.053) .573 (.029) .575 (.043)

.633 (.006) .660 (.007) .626 (.017) .659 (.008) .637 (.006) .659 (.008) .500 (.000) .667 (.005) .665 (.004) .665 (.004) .709 (.004) .679 (.005) .709 (.004) .694 (.005) .708 (.005) .677 (.008) .671 (.017) .670 (.005) .696 (.005) .670 (.005) .696 (.004) .686 (.005) .696 (.005) .671 (.007) .668 (.009)

.633 (.006) .660 (.007) .626 (.017) .659 (.008) .637 (.006) .659 (.008) .500 (.000) .667 (.005) .665 (.004) .559 (.015) .651 (.014) .559 (.017) .652 (.013) .649 (.009) .648 (.012) .604 (.039) .571 (.046) .633 (.006) .662 (.006) .640 (.006) .663 (.006) .646 (.009) .662 (.005) .629 (.015) .624 (.019)

52

Table 29: Our methods vs. the rest: mean classiﬁer AUC for = 0.5, unbalanced datasets, pre-selected 4 features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.788 (.024) .779 (.024) .788 (.023) .780 (.024) .776 (.027) .768 (.028) .746 (.022) .795 (.024) .821 (.022) .815 (.021) .791 (.023) .815 (.021) .790 (.023) .778 (.024) .791 (.023) .789 (.028) .772 (.037) .792 (.024) .785 (.027) .786 (.024) .782 (.028) .771 (.025) .787 (.027) .769 (.025) .770 (.029)

.788 (.024) .779 (.024) .788 (.023) .780 (.024) .776 (.027) .768 (.028) .746 (.022) .795 (.024) .821 (.022) .750 (.053) .679 (.080) .750 (.052) .679 (.080) .698 (.067) .679 (.080) .699 (.082) .655 (.090) .767 (.030) .751 (.037) .759 (.031) .749 (.034) .732 (.036) .754 (.033) .739 (.032) .736 (.038)

.611 (.033) .616 (.027) .608 (.039) .617 (.033) .596 (.033) .583 (.032) .638 (.022) .641 (.023) .669 (.021) .662 (.023) .641 (.025) .664 (.024) .640 (.027) .618 (.028) .627 (.027) .616 (.030) .595 (.039) .609 (.027) .633 (.028) .604 (.028) .610 (.032) .599 (.028) .625 (.028) .583 (.028) .608 (.030)

.611 (.033) .616 (.027) .608 (.039) .617 (.033) .596 (.033) .583 (.032) .638 (.022) .641 (.023) .669 (.021) .601 (.023) .578 (.022) .602 (.022) .574 (.024) .572 (.028) .569 (.024) .557 (.034) .541 (.036) .589 (.033) .605 (.029) .583 (.032) .575 (.032) .583 (.033) .601 (.029) .573 (.036) .576 (.040)

.645 (.033) .655 (.030) .644 (.035) .658 (.026) .643 (.030) .623 (.031) .684 (.023) .683 (.016) .701 (.027) .728 (.026) .727 (.024) .749 (.020) .729 (.024) .710 (.019) .720 (.024) .684 (.023) .669 (.042) .682 (.025) .707 (.020) .680 (.027) .671 (.024) .667 (.019) .684 (.021) .655 (.021) .657 (.024)

.645 (.033) .655 (.030) .644 (.035) .658 (.026) .643 (.030) .623 (.031) .684 (.023) .683 (.016) .701 (.027) .666 (.023) .595 (.051) .676 (.016) .595 (.050) .616 (.022) .590 (.050) .543 (.047) .513 (.047) .643 (.032) .655 (.026) .642 (.030) .620 (.030) .574 (.022) .624 (.032) .586 (.023) .576 (.035)

.622 (.008) .646 (.006) .618 (.013) .644 (.006) .625 (.009) .645 (.007) .500 (.000) .653 (.004) .650 (.005) .651 (.005) .696 (.005) .660 (.006) .696 (.005) .686 (.005) .694 (.005) .665 (.009) .658 (.015) .652 (.005) .673 (.005) .652 (.005) .673 (.005) .667 (.005) .673 (.005) .651 (.007) .648 (.009)

.622 (.008) .646 (.006) .618 (.013) .644 (.006) .625 (.009) .645 (.007) .500 (.000) .653 (.004) .650 (.005) .554 (.013) .637 (.011) .553 (.014) .637 (.011) .643 (.007) .635 (.010) .597 (.041) .559 (.043) .624 (.006) .645 (.006) .628 (.006) .645 (.006) .638 (.007) .645 (.006) .618 (.013) .612 (.018)

53

Table 30: Our methods vs. the rest: mean classiﬁer accuracy for = 0.0, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.859 (.015) .855 (.017) .860 (.015) .851 (.028) .859 (.014) .847 (.016) .743 (.024) .850 (.018) .861 (.014) .858 (.015) .862 (.015) .746 (.025) .854 (.017) .860 (.014) .858 (.015) .859 (.015) .847 (.016) .819 (.019) .854 (.017) .853 (.018) .846 (.023) .860 (.014) .858 (.015) .859 (.014) .847 (.017) .819 (.019) .854 (.018) .852 (.017) .847 (.021)

.859 (.015) .855 (.017) .860 (.015) .851 (.028) .859 (.014) .847 (.016) .743 (.024) .850 (.018) .861 (.014) .858 (.015) .862 (.015) .746 (.025) .854 (.017) .859 (.031) .853 (.036) .857 (.031) .827 (.042) .700 (.103) .844 (.039) .803 (.070) .809 (.084) .860 (.014) .858 (.015) .858 (.015) .847 (.017) .820 (.019) .854 (.017) .851 (.017) .848 (.023)

.671 (.030) .674 (.027) .673 (.032) .653 (.051) .679 (.027) .676 (.025) .564 (.047) .650 (.036) .671 (.027) .675 (.025) .682 (.025) .565 (.047) .648 (.036) .682 (.026) .676 (.024) .681 (.025) .676 (.024) .645 (.027) .661 (.028) .657 (.030) .660 (.034) .682 (.026) .676 (.024) .681 (.025) .676 (.024) .644 (.027) .662 (.028) .659 (.031) .660 (.034)

.671 (.030) .674 (.027) .673 (.032) .653 (.051) .679 (.027) .676 (.025) .564 (.047) .650 (.036) .671 (.027) .675 (.025) .682 (.025) .565 (.047) .648 (.036) .580 (.090) .583 (.090) .580 (.090) .583 (.090) .538 (.062) .577 (.087) .563 (.079) .568 (.084) .682 (.026) .676 (.024) .681 (.025) .676 (.024) .644 (.026) .661 (.028) .659 (.031) .660 (.035)

.726 (.028) .741 (.031) .709 (.032) .729 (.034) .718 (.026) .739 (.030) .500 (.017) .739 (.031) .666 (.036) .742 (.030) .675 (.038) .500 (.017) .739 (.032) .725 (.026) .742 (.031) .719 (.026) .739 (.030) .728 (.032) .739 (.031) .717 (.029) .728 (.047) .725 (.026) .742 (.031) .719 (.027) .740 (.030) .728 (.032) .740 (.031) .717 (.028) .726 (.050)

.726 (.028) .741 (.031) .709 (.032) .729 (.034) .718 (.026) .739 (.030) .500 (.017) .739 (.031) .666 (.036) .742 (.030) .675 (.038) .500 (.017) .739 (.032) .643 (.069) .678 (.100) .659 (.045) .675 (.096) .654 (.085) .666 (.101) .617 (.056) .662 (.100) .725 (.026) .742 (.031) .719 (.029) .738 (.030) .729 (.031) .740 (.032) .716 (.028) .727 (.050)

.686 (.006) .695 (.005) .647 (.018) .634 (.006) .687 (.006) .696 (.005) .700 (.005) .701 (.005) .499 (.004) .695 (.005) .686 (.006) .701 (.005) .701 (.005) .688 (.006) .695 (.005) .698 (.005) .696 (.006) .701 (.005) .701 (.005) .675 (.038) .678 (.054) .688 (.006) .695 (.005) .698 (.005) .695 (.005) .701 (.005) .701 (.005) .680 (.031) .676 (.056)

.686 (.006) .695 (.005) .647 (.018) .634 (.006) .687 (.006) .696 (.005) .700 (.005) .701 (.005) .499 (.004) .695 (.005) .686 (.006) .701 (.005) .701 (.005) .606 (.079) .499 (.004) .578 (.024) .504 (.018) .527 (.028) .519 (.033) .516 (.025) .490 (.050) .688 (.006) .695 (.005) .698 (.006) .696 (.005) .701 (.005) .701 (.005) .670 (.044) .679 (.053)

54

Table 31: Our methods vs. the rest: mean classiﬁer accuracy for = 0.1, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.817 (.020) .821 (.019) .817 (.021) .817 (.020) .804 (.024) .798 (.024) .768 (.037) .814 (.020) .825 (.018) .826 (.017) .824 (.018) .685 (.029) .818 (.021) .822 (.019) .824 (.017) .823 (.017) .824 (.017) .793 (.020) .821 (.020) .815 (.019) .813 (.038) .831 (.019) .826 (.019) .828 (.019) .818 (.019) .793 (.022) .822 (.021) .811 (.025) .807 (.025)

.817 (.020) .821 (.019) .817 (.021) .817 (.020) .804 (.024) .798 (.024) .768 (.037) .814 (.020) .825 (.018) .826 (.017) .824 (.018) .717 (.084) .678 (.098) .822 (.035) .797 (.048) .819 (.035) .784 (.051) .637 (.113) .775 (.078) .740 (.094) .756 (.107) .539 (.070) .537 (.071) .537 (.067) .536 (.065) .534 (.060) .536 (.067) .529 (.068) .527 (.064)

.645 (.029) .646 (.028) .652 (.028) .645 (.038) .633 (.032) .628 (.033) .573 (.039) .629 (.033) .649 (.033) .653 (.029) .661 (.025) .544 (.038) .615 (.038) .662 (.027) .654 (.026) .662 (.026) .655 (.025) .625 (.029) .644 (.029) .636 (.030) .635 (.034) .640 (.029) .640 (.029) .637 (.029) .631 (.038) .614 (.028) .629 (.031) .611 (.029) .624 (.034)

.645 (.029) .646 (.028) .652 (.028) .645 (.038) .633 (.032) .628 (.033) .573 (.039) .629 (.033) .649 (.033) .653 (.029) .661 (.025) .544 (.046) .543 (.045) .574 (.082) .576 (.082) .575 (.082) .574 (.083) .540 (.060) .565 (.080) .556 (.070) .560 (.073) .506 (.029) .513 (.037) .506 (.025) .510 (.032) .506 (.026) .511 (.034) .506 (.027) .514 (.038)

.690 (.029) .714 (.030) .693 (.029) .716 (.029) .672 (.032) .701 (.031) .500 (.018) .713 (.028) .664 (.035) .719 (.030) .667 (.034) .500 (.018) .721 (.028) .722 (.026) .731 (.027) .719 (.026) .729 (.027) .720 (.028) .730 (.027) .710 (.029) .711 (.049) .699 (.027) .721 (.029) .694 (.031) .720 (.028) .703 (.028) .720 (.028) .682 (.031) .706 (.036)

.690 (.029) .714 (.030) .693 (.029) .716 (.029) .672 (.032) .701 (.031) .500 (.018) .713 (.028) .664 (.035) .719 (.030) .667 (.034) .500 (.018) .616 (.071) .658 (.062) .600 (.103) .660 (.051) .615 (.096) .609 (.073) .592 (.098) .618 (.060) .596 (.098) .509 (.027) .518 (.045) .509 (.025) .519 (.044) .511 (.028) .520 (.045) .509 (.031) .510 (.033)

.665 (.006) .672 (.005) .665 (.006) .670 (.009) .665 (.006) .672 (.006) .673 (.006) .672 (.006) .499 (.005) .670 (.006) .680 (.007) .687 (.006) .695 (.005) .678 (.006) .691 (.005) .694 (.006) .693 (.006) .689 (.006) .697 (.005) .669 (.027) .661 (.062) .676 (.009) .690 (.006) .681 (.008) .691 (.006) .688 (.006) .694 (.005) .672 (.012) .656 (.054)

.665 (.006) .672 (.005) .665 (.006) .670 (.009) .665 (.006) .672 (.006) .673 (.006) .672 (.006) .499 (.005) .670 (.006) .680 (.007) .566 (.031) .565 (.034) .616 (.069) .512 (.039) .556 (.007) .563 (.028) .541 (.032) .551 (.031) .553 (.030) .557 (.051) .528 (.038) .525 (.038) .518 (.032) .514 (.025) .529 (.034) .525 (.035) .521 (.032) .520 (.035)

55

Table 32: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.794 (.023) .795 (.022) .793 (.019) .794 (.021) .778 (.025) .771 (.025) .770 (.030) .793 (.022) .789 (.019) .801 (.020) .794 (.020) .664 (.030) .795 (.025) .795 (.019) .800 (.019) .799 (.019) .801 (.019) .771 (.025) .797 (.020) .789 (.021) .791 (.024) .809 (.022) .801 (.024) .807 (.022) .796 (.026) .776 (.026) .803 (.024) .780 (.023) .777 (.029)

.794 (.023) .795 (.022) .793 (.019) .794 (.021) .778 (.025) .771 (.025) .770 (.030) .793 (.022) .789 (.019) .801 (.020) .794 (.020) .686 (.087) .637 (.089) .790 (.026) .767 (.041) .789 (.035) .752 (.069) .638 (.096) .750 (.073) .720 (.086) .714 (.111) .548 (.057) .541 (.059) .546 (.057) .550 (.064) .543 (.046) .560 (.069) .538 (.057) .536 (.055)

.624 (.032) .631 (.031) .631 (.032) .629 (.042) .614 (.031) .608 (.036) .576 (.039) .620 (.030) .632 (.034) .637 (.030) .659 (.029) .537 (.033) .604 (.037) .649 (.028) .638 (.028) .649 (.028) .638 (.030) .613 (.032) .628 (.033) .624 (.031) .613 (.040) .617 (.029) .620 (.032) .615 (.029) .616 (.034) .597 (.031) .614 (.032) .591 (.033) .608 (.036)

.624 (.032) .631 (.031) .631 (.032) .629 (.042) .614 (.031) .608 (.036) .576 (.039) .620 (.030) .632 (.034) .637 (.030) .659 (.029) .533 (.046) .534 (.044) .571 (.073) .571 (.075) .570 (.073) .572 (.075) .541 (.056) .568 (.073) .553 (.064) .554 (.067) .505 (.025) .514 (.038) .504 (.024) .514 (.040) .504 (.025) .515 (.038) .501 (.023) .514 (.039)

.677 (.031) .693 (.032) .680 (.030) .692 (.032) .654 (.033) .679 (.036) .501 (.018) .692 (.032) .665 (.043) .696 (.029) .655 (.042) .500 (.017) .706 (.029) .713 (.028) .719 (.027) .714 (.026) .717 (.027) .709 (.028) .716 (.026) .697 (.030) .697 (.046) .681 (.027) .704 (.028) .677 (.029) .703 (.028) .685 (.027) .703 (.030) .666 (.027) .679 (.035)

.677 (.031) .693 (.032) .680 (.030) .692 (.032) .654 (.033) .679 (.036) .501 (.018) .692 (.032) .665 (.043) .696 (.029) .655 (.042) .501 (.021) .576 (.061) .654 (.055) .569 (.090) .655 (.049) .576 (.083) .589 (.068) .566 (.082) .610 (.058) .563 (.086) .515 (.032) .520 (.041) .514 (.031) .523 (.043) .521 (.033) .527 (.045) .513 (.031) .515 (.033)

.648 (.005) .649 (.005) .648 (.005) .648 (.005) .648 (.005) .647 (.005) .651 (.006) .648 (.006) .498 (.004) .646 (.007) .671 (.008) .678 (.006) .688 (.005) .667 (.008) .686 (.005) .685 (.005) .689 (.005) .679 (.005) .689 (.005) .659 (.016) .652 (.045) .664 (.008) .685 (.005) .667 (.007) .683 (.006) .678 (.005) .687 (.005) .659 (.018) .658 (.029)

.648 (.005) .649 (.005) .648 (.005) .648 (.005) .648 (.005) .647 (.005) .651 (.006) .648 (.006) .498 (.004) .646 (.007) .671 (.008) .564 (.032) .568 (.032) .622 (.053) .504 (.022) .536 (.005) .538 (.016) .553 (.031) .540 (.025) .556 (.035) .550 (.042) .588 (.041) .584 (.030) .549 (.046) .548 (.044) .579 (.023) .587 (.023) .583 (.028) .592 (.026)

56

Table 33: Our methods vs. the rest: mean classiﬁer accuracy for = 0.3, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.770 (.022) .769 (.023) .772 (.022) .769 (.022) .756 (.028) .748 (.027) .756 (.026) .767 (.022) .760 (.026) .774 (.025) .770 (.027) .663 (.040) .770 (.028) .773 (.023) .772 (.023) .777 (.022) .771 (.023) .751 (.023) .769 (.024) .763 (.027) .762 (.034) .788 (.025) .776 (.027) .783 (.026) .773 (.028) .764 (.026) .777 (.028) .768 (.029) .757 (.031)

.770 (.022) .769 (.023) .772 (.022) .769 (.022) .756 (.028) .748 (.027) .756 (.026) .767 (.022) .760 (.026) .774 (.025) .770 (.027) .649 (.099) .608 (.085) .758 (.038) .719 (.094) .750 (.047) .702 (.105) .630 (.101) .694 (.112) .666 (.105) .694 (.113) .576 (.071) .569 (.074) .573 (.069) .578 (.076) .572 (.057) .588 (.075) .570 (.068) .579 (.079)

.608 (.032) .616 (.032) .614 (.033) .616 (.041) .599 (.034) .590 (.036) .566 (.039) .606 (.034) .622 (.037) .621 (.037) .631 (.033) .530 (.034) .588 (.037) .633 (.031) .624 (.033) .632 (.031) .622 (.036) .600 (.032) .616 (.036) .606 (.035) .601 (.042) .598 (.033) .606 (.032) .596 (.031) .599 (.034) .587 (.031) .599 (.033) .575 (.033) .592 (.036)

.608 (.032) .616 (.032) .614 (.033) .616 (.041) .599 (.034) .590 (.036) .566 (.039) .606 (.034) .622 (.037) .621 (.037) .631 (.033) .526 (.037) .533 (.045) .567 (.069) .569 (.071) .567 (.069) .571 (.071) .543 (.055) .565 (.068) .546 (.058) .552 (.061) .514 (.030) .522 (.040) .514 (.029) .517 (.035) .513 (.031) .525 (.040) .508 (.027) .522 (.043)

.668 (.028) .680 (.030) .670 (.027) .681 (.030) .644 (.032) .664 (.033) .505 (.016) .682 (.033) .667 (.044) .682 (.030) .671 (.034) .503 (.015) .699 (.027) .708 (.026) .710 (.027) .709 (.025) .709 (.029) .697 (.026) .711 (.028) .691 (.029) .694 (.040) .669 (.025) .695 (.028) .666 (.029) .692 (.028) .670 (.027) .696 (.029) .655 (.028) .674 (.032)

.668 (.028) .680 (.030) .670 (.027) .681 (.030) .644 (.032) .664 (.033) .505 (.016) .682 (.033) .667 (.044) .682 (.030) .671 (.034) .501 (.019) .568 (.070) .654 (.057) .567 (.097) .651 (.059) .585 (.093) .563 (.066) .564 (.089) .594 (.077) .579 (.091) .534 (.038) .556 (.060) .533 (.043) .556 (.056) .550 (.039) .568 (.063) .537 (.043) .557 (.057)

.636 (.006) .637 (.006) .635 (.006) .637 (.006) .635 (.006) .636 (.006) .636 (.006) .637 (.006) .499 (.009) .633 (.007) .660 (.010) .666 (.008) .679 (.005) .658 (.010) .679 (.006) .677 (.005) .680 (.005) .668 (.006) .680 (.005) .648 (.013) .652 (.030) .653 (.011) .676 (.005) .655 (.009) .674 (.006) .668 (.006) .678 (.005) .651 (.013) .648 (.021)

.636 (.006) .637 (.006) .635 (.006) .637 (.006) .635 (.006) .636 (.006) .636 (.006) .637 (.006) .499 (.009) .633 (.007) .660 (.010) .553 (.039) .556 (.039) .599 (.059) .505 (.026) .520 (.005) .527 (.027) .549 (.038) .533 (.035) .561 (.042) .551 (.048) .609 (.018) .596 (.011) .568 (.043) .560 (.042) .590 (.012) .595 (.011) .594 (.021) .595 (.022)

57

Table 34: Our methods vs. the rest: mean classiﬁer accuracy for = 0.4, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.751 (.024) .747 (.026) .753 (.024) .747 (.026) .734 (.027) .727 (.029) .726 (.028) .744 (.027) .734 (.030) .745 (.026) .751 (.026) .651 (.042) .752 (.030) .755 (.023) .740 (.027) .755 (.021) .741 (.026) .725 (.026) .742 (.026) .738 (.028) .727 (.035) .766 (.027) .758 (.025) .762 (.029) .753 (.027) .743 (.028) .762 (.025) .749 (.029) .747 (.028)

.751 (.024) .747 (.026) .753 (.024) .747 (.026) .734 (.027) .727 (.029) .726 (.028) .744 (.027) .734 (.030) .745 (.026) .751 (.026) .624 (.104) .604 (.097) .726 (.053) .671 (.107) .718 (.056) .657 (.115) .623 (.101) .652 (.117) .625 (.110) .640 (.116) .625 (.061) .622 (.074) .614 (.065) .624 (.079) .625 (.056) .650 (.071) .621 (.063) .635 (.081)

.598 (.034) .604 (.033) .602 (.033) .601 (.039) .585 (.036) .578 (.035) .566 (.035) .595 (.034) .606 (.038) .608 (.036) .608 (.039) .531 (.030) .574 (.036) .619 (.030) .611 (.031) .619 (.030) .609 (.032) .590 (.032) .604 (.031) .594 (.033) .587 (.041) .582 (.030) .588 (.034) .578 (.029) .580 (.032) .574 (.030) .586 (.033) .565 (.031) .574 (.036)

.598 (.034) .604 (.033) .602 (.033) .601 (.039) .585 (.036) .578 (.035) .566 (.035) .595 (.034) .606 (.038) .608 (.036) .608 (.039) .525 (.036) .530 (.043) .568 (.062) .567 (.063) .569 (.062) .566 (.063) .549 (.050) .561 (.061) .546 (.054) .553 (.058) .526 (.031) .526 (.040) .524 (.031) .521 (.039) .524 (.030) .527 (.040) .520 (.032) .526 (.038)

.656 (.030) .667 (.030) .656 (.030) .670 (.029) .626 (.038) .654 (.035) .502 (.022) .667 (.031) .646 (.052) .659 (.031) .677 (.042) .498 (.020) .685 (.028) .697 (.027) .696 (.027) .700 (.026) .691 (.031) .681 (.024) .695 (.027) .680 (.026) .680 (.044) .653 (.027) .684 (.027) .649 (.031) .678 (.029) .654 (.026) .685 (.026) .652 (.026) .667 (.032)

.656 (.030) .667 (.030) .656 (.030) .670 (.029) .626 (.038) .654 (.035) .502 (.022) .667 (.031) .646 (.052) .659 (.031) .677 (.042) .498 (.019) .573 (.073) .652 (.054) .558 (.087) .649 (.058) .571 (.086) .563 (.065) .562 (.083) .584 (.073) .564 (.084) .572 (.045) .609 (.055) .552 (.048) .585 (.060) .576 (.040) .612 (.053) .584 (.045) .600 (.054)

.627 (.006) .627 (.006) .627 (.006) .627 (.006) .626 (.007) .626 (.006) .624 (.007) .625 (.007) .498 (.007) .617 (.006) .652 (.012) .657 (.007) .670 (.004) .649 (.013) .670 (.005) .667 (.005) .671 (.005) .659 (.006) .670 (.005) .637 (.015) .635 (.031) .641 (.010) .668 (.005) .643 (.010) .667 (.006) .658 (.006) .669 (.005) .641 (.012) .642 (.018)

.627 (.006) .627 (.006) .627 (.006) .627 (.006) .626 (.007) .626 (.006) .624 (.007) .625 (.007) .498 (.007) .617 (.006) .652 (.012) .555 (.046) .556 (.046) .582 (.062) .500 (.004) .510 (.005) .511 (.012) .554 (.045) .520 (.032) .551 (.047) .545 (.052) .614 (.010) .599 (.006) .597 (.031) .593 (.028) .596 (.009) .599 (.006) .601 (.016) .599 (.014)

58

Table 35: Our methods vs. the rest: mean classiﬁer accuracy for = 0.5, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.723 (.026) .718 (.026) .725 (.025) .717 (.027) .704 (.030) .701 (.029) .705 (.031) .714 (.025) .711 (.031) .717 (.023) .726 (.026) .653 (.042) .732 (.030) .734 (.024) .706 (.029) .733 (.025) .704 (.030) .706 (.030) .704 (.029) .711 (.032) .690 (.037) .742 (.028) .736 (.029) .733 (.030) .730 (.028) .725 (.028) .742 (.027) .729 (.031) .726 (.028)

.723 (.026) .718 (.026) .725 (.025) .717 (.027) .704 (.030) .701 (.029) .705 (.031) .714 (.025) .711 (.031) .717 (.023) .726 (.026) .621 (.099) .607 (.093) .707 (.043) .641 (.098) .700 (.048) .627 (.104) .627 (.094) .628 (.103) .616 (.102) .612 (.103) .650 (.056) .655 (.064) .633 (.059) .633 (.066) .645 (.045) .669 (.056) .652 (.052) .656 (.064)

.580 (.034) .582 (.034) .582 (.034) .579 (.038) .567 (.034) .558 (.040) .559 (.038) .578 (.036) .588 (.037) .588 (.036) .587 (.037) .529 (.030) .560 (.034) .598 (.033) .589 (.035) .598 (.032) .581 (.041) .576 (.029) .583 (.035) .575 (.035) .570 (.041) .565 (.031) .569 (.033) .563 (.031) .559 (.035) .560 (.031) .570 (.034) .551 (.033) .560 (.032)

.580 (.034) .582 (.034) .582 (.034) .579 (.038) .567 (.034) .558 (.040) .559 (.038) .578 (.036) .588 (.037) .588 (.036) .587 (.037) .520 (.035) .525 (.041) .553 (.054) .551 (.055) .553 (.054) .545 (.057) .538 (.046) .546 (.053) .538 (.044) .535 (.049) .531 (.031) .528 (.039) .529 (.031) .518 (.037) .530 (.032) .529 (.039) .528 (.029) .529 (.037)

.642 (.031) .657 (.032) .644 (.028) .659 (.031) .618 (.033) .641 (.029) .506 (.022) .655 (.031) .636 (.045) .638 (.031) .655 (.038) .503 (.019) .681 (.029) .684 (.026) .681 (.029) .686 (.026) .680 (.031) .667 (.029) .681 (.028) .666 (.027) .655 (.054) .643 (.029) .675 (.031) .634 (.034) .669 (.034) .641 (.027) .677 (.030) .641 (.030) .664 (.030)

.642 (.031) .657 (.032) .644 (.028) .659 (.031) .618 (.033) .641 (.029) .506 (.022) .655 (.031) .636 (.045) .638 (.031) .655 (.038) .503 (.022) .581 (.073) .640 (.058) .543 (.076) .634 (.064) .556 (.077) .565 (.061) .545 (.074) .577 (.067) .555 (.076) .590 (.037) .626 (.047) .566 (.044) .604 (.060) .581 (.031) .630 (.046) .602 (.036) .623 (.043)

.616 (.006) .617 (.006) .616 (.006) .617 (.006) .616 (.006) .616 (.006) .613 (.008) .614 (.009) .497 (.005) .605 (.008) .638 (.015) .647 (.006) .657 (.005) .634 (.013) .659 (.005) .656 (.005) .659 (.006) .648 (.006) .657 (.005) .627 (.010) .626 (.024) .627 (.010) .657 (.005) .626 (.012) .655 (.008) .648 (.006) .657 (.005) .626 (.010) .626 (.015)

.616 (.006) .617 (.006) .616 (.006) .617 (.006) .616 (.006) .616 (.006) .613 (.008) .614 (.009) .497 (.005) .605 (.008) .638 (.015) .553 (.049) .554 (.049) .584 (.053) .503 (.019) .505 (.004) .515 (.032) .551 (.048) .516 (.034) .555 (.047) .545 (.047) .611 (.009) .602 (.006) .603 (.017) .597 (.022) .599 (.007) .602 (.006) .599 (.012) .599 (.011)

59

Table 36: Our methods vs. the rest: mean classiﬁer AUC for = 0.0, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.904 (.013) .909 (.013) .908 (.013) .909 (.022) .906 (.013) .907 (.013) .900 (.013) .909 (.013) .904 (.014) .904 (.014) .909 (.013) .905 (.013) .908 (.013) .884 (.015) .909 (.014) .900 (.016) .894 (.022) .904 (.014) .909 (.013) .905 (.013) .908 (.013) .885 (.015) .910 (.014) .901 (.015) .893 (.022)

.904 (.013) .909 (.013) .908 (.013) .909 (.022) .906 (.013) .907 (.013) .900 (.013) .909 (.013) .904 (.014) .903 (.015) .896 (.021) .902 (.014) .896 (.021) .870 (.041) .896 (.044) .869 (.032) .878 (.054) .904 (.014) .909 (.013) .905 (.013) .908 (.013) .884 (.015) .909 (.014) .900 (.015) .894 (.022)

.725 (.033) .724 (.027) .723 (.032) .699 (.056) .737 (.028) .727 (.025) .728 (.025) .730 (.026) .744 (.027) .741 (.027) .729 (.026) .741 (.027) .730 (.025) .698 (.028) .702 (.027) .703 (.031) .704 (.031) .741 (.027) .729 (.026) .741 (.027) .730 (.025) .698 (.028) .703 (.028) .703 (.031) .704 (.031)

.725 (.033) .724 (.027) .723 (.032) .699 (.056) .737 (.028) .727 (.025) .728 (.025) .730 (.026) .744 (.027) .684 (.063) .662 (.072) .684 (.062) .664 (.071) .641 (.076) .640 (.078) .620 (.094) .618 (.094) .741 (.027) .729 (.026) .741 (.027) .730 (.025) .698 (.028) .703 (.028) .702 (.033) .702 (.033)

.781 (.028) .787 (.032) .763 (.032) .758 (.037) .778 (.028) .788 (.031) .729 (.037) .791 (.032) .741 (.041) .786 (.027) .791 (.032) .779 (.027) .790 (.031) .794 (.033) .786 (.032) .773 (.032) .770 (.038) .786 (.027) .791 (.032) .779 (.027) .790 (.031) .794 (.033) .786 (.032) .772 (.030) .770 (.038)

.781 (.028) .787 (.032) .763 (.032) .758 (.037) .778 (.028) .788 (.031) .729 (.037) .791 (.032) .741 (.041) .734 (.050) .728 (.085) .749 (.027) .738 (.060) .771 (.042) .717 (.094) .738 (.040) .703 (.102) .786 (.027) .791 (.032) .780 (.027) .790 (.031) .794 (.034) .785 (.032) .772 (.030) .770 (.038)

.696 (.007) .729 (.006) .646 (.013) .644 (.009) .700 (.009) .731 (.005) .500 (.000) .730 (.005) .702 (.007) .703 (.005) .730 (.005) .731 (.009) .730 (.005) .734 (.005) .730 (.005) .717 (.015) .710 (.027) .703 (.005) .730 (.005) .730 (.009) .730 (.005) .734 (.005) .730 (.005) .718 (.017) .711 (.021)

.696 (.007) .729 (.006) .646 (.013) .644 (.009) .700 (.009) .731 (.005) .500 (.000) .730 (.005) .702 (.007) .665 (.009) .500 (.000) .680 (.014) .587 (.003) .580 (.023) .527 (.077) .628 (.041) .498 (.086) .703 (.005) .730 (.005) .731 (.010) .730 (.005) .734 (.005) .730 (.005) .715 (.018) .712 (.020)

60

Table 37: Our methods vs. the rest: mean classiﬁer AUC for = 0.1, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.892 (.015) .890 (.017) .892 (.015) .890 (.017) .888 (.016) .877 (.019) .878 (.016) .894 (.014) .895 (.013) .894 (.014) .895 (.013) .895 (.014) .893 (.014) .866 (.016) .893 (.016) .880 (.022) .872 (.039) .883 (.017) .887 (.016) .880 (.018) .879 (.017) .862 (.018) .881 (.018) .869 (.021) .865 (.024)

.892 (.015) .890 (.017) .892 (.015) .890 (.017) .888 (.016) .877 (.019) .878 (.016) .894 (.014) .895 (.013) .889 (.018) .863 (.021) .889 (.018) .863 (.021) .850 (.034) .860 (.027) .848 (.050) .838 (.043) .803 (.066) .786 (.074) .795 (.074) .805 (.056) .800 (.055) .805 (.049) .718 (.114) .704 (.123)

.685 (.033) .686 (.028) .694 (.032) .685 (.041) .688 (.032) .669 (.033) .701 (.027) .704 (.028) .720 (.031) .717 (.029) .703 (.027) .718 (.029) .705 (.027) .670 (.030) .680 (.030) .676 (.034) .676 (.033) .682 (.032) .686 (.030) .670 (.035) .672 (.035) .659 (.031) .666 (.032) .650 (.031) .661 (.035)

.685 (.033) .686 (.028) .694 (.032) .685 (.041) .688 (.032) .669 (.033) .701 (.027) .704 (.028) .720 (.031) .670 (.054) .648 (.061) .670 (.054) .649 (.061) .629 (.064) .630 (.065) .619 (.075) .610 (.075) .602 (.054) .614 (.055) .598 (.060) .601 (.061) .603 (.054) .617 (.057) .569 (.049) .583 (.058)

.750 (.030) .764 (.031) .748 (.029) .758 (.030) .734 (.035) .757 (.033) .728 (.038) .774 (.030) .731 (.035) .781 (.025) .782 (.029) .776 (.027) .782 (.028) .784 (.031) .776 (.029) .764 (.031) .758 (.036) .754 (.028) .773 (.030) .750 (.030) .769 (.030) .764 (.031) .761 (.030) .734 (.032) .748 (.031)

.750 (.030) .764 (.031) .748 (.029) .758 (.030) .734 (.035) .757 (.033) .728 (.038) .774 (.030) .731 (.035) .744 (.032) .698 (.086) .744 (.027) .710 (.067) .753 (.036) .680 (.097) .727 (.039) .647 (.106) .600 (.102) .659 (.106) .642 (.089) .691 (.069) .667 (.077) .679 (.071) .615 (.068) .623 (.081)

.675 (.007) .705 (.006) .675 (.006) .704 (.005) .680 (.006) .706 (.006) .500 (.000) .710 (.005) .695 (.007) .697 (.006) .726 (.005) .723 (.009) .727 (.005) .718 (.005) .726 (.005) .710 (.013) .703 (.028) .694 (.006) .725 (.006) .705 (.007) .725 (.006) .717 (.005) .723 (.005) .709 (.010) .695 (.033)

.675 (.007) .705 (.006) .675 (.006) .704 (.005) .680 (.006) .706 (.006) .500 (.000) .710 (.005) .695 (.007) .662 (.006) .674 (.027) .676 (.013) .681 (.020) .628 (.040) .666 (.041) .664 (.023) .614 (.066) .584 (.077) .589 (.123) .598 (.090) .624 (.077) .612 (.054) .598 (.084) .621 (.050) .627 (.050)

61

Table 38: Our methods vs. the rest: mean classiﬁer AUC for = 0.2, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.876 (.017) .872 (.016) .876 (.017) .871 (.017) .868 (.021) .850 (.023) .852 (.018) .874 (.015) .881 (.015) .878 (.015) .877 (.014) .879 (.015) .875 (.014) .849 (.019) .876 (.016) .861 (.024) .857 (.028) .866 (.019) .867 (.018) .862 (.020) .860 (.020) .847 (.022) .861 (.020) .840 (.021) .835 (.025)

.876 (.017) .872 (.016) .876 (.017) .871 (.017) .868 (.021) .850 (.023) .852 (.018) .874 (.015) .881 (.015) .869 (.016) .833 (.024) .869 (.016) .833 (.024) .821 (.036) .832 (.025) .836 (.040) .803 (.057) .788 (.059) .759 (.066) .792 (.065) .792 (.055) .777 (.053) .792 (.045) .694 (.096) .681 (.100)

.659 (.038) .664 (.032) .668 (.036) .662 (.044) .657 (.033) .641 (.036) .681 (.030) .683 (.029) .721 (.026) .703 (.031) .686 (.028) .704 (.031) .688 (.028) .656 (.034) .663 (.031) .664 (.035) .649 (.041) .657 (.033) .666 (.032) .651 (.034) .654 (.036) .641 (.034) .650 (.033) .626 (.037) .643 (.038)

.659 (.038) .664 (.032) .668 (.036) .662 (.044) .657 (.033) .641 (.036) .681 (.030) .683 (.029) .721 (.026) .659 (.051) .638 (.057) .661 (.051) .639 (.058) .623 (.059) .622 (.061) .616 (.066) .602 (.067) .584 (.053) .604 (.055) .589 (.052) .601 (.058) .592 (.048) .612 (.051) .551 (.044) .580 (.054)

.735 (.035) .744 (.033) .735 (.034) .742 (.032) .718 (.036) .732 (.034) .731 (.041) .755 (.032) .725 (.042) .772 (.029) .773 (.030) .770 (.029) .771 (.030) .774 (.030) .766 (.029) .753 (.030) .743 (.041) .735 (.029) .755 (.032) .731 (.029) .750 (.033) .744 (.030) .746 (.032) .714 (.030) .726 (.036)

.735 (.035) .744 (.033) .735 (.034) .742 (.032) .718 (.036) .732 (.034) .731 (.041) .755 (.032) .725 (.042) .732 (.031) .670 (.073) .735 (.029) .678 (.066) .727 (.039) .662 (.073) .714 (.040) .617 (.091) .605 (.101) .645 (.091) .638 (.089) .676 (.061) .657 (.067) .671 (.069) .611 (.060) .616 (.071)

.661 (.005) .692 (.006) .662 (.006) .691 (.006) .666 (.007) .691 (.006) .500 (.000) .694 (.006) .689 (.005) .689 (.005) .721 (.005) .713 (.007) .721 (.005) .707 (.006) .719 (.006) .700 (.009) .692 (.021) .687 (.005) .717 (.006) .693 (.006) .717 (.006) .707 (.006) .716 (.006) .698 (.012) .697 (.013)

.661 (.005) .692 (.006) .662 (.006) .691 (.006) .666 (.007) .691 (.006) .500 (.000) .694 (.006) .689 (.005) .655 (.006) .680 (.011) .668 (.010) .682 (.009) .637 (.033) .672 (.014) .653 (.026) .621 (.051) .626 (.035) .665 (.034) .639 (.019) .656 (.048) .642 (.026) .655 (.037) .649 (.022) .647 (.028)

62

Table 39: Our methods vs. the rest: mean classiﬁer AUC for = 0.3, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.853 (.022) .846 (.024) .853 (.022) .846 (.023) .841 (.023) .825 (.028) .820 (.024) .850 (.021) .862 (.016) .860 (.018) .854 (.019) .860 (.018) .853 (.020) .832 (.023) .855 (.020) .838 (.026) .829 (.036) .846 (.019) .842 (.021) .842 (.020) .838 (.024) .829 (.021) .838 (.022) .819 (.024) .811 (.028)

.853 (.022) .846 (.024) .853 (.022) .846 (.023) .841 (.023) .825 (.028) .820 (.024) .850 (.021) .862 (.016) .842 (.024) .798 (.043) .841 (.024) .798 (.043) .797 (.038) .797 (.043) .795 (.070) .766 (.073) .767 (.081) .733 (.093) .784 (.053) .776 (.054) .757 (.055) .766 (.055) .705 (.076) .703 (.076)

.641 (.038) .646 (.036) .647 (.038) .645 (.042) .634 (.036) .615 (.036) .662 (.035) .662 (.035) .676 (.036) .683 (.035) .668 (.037) .685 (.034) .669 (.037) .638 (.036) .647 (.035) .642 (.038) .630 (.046) .633 (.036) .647 (.037) .628 (.036) .638 (.041) .621 (.035) .636 (.037) .601 (.036) .624 (.041)

.641 (.038) .646 (.036) .647 (.038) .645 (.042) .634 (.036) .615 (.036) .662 (.035) .662 (.035) .676 (.036) .641 (.049) .622 (.056) .643 (.049) .622 (.057) .602 (.057) .609 (.054) .596 (.061) .587 (.063) .585 (.046) .602 (.051) .581 (.044) .597 (.045) .583 (.051) .610 (.045) .551 (.042) .573 (.054)

.726 (.028) .728 (.031) .727 (.027) .728 (.031) .700 (.033) .718 (.034) .737 (.036) .739 (.030) .737 (.029) .768 (.024) .762 (.029) .767 (.024) .760 (.029) .760 (.029) .754 (.029) .743 (.031) .738 (.035) .720 (.027) .739 (.031) .718 (.029) .736 (.031) .722 (.030) .732 (.031) .700 (.031) .714 (.032)

.726 (.028) .728 (.031) .727 (.027) .728 (.031) .700 (.033) .718 (.034) .737 (.036) .739 (.030) .737 (.029) .727 (.028) .664 (.073) .730 (.027) .668 (.072) .691 (.062) .659 (.070) .700 (.048) .629 (.083) .623 (.083) .651 (.088) .651 (.062) .668 (.060) .653 (.070) .664 (.072) .631 (.050) .638 (.061)

.653 (.006) .675 (.007) .653 (.006) .677 (.006) .655 (.006) .677 (.006) .502 (.014) .680 (.006) .682 (.006) .684 (.005) .714 (.005) .705 (.008) .714 (.006) .698 (.006) .711 (.006) .691 (.009) .687 (.015) .678 (.005) .707 (.006) .680 (.006) .706 (.006) .695 (.006) .706 (.006) .690 (.008) .686 (.010)

.653 (.006) .675 (.007) .653 (.006) .677 (.006) .655 (.006) .677 (.006) .502 (.014) .680 (.006) .682 (.006) .648 (.005) .672 (.009) .661 (.008) .673 (.009) .649 (.013) .667 (.011) .634 (.029) .619 (.040) .634 (.012) .671 (.011) .630 (.021) .667 (.013) .649 (.013) .669 (.014) .649 (.017) .644 (.021)

63

Table 40: Our methods vs. the rest: mean classiﬁer AUC for = 0.4, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.827 (.022) .817 (.023) .827 (.021) .818 (.022) .807 (.027) .794 (.030) .788 (.025) .826 (.022) .846 (.022) .841 (.020) .828 (.020) .840 (.020) .827 (.021) .807 (.024) .827 (.022) .814 (.028) .805 (.037) .823 (.024) .816 (.023) .818 (.025) .814 (.025) .802 (.026) .816 (.024) .798 (.028) .792 (.027)

.827 (.022) .817 (.023) .827 (.021) .818 (.022) .807 (.027) .794 (.030) .788 (.025) .826 (.022) .846 (.022) .810 (.028) .754 (.050) .809 (.026) .754 (.049) .762 (.038) .753 (.049) .755 (.065) .720 (.077) .781 (.042) .759 (.055) .777 (.041) .771 (.041) .749 (.043) .775 (.038) .725 (.048) .728 (.060)

.623 (.035) .627 (.035) .628 (.036) .625 (.041) .612 (.037) .598 (.038) .640 (.034) .641 (.033) .663 (.035) .665 (.033) .647 (.034) .665 (.033) .647 (.034) .624 (.037) .633 (.036) .627 (.035) .618 (.043) .611 (.034) .627 (.040) .606 (.034) .617 (.039) .601 (.035) .621 (.038) .584 (.035) .601 (.043)

.623 (.035) .627 (.035) .628 (.036) .625 (.041) .612 (.037) .598 (.038) .640 (.034) .641 (.033) .663 (.035) .629 (.044) .609 (.048) .630 (.044) .609 (.049) .599 (.047) .601 (.047) .592 (.051) .585 (.056) .587 (.041) .601 (.041) .582 (.042) .590 (.041) .585 (.042) .603 (.039) .557 (.042) .576 (.048)

.709 (.029) .707 (.027) .710 (.029) .707 (.027) .680 (.039) .696 (.031) .728 (.038) .720 (.026) .748 (.030) .758 (.025) .744 (.031) .758 (.024) .742 (.031) .742 (.028) .739 (.031) .731 (.027) .719 (.041) .703 (.027) .724 (.026) .700 (.029) .720 (.027) .700 (.027) .719 (.026) .689 (.028) .703 (.031)

.709 (.029) .707 (.027) .710 (.029) .707 (.027) .680 (.039) .696 (.031) .728 (.038) .720 (.026) .748 (.030) .714 (.028) .642 (.068) .716 (.029) .644 (.066) .687 (.033) .641 (.066) .675 (.050) .605 (.081) .654 (.046) .672 (.057) .656 (.042) .672 (.045) .660 (.044) .674 (.052) .646 (.042) .652 (.046)

.642 (.006) .661 (.006) .643 (.006) .662 (.006) .644 (.007) .662 (.007) .501 (.010) .667 (.006) .678 (.005) .678 (.005) .706 (.006) .694 (.007) .706 (.006) .691 (.006) .704 (.005) .680 (.009) .674 (.016) .666 (.006) .693 (.006) .666 (.006) .692 (.006) .682 (.006) .692 (.006) .675 (.008) .673 (.010)

.642 (.006) .661 (.006) .643 (.006) .662 (.006) .644 (.007) .662 (.007) .501 (.010) .667 (.006) .678 (.005) .639 (.006) .657 (.009) .648 (.008) .659 (.009) .648 (.009) .657 (.009) .622 (.021) .589 (.051) .630 (.008) .660 (.008) .630 (.013) .659 (.009) .647 (.009) .659 (.008) .638 (.012) .634 (.017)

64

Table 41: Our methods vs. the rest: mean classiﬁer AUC for = 0.5, balanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.782 (.026) .775 (.026) .783 (.026) .775 (.027) .765 (.033) .758 (.030) .744 (.028) .790 (.023) .819 (.021) .811 (.024) .788 (.023) .812 (.023) .788 (.023) .782 (.025) .787 (.024) .781 (.029) .765 (.039) .784 (.028) .778 (.030) .778 (.030) .776 (.029) .770 (.030) .783 (.029) .766 (.031) .762 (.029)

.782 (.026) .775 (.026) .783 (.026) .775 (.027) .765 (.033) .758 (.030) .744 (.028) .790 (.023) .819 (.021) .771 (.031) .713 (.047) .771 (.031) .713 (.047) .736 (.036) .712 (.046) .725 (.063) .685 (.069) .757 (.034) .744 (.039) .747 (.036) .741 (.037) .727 (.039) .748 (.035) .715 (.045) .715 (.045)

.601 (.037) .600 (.036) .603 (.037) .599 (.039) .589 (.035) .572 (.039) .614 (.035) .614 (.035) .639 (.031) .638 (.036) .622 (.037) .639 (.036) .619 (.040) .606 (.036) .609 (.038) .602 (.040) .591 (.048) .590 (.034) .605 (.040) .586 (.035) .592 (.041) .581 (.034) .601 (.039) .565 (.035) .581 (.038)

.601 (.037) .600 (.036) .603 (.037) .599 (.039) .589 (.035) .572 (.039) .614 (.035) .614 (.035) .639 (.031) .602 (.044) .584 (.047) .603 (.044) .581 (.051) .581 (.045) .577 (.047) .572 (.050) .561 (.053) .575 (.036) .581 (.040) .568 (.036) .563 (.042) .571 (.038) .582 (.038) .555 (.037) .564 (.043)

.684 (.030) .683 (.032) .685 (.029) .683 (.032) .656 (.031) .671 (.032) .720 (.036) .700 (.030) .724 (.031) .746 (.026) .727 (.032) .747 (.026) .725 (.031) .725 (.031) .725 (.031) .716 (.032) .704 (.039) .681 (.029) .702 (.031) .676 (.036) .698 (.032) .678 (.029) .703 (.031) .672 (.033) .688 (.031)

.684 (.030) .683 (.032) .685 (.029) .683 (.032) .656 (.031) .671 (.032) .720 (.036) .700 (.030) .724 (.031) .698 (.027) .618 (.059) .700 (.028) .619 (.058) .667 (.033) .618 (.058) .659 (.046) .590 (.067) .649 (.037) .669 (.034) .641 (.038) .663 (.034) .651 (.032) .670 (.033) .646 (.034) .654 (.039)

.629 (.006) .646 (.006) .629 (.005) .646 (.006) .631 (.006) .646 (.006) .501 (.007) .653 (.006) .672 (.005) .672 (.005) .696 (.005) .685 (.007) .696 (.005) .682 (.006) .694 (.005) .668 (.009) .662 (.015) .649 (.006) .673 (.006) .646 (.007) .671 (.006) .664 (.006) .672 (.006) .655 (.008) .653 (.009)

.629 (.006) .646 (.006) .629 (.005) .646 (.006) .631 (.006) .646 (.006) .501 (.007) .653 (.006) .672 (.005) .629 (.006) .642 (.009) .635 (.007) .643 (.008) .641 (.007) .642 (.008) .605 (.029) .581 (.043) .621 (.007) .645 (.007) .620 (.009) .641 (.010) .636 (.007) .644 (.007) .622 (.013) .620 (.014)

65

Table 42: Our methods vs. the rest: mean classiﬁer accuracy for = 0.0, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.848 (.019) .850 (.014) .851 (.016) .841 (.033) .851 (.015) .848 (.015) .735 (.027) .845 (.014) .855 (.014) .852 (.015) .855 (.014) .735 (.027) .847 (.015) .854 (.014) .852 (.014) .850 (.015) .848 (.014) .820 (.015) .848 (.015) .846 (.017) .842 (.020) .854 (.014) .852 (.014) .850 (.015) .849 (.015) .820 (.015) .848 (.014) .844 (.018) .841 (.028)

.848 (.019) .850 (.014) .851 (.016) .841 (.033) .851 (.015) .848 (.015) .735 (.027) .845 (.014) .855 (.014) .852 (.015) .855 (.014) .735 (.027) .847 (.015) .827 (.093) .817 (.092) .823 (.092) .786 (.091) .666 (.097) .803 (.090) .745 (.112) .759 (.119) .854 (.014) .852 (.014) .850 (.015) .848 (.014) .820 (.016) .848 (.015) .846 (.018) .841 (.028)

.725 (.016) .716 (.018) .719 (.017) .709 (.016) .727 (.016) .720 (.016) .472 (.135) .695 (.025) .715 (.020) .720 (.018) .731 (.016) .464 (.129) .689 (.029) .733 (.015) .720 (.018) .733 (.015) .721 (.017) .700 (.018) .708 (.022) .719 (.022) .708 (.022) .733 (.015) .720 (.018) .733 (.016) .720 (.018) .701 (.017) .709 (.021) .719 (.022) .706 (.023)

.725 (.016) .716 (.018) .719 (.017) .709 (.016) .727 (.016) .720 (.016) .472 (.135) .695 (.025) .715 (.020) .720 (.018) .731 (.016) .464 (.129) .689 (.029) .698 (.013) .698 (.013) .698 (.013) .698 (.013) .698 (.013) .698 (.013) .698 (.013) .698 (.013) .733 (.015) .720 (.018) .732 (.014) .722 (.018) .700 (.018) .710 (.021) .721 (.020) .706 (.023)

.932 (.003) .932 (.003) .932 (.003) .931 (.003) .931 (.003) .931 (.003) .085 (.004) .931 (.004) .931 (.003) .931 (.003) .931 (.003) .084 (.004) .929 (.005) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .930 (.005) .931 (.004) .931 (.003) .930 (.004) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .930 (.005) .931 (.004) .930 (.003) .930 (.004)

.932 (.003) .932 (.003) .932 (.003) .931 (.003) .931 (.003) .931 (.003) .085 (.004) .931 (.004) .931 (.003) .931 (.003) .931 (.003) .084 (.004) .929 (.005) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .930 (.005) .931 (.004) .931 (.003) .930 (.004)

.809 (.002) .820 (.003) .809 (.009) .820 (.003) .820 (.003) .820 (.003) .818 (.003) .820 (.003) .779 (.002) .820 (.003) .808 (.003) .819 (.003) .819 (.003) .809 (.002) .820 (.003) .820 (.003) .820 (.003) .819 (.003) .819 (.003) .812 (.007) .808 (.013) .809 (.002) .820 (.003) .820 (.003) .820 (.003) .819 (.003) .819 (.003) .812 (.006) .807 (.011)

.809 (.002) .820 (.003) .809 (.009) .820 (.003) .820 (.003) .820 (.003) .818 (.003) .820 (.003) .779 (.002) .820 (.003) .808 (.003) .819 (.003) .819 (.003) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .769 (.053) .761 (.097) .809 (.002) .820 (.003) .820 (.003) .820 (.003) .819 (.003) .819 (.003) .812 (.006) .804 (.016)

66

Table 43: Our methods vs. the rest: mean classiﬁer accuracy for = 0.1, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.810 (.014) .814 (.016) .814 (.015) .814 (.019) .794 (.021) .792 (.020) .770 (.030) .808 (.016) .812 (.014) .822 (.015) .811 (.016) .678 (.021) .811 (.021) .816 (.015) .824 (.015) .820 (.016) .824 (.015) .794 (.020) .821 (.018) .810 (.018) .818 (.018) .828 (.018) .820 (.015) .823 (.017) .818 (.017) .796 (.018) .816 (.022) .807 (.019) .802 (.022)

.810 (.014) .814 (.016) .814 (.015) .814 (.019) .794 (.021) .792 (.020) .770 (.030) .808 (.016) .812 (.014) .822 (.015) .811 (.016) .744 (.060) .697 (.082) .760 (.104) .740 (.103) .755 (.104) .732 (.103) .635 (.091) .726 (.106) .679 (.098) .694 (.120) .576 (.051) .570 (.046) .581 (.055) .570 (.037) .578 (.050) .573 (.042) .564 (.043) .564 (.040)

.699 (.018) .695 (.017) .702 (.016) .699 (.016) .688 (.021) .692 (.017) .512 (.107) .684 (.019) .701 (.016) .704 (.019) .720 (.017) .410 (.094) .664 (.031) .726 (.015) .716 (.019) .726 (.016) .717 (.017) .686 (.023) .707 (.019) .712 (.019) .704 (.024) .709 (.015) .703 (.017) .708 (.015) .703 (.019) .681 (.019) .695 (.018) .696 (.019) .696 (.019)

.699 (.018) .695 (.017) .702 (.016) .699 (.016) .688 (.021) .692 (.017) .512 (.107) .684 (.019) .701 (.016) .704 (.019) .720 (.017) .591 (.070) .689 (.017) .700 (.015) .700 (.015) .700 (.015) .700 (.015) .699 (.015) .700 (.015) .698 (.015) .697 (.020) .696 (.014) .696 (.015) .696 (.015) .696 (.014) .696 (.015) .696 (.015) .695 (.016) .695 (.015)

.930 (.004) .929 (.004) .930 (.004) .929 (.004) .930 (.004) .930 (.004) .081 (.004) .928 (.004) .931 (.004) .930 (.004) .931 (.004) .077 (.004) .922 (.006) .931 (.004) .930 (.004) .931 (.004) .931 (.004) .929 (.005) .929 (.004) .930 (.004) .927 (.007) .930 (.004) .929 (.003) .930 (.004) .930 (.004) .927 (.005) .927 (.005) .930 (.005) .929 (.004)

.930 (.004) .929 (.004) .930 (.004) .929 (.004) .930 (.004) .930 (.004) .081 (.004) .928 (.004) .931 (.004) .930 (.004) .931 (.004) .137 (.116) .929 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .930 (.004) .930 (.004) .930 (.004) .930 (.004) .930 (.004) .930 (.004) .930 (.004) .930 (.004)

.803 (.003) .804 (.003) .795 (.013) .804 (.003) .803 (.003) .802 (.003) .802 (.003) .804 (.002) .778 (.003) .804 (.003) .802 (.003) .810 (.002) .816 (.002) .804 (.002) .816 (.002) .816 (.002) .816 (.002) .813 (.003) .816 (.002) .810 (.006) .811 (.006) .804 (.003) .816 (.003) .810 (.003) .814 (.003) .813 (.003) .816 (.002) .809 (.005) .810 (.004)

.803 (.003) .804 (.003) .795 (.013) .804 (.003) .803 (.003) .802 (.003) .802 (.003) .804 (.002) .778 (.003) .804 (.003) .802 (.003) .779 (.003) .779 (.005) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003) .778 (.003)

67

Table 44: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.787 (.020) .788 (.020) .791 (.020) .788 (.020) .775 (.019) .768 (.021) .767 (.024) .788 (.020) .777 (.023) .791 (.020) .793 (.020) .649 (.028) .797 (.021) .796 (.020) .794 (.020) .798 (.021) .793 (.021) .765 (.017) .795 (.020) .786 (.017) .780 (.030) .810 (.021) .800 (.019) .805 (.021) .799 (.021) .778 (.021) .801 (.020) .782 (.019) .784 (.024)

.787 (.020) .788 (.020) .791 (.020) .788 (.020) .775 (.019) .768 (.021) .767 (.024) .788 (.020) .777 (.023) .791 (.020) .793 (.020) .711 (.073) .679 (.068) .734 (.100) .681 (.112) .722 (.096) .674 (.112) .601 (.070) .675 (.111) .653 (.093) .644 (.106) .580 (.035) .573 (.038) .578 (.031) .581 (.043) .575 (.030) .590 (.048) .577 (.039) .576 (.039)

.693 (.018) .681 (.017) .692 (.022) .691 (.017) .676 (.024) .692 (.022) .550 (.083) .676 (.020) .696 (.017) .693 (.016) .711 (.019) .420 (.094) .652 (.043) .720 (.019) .708 (.016) .719 (.018) .707 (.016) .686 (.019) .699 (.017) .704 (.021) .687 (.025) .699 (.019) .698 (.018) .693 (.020) .697 (.020) .672 (.024) .681 (.023) .692 (.020) .689 (.023)

.693 (.018) .681 (.017) .692 (.022) .691 (.017) .676 (.024) .692 (.022) .550 (.083) .676 (.020) .696 (.017) .693 (.016) .711 (.019) .669 (.033) .699 (.017) .701 (.016) .701 (.016) .701 (.016) .701 (.016) .700 (.016) .701 (.016) .698 (.018) .685 (.055) .698 (.017) .699 (.017) .697 (.017) .700 (.016) .697 (.017) .700 (.016) .697 (.017) .699 (.017)

.929 (.003) .928 (.003) .929 (.003) .929 (.003) .929 (.003) .929 (.003) .083 (.018) .929 (.004) .931 (.003) .931 (.003) .931 (.003) .075 (.003) .921 (.005) .931 (.003) .930 (.003) .931 (.003) .931 (.003) .928 (.004) .929 (.004) .930 (.003) .928 (.006) .929 (.003) .929 (.003) .929 (.003) .929 (.003) .924 (.004) .926 (.004) .928 (.004) .928 (.003)

.929 (.003) .928 (.003) .929 (.003) .929 (.003) .929 (.003) .929 (.003) .083 (.018) .929 (.004) .931 (.003) .931 (.003) .931 (.003) .573 (.085) .930 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .931 (.003) .929 (.003) .929 (.003) .929 (.003) .929 (.003) .928 (.003) .929 (.003) .929 (.003) .929 (.003)

.800 (.003) .800 (.003) .791 (.010) .800 (.003) .801 (.003) .801 (.003) .800 (.003) .799 (.003) .779 (.003) .800 (.003) .801 (.003) .810 (.003) .814 (.003) .802 (.003) .814 (.003) .813 (.002) .814 (.003) .812 (.003) .815 (.003) .807 (.005) .803 (.011) .803 (.004) .814 (.003) .806 (.003) .810 (.003) .811 (.003) .813 (.003) .806 (.005) .807 (.005)

.800 (.003) .800 (.003) .791 (.010) .800 (.003) .801 (.003) .801 (.003) .800 (.003) .799 (.003) .779 (.003) .800 (.003) .801 (.003) .781 (.004) .782 (.006) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .780 (.004) .779 (.003) .780 (.004) .768 (.042) .779 (.004) .781 (.006) .778 (.003) .778 (.003) .782 (.007) .785 (.010) .782 (.007) .781 (.006)

68

Table 45: Our methods vs. the rest: mean classiﬁer accuracy for = 0.3, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.774 (.023) .771 (.025) .779 (.024) .776 (.022) .755 (.029) .746 (.030) .741 (.027) .770 (.025) .762 (.035) .771 (.025) .777 (.024) .645 (.031) .774 (.021) .781 (.018) .771 (.017) .783 (.019) .769 (.019) .751 (.025) .770 (.017) .767 (.020) .758 (.030) .788 (.021) .774 (.024) .785 (.021) .777 (.023) .759 (.021) .777 (.025) .768 (.024) .763 (.027)

.774 (.023) .771 (.025) .779 (.024) .776 (.022) .755 (.029) .746 (.030) .741 (.027) .770 (.025) .762 (.035) .771 (.025) .777 (.024) .684 (.078) .657 (.073) .722 (.080) .688 (.100) .706 (.087) .676 (.103) .632 (.084) .679 (.104) .653 (.092) .677 (.099) .611 (.052) .597 (.051) .613 (.056) .614 (.055) .624 (.050) .628 (.063) .606 (.053) .608 (.056)

.679 (.019) .682 (.019) .683 (.018) .688 (.016) .671 (.021) .692 (.017) .590 (.073) .680 (.020) .697 (.013) .694 (.015) .707 (.016) .442 (.076) .653 (.032) .710 (.015) .702 (.017) .710 (.015) .704 (.016) .674 (.021) .691 (.017) .694 (.019) .681 (.041) .688 (.017) .694 (.018) .688 (.017) .694 (.018) .656 (.022) .682 (.021) .678 (.018) .687 (.023)

.679 (.019) .682 (.019) .683 (.018) .688 (.016) .671 (.021) .692 (.017) .590 (.073) .680 (.020) .697 (.013) .694 (.015) .707 (.016) .685 (.026) .695 (.016) .697 (.013) .697 (.013) .697 (.013) .697 (.013) .697 (.013) .697 (.013) .692 (.020) .684 (.046) .692 (.014) .695 (.013) .690 (.015) .695 (.013) .689 (.015) .695 (.013) .689 (.016) .694 (.014)

.924 (.004) .923 (.005) .924 (.004) .924 (.005) .924 (.004) .924 (.004) .084 (.017) .930 (.005) .931 (.004) .931 (.004) .931 (.004) .078 (.004) .922 (.005) .931 (.004) .930 (.004) .931 (.004) .930 (.003) .926 (.004) .929 (.004) .930 (.004) .928 (.007) .924 (.005) .923 (.004) .924 (.004) .924 (.004) .916 (.004) .920 (.004) .923 (.004) .922 (.004)

.924 (.004) .923 (.005) .924 (.004) .924 (.005) .924 (.004) .924 (.004) .084 (.017) .930 (.005) .931 (.004) .931 (.004) .931 (.004) .731 (.008) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .930 (.004) .931 (.004) .924 (.004) .924 (.004) .924 (.004) .924 (.004) .921 (.004) .924 (.004) .924 (.004) .924 (.004)

.796 (.002) .796 (.002) .791 (.009) .796 (.002) .796 (.002) .796 (.002) .791 (.007) .795 (.004) .779 (.002) .781 (.006) .798 (.003) .807 (.002) .811 (.002) .799 (.003) .809 (.003) .809 (.003) .811 (.002) .809 (.003) .811 (.002) .804 (.004) .804 (.005) .798 (.003) .807 (.002) .797 (.003) .802 (.003) .804 (.002) .807 (.002) .800 (.004) .796 (.017)

.796 (.002) .796 (.002) .791 (.009) .796 (.002) .796 (.002) .796 (.002) .791 (.007) .795 (.004) .779 (.002) .781 (.006) .798 (.003) .780 (.002) .780 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .779 (.002) .766 (.056) .776 (.002) .782 (.007) .775 (.002) .775 (.002) .781 (.005) .783 (.006) .778 (.005) .780 (.005)

69

Table 46: Our methods vs. the rest: mean classiﬁer accuracy for = 0.4, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.755 (.021) .753 (.023) .756 (.021) .753 (.023) .734 (.028) .723 (.033) .725 (.033) .751 (.021) .741 (.029) .746 (.021) .734 (.039) .628 (.025) .760 (.035) .759 (.020) .747 (.020) .759 (.021) .746 (.019) .730 (.024) .745 (.020) .746 (.026) .736 (.030) .769 (.024) .765 (.020) .763 (.023) .763 (.023) .743 (.021) .771 (.021) .750 (.027) .752 (.027)

.755 (.021) .753 (.023) .756 (.021) .753 (.023) .734 (.028) .723 (.033) .725 (.033) .751 (.021) .741 (.029) .746 (.021) .734 (.039) .628 (.077) .619 (.075) .660 (.099) .636 (.097) .642 (.095) .637 (.098) .612 (.075) .621 (.093) .641 (.091) .633 (.091) .642 (.055) .628 (.061) .625 (.055) .631 (.064) .642 (.047) .676 (.065) .646 (.057) .654 (.064)

.669 (.018) .685 (.022) .675 (.018) .685 (.022) .670 (.022) .689 (.024) .604 (.051) .680 (.024) .698 (.015) .696 (.016) .705 (.017) .479 (.063) .645 (.041) .706 (.019) .701 (.017) .704 (.019) .701 (.016) .670 (.022) .691 (.020) .688 (.020) .682 (.039) .671 (.017) .687 (.020) .669 (.018) .689 (.021) .648 (.018) .676 (.024) .672 (.018) .681 (.022)

.669 (.018) .685 (.022) .675 (.018) .685 (.022) .670 (.022) .689 (.024) .604 (.051) .680 (.024) .698 (.015) .696 (.016) .705 (.017) .689 (.020) .696 (.017) .698 (.015) .698 (.015) .698 (.015) .698 (.015) .698 (.015) .698 (.015) .693 (.020) .685 (.054) .684 (.015) .692 (.019) .683 (.016) .691 (.018) .678 (.017) .690 (.019) .682 (.018) .692 (.018)

.910 (.004) .908 (.004) .910 (.004) .909 (.004) .909 (.004) .909 (.004) .081 (.010) .931 (.004) .932 (.004) .931 (.004) .932 (.004) .092 (.004) .923 (.006) .932 (.004) .930 (.004) .931 (.004) .931 (.004) .925 (.005) .929 (.004) .931 (.004) .927 (.007) .909 (.005) .908 (.005) .909 (.004) .909 (.004) .895 (.007) .906 (.006) .908 (.005) .907 (.005)

.910 (.004) .908 (.004) .910 (.004) .909 (.004) .909 (.004) .909 (.004) .081 (.010) .931 (.004) .932 (.004) .931 (.004) .932 (.004) .822 (.007) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .931 (.004) .910 (.004) .909 (.004) .909 (.004) .909 (.004) .902 (.005) .909 (.004) .909 (.004) .909 (.004)

.786 (.002) .786 (.002) .779 (.010) .786 (.002) .786 (.003) .786 (.002) .777 (.022) .782 (.014) .778 (.002) .778 (.002) .795 (.003) .803 (.003) .808 (.003) .796 (.003) .805 (.003) .805 (.003) .808 (.003) .805 (.003) .808 (.003) .803 (.004) .797 (.011) .785 (.003) .794 (.003) .782 (.003) .789 (.003) .791 (.002) .794 (.003) .788 (.004) .786 (.008)

.786 (.002) .786 (.002) .779 (.010) .786 (.002) .786 (.003) .786 (.002) .777 (.022) .782 (.014) .778 (.002) .778 (.002) .795 (.003) .779 (.002) .779 (.002) .778 (.002) .778 (.002) .778 (.002) .778 (.002) .779 (.002) .778 (.002) .778 (.002) .770 (.034) .766 (.003) .777 (.007) .764 (.002) .766 (.003) .775 (.006) .777 (.007) .770 (.005) .770 (.007)

70

Table 47: Our methods vs. the rest: mean classiﬁer accuracy for = 0.5, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. MAJ MAJ w/ disc. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.735 (.025) .725 (.026) .738 (.023) .724 (.024) .720 (.033) .714 (.026) .721 (.029) .725 (.026) .712 (.031) .715 (.027) .704 (.038) .648 (.032) .746 (.031) .733 (.021) .721 (.032) .733 (.019) .725 (.026) .714 (.023) .724 (.023) .721 (.031) .706 (.058) .750 (.023) .743 (.026) .739 (.026) .738 (.029) .730 (.028) .749 (.024) .731 (.029) .731 (.025)

.735 (.025) .725 (.026) .738 (.023) .724 (.024) .720 (.033) .714 (.026) .721 (.029) .725 (.026) .712 (.031) .715 (.027) .704 (.038) .623 (.083) .614 (.075) .618 (.085) .569 (.059) .604 (.080) .569 (.058) .571 (.056) .568 (.058) .599 (.073) .578 (.071) .659 (.042) .658 (.053) .639 (.047) .634 (.049) .658 (.043) .673 (.051) .661 (.037) .670 (.042)

.660 (.022) .683 (.029) .662 (.020) .683 (.028) .667 (.019) .690 (.019) .622 (.064) .696 (.017) .700 (.012) .699 (.012) .702 (.011) .518 (.054) .641 (.040) .700 (.017) .696 (.016) .700 (.015) .698 (.013) .675 (.023) .691 (.017) .684 (.023) .679 (.045) .655 (.019) .682 (.022) .658 (.018) .685 (.021) .634 (.020) .669 (.027) .657 (.018) .674 (.027)

.660 (.022) .683 (.029) .662 (.020) .683 (.028) .667 (.019) .690 (.019) .622 (.064) .696 (.017) .700 (.012) .699 (.012) .702 (.011) .698 (.013) .700 (.012) .700 (.012) .700 (.012) .700 (.012) .700 (.012) .700 (.012) .700 (.012) .682 (.046) .689 (.045) .672 (.016) .690 (.019) .673 (.017) .691 (.019) .667 (.016) .689 (.020) .670 (.016) .684 (.026)

.877 (.005) .878 (.011) .877 (.005) .878 (.011) .877 (.005) .879 (.011) .087 (.016) .930 (.003) .931 (.003) .930 (.003) .931 (.003) .126 (.005) .924 (.004) .931 (.003) .929 (.003) .930 (.003) .930 (.003) .920 (.004) .929 (.004) .929 (.003) .928 (.003) .877 (.005) .878 (.011) .877 (.005) .879 (.011) .862 (.005) .875 (.011) .876 (.005) .876 (.011)

.877 (.005) .878 (.011) .877 (.005) .878 (.011) .877 (.005) .879 (.011) .087 (.016) .930 (.003) .931 (.003) .930 (.003) .931 (.003) .879 (.005) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .930 (.003) .877 (.005) .879 (.011) .877 (.005) .879 (.011) .866 (.005) .878 (.011) .876 (.005) .878 (.011)

.764 (.004) .764 (.004) .759 (.009) .764 (.004) .764 (.004) .764 (.003) .754 (.020) .773 (.024) .779 (.003) .779 (.003) .793 (.003) .800 (.003) .804 (.002) .793 (.003) .803 (.003) .802 (.003) .804 (.003) .802 (.003) .804 (.002) .796 (.012) .790 (.031) .760 (.004) .769 (.003) .756 (.004) .764 (.004) .766 (.003) .769 (.003) .764 (.005) .763 (.007)

.764 (.004) .764 (.004) .759 (.009) .764 (.004) .764 (.004) .764 (.003) .754 (.020) .773 (.024) .779 (.003) .779 (.003) .793 (.003) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .779 (.003) .776 (.010) .771 (.027) .746 (.004) .757 (.005) .745 (.004) .747 (.004) .755 (.005) .758 (.006) .751 (.006) .752 (.006)

71

Table 48: Our methods vs. the rest: mean classiﬁer AUC for = 0.0, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.906 (.017) .910 (.014) .910 (.014) .905 (.038) .907 (.015) .909 (.014) .900 (.014) .911 (.014) .904 (.016) .907 (.015) .911 (.014) .907 (.015) .911 (.014) .888 (.014) .912 (.015) .902 (.018) .890 (.028) .907 (.015) .911 (.014) .907 (.015) .911 (.014) .887 (.014) .912 (.014) .902 (.016) .892 (.033)

.906 (.017) .910 (.014) .910 (.014) .905 (.038) .907 (.015) .909 (.014) .900 (.014) .911 (.014) .904 (.016) .897 (.034) .875 (.062) .897 (.034) .875 (.062) .848 (.102) .847 (.164) .842 (.106) .837 (.113) .907 (.015) .911 (.014) .907 (.014) .910 (.014) .888 (.014) .912 (.015) .903 (.016) .893 (.034)

.710 (.031) .725 (.021) .684 (.044) .638 (.057) .728 (.037) .733 (.019) .730 (.020) .735 (.018) .751 (.018) .749 (.019) .734 (.018) .748 (.019) .734 (.017) .705 (.021) .709 (.021) .712 (.025) .698 (.030) .749 (.019) .734 (.018) .748 (.019) .734 (.018) .705 (.020) .708 (.021) .712 (.024) .697 (.030)

.710 (.031) .725 (.021) .684 (.044) .638 (.057) .728 (.037) .733 (.019) .730 (.020) .735 (.018) .751 (.018) .640 (.030) .606 (.033) .640 (.028) .608 (.034) .561 (.039) .569 (.045) .507 (.020) .509 (.042) .749 (.019) .734 (.018) .748 (.019) .734 (.018) .704 (.020) .708 (.021) .713 (.023) .695 (.034)

.748 (.061) .788 (.052) .745 (.064) .727 (.047) .785 (.040) .800 (.029) .725 (.048) .803 (.029) .727 (.048) .777 (.027) .803 (.029) .798 (.020) .803 (.029) .794 (.033) .789 (.030) .739 (.031) .731 (.052) .777 (.027) .803 (.029) .797 (.020) .803 (.028) .794 (.033) .790 (.030) .738 (.030) .726 (.049)

.748 (.061) .788 (.052) .745 (.064) .727 (.047) .785 (.040) .800 (.029) .725 (.048) .803 (.029) .727 (.048) .605 (.095) .482 (.124) .723 (.032) .626 (.064) .666 (.063) .447 (.117) .502 (.004) .490 (.044) .777 (.027) .803 (.029) .798 (.019) .803 (.029) .794 (.034) .789 (.030) .742 (.031) .733 (.048)

.705 (.005) .730 (.005) .672 (.032) .718 (.004) .717 (.012) .729 (.009) .500 (.000) .731 (.004) .705 (.005) .705 (.005) .731 (.004) .729 (.012) .732 (.004) .737 (.004) .732 (.004) .708 (.019) .714 (.017) .705 (.005) .731 (.004) .723 (.013) .732 (.004) .737 (.004) .732 (.004) .710 (.015) .714 (.018)

.705 (.005) .730 (.005) .672 (.032) .718 (.004) .717 (.012) .729 (.009) .500 (.000) .731 (.004) .705 (.005) .560 (.012) .500 (.000) .646 (.016) .588 (.003) .535 (.046) .528 (.074) .471 (.042) .473 (.063) .705 (.005) .731 (.004) .725 (.014) .732 (.004) .737 (.004) .732 (.004) .710 (.013) .710 (.022)

72

Table 49: Our methods vs. the rest: mean classiﬁer AUC for = 0.1, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.888 (.018) .888 (.015) .888 (.017) .890 (.013) .884 (.014) .874 (.015) .875 (.013) .893 (.011) .893 (.013) .891 (.014) .893 (.013) .891 (.014) .891 (.013) .863 (.017) .893 (.017) .874 (.019) .874 (.023) .882 (.014) .885 (.014) .881 (.015) .881 (.014) .861 (.015) .879 (.016) .863 (.018) .862 (.021)

.888 (.018) .888 (.015) .888 (.017) .890 (.013) .884 (.014) .874 (.015) .875 (.013) .893 (.011) .893 (.013) .869 (.049) .825 (.090) .868 (.049) .826 (.084) .827 (.071) .817 (.104) .798 (.123) .763 (.143) .798 (.060) .781 (.065) .792 (.097) .806 (.053) .791 (.055) .803 (.046) .669 (.121) .654 (.119)

.656 (.043) .669 (.046) .660 (.050) .645 (.067) .663 (.036) .651 (.045) .705 (.021) .707 (.020) .728 (.020) .728 (.022) .710 (.024) .728 (.024) .711 (.025) .674 (.031) .689 (.023) .687 (.027) .673 (.029) .698 (.021) .699 (.020) .694 (.024) .691 (.023) .670 (.025) .677 (.026) .667 (.027) .665 (.031)

.656 (.043) .669 (.046) .660 (.050) .645 (.067) .663 (.036) .651 (.045) .705 (.021) .707 (.020) .728 (.020) .628 (.030) .587 (.037) .630 (.029) .589 (.036) .568 (.040) .562 (.043) .522 (.036) .523 (.037) .590 (.059) .628 (.049) .594 (.066) .603 (.066) .602 (.052) .616 (.055) .536 (.045) .549 (.057)

.652 (.081) .696 (.069) .659 (.072) .664 (.102) .644 (.092) .707 (.048) .709 (.042) .770 (.026) .727 (.043) .764 (.029) .788 (.028) .787 (.022) .790 (.030) .777 (.034) .773 (.028) .728 (.027) .722 (.038) .745 (.028) .780 (.027) .762 (.021) .764 (.031) .758 (.033) .739 (.031) .712 (.032) .701 (.046)

.652 (.081) .696 (.069) .659 (.072) .664 (.102) .644 (.092) .707 (.048) .709 (.042) .770 (.026) .727 (.043) .652 (.082) .594 (.083) .713 (.018) .633 (.059) .612 (.051) .512 (.111) .502 (.004) .497 (.036) .563 (.106) .676 (.063) .636 (.114) .700 (.055) .531 (.035) .576 (.069) .503 (.004) .501 (.004)

.661 (.009) .670 (.034) .632 (.069) .676 (.025) .675 (.010) .691 (.014) .500 (.000) .711 (.005) .691 (.004) .693 (.005) .731 (.002) .720 (.006) .732 (.002) .717 (.003) .732 (.003) .704 (.010) .702 (.016) .698 (.003) .724 (.004) .711 (.007) .725 (.004) .717 (.003) .725 (.004) .704 (.013) .702 (.013)

.661 (.009) .670 (.034) .632 (.069) .676 (.025) .675 (.010) .691 (.014) .500 (.000) .711 (.005) .691 (.004) .563 (.003) .661 (.026) .600 (.008) .676 (.018) .608 (.044) .673 (.019) .553 (.054) .513 (.064) .487 (.121) .552 (.125) .559 (.109) .642 (.076) .539 (.067) .474 (.127) .566 (.064) .536 (.039)

73

Table 50: Our methods vs. the rest: mean classiﬁer AUC for = 0.2, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.870 (.015) .869 (.016) .869 (.018) .867 (.019) .865 (.021) .850 (.023) .847 (.018) .872 (.016) .878 (.017) .874 (.017) .870 (.018) .875 (.017) .868 (.018) .842 (.018) .867 (.018) .858 (.019) .843 (.034) .869 (.020) .868 (.019) .863 (.018) .861 (.019) .848 (.019) .863 (.018) .837 (.022) .844 (.020)

.870 (.015) .869 (.016) .869 (.018) .867 (.019) .865 (.021) .850 (.023) .847 (.018) .872 (.016) .878 (.017) .842 (.054) .780 (.105) .843 (.052) .783 (.097) .764 (.102) .769 (.133) .773 (.123) .721 (.135) .776 (.065) .740 (.086) .781 (.064) .796 (.056) .767 (.064) .764 (.055) .651 (.107) .640 (.102)

.643 (.036) .664 (.027) .641 (.045) .619 (.061) .634 (.044) .616 (.045) .683 (.021) .687 (.020) .710 (.022) .706 (.022) .685 (.020) .706 (.022) .687 (.019) .658 (.023) .665 (.021) .666 (.024) .641 (.036) .670 (.025) .680 (.025) .662 (.029) .666 (.028) .648 (.027) .661 (.031) .645 (.031) .650 (.031)

.643 (.036) .664 (.027) .641 (.045) .619 (.061) .634 (.044) .616 (.045) .683 (.021) .687 (.020) .710 (.022) .615 (.031) .576 (.033) .616 (.031) .577 (.035) .566 (.037) .560 (.031) .525 (.039) .525 (.045) .584 (.049) .620 (.046) .587 (.055) .593 (.054) .588 (.040) .621 (.049) .521 (.032) .535 (.049)

.621 (.090) .701 (.047) .603 (.098) .699 (.049) .654 (.094) .691 (.034) .733 (.041) .761 (.020) .741 (.036) .768 (.022) .781 (.018) .786 (.016) .785 (.018) .774 (.016) .768 (.020) .722 (.028) .707 (.051) .742 (.020) .771 (.021) .755 (.016) .757 (.023) .747 (.017) .727 (.026) .709 (.023) .712 (.029)

.621 (.090) .701 (.047) .603 (.098) .699 (.049) .654 (.094) .691 (.034) .733 (.041) .761 (.020) .741 (.036) .655 (.074) .625 (.070) .708 (.018) .644 (.058) .576 (.046) .576 (.107) .504 (.004) .499 (.010) .591 (.078) .666 (.062) .655 (.085) .692 (.032) .515 (.020) .593 (.056) .509 (.013) .505 (.012)

.640 (.011) .676 (.015) .633 (.018) .676 (.015) .649 (.010) .680 (.011) .500 (.000) .696 (.006) .682 (.004) .683 (.004) .726 (.005) .707 (.008) .728 (.005) .712 (.005) .727 (.005) .698 (.011) .688 (.016) .690 (.004) .719 (.004) .700 (.005) .721 (.004) .711 (.006) .720 (.004) .696 (.011) .694 (.012)

.640 (.011) .676 (.015) .633 (.018) .676 (.015) .649 (.010) .680 (.011) .500 (.000) .696 (.006) .682 (.004) .558 (.003) .666 (.018) .572 (.008) .672 (.014) .641 (.021) .661 (.015) .589 (.048) .524 (.042) .625 (.046) .672 (.027) .649 (.020) .685 (.013) .644 (.016) .657 (.051) .619 (.026) .621 (.021)

74

Table 51: Our methods vs. the rest: mean classiﬁer AUC for = 0.3, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.856 (.019) .850 (.019) .856 (.020) .850 (.019) .844 (.023) .831 (.028) .822 (.022) .853 (.017) .866 (.015) .861 (.017) .854 (.016) .862 (.017) .853 (.015) .826 (.020) .850 (.021) .835 (.026) .831 (.022) .854 (.020) .848 (.019) .847 (.021) .841 (.022) .824 (.024) .841 (.025) .818 (.027) .818 (.026)

.856 (.019) .850 (.019) .856 (.020) .850 (.019) .844 (.023) .831 (.028) .822 (.022) .853 (.017) .866 (.015) .823 (.050) .770 (.077) .825 (.045) .771 (.073) .765 (.086) .769 (.075) .758 (.091) .749 (.070) .801 (.042) .774 (.050) .792 (.053) .796 (.036) .760 (.051) .780 (.043) .719 (.061) .705 (.074)

.621 (.042) .643 (.036) .614 (.050) .612 (.058) .604 (.040) .594 (.048) .660 (.028) .662 (.026) .687 (.023) .680 (.023) .659 (.026) .681 (.024) .661 (.027) .630 (.026) .646 (.026) .642 (.027) .616 (.047) .647 (.021) .664 (.027) .644 (.025) .636 (.040) .623 (.027) .649 (.031) .625 (.025) .639 (.030)

.621 (.042) .643 (.036) .614 (.050) .612 (.058) .604 (.040) .594 (.048) .660 (.028) .662 (.026) .687 (.023) .595 (.023) .562 (.024) .596 (.024) .560 (.027) .554 (.031) .549 (.028) .524 (.029) .518 (.035) .584 (.045) .613 (.042) .594 (.046) .586 (.039) .581 (.049) .607 (.044) .540 (.034) .551 (.046)

.630 (.070) .652 (.077) .638 (.065) .654 (.081) .681 (.056) .657 (.039) .724 (.048) .732 (.024) .733 (.041) .755 (.027) .758 (.025) .772 (.022) .760 (.025) .749 (.027) .750 (.025) .711 (.025) .693 (.052) .724 (.028) .744 (.029) .737 (.026) .721 (.029) .722 (.026) .705 (.030) .698 (.033) .687 (.028)

.630 (.070) .652 (.077) .638 (.065) .654 (.081) .681 (.056) .657 (.039) .724 (.048) .732 (.024) .733 (.041) .665 (.040) .590 (.073) .695 (.020) .607 (.059) .614 (.048) .578 (.079) .509 (.019) .503 (.011) .626 (.083) .625 (.095) .678 (.072) .654 (.049) .551 (.033) .550 (.088) .543 (.030) .554 (.042)

.639 (.007) .668 (.010) .633 (.016) .668 (.010) .642 (.008) .666 (.012) .500 (.000) .679 (.005) .673 (.006) .673 (.005) .716 (.004) .694 (.006) .718 (.004) .703 (.005) .717 (.004) .687 (.007) .680 (.015) .680 (.004) .708 (.004) .684 (.005) .709 (.004) .700 (.005) .709 (.004) .681 (.010) .678 (.016)

.639 (.007) .668 (.010) .633 (.016) .668 (.010) .642 (.008) .666 (.012) .500 (.000) .679 (.005) .673 (.006) .559 (.015) .659 (.014) .563 (.019) .662 (.012) .651 (.012) .657 (.011) .601 (.046) .550 (.052) .638 (.008) .671 (.010) .648 (.007) .674 (.007) .649 (.013) .670 (.010) .632 (.022) .632 (.019)

75

Table 52: Our methods vs. the rest: mean classiﬁer AUC for = 0.4, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.826 (.017) .820 (.017) .826 (.017) .820 (.017) .806 (.023) .800 (.025) .788 (.019) .829 (.017) .846 (.017) .840 (.020) .825 (.020) .839 (.021) .823 (.021) .801 (.024) .825 (.021) .812 (.026) .791 (.039) .825 (.019) .822 (.020) .820 (.019) .818 (.021) .802 (.021) .820 (.022) .796 (.024) .800 (.021)

.826 (.017) .820 (.017) .826 (.017) .820 (.017) .806 (.023) .800 (.025) .788 (.019) .829 (.017) .846 (.017) .788 (.054) .724 (.082) .790 (.054) .724 (.080) .734 (.083) .724 (.080) .727 (.106) .645 (.122) .797 (.033) .777 (.037) .790 (.036) .782 (.035) .751 (.049) .780 (.036) .720 (.046) .731 (.043)

.603 (.040) .617 (.033) .600 (.040) .603 (.049) .581 (.043) .577 (.041) .641 (.027) .645 (.030) .673 (.024) .671 (.021) .649 (.024) .672 (.021) .651 (.024) .620 (.033) .635 (.025) .626 (.028) .602 (.040) .621 (.024) .637 (.030) .618 (.024) .608 (.039) .607 (.028) .628 (.029) .597 (.027) .613 (.031)

.603 (.040) .617 (.033) .600 (.040) .603 (.049) .581 (.043) .577 (.041) .641 (.027) .645 (.030) .673 (.024) .592 (.028) .556 (.030) .592 (.028) .555 (.030) .553 (.034) .547 (.029) .522 (.033) .507 (.032) .584 (.033) .601 (.037) .581 (.030) .569 (.041) .584 (.037) .598 (.033) .562 (.039) .539 (.040)

.665 (.043) .687 (.035) .664 (.041) .686 (.035) .683 (.030) .655 (.039) .729 (.033) .722 (.018) .741 (.030) .756 (.025) .757 (.022) .766 (.023) .759 (.022) .733 (.021) .746 (.025) .710 (.024) .686 (.036) .717 (.024) .734 (.022) .724 (.023) .709 (.026) .699 (.025) .700 (.025) .687 (.026) .680 (.029)

.665 (.043) .687 (.035) .664 (.041) .686 (.035) .683 (.030) .655 (.039) .729 (.033) .722 (.018) .741 (.030) .673 (.035) .621 (.039) .684 (.020) .627 (.039) .631 (.035) .611 (.044) .511 (.017) .498 (.027) .663 (.044) .681 (.036) .681 (.030) .652 (.038) .585 (.040) .605 (.068) .588 (.050) .585 (.053)

.634 (.007) .662 (.007) .624 (.021) .662 (.007) .638 (.006) .661 (.008) .500 (.000) .668 (.004) .665 (.005) .665 (.005) .709 (.005) .679 (.007) .709 (.005) .696 (.005) .709 (.005) .679 (.007) .673 (.017) .670 (.004) .696 (.005) .670 (.005) .697 (.005) .687 (.005) .697 (.005) .673 (.007) .668 (.010)

.634 (.007) .662 (.007) .624 (.021) .662 (.007) .638 (.006) .661 (.008) .500 (.000) .668 (.004) .665 (.005) .558 (.014) .652 (.013) .555 (.017) .654 (.013) .651 (.007) .651 (.012) .605 (.042) .545 (.048) .634 (.007) .663 (.007) .638 (.008) .664 (.006) .648 (.007) .662 (.007) .629 (.016) .625 (.030)

76

Table 53: Our methods vs. the rest: mean classiﬁer AUC for = 0.5, unbalanced datasets, each classiﬁer selects 4 best features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HC(LR) HC(LR) w/ disc. HCAPP(LR) HCAPP(LR) w/ disc. HC(ANN) HC(ANN) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc. IMP(ANN) IMP(ANN) w/ disc. IMP(RF) IMP(RF) w/ disc. IMP(KNN) IMP(KNN) w/ disc. R-F(LR) R-F(LR) w/ disc. R-F(ANN) R-F(ANN) w/ disc. R-F(RF) R-F(RF) w/ disc. R-F(KNN) R-F(KNN) w/ disc.

.788 (.024) .779 (.023) .787 (.025) .779 (.023) .777 (.027) .770 (.025) .751 (.022) .796 (.022) .819 (.021) .809 (.023) .794 (.021) .809 (.022) .792 (.022) .781 (.028) .790 (.023) .782 (.031) .757 (.036) .790 (.024) .785 (.025) .784 (.026) .783 (.025) .774 (.024) .787 (.024) .769 (.028) .767 (.023)

.788 (.024) .779 (.023) .787 (.025) .779 (.023) .777 (.027) .770 (.025) .751 (.022) .796 (.022) .819 (.021) .729 (.063) .658 (.082) .732 (.060) .659 (.080) .683 (.078) .659 (.080) .681 (.078) .600 (.105) .765 (.038) .746 (.045) .755 (.041) .751 (.036) .732 (.034) .757 (.034) .728 (.033) .727 (.038)

.596 (.041) .598 (.035) .599 (.038) .593 (.040) .575 (.037) .556 (.030) .618 (.025) .620 (.028) .643 (.026) .641 (.027) .622 (.027) .641 (.026) .621 (.027) .602 (.031) .606 (.026) .598 (.040) .578 (.037) .599 (.029) .615 (.031) .595 (.030) .584 (.033) .591 (.030) .607 (.031) .574 (.032) .594 (.032)

.596 (.041) .598 (.035) .599 (.038) .593 (.040) .575 (.037) .556 (.030) .618 (.025) .620 (.028) .643 (.026) .571 (.026) .548 (.029) .572 (.027) .544 (.032) .544 (.030) .536 (.027) .512 (.032) .506 (.023) .576 (.036) .586 (.034) .567 (.032) .557 (.034) .573 (.031) .580 (.030) .557 (.034) .543 (.037)

.657 (.039) .674 (.028) .656 (.042) .675 (.027) .659 (.033) .631 (.040) .722 (.029) .697 (.023) .731 (.028) .742 (.018) .731 (.022) .754 (.016) .733 (.023) .715 (.021) .726 (.023) .689 (.028) .675 (.032) .696 (.022) .710 (.022) .692 (.026) .678 (.026) .673 (.019) .685 (.026) .669 (.024) .659 (.023)

.657 (.039) .674 (.028) .656 (.042) .675 (.027) .659 (.033) .631 (.040) .722 (.029) .697 (.023) .731 (.028) .665 (.029) .594 (.049) .676 (.020) .596 (.048) .621 (.028) .591 (.048) .506 (.014) .497 (.030) .655 (.039) .672 (.028) .662 (.030) .636 (.039) .581 (.028) .636 (.036) .592 (.035) .586 (.039)

.622 (.006) .643 (.006) .617 (.013) .643 (.006) .624 (.004) .643 (.006) .500 (.000) .651 (.005) .652 (.005) .652 (.005) .696 (.006) .661 (.006) .696 (.006) .684 (.006) .695 (.006) .663 (.011) .656 (.016) .651 (.006) .673 (.006) .650 (.006) .674 (.006) .665 (.007) .673 (.006) .652 (.008) .649 (.007)

.622 (.006) .643 (.006) .617 (.013) .643 (.006) .624 (.004) .643 (.006) .500 (.000) .651 (.005) .652 (.005) .554 (.012) .632 (.014) .552 (.015) .633 (.013) .640 (.006) .631 (.013) .574 (.049) .553 (.047) .622 (.005) .645 (.006) .623 (.005) .646 (.006) .634 (.006) .645 (.005) .615 (.012) .615 (.015)

Table 54: Our methods vs. the rest: mean classiﬁer accuracy for = 0.0, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.848 (.016) .851 (.038) .498 (.019) .562 (.030) .862 (.015) .857 (.016) .860 (.015) .851 (.016) .856 (.014)

.848 (.016) .851 (.038) .498 (.019) .562 (.030) .862 (.015) .857 (.016) .860 (.015) .768 (.071) .829 (.034)

.655 (.040) .648 (.063) .498 (.020) .504 (.020) .691 (.024) .690 (.023) .693 (.021) .687 (.023) .688 (.021)

.655 (.040) .648 (.063) .498 (.020) .504 (.020) .691 (.024) .690 (.023) .693 (.021) .524 (.043) .556 (.067)

.662 (.068) .741 (.045) .500 (.017) .611 (.032) .628 (.048) .817 (.017) .565 (.063) .746 (.022) .814 (.018)

.662 (.068) .741 (.045) .500 (.017) .611 (.032) .628 (.048) .817 (.017) .565 (.063) .508 (.032) .505 (.026)

.680 (.013) .651 (.035) .499 (.005) .608 (.011) .587 (.024) .705 (.005) .669 (.008) .669 (.006) .705 (.005)

.680 (.013) .651 (.035) .499 (.005) .608 (.011) .587 (.024) .705 (.005) .669 (.008) .501 (.005) .501 (.005)

77

Table 55: Our methods vs. the rest: mean classiﬁer accuracy for = 0.1, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.808 (.022) .812 (.040) .497 (.020) .542 (.026) .833 (.018) .831 (.021) .839 (.020) .822 (.021) .832 (.021)

.808 (.022) .812 (.040) .497 (.020) .542 (.026) .833 (.018) .831 (.021) .839 (.020) .738 (.051) .774 (.047)

.640 (.029) .634 (.052) .498 (.019) .502 (.019) .666 (.029) .666 (.025) .670 (.023) .672 (.028) .668 (.024)

.640 (.029) .634 (.052) .498 (.019) .502 (.019) .666 (.029) .666 (.025) .670 (.023) .522 (.037) .555 (.061)

.658 (.042) .734 (.024) .501 (.018) .503 (.018) .609 (.064) .788 (.016) .550 (.055) .739 (.024) .795 (.018)

.658 (.042) .734 (.024) .501 (.018) .503 (.018) .609 (.064) .788 (.016) .550 (.055) .507 (.033) .501 (.019)

.658 (.007) .669 (.006) .500 (.004) .560 (.008) .592 (.008) .684 (.006) .666 (.007) .665 (.005) .701 (.004)

.658 (.007) .669 (.006) .500 (.004) .560 (.008) .592 (.008) .684 (.006) .666 (.007) .500 (.004) .500 (.004)

Table 56: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.778 (.026) .785 (.042) .498 (.022) .535 (.030) .801 (.021) .806 (.020) .806 (.023) .803 (.020) .811 (.019)

.778 (.026) .785 (.042) .498 (.022) .535 (.030) .801 (.021) .806 (.020) .806 (.023) .700 (.062) .722 (.075)

.616 (.038) .620 (.052) .501 (.022) .504 (.023) .653 (.027) .651 (.031) .661 (.028) .662 (.030) .657 (.027)

.616 (.038) .620 (.052) .501 (.022) .504 (.023) .653 (.027) .651 (.031) .661 (.028) .520 (.043) .549 (.065)

.656 (.030) .727 (.021) .498 (.018) .498 (.017) .603 (.064) .773 (.018) .538 (.068) .729 (.020) .782 (.018)

.656 (.030) .727 (.021) .498 (.018) .498 (.017) .603 (.064) .773 (.018) .538 (.068) .508 (.025) .503 (.018)

.639 (.008) .648 (.032) .500 (.004) .550 (.007) .585 (.009) .669 (.004) .663 (.006) .659 (.005) .698 (.005)

.639 (.008) .648 (.032) .500 (.004) .550 (.007) .585 (.009) .669 (.004) .663 (.006) .500 (.004) .500 (.004)

Table 57: Our methods vs. the rest: mean classiﬁer accuracy for = 0.3, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.752 (.022) .757 (.037) .503 (.020) .546 (.030) .776 (.019) .784 (.021) .781 (.017) .779 (.019) .784 (.020)

.752 (.022) .757 (.037) .503 (.020) .546 (.030) .776 (.019) .784 (.021) .781 (.017) .688 (.051) .685 (.086)

.589 (.038) .607 (.043) .502 (.020) .508 (.022) .635 (.026) .634 (.026) .647 (.032) .647 (.030) .640 (.027)

.589 (.038) .607 (.043) .502 (.020) .508 (.022) .635 (.026) .634 (.026) .647 (.032) .517 (.038) .537 (.055)

.655 (.042) .715 (.039) .499 (.016) .499 (.016) .607 (.074) .757 (.021) .530 (.054) .725 (.021) .771 (.024)

.655 (.042) .715 (.039) .499 (.016) .499 (.016) .607 (.074) .757 (.021) .530 (.054) .505 (.020) .503 (.015)

.631 (.007) .627 (.043) .498 (.004) .543 (.007) .584 (.005) .655 (.006) .652 (.007) .649 (.006) .691 (.004)

.631 (.007) .627 (.043) .498 (.004) .543 (.007) .584 (.005) .655 (.006) .652 (.007) .502 (.004) .502 (.004)

78

Table 58: Our methods vs. the rest: mean classiﬁer accuracy for = 0.4, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.724 (.031) .732 (.037) .503 (.021) .550 (.035) .747 (.026) .760 (.025) .746 (.038) .765 (.023) .763 (.025)

.724 (.031) .732 (.037) .503 (.021) .550 (.035) .747 (.026) .760 (.025) .746 (.038) .670 (.050) .660 (.090)

.571 (.038) .582 (.042) .496 (.020) .502 (.021) .616 (.028) .613 (.030) .623 (.025) .628 (.026) .621 (.025)

.571 (.038) .582 (.042) .496 (.020) .502 (.021) .616 (.028) .613 (.030) .623 (.025) .531 (.042) .550 (.052)

.643 (.034) .708 (.038) .500 (.015) .500 (.015) .597 (.075) .747 (.026) .522 (.058) .719 (.023) .763 (.019)

.643 (.034) .708 (.038) .500 (.015) .500 (.015) .597 (.075) .747 (.026) .522 (.058) .507 (.022) .502 (.016)

.622 (.008) .614 (.044) .500 (.004) .544 (.007) .579 (.007) .640 (.008) .648 (.009) .646 (.006) .684 (.004)

.622 (.008) .614 (.044) .500 (.004) .544 (.007) .579 (.007) .640 (.008) .648 (.009) .502 (.012) .500 (.004)

Table 59: Our methods vs. the rest: mean classiﬁer accuracy for = 0.5, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.703 (.028) .704 (.042) .508 (.023) .564 (.038) .723 (.025) .735 (.024) .712 (.044) .737 (.021) .730 (.023)

.703 (.028) .704 (.042) .508 (.023) .564 (.038) .723 (.025) .735 (.024) .712 (.044) .653 (.057) .649 (.083)

.566 (.030) .572 (.044) .499 (.022) .509 (.027) .606 (.030) .606 (.028) .607 (.036) .615 (.027) .610 (.027)

.566 (.030) .572 (.044) .499 (.022) .509 (.027) .606 (.030) .606 (.028) .607 (.036) .523 (.039) .542 (.049)

.634 (.043) .694 (.036) .498 (.018) .498 (.018) .590 (.071) .738 (.019) .508 (.044) .712 (.022) .758 (.019)

.634 (.043) .694 (.036) .498 (.018) .498 (.018) .590 (.071) .738 (.019) .508 (.044) .511 (.025) .513 (.026)

.616 (.008) .621 (.007) .500 (.004) .546 (.007) .571 (.007) .627 (.010) .642 (.008) .639 (.007) .674 (.005)

.616 (.008) .621 (.007) .500 (.004) .546 (.007) .571 (.007) .627 (.010) .642 (.008) .502 (.009) .500 (.004)

Table 60: Our methods vs. the rest: mean classiﬁer AUC for = 0.0, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.901 (.013) .907 (.045) .911 (.013) .921 (.011) .913 (.012) .910 (.012) .922 (.011)

.901 (.013) .907 (.045) .911 (.013) .921 (.011) .913 (.012) .892 (.016) .919 (.012)

.703 (.037) .700 (.065) .751 (.024) .753 (.024) .761 (.026) .752 (.025) .748 (.024)

.703 (.037) .700 (.065) .751 (.024) .753 (.024) .761 (.026) .687 (.047) .691 (.050)

.697 (.090) .791 (.052) .705 (.037) .895 (.012) .691 (.026) .801 (.021) .895 (.012)

.697 (.090) .791 (.052) .705 (.037) .895 (.012) .691 (.026) .654 (.062) .797 (.069)

.693 (.018) .675 (.040) .634 (.007) .768 (.005) .719 (.007) .721 (.006) .768 (.005)

.693 (.018) .675 (.040) .634 (.007) .768 (.005) .719 (.007) .619 (.013) .550 (.037)

79

Table 61: Our methods vs. the rest: mean classiﬁer AUC for = 0.1, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.878 (.016) .888 (.043) .892 (.014) .904 (.013) .901 (.014) .894 (.015) .907 (.013)

.878 (.016) .888 (.043) .892 (.014) .904 (.013) .901 (.014) .877 (.016) .890 (.017)

.681 (.033) .677 (.056) .723 (.025) .723 (.025) .732 (.019) .729 (.029) .727 (.025)

.681 (.033) .677 (.056) .723 (.025) .723 (.025) .732 (.019) .671 (.049) .677 (.048)

.686 (.056) .791 (.025) .702 (.041) .869 (.017) .680 (.045) .792 (.024) .874 (.017)

.686 (.056) .791 (.025) .702 (.041) .869 (.017) .680 (.045) .636 (.058) .680 (.119)

.679 (.009) .697 (.010) .629 (.006) .743 (.006) .715 (.005) .719 (.005) .764 (.005)

.679 (.009) .697 (.010) .629 (.006) .743 (.006) .715 (.005) .637 (.010) .697 (.021)

Table 62: Our methods vs. the rest: mean classiﬁer AUC for = 0.2, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.849 (.024) .864 (.043) .865 (.018) .883 (.016) .880 (.017) .878 (.018) .890 (.016)

.849 (.024) .864 (.043) .865 (.018) .883 (.016) .880 (.017) .854 (.027) .861 (.026)

.656 (.039) .663 (.056) .706 (.029) .707 (.029) .722 (.028) .714 (.030) .713 (.030)

.656 (.039) .663 (.056) .706 (.029) .707 (.029) .722 (.028) .662 (.044) .666 (.049)

.688 (.037) .785 (.025) .692 (.037) .850 (.018) .686 (.045) .785 (.019) .859 (.015)

.688 (.037) .785 (.025) .692 (.037) .850 (.018) .686 (.045) .649 (.050) .634 (.121)

.662 (.007) .682 (.038) .622 (.006) .727 (.004) .711 (.004) .713 (.004) .758 (.004)

.662 (.007) .682 (.038) .622 (.006) .727 (.004) .711 (.004) .637 (.008) .697 (.011)

Table 63: Our methods vs. the rest: mean classiﬁer AUC for = 0.3, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.826 (.022) .839 (.042) .839 (.018) .861 (.016) .861 (.018) .860 (.018) .868 (.017)

.826 (.022) .839 (.042) .839 (.018) .861 (.016) .861 (.018) .832 (.023) .831 (.021)

.628 (.040) .647 (.047) .685 (.027) .684 (.028) .708 (.026) .700 (.030) .694 (.027)

.628 (.040) .647 (.047) .685 (.027) .684 (.028) .708 (.026) .649 (.041) .649 (.044)

.690 (.048) .768 (.044) .713 (.041) .836 (.020) .698 (.052) .781 (.021) .850 (.022)

.690 (.048) .768 (.044) .713 (.041) .836 (.020) .698 (.052) .636 (.046) .603 (.091)

.653 (.007) .664 (.054) .618 (.006) .711 (.005) .702 (.007) .706 (.005) .751 (.005)

.653 (.007) .664 (.054) .618 (.006) .711 (.005) .702 (.007) .635 (.007) .689 (.008)

80

Table 64: Our methods vs. the rest: mean classiﬁer AUC for = 0.4, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.796 (.030) .812 (.042) .812 (.024) .836 (.023) .835 (.021) .844 (.022) .847 (.021)

.796 (.030) .812 (.042) .812 (.024) .836 (.023) .835 (.021) .810 (.033) .795 (.036)

.599 (.043) .615 (.046) .658 (.032) .658 (.033) .677 (.026) .674 (.031) .670 (.030)

.599 (.043) .615 (.046) .658 (.032) .658 (.033) .677 (.026) .633 (.041) .633 (.043)

.678 (.046) .765 (.045) .718 (.044) .822 (.023) .713 (.045) .771 (.026) .838 (.019)

.678 (.046) .765 (.045) .718 (.044) .822 (.023) .713 (.045) .638 (.044) .600 (.091)

.645 (.008) .650 (.056) .610 (.007) .696 (.007) .695 (.006) .699 (.006) .742 (.005)

.645 (.008) .650 (.056) .610 (.007) .696 (.007) .695 (.006) .627 (.008) .676 (.009)

Table 65: Our methods vs. the rest: mean classiﬁer AUC for = 0.5, balanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.771 (.028) .783 (.041) .784 (.022) .810 (.020) .814 (.018) .818 (.019) .818 (.018)

.771 (.028) .783 (.041) .784 (.022) .810 (.020) .814 (.018) .786 (.026) .759 (.036)

.590 (.033) .604 (.049) .648 (.031) .649 (.029) .669 (.028) .657 (.032) .654 (.031)

.590 (.033) .604 (.049) .648 (.031) .649 (.029) .669 (.028) .618 (.044) .618 (.044)

.674 (.049) .748 (.042) .721 (.031) .811 (.022) .700 (.046) .766 (.024) .828 (.020)

.674 (.049) .748 (.042) .721 (.031) .811 (.022) .700 (.046) .639 (.042) .650 (.065)

.639 (.007) .661 (.006) .602 (.007) .681 (.005) .687 (.006) .692 (.007) .731 (.006)

.639 (.007) .661 (.006) .602 (.007) .681 (.005) .687 (.006) .624 (.007) .663 (.009)

Table 66: Our methods vs. the rest: mean classiﬁer accuracy for = 0.0, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.825 (.072) .838 (.055) .444 (.016) .523 (.031) .858 (.013) .858 (.014) .858 (.012) .850 (.014) .855 (.013)

.825 (.072) .838 (.055) .444 (.016) .523 (.031) .858 (.013) .858 (.014) .858 (.012) .733 (.093) .786 (.089)

.709 (.021) .702 (.018) .299 (.016) .316 (.017) .733 (.017) .736 (.016) .746 (.016) .743 (.017) .735 (.018)

.709 (.021) .702 (.018) .299 (.016) .316 (.017) .733 (.017) .736 (.016) .746 (.016) .701 (.016) .701 (.016)

.931 (.003) .933 (.006) .078 (.004) .364 (.040) .931 (.003) .948 (.003) .931 (.003) .930 (.004) .952 (.004)

.931 (.003) .933 (.006) .078 (.004) .364 (.040) .931 (.003) .948 (.003) .931 (.003) .931 (.003) .931 (.003)

.794 (.016) .798 (.022) .222 (.003) .550 (.020) .779 (.003) .819 (.002) .801 (.005) .811 (.003) .819 (.002)

.794 (.016) .798 (.022) .222 (.003) .550 (.020) .779 (.003) .819 (.002) .801 (.005) .779 (.003) .779 (.003)

81

Table 67: Our methods vs. the rest: mean classiﬁer accuracy for = 0.1, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.804 (.024) .809 (.021) .447 (.014) .500 (.025) .828 (.021) .838 (.015) .831 (.018) .826 (.015) .836 (.016)

.804 (.024) .809 (.021) .447 (.014) .500 (.025) .828 (.021) .838 (.015) .831 (.018) .684 (.072) .709 (.084)

.696 (.018) .694 (.013) .303 (.012) .318 (.015) .714 (.016) .713 (.018) .728 (.017) .733 (.014) .724 (.017)

.696 (.018) .694 (.013) .303 (.012) .318 (.015) .714 (.016) .713 (.018) .728 (.017) .697 (.012) .697 (.012)

.929 (.003) .930 (.003) .071 (.002) .085 (.006) .929 (.003) .938 (.003) .929 (.002) .929 (.004) .945 (.002)

.929 (.003) .930 (.003) .071 (.002) .085 (.006) .929 (.003) .938 (.003) .929 (.002) .929 (.002) .929 (.002)

.791 (.012) .792 (.015) .224 (.002) .459 (.027) .776 (.002) .805 (.002) .798 (.009) .805 (.002) .815 (.002)

.791 (.012) .792 (.015) .224 (.002) .459 (.027) .776 (.002) .805 (.002) .798 (.009) .776 (.002) .776 (.002)

Table 68: Our methods vs. the rest: mean classiﬁer accuracy for = 0.2, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.775 (.023) .786 (.018) .450 (.023) .495 (.024) .793 (.023) .804 (.022) .798 (.018) .802 (.019) .808 (.019)

.775 (.023) .786 (.018) .450 (.023) .495 (.024) .793 (.023) .804 (.022) .798 (.018) .662 (.088) .672 (.102)

.692 (.016) .695 (.015) .303 (.014) .321 (.019) .709 (.018) .712 (.017) .725 (.018) .725 (.017) .720 (.017)

.692 (.016) .695 (.015) .303 (.014) .321 (.019) .709 (.018) .712 (.017) .725 (.018) .698 (.014) .698 (.014)

.931 (.003) .931 (.003) .069 (.003) .070 (.003) .931 (.003) .936 (.003) .931 (.003) .929 (.003) .943 (.004)

.931 (.003) .931 (.003) .069 (.003) .070 (.003) .931 (.003) .936 (.003) .931 (.003) .931 (.003) .931 (.003)

.789 (.010) .789 (.009) .221 (.003) .424 (.038) .779 (.003) .799 (.003) .803 (.003) .805 (.004) .813 (.003)

.789 (.010) .789 (.009) .221 (.003) .424 (.038) .779 (.003) .799 (.003) .803 (.003) .779 (.003) .779 (.003)

Table 69: Our methods vs. the rest: mean classiﬁer accuracy for = 0.3, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.756 (.022) .768 (.027) .445 (.023) .509 (.032) .770 (.023) .782 (.022) .764 (.035) .788 (.024) .792 (.024)

.756 (.022) .768 (.027) .445 (.023) .509 (.032) .770 (.023) .782 (.022) .764 (.035) .651 (.080) .650 (.089)

.688 (.014) .696 (.016) .299 (.013) .324 (.015) .700 (.017) .703 (.014) .716 (.018) .716 (.019) .713 (.017)

.688 (.014) .696 (.016) .299 (.013) .324 (.015) .700 (.017) .703 (.014) .716 (.018) .701 (.013) .701 (.013)

.931 (.004) .931 (.004) .069 (.004) .069 (.004) .931 (.004) .933 (.005) .931 (.004) .929 (.005) .941 (.004)

.931 (.004) .931 (.004) .069 (.004) .069 (.004) .931 (.004) .933 (.005) .931 (.004) .931 (.004) .931 (.004)

.785 (.006) .787 (.010) .221 (.003) .468 (.019) .779 (.003) .794 (.004) .798 (.004) .800 (.003) .810 (.002)

.785 (.006) .787 (.010) .221 (.003) .468 (.019) .779 (.003) .794 (.004) .798 (.004) .779 (.003) .779 (.003)

82

Table 70: Our methods vs. the rest: mean classiﬁer accuracy for = 0.4, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.733 (.026) .736 (.026) .439 (.014) .514 (.036) .744 (.019) .762 (.019) .747 (.026) .769 (.020) .766 (.019)

.733 (.026) .736 (.026) .439 (.014) .514 (.036) .744 (.019) .762 (.019) .747 (.026) .637 (.064) .619 (.064)

.676 (.013) .691 (.020) .305 (.015) .350 (.024) .690 (.017) .689 (.018) .704 (.016) .709 (.012) .704 (.014)

.676 (.013) .691 (.020) .305 (.015) .350 (.024) .690 (.017) .689 (.018) .704 (.016) .696 (.015) .696 (.015)

.929 (.004) .930 (.003) .070 (.003) .070 (.003) .930 (.003) .932 (.003) .930 (.003) .927 (.003) .938 (.003)

.929 (.004) .930 (.003) .070 (.003) .070 (.003) .930 (.003) .932 (.003) .930 (.003) .930 (.003) .930 (.003)

.779 (.002) .785 (.009) .221 (.002) .507 (.022) .779 (.002) .788 (.003) .795 (.002) .797 (.002) .806 (.003)

.779 (.002) .785 (.009) .221 (.002) .507 (.022) .779 (.002) .788 (.003) .795 (.002) .779 (.002) .779 (.002)

Table 71: Our methods vs. the rest: mean classiﬁer accuracy for = 0.5, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. MINCUT MINCUT w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.696 (.033) .707 (.047) .457 (.023) .542 (.038) .716 (.025) .730 (.028) .716 (.040) .751 (.022) .741 (.016)

.696 (.033) .707 (.047) .457 (.023) .542 (.038) .716 (.025) .730 (.028) .716 (.040) .606 (.067) .596 (.072)

.669 (.019) .695 (.016) .304 (.016) .377 (.030) .686 (.018) .689 (.018) .699 (.022) .699 (.018) .699 (.018)

.669 (.019) .695 (.016) .304 (.016) .377 (.030) .686 (.018) .689 (.018) .699 (.022) .698 (.017) .698 (.017)

.930 (.003) .931 (.003) .068 (.003) .068 (.003) .931 (.002) .934 (.002) .932 (.002) .929 (.003) .937 (.003)

.930 (.003) .931 (.003) .068 (.003) .068 (.003) .931 (.002) .934 (.002) .932 (.002) .932 (.003) .932 (.003)

.779 (.002) .780 (.002) .222 (.002) .567 (.009) .780 (.002) .783 (.003) .792 (.003) .795 (.002) .804 (.002)

.779 (.002) .780 (.002) .222 (.002) .567 (.009) .780 (.002) .783 (.003) .792 (.003) .780 (.002) .780 (.002)

Table 72: Our methods vs. the rest: mean classiﬁer AUC for = 0.0, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.881 (.077) .900 (.067) .915 (.010) .924 (.010) .917 (.010) .915 (.011) .926 (.010)

.881 (.077) .900 (.067) .915 (.010) .924 (.010) .917 (.010) .867 (.065) .904 (.041)

.616 (.069) .597 (.067) .758 (.017) .762 (.017) .774 (.018) .767 (.020) .758 (.017)

.616 (.069) .597 (.067) .758 (.017) .762 (.017) .774 (.018) .657 (.028) .653 (.024)

.635 (.142) .703 (.080) .701 (.024) .911 (.010) .708 (.028) .796 (.025) .913 (.009)

.635 (.142) .703 (.080) .701 (.024) .911 (.010) .708 (.028) .412 (.045) .422 (.154)

.639 (.059) .649 (.081) .631 (.004) .770 (.003) .718 (.003) .724 (.003) .770 (.003)

.639 (.059) .649 (.081) .631 (.004) .770 (.003) .718 (.003) .594 (.006) .562 (.014)

83

Table 73: Our methods vs. the rest: mean classiﬁer AUC for = 0.1, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.878 (.020) .892 (.013) .893 (.013) .907 (.011) .900 (.011) .901 (.015) .909 (.014)

.878 (.020) .892 (.013) .893 (.013) .907 (.011) .900 (.011) .854 (.062) .871 (.054)

.599 (.062) .563 (.072) .731 (.020) .731 (.018) .748 (.019) .743 (.020) .734 (.021)

.599 (.062) .563 (.072) .731 (.020) .731 (.018) .748 (.019) .645 (.021) .642 (.024)

.529 (.121) .620 (.169) .684 (.024) .878 (.007) .700 (.028) .770 (.027) .892 (.011)

.529 (.121) .620 (.169) .684 (.024) .878 (.007) .700 (.028) .445 (.063) .451 (.134)

.634 (.042) .638 (.076) .627 (.006) .748 (.001) .715 (.007) .722 (.005) .769 (.003)

.634 (.042) .638 (.076) .627 (.006) .748 (.001) .715 (.007) .604 (.006) .690 (.013)

Table 74: Our methods vs. the rest: mean classiﬁer AUC for = 0.2, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.849 (.030) .866 (.015) .867 (.015) .882 (.013) .878 (.015) .878 (.014) .885 (.015)

.849 (.030) .866 (.015) .867 (.015) .882 (.013) .878 (.015) .821 (.061) .824 (.071)

.568 (.052) .563 (.063) .721 (.020) .723 (.019) .741 (.018) .729 (.025) .720 (.023)

.568 (.052) .563 (.063) .721 (.020) .723 (.019) .741 (.018) .635 (.025) .627 (.022)

.572 (.091) .656 (.104) .699 (.038) .860 (.008) .708 (.036) .772 (.017) .883 (.012)

.572 (.091) .656 (.104) .699 (.038) .860 (.008) .708 (.036) .448 (.056) .519 (.093)

.619 (.036) .648 (.045) .619 (.005) .724 (.007) .705 (.003) .710 (.003) .756 (.003)

.619 (.036) .648 (.045) .619 (.005) .724 (.007) .705 (.003) .602 (.005) .690 (.004)

Table 75: Our methods vs. the rest: mean classiﬁer AUC for = 0.3, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.827 (.023) .849 (.018) .842 (.018) .863 (.016) .861 (.017) .864 (.017) .870 (.019)

.827 (.023) .849 (.018) .842 (.018) .863 (.016) .861 (.017) .803 (.064) .792 (.087)

.562 (.046) .555 (.051) .691 (.021) .690 (.023) .712 (.021) .699 (.023) .688 (.021)

.562 (.046) .555 (.051) .691 (.021) .690 (.023) .712 (.021) .620 (.025) .613 (.020)

.557 (.062) .654 (.076) .705 (.027) .843 (.010) .711 (.033) .769 (.020) .875 (.010)

.557 (.062) .654 (.076) .705 (.027) .843 (.010) .711 (.033) .477 (.043) .575 (.089)

.585 (.061) .621 (.037) .613 (.004) .711 (.005) .697 (.003) .702 (.004) .749 (.005)

.585 (.061) .621 (.037) .613 (.004) .711 (.005) .697 (.003) .597 (.004) .684 (.006)

84

Table 76: Our methods vs. the rest: mean classiﬁer AUC for = 0.4, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.804 (.026) .820 (.022) .811 (.022) .838 (.016) .841 (.021) .843 (.015) .844 (.017)

.804 (.026) .820 (.022) .811 (.022) .838 (.016) .841 (.021) .796 (.041) .781 (.049)

.548 (.038) .532 (.053) .673 (.022) .673 (.023) .694 (.023) .689 (.024) .678 (.021)

.548 (.038) .532 (.053) .673 (.022) .673 (.023) .694 (.023) .609 (.026) .605 (.026)

.539 (.047) .599 (.085) .708 (.022) .832 (.011) .716 (.024) .759 (.018) .854 (.010)

.539 (.047) .599 (.085) .708 (.022) .832 (.011) .716 (.024) .511 (.055) .629 (.063)

.557 (.028) .611 (.049) .607 (.006) .699 (.004) .691 (.004) .695 (.004) .741 (.004)

.557 (.028) .611 (.049) .607 (.006) .699 (.004) .691 (.004) .598 (.005) .674 (.008)

Table 77: Our methods vs. the rest: mean classiﬁer AUC for = 0.5, unbalanced datasets, all features.

Classiﬁer

Australia

Tru.

Str.

Germany

Tru.

Str.

Poland

Tru.

Str.

Taiwan

Tru.

Str.

HCAPP(LR) HCAPP(LR) w/ disc. IC-LR IC-LR w/ disc. IC-LR w/ neg. IMP(LR) IMP(LR) w/ disc.

.775 (.034) .780 (.066) .784 (.027) .810 (.020) .818 (.026) .816 (.015) .813 (.018)

.775 (.034) .780 (.066) .784 (.027) .810 (.020) .818 (.026) .748 (.055) .713 (.077)

.526 (.036) .552 (.038) .651 (.022) .655 (.025) .673 (.019) .667 (.027) .658 (.029)

.526 (.036) .552 (.038) .651 (.022) .655 (.025) .673 (.019) .596 (.024) .590 (.024)

.537 (.034) .651 (.054) .713 (.021) .822 (.014) .721 (.021) .763 (.016) .845 (.013)

.537 (.034) .651 (.054) .713 (.021) .822 (.014) .721 (.021) .521 (.043) .663 (.057)

.538 (.029) .559 (.043) .603 (.008) .682 (.003) .682 (.005) .687 (.004) .732 (.004)

.538 (.029) .559 (.043) .603 (.008) .682 (.003) .682 (.005) .593 (.005) .660 (.003)

85

