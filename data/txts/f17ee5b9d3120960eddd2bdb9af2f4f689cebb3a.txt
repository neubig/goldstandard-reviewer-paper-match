Indirect Supervision for Relation Extraction using Question-Answer Pairs

arXiv:1710.11169v2 [cs.CL] 23 Nov 2017

Zeqiu Wu
University of Illinois, Urbana-Champaign zeqiuwu1@illinois.edu

Xiang Ren
University of Southern California Los Angeles, California xiangren@usc.edu

Frank F. Xu
Shanghai Jiao Tong University Shanghai, China
frankxu@sjtu.edu.cn

Ji Li
University of Illinois, Urbana-Champaign
jili3@illinois.edu
ABSTRACT
Automatic relation extraction (RE) for types of interest is of great importance for interpreting massive text corpora in an efficient manner. For example, we want to identify the relationship “president_of” between entities “Donald Trump” and “United States” in a sentence expressing such a relation. Traditional RE models have heavily relied on human-annotated corpus for training, which can be costly in generating labeled data and become obstacles when dealing with more relation types. Thus, more RE extraction systems have shifted to be built upon training data automatically acquired by linking to knowledge bases (distant supervision). However, due to the incompleteness of knowledge bases and the context-agnostic labeling, the training data collected via distant supervision (DS) can be very noisy. In recent years, as increasing attention has been brought to tackling question-answering (QA) tasks, user feedback or datasets of such tasks become more accessible. In this paper, we propose a novel framework, ReQuest, to leverage question-answer pairs as an indirect source of supervision for relation extraction, and study how to use such supervision to reduce noise induced from DS. Our model jointly embeds relation mentions, types, QA entity mention pairs and text features in two low-dimensional spaces (RE and QA), where objects with same relation types or semantically similar question-answer pairs have similar representations. Shared features connect these two spaces, carrying clearer semantic knowledge from both sources. ReQuest, then use these learned embeddings to estimate the types of test relation mentions. We formulate a global objective function and adopt a novel margin-based QA loss to reduce noise in DS by exploiting semantic evidence from the QA dataset. Our experimental results achieve an average of 11% improvement in F1 score on two public RE datasets combined with TREC QA dataset. Codes and datasets can be downloaded at https://github.com/ellenmellon/ReQuest.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5581-0/18/02. . . $15.00 https://doi.org/10.1145/3159652.3159709

Jiawei Han
University of Illinois, Urbana-Champaign hanj@illinois.edu

ID Sentence

S1 Donald Trump is the 45th and current President of the United States.

S2 Donald Trump is a citizen of the New York City, USA.

S3 Trump traveled on his private jet from UK back to the US.

S4 Ellen, a native of China, went to the United States four years ago.

…

…

Entity 1: Donald Trump

Relation Instance Entity 2: United States

Text Corpus
KB Relation of targets

Relation Type president_of
citizen_of

Candidate Relation Types

Entity 1 Donald Trump Donald Trump

Entity 2 United States United States

Q1: What is Jack’s nationality?

A1: Jack is a citizen of Germany.

+

A2: Jack, a native of Germany, like beer.

+

A3: Jack just boarded on a flight to France. -

QA Pairs as Indirect Supervision

Error Noise Reduction

False Positive:

Two Types of Errors

Caused by context-agnostic labeling

False Negative:

True relations not present in KB

Automatically Labeled Training Data

Relation Mention: (“Donald Trump”, “United States”, S1) Relation Types: {president_of, citizen_of}

Relation Mention: (“Donald Trump”, “USA”, S2) Relation Types: {president_of, citizen_of}

Relation Mention: (“Trump”, “US”, S3) Relation Types: {president_of, citizen_of}
Relation Mention: (“Ellen”, “China”, S4) Relation Types: {None}

Figure 1: Distant supervision generates training data by linking relation mentions in sentences S1-S4 to KB and assigning the linkable relation types to all relation mentions. Those unlinkable entity mention pairs are treated as negative examples. This automatic labeling process may cause errors of false positives (highlighted in red) and false negatives (highlighted in purple). QA pairs provide indirect supervision for correcting such errors.

1 INTRODUCTION
Relation extraction is an important task for understanding massive text corpora by turning unstructured text data into relation triples for further analysis. For example, it detects the relationship “president_of” between entities “Donald Trump” and “United States” in a sentence. Such extracted information can be used for more downstream text analysis tasks (e.g. serving as primitives for information extraction and knowledge base (KB) completion, and assisting question answering systems).
Typically, RE systems rely on training data, primarily acquired via human annotation, to achieve satisfactory performance. However, such manual labeling process can be costly and non-scalable when adapting to other domains (e.g. biomedical domain). In addition, when the number of types of interest becomes large, the generation of handcrafted training data can be error-prone. To alleviate such an exhaustive process, the recent trend has deviated towards the adoption of distant supervision (DS). DS replaces the manual training data generation with a pipeline that automatically links texts to a knowledge base (KB). The pipeline has the following steps: (1) detect entity mentions in text; (2) map detected entity mentions to entities in KB; (3) assign, to the candidate type set of each entity mention pair, all KB relation types between their

WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

Z. Wu et al.

Candidate Generation & Distant Supervision
ID Sentence S1 Donald Trump is the 45th and current President of the United States. S2 Donald Trump is a citizen of the New York City, USA. S3 Trump took a flight from UK back to the US.

S4 Jinping Xi is the current President of China and Chairman of the Central Military Commission.

…

…

Automatically Labeled Training Data

Relation Mention: (“Donald Trump”, “United States”, S1) Relation Types: {president_of, citizen_of}

Relation Mention: (“Donald Trump”, “USA”, S2) Relation Types: {president_of, citizen_of}

Text Corpus

Relation Mention: (“Trump”, “US”, S3) Relation Types: {president_of, citizen_of}

Mention Feature
Type

Heterogeneous Network Construction

(“Donald Trump”, “United States”, S1)

(“Donald Trump”, “USA”, S2)

(“Trump”, “US”, S3)

None

EM1_ Donald Trump

Relation Mentions

EM1_Trump

president_of citizen_of

QA Entity Pairs
EM2_ Germany

BETWEEN_ president

BETWEEN_ citizen

Overlapping Features

EM2_ France

Q1: What is Jack’s nationality?

Q2: Who is the President of France?

A1: Jack is a citizen of Germany.

+

A1: Emmanuel Macron was elected as President of France.

+

A2: Jack, a native of Germany, drinks beer every day.

+

A2: Emmanuel Macron became head of France on May 7, 2017.

+

A3: Jack just boarded on a flight to France.

-

A3: Emmanuel Macron was born in Amiens, France.

-

+Feature Question

A3

A1

A2

Q1

A3

A1

A2

Q2

Joint RE & QA Embedding
None
(“Trump”, “US”, S3) EM1_Trump

(“Donald Trump”, “USA”, S2) EM1_Donald Trump
citizen_of

president_of (“Donald Trump”,
“United States”, S1)

BETWEEN_citizen BETWEEN_president

Q1A1 Q1A2
EM2_Germany Q1A3

Q2A1 Q2A2 EM2_France
Q2A3

Relation Type Inference

Target Relation Type Set

None

born_in

…

president_of citizen_of

Text Features from S4

(“Jinping Xi”, “China”, S4)

BETWEEN_ President
RIGHT_ Chairman
EM1_ Jinping Xi

Test Relation Mention

…

Figure 2: Overall Framework.

KB-mapped entities. However, the noise introduced to the automatically generated training data is not negligible. There are two major causes of error: incomplete KB and context-agnostic labeling process. If we treat unlinkable entity pairs as the pool of negative examples, false negatives can be commonly encountered as a result of the insufficiency of facts in KBs, where many true entity or relation mentions fail to be linked to KBs (see example in Figure 1). In this way, models counting on extensive negative instances may suffer from such misleading training data. On the other hand, context-agnostic labeling can engender false positive examples, due to the inaccuracy of the DS assumption that if a sentence contains any two entities holding a relation in the KB, the sentence must be expressing such relation between them. For example, entities “Donald Trump” and “United States” in the sentence “Donald Trump flew back to United States” can be labeled as “president_of” as well as “born_in”, although only an out-of-interest relation type “travel_to” is expressed explicitly (as shown in Figure 1).
Towards the goal of diminishing the negative effects by noisy DS training data, distantly supervised RE models that deal with training noise, as well as methods that directly improve the automatic training data generation process have been proposed. These methods mostly involve designing distinct assumptions to remove redundant training information [16, 21, 26, 36]. For example, method applied in [16, 36] assumes that for each relation triple in the KB, at least one sentence might express the relation instead of all sentences. Moreover, these noise reduction systems usually only address one type of error, either false positives or false negatives. Hence, current methods handling DS noises still have the following challenges:
(1) Lack of trustworthy sources: Current de-noising methods mainly focus on recognizing labeling mistakes from the labeled data itself, assisted by pre-defined assumptions or patterns. They do not have external trustworthy sources as guidance to uncover incorrectly labeled data, while not at the expense of excessive human efforts. Without other separate information sources, the reliability of false label identification can be limited.
(2) Incomplete noise handling: Although both false negative and false positive errors are observed to be significant, most existing works only address one of them.
In this paper, to overcome the above two issues derived from relation extraction with distant supervision, we study the problem of relation extraction with indirect supervision from external sources.

Recently, the rapid emergence of QA systems promotes the availability of user feedback or datasets of various QA tasks. We investigate to leverage QA, a downstream application of relation extraction, to provide additional signals for learning RE models. Specifically, we use datasets for the task of answer sentence selection to facilitate relation typing. Given a domain-specific corpus and a set of target relation types from a KB, we aim to detect relation mentions from text and categorize each in context by target types or Non-TargetType (None) by leveraging an independent dataset of QA pairs in the same domain. We address the above two challenges as follows: (1) We integrate indirect supervision from another same-domain data source in the format of QA sentence pairs, that is, each question sentence maps to several positive (where a true answer can be found) and negative (where no answer exists) answer sentences. We adopt the principle that for the same question, positive pairs of (question, answer) should be semantically similar while they should be dissimilar from negative pairs. (2) Instead of differentiating types of labeling errors at the instance level, we concentrate on how to better learn semantic representation of features. Wrongly labeled training examples essentially misguide the understanding of features. It increases the risk of having a non-representative feature learned to be close to a relation type and vice versa. Therefore, if the feature learning process is improved, potentially both types of error can be reduced. (See how QA pairs improve the feature embedding learning process in Figure 3).
To integrate all the above elements, a novel framework, ReQuest, is proposed. First, ReQuest constructs a heterogeneous graph to represent three kinds of objects: relation mentions, text features and relation types for RE training data labeled by KB linking. Then, ReQuest constructs a second heterogeneous graph to represent entity mention pairs (include question, answer entity mention pairs) and features for QA dataset. These two graphs are combined into a single graph by overlapped features. We formulate a global objective to jointly embed the graph into a low-dimensional space where, in that space, RE objects whose types are semantically close also have similar representations and QA objects linked by positive (question, answer) entity mention pairs of a same question should have close representations. In particular, we design a novel margin-based loss to model the semantic similarity between QA pairs and transmit such information into feature and relation type representations via shared features. With the learned embeddings, we can efficiently

Indirect Supervision for Relation Extraction using Question-Answer Pairs WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

ID

Sentence

S1 Donald Trump is a citizen of the United States.

S2 Donald Trump was on a flight back to the United States.

S3 Ellen, a native of China, came here for school 4 years ago.

Automatically generated relation mentions

Relation Mentions with Relation Type “citizen_of”: (“Donald Trump”, “United States”, S1) (“Donald Trump”, “United States”, S2)

Relation Mentions with Relation Type “Not-of-Interest: (“Ellen”, “China”, S3)

Question: What is Jack’s nationality? A1: Jack is a citizen of Germany. A2: Jack, a native of Germany, drinks beer every day. A3: Jack just boarded on a flight to France.

Positive Positive Negative

RE QA

BETWEEN_citizen

citizen_of BETWEEN_flight

None BETWEEN_native

BETWEEN_native

citizen_of BETWEEN_citizen

None

BETWEEN_flight

Figure 3: Due to the noise in the automatically generated RE train-
ing corpus, the associations between learned feature embeddings
and relation types can be affected by the wrongly labeled training
examples. However, the idea of QA pairwise interactions has the
potential to correct such embedding deviations by bringing extra
semantic clues from overlapped features in QA corpus.
estimate the types for test relation mentions. In summary, this paper makes the following contributions:
(1) We propose the novel idea of applying indirect supervision from question answering datasets to help eliminate noise from distant supervision for the task of relation extraction.
(2) We design a novel joint optimization framework, ReQuest, to extract typed relations in domain-specific corpora.
(3) Experiments with two public RE datasets combined with TREC QA demonstrate that ReQuest improves the performance of state-of-the-art RE systems significantly.
2 DEFINITIONS AND PROBLEM
Our proposed ReQuest framework takes the following input: an automatically labeled training corpus DL obtained by linking a text corpus D to a KB (e.g. Freebase) Ψ, a target relation type set R and a set of QA sentence pairs DQAS with extract answers labeled.
Entity and Relation Mention. An entity mention (denoted by m) is a token span in text which represents an entity e. A relation instance r (e1, e2, . . . , en ) denotes some type of relation r ∈ R between multiple entities. In this paper, we focus on binary relations, i.e., r (e1, e2). We define a relation mention (denoted by z) for some relation instance r (e1, e2) as a (ordered) pair of entities mentions of e1 and e2 in a sentence s, and represent a relation mention with entity mentions m1 and m2 in sentence s as z = (m1, m2, s).
Knowledge Bases and Target Types. A KB contains a set of entities EΨ, entity types Y and relation types R, as well as humancurated facts on both relation instances IΨ = {r (e1, e2)} ⊂ RΨ ×EΨ× EΨ, and entity-type facts TΨ = {(e, y)} ⊂ EΨ × YΨ. Target relation type set R covers a subset of relation types that the users are interested in from Ψ, i.e., R ⊂ RΨ.
Automatically Labeled Training Corpora. Distant supervision maps the set of entity mentions extracted from the text corpus to KB entities EΨ with an entity disambiguation system [15, 24]. Between any two linkable entity mentions m1 and m2 in a sentence, a relation mention zi is formed if there exists one or more KB relations between their KB-mapped entities e1 and e2. Relations between e1 and e2 in KB are then associated to zi to form its candidate relation type set Ri , i.e., Ri = {r | r (e1, e2) ∈ RΨ}.

Let

Z

=

{z

i

}NZ
i =1

denote

the

set

of

extracted

relation

mentions

that can be mapped to KB. Formally, we represent the automatically

labeled training corpus DL for relation extraction, using a set of

tuples

DL

=

{(zi ,

R

i

)

}NZ
i =1

.

There

exists

publicly

available

automat-

ically labeled corpora such as the NYT dataset [36] where relation

mentions have already been extracted and mapped to KB.

QA Entity Mention Pairs. The set of QA sentence pairs DQAS consists of questions Q in the same domain as the training text
corpus. For each question qi , there will be a number of positive sentences Ai+, each of which contains a correct answer to the question and another set of negative sentences Ai− where no answer can be found. And the tokens spans of the exact answer in each positive is

marked as well. For each question, we extract positive QA (ordered) entity mention pairs Pi+ from Ai+ and negative entity mention pairs Pi− from Ai−. A positive QA entity mention pair pk contains an entity mention being asked about (question entity mention m1)
and an entity mention serving as the answer (answer entity men-
tion m2) to a question. That being said, we can get one positive
QA entity mention pair from each positive answer sentence if both

entity mentions can be found. In contrast, A negative QA entity

mention pair does not follow such pattern for the corresponding

question.

Let

Q

=

{qi

}iN=q1

denote

the

set

of

questions;

P

=

{pk

}Np
k =1

denote

all QA entity mention pairs; Pi+ = {pk+ }kN+i+=1 denNo−te the set of posi-

tive QA entity mention pairs for qi ; Pi− = {pk− }k−i=1 denote the set

of negative QA entity mention pairs for qi . Formally, the QA entity

mention pairs corpus is represented as DQA = {(qi, Pi+, Pi−)}iN=q1.

Definition 2.1 (Problem Definition). Given an automatically gen-
erated training corpus DL, a target relation type set R ⊂ RΨ and a set of QA sentence pairs DQAS in the same domain, the relation extraction task aims to (1) extract QA entity mention pairs to generate DQA; (2) estimate a relation type r ∗ ∈ R ∪{None} for each test relation mention, using both the training corpus and the extracted

QA pairs with their contexts.

3 APPROACH
Framework Overview. We propose an embedding-based framework with indirect supervision (illustrated in Figure 2) as follows:
(1) Generate text features for each relation mention or QA entity mention pair, and construct a heterogeneous graph using four kinds of objects in combined corpus, namely relation mentions from RE corpus, entity mention pairs from QA corpus, target relation types and text features to encode aforementioned signals in a unified form (Section 3.1).
(2) Jointly embed relation mentions, QA pairs, text features, and type labels into two low-dimensional spaces connected by shared features, where close objects tend to share the same types or questions (Section 3.2).
(3) Estimate type labels r ∗ for each test relation mention z from learned embeddings, by searching the target type set R (Section 3.3).

3.1 Heterogeneous Network Construction
Relation Mentions and Types Generation. We get the relation mentions along with their heuristically obtained relation types from

WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

Z. Wu et al.

the automatically labeled training corpus DL. And we randomly sample a set of unlinkable entity mention pairs as the negative relation mentions (i.e., relation mentions assigned with type “None”).

QA Entity Mention Pairs Generation. We apply Stanford NER [23]
to extract entity mentions in each question or answer sentence. First,
we detect the target entity being asked about in each question sen-
tence. For example, in the question “Who is the president of United
States”, the question entity is “United States”. In most cases, a ques-
tion only contains one entity mention and for those containing
multiple entity mentions, we notice the question entity is mostly
mentioned at the very last. Thus, we follow this heuristic rule to
assign the lastly occurred entity mention to be the question entity mention m0 in each question sentence qi . Then, in each positive answer sentence of qi , we extract the entity mention with matched head token and smallest edit string distance to be the question entity mention m1, and the entity mention matching the exact answer string to be the answer entity mention m2. Then we form a positive QA entity mention pair with its context s, pk = (m1, m2, s) ∈ Pi+ for qi . If either m1 or m2 can not be found, this positive answer sentence is dropped. We randomly select pairs of entity mentions
in each negative answer sentence to be negative QA entity mention pairs for qi (e.g., if a negative sentence includes 3 entity mentions, we randomly select negative examples from the 3 · 2 · 1 = 6 different pairs of entity mentions in total, if we ignore the order), with each negative example marked as pk ′ = (m1′, m2′, s′) ∈ Pi− for qi .

Text Feature Extraction. We extract lexical features of various

types from not only the mention itself (e.g., head token), as well

as the context s (e.g., bigram) in a POS-tagged corpus. It is to cap-

ture the syntactic and semantic information for any given relation

mentions or entity mention pairs. See Table 1 for all types of text

features used, following those in [4, 26] (excluding the dependency

parse-based features and entity type features).

We denote the set of Mz unique features extracted from relation

mentions

Z

as

Fz

=

{f

j

}Mz
j =1

and

the

set

of

MQA

unique

features

extracted of QA entity mention pairs P as FQA = {fj }jM=Q1 A . As our

embedding learning process will combine these two sets of features

and their shared ones will act as the bridge of two embedding spaces,

we denote the overall feature set as F = {fj }jM=1.

Heterogeneous Network Construction. After the nodes generation process, we construct a heterogeneous network connected by text features, relation mentions, relation types, questions, QA entity mention pairs, as shown in the second column of Figure 2.

3.2 Joint RE and QA Embedding
This section first introduces how we model different types of interactions between linkable relation mentions Z, QA entity mention pairs P, relation type labels R and text features F into a ddimensional relation vector space and a d-dimensional QA pair vector space. In the relation vector space, objects whose types are close to each other should have similar representation and in the QA pair vector space, positive QA mention pairs who share the same question are close to each other. (e.g., see the 3rd col. in Figure 2). We then combine multiple objectives and formulate a joint optimization problem.

We propose a novel global objective, which employs a marginbased rank loss [30] to model noisy mention-type associations and utilizes the second-order proximity idea [44] to model mentionfeature (QA pair-feature) co-occurrences. In particular, we adopt a pairwise margin loss, following the intuition of pairwise rank [34] to capture the interactions between QA pairs, and the shared features Fz ∩ FQA between relation mentions Z and QA pairs P connect the two vector spaces.
Modeling Types of Relation Mentions. We introduce the concepts of both mention-feature co-occurrences and mention-type associations in the modeling of relation types for relation mentions in set Z .
The first hypothesis involved in modeling types of relation mentions is as follows.
Hypothesis 1 (Mention-Feature Co-occurrence). If two relation mentions share many text features, they tend to share similar types (close to each other in the embedding space). If two features co-occur with a similar set of relation mentions, they tend to have similar embedding vectors.

This is based on the intuition that if two relation mentions share
many text features, they have high distributional similarity over the set of text features Fz and likely they have similar relation types. On the other hand, if text features co-occur with many relation
mentions in the corpus, such features tend to represent close type semantics. For example, in sentences s1 and s4 in the first column of Figure 2, the two relation mentions (“Donald Trump”, “United States”, s1) and (“Jinping Xi”, “China”, s4) share many text features including “BETWEEN_President” and they indeed have the same
relation type “president_of” Formally, let vectors zi , cj ∈ Rd represent relation mention
zi ∈ Z and text feature fj ∈ Fz in the d-dimensional relation embedding space. Similar to the distributional hypothesis [25] in
text corpora, we apply second-order proximity [44] to model the
idea in Hypothesis 1 as follows.

LZ F = −

wi j · log p(fj |zi ),

(1)

zi ∈Z fj ∈ Fz

where p(fj |zi ) = exp(zTi cj ) f ′∈Fz exp(zTi cj′ ) denotes the probability of fj generated by zi , and wij is the co-occurrence frequency
between (zi, fj ) in corpus D.
For the goal of efficient optimization, we apply negative sampling
strategy [25] to sample multiple false features for each (zi, fj ) based on some noise distribution Pn (f ) ∝ Df3/4 [25] (with Df denotes the number of relation mentions co-occurring with f ). Term log p(fj |zi )
in Eq. (1) is replaced with the term as follows.

V

log σ (zTi cj ) + Efj′ ∼Pn (f ) log σ (−zTi cj′ ) ,

(2)

v =1

where σ (x ) = 1/ 1 + exp(−x ) is the sigmoid function. The first term in Eq. (2) models the observed co-occurrence, and the second term models the V negative feature samples.
In DL, each relation mention zi is associated with a set of candidate types Ri in a context-agnostic setting, which leads to some false associations between zi and r ∈ Ri (i.e., false positives). For example, in the first column of Figure 2, the two relation mentions

Indirect Supervision for Relation Extraction using Question-Answer Pairs WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

Feature
Entity mention (EM) head Entity Mention Token Tokens between two EMs Part-of-speech (POS) tag Collocations Entity mention order Entity mention distance Entity mention context Special pattern Brown cluster (learned on D)

Description
Syntactic head token of each entity mention Tokens in each entity mention Each token between two EMs POS tags of tokens between two EMs Bigrams in left/right 3-word window of each EM Whether EM 1 is before EM 2 Number of tokens between the two EMs Unigrams before and after each EM Occurrence of pattern “em1_in_em2” Brown cluster ID for each token

Example
“HEAD_EM1_Trump” “TKN_EM1_Donald” “is”, “the”, “current”, “President”, “of ”, “the” “VBZ”, “DT ”, “JJ”, “NN ”, “IN ”, “DT ” “NYC native”, “native Donald”, ... “EM1_BEFORE_EM2” “EM_DISTANCE_6” “native”, “is”, “the”, “.” “PATTERN_NULL” “8_1101111”, “12_111011111111”

Table 1: Text features for relation mentions used in this work [36, 54] (excluding dependency parse-based features and entity type features). (“Donald Trump”, “United States”) is used as an example relation mention from the sentence “NYC native Donald Trump is the current President of the United States.”.

(“Donald Trump”, “United States”, s1) and (“Donald Trump”, “USA”, s2) are assigned to the same relation types while each mention actually only has one true type. To handle such conflicts, we use the
following hypothesis to model the associations between each linkable relation mention zi (in set Z) and its noisy candidate relation type set Ri .

Hypothesis 2 (Partial-Label Association). A relation mention’s embedding vector should be more similar (closer in the lowdimensional space) to its “most relevant” candidate type, than to any other non-candidate type.

Let vector rk ∈ Rd denote relation type rk ∈ R in the embedding space, the similarity between (zi, rk ) is defined as the dot product of their embedding vectors, i.e., ϕ(zi, rk ) = zTi rk . Ri = R \ Ri denotes the set of non-candidate types. We extend the margin-based loss
in [30] to define a partial-label loss ℓi for each linkable relation
mention zi ∈ ML as follows.

ℓi = max 0, 1 − max ϕ(zi , r ) − max ϕ(zi , r ′) . (3)

r ∈Ri

r ′ ∈Ri

To comprehensively model the types of relation mentions, we integrate the modeling of mention-feature co-occurrences and mention-type associations by the following objective, so that feature embeddings also participate in modeling the relation type embeddings.

NZ

λ NZ

2 λ Kr

2

OZ = LZ F + ℓi + 2 ∥zi ∥2 + 2 ∥rk ∥2 ,

(4)

i =1

i =1

k =1

where tuning parameter λ > 0 on the regularization terms is used to control the scale of the embedding vectors.

Modeling Associations between QA Entity Mention Pairs. We
follow Hypothesis 1 to model the QA pair-feature co-occurrence in a similar way. Formally, let vectors pi, c′j ∈ Rd represent QA entity mention pair pi ∈ P and text features (for entity mentions) fj ∈ FQAin a d-dimensional QA entity pair embedding space, respectively. We model the corpus-level co-occurrences between QA
entity mention pairs and text features by second-order proximity
as follows.

LPF = −

wi j · log p(fj |pi ),

(5)

pi ∈ P fj ∈ FQ A

where the term log p(fj |pi ) is defined as log p(fj |pi ) = log σ (pTi c′j ) +

V
v =1 Efj′ ∼Pn (f )

log σ (−pTi c′j′ ) .

For each QA entity mention pair, if we consider it as a relation

mention with an unknown type, intuitively, positive pairs sharing

a same question are relation mentions with the same relation type

or more specifically, are semantically similar relation mentions. In

contrast, a positive pair and a negative pair for a question should

be semantically far away from each other. For example, in Figure 3,

the embeddings of the entity mention pair in answer sentence A1 should be close to the pair in A2 while far away from the pair in A3. To impose such idea, we model the interactions between QA

entity mention pairs based on the following hypothesis.

Hypothesis 3 (QA Pairwise Interaction). A positive QA entity mention pair’s embedding vector should be more similar (closer in the low-dimensional space) to any other positive QA entity mention pair, than to any negative QA entity mention pair of the same question.
Specifically, we use vector pk ∈ Rd to represent a positive QA entity mention pair pk in the embedding space. The similarity between two QA entity mention pairs pk1 and pk2 is defined as the dot product of their embedding vectors. For a positive QA entity mention pair pk of a question qi (e.g. pk ∈ Pi+), we define the pairwise margin-based loss as follows.

ℓi,k =
pk1 ∈ Pi+,pk2 ∈ Pi−,k1

max
k

0, 1 −

ϕ(pk , pk1 ) − ϕ(pk , pk2 ) . (6)

To integrate both the modeling of QA pair-feature co-occurrence and QA pairs interaction, we formulate the following objective.

NQ Ni+

λ NP

OQA = LPF +

ℓi,k +

∥pk ∥22.

(7)

i=1 k =1

2 k=1

By doing so, we can extend the semantic relationships between QA pairs to feature embeddings, such that features of close QA pairs also have similar embeddings. Thus, the learned embeddings of text features from QA corpus carry semantic information inferred from QA pairs. The shared features can propagate such extra semantic knowledge into relation vector space and help better learn the semantic embeddings of both text features and relation types. While

WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

Algorithm 1: Model Learning of ReQuest

Input: labeled training corpus DL , text features { F }, regularization parameter λ, learning rate α , number of negative samples V ,

dim. d

Output: relation mention/QA entity mention pair embeddings {zi }/{pk }, feature embeddings {cj }, {c′j }, relation type embedding {rk }
1 Initialize: vectors {zi },{pk },{cj },{c′j },{rk } as random vectors

2 while O in Eq. (8) not converge do

3 Sample one component Ocur from {OZ , OQA }

4

if Ocur is OZ then

5

Sample a mention-feature co-occurrence wi j ; draw V

negative samples; update {z, c} based on LZ F

6

Sample a relation mention zi ; get its candidate types Ri ;

update z and {r} based on OZ − LZ F

7 end

8

if Ocur is OQ A then

9

Sample a pair-feature co-occurrence wi j ; draw V negative

samples; update {p, c′} based on LP F

10

Sample an positive QA entity mention pair pk of question qi ;

sample one more positive pair and one negative pair of

question qi ; update p based on OQA − LP F

11

end

12 end

feature embeddings of both false positive or false negative examples in the training corpus can deviate towards unrepresentative relation types, the transmitted knowledge from QA space has the potential to adjust such semantic inconsistency. For example, as illustrated in Figure 3, the false labeled examples in s2 and s3 lead the features “BETWEEN_flight” and “BETWEEN_native” to be close to “citizen_of” and “None” type respectively. After injecting the QA pairwise interactions from the example question, these wrongly placed features are brought back towards the relation types they actually indicate. Minimizing the objective OQA yields an QA pair embedding space where, in that space, positive QA mention pairs who share the same question are close to each other.
A Joint Optimization Problem. Our goal is to embed all the available information for relation mentions and relation types, QA entity mention pairs and text features into a single d-dimensional embedding space. An intuitive solution is to collectively minimize the two objectives OZ and OQA as the embedding vectors of overlapped text features are shared across relation vector space and QA pair vector space. To achieve the goal, we formulate a joint optimization problem as follows.

min

O = OZ + OQA.

(8)

{zi }, {cj }, {rk }, {pk }, {c′j }

When optimizing the global objective O, the learning of RE and QA embeddings can be mutually influenced as errors in each component can be constrained and corrected by the other. This mutual enhancement also helps better learn the semantic relations between features and relation types. We apply edge sampling strategy [44] with a stochastic sub-gradient descent algorithm [40] to efficiently solve Eq. (8). In each iteration, we alternatively sample from each of the two objectives {OZ , OM } a batch of edges (e.g., (zi, fj )) and their negative samples, and update each embedding vector based on the derivatives. The detailed learning process of ReQuest can

Z. Wu et al.
be seen in Algorithm 1. To prove convergence of this algorithm (to the local minimum), we can adopt the proof procedure in [40].
3.3 Type Inference
To predict the type for each test relation mention z, we search for nearest neighbor in the target relation type set R, with the learned embeddings of features and relation types (i.e., {ci }, {c′i }, {rk }). Specifically, we represent test relation mention z in our learned relation embedding space by z = fj ∈Fz (z) cj where Fz (z) is the set of text features extracted from z’s local context s. We categorize z to None type if the similarity score is below a pre-defined threshold (e.g. η > 0).
4 EXPERIMENTS
4.1 Data Preparation and Experiment Setting
Our experiments consists of two different type of datasets, one for relation extraction and another answer sentence selection dataset for indirect supervision. Two public datasets are used for relation extraction: NYT [16, 36]and KBP [10, 22]. The test data are manually annotated with relation types by their respective authors. Statistics of the datasets are shown in Table 2. Automatically generated training data by distant supervision on these two training corpora have been used in [35, 36] and is accessible via public links, as well as the test data1. The automatic data generation process is the same as described in Section 2 by utilizing DBpedia Spotlight2, a state-of-the-art entity disambiguation tool, and Freebase, a large entity knowledge base. As for QA dataset, we use the answer sentence selection dataset extracted from TREC-QA dataset [47] used by many researchers [8, 43, 48]. We obtain the compiled version of the dataset from [52, 53], which can be accessed via publicly available link3. Then, we parse this QA dataset to generate QA entity mention pairs following the steps described in Section 3.1. During this procedure, we drop the question or answer sentences where no valid QA entity mention pairs can be found. The statistics of this dataset is presented in Table 3.
Feature Generation. This step is run on both relation extraction dataset and preprocessed QA entity mention pairs and sentences. Table 1 lists the set of text features of both relation mentions and QA entity mention pairs used in our experiments. We use a 6-word window to extract context features for each mention (3 words on the left and the right). We apply the Stanford CoreNLP tool [23] to get POS tags. Brown clusters are derived for each corpus using public implementation4. The same kinds of features are used in all the compared methods in our experiments. As the overlapped features in both RE and QA datasets play an important role in the optimization process, we put the statistics of the shared features in Table 4.
Evaluation Sets. The provided train/test split are used in NYT and KBP relation extraction datasets. The relation mentions in test data have been manually annotated with relation types in the released dataset (see Table 2 for the data statistics). A validation set is created
1 https://github.com/shanzhenren/CoType/tree/master/data/source 2 http://spotlight.dbpedia.org/ 3 https://github.com/xuchen/jacana/tree/master/tree- edit- data 4 https://github.com/percyliang/brown- cluster

Indirect Supervision for Relation Extraction using Question-Answer Pairs WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

Data sets
#Relation types #Documents #Sentences #Training RMs #Text features #Test Sentences #Ground-truth RMs

NYT
24 294,977 1.18M 353k 2.6M 395 3,880

KBP
19 780,549 1.51M 148k 1.3M 289 2,209

Table 2: Statistics of relation extraction datasets.

Versions of QA dataset
#Questions #Positive Answer Sentences #Negative Answer Sentences #Positive entity mention pairs #Negative entity mention pairs

COMPLETE
1.4K 6.9K 49K -

FILTERED
186 969 5.5K 969 28K

Table 3: Statistics of the answer sentence selection datasets. The complete version is the raw corpus we obtain from the public link. The filtered version is the input to ReQuest after dropping sentences where no valid QA entity mention pair can be found.

Data sets
% distinct shared features with TREC QA % occurrences of shared features with TREC QA

NYT
10.0% 90.1%

KBP
11.6% 85.6%

Table 4: Statistics of overlapped features. For example, if we have
the following observations in NYT and TREC QA respectively:
(f1, f1, f1, f2, f3) and (f1, f2, f4), then % distinct shared features with TREC QA of NYT is 66.7% (f1, f2) and % occurrences of shared features with TREC QA of NYT is 80.0%.
through randomly sampling 10% of relation mentions from test data, and the rest are used as evaluation set.
Compared Methods. We compare ReQuest with its variants which model parts of the proposed hypotheses. Several state-of-the-art relation extraction methods (e.g., supervised, embedding, neural network) are also implemented (or tested using their published codes): (1) DS+Perceptron [22]: adopts multi-label learning on automatically labeled training data DL. (2) DS+Kernel [28]: applies bag-of-feature kernel [28] to train a SVM classifier using DL; (3) DS+Logistic [26]: trains a multi-class logistic classifier5 on DL; (4) DeepWalk [31]: embeds mention-feature co-occurrences and mention-type associations as a homogeneous network (with binary edges); (5) LINE [44]: uses second-order proximity model with edge sampling on a feature-type bipartite graph (where edge weight wjk is the number of relation mentions having feature fj and type rk ); (6) MultiR [16]: is a state-of-the-art distant supervision method, which models noisy label in DL by multi-instance multi-label learning; (7) FCM [12]: adopts neural language model to perform compositional embedding; (8) DS+SDP-LSTM [50, 51]: current state-of-the-art in SemEval 2010 Task 8 relation classification task [14], leverages a multi-channel input along the shortest dependency path between two entities into stacked deep recurrent neural network model. We
5We use liblinear package from https://github.com/cjlin1/liblinear

Relation Mention

.. traveling to Amman ,

Jordan ..

The

photograph

showed Gov. Ernie

Fletcher of Kentucky ..

.. as chairman of the

Securities and Exchange

Commission , Christo-

pher Cox ..

ReQuest /location/location/contains /people/person/place_lived
/business/person/company

CoType-RM None None
None

Table 5: Case Study.
use DL to train the model. (9) DS+LSTM-ER [27]: current stateof-the-art model on ACE2005 and ACE2004 relation classification task [7, 20]. It is a multi-layer LSTM-RNN based model that captures both word sequence and dependency tree substructure information. We use DL to train the model. (10) CoType-RM [35]: A distant supervised model which adopts the partial-label loss to handle label noise and train the relation extractor.
Besides the proposed joint optimization model, ReQuest-Joint, we conduct experiments on two other variations to compare the performance (1) ReQuest-QA_RE: This variation optimizes objective OQA first and then uses the learned feature embeddings as the initial state to optimize OZ ; and (2) ReQuest-RE_QA: It first optimizes OZ , then optimizes OQA to finely tune the learned feature embeddings.
Parameter Settings. In the testing of ReQuest and its variants, we set η = 0.35 and λ = 10−4 and V = 3 based on validation sets. We stop further optimization if the relative change of O in Eq. (8) is smaller than 10−4. The dimensionality of embeddings d is set to 50 for all embedding methods. For other parameters, we tune them on validation sets and picked the values which lead to the best performance.
Evaluation Metrics. We adopt standard Precision, Recall and F1 score [2, 28] for measuring the performance of relation extraction task. Note that all our evaluations are sentence-level or mention-level (i.e., context-dependent), as discussed in [16].
4.2 Experiments and Performance Study
Performance Comparison with Baselines. To test the effectiveness of our proposed framework ReQuest, we compare with other methods on the relation extraction task. The precision, recall, F1 scores as well as the model learning time measured on two datasets are reported in Table 6. As shown in the table, ReQuest achieves superior F1 score on both datasets compared with other models. Among all these baselines, MultiR and CoType-RM handle noisy training data while the remaining ones assume the training corpus is perfectly labeled. Due to their nature of being cautious towards the noisy training data, both MultiR and CoType-RM reach relatively high results confronting with other models that blindly exploit all heuristically obtained training examples. However, as external reliable information sources are absent and only the noise from multi-label relation mentions (while none or only one assigned label is correct) is tackled in these models, MultiR and CoTypeRM underperform ReQuest. Especially from the comparison with CoType-RM, which is also an embedding learning based relation

WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

Z. Wu et al.

Method
DS+Perceptron [22] DS+Kernel [28] DS+Logistic [26] DeepWalk [31] LINE [44] MultiR [16] FCM [12] DS+SDP-LSTM [50, 51] DS+LSTM-ER [27] CoType-RM [35] ReQuest-QA_RE ReQuest-RE_QA ReQuest-Joint

Prec
0.068 0.095 0.258 0.176 0.335 0.338
0.553
0.307 0.373 0.467 0.407 0.435 0.404

NYT [16, 36]

Rec

F1

0.641
0.490 0.393 0.224 0.329 0.327 0.154 0.532 0.171 0.380 0.437 0.419 0.480

0.123 0.158 0.311 0.197 0.332 0.333 0.240 0.389 0.234 0.419 0.422 0.427
0.439

Time
15min 56hr 25min 1.1hr 2.3min 5.8min 1.3hr 21hr 49hr 2.6min 10.2min 8.0min 4.0min

Prec
0.233 0.108 0.296 0.101 0.360 0.325 0.151 0.249 0.338 0.342
0.459
0.356 0.386

KBP [10, 22]

Rec

F1

0.457 0.239 0.387 0.296 0.257 0.278
0.500
0.300 0.106 0.339 0.300 0.352 0.410

0.308 0.149 0.335 0.150 0.299 0.301 0.301 0.272 0.161 0.340 0.363 0.354
0.397

Time
7.7min 9.8hr 14min 27min 1.5min 4.1min 25min 10hr 30hr 1.5min 5.3min 13.2min 5.9min

Table 6: Performance comparison on end-to-end relation extraction (at the highest F1 point) on the two datasets.

extraction model with the idea of partial-label loss incorporated, we can conclude that the extra semantic inklings provided by the QA corpus do help boost the performance of relation extraction.
Performance Comparison with Ablations. We experiment with two variations of ReQuest, ReQuest-QA_RE and ReQuest-RE_QA, in order to validate the idea of joint optimization. As presented in Table 6, both ReQuest-QA_RE and ReQuest-RE_QA outperform most of the baselines, with the indirect supervision from QA corpus. However, their results still fall behind ReQuest’s. Thus, separately training the two components may not capture as much information as jointly optimizing the combined objective. The idea of constraining each component in the joint optimization process proves to be effective in learning embeddings to present semantic meanings of objects (e.g. features, types and mentions).
4.3 Case Study
Example Outputs. We have done some interesting investigations regarding the type of prediction errors that can be corrected by the indirection supervision from QA corpus. We have analyzed the prediction results on NYT dataset from CoType-RM and ReQuest and find out the top three target relation types that can be corrected by ReQuest are “contains_location”, “work_for”, “place_lived”. Both the issues of KB incompleteness and context-agnostic labeling are severe for these relation types. For example, there can be lots of not that well-known suburban areas belonging to a city, a state or a country while not marked in KB. And a person can has lived in tens or even hundreds places for various lengths of period. These are hard to be fully annotated into a KB. Thus, the automatically obtained training corpus may end up containing a large percentage of false negative examples for such relation types. On the other hand, there are abundant entity pairs having both “contains_location” and “capital_of”, or both “place_lived” and “born_in” relation types in KB. Naturally, training examples of such entity pairs can be greatly polluted by false positives. In this case, it becomes tough to learn semantic embeddings for relevant features of these relation types. However, we notice there are quite a few answer sentences for relevant questions like “Where is XXX located”, “Where did

0.4 0.365
0.35

0.348

0.358

0.386

0.397

F1 Score

0.3 P_NP-N_NP P_NP-N_NER DepPath

NFromP

ReQuest

Figure 4: Effect of QA dataset processing on F1 scores. P_NP-N_NP: positive QA noun phrase pairs + negative QA noun phrase pairs, P_NP-N_NER: positive QA noun phrase pairs + negative QA named entity pairs, DepPath: convert QA sentences to dep paths, NFromP: sample negative QA pairs from both positive and negative answer sentences

XXX live”, “What company is XXX with” in the QA corpus, which plays an important role in adjusting vectors for features that are supposed to be the indicators for these relation types. Table 5 shows some prediction errors from CoType-RM that are fixed in ReQuest.

Study the effect of QA dataset processing on F1 scores. As stated in Section 3.1, ReQuest uses Stanford NER to extract entity mentions in QA dataset and all QA pairs consist of two entity mentions and if either question or answer entity mention is not found, it drops the sentence. Beyond that, we have conducted experiments with four other ways to construct QA pairs from the raw QA sentences. As shown in Table 3, we lose many positive QA pairs if we only remain answer (or question) targets that are detected as named entities. Thus, we have tried to keep more positive pairs by relaxing the restriction from named entities to noun phrases. In addition, we have tried to evaluate the performance by 1) keeping negative pairs as named entity pairs or 2) changing them to noun phrase pairs. Besides that, inspired by [50, 51], the third processing variation we have tried is to parse the QA sentences into dependency paths and to extract features from these paths instead of the

Indirect Supervision for Relation Extraction using Question-Answer Pairs WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA

full sentences. The last one is that, we sample negative QA pairs not only from negative answer sentences, but also from positive sentence when extracting QA pairs. However, ReQuest achieves highest F1 score compared with these four processing variations (as shown in Figure 4) by filtering out all non entity mention answers, keeping full sentences and extracting only positive QA pairs from positive answer sentences.
Although by doing so, ReQuest filters out a large number of question/answer sentences and fewer QA pairs are constructed to provide semantic knowledge for RE, the remaining QA pairs provide cleaner and more consistent information with RE dataset. Thus, it still outperforms the other variations. Another interesting highlight is the comparison between using negative named entity pairs and using negative noun phrase pairs when positive QA pairs are formed by noun phrases. Although enforcing named entities is more consistent with RE datasets, a trade-off exists when the data format of positive and negative QA pairs are inconsistent. As we can see from the bar chart, the performance by using negative noun phrase pairs is better than negative named entity pairs.
5 RELATED WORK
Classifying relation types between entities in a certain sentence and automatically extracting them from large corpora plays a key role in information extraction and natural language processing applications and thus has been a hot research topic recently. Even though many existing knowledge bases are very large, they are still far from complete. A lot of information is hidden in unstructured data, such as natural language text. Most tasks focus on knowledge base completion (KBP) [42] as a goal of relation extraction from corpora like New York Times (NYT) [36]. Others extract valuable relation information from community question-answer texts, which may be unique to other sources [39].
For supervised relation extraction, feature-based methods [14] and neural network techniques [9, 41] are most common. Most of them jointly leverage both semantic and syntactic features [27], while some use multi-channel input information as well as shortest dependency path to narrow down the attention [50, 51]. Two of he aforementioned papers perform the best on the SemEval-2010 Task 8 and constitutes our neural baseline methods.
However, most of these methods require large amount of annotated data, which is time consuming and labor intensive. To address this issue, most researchers align plain text with knowledge base by distant supervision [26] for relation extraction. However, distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the wrong labeling problem, multi-instance and multi-label learning are used [16, 36]. Others [20, 35] propose joint extraction of typed entities and relations as joint optimization problem and posing cross-constraints of entities and relations on each other. Neural models with selective attention [21] are also proposed to automatically reduce labeling noise.
The distant supervision provides one solution to the cost of massive training data. However, traditional DS methods mostly only exploit one specific kind of indirect supervision knowledge - the relations/facts in a given knowledge base, thus often suffer from the problem of lack of supervision. There exist other indirect supervision methods for relation extraction, where some utilize globally

and cross sentence boundary supervision [13, 33], some leverage the power of passage retrieval model for providing relevance feedback on sentences [49], and others [3, 32, 45]. Recently, with the prevalence of reinforcement learning applications, many information extraction and relation extraction tasks have adopted such techniques to boost existing approaches [18, 29]. Our methodology follows the success of indirect supervision, by adding question-answering pairs as another source of supervision for relation extraction task along with knowledge base auto-labeled distant supervision as well as partial supervision.
Another indirect supervision source we use in the paper, passage retrieval, as described here, is the task of retrieving only the portions of a document that are relevant to a particular information need. It could be useful for limiting the amount of non-relevant material presented to a searcher, or for helping the searcher locate the relevant portions of documents more quickly. Passage retrieval is also often an intermediate step in other information retrieval tasks, like question answering [11, 17, 19, 38] and combining with summarization. Some passage retrieval approaches [46] include calculating query-likelihood and relevance modeling [5], others show that language model approaches used for document retrieval can be applied to answer passage retrieval [6]. Following the success of passage retrieval usage in question-answering pipelines, to the best of our knowledge, we are the first to utilize passage retrieval, or specifically, answer sentence selection from question-answer pairs to provide additional indirect feedback and supervision for relation extraction task.
6 CONCLUSION
We present a novel study on indirect supervision (from questionanswering datasets) for the task of relation extraction. We propose a framework, ReQuest, that embeds information from both training data automatically generated by linking to knowledge bases and QA datasets, and captures richer semantic knowledge from both sources via shared text features so that better feature embeddings can be learned to infer relation type for test relation mentions despite the noisy training data. Our experiment results on two datasets demonstrate the effectiveness and robustness of ReQuest. Interesting future work includes identifying most relevant QA pairs for target relation types, generating most effective questions to collect feedback (or answers) via crowd-sourcing, and exploring approaches other than distant supervision [1, 37].
ACKNOWLEDGMENTS
Research was sponsored in part by the U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-1741317, and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.
REFERENCES
[1] Yoav Artzi and Luke S. Zettlemoyer. 2013. Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions. TACL 1 (2013), 49–62.

WSDM 2018, February 5–9, 2018, Marina Del Rey, CA, USA
[2] Nguyen Bach and Sameer Badaskar. 2007. A Review of Relation Extraction. In Literature review for Language and Statistics II.
[3] Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open Information Extraction from the Web. In IJCAI.
[4] Yee Seng Chan and Dan Roth. 2010. Exploiting background knowledge for relation extraction. In COLING.
[5] Charles L. A. Clarke, Gordon V. Cormack, D. I. E. Kisman, and Thomas R. Lynam. 2000. Question Answering by Passage Selection (MultiText Experiments for TREC-9). In TREC.
[6] Andres Corrada-Emmanuel, W. Bruce Croft, and Vanessa Murdock. 2003. Answer Passage Retrieval for Question Answering. In Tech. Reports of CIIR UMass.
[7] George R. Doddington, Alexis Mitchell, Mark A. Przybocki, Lance A. Ramshaw, Stephanie Strassel, and Ralph M. Weischedel. 2004. The Automatic Content Extraction (ACE) Program - Tasks, Data, and Evaluation. In LREC.
[8] Cícero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive Pooling Networks. arXiv preprint arXiv:1602.03609.
[9] Javid Ebrahimi and Dejing Dou. 2015. Chain Based RNN for Relation Classification. In NAACL.
[10] Joe Ellis, Jeremy Getman, Justin Mott, Xuansong Li, Kira Griffitt, Stephanie M Strassel, and Jonathan Wright. 2014. Linguistic Resources for 2013 Knowledge Base Population Evaluations. In TAC.
[11] David Elworthy. 2000. Question Answering Using a Large NLP System. In TREC. [12] Matthew R Gormley, Mo Yu, and Mark Dredze. 2015. Improved relation extraction
with feature-rich compositional embedding models. In EMNLP. [13] Xianpei Han and Le Sun. 2016. Global Distant Supervision for Relation Extraction.
In AAAI. [14] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó
Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals. In SemEval@ACL. [15] Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In EMNLP. [16] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations. In ACL. [17] Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, Adwait Ratnaparkhi, and Richard J. Mammone. 2000. IBM’s Statistical Question Answering System. In TREC. [18] Pallika H. Kanani and Andrew McCallum. 2012. Selecting actions for resourcebounded information extraction using reinforcement learning. In WSDM. [19] Mahboob Khalid and Suzan Verberne. 2008. Passage Retrieval for Question Answering using Sliding Windows. In IRQA@COLING. 26–33. [20] Qi Li and Heng Ji. 2014. Incremental Joint Extraction of Entity Mentions and Relations. In ACL. [21] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural Relation Extraction with Selective Attention over Instances. In ACL. [22] Xiao Ling and Daniel S Weld. 2012. Fine-Grained Entity Recognition. In AAAI. [23] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In ACL. [24] Pablo N Mendes, Max Jakob, Andrés García-Silva, and Christian Bizer. 2011. DBpedia spotlight: shedding light on the web of documents. In I-Semantics. [25] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. [26] Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL/IJCNLP. [27] Makoto Miwa and Mohit Bansal. 2016. End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures. arXiv preprint arXiv:1601.00770 (2016). [28] Raymond J Mooney and Razvan C Bunescu. 2005. Subsequence kernels for relation extraction. In NIPS. [29] Karthik Narasimhan, Adam Yala, and Regina Barzilay. 2016. Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning. In EMNLP. [30] Nam Nguyen and Rich Caruana. 2008. Classification with partial labels. In KDD. [31] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In KDD. [32] Hoifung Poon and Pedro M. Domingos. 2008. Joint Unsupervised Coreference Resolution with Markov Logic. In EMNLP. [33] Chris Quirk and Hoifung Poon. 2016. Distant Supervision for Relation Extraction beyond the Sentence Boundary. arXiv preprint arXiv:1609.04873 (2016). [34] Jinfeng Rao, Hua He, and Jimmy J. Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In CIKM. [35] Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek F. Abdelzaher, and Jiawei Han. 2017. CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases. In WWW.

Z. Wu et al.
[36] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling Relations and Their Mentions without Labeled Text. In ECML/PKDD.
[37] Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation Extraction with Matrix Factorization and Universal Schemas. In NAACL.
[38] Denis Savenkov and Eugene Agichtein. 2016. When a Knowledge Base Is Not Enough: Question Answering over Knowledge Bases with External Text Data. In SIGIR.
[39] Denis Savenkov, Wei-Lwun Lu, Jeff Dalton, and Eugene Agichtein. 2015. Relation Extraction from Community Generated Question-Answer Pairs. In NAACL.
[40] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming 127, 1 (2011), 3–30.
[41] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In EMNLP.
[42] Mihai Surdeanu and Heng Ji. 2014. Overview of the English Slot Filling Track at the TAC2014 Knowledge Base Population Evaluation. In TAC.
[43] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. Lstmbased deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108 (2015).
[44] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW.
[45] Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text and Knowledge Bases. In EMNLP.
[46] Courtney Wade and James Allan. 2005. Passage Retrieval and Evaluation. In Tech. Reports of DTIC.
[47] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In EMNLP-CoNLL.
[48] Zhiguo Wang and Abraham Ittycheriah. 2015. FAQ-based Question Answering via Word Alignment. arXiv preprint arXiv:1507.02628 (2015).
[49] Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction. In ACL.
[50] Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. 2016. Improved relation classification by deep recurrent neural networks with data augmentation. arXiv preprint arXiv:1601.03651 (2016).
[51] Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths.. In EMNLP.
[52] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In NAACL.
[53] Xuchen Yao, Benjamin Van Durme, and Peter Clark. 2013. Automatic Coupling of Answer Extraction and Information Retrieval. In ACL.
[54] Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring Various Knowledge in Relation Extraction. In ACL.

