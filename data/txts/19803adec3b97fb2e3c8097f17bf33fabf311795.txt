Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach
Yue Yu∗ Simiao Zuo∗ Haoming Jiang Wendi Ren Tuo Zhao Chao Zhang Georgia Institute of Technology, Atlanta, GA, USA
{yueyu,simiaozuo,jianghm,wren44,tourzhao,chaozhang}@gatech.edu

arXiv:2010.07835v3 [cs.CL] 31 Mar 2021

Abstract
Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the ﬁnetuning stage. We study the problem of ﬁnetuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overﬁtting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable ﬁne-tuning LMs with weak supervision. Underpinned by contrastive regularization and conﬁdence-based reweighting, our framework gradually improves model ﬁtting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classiﬁcation tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised ﬁne-tuning methods. Our implementation is available on https:// github.com/yueyu1030/COSINE.
1 Introduction
Language model (LM) pre-training and ﬁne-tuning achieve state-of-the-art performance in various natural language processing tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2019). Such approaches stack task-speciﬁc layers on top of pre-trained language models, e.g., BERT (Devlin et al., 2019), then ﬁne-tune the models with task-speciﬁc data. During ﬁne-tuning, the semantic and syntactic knowledge in the pre-trained LMs is adapted for the target task. Despite their success, one bottleneck for ﬁne-tuning LMs is the requirement of labeled data. When labeled data are scarce, the ﬁne-tuned models often suffer from degraded performance, and the large number of parameters can cause severe overﬁtting (Xie et al., 2019).
∗Equal Contribution.

To relieve the label scarcity bottleneck, we ﬁnetune the pre-trained language models with only weak supervision. While collecting large amounts of clean labeled data is expensive for many NLP tasks, it is often cheap to obtain weakly labeled data from various weak supervision sources, such as semantic rules (Awasthi et al., 2020). For example, in sentiment analysis, we can use rules ‘terrible’→Negative (a keyword rule) and ‘* not recommend *’→Negative (a pattern rule) to generate large amounts of weak labels.
Fine-tuning language models with weak supervision is nontrivial. Excessive label noise, e.g., wrong labels, and limited label coverage are common and inevitable in weak supervision. Although existing ﬁne-tuning approaches (Xu et al., 2020; Zhu et al., 2020; Jiang et al., 2020) improve LMs’ generalization ability, they are not designed for noisy data and are still easy to overﬁt on the noise. Moreover, existing works on tackling label noise are ﬂawed and are not designed for ﬁne-tuning LMs. For example, Ratner et al. (2020); Varma and Ré (2018) use probabilistic models to aggregate multiple weak supervisions for denoising, but they generate weaklabels in a context-free manner, without using LMs to encode contextual information of the training samples (Aina et al., 2019). Other works (Luo et al., 2017; Wang et al., 2019b) focus on noise transitions without explicitly conducting instance-level denoising, and they require clean training samples. Although some recent studies (Awasthi et al., 2020; Ren et al., 2020) design labeling function-guided neural modules to denoise each sample, they require prior knowledge on weak supervision, which is often infeasible in real practice.
Self-training (Rosenberg et al., 2005; Lee, 2013) is a proper tool for ﬁne-tuning language models with weak supervision. It augments the training set with unlabeled data by generating pseudo-labels for them, which improves the models’ generalization power. This resolves the limited coverage issue in

weak supervision. However, one major challenge of self-training is that the algorithm still suffers from error propagation—wrong pseudo-labels can cause model performance to gradually deteriorate.
We propose a new algorithm COSINE1 that ﬁne-tunes pre-trained LMs with only weak supervision. COSINE leverages both weakly labeled and unlabeled data, as well as suppresses label noise via contrastive self-training. Weakly-supervised learning enriches data with potentially noisy labels, and our contrastive self-training scheme fulﬁlls the denoising purpose. Speciﬁcally, contrastive self-training regularizes the feature space by pushing samples with the same pseudo-labels close while pulling samples with different pseudo-labels apart. Such regularization enforces representations of samples from different classes to be more distinguishable, such that the classiﬁer can make better decisions. To suppress label noise propagation during contrastive self-training, we propose conﬁdence-based sample reweighting and regularization methods. The reweighting strategy emphasizes samples with high prediction conﬁdence, which are more likely to be correctly classiﬁed, in order to reduce the effect of wrong predictions. Conﬁdence regularization encourages smoothness over model predictions, such that no prediction can be over-conﬁdent, and therefore reduces the inﬂuence of wrong pseudo-labels.
Our model is ﬂexible and can be naturally extended to semi-supervised learning, where a small set of clean labels is available. Moreover, since we do not make assumptions about the nature of the weak labels, COSINE can handle various types of label noise, including biased labels and randomly corrupted labels. Biased labels are usually generated by semantic rules, whereas corrupted labels are often produced by crowd-sourcing.
Our main contributions are: (1) A contrastiveregularized self-training framework that ﬁne-tunes pre-trained LMs with only weak supervision. (2) Conﬁdence-based reweighting and regularization techniques that reduce error propagation and prevent over-conﬁdent predictions. (3) Extensive experiments on 6 NLP classiﬁcation tasks using 7 public benchmarks verifying the efﬁcacy of COSINE. We highlight that our model achieves competitive performance in comparison with fullysupervised models on some datasets, e.g., on the
1Short for Contrastive Self-Training for Fine-Tuning Pretrained Language Model.

Yelp dataset, we obtain a 97.2% (fully-supervised) v.s. 96.0% (ours) accuracy comparison.
2 Background
In this section, we introduce weak supervision and our problem formulation. Weak Supervision. Instead of using humanannotated data, we obtain labels from weak supervision sources, including keywords and semantic rules2. From weak supervision sources, each of the input samples x ∈ X is given a label y ∈ Y ∪ {∅}, where Y is the label set and ∅ denotes the sample is not matched by any rules. For samples that are given multiple labels, e.g., matched by multiple rules, we determine their labels by majority voting. Problem Formulation. We focus on the weaklysupervised classiﬁcation problems in natural language processing. We consider three types of tasks: sequence classiﬁcation, token classiﬁcation, and sentence pair classiﬁcation. These tasks have a broad scope of applications in NLP, and some examples can be found in Table 1.
Formally, the weakly-supervised classiﬁcation problem is deﬁned as the following: Given weaklylabeled samples Xl = {(xi, yi)}Li=1 and unlabeled samples Xu = {xj}Uj=1, we seek to learn a classiﬁer f (x; θ) : X → Y. Here X = Xl ∪ Xu denotes all the samples and Y = {1, 2, · · · , C} is the label set, where C is the number of classes.
3 Method
Our classiﬁer f = g ◦ BERT consists of two parts: BERT is a pre-trained language model that outputs hidden representations of input samples, and g is a task-speciﬁc classiﬁcation head that outputs a C-dimensional vector, where each dimension corresponds to the prediction conﬁdence of a speciﬁc class. In this paper, we use RoBERTa (Liu et al., 2019) as the realization of BERT.
The framework of COSINE is shown in Figure 1. First, COSINE initializes the LM with weak labels. In this step, the semantic and syntactic knowledge of the pre-trained LM are transferred to our model. Then, it uses contrastive self-training to suppress label noise propagation and continue training.

3.1 Overview

The training procedure of COSINE is as follows.

Initialization with Weakly-labeled Data. We

ﬁne-tune f (·; θ) with weakly-labeled data Xl by

solving the optimization problem

1

min θ |Xl|

CE (f (xi; θ), yi) , (1)

(xi,yi)∈Xl

2Examples of weak supervisions are in Appendix A.

Weak Supervision

Classification Loss 𝐿$ Prediction Feature

Knowledge Bases
Patterns & Dictionaries
Semantic Rules

Matched Samples 𝑋𝑙

Soft pseudoLabel 𝑦$
Initialize

Sharpen

High-conf Samples 𝐶

BEBRETRT
𝑓(# ; 𝜃)

--------- Threshold -----------
Low-conf Samples

All Samples

Unmatched

𝑋𝑙

Input Corpus

Samples 𝑋𝑢

Contrastive Loss 𝑅#
Confidence Regularization 𝑅!

Figure 1: The framework of COSINE. We ﬁrst ﬁne-tune the pre-trained language model on weakly-labeled data with early stopping. Then, we conduct contrastive-regularized self-training to improve model generalization and reduce the label noise. During self-training, we calculate the conﬁdence of the prediction and update the model with high conﬁdence samples to reduce error propagation.

Formulation Sequence Classiﬁcation
Token Classiﬁcation Sentence Pair Classiﬁcation

Example Task
Sentiment Analysis, Topic Classiﬁcation, Question Classiﬁcation
Slot Filling, Part-of-speech Tagging, Event Detection
Word Sense Disambiguation, Textual Entailment, Reading Comprehension

Input [x1, . . . , xN ] [x1, . . . , xN ]
[x1, x2]

Output y
[y1, . . . , yN ] y

Table 1: Comparison of different tasks. For sequence classiﬁcation, input is a sequence of sentences, and we output a scalar label. For token classiﬁcation, input is a sequence of tokens, and we output one scalar label for each token. For sentence pair classiﬁcation, input is a pair of sentences, and we output a scalar label.

where CE(·, ·) is the cross entropy loss. We adopt early stopping (Dodge et al., 2020) to prevent the

model from overﬁtting to the label noise. However,

early stopping causes underﬁtting, and we resolve

this issue by contrastive self-training. Contrastive Self-training with All Data. The

goal of contrastive self-training is to leverage all

data, both labeled and unlabeled, for ﬁne-tuning, as

well as to reduce the error propagation of wrongly

labelled data. We generate pseudo-labels for the

unlabeled data and incorporate them into the train-

ing set. To reduce error propagation, we introduce

contrastive representation learning (Sec. 3.2) and

conﬁdence-based sample reweighting and regular-

ization (Sec. 3.3). We update the pseudo-labels (denoted by y) and the model iteratively. The pro-

cedures are summarized in Algorithm 1. Update y with the current θ. To generate the
pseudo-label for each sample x ∈ X , one straightforward way is to use hard labels (Lee, 2013)

yhard = argmax [f (x; θ)]j .

(2)

j∈Y

Notice that f (x; θ) ∈ RC is a probability vector and [f (x; θ)]j indicates the j-th entry of it. However, these hard pseudo-labels only keep the most

likely class for each sample and result in the prop-

agation of labeling mistakes. For example, if a

sample is mistakenly classiﬁed to a wrong class,

assigning a 0/1 label complicates model updating

(Eq. 4), in that the model is ﬁtted on erroneous

labels. To alleviate this issue, for each sample x

in a batch B, we generate soft pseudo-labels3 (Xie

et al., 2016, 2019; Meng et al., 2020; Liang et al.,

2020) y ∈ RC based on the current model as

[f (x; θ)]2j /fj

yj =

[f (x; θ)]2 /fj ,

(3)

j ∈Y

j

where fj = x ∈B[f (x ; θ)]2j is the sum over soft frequencies of class j. The non-binary soft pseudo-

labels guarantee that, even if our prediction is in-

accurate, the error propagated to the model update

step will be smaller than using hard pseudo-labels. Update θ with the current y. We update the
model parameters θ by minimizing

L(θ; y) = Lc(θ; y) + R1(θ; y) + λR2(θ), (4)

where Lc is the classiﬁcation loss (Sec. 3.3), R1(θ; y) is the contrastive regularizer (Sec. 3.2), R2(θ) is the conﬁdence regularizer (Sec. 3.3), and
λ is the hyper-parameter for the regularization.

3.2 Contrastive Learning on Sample Pairs The key ingredient of our contrastive self-training method is to learn representations that encourage

3More discussions on hard vs.soft are in Sec. 4.5.

Algorithm 1: Training Procedures of COSINE.
Input: Training samples X ; Weakly labeled samples Xl ⊆ X ; Pre-trained LM f (·; θ).
// Fine-tune the LM with weakly-labeled data. for t = 1, 2, · · · , T1 do
Sample a minibatch B from Xl. Update θ by Eq. 1 using AdamW.
// Conduct contrastive self-training with all data. for t = 1, 2, · · · , T2 do
Update pseudo-labels y by Eq. 3 for all x ∈ X . for k = 1, 2, · · · , T3 do
Sample a minibatch B from X . Select high conﬁdence samples C by Eq. 9. Calculate Lc by Eq. 10, R1 by Eq. 6, R2 by
Eq. 12, and L by Eq. 4. Update θ using AdamW.
Output: Fine-tuned model f (·; θ).
Contrastive Learning

High-confidence Sample Pairs

Compact Clusters of Samples

Figure 2: An illustration of contrastive learning. The black solid lines indicate similar sample pairs, and the red dashed lines indicate dissimilar pairs.

data within the same class to have similar representations and keep data in different classes separated. Speciﬁcally, we ﬁrst select high-conﬁdence samples (Sec. 3.3) C from X . Then for each pair xi, xj ∈ C, we deﬁne their similarity as

Wij =

1, if argmax[yi]k = argmax[yj]k

k∈Y

k∈Y

,

0, otherwise

(5)

where yi, yj are the soft pseudo-labels (Eq. 3) for

xi, xj, respectively. For each x ∈ C, we calculate its representation v = BERT(x) ∈ Rd, then we

deﬁne the contrastive regularizer as

R1(θ; y) =

(vi, vj, Wij), (6)

(xi,xj )∈C×C

where

= Wijd2ij + (1 − Wij)[max(0, γ − dij)]2. (7)

Here, (·, ·, ·) is the contrastive loss (Chopra et al., 2005; Taigman et al., 2014), dij is the distance4 between vi and vj, and γ is a pre-deﬁned margin.
For samples from the same class, i.e. Wij = 1, Eq. 6 penalizes the distance between them, and for samples from different classes, the contrastive loss is large if their distance is small. In this way, the regularizer enforces similar samples to be close,

4We use scaled Euclidean distance dij = d1 vi − vj 22 by default. More discussions on Wij and dij are in Appendix E.

while keeping dissimilar samples apart by at least γ. Figure 2 illustrates the contrastive representations. We can see that our method produces clear interclass boundaries and small intra-class distances, which eases the classiﬁcation tasks.

3.3 Conﬁdence-based Sample Reweighting

and Regularization

While contrastive representations yield better de-

cision boundaries, they require samples with high-

quality pseudo-labels. In this section, we introduce

reweighting and regularization methods to suppress

error propagation and reﬁne pseudo-label qualities. Sample Reweighting. In the classiﬁcation task,

samples with high prediction conﬁdence are more

likely to be classiﬁed correctly than those with

low conﬁdence. Therefore, we further reduce label

noise propagation by a conﬁdence-based sample

reweighting scheme. For each sample x with the

soft pseudo-label y, we assign x with a weight

ω(x) deﬁned by

H (y)

C

ω = 1 − log(C) , H(y) = − yi log yi, (8)

i=1

where 0 ≤ H(y) ≤ log(C) is the entropy of y.

Notice that if the prediction conﬁdence is low, then

H(y) will be large, and the sample weight ω(x)

will be small, and vice versa. We use a pre-deﬁned threshold ξ to select high conﬁdence samples C from each batch B as

C = {x ∈ B | ω(x) ≥ ξ}.

(9)

Then we deﬁne the loss function as

Lc(θ, y) = 1 |C|

ω(x)DKL (y f (x; θ)) , (10)

x∈C

where

DKL(P Q) = pk log pk

(11)

k qk

is the Kullback–Leibler (KL) divergence.

Conﬁdence regularization The sample reweight-

ing approach promotes high conﬁdence samples

during contrastive self-training. However, this strat-

egy relies on wrongly-labeled samples to have low

conﬁdence, which may not be true unless we pre-

vent over-conﬁdent predictions. To this end, we

propose a conﬁdence-based regularizer that encour-

ages smoothness over predictions, deﬁned as R2(θ) = 1 DKL (u f (x; θ)) , (12) |C| x∈C
where DKL is the KL-divergence and ui = 1/C for i = 1, 2, · · · , C. Such term constitutes a regu-

larization to prevent over-conﬁdent predictions and

leads to better generalization (Pereyra et al., 2017).

4 Experiments
Datasets and Tasks. We conduct experiments on 6 NLP classiﬁcation tasks using 7 public benchmarks: AGNews (Zhang et al., 2015) is a Topic Classiﬁcation task; IMDB (Maas et al., 2011) and Yelp (Meng et al., 2018) are Sentiment Analysis tasks; TREC (Voorhees and Tice, 1999) is a Question Classiﬁcation task; MIT-R (Liu et al., 2013) is a Slot Filling task; Chemprot (Krallinger et al., 2017) is a Relation Classiﬁcation task; and WiC (Pilehvar and Camacho-Collados, 2019) is a Word Sense Disambiguation (WSD) task. The dataset statistics are summarized in Table 2. More details on datasets and weak supervision sources are in Appendix A5. Baselines. We compare our model with different groups of baseline methods: (i) Exact Matching (ExMatch): The test set is directly labeled by weak supervision sources. (ii) Fine-tuning Methods: The second group of baselines are ﬁne-tuning methods for LMs:
RoBERTa (Liu et al., 2019) uses the RoBERTabase model with task-speciﬁc classiﬁcation heads.
Self-ensemble (Xu et al., 2020) uses selfensemble and distillation to improve performances.
FreeLB (Zhu et al., 2020) adopts adversarial training to enforce smooth outputs.
Mixup (Zhang et al., 2018) creates virtual training samples by linear interpolations.
SMART (Jiang et al., 2020) adds adversarial and smoothness constraints to ﬁne-tune LMs and achieves state-of-the-art result for many NLP tasks. (iii) Weakly-supervised Models: The third group of baselines are weakly-supervised models6:
Snorkel (Ratner et al., 2020) aggregates different labeling functions based on their correlations.
WeSTClass (Meng et al., 2018) trains a classiﬁer with generated pseudo-documents and use selftraining to bootstrap over all samples.
ImplyLoss (Awasthi et al., 2020) co-trains a rulebased classiﬁer and a neural classiﬁer to denoise.
Denoise (Ren et al., 2020) uses attention network to estimate reliability of weak supervisions, and then reduces the noise by aggregating weak labels.
UST (Mukherjee and Awadallah, 2020) is stateof-the-art for self-training with limited labels. It estimates uncertainties via MC-dropout (Gal and Ghahramani, 2015), and then select samples with low uncertainties for self-training.
5Note that we use the same weak supervision signals/rules for both our method and all the baselines for fair comparison.
6All methods use RoBERTa-base as the backbone unless otherwise speciﬁed.

Evaluation Metrics. We use classiﬁcation accuracy on the test set as the evaluation metric for all datasets except MIT-R. MIT-R contains a large number of tokens that are labeled as “Others”. We use the micro F1 score from other classes for this dataset.7 Auxiliary. We implement COSINE using PyTorch8, and we use RoBERTa-base as the pretrained LM. Datasets and weak supervision details are in Appendix A. Baseline settings are in Appendices B. Training details and setups are in Appendix C. Discussions on early-stopping are in Appendix D. Comparison of distance metrics and similarity measures are in Appendix E.
4.1 Learning From Weak Labels
We summarize the weakly-supervised leaning results in Table 3. In all the datasets, COSINE outperforms all the baseline models. A special case is the WiC dataset, where we use WordNet9 to generate weak labels. However, this enables Snorkel to access some labeled data in the development set, making it unfair to compete against other methods. We will discuss more about this dataset in Sec. 4.3.
In comparison with directly ﬁne-tuning the pretrained LMs with weakly-labeled data, our model employs an “earlier stopping” technique10 so that it does not overﬁt on the label noise. As shown, indeed “Init” achieves better performance, and it serves as a good initialization for our framework. Other ﬁne-tuning methods and weakly-supervised models either cannot harness the power of pretrained language models, e.g., Snorkel, or rely on clean labels, e.g., other baselines. We highlight that although UST, the state-of-the-art method to date, achieves strong performance under few-shot settings, their approach cannot estimate conﬁdence well with noisy labels, and this yields inferior performance. Our model can gradually correct wrong pseudo-labels and mitigate error propagation via contrastive self-training.
It is worth noticing that on some datasets, e.g., AGNews, IMDB, Yelp, and WiC, our model achieves the same level of performance with models (RoBERTa-CL) trained with clean labels. This makes COSINE appealing in the scenario where only weak supervision is available.
7The Chemprot dataset also contains “Others” type, but such instances are few, so we still use accuracy as the metric.
8https://pytorch.org/ 9https://wordnet.princeton.edu/ 10We discuss this technique in Appendix D.

Dataset
AGNews IMDB Yelp MIT-R TREC
Chemprot WiC

Task
Topic Sentiment Sentiment Slot Filling Question Relation
WSD

Class
4 2 2 9 6 10 2

# Train
96k 20k 30.4k 6.6k 4.8k 12.6k 5.4k

# Dev
12k 2.5k 3.8k 1.0k 0.6k 1.6k 0.6k

# Test
12k 2.5k 3.8k 1.5k 0.6k 1.6k 1.4k

Cover
56.4 87.5 82.8 13.5 95.0 85.9 63.4

Accuracy
83.1 74.5 71.5 80.7 63.8 46.5 58.8

Table 2: Dataset statistics. Here cover (in %) is the fraction of instances covered by weak supervision sources in the training set, and accuracy (in %) is the precision of weak supervision.

Method

AGNews IMDB Yelp MIT-R TREC Chemprot WiC (dev)

ExMatch Fully-supervised Result RoBERTa-CL (Liu et al., 2019) Baselines RoBERTa-WL† (Liu et al., 2019) Self-ensemble (Xu et al., 2020) FreeLB (Zhu et al., 2020) Mixup (Zhang et al., 2018) SMART (Jiang et al., 2020) Snorkel (Ratner et al., 2020)
WeSTClass (Meng et al., 2018) ImplyLoss (Awasthi et al., 2020) Denoise (Ren et al., 2020) UST (Mukherjee and Awadallah, 2020) Our COSINE Framework Init COSINE

52.31
91.41
82.25 85.72 85.12 85.40 86.12 62.91 82.78 68.50 85.71 86.28
84.63 87.52

71.28
94.26
72.60 86.72 88.04 86.92 86.98 73.22 77.40 63.85 82.90 84.56
83.58 90.54

68.68
97.27
74.89 80.08 85.68 92.05 88.58 69.21 76.86 76.29 87.53 90.53
81.76 95.97

34.93
88.51
70.95 72.88 73.04 73.68 73.66 20.63 ---⊗ 74.30 70.58 74.41
72.97 76.61

60.80
96.68
62.25 66.18 67.33 66.83 68.17 58.60 37.31 80.20 69.20 65.52
65.67 82.59

46.52
79.65
44.80 44.62 45.68 51.59 48.26 37.50 ---⊗ 53.48 50.56 52.14
51.34 54.36

58.80
70.53
59.36 62.71 63.45 64.88 63.55
---∗ 48.59 54.48 62.38 63.48
63.46 67.71

: RoBERTa is trained with clean labels. †: RoBERTa is trained with weak labels. ∗: unfair comparison. ⊗: not applicable.

Table 3: Classiﬁcation accuracy (in %) on various datasets. We report the mean over three runs.

100

Accuracy (in %)

80

60

Init SMART

UST

40

COSINE Fully supervised

40 C5o0rruption6R0atio (in7%0 ) 80

Figure 3: Results of label corruption on TREC. When the corruption ratio is less than 40%, the performance is close to the fully supervised method.

4.2 Robustness Against Label Noise
Our model is robust against excessive label noise. We corrupt certain percentage of labels by randomly changing each one of them to another class. This is a common scenario in crowd-sourcing, where we assume human annotators mis-label each sample with the same probability. Figure 3 summarizes experiment results on the TREC dataset. Compared with advanced ﬁne-tuning and self-training methods (e.g. SMART and UST)11, our model consistently outperforms the baselines.
11Note that some methods in Table 3, e.g., ImplyLoss and Denoise, are not applicable to this setting since they require weak supervision sources, but none exists in this setting.

Model
Human Baseline
BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) T5 (Raffel et al., 2019)
Semi-Supervised Learning
SenseBERT (Levine et al., 2020) RoBERTa-WL† (Liu et al., 2019) w/ MT† (Tarvainen and Valpola, 2017) w/ VAT† (Miyato et al., 2018) w/ COSINE†
Transductive Learning Snorkel† (Ratner et al., 2020) RoBERTa-WL† (Liu et al., 2019) w/ MT† (Tarvainen and Valpola, 2017) w/ VAT† (Miyato et al., 2018) w/ COSINE†

Dev Test 80.0
--- 69.6 70.5 69.9
--- 76.9
--- 72.1 72.3 70.2 73.5 70.9 74.2 71.2 76.0 73.2
80.5 --81.3 76.8 82.1 77.1 84.9 79.5 89.5 85.3

#Params ---
335M 356M 11,000M
370M 125M 125M 125M 125M
1M 125M 125M 125M 125M

Table 4: Semi-supervised Learning on WiC. VAT (Vir-
tual Adversarial Training) and MT (Mean Teacher) are semi-supervised methods. †: has access to weak labels.

4.3 Semi-supervised Learning
We can naturally extend our model to semisupervised learning, where clean labels are available for a portion of the data. We conduct experiments on the WiC dataset. As a part of the SuperGLUE (Wang et al., 2019a) benchmark, this dataset proposes a challenging task: models need to determine whether the same word in different sentences has the same sense (meaning).

Different from previous tasks where the labels in the training set are noisy, in this part, we utilize the clean labels provided by the WiC dataset. We further augment the original training data of WiC with unlabeled sentence pairs obtained from lexical databases (e.g., WordNet, Wictionary). Note that part of the unlabeled data can be weaklylabeled by rule matching. This essentially creates a semi-supervised task, where we have labeled data, weakly-labeled data and unlabeled data.
Since the weak labels of WiC are generated by WordNet and partially reveal the true label information, Snorkel (Ratner et al., 2020) takes this unfair advantage by accessing the unlabeled sentences and weak labels of validation and test data. To make a fair comparison to Snorkel, we consider the transductive learning setting, where we are allowed access to the same information by integrating unlabeled validation and test data and their weak labels into the training set. As shown in Table 4, COSINE with transductive learning achieves better performance compared with Snorkel. Moreover, in comparison with semi-supervised baselines (i.e. VAT and MT) and ﬁne-tuning methods with extra resources (i.e., SenseBERT), COSINE achieves better performance in both semi-supervised and transductive learning settings.
4.4 Case Study
Error propagation mitigation and wrong-label correction. Figure 4 visualizes this process. Before training, the semantic rules make noisy predictions. After the initialization step, model predictions are less noisy but more biased, e.g., many samples are mis-labeled as “Amenity”. These predictions are further reﬁned by contrastive self-training. The rightmost ﬁgure demonstrates wrong-label correction. Samples are indicated by radii of the circle, and classiﬁcation correctness is indicated by color, i.e., blue means correct and orange means incorrect. From inner to outer tori specify classiﬁcation accuracy after the initialization stage, and the iteration 1,2,3. We can see that many incorrect predictions are corrected within three iterations. To illustrate: the right black dashed line means the corresponding sample is classiﬁed correctly after the ﬁrst iteration, and the left dashed line indicates the case where the sample is mis-classiﬁed after the second iteration but corrected after the third. These results demonstrate that our model can correct wrong predictions via contrastive self-training. Better data representations. We visualize sam-

ple embeddings in Fig. 7. By incorporating the contrastive regularizer R1, our model learns more compact representations for data in the same class, e.g., the green class, and also extends the inter-class distances, e.g., the purple class is more separable from other classes in Fig. 7(b) than in Fig. 7(a).
Label efﬁciency. Figure 8 illustrates the number of clean labels needed for the supervised model to outperform COSINE. On both of the datasets, the supervised model requires a signiﬁcant amount of clean labels (around 750 for Agnews and 120 for MIT-R) to reach the level of performance as ours, whereas our method assumes no clean sample.
Higher Conﬁdence Indicates Better Accuracy. Figure 6 demonstrates the relation between prediction conﬁdence and prediction accuracy on IMDB. We can see that in general, samples with higher prediction conﬁdence yield higher prediction accuracy. With our sample reweighting method, we gradually ﬁlter out low-conﬁdence samples and assign higher weights for others, which effectively mitigates error propagation.
4.5 Ablation Study
Components of COSINE. We inspect the importance of various components, including the contrastive regularizer R1, the conﬁdence regularizer R2, and the sample reweighting (SR) method, and the soft labels. Table 5 summarizes the results and Fig. 9 visualizes the learning curves. We remark that all the components jointly contribute to the model performance, and removing any of them hurts the classiﬁcation accuracy. For example, sample reweighting is an effective tool to reduce error propagation, and removing it causes the model to eventually overﬁt to the label noise, e.g., the red bottom line in Fig. 9 illustrates that the classiﬁcation accuracy increases and then drops rapidly. On the other hand, replacing the soft pseudo-labels (Eq. 3) with the hard counterparts (Eq. 2) causes drops in performance. This is because hard pseudo-labels lose prediction conﬁdence information. Hyper-parameters of COSINE. In Fig. 5, we examine the effects of different hyper-parameters, including the conﬁdence threshold ξ (Eq. 9), the stopping time T1 in the initialization step, and the update period T3 for pseudo-labels. From Fig. 5(a), we can see that setting the conﬁdence threshold too big hurts model performance, which is because an over-conservative selection strategy can result in insufﬁcient number of training data. The stop-

Correct Incorrect
Init Iter 1 Iter 2 Iter 3

Figure 4: Classiﬁcation performance on MIT-R. From left to right: visualization of ExMatch, results after the initialization step, results after contrastive self-training, and wrong-label correction during self-training.

Accuracy (in %) Accuracy (in %)

95

AGNews

IMDB 90

92 AGNews

IMDB

90

80

90 88

85

Init COSINE

86

80

70 Clean label

84

0.4 0.5 0.6 0.7 0.8

80 120T1 /1S6t0eps2(I4M0DB)320 400

10 50 1T530/ Ste2p5s0 350 450

Accuracy (in %) Accuracy (in %)

100 80 60 40 20
0 Con0f.i2den0c.e4 Sc0o.6re (I0M.8DB) 1

(a) Effect of ξ.

(b) Effect of T1.

(c) Effect of T3.

Figure 5: Effects of different hyper-parameters.

Figure 6: Accuracy vs. Conﬁdence score.

(a) Embedding w/o R1.

(b) Embedding w/ R1.

Figure 7: t-SNE (Maaten and Hinton, 2008) visualization on TREC. Each color denotes a different class.

90

88

86

84

Supervised w/ Roberta

82

Weak Labels w/ COSINE Weak Labels w/ Init

80

#0 o2f0H0um5a0n0 Anno1ta00te0d Sam15p0le0s

(a) Results on Agnews.

90

80

70

60

Supervised w/ Roberta

50

Weak Labels w/ COSINE Weak Labels w/ Init

40

#0of H50um1a0n0Annot2a0t0ed Sam3p0l0es

(b) Results on MIT-R.

Figure 8: Accuracy vs. Number of annotated labels.

ping time T1 has drastic effects on the model. This is because ﬁne-tuning COSINE with weak labels for excessive steps causes the model to unavoidably overﬁt to the label noise, such that the contrastive self-training procedure cannot correct the error. Also, with the increment of T3, the update period of pseudo-labels, model performance ﬁrst increases and then decreases. This is because if we update pseudo-labels too frequently, the contrastive self-training procedure cannot fully suppress the label noise, and if the updates are too infrequent,

0.8

Accuracy

0.7

0.6 0.5 0.4
0

COSINE w/o R1 w/o R2 w/o SR

w/o R1/R2 w/o R1/R2/SR Direct Training

500 1000 1500 2000 2500 Number of Iterations

Figure 9: Learning curves on TREC with different settings. Mean and variance are calculated over 3 runs.

Method Init COSINE w/o R1 w/o R2 w/o SR w/o R1/R2 w/o R1/R2/SR w/o Soft Label

AGNews 84.63 87.52 86.04 85.91 86.72 86.33 86.61 86.07

IMDB 83.58 90.54 88.32 89.32 87.10 84.44 83.98 89.72

Yelp 81.76 95.97 94.64 93.96 93.08 92.34 82.57 93.73

MIT-R 72.97 76.61 74.11 75.21 74.29 73.67 73.59 73.05

TREC 66.50 82.59 78.28 77.11 79.77 76.95 74.96 71.91

Table 5: Effects of different components. Due to space limit we only show results for 5 representative datasets.

the pseudo-labels cannot capture the updated information well.
5 Related Works
Fine-tuning Pre-trained Language Models. To improve the model’s generalization power during ﬁne-tuning stage, several methods are proposed (Peters et al., 2019; Dodge et al., 2020; Zhu

Accuracy (in %) Accuracy (in %)

et al., 2020; Jiang et al., 2020; Xu et al., 2020; Kong et al., 2020; Zhao et al., 2020; Gunel et al., 2021; Zhang et al., 2021; Aghajanyan et al., 2021; Wang et al., 2021), However, most of these methods focus on fully-supervised setting and rely heavily on large amounts of clean labels, which are not always available. To address this issue, we propose a contrastive self-training framework that ﬁne-tunes pre-trained models with only weak labels. Compared with the existing ﬁne-tuning approaches (Xu et al., 2020; Zhu et al., 2020; Jiang et al., 2020), our model effectively reduce the label noise, which achieves better performance on various NLP tasks with weak supervision. Learning From Weak Supervision. In weaklysupervised learning, the training data are usually noisy and incomplete. Existing methods aim to denoise the sample labels or the labeling functions by, for example, aggregating multiple weak supervisions (Ratner et al., 2020; Lison et al., 2020; Ren et al., 2020), using clean samples (Awasthi et al., 2020), and leveraging contextual information (Mekala and Shang, 2020). However, most of them can only use speciﬁc type of weak supervision on speciﬁc task, e.g., keywords for text classiﬁcation (Meng et al., 2020; Mekala and Shang, 2020), and they require prior knowledge on weak supervision sources (Awasthi et al., 2020; Lison et al., 2020; Ren et al., 2020), which somehow limits the scope of their applications. Our work is orthogonal to them since we do not denoise the labeling functions directly. Instead, we adopt contrastive self-training to leverage the power of pretrained language models for denoising, which is task-agnostic and applicable to various NLP tasks with minimal additional efforts.
6 Discussions
Adaptation of LMs to Different Domains. When ﬁne-tuning LMs on data from different domains, we can ﬁrst continue pre-training on in-domain text data for better adaptation (Gururangan et al., 2020). For some rare domains where BERT trained on general domains is not optimal, we can use LMs pretrained on those speciﬁc domains (e.g. BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019)) to tackle this issue. Scalability of Weak Supervision. COSINE can be applied to tasks with a large number of classes. This is because rules can be automatically generated beyond hand-crafting. For example, we can use label names/descriptions as weak supervision

signals (Meng et al., 2020). Such signals are easy to obtain and do not require hand-crafted rules. Once weak supervision is provided, we can create weak labels to further apply COSINE. Flexibility. COSINE can handle tasks and weak supervision sources beyond our conducted experiments. For example, other than semantic rules, crowd-sourcing can be another weak supervision source to generate pseudo-labels (Wang et al., 2013). Moreover, we only conduct experiments on several representative tasks, but our framework can be applied to other tasks as well, e.g., namedentity recognition (token classiﬁcation) and reading comprehension (sentence pair classiﬁcation).
7 Conclusion
In this paper, we propose a contrastive regularized self-training framework, COSINE, for ﬁnetuning pre-trained language models with weak supervision. Our framework can learn better data representations to ease the classiﬁcation task, and also efﬁciently reduce label noise propagation by conﬁdence-based reweighting and regularization. We conduct experiments on various classiﬁcation tasks, including sequence classiﬁcation, token classiﬁcation, and sentence pair classiﬁcation, and the results demonstrate the efﬁcacy of our model.
Broader Impact
COSINE is a general framework that tackled the label scarcity issue via combining neural nets with weak supervision. The weak supervision provides a simple but ﬂexible language to encode the domain knowledge and capture the correlations between features and labels. When combined with unlabeled data, our framework can largely tackle the label scarcity bottleneck for training DNNs, enabling them to be applied for downstream NLP classiﬁcation tasks in a label efﬁcient manner.
COSINE neither introduces any social/ethical bias to the model nor amplify any bias in the data. In all the experiments, we use publicly available data, and we build our algorithms using public code bases. We do not foresee any direct social consequences or ethical issues.
Acknowledgments
We thank anonymous reviewers for their feedbacks. This work was supported in part by the National Science Foundation award III-2008334, Amazon Faculty Award, and Google Faculty Award.

References
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. 2021. Better ﬁne-tuning by reducing representational collapse. In International Conference on Learning Representations.

Laura Aina, Kristina Gulordava, and Gemma Boleda. 2019. Putting words in context: LSTM language models and lexical ambiguity. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3342–3348.

Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. 2020. Learning from rules generalizing labeled exemplars. In International Conference on Learning Representations.

Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientiﬁc text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615– 3620.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric discriminatively, with application to face veriﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR’05), volume 1, pages 539–546.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.

Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. CoRR, abs/2002.06305.

Yarin Gal and Zoubin Ghahramani. 2015. convolutional neural networks with approximate variational inference. abs/1506.02158.

Bayesian bernoulli
CoRR,

Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. 2021. Supervised contrastive learning for pre-trained language model ﬁne-tuning. In International Conference on Learning Representations.

Suchin Gururangan, Ana Marasovic´, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342–8360.

Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. 2020. SMART: Robust and efﬁcient ﬁne-tuning for pretrained natural language models through principled regularized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2177–2190.
Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, and Chao Zhang. 2020. Calibrated language model ﬁne-tuning for in- and out-ofdistribution data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1326–1340.
Martin Krallinger, Obdulia Rabal, Saber A Akhondi, et al. 2017. Overview of the biocreative VI chemical-protein interaction track. In Proceedings of the sixth BioCreative challenge evaluation workshop, volume 1, pages 141–146.
Dong-Hyun Lee. 2013. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 2.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240.
Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2020. SenseBERT: Driving some sense into BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4656–4667.
Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao Zhang. 2020. Bond: Bert-assisted open-domain named entity recognition with distant supervision. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, page 1054–1064.
Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, and Samia Touileb. 2020. Named entity recognition without labelled data: A weak supervision approach. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1518–1533.
Jingjing Liu, Panupong Pasupat, Yining Wang, Scott Cyphers, and James R. Glass. 2013. Query understanding enhanced by hierarchical parsing structures. In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pages 72– 77. IEEE.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.
Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing Zhu, Songfang Huang, Rui Yan, and Dongyan Zhao. 2017. Learning with noise: Enhance distantly supervised relation extraction with dynamic transition matrix. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 430–439.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150.
Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605.
Dheeraj Mekala and Jingbo Shang. 2020. Contextualized weak supervision for text classiﬁcation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 323–333.
Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2018. Weakly-supervised neural text classiﬁcation. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, page 983–992.
Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. 2020. Text classiﬁcation using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9006–9017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2018. Virtual adversarial training: a regularization method for supervised and semisupervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979– 1993.
Subhabrata Mukherjee and Ahmed Hassan Awadallah. 2020. Uncertainty-aware self-training for text classiﬁcation with few labels. CoRR, abs/2006.15315.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. 2017. Regularizing neural networks by penalizing conﬁdent output distributions. CoRR, abs/1701.06548.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227– 2237.

Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14.
Mohammad Taher Pilehvar and Jose CamachoCollados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267–1273.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. CoRR, abs/1910.10683.
Alexander Ratner, Stephen H. Bach, Henry R. Ehrenberg, Jason A. Fries, Sen Wu, and Christopher Ré. 2020. Snorkel: rapid training data creation with weak supervision. VLDB Journal, 29(2):709–730.
Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao Zhang. 2020. Denoising multi-source weak supervision for neural text classiﬁcation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3739–3754.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. 2005. Semi-supervised self-training of object detection models. WACV/MOTION, 2.
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. Deepface: Closing the gap to human-level performance in face veriﬁcation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pages 1195–1204.
Paroma Varma and Christopher Ré. 2018. Snuba: Automating weak supervision to label training data. Proc. VLDB Endowment, 12(3):223–236.
Ellen M. Voorhees and Dawn M. Tice. 1999. The TREC-8 question answering track evaluation. In Proceedings of The Eighth Text REtrieval Conference, volume 500-246.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3261–3275.

Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013. Perspectives on crowdsourcing annotations for natural language processing. Language resources and evaluation, 47(1):9–31.
Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2021. InfoBERT: Improving robustness of language models from an information theoretic perspective. In International Conference on Learning Representations.
Hao Wang, Bing Liu, Chaozhuo Li, Yan Yang, and Tianrui Li. 2019b. Learning with noisy labels for sentence-level sentiment classiﬁcation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6286–6292.
Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised deep embedding for clustering analysis. In Proceedings of The 33rd International Conference on Machine Learning, pages 478–487.
Qizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. 2019. Unsupervised data augmentation for consistency training. CoRR, abs/1904.12848.
Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang. 2020. Improving BERT ﬁne-tuning via self-ensemble and self-distillation. CoRR, abs/2002.10345.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations.
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. 2021. Revisiting fewsample BERT ﬁne-tuning. In International Conference on Learning Representations.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation. In Advances in Neural Information Processing Systems 28, pages 649–657.
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. 2020. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2226–2241.
Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, and Xiang Ren. 2020. NERO: A neural rule grounding framework for label-efﬁcient relation extraction. In Proceedings of The Web Conference 2020, page 2166–2176.
Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations.

A Weak Supervision Details
COSINE does not require any human annotated examples in the training process, and it only needs weak supervision sources such as keywords and semantic rules. According to some studies in existing works Awasthi et al. (2020); Zhou et al. (2020), such weak supervisions are cheap to obtain and are much efﬁcient than collecting clean labels. In this way, we can obtain signiﬁcantly more labeled examples using these weak supervision sources than human labor.
There are two types of semantic rules that we apply as weak supervisions:
Keyword Rule: HAS(x, L) → C. If x matches one of the words in the list L, we label it as C.
Pattern Rule: MATCH(x, R) → C. If x matches the regular expression R, we label it as C.
In addition to the keyword rule and the pattern rule, we can also use third-party tools to obtain weak labels. These tools (e.g. TextBlob12) are available online and can be obtained cheaply, but their prediction is not accurate enough (when directly use this tool to predict label for all training samples, the accuracy on Yelp dataset is around 60%). We now introduce the semantic rules on each dataset:
AGNews, IMDB, Yelp: We use the rule in Ren et al. (2020). Please refer to the original paper for detailed information on rules.
MIT-R, TREC: We use the rule in Awasthi et al. (2020). Please refer to the original paper for detailed information on rules.
ChemProt: There are 26 rules. We show part of the rules in Table 6.
WiC: Each sense of each word in WordNet has example sentences. For each sentence in the WiC dataset and its corresponding keyword, we collect the example sentences of that word from WordNet. Then for a pair of sentences, the corresponding weak label is “True” if their deﬁnitions are the same, otherwise the weak label is “False”.
12https://textblob.readthedocs.io/en/ dev/index.html.

B Baseline Settings
We implement Self-ensemble, FreeLB, Mixup and UST based on their original paper. For other baselines, we use their ofﬁcial release:
WeSTClass (Meng et al., 2018): https: //github.com/yumeng5/WeSTClass.
RoBERTa (Liu et al., 2019): https: //github.com/huggingface/transformers.
SMART (Jiang et al., 2020): https: //github.com/namisan/mt-dnn.
Snorkel (Ratner et al., 2020): https: //www.snorkel.org/.
ImplyLoss (Awasthi et al., 2020): https:
//github.com/awasthiabhijeet/
Learning-From-Rules. Denoise (Ren et al., 2020): https:
//github.com/weakrules/
Denoise-multi-weak-sources.
C Details on Experiment Setups
C.1 Computing Infrastructure
System: Ubuntu 18.04.3 LTS; Python 3.7; Pytorch 1.2. CPU: Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz. GPU: GeForce GTX TITAN X.
C.2 Hyper-parameters
We use AdamW (Loshchilov and Hutter, 2019) as the optimizer, and the learning rate is chosen from 1 × 10−5, 2 × 10−5, 3 × 10−5}. A linear learning rate decay schedule with warm-up 0.1 is used, and the number of training epochs is 5.
Hyper-parameters are shown in Table 7. We use a grid search to ﬁnd the optimal setting for each task. Speciﬁcally, we search T1 from 10 to 2000, T2 from 1000 to 5000, T3 from 10 to 500, ξ from 0 to 1, and λ from 0 to 0.5. All results are reported as the average over three runs.
C.3 Number of Parameters
COSINE and most of the baselines (RoBERTaWL / RoBERTa-CL / SMART / WeSTClass / SelfEnsemble / FreeLB / Mixup / UST) are built on the RoBERTa-base model with about 125M parameters. Snorkel is a generative model with only a few parameters. ImplyLoss and Denoise freezes the embedding and has less than 1M parameters. However, these models cannot achieve satisfactory performance in our experiments.

Rule
HAS (x, [amino acid,mutant, mutat, replace] ) → part_of
HAS (x, [bind, interact, affinit] ) → regulator
HAS (x, [activat, increas, induc, stimulat, upregulat] ) → upregulator/activator
HAS (x, [downregulat, inhibit, reduc, decreas] ) → downregulator/inhibitor
HAS (x, [ agoni, tagoni]* ) → agonist * (note the leading whitespace in both cases) HAS (x, [antagon] ) → antagonist HAS (x, [modulat, allosteric] ) → modulator
HAS (x, [cofactor] ) → cofactor
HAS (x, [substrate, catalyz, transport, produc, conver] ) → substrate/product HAS (x, [not] ) → not

Example
A major part of this processing requires endoproteolytic cleavage at speciﬁc pairs of basic [CHEMICAL]amino acid[CHEMICAL] residues, an event necessary for the maturation of a variety of important biologically active proteins, such as insulin and [GENE]nerve growth factor[GENE]. The interaction of [CHEMICAL]naloxone estrone azine[CHEMICAL] (N-EH) with various [GENE]opioid receptor[GENE] types was studied in vitro. The results of this study suggest that [CHEMICAL]noradrenaline[CHEMICAL] predominantly, but not exclusively, mediates contraction of rat aorta through the activation of an [GENE]alphalD-adrenoceptor[GENE]. These results suggest that [CHEMICAL]prostacyclin[CHEMICAL] may play a role in downregulating [GENE]tissue factor[GENE] expression in monocytes, at least in part via elevation of intracellular levels of cyclic AMP. Alprenolol and BAAM also caused surmountable antagonism of [CHEMICAL]isoprenaline[CHEMICAL] responses, and this [GENE]beta 1-adrenoceptor[GENE] antagonism was slowly reversible. It is concluded that [CHEMICAL]labetalol[CHEMICAL] and dilevalol are [GENE]beta 1-adrenoceptor[GENE] selective antagonists. [CHEMICAL]Hydrogen sulﬁde[CHEMICAL] as an allosteric modulator of [GENE]ATP-sensitive potassium channels[GENE] in colonic inﬂammation. The activation appears to be due to an increase of [GENE]GAD[GENE] afﬁnity for its cofactor, [CHEMICAL]pyridoxal phosphate[CHEMICAL] (PLP). Kinetic constants of the mutant [GENE]CrAT[GENE] showed modiﬁcation in favor of longer [CHEMICAL]acyl-CoAs[CHEMICAL] as substrates. [CHEMICAL]Nicotine[CHEMICAL] does not account for the CSE stimulation of [GENE]VEGF[GENE] in HFL-1.

Table 6: Examples of semantic rules on Chemprot.

Hyper-parameter AGNews IMDB Yelp MIT-R TREC Chemprot WiC

Dropout Ratio Maximum Tokens
Batch Size Weight Decay Learning Rate
T1 T2 T3 ξ λ

128 32
10−5 160 3000 250 0.6 0.1

256 16
10−5 160 2500 50 0.7 0.05

512 16
10−5 200 2500 100 0.7 0.05

0.1 64 64 10−4 10−5 150 1000 15 0.2 0.1

64 16
10−5 500 2500 30 0.3 0.05

400 24
10−5 400 1000 15 0.7 0.05

256 32
10−5 1700 3000 80 0.7 0.05

Table 7: Hyper-parameter conﬁgurations. Note that we only keep certain number of tokens.

D Early Stopping and Earlier Stopping
Our model adopts the earlier stopping strategy during the initialization stage. Here we use “earlier stopping” to differentiate from “early stopping”, which is standard in ﬁne-tuning algorithms. Early stopping refers to the technique where we stop training when the evaluation score drops. Earlier stopping is self-explanatory, namely we ﬁne-tune

the pre-trained LMs with only a few steps, even before the evaluation score starts dropping. This technique can efﬁciently prevent the model from overﬁtting. For example, as Figure 5(b) illustrates, on IMDB dataset, our model overﬁts after 240 iterations of initialization with weak labels. In contrast, the model achieves good performance even after 400 iterations of ﬁne-tuning when using clean labels. This veriﬁes the necessity of earlier stopping.

Distance d

Euclidean

Cos

Similarity W Hard KL-based L2-based Hard KL-based

AGNews MIT-R

87.52 76.61

86.44 76.68

86.72 76.49

87.34 76.55

86.98 76.76

Table 8: Performance of COSINE under different settings.

L2-based
86.55 76.58

E Comparison of Distance Measures in Contrastive Learning
The contrastive regularizer R1(θ; y) is related to two designs: the sample distance metric dij and the sample similarity measure Wij. In our implementation, we use the scaled Euclidean distance as the default for dij and Eq. 5 as the default for Wij13. Here we discuss other designs.

E.1 Sample distance metric d
Given the encoded vectorized representations vi and vj for samples i and j, we consider two distance metrics as follows. Scaled Euclidean distance (Euclidean): We calculate the distance between vi and vj as
dij = d1 vi − vj 22 . (13) Cosine distance (Cos)14: Besides the scaled Euclidean distance, cosine distance is another widelyused distance metric:
dij = 1 − cos (vi, vj) = 1 − vi · vj . (14) vi vj

E.2 Sample similarity measures W

Given the soft pseudo-labels yi and yj for samples i and j, the following are some designs for Wij. In all of the cases, Wij is scaled into range [0, 1] (we set γ = 1 in Eq. 7 for the hard similarity). Hard Similarity: The hard similarity between two samples is calculated as

Wij =

1, if argmax[yi]k = argmax[yj]k,

k∈Y

k∈Y

0, otherwise.

(15)

This is called a “hard” similarity because we obtain

a binary label, i.e., we say two samples are similar

if their corresponding hard pseudo-labels are the

same, otherwise we say they are dissimilar.

13To accelerate contrastive learning, we adopt the doubly
stochastic sampling approximation to reduce the computa-
tional cost. Speciﬁcally, the high conﬁdence samples C in each batch B yield O(|C|2) sample pairs, and we sample |C|
pairs from them. 14We use Cos to distinguish from our model name CO-
SINE.

Soft KL-based Similarity: We calculate the simi-

larity based on KL distance as follows.

Wij = exp − β DKL(yi yj) + DKL(yj yi) , 2 (16)
where β is a scaling factor, and we set β = 10 by

default. Soft L2-based Similarity: We calculate the simi-

larity based on L2 distance as follows.

Wij = 1 − 1 ||yi − yj||22,

(17)

2

E.3 COSINE under different d and W .
We show the performance of COSINE with different choices of d and W on Agnews and MIT-R in Table 8. We can see that COSINE is robust to these choices. In our experiments, we use the

scaled euclidean distance and the hard similarity

by default.

