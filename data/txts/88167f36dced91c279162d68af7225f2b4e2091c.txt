Pre-Training a Language Model Without Human Language

Cheng-Han Chiang National Taiwan University, Taiwan
dcml0714@gmail.com

Hung-yi Lee National Taiwan University, Taiwan
hungyilee@ntu.edu.tw

arXiv:2012.11995v1 [cs.CL] 22 Dec 2020

Abstract
In this paper, we study how the intrinsic nature of pre-training data contributes to the ﬁne-tuned downstream performance. To this end, we pre-train different transformer-based masked language models on several corpora with certain features, and we ﬁne-tune those language models on GLUE benchmarks. We ﬁnd that models pre-trained on unstructured data beat those trained directly from scratch on downstream tasks. Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks. To our great astonishment, we uncover that pre-training on certain non-human language data gives GLUE performance close to performance pre-trained on another nonEnglish language.
1 Introduction
Neural language models (LMs) are prevalent in nowadays natural language processing (NLP) community, and they are indispensable to a variety of NLP tasks. Researchers have devoted themselves to understanding what these models have learned and how they work. Probing a trained model is widely used to understand to what extent a model learns certain linguistic features (Kovaleva et al., 2019; Hewitt and Manning, 2019; Tenney et al., 2019, 2018; Lin et al., 2019). Another line of research focuses more on how training corpora affect the trained LMs (Micheli et al., 2020; Gururangan et al.; Zhang et al., 2020).
In this work, we aim to understand how downstream performance varies across models pretrained on data of particular traits. The core problem we determine to answer is: What factors in the pre-training data make a pre-trained transformer LM perform better on downstream tasks than their trained from scratch counterparts? To answer this question, we pre-train many different transformer

LMs on dataset from miscellaneous disciplines, ranging from amino acid sequences in complex living organisms to artiﬁcial data generated by a simple python script. We then ﬁne-tune them on English downstream tasks. The process is illustrated in Figure 1.
Recently, Papadimitriou and Jurafsky (2020) proposed to train an LSTM LM on a non-natural language dataset and test the LM’s perplexity on natural language. They observed that LSTM LM trained on structured dataset gives perplexity far lower than those trained on unstructured data. While the observations are intriguing, this setting doesn’t match the common setting widely applied nowadays, in which we ﬁne-tune pre-trained LMs on downstream tasks. This is the ﬁrst paper investigating whether masked language model (MLM) pre-training on non-natural language aids downstream natural language tasks’ performance.
Based on the experiments, we have the following observations:
• We reveal that ﬁne-tuning models pre-trained on unstructured data outperforms model trained from scratch on downstream tasks.
• We ﬁnd that structured pre-training data is not a sufﬁcient condition to a pre-trained model that can perform well on NLP tasks.
• We discover that pre-training on a simple artiﬁcial dataset with hierarchical structure leads to downstream performance comparable to models pre-trained on human language.
• Our experiments show that token distribution is not the key factors to how well the model transferred to downstream tasks, while the number of token embeddings used during pretraining affects downstream performance.

Figure 1: Work ﬂow of our experiments: We ﬁrst pre-train the whole masked language model on L1 (protein sequence in this ﬁgure), and ﬁne-tune the whole model on English downstream tasks. We then test the performance on the ﬁne-tuned downstream task. It takes about 3 days to ﬁnish the whole process on a single V100.

2 Experiment Setups
In our experiments, we pre-train n RoBERTabase (Liu et al., 2019) models on n different types of pre-training data. We call the pre-training data L1 (ﬁrst language). We then evaluate the pretrained models’ ability by ﬁne-tuning them on different downstream tasks. The overall workﬂow is illustrated in Figure 1.
We adopt the classic GLUE (Wang et al., 2019) benchmarks to evaluate the models pre-trained on different L1s while excluding WNLI following Devlin et al. (2019). For each task, we use a certain set of hyperparameters and the same random seed to ﬁne-tune the model, and we report the results on the evaluation set. Details regarding all experiments can be found in Appendix A.
Our experiment setup may seem to resemble the Test for Inductive Bias via Language Model Transfer (TILT) proposed in Papadimitriou and Jurafsky (2020) at ﬁrst sight, which pre-trains an LSTM LM on L1, follows by only ﬁne-tuning word embeddings on Spanish, and test the perplexity on Spanish. However, the main purpose of TILT is to analyze the encoding of grammatical structure in LMs, so they do not ﬁne-tune LSTM on Spanish. On the contrary, our goal is to understand what factors in pre-training data make the pre-trained model perform better than models trained from scratch on downstream tasks.
3 Pre-training Data
We use two baseline pre-training dataset for our experiments: the random baseline and the Zipf baseline, both corpora have 29995 tokens, exclud-

ing 5 special tokens. For the random baseline, we draw the tokens from a uniform distribution and form sequences with a length of 90 to 120 tokens. For the Zipf baseline, we sample the tokens from the same uni-gram distribution of English. We also pre-train an English MLM with a subset of the English Wikipedia to serve as the performance upper bound. The pre-training corpora size is around 80MB for the previous three datasets.
We select several pre-training corpora in distinct disciplines that contain structure, including a biological dataset, a programming language corpus, an artiﬁcial dataset with a hierarchical structure, and a human language.
The biological dataset we adopt is amino acid sequence corpora obtained from Min et al. (2019). The characteristic of a protein is determined by its primary structure, i.e. the amino acid sequence. Chemical bonds between amino acids determine the secondary and tertiary structure of the folded protein, which further determines the functions of the protein. We use the one-letter abbreviation (A-Z) to represent each amino acid, and the total number of tokens in this dataset is 36M.
For programming language, we use Habeas corpus from Movshovitz-Attias and Cohen (2013), which contains tokenized Java script. We use the code from Papadimitriou and Jurafsky (2020) to extract the data and remove tokens that are labeled as a comment, making the training corpus contain only programming language. The total number of tokens in the pre-training data is 10M, and the vocabulary size of the model is 30K.
The artiﬁcial dataset we construct has a vocabu-

5 2 0 0 2 33 33 5
Figure 2: An illustration of the artiﬁcial dataset.
lary size of 28996, and the total number of tokens in training data is 23.5M. The dataset is generated by the following stack-based grammar, following Papadimitriou and Jurafsky (2020): At each time step t, we sample Xt from a Bernoulli distribution with P (Xt = 1) = 0.4. If Xt = 1, we sample a token based on English’s uni-gram distribution, place the sampled token at position t of the generated sequence, and push the same token into the stack. When Xt = 0, we pop the top element of the stack and put the popped token at position t of the generated sequence. Figure 2 shows a simple example. We can observe from Figure 2 that sequence generated in this manner contains a nesting hierarchical parentheses structure, which is similar to the dependency tree structure in natural language.
The last dataset used is a human language. We select a human language different from downstream tasks to compare the effect of non-human language pre-training data. We use Kannada from OSCAR dataset (Suárez et al., 2020). Kannada is a language predominantly spoken by the people in the southern western region of India. The main reason we choose this dataset lies in its subject(S)object(O)-verb(V) structure, different from the SV-O structure of our target language used in ﬁnetuning. The pre-training corpora size is 160MB, and the vocabulary size used in pre-training is 30K.
4 Experiments and Results
The overall results are illustrated in Table 1. In this section, we discuss how certain aspects of the pre-training corpora affect how good a model can become. By the word good, we refer to the model’s ability to be ﬁne-tuned on downstream tasks, which is the performance improvement over training the model from scratch on downstream tasks.
4.1 Is Structured Data All You Need For Pre-training?
We intend to answer this question: Is structured data the key to a good pre-trained model? We compare the models pre-trained on structured data with models pre-trained on unstructured baselines. If the downstream performance of models pre-trained

on structured data can beat their unstructured counterparts, then we may conclude that structure in the pre-training data is a key factor in the success of pre-trained transformer language models.
From the ﬁrst two blocks of Table 1, we ﬁnd that models pre-trained on unstructured data outperform the models trained from scratch. This suggests that the pre-trained model can still aid downstream performance, albeit the seemingly meaningless pretraining corpora.
From the third block in Table 1, we ﬁnd that pretraining on structured data may not always lead to a better model. Models pre-trained on amino acid and Java scripts are almost on a par with the models trained from scratch. Not much to our surprise, the model pre-trained on Kannada performs far better than the two baseline models.
Amazingly, ﬁne-tuning the model pre-trained on artiﬁcial data gives comparable performance compared with the model pre-trained on Kannada. This implies that it might be worth trying to pre-train a model on this kind of hierarchical nesting structured dataset, and ﬁne-tune the model on some low resource languages to obtain decent downstream performance. The artiﬁcial dataset consists of no semantic knowledge useful for downstream natural language tasks, so it is reasonable to infer that most knowledge the model learns from pre-training is the skill to model the hierarchical structure and long-term dependency. Equipped with this ability, the model can outrun models trained from unstructured data.
Our results show that models beneﬁt from pretraining on a certain type of structured corpora, while not every structured corpus leads to a good pre-trained model for NLP downstream tasks.
4.2 Does Pre-training Data Token Distribution Affect the Performance on Downstream Tasks?
We notice that the two baseline models’ performance is similar in almost all downstream tasks. This indicates that the uni-gram distribution of tokens in the training corpora makes little difference to the downstream performance when the pretraining data themselves are unstructured. We further ask whether this is also the case when the data is structured. We construct the artiﬁcial dataset as in Section 3, and aside from sampling based on Zipf distribution, we create another dataset whose tokens are sampled from the uniform distribution

L1

STS-B QNLI QQP CoLA SST-2 MNLI MRPC RTE Avg

1 No Pre-train Pre-train En

0.17 0.60 0.75 0.13 0.83 0.64 0.76 0.83 0.86 0.34 0.88 0.76

0.67 0.50 0.54 0.77 0.53 0.72

2 Rand. Baseline 0.29 Zipf Baseline 0.38

0.66 0.80 0.14 0.83 0.65 0.66 0.80 0.11 0.82 0.64

0.77 0.51 0.58 0.81 0.49 0.59

Amino Acid 0.16 0.65 0.79 0.07 0.82 0.60 0.81 0.44 0.55

3 Java Script

0.31 0.67 0.77 0.02 0.81 0.66 0.75 0.51 0.56

Kannada

0.76 0.77 0.83 0.12 0.81 0.69 0.80 0.55 0.67

4 Artiﬁcial (Uni.) 0.72 Artiﬁcial (Zipf) 0.76

0.77 0.80 0.14 0.81 0.69 0.77 0.83 0.11 0.82 0.69

0.77 0.52 0.65 0.75 0.53 0.66

Artiﬁcial (5000) 0.73 0.76 0.82 0.09 0.84 0.69 0.82 0.53 0.66

5 Artiﬁcial (500) 0.42 Artiﬁcial (50) 0.18

0.68 0.80 0.08 0.81 0.68 0.62 0.74 0.06 0.82 0.61

0.79 0.51 0.60 0.77 0.52 0.54

Artiﬁcial (50-s) 0.65 0.73 0.84 0.06 0.80 0.64 0.75 0.50 0.62

Table 1: Downstream results of different pre-trained models, and the model trained from scratch on downstream tasks (no pre-train in the ﬁrst row). The evaluation metrics of MRPC and QQP are F1 score, Spearman correlation coefﬁcient is reported for STS-B, and the rest tasks are evaluated with accuracy. Results of MNLI are the average of matched and mismatched. Please refer to Section 4.2 and Section 4.3 for the meaning of parentheses in the last two blocks. 50-s stands for 50-substitute in Section 4.3. Abbreviation used: En: English, Rand: random, Uni: uniform.

over tokens except for special tokens. The results, demonstrated in the fourth block in Table 1, show that even when the pre-training data is structured, token distribution still has little inﬂuence on how well the model can be ﬁne-tuned.
4.3 Does Token Numbers Mismatch between Pre-training and Fine-tuning Affect Downstream Performance?
This section investigates whether the mismatch between vocabulary size during pre-training1 and ﬁne-tuning contributes to how well the pre-trained model performs on downstream tasks. To study the inﬂuence of vocabulary size, we construct different artiﬁcial data by sampling tokens from different bin sizes (50, 500, and 5000). While the vocabulary size during pre-training is different for those models, their actual word embedding table sizes are still the same.
From the last block in Table 1, we observe that the averaged performance signiﬁcantly degrades in the case when only 50 tokens are used during pretraining, while the performance gradually recover when the token number mismatch between pretraining and ﬁne-tuning narrows. Tokens appearing in the pre-training data receive disproportionately larger gradients than tokens not in the pre-training data during pre-training, and this artifact cripples
1The number of different tokens in pre-training data.

the downstream performance. The above observation make it hard to tell
whether model pre-trained with amino acid sequence failed to perform well on downstream tasks due to the token number mismatch. Thus, we conduct further experiments to remove the undesirable artifact arise from the mismatch. Say we only use the ﬁrst 50 tokens (excluding special tokens) during pre-training while the rest 29950 token embeddings are not used, then before ﬁne-tuning the model on downstream tasks, we substitute those unused token embeddings with those 50 used token embeddings. We call the above setting 50-substitute. In this case, different tokens will share the same token embeddings when the model starts to be ﬁne-tuned.
From the last row in Table 1, we ﬁnd that the model recovers its ability to be ﬁne-tuned when pre-trained on artiﬁcial dataset. However, when performing the same substitution on the model pre-trained with amino acid, the model still fail to be ﬁne-tuned. Together with Section 4.1, we can conclude that the main reason a pre-trained model failed to transfer to human language downstream tasks lies in the intrinsic property of the pre-training data.
4.4 Further Fine-tuning with English MLM before Fine-tuning on GLUE
It is innate to ﬁne-tune the word embeddings of pre-trained models on English before ﬁne-tuning

on GLUE. This is for aligning the word embeddings of L1 acquired during pre-training with the word embeddings of English. We conduct experiments similar to Table 1, and the only difference lies in that we ﬁne-tune the word embeddings and language model head of the pre-trained model with MLM on English before ﬁne-tuning on GLUE. We ﬁnd the performance slightly advance mostly, with improvement in Java script being the most salient. We leave detailed results in Appendix B.
5 Conclusion
We study how pre-trained data might and might not affect the downstream performance of a transformer-based pre-trained LM. We ﬁnd that ﬁne-tuning with models pre-trained on data without any structures can surpass performance obtained directly trained from scratch on downstream tasks. Our results also show that pre-training with structured non-human language corpora does not always equip the model to perform competently on downstream tasks in general. We also discover that pre-training on a certain artiﬁcial dataset gives downstream performance comparable to pretraining on another natural language. We reveal that token distribution in the pre-training corpora merely affects pre-trained model performance on downstream tasks. Last, our experiments show that the number of token embeddings used during pre-training greatly contribute the downstream performance, while this can be mitigate by some manipulations on the token embeddings in certain cases. We hope our analysis provides insights into what kind of pre-training data makes a pre-trained model a pre-trained model.
Broader Impact
We ﬁnd an surprising simple artiﬁcial dataset to pre-train an language model, and we believe that our work have the potential to be applied to low-resource language when pre-training data are scarce. We think our work do not cause any ethical issues.
References
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for

Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.
Suchin Gururangan, Ana Marasovic´, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342–8360.
John Hewitt and Christopher D Manning. 2019. A structural probe for ﬁnding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138.
Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of bert. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4356–4365.
Yongjie Lin, Yi Chern Tan, and Robert Frank. 2019. Open sesame: Getting inside bert’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241–253.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Vincent Micheli, Martin d’Hoffschmidt, and François Fleuret. 2020. On the importance of pre-training data volume for compact language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7853–7858.
Seonwoo Min, Seunghyun Park, Siwon Kim, HyunSoo Choi, and Sungroh Yoon. 2019. Pre-training of deep bidirectional protein sequence representations with structural information. arXiv preprint arXiv:1912.05625.
Dana Movshovitz-Attias and William Cohen. 2013. Natural language models for predicting programming comments. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 35–40.
Isabel Papadimitriou and Dan Jurafsky. 2020. Learning music helps you read: Using transfer to study linguistic structure in language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6829–6839.

Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2020. A monolingual approach to contextualized word embeddings for mid-resource languages. In ACL 2020-58th Annual Meeting of the Association for Computational Linguistics.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601.
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2018. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In the Proceedings of ICLR.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Yian Zhang, Alex Warstadt, Haau-Sing Li, and Samuel R Bowman. 2020. When do you need billions of words of pretraining data? arXiv preprint arXiv:2011.04946.

A Experiment Details
We give detailed model architectures of our RoBERTa-base model and hyperparameters used in pre-training.

A.1 Model
We use RoBERTa-base, a 12-layered transformer model with hidden dimension 768 and 12 attention heads per layer. The total number of parameters of the model is around 110M. We pre-train RoBERTa using Huggingface (Wolf et al., 2019) code base.

A.2 Hyperparameters
The hyperparameters used in all pre-training experiments are listed in Table 2

Batch size Learning rate
Total steps Warmup steps Max Position

150 5E-5 200K 10k 128

Table 2: Pre-training hyperparemeters for BERT.

A.5 Resource
Out computation resource is V100 GPU. Pretraining a RoBERTa following our parameters given in 2 takes 60 hours on a single V100, and ﬁne-tuning the pre-trained models on the 8 GLUE tasks following hyperparameters in 5 takes about 12 hours on a V100.
B Fine-tune the Model on English MLM Before Fine-tuning on GLUE
This is the detailed experiment data for Section 4.4

A.3 Pre-training Data
We put all details related to all pre-training data in Table 3. We provide download link to the pretraining dataset, along with the training and validation loss at the end of pre-training. The artiﬁcial data and baseline dataset can be generated following the script in our code. The train/evaluation split can be found in the supplementary materials. We also include the vocabulary size (including special tokens) of each model on the last column. The vocabulary ﬁle is obtained by training a WordPiece tokenizer on the training data for Java, Kannada, and Wikipedia dataset.
A.4 Fine-tuning Details
We ﬁne-tune GLUE using Huggingface (Wolf et al., 2019) code base. The model ﬁne-tuned in this section is RoBERTa base with classiﬁer on top of the last transformer layer. The whole model ﬁnetuned is has 110M parameters.
A.4.1 Dataset We provide statistics on the 8 GLUE tasks we used in Table 4
A.4.2 Fine-tuning Hyperparameters We list the hyperparameters used in ﬁne-tuning GLUE in Tabel 5.

Dataset Wikipedia
Java Amino Acid
Kannada Random baseline
Zipf Baseline Artiﬁcial (Uniform)
Artiﬁcial (Zipf) Artiﬁcial (50) Artiﬁcial (500) Artiﬁcial (5000)

Link Wikidump Java data
PLUS OSCAR
NA NA NA NA NA NA NA

Training Loss 2.204
0.03227 2.041 2.366 9.428 6.351 1.996 1.599 1.558 1.563 1.548

Eval. Loss 3.354 1.025 2.201 3.128 9.467 6.446 2.409 1.774 1.754 1.762 1.701

Vocab Size 30000 30000 28895 30000 30000 30000 29991 29991 29991 29991 29991

Table 3: Details for dataset used in pre-training.

LM Head

LM Head

Classifier Head

Classifier Head (fixed)

Transformer

Transformer (fixed)

Transformer

Transformer (fixed)

Word Embedding

Word Embedding

Word Embedding

Word Embedding (fixed)

LAABYUQ
Stage 1 L1 MLM pre-train

English Wikipedia
Stage 2 En MLM fine-tune

Stage 3 GLUE fine-tune

Stage 4 GLUE testing

Figure 3: Work ﬂow of our experiments for Section 4.4: We ﬁrst pre-train the whole masked language model on L1 (protein sequence in this ﬁgure), and then only ﬁne-tune the word embedding and language model head on English Wikipedia. The third stage is ﬁne-tuning the whole model on English downstream tasks, and the last stage is to test the performance on the ﬁne-tuned downstream task.

Task MRPC RTE STS-B QNLI QQP CoLA MNLI SST-2

Examples 3.6K / 0.4K / 1.7K 2.4K / 0.2K / 3K 5.7K / 1.5K / 1.3K 104K / 5.4K / 5.4K 363K / 40.4K / 391.0K 8.5K / 1.0K / 1.1K 392.7K / 9.8K + 9.8K / 9.8K + 9.8K 67.4K / 0.9K / 1.8K

Table 4: Statistics of (train / dev/ test) in GLUE tasks MNLI contains matched and mismatched in dev and test set. We didn’t evaluate our models’ performance on test set.

LR BSZ RoBERTa DR Classiﬁer DR TS WS MSL

CoLA 1.00E-05 16

0

0.1

5336 320 128

STS-B 2.00E-05 16

0

0.1

3598 214 128

SST-2 1.00E-05 32

0

0.1

20935 1256 128

MNLI 3.00E-05 128

0

0.1

10000 1000 128

QNLI 1.00E-05 32

0

0.1

33112 1986 128

QQP 5.00E-05 128

0

0.1

14000 1000 128

RTE 3.00E-05 32

0

0.1

800 200 128

MRPC 2.00E-05 32

0

0.1

800 200 128

SQuAD2.0 3.00E-05 48

0

0.1

8144 814 128

Table 5: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch Size. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence Length

L1

STS-B QNLI QQP CoLA SST-2 MNLI MRPC RTE Avg

No Pre-train

0.17 0.60 0.75 0.13 0.83 0.65 0.67 0.50 0.54

Pre-train on English 0.76 0.83 0.86 0.34 0.88 0.76 0.77 0.53 0.72

Random Baseline 0.28 0.67 0.80 0.12 0.83 0.66 0.71 0.56 0.57

Zipf Baseline

0.34 0.71 0.81 0.17 0.84 0.67 0.81 0.53 0.61

Amino Acid

0.24 0.65 0.79 0.07 0.82 0.65 0.75 0.50 0.56

Java Script

0.25 0.78 0.82 0.12 0.82 0.71 0.78 0.51 0.60

Kannada

0.79 0.78 0.84 0.15 0.85 0.71 0.81 0.57 0.69

Artiﬁcial (Uniform) 0.73 0.79 0.82 0.17 0.82 0.71 0.75 0.55 0.67

Artiﬁcial (Zipf)

0.79 0.79 0.83 0.11 0.82 0.72 0.75 0.57 0.67

Table 6: Downstream results of different pre-trained models, and the model trained from scratch on downstream tasks (no pre-train in the ﬁrst row). The evaluation metric of MRPC and QQP are F1 score, spearman correlation coefﬁcient is reported for STS-B, and the rest tasks are evaluated with accuracy. Result of MNLI is averaged between matched and mismatched. Please refer to Section 4.2 and Section 4.3 for the meaning of parentheses in the last two blocks.

