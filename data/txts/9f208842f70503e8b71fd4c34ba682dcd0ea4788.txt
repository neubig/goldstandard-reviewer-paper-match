Adaptive Incentive Design

arXiv:1806.05749v1 [cs.GT] 14 Jun 2018

Lillian J. Ratliﬀ and Tanner Fiez {ratliffl,fiezt}@uw.edu University of Washington∗

Abstract

We apply control theoretic and optimization techniques to adaptively design incentives. In particular, we consider the problem of a planner with an objective that depends on data from strategic decision makers. The planner does not know the process by which the strategic agents make decisions. Under the assumption that the agents are utility maximizers, we model their interactions as a non–cooperative game and utilize the Nash equilibrium concept as well as myopic update rules to model the selection of their decision. By parameterizing the agents’ utility functions and the incentives oﬀered, we develop an algorithm that the planner can employ to learn the agents’ decision-making processes while simultaneously designing incentives to change their response to a more desirable response from the planner’s perspective. We provide convergence results for this algorithm both in the noise-free and noisy cases and present illustrative examples.

1. Introduction
Due in large part to the increasing adoption of digital technologies, many applications that once treated users are passive entities must now consider users as active participants. In many application domains, a planner or coordinator, such as a platform provider (e.g., transportation network companies), is tasked with optimizing the performance of a system that people are actively interacting with, often in real-time. For instance, the planner may want to drive the system performance to a more desirable behavior. While perhaps on competing ends of the spectrum, both revenue maximization and social welfare maximization fall under this umbrella.
A signiﬁcant challenge in optimizing such an objective is the fact that human preferences are unknown a priori and perhaps their solicited responses, on which the system depends, may not be reported truthfully (i.e. in accordance with their true preferences) due to issues related privacy or trust.
We consider a class of incentive design problems in which a planner does not know the underlying preferences, or decision–making process, of the agents that it is trying to coordinate. In the economics literature these types of problems are known as problems of asymmetric information—meaning that the involved parties do not possess the same information sets and, as is often the case, one party posses some information to which the other party is not privy.
The particular type of information asymmetry which we consider, i.e. where the preferences of the agents are unknown to the planner, results in a problem of adverse selection. The classic example of adverse selection is the market for lemons [1] in which the seller of a used car knows more about the car than the buyer. There are a number of components that are hidden from the buyer such as the maintenance upkeep history, engine health, etc.
∗. This work is supported by NSF Award CNS-1656873. T. Fiez was also supported in part by an NDSEG Fellowship.

Ratliff and Fiez
Hence, the buyer could end up with a lemon instead of a cherry—i.e. a broken down piece of junk versus a sweet ride. Such problems have long been studied by economists.
The incentive design problem has also been explored by the control community, usually in the context of (reverse) Stackelberg games (see, e.g., [2–4]). More recently, dynamic incentive design in the context of applications such as the power grid [5] or network congestion games [6]. We take a slightly diﬀerent view by employing techniques from learning and control to develop an adaptive method of designing incentives in a setting where repeated decisions are made by multiple, competiting agents whose preferences are unknown to the designer, yet they are subjected to the incentives.
We assume that agents, including the planner, are cost minimizers1. The decision space of the agents are assumed continuous. We model each agent’s cost as a parametric function that is dependent on the choices of other agents and is modiﬁed by an incentive chosen by the planner. The planner not knowing the underlying preferences of the agents is tantamount to it not knowing the value of the parameters of the agents’ cost functions. Such parameters can be thought of as the type of the agent.
We formulate an adaptive incentive design problem in which the planner iteratively learns the agents’ preferences and optimizes the incentives oﬀered to the agents so as to drive them to a more desirable set of choices. We derive an algorithm to solve this problem and provide theoretical results on convergence for both the case when the agents play according to a Nash equilibrium as well as the case when the agents play myopically— e.g. the agents play according to a myopic update rule common in the theory of learning in games [7]. Speciﬁcally, we formulate an algorithm for iteratively estimating preferences and designing incentives. By adopting tools from adaptive control and online learning, we show that the algorithm converges under reasonable assumptions.
The results have strong ties to both the adaptive control literature [8–10] and the online learning literature [11–13]. The former gives us tools to do tracking of both the observed output (agents’ strategies) and the control input (incentive mechanism). It also allows us to go one step further and prove parameter convergence under some additional assumptions— persistence of excitation—on the problem formulation and, in particular, the utility learning and incentive design algorithm. The latter provides tools that allow us to generalize the algorithm and get faster convergence of the observed actions of the agents to a more desirable or even socially optimal outcome.
The remainder of the paper is organized as follows. We ﬁrst introduce the problem of interest in Section 2. In Sections 3 and 4, we mathematically formulate the utility learning and incentive design problems and provide an algorithm for adaptive incentive design. We present convergence results for the Nash and myopic-play cases in Section 5 after which we draw heavily on adaptive control techniques to provide convergence results when the planner receives noisy observations. We provide illustrative numerical examples in Section 7 and conclude in Section 8.
1. While in the remainder, we formulate the entire problem given all agents are cost minimizers, the utility maximization formulation is completely analogous.
2

Adaptive Incentive Design

2. Problem Formulation

We consider a problem in which there is a coordinator or planner with an objective, fP (x, v), that it desires to optimize by selecting v ∈ V ⊂ Rn; however, this objective is a function of x = (x1, . . . , xn) ∈ X ⊂ Rn which is the response of n non–cooperative strategic agents each providing response xi ∈ Xi ⊂ R. Regarding the dimension of each xi, we assume without
loss of generality that they are scalars. All the theoretical results and insights apply to the

more general setting where each agent’s choice is of arbitrary ﬁnite dimension.

The goal of the planner is to design a mechanism to coordinate the agents by incentivizing

them choose an x that ultimately leads to minimization of fP (x, v). Yet, the coordinator does not know the decision making process by which these agents arrive at their collective

response x. As a consequence there is asymmetric information between the agents and the

planner.

Let us suppose that the each agent has some type θ and a process Mθi that determines their choice xi(θi). This process is dependent on the other agents and any mechanism
designed by the planner. The classical approach in the economics literature is to solve this

problem of so-called adverse selection [14] by designing mechanisms that induce agents’ to take actions in a way that corresponds with their true decision-making process Mθi . In this approach, it is assumed that the coordinator has a prior on the type space of the agents—

e.g., a probability distribution on θ. The coordinator then designs a mechanism (usually

static) based on this assumed prior that encourages agents to act in accordance with their

true preferences.

We take an alternative view in which we adopt control theoretic and optimization tech-

niques to adaptively learn the agents’ types while designing incentives to coordinate the

agents around a more desirable (from the point of view of the planner) choice x. Such a

framework departs from one-shot decisions that assume all prior information is known at

the start of the engagement and opens up opportunities for mechanisms that are dynamic

and can learn over time.

We thus take the view that, in order to optimize its objective, the planner must learn the

decision-making process and simultaneously design a mechanism that induces the agents to

respond in such a way such that the planner’s objective is optimized.

The planner ﬁrst optimizes its objective function to ﬁnd the desired response xd and the desired vd. That is, it determines the optimizers of its cost as if x and v are its decision

variables. Of course, it may be the case that the set of optimizers of fP contains more than

one pair (xd, vd); in this case, the coordinator must choose amongst the set of optimizers. In

order to realize (xd, vd), the planner must incentivize the agents to play xd by synthesizing

mappings γi ∈ Γ ⊂ C2(X, R) for each i ∈ I = {1, . . . , n} such that γi(xd) = vid ∈ Vi ⊂ R

and

xd

=

(x

d 1

,

.

.

.

,

x

d n

)

is

the

collective

response

of

the

agents

under

their

true

processes

{Mθi }i∈I .

We will consider two scenarios: (i) Agents play according to a Nash equilibrium strategy;

(ii) Agents play according to a myopic update rule—e.g. approximate gradient play or

ﬁctitious play [7].

In the ﬁrst scenario, if the agents are assumed to play according to a Nash equilibrium strategy, then xd must be a Nash equilibrium in the game induced by γ = (γ1, . . . , γn). In particular, using the notation x−i = {x1, . . . , xi−1, xi+1, . . . , xn}, let agent i have nominal

3

Ratliff and Fiez

cost fi ∈ C2(X, R) and incentivized cost

fiγi (xi, x−i) = fi(xi, x−i) + γi(xi, x−i).

(1)

The

desired

response

xd

is

a

Nash

equilibrium

of

the

incentivized

game

(f1γ1

,

.

.

.

,

f

γn n

)

if

xdi ∈ arg min fiγi (xi, xd−i).

(2)

xi∈Xi

Hence, xdi is a best response to xd−i for each i ∈ I. Formally, we deﬁne a Nash equilibrium as follows.

Deﬁnition 1 (Nash Equilibrium of the Incentivized Game). A point x ∈ X is a Nash

equilibrium

of

the

incentivized

game

(f1γ1 ,

..

.

,

f

γn n

)

if

fiγi (x) ≤ fiγi (xi, x−i), ∀ xi ∈ Xi.

(3)

If, for each i ∈ I, the inequality in (3) holds only for a neighborhood Wi ⊂ Xi of xi, then x is a local Nash equilibrium.
We make use of a sub-class of Nash equilibria called diﬀerential Nash equilibria, as they can be characterized locally and thus, amenable to computation. Let the diﬀerential game form [15, Deﬁnition 2] ω : X → Rn be deﬁned by ω(x) = (D1f1(x), . . . , Dnfn(x)).

Deﬁnition 2 ( [15, Deﬁnition 4]). A strategy x = (x1, . . . , xn) ∈ X is a diﬀerential Nash

equilibrium

of

(f1γ1 ,

..

.

,

f

γn n

)

if

ω(x)

=

0

and

Di2ifi(x)

is

positive

deﬁnite

for

each

i

∈

I.

Diﬀerential Nash equilibria are known to be generic amongst local Nash equilibria [16], structurally stable and attracting under tˆatonnement [15].
In the second scenario, we assume the agents play according to a myopic update rule [7] deﬁned as follows. Given the incentive γi, agent i’s response is determined by the mapping

giγi(x) = gi(x) + γi(x)

(4)

In addition, function giγi ∈ C2(Xk+1, R) maps the history, from time 0 up to time k, of the agents’ previous collective response to the current response where Xk+1 is the product space X × · · · × X with k + 1 copies of the space X.
We aim to design an algorithm in which the planner performs a utility learning step and an incentive design step such that as the planner iterates through the algorithm, agents’ collective observed response converges to the desired response xd and the value of the incentive mapping evaluated at xd converges to the desired value vd. In essence, we aim to ensure asymptotic or approximate incentive compatibility. In the sections that follow, we describe the utility learning and the incentive design steps of the algorithm and then, present the algorithm itself.

3. Utility Learning Formulation
We ﬁrst formulate a general utility learning problem, then we give examples in the the Nash–play and myopic–play cases.

4

Adaptive Incentive Design

3.1 Utility Learning Under Nash–Play

We assume that the planner knows the parametric structure of the agents’ nominal cost

functions and receives observations of the agents’ choices over time. That is, for each i ∈ I,

we assume that the nominal cost function of agent i has the form of a generalized linear

model

fi(x) = Φ(x), θi∗ =

m j=1

φj

(x)θi∗,j

(5)

where Φ(x) is a vector of basis functions given by Φ(x) = [φ1(x) · · · φm(x)]T , assumed to be known to the planner, and θi∗ ∈ Rm is a parameter vector, θi∗ = [θi∗,1 · · · θi∗,m]T , assumed unknown to the planner.

While our theory is developed for this case, we show through simulations in Section 7

that the planner can be agnostic to the agents’ decision-making processes and still drive

them to the desired outcome.

Let the set of basis functions for the agents’ cost functions be denoted by Fφ. We assume that elements of Fφ are C2(X, R) and Lipschitz continuous. Thus the derivative of
any function in Fφ is uniformly bounded.

The admissible set of parameters for agent i, denoted by Θi, is assumed to be a compact subset of Rm and to contain the true parameter vector θi∗. We will use the notation fi(x; θ) when we need to make the dependence on the parameter θ explicit.

Note that we are limiting the problem of asymmetric information to one of adverse

selection [14] since it is the parameters of the cost functions that are unknown to the

coordinator.

Similarly, we assume that the admissible incentive mappings have a generalized linear

model of the form

γi(x) = Ψ(x), αi =

s j=1

ψj

(x)αi,j

(6)

where Ψ(x) = [ψ1(x) · · · ψs(x)]T is a vector of basis functions, belonging to a ﬁnite collection Fψ, and assumed to be C2(X, R) and Lipschitz continuous, and αi = (αi,1 · · · αi,s) ∈ Rs
are parameters.

Remark 1. This framework can be generalized to use diﬀerent subsets of the basis functions for diﬀerent players, simply by constraining some of the parameters θi or αi to be zero. We choose to present the theory with a common number of basis functions across players in an eﬀort to minimize the amount of notation that needs to be tracked by the reader.

At each iteration k, the planner receives the collective response from the agents, i.e. xk =

(xk1, . . . , xkn), and has the incentive parameters αk = (α1k, . . . , αnk ) that were issued. We denote the set of observations up to time k + 1 by {xt}kt=+01—where x0 is the ob-
served Nash equilibrium of the nominal game (without incentives)—and the set of incentive

parameters {αt}kt=0. Each of the observations is assumed to be an Nash equilibrium.

For

the

incentivized

game

(f1γi

,

.

.

.

,

f

γn n

),

a

Nash

equilibrium

x

necessarily

satisﬁes

the

ﬁrst- and second-order conditions Difiγi(x) = 0 and Di2ifiγi(x) ≥ 0 for each i ∈ I (see [15,

Proposition 1]).

Under this model, we assume that the agents are playing a local Nash—that is, each xk

is a local Nash equilibrium so that

0 = Difiγi (xk) = DiΦ(xk), θi∗ + DiΨ(xk), αik−1 ,

(7)

5

Ratliff and Fiez

for k ∈ N+ and 0 = DiΦ(x0), θi∗ , where DiΦ(xk) = [Diφ1(xk) · · · Diφm(xk)]T with Diφj denoting the derivative of φj with respect to xi and where we deﬁne DiΨ(xk) similarly. By an abuse of notation, we treat derivatives as vectors instead of co-vectors.
As noted earlier, without loss of generality, we take xi ∈ R. This makes the notation signiﬁcantly simpler and the presentation of results much more clear and clean. All details for the general setting are provided in [17].
In addition, for each i ∈ I, we have

0 ≤ Di2ifiγi (xk) =

D

2 ii

Φ(x

k

),

θi∗

+

D

2 ii

Ψ(x

k

),

αik

−

1

(8)

for k ∈ N+ and 0 ≤

D

2 ii

Φ(x

0

),

θi∗

where Di2iΦ and Di2iΨ are the second derivative of Φ and

Ψ, respectively, with respect to xi.
Let the admissible set of θi’s at iteration k be denoted by Θki . They are deﬁned using the second–order conditions from the assumption that the observations at times t ∈ {1, . . . , k}

are local Nash equilibria and are given by

Θki ={θi ∈ Θi|

D

2 ii

Φ(x

t

),

θi

+

D

2 ii

Ψ(x

t

),

αit−1

≥ 0,

t ∈ {1, . . . , k}} ⊆ Θi.

(9)

These sets are nested, i.e. Θki ⊆ Θki −1 ⊆ · · · ⊆ Θ0i ⊆ Θi, since at each iteration an additional constraint is added to the previous set. These sets are also convex since they are deﬁned by semi–deﬁnite constraints [18]. Moreover, θi∗ ∈ Θki for all k since, by assumption, each observation xk is a local Nash equilibrium.
Since the planner sets the incentives, given the response xk, they can compute the quantity − DiΨi(xk), αik−1 , which is equal to

− DiΨi(xk), αik−1 = DiΦi(xk), θi∗

by the ﬁrst order Nash condition (7). Thus, if we let yik+1 = − DiΨi(xk+1), αik and

ξik = DiΦi(xk+1), we have

yik+1 =

ξ

k i

,

θi∗

.

Then, the coordinator has observations {yit}kt=+11 and regression vectors {ξit}kt=0. We use the notation ξk = (ξ1k, . . . , ξnk) for the regression vectors of all the agents at iteration k.

3.2 Utility Learning Under Myopic–Play
As in the Nash–play case, we assume the planner knows the parametric structure of the myopic update rule. That is to say, the nominal update function gi is parameterized by θi over basis functions {φ1, . . . , φm} ⊂ Fφ and the incentive mapping γik at iteration k is parameterized by αik over basis functions {ψ1, . . . , ψs} ⊂ Fψ. We assume the planner observes the initial response x0 and we denote the past responses up to iteration k by x(0,k) = (x0, . . . , xk). The general architecture for the myopic update rule is given by

xki +1 = giγi (xi(0,k), x(−0i,k)) = Φi(x(i0,k), x(−0i,k)), θi∗ + Ψi(x(i0,k), x(−0i,k)), αik .

(10) (11)

6

Adaptive Incentive Design

Note that the update rule does not need to depend on the whole sequence of past response.

It could depend just on the past response xk or a subset, say x(j,l) for 0 ≤ j ≤ l ≤ k.

As before, we denote the set of admissible parameters for player i by Θi which we assume

to be a compact subset of Rm. In contrast to the Nash–play case, our admissible set of

parameters is no long time varying so that Θki = Θi for all k.

Keeping consistent with the notation of the previous sections, we let ξik = Φi(xk) and

yik+1 = xki +1 − Ψi(xk), αik so that the myopic update rule can be re-written as yik+1 =

ξ

k i

,

θi∗

.

Analogous

to

the

previous

case,

the

coordinator

has

observations

{yit}kt=+11

and

regression vectors {ξit}kt=0. Again, we use the notation ξk = (ξ1k, . . . , ξnk) for the regression

vectors of all the agents at iteration k.

Note that the form of the myopic update rule is general enough to accommodate a

number of game-theoretic learning algorithms including approximate ﬁctitious play and

gradient play [7].

3.3 Uniﬁed Framework for Utility Learning
We can describe both the Nash–play and myopic–play cases in a uniﬁed framework as follows. At iteration k, the planner receives a response xk which lives in the set Xk(x(0,k−1), θ∗, αk−1) which is either the set of local Nash equilibria of the incentivized game or the unique response determined by the incentivized myopic update rule at iteration k. The planner uses the past responses {xt}kt=0 and incentive parameters {αt}kt=0 to generate the set of observations {yt}kt=+11 and regression vectors {ξt}kt=0.
The utility learning problem is formulated as an online optimization problem in which parameter updates are calculated following the gradient of a loss function. For each i ∈ I, consider the loss function given by

(θik )

=

1 2

yik+1 −

ξik, θik

2 2

(12)

that evaluates the error between the predicted observation and the true observation at time
k for each player.
In order to minimize this loss, we introduce a well-known generalization of the projection operator. Denote by ∂f (x) the set of subgradients of f : X → R at x. A convex continuous function β : Θ → R is a distance generating function with modulus ν > 0 with respect to a reference norm · , if the set Θ◦ = {θ ∈ Θ|∂β(θ) = ∅} is convex and restricted to Θ◦, β is continuously diﬀerentiable and strongly convex with parameter ν, that is

θ − θ, ∇β(θ ) − ∇β(θ) ≥ ν θ − θ 2, ∀ θ , θ ∈ Θ◦.

The function V : Θ◦ × Θ → R+, deﬁned by

V (θ1, θ2) = β(θ2) − β(θ1) + ∇β(θ1)T (θ2 − θ1)

is the Bregman divergence [19] associated with β. By deﬁnition, V (θ1, ·) is non-negative and strongly convex with modulus ν. Given a subset Θk ⊂ Θ and a point θ ∈ Θk, the mapping PΘk,θ : Rm → Θk deﬁned by

PΘk,θ(g) = arg min g, θ − θ + V (θ, θ ) .

(13)

θ ∈Θk

7

Ratliff and Fiez

is the prox-mapping induced by V on Θk. This mapping is well-deﬁned, the minimizer

is unique by strong convexity of V (θ, ·), and is a contraction at iteration k, [20, Proposi-

tion 5.b].

Given the loss function : Θ → R, a positive, non-increasing sequence of learning rates

(ηk), and a distance generating function β, the parameter estimate of each θi is updated at

iteration k as follows

θik+1 = PΘk+k,1θk ηk∇ (θik) . ii

(14)

Note that if the distance generating function is β(θ) = 21 θ 22, then the associated Bregman divergence is the Euclidean distance, V (θ, θ ) = 12 θ − θ 22, and the corresponding prox– mapping is the Euclidean projection on the set Θi, which we denote by PΘi,θ(g) = ΠΘi(θ−g),

so that

θik+1 = ΠΘk θik − ηk∇ (θik) . i

(15)

4. Incentive Design Formulation

In the previous section, we described the parameter update step that will be used in our

utility learning and incentive design problem. We now describe how the incentive pa-

rameters αk for each iteration are selected. In particular, at iteration k, after updat-

ing parameter estimates for each agent, the data the planner has includes the past ob-

servations {yt}kt=+11, incentive parameters {αt}kt=0, and has an estimate of each θik+1 for

i ∈ I. The planner then uses the past data along with the parameter estimates to ﬁnd an

αk+1 = (α1k+1, . . . , αnk+1) such that the incentive mapping for each player evaluates to vid at

xdi and xd ∈ Xk+1(x(0,k+1), θ∗, αk+1). This is to say that if the agents are rational and play

Nash, then xd is a local Nash equilibrium of the game (f1γ1(x; θ1k+1), . . . , fnγn(x; θnk+1)) where

fiγi

(x

;

θ

k i

+1

)

denotes

the

incentivized

cost

of

player

i

parameterized

by

θik+1.

On

the

other

hand, if the agents are myopic, then, for each i ∈ I, xdi = Φi(xk), θik+1 + Ψi(xk), αik+1 .

In the following two subsections, for each of these cases, we describe how αk+1 is selected.

4.1 Incentive Design: Nash–Play Given that γik+1 is parameterized by αik+1, the goal is to ﬁnd αik+1 for each i ∈ I such that xd is a local Nash equilibrium of the game

Φ1(x), θ1k+1 + Ψ1(x), α1k+1 , . . . , Φn(x), θnk+1 + Ψn(x), αnk+1)

and such that Ψi(xd), αik+1 = vid for each i ∈ I.

Assumption 1. For every {θi}ni=1 where θi ∈ Θi, there exist αi ∈ Rs for each i ∈ I such

that

xd

is

the

induced

diﬀerential

Nash

equilibrium

in

the

game

(

f1γ1

(x

;

θ

1

),

.

.

.

,

f

γn n

(x

;

θ

n

))

and γi(xd) = vid where γi(x) = Ψi(x), αi .

We remark that the above assumption is not restrictive in the following sense. Finding
αi that induces the desired Nash equilibrium and results in γi evaluating to the desired incentive value amounts to ﬁnding αik+1 such that the ﬁrst– and second–order suﬃcient conditions for a local Nash equilibrium are satisﬁed given our estimate of the agents’ cost

8

Adaptive Incentive Design

functions. That is, for each i ∈ I, we need to ﬁnd αik+1 satisfying

02×1 0

= ζik+1 + Λiαik+1

<

D

2 ii

Φ

i

(

x

d

)

,

θik

+

D

2 ii

Ψi

(x

d

)

,

αik

+1

(16)

where

ζik+1 =

DiΦi(xd), θik+1 −vid

and Λi = DΨiΨi(ix(xd)dT)T ∈ R2×s

If Λi is full rank, i.e. has rank 2, then there exists a αik+1 that solves the ﬁrst equation in (16). If the number of basis functions s satisﬁes s > 2, then the rank condition is not
unreasonable and in fact, there are multiple solutions. In essence, by selecting s to be large
enough, the planner is allowing for enough degrees of freedom to ensure there exists a set of parameters α that induce the desired result. Moreover, the problem of ﬁnding αik+1 reduces to a convex feasibility problem.
The convex feasibility problem deﬁned by (16) can be formulated as a constrained least– squares optimization problem. Indeed, for each i ∈ I,

  min (P1) αki +1
 s.t.

ζik+1 + Λiαik+1

2 2

D

2 ii

Φ

i

(

xd

),

θik

+1

+

Di2iΨi(xd), αik+1)

≥ε

for some ε > 0. By Assumption 1, for each i ∈ I, there is an αk+1 such that the cost is exactly minimized.
The choice of ε determines how well-conditioned the second-order derivatives of agents’ costs with respect to their own choice variables is. In addition, we note that if there are a large number of incentive basis functions, it may be reasonable to incorporate a cost for sparsity—e.g., λi αik+1 1; however, the optimal solution in this case is not guaranteed to satisfy (16).
It is desirable for the induced local Nash equilibrium to be a stable, non-degenerate diﬀerential Nash equilibrium so that it is attracting in a neighborhood under the gradient ﬂow [15]. To enforce this, the planner must add additional constraints to the feasibility problem deﬁned by (16). In particular, second–order conditions on player cost functions must be satisﬁed, i.e. that the derivative of the diﬀerential game form ω is positive–deﬁnite [15, Theorem 2]. This reduces to ensuring D2Φ(x, θk+1) + D2Ψ(x, αk+1) > 0 where

 D121Φ1(x), θ1k+1

···

D

2 n1

Φ1

(x

),

θ1k

+1



D2Φ(x, θk+1) =  ... . . . ...  ,

D

2 1n

Φ

n

(x

)

,

θnk+1

···

D

2 nn

Φn

(x

),

θnk+1

and D2Ψ(x, α(k+1)) is deﬁned analogously. Notice that this constraint is a semi–deﬁnite constraint [18] and thus, the problem of ﬁnding αk+1 that induces xd to be a stable, non–
degenerate diﬀerential Nash equilibrium can be formulated as a constrained least–squares

9

Ratliff and Fiez

optimization problem. Indeed,

(P2)

min

i

ζik+1 + Λiαik+1

2 2

αk+1

s.t. D2Φ(xd, θk+1) + D2Ψ(xd, αk+1) ≥ ε

for some ε > 0. The optimization problem (P2) can be written as a semi–deﬁnite program. Again, a regularization term can be incorporated in order to ﬁnd sparse parameters
αik+1. However, by introducing regularization, the condition ζik+1 + Λiαik+1 = 0 will in general no longer be satisﬁed by the solution.
Ensuring the desired Nash equilibrium is a stable, non–degenerate diﬀerential Nash equilibrium means that, ﬁrst and foremost, the desired Nash equilibrium is isolated [15, Theorem 2]. Thus, there is no nearby Nash equilibria. Furthermore, non–degenerate diﬀerential Nash equilibria are generic [16, Theorem 1] and structurally stable [15, Theorem 3] so that they are robust to small modeling errors and environmental noise. Stability ensures that if at each iteration players play according to a myopic approximate best response strategy (gradient play), then they will converge to the desired Nash equilibrium [15, Proposition 2]. Hence, if a stable equilibrium is desired by the planner, we can consider a modiﬁed version of Assumption 1.

Assumption 1’ (Modiﬁed—Stable Diﬀerential Nash). For every {θi}ni=1 where θi ∈ Θi,

there exist αi ∈ Rs for each i ∈ I such that xd is the induced stable, non–degenerate

diﬀerential

Nash

equilibrium

of

(f1γ1

(x

;

θ

1

)

,

.

.

.

,

f

γn n

(x

;

θ

n

))

and

γi(xd)

=

vid

where

γi(x)

=

Ψi(x)T αi.

Given

θk

=

(θ

k 1

,

.

.

.

,

θ

k n

),

let

A(θk, x, v)

be

the

set

of

αk

=

(α1k

,

.

.

.

,

α

k n

)

such

that

x

is

a

diﬀerential

Nash

equilibrium

of

(

f1γ1

(

x

),

.

.

.

,

f

γn n

(

x

))

and

γi(x)

=

vi

where

γi(x)

=

Ψi(x), αik . Similarly, let As(θk, x, v) be the set of αk that induce x to be a stable, non–

degenerate diﬀerential Nash equilibrium where γi(x) = vi. By Assumptions 1 and 1’,

A(θk, x, v) and As(θk, x, v), respectively, are non–empty. Further, it is straightforward to

ﬁnd an αk belonging to A(θk, x, v) (resp., As(θk, x, v)) by solving the convex problem stated

in (P1) (resp., (P2)).

4.2 Incentive Design: Myopic–Play

Given

θik+1

for

each

i

∈

I,

the

planner

seeks

an

incentive

mapping

γk+1

=

(γ

k 1

+1

,

.

.

.

,

γ

k n

+1

)

that induces the desired response xd and such that γik+1(xd) = vid for each i ∈ I. As before,

given that γi has been parameterized, this amounts to ﬁnding αik+1 such that

xdi = Φ(xk), θik+1 + Ψ(xk), αik+1

(17)

and such that Ψ(xd), αik+1 = vid for each i ∈ I.

Assumption 2. For every {θi}ni=1 where θi ∈ Θi, there exist αi ∈ Rsi for each i ∈ I such that xd is the estimated collective response—that is, (17) is satisﬁed for each i ∈ I—and such that Ψ(xd), αik+1 = vid.

As in the Nash–play case, ﬁnding the incentive parameters at each iteration that induce the desired response amounts to solving a set of linear equations for each player. That is,

10

Adaptive Incentive Design

Algorithm 1: Adaptive Incentive Design

1: k ← 0 2: If Myopic–play: receive x0

3: Choose θi0 ∈ Θi, αi0 ∈ Rs for each i ∈ I 4: Issue incentive mapping with parameters αi0 5: do

6: issue incentives with parameters αk

7: Receive xk+1 ∈ Xk(x(0,k), θ∗, αk). 8: Compute yk+1, ξk and incur loss (θik) 9: θik+1 = Pθkk+1 ηk∇ (θik) for each i ∈ I
i
10: for i ∈ I:

11: if Nash–play:

12:

αik+1 ∈ A(θk+1, xd, vd) [or As(θk+1, xd, vd)]

13: elif myopic–play: αik+1 ∈ Am(θk+1, xd, vd)

14: k ← k + 1

15: end do

for each i ∈ I, the coordinator must solve

0p ×1 = ζ˜k+1 − Λ˜ kαk+1 = xdi − Φ(xk), θik+1

i

i

ii

vid

− Ψi(xk)T αk+1 Ψi(xd)T i

(18)

for αik+1. Deﬁne Am(θk+1, xk, xd, vd) to be the set of αik+1 that satisfy (18). The above set of equations will have a solution if the matrix Λki has rank 2. Choosing the
set of basis functions {ψj}sj=1 such that s > 2 makes this rank condition not unreasonable. One unfortunate diﬀerence between the Nash–play case and the present case of myopic–play
is that in the former the coordinator could check the rank condition a priori given that it does not depend on the observations. On the other hand, Λki depends on the observation at each iteration and thus, can only be verify online.
As before, the problem can be cast as a least–squares optimization problem with cost ζik+1 − Λki αik+1 22.

5. Convergence in the Noise Free Case
In Algorithm 1, the steps of the utility learning and incentive design algorithm are formalized. We now discuss the convergence results for the proposed algorithm.
Deﬁnition 3. If, for each i ∈ I, there exists a constant 0 < ci,s < ∞ such that ξik(ξik)T ≤ ci,sI for all k, then we say the algorithm is stable.
Lipschitz continuity of the functions in Fφ implies stability.
Deﬁnition 4. If for each i ∈ I, there exists a constant 0 < ci,p < ∞ such that ci,pI ≤ ξik(ξik)T for all k, we will say the algorithm is persistently exciting.
Let cs = maxi∈I ci,s and cp = mini∈I ci,p. The following lemma is a straghtforward extension of [12, Lemma 2.1] and we leave the proof to Appendix A.

11

Ratliff and Fiez

Lemma 1. For every θ∗ ∈ Θk+1, θk ∈ (Θk)◦, and g ∈ Rm, we have

V (Pθkk+1(g), θ∗) ≤ V (θk, θ∗) +

g, θ∗ − θk

+ 21ν

g

2 ∗

To compactify the notation, let Vk(θi) ≡ V (θik, θi) and ∆θik = θi∗ − θik.

Theorem 1. Suppose Algorithm 1 with prox–mapping deﬁned by β (modulus ν) is persistently exciting, stable and that the step–size η is chosen such that η − 2ην2 cs > ε for some ε such that 0 < ε < 21cp with cp = mini∈I ci,p and where 0 < cs < ∞ is such that ξik 2∗ ≤ cs. We have the following:
(a) For each i ∈ I, Vk(θi∗) converges and

lim (ξik)T (θi∗ − θik) 2 = 0.

(19)

k→∞

(b)

If

β(θi)

=

1 2

θi

22, then for each i ∈ I, θik

converges exponentially fast to θi∗.

Proof. We prove part (a) ﬁrst. Since elements of Fφ are Lipschitz, ξik(ξik)T ≤ cˆsI so that

we may ﬁnd a cs > 0 such that for all k,

ξik

2 ∗

≤

cs.

Lemma 1 implies that

Vk+1(θi∗)

≤

Vk(θi∗) − η

∆θ

k i

,

∇

(θik )

+ 2ην2

∇ (θik)

2∗.

(20)

Hence,

Vk+1(θi∗) ≤ Vk(θi∗) − η

(ξ

k i

)T

(∆θ

k i

)

2 + 21ν η2

ξik

2 ∗

(

ξik

)

T

(∆θ

k i

)

2

≤ Vk(θi∗) −

η − 21ν η2

ξik

2 ∗

(ξ

k i

)

T

(∆

θ

k i

)

2

≤ Vk(θi∗) −

η

−

1 2ν

η

2

cs

(ξ

k i

)T

(∆θ

k i

)

2

≤ Vk(θi∗) − ε

(ξ

k i

)T

(∆

θ

k i

)

2

(21)

Thus,

(

ξik

)T

(∆θ

k i

)

2≤

1 ε

(Vk

(θi∗)

−

Vk+1(θi∗)).

Summing k from 0 to K, we have

K k=0

(ξik)T (θi∗ − θik) 2 ≤ ε−1(V0(θi∗) − VK+1(θi∗)) ≤ ε−1V0(θi∗)

so that limK→∞

K k=0

(ξ

k i

)

T

(

θ

∗ i

− θik)

2

≤

ε−1V0(θi∗)

<

∞.

This, in turn, implies that

limk→∞ ξik(θi∗ − θik) 2 = 0. From (21) and the fact that Vk(θi∗) is always postive, we see

that Vk(θi∗) is a decreasing sequence and hence, it converges. The analysis holds for each

i ∈ I.

Now, we show part (b).

Suppose

that

β(θi)

=

1 2

θi

22.

Then, starting with the inequality

in (21), we have that

Vk+1(θi∗) ≤ Vk(θi∗) − ε(∆θik)T ξik(ξik)T (∆θik) ≤ Vk(θi∗) − εcp(∆θik)T (∆θik) ≤ Vk(θi∗)(1 − 2cpε)

since ξik(ξik)T ≤ csI (i.e. stability), η − 21 η2cs > ε by construction, and cpI ≤ ξik(ξik)T (i.e. persistence of excitation). Since 0 < ε < 1/(2cp), we have that 1 − 2cpε < e−2cpε so that Vk+1(θi∗) < e−2cpεVk(θi∗). This implies that VK (θi∗) < e−2cpKεV0(θi∗). Therefore we have that θik → θi∗ exponentially fast. The same argument holds for each i ∈ I.

12

Adaptive Incentive Design

For general prox–mappings, Theorem 1 lets us conclude that the observations converge

to zero and that the prox–function Vk(θi) converges. Knowing the parameter values—a

consequence

of

choosing

β(θi)

=

1 2

θi

22—allows

for

the

opportunity

to

gain

qualitative

insights into how agents’ preferences aﬀect the outcome of their strategic interaction. On

the other hand, more general distance generating functions selected to reﬂect the geometry

of Θ have the potential to improve convergence rates [12].

Corollary 1. Suppose agents play according to the myopic update rule (11). Under the assumptions of Theorem 1, for each i ∈ I, xti+1 − xdi 2 → 0 and |vit+1 − vd|2 → 0.

Proof. Since agents play myopically, we have that xdi = Φ(xk), θik + Ψ(xk), αik and the pre-

dicted induced response is xdi =

Φ(xk), θik + Ψ(xk), αik

so that

xki +1 −xdi 2 =

(ξ

k i

)

T

(

θ

∗ i

−

θik) 2. Thus, by Theorem 1-(a), limk→∞ xki +1 − xdi 2 = 0. Moreover, by Assumption 2, we

know that each αik+1 satisﬁes Ψ(xd), αik+1 = vid. Deﬁne vik+1 = Ψ(xk+1), αk+1 . Then,

|vik+1 − vid|2 = | Ψ(xk+1) − Ψ(xd), αik+1 |2 ≤ Ψ(xk+1) − Ψ(xd) 2 αik+1 2∗.

Since each element of Fψ is Lipschitz, Ψ(xk+1)−Ψ(xd) ≤ C xk+1 −xd for some constant

C > 0.

Hence, |vik+1 − vid|2 ≤ C2

αik+1

2 ∗

xk+1 − xd

2

and since

xk+1 − xd 2 converges to

zero and

αik+1

2 ∗

<

∞,

we

get

that

|vik+1

−

vd|2

converges to zero.

In the Nash–play case, we can use the fact that non–degenerate diﬀerential Nash equilib-

ria are structurally stable [15, Theorem 3] to determine a bound on how close an equilibrium

of

the

incentived

game,

(f1γ1

(x

;

θ

∗ 1

)

,

.

.

.

,

f

γn n

(x

;

θ

∗ n

))

with

γi(x)

=

Ψ(x), αik

for each i ∈ I,

is to the desired Nash equilibrium xd. Note that the observed Nash equilibrium xk+1 is in

the set of Nash equilirbia of the incentivized game.

Let ωGγ (θ, x) = (D1f1γ1(x; θ1), . . . , Dnfnγn(x; θ)) be the diﬀerential game form [15] of

the incentivized game Gγ

=

(f1γ1

,

.

.

.

,

f

γn n

).

By a slight abuse of notation, we will denote

D1ωGγ (θ, x) and D2ωGγ (θ, x) as the local representation of the diﬀerential of ωGγ with respect

to θ and x respectively. If the parameters of the incentive mapping αk at each iteration k

are C2 with respect to xk and θk, then the diﬀerential of ω is well–deﬁned. We remark that

we formulated the optimization problem for ﬁnding the α’s as a constrainted least–squares

problem and there are existing results for determining when solutions to such problems are

continuously dependent on parameter perturbations [21, 22].

Theorem 2. Suppose that for each k, αk(θ, x) ∈ C2(Rnm × Rn, Rs) is chosen such that xd

is a non–degenerate diﬀerential Nash equilibrium. For θk − θ∗ suﬃciently small, there

is

a

Nash

equilibrium

x∗

of

Gγ

=

(

f1γ1

(

x

;

θ

∗ 1

),

.

.

.

,

f

γn n

(

x

;

θ

∗ n

))

that

is

near

the

desired

Nash

equilibrium, i.e. there exists ε¯ > 0, such that for all θk ∈ Bε¯(θ∗),

x∗ − xd ≤ sup0≤λ≤1 Dg((1 − λ)θ∗ + λθt) θk − θ∗
where Dg(θ) = −(D2ωGγ )−1(θ, xd) ◦ D1ωGγ (θ, xd). Furthermore, if Dg(θ) bounded by M > 0 on Bε¯(θ∗), then x∗ − xd ≤ M θk − θ∗ .

is uniformly

Proof. Consider the diﬀerential game form ωGγ (θ, x) which is given by

ωGγ (θ, x) =

n i=1

m j=1

Di φj (x)θi,j

+

s k=1

Di

ψk

(x)αi,j

dxi.

13

Ratliff and Fiez

Since xd is a non–degenerate diﬀerential Nash equilibrium, D2ωGγ (θ∗, xd) is an isomorphism. Thus, by the Implicit Function Theorem [23, Theorem 2.5.7], there exists a neighborhood W0 of θ∗ and a C1 function g : W0 → X such that for all θ ∈ W0, ω(θ, g(θ)) = 0. Furthermore, Dg(θ) = −(D2ωGγ )−1(θ, xd) ◦ D1ωGγ (θ, xd). Let Bε¯(θ∗) be the largest ε¯–ball inside of W0. Since Bε¯(θ∗) is convex, by Proposition [23, Proposition 2.4.7], we have that
g(θk) − g(θ∗) = ( 01 Dg((1 − λ)θ∗ + λθk) dλ) · (θk − θ∗)
Hence, since x∗ − xd = g(θk) − g(θ∗) , we have that

x∗ − xd ≤ (sup0≤λ≤1 Dg((1 − λ)θ∗ + λθk) ) θk − θ∗
Now, if Dg(θ∗) is uniformly bounded by M > 0 on Bε¯(θ∗), then its straightforward to see from the above inequality that x∗ − xd ≤ M θk − θ∗ .

As a consequence of Theorem 1-(b) and Theorem 2, there exists a ﬁnite iteration k for

which

θi∗ − θik

2 2

is suﬃciently small for each i ∈ I

so that a local Nash equilibrium of the

incentivized game at time k is arbitrarily close to the desired Nash equilibrium xd.

There may be multiple Nash equilibria of the incentivized game; hence, if the agents

converge to x∗ then the observed local Nash equilibrium is near the desired Nash equilibria.

We know that for stable, non–degenerate diﬀerential Nash equilibria, agents will converge

locally if following the gradient ﬂow determined by the diﬀerential game form ω [15, Propo-

sition 2].

Corollary 2. Suppose the assumptions of Theorem 2 hold and that x∗ is stable. If agents
follow the gradient of their cost, i.e. −Difi, then they will converge locally to x∗. Moreover, there exists an ε¯ > 0 such that for all θk ∈ Bε¯(θ∗),

x∗ − xd ≤ sup0≤λ≤1 Dg((1 − λ)θ∗ + λθk) θk − θ∗

The proof follows directly from Theorem 2 and [15, Proposition 2].
The size of the neighborhood of initial conditions for which agents converge to the
desired Nash can be approximated using techniques for computation of region of attraction
via a Lyapunov function [10, Chapter 5]. This is in part due to the fact that in the case where αk is chosen so that xd is stable, i.e. dωGγ (θk, xd) > 0, we have that dωGγ (θ∗, x∗) > 0 for θk near θ∗ since the spectrum of dωGγ varies continuously.
Moreover, it is possible to explicitly construct the neighborhood W0 obtained via the Implicit Function Theorem in Theorem 2 (see, e.g. [24, Theorem 2.9.10] or [17]).
The result of Theorem 1-(b) implies that the incentive value under x∗ from Theorem 2
is arbitrarily close to the desired incentive value.

Corollary 3. Under the assumptions of Theorem 1-(b) and Theorem 2, there exists a ﬁ-

nite K such that for all k ≥ K,

x∗ − xd

2 2

≤

M C¯e−2cpkε

where

C¯

=

n

max

i

{

2

V0

(

θ

∗ i

)

},

cp = mini∈I ci,p, and x∗ is the (local) Nash equilibrium of the incentivized game sat-

isfying x∗ − xd ≤ M θk − θ∗ for all θk ∈ Bε¯(θ∗). Furthermore, for each i ∈ I,

|vi∗ − vid|2 ≤ M C2 αik 22C¯e−2cpkε for all k ≥ K where C is the Lipschitz bound on Ψ and vi∗ = Ψ(x∗), αik .

14

Adaptive Incentive Design

Proof. Choose K such that, for each i ∈ I, 2V0(θi∗)e−2εcpK < ε¯ so that

θik − θi

2 2

≤

2V0(θi∗)e−2εcpk for all k ≥ K. Thus,

θk − θ∗

2 2

≤

C e−2εcp k ,

for

all

k

≥

K.

By Theorem 2,

we have that

x∗ − xd

2 2

≤

M C¯e−2εcpk

for

all

k

≥

K

where

M

is

the

uniform

bound

on

Dg(θ∗) . We know that vid = Ψ(xd), αik and vi∗ = Ψ(x∗), αik . Hence, since each element

of Fψ is Lipschitz, we have that for all k ≥ K, that

|vi∗ − vid|2 = | Ψ(x∗) − Ψ(xd), αik |2

≤ Ψ(x∗) − Ψ(xd)

αik

2 2

≤ C2

αik

2 2

x∗ − xd

2 2

≤ M C2 αik 22C¯e−2εcpk.

We have argued that following Algorithm 1 with a particular choice of prox–mapping, the parameter estimates of each θi∗ converge to the true values and as a consequence we can characterize the bound on how close the observed response and incentive value are to
their desired values. Knowing the true parameter values (even if obtained asymptotically) for θ∗ allows the planner to make qualitative insights into the rationale behind the observed
responses.

6. Convergence in the Presence of Noise

In this section, we will use the uniﬁed framework that describes both the case where the

agents play according to Nash and where the agents play myopically. However, we consider

noisy updates given by

yik+1 = ξik, θi∗ + wik+1

(22)

for each i ∈ I where wik+1 is an indepdent, identically distributed (i.i.d) real stochastic

process deﬁned on a probability space (Ω, F, P) adapted to the sequence of increasing sub-

σ–algebras

(Fk, k

∈

N),

where

Fk

is

the

σ–algebra

generated

by

the

set

{y

t i

,

αit

,

wit

,

t

≤

k}

and such that the following hold2:

E[wik+1|Fk] = 0 ∀k E[(wik+1)2|Fk] = σ2 > 0 a.s. ∀k supkE[(wik+1)4|Fk] < +∞ a.s.

(23a) (23b) (23c)

Note

that

Fk

is

also

the

σ–algebra

generated

by

{y

t i

,

ξit

,

t

≤

k}

since

wik

can

be

deduced

from yik and ξik−1 through the relationship wik = yik − ξik−1, θi∗ [9].

Theorem 3. Suppose that for each i ∈ I, {wik} satisﬁes (23a) and (23b) and that Algo-

rithm 1, with prox–mapping Pθ associated with β (modulus ν), is persistently exciting and

stable. Let the step–size ηk be selected such that

∞ k=1

ηk2

<

∞

and

ηk

−

2ηνk2 c˜s

>

0

where

0 < c˜s < ∞ is such that

ξik

2 ∗

≤

c˜s.

Then,

for

each

i ∈ I,

Vk (θi∗ )

converges

a.s.

Further-

more, if the sequence {rk} where rk = (ηk)−1 is a non-decreasing, non-negative sequence

2. We use the abbreviation a.s. for almost surely.

15

Ratliff and Fiez

such that rk is Fk measurable and there exists constants 0 < K1, K2 < ∞ and 0 < T < ∞

such that

k1 rk−1

≤

K1

+

K2 k

k−1 t=0

yit+1 −

ξ

t i

,

θit

− wit+1 2,

∀ k ≥ T,

(24)

then

limk→∞

1 k

k−1 t=0

E[

wit+1 −

ξit, θit − θi∗

2|Ft] = σ2 a.s.

(25)

In addition, if (23c) holds, then

limk→∞

1 k

k−1 t=0

wit+1 −

ξit, θit − θi∗

2 = σ2 a.s.

(26)

The proof follows a similar technique to that presented in [9, Chapter 13.4]; hence,

we

leave

it

to

Appendix

A.

We

remark

that

if

β(θi)

=

1 2

θi

22,

then

Theorem

3

implies

Vk (θi∗ )

=

1 2

θi∗ − θik

2 2

converges

a.s.

Corollary 4. Suppose agents play according to the myopic update rule (11) and that the assumptions of Theorem 3 hold. Then,

limk→∞

1 k

k−1 t=0

E[

xti+1 + wit+1 − xdi

2|Ft] = σ2

a.s.

Proof. Agent i’s response is xki +1 = ξik, θi∗ + Ψ(xk), αk and the planner designs αik to

satisfy xdi =

ξ

k i

,

θik

+ Ψ(xk), αk .

Hence, replacing

ξit, θit − θi∗

in (25) completes the

proof.

The results of Theorem 3 imply that the average mean square error between the observations and the predictions converges to σ2 a.s. and, if we recall, the observations are derived from noisy versions of the ﬁrst–order conditions for Nash. Indeed, we have shown that

limk→∞

1 k

kt=−01E[ DiΦ(xt+1), θi∗ + DiΨ(xt+1), αit + wit+1 2|Ft] = σ2 a.s.

or, equivalently,

limk→∞

1 k

kt=−01E[ DiΦ(xt+1), θi∗ − θit + wit+1 2|Ft] = σ2 a.s.

On the other hand, in the Nash–play case, it is diﬃcult to say much about the observed Nash equilibrium except in expectation. In particular, we can consider a modiﬁed version of Theorem 2 where we consider the diﬀerential game form in expectation—i.e. at iteration k, the diﬀerential game form for the induced game is

ωGγ (θ, x) =

n
i=1 E

DiΦ(x), θi

+ DiΨ(x), αik

+ wik+1|Fk−1 .

Proposition 1. Suppose that D2ωGγ (θ, xd) is an isomorphism. Then there exists an > 0, such that for all θk ∈ B (θ∗),

x∗ − xd ≤ (sup0≤λ≤1 Dg((1 − λ)θ∗ + λθk) ) θk − θ

16

Adaptive Incentive Design

(a) nominal

(b) incentivized, λ = 0.0

(c) incentivized, λ = 0.1

Figure 1: Visualization of Nash equilibria for the (a) nominal, (b) incentivized (without regularization),

and (c) incentivized (with regularization λ = 0.1) games. Curves indicate the set of points where Difi ≡ 0

for each i ∈ {1, 2}; we have ω = (D1f1, D2f2) = 0 wherever the curves intersect. The stable, diﬀerential

Nash equilibria are indicated by a dark circle of which there are two for the nominal game. Given an initial

condition, Nash is calculated via a steepest descent algorithm [15]. The empirical basin of attraction for

each Nash equilibrium is illustrated by the ﬁlled region containing the point. In the incentivized game, the

coordinator aims to drive the agents to (xd1, xd2) = (−1.8, 0.5). For λ = 0.0, the unique Nash equilibrium

of

the

incentivized

game

with

parameters

(α1, α2)

=

(0.16, −0.94, 0.29, −0.11)

is

(

xd1

,

x

d 2

);

however,

for

the

regularized incentivized game using with parameters (α1, α2) = (0.13, 0, 0.15, 0), there are two stable Nash

equilibria at (1.4, −0.9) and (−1.8, 0.5).

where Dg(θ∗) = −(D2ωGγ )−1(θ∗, xd) ◦ D1ωGγ (θ∗, xd). and x∗ is a (local) Nash equilibrium

of

the

incentivized

game

Gγ

=

(

f1γ1

(x

;

θ

∗ 1

)

,

.

.

.

,

f

γn n

(x,

θn∗

))

with

γi(x)

=

Ψ(x), αik

for each

i ∈ I. Furthermore, if Dg(θ∗) is uniformly bounded by M > 0 on B (θ∗), then

x∗ − xd ≤ M θk − θ∗ .

To apply Proposition 1, we need a result ensuring that the parameter estimate θk con-

verges to the true parameter value θ∗. One of the consequences of Theorem 3 is that Vk(θi∗)

converges

a.s.

and

when

β(θi)

=

1 2

θi

22,

θi∗ − θik

2 2

converges

a.s.

If it is the case that

it converges a.s. to a value less than ε, then Proposition 1 would guarantee that a local

Nash equilibrium of the incentivized game is near the desired nondegenerate diﬀerential

Nash equilibrium in expectation. We leave further exploration of the convergence of the

parameter estimate θik as future work.

7. Numerical Examples
In this section we present several examples to illustrate the theoretical results of the previous sections3.

7.1 Two-Player Coupled Oscillator Game The ﬁrst example we consider is a game between two players trying to control oscillators that are coupled. Coupled oscillator models are used widely for applications including
3. Code for examples can be found at github.com/fiezt/Online-Utility-Learning-Incentive-Design.

17

Ratliff and Fiez

1.0 0.8 θ 0.6 0.4 0.2

Estimation of θ, λ = 0.0 ||θ1 − θˆ1||
||θ2 − θˆ2||

0 20 40 60 80 100 iteration
(a)

1.0 0.8 θ 0.6 0.4 0.2 0.0
0

Estimation of θ, λ = 0.1 ||θ1 − θˆ1||
||θ2 − θˆ2||

20 40 60 iteration
(d)

80 100

0.4 0.3 (θi) 0.2 0.1 0.0
0
0.4 0.3 (θi) 0.2 0.1 0.0
0

Loss, λ = 0.0
(θ1) (θ2)

25

50

75 100

iteration

(b)

Loss, λ = 0.1
(θ1) (θ2)

25

50

75 100

iteration

(e)

Persistence of Excitation, λ = 0.0 1.0

0.8

||ξi||22 0.6 0.4

0.2
0.0 0

25

50

iteration

(c)

||ξ1||22 ||ξ2||22 75 100

Persistence of Excitation, λ = 0.1 1.0

0.8

||ξi||22 0.6 0.4

0.2 0.0 0

25

50

iteration

(f )

||ξ1||22 ||ξ2||22 75 100

Figure 2: Estimation error for (a) λ = 0.0 and (d) λ = 0.1. Loss for (b) λ = 0.0 and (e) λ = 0.1. Persistence of excitation (as measured by ξ 2) for the coupled oscillator with regularization (c) λ = 0.0 and (f) λ = 0.1
on the incentive parameters.

power [25], traﬃc [26], and biological [27] networks, and in coordinated motion control [28] among many others. Furthermore, it is often the case that coupled oscillators are viewed in a game–theoretic context in order to gain further insight into the system properties [29,30]. While the example is simple, it demonstrates that even in complex games on non–convex strategy spaces (in particular, smooth manifolds), Algorithm 1 still achieves the desired result.
Consider a game of coupled oscillators in which we have two players each one aiming to minimize their nominal cost

fi(xi, x−i) = −θi∗ cos(xi) + cos(xi − x−i).

(27)

Essentially, the game takes the form of a location game in which each player wants to be

near the origin while also being as far away as possible from the other player’s phase. The

coordinator

selects

αi

∈

R2,

i

∈

{1, 2}

such

that

(x

d 1

,

xd2

)

is

the

Nash

equilibrium

of

the

game with incentivized costs

fiγ(xi, x−i) = fi(xi, x−i) + αi,1 sin(xi − xdi ) + αi,2 cos(xi − xdi ).

(28)

Using Algorithm 1, we estimate θ1 and θ2 while designing incentives that induce the players

to

play

(

x

d 1

,

xd2

).

In

this

example,

we

set

θ1

=

1.0

and

θ2

=

1.05

and

set

the

desired

Nash

at

(xd1, xd2) = (1.8, −0.5).

In Fig. 1a, we show a visualization of the Nash equilibria for the game where the

square represents the unfolded torus with player 1’s choice along the x-axis and player

18

Adaptive Incentive Design
2’s choice along the y-axis. The solid and dashed black lines depict the zero sets for the derivatives of player 1 and 2 respectively. The intersections of these lines are all the candidate Nash equilibrium (all the Nash equilibria of the game must be in the set of intersection points of these lines). There are four intersection points: {x|ω(x) = 0} = {(0, π), (π, π), (1.1, −1.0), (−1.1, 1.0)}. Of the four, the ﬁrst two are not Nash equilibria which can be veriﬁed by checking the necessary conditions for Nash. However, (0, π) is a saddle point and (π, π) is an unstable equilibrium for the dynamics x˙ = −ω(x). The last two points are stable diﬀerential Nash equilibria. We indicate each of them with a circle in Fig. 1a and the colored regions show the empirical basin of attraction for each of the points.
In Fig. 1b and 1c, we show similar visualizations of the Nash equilibria but for the incentivized game. In the incentive design step of the algorithm, we can add a regularization term to the constrained least squares for selecting the incentive parameters. The regularization changes the problem and may be added if it is desirable for the incentives to be small or sparse for instance. Fig. 1b and 1c are for the case without and with regularization, respectively, where we regularize with λ α 22.
As we adjust the regularization parameter λ, it has the eﬀect of reducing the norm of the incentive parameters and as a consequence, what happens is that the number of stable Nash equilibria changes. In particular, a larger value of λ results in small incentive values and, in turn, non-uniqueness in the number of stable Nash equilibria in the incentivized game. There is a tradeoﬀ between uniqueness of the desired Nash equilibrium and how much the coordinator is willing to expend to incentivize the agents.
Moreover, in Fig. 2, we see that in the unregularized case (λ = 0) the estimation error for θ2 appears not to converge to zero while in the regularized case (λ = 0.1) it does. This is likely due to the fact that Algorithm 1 is not persistently exciting for the unregularized problem (see Fig. 2c where ξ1 2 drops to zero) while it is in the regularized case (see Fig. 2f where ξi 2, i = 1, 2 are always non-zero).
We remark that this example highlights that the technical conditions of the theoretical results in the preceding sections—in particular, persistence of excitation—matter in practice. Indeed, regularization allows for the problem to be persistently exciting. There are other methods for inducing persistence of excitation such as adding noise or ensuring the input is suﬃciently rich [31].
7.2 Bertrand Nash Competition
In this section, we explore classical Bertrand competition. Following [32], we adopt a stylized model inspired by [33]. We consider a two ﬁrm competition. Firms are assumed to have full information and they choose their prices (x1, x2) to maximize their revenue Ri(xi, x−i, τ ) = xiFi(xi, x−i, τ ) where Fi is the demand and τ is a normally distributed i.i.d. random variable that is common knowledge and represents economic indicators such as gross domestic product.
When ﬁrms compete with their nominal revenue functions Ri, the random variable τ changes over time causing demand shocks which, in turn, cause prices to shift. In our framework, we introduce a regulator that wants to push the prices chosen by ﬁrms towards some desired pricing level by introducing incentives. It is assumed that the regulator also has knowledge of τ .
19

Ratliff and Fiez

8 6 xi 4 2 0

Agent Response

xd1

xd2

x1

x2

xˆ1

xˆ2

0

50

100

iteration

(a)

θi − θˆi 2

Parameter Error

1.25

θ1 − θˆ1 2

1.00 θ2 − θˆ2 2

0.75

0.50

0.25

0.00 0 1000 2000 3000 4000 5000 iteration
(b)

Figure 3: (a) Agent response and (b) parameter estimation error. While the agents’ responses converge relatively quickly to the desired responses (within a range dictated by the statistics of the noise term τ ), the parameter estimation takes quite a bit longer to converge within 1e-2. This is in part due to the fact that the response converges quickly and the information that can be attained from the responses is limited; hence, it takes a while to improve the estimate.

We explore several variants of this problem to highlight the theoretical results as well as desirable empirical results we observe under relaxed assumptions on the knowledge the planner has of agent behavior.
As noted in the previous example, the assumption of persistence of excitation, which also appears in the adaptive control literature, is hard to verify a priori ; however, selecting basis functions from certain classes such as radial basis functions which are known to be persistently exciting [34, 35] can help to overcome this issue. We note that this is not a perfect solution; it may make the algorithm persistently exciting, yet result in a solution which is not asymptotically incentive compatible, i.e. xki xdi . Since we seek classes of problems that are persistently exciting and asymptotically incentive compatible, this exposes an interesting avenue for future research.

7.2.1 True Model Known to Planner

We consider each ﬁrm i to have a linear marginal revenue function given by

Mi(xk, τ k) = θi∗,1xk1 + θi∗,2xk2 + θi∗,ixki + τ k,

(29)

and the ﬁrm price to evolve according to a gradient play update

xki +1 = xki + ζik(Mi(xki , xk−i, τ k) +

Ψi

(

x

k i

,

xk−i

)

,

αik

)

(30)

where ζik > 0 is the learning rate which we choose to ensure stability of the process. We let the incentive basis functions be the following set of Gaussian radial basis functions:

 Ψi,1(xki , xk−i) = exp(−κ(xki − xdi )2)

Ψi,

2

(x

k i

,

xk−

i

)

=

exp(−κ(xki + xdi )2)

(31)



Ψi,

3

(x

k i

,

xk−

i

)

=

exp(−κ(xki )2)

with κ = 0.01. In order to test the theoretical results, in this example we assume the regulator has knowledge of the true nominal basis functions and assumes a gradient play

20

Adaptive Incentive Design

Agent Response

6

xi 4

2
0 0

xd1 x1 xˆ1
50 iteration

xd2 x2 xˆ2
100

Figure 4: Prices for best response (gradient and ﬁctitious play are similar).

model

for

the

ﬁrms,

Φ(

x

k 1

,

xk2

)

=

{

x

k 1

,

xk2

}

,

and

updates

its

estimate

of

the

ﬁrm

prices

according to

xki +1

=

xki

+

ζik (

Φ

i

(x

k i

,

xk−

i

),

θˆik

+ τk +

Ψi

(x

k i

,

xk−

i

)

,

αik

).

For this simulation we use τ ∼ N (5, 0.25), θ1∗ = (−1.2, −0.5), and θ2∗ = (0.3, −1), and for all Bertrand examples we set the desired prices at (xd1, xd2) = (5, 7).
In Fig. 3a, we show the ﬁrm prices and the regulator’s estimates of the ﬁrm prices over the ﬁrst 100 iterations of the simulation. The ﬁrm prices xi almost immediately converge to the desired prices xdi . Likewise, the regulator’s estimates of the ﬁrm prices xˆi rapidly converge to the true ﬁrm prices. We remark that in our simulations we observe that even with fewer incentive basis functions, the ﬁrm prices can still be pushed to the desired prices. Correspondingly, we ﬁnd that increasing the number of incentive basis functions can increase the speed of convergence to the desired prices as well as mitigate the impact of noise due to more ﬂexibility in the choice of incentives.
In Fig. 3b, we show that the regulator’s estimates of the parameters of the marginal revenue function for each ﬁrm converge to the true parameters. While in this example the speed of estimation convergence is considerably slower than the convergence of the ﬁrm prices to the desired prices, we observe multiple factors that can signiﬁcantly increase the speed of estimation convergence. Namely, as the variance of the economic indicator term τ increases, consequently providing more information at each iteration, the convergence speed also increases. Moreover, the stability of the process, dictated by the parameters of the marginal revenue functions, is positively correlated with the speed of convergence. We also note that the problem is persistently exciting and the expected loss decreases rapidly to zero.

7.2.2 Planner Agnostic to Myopic Update Rule
We now transition to examples which demonstrate that our framework is suitable for problem instances that go beyond our theoretical results. In particular, we ﬁnd even if the planner does not have access to the true nominal basis functions that govern the dynamics of agent play, agent responses can still be driven to a desired response for several forms of myopic updates. In these examples, we again consider ﬁrms to have the linear marginal revenue function from (29). We then allow the ﬁrm price to evolve according to either the

21

Ratliff and Fiez

x − ˆx 2 x − xd 2

Estimation Error

Gradient Play

4

Best Response

Fictitious Play

3

2

1

Incentive Error

Gradient Play

6

Best Response

Fictitious Play

4

2

0 0

50

100

iteration

(a)

0 0

50

100

iteration

(b)

Figure 5: (a) Response estimation error for diﬀerent update methods and (b) diﬀerence between desired and true response for diﬀerent update methods. Note that in (a) and (b) the best response and ﬁctitious play methods are nearly identical; this is due to the fact that ﬁctitious play is best response to the historical average. We remark that it takes longer for ﬁctitious play to converge given the incentive and noise at each iteration.

gradient play update from (30), a best response update given by

xki +1 = arg max Ri(xi, xk−i, τ k) + γik(xi, xk−i),
xi

or a ﬁctitious play update given by

xki +1 = arg max Ri(xi, x¯(−l,ik), τ k) + γik(xi, x¯(−l,ik))
xi

where x¯(−l,ik) is the average over the history from l to k with 0 ≤ l ≤ k. For all simulations we choose to use the complete history, i.e. l = 0.
We choose the set of incentive basis functions to be same as in (31), but now consider the nominal basis functions to not include the true basis functions and instead be the following set of Gaussian radial basis functions:

 Φi,1(xki , xk−i) = exp(−κ(xki )2)

Φ

i,

2

(x

k i

,

xk−

i

)

=

exp(−κ(xki − xk−i)2)

(32)



Φ

i,

3

(x

k i

,

xk−

i

)

=

exp(−κ(xk−i)2)

with κ = 0.01 as before. In this example, the regulator uses an update to its estimate of the prices, given by

xki +1 = Φi(xki , xk−i), θˆik + τ k + Ψi(xki , xk−i), αik ,

(33)

which is agnostic to the ﬁrm update method. We use τ ∼ N (5, 0.1) and, as before, let the parameters of the marginal revenue functions be θ1∗ = (−1.2, −0.5), and θ2∗ = (0.3, −1).
In Fig. 5, we show the ﬁrm prices and the regulator’s estimates of the ﬁrm prices when
the ﬁrms use best response (each of the other methods have similar plots). As was the case
when the regulator had knowledge of the true nominal basis functions, the regulator is able
to quickly estimate the true ﬁrm prices accurately and push the ﬁrm prices to the desired

22

Adaptive Incentive Design

Utility Component Utility Component

6
4
2
0 0

50

100

iteration

(a)

Φ1(x), θˆ1 + ν 6 Ψ1(x), α1

Φ2(x), θˆ2 + ν

Ψ2(x), α2

4

2

0 0

50

100

iteration

(b)

Φ1(x), θˆ1 + ν Ψ1(x), α1 Φ2(x), θˆ2 + ν Ψ2(x), α2

8 6 xi 4

2

0

Agent Response

xd1

xd2

x1

x2

xˆ1

xˆ2

0

50

100

iteration

(c)

Figure 6: Estimated cost components for each ﬁrm when the ﬁrms use the best response update method: (a) linear marginal revenue and (b) non-linear marginal revenue. (c) Agent responses for gradient play with a non-linear marginal revenue functions.

prices. Figs. 5a and 5b show that the regulator’s estimate of the ﬁrm prices converges to the true ﬁrm prices—i.e. xˆk − x 2 → 0—and the true ﬁrm responses converge to the desired responses—i.e. xk − xd 2 → 0—at nearly the same rates.
In Fig. 6a, we show the nominal and incentive cost components that form the regulator’s estimate of the ﬁrm prices when the ﬁrms are use best response. Analogous to the ﬁrm prices, we note that these components are similar for each of the ﬁrms’ update methods. We highlight this to point out that the nominal cost and incentive cost components do not simply cancel one another out, indicating that a portion of the true dynamics are maintained.
7.2.3 Nonlinear Marginal Revenue
To conclude our examples, we explore a Bertrand competition in which each ﬁrm has a nonlinear marginal revenue function given by
Mi(xk, τ k) = log(xki ) + θi∗,1xk1 + θi∗,2xk2 + θi∗,ixki + θi,3 + τ k + 1.
Here, we let the ﬁrm price evolve according to the gradient play update from (30) with the nonlinear marginal revenue function and let the regulator use the agnostic method to update its estimate of the ﬁrms’ prices from (33) with the nominal basis functions from (32) and the incentive basis functions from (31). We use the same noise parameters as in the preceding examples and and let the parameters of the marginal revenue functions be θ1∗ = (−1.2, −0.5, 7.5), and θ2∗ = (0.3, −1, 1.5).
In Fig. 6b, we show the estimated revenue and incentive components for the two ﬁrms. In Fig. 6c, we observe that the response of the ﬁrms to the regulator is marginally diﬀerent from what we observed in previous examples when the ﬁrms had linear marginal revenue functions, but nonetheless the regulator’s estimates of the ﬁrm prices converge to the true ﬁrm prices and the true ﬁrm prices converge to the desired prices.
23

Ratliff and Fiez

8. Conclusion
We present a new method for adaptive incentive design when a planner faces competing agents of unknown type. Speciﬁcally, we provide an algorithm for learning the agents’ decision-making process and updating incentives. We provide convergence guarantees on the algorithm. We show that under reasonable assumptions, the agents’ true response is driven to the desired response and, under slightly more restrictive assumptions, the true preferences can be learned asymptotically. We provide several numerical examples that both verify the theory as well as demonstrate the performance when we relax the theoretical assumptions.

Appendix A. Proof of Lemma 1

The following proof uses Young’s inequality:

v1T v2 ≤

v1 ∗ v2

≤

1 2

vν1 2∗ + ν v2 2

(34)

for any v1, v2 ∈ Rm.

Proof of Lemma 1. The proof is the essentially the same as the proof for [12, Lemma 2.1]
with a few modiﬁcations. Let θk ∈ (Θk)◦ and θk+1 = Pθkk+1(g). Note that

θk+1 ∈ argminθ ∈Θk+1 g, θ − θk + V (θk, θ )

(35)

or equivalently,

θk+1 ∈ argminθ ∈Θk+1 β(θ ) − ∇β(θk) − g, θ

(36)

where the latter form tells us that β is diﬀerentiable at θk+1 and θk+1 ∈ (Θk+1)◦. Since ∇2V (θk, θk+1) = ∇β(θk+1) − ∇β(θk), the optimality conditions for (36) imply that

∇β(θk+1) − ∇β(θk) + g, θk+1 − θ ≤ 0, ∀θ ∈ Θk+1

(37)

Note that this is where the proof of [12, Lemma 2.1] and the current proof are diﬀerent. The above inequality holds here for all θ ∈ Θk+1 whereas in the proof of [12, Lemma 2.1]—using the notation of the current Lemma—the inequality would have held for all θ ∈ Θk. In particular, we need the inequality to hold for θ∗ and it does since by assumption θ∗ ∈ Θk+1 for each k.
First, ∇β(θk), θk+1 − θk ≤ β(θk+1) − β(θk). Hence, for θ∗ ∈ Θk+1, we have that
V (θk+1, θ∗) − V (θk, θ∗) = β(θ∗) − ∇β(θk+1), θ∗ − θk+1) − β(θk+1)
− (β(θ∗) − ∇β(θk), θ∗ − θk − β(θk)) = ∇β(θk+1) − ∇β(θk) + g, θk+1 − θ∗ + g, θ∗ − θk+1) ≤ g, θ∗ − θk+1 − V (θk, θk+1)

24

Adaptive Incentive Design

where the last inequality holds due to (37). By (34), we have that

g, θk − θk+1

≤

1 2ν

g

2 ∗

+

ν 2

θk − θk+1

2.

(38)

Further,

ν 2

θk − θk+1

2 ≤ V (θk, θk+1) since V (θk, ·) is strongly convex.

Thus,

V (θk+1, θ∗) − V (θk, θ∗)

≤ g, θ∗ − θk+1 − V (θk, θk+1)

= g, θ∗ − θk + g, θk − θk+1 − V (θk, θk+1)

≤

g, θ∗ − θk

+ 21ν

g

2 ∗

so that

V (Pθk(+k)1(g), θ∗) ≤ V (θ(k), θ∗) + g, θ∗ − θ(k) + 21ν g 2∗.

Appendix B. Proof of Theorem 3

Proposition 2 ( [36]). Let {xt} be a zero conditional mean sequence of random variables

adapted to {Ft}. If

∞ t=0

1 t2

E[

x

2 t

|F

t−

1

]

<

∞

a.s,

then

limN →∞

1 N

N t=1

xt

=

0

a.s.

Proof of Theorem 3. Starting from Lemma 1, we have

E[Vt+1(θi∗)|Ft] ≤ Vt(θi∗) − ηt

ξit

,

∆

θ

t i

(E[yit+1 −

ξit, θit

|Ft])

+ 2ηνt2 ξit 2∗ ( E[yit+1 − (ξit)T θit|Ft] )2 + σ2

≤ Vt(θi∗) − ηt − ηt2 2ξνit 2∗ E[yit+1 − (ξit)T θit|Ft] 2 + 2ηνt2 ξit 2∗σ2

≤ Vt(θi∗) − ηt − 2ηνt2 c˜1 E[yit+1 − (ξit)T θit|Ft] 2 + 2ηνt2 c˜1σ2.

By the assumptions that ηt − 2ηνt2 c˜1 > 0 and

∞ t=1

ηt2

<

∞,

we

can

use

the

fact

that

yit+1

=

ξ

t i

,

θi∗

+ wit+1 and apply the almost supermartingale convergence theorem [37] to get that

∞ t=1

ηt − 2ηνt2 c˜1

E[wit+1 −

ξ

t i

,

θit

− θi∗

|Ft]

2<∞

a.s. and that Vt(θi∗) converges a.s. Now, we argue (25) holds; the argument follows that which is presented in [8, Chapter
8]. To do this, we ﬁrst show that

limT →∞

1 T

T −1 t=0

ξit, θit − θi∗

2 = 0 a.s.

(39)

Note that (B) implies that

limT →∞ Tt=−01 r1t ξit, θit − θi∗ 2 < ∞ a.s.

(40)

Where rt = (ηt − 2ηνt2 c˜1)−1. Suppose that rt is bounded—i.e. there exists K3 such that rt < K3 < ∞. In this case, it is immediate from (40) that

limT →∞ K13 Tt=−01 ξit, θit − θi∗ 2 < ∞ a.s.

(41)

25

Ratliff and Fiez

so that (39) follows trivially. On the other hand, suppose rt is unbounded. Then we can apply Kronecker’s Lemma [9] to conclude that

limT →∞ r1T

T −1 t=0

ξit, θit − θi∗

2 = 0 a.s.

(42)

Hence, from (24), we have

lim 1 T

T −1 t=0

ξit,θit−θi∗ 2

T →∞ K1+ KT2

T −1 t=0

ξit ,θit −θi∗

2 =0

a.s.

(43)

so that (39) follows immediately. Note that

E[(yit+1 − (ξit)T θit)2|Ft] = E[(yit+1 −

ξ

t i

,

θit

− wit+1 + wit+1)2|Ft]

= E[(yit+1 − ξit, θi(t) − wit+1)2 + (wit+1)2

+ 2 yit+1 − ξit, θi(t) − wit+1, wit+1 |Ft]

Since yit+1 − wit+1 and

ξ

t i

,

θit

are Ft–measurable and E[wit+1|Ft] = 0 a.s., we have

E[(yit+1 −

ξ

t i

,

θit

)2|Ft]

=

(yit+1

−

ξ

t i

,

θit

− wit+1)2 − E[(wit+1)2|Ft].

Replacing yit+1 =

ξ

t i

,

θi∗

+wit+1 and using (39), we see that (25) holds since E[(wit+1)2|Ft] =

σ2 a.s.

Finally, if supt E[(wit+1)4|Ft] < +∞ almost surely, then by Proposition 2, we have

limT →∞

1 T

T −1 t=0

wit+1 −

ξit, θit − θi∗

2 = σ2 a.s.

which concludes the proof.

References
[1] G. A. Akerlof, “The Market for ’Lemons’: Quality Uncertainty and the Market Mechanism,” Quarterly J. Economics, vol. 84, no. 3, pp. 488–500, 1970.
[2] Y.-C. Ho, P. Luh, and R. Muralidharan, “Information structure, stackelberg games, and incentive controllability,” IEEE Trans. Automat. Control, vol. 26, no. 2, pp. 454– 460, Apr 1981.
[3] Y. Ho and D. Teneketzis, “On the interactions of incentive and information structures,” IEEE Trans. Automat. Control, vol. 29, no. 7, pp. 647–650, Jul 1984.
[4] X. Liu and S. Zhang, “Optimal incentive strategy for leader-follower games,” IEEE Trans. Automat. Control, vol. 37, no. 12, pp. 1957–1961, Dec 1992.
[5] X. Zhou, E. Dall’Anese, L. Chen, and A. Simonetto, “An incentive-based online optimization framework for distribution grids,” IEEE Trans. Automat. Control, 2017.
[6] J. Barrera and A. Garcia, “Dynamic incentives for congestion control,” IEEE Trans. Automat. Control, vol. 60, no. 2, pp. 299–310, Feb 2015.

26

Adaptive Incentive Design
[7] D. Fudenberg and D. K. Levine, The theory of learning in games. MIT press, 1998, vol. 2.
[8] G. C. Goodwin and K. S. Sin, Adaptive ﬁltering prediction and control. Englewood Cliﬀs, NJ: Prentice–Hall, 1984.
[9] P. R. Kumar and P. Varaiya, Stochastic systems: estimation, identiﬁcation and adaptive control. Englewood Cliﬀs, NJ: Prentice–Hall, 1986.
[10] S. Sastry, Nonlinear Systems. Springer New York, 1999.
[11] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games. Cambridge University Press, 2006.
[12] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, “Robust stochastic approximation approach to stochastic programming,” SIAM J. Optimization, vol. 19, no. 4, pp. 1574– 1609, 2009.
[13] M. Raginsky, A. Rakhlin, and S. Yu¨ksel, “Online convex programming and regularization in adaptive control,” in Proce. 49th IEEE Conf. Decision and Control, 2010, pp. 1957–1962.
[14] P. Bolton and M. Dewatripont, Contract theory. MIT press, 2005.
[15] L. J. Ratliﬀ, S. A. Burden, and S. S. Sastry, “On the Characterization of Local Nash Equilibria in Continous Games,” IEEE Trans. Automat. Control, 2016.
[16] ——, “Generictiy and Structural Stability of Non–Degenerate Diﬀerential Nash Equilibria,” in Proc. 2014 Amer. Controls Conf., 2014.
[17] L. J. Ratliﬀ, “Incentivizing eﬃciency in societal-scale cyber-physical systems,” Ph.D. dissertation, University of California, Berkeley, 2015.
[18] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004.
[19] L. M. Bregman, “The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming,” USSR computational mathematics and mathematical physics, vol. 7, no. 3, pp. 200–217, 1967.
[20] J.-J. Moreau, “Proximit´e et dualit´e dans un espace hilbertien,” Bulletin de la Soci´et´e math´ematique de France, vol. 93, pp. 273–299, 1965.
[21] P. L¨otstedt, “Perturbation bounds for the linear least squares problem subject to linear inequality constraints,” BIT Numerical Mathematics, vol. 23, no. 4, pp. 500–519, 1983.
[22] J. F. Bonnans and A. Shapiro, Perturbation analysis of optimization problems. Springer Science & Business Media, 2000.
[23] R. Abraham, J. E. Marsden, and T. Ratiu, Manifolds, Tensor Analysis, and Applications, 2nd ed. Springer, 1988.
27

Ratliff and Fiez
[24] J. H. Hubbard and B. B. Hubbard, Vector calculus, linear algebra, and diﬀerential forms: a uniﬁed approach. Prentice Hall, 1998.
[25] F. D¨orﬂer and F. Bullo, “Synchronization and transient stability in power networks and nonuniform Kuramoto oscillators,” SIAM J. Control Optim., vol. 50, no. 3, pp. 1616–1642, Jan 2012.
[26] S. Coogan, G. Gomes, E. Kim, M. Arcak, and P. Varaiya, “Oﬀset optimization for a network of signalized intersections via semideﬁnite relaxation,” in Proc. 54th IEEE Conf. Decision and Control, 2015.
[27] Y. Wang, Y. Hori, S. Hara, and F. Doyle, “Collective oscillation period of inter-coupled biological negative cyclic feedback oscillators,” IEEE Trans. Automat. Control, vol. 60, no. 5, pp. 1392–1397, May 2015.
[28] D. Paley, N. E. Leonard, R. Sepulchre, D. Gru¨nbaum, J. K. Parrish et al., “Oscillator models and collective motion,” IEEE Control Syst. Mag., vol. 27, no. 4, pp. 89–105, 2007.
[29] H. Yin, P. Mehta, S. Meyn, and U. Shanbhag, “Synchronization of coupled oscillators is a game,” IEEE Trans. Automat. Control, vol. 57, no. 4, pp. 920–935, April 2012.
[30] T. Goto, T. Hatanaka, and M. Fujita, “Potential game theoretic attitude coordination on the circle: Synchronization and balanced circular formation,” in IEEE Int. Symp. Intelligent Control, Sept 2010, pp. 2314–2319.
[31] S. Boyd and S. S. Sastry, “Necessary and suﬃcient conditions for parameter convergence in adaptive control,” Automatica, vol. 22, no. 6, pp. 629–640, 1986.
[32] D. Bertsimas, V. Gupta, and I. C. Paschalidis, “Data-driven estimation in equilibrium using inverse optimization,” Mathematical Programming, vol. 153, no. 2, pp. 595–633, 2015.
[33] S. T. Berry, “Estimating Discrete-Choice Models of Product Diﬀerentiation,” RAND J. Economics, vol. 25, no. 2, pp. 242–262, 1994.
[34] A. J. Kurdila, F. J. Narcowich, and J. D. Ward, “Persistency of excitation in identiﬁcation using radial basis function approximants,” SIAM J. Control Optim., vol. 33, no. 2, pp. 625–642, 1995.
[35] D. Gorinevsky, “On the persistency of excitation in radial basis function network identiﬁcation of nonlinear systems,” IEEE Tran. Neural Networks, vol. 6, no. 5, pp. 1237– 1244, 1995.
[36] J. Neveu, Discrete-Parameter Martingales. North–Holland Publishing Company, 1975.
[37] H. Robbins and D. Siegmund, “A convergence theorem for non negative almost supermartingales and some applications,” in Herbert Robbins Selected Papers, T. Lai and D. Siegmund, Eds. Springer New York, 1985, pp. 111–135.
28

