Convex optimization based on global lower second-order models

arXiv:2006.08518v2 [math.OC] 21 Dec 2020

Nikita Doikov∗ Catholic University of Louvain,
Louvain-la-Neuve, Belgium Nikita.Doikov@uclouvain.be

Yurii Nesterov† Catholic University of Louvain,
Louvain-la-Neuve, Belgium Yurii.Nesterov@uclouvain.be

Abstract
In this paper, we present new second-order algorithms for composite convex optimization, called Contracting-domain Newton methods. These algorithms are afﬁne-invariant and based on global second-order lower approximation for the smooth component of the objective. Our approach has an interpretation both as a second-order generalization of the conditional gradient method, or as a variant of trust-region scheme. Under the assumption, that the problem domain is bounded, we prove O(1/k2) global rate of convergence in functional residual, where k is the iteration counter, minimizing convex functions with Lipschitz continuous Hessian. This signiﬁcantly improves the previously known bound O(1/k) for this type of algorithms. Additionally, we propose a stochastic extension of our method, and present computational results for solving empirical risk minimization problem.
1 Introduction
Classical Newton method is one of the most popular optimization schemes for solving ill-conditioned problems. The method has very fast quadratic convergence, provided that the starting point is sufﬁciently close to the optimum [3, 22, 31]. However, the questions related to its global behaviour for a wide class of functions are still open, being in the area of active research.
The signiﬁcant progress in this direction was made after [33], where Cubic regularization of Newton method with its global complexity bounds were justiﬁed. The main idea of [33] is to use a global upper approximation model of the objective, which is the second-order Taylor’s polynomial augmented by a cubic term. The next point in the iteration process is deﬁned as the minimizer of this model. Cubic Newton attains global convergence for convex functions with Lipschitz continuous Hessian. The rate of convergence in functional residual is of the order O(1/k2) (here and later on, k is the iteration counter). This is much faster than the classical O(1/k) rate of the Gradient Method [31]. Later on, accelerated [27], adaptive [7, 8] and universal [17, 12, 18] second-order schemes based on cubic regularization were developed. Randomized versions of Cubic Newton, suitable for solving high-dimensional problems were proposed in [13, 19].
Another line of results on global convergence of Newton method is mainly related to the framework of self-concordant functions [32, 31]. This class is afﬁne-invariant. From the global perspective, it provides us with an upper second-order approximation of the objective, which naturally leads to the Damped Newton Method. Several new results are related to its analysis for generalized self-concordant functions [2, 38], and the notion of Hessian stability [23]. However, for more reﬁned problem classes, we can often obtain much better complexity estimates, by using the cubic regularization technique [14].
∗Institute of Information and Communication Technologies, Electronics and Applied Mathematics (ICTEAM) †Center for Operations Research and Econometrics (CORE)
June 12, 2020

In this paper, we investigate a different approach, which is motivated by a new global second-order lower model of the objective function, introduced in Section 3.
We incorporate this model into a new second-order optimization algorithm, called ContractingDomain Newton Method (Section 4). At every iteration, it minimizes a lower approximation of the smooth component of the objective, augmented by a composite term. The next point is deﬁned as a convex combination of the minimizer, and the previous point. By its nature, it is similar to the scheme of Conditional Gradient Method (or, Frank-Wolfe algorithm, [15, 30]). Under assumption of boundedness of the problem domain, for convex functions with Hölder continuous Hessian of degree ν ∈ [0, 1], we establish its O(1/k1+ν) global rate of convergence in functional residual. In the case ν = 1, for the class of convex function with Lipschitz continuous Hessian, this gives O(1/k2) rate of convergence. As compared with Cubic Newton, the new method is afﬁne-invariant and universal, since it does not depend on the norms and parameters of the problem class. When the composite component is strongly convex (with respect to arbitrary norm), we show O(1/k2+2ν) rate for a universal scheme. If the parameters of problem class are known, we can prove a global linear convergence. We also provide different trust-region interpretations for our algorithm.
In Section 5, we present aggregated models, which accumulate second-order information into quadratic Estimating Functions [31]. This leads to another optimization process, called Aggregating Newton Method, with the global convergence of the same order O(1/k1+ν) as for general convex case. The latter method can be seen as a second-order counterpart of the dual averaging gradient schemes [28, 29].
In Section 6, we consider the problem of ﬁnite-sum minimization. We propose stochastic extensions of our method. During the iterations of the basic variant, we need to increase the batch size for randomized estimates of gradients and Hessians up to the order O(k4) and O(k2) respectively. Using the variance reduction technique for the gradients, we reduce the batch size up to the level O(k2) for both estimates. At the same time, the global convergence rate of the resulting methods is of the order O(1/k2), as for general convex functions with Lipschitz continuous Hessian.
Section 7 contains numerical experiments. Section 8 contains some ﬁnal remarks. All necessary proofs are provided in Appendix.

2 Problem formulation and notations

Our goal is to solve the following composite convex minimization problem:

min F (x) := f (x) + ψ(x),

(1)

x

where ψ : Rn → R ∪ {+∞} is a simple proper closed convex function, and function f is convex

and twice continuously differentiable at every point x ∈ dom ψ. Let us ﬁx an arbitrary (possibly non-Euclidean) norm · on Rn. We denote by D the corresponding diameter of dom ψ:

D := sup x − y .

(2)

x,y∈dom ψ

Our main assumption on problem (1) is that dom ψ is bounded:

D < +∞.

(3)

The most important example of ψ is {0, +∞}-indicator of a simple compact convex set Q = dom ψ. In particular, for a ball in · p-norm with p ≥ 1, this is



1/p

ψ(x) = 0,

x p := ni=1 |x(i)|p

≤ D2 ,

(4)

+∞, else.

From the machine learning perspective, D is usually considered as a regularization parameter in this setting. We denote by ·, · the standard scalar product of two vectors, x, y ∈ Rn:

x, y := ni=1 x(i)y(i).

For function f , we denote its gradient by ∇f (x) ∈ Rn, and its Hessian matrix by ∇2f (x) ∈ Rn×n.

Having ﬁxed the norm · for primal variables x ∈ Rn, the dual norm can be deﬁned in the standard

way:

s ∗ :=

sup s, h .

h∈Rn: h ≤1

2

The dual norm is necessary for measuring the size of gradients. For a matrix A ∈ Rn×n, we use the corresponding induced operator norm, deﬁned as

A :=

sup Ah ∗.

h∈Rn: h ≤1

3 Second-order lower model of objective function

To characterize the complexity of problem (1), we need to introduce some assumptions on the growth

of derivatives. Let us assume that the Hessian of f is Hölder continuous of degree ν ∈ [0, 1] on

dom ψ:

∇2f (x) − ∇2f (y) ≤ Hν x − y ν , x, y ∈ dom ψ.

(5)

The actual parameters of this problem class may be unknown. However, we assume that for some ν ∈ [0, 1] inequality (5) is satisﬁed with corresponding constant 0 ≤ Hν < +∞. The direct consequence of (5) is the following global bounds for Taylor’s approximation, for all x, y ∈ dom ψ

∇f (y) − ∇f (x) − ∇2f (x)(y − x) ∗ ≤ Hν y1−+xν 1+ν , (6)

|f (y) − f (x) − ∇f (x), y − x − 1 ∇2f (x)(y − x), y − x |

≤

Hν

y−x

2+ν
.

(7)

2

(1+ν)(2+ν)

Recall, that in addition to (5), we assume that f is convex:

f (y) ≥ f (x) + ∇f (x), y − x , x, y ∈ dom ψ.

(8)

Employing both smoothness and convexity, we are able to enhance this global lower bound, as follows.

Lemma 1 For all x, y ∈ dom ψ and t ∈ [0, 1], it holds

f (y)

≥

f (x) +

∇f (x), y − x

+t

∇2f (x)(y − x), y − x

− t1+ν Hν

y−x

2+ν
.

(9)

2

(1+ν)(2+ν)

Note that the right-hand side of (9) is concave in t ∈ [0, 1], and for t = 0 we obtain the standard ﬁrst-order lower bound. The maximization of (9) over t gives

f (y) ≥ f (x) + ∇f (x), y − x + γ¯x2,y ∇2f (x)(y − x), y − x , (10)

with

1
γ¯x,y := 1+ν ν min 1, (2+ν)2∇H2νf(yx−)x(y−2+xν),y−x ν , x = y, ν ∈ (0, 1].

Thus, (10) is always tighter than (8), employing additional global second-order information. The relationship between them is shown on Figure 1. Hence, it seems natural to incorporate the secondorder lower bounds into optimization schemes.

4

3

2

1

0

Second-order

1

First-order

3

2

1

0

1

2

3

4

Figure 1: Global lower bounds for logistic regression loss, f (x) = log(1 + exp(x)).

3

4 Contracting-Domain Newton Methods

Let us introduce a general scheme of Contracting-Domain Newton Method, which is based on global second-order lower bounds. Note, that the right hand side of (10) is nonconvex in y. Hence, it can hardly be used directly in a computational algorithm. To tackle this issue, we use a sequence of contracting coefﬁcients {γk}k≥0. Each coefﬁcient γk ∈ (0, 1] can be seen as an appropriate substitute of γ¯x,y in (10). Then, we minimize the corresponding global lower bound augmented by the composite component ψ(·). The next point is taken as a convex combination of the minimizer and the current point. Let us present this method formally, as Algorithm 1.

Algorithm 1: Contracting-Domain Newton Method, I

Initialization. Choose x0 ∈ dom ψ. Iteration k ≥ 0. 1: Pick up γk ∈ (0, 1]. 2: Compute
vk+1 ∈ Argmin ∇f (xk), y − xk
y
3: Set xk+1 := xk + γk(vk+1 − xk).

+

γk 2

∇2f (xk)(y − xk), y − xk

+ ψ(y) .

There is a clear connection of this method with Frank-Wolfe algorithm, [15]. Indeed, instead of the standard ﬁrst-order approximation (8), we use the lower global quadratic model. Thus, as compared with the gradient methods, every iteration of Algorithm 1 is more expensive. However, this is a standard situation with the second-order schemes (see the below discussion on the iteration complexity). At the same time, our method is afﬁne-invariant, since it does not depend on the norms.

It is clear, that for γk ≡ 1 we obtain iterations of the classical Newton method. Its local quadratic convergence for composite optimization problems was established in [26]. However, for the global convergence, we need to adjust the contracting coefﬁcients accordingly. To state the global convergence result, let us introduce the following linear Estimating Functions (see [31]):

φk(x) d=ef

k i=1

ai

f (xi) +

∇f (xi), x − xi

+ ψ(x) ,

φ∗k := min φk(x), (11)
x

for the sequence of test points {xk : xk ∈ dom ψ}k≥1 and positive scaling coefﬁcients {ak}k≥1. We relate them with contracting coefﬁcients, as follows

γk := Aakk++11 ,

Ak d=ef

k i=1

ai.

(12)

Theorem 1 Let Ak := k3, and consequently, γk := 1 − k+k 1 3 = O k1 . Then for the sequence {xk}k≥1 generated by Algorithm 1, we have

F (xk) − F ∗ ≤ k d=ef F (xk) − Aφ∗kk ≤ O HνkD1+2ν+ν .

(13)

For the case ν = 1 (convex functions with Lipschitz continuous Hessian), estimate (13) gives the

convergence

rate

of

the

order

O(

1 k2

).

This

is

the

same

rate,

as

we

can

achieve

on

this

functional

class

by Cubic Regularization of Newton Method [33]. In accordance to (13), in order to obtain ε-accuracy

in functional residual, F (xK ) − F ∗ ≤ ε, it is enough to perform

K = O Hν Dε2+ν 1/(1+ν)

(14)

iterations of Algorithm 1. In [17], there were proposed ﬁrst universal second-order methods (which do not depend on parameters ν and Hν of the problem class), having complexity guarantees of the same order (14). These methods are based on Cubic regularization and an adaptive search for estimating the regularization parameter at every iteration. It is important that Algorithm 1 is both universal and afﬁne-invariant. Additionally, convergence result (13) provides us with a sequence { k}k≥1 of computable accuracy certiﬁcates, which can be used as a stopping criterion of the method.

Now, let us assume that the composite component is strongly convex with parameter µ > 0. Thus, for all x, y ∈ dom ψ and ψ (x) ∈ ∂ψ(x), it holds
ψ(y) ≥ ψ(x) + ψ (x), y − x + µ2 y − x 2. (15) In this situation, we are able to improve convergence estimate (13), as follows.

4

Theorem 2 Let Ak := k5, and consequently, γk := 1 − k+k 1 5 = O k1 . Then for the sequence {xk}k≥1 generated by Algorithm 1, we have

F (xk) − F ∗ ≤

≤ O · . Hν Dν Hν D2+ν

k

µ

k2+2ν

(16)

Moreover, if the second-order condition number

ων d=ef

1
Hν Dν 1+ν
(1+ν)µ

(17)

is known, then, deﬁning Ak := (1 + ων−1)k, k ≥ 1, A0 := 0, and γk := 1+1ων , k ≥ 1, γ0 := 1, we obtain the global linear rate of convergence

F (xk) − F ∗ ≤ k ≤ exp − 1k+−ω1ν · Hν1D+2ν+ν .

(18)

According to the estimate (18), in order to get ε-accuracy in function value, it is enough to perform
K = O (1 + ων ) · log F (x0ε)−F ∗ iterations of the method. Hence, condition number ων plays the role of the main complexity factor. This rate corresponds to that one of Cubically Regularized Newton Method (see [11, 12]). At the same time, there exists a second variant of Contracting-Domain Newton Method, where the next point is deﬁned by minimization of the full second-order model for the smooth component augmented by the composite term over the contracted domain (this explains the names of our methods).

Algorithm 2: Contracting-Domain Newton Method, II

Initialization. Choose x0 ∈ dom ψ.

Iteration k ≥ 0.

1: Pick up γk ∈ (0, 1].

2: Denote

Sk(y) :=

ψ(y), +∞,

3: Compute

xk+1 ∈ Argmin ∇f (xk), y − xk
y

y ∈ γkdom ψ + (1 − γk)xk, else.

+

1 2

∇2f (xk)(y − xk), y − xk

+ Sk(y) .

Note, that Algorithm 1 admits similar representation as well. 3 Both methods produce the same sequences of points when ψ(·) is {0, +∞}-indicator of a convex set. Otherwise, they are different. Using the same contraction technique, it was shown in [30] that the classical Frank-Wolfe algorithm can be extended onto the case of the composite optimization problems. Additionally, the second-order Contracting Trust-Region method was proposed, which has the same form as Algorithm 2. However, its convergence rate was established only at the level O( k1 ). Here, we improve its rate as follows.
Theorem 3 Let Ak := k3 and γk := 1 − k+k 1 3 = O k1 . Then for the sequence {xk}k≥1 generated by Algorithm 2, we have

F (xk) − F ∗ ≤

≤ O . Hν D2+ν

k

k1+ν

(19)

This result is very similar to Theorem 1. However, the ﬁrst algorithm can be accelerated on the class of strongly convex functions (see Theorem 2). Thus, it seems that it is more preferable.
Finally, let us consider an example, when the composite component ψ(·) is an p-ball, as in (4). Then, iterations of the method can be represented as
xk+1 ∈ xk + Arghmin ∇f (xk), h + 12 ∇2f (xk)h, h : xk + γ1k h p ≤ D2 . (20) In this form, it looks as a variant of Trust-Region scheme. To solve the subproblem in (20), we can use Interior Point Methods (e.g. Chapter 5 in [31]). See also [9], for techniques, developed for TrustRegion schemes. Usually, complexity of this step can be estimated as O(n3) arithmetic operations,
3Indeed, it is enough to take Sk(y) := γkψ(xk + γ1k (y − xk)).

5

which comes from the cost of computing a suitable factorization for the Hessian matrix. Alternatively, Hessian-free gradient methods can be applied, for computing an inexact step (see [6, 5]).

5 Aggregated second-order models

In this section, we propose more advanced second-order models, based on global lower bound (9).
Using the same notation as before, consider a sequence of test points {xk : xk ∈ dom ψ}k≥0 and sequences of coefﬁcients {ak}k≥1, {γk}k≥0, satisfying the relations (12). Then, we can introduce the following Quadratic Estimating Functions (compare with deﬁnition (11)):

Qk(x) d=ef

k−1 i=0

ai+1

f (xi) +

∇f (xi), x − xi

+ γ2i

∇2f (xi)(x − xi), x − xi

+ ψ(x) .

By (9), we have the main property of Estimating Functions being satisﬁed. Namely, for all x ∈ dom ψ

(9)
AkF (x) ≥ Qk(x) −

k−1 ai+1γi1+ν Hν x−xi 2+ν

i=0

(1+ν)(2+ν)

(≥2) Qk(x) − (1H+ννD)(22++νν)

k−1 i=0

ai+1γi1+ν

=:

Qk (x)

−

Ck 2

.

(21)

Therefore, if we would be able to guarantee for our test points the relation

Q∗k

:=

min Qk(x)

≥

AkF (xk)

−

Ck 2

,

(22)

x

then we could immediately obtain the global convergence in function value. Fortunately, relation (22) can be achieved by simple iterations.

Algorithm 3: Aggregating Newton Method

Initialization. Choose x0 ∈ dom ψ. Set A0 := 0, Q0(x) ≡ 0. Iteration k ≥ 0. 1: Pick up ak+1 > 0. Set Ak+1 := Ak + ak+1 and γk := Aakk++11 . 2: Update Estimating Function
Qk+1(x) ≡ Qk(x) + ak+1 f (xk)+ ∇f (xk), x−xk + γ2k ∇2f (xk)(x−xk), x−xk +ψ(x) .

3: Compute

vk+1 ∈ Argmin Qk+1(x).

x

4: Set xk+1 := xk + γk(vk+1 − xk).

Clearly, the most complicated part of this process is Step 3, which is computation of the minimum of Estimating Function. However, the complexity of this step remains the same, as that one for Contracting-Domain Newton Method. We obtain the following convergence result.

Theorem 4 For the sequence {xk}k≥1 generated by Algorithm 3, relation (22) is satisﬁed. Consequently, for the choice Ak := k3, we obtain

F (xk) − F ∗ (2≤1) F (xk) − QAk∗k + 2CAkk (2≤2) ACkk ≤ O HνkD1+2ν+ν .

(23)

Now,

for

the

accuracy

certiﬁcate

we

have

new

expression

¯
k

:=

F (xk) −

Q∗k A

+

Ck 2A

.

The

value

of

Q∗k

k

k

is

available

within

the

method

directly.

However,

in

order

to

compute

¯
k

in

practice,

some

estimate

for Ck is required. Note, that for the given choice of coefﬁcients Ak := k3, we have ak = O(k2)

and γk = O( k1 ). Therefore, new information enters into the model with increasing weights, which

seems to be natural.

6 Stochastic ﬁnite-sum minimization

In this section, we consider the case when the smooth part f of the objective (1) is represented as a sum of M convex twice-differentiable components,

f (x) := M1

M i=1

fi

(x).

(24)

6

This setting appears in many machine learning applications, such as empirical risk minimization. Often, the number M is very big. Thus, it becomes expensive to evaluate the whole gradient or the Hessian at every iteration. Hence, stochastic or incremental methods are the methods of choice in this situation. See [4] for a survey of ﬁrst-order incremental methods. The Newton-type Incremental Method with superlinear local convergence was proposed in [35]. Local linear rate of stochastic Newton methods was studied in [25]. Global convergence of sub-sampled Newton schemes, based on Damped iterations, and on Cubic regularization, was established in [36, 24, 39].

The basic idea of stochastic algorithms is to substitute the true gradients and Hessians by some random unbiased estimators gk, and Hk, respectively, with E[gk] = ∇f (xk) and E[Hk] = ∇2f (xk).

First, let us consider the simplest estimation strategy. At iteration k, we sample uniformly and independently two subsets of indices Skg, SkH ⊆ {1, . . . , M }. Their sizes are mgk := |Skg| and mHk := |SkH |, which are possibly different. Then, in Algorithm 1, we can use the following random estimators:

gk

:=

1 mg

k

i∈Sg ∇fi(xk), k

Hk :=

1
H

mk

i∈SH ∇2fi(xk). k

(25)

Let us present for this process a result on its global convergence. Note that in this section, we use the

standard Euclidean norm for vectors and the corresponding induced spectral norm for matrices.

Theorem 5 Let each component fi(·) be Lipschitz continuous on dom ψ with constant L0, and have Lipschitz continuous gradients and Hessians on dom ψ with constants L1 and L2, respectively. Let γk := 1 − k+k 1 3 = O k1 . Set

mgk := 1/γk4, mHk := 1/γk2.

(26)

Then, for the iterations {xk}k≥1 of Algorithm (1), based on estimators (25), it holds

E[F (xk) − F ∗] ≤ O L2D3 + L1D2(k1+2 log(n)) + L0D .

(27)

Therefore, in order to solve our problem with ε-accuracy in expectation, E[F (xK) − F ∗] ≤ ε, we need to perform K = O ε11/2 iterations of the method. In this case, the total number of gradient and Hessian samples are O ε51/2 and O ε31/2 , respectively. It is interesting that we need higher accuracy for estimating the gradients, which results in a bigger batch size.
To improve this result, we incorporate a simple variance reduction strategy for the gradients. This is a popular technique in stochastic convex optimization (see [37, 21, 10, 20, 1, 34, 16] and references therein). At some iterations, we recompute the full gradient. However, during the whole optimization process this happens logarithmic number of times in total. Let us denote by π(k) the maximal power of two, which is less than or equal to k: π(k) := 2 log2 k , for k > 0, and deﬁne π(0) := 0. The entire scheme looks as follows.

Algorithm 4: Stochastic Variance-Reduced Contracting-Domain Newton

Initialization. Choose x0 ∈ dom ψ.

Iteration k ≥ 0.

1: Set anchor point zk := xπ(k).

2: Sample random batch Sk ⊆ {1, . . . , M } of size mk.

3: Compute variance-reduced stochastic gradient

gk := m1k i∈Sk ∇fi(xk) − ∇fi(zk) + ∇f (zk) .

4: Compute stochastic Hessian Hk := m1k

i∈Sk ∇2fi(xk).

5: Pick up γk ∈ (0, 1].

6: Perform the main step

xk+1 ∈ Argymin gk, y − xk + 21 Hk(y − xk), y − xk + γkψ(xk + γ1k (y − xk)) .

Note that this is just Algorithm 1 with random estimators gk and Hk instead ot the true gradient and Hessian. The following global convergence result holds.

7

Theorem 6 Let each component fi(·) have Lipschitz continuous gradients and Hessians on dom ψ with constants L1 and L2, respectively. Let γk := 1 − k+k 1 3 = O( k1 ). Set batch size

mk := 1/γk2.

(28)

Then, for all iterations {xk}k≥1 of Algorithm 4, we have

E[F (xk) − F ∗] ≤ O L2D3 + L1D2(1+log(nk)2) + L11/2D(F (x0)−F ∗) .

(29)

It is thanks to the variance reduction that we can use the same batch size for both estimators now. To solve the problem with ε-accuracy in expectation, we need K = O ε11/2 iterations of the method. And the total number of gradient and Hessian samples during these iterations is O ε31/2 .

7 Experiments

Let us demonstrate computational results for the problem of training Logistic Regression model, regularized by 2-ball constraints. Thus, the smooth part of the objective has the ﬁnite-sum representation (24), each component is fi(x) := log(1 + exp( ai, x )). The composite part is given by (4), with p = 2. Diameter D plays the role of regularization parameter, while vectors {ai : ai ∈ Rn}M i=1 are determined by the dataset4. First, we compare the performance of Contracting-Domain Newton Method (Algorithm 1) and Aggregating Newton Method (Algorithm 3) with ﬁrst-order optimization schemes: Frank-Wolfe algorithm [15], the classical Gradient Method, and the Fast Gradient Method [29]. For the latter two we use a line-search at each iteration, to estimate the Lipschitz constant. The results are shown on Figure 2.

Func. residual Func. residual Func. residual

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7
0

w8a, D = 20 Frank-Wolfe Grad. Method Fast Grad. Method Contr. Newton Aggr. Newton
0.5s
0.25s 4.58s 7s 0.28s
50 Iter1a0t0ions 150 200

w8a, D = 100

101

100

10 1

10 2

2.4s

5.1s

10 3

10 4

2.5s

10 5 6.99s

5s

10 6 4.48s

10 7

4.59s

0

500 Ite1ra0t0i0ons 1500 2000

102 w8a, D = 500

100

10 2
10 4 15s 10 6 1.39s

12s 15s
15s

0 1000 Ite2ra0t0i0ons 3000 4000

Figure 2: Training logistic regression, w8a (M = 49749, n = 300).

We see, that for bigger D, it becomes harder to solve the optimization problem. Second-order methods demonstrate good performance both in terms of the iterations, and the total computational time. 5

Func. residual Func. residual Func. residual

10 1 10 2 10 3 10 4 10 5 10 6 10 7 0

covtype, D = 20

19.92s 20s

20.17s
1s 20 40

E6p0ochs80

SGD SVRG SNewton SVRNewton 100 120

covtype, D = 100
100

10 1

20.03s

10 2

10 3

19.75s

10 4

20s

10 5

10 6

10 7

19.71s

0 25 50 Ep7o5chs 100 125 150

101 covtype, D = 500

100

10 1

10 2

10 3 20.2s 20s

10 4

20s

10 5

20.02s

10 6

10 7 0 25 50 E7p5ochs100 125 150

Figure 3: Stochastic methods for training logistic regression, covtype (M = 581012, n = 54).

4https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ 5Clock time was evaluated using the machine with Intel Core i5 CPU, 1.6GHz; 8 GB RAM. All methods were implemented in C++. The source code can be found at https://github.com/doikov/
contracting-newton/

8

In the next set of experiments, we compare the basic stochastic version of our method, using estimators (25) — SNewton, the method with the variance reduction (Algorithm 4) — SVRNewton, and ﬁrst-order algorithms (with constant step-size, tuned for each problem): SGD and SVRG [21]. We see (Figure 3) that using the variance reduction strategy signiﬁcantly improve the convergence for both ﬁrst-order and second-order stochastic optimization methods.
According to these graphs, our second-order algorithms can be more efﬁcient when solving illconditioned problems, producing the better solution within a given computational time. See also Section E in Appendix for extra experiments.
8 Discussion
Let us discuss complexity estimates, which we established in our work. For the basic versions of our method we have the global convergence in the functional residual of the form
F (xk) − F ∗ ≤ O HνkD1+2ν+ν . Note that the complexity parameter Hν depends only on the variation of the Hessian (in arbitrary norm). It can be much smaller than the maximal eigenvalue of the Hessian, which typically appears in the rates of ﬁrst-order methods. It is important that our algorithms are free from using the norms or any other particular parameters of the problem class.
At the same time, the arithmetic complexity of one step of our methods for simple sets can be estimated as the sum of the cost of computing the Hessian, and O(n3) additional operations (to compute a suitable factorization of the matrix). For example, the cost of computing the gradient of Logistic Regression is O(M n), and the Hessian is O(M n2), where M is the dataset size. Hence, it is preferable to use our algorithms with exact steps in the situation when M is much bigger than n.
Acknowledgments
The research results of this paper were obtained in the framework of ERC Advanced Grant 788368.
References
[1] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. The Journal of Machine Learning Research, 18(1):8194–8244, 2017.
[2] Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384–414, 2010.
[3] Albert A Bennett. Newton’s method in general analysis. Proceedings of the National Academy of Sciences, 2(10):592–598, 1916.
[4] Dimitri P Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. Optimization for Machine Learning, 2010(1-38):3, 2011.
[5] Alejandro Carderera and Sebastian Pokutta. Second-order conditional gradients. arXiv preprint arXiv:2002.08907, 2020.
[6] Yair Carmon and John C Duchi. First-order methods for nonconvex quadratic minimization. arXiv preprint arXiv:2003.04546, 2020.
[7] Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation methods for unconstrained optimization. Part I: motivation, convergence and numerical results. Mathematical Programming, 127(2):245–295, 2011.
[8] Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation methods for unconstrained optimization. Part II: worst-case function-and derivative-evaluation complexity. Mathematical programming, 130(2):295–319, 2011.
[9] Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust region methods. SIAM, 2000. [10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646–1654, 2014.
9

[11] Nikita Doikov and Yurii Nesterov. Local convergence of tensor methods. CORE Discussion Papers 2019/21, 2019.
[12] Nikita Doikov and Yurii Nesterov. Minimizing uniformly convex functions by cubic regularization of Newton method. arXiv preprint arXiv:1905.02671, 2019.
[13] Nikita Doikov and Peter Richtárik. Randomized block cubic Newton method. In International Conference on Machine Learning, pages 1289–1297, 2018.
[14] Pavel Dvurechensky and Yurii Nesterov. Global performance guarantees of second-order methods for unconstrained convex minimization. Technical report, CORE Discussion Paper, 2018.
[15] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95–110, 1956.
[16] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. A uniﬁed theory of sgd: Variance reduction, sampling, quantization and coordinate descent. arXiv preprint arXiv:1905.11261, 2019.
[17] Geovani N Grapiglia and Yurii Nesterov. Regularized Newton methods for minimizing functions with Hölder continuous Hessians. SIAM Journal on Optimization, 27(1):478–506, 2017.
[18] Geovani N Grapiglia and Yurii Nesterov. Accelerated regularized Newton methods for minimizing composite convex functions. SIAM Journal on Optimization, 29(1):77–99, 2019.
[19] Filip Hanzely, Nikita Doikov, Peter Richtárik, and Yurii Nesterov. Stochastic subspace cubic Newton method. arXiv preprint arXiv:2002.09526, 2020.
[20] Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In International Conference on Machine Learning, pages 1263–1271, 2016.
[21] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315–323, 2013.
[22] Leonid V Kantorovich. Functional analysis and applied mathematics. Uspekhi Matematicheskikh Nauk, 3(6):89–185, 1948.
[23] Sai Praneeth Karimireddy, Sebastian U Stich, and Martin Jaggi. Global linear convergence of Newton’s method without strong-convexity or Lipschitz gradients. arXiv preprint arXiv:1806.00413, 2018.
[24] Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex optimization. In International Conference on Machine Learning, pages 1895–1904, 2017.
[25] Dmitry Kovalev, Konstantin Mishchenko, and Peter Richtárik. Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates. arXiv preprint arXiv:1912.01597, 2019.
[26] Jason D Lee, Yuekai Sun, and Michael A Saunders. Proximal Newton-type methods for minimizing composite functions. SIAM Journal on Optimization, 24(3):1420–1443, 2014.
[27] Yurii Nesterov. Accelerating the cubic regularization of Newton’s method on convex problems. Mathematical Programming, 112(1):159–181, 2008.
[28] Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221–259, 2009.
[29] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming, 140(1):125–161, 2013.
[30] Yurii Nesterov. Complexity bounds for primal-dual methods minimizing the model of objective function. Mathematical Programming, 171(1-2):311–330, 2018.
[31] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[32] Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM, 1994.
[33] Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton’s method and its global performance. Mathematical Programming, 108(1):177–205, 2006.
[34] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takácˇ. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In International Conference on Machine Learning, pages 2613–2621, 2017.
10

[35] Anton Rodomanov and Dmitry Kropotov. A superlinearly-convergent proximal Newton-type method for the optimization of ﬁnite sums. In International Conference on Machine Learning, pages 2597–2605, 2016.
[36] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods i: globally convergent algorithms. arXiv preprint arXiv:1601.04737, 2016.
[37] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83–112, 2017.
[38] Tianxiao Sun and Quoc Tran-Dinh. Generalized self-concordant functions: a recipe for Newtontype methods. Mathematical Programming, 178(1-2):145–213, 2019.
[39] Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic regularization for fast nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2899–2908, 2018.
[40] Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends® in Machine Learning, 8(1-2):1–230, 2015.
11

Appendix

A Proof of Lemma 1

First, let us note that inequality (6) follows from the following simple observation, using NewtonLeibniz formula and Hölder continuity of the Hessian, for all x, y ∈ dom ψ

∇f (y) − ∇f (x) − ∇2f (x)(y − x) ∗ =

1
(∇2f (x + τ (y − x)) − ∇2f (x))(y − x)dτ ∗
0

We are ready to prove the lemma.

(5)
≤

Hν

y−x

1+ν
.

1+ν

Lemma 1 For all x, y ∈ dom ψ and t ∈ [0, 1], it holds

f (y)

≥

f (x) +

∇f (x), y − x

+t

∇2f (x)(y − x), y − x

− t1+ν Hν

y−x

2+ν
.

2

(1+ν)(2+ν)

Proof: Let us prove the following bound, for all x, y ∈ dom ψ and t ∈ [0, 1]

∇f (y) − ∇f (x), y − x

≥

t ∇2f (x)(y − x), y − x

− t1+ν Hν

y−x

2+ν
.

(30)

1+ν

For t = 1 it follows from (6). Therefore, we may assume that t < 1. Let us take zt := x + t(y − x). Then, by convexity of f , we have

∇f (y), y − x

=

1 1−t

∇f (y), y − zt

≥

1 1−t

∇f (zt), y − zt

=

∇f (zt), y − x .

Now, from Hölder continuity of the Hessian, we get

∇f (zt), y − x (≥6) ∇f (x), y − x + ∇2f (x)(zt − x), y − x − Hν zt−x1+1ν+ν y−x

=

∇f (x), y − x

+ t ∇2f (x)(y − x), y − x

− t1+ν Hν

y−x

2+ν
.

1+ν

Thus we prove (30). Then, the claim of the lemma can be obtained by simple integration:

f (y) − f (x) − ∇f (x), y − x

1
= ∇f (zτ ) − ∇f (x), y − x dτ
0

(30)
≥

1
tτ

∇2f (x)(y − x), y − x

− (tτ )1+ν Hν

y−x

2+ν
dτ

1+ν

0

=

t

∇2f (x)(y − x), y − x

− t1+ν Hν

y−x

2+ν
.

2

(1+ν)(2+ν)

12

B Convergence of Contracting-Domain Newton Method

In this section, we prove the global convergence of Algorithms 1 and 2. We use the same notation as in the main part. There is a sequence of controlling coefﬁcients {ak}k≥1 (see relations (12)), and a sequence of linear Estimating Functions {φk(x)}k≥0. We denote by µ ≥ 0 the constant of strong convexity of ψ(·). We allow µ = 0 in the following auxiliary lemma, in order to cover both the
general convex and the strongly convex cases.

Lemma 2 For the sequences {xk}k≥1 and {vk}k≥1, produced by Algorithm 1, we have

AkF (xk) ≤ φk(x) + Bk(x), x ∈ dom ψ,

(31)

with

B (x) ≡ − − . k Hν a2i +ν x−vi · xi−1−vi 1+ν

µai x−vi 2

µaiAi−1 xi−1−vi 2

k i=1 (1+ν)A1i +ν

2

2Ai

(32)

Proof:

Let us prove (31) by induction. It obviously holds for k = 0, since A0 := 0, φ0(x) ≡ 0, and B0(x) ≡ 0 by deﬁnition. Assume that it holds for the current k ≥ 0, and consider the next iterate. Stationary condition for the method step is

∇f (xk) + ∇2f (xk)(xk+1 − xk), x − vk+1 + ψ(x)

≥

ψ(vk+1)

+

µ 2

x − vk+1

2,

(33)

for all x ∈ dom ψ. Then, we have

φk+1(x) ≡ ak+1 f (xk+1) + ∇f (xk+1), x − xk+1 + ψ(x) + φk(x)

(31)
≥ ak+1 f (xk+1) + ∇f (xk+1), x − xk+1 + ψ(x) + AkF (xk) − Bk(x)

(∗)
≥ Ak+1 f (xk+1) + ∇f (xk+1), ak+1Axk++A1 kxk − xk+1 + ak+1ψ(x) + Akψ(xk) − Bk(x)

= Ak+1f (xk+1) + ak+1 ∇f (xk+1), x − vk+1 + ak+1ψ(x)

+ Akψ(xk) − Bk(x) = Ak+1f (xk+1) + ak+1 ∇f (xk) + ∇2f (xk)(xk+1 − xk), x − vk+1 + ψ(x)
+ ak+1 ∇f (xk+1) − ∇f (xk) − ∇2f (xk)(xk+1 − xk), x − vk+1

+ Akψ(xk) − Bk(x)

(33),(6)
≥

Ak+1f (xk+1) + ak+1

ψ(vk+1)

+

µ 2

x − vk+1

2

− Hν a2k++ν1 x−(1v+k+ν1)A·1+vkν+1−xk 1+ν + Akψ(xk) − Bk(x) k+1

(∗∗)
≥

Ak+1F (xk+1) + µak+1 x2−vk+1 2 + µ2aAk+k1+A1 k xk − vk+1 2

− Hν a2k++ν1 x−(1v+k+ν1)A·1+vkν+1−xk 1+ν + Akψ(xk) − Bk(x) k+1

≡ Ak+1F (xk+1) − Bk+1(x),
where (∗) and (∗∗) stand for convexity of f , and strong convexity of ψ, correspondingly. Thus we have (31) established for all k ≥ 0.

13

B.1 Proof of Theorem 1
Theorem 1 Let Ak := k3, and consequently, γk := 1 − k+k 1 3 = O k1 . Then for the sequence {xk}k≥1 generated by Algorithm 1, we have
F (xk) − F ∗ ≤ k d=ef F (xk) − Aφ∗kk ≤ O HνkD1+2ν+ν .

Proof:

First, by convexity of f we have, for all x ∈ dom ψ

φk(x) ≤ AkF (x). Therefore, for the solution x∗ of our problem: F ∗ = F (x∗), it holds

F (xk) − F ∗ ≤ F (xk) − φkA(xk∗) ≤ and this is the ﬁrst part of (13).

k d=ef F (xk) − Aφ∗kk ,

At the same time, by Lemma 2, and using boundness of the domain, we have

φ∗k := min φk(x)
x∈dom ψ

(31)
≥ min AkF (xk) − Bk(x)
x∈dom ψ

≥ AkF (xk) − Hν1D+2ν+ν k Aa2i1++νν
i=1 i

Therefore, for the choice Ak := k3, we ﬁnally obtain

≤ = Hν D2+ν k a2i +ν

Hν D2+ν k (i3−(i−1)3)2+ν

k (1+ν)Ak i=1 A1i +ν (1+ν)k3 i=1 i3(1+ν)

≤ = i Hν D2+ν k 32+ν i2(2+ν)

(1+ν)k3 i=1

i3(1+ν)

32+ν Hν D2+ν k (1+ν)k3 i=1

1−ν

= O . Hν D2+ν
k1+ν

B.2 Proof of Theorem 2

Theorem 2 Let Ak := k5, and consequently, γk := 1 − k+k 1 5 = O k1 . Then for the sequence {xk}k≥1 generated by Algorithm 1, we have

F (xk) − F ∗ ≤

≤ O · . Hν Dν Hν D2+ν

k

µ

k2+2ν

Moreover, if the second-order condition number

ων d=ef

1
Hν Dν 1+ν
(1+ν)µ

is known, then, deﬁning Ak := (1 + ων−1)k, k ≥ 1, A0 := 0, and γk := 1+1ων , k ≥ 1, γ0 := 1, we obtain the global linear rate of convergence

F (xk) − F ∗ ≤ k ≤ exp − 1k+−ω1ν · Hν1D+2ν+ν .

Proof: Starting from the same reasoning, as in the proof of Theorem 1, we get
F (xk) − F ∗ ≤ k d=ef F (xk) − Aφ∗kk .

14

Let us denote by uk the minimum of the Estimating Function φk. Thus,

(31)

k

k = F (xk) − φkA(ukk) ≤ A1k Bk(uk) ≡ A1k i=1 Bk(i),

with

B = a − − (i) def
k

Hν a1i +ν uk −vi · xi−1−vi 1+ν i (1+ν)A1i +ν

µ uk−vi 2 2

µaiAi−1 xi−1−vi 2 2Ai

≤ a max − − Hν a1i +ν xi−1−vi 1+ν t

µt2

i t≥0

(1+ν)A1i +ν

2

µaiAi−1 xi−1−vi 2 2Ai

= − . ai Hν a1i +ν xi−1−vi 1+ν 2
2µ (1+ν)A1i +ν

µaiAi−1 xi−1−vi 2 2Ai

Therefore, for the choice Ak := k5, we have

≤ ≤ 1

k ai

Hν a1i +ν xi−1−vi 1+ν 2

Hν2 D2(1+ν )

k a2(1+ν)+1
i

k Ak i=1 2µ (1+ν)A1i +ν

2µ(1+ν)2Ak i=1 A2i (1+ν)

= ≤ i Hν2D2(1+ν) k (i5−(i−1)5)2(1+ν)+1

2µ(1+ν )2 k5 i=1

i10(1+ν)

52(1+ν)+1Hν2D2(1+ν) k 2µ(1+ν )2 k5 i=1

2−2ν

= O · . Hν Dν
µ

Hν D2+ν k2+2ν

Thus we have justiﬁed (16). To obtain the linear rate (18), we set

Ak := (1 + ων−1)k, k ≥ 1,

and A0 := 0. So, a1 = A1 and ai = Ai − Ai−1 = ων−1Ai−1,

i ≥ 2.

Therefore, for the values {Bk(i)}ki=1, we have

Bk(1)

≤

a Hν D2+ν
1 1+ν

= A1 Hν1D+2ν+ν ,

and

Bk(i)

(34)
≤

− Hν2D2ν xi−1−vi 2a3i +2ν
2µ(1+ν)2A2i +2ν

µaiAi−1 xi−1−vi 2 2Ai

= − 1 µaiAi−1 xi−1−vi 2
2Ai

Hν Dν (1+ν)µ

2 a2i +2ν A1i +2ν Ai−1

≤ − 1 µaiAi−1 xi−1−vi 2
2Ai

Hν Dν 2 ai 2(1+ν)

(1+ν)µ

Ai−1

since by our choice
Finally, we obtain
k

= 0, 2 ≤ i ≤ k,

ai Ai−1

=

ω−1

(17)
=

ν

≤ A1k Bk(1) ≤ AAk1 · Hν1D+2ν+ν ≤ exp − 1k+−ω1ν · Hν1D+2ν+ν .

(1+ν)µ Hν Dν

1
. 1+ν

= · 1
(1+ων−1 )k−1

Hν D2+ν 1+ν

(34)

15

B.3 Proof of Theorem 3

Theorem 3 Let Ak := k3 and γk := 1 − k+k 1 3 = O k1 . Then for the sequence {xk}k≥1 generated by Algorithm 2, we have

F (xk) − F ∗ ≤

≤ O . Hν D2+ν

k

k1+ν

Proof:
The proof is very similar to that one for Algorithm 1. First, stationary condition for one iteration of Algorithm 2 is
∇f (xk) + ∇2f (xk)(xk+1 − xk), x − vk+1 + γ1k ψ γkx + (1 − γk)xk (35)
≥ γ1k ψ(xk+1),
for all x ∈ dom ψ and k ≥ 0 (compare with (33)), where
vk+1 := xk + γ1k (xk+1 − xk) ∈ dom ψ.

Now, let us prove by induction the following bound

φk(x) ≥ AkF (xk) − Bk, x ∈ dom ψ,

(36)

with Bk := Hν1D+2ν+ν

ki=1 Aa2i1++νν . It obviously holds for k = 0, since both sides are zero. Assume i

that it holds for the current k ≥ 0. Then, we have for the next iterate

φk+1(x) ≡ ak+1 f (xk+1) + ∇f (xk+1), x − xk+1 + ψ(x) + φk(x)

(36)
≥ ak+1 f (xk+1) + ∇f (xk+1), x − xk+1 + ψ(x) + AkF (xk) − Bk

(∗)
≥ Ak+1 f (xk+1) + ∇f (xk+1), ak+1Axk++A1 kxk − xk+1 + ak+1ψ(x) + Akψ(xk)

− Bk

(∗∗)
≥ Ak+1 f (xk+1) + ∇f (xk+1), ak+1Axk++A1 kxk − xk+1 + ψ ak+1Axk++A1 kxk

− Bk,

where (∗) and (∗∗) stand for convexity of f and ψ, correspondingly. Using both stationary condition and smoothness, we obtain, for all x ∈ dom ψ

∇f (xk+1), ak+1Axk++A1 kxk − xk+1 + ψ ak+1Axk++A1 kxk

= γk ∇f (xk+1), x − vk+1 + ψ γkx + (1 − γk)xk

= γk ∇f (xk) + ∇2f (xk)(xk+1 − xk), x − vk+1 + ψ γkx + (1 − γk)xk

+ γk ∇f (xk+1) − ∇f (xk) − ∇2f (xk)(xk+1 − xk), x − vk+1

(35≥),(6) ψ(xk+1) − γkHν xk+1−1x+k ν1+ν x−vk+1

= ψ(x ) − k+1

γk2+ν Hν vk+1−xk 1+ν x−vk+1 1+ν

≥ ψ(xk+1) − γk2+ν1H+ννD2+ν . Therefore, we have
φk+1(x) ≥ Ak+1 f (xk+1) + ψ(xk+1) − γk2+ν1H+ννD2+ν

− Bk

= Ak+1F (xk+1) − Bk+1,

16

and (36) is justiﬁed for all k ≥ 0. Finally, by convexity of f , we get F (xk) − F ∗ ≤ k d=ef F (xk) − Aφ∗kk

≤ = (36)
Bk Ak

Hν D2+ν k a2i +ν (1+ν)Ak i=1 A1i +ν

= O , Hν D2+ν
k1+ν
where the last equation holds from the choice Ak := k3 (see the end of the proof of Theorem 1).

C Convergence of Aggregating Newton Method

In this section, we establish the convergence result for Algorithm 3.

C.1 Proof of Theorem 4 Theorem 4 For the sequence {xk}k≥1 generated by Algorithm 3, relation (22) is satisﬁed.

Proof: Let us establish the relation (22) by induction. It obviously holds for k = 0. Assume that it is proven for the current iterate k ≥ 0, and consider the next step:
Qk+1(vk+1)
≡ ak+1 f (xk) + ∇f (xk), vk+1 − xk + γ2k ∇2f (xk)(vk+1 − xk), vk+1 − xk

+ ψ(vk+1) + Qk(vk+1)

(22)
≥ ak+1 f (xk) + ∇f (xk), vk+1 − xk + γ2k ∇2f (xk)(vk+1 − xk), vk+1 − xk

+ ψ(vk+1)

+

AkF (xk) −

Ck 2

= Ak+1 f (xk) + γk ∇f (xk), vk+1 − xk + γ2k2 ∇2f (xk)(vk+1 − xk), vk+1 − xk ]

+

ak+1ψ(vk+1)

+

Ak ψ (xk )

−

Ck 2

= Ak+1 f (xk) + ∇f (xk), xk+1 − xk + 12 ∇2f (xk)(xk+1 − xk), xk+1 − xk ]

+

ak+1ψ(vk+1)

+

Ak ψ (xk )

−

Ck 2

(≥7) Ak+1 f (xk+1) − Hν (1x+k+ν1)(−2x+kν)2+ν + ak+1ψ(vk+1) + Akψ(xk) − C2k

= Ak+1f (xk+1) − Ak+1γk2+(ν1H+νν)(v2k++ν1)−xk 2+ν + ak+1ψ(vk+1) + Akψ(xk) − C2k

≥ Ak+1f (xk+1) − ak+(11γ+k1+ ν)ν(H2+ν Dν)2+ν + Ak+1ψ(xk+1) − C2k

=

Ak+1F (xk+1)

−

. Ck+1
2

Thus, we have (22) justiﬁed for all k ≥ 0.

17

D Convergence of stochastic methods

Let us consider the following general iterations, for solving optimization problem (1):

xk+1 ∈ Argmin gk, y − xk + 21 Hk(y − xk), y − xk + Sk(y) , k ≥ 0

(37)

y

with Sk(y) := γkψ(xk + γ1k (y − xk)). This is Algorithm 1 with substituted vector gk and matrix Hk instead of the true gradient and the Hessian. First, we need to study the convergence of this process. For simplicity, let us study the case ν = 1 only (convex functions with Lipschitz continuous Hessian, we denote the corresponding Lipschitz constant by L2). Recall, that in this section we use the standard Euclidean norm for vectors and induced spectral norm for matrices.
As before, we use the sequence of positive numbers {ak}k≥1, and set

γk := Aakk++11 ,

Ak d=ef

k
ai.

i=1

Lemma 3 For iterations (37), we have for all k ≥ 1

F (xk) − F ∗ ≤ BAkk , (38)

with

Bk := L22D3 k−1 Aa3i2+1 + D k−1 ai+1 ∇f (xi) − gi + D2 k−1 Aa2ii++11 ∇2f (xi) − Hi .

i=0 i+1

i=0

i=0

Proof: Let us prove by induction the following inequality

AkF (x) ≥ AkF (xk) − Bk, x ∈ dom ψ.

(39)

It obviously holds for k = 0, and for k ≥ 1 it is equivalent to (38). Assume that (39) is satisﬁed for some k ≥ 0, and consider the next step:

Ak+1F (x) = ak+1F (x) + AkF (x)

(39)
≥ ak+1F (x) + AkF (xk) − Bk

(∗)
≥ Ak+1f ak+1Axk++A1 kxk + ak+1ψ(x) + Akψ(xk) − Bk
(∗)
≥ Ak+1 f (xk+1) + ∇f (xk+1), ak+1Axk++A1 kxk − xk+1 + ak+1ψ(x) + Akψ(xk) − Bk,

(40)

where (∗) stands for convexity of f . Now, let us denote the point

vk+1 := xk + γ1k (xk+1 − xk) ∈ dom ψ. Then, stationary condition for the method step (37) can be written as

gk + Hk(xk+1 − xk), x − vk+1 + ψ(x) ≥ ψ(vk+1),

(41)

18

for all x ∈ dom ψ. Therefore, Ak+1 ∇f (xk+1), ak+1Axk++A1 kxk − xk+1 + ak+1ψ(x)

= ak+1 ∇f (xk+1), x − vk+1 + ψ(x)

= ak+1 gk + Hk(xk+1 − xk), x − vk+1 + ψ(x)

+ ∇f (xk) − gk, x − vk+1

+ (∇2f (xk) − Hk)(xk+1 − xk), x − vk+1

+ ∇f (xk+1) − ∇f (xk) − ∇2f (xk)(xk+1 − xk), x − vk+1

(42)

(41),(6)
≥ ak+1 ψ(vk+1) − ∇f (xk) − gk · x − vk+1
− γk ∇2f (xk) − Hk · vk+1 − xk · x − vk+1
− L2γk2 vk+1−xk 2· x−vk+1
2
≥ ak+1ψ(vk+1) − ak+1D ∇f (xk) − gk ∗ − a2k+1D2 ∇Ak2+f (1xk)−Hk − a3k+A12kL+21D3 . Thus, combining all together, and using convexity of ψ, we obtain

(40),(42)
Ak+1F (x) ≥ Ak+1f (xk+1) + ak+1ψ(vk+1) + Akψ(xk) − Bk

− ak+1D ∇f (xk) − gk

− a2k+1D2 ∇2f (xk)−Hk
Ak+1

− a3k+1L2D3
A2k+1

≥ Ak+1F (xk+1) − Bk+1. So, we have (39) justiﬁed for all k ≥ 0.

Now, we are ready to prove convergence results for the process (37) with the basic variant of stochastic estimators (25), and with the variance reduction strategy for the gradients, incorporated into Algorithm 4.

D.1 Proof of Theorem 5
Theorem 5 Let each component fi(·) be Lipschitz continuous on dom ψ with constant L0, and have Lipschitz continuous gradients and Hessians on dom ψ with constants L1 and L2, respectively. Let γk := 1 − k+k 1 3 = O k1 . Set
mgk := 1/γk4, mHk := 1/γk2. Then, for the iterations {xk}k≥1 of Algorithm (1), based on estimators (25), it holds
E[F (xk) − F ∗] ≤ O L2D3 + L1D2(k1+2 log(n)) + L0D .

Proof: Let us ﬁx iteration k ≥ 0. For one uniform random sample i ∈ {1, . . . , M }, we have

E ∇f (xk) − ∇fi(xk) 2 = E ∇fi(xk) 2 − ∇f (xk) 2 ≤ L20.

(43)

19

Therefore, for the random batch of size mgk, we obtain E ∇f (xk) − gk ≤ E ∇f (xk) − gk 2

=

1 (mg

)2

E

i∈Sg (∇f (xk) − ∇fi(xk)) 2

k

k

=

1 (mg )2

i∈Sg E ∇f (xk) − ∇fi(xk) 2

k

k

(44)

(43)
≤ √Lm0 g . k
More advanced reasoning for matrices (Matrix Bernstein Inequality; see Chapter 6 in [40]) gives

E ∇2f (xk) − Hk

≤ L1

2 log(2n)
H

+

2 log(2n)
H

mk

3mk

√

≤ L1(3 2 log√(2n)+2 log(2n)) ≤ L1(6+√7 log(2n)) .

3 mH k

6 mH k

(45)

So, using these estimates together, we have, for every k ≥ 1

E[F (xk) − F ∗] (3≤8) A1k L22D3 k−1 Aa3i2i++11 + D k−1 ai+1E ∇f (xi) − gi

i=0

i=0

k−1
+ D2

E a2i+1

∇2f (xi) − Hi

Ai+1

i=0

(44),(45)
≤

+ L D √ 1 L2D3 k−1 a3i+1
Ak 2 i=0 A2i+1

k−1 ai+1
0 i=0 mgi

+ √ L1D2(6+7 log(2n)) k−1 a2i+1

6

i=0 Ai+1 mH i

(26)
=

A1k L22D3 + L0D + L1D2(6+67 log(2n)) k−1 Aa3i2+1 .
i=0 i+1

Thus, for the choice Ak := k3, we get

E[F (xk) − F ∗] ≤ O L2D3+L1D2(k1+2 log(n))+L0D .

D.2 Proof of Theorem 6 Theorem 6 Let each component fi(·) have Lipschitz continuous gradients and Hessians on dom ψ with constants L1 and L2, respectively. Let γk := 1 − k+k 1 3 = O( k1 ). Set batch size
mk := 1/γk2. Then, for all iterations {xk}k≥1 of Algorithm 4, we have
E[F (xk) − F ∗] ≤ O L2D3 + L1D2(1+log(nk)2) + L11/2D(F (x0)−F ∗) .
Proof:
Let us consider the following stochastic estimate gki := ∇fi(xk) − ∇fi(zk) + ∇f (zk),
20

for a uniform random sample i ∈ {1, . . . , M }, and a current iterate k ≥ 0. We denote by x∗ the solution of our problem: F ∗ = F (x∗), stationary condition for which is

∇f (x∗), x − x∗ + ψ(x) ≥ ψ(x∗), x ∈ dom ψ.

(46)

Then, it holds E ∇f (xk) − gki 2 = E (∇f (xk) − ∇f (x∗))

+ (∇fi(zk) − ∇fi(x∗) − ∇f (zk) + ∇f (x∗))

+ (∇fi(x∗) − ∇fi(xk)) 2

≤ 3E ∇f (xk) − ∇f (x∗) 2

+ 3E (∇fi(zk) − ∇fi(x∗)) − (∇f (zk) − ∇f (x∗)) 2

+ 3E ∇fi(xk) − ∇fi(x∗) 2

≤ 3 E ∇f (xk) − ∇f (x∗) 2 + E ∇fi(zk) − ∇fi(x∗) 2

+ E ∇fi(xk) − ∇fi(x∗) 2 ,

where we used the following simple bounds: a + b + c 2 ≤ 3 a 2 + 3 b 2 + 3 c 2,

E ξ − Eξ 2 ≤ E ξ 2, which are valid for any a, b, c ∈ Rn and arbitrary random vector ξ ∈ Rn. Now, by Lipschitz continuity of the gradients, we have (see Theorem 2.1.5 in [31])
∇f (xk) − ∇f (x∗) 2 ≤ 2L1 f (xk) − f (x∗) − ∇f (x∗), xk − x∗

(46)
≤ 2L1 F (xk) − F ∗ . The same holds for the random sample i, for arbitrary ﬁxed x ∈ dom ψ
Ei ∇fi(x) − ∇fi(x∗) 2 ≤ 2L1Ei fi(x) − fi(x∗) − ∇fi(x∗), x − x∗

= 2L1 f (x) − f (x∗) − ∇f (x∗), x − x∗

(46)
≤ 2L1 F (x) − F ∗).

Thus, we obtain

E ∇f (xk) − gki 2 ≤ 12L1E[F (xk) − F ∗] + 6L1E[F (zk) − F ∗].

(47)

Consequently, for the random batch

we have (compare with (44))

gk := m1k i∈Sk gki ,

E ∇f (xk) − gk ≤

1 (mk )2

i∈Sk E ∇f (xk) − gki 2

(47)
≤ 6mLk1 2E[F (xk) − F ∗] + E[F (zk) − F ∗] (48)

≤ 1m2Lk1 E[F (xk) − F ∗] + 6mLk1 E[F (zk) − F ∗].

21

So, using the variance reduction for the gradients, and the basic estimate for the Hessians, we have, for every k ≥ 1

E[F (xk) − F ∗]

(38),(48),(45)
≤

1 L2D3 k−1 a3i+1 Ak 2 i=0 A2i+1

+ D√6L1 k−1 √aim+1i
i=0

2E[F (xi) − F ∗] + E[F (zi) − F ∗]

+ L1D2(6+7 log(2n)) k−1 a2i+1

√

6

Ai+1 mi

i=0

=(28)

1

3L2D3+L1D2(6+7 log(2n)) k−1 a3i+1

Ak 6 i=0 A2i+1

√

k−1 a2

+ D 6L1

i+1
A

i=0 i+1

2E[F (xi) − F ∗] + E[F (zi) − F ∗] .

Now, let us set Ai+1 := (i + 1)3, and thus ai+1 := (i + 1)3 − i3 ≤ 3(i + 1)2, so we have

√
E[F (xk) − F ∗] ≤ α + β( 2+1k)2(F (x0)−F ∗)

k−1

(49)

+ kβ3

(i + 1) 2E[F (xi) − F ∗] + E[F (zi) − F ∗] ,

i=1

where

α := 27 · 3L2D3+L1D2(6+7 log(2n)) ,
6

√ β := 9 · D 6L1.

We are going to prove by induction, for every k ≥ 1

E[F (xk) − F ∗]

≤

c k2

,

(50)

with
c := 4β + α + 3β(F (x0) − F ∗) + 16β2 2 ≤ 74β2 + 2α + 6β(F (x0) − F ∗) (51)
= O L2D3 + L1D2(1 + log(n)) + L11/2D(F (x0) − F ∗) .

Hence, if (50) is true, then we essentially obtain the claim of the theorem. For k = 1, (50) follows directly from (49). Assume that (50) holds for all 1 ≤ i ≤ k, and consider iteration k + 1:

E[F (xk+1) − F ∗]

(49),(50)
≤

√

∗

k

α + β(

2+1)(F (x0)−F

)+

β
3

k2

k

i=1

(i + 1)

√

√

i2c + π(ic)

(∗)

√

∗

√k

√

α + β(

2+1)(F (x0)−F

)

+

β

c
3

(i + 1) 2 2 + 4

≤

k2

k

i+1

i=1

α + (√2+1)β(F (x0)−F ∗) + (2√2+4)β√c

=

k2

≤

α

+

3β

(F

(x0

)−F

∗

)

+

8β

√ c

(51)
=

c,

k2

k2

where in (∗) we have used two simple bounds: i ≤ 2π(i), and i + 1 ≤ 2i, valid for all i ≥ 1.

22

E Extra experiments

In this section, we provide additional experimental results for the problem of training Logistic Regression model, regularized by 2-ball constraints: Figure 4 for the exact methods, and Figure 6 for the stochastic algorithms.

Func. residual

Func. residual

a9a, D = 20

10 1

10 2 0.51s 0.4s

10 3

0.51s

10 4

Frank-Wolfe

10 5 0.46s Grad. Method

Fast Grad. Method

10 6 0.45s 10 7

Contr. Newton Aggr. Newton

0

100 Itera2t0io0ns 300 400

connect-4, D = 20

100

10 1

2.03s 1.98s

10 2

3.97s

10 3 1.04s

10 4 1.02s 10 5

10 6 4.02s

10 7

0

100

Frank-Wolfe Grad. Method Fast Grad. Method Contr. Newton Aggr. Newton Iter2a0ti0ons 300 400

mnist, D = 20

102

100

149.96s

10 2

50.01s

10 4 148.66s

10 6 52.44s 0 2000

150.04s Frank-Wolfe Grad. Method Fast Grad. Method Contr. Newton Aggr. Newton
Ite4ra00ti0ons 6000 8000

Func. residual

Func. residual

Func. residual

a9a, D = 100

10 1

10 2

4.03s

10 3

4.98s

10 4

10 5

5.04s

10 6 1.04s

10 7 0.34s

0 500 1000 1It5e0r0ati2o0n0s0 2500 3000 3500

connect-4, D = 100
101

10 1 10 3 1.08s

5.05s 4.05s 5.03s

10 5

10 7 1.09s
0 100 200 I3te00ratio40n0s 500 600 700
mnist, D = 100
104

102

100

100.04s

10 2

99.97s

101.4s 99.99s

10 4

10 6 56.35s 0 1000 2000 3It0e0r0at4io0n0s0 5000 6000 7000

Func. residual

Func. residual

Func. residual

100 10 1 10 2 10 3 10 4 10 5 10 6 1.01s 10 7 0.36s

a9a, D = 500
10s 5.04s 9.99s

0 1000 20It0e0ratio3n0s00 4000 5000 connect-4, D = 500

101 4.97s

10 1

9.98s

10 3

9.96s

10 5 3.65s
10 7 1.1s 0 200 4I0te0ratio6n0s0 800 1000
mnist, D = 500
104
102 99.96s
100 10 2 99.3s 100.10040ss 10 4
10 6 50.36s 0 2000 Iter4a0ti0o0ns 6000 8000

Func. residual

Figure 4: Training logistic regression, datasets: a9a (M = 32561, n = 123), connect-4 (M = 67557, n = 126), mnist (M = 60000, n = 780).

We see, that the second-order schemes usually outperforms ﬁrst-order methods, in terms of the number of iterations, and the number of epochs. Despite the fact, that the Newton step is more expensive, in many situations we see superiority of the second-order schemes in terms of the total computational time as well.
Comparing Contracting-Domain Newton Method (Algorithm 1), and Aggregating Newton Method (Algorithm 3), we conclude that both of the algorithms show reasonably good performance in practice. The latter one works a bit slower. However, the aggregation of the Hessians helps to improve numerical stability. On Figure 5, we demonstrate inﬂuence of the parameter of inner accuracy (EPS), which we use in our subsolver, on the convergence of the algorithms. We see much more robust behaviour for Aggregating Newton Method, while the ﬁrst algorithm can potentially stop, or even start to diverge, if the parameter is chosen in a wrong way.
To compute one step of our second-order methods for this task, we need to solve subproblem (20) for p = 2. This is minimization of quadratic function over the standard Euclidean ball. First, we compute tridiagonal decomposition of the Hessian (it requires O(n3) arithmetical operations). Then, we solve the dual to our subproblem (which is maximization of one-dimensional concave function) by classical Newton iterations (the cost of each iteration is O(n)). For more details, see Chapter 7 in [9].

23

Func. residual

10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8
0
103
101
10 1
10 3
10 5
0

a9a, D = 20, EPS = 10 5 Contr. Newton Aggr. Newton
1.99s
1.99s 50 Iter1a0ti0ons 150 200
mnist, D = 20, EPS = 10 5
1.99s Frank-Wolfe Contr. Newton Aggr. Newton
99.9s 52.85s 20 40 It6e0ratio8n0s 100 120 140

Func. residual

Func. residual

10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8
0
103
101
10 1
10 3
10 5 0

a9a, D = 20, EPS = 10 9 Contr. Newton Aggr. Newton

1.27s 2.05s

20 It4e0rations60 80

mnist, D = 20, EPS = 10 9

98.78s

1.94s

100.01s

Frank-Wolfe Contr. Newton Aggr. Newton
299.74s

20 40 It6e0ratio8n0s 100 120 140

Figure 5: Inﬂuence of the parameter of inner accuracy.

Func. residual

Func. residual

Func. residual

mnist, D = 20

SGD

SVRG

10 1

SNewton SVRNewton

48s

10 2 50.38s

69.54s

10 3 50.01s

10 4 0

100 200 Ep3o0c0hs 400 500 600

YearPredictionMSD, D = 20

100 10 1 10 2 19.75s 20s

SGD SVRG SNewton SVRNewton

10 3

10 4 19.85s 20.14s 10 5

10 6 0

50 Ep1o0c0hs 150 200

101

HIGGS2m, D = 20

100

SGD SVRG

10 1

SNewton

SVRNewton

10 2

1100 43 50.37s 50s

10 5

10 6

10.5s

28.21s

0 10 20Epochs30 40 50

Func. residual

Func. residual

Func. residual

mnist, D = 100
100

10 1 10 2 49.69s

30.03s

10 3 148.74s

50.01s

10 4 0 100

100 200 Ep3o0c0hs 400 500 600 YearPredictionMSD, D = 100

10 1

10 2 19.68s 10 3 19.61s 202s0.06s

10 4 10 5 10 6
0

SGD SVRG SNewton SVRNewton 50 Ep1o0c0hs 150 200

HIGGS2m, D = 100
10 1

10 2 10 3 39.79s

10 4

50s

10 5

10 6

37.44s

10 7

10 8

8.66s

0 10 20Epochs30 40 50

Func. residual

Func. residual

Func. residual

mnist, D = 500
102 101 50.61s

100

10 1

50s

10 2 50.62s

10 3

50.02s

10 4 0

100 200 Ep3o0c0hs 400 500 600

YearPredictionMSD, D = 500

101

SGD

100

SVRG

10 1

20s

SNewton SVRNewton

10 2 20.3s

10 3 19.52s 20.11s

10 4

10 5

10 6

0

50 Ep1o0c0hs 150 200

HIGGS2m, D = 500
100

10 1

10 2

1100 43 50.07s 50s

10 5
10 6
10 7 0

8.46s 10 20Epochs30

28.36s 40 50

Func. residual

Figure 6: Stochastic methods for training logistic regression, datasets: mnist (M = 60000, n = 780), YearPredictionMSD (M = 463715, n = 90), HIGGS2m (M = 2 · 106, n = 28).

24

