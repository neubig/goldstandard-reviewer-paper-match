CONTINUOUS SPEECH SEPARATION USING SPEAKER INVENTORY FOR LONG MULTI-TALKER RECORDING
Cong Han1, Yi Luo1, Chenda Li2, Tianyan Zhou3, Keisuke Kinoshita4, Shinji Watanabe5, Marc Delcroix4, Hakan Erdogan6, John R. Hershey6, Nima Mesgarani1, Zhuo Chen3
1Columbia University, 2Shanghai Jiao Tong University, 3Microsoft Corporation, 4NTT Corporation, 5Johns Hopkins University, 6Google Research

arXiv:2012.09727v2 [eess.AS] 18 Dec 2020

ABSTRACT
Leveraging additional speaker information to facilitate speech separation has received increasing attention in recent years. Recent research includes extracting target speech by using the target speaker’s voice snippet and jointly separating all participating speakers by using a pool of additional speaker signals, which is known as speech separation using speaker inventory (SSUSI). However, all these systems ideally assume that the pre-enrolled speaker signals are available and are only evaluated on simple data conﬁgurations. In realistic multi-talker conversations, the speech signal contains a large proportion of non-overlapped regions, where we can derive robust speaker embedding of individual talkers. In this work, we adopt the SSUSI model in long recordings and propose a self-informed, clustering-based inventory forming scheme for long recording, where the speaker inventory is fully built from the input signal without the need for external speaker signals. Experiment results on simulated noisy reverberant long recording datasets show that the proposed method can signiﬁcantly improve the separation performance across various conditions.
Index Terms— Speech separation, continuous speech separation, speaker inventory, embedding clustering
1. INTRODUCTION
Single-channel speech separation has been a challenging speech signal processing problem, and deep learning has provided advanced methods toward solving this problem [1–7]. In recent years, research that leverages additional speaker information has received increasing attention [8–14]. We can categorize them into two main categories. The ﬁrst category is informed speech extraction, which exploits an additional voice snippet of the target speaker to distinguish his/her speech from the mixture. SpeakerBeam [8, 9] derives a speaker embedding from an utterance of the target speaker by using a sequence summary network [15] and uses the embedding to guide an extraction network to extract the speaker of interest. VoiceFilter [10] concatenates spectral features of the mixture with the d-vector [16] of a voice snippet to extract the target speaker. Xiao et al. [11] uses an attention mechanism to generate context-dependent biases for target speech extraction. Informed speech extraction naturally solves the permutation problem and unknown number of speakers. However, it has two limitations. Firstly, the computation cost is proportional to the number of speakers to be extracted, so in a multi-speaker conversation, the system needs to run multiple times to extract each speaker one by one. Most importantly, the extraction usually fails when the target speaker’s biased information is not strong enough [9].

The second category is speech separation using speaker inventory (SSUSI) [14]. The method employs a pool of additional enrollment utterances from a list of candidate speakers, from which proﬁles of relevant speakers involved in the mixture are ﬁrst selected. Then the method fuses the selected proﬁles and the mixed speech to separate all speakers simultaneously. As multiple proﬁles are provided during separation, more substantial speaker discrimination can be expected, which yields better speech separation. The method can also employ permutation invariant training (PIT) [2] to compensate for weak biased information and wrong selection.
Though with promising results reported in prior arts, both categories suffer from two issues. Firstly, as the separation performance heavily relies on the proﬁle quality, when there is a severe acoustic mismatch between the mixed signal and the enrolled utterances, the effectiveness of speaker information could be largely degraded. Secondly, methods in both categories assume additional speaker information is available ahead of extraction or separation, which may be impractical in real scenarios. Wavesplit [17] uses clustering to infer source embeddings from the mixed signal and then uses them to guide speaker separation. However, the number of source embeddings must be ﬁxed and identical to the speakers to be separated, limiting its application in a long recording with various speakers. Also, all the methods mentioned above mostly prove their successes on relative simple datasets, e.g., LibriMix [18] that contains only anechoic speech, or WSJ0-2mix [1] and its variants that contain pre-segmented speech utterances that are usually fully overlapped. These further blur the practicality of these methods as overlap in real conversation usually possess very different characteristics [19–22].
In this paper, we address these problems on the continuous speech separation (CSS) task [23, 24]. CSS focuses on separating long recordings where the overall overlap ratio is low and the speaker activations are sparse. A large proportion of non-overlapped regions in the recording enables the derivation of robust features for the participants. We adopt the SSUSI in the CSS task and propose continuous SSUSI (CSSUSI), which constructs the speaker inventory from the mixed signal itself, instead of external speaker enrollments, by using speaker clustering methods. CSSUSI informs the separation network with relevant speaker proﬁles dynamically selected from the inventory to facilitate source separation at local regions. The outputs from local regions are then concatenated such that the output audio streams are continuous speech that do not contain any overlap. We create a more realistic dataset that simulates natural multi-talker conversations in conference rooms to test CSSUSI on the CSS task. Experimental results show that CSSUSI can successfully build a speaker inventory from the long speech mixture using the clustering-based method and take advantage of the global information to improve separation performance signiﬁcantly.

The rest of the paper is organized as follows. We introduce the SSUSI framework in Section 2, describe the CSSUSI system for long recording in Section 3, present the experiment conﬁgurations in Section 4, analyze the experiment results in Section 5, and conclude the paper in Section 6.
2. SSUSI USING PRE-ENROLLED UTTERANCE
We ﬁrst overview the original SSUSI system [14], which requires pre-enrolled speaker signals. A SSUSI system contains three modules: a speaker identiﬁcation module, a speaker proﬁle selection module, and a biased speech separation module. The speaker identiﬁcation module is responsible for embedding extraction from both the speaker enrollments and input mixture. Embeddings of speaker enrollments are used for speaker inventory construction. The speaker proﬁle selection module selects from the inventory the best-matched speaker proﬁles with the mixture embeddings. The selected proﬁles are then fed into the biased separation module to separate speakers in the mixture.
Since each speech segment is short (4s in this paper) and typically contains at most two speakers, we focus on two-speaker separation for each speech segment, and the model always generates two outputs. Moreover, we make several modiﬁcations to the original SSUSI architecture [14] for better performance.

2.1. Speaker identiﬁcation module

The speaker identiﬁcation module is used to construct the speaker
inventory ﬁrst. The inventory is a pool of K-dimensional speaker embeddings ej M , ej ∈ RK , which are extracted from a col-
j=1
lection of time-domain enrollment speech aj M , aj ∈ RLaj , j=1
where Laj is the temporal dimension of speech signal aj. M is typically larger than the maximum number of speakers in the mixture to
be separated. We also assume that each speaker only has one enroll-
ment sentence. A speaker identiﬁcation network, referred to as the
SNet, is applied for embedding extraction:

Ej = SNet(aj)

(1)

where Ej ∈ RTj×K and Tj is the temporal dimension of the em-
bedding sequence. Here we simply use mean-pooling across the Tj frames of Ej to obtain the single vector ej ∈ RK .
The mixture embeddings are directly extracted from the input mixture y ∈ RT with the temporal dimension T:

Ey = SNet(y)

(2)

where Ey ∈ RTy×K and Ty is the temporal dimension of the mixture embeddings.

2.2. Speaker proﬁle selection module

The speaker proﬁle selection module selects the relevant speaker proﬁles from the inventory that are best matched with the mixture embeddings Ey in equation 2. The selection is performed by calculating the similarity between the mixture embeddings and items in the inventory, and two items with the highest similarity are selected. The similarity are calculated by applying the Softmax function on the dot-product between the mixture and inventory embeddings:

dys,j = eys · ej

y,j

exp(dys ,j)

(3)

ws = M

y,p

p=1 exp(ds )

where eys denotes Ey at temporal index s. We then calculate the average score wy,j across the Ty frames:
wy,j = Ts=y 1 wys,j (4) Ty
Two inventory items ep1 and ep2 are then selected according to the two highest scores in wy,j M .
j=1

2.3. Biased speech separation module

The biased speech separation module is adapted to the speech characteristics of the speakers selected from the inventory for biased source separation. The module contains three layers, a feature extraction layer, a proﬁle adaptation layer, and a separation layer. Both feature extraction and separation layers are 2-layer BLSTM in this paper. Previous research [8] has shown that a multiplicative adaptation layer, i.e., multiplying the speaker embedding with the output of one of the middle layers of the network, is a simple yet effective way to realize adaptation, so we use the same method here. Given the two selected speaker proﬁles ep1 and ep2 , two target-biased adaptation features are calculated by frame-level element-wise multiplication between the proﬁles and the output of the feature extraction layer:

apl 1 = bl ep1

(5)

apl 2 = bl ep2

(6)

where bl ∈ RK denotes the output of the feature layer, l denotes the frame index, and denotes the element-wise multiplication. The two target-biased features are then concatenated:

A = concat([Ap1 , Ap2 ])

(7)

where Ap1 = [ap11 , . . . , apL1 ] ∈ RL×K , Ap2 = [ap12 , . . . , apL2 ] ∈ RL×K , and A ∈ RL×2K . The separation layer takes A as the input and estimates two time-frequency (T-F) masks M1, M2 ∈ RL×F .

3. CONTINUOUS SSUSI USING SELF-INFORMED MECHANISM FOR INVENTORY CONSTRUCTION
SSUSI assumes that pre-recorded utterances of all speakers are available for the speaker inventory construction. However, such an assumption may not be realistic, especially for unseen speakers or meeting scenarios where the collection of pre-recorded speech from the participants is not feasible.
Continuous speech separation (CSS) aims at estimating the individual target signals from a continuous mixed signal which is usually a hours long signal and contains both overlapped and non-overlap speech, but the overlap ratio is low. So, single-speaker regions can be exploited to derive robust acoustic characteristics of participating speakers without the need for external utterances, which makes the self-informed speaker inventory construction possible. This section introduces how we adopt SSUSI in the CSS task and eliminate the need for pre-recorded speech by using a clustering method.
Figure 1 (A) shows the overall ﬂowchart of the continuous SSUSI (CSSUSI) framework. The main difference between CSSUSI and the original SSUSI is the construction of the speaker inventory. Original SSUSI applies the speaker identiﬁcation module on extra enrollment utterances, whereas CSSUSI ﬁrst splits the mixture recording y into B small chunks, and directly extracts the mixture embeddings {eyb }Bb=1, where eyb ∈ RK denotes the embedding vector in chunk b. Then, CSSUSI applies Kmeans clustering on {eyb }Bb=1 to form M clusters, and the cluster centroids form the speaker inventory. In Section 5 we will show that the separation

Fig. 1. (A) The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker proﬁle selection module selects two relevant proﬁles from the inventory for each segment; the speech separation module fuses the selected speaker proﬁles into the system for source separation. (B) Multiplicative adaptation of the selected proﬁles ep1 and ep2 . (C) Stitching procedure of adjacent segment outputs in a long recording.

performance is insensitive to the choice of M as long as M is no smaller than the actual number of active speakers in the recording.
CSUSSI uniformly segments the mixture recording and exploits the inventory to facilitate source separation in each segment. Except for the self-informed speaker inventory, CSSUSI uses the same speaker proﬁle selection and biased speech separation methods as introduced in Section 2.2 and Section 2.3, respectively. To stitch the outputs from different segments to form output streams where each stream only contains non-overlapped speakers, the similarity between the overlapped regions in adjacent blocks determines the pair of segments to be stitched. Figure 1 (C) shows the stitching procedure of adjacent segment outputs.
4. EXPERIMENTAL SETTINGS
4.1. Dataset
In our training set, we randomly generate 3000 rooms. The length and width of the rooms are randomly sampled between 5 and 12 meters, and the height is randomly sampled between 2.5 and 4.5 meters. A microphone is randomly placed in the room, and its location is constrained to be within 2 meters of the room center. The height of the microphone is randomly sampled between 0.4 and 1.2 meters. We randomly sample 10 speakers from the LibriSpeech corpus [25] for each room. All the speakers are at least 0.5 meters away from the room walls and the height of the speakers are between 1 and 2 meters. The reverberation time is uniformly sampled between 0.1 and 0.5 seconds. We randomly choose 2 speakers as relevant speakers and arrange them according to one of the four following patterns:
1. Inclusive: one speaker talks a short period while the other one is talking.
2. Sequential: one talks after the other one ﬁnishes talking.
3. Fully-overlapped: two speakers always talk simultaneously.

4. Partially-overlapped: two speakers talk together only in a certain period.
The frequencies for the four patterns are 10%, 20%, 35%, and 35%, respectively. The minimal length of the overlapped periods in inclusive and partially-overlapped patterns is set to 1 second. The maximal length of the silent periods between the two speakers in the sequential pattern is 0.5 second. Moreover, to generate single-speaker utterances, there is a 0.1 probability that one of the speakers is muted in each pattern. We use the remaining 8 speakers as the irrelevant speakers that will not appear in the mixture. Each of the room conﬁgurations is used for 8 times. The mixture length is 4 seconds. So, the total training time is 3000 × 8 × 4s = 26.7 hours. For both the relevant and irrelevant speakers, a 10-second utterance is sampled to form the speaker inventory. All speech signals are single-channel and sampled at 16 kHz. Gaussian noise with SNR randomly chosen between 0 and 20 dB is added into the mixture.
In our testing set, we set three conﬁgurations: 60-second mixture containing 2 speakers, 150-second mixture containing 5 speakers, and 240-second mixture containing 8 speakers. We generate 300 recordings for each conﬁguration. The overall overlap ratio of each recording is 30% complying with natural conversion [26].
4.2. Implementation details
All models contain 4 bidirectional LSTM (BLSTM) layers with 600 hidden units in each direction. In the CSSUSI models, the speaker identiﬁcation module adopts the similar design in [27], and the module is pretrained on the VoxCeleb2 dataset [28] and achieves 2.04% equal error rate on the VoxCeleb1 test set [29]. The module extracts 128-dimensional speaker embeddings for every 1.2-second (30-frame) segment. We use SNR as training objective [30] and Adam [31] as the optimizer with initial learning rate of 0.001. The learning rate is decayed by 0.98 for every two epochs.

Table 1. SNR (dB) on eight-speaker long recordings (segment-wise evaluation). The performance on different overlap ratios is reported.

Method

Speaker enrollment

Overlap ratio in % 0 0-25 25-50 50-75 75-100 Average

Unprocessed

-

8.6 -9.7 -1.2 -0.9

-0.7

-0.1

BLSTM

-

15.5 8.0

8.6

7.5

6.9

10.6

Two wrong proﬁles

15.2 7.1

8.4

7.8

7.1

10.3

SSUSI

One correct and one wrong proﬁles 15.4 7.8

9.0

8.2

7.6

10.7

Two correct proﬁles

15.9 9.5 10.6

9.4

8.7

11.9

Selected proﬁles

15.7 8.8 10.0

9.0

8.3

11.5

5. RESULTS AND DISCUSSIONS

Table 2. SNR (dB) on long recordings with different conﬁgurations

(segment-wise evaluation).

Speaker number

Method

External utterances Clusters Avg.

Unprocessed

-

-

1.6

BLSTM

-

-

11.2

2 speakers

SSUSI

2

CSSUSI

No

No

12.2

2

12.1

3

11.9

4

11.9

Unprocessed

-

-

0

BLSTM

-

-

10.6

SSUSI

5

No

11.5

5 speakers

3

10.9

CSSUSI

No

5

11.3

8

11.2

10

11.2

Unprocessed

-

-

-0.1

BLSTM

-

-

10.6

SSUSI

8

No

11.5

8 speakers

5

11.0

CSSUSI

No

8

11.3

12

11.3

16

11.2

Table 1 compares different models on 4-second segments of eight-speaker recordings. The inventory contains eight speakers’ proﬁles that are derived from eight external utterances. SSUSI achieves leading performance on all levels of overlap ratios when two correct speaker proﬁles are used; however, the performance of SSUSI drops greatly with two wrong speaker proﬁles randomly chosen from the 8 irrelevant speakers, which indicates that performance gain obtained by SSUSI mainly comes from leveraging the target speaker information. We also notice that the performance of SSUSI with two wrong proﬁles is only slightly worse than the baseline BLSTM, and when only one correct speaker proﬁle is enrolled, SSUSI can still outperform the baseline model, which proves that PIT can compensate for wrong selection and the separation module is robust to adaptation features. When the speaker proﬁles are selected by the proﬁle selection module, the SSUSI model performs slightly better on the non-overlapped mixtures (overlap ratio is 0) but much better on the overlapped mixtures at all overlap ratios. This conﬁrms the effectiveness of the SSUSI framework on improving separation performance across various settings, which is consistent with the observations in [14] that conducted experiments on Librispeech although the model architectures are different.
Table 2 compares CSSUSI with different clusters on recordings with different number of speakers. Since the number of participating speakers in a meeting may be unknown, we intend to do overclustering, i.e., setting the number of clusters greater than the number of speakers in a meeting. Table 2 compares CSSUSI with different clustering settings. The performance of CSSUSI is almost identical once the number of clusters is not fewer than the number of

Table 3. Utterance-level evaluation. SI-SDR(dB) is reported.

Method

Need external utterances? 2 spk 5 spk 8spk

Unprocessed

-

6.0

4.5

4.3

BLSTM

No

11.7 10.8 10.6

SSUSI

Yes

13.2 12.0 11.7

CSSUSI

No

13.1 11.9 11.7

speakers. Over-clustering has very little impact on the performance as it ensures each speaker possesses at least one cluster center. Some extra clusters may represent acoustic characteristics of overlapped regions, which will be regarded as irrelevant proﬁles during proﬁle selection. We can see that CSSUSI outperforms the baseline model BLSTM on all conﬁgurations. As we conclude from Table 1, the performance gain is achieved via leveraging relevant speakers’ information. So the performance gain from CSUSSI suggests the successful construction of the speaker inventory from the mixture itself and effective utilization of speaker information. Furthermore, we compare CSSUSI with SSUSI that derives speaker proﬁles from external utterances that contain only a single speaker in each utterance. CSSUSI sacriﬁces very little performance but does not require external utterances, which shows CSSUSI is a better model than SSUSI for long recording speech separation.
Table 3 compares utterance-wise separation performance. After segments are stitched, each complete utterance is extracted from the output streams by using ground-truth segmentation information, i.e., onset and offset of each utterance. We ﬁnd that CSSUSI surpasses the baseline in all conﬁgurations by a large margin, which further proves the strength of CSSUSI in the long recordings.
6. CONCLUSION
In this paper, we investigated continuous speech separation using speaker inventory for long multi-talker recordings. In the CSS task, we made use of the fact that long recording, in general, contains a large proportion of non-overlapped regions and proposed continuous SSUSI (CSSUSI) that extracted speaker embeddings from the long recordings and performed “over-clustering” on the embeddings to construct the self-informed speaker inventory. CSSUSI overcomes the limitation of the original SSUSI that required external enrollments. Experiments on a simulated noisy reverberant dataset showed that CSSUSI signiﬁcantly outperformed the baseline models across various conditions. Future works include extending the CSSUSI system into real-world recordings, designing a block-online system instead of an ofﬂine system, and investigate better model architectures.

7. ACKNOWLEDGEMENT
The work reported here was started at JSALT 2020 at JHU, with support from Microsoft, Amazon and Google.

8. REFERENCES
[1] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 31–35.
[2] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 241–245.
[3] Y. Luo, Z. Chen, and N. Mesgarani, “Speaker-independent speech separation with deep attractor network,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 4, pp. 787–796, 2018.
[4] C. Han, Y. Luo, and N. Mesgarani, “Online deep attractor network for real-time single-channel speech separation,” in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 361–365.
[5] Y. Luo and N. Mesgarani, “Conv-tasnet: Surpassing ideal time– frequency magnitude masking for speech separation,” IEEE/ACM transactions on audio, speech, and language processing, vol. 27, no. 8, pp. 1256–1266, 2019.
[6] S. Wisdom, E. Tzinis, H. Erdogan, R. J. Weiss, K. Wilson, and J. R. Hershey, “Unsupervised sound separation using mixtures of mixtures,” in Advances in Neural Information Processing Systems (NeurIPS), 2020.
[7] T. Jenrungrot, V. Jayaram, S. Seitz, and I. Kemelmacher-Shlizerman, “The cone of silence: speech separation by localization,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 33, 2020.
[8] K. Zˇ mol´ıkova´, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, and J. Cˇ ernocky`, “Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures,” IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800–814, 2019.
[9] M. Delcroix, T. Ochiai, K. Zmolikova, K. Kinoshita, N. Tawara, T. Nakatani, and S. Araki, “Improving speaker discrimination of target speech extraction with time-domain speakerbeam,” in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 691–695.
[10] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. Hershey, R. A. Saurous, R. J. Weiss, Y. Jia, and I. L. Moreno, “Voiceﬁlter: Targeted voice separation by speaker-conditioned spectrogram masking,” arXiv preprint arXiv:1810.04826, 2018.
[11] X. Xiao, Z. Chen, T. Yoshioka, H. Erdogan, C. Liu, D. Dimitriadis, J. Droppo, and Y. Gong, “Single-channel speech extraction using speaker inventory and attention network,” in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 86–90.
[12] J. Wang, J. Chen, D. Su, L. Chen, M. Yu, Y. Qian, and D. Yu, “Deep extractor network for target speaker recovery from single channel speech mixtures,” arXiv preprint arXiv:1807.08974, 2018.
[13] T. Ochiai, M. Delcroix, K. Kinoshita, A. Ogawa, and T. Nakatani, “A uniﬁed framework for neural speech separation and extraction,” in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6975–6979.
[14] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, and J. Li, “Speech separation using speaker inventory,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 230–236.
[15] K. Vesely`, S. Watanabe, K. Zˇ mol´ıkova´, M. Karaﬁa´t, L. Burget, and J. H. Cˇ ernocky`, “Sequence summarizing neural network for speaker adaptation,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5315–5319.
[16] E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. GonzalezDominguez, “Deep neural networks for small footprint text-dependent speaker veriﬁcation,” in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4052–4056.

[17] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech separation by speaker clustering,” arXiv preprint arXiv:2002.08933, 2020.
[18] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, “Librimix: An open-source dataset for generalizable speech separation,” arXiv preprint arXiv:2005.11262, 2020.
[19] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke et al., “The icsi meeting corpus,” in 2003 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 1. IEEE, 2003, pp. I–I.
[20] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal et al., “The ami meeting corpus: A pre-announcement,” in International Workshop on Machine Learning for Multimodal Interaction. Springer, 2005, pp. 28–39.
[21] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimitriadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang et al., “Advances in online audio-visual meeting transcription,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 276–283.
[22] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁfth’chime’speech separation and recognition challenge: Dataset, task and baselines,” in INTERSPEECH, 2018, pp. 1561–1565.
[23] T. Yoshioka, H. Erdogan, Z. Chen, and F. Alleva, “Multi-microphone neural speech separation for far-ﬁeld multi-talker speech recognition,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5739–5743.
[24] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, “Continuous speech separation: Dataset and analysis,” in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7284–7288.
[25] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.
[26] O¨ . C¸ etin and E. Shriberg, “Analysis of overlaps in meetings by dialog factors, hot spots, speakers, and collection site: Insights for automatic speech recognition,” in 9th International Conference on Spoken Language Processing, 2006.
[27] T. Zhou, Y. Zhao, J. Li, Y. Gong, and J. Wu, “CNN with Phonetic Attention for Text-Independent Speaker Veriﬁcation,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 718–725.
[28] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,” in INTERSPEECH, 2018, pp. 1086–1090.
[29] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: A large-scale speaker identiﬁcation dataset,” in INTERSPEECH, 2017, pp. 2616– 2620.
[30] C. Han, Y. Luo, and N. Mesgarani, “Real-time binaural speech separation with preserved spatial cues,” in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6404–6408.
[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.

