arXiv:1605.06523v2 [cs.AI] 19 Jul 2016

TensorLog: A Differentiable Deductive Database
William W. Cohen Department of Machine Learning
Carnegie Mellon University Pittsburgh, PA 15213 wcohen@cs.cmu.edu
Abstract
Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into “deep” gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is ﬁrst converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are “unrolled” into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efﬁcient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other ﬁrst-order probabilistic logics.
1 Introduction
Large knowledge bases (KBs) have proven useful in many tasks, but it is unclear how to integrate this sort of knowledge into “deep” gradient-based learning systems. Motivated by this, we describe a probabilistic deductive database (PrDDB) system in which reasoning is performed by a differentiable process. In addition to enabling novel gradient-based learning algorithms for PrDDBs, this approach could potentially enable tight integration of logical reasoning into deep learners (or conversely, of deep learning into reasoning systems.
In a traditional deductive database (DDB), a database DB with a theory T together deﬁne a set of facts f1, . . . , fn which can be derived by accessing the database and reasoning using T . As an example, Figure 1 contains a small theory and an associated database. End users can test to see if a fact f is derivable, or retrieve all derivable facts that match some query: e.g., one could test if f = uncle(joe,bob) is derivable in the sample database, or ﬁnd all values of Y such that uncle(joe,Y) holds. A probabilistic DDB is a “soft” extension of a DDB, where derived facts f have a numeric conﬁdence, typically based on augmenting DB with a set of parameters Θ. In many existing PrDDB models computation of conﬁdences is computationally expensive, and often not be conducive to learning the parametersb Θ.
Here we describe a probabilistic deductive database called TensorLog in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is ﬁrst converted into certain type of factor graph, in which each logical variable appearing in the clause is associated with a random variable in the factor graph, and each literal is associated with a factor (as shown in Figure 2). Then, for each type of query to the factor graph, the message-passing steps required to perform BP are “unrolled” into a function, which is differentiable. Each function will answer queries for a particular combination of evidence variables and query variables in the factor graph, which in turn corresponds to logical queries in a particular syntactic form. We also show how these functions can be composed
29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

1. uncle(X,Y):-child(X,W),brother(W,Y). 2. uncle(X,Y):-aunt(X,W),husband(W,Y). 3. status(X,tired):-child(W,X),infant(W).

child(liam,eve),0.99 child(dave,eve),0.99 child(liam,bob),0.75 husband(eve,bob),0.9

infant(liam),0.7 infant(dave),0.1 aunt(joe,eve),0.9 brother(eve,chip),0.9

Figure 1: An example database and theory. Uppercase symbols are universally quantiﬁed variables, and so clause 3 should be read as a logical implication: for all database constants cX and cW , if child(cX ,cW ) and infant(cW ) can be proved, then status(cX ,tired) can also be proved.

recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates.
In TensorLog, compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. Most importantly, inference is also differentiable, enabling gradient-based parameter learning. Formally, we can show that TensorLog subsumes some prior probabilistic logic programming models, including several variants of stochastic logic programs (SLPs) [3, 17], and approximates others [9, 5].
Below, we ﬁrst present background material, then introduce our main results for differentiable inference, We then discuss related work, in particular the relationship between TensorLog and existing probabilistic logics, present experimental results, and conclude.
2 Background: Deductive and Probabilistic DBs
To begin, we review the deﬁnition for an ordinary DDB, an example of which is in Figure 1. A database, DB, is a set {f1, . . . , fN } of ground facts. We focus here on DB relations which are unary or binary (e.g., from a “knowledge graph”), hence, facts will be written as p(a, b) or q(c) where p and q are predicate symbols, and a, b, c are constants from a ﬁxed domain C. A theory, T , is a set of function-free Horn clauses. Clauses are written A:-B1, . . . , Bk, where A is called the head of the clause, B1, . . . , Bk is the body, and A and the Bi’s are called literals. Literals must be of the form q(X), p(X, Y ), p(c, Y ), or p(X, c), where X and Y are logical variables, and c is a database constant.
Clauses can be understood as logical implications. Let σ be a substitution, i.e., a mapping from logical variables to constants in C, and let σ(L) be the result of replacing all logical variables X in the literal L with σ(X). A set of tuples S is deductively closed with respect to the clause A ← B1, . . . , Bk iff for all substitutions σ, either σ(A) ∈ S or ∃Bi : σ(Bi) ∈ S. For example, if S contains the facts of Figure 1, S is not deductively closed with respect to the clause 1 unless it also contains uncle(chip,liam) and uncle(chip,dave). The least model for a pair DB, T , written Model(DB, T ), is the smallest superset of DB that is deductively closed with respect to every clause in T . In the usual DDB semantics, a ground fact f is considered “true” iff f ∈ Model(DB, T ).
To introduce “soft” computation into this model, we add a parameter vector Θ which associates each fact f ∈ DB with a positive scalar θf (as shown in the example). The semantics of this parameter vary in different PrDDB models, but Θ will always deﬁne a distribution Pr(f |T , DB, Θ) over the facts in Model(T , DB).
3 Differentiable soft reasoning
Numeric encoding of PrDDB’s and queries. We will implement reasoning in PrDDB’s by deﬁning a series of numeric functions, each of ﬁnds answers to a particular family of queries. It will be convenient to encode the database numerically. We will assume all constants have been mapped to integers. For a constant c ∈ C, we deﬁne uc to be a one-hot row-vector representation for c, i.e., a row vector of dimension |C| where u[c] = 1 and u[c ] = 0 for c = C. We can also represent a binary predicate p by a sparse matrix Mp, where Mp[a, b] = θp(a,b) if p(a, b) ∈ DB, and a unary predicate q as an analogous row vector vq. Note that Mp encodes information not only about the database facts in predicate p, but also about their parameter values.
PrDDB’s are commonly used test to see if a fact f is derivable, or retrieve all derivable facts that match some query: e.g., one could test if f = uncle(joe,bob) is derivable in the sample database, or

2

Figure 2: Examples of factor graphs for the example theory.

ﬁnd all values of Y such that uncle(joe,Y) holds. We focus here on the latter type of query, which we call an argument-retrieval query. An argument-retrieval query Q is of the form p(c, Y ) or p(Y, c): we say that p(c, Y ) has an input-output mode of in,out and p(Y, c) has an input-output mode of out,in. For the sake of brevity, below we will assume below the mode in,out when possible, and abbreviate the two modes as io and io.
The response to a query p(c, Y ) is a distribution over possible substitutions for Y , encoded as a vector vY such that for all constants d ∈ C, vY [d] = Pr(p(c, d)|T , DB, Θ). Alternatively (since often we care only about the relative scores of the possible answers), the system might instead return a conditional probability vector vY |c: if Up(c,Y ) is the set of facts f that “match” p(c, Y ), then vY |c[d] = Pr(f = p(c, d)|f ∈ Up(c,Y ), T , DB, Θ).
Since the ultimate goal of our reasoning system is to correctly answer queries using functions, we also introduce a notation for functions that answer particular types of queries: in particular, for a predicate symbol fipo denotes a query response function for all queries with predicate p and mode io, i.e., queries of the form p(c, Y ), when given a one-hot encoding of c, fipo returns the appropriate conditional probability vector:
fipo(uc) ≡ vY |X where ∀d ∈ C : vY |c[d] = Pr(f = p(c, d)|f ∈ Up(c,Y ), T , DB, Θ) (1)
and similarly for fopi. Syntactic restrictions. Algorithmically it will be convenient to constrain the use of constants in clauses. We introduce a special DB predicate assign, which will be used only in literals of the form assign(W ,c), which in turn will be treated as literals for a special unary predicate assign_c. Without loss of generality, we can now assume that constants only appear in assign literals. For instance, the clause 3 of Figure 1 would be rewritten as

status(X,T):-assign_tired(T),child(X,W),infant(W).

(2)

We will also introduce another special DB predicate any, where any(a, b) is conceptually true for any pair of constants a, b; however, as we show below, the matrix Many need not be explicitly stored. We also constrain clause heads to contain distinct variables which all appear also in the body.
A factor graph for a one-clause program. We will start by considering a highly restricted class of theories T , namely programs containing only one non-recursive clause r that obeys the restrictions above. We build a factor graph Gr for r as follows: for each logical variable W in the body, there is a random variable W ; and for every literal q(Wi, Wj) in the body of the clause, there is a factor with potentials Mq linking variables Wi and Wj. Finally, if the factor graph is disconnected, we add any factors between the components until it is connected. Figure 2 gives examples. The variables appearing in the clause’s head are starred.
We now argue that Gr imposes a valid distribution Pr(f |T , DB, Θ) over facts in Model(T , DB). In Gr the variables are multinomials over C, the factors represent predicates and the graph Gr represents a distribution of possible bindings to the logical variables in the clause f , i.e., to possible substitutions σ. Let W1, . . . , Wm be the variables of Gr, and for each factor/edge e let pe(Wie , Wje ) be the literal

3

deﬁne compileMessage(L → X): assume wolg that L = q(X) or L = p(Xi, Xo) generate a new variable name vL,X if L = q(X) then emitOperation( vL,X = vq) else if X is the output variable Xo of L then vi = compileMessage(Xi → L) emitOperation( vL,X = vi · Mp ) else if X is the input variable Xi of L then vo = compileMessage(Xi → L) emitOperation( vL,X = vo · MTp ) return vL,X

deﬁne compileMessage(X → L): if X is the input variable X then return uc, the input else generate a new variable name vX assume L1, L2, . . . , Lk are the neighbors of X excluding L for i = 1, . . . , k do vi = compileMessage(Li → X) emitOperation(vX = v1 ◦ · · · ◦ vk) return vX

Figure 3: Algorithm for unrolling belief propagation on a polytree into a sequence of message-computation operations. Notes: (1) if L = p(Xo, Xi) then replace Mp with MTp (the transpose). (2) Here v1 ◦ v2 denotes the Hadamard (component-wise) product, and if k = 0 an all-ones vector is returned.

associated with it. In the distribution deﬁned by Gr

1

Pr(W1 = c1, . . . , Wm = cm) =

Gr

Z

φe(ci, cj) =

θpe(cie ,cje )

(ci,cj)∈edges e

(ci,cj)∈edges e

Recall ∀f, θf > 0, so if Pr(W1 = c1, . . . , Wm = cm) > 0 then for each edge pe(cie , cje ) ∈ DB, and hence the substitution σ = {W1 = c1, . . . , Wm = cm} makes the body of clause r true. The converse is also clearly true: so Gr deﬁnes a distribution over exactly those substitutions σ that make the the body of r true.
BP over Gr can now be used to compute the conditional vectors fipo(uc) and fopi(uc). For example to compute fipo(uc) for clause 1, we would set the message for the evidence variable X to uc, run BP, and read out as the value of f the marginal distribution for Y .

However, we would like to do more: we would like to compute an explicit, differentiable, query response function, which computes fipo(uc). To do this we “unroll” the message-passing steps into a series of operations, following [6].

For completeness, we include in Figure 3 a sketch of the algorithm used in the current implementation of TensorLog, which makes the (strong) assumption that Gr is a tree. In the code, we found it convenient to extend the notion of input-output modes for a query, a variable X appearing in a literal L = p(X, Y ) in a clause body is an nominal input if it appears in the input position of the head, or any literal to the left of L in the body, and is an nomimal output otherwise. In Prolog a convention is that nominal inputs appear as the ﬁrst argument of a predicate, and in TensorLog, if the user respects this convention, then “forward” message-passing steps use Mp rather than MpT w (reducing the cost of transposing large DB-derived matrices, since our message-passing schedule tries to maximize forward messages.) The code contains two mutually recursive routines, and is invoked by requesting a message from the output variable to a ﬁctional output literal. The result will be to emit a series of operations, and return the name of a register that contains the unnormalized conditional probability vector for the output variable: e.g., for the sample clauses the functions returned are:
r1 gir1o(uc) = { v1,W = ucMparent; vW = v1,W ; v2,Y = vW Mbrother; vY = v2,Y ; return vY } r2 gir2o(uc) = { v1,W = ucMaunt; vW = v1,W ; v2,Y = vW Mhusband; vY = v2,Y ; return vY } r3 gir3o(uc) = { v2,W = ucMparent; v3,W = vinfant; W = v2,W ◦ v3,W ;
v1,T = vassign_tired; v4,T = vW Many; T = v1,T ◦ v4,T ; return vT }
Here we use giro(uc) for the unnormalized version of the query response function build from Gr, i.e.,
fipo(uc) ≡ giro(uc)/||giro(uc)||1
where r is the one-clause theory deﬁning p.
Sets of factor graphs for multi-clause programs. We now extend this idea to theories with many clauses. We ﬁrst note that if there are several clauses with the same predicate symbol in the head, we simply sum the unnormalized query response functions: e.g., for the predicate cduncle, deﬁned by

4

rules r1 and r2, we can deﬁne

giuoncle = gir1o + gir2o

and then re-normalize. This is equivalent to building a new factor graph G, which would be

approximately ∪iGri, together global input and output variables, and a factor that constrains the

input variables of the Gri’s to be equal, and a factor that constrains the output variable of G to be the

sum of the outputs of the Gri’s.

A more complex situation is when the clauses for one predicate, p, use a second theory predicate q, in their body: for example, this would be the case if aunt was also deﬁned in the theory, rather
than the database. For a theory with no recursion, we can replace the message-passing operations vY = vX Mq with the function call vY = giqo(vX ), and likewise the operation vY = vX MTq with the function call vY = goqi(vX ). It can be shown that this is equivalent to taking the factor graph for q and “splicing” it into the graph for p.

It is also possible to allow function calls to recurse to a ﬁxed maximum depth: we must simply add some sort of extra argument that tracks depth to the recursively-invoked gq functions, and make sure that gp returns an all-zeros vector (indicating no more proofs can be found) when the depth bound is exceeded. Currently this is implemented by marking learned functions g with the predicate q, a mode, and a depth argument d, and ensuring that function calls inside gipo,d to q always call the next-deeper version of the function for q, e.g., giqo,d+1.
Uncertain inference rules. Notice that Θ associates conﬁdences with facts in the databases, not with clauses in the theory. To attach a probability to a clause, a standard trick is to introduce a special clause-speciﬁc fact, and add it to the clause body [9]. For example, a soft version of clause 3 could be re-written as

status(X,tired):-assign(RuleId,c3),weighted(RuleId),child(W,X),infant(W)

where the (parameterized) fact weighted(c3) appears in DB, and the constant c3 appears nowhere else in T . TensorLog supports some special syntax to make it easy to build rules with associated weights: for instance, status(X,tired) :- assign(C3,c3), weighted(C3), child(W,X), infant(W) can be written simply as status(X,tired) :- child(W,X), infant(W) {c3}.
Discussion. Collectively, the computation performed by TensorLog’s functions are equivalent to computing a set of marginals over a particular factor graph G: speciﬁcally G would be formed by using the construction for multiple clauses with the same head (described above), and then splicing in the factor graphs of subpredicates. The unnormalized messages over this graph, and their functional equivalent, can be viewed implementing a ﬁrst-order version of weighted model counting, a well-studied problem in satisﬁability.
Computationally, the algorithm we describe is quite efﬁcient. Assuming the matrices Mp exist, the additional memory needed for the factor-graph Gr is linear in the size of the clause r, and hence the compilation to response functions is linear in the theory size and the number of steps of BP. For theories where every Gr is a tree, the number of message-passing steps is also linear. Message size is (by design) limited to |C|, and is often smaller due to sparsity.

The current implementation of TensorLog includes many restrictions that could be relaxed: e.g., predicates must be unary or binary, only queries of the types discussed here are allowed, and every factor graph Gr must be a tree. Matrix operations are implemented in the scipy sparse-matrix package, and the “unrolling” code performs a number of optimizations to the sequence in-line: one important one is to use the fact that vX ◦ (vY Many) = vX ||vY ||1 to avoid explicitly building Many.

4 Related Work
Hybrid logical/neural systems. There is a long tradition of embedding logical expressions in neural networks for the purpose of learning, but generally this is done indirectly, by conversion of the logic to a boolean formula, rather than developing a differentiable theorem-proving mechanism, as considered here. Embedding logic may lead to a useful architecture [15] or regularizer [12].
Recently [11] have proposed a differentiable theorem prover, in which a proof for an example is unrolled into a network. Their system includes representation-learning as a component, as well as a template-instantiation approach (similar to [18]), allowing structure learning as well. However,

5

published experiments with the system been limited to very small datasets. Another recent paper [1] describes a system in which non-logical but compositionally deﬁned expressions are converted to neural components for question-answering tasks.

Explicitly grounded probabilistic ﬁrst-order languages. Many ﬁrst-order probabilistic models are implemented by “grounding”, i.e., conversion to a more traditional representation.1 For example,
Markov logic networks (MLNs) are a widely-used probabilistic ﬁrst-order model [10] in which a Bernoulli random variable is associated with each potential ground database fact (e.g., in the binary-predicate case, there would be a random variable for each possible p(a, b) where a and b are any facts in the database and p is any binary predicate) and each ground instance of a clause is a factor. The Markov ﬁeld built by an MLN is hence of size O(|C|2) for binary predicates, which is
much larger than the factor graphs used by TensorLog, which are of size linear in the size of the
theory. In our experiments we compare to ProPPR, which has been elsewhere compared extensively
to MLNs.

Inference on the Markov ﬁeld can also be expensive, which motivated the development of probabilistic similarity logic (PSL), [2] a MLN variant which uses a more tractible hinge loss, as well as lifted relational neural networks [13], a recent model which grounds ﬁrst-order theories to a neural network. However, any grounded model for a ﬁrst-order theory can be very large, limiting the scalability of such techniques.

Probabilistic deductive databases and tuple independence.

TensorLog is superﬁcially similar to the tuple independence model for PrDDB’s [14], which use Θ
to deﬁne a distribution, Pr(I|DB, Θ), over “hard” databases (aka interpretations) I. In particular,
to generate I, each fact f ∈ DB sampled by independent coin tosses, i.e., PrTupInd(I|DB, Θ) ≡ t∈I θt · t∈DB−I (1 − θt). The probability of a derived fact f is deﬁned as follows, where |[·]| is a
zero-one indicator function:

Pr (f |T , DB, Θ) ≡ |[f ∈ Model(I, T )]| · Pr(I|DB, Θ)

(3)

TupInd I

There is a large literature (for surveys, see [14, 4]) on approaches to tractibly estimating Eq 3, which naively requires marginalizing over all 2|DB| interpretations. One approach, taken by the ProbLog system [5], relies on the notion of an explanation. An explanation E for f is a minimal interpretation that supports f : i.e., f ∈ Model(T , E) but f ∈ Model(T , E ) for all E ⊂ E. It is easy to show that if E ⊃ E then f ∈ Model(T , E ); hence, the set Ex(f ) of all explanations for f is a more concise representation of the interpretations that support f .

Under the tuple independence model, the marginal probability of drawing some interpretation I ⊇ E is simply

θf

(1 − θf ) =

θf

I⊇E f ∈I f ∈DB−I

f ∈E

while in TensorLog,

1

Pr (f ) TenLog

=

Z gTenLog(f ),

where

gTenLog(f )

=

θf

E∈Ex(f ) f ∈E

So TensorLog’s score for a single-explanation fact is the same as under PrTupInd, but more generally only approximates Eq 3, since

Pr (f ) = TupInd

|[f ∈ Model(I, T )]| · Pr(I) =

θf

(1 − θf )

I

I:f ∈Model(I,T ) f ∈I f ∈DB−I

=

θf =

θf = gTenLog(f )

E∈Ex(I) I⊇E f ∈I

E∈Ex(f ) f ∈E

the inequality occurring because TensorLog overcounts interpretations I that are supersets of more than one explanation.

This approximation step is important to TensorLog’s efﬁciency, however. Exact computation of probabilities in the tuple independence model are #P hard to compute [5] in the size of the set of

1For a survey of such models see [7].

6

explanations, which as noted, can itself be exponentially large. A number of methods have been developed for approximating this computation, or performing it as efﬁciently as can be done—for example, by grounding to a boolean formula and converting to a decision-tree like format that facilitates counting [14]. Below we experimentally compare inference times to ProbLog2, one system which adopts these semantics.

Stochastic logic programs and ProPPR. TensorLog is more closely related to stochastic logic

programs (SLPs) [3]. In an SLP, a probabilistic process is associated with a top-down theorem-prover:

i.e., each clause r used in a derivation has an assocated probability θr. Let N (r, E) be the number of

times

r

was

used

in

deriving

the

explanation

E:

then

in

SLPs,

PrSLP(f )

=

1 Z

E∈Ex(f ) r θrN (r,E).

The same probability distribution can be generated by TensorLog if (1) for each rule r, the body of r is

preﬁxed with the literals assign(RuleId,r),weighted(RuleId), where r is a unique identiﬁer for the

rule and (2) Θ is constructed so that θf = 1 for ordinary database facts f , and θweighted(r) = θr,

where Θ is the parameters for a SLP.

SLPs can be normalized or unnormalized; in normalized SLPs, Θ is deﬁned so for each set of clauses Sp of clauses with the same predicate symbol p in the head, r∈Sp θr = 1. TensorLog can represent both normalized and unnormalized SLPs (although clearly learning must be appropriately constrained to learn parameters for normalized SLPs.) Normalized SLPs generalize probabilistic context-free grammars, and unnormalized SLPs can express Bayesian networks or Markov random ﬁelds [3].

ProPPR [17] is a variant of SLPs in which (1) the stochastic proof-generation process is augmented with a reset, and (2) the transitional probabilities are based on a normalized soft-thresholded linear weighting of features. The ﬁrst extension to SLPs can be easily modeled in TensorLog, but the second cannot: the equivalent of ProPPR’s clause-speciﬁc features can be incorporated, but they are globally normalized, not locally normalized as in ProPPR.

ProPPR also includes an approximate grounding procedure which generates networks of size linear in m, α−1, −1, and where m is the number of training examples, α is the reset parameter, degitmax is the maximum degree of the proof graph, and is the pointwise error of the approximation. Asymptotic
analysis suggests that ProPPR should be faster for very large database and small numbers of training
examples (assuming moderate values of and α are feasible to use), but that TensorLog should be
faster with large numbers of training examples and moderate-sized databases.

5 Experiments
We compared TensorLog’s inference time with ProbLog2, a mature probabilistic logic programming system which implements the tuple independence semantics, on two inference problems described in [5]. One is a version of the “friends and smokers” problem, a toy model of social inﬂuence. In [5] small graphs were artiﬁcially generated using a preferential attachment model, the details of which were not described; instead we used a small existing network dataset2 which displays preferential-attachment statistics. The inference times we report are for the same inference tasks, for a subset of 120 randomly-selected entities. In spite of querying six times as many entities, TensorLog is many times faster.
We also compare on a path-ﬁnding task, also described in [5], which is intended to test performance on deeply recursive tasks. The goal here is to compute ﬁxed-depth transitive closure on a grid: in [5] a 16-by-16 grid was used, with a maximum path length of 10. Again TensorLog shows much faster performance, and better scalability3, as shown by run times on a larger 64-by-64 grid.
These results demonstrate that TensorLog’s approximation to ProbLog2’s semantics is efﬁcient, but not that it is useful. To demonstrate that TensorLog can efﬁciently and usefully approximate deeply recursive concepts, we posed a learning task on the 16-by-16 grid, and trained TensorLog to approximate the distribution for this task. The dataset consists of 256 grid cells connected by 2116 edges, so there are 256 example queries of the form path(a,X) where a is a particular grid cell. We picked 1/3 of these queries as test, and the remainder as train, and trained so that that the single positive answer to the query path(a,X) is the extreme corner closest to a—i.e., one of the corners
2The Citeseer dataset from [8]. 3We set TensorLog’s maximum depth to 10 for the 16-by-16 grid, and to 99 for the larger grid.

7

Table 1: Comparison of TensorLog to ProbLog2 and ProPPR

ProbLog2 TensorLog

Social Inﬂuence Task 20 nodes 40-50 sec 3327 nodes 0.84 msec

ProbLog2 TensorLog (trained)

Path-ﬁnding

Size

Time

16x16 grid, d = 10 100-120 sec

16x16 grid, d = 10

5.2 msec

16x16 grid, d = 10 18.7 msec

64x64 grid, d = 64

5.4 msec

Acc 96.5%

Cora (13k facts,10 rules) Wordnet (276k facts) Hypernym (46 rules) Hyponym (46 rules) Deriv. Related (49 rules) Freebase15k (923k facts) division-2nd-level person-profession actor-performance

ProPPR AUC sec 83.2 97.9s
93.4 166.8s 92.1 165.6s 8.2 166.6s
56.4 128.5s 45.8 24.4s 37.4 19.0s

TensorLog AUC sec 97.6 102.8s
93.3 154.9s 92.8 152.5s 6.7 168.2s
50.8 95.7s 50.0 13.7s 38.0 13.7s

(1,1), (1,16), (16,1) or (16,16). Training for 20 epochs brings the accuracy from to 0% to 96.5% (for test), and learning takes approximately 3 sec/epoch. After learning query times are still quite fast.
We note, however, that ProbLog2, in addition to implementing the full tuple-independence semantics, implements a much more expressive logic than considered here, including a large portion of full Prolog. In contrast TensorLog includes only a subset of Datalog.
The table also includes a visualization of the learned weights for a small 6x6 grid. For every pair of adjacent grid cells u, v, there are two weights to learn, one for the edge from u to v and one for its converse. For each weight pair, we show a single directed edge (the heavy blue squares are the arrows) colored by the magnitude of the difference.
We also compared experimentally with ProPPR on several tasks. One was a citation-matching task (from [17]), in which ProPPR was favorable compared to MLNs4. Motivated by recent comparisons between ProPPR and embedding-based approaches to knowledge-base completion [16], we also compared to ProPPR on six relation-prediction tasks5 involving two databases, Wordnet and FreeBase15k, a 15,000-entity subset of FreeBase, using rules from the (non-recursive) theory used in [16].
In all of these tasks parameters are learned on a separate training set. For TensorLog’s learner, we optimized unregularized cross-entropy loss, using a ﬁxed-rate gradient descent learner. We set the learning rate to 0.1, used no regularization, and used a ﬁxed number of epochs (30), which approximately matched ProPPR’s learning time.6 The parameters θf are simply “clipped” to prevent them becoming negative (as in a rectiﬁed linear unit) and we use softmax to convert the output of the gp functions to distributions. We used the default parameters for ProPPR’s learning.
4We replicated the experiments with the most recent version of ProPPR, obtaining a result slightly higher than the 2013 version’s published AUC of 80.0
5We chose this protocol since the current TensorLog implementation can only learn parameters for one target relation at a time.
6Since the current TensorLog implementation is single-threaded we used only one thread for ProPPR as well.
8

Encouragingly, the accuracy of the two systems after learning is comparable, even with TensorLog’s rather simplistic learning scheme. ProPPR, of course, is not well suited to tight integration with deep learners.
6 Concluding Remarks
Large knowledge bases (KBs) are useful in many tasks, but integrating this knowledge into deep learners is a challenge. To address this problem, we described a probabilistic deductive database, called TensorLog, in which reasoning is performed with a differentiable process. The current TensorLog prototype is limited in many respects: for instance, it is not multithreaded, and only the simplest learning algorithms have been tested. In spite of this, it appears to be comparable to more mature ﬁrst-order probabilistic learners in learning performance and inference time—while holding the promise of allowing large KBs to be tightly integrated with deep learning. Acknowledgements Thanks to William Wang for providing some of the datasets used here; and to William Wang, Katie Mazaitis, and many other colleagues contributed with technical discussions and advice. The author is greatful to Google for ﬁnancial support, and also to NSF for their support of his work via grants CCF-1414030 and IIS-1250956.
9

References
[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. CoRR, abs/1601.01705, 2016.
[2] Matthias Brocheler, Lilyana Mihalkova, and Lise Getoor. Probabilistic similarity logic. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, 2010.
[3] James Cussens. Parameter estimation in stochastic logic programs. Machine Learning, 44(3):245–271, 2001.
[4] Luc De Raedt and Kristian Kersting. Probabilistic inductive logic programming. Springer, 2008.
[5] Daan Fierens, Guy Van Den Broeck, Joris Renkens, Dimitar Shterionov, Bernd Gutmann, Ingo Thon, Gerda Janssens, and Luc De Raedt. Inference and learning in probabilistic logic programs using weighted boolean formulas. To appear in Theory and Practice of Logic Programming, 2016.
[6] Matthew R. Gormley, Mark Dredze, and Jason Eisner. Approximation-aware dependency parsing by belief propagation. Transactions of the Association for Computational Linguistics (TACL), 2015.
[7] Angelika Kimmig, Lilyana Mihalkova, and Lise Getoor. Lifted graphical models: a survey. Machine Learning, 99(1):1–45, 2015.
[8] Frank Lin and William W. Cohen. Semi-supervised classiﬁcation of network data using very few labels. In Nasrullah Memon and Reda Alhajj, editors, ASONAM, pages 192–199. IEEE Computer Society, 2010.
[9] David Poole. The independent choice logic for modelling multiple agents under uncertainty. Artiﬁcial intelligence, 94(1):7–56, 1997.
[10] Matthew Richardson and Pedro Domingos. Markov logic networks. Mach. Learn., 62(1-2):107– 136, 2006.
[11] Tim Rocktäschel and Sebastian Riedel. Learning knowledge base inference with neural theorem provers. In NAACL Workshop on Automated Knowledge Base Construction (AKBC), 2016.
[12] Tim Rocktäschel, Sameer Singh, and Sebastian Riedel. Injecting logical background knowledge into embeddings for relation extraction. In Proc. of ACL/HLT, 2015.
[13] Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezný, and Ondrej Kuzelka. Lifted relational neural networks. CoRR, abs/1508.05128, 2015.
[14] Dan Suciu, Dan Olteanu, Christopher Ré, and Christoph Koch. Probabilistic databases. Synthesis Lectures on Data Management, 3(2):1–180, 2011.
[15] Geoffrey Towell, Jude Shavlik, and Michiel Noordewier. Reﬁnement of approximate domain theories by knowledge-based artiﬁcial neural networks. In Proceedings of the Eighth National Conference on Artiﬁcial Intelligence, Boston, Massachusetts, 1990. MIT Press.
[16] William Yang Wang and William W. Cohen. Learning ﬁrst-order logic embeddings via matrix factorization. In Proceedings of the 25th International Joint Conference on Artiﬁcial Intelligence (IJCAI 2015), New York, NY, July 2016. AAAI.
[17] William Yang Wang, Kathryn Mazaitis, and William W Cohen. Programming with personalized PageRank: a locally groundable ﬁrst-order probabilistic logic. In Proceedings of the 22nd ACM International Conference on Conference on Information & Knowledge Management, pages 2129–2138. ACM, 2013.
[18] William Yang Wang, Kathryn Mazaitis, and William W Cohen. Structure learning via parameter learning. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1199–1208. ACM, 2014.
10

