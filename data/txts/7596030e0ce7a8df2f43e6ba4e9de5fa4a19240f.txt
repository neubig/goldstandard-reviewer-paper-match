Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling

arXiv:2003.10162v2 [math.OC] 5 Nov 2020

Yu-Guan Hsieh Univ. Grenoble Alpes, LJK
38000 Grenoble, France. yu-guan.hsieh@univ-grenoble-alpes.fr

Franck Iutzeler Univ. Grenoble Alpes, LJK
38000 Grenoble, France. franck.iutzeler@univ-grenoble-alpes.fr

Jérôme Malick CNRS, LJK
38000 Grenoble, France. jerome.malick@univ-grenoble-alpes.fr

Panayotis Mertikopoulos Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG
38000 Grenoble, France. Criteo AI Lab, France
panayotis.mertikopoulos@imag.fr

Abstract
Owing to their stability and convergence speed, extragradient methods have become a staple for solving large-scale saddle-point problems in machine learning. The basic premise of these algorithms is the use of an extrapolation step before performing an update; thanks to this exploration step, extragradient methods overcome many of the non-convergence issues that plague gradient descent/ascent schemes. On the other hand, as we show in this paper, running vanilla extragradient with stochastic gradients may jeopardize its convergence, even in simple bilinear models. To overcome this failure, we investigate a double stepsize extragradient algorithm where the exploration step evolves at a more aggressive time-scale compared to the update step. We show that this modiﬁcation allows the method to converge even with stochastic gradients, and we derive sharp convergence rates under an error bound condition.
1 Introduction
A major obstacle in the training of generative adversarial networks (GANs) is the lack of an implementable, strongly convergent method based on stochastic gradients. The reason for this is that the coupling of two (or more) neural networks gives rise to behaviors and phenomena that do not occur when minimizing an individual loss function, irrespective of the complexity of its landscape. As a result, there has been signiﬁcant interest in the literature to codify the failures of GAN training, and to propose methods that could potentially overcome them.
Perhaps the most prominent of these failures is the appearance of cycles [5, 8, 9, 23, 24] and, potentially, the transition to aperiodic orbits and chaos [3, 10, 32, 34, 39]. Surprisingly, nonconvergent phenomena of this kind are observed even in very simple saddle-point problems such as two-dimensional, unconstrained bilinear games [5, 9, 24]. In view of this, it is quite common to examine the convergence (or non-convergence) of a gradient training scheme in bilinear models before applying it to more complicated, non-convex/non-concave problems.
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

A key observation here is that the non-convergence of standard gradient descent-ascent methods in bilinear saddle-point problems can be overcome by incorporating a “gradient extrapolation” step before performing an update. The resulting algorithm, due to Korpelevich [16], is known as the extragradient (EG) method, and it has a long history in optimization; for an appetizer, see Facchinei & Pang [6], Juditsky et al. [14], Nemirovski [29], Nesterov [31], and references therein. In particular, the extragradient algorithm converges for all pseudomonotone variational inequalities (a large problem class that contains all bilinear games, cf. [16]), and the time-average of the generated iterates achieves an O(1/t) rate of convergence in monotone problems [29].
The above concerns the application of extragradient methods with perfect, deterministic gradients and a non-vanishing stepsize. By contrast, in the type of saddle-point problems that are encountered in machine learning (GANs, robust reinforcement learning, etc.), there are two important points to keep in mind: First, the size of the datasets involved precludes the use of full gradients (for more than a few passes at least), so the method must be run with stochastic gradients instead. Second, because the landscapes encountered are not convex-concave, the method’s last iterate is typically preferred to its time-average (which offers no tangible beneﬁts when Jensen’s inequality no longer applies). We are thus led to the following questions: (i) are the superior last-iterate convergence properties of the EG algorithm retained in the stochastic setting? And, if not, (ii) is there a principled modiﬁcation that would restore them?
Our contributions. To motivate our analysis, we ﬁrst analyse a counterexample to show that the last iterate of stochastic EG fails to converge, even in bilinear min-max problems where deterministic EG methods converge from any initialization. We then consider a class of double stepsize extragradient (DSEG) methods with an exploration step evolving more aggressively than the update step and prove it enjoys better convergence guarantees than standard EG in stochastic problems. In more detail:
1. We show that the DSEG algorithm converges with probability 1 in a large class of problems that contains all monotone saddle-point problems.
2. We derive explicit convergence rates for the algorithm’s last iterate under an error bound condition. This is the ﬁrst time that such condition is considered in the analysis of stochastic EG methods, albeit its popularity in the optimization community.
3. For bilinear min-max problems in particular, our analysis establishes that stochastic DSEG methods converge at a O(1/t) rate. Prior to our work, last-iterate convergence rate for bilinear min-max games had only been studied in the deterministic setting.1
4. To account for non-monotone problems, we also provide local versions of these results that hold with (arbitrarily) high probability. Importantly, thanks to the use of a local error bound condition, we can obtain local convergence rates even if the Jacobian at a solution contains purely imaginary eigenvalues.
Related works. The approaches that have been explored in the literature to ensure the convergence of stochastic ﬁrst-order methods, in monotone problems and beyond, include variance reduction with increasing batch size and schemes with vanishing regularization (or “anchoring”). In regard to the former, Iusem et al. [12] showed that using increasing batch size can ensure convergence in pseudomonotone variational inequalities. As for the latter, Koshal et al. [17] and Ryu et al. [38] regularized the problem via the addition of a strongly monotone term with vanishing weight; by properly controlling the weight reduction schedule of this regularization term, it is possible to show the method’s convergence in monotone problems.
In contrast to the above, our approach is based on a modiﬁcation of the choice of the stepsizes, which has only been studied theoretically in the deterministic setting. Zhang & Yu [43] recently examined the convergence of several gradient-based algorithms in unconstrained zero-sum bilinear games with deterministic oracle feedback. Interestingly, they show that the optimal (geometric) rate of convergence in bilinear games is recovered for asymptotically large “exploration” parameters γ → ∞ and inﬁnitesimally small “update” parameters η → 0. Even though the setting there is quite
1Let us still mention the work of Loizou et al. [20] which appeared on arxiv a few weeks after the submission of our manuscript: it proved that stochastic Hamiltonian methods applied to (sufﬁciently) bilinear games ensures also a O(1/t) convergence rate. Nonetheless, Hamiltoninan gradient descent is not guaranteed to converge to a solution in monotone games and in general when it converges, it may converge to an unstable stationary point.
2

Extragradient

[14]

(Mirror-prox)

[15]

[24]

Increasing batch size [12]

Repeated sampling [26]

Assumption monotone strongly monotone strictly coherent
pseudo-monotone
monotone

Guarantee
ergodic last last
best last ergodic

Rate √
1/ t 1/t asymptotic √ 1/ t
asymptotic √
1/ t

SVRE

[2] strongly monotone + ﬁnite sum

last

e−ρt

Double stepsize

variational stability (VS)

Ours

VS + error bound

monotone + afﬁne

last

asymptotic

last

1/t1/3

last

1/t

Table 1: Summary of known convergence results of stochastic extragradient methods. For ergodic, last iterate and best iterate guarantees, the convergence metrics are respectively dual gap, squared distance to the solution set and squared residual. Results for single-call [11, 19] and non-extragradient methods [17, 20, 38] are not included.

different from our own, it is interesting to note that the principle of a smaller update stepsize also applies in their case – see also Liang & Stokes [18] and Mishchenko et al. [26] for a concurrent series of results, and Ryu et al. [38] for an empirical investigation into the stochastic setting.
Regarding convergence counterexamples, in a recent paper, Chavdarova et al. [2] showed that if EG is run with a constant stepsize and noise with unbounded variance, the method’s iterates actually diverge at a geometric rate. Motivated by this, they proposed a SVRG-type variance reduced EG method for ﬁnite-sum problems and proved a geometric convergence of the algorithm when the involved operator is strongly monotone. Compared to this situation, our counterexample illustrates that the non-convergence persists for any error distribution with positive variance (no matter how small) and any stepsize sequence (constant, decreasing, or otherwise). In particular, if EG is run with noisy feedback, its trajectories remain non-convergent even if the noise is almost surely bounded and a vanishing stepsize schedule is employed.
Finally, to make our paper’s position clear with respect to the large corpus of work on stochastic EG methods, we further provide an overview of the most relevant results in Table 1 and refer the interested reader to the supplement for further discussion.

2 Preliminaries

In this section, we brieﬂy review some basics for the class of problems under consideration – namely, saddle-point problems and the associated vector ﬁeld formulation.

Saddle-point problems. The ﬂurry of activity surrounding the training of GANs has sparked

renewed interest in saddle-point problems and zero-sum games. To deﬁne this class of problems

formally, consider a value function L : Rd1 × Rd2 → R which assigns a cost of L(θ, φ) to a player

controlling θ ∈ Rd1 , and a payoff of L(θ, φ) to a player choosing φ ∈ Rd2 . Then, the saddle-point

problem associated to a L consists of ﬁnding a proﬁle (θ , φ ) ∈ Rd1 × Rd2 such that, for all θ ∈ Rd1 ,

φ ∈ Rd2 , we have:

L(θ , φ) ≤ L(θ , φ ) ≤ L(θ, φ ).

(SP)

In this setting, the pair (θ , φ ) is called a (global) saddle point of L – or, in game-theoretic
terminology, a Nash equilibrium (NE). For concision and generality, we will often abstract away from θ and φ by setting x = (θ, φ) ∈ Rd (where, in obvious notation, d = d1 + d2).

Vector ﬁeld formulation. In most cases of interest, the objective L is differentiable and is usually accessed through a ﬁrst-order oracle returning values of the vector ﬁeld V (θ, φ) = (∇θL(θ, φ), −∇φL(θ, φ)). As usual for gradient-based methods, we will frequently (though not always) assume that V is Lipschitz continuous:
Assumption 1. The ﬁeld V is β-Lipschitz continuous i.e., for all x, x ∈ Rd,

V (x ) − V (x) ≤ β x − x .

(LC)

3

The importance of the above is that (SP) is often intractable, so it is natural to examine instead the ﬁrst-order stationarity conditions for V , i.e., the problem:

Find x ∈ Rd such that V (x ) = 0.

(Opt)

This “vector ﬁeld formulation” is the unconstrained case of what is known in the literature as a variational inequality (VI) problem – see e.g., Facchinei & Pang [6] for a comprehensive introduction. In what follows, we will not need the full generality of the VI framework and we will develop our results in the context of (Opt) above; our only blanket assumption in this regard is that the set of solutions X of (Opt) is nonempty.

Feedback assumptions Throughout the sequel, we will assume that the optimizer can access V
via a stochastic ﬁrst-order oracle (SFO). This means that at every stage t of an iterative algorithm, the optimizer can call this black-box mechanism at a point Xt ∈ Rd to get a feedback of the form Vˆt = V (Xt) + Zt where Zt ∈ Rd is an additive noise variable. Our bare-bones assumptions for this
oracle will then be as follows:

Assumption 2. The noise term Zt of SFO satisﬁes

a) Zero-mean:

E[Zt | Ft] = 0.

(1a)

b) Variance control: E[ Zt 2 | Ft] ≤ (σ + κ Xt − x )2 for all x ∈ X .

(1b)

where σ, κ ≥ 0 and Ft denotes the history (natural ﬁltration) of Xt.

It is important to note that in (1b), σ and κ play different roles. When κ = 0, the condition corresponds to the classic bounded variance assumption on the noise. At the other end of the spectrum, σ = 0 implies that the noise vanish on the solution set. This kind of condition has been popularized recently in the machine learning community under the name of interpolation [42]. In the most general case, we have both σ > 0 and κ > 0; then condition (1b) allows the variance of the noise to exhibit quadratic growth with respect to the distance to the solution set. For example, for a stochastic oracle of the form Vˆt = Vˆ (ξ, Xt) where ξ is a random variable and Vˆ is a Carathéodory function,2 this is trivially satisﬁed if Vˆ (ξ, ·) is Lipschitz and the variance of the noise is bounded on X . Therefore, Assumption 2 is fairly weak and veriﬁed by most relevant problems.

3 The extragradient method and its limitations

As discussed earlier, the go-to method for saddle-point problems and variational inequalities is the extragradient (EG) algorithm of Korpelevich [16] and its variants. Formally, in the general setting of the previous section, the EG algorithm can be stated recursively as:

Xt+ 1 = Xt − γtVˆt , 2

Xt+1 = Xt − γtVˆt+ 1 2

(EG)

where γt > 0 is a variable stepsize sequence. Heuristically, the basic idea of the method is as

follows: starting from a base state Xt, the algorithm ﬁrst performs a look-ahead step to generate an

intermediate – or leading – state Xt+ 1 ; subsequently, the oracle is called at Xt+ 1 , and the method

2

2

proceeds to a new state Xt+1 by taking a step from the base state Xt. Hence, the generation of the

leading state can be seen as an exploration step while the second part is the bona ﬁde update step.

One of the reasons for the widespread popularity of (EG) is that it achieves convergence in all monotone problems, without suffering from the non-convergence phenomena (limit cycles or otherwise) that plague vanilla one-step gradient algorithms [6]. However, this guarantee requires the method to be run with deterministic, perfect oracle feedback (i.e., Zt = 0 for all t); if the method is run with genuinely stochastic feedback, the situation is considerably more complicated.

To understand the issues involved, it will be convenient to consider the following elementary example:

min max θφ.

(2)

θ∈R φ∈R

Trivially, the vector ﬁeld associated to (2) is V (θ, φ) = (φ, −θ) and the problem’s unique solution is (θ , φ ) = (0, 0). Given the problem’s simple structure, one would expect that (EG) should be easily capable of reaching a solution; however, as we show below, this is not the case.

2That is, Vˆ (ξ, ·) is continuous for almost all ξ and Vˆ (·, x) is measurable for all x.

4

1.25 1.00

γt = 1/t0.1, ηt = 1/t0.9 γt = ηt = 1/t0.6

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75 −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00

Figure 1: Behavior of (EG) and (DSEG) on Prob-
lem (2) with Gaussian oracle noise. Even with a vanishing, square-summable stepsize γt = 1/t0.6,
the iterates of (EG) cycle; in contrast, (DSEG) with γt = 1/t0.1 and ηt = 1/t0.9 converges.

rη O 1t for Afﬁne Operators 1 rη > 1/2
Assumption 4
Local results

2rγ + rγ > 1

0.5

rγ > 1/q

O t11/3
r γ
+ r γ ≤ 1
Vanilla EG policies

0 1/q 0.5

rγ 1

Figure 2: The stepsize exponents allowed by Assumption 4 for convergence (shaded green). Dashed lines are strict frontiers. Note that vanilla EG (the separatrix rγ = rη) passes just outside of this region, explaining the method’s failure.

Proposition 1. Suppose that (EG) is run on the problem (2) with oracle feedback Vˆt = V (θt, φt) + (ξt, 0) for some zero-mean random variable ξt with variance σ2 > 0. We then have lim inft→∞ E[θt2+ φ2t ] > 0, i.e., the iterates of (EG) remain on average a positive distance away from 0.
Importantly, Proposition 1 places no restrictions on the algorithm’s stepsize sequence and the variance of the noise could be arbitrarily small. Relegating the details to the appendix, the key to showing this result is the recursion
E[θt2+1 + φ2t+1] = (1 − γt2 + γt4) E[θt2 + φ2t ] + (1 + γt2)γt2 σ2.
from which it follows that lim inft E[θt2 + φ2t ] > 0. In turn, this implies that the iterates of (EG) remain on average a positive distance away from the origin. This behavior is illustrated clearly in Fig. 1 which shows a typical non-convergent trajectory of (EG) in the planar problem (2).

4 Extragradient with stepsize scaling

At a high level, Proposition 1 suggests that the beneﬁt of the exploration step is negated by the noise as the iterates of (EG) get closer to the problem’s solution set. To rectify this issue, we will consider a more ﬂexible, double stepsize extragradient (DSEG) method of the form

Xt+ 1 = Xt − γtVˆt, 2

Xt+1 = Xt − ηtVˆt+ 1 , 2

(DSEG)

with γt ≥ ηt > 0. The key idea in (DSEG) is that the scaling of the method’s stepsize parameters affords us an extra degree of freedom which can be tuned to order. In particular, motivated by the failure of (EG) described in the previous section, we will take a stepsize scaling schedule in which the exploration step evolves at a more aggressive time-scale compared to the update step. In so doing, the method will keep exploring (possibly with a near-constant stepsize) while maintaining a cautious update policy that does not blindly react to the observed oracle signals.
For illustration and comparison, we plot in Fig. 1 an instance of this method with a fairly aggressive exploration schedule and a respectively conservative update policy. In contrast to (EG), the iterates of (DSEG) now converge to a solution. We encode this as a positive counterpart to Proposition 1 below:
Proposition 1 . Suppose that (DSEG) is run on the problem (2) with oracle feedback Vˆt = V (θt, φt) + (ξt, 0) for some zero-mean random variable ξt with variance σ2 > 0. If the method’s stepsize policies are of the form γt = 1/trγ and ηt = 1/trη for some rη > rγ ≥ 0 with rγ + rη ≤ 1, we have limt→∞ E[θt2 + φ2t ] → 0.

From an analytic viewpoint, what distinguishes (EG) from (DSEG) is the following reﬁned bound:

5

Lemma 1. Under Assumptions 1 and 2, for all t = 1, 2, . . . and all x ∈ X , it holds

E[ Xt+1 − x 2 | Ft] ≤ (1 + Ct κ2) Xt − x 2 − 2ηt E[ V (Xt+ 1 ), Xt+ 1 − x | Ft]

2

2

− γtηt(1 − γt2β2 − 8γtηt κ2) V (Xt) 2 + Ctσ2,

(3)

with constant Ct = 4γt2ηtβ + 2γt3ηtβ2 + 4ηt2 + 16γt2ηt2 κ2.

The proof of Lemma 1, which we defer to the supplement, relies on a careful analysis of the update between successive iterates to separate the deterministic and the stochastic effects. Analyzing the bound of Lemma 1 term-by-term gives a clear picture of how an aggressive exploration stepsize policy can be helpful:

• The term γtηt(1 − γt2β2 − 8γtηt κ2) V (Xt) 2 provides a consistently negative contribution as long as supt γt < 1/3 max(β, κ).

• The term Ct is antagonistic and needs to be made as small as possible.

• The term E[ V (Xt+ 1 ), Xt+ 1 − x | Ft] plays a lesser role since it is non-negative for variational

2

2

stable problems (see upcoming Assumption 3) and is even identically zero in bilinear problems.

Therefore, to obtain convergence, one needs the coefﬁcient γtηt to be as large as possible and, concurrently, each of the terms γt2ηt, γt3ηt, ηt2 and γt2ηt2 that appear in Ct should be as small as possible. Formally, this would lead to the requirement t γtηt = ∞ and t γt2ηt + ηt2 < ∞. These conditions can be simultaneously achieved by a suitable choice of γt and ηt (cf. Proposition 1 above), but they are mutually exclusive if γt = ηt. This observation is the key motivation for the scale separation between the exploration and the update mechanisms in (DSEG), and is the principal
reason that (EG) fails to converge in bilinear problems.

5 Convergence analysis

We now proceed with our main results for the DSEG algorithm. We begin in Section 5.1 with an asymptotic convergence analysis for (DSEG); subsequently, in Section 5.2, we examine the algorithm’s rate of convergence; ﬁnally, in Section 5.3, we zero in on afﬁne problems. Given our interest in non-monotone problems, we make a clear distinction between global results (which require global assumptions) and local ones (which apply to more general problems).

5.1 Asymptotic convergence

Global convergence. Our assumption for global convergence is a variational stability condition. Assumption 3. The operator V satisﬁes V (x), x − x ≥ 0 for all x ∈ Rd, x ∈ X .

Assumption 3 is veriﬁed for all monotone operators but it also encompasses a wide range of nonmonotone problems; for an overview see e.g., [6, 12, 15, 19, 24] and references therein.

To leverage this assumption, we will further need the algorithm’s update step to decrease sufﬁciently quickly relative to the corresponding exploration step. Formally (and with a fair degree of hindsight), this boils down to the following:
Assumption 4. The stepsizes of (DSEG) satisfy t γtηt = ∞, t ηt2 < ∞, and t γt2ηt < ∞.

Assumption 4 essentially posits that ηt/γt → 0 as t → ∞, so it reﬂects precisely the principle of “aggressive exploration, conservative updates”. In particular, Assumption 4 rules out the choice γt = ηt which would yield the vanilla EG algorithm, providing further evidence for the use of a double stepsize policy. A typical stepsize policy for (DSEG) is

γ

η

γt = (t + b)rγ and ηt = (t + b)rη

(4)

for some γ, η, b > 0 and exponents rγ, rη ∈ [0, 1]. Assumption 4 then translates as rγ + rη ≤ 1, 2rη > 1, and 2rγ + rη > 1 as represented in Fig. 2. With this in mind, we have the following convergence result.
Theorem 1. Let Assumptions 1–4 hold and supt γt < 1/3 max(β, κ), then the iterates Xt of (DSEG) converge almost surely to a solution x of (Opt).

6

As far as we are aware, this is the ﬁrst result of this type for stochastic ﬁrst-order methods: almost sure convergence typically requires stronger hypotheses guaranteeing that V (x), x − x is uniformly positive when x ∈/ X [15, 24]. In particular, Theorem 1 implies the almost sure convergence of the algorithm for bilinear problems like (2) where EG and standard gradient methods do not converge.

Local convergence. To extend Theorem 1 to fully non-monotone settings, we will consider the following local version of Assumptions 1–3 near a solution point x :
Assumption 1 . The ﬁeld V is β-Lipschitz continuous near x , i.e., for all x, x near x ,
V (x ) − V (x) ≤ β x − x .

Assumption 2 . Let x ∈ X and U be a neighborhood of x . The noise term Zt of SFO satisﬁes

a) Zero-mean:

E[Zt | Ft] 1{Xt∈U} = 0.

(5a)

b) Moment control: E[ Zt q | Ft] 1{Xt∈U} ≤ (σ + κ Xt − x )q. (5b)

for some q > 2 and σ, κ ≥ 0.

Assumption 3 . The operator V satisﬁes V (x), x − x ≥ 0 for all x near x .

Notice that (5b) is slightly stronger than (1b) in the sense that we now require to control the qth moment of the noise for some q > 2. Nonetheless, this condition as well as the unbiasedness assumption only need to be satisﬁed in a neighborhood of x . Our next result shows that, with these modiﬁed assumptions, the DSEG algorithm converges locally to solutions with high probability:
Theorem 2. Fix a tolerance level δ > 0 and suppose that Assumptions 1 –3 hold for some isolated solution x of (Opt). Assume further that (DSEG) is run with stepsize parameters of the form (4) with small enough γ, η and proper choice of rγ, rη (cf. Fig. 2). If the algorithm is not initialized too far from x , its iterates converge to x with probability at least 1 − δ.

The ﬁrst step towards proving Theorem 2 is to show that the generated iterates stay close to x with arbitrarily high probability. To achieve this, one needs to control the total noise accumulating from each noisy step, a task which is made difﬁcult by the fact that the norm of the SFO feedback can only be upper bounded recursively and thus depends on previous iterates. In the supplement, we dedicate a lemma to the study of such recursive stochastic processes, and we build our analysis on this lemma.

5.2 Convergence rates

Global rate. To study the algorithm’s convergence rate, we will require the following error bound condition: Assumption 5. For some τ > 0 and all x ∈ Rd, we have

V (x) ≥ τ dist(x, X ).

(EB)

This kind of error bound is standard in the literature on variational inequalities for deriving last iterate convergence rates [see e.g., 6, 21, 22, 40, 41]. In particular, Assumption 5 is satisﬁed by
a) Strongly monotone operators: here, τ is the strong monotonicity modulus. b) Afﬁne operators: for V (x) = M x + v where M is a matrix of size d × d and v is a d-dimensional
vector, τ is the minimum non-zero singular value of M .
In this sense, Assumption 5 provides a uniﬁed umbrella for two types of problems that are typically considered to be poles apart. Our ﬁrst result in this context is as follows:
Theorem 3. Suppose that Assumptions 1–3 and 5 hold and assume that γt ≤ c/β with c < 1. Then:
1. If (DSEG) is run with γt ≡ γ, ηt ≡ η, we have: E[dist(Xt, X )2] ≤ (1 − ∆)t−1 dist(X1, X )2 + C ∆
with constants C = (2γ2ηβ + γ3ηβ2 + η2)σ2 and ∆ = γητ 2(1 − c2).3

3For better readability, these constants are stated for the case κ = 0. On the other hand, if σ = 0 (and κ ≥ 0), a geometric convergence can be proved. The same arguments apply to Theorem 5.

7

2. If (DSEG) is run with γt = γ/(t + b)1−ν and ηt = η/(t + b)ν for some ν ∈ (1/2, 1), we have:

E[dist(Xt, X )2] ≤

C

1 +o

1

∆ − r tr

tr

where r = min(1 − ν, 2ν − 1) and we further assume that γητ 2(1 − c2) > r. In particular, the optimal rate is attained when ν = 2/3, which gives E[dist(Xt, X )2] = O(1/t1/3).

The ﬁrst part of Theorem 3 shows that, if (DSEG) is run with constant stepsizes, the initial condition is forgotten exponentially fast and the iterates converge to a neighborhood of X (though, in line with previous results, convergence cannot be achieved in this case). To make this neighborhood small, we need to decrease both γ and η/γ; this would be impossible for vanilla (EG) for which η/γ = 1.
The second part of Theorem 3 provides an O(1/t1/3) last-iterate convergence rate. In Section 5.3, we further improve this rate to O(1/t) for afﬁne operators by exploiting their particular structure.

Local rate. To study the algorithm’s local rate of convergence, we will focus on solutions of (Opt) that satisfy the following Jacobian regularity condition:
Assumption 5 . V is differentiable at x and its Jacobian matrix JacV (x ) is invertible.
The link between Assumptions 5 and 5 is provided by the following proposition:
Proposition 2. If a solution x satisﬁes Assumption 5 , it satisﬁes (EB) in a neighborhood of x .
The proof of Proposition 2 follows by performing a Taylor expansion of V and invoking the minimax characterization of the singular values of a matrix; we give the details in the supplement. For our purposes, what is more important is that (EB) has now been reduced to a pointwise condition; under this much lighter requirement, we have:
Theorem 4. Fix a tolerance level δ > 0 and suppose that Assumptions 1 –3 and 5 hold for some isolated solution x of (Opt) with q > 3. Assume further x satisﬁes Assumption 5 and (DSEG) is run with stepsize parameters of the form γt = γ/(t + b)1/3 and ηt = η/(t + b)2/3 with large enough b, η > 0. Then, there exist neighborhoods U , U of x and an event EU such that:
a) P(EU | X1 ∈ U ) ≥ 1 − δ. b) P(Xt ∈ U for all t | EU ) = 1. c) E[ Xt − x 2 | EU ] = O 1/t1/3
In words, if (DSEG) is not initialized too far from x , the iterates Xt remain close to x with probability at least 1 − δ and, conditioned on this event, Xt converges to x at a rate O(1/t1/3) in mean square error.
Taken together, Theorems 1 and 4 show that for all monotone stochastic problems with a nondegenerate critical point, employing the suggested stepsize policy yields an asymptotic O(1/t1/3) rate. In more detail, the last point of Theorem 4 shows that, with the same kind of stepsizes as in the second part of Theorem 3, we can retrieve a O(1/t1/3) convergence rate provided that the iterates stay close to the solution. Note that this rate is not a localization of Theorem 3 because, after conditioning, the unbiasedness of the noise is not guaranteed. To overcome this issue, our proof draws inspiration from Hsieh et al. [11] but the use of double stepsizes requires a much more intricate analysis which is reﬂected in the stronger noise assumption.

5.3 A case study of afﬁne operators
We terminate our analysis with a dedicated treatment of afﬁne operators which are commonly studied as a ﬁrst step to understand the training of GANs [1, 5, 9, 18, 25, 43]. The following result improves the O(1/t1/3) rate of Theorem 3 to O(1/t) for afﬁne operators.
Theorem 5. Let V be an afﬁne operator satisfying Assumption 3, and suppose that Assumption 2 holds. Take a constant exploration stepsize γt ≡ γ ≤ c/β with c < 1 (here β is the largest singular value of the associated matrix). Then, the iterates (Xt)t∈N of (DSEG) enjoy the following rates:

8

2 log10 ∥ Xt − x⋆ ∥ 2 log10 ∥ Xt − x⋆ ∥ 2 log10 ∥ V(Xt) ∥

0.0

−0.5

−1.0 −1.5 −2.0

rγ=0, rη=0.7 rγ=0.2, rη=0.7 rγ=0.5, rη=0.7 rγ=0.7, rη=0.7

−2.5 0

1

2

3

4

5

10

10

10

10

10

10

# Iterations

0

−1

−2

rγ=0, rη=0.7

rγ=0.5, rη=0.7

−3

rγ=0.3, rη=0.9

rγ=0.9, rη=0.9

100 101 102 103 104 105 # Iterations

4

2

0

rγ=0, rη=0.7

rγ=0.2, rη=0.7

−2

rγ=0.5, rη=0.7

rγ=0.7, rη=0.7

100 101 102 103 104 105 106 # Iterations

Figure 3: Convergence of a (DSEG) scheme in stochastic bilinear (left), strongly convex-concave (middle) and non convex-concave linear quadratic Gaussian GAN (right) problems. All curves are averaged over 10 runs with the shaded area indicating the standard deviation. The beneﬁt of aggressive exploration is evident.

1. If the update stepsize is constant ηt ≡ η ≤ γ, then:

E[dist(Xt, X )2] ≤ (1 − ∆)t−1 dist(X1, X )2 + C ∆
with C = η2(1 + c2)σ2 and ∆ = γητ 2(1 − c2). 2. If the update stepsize is of the form ηt = η/(t + b) for η > 1/(τ 2γ(1 − c2)) and b > η/γ, then:

E[dist(Xt, X )2] ≤

C

1

1

+o

.

∆−1 t

t

The proof of this theorem relies on the derivation of another descent lemma similar to Lemma 1 but tailored to afﬁne operators. Note also that Assumptions 1 and 5 are automatically veriﬁed in this case.
Theorem 5 mirrors Theorem 3; however, in Part 1 of Theorem 5, the ﬁnal precision is only determined by σ2 and η/γ. Thus, compared to Theorem 3, there is no need to decrease γ to obtain an arbitrarily high accuracy solution. The weaker dependence on γ is further conﬁrmed by Part 2, which shows a O(1/t) rate with γt constant. As far as we are aware, this result gives the best convergence rate for stochastic afﬁne operators compared to the literature, and it gives yet another motivation for the use of a double stepsize strategy.

6 Numerical experiments
This section investigates numerically the beneﬁts of double stepsizes. We run (DSEG) with stepsize of the form (4) on three different problems: i) a bilinear zero-sum game, ii) a strongly convex-concave game and iii) a non convex-concave linear quadratic Gaussian GAN model [5, 28]. We examine their behavior when rγ and rη vary. The exact description of the problems and the experimental details are deferred to the supplement.
As shown in Fig. 3, for bilinear game and Gaussian GAN examples, choosing rη < rγ turns out to be necessary for the convergence of the algorithm, and the convergence speed is positively related to the difference rγ − rη, as per our analysis. For a strongly convex-concave problem, it is known that the iterates produced by (EG) with noisy feedback achieve O(1/t) convergence for proper choice of (γt)t∈N [11, 15]. Our experiment moreover reveals that when a double step-size policy is considered, the convergence speed of the algorithm seems to only depend on (ηt)t∈N and using aggressive (γt)t∈N has little inﬂuence, if any, suggesting that taking a larger exploration step may be a universal solution. Going one step further, we conduct experiments and observe similar phenomena for the generalized optimistic gradient method [27, 38] when the output vector is appropriately chosen. We refer the interested reader to the supplement for a dedicated discussion.

7 Conclusion
In this paper, we examined the beneﬁts of employing a double stepsize extragradient method for which the exploration step is more aggressive than the update step. This additional ﬂexibility turns out to be both necessary and sufﬁcient for the method to achieve superior convergence properties relative to vanilla stochastic extragradient methods in a large spectrum of problems including bilinear games and some non convex-concave models.

9

Our results constitute a ﬁrst attempt towards designing an algorithm that provably avoids cycles and similar non-convergent phenomena in a fully stochastic setting. Several interesting future directions include an extended analysis with relaxation of the variational stability assumption as well as the design of a fully adaptive and/or universal method on the basis of our results.
Broader impact
This work does not present any foreseeable societal consequence.
Acknowledgments
This work has been partially supported by MIAI@Grenoble Alpes, (ANR-19-P3IA-0003).
References
[1] Azizian, W., Scieur, D., Mitliagkas, I., Lacoste-Julien, S., and Gidel, G. Accelerating smooth games by manipulating spectral shapes. In AISTATS ’20: Proceedings of the 23rd International Conference on Artiﬁcial Intelligence and Statistics, 2020.
[2] Chavdarova, T., Gidel, G., Fleuret, F., and Lacoste-Julien, S. Reducing noise in gan training with variance reduced extragradient. In NeurIPS ’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 391–401, 2019.
[3] Cheung, Y. K. and Piliouras, G. Vortices instead of equilibria in minmax optimization: Chaos and butterﬂy effects of online learning in zero-sum games. In COLT ’19: Proceedings of the 32nd Annual Conference on Learning Theory, 2019.
[4] Chung, K.-L. On a stochastic approximation method. The Annals of Mathematical Statistics, 25(3):463–483, 1954.
[5] Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H. Training GANs with optimism. In ICLR ’18: Proceedings of the 2018 International Conference on Learning Representations, 2018.
[6] Facchinei, F. and Pang, J.-S. Finite-Dimensional Variational Inequalities and Complementarity Problems. Springer Series in Operations Research. Springer, 2003.
[7] Fallah, A., Ozdaglar, A., and Pattathil, S. An optimal multistage stochastic gradient method for minimax problems. https://arxiv.org/abs/2002.05683.pdf, 2019.
[8] Flokas, L., Vlatakis-Gkaragkounis, E. V., and Piliouras, G. Poincaré recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave zero-sum games. In NeurIPS ’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019.
[9] Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial networks. In ICLR ’19: Proceedings of the 2019 International Conference on Learning Representations, 2019.
[10] Hofbauer, J. and Sigmund, K. Evolutionary Games and Population Dynamics. Cambridge University Press, Cambridge, UK, 1998.
[11] Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. On the convergence of single-call stochastic extra-gradient methods. In NeurIPS ’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 6936–6946, 2019.
[12] Iusem, A. N., Jofré, A., Oliveira, R. I., and Thompson, P. Extragradient method with variance reduction for stochastic variational inequalities. SIAM Journal on Optimization, 27(2):686–724, 2017.
[13] Jelassi, S., Enrich, C. D., Scieur, D., Mensch, A., and Bruna, J. Extra-gradient with player sampling for provable fast convergence in n-player games. https://arxiv.org/abs/1905. 12363.pdf, 2019.
10

[14] Juditsky, A., Nemirovski, A. S., and Tauvel, C. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.
[15] Kannan, A. and Shanbhag, U. V. Optimal stochastic extragradient schemes for pseudomonotone stochastic variational inequality problems and their variants. Computational Optimization and Applications, 74(3):779–820, 2019.
[16] Korpelevich, G. M. The extragradient method for ﬁnding saddle points and other problems. Èkonom. i Mat. Metody, 12:747–756, 1976.
[17] Koshal, J., Nedic, A., and Shanbhag, U. V. Regularized iterative stochastic approximation methods for stochastic variational inequality problems. IEEE Transactions on Automatic Control, 58(3):594–609, 2012.
[18] Liang, T. and Stokes, J. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In AISTATS ’19: Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics, 2019.
[19] Liu, M., Mroueh, Y., Ross, J., Zhang, W., Cui, X., Das, P., and Yang, T. Towards better understanding of adaptive gradient algorithms in generative adversarial nets. In ICLR ’20: Proceedings of the 2020 International Conference on Learning Representations, 2020.
[20] Loizou, N., Berard, H., Jolicoeur-Martineau, A., Vincent, P., Lacoste-Julien, S., and Mitliagkas, I. Stochastic hamiltonian gradient methods for smooth games. In ICML ’20: Proceedings of the 35th International Conference on Machine Learning, 2020.
[21] Luo, Z.-Q. and Tseng, P. Error bounds and convergence analysis of feasible descent methods: a general approach. Annals of Operations Research, 46(1):157–178, 1993.
[22] Malitsky, Y. Golden ratio algorithms for variational inequalities. Mathematical Programming, pp. 1–28, 2019.
[23] Mertikopoulos, P., Papadimitriou, C. H., and Piliouras, G. Cycles in adversarial regularized learning. In SODA ’18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms, 2018.
[24] Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In ICLR ’19: Proceedings of the 2019 International Conference on Learning Representations, 2019.
[25] Mescheder, L., Nowozin, S., and Geiger, A. Which training methods for gans do actually converge? In ICML ’18: Proceedings of the 35th International Conference on Machine Learning, 2018.
[26] Mishchenko, K., Kovalev, D., Shulgin, E., Richtárik, P., and Malitsky, Y. Revisiting stochastic extragradient. In AISTATS ’20: Proceedings of the 22rd International Conference on Artiﬁcial Intelligence and Statistics, 2020.
[27] Mokhtari, A., Ozdaglar, A., and Pattathil, S. A uniﬁed analysis of extra-gradient and optimistic gradient methods for saddle point problems: proximal point approach. In AISTATS ’20: Proceedings of the 23rd International Conference on Artiﬁcial Intelligence and Statistics, 2020.
[28] Nagarajan, V. and Kolter, J. Z. Gradient descent gan optimization is locally stable. In NIPS ’17: Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 5585–5595, 2017.
[29] Nemirovski, A. S. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229–251, 2004.
[30] Nemirovski, A. S. and Yudin, D. B. Problem Complexity and Method Efﬁciency in Optimization. Wiley, New York, NY, 1983.
11

[31] Nesterov, Y. Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 109(2):319–344, 2007.
[32] Palaiopanos, G., Panageas, I., and Piliouras, G. Multiplicative weights update with constant step-size in congestion games: Convergence, limit cycles and chaos. In NIPS ’17: Proceedings of the 30th International Conference on Neural Information Processing Systems, 2017.
[33] Peng, W., Dai, Y.-H., Zhang, H., and Cheng, L. Training GANs with centripetal acceleration. https://arxiv.org/abs/1902.08949, 2019.
[34] Piliouras, G. and Shamma, J. S. Optimization despite chaos: Convex relaxations to complex limit sets via Poincaré recurrence. In SODA ’14: Proceedings of the 25th annual ACM-SIAM Symposium on Discrete Algorithms, 2014.
[35] Polyak, B. T. Introduction to Optimization. Optimization Software, New York, NY, USA, 1987. [36] Popov, L. D. A modiﬁcation of the Arrow–Hurwicz method for search of saddle points.
Mathematical Notes of the Academy of Sciences of the USSR, 28(5):845–848, 1980. [37] Robbins, H. and Siegmund, D. A convergence theorem for non negative almost supermartingales
and some applications. In Optimizing methods in statistics, pp. 233–257. Elsevier, 1971. [38] Ryu, E. K., Yuan, K., and Yin, W. ODE analysis of stochastic gradient methods with optimism
and anchoring for minimax problems and GANs. https://arxiv.org/abs/1905.10899, 2019. [39] Sandholm, W. H. Population Games and Evolutionary Dynamics. MIT Press, Cambridge, MA, 2010. [40] Solodov, M. V. Convergence rate analysis of iteractive algorithms for solving variational inequality problems. Mathematical Programming, 96(3):513–528, 2003. [41] Tseng, P. On linear convergence of iterative methods for the variational inequality problem. Journal of Computational and Applied Mathematics, 60(1-2):237–252, June 1995. [42] Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., and Lacoste-Julien, S. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In NeurIPS ’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3732–3745, 2019. [43] Zhang, G. and Yu, Y. Convergence behaviour of some gradient-based methods on bilinear zero-sum games. In ICLR ’20: Proceedings of the 2020 International Conference on Learning Representations, 2020.
12

A Additional related work
The ﬁrst analysis of extra√gradient (EG) with stochastic feedback traces back to the work of Juditsky et al. [14], where a O(1/ t) ergodic convergence was shown for monotone problems, and this rate is known to be optimal without further assumptions [30].4 Since then, a large number of works have been dedicated to studying the convergence behavior of stochastic EG-type algorithms, either for better understanding of the algorithm itself or in the hope of ﬁnding a better way to incorporate EG with stochasticity.
Almost sure convergence of stochastic EG was ﬁrst investigated in Kannan & Shanbhag [15]. In the said paper, almost convergence was shown for pseudomonotone plus operators and by additionally assuming that the map is strongly pseudomonotone or monotone and weak-sharp, the authors managed to prove a O(1/t) convergence of the iterate produced by the algorithm. In [24], the pseudo-monotonicity-plus assumption is relaxed to show that stochastic EG still enjoys last-iterate convergence in strict coherent problems. Nonetheless, these results fail to justify the use of EG for stochastic monotone problems, as illustrated in Section 3. Therefore, to improve the convergence behavior of EG in stochastic problems, several modiﬁcations to the original stochastic EG have been proposed [2, 12, 26]. In addition to the ones discussed in Section 1, Mishchenko et al. [26] advocated a repeated sampling strategy and illustrated numerically its better performance when applied to GAN training. They also showed that their proposed algorithm retain the same convergence guarantee as traditional stochastic EG.
In order to reduce the overall computational cost, another line of research aims at designing optimization methods that solve variational problems with a single oracle call per iteration (instead of the two in EG). Algorithms of this family include for example optimistic gradient (OG) [5] and extragradient with extrapolation from the past (PEG) [9, 36]. See Hsieh et al. [11] for a recent overview and corresponding treatment in the stochastic setting. Very recently, the convergence of stochastic OG are further improved in two different ways. In [7], the authors introduced a multistage version of OG for stochastic strongly monotone problems to optimize the dependence of convergence speed on initial error and noise characteristics. On the other hand, inspired by the success of adaptive methods in deep learning, Liu et al. [19] designed an adaptive variant of OG and showed that it enjoyed an adaptive complexity that varies according to the growth rate of the cumulative stochastic gradient. To complete the list, also in the goal of reducing overall computation though under a quite different perspective, Jelassi et al. [13] analyzed a randomized version of stochastic EG in multiplayer game to make the extrapolation step amenable to massive multiplayer settings.

B Generalized optimistic gradient

Considering the similarity between EG and its single-call variants, we believe our analysis on (DSEG) also suggests essential modiﬁcations in terms of stepsizes that should be carried out for these algorithms in the face of stochasticity. As an example, we investigate the OG method of Daskalakis et al. [5], and ﬁnd out that some surprising conclusions can be drawn after applying the double stepsize rule. The generalized OG recursion is commonly stated as follows [27, 38]:

Xt+1 = Xt − ηtVˆt − γt(Vˆt − Vˆt−1)

(OG)

where γt is sometimes called the optimism rate. Similarly to our conclusions, it has been empirically observed that taking large optimism rate often yields better convergence in stochastic problems [33].

Hsieh et al. [11] pointed out that OG is equivalent to the modiﬁed Arrow-Hurwitz method introduced

by Popov [36] and also referred to as PEG by Gidel et al. [9]. Using a double stepsize policy, PEG

becomes:

Xt+ 1 = Xt − γtVˆt− 1 , Xt+1 = Xt − ηtVˆt+ 1 .

2

2

2

(DSPEG)

Hence, leading states can be recursively written as

Xt+ 1 = Xt− 1 − ηt− 3 Vˆt− 1 − γt− 1 Vˆt− 1 + γt− 3 Vˆt− 3 .

2

2

2

2

2

2

2

2

We thereby see that (OG) and (DSPEG) are almost equivalent and they mostly differ in the choice of vectors that the method outputs at the end: OG suggests outputting Xt while PEG instead looks

4Precisely, the results of [14, 15, 24] concern the more general mirror-prox algorithm, which generalized extragradient to the Bregman setting.

13

2 log10 ∥ Xt − x⋆ ∥

0.0

−0.5

−1.0 −1.5 −2.0

rγ=0, rη=0.7 rγ=0.2, rη=0.7 rγ=0.5, rη=0.7 rγ=0.7, rη=0.7

−2.5 0

1

2

3

4

5

10

10

10

10

10

10

# Iterations

2

1

0

−1

rγ=0, rη=0.7

rγ=0.2, rη=0.7

−2

rγ=0.5, rη=0.7

rγ=0.7, rη=0.7

−3 0 1 2 3 4 5

10

10

10

10

10

10

# Iterations

2 log10 ∥ Xt − x⋆ ∥

2 log10 ∥ Xt − x⋆ ∥

0

−1

−2

rγ=0, rη=0.7

rγ=0.5, rη=0.7

−3

rγ=0.3, rη=0.9

rγ=0.9, rη=0.9

100 101 102 103 104 105 # Iterations

0

−1

−2

rγ=0, rη=0.7

rγ=0.5, rη=0.7

−3

rγ=0.3, rη=0.9

rγ=0.9, rη=0.9

100 101 102 103 104 105 # Iterations

2 log10 ∥ V(Xt) ∥

2 log10 ∥ V(Xt) ∥

4

2

0

rγ=0, rη=0.7

rγ=0.2, rη=0.7

−2

rγ=0.5, rη=0.7

rγ=0.7, rη=0.7

100 101 102 103 104 105 106 # Iterations

4

2

0

rγ=0, rη=0.7

rγ=0.2, rη=0.7

−2 rγ=0.5, rη=0.7 rγ=0.7, rη=0.7

100 101 102 103 104 105 106 # Iterations

2 log10 ∥ Xt − x⋆ ∥

Figure 4: Convergence of (DSEG) (top) and (OG) (bottom) schemes in stochastic bilinear (left), strongly convex-concave (middle) and non convex-concave covariance matrix learning (right) problems. In the second row the dashed lines and the solid lines depict respectively the results for optimistic iterates and residual iterates. We observe clearly the beneﬁt of (i) aggressive exploration and (ii) using residual iterates in generalized OG methods. All curves are averaged over 10 runs with the shaded area indicating the standard deviation.

at Xt + γt−1Vˆt−1. This nuance turns out to be of importance when generalized OG is applied to stochastic problems. By analogy with our analysis for (DSEG), we reasonably conjecture that taking ηt < γt guarantees the convergence of Xt + γt−1Vˆt−1, and this may occur even if γt is set to constant. Nonetheless, this also implies that if the noise is not vanishing at the solution, Xt, which corresponds to the exploration state in PEG, might exhibit much slower convergence or even not converge at all.
To summarize, when running (OG) for stochastic problems, we should look at the residual iterate Xt + γt−1Vˆt−1 instead of the optimistic iterate Xt. Interestingly, this conclusion is consistent with the ODE analysis of OG by Ryu et al. [38], and explains some experimental results of said work. Furthermore, taking an aggressive exploration step γt and a more conservative update step ηt may be very beneﬁcial both in theory (for the last iterate convergence and rate) and in practice as conﬁrmed by our experiments just below.
C Experimental details and additional experiments
We provide here a detailed explanation of the problems that we consider in our experiments and elucidate the used parameters. Additional experimental results are also presented.
Bilinear zero-sum games. The bilinear zero-sum game takes the form
L(θ, φ) = θ Cφ
where C is a 50 × 50 invertible matrix in our experiment; in that case, (θ , φ ) = (0, 0) is the only equilibrium point. We simulate the stochastic oracle by adding a Gaussian noise Z ∼ N (0, σI) with σ = 0.5 to the vector ﬁeld.
Strongly convex-concave game. To understand the effect of aggressive exploration in strongly convex-concave problems, we inspect the following example
L(θ, φ) = θ A2θ 2 + 2θ A1θ + 4θ Cφ − 2φ B1φ − φ B2φ 2,
where A1, A2, B1, B2 are 50 × 50 positive deﬁnite matrices so (θ , φ ) = (0, 0) is again the only solution of the problem. We take the same noise distribution to construct the stochastic oracle.
Linear Quadratic Gaussian GAN. Finally, to examine the convergence of (DSEG) in stochastic non convex-concave problems, we consider the following problem from Daskalakis et al. [5] and Nagarajan & Kolter [28]:
L(Y, W ) = Ex∼N (0,Σ)[x W x] − Ez∼N (0,I)[z Y W Y z].

14

Bilinear Strongly convex-concave Gaussian GAN

Double stepsize extragradient (DSEG)

γ1

η1

b

1

0.1

19

0.1

0.05

19

0.5

0.05

49

Generalized optimistic gradient (OG)

γ1

η1

b

0.5

0.05

19

0.1

0.05

19

0.05

0.025

99

Table 2: The stepsize parameters for (DSEG) and (OG) in the experiments.

This saddle-point problem corresponds to the WGAN formulation without clipping when data are sampled from a normal distribution with covariance matrix Σ, i.e., x ∼ N (0, Σ), and the generator
and the discriminator are respectively deﬁned by G(z) = Y z, D(x) = x W x. The stochasticity is induced by the sampling of x and z. For the experiments we take a mini-batch of size 128 and x and z of dimension 10. As the game may possess multiple equilibria, the squared norm of V is traced as the convergence measure.

Results for (DSEG) and (OG). Following the discussion of Appendix B, we complement the illustration of our method (DSEG) by a comparison with (OG) with properly chosen outputs. In the experiments, both (DSEG) and (OG) are run with stepsize of the form (4) with various rγ and rη. In order to start with the same value for different exponents, we ﬁx b, γ1, and η1 as indicated in Table 2, from which we deduce γ = γ1(1 + b)rγ and η = η1(1 + b)rη .
As shown in Fig. 4, for bilinear game and Gaussian GAN examples, the convergence speed of (DSEG) is positively related to the difference rγ − rη, as per our analysis. For the strongly convex-concave problem, the vanilla (EG) already achieves O(1/t) convergence, and the plot shows that using aggressive (γt)t∈N has little inﬂuence on it.
Regarding (OG) with the residual iterates, the algorithm has roughly the same convergence behavior as for (DSEG). In contrast, the optimistic iterates tend to converge much slower. In particular, choosing a constant exploration step gives the fastest convergence of the residual iterate though the optimistic iterate does not converge, in line with our discussion in Appendix B.

Additional discussions for bilinear games. Few algorithms provably converge in stochastic bilinear games, and among them there are stochastic Hamiltonian gradient descent (SHGD) [20] and gradient descent with anchoring [38]. In Fig. 5 we illustrate the convergences of DSEG and these two algorithms for the stochastic bilinear saddle-point example. For (DSEG) we adopt the optimal stepsize schedule as described in Theorem 5-2. The leading stepsize is set to constant γt ≡ 1 and the update stepsize is ηt = η/(t + b) with η = 2 and b = 19. The same (ηt)t∈N is also used as the stepsize of SHGD, in accordance with the decreasing stepsize strategy presented in [20]. As for the anchored gradient methods, its update is written as

1 − r (1 − r)γ Xt+1 = Xt − tr + tν (X1 − Xt),

and it is proved to converge in all stochastic monotone problems for γ > 0 and r, ν ∈ (1/2, 1). Since no explicit rate is proven for this algorithm when stochastic gradients are used, we run hyperparameter optimization to search for the best γ, r and ν, and end up with γ = 1, r = 0.7, ν = 0.9.
1

Fig. 5 conﬁrms that asymptotically both DSEG and SHGD converge in O(1/t) as predicted by the theory. SHGD converges slightly faster than DSEG for the ﬁrst few iterations as it circumvents the rotational dynamics by directly performing stochastic gradient descent on V (·) 2, which turns out to be a positive deﬁnite quadratic form when V is linear. This however comes at the cost of the use of second-order information. In fact, SHGD requires access

2 log10 ∥ Xt − x⋆ ∥

0

−1

−2 DSEG

−3

Hamiltonian

Anchoring

100 101 102 103 104 105 # Iterations

to an unbiased estimator of JacV V at every iteration. Finally, anchoring converges much slower compared to these
two methods. Without further theoretical investigation we
do not know if this kind of algorithms can achieve the same O(1/t) convergence rate in this problem.

Figure 5: Comparison of DSEG, stochastic Hamiltonian gradient descent and anchored gradient in the stochastic bilinear example. All curves are averaged over 10 runs with the shaded area indicating the standard deviation.

15

D Technical lemmas

In this section we recall several important lemmas that are frequently used in the analysis of stochastic iterative methods. The ﬁrst three lemmas on numerical sequences are useful for deriving convergence rates of the algorithms. See e.g., Polyak [35] for an abundance of results of this type. Lemma D.1. Let (at)t∈N be a sequence of real numbers such that for all t,
at+1 ≤ (1 − q)at + q ,
where 1 > q > 0 and q > 0. Then,
at ≤ (1 − q)t−1a1 + q . q

The above lemma comes into play when an algorithm is run with constant stepsize sequences, whereas we resort to the following two lemmas in case of decreasing stepsize sequences of the form (4).

Lemma D.2 (Chung [4, Lemma 1]). Let (at)t∈N be a sequence of real numbers and b ∈ N such that for all t,

q

q

at+1 ≤

1− t+b

at + (t + b)r+1 ,

where q > r > 0 and q > 0. Then,

q1

1

at ≤ q − r tr + o tr .

Lemma D.3 (Chung [4, Lemma 4]). Let (at)t∈N be a sequence of real numbers and b ∈ N such that for all t,

at+1 ≤

q 1 − (t + b)ν

q at + (t + b)r+ν ,

where 1 > ν > 0 and r, q, q > 0. Then,

1 at = O tr .

To establish almost sure convergence of the iterates, we rely on the Robbins–Siegmund theorem which apply to non-negative almost-supermatingales.

Lemma D.4 (Robbins & Siegmund [37]). Consider a ﬁltration (Ft)t∈N and four non-negative

(Ft)t∈N-adapted processes (Ut)t∈N, (λt)t∈N, (χt)t∈N, (ζt)t∈N such that ∞ with probability one and ∀t ∈ N,

t λt < ∞ and

t χt <

E[Ut+1 | Ft] ≤ (1 + λt)Ut + χt − ζt.

(D.1)

Then (Ut)t∈N converges almost surely to a random variable U∞ and t ζt < ∞ almost surely.

E Proofs for global convergence results

We then start with the proofs of the global results to highlight the effect of double stepsize, before tackling the more challenging local convergence analysis.

E.1 Proof of Proposition 1: failure of stochastic extragradient Proposition 1. Suppose that (EG) is run on the problem (2) with oracle feedback Vˆt = V (θt, φt) + (ξt, 0) for some zero-mean random variable ξt with variance σ2 > 0. We then have lim inft→∞ E[θt2+ φ2t ] > 0, i.e., the iterates of (EG) remain on average a positive distance away from 0.

16

Proof. We write the updates of the algorithm

θt+ 1 = θt − γtφt − γtξt 2
φt+ 1 = φt + γtθt 2

θt+1 = θt − γtφt − γt2θt − γtξt+ 1 2
φt+1 = φt + γtθt − γt2φt − γt2ξt

Therefore

θt2+1 + φ2t+1 = (1 − γt2 + γt4)(θt2 + φ2t ) + γt2ξt2+ 1 + γt4ξt2 2 − 2γtξt+ 1 ((1 − γt2)θt − γtφt) − 2γt2ξt((1 − γt2)φt + γtθt). 2

Taking expectation leads to

E[θt2+1 + φ2t+1] = (1 − γt2 + γt4) E[θt2 + φ2t ] + (γt2 + γt4)σ2.

For sake of simplicity, let us denote at = E[θt2 + φ2t ]. We consider two scenarios:

Case 1: γt2 ≥ 1. We have 1 − γt2 + γt4 ≥ 1 and consequently at+1 ≥ at.

Case 2: γt2 < 1. Notice that

(1 + γt2)σ2

2

4

(1 + γt2)σ2

at+1 − 1 − γ2 = (1 − γt + γt ) at − 1 − γ2 .

t

t

We then set νt = (1 + γt2)/(1 − γt2). Since 1 − γt2 + γt4 < 1, at+1 gets closer to νtσ2 than at. In particular, if at < νtσ2, we have at < at+1 < νtσ2; otherwise, at ≥ at+1 ≥ νtσ2. As νt ≥ 1, the above implies at+1 ≥ min(at, νtσ2) ≥ min(at, σ2).

To conclude, in the two cases we have at+1 ≥ min(at, σ2), showing lim inft→∞ E[θt2 + φ2t ] > 0.

A remedy with double stepsize extragradient. rithm write
θt+ 1 = θt − γtφt − γtξt 2
φt+ 1 = φt + γtθt 2

With different stepsizes, the updates of the algoθt+1 = θt − ηtφt − γtηtθt − ηtξt+ 1
2
φt+1 = φt + ηtθt − γtηtφt − γtηtξt

This now leads to E[θt2+1 + φ2t+1] = ((1 − γtηt)2 + ηt2) E[θt2 + φ2t ] + (ηt2 + γt2ηt2)σ2 = (1 − 2γtηt + ηt2 + γt2ηt2) E[θt2 + φ2t ] + (ηt2 + γt2ηt2)σ2.

Taking γt =

1 trγ

and ηt =

1 trη

,

we

get

E[θt2+1 + φ2t+1] = 1 − t(rγ2+rη) + t21rη + t2(rγ1+rη) E[θt2 + φ2t ] + t21rη + t2(rγ1+rη) σ2

1.5

2 2 2σ2

≤ 1 − t(rγ +rη) E[θt + φt ] + t2rη

=O

1 t(rη−rγ )

where the inequality comes from 1 − 2/t(rγ+rη) + 1/t2rη + 1/t2(rγ+rη) ≤ 1 − 1.5/t(rγ+rη) for large enough t and the last part is an application of either Lemma D.2 or Lemma D.3 with q = 1.5 > r = rη − rγ > 0 (starting at large enough t).
Hence, E[θt2 + φ2t ] → 0, i.e. we can ﬁnd a double stepsize choice, with an aggressive extrapolation step and a conservative update step (rγ < rη) such that (θt, φt) → (0, 0) in mean squared error.

E.2 Proof of Lemma 1

Lemma 1. Under Assumptions 1 and 2, for all t = 1, 2, . . . and all x ∈ X , it holds

E[ Xt+1 − x 2 | Ft] ≤ (1 + Ct κ2) Xt − x 2 − 2ηt E[ V (Xt+ 1 ), Xt+ 1 − x | Ft]

2

2

− γtηt(1 − γt2β2 − 8γtηt κ2) V (Xt) 2 + Ctσ2,

(3)

with constant Ct = 4γt2ηtβ + 2γt3ηtβ2 + 4ηt2 + 16γt2ηt2 κ2.

17

Proof. Let us denote by Et[·] = E[· | Ft] the conditional expectation with respect to the ﬁltration up

to time t and X˜t+ 1 = Xt − γtV (Xt) the leading state that is generated with deterministic update so

2

that Xt+ 1 = X˜t+ 1 − γtZt. We develop

2

2

Xt+1 − x 2 = Xt − ηtVˆt+ 1 − x 2 2

= Xt − x 2 − 2ηt Vˆt+ 1 , Xt − x + ηt2 Vˆt+ 1 2

2

2

= Xt − x

2 − 2ηt Vˆt+ 1 , X˜t+ 1 − x

2

2

− 2γtηt Vˆt+ 1 , V (Xt) + ηt2 Vˆt+ 1 2.

2

2

(E.1)

We would then like to bound the different terms appearing on the right-hand side (RHS) of the equality. With the zero-mean assumption (1a), conditioning on Ft leads to

Et[ Vˆt+ 1 , X˜t+ 1 − x ] = Et[ V (Xt+ 1 ), X˜t+ 1 − x ]

2

2

2

2

= Et[ V (Xt+ 1 ), X˜t+ 1 − γtZt − x ] + Et[ V (Xt+ 1 ), γtZt ]

2

2

2

= Et[ V (Xt+ 1 ), Xt+ 1 − x ] + γt Et[ V (Xt+ 1 ) − V (X˜t+ 1 ), Zt ],

2

2

2

2

(E.2)

where in the last line we use the fact that V (X˜t+ 1 ) is Ft-measurable so 2

Et[ V (X˜t+ 1 ), Zt ] = V (X˜t+ 1 ), Et[Zt] = 0.

2

2

By Lipschitz continuity of V

− V (Xt+ 1 ) − V (X˜t+ 1 ), Zt ≤ V (Xt+ 1 ) − V (X˜t+ 1 ) Zt ≤ γtβ Zt 2.

2

2

2

2

(E.3)

On the other hand, Et[ Vˆt+ 1 , V (Xt) ] = Et[ V (Xt+ 1 ), V (Xt) ] and Et[ Vˆt+ 1 2] =

2

2

2

Et[ V (Xt+ 1 ) 2] + Et[ Zt+ 1 2]. By ηt ≤ γt, Lipschitz continuity of V and Xt − Xt+ 1 = γtVˆt, we get 2 2 2

− 2γtηt V (Xt+ 1 ), V (Xt) + ηt2 V (Xt+ 1 ) 2

2

2

≤ −2γtηt V (Xt+ 1 ), V (Xt) + γtηt V (Xt+ 1 ) 2

2

2

= γtηt( V (Xt) − V (Xt+ 1 ) 2 − V (Xt) 2) 2

≤ γt3ηtβ2 Vˆt 2 − γtηt V (Xt) 2,

(E.4)

Similar to before we may write Et[ Vˆt 2] = Et[ V (Xt) 2] + Et[ Zt 2]. Therefore, combining

(E.1), (E.2), (E.3), (E.4), we deduce the following

Et[ Xt+1 − x 2] ≤ Xt − x 2 − 2ηt Et[ V (Xt+ 1 ), Xt+ 1 − x ] − (γtηt − γt3ηtβ2) V (Xt) 2

2

2

+ (2γt2ηtβ + γt3ηtβ2) E[ Zt 2] + ηt2 E[ Zt+ 1 2]. 2

(E.5)

To ﬁnish the proof, we would like to bound the noise terms. Using (1b) and Jensen’s inequality (recall

that q ≥ 2), we have

E[ Zt 2] ≤ (σ + κ Xt − x )2 ≤ 2σ2 + 2 κ2 Xt − x 2.

(E.6)

Similarly, E[ Zt+ 1
2

2] ≤ 2σ2 + 2 κ2 Xt+ 1 − x 2 2 ≤ 2σ2 + 4 κ2 Xt+ 1 − Xt 2 + 4 κ2 Xt − x 2 2 ≤ 4γt2 κ2 Vˆt 2 + 4 κ2 Xt − x 2 + 2σ2 ≤ 8γt2 κ2 V (Xt) 2 + 16γt2 κ2 σ2 + 16γt2 κ4 Xt − x

2 + 4 κ2 Xt − x

Substituting (E.7) and (E.6) in (E.5), we obtain

Et[ Xt+1 − x 2] ≤ (1 + 4γt2ηtβ κ +2γt3ηtβ2 κ +4ηt2 κ2 +16γt2ηt2 κ4)

− 2ηt Et[ V (Xt+ 1 ), Xt+ 1 − x ]

2

2

− (γtηt − γt3ηtβ2 − 8γt2ηt2 κ2) V (Xt) 2

+ (4γt2ηtβ + 2γt3ηtβ2 + 2ηt2 + 16γt2ηt2 κ2)σ2.

We recover (3) by using 2ηt2σ2 ≤ 4ηt2σ2.

Xt − x

2 + 2σ2 (E.7)
2

18

E.3 Proof of Theorem 1
Theorem 1. Let Assumptions 1–4 hold and supt γt < 1/3 max(β, κ), then the iterates Xt of (DSEG) converge almost surely to a solution x of (Opt).

Proof. The proof is divided into three key steps.

(1) With probability 1, lim inft→∞ V (Xt) = 0. Let x ∈ X . Using Lemma 1 and Assumption 3, we get the following

E[ Xt+1 − x

2 | Ft] ≤ (1 + Ct κ2) Xt − x 2 − 2ηt E[ V (Xt+ 1 ), Xt+ 1 − x | Ft]

2

2

− γtηt(1 − γt2β2 − 8γtηt κ2) V (Xt) 2 + Ctσ2,

≤ (1 + Ct κ2) Xt − x 2 − γtηt(1 − γt2β2 − 8γtηt κ2) V (Xt) 2 + Ctσ2

Since γt < 1/3 max(β, κ) and ηt ≤ γt, the coefﬁcient ρt := γtηt − γt3ηtβ2 − 8γt2ηt2 κ2 is nonnegative. Recalling that Ct = 4γt2ηtβ + 2γt3ηtβ2 + 4ηt2 + 16γt2ηt2 κ2, from our stepsize conditions
t ηt2 < ∞, t γt2ηt < ∞ and (γt)t∈N being upper-bounded, it holds t Ct < ∞. We can therefore apply the Robbins–Siegmund theorem (Lemma D.4) to get that (i) Xt − x converges almost surely and (ii) t ρt V (Xt) 2 < ∞ almost surely. As the stepsize conditions also imply
t ρt = ∞, using (ii), we deduce immediately lim inft→∞ V (Xt) = 0 almost surely.
(2) With probability 1, Xt − x converges for all x ∈ X . In other words, we would like to prove the existence of an event E ⊂ Ω satisfying P(E) = 1 and that for every realization of the event and every x ∈ X , Xt − x converges. Since Rd is a separable metric space, X is also separable and we can ﬁnd a countable set Z such that X = cl(Z) (X is closed by continuity of V ). We claim that the choice E = { Xt − z converges for all z ∈ Z} is the good candidate.
In effect, taking an arbitrary z from Z, from (i) we know that

P({ Xt − z converges}) = 1.

Therefore from the countability of Z we have P(E) = 1. We now ﬁx x ∈ X . As Z is dense in X ,
there exists a sequence (zi)i∈N of points in Z such that limi→∞ zi = x . Consider a realization of E, for every zi we have limt→∞ Xt − zi = νi for some νi ≥ 0. The triangular inequality gives

− zi − x ≤ Xt − x − Xt − zi ≤ zi − x

for all i, t ∈ N. Consequently, for all i ∈ N,

− zi − x

≤ lim inf Xt − x − lim Xt − zi

t→∞

t→∞

= lim inf Xt − x − νi
t→∞

≤ lim sup Xt − x − νi
t→∞

= lim sup Xt − x − lim Xt − zi ≤ zi − x .

t→∞

t→∞

Taking the limit as i → ∞ we obtain the convergence of ( Xt − x t)t∈N; more precisely, limt→∞ Xt − x = limi→∞ νi. We have thus proved E satisﬁes the requirements.

(3) Conclude. Combining the points (1) and (2), we get

P(E ∩{lim inf V (Xt) = 0}) = 1.
t→∞

Let us take a realization of this event. It holds lim inft→∞ V (Xt) = 0 and we can thus extract a subsequence (Xω(t))t∈N such that limt→∞ V (Xω(t)) = 0. Let x ∈ X , we know that Xt − x converges, implying that (Xt)t∈N is bounded. As Rd is ﬁnite dimensional, we can then further extract (Xω(ψ(t)))t∈N so that limt→∞ Xω(ψ(t)) = x∞ for some x∞ ∈ Rd. By continuity of V , we have
V (x∞) = 0, i.e., x∞ ∈ X . By the choice of E, we have the convergence of ( Xt − x∞ t)t∈N, and

lim Xt − x∞ = lim Xω(ψ(t)) − x∞ = x∞ − x∞ = 0.

t→∞

t→∞

To conclude, we have proved that that Xt converges to some x ∈ X almost surely.

19

E.4 Proof of Theorem 3

Theorem 3. Suppose that Assumptions 1–3 and 5 hold and assume that γt ≤ c/β with c < 1. Then:

1. If (DSEG) is run with γt ≡ γ, ηt ≡ η, we have:

E[dist(Xt, X )2] ≤ (1 − ∆)t−1 dist(X1, X )2 + C ∆
with constants C = (2γ2ηβ + γ3ηβ2 + η2)σ2 and ∆ = γητ 2(1 − c2). 2. If (DSEG) is run with γt = γ/(t + b)1−ν and ηt = η/(t + b)ν for some ν ∈ (1/2, 1), we have:

E[dist(Xt, X )2] ≤

C

1 +o

1

∆ − r tr

tr

where r = min(1 − ν, 2ν − 1) and we further assume that γητ 2(1 − c2) > r. In particular, the optimal rate is attained when ν = 2/3, which gives E[dist(Xt, X )2] = O(1/t1/3).

For the sake of readability, the involved constants are stated for the case κ = 0. On the other hand, if σ = 0 and κ ≥ 0, a geometric convergence can be proved.

Proof. We ﬁrst consider the case κ = 0 so that E[ Zt 2] ≤ σ2 and E[ Zt+ 1 2] ≤ σ2. Since 2
γt ≤ c/β, from (E.5) we deduce

Et[ Xt+1 − x 2] ≤ Xt − x 2 − γtηt(1 − c2) V (Xt) 2 + (2γt2ηtβ + γt3ηtβ2 + ηt2)σ2.

By concavity of the minimum operator, we then obtain

Et[ min
x ∈X

Xt+1 − x

2] ≤ min Et[ Xt+1 − x 2]
x ∈X
≤ min Xt − x 2 − γtηt(1 − c2) V (Xt) 2
x ∈X
+ (2γt2ηtβ + γt3ηtβ2 + ηt2)σ2.

In other words,

Et[dist(Xt+1, X )2] ≤ dist(Xt, X )2 − γtηt(1 − c2) V (Xt) 2 + (2γt2ηtβ + γt3ηtβ2 + ηt2)σ2.

Using Assumption 5 and the law of total expectation, this gives

E[dist(Xt+1, X )2] ≤ (1 − γtηtτ 2(1 − c2)) E[dist(Xt, X )2] + (2γt2ηtβ + γt3ηtβ2 + ηt2)σ2.

Points 1 and 2 are obtained respectively by applying Lemma D.1 and Lemma D.2.
For the case κ = 0, the term before E[dist(Xt, X )2] is replaced by 1 + Ct κ2 −ρtτ 2 where ρt = γtηt − γt3ηtβ2 − 8γt2ηt2 κ2 is deﬁned in the proof of Theorem 1. In point 1, the term 1 + Ct κ2 −ρtτ 2 can be made in (0, 1) for γ and η properly chosen. Precisely, we need

(4γβ + 2γ2β2 + 4η + 16γη κ2) κ2 +(γ2β2 + 8γη κ2)τ 2 < τ 2. γ

To prove point 2, notice that the conditions of Lemma D.2 are still veriﬁed when γ, η and b are large enough. For example, if it holds for all t

(4γtβ + 2γ2β2 + 4ηt + 16γtηt κ2) κ2 +(γ2β2 + 8γtηt κ2)τ 2 ≤ τ 2/2

t

γt

t

and γητ 2/2 > r then Lemma D.2 can be applied. Finally, if σ = 0, the key inequality becomes

Et[ Xt+1 − x 2] ≤ (1 + Ct κ2) Xt − x 2 − ρt V (Xt) 2.

We therefore obtain geometric convergence for 1 + Ct κ2 −ρtτ 2 ∈ (0, 1).

20

E.5 Proof of Theorem 5

Theorem 5. Let V be an afﬁne operator satisfying Assumption 3, and suppose that Assumption 2 holds. Take a constant exploration stepsize γt ≡ γ ≤ c/β with c < 1 (here β is the largest singular value of the associated matrix). Then, the iterates (Xt)t∈N of (DSEG) enjoy the following rates:

1. If the update stepsize is constant ηt ≡ η ≤ γ, then:

E[dist(Xt, X )2] ≤ (1 − ∆)t−1 dist(X1, X )2 + C ∆
with C = η2(1 + c2)σ2 and ∆ = γητ 2(1 − c2). 2. If the update stepsize is of the form ηt = η/(t + b) for η > 1/(τ 2γ(1 − c2)) and b > η/γ, then:

E[dist(Xt, X )2] ≤

C

1

1

+o

.

∆−1 t

t

For the sake of readability, the involved constants are stated for the case κ = 0.

Proof. To focus on the most important points of the proof, we shall consider the case κ = 0, while it is straightforward to derive the same kind of result when κ > 0 by following the reasoning of previous proofs. The crucial step here is then the derivation of a stochastic descent inequality in the form of (3). This is again based on (E.1). Writing V (x) = M x + v, we can expand

Vˆt+ 1 = M Xt − γtM 2Xt − γtM v − γtM Zt + v + Zt+ 1 = V (X˜t+ 1 ) − γtM Zt + Zt+ 1 .

2

2

2

2

We recall that X˜t+ 1 = Xt − γtV (Xt). Let x ∈ X . Together with the zero-mean assumption (1a), 2
the above shows that

Et[ Vˆt+ 1 , X˜t+ 1 − x ] = V (X˜t+ 1 ), X˜t+ 1 − x ,

2

2

2

2

Et[ Vˆt+ 1 , V (Xt) ] = V (X˜t+ 1 ), V (Xt) ,

2

2

Et[ Vˆt+ 1 2] = V (X˜t+ 1 ) 2 + Et[ γtM Zt 2] + Et[ Zt+ 1 2].

2

2

2

Similar to (E.4), we write

− 2γtηt V (X˜t+ 1 ), V (Xt) + ηt2 V (X˜t+ 1 ) 2

2

2

≤ −2γtηt V (X˜t+ 1 ), V (Xt) + γtηt V (X˜t+ 1 ) 2

2

2

= γtηt( V (Xt) − V (X˜t+ 1 ) 2 − V (Xt) 2) 2

≤ γtηt(γt2β2 − 1) V (Xt) 2.

We have V (X˜t+ 1 ), X˜t+ 1 − x ≥ 0 by Assumption 3 and Et[ γtM Zt 2] + Et[ Zt+ 1 2] ≤

2

2

2

(γt2β2 + 1)σ2 by Lipschitz continuity of V and the ﬁnite variance assumption (i.e., (1b) with κ = 0).

Taking expectation with respect to Ft over (E.1) then leads to

Et[ Xt+1 − x

2] ≤ Xt − x = Xt − x

2 − γtηt(1 − γt2β2) V (Xt) 2 + ηt2(γt2β2 + 1)σ2 2 − γtηt(1 − c2) V (Xt) 2 + ηt2(1 + c2)σ2.

Proceeding as in the proof of Theorem 3, we get

Et[dist(Xt+1, X )2] ≤ dist(Xt, X )2 − γtηt(1 − c2) V (Xt) 2 + ηt2(1 + c2)σ2.

Since V is afﬁne, it veriﬁes the error bound condition (EB). Writing γ in the place of γt and applying the law of total expectation, we obtain

E[dist(Xt+1, X )2] ≤ (1 − γηtτ 2(1 − c2)) E[dist(Xt, X )2] + ηt2(1 + c2)σ2.

We conclude with help of Lemma D.1 and Lemma D.2.

21

F Proofs for local convergence results

F.1 Local assumptions

For sake of clarity, we recall here the local assumptions that will bu used in the local convergence results. Assumption 1 . The ﬁeld V is β-Lipschitz continuous near x , i.e., for all x, x near x ,
V (x ) − V (x) ≤ β x − x .

Assumption 2 . Let x ∈ X and U be a neighborhood of x . The noise term Zt of SFO satisﬁes

a) Zero-mean:

E[Zt | Ft] 1{Xt∈U} = 0.

(5a)

b) Moment control: E[ Zt q | Ft] 1{Xt∈U} ≤ (σ + κ Xt − x )q. (5b)

for some q > 2 and σ, κ ≥ 0.

Assumption 3 . The operator V satisﬁes V (x), x − x ≥ 0 for all x near x .

Assumption 5 . V is differentiable at x and its Jacobian matrix JacV (x ) is invertible.

For Assumption 2 in particular, when the neighborhood U is bounded, the term κ Xt − x bounded and therefore, by choosing a larger σ if needed, (5b) can be simpliﬁed to
E[ Zt q | Ft] 1{Xt∈U} ≤ σq for all x ∈ X .
We will consider (5b) under this form in the sequel.

is also

F.2 Preparatory lemmas

The proofs of the local statements are much more demanding. The principle pillar of our analysis is a stability result formally stated in Appendix F.3. To prepare us for the challenge, we start by introducing the following lemma for bounding a recursive stochastic process.
Lemma F.1. Consider a ﬁltration (Ft)t∈N and four (Ft)t∈N-adapted processes (Dt)t∈N, (ζt)t∈N, (χt)t∈N, (ξt)t∈N such that (χt)t∈N is non-negative and the following recursive inequality is satisﬁed for all t ≥ 1
Dt+1 ≤ Dt − ζt + χt+1 + ξt+1.
Fixing a constant C > 0, we deﬁne the events (At)t∈N by A1 := {D1 ≤ C/2} and At := {Dt ≤ C} ∩ {χt ≤ C/4} for t ≥ 2. We consider also the decreasing sequence of events (It)t∈N deﬁned by It := 1≤s≤t As. If the following three assumptions hold true

(i) ∀t, ζt 1It ≥ 0,

(ii) ∀t, E[ξt+1 | Ft] 1It = 0,

(iii)

∞ t=1

E[(ξt2+1

+

χt+1)

1It ]

≤

δε

P(A1),

where ε = min(C2/16, C/4) and δ ∈ (0, 1), then P

t≥1 At | A1 ≥ 1 − δ.

Proof. Let us start by introducing the following two (Ft)t∈N-adapted submartingale sequences

t

t

St := ξs and Qt := St2 + χs.

s=2

s=2

Subsequently, we deﬁne an auxiliary sequence of events

Ht := A1 ∩ { max Qs ≤ ε}
2≤s≤t

which is also decreasing. With this at hand, we are ready to start our proof.

(1) Inclusion Ht ⊂ It. We prove the inclusion by induction. The statement is true when t = 1 as H1 = I1 = A1. For t ≥ 2, we write

t−1

t−1

t−1

Dt ≤ D1 − ζs + χs+1 + ξs+1.

s=1

s=2

s=2

(F.1)

22

By induction hypothesis, Ht−1 ⊂ It−1, and thus for all s ≤ t − 1, we have Ht ⊂ It−1 ⊂ Is.

Combining with (i) we deduce that for any realization of Ht, deﬁnition of Ht, it holds Qt 1Ht ≤ ε. This implies

t−1 s=1

ζs

≥

0.

On

the

other

hand,

by

t−1

√

ξs+1 1Ht = St 1Ht ≤ ε ≤ C/4,

s=2

t−1

χs+1 1Ht ≤ ε ≤ C/4.

s=2

(F.2) (F.3)

Finally as Ht ⊂ A1 we have D1 1Ht ≤ C/2. Therefore, for any realization of Ht, using (F.1) gives

Dt ≤ C/2 − 0 + C/4 + C/4 = C.

In the meantime (F.2) ensures as well χt 1Ht ≤ C/4 and we have thus proven Ht ⊂ At. Using Ht ⊂ Ht−1 ⊂ It−1, we conclude Ht ⊂ It.

(2) Recursive bound on E[Qt 1Ht−1 ]. Since Ht−1 ⊆ Ht−2, it holds Ht−1 = Ht−2 \ (Ht−2 \ Ht−1). We can therefore decompose

E[Qt 1Ht−1 ] = E[(Qt − Qt−1) 1Ht−1 ] + E[Qt−1 1Ht−1 ] = E[(ξt2 + 2ξtSt−1 + χt) 1Ht−1 ] + E[Qt−1 1Ht−2 ] − E[Qt−1 1Ht−2\Ht−1 ].

From the law of total expectation, Ht−1 ⊂ It−1 and (ii) we have

E[ξtSt−1 1Ht−1 ] = E[E[ξt | Ft−1]St−1 1Ht−1 ] = 0.

As ξt2 + χt is non-negative, using again Ht−1 ⊂ It−1, we get E[(ξt2 + χt) 1Ht−1 ] ≤ E[(ξt2 + χt) 1It−1 ].

By deﬁnition for any realization in Ht−2 \ Ht−1, it holds Qt−1 > ε and thus

E[Qt−1 1 ] Ht−2\Ht−1 ≥ ε E[1Ht−2\Ht−1 ] = ε P(Ht−2 \ Ht−1).

Combining the above we deduce the following recursive bound E[Qt 1Ht−1 ] ≤ E[Qt−1 1Ht−2 ] + E[(ξt2 + χt) 1It−1 ] − ε P(Ht−2 \ Ht−1).

(F.4)

(3) Conclude. Summing (F.4) from t = 3 to T we obtain

T

T

E[QT 1HT −1 ] ≤ E[Q2 1H1 ] + E[(ξt2 + χt) 1It−1 ] − ε P(Ht−2 \ Ht−1)

t=3

t=3

T
= E[(ξt2 + χt) 1It−1 ] − ε P(A1 \ HT −1),

t=2

(F.5)

where in the second line we use Q2 = ξ22 + χ2, H1 = I1 = A1 and H1 \ HT −1 = ˙ 3≤t≤T (Ht−2 \ Ht−1) with ˙ denoting the disjoint union (true since (Ht)t≥1 is a decreasing sequence of events). By repeating the same arguments that are used before and using the fact that QT is non-negative,

P(A1 \ HT ) = P(HT −1 \ HT ) + P(A1 \ HT −1) 1
≤ ε E[QT 1HT −1\HT ] + P(A1 \ HT −1) 1
≤ ε E[QT 1HT −1 ] + P(A1 \ HT −1).
(F.6), (F.5) along with (iii) lead to

(F.6)

1T

(A1 \ HT ) ≤

E[(ξt2 + χt) 1It−1 ] ≤ δ P(A1).

P

ε

t=2

23

Subsequently,

P(HT | A1) = 1 − P(A1 \ HT ) ≥ 1 − δ. P(A1)

With HT ⊂ IT this also gives P(IT | A1) ≥ 1 − δ. We notice that t≥1 It = is decreasing, by continuity from above we conclude





P  At | A1 = lim P(It | A1) ≥ 1 − δ.
t→∞ t≥1

t≥1 At. As (It)t≥1

To apply Lemma F.1, we establish another quasi-descent lemma which holds without taking expectation values.

Lemma F.2. For all x ∈ X , t ∈ N, the iterates generated by (DSEG) satisfy the following inequality

Xt+1 − x

2 ≤ Xt − x 2 − 2ηt V (Xt+ 1 ), Xt+ 1 − x

2

2

− 2γtηt V (Xt) ( V (Xt) − V (X˜t+ 1 ) − V (Xt) ) 2

− 2ηt Zt+ 1 , Xt − x − 2γtηt V (X˜t+ 1 ), Zt

2

2

+ 2γtηt Vˆt V (Xt+ 1 ) − V (X˜t+ 1 ) + ηt2 Vˆt+ 1 2

2

2

2

(F.7)

If we assume Assumption 1 for some solution x and that Xt, X˜t+ 1 , Xt+ 1 all lie in this neighbor-

2

2

hood, then

Xt+1 − x

2 ≤ Xt − x 2 − 2ηt V (Xt+ 1 ), Xt+ 1 − x − 2γtηt(1 − γtβ) V (Xt) 2

2

2

− 2ηt Zt+ 1 , Xt − x 2

− 2γtηt V (X˜t+ 1 ), Zt + 2γt2ηtβ Zt 2

Vˆt + ηt2 Vˆt+ 1 2. 2 (F.8)

Proof. Similar to (E.1), we develop

Xt+1 − x 2 = Xt − x 2 − 2ηt V (Xt+ 1 ), Xt − x − 2ηt Zt+ 1 , Xt − x + ηt2 Vˆt+ 1 2.

2

2

2

We further develop the second term on the RHS of the equality

V (Xt+ 1 ), Xt − x 2

= V (Xt+ 1 ), Xt+ 1 − x

2

2

= V (Xt+ 1 ), Xt+ 1 − x

2

2

+ γt V (Xt+ 1 ), Vˆt 2

+ γt V (Xt+ 1 ) − V (X˜t+ 1 ), Vˆt

2

2

+ γt V (X˜t+ 1 ), Vˆt . 2

To deal with the last term

V (X˜t+ 1 ), Vˆt = V (X˜t+ 1 ), V (Xt) + V (X˜t+ 1 ), Zt

2

2

2

= V (X˜t+ 1 ) − V (Xt), V (Xt) + V (Xt) 2 + V (X˜t+ 1 ), Zt .

2

2

By combing all the above, we readily get (F.7) with Cauchy’s inequality. If Assumption 1 holds on a

set that Xt, X˜t+ 1 , Xt+ 1 belong to, we can further bound

2

2

2γtηt V (Xt+ 1 ) − V (X˜t+ 1 ) Vˆt ≤ 2γt2ηtβ Zt Vˆt ,

2

2

2γtηt V (X˜t+ 1 ) − V (Xt) V (Xt) ≤ 2γt2ηtβ V (Xt) 2, 2

which gives (F.8).

F.3 A stability result
The following theorem characterizes the stability of the algorithm around a solution. The subsequent stepsize condition encompasses the stepsizes employed in Theorem 2 and Theorem 4 as special cases. We recall that X˜t+ 1 = Xt − γtV (Xt).
2

24

Theorem F.1. Let x be an isolated solution of (Opt) such that Assumptions 1 –3 are satisﬁed on Br(x ) for some q > 2, r > 0. We ﬁx a tolerance level δ ∈ (0, 1). For every ρ ∈ (0, 1), there is a neighborhood Uρ of x and a constant Γ > 0 such that if (DSEG) is initialized at X1 ∈ Uρ and is run with stepsizes satisfying t γtηt = ∞, t ηt2 < Γ, t γt2ηt < Γ and t γtq < Γ, then

E∞ ρ = {Xt+ 1 ∈ Br(x ), Xt, X˜t+ 1 ∈ Bρr(x ) for all t = 1, 2, . . . }

2

2

occurs with probability at least 1 − δ, i.e., P(E∞ ρ | X1 ∈ Uρ) ≥ 1 − δ.

Proof. We would like to apply Lemma F.1, but instead of indexing by t ∈ N, we index by s ∈ N/2. We invoke (F.7) from Lemma F.2 and set the random variables accordingly

Xt+1 − x
Dt+1

2 ≤ Xt − x 2 − 2ηt V (Xt+ 1 ), Xt+ 1 − x

2

2

Dt

ζ1

t+ 2

− 2γtηt V (Xt) ( V (Xt) − V (X˜t+ 1 ) − V (Xt) ) 2

ζt

+ (−2ηt Zt+ 1 , Xt − x ) + (−2γtηt V (X˜t+ 1 ), Zt )

2

2

ξt+1

ξt+ 1
2

+ 2γtηt Vˆt V (Xt+ 1 ) − V (X˜t+ 1 ) + ηt2 Vˆt+ 1 2

2

2

2

(F.9)

χt+1

We additionally deﬁne χt+ 1 := γtq Zt q and Dt+ 1 := Dt − ζt + χt+ 1 + ξt+ 1 so that (F.9) implies

2

2

2

2

Dt+1 ≤ Dt+ 1 − ζt+ 1 + χt+1 + ξt+1. With the deﬁnition of Dt+ 1 the inequality (F.1) is indeed

2

2

2

checked with all half integers. We should now verify that the assumptions (i), (ii) and (iii) in

Lemma F.1 are satisﬁed for a C that is properly chosen. Let M denote the supremum of V (x) for

x ∈ U where U = Br (x ) and r := ρr. We then choose C := min(r 2/9, 4(r /3)q). We also set

Γ small enough to guarantee γt ≤ min(r /(3M ), 1/β).

(a.0) Inclusion It ⊂ {Xt, X˜t+ 1 ∈ U } and It+ 1 ⊂ {Xt, X˜t+ 1 , Xt+ 1 ∈ U }.

2

2

2

2

any realization of It, we have Xt − x 2 ≤ C ≤ r 2/9. It follows

Since It ⊂ At, for

X˜t+ 12 − x 2 ≤ 2 Xt − x 2 + 2γt2 V (Xt) 2 ≤ 2r9 2 + 2γt2M 2 ≤ 4r9 2 .

We have shown It ⊂ {Xt, X˜t+ 1 ∈ U }. On the other hand, It+ 1 ⊂ At ∩ At+ 1 ⊂ {Dt ≤

2

2

2

C} ∩ {χt+ 1 ≤ C/4}. Therefore for any realization of It+ 1 ,

2

2

Subsequently,

γtq Zt q = χt+ 12 ≤ C4 ≤ (r /3)q.

2

2

2

2

2

2 r2 r2

r2

2

Xt+ 1 − x 2

≤ 3 Xt − x

+ 3γt V (Xt)

+ 3γt Zt

≤ + +3 33

3

≤r .

This proves It+ 1 ⊂ {Xt, X˜t+ 1 , Xt+ 1 ∈ U }.

2

2

2

(a.1) Assumption (i). We start by ζt+ 12 1It+ 12 ≥ 0. This is true because It+ 12 ⊂ {Xt+ 12 ∈ U }

and U ⊂ Br(x ), which allows us to apply Assumption 3 to obtain V (Xt+ 1 ), Xt+ 1 − x ≥ 0

2

2

whenever It+ 1 occurs. Similarly, by It ⊂ {Xt, X˜t+ 1 ∈ U } and Assumption 1 we then have

2

2

ζt 1It ≥ 2γtηt(1 − γtβ) V (Xt) 2 ≥ 0.

(a.2) Assumption (ii). Immediate from (5a), (a.0) and the law of the total expectation.

25

(a.3) Assumption (iii). By using that It ⊂ {X˜t+ 1 ∈ U } and It ⊂ {Xt ∈ Br(x )} , we get 2

E[

ξ

2 t+

1

1It ]

≤

4γt2ηt2

E[

V (X˜t+ 1 )

2 1It

Zt

2 1It ]

2

2

≤ 4γt2ηt2M 2 E[ Zt 2 1{Xt∈Br(x )}] ≤ 4γt2ηt2M 2σ2.

For the last inequality we use (5b) and Jensen’s inequality to bound E[ Zt 2 1{Xt∈Br(x )}]. Similarly,

E[ Zt 1{Xt∈Br(x )}] ≤ σ,

E[ Zt+ 1 2 1{X 1 ∈Br(x )}] ≤ σ2.

2

t+ 2

Using It+ 1 ⊂ {Xt, X˜t+ 1 , Xt+ 1 ∈ U } and Assumption 1 then gives

2

2

2

E[χt+1 1I 1 ] ≤ 2γt2ηtβ E[ Zt ( V (Xt) + Zt ) 1I 1 ]

t+ 2

t+ 2

+ ηt2 E[( V (Xt+ 1 ) 2 + Zt+ 1 2) 1I 1 ]

2

2

t+ 2

≤ 2γt2ηtβ(E[ Zt 2 1{Xt∈Br(x )}] + E[ Zt 1{Xt∈Br(x )} V (Xt) 1{Xt∈U }])

+ ηt2(E[ V (Xt+ 1 ) 2 1{X 1 ∈U }] + E[ Zt+ 1 2 1{X 1 ∈Br(x )}])

2

t+ 2

2

t+ 2

≤ 2γt2ηtβ(M σ + σ2) + ηt2(M 2 + σ2).

(F.10)

By similar arguments and in particular by invoking It+ 1 ⊂ {Dt ≤ C} and the deﬁnition of C, it 2
follows

E[ξ2 1I ] ≤ 4 η2r 2σ2,

t+1 t+ 21

9t

Combining the above with E[χt+ 1 1It ] ≤ γtqσq, we have 2

(ξs2+ 1 + χs+ 1 ) 1Is

2

2

s∈1,3/2,...

∞
≤
t=1

γtqσq + 2γt2ηtβ(M σ + σ2) + 4γt2ηt2M 2σ2 + ηt2(M 2 + σ2 + 94 r 2σ2)

≤

σq

+

2β(M σ

+

σ2)

+

4 M2σ2

+

M2

+

σ2

+

4 r

2σ2

Γ.

β

9

We can thus pick Γ small enough to make (iii) veriﬁed. (a.4) Conclude. We set Uρ = B√C/2(x ) so that A1 = {X1 ∈ Uρ}. By invoking Lemma F.1 we

get P t≥1 At | A1 ≥ 1 − δ. Additionally, (a.0) along with U ⊂ Br(x ) imply t≥1 At ⊂ E∞ ρ , concluding the proof.

F.4 Proof of Theorem 2
Theorem 2. Fix a tolerance level δ > 0 and suppose that Assumptions 1 –3 hold for some isolated solution x of (Opt). Assume further that (DSEG) is run with stepsize parameters of the form (4) with small enough γ, η and proper choice of rγ, rη (cf. Fig. 2). If the algorithm is not initialized too far from x , its iterates converge to x with probability at least 1 − δ.

Proof. Let r > 0, ρ ∈ (0, 1). By Theorem F.1, we know that if (DSEG) is run as stated in Theorem 2 with rγ + rη ≤ 1, 2rη > 1, 2rγ + rη > 1, rγq > 1 and small enough γ, η, the event E∞ ρ occurs with probability 1 − δ. With this at hand we are ready to prove the large probability convergence result.
For t ∈ N, let us deﬁne the following events

Et := {Xs, X˜s+ 1 ∈ Bρr(x ) for all s = 1, 2, . . . , t} 2

Et+ 1 := Et ∩ {Xs+ 1 ∈ Br(x ) for all s = 1, 2, . . . , t}.

2

2

We notice that E∞ ρ = t≥1 Et+ 1 . We would like to establish a recursive inequality in the form 2
of (D.1) by taking Ut = Xt − x 1Et− 12 . The main difﬁculty consists in controlling the term

26

Et[ V (X˜t+ 1 ), Zt 1E 1 ], which is generally non-zero as 1E 1 is not Ft-measurable. To achieve

2

t+ 2

t+ 2

this, we rely on the following key observation.

Et[Zt 1Et ] = Et[Zt 1Et+ 12 ] + Et[Zt 1Et\Et+ 12 ].

As 1Et is Ft-measurable and Et ⊂ {Xt ∈ Br(x )}, Et[Zt 1Et ] is indeed zero and this implies

Et[Zt 1Et+ 12 ] = Et[Zt 1Et\Et+ 12 ] .

(F.11)

The problem then reduces to ﬁnding an upper bound of Et[Zt 1Et\Et+ 12 ] . By deﬁnition, for any

realization of Et \ Et+ 1 , X˜t+ 1 ∈ Bρr(x ) and Xt+ 1 ∈/ Br(x ). Since Xt+ 1 = X˜t+ 1 − γtZt, we

2

2

2

2

2

deduce

Et \ Et+ 1 ⊂ { γtZt ≥ (1 − ρ)r}. 2

Therefore, using Et ⊂ {Xt ∈ Br(x )} along with the Chebyshev’s inequality yields

P(Et \ Et+ 1 | Ft) ≤ P 2

(1 − ρ)r Zt 1{Xt∈Br(x )} ≥ γt | Ft

≤ σ2γt2 . (1 − ρ)2r2

Applying the Cauchy–Schwarz inequality leads to

Et[Zt 1Et\Et+ 12 ] ≤

Et[ Zt 1Et 2]

Et[12Et\Et+ 1 ] ≤ (1σ−2γρt)r . 2

(F.12)

Then, by using (F.11), (F.12) and Et+ 1 ⊂ Et, 2

Et[ V (X˜t+ 1 ), Zt 2

1E 1 ] = Et[ V (X˜t+ 1 ) 1Et , Zt 1E 1 ]

t+ 2

2

t+ 2

= V (X˜t+ 1 ) 1Et , Et[Zt 1E 1 ]

2

t+ 2

≤ V (X˜t+ 1 ) 1Et 2

Et[Zt 1Et+ 21 ]

≤ M σ2γt , (1 − ρ)r

(F.13)

where M := supx∈Br(x ) V (x) . We can now derive a recursive bound on E[ Xt+1 − x 1Et+ 12 ] by invoking Lemma F.2. The inequality (F.8) multiplied by 1Et+ 12 holds true by deﬁnition of Et+ 21 and Assumption 1 . The desired inequality can then be obtained by taking expectation conditioned
on Ft. On the one hand, we use

V (Xt+ 21 ), Xt+ 12 − x 1Et+ 21 ≥ 0 Et[ Zt+ 12 , Xt − x 1Et+ 12 ] = Et[ Et+ 12 [Zt+ 12 ] 1Et+ 12 , Xt − x

] = 0.

On the other hand, the last two terms of (F.8) can be bounded similarly as in (F.10) and the antepenultimate term can now be bounded thanks to (F.13). We then obtain

Et[ Xt+1 − x

2 1E 1 ] ≤ Et[ Xt − x 2 1E 1 ] − 0 − 2γtηt(1 − γtβ) Et[ V (Xt) 2 1E 1 ]

t+ 2

t+ 2

t+ 2

2 Mσ2

22

2

2

2

− 0 + 2γt ηt (1 − ρ)r + ηt (M + σ ) + 2γt ηtβ(M σ + σ ). (F.14)

Without loss of generality we may suppose γtβ ≤ 1/2. To simplify the notation, we set

ζt = min

Xt − x

2, γtηt V (Xt) 2 , M1 = 2 M σ2 + 2β(M σ + σ2), M2 = M 2 + σ2. (1 − ρ)r

It follows from (F.14)

Et[ Xt+1 − x 2 1E 1 ] ≤ Et[( Xt − x 2 − ζt) 1E 1 ] + γt2ηtM1 + ηt2M2.

t+ 2

t+ 2

As Xt − x 2 − ζt ≥ 0 and Et+ 1 ⊂ Et− 1 , this implies

2

2

Et[ Xt+1 − x 2 1E 1 ] ≤ Xt − x 2 1E 1 −ζt 1E 1 +γt2ηtM1 + ηt2M2.

t+ 2

t− 2

t− 2

27

Invoking the Robbins–Siegmund theorem (Lemma D.4) gives the almost sure convergence of t ζt 1Et− 12 and Xt − x 2 1Et− 21 . We use P(E∞ ρ ) > 1 − δ and deduce that





∞

P E∞ ρ ∩

ζt 1Et− 12 < ∞ ∩



t=1



Xt − x 2 1E 1 converges  ≥ 1 − δ.

t− 2





E

Since E∞ ρ = t≥1 Et+ 1 , for any realization of the above event it holds t ζt < ∞ and Xt − x 2 2
converges. We assume by contradiction that Xt − x 2 converges to some constant ν > 0. From the summability of (ζt)t∈N we know that ζt → 0 and therefore for all t large enough we have in fact ζt = γtηt V (Xt) 2. It follows that t γtηt V (Xt) 2 < ∞. Repeating the arguments of Appendix E.3 (Proof of Theorem 1) we then show that Xt − x → 0, which is a contradiction (we take r small enough so that x is the only solution of (Opt) in Br(x )). We have therefore proved that Xt − x → 0 for any realization of E. In conclusion, Xt converges to x with probability at least 1 − δ.

F.5 Proof of Proposition 2
Proposition 2. If a solution x satisﬁes Assumption 5 , then for every ε > 0, there is a neighborhood U of x such that the error bound condition (EB) is satisﬁed on U with constant τ = σmin − ε where σmin denotes the smallest singular value of JacV (x ).

Proof. By deﬁnition of Jacobian we have V (x) = V (x ) + JacV (x )(x − x ) + o( x − x ).
By the min-max principle of singular value it holds JacV (x )(x − x ) ≥ σmin x − x .
Since V (x ) = 0, combining (F.15) and (F.16) gives V (x) ≥ σmin x − x − o( x − x ).
We conclude by noticing dist(x, X ) = x − x when U is small enough.

(F.15) (F.16)

F.6 Proof of Theorem 4
Theorem 4. Fix a tolerance level δ > 0 and suppose that Assumptions 1 –3 and 5 hold for some isolated solution x of (Opt) with q > 3. Assume further x satisﬁes Assumption 5 and (DSEG) is run with stepsize parameters of the form γt = γ/(t + b)1/3 and ηt = η/(t + b)2/3 with large enough b, η > 0. Then, there exist neighborhoods U , U of x and an event EU such that:
a) P(EU | X1 ∈ U ) ≥ 1 − δ. b) P(Xt ∈ U for all t | EU ) = 1. c) E[ Xt − x 2 | EU ] = O 1/t1/3
In words, if (DSEG) is not initialized too far from x , the iterates Xt remain close to x with probability at least 1 − δ and, conditioned on this event, Xt converges to x at a rate O(1/t1/3) in mean square error.

Proof. Both a) and b) are direct consequences of Theorem F.1. In effect, since q > 3, the sum of the series t ηt2, t γt2ηt and t γtq can be made arbitrarily small by taking sufﬁciently large b. Moreover, x is an isolated solution because JacV (x ) is non-singular. Therefore, taking EU := E∞ ρ , U := U ρ and U := Bρr(x ) readily gives a) and b). Finally, to guarantee c), we need to have ρ small enough and enforce γησm2 in(1 − γ1β) > 1/6. In fact, from γησm2 in(1 − γ1β) > 1/6 we deduce the existence of ε ∈ (0, σmin) such that γη(σmin − ε)2(1 − γ1β) > 1/6. Since JacV (x ) is non-singular, by Proposition 2 we can choose ρ > 0 so that

28

the error bound condition (EB) is satisﬁed on Bρr(x ) with τ = σmin − ε. Let M1, M2 be deﬁned as in Appendix F.4. We then obtained from (F.14)

E[ Xt+1 − x 2 1E 1 ] ≤ (1 − 2γtηtτ 2(1 − γtβ)) E[ Xt − x 2 1E 1 ] + γt2ηtM1 + ηt2M2.

t+ 2

t+ 2

By using Et+ 1 ⊂ Et− 1 , we get

2

2

E[ Xt+1 − x 2 1E 1 ] ≤ (1 − 2γtηtτ 2(1 − γtβ)) E[ Xt − x 2 1E 1 ] + γt2ηtM1 + ηt2M2

t+ 2

t− 2

Therefore, with the speciﬁed stepsize policy and the condition γητ 2(1 − γ1β) > 1/6, applying Lemma D.2 yields E[ Xt+1 − x 2 1E 1 ] = O(1/t1/3). Finally
t+ 2

E[ Xt − x

2ρ

E[ Xt − x 2 1Eρ ]

E[ Xt − x 21E 1 ] t−

| E∞] =

P(E∞ ρ ) ∞ ≤

2, 1−δ

which proves E[ Xt − x 2 | E∞ ρ ] = O(1/t1/3).

29

