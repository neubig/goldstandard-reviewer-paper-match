Auctions and Prediction Markets for Scientiﬁc Peer Review

arXiv:2109.00923v1 [econ.GN] 27 Aug 2021

Siddarth Srinivasan School of Computer Science and Engineering
University of Washington Seattle, WA 98195
sidsrini@cs.washington.edu

Jamie Morgenstern School of Computer Science and Engineering
University of Washington Seattle, WA 98195
jamiemmt@cs.washington.edu

Abstract
Peer reviewed publications are considered the gold standard in certifying and disseminating ideas that a research community considers valuable. However, we identify two major drawbacks of the current system: (1) the overwhelming demand for reviewers due to a large volume of submissions, and (2) the lack of incentives for reviewers to participate and expend the necessary effort to provide high-quality reviews. In this work, we adopt a mechanism-design approach to propose improvements to the peer review process. We present a two-stage mechanism which ties together the paper submission and review process, simultaneously incentivizing high-quality reviews and high-quality submissions. In the ﬁrst stage, authors participate in a VCG auction for review slots by submitting their papers along with a bid that represents their expected value for having their paper reviewed. For the second stage, we propose a novel prediction market-style mechanism (H-DIPP) building on recent work in the information elicitation literature, which incentivizes participating reviewers to provide honest and effortful reviews. The revenue raised by the Stage I auction is used in Stage II to pay reviewers based on the quality of their reviews.
1 Introduction
Scientiﬁc publishing through peer review is widely regarded as the gold standard for disseminating scientiﬁc research. Yet, despite its widespread use and acceptance, it is plagued by a number of well-documented problems: willing reviewers are often in short supply and reviews themselves are often of poor quality, giving rise to noisy acceptance decisions (Lawrence and Cortes, 2015; Shah et al., 2018). Studying the history of peer review in sociology, Merriman (2020) writes that while peer review is ‘a practice intended to assess the merits of pieces of scholarship, its features did not arise because they were believed to be an especially apt means of identifying the best work.’ The issues with peer review have become especially salient in machine learning as well as computing more broadly. Computing relies largely on a conference model for publishing and disseminating research, and while the number of submissions to major conferences has exploded, the pool of qualiﬁed reviewers has grown more slowly (Sculley et al., 2018b; Shah, 2019; Stelmakh et al., 2020). As these issues are only likely to persist, a solution addressing the problem of low-quality reviews is needed.
In this work, we adopt a mechanism-design approach to tackle two drawbacks of the current system of conference-based publishing in machine learning: (1) a large number of submissions, leading to poor and often arbitrary criteria for desk rejection, and (2) poor incentives for reviewers that make it challenging to ﬁnd qualiﬁed reviewers willing to provide effortful and honest reviews. Our proposed mechanism tackles (1) by requiring authors to participate in an auction for review slots, and tackles (2) via a prediction market-style mechanism (involving predicting peers’ review scores) that compensates reviewers for honest and effortful reviews. The mechanism involves minimal modiﬁcation to the current peer review system; it requires authors to submit a bid accompanying every
Preprint. Under review.

Figure 1: Diagram of proposed mechanism for scientiﬁc peer review
paper, and reviewers to participate in a simple prediction game after submitting their reviews. The paper-reviewer matching process and accept/reject decision process may be independently improved or remain unchanged. Our mechanism would adopt a novel currency usable within a consortium of conferences, with the goal of incentivizing authors to provide the much needed service of peer review in order to earn credits exchangeable for having their own papers reviewed. While our proposal is generally applicable to peer review, our perspective is informed by the conference publication model in machine learning. There remain many questions to be answered before our proposed mechanism can be deployed in practice, and we intend for this work to serve as a plausible (but by no means comprehensive) approach to be iterated upon in designing a robust scientiﬁc publication process.
Our paper is structured as follows: in section 2, we provide some background on scientiﬁc peer review and motivate the problem; in section 3, we present related work; in section 4, we describe our proposed mechanism; in section 5, we discuss some important considerations for a practical implementation of the mechanism, and in Section 6, we conclude with ideas for future work.
Main Contributions We make two main contributions: (1) we propose a two-stage peer review mechanism consisting of a submission stage and a reviewing stage, where the revenue raised by the former is used to pay for the latter, and (2) we design the reviewing stage by building on previous work in the information elicitation literature to develop a novel prediction market-style mechanism that incentivizes reviewers to provide honest and effortful reviews.
2 Background
Although there are some minor variations, the process of peer review can be summarized as follows: (1) authors submit their work to a conference, (2) some papers receive desk rejection, (3) a paper that is not desk rejected is matched with a small number (3-5) of willing reviewers (who do not know the identity of the authors and vice-versa), (4) the reviewers independently score the paper’s quality for publication based on several criteria such as technical correctness, interest to the community, and quality of exposition (the reviewers may or may not know each others’ identities), (5) a meta-reviewer aggregates the reviewers’ scores and makes an accept/reject decision, incorporating the conference’s targeted acceptance rate into their decision. With this context, we now discuss the often cited primary motivations for and criticisms of peer review below. Merriman (2020) argues that these ‘intellectual’ motivations tend to be post-hoc rationalizations, and the current process of peer review is ‘best understood as the product of continuous efforts to steward editors’ scarce attention while preserving an open submission policy that favors authors’ interests’. Part of our goal then is to develop a peer review mechanism that lives up to the stated ‘intellectual’ motivations for the process.
2.1 Motivations for Peer Review
M1 Screening Results for Validity: Peer review serves as a community-issued certiﬁcate that the approach and results in a publication seem plausible, replicable, and not fraudulent.
2

M2 Screening Results for Importance to the Community: The publication record serves as a central repository of curated results that the ﬁeld considers important or impactful and wishes to focus on; the conference itself serves to build consensus and host debates on the cutting-edge issues in the ﬁeld.
M3 Reputation Building/Signaling: Publication at selective venues also serves to build reputation and advance careers of authors; the idea is that a selective acceptance process that identiﬁes valid and interesting results also identiﬁes individuals who do good work.
M4 Feedback and Networking: Peer review serves as a systematic approach to obtaining feedback on both ongoing and completed work, as well as ﬁnding collaborators interested in tackling similar problems.
M1 and M2 are often cited as the primary motivations for peer review, and are arguably the most foundational. While M3 is not often cited explicitly as a motivation, it is well-known that publishing frequently at top-tier venues improves career advancement prospects; peer review clearly serves as a mechanism to identify individuals who should be rewarded. M3 derives its legitimacy from M1 and M2. Meanwhile, M4 is an auxiliary beneﬁt derived from M1 and M2. We argue that the peer review system should be judged on its ability to satisfy M1 and M2 while cognizant of its social functions M3 and M4 so as to avoid unintended consequences.
2.2 Criticisms of Peer Review
The general criticism of peer review is that the reviews are often sub-par and the accept/reject decisions are too noisy. Various NeurIPS experiments (Lawrence and Cortes, 2015; Shah et al., 2018; NeurIPS, 2019; Lawrence, 2021) provide some empirical basis for this criticism; the 2014 NeurIPS experiment in particular (see Appendix B) found that if the conference was re-run, about a quarter of papers would have their accept/reject decisions swapped, and only about half of accepted papers would be re-accepted. There is also a perception that reviewers are often misled by unsubstantiated claims (Lipton and Steinhardt, 2018) or work that looks important but isn’t (Vazire, 2017). This may incentivize the submission of premature, lower quality work which if rejected, may be resubmitted at no cost to future conferences with minor tweaks with the goal of getting through a noisy review process (Bengio, 2020). In this context, we discuss some of the explanations for why the current system of peer review within machine learning may be sub-par, although our discussion may apply to peer review in general.
C1 The Extensive Margin of Reviewing: There are too many submissions and not enough reviewers, so the current system struggles to match submissions with qualiﬁed and interested reviewers. There are no explicit incentives for reviewers, so reviewers participate entirely for pro-social reasons. The shortage of reviewers has been exacerbated by the recent explosion in submissions to top conference venues in machine learning over the past decade. The large number of submissions can also lead to desk-rejection policies that can be somewhat opaque and arbitrary. Poor matching due to a scarcity of reviewers could explain why reviews are often sub-par, so a mechanism that incentivizes reviewers to participate could be valuable.
C2 The Intensive Margin of Reviewing: Even when reviewers participate voluntarily, they may not be willing to exert enough effort to provide a high-quality assessment of the paper, and there is currently no explicit incentive to do so. In an attempt to redress the shortage of reviewers, some conferences require authors to also participate in review. While this would increase the supply of reviewers, it could potentially crowd-out pro-social motivations and worsen the average quality of reviews without proper incentives. Thus, we may need to provide the appropriate incentives to elicit effortful reviews, especially when requiring participation in the review process.
C3 Dishonest Reviews: Some have noted that even when reviewers exert enough effort to understand a paper, they may not report their honest opinion for various reasons. Thus, it is desirable to design the review process to remove obstacles to truthful reviewing.
C4 Ability of Reviewers: Another strain of criticism is that peer review is fundamentally ﬂawed and it cannot work even when reviewers provide honest and effortful reviews on well-matched papers. There are two versions of this criticism: the ﬁrst is that reviewing is too subjective for reviewers to agree on results’ validity and importance to the community, and the second is that even when reviewers agree, their reviews do not track any meaningful measure of importance to the community. Empirical evidence from the 2014 NeurIPS experiment (where all the
3

aforementioned incentive issues persist) suggests that reviewers’ scores correlate moderately well with each other as well as with log-citation counts; it may be quite possible to improve upon these results by remedying incentive issues.
Our focus in this paper is to address criticisms C1-C3 head on with a mechanism-design approach. The key challenge we face is the lack of a ground-truth veriﬁcation for the submitted reviews, which makes it difﬁcult to verify whether reviewers’ assessments are truthful and effortful. We tackle this issue by building on recently proposed mechanisms from the information elicitation without veriﬁcation (IEWV) literature. On the paper submission side of the peer review process, our proposed mechanism disincentivizes lower-quality submissions aimed at getting through a noisy review process by only reviewing as many papers as can receive high-quality reviews. On the paper reviewing side, our proposed mechanism incentivizes participating reviewers to exert effort to obtain a clearer signal of the quality of a paper and report that information truthfully.
3 Related Work
We discuss three bodies of related work: 1) proposals to improve scientiﬁc peer review from the machine learning community, 2) general proposals to reform or replace scientiﬁc peer review, and 3) the information elicitation without veriﬁcation literature (Faltings and Radanovic, 2017; Waggoner and Chen, 2013). Readers may also be interested in a broader historical overview of the development of peer review (focused primarily on the social sciences) presented by Merriman (2020).
3.1 Proposals from the Machine Learning Community
A number of proposals to handle the rapid rise in submissions and reviewing load have been put forward by the machine learning community. One idea is to allow some editorial screening, where Area Chairs can reject submissions without full reviews; this would eliminate the need for ∼ 4% of reviews but some papers may still be screened out unfairly. Another proposal is to introduce submission caps, which limit the number of submissions by a single author, but a cap of 10 submissions would again only eliminate ∼ 4% of the reviewing load. An analysis of NeurIPS 2019 data (NeurIPS, 2019) concluded that it was unclear how best to rapidly ﬁlter papers prior to full review; our proposed submission mechanism uses a VCG auction to determine which papers are accepted for review, where authors signal how ready their work is and the mechanism accepts as many papers for review as it can ﬁnd quality reviewers for.
Criticizing the slow pace of peer review, LeCun (2012) and Zhao (2012) propose an ‘open-market’, where all papers are uploaded to an online repository as soon as they are ready for viewing. ‘Reviewing entities’ may then identify papers they wish to review, although authors may request reviews as well. However, data from past NeurIPS conferences (Shah et al., 2018; NeurIPS, 2019) suggest that under the current implementation, reviewers do not make many eager bids to review papers, and eager bids are not predictive of eventual acceptance. Thus, mechanisms that rely on reviewers to identify and review papers may need to provide explicit incentives. Identifying the tension between rapid dissemination of ideas and the need to carefully screen results, Bengio (2020) proposes a hybrid arXiv-journal model, where arXiv addresses the former need and journals take on the role of careful screening of results by promoting a slower, iterative process that promotes higher-quality publications. In these aforementioned approaches, reviewing incentive issues would still persist (as noted in ﬁelds with journal publication models); indeed, the reviewing burden may move from conferences to the open-market system or journals, as authors from less well-known backgrounds seeking community-issued certiﬁcates of the quality of their work for reputation-building purposes begin to substitute conference submissions for journal submissions. Our submission mechanism disincentivizes weak submissions that add to the reviewing load while being unlikely to be accepted, and our reviewing mechanism provides direct incentives for honest and effortful reviews.
Other proposals involve improvements to various procedural aspects of peer review; these include a recruitment and mentorship pipeline for junior academics to alleviate the scarcity of reviewers (Stelmakh et al., 2020), optimizing reviewer assignment to obtain less noisy reviews (Shah, 2019) and mitigate conﬂicts of interest (Xu et al., 2018; Jecmen et al., 2020), and better aggregation of subjective opinions on criteria scores to accept/reject recommendations (Noothigattu et al., 2021). Such ideas are complementary to our Stage II reviewing mechanism; they focus on improving paper-reviewer matching or the acceptance decision process, while we focus on incentivizing honest and effortful reviews after reviewers are assigned and before scores are aggregated into accept/reject decisions.
4

3.2 General Proposals to Reform or Replace Peer Review
One strain of research proposes modest changes to the system, such as allowing reviewers to re-purpose reviews from past submissions (Alberts et al., 2008) to reduce the demand for reviews, or improving the publication culture so authors submit more complete and relevant work to the appropriate venues instead of simply targeting the most prestigious venues. Others (Arns, 2014) propose reducing the reviewing burden by allowing some papers (e.g. null results) to be accepted without peer review. However, these proposals do not systematically tackle the previously discussed core incentive issues.
Fox and Petchey (2010) propose an approach similar to ours, where authors must pay in an artiﬁcial currency to have their paper reviewed, and reviewers are paid for their completed reviews. However, reviewers receive ﬂat payment for their participation, and so may not provide honest or effortful reviews. Sculley et al. (2018a) propose pecuniary and non-pecuniary compensation for reviewers based on review quality (as judged by a rubric), but rating reviews would increase the workload on area chairs.
In a different approach, de la Rosa and Szymanski (2007b,a) propose entirely replacing the existing peer review system with market-based citation auctions, i.e., where earned citations are treated as currency, every submission comes with a “citation bid", and the papers with the highest bids are accepted for publication. The citations that the paper goes on to earn are used to recoup the bet, with the idea being that the bid will reveal how impactful (as measured by number of citations) a researcher expects their work to be and the mechanism will publish papers expected to be highly cited. They attempt to use citation count to avoid the fundamental problem of verifying review quality, and citations certainly provide some crowd-sourced opinion on the quality of a paper. However, even as it ﬁxes the shortage of reviewers by getting rid of the need for them in the ﬁrst place, predicting citations is likely extremely high variance, can lead to distorted citation practices, and poses equity issues if highly-cited authors can publish any paper they want by simply outbidding everyone else regardless of the quality of their work.
Prüfer and Zetland (2010) also propose a citation-based paper submission system for journals, although their proposal does not abolish scientiﬁc peer review. Instead, they propose a new currency of ‘academic dollars’ used by editors of various journals to bid for submitted papers in an auction; if accepted for review, the dollar value is paid to the authors, referees, and editors of cited works. This is essentially a citation betting market for journal editors; a paper’s bid will be commensurate with its future expected citation count. Referees will exert effort to provide better feedback to authors to improve the quality of their work in order to earn dollars from future citations, and authors have a marginally higher incentive to write better to earn citations. While an intriguing proposal, we suspect the task of predicting citations is challenging and the singular focus on optimizing for citations may distort citation practices.
Frijters and Torgler (2019) also propose a modiﬁed peer review incorporating market-based ideas, with a focus on mitigating ‘gate-keeping’ by insiders and improving transparency. Their proposal includes allowing authors to bid for reviews of varying quality, and opening up the reviews themselves to be rated thereby allowing for ‘professionalization’ of the process. However, the system is somewhat complicated and the market for rating reviews may not be very liquid, especially in technical ﬁelds. Xiao et al. (2018) propose a peer review mechanism to elicit effortful reviews by assigning papers to reviewers based on the quality of the authors’ past reviews, but it computes this review quality by asking authors to rate the reviews they received and assuming these ratings to be honest. This is likely too strong an assumption in our setting where authors may simply report how favourable a review was; this would incentivize ﬂattering and likely dishonest reviews, undermining the mechanism.
3.3 Information Elicitation without Veriﬁcation
The IEWV literature is focused on the problem of eliciting truthful private ‘signals’ when the mechanism designer is unable to verify the responses; this may be because the ground truth is difﬁcult or impossible for the mechanism to access or there is simply no ground truth since the responses are subjective. The key application domains of interest include tasks like crowdsourcing data, product ratings, community sensing, and peer grading (Cai et al., 2015; Radanovic et al., 2016; Dasgupta and Ghosh, 2013; Kong and Schoenebeck, 2018). More recent work is also interested in ensuring that private signals are obtained with effort. The typical setting in this literature consists of risk-neutral, utility-maximizing agents who maintain some prior distribution over ‘signals’ they can receive from a mechanism. When the agents participate in the mechanism, the world reveals to them their private information or ‘signal’, and they subsequently form posterior beliefs over the distribution of signals observed by the other agents. In our setting, this would be analogous to reviewers having some prior
5

belief over the quality of papers they may see, and when presented with a speciﬁc paper, the reviewer forms their opinion of that particular paper and develops a posterior belief over other reviewers’ assessments of the paper. The task of the mechanism designer is to elicit agents’ private signals when veriﬁcation is not possible by providing incentives for truthful reports.
Two of the foundational approaches to this problem are the peer prediction framework (Miller et al., 2005) and the Bayesian Truth Serum (BTS) (Prelec, 2004). The fundamental idea is to design payments by reasoning about agents’ reports and posteriors over other agents’ reports in a way that makes truth-telling a Bayesian Nash equilibrium. However, these mechanisms make strong assumptions that are unlikely to hold in practice; subsequent works have relaxed these assumptions (Witkowski and Parkes, 2012a, 2013, 2012b; Radanovic and Faltings, 2013, 2014). There are also output agreement mechanisms (Waggoner and Chen, 2014) with weaker assumptions, but these eschew truthfulness and seek to elicit common knowledge. A directly relevant proposal to our work is by Carvalho et al. (2013), which proposes a peer prediction-like mechanism for the same task of scientiﬁc peer review. They assume that reviewers’ signals on a given paper are drawn from a multinomial distribution, and model reviewers’ prior over the multinomials with the conjugate (Dirichlet) prior so inference is tractable, but this a somewhat restrictive assumption. They further test their mechanism on an experiment with Amazon Mechanical Turk workers, and ﬁnd that their mechanism does incentivize more accurate reviews and such reviews also produce higher payments for the participating agents. The truthful mechanism applicable in the most general setting is the recently proposed prediction market-style Differential Peer Prediction (DPP) mechanism (Schoenebeck and Yu, 2020). The truth-telling equilibrium maximizes agent welfare, and the mechanism is applicable to a small group of heterogeneous agents. Our proposed reviewing mechanism is a direct extension of this work.
These aforementioned works do not incorporate any model of agent effort, and instead assume that nature or some black box simply reveals the private signal to the agents. The early literature incorporating effort models (Dasgupta and Ghosh, 2013; Witkowski et al., 2014; Radanovic et al., 2016; Liu and Chen, 2016) is limited to the case of binary effort: either agents exert low/no effort, or they exert high/full effort. A binary effort model may be appropriate for quick tasks like those on crowdsourcing platforms, but reviewing technical papers is a time-intensive process, so a more sophisticated effort model is needed. Cai et al. (2015) propose a continuous effort model for data elicitation, where exerting effort gives agents a lower variance signal. However, in our setting, lower effort reviews may not only be noisier, they may also be systematically biased (Kong and Schoenebeck, 2018). Gao et al. (2016) caution that in practice (and as highly relevant in our case), agents have a variety of low-effort signals such as paper length, topic, etc., potentially leading to low-effort coordination into uninformative equilibria. Indeed, Gao et al. (2014) present empirical information showing that agents can successfully coordinate into uninformative equilibria, especially if these equilibria pay more than truthful equilibria.
With these considerations, Kong and Schoenebeck (2018) propose a Hierarchical Mutual Information Paradigm (HMIP) that constructs a hierarchy of criteria that capture a range of cheap to expensive signals, and pays agents the information gained from more expensive signals. This work builds on the Mutual Information Paradigm (Kong and Schoenebeck, 2019) which shows that mechanisms incentivize truth-telling when the payment is based on the mutual information between agents’ responses, and this is what peer prediction and BTS are attempting to do. We integrate the very general differential peer prediction (DPP) mechanism into the hierarchical mutual information paradigm (HMIP) to develop a strictly truthful prediction market-style mechanism that incentivizes effortful reviews, with the desired equilibria paying more than uninformative equilibria.
4 A Mechanism for Scientiﬁc Peer Review
We now propose a two-stage mechanism for scientiﬁc peer review that incentivizes authors to submit papers they believe are likely to be accepted, and reviewers to participate, exert effort, and honestly review papers. Stage I deals with the the paper submission process, where authors bid for review slots, and Stage II deals with the paper review process, where reviewers are paid based on the quality of their reviews. The paper-reviewer matching process and accept/reject decision process may remain unchanged. We envision the mechanism using its own currency (henceforth referred to simply as ‘credits’) that can be utilized within a consortium of conferences adopting the mechanism for peer review.
Summary In the Stage I submission mechanism, authors participate in a VCG auction for review slots, where they submit their papers ωi along with an accompanying bid bi (in mechanism-speciﬁc
6

credits). The winners of the auction pay the highest losing bid and have their papers accepted for review. In the Stage II reviewing mechanism (which we call H-DIPP), every paper ωi accepted for review is assigned 3 reviewers (denoted A, B, and C). Reviewers proceed per the usual peer review process, supplying reviews and scoring the paper on a set of T criteria (e.g. novelty, correctness, interest to the community, quality of writing). The reviewers will then also participate in a simple prediction market-like game to report predictions (as probability distributions) of the scores of one other peer, twice for each criterion; reviewers ﬁrst predict the peer’s review score on a given criterion, are shown the third peer’s score on that criterion, and are asked to update their prediction. This proceeds sequentially for each criterion in a pre-determined order. Reviewers’ predictions of a peer’s review scores are rewarded based on their accuracy (with the log proper scoring rule), and reviewers’ review scores on each criterion are rewarded based on how much better their peer’s second prediction of that review score was compared to their ﬁrst. We show that reviewers maximize their expected utility under such a payment scheme when they provide honest and effortful reviews. We emphasize that the prediction game involves predicting other reviewers’ review scores, and not something like the paper’s expected impact. The revenue raised from Stage I of the auction can be used to pay reviewers for their reviews in Stage II, and the mechanism will seek to accept as many papers for review as it can ﬁnd well-matched reviewers for within this budget constraint. Area chairs then determine which papers are accepted for publication per the usual process. Authors who submitted bids must ensure that they have (or can earn) enough credits to cover their bids from Stage I, or risk having their reviews and potential acceptance withheld. In general, authors face the budget constraint of submitting bids no larger than their store of credits and credits they are willing and able to earn through reviewing.
Remark 1. We note that there are two simple ways to incentivize authors to earn sufﬁcient credits in Stage II to cover their bid from Stage I. The ﬁrst is to refuse to accept papers without sufﬁcient reviewing credit; the second is to model the conference submission process as a repeated game where an author can either save credits or be indebted credits from round to round.

4.1 Setup
Notation Suppose we have N papers submitted to the conference peer review mechanism denoted ωi (for i ∈ [N ] ) with K participating agents. We use the index i when referring to papers in general and the index k in referring to agents in general. For ease of exposition, we treat a submitted paper as the product of a single agent (we relax this in Section 5). We represent the papers submitted by agent k as {ωiks } and the papers reviewed by an agent k as {ωikr } with the natural condition that {ωiks }∩{ωikr } = ∅. Author k’s Stage I bid accompanying paper ωi is bi. Reviewer k’s scores of paper ωi on the T criteria are written as (x[kT ])i, their ﬁrst predictions of peer k ’s reported scores as (pˆk [T]←k)i, and their second predictions of peer k ’s reported scores as (pˆ+k [T]←k)i. When reviewer k exerts enough effort to receive informed private signals up to criterion tk, the cost of the effort is ek(tk). A complete table of notation is presented in Appendix A.

Mechanism Design Background We deﬁne an agent k’s strategy as a map from their private

information to a distribution over reports elicited by a mechanism. An agent’s Stage I strategy consists

of their bids for all their submitted papers, denoted s(k1) = (bi)i∈{ωk }. An agent’s Stage II strategy is
consists of the effort they exerted to arrive at an informed review score on tk criteria, reported scores on

all T criteria and their two predictions of a peer’s score on every criterion for every paper they review,

denoted s(k2) =

ek(tk),xˆ[kT ],pˆk [T ]←k,pˆk [T ]←k

. The complete strategy for an agent

i i∈{ωikr }

participating in the mechanism is simply sk = s(k1) ×s(k2). A strategy proﬁle is a tuple of every agent’s strategy s = (s1,s2,...,sK ). We represent the strategy proﬁle of all agents excluding agent k as s−k.

Deﬁnition 1 ((Strictly/Weakly) Dominant Strategy). A strategy sk is (strictly/weakly) dominant if the strategy proﬁle (sk,s−k) (strictly/weakly) maximizes agent k’s payoff for any strategy proﬁle s−k.

Deﬁnition 2 ((Strict) Bayesian Nash Equilibrium (BNE)). A strategy proﬁle s = (s1,s2,...,sK ) is a Bayesian Nash Equilibrium if no agent k can increase their payoff by deviating from sk when the other agents play s−k. The BNE is strict if deviating from sk decreases agent k’s payoff. If the strategy proﬁle of the BNE consists of identical strategies for all agents, the BNE is symmetric.

7

Utility Function We specify the utility function Uk(1) of an agent k participating in the submission mechanism (Stage I) as follows:

Uk(1) = i∈{ωiks }(vkδi −ci) accepted for review (1)

0

reject without review

where δi is a random variable indicating whether or not the paper is accepted for publication, vk is the agent’s value in credits for a paper accepted for publication and ci is the price charged by the Stage I mechanism for a review slot. We can similarly specify the utility function Uk(2) of an agent k participating in the H-DIPP reviewing mechanism (second stage) as follows:

Uk(2) =

rk s(k2) , s(−2k) −(ek(tk))i

(2)

i

i

i∈{ωikr }

where rk is the credits rewarded by the H-DIPP mechanism for reviewer k’s reported scores and predictions. The total utility of an agent participating in the mechanism is Uk = Uk(1) +Uk(2).
We note that the utilities and strategies of the two stages are independent, in that the strategy in one stage has no bearing on the utility achieved in the other stage. Hence, we can reason about the incentives and strategy proﬁles of the two stages separately. With this setup, we now present the two stages of the mechanism.

4.2 Stage I: Paper Submission

Our Stage I mechanism uses a VCG auction to allocate review slots and charge winners the threshold bid necessary for their paper to be reviewed. We discuss how the number of review slots P may be determined in Section 4.4, but the goal is to determine P based on how many high-quality reviews can be paid for in Stage II. The auction asks authors to submit a bid bi along with every paper ωi indicating how much they are willing to pay for a review slot for that paper. If an author truly values a paper being accepted for publication at bi credits, it can equivalently be stated as the willingness to spend the necessary time and effort to earn bi credits as a reviewer. Furthermore, authors can observe the quality of their paper ωi and arrive at a belief that the paper will be accepted for publication with probability ηi conditional on review under the equilibrium of the submission mechanism (bids have no bearing on the chance of acceptance for publication). Hence, denoting reviewer k’s value of a paper accepted for publication as vk, the expected value of a review slot is E[vkδi] = vkηi. We also have the following natural assumptions: (1) the review slots are identical, (2) each paper may only receive a single review
slot, and (3) package bids are not allowed, i.e., authors cannot make a single bid for multiple review
slots for multiple papers, but must instead make a separate bid for each slot for each paper. These
assumptions are critical to making it simple and practical to identify the winners of the auction when
a single author can submit multiple papers for review.

After collecting bids, the VCG mechanism will accept the P papers with the highest bids for review,

denoted {ω1∗,...,ωP∗ }, with associated bids {b∗p}p∈[P ]. Denoting the paper with the P + 1-highest bid b∗P +1 as ωP∗ +1, the mechanism charges all authors of the papers with winning bids a price of

ci =

P=+11, =ib∗ −

P =1,

=ib∗

= b∗P +1.

Thus,

all

papers

accepted

for

review

are

charged

the

highest

losing bid, and accepting these papers maximizes author welfare for a given number of review slots P .

Truthful bidding is a weakly dominant strategy for VCG, so we expect that rational actors will place bids equal to their value for their paper being reviewed. The natural assumption of identical review slots and prohibition of package bids precludes the need to solve a computationally challenging knapsack problem to identify winners, and preserves the weak dominance of truthful bidding by authors who submit multiple papers. Thus, the Stage I strategy proﬁle that maximizes aggregate

author welfare under a given number of review slots P is s(1) =

{vkηi}i∈{ωk }

. The

is k=1,...,K

mechanism allocates review slots to the highest bidding papers, which will generally be papers that

are likely to be accepted for publication and/or have authors who generate a lot of value as reviewers.

Thus, the papers accepted for review should also ideally be of higher quality if the review process

screens well for quality. Lastly, this should also get rid of the need for desk rejection of papers.

8

4.3 Stage II: Peer Review
We now present the second stage of the mechanism which focuses on incentivizing honest and effortful reviews of the P papers accepted by the VCG auction. The context here is as follows: the bid requirement for the submission process in Stage I creates demand for credits, and since this can only be earned by reviewing, it incentivizes researchers to join the reviewer pool. While this is desirable, we must structure the payment for reviews carefully. A ﬂat fee to reviewers could result in reviewers putting in minimal effort and getting paid anyway; if credits could be earned with little effort, bids could be made arbitrarily high, defeating the point of the mechanism. Instead, we need to structure the payment such that honest and effortful reviews are paid more than low-effort or false reviews. This is a challenging problem in the absence of ground truth veriﬁcation; and our proposed H-DIPP mechanism is a novel solution to this problem. Note that the H-DIPP mechanism stage describes only the reviewing process for a single paper; we are agnostic about the paper-reviewer matching process, and meta-reviewers can make ﬁnal accept/reject decisions as usual.
Thus, our two main considerations are incentivizing reviewers to exert effort, and to report their true reviews. We achieve both by adapting the Target Differential Peer Prediction (TDPP) (Schoenebeck and Yu, 2020) to operate within the Hierarchical Mutual Information Paradigm (HMIP) (Kong and Schoenebeck, 2018). The Differential Peer Prediction mechanism is a recently proposed prediction market-style mechanism applicable to our small-group, heterogeneous reviewer setting and determines payments for reviews of each paper independently. They present their mechanism for a group of 3 agents, ideal for our peer review setting. The mechanism works as follows: ﬁrst, it assigns the 3 agents the roles of expert, target, and source. Then, it asks the expert for a probability distribution representing their prediction of the target’s signal. Finally, it reveals the source’s signal and asks the expert for an updated prediction of the target’s signal. Schoenebeck and Yu (2020) show that the total expected payment to agents is the mutual information of the source and target’s signals conditioned on the expert’s signals, and this yields a strong truthfulness guarantee (Kong and Schoenebeck, 2019).
However, truthfulness is not enough; reporting low-effort signals can still be truthful. Additionally, since these truthful signals are paid their mutual information, when low-effort signals correlate more than high-effort signals, it becomes natural to coordinate into less informative equilibria. HMIP proposes a framework to get around this issue by constructing a hierarchy of criteria that range from requiring low-effort to high-effort signals, and paying agents the gain in information on higher level criteria. Single-HMIM is an instantiation of the HMIP framework to incentivize effortful reviews on a single task, but uninformative equilibria can pay more than truthful equilibria and we seek to overcome this.
We combine the insights from TDPP and HMIP to propose a Hierarchical-Differential Peer Prediction (H-DIPP) mechanism. We present some fundamentals of the Mutual Information Paradigm (Kong and Schoenebeck, 2019) in Appendix C, and detail the relationship between our H-DIPP mechanism and the related single-HMIM and TDPP mechanisms in Appendix D. Proofs are presented in Appendix E. We now give a more detailed discussion of our reviewing mechanism H-DIPP (Mechanism 1).

4.3.1 Model Preliminaries

We begin by assuming that a given paper ωi will be scored on a set of T criteria by exactly 3 reviewers1

A,B,C. Each reviewer rates the paper ωi on criterion t on a Dt-point scale, i.e., reviewers’ reported

scores can take on any value in a Σt = {1,2,...,Dt}. The same H-DIPP mechanism will be used deter-

mine payments for every paper’s reviewers, so we drop the index i referring to a speciﬁc paper going for-

ward. Now, suppose we have a sample space Ω of all the possible realizations of papers (‘world states’) a

reviewer may see. A reviewer k’s private review score on criterion t is the random variable Xkt : Ω → Σt describing their honest private assessment (or ‘signal’) of the paper on that criterion, when they exert the

necessary effort to arrive at an informed opinion. A reviewer’s joint distribution of effort-informed re-

view

sco

res

for

all

criteria

is

represented

by

the

multivariate

random

variable

X

[T k

]

=

(

Xk1

,Xk2

,...,XkT

)

∈

Σ1 ×Σ2 ×...×ΣT , and the full joint distribution of effort-informed review scores over all criteria for

all reviewers is P(X[AT ],X[BT ],X[CT ]). We assume that this joint distribution is common knowledge and has full support, i.e., every assignment of outcomes has non-zero probability. Unlike HMIP, differential

peer prediction (and our approach) does not require the assumption that the common prior be symmetric,

and is applicable for heterogeneous agents. We state this assumption formally below:

1This can be extended to any number of reviewers by cyclically assigning the roles of expert, target, and source.

9

Assumption 1 (Common Prior with Full Support). Reviewers’ effort-informed signals on all criteria {x[AT ],x[BT ],x[CT ]} are drawn from the common prior P(X[AT ],X[BT ],X[CT ]), and the common prior is common knowledge. The common prior also has full support.
Criteria Hierarchy Next, we construct a hierarchy of criteria by imposing an ordering on them where coming to an informed private assessment on ‘lower’ level criteria requires less effort than ‘higher’ level criteria. Naturally, when a reviewer exerts enough effort to come to an informed private assessment on some higher level criterion, they will also have arrived at an informed private assessment on all lower level criteria, so the total effort required to arrive at informed private assessments increases up the hierarchy. Thus, we say ti > tj if scoring the paper on criterion ti requires strictly more effort than providing a score on criterion tj. When a reviewer has exerted enough effort to arrive at an informed review score xtk on criterion t, we say the reviewer has completed that criterion. The idea is to explicitly construct lower level criteria to collect the cheap signals and pay agents based on the information added by valuable higher level criteria. We can deﬁne the positive, increasing function ek : [T ] → R+ specifying reviewer k’s total effort cost (in the mechanism-speciﬁc credits) to obtain effort-informed signals up to criterion t, where ek(t) > ek(t ) if t > t . Note that different agents may have different cost functions ek, so for instance, veteran peer reviewers may require less effort to come to an honest assessment of the paper on every criterion. We give a sample plausible criteria hierarchy below.
In the current peer review setting at conferences like NeurIPS, reviewers are already required to report scores on a variety of criteria, so we simply require a particular structuring of existing criteria. As an example, one possible hierarchy of criteria could be the following (where responses fall on a 5-point scale):
• Criterion 1: (presentation) “How clear is the writing/exposition?”
• Criterion 2: (completeness) “How easy would it be to reproduce major results?”
• Criterion 3: (correctness) “How accurate are the technical claims and methodology?”
• Criterion 4: (contribution) “How interesting and valuable of a contribution is the work? ”
• Criterion 5: (overall quality) “In which quintile of submitted papers would this work fall?"
We see that for example, answering the level 4 question on contribution requires a level of effort that would also yield informed opinions on the presentation, completeness, and correctness; the reverse isn’t obviously true. In general, the criteria hierarchy should be structured so higher level criteria require more effort.
Strategic and Uninformed Reports Our mechanism will elicit reviewers’ private review scores Xkt on all criteria, and we denote the (potentially strategic) reported review score as Xˆkt . With θk as the transition matrix describing reviewer k’s reporting strategy, the joint distribution of true signals and reported signals under a signal strategy proﬁle (θA,θB,θC ) is Q(Xˆ [AT ],Xˆ [BT ],Xˆ [CT ],X[AT ],X[BT ],X[CT ]) = θA(Xˆ [AT ]|X[AT ])θB(Xˆ [BT ]|X[BT ])θC (Xˆ [CT ]|X[CT ])P(X[AT ],X[BT ],X[CT ]). When a reviewer plays a truthful reporting strategy (i.e., θk is identity), we denote it is as τk.
Now, if a reviewer only completes up to t criteria, they will not arrive at an informed private score on criteria at higher levels. However, their signals on lower level criteria do provide some information, allowing them to make reasonable guesses of the scores on higher level criteria2. Indeed, we assume that on incomplete higher level criteria, a reviewer’s default private signal is their ‘best guess’ of the signal that other agents would expect of them given the information they already have, and this may depend on how much effort the other agents exert. This is given by some function gkt |t : Σ1 × ... × Σt → Σt where xtk = gkt |t(x[kt]) represents reviewer k’s best guess of their private assessment on criterion t when they have completed only criteria at level t (where t > t). Note that xtk thus either refers to an effort-informed signal drawn from the common prior or a best-guess, depending on the reviewer’s effort level. We use the notation x[ktk] if we wish to emphasize that we are only referring to the effort-informed signals on criteria tk or less (when tk < T ).
Other Assumptions We now list some other assumptions we make. First, we adopt a conditional independence assumption from Kong and Schoenebeck (2018), which states that a reviewer k ’s
2This is an important difference between our approach and the original HMIP, where reporting on incomplete criteria is optional. See Appendix D for more details.
10

private score on some criterion t contains all the information needed to predict their peer k’s score on that criterion, given that peer’s scores on lower level criteria. We write I(·) to represent the mutual information (see Appendix C for more details).
Assumption 2 (Conditional Independence). We make the following conditional independence assumption on the common prior:

I(X[kT ],Xkt |X[kt−1]) = I(Xkt ,Xkt |Xk[t−1])

(3)

or equivalently,

I(X[kT ]\Xkt ,Xkt |X[kt−1],Xkt ) = 0

(4)

Finally, we make an assumption on the informativeness of the signals a reviewer might observe. When a reviewer k arrives at review scores x[kt] on [t] criteria, they also arrive at a posterior belief P(X[−Tk]|x[kt]) about the review scores of other reviewers. Informally, we assume there is some signal a reviewer could observe, which when paired with two different signals from a second reviewer, would induce two different beliefs about the signals of the third reviewer.
Assumption 3 (Second Order Stochastic Relevance). Suppose that for any distinct signals xtB,x˜tB ∈ Σt on any criterion t, there is xtA ∈ Σt such that:
P(XCt |xtA,xtB) = P(XCt |xtA,x˜tB)
We assume the above holds for any permutation of the reviewers {A,B,C}, so the common prior P is second order stochastic relevant.

Figure 2: An illustration of the H-DIPP mechanism from reviewer A’s perspective, when reviewer B is their assigned target and reviewer C is their assigned source.
4.3.2 Mechanism and Payments
Having detailed our assumptions, we now describe exactly how the mechanism collects reports and computes the payment. The mechanism is summarized as Mechanism 1 and illustrated in Figure 2.
Reports First, each reviewer reports their review scores to the mechanism per the usual process. Reviewers arrive at private review scores x[ktk] on tk criteria by exerting ek(tk) effort, and report this to the mechanism (potentially strategically) as xˆ[kT ] ∼ θk(·|xk[tk]). Once all the reviews are in, every reviewer (independently) plays a simple prediction game. We refer to the reviewer playing the prediction game as an ‘expert’, and every ‘expert’ will be assigned a ‘source’ and a ‘target’. We use the following assignments of (expert, target, source) roles: (A,B,C), (B,C,A), and (C,A,B), i.e., when A is plays the game as ‘expert’, B is the assigned ‘target’ and C is the assigned ‘source’, and so on. During the prediction game, the expert must make two probabilistic predictions of their target’s reported review score on each criterion; the expert’s ﬁrst prediction on a criterion is their best guess of their target’s review score on that criterion, and the expert’s second prediction
11

is their updated best guess after the mechanism reveals their source’s review score on that criterion. The criteria are revealed in a pre-determined order speciﬁed by the criteria hierarchy. As an example, consider the case where reviewer A plays the prediction game after all the reviews are in (with target B and source C). The mechanism will ﬁrst elicit the expert A’s prediction pˆB1←A = P XˆB1 |x[AtA] of the target B’s score on criterion 1, and then reveals the source C’s signal xˆ1C on that criterion and ask for an updated prediction pˆ+B1←A = P XˆB1 |x[AtA],xˆ1C . Next on criterion 2, the mechanism
asks for the expert A’s prediction pˆB2←A = P XˆB2 |x[AtA],x1C of the target B’s score on criterion 2, and then reveals the source C’s signal xˆ2C on that criterion and asks for an updated prediction pˆ+B2←A = P XˆB2 |x[AtA],xˆ[C2] . This proceeds in order until the ﬁnal T th criterion. The mechanism thus requests 3T reports: reviewers’ T review scores, their ﬁrst predictions of their assigned target’s scores on T criteria, and updated second predictions of that target peer’s scores on T criteria given the source peer’s scores. A reviewer’s strategy on the reviewing task is sk = ek(tk),xˆ[kT ],pˆk t←k,pˆ+k t←k .

Mechanism 1: H-DIPP

Input: ωi (paper under review), {t1,t2,...,tT } (hierarchical criteria), {A,B,C} (set of 3 reviewers), {α[AT ],β[AT ],α[BT ],β[BT ],α[CT ],β[CT ]} (mechanism hyperparameters criteria)
Output: {rA,rB,rC } (payments for the reviewers) 1 Elicit reviewers’ scores on each criterion {xˆ[kT ]}k∈{A,B,C} per the usual peer review process 2 Assign (expert, target, source) roles as follows: (A,B,C), (B,C,A), (C,A,B).

3 for each criterion t ascending the criteria hierarchy do

4 Elicit {pˆBt←A,pˆCt←B,pˆAt←C }, i.e.,

reviewers A,B,C predictions of the reports of reviewers B,C,A respectively on criterion t.

5 Reveal source scores on criterion t, i.e., reveal reviewer A’s scores xˆtA to reviewer B, reviewer B’s score xˆtB to reviewer C, and reviewer C’s score xˆtC to reviewer A.
6 Elicit {pˆ+Bt←A,pˆ+Ct←B,pˆ+At←C }, i.e., reviewers A,B,C updated predictions of the reports of reviewers B,C,A respectively on criterion t.

7

rAextpert

←

Slog

(xˆ

t B

,pˆ B

t

←

A

)

+

Slog

(xˆ

t B

,

pˆ +B

t

←A

);rAtartget

←

Slog

(

xˆtA

,pˆ +At

←C

)

−

Slog

(xˆ

t A

,pˆ A

t

←

C

)

8

rBextpert

←

Slog

(xˆ

t C

,

pˆ C

t

←

B

)

+

Slog

(xˆ

t C

,pˆ +C

t

←B

);rBtartget

←

Slog

(xˆ

t B

,

pˆ +B

t

←A

)

−

Slog

(xˆ

t B

,

pˆ B

t

←A

)

9

rCextpert

←

Slog

(xˆ

t A

,pˆ A

t

←C

)

+

Slog

(xˆ

t A

,pˆ +At

←

C

);rCtartget

←

Slog

(xˆ

t C

,pˆ +C

t

←B

)

−

Slog

(

xˆ

t C

,pˆ C

t

←B

)

10 rA ← 11 rB ← 12 rC ←

T t=1

αAt

rAextpert

+

β

t A

rAtartget

T t=1

αBt

rBextpert

+

β

t B

rBtartget

T t=1

αCt

rCextpert

+

β

t C

rCtartget

Payments The mechanism will pay each reviewer for their roles as expert and target (source payment is 0). Reviewers receive ‘expert payment’ based on their performance in the prediction game, and receive ‘target payment’ based on a peer’s performance in the prediction game. The goal is for the expert payment to incentivize truthful predictions, and the ‘target payment’ to incentivize honest reports of private review scores. With reviewer-speciﬁc mechanism hyperparameters {α[AT ],β[AT ]}, the criterion-t speciﬁc payment to reviewer A using the log scoring rule Slog is:







rAt = αAt Slog(xˆtB ,pˆBt←A)+Slog(xˆtB ,pˆ+Bt←A)+βAt Slog(xˆtA,pˆ+At←C )−Slog(xˆtA,pˆAt←C ) (5)

expert payment

target payment

Intuitively, the expert payment depends on how good a reviewer’s predictions were during the
prediction game, and the target payment depends how much better a peer expert’s second prediction was over their ﬁrst. To compute reviewer A’s total payment, we simply sum the individual criterion-speciﬁc payments rA = trAt . Payments for the other reviewers B,C are determined analogously. The key idea behind the target payment is that in expectation (shown shortly in Lemma 2), it pays a reviewer
the mutual information of their private score and the source’s score, conditioned on the expert’s

12

information and the source’s scores on lower level criteria. In other words, a reviewer’s target payment for their reported review score on some criterion t is the additional information it contains about the source’s score on that criterion, over and above the source’s cheaper, lower effort information and expert’s information on the lower level criteria.
The function of the mechanism hyper-parameters is to appropriately weight the criterion-speciﬁc payments to cover the marginal effort costs of completing higher level criteria, and will describe how to set them in relation to reviewers’ common prior and effort costs in the next subsection. The task of gleaning information about the common prior and reviewers’ effort costs is important, but we leave this to future work.

4.3.3 Equilibria
We now analyze the equilibria of our mechanism3. First, we give the strict truthfulness of our mechanism when all reviewers complete a certain number of criteria.
Theorem 1 (Strict Truthfulness). Suppose reviewers A,B,C complete [tA],[tB], and [tC ] criteria respectively. Then, on shared complete criteria [t ] = [min(tA,tB,tC )], the H-DIPP mechanism is strictly truthful, i.e., all reviewers honestly reporting their true predictions and private review scores on shared complete criteria [t ] is a strict Bayesian Nash equilibrium.

However, mere truthfulness is not enough; the mechanism must also incentivize reviewers to exert the necessary effort to complete all criteria. After all, we care about eliciting reviewers’ effort-informed signals on all criteria. To achieve this, we must reason about reviewers’ marginal payments and costs of effort. Going forward, we use R to denote expected payment. We break up the total marginal expected payment ∆Rk into the marginal expected expert and target payments ∆Rkexpert and ∆Rktarget, which in turn are each broken up into marginal payments ∆Rkextpert and ∆Rktatrget on each criterion. We give the following lemma on reviewers’ marginal expected payments for completing an additional criterion and show that it is always non-negative (when peers have completed all criteria and are reporting truthfully).
Lemma 1 (Non-negative Marginal Payments). Suppose reviewers B,C complete all criteria and truthfully report their private review scores and predictions. Then, reviewer A’s marginal expected payment for completing criterion tA and honestly reporting private review scores and predictions is ∆RA(tA) := RA(tA)−RA(tA −1) = ∆RAexpert(tA)+∆RAtarget(tA), where:

1. ∆RAexpert(tA) := RAexpert(tA)−RAexpert(tA −1) = Tt=1αAt ∆RAextpert(tA) 2. ∆RAtarget(tA) := RAtarget(tA)−RAtarget(tA −1) = Tt=1βAt ∆RAtartget(tA)

3. ∆RAexptert(tA) := I XBt ;XAtA |X[AtA−1],X[Ct] +I XBt ;XAtA |X[AtA−1],X[Ct−1]

 0, t < tA



4. ∆RAtartget(tA) := I XBtA ;XAtA |X[CT ],X[BtA−1] −I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1] ,

t = tA

 E

KL

P(XBt |gAt|tA (X[AtA]),X[CT ],X[Bt−1])||P(XBt |gAt|tA−1(X[AtA−1]),X[CT ],X[Bt−1])

,

t > tA

Furthermore, the marginal expected expert and target payments on each criterion are always nonnegative, i.e., ∆RAextpert(tA),∆RAtartget(tA) ≥ 0 for all t,tA ∈ [T ] and the inequality is strict when t = tA. Similar results hold for reviewers B and C when peers complete all criteria and report truthfully.

With this, we can now specify how to set the mechanism hyperparameters to incentivize a reviewer to complete all criteria, such that all reviewers completing all criteria and reporting truthfully is a strict BNE (equivalent to the potent property of hyperparameters in Kong and Schoenebeck (2018)).

3We ignore permutation equilibria, where the labels for signals are permuted. Such equilibria exist in our mechanism, but they are not of serious concern; see Kong and Schoenebeck (2019) for details.
13

Theorem 2 (Optimal Mechanism Hyperparameters). Suppose mechanism hyperparameters {α[AT ],β[AT ],α[BT ],β[BT ],α[CT ],β[CT ]} are solutions to the following linear program:

T

min

αkt Rkextpert(T )+βkt Rktatrget(T )

k∈{A,B,C} t=1

T
s.t. αAt ∆RAexptert(tA)+βAt ∆RAtartget(tA) > ∆eA(tA) for 1 ≤ tA ≤ T
t=1

T

(6)

αBt ∆RBexptert(tB )+βBt ∆RBtartget(tB ) > ∆eB (tB ) for 1 ≤ tB ≤ T

t=1

T
αCt ∆RCexptert(tC )+βCt ∆RCtartget(tC ) > ∆eC (tC ) for 1 ≤ tC ≤ T
t=1
αkt ,βkt ≥ 0

Then, for any reviewer k, if their peers complete all criteria and report predictions and review scores truthfully, reviewer k strictly maximizes their expected utility by also completing all criteria and reporting predictions and review scores truthfully. In other words, completing all criteria and reporting truthfully is a strict Bayesian-Nash equilibrium. We refer to this equilibrium as the fully-informative equilibrium.

Having shown that our desired strategy proﬁle (all reviewers completing all criteria and reporting truthfully) is a strict BNE, we now discuss how this compares to other equilibria and strategy proﬁles. A common challenge is that truthful equilibria pay less than effortless uninformative equilibria (like in single-HMIM), so the truthful equilibria are not realized in practice. However, this is not the case with H-DIPP; we will show that an uninformative equilibrium pays nothing in expectation and the fully informative equilibrium pays strictly more. We begin with the following lemma on the total payment to reviewers in any equilibrium:
Lemma 2 (Aggregate Reviewer Payments). Let sk = ek(tk),xˆ[kT ],pˆ+k [T]←k,pˆk [T]←k be reviewer k’s strategy, where k is reviewer k’s target. If s = (sA,sB,sC ) is an equilibrium strategy proﬁle, reviewer A’s expected payment is:

T
E[rA] = αAt −E H Q XˆBt |x[AtA],xˆ[Ct]
t=1

−E H Q XˆBt |x[AtA],xˆ[Ct−1]

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ [Bt−1] (7)

(and expected payments for peers B,C computed analogously). Reviewers’ total expected payment is:

T
E[rA +rB +rC ] = αAt −E H Q XˆBt |x[AtA],xˆ[Ct]
t=1
+αBt −E H Q XˆCt |xB[tB],xˆ[At]

−E H Q XˆBt |x[AtA],xˆC[t−1] −E H Q XˆCt |x[BtB ],xˆ[At−1]

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ [Bt−1] +βBt I XˆCt ;XˆBt |X[AtA],Xˆ [Ct−1]

+αCt −E H Q XˆAt |x[CtC ],xˆ[Bt] −E H Q XˆAt |x[CtA],xˆ[Bt−1]

+βCt I XˆAt ;XˆCt |X[BtB ],Xˆ [At−1] (8)

We make several observations. First, unlike the ideal-HMIP proposal (and like the concrete single-HMIM proposal) in Kong and Schoenebeck (2018), we cannot claim that truthful reporting of private signals is a dominant strategy, since a reviewer’s target payment for reporting private scores will depend on their peer’s expert prediction strategy. However, it is the case that individual reviewers earn higher target payments in equilibria where they report their private review score truthfully, compared to ones where they strategically manipulate the score, i.e., I XˆBt ;XˆAt |XC[tC],Xˆ [Bt−1] ≤ I XˆBt ;XAt |X[CtC],Xˆ [Bt−1] for all t ∈ [T ] (and similarly for other reviewers) due to the data processing inequality (Kong and Schoenebeck, 2019).
Second, unlike TDPP (which does not account for effort), we cannot guarantee the strong truthfulness of the fully informative truth-telling equilibrium, i.e., we cannot guarantee that the fully informative equilibrium has higher aggregate utility than any other equilibrium. It may be possible for a reviewer to

14

strategically manipulate their private signals so that when used as a source for some expert’s prediction, the target receives higher payment; e.g., it may be possible that reviewer B’s strategic reports provides a higher target payment for reviewer A with I XBt ;XˆAt |X[CtC],X[Bt−1] ≤ I XˆBt ;XˆAt |X[CtC],Xˆ [Bt−1] . Reviewer B’s strategic reports could then increase reviewer A’s target payment by more than it hurts their own target payment, thereby raising aggregate utility (costs are the same). However, reviewers do not receive payments for their role as source, and further must accept lower target payment for such strategic manipulation, so reviewers are disincentivized from pursuing such equilibria unilaterally. Nevertheless, we cannot rule out the presence of equilibria where all reviewers lie strategically to raise another reviewer’s payment.
Thirdly, a reviewer’s expected expert payment is always non-positive (since the Shannon entropy is non-negative), and a reviewer’s expected target payment is always non-negative. Thus, we must ensure that a reviewer’s expected payment is greater than their cost in the fully informative equilibrium, so that it is individually rational for reviewers to participate in the mechanism. This will naturally involve scaling the hyperparameters such that the target payment pays more than the expert payment subtracts. This also grants that the fully-informative equilibrium pays more than an uninformative equilibrium where no reviewers exert effort. We state this formally below.
Theorem 3 (Uninformative Equilibria and Individual Rationality).

(a) In the highest paying uninformative equilibrium (where all reviewers exert zero effort), every reviewer’s expected payment (and hence aggregate reviewer welfare) is zero.

(b) Suppose that in addition to the constraints speciﬁed in Theorem 2, the mechanism hyperpa-

rameters also satisfy Rk(T )−ek(T ) > 0 ⇔

T t=1

αkt

Rkextpert(T

)

+

βkt

Rktatrget(T

)

>

ek

(T

)

for

k ∈ {A,B,C}, i.e., reviewers’ expected utility in the fully-informative equilibrium is positive.

Then, it is individually rational to participate in the mechanism and complete all criteria

and report truthfully, given peers are doing the same. Additionally, the fully-informative

equilibrium has higher individual and aggregate utility than the uninformed equilibrium.

Finally, we note that while the mechanism can hide reviewers’ identities from each other to provide some measure of protection against collusion, a formal analysis of H-DIPP’s robustness to collusion would be valuable direction for future work.

4.4 Demand and Supply
In Section 4.2, we presented the Stage I VCG auction that accepts P papers for review. However, the question of how a conference should determine the number of review slots P still remains to be answered. On this issue, we have the following considerations:
1. Balanced Budget: We aim to set P based on the number of reviews that can be paid for under the Stage II H-DIPP payment scheme using revenue from the Stage I VCG auction. Recall that the cost to the mechanism to have one paper reviewed by a given reviewer triplet in the fully-informative equilibrium (if this is indeed the observed equilibrium) is given by Lemma 2, where hyperparameters have been set using Theorem 2. We treat this as a hard constraint.
2. Maximize P : We would like to accept as many papers for review as feasible.
3. Fair Expertise-based Matching: Reviewers must be well-qualiﬁed to review their assigned papers and have the appropriate domain expertise. Additionally, all papers accepted for review should have equal consideration when being assigned reviewers; the bid accompanying a paper should have no bearing on the quality of reviewer matches4. We assume access to a black-box algorithm that can score a reviewer’s match with a paper, and assign reviewers to papers based on this score (e.g. Toronto Paper Matching System (Charlin and Zemel, 2013)).
The third consideration comes from the observation that the mechanism should not blindly seek to minimize its total cost over all papers in an attempt to squeeze in as many review acceptances P as possible. The cost to the mechanism for reviewing a single paper depends both on the hyperparameters (optimally set via Theorem 2) as well as the mutual information in reviewers’ signals, so total costs
4Aside from the motivation of fairness, if higher bidding papers received priority in matching reviewers, reviewers could infer a paper’s bid from the quality of their match, giving them a low-effort signal of paper quality.

15

can be made abnormally low by badly matching reviewers to papers such that reviewers’ signals have naturally low mutual information. Heuristically, we expect that well-matched expert reviewers hit the sweet spot of having low effort costs to review a paper and more mutually informative review scores with expert peers, so the mechanism pays a fair price for such expert reviews.

Nevertheless, we face a trade-off where accepting more papers for review may decrease the average

quality of the paper-reviewer matches. Formally, let Si∗ denote the match score for the k-th reviewer

∗

k
denote the expected payment to the k-th reviewer of paper ωi under the optimal

of paper ωi and Rik

assignment of reviewers (as determined by the black-box algorithm) for some given number of papers

P accepted for review. Then, the optimal number of review slots is the largest P with an acceptable

average reviewer match score that also satisﬁes the balanced budget constraint:

1P ∗

∗

∗

argPmax P i=1SiA +SiB +SiC +λP

(9)

P

s.t. b∗P +1 ·P ≥ Ri∗A +Ri∗B +Ri∗C

i=1

The parameter λ controls the tradeoff between match quality and number of papers accepted for review, and is a design decision to be made by individual conferences. The objective can be modiﬁed if desired; substituting the mean reviewer match score for the median or an additional term penalizing variance in reviewer match scores are alternative choices. Algorithmically, we can ﬁnd the optimal number of review slots by ﬁrst computing all the valid number of review slots under the balanced budget constraint, and picking the largest one with an acceptable average reviewer match score (summarized in Algorithm 2).

Algorithm 2: Determination of Number of Review Slots P
Input: N papers submitted to the conference, a black-box algorithm matching reviewers to papers based on expertise (e.g. Toronto Paper Matching System)
Output: Optimal Number of Review Slots P 1 for n = 1 : N do 2 Obtain optimal matching of reviewers to n papers from black-box algorithm 3 If accepting n papers satisﬁes the budget constraint b∗P +1 ·P ≥ Pi=1Ri∗A +Ri∗B +Ri∗C ,
add p to the list of allowed number of review slots P ← P ∪{n}
4 Select the largest P ∈ P with an acceptable average reviewer
match score, i.e., largest P that maximizes the objective P1 Pi=1Si∗A +Si∗B +Si∗C +λP

We conclude with a caveat. Setting the mechanism hyperparameters by solving the linear program in Theorem 2 and computing the expected payment in the fully informative equilibrium (Lemma 2) requires estimating or eliciting information about the common prior P and the reviewer triplet’s effort cost functions ek(·). Eliciting this information truthfully from reviewers will typically require payment to be independent of the elicitation, i.e., reviewer triplets’ hyperparameters must be set independently of their elicited/estimated costs, or reviewers will have an incentive to misrepresent their costs. Practically, we may need a single set of mechanism hyperparameters {α[T ],β[T ]} that determine payments to all reviewers. These hyperparameters could be set so that they satisfy the constraints of the linear program for the ‘most expensive’ reviewer in the reviewer pool, and all lower-cost reviewers will enjoy surplus. However, doing this still requires estimating or truthfully eliciting necessary information about reviewer triplets’ common priors and reviewers’ marginal costs and is an important direction for future work.
5 Discussion
Here, we discuss some important considerations for a practical implementation of our mechanism.
Collaboration We assumed for ease of exposition that every paper has a single author and this author must participate in the review process to earn enough credits to cover their bids. In practice, papers are largely multi-author efforts, and not every co-author is equally qualiﬁed to participate in the review process. The natural relaxation of the single-author assumption is to formalize the role

16

of a single ‘sponsor’5 who puts up the credits needed for the bid, and will typically be a senior author who has banked enough credits by reviewing. However, non-author researchers may also supply the credits for a paper’s accompanying bid and be acknowledged in their capacity as sponsor, mitigating incentives to add credit-rich non-author sponsors as authors in exchange for a high bid. Additionally, we may wish to grant a lump-sum of credits to ﬁrst-time authors to give them the right to submit some number of papers while they build the necessary expertise for review. We emphasize that bids do not have any bearing on a paper’s acceptance for publication, so sponsors will still be sensitive about their bids as a weak paper accepted for review will likely be rejected and the spent credits will be wasted.
Common Prior Assumption We have assumed that reviewers’ review scores are drawn from a common prior that is common knowledge. This assumption can be made more realistic by making several of a reviewer’s past reviews available for reference, so any reviewer can gauge how their opinion correlates with their peers’.
Post-processing Review Scores The H-DIPP mechanism elicits honest and effortful opinions from heterogeneous reviewers, with the goal of reducing the noise in estimates of a paper’s quality. However, this also means that reviewers’ honest and effortful opinions may be miscalibrated with respect to the rating scale (Shah et al., 2018). Thus, some post-processing of these scores (Wang and Shah, 2018; Lawrence, 2021) to correct reviewer’s individual biases may produce a better picture of a paper’s quality.
Conﬂicts of Interest We note that our results on incentivizing effortful and truthful reviews hold under the assumption that there are no other incentives in play. However, if the matched reviewers have a conﬂict of interest, this would weaken the incentive to report honest and effortful reviews. Thus, related work on optimizing paper-reviewer matches (Xu et al., 2018; Jecmen et al., 2020) to avoid conﬂicts of interest is complementary and important to the success of our mechanism.
Behavioural and Social Considerations Our focus in this paper has been to develop a mechanism for paper submission and reviewing with desirable theoretical properties. However, our fundamental assumptions about human behaviour may or may not hold in practice. A primary concern is that introducing payments for reviewers could crowd-out pro-social motivations (Squazzoni et al., 2013; Gneezy and Rustichini, 2000). Empirical evidence for when such prediction market-style mechanisms succeed (Debmalya et al., 2020) and fail (Gao et al., 2014) can help reﬁne our mechanism and similar ideas.
A Secondary Market We have assumed that our mechanism operates with its own currency, but we could further allow the sale and purchase of credits on a secondary market. This has the potential to allow individuals to earn real-world currency as reviewers, and could even encourage specialization in the review process: researchers who excel at reviewing in H-DIPP mechanism could allocate more of their time providing high quality reviews and be appropriately compensated for their work.
6 Conclusion and Future Work
We tackle incentive issues in scientiﬁc peer review by proposing a two-stage mechanism that allocates review slots to papers via a VCG auction and uses the revenue raised to pay participating reviewers in our novel H-DIPP mechanism. The H-DIPP mechanism aims to incentivize reviewers to obtain and report a clearer signal of a paper’s quality in order to reduce the noise in the accept/reject decisions. We showed that the H-DIPP mechanism is strictly truthful, and described how to set mechanism hyperparameters so that exerting enough effort to score the paper honestly on all criteria is a strict Bayesian Nash equilibrium. We also showed that such a fully informative equilibrium pays more in expectation than any uninformative equilibrium, and described how the mechanism hyperparameters can be further tweaked so participating in the H-DIPP reviewing mechanism is individually rational. Finally, we discussed the trade-off between the number of available review slots and quality of paper-reviewer matches. However, there remain important questions to be answered, and we present some of them below:
1. Criteria Hierarchy: What is the best construction of the criteria hierarchy?
5A paper can only have a single sponsor; allowing multiple sponsors to contribute to a bid would privilege papers with more authors. For the same reason, credits should not be giftable to others.
17

2. Estimating Hyperparameters and Costs: Setting mechanism hyperparameters and the number of review slots P requires information about reviewers’ marginal payments and costs, so identifying ways to learn (Liu and Chen, 2016) or elicit this information would be valuable.
3. Variance in Payments: Is the variance in payments so large that honest and effortful reviews get negative payments with unacceptable regularity? If so, can the addition of a few more reviewers reduce this variance?
4. Improving Equilibria: Can we leverage information across reviewing tasks (Dasgupta and Ghosh, 2013; Agarwal et al., 2017) in an efﬁcient way to further increase total payoffs in the truth-telling equilibrium? How does the utility of untruthful equilibria compare to the fully-informative equilibrium? How robust is the mechanism to collusion?
5. Truthful, but Useful?: H-DIPP is strictly truthful, but whether or not this is useful is a different empirical question. If H-DIPP-incentivized review scores vary widely even with post-processing (i.e., truthful reviews end up being quite idiosyncratically subjective), or do not track other measures of paper quality, it may not be possible to extract a meaningful signal (criticism C4). Running small scale experiments with real money could shed light on whether the beneﬁts of our proposed mechanism warrant the overhead.
Such issues merit further in-depth discussion and analysis. Our proposed mechanism is intended to serve as a theoretically well-grounded solution to incentive issues in scientiﬁc peer review. Ultimately, experimental validation will be the ﬁnal arbiter of the plausibility of our approach.
Acknowledgments and Disclosure of Funding
We thank Sandesh Adhikary for assistance with ﬁgures in the paper.
18

References
Agarwal, A., Mandal, D., Parkes, D. C., and Shah, N. (2017). Peer prediction with heterogeneous users. In Proceedings of the 2017 ACM Conference on Economics and Computation, pages 81–98.
Alberts, B., Hanson, B., and Kelner, K. L. (2008). Reviewing peer review.
Arns, M. (2014). Open access is tiring out peer reviewers. Nature, 515(7528):467–467.
Bengio, Y. (2020). Time to rethink the publication process in machine learning.
Cai, Y., Daskalakis, C., and Papadimitriou, C. (2015). Optimum statistical estimation with strategic data sources. In Conference on Learning Theory, pages 280–296.
Carvalho, A., Dimitrov, S., and Larson, K. (2013). Inducing honest reporting without observing outcomes: An application to the peer-review process. arXiv preprint arXiv:1309.3197.
Charlin, L. and Zemel, R. (2013). The toronto paper matching system: an automated paper-reviewer assignment system.
Dasgupta, A. and Ghosh, A. (2013). Crowdsourced judgement elicitation with endogenous proﬁciency. In Proceedings of the 22nd international conference on World Wide Web, pages 319–330.
de la Rosa, J. L. and Szymanski, B. K. (2007a). Citation auctions as a method to improve selection of scientiﬁc papers. In 2007 2nd International Conference on Digital Information Management, volume 1, pages 479–486. IEEE.
de la Rosa, J. L. and Szymanski, B. K. (2007b). Selecting scientiﬁc papers for publication via citation auctions. IEEE Intelligent Systems, 22(6):16–20.
Debmalya, M., Goran, R., and David, P. (2020). The effectiveness of peer prediction in long-term forecasting. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 2160–2167.
Faltings, B. and Radanovic, G. (2017). Game theory for data science: Eliciting truthful information. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, 11(2):1–151.
Fox, J. and Petchey, O. L. (2010). Pubcreds: ﬁxing the peer review process by “privatizing” the reviewer commons. The Bulletin of the Ecological Society of America, 91(3):325–333.
Frank, R. H. (2016). Success and Luck. Princeton University Press.
Frijters, P. and Torgler, B. (2019). Improving the peer review process: a proposed market system. Scientometrics, 119(2):1285–1288.
Gao, A., Wright, J. R., and Leyton-Brown, K. (2016). Incentivizing evaluation via limited access to ground truth: Peer-prediction makes things worse. arXiv preprint arXiv:1606.07042.
Gao, X. A., Mao, A., Chen, Y., and Adams, R. P. (2014). Trick or treat: putting peer prediction to the test. In Proceedings of the ﬁfteenth ACM conference on Economics and computation, pages 507–524.
Gneezy, U. and Rustichini, A. (2000). Pay enough or don’t pay at all. The Quarterly journal of economics, 115(3):791–810.
Jecmen, S., Zhang, H., Liu, R., Shah, N., Conitzer, V., and Fang, F. (2020). Mitigating manipulation in peer review via randomized reviewer assignments. Advances in Neural Information Processing Systems, 33.
Kong, Y. and Schoenebeck, G. (2018). Eliciting expertise without veriﬁcation. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 195–212.
Kong, Y. and Schoenebeck, G. (2019). An information theoretic framework for designing information elicitation mechanisms that reward truth-telling. ACM Transactions on Economics and Computation (TEAC), 7(1):1–33.
Lawrence, N. D. (2021). A retrospective on the 2014 neurips experiment.
19

Lawrence, N. D. and Cortes, C. (2015). Peer review and the nips experiment.
LeCun, Y. (2012). A new publishing model in computer science.
Lipton, Z. C. and Steinhardt, J. (2018). Troubling trends in machine learning scholarship. arXiv preprint arXiv:1807.03341.
Liu, Y. and Chen, Y. (2016). Learning to incentivize: Eliciting effort via output agreement. arXiv preprint arXiv:1604.04928.
Merriman, B. (2020). Peer review as an evolving response to organizational constraint: Evidence from sociology journals, 1952–2018. The American Sociologist, pages 1–26.
Miller, N., Resnick, P., and Zeckhauser, R. (2005). Eliciting informative feedback: The peer-prediction method. Management Science, 51(9):1359–1373.
NeurIPS (2019). What we learned from neurips 2019 data. Medium.
Noothigattu, R., Shah, N., and Procaccia, A. (2021). Loss functions, axioms, and peer review. Journal of Artiﬁcial Intelligence Research, 70:1481–1515.
Prelec, D. (2004). A bayesian truth serum for subjective data. science, 306(5695):462–466.
Prüfer, J. and Zetland, D. (2010). An auction market for journal articles. Public Choice, 145(3-4):379–403.
Radanovic, G. and Faltings, B. (2013). A robust bayesian truth serum for non-binary signals. In Proceedings of the 27th AAAI Conference on Artiﬁcial Intelligence (AAAI’13), number CONF, pages 833–839.
Radanovic, G. and Faltings, B. (2014). Incentives for truthful information elicitation of continuous signals. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 28.
Radanovic, G., Faltings, B., and Jurca, R. (2016). Incentives for effort in crowdsourcing using the peer truth serum. ACM Transactions on Intelligent Systems and Technology (TIST), 7(4):1–28.
Schoenebeck, G. and Yu, F.-Y. (2020). Two strongly truthful mechanisms for three heterogeneous agents answering one question. In International Conference on Web and Internet Economics, pages 119–132. Springer.
Sculley, D., Snoek, J., and Wiltschko, A. (2018a). Avoiding a tragedy of the commons in the peer review process. arXiv preprint arXiv:1901.06246.
Sculley, D., Snoek, J., Wiltschko, A., and Rahimi, A. (2018b). Winner’s curse? on pace, progress, and empirical rigor.
Shah, N. B. (2019). Principled methods to improve peer review. Retrieved May, 27:2020.
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and Von Luxburg, U. (2018). Design and analysis of the nips 2016 review process. Journal of machine learning research.
Squazzoni, F., Bravo, G., and Takács, K. (2013). Does incentive provision increase the quality of peer review? an experimental study. Research Policy, 42(1):287–294.
Stelmakh, I., Shah, N. B., Singh, A., and Daumé III, H. (2020). A novice-reviewer experiment to address scarcity of qualiﬁed reviewers in large conferences. arXiv preprint arXiv:2011.15050.
Vazire, S. (2017). Quality uncertainty erodes trust in science. Collabra: Psychology, 3(1).
Waggoner, B. and Chen, Y. (2013). Information elicitation sans veriﬁcation. In Proceedings of the 3rd Workshop on Social Computing and User Generated Content (SC13).
Waggoner, B. and Chen, Y. (2014). Output agreement mechanisms and common knowledge. In HCOMP.
20

Wang, J. and Shah, N. B. (2018). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. arXiv preprint arXiv:1806.05085.
Witkowski, J., Bachrach, Y., Key, P., and Parkes, D. C. (2014). Dwelling on the negative: Incentivizing effort in peer prediction.
Witkowski, J. and Parkes, D. C. (2012a). Peer prediction without a common prior. In Proceedings of the 13th ACM Conference on Electronic Commerce, pages 964–981.
Witkowski, J. and Parkes, D. C. (2012b). A robust bayesian truth serum for small populations. Witkowski, J. and Parkes, D. C. (2013). Learning the prior in minimal peer prediction. In Proceedings
of the 3rd Workshop on Social Computing and User Generated Content at the ACM Conference on Electronic Commerce, volume 14. Xiao, Y., Dörﬂer, F., and Van Der Schaar, M. (2018). Incentive design in peer review: Rating and repeated endogenous matching. IEEE Transactions on Network Science and Engineering, 6(4):898–908. Xu, Y., Zhao, H., Shi, X., Zhang, J., and Shah, N. B. (2018). On strategyproof conference peer review. arXiv preprint arXiv:1806.06266. Zhao, S. (2012). Two proposals for a new publishing and reviewing system in computer science.
21

A Table of Notation

Symbol
N K T i k t ωi s(k·) sk s s−k Uk(·) Uk
bi ci b∗p ωp∗ δi ηi vk P
A,B,C Σt
ek (t) P Q X[kt]
gkt |t(x[kt]) Xˆ [kT ] θk
pˆk [T ]←k pˆ+k [T ]←k
rk Rk (t)
Rkrole(t)
Rkrotle(t)
∆Rk (t)
∆Rkrole(t)
∆Rkrotle(t)
Slog(·,·) I (·)

Description
Number of papers submitted to the mechanism Number of agents participating in the mechanism Number of criteria reviewers must evaluate a paper on Index for a paper Index for a reviewer Index for a criterion The i-th paper submitted to the mechanism
Agent k’s Stage I/II strategy Agent k’s full Stage I and Stage II strategy Full strategy proﬁle of all agents on Stage I and Stage II Strategy proﬁle of all agents except agent k
Agent k’s Stage I/II utility Agent k’s total utility
The bid accompanying paper ωi for a review slot The price charged for accepting paper ωi for review The p-th highest bid The paper with the p-th highest bid Boolean denoting whether paper ωi is accepted for publication after review Probability that paper ωi is accepted for publication conditional on review Agent k’s value for a paper accepted for publication The number of papers accepted for review
The reviewer triplet assigned to a given a paper Set of allowed review scores on criterion t Reviewer k’s cost for completing up to t criteria A reviewer triplet’s common prior over effort-informed scores A reviewer triplet’s joint distribution over reported scores and effort-informed scores
Reviewer k’s true review scores on criteria 1,...,t Reviewer k’s ‘best guess’ of their signal on criterion t given signals x[kt] (where t > t) Reviewer k’s (potentially strategically) reported review scores on criteria 1,...,T Transition matrix representing reviewer k’s signal reporting strategy Probability distribution of reviewer k’ ﬁrst prediction of k ’s scores on criteria 1,...,T Probability distribution of reviewer k’s second prediction of k ’s scores on criteria 1,...,T Reviewer k’s payment for reviewing Reviewer k’s expected payment for completing t criteria when peers complete T criteria and report truthfully Reviewer k’s expected expert/target payment for completing t criteria when peers complete T criteria and report truthfully Reviewer k’s expected expert/target payment on criterion t for completing t criteria when peers complete T criteria and report truthfully Reviewer k’s marginal expected payment for completing criterion t when peers complete T criteria and report truthfully Reviewer k’s marginal expected expert/target payment for completing criterion t when peers complete T criteria and report truthfully Reviewer k’s marginal expected expert/target payment on criterion t for completing criterion t when peers complete T criteria and report truthfully Log proper scoring rule Mutual information

22

B The 2014 NeurIPS Experiment
In 2014, program chairs of the NeurIPS conference sought to study the consistency of the peer review process, and set up an experiment where 10% of submitted papers underwent review by two independent committees. We describe the major ﬁndings below:
• Inconsistency of Reviews: About 26% of papers received different outcomes from the two committees (compared to 37.5% expected from random acceptance).
• Accept Precision: Of the accepted papers, only about 50% of papers would have survived re-review by an independent committee (compared to 25% expected from random acceptance).
• Reject Precision: Of the rejected papers, about 80% would have also been rejected in a re-review by an independent committee (compared to 75% expected from random rejection).
These ﬁndings showed that the accept/reject decisions were moderately better than pure chance, although still a fair bit noisy. Some of this is unavoidable in any highly selective mechanism seeking to allocate a limited number of spots to a large number of candidates on the basis of some noisily observed measure of ‘quality’ (Frank, 2016). When a mechanism seeks to accept some small percentage of top quality candidates, even a small amount of noise in observing the quality of a candidate can add a great deal of randomness to which candidates make the cut. Yet it will be the case that the bulk of the randomness will be faced by the candidates clustered around the cutoff (‘messy middle’); candidates well above the cutoff will likely be consistently accepted, and candidates well below the cutoff will likely be consistently rejected. Thus, the noise in observations of paper quality is an important factor affecting the randomness faced by authors in a highly selective acceptance process. Shah et al. (2018) ﬁnd that about 45% of NeurIPS 2015 papers and 30% of NeurIPS 2016 papers are in ‘messy middle’ where ‘inter-reviewer agreements are near random’. Back in the 2014 NeurIPS experiment, an analysis of the the independent committees’ ratings of paper quality, ﬁnds the following:
• Correlation of Quality Scores: The correlation between calibrated ‘quality’ scores assigned by the two independent committees (adjusted for a reviewers’ systematic biases) was 0.546.
• Citation Impact: For accepted papers, there was little correlation (0.051) between reviewers’ assessments of paper quality and papers’ log-citation count (likely due to the restricted range effect). For rejected papers, there was a positive correlation (0.22) between reviewers’ assessments of paper quality and papers’ log citation count.
The ﬁnding that about half the variance in scores is intrinsic to the paper (‘objective’) and half is idiosyncratic to the reviewer (‘subjective’) (Lawrence, 2021) provides more information on the noise in observations. This subjectivity can arise from more readily identiﬁable sources like differences in effort or expertise, or from truly subjective differences of perspective. Naturally, if reviewers exert different levels of effort or have differing levels of expertise, their reviews may be systematically different from each other and would weaken the ‘objective’ proportion of review scores. The correlation between quality scores and impact also suggests that the objective proportion of review scores has some better-than-random ability to predict the long-term impact of papers as measured by citations (relevant for criticism C4). In sum, the above results on accept/reject decisions as well as reviewers’ quality scores paint a picture of a peer review system that performs noticeably better than random, but with a lot of room for improvement.
These results provide a baseline against which to judge the performance of our proposed mechanism. The goal then is for our proposed mechanism to incentivize reviewers to coordinate into a high-effort equilibrium where the ‘subjectivity’ of reviewers’ scores is lower, the correlation between independent reviewers’ quality scores is higher, and the messy middle is smaller. The hypothesis is that the provision of incentives for effort during the review process may help reviewers better observe the quality of papers, and reduce the variance of decisions under the current selective acceptance process.
Finally, we note that conference selectiveness serves the dual purposes of focusing the community’s attention on major results (motivation M2), as well as building reputation for authors who do high-quality work (motivation M3). While Lawrence (2021) argues against using conference publications as a proxy for research quality or career advancement, decision-makers looking to make hiring decisions or determine what research to focus on will implicitly or explicitly be seeking relevant signals; in the absence of an alternative signal, we argue that it is important to ensure a peer-reviewed publication gives an appropriate signal of quality.
23

C Key elements of the Mutual Information Paradigm (Kong and Schoenebeck, 2019)
A scoring rule S : Σ×∆Σ → R is a function that accepts some outcome σ (from some signal space Σ) and a probability distribution δ (representing a forecast over that signal space ∆Σ) and scores the forecast with a real number. The scoring rule is proper if whenever the outcome σ is drawn from a distribution δ ∈ ∆Σ, the expected score Eσ[S(σ,·)] is maximized when the forecast is δ. The scoring rule is strictly proper if this maximum is unique. Our mechanism will use the log scoring rule Slog(σ,δ) = lnδ(σ), which is a strictly proper scoring rule. Next, recall that the Shannon Mutual Information of two random variables is deﬁned as the KL divergence of the product of their marginal distributions from the joint distribution: I(X;Y ) = DKL(P (X,Y )||P (X)⊗P (Y )). The conditional Shannon mutual information is given as I(X;Y |Z) = zP (z)DKL(P (X,Y |z)||P (X|z)⊗P (Y |z)) = zP (z)I(X;Y |z). Kong and Schoenebeck (2019) show an interesting connection between conditional Shannon mutual information and the log scoring rule:

I(X;Y |Z) = EX,Y,Z [Slog(Y,P (Y |X,Z))−Slog(Y,P (Y |Z))]

(10)

Finally, we present the important data processing inequality, typically useful in proving that deviating from truth-telling decreases payment. The inequality states that whenever a signal Y is manipulated in a way independent of another signal X, the manipulated signal can only have lower mutual information with X.

Lemma 3 (Strict Data Processing Inequality (Schoenebeck and Yu, 2020)). If (X,Y ) on a ﬁnite space X ×Y is stochastic relevant (i.e., for any distinct x,x ∈ X , P (Y |x) = P (Y |x )) and has full support, then for any random function θ : Y → Y where the randomness is independent of (X, Y), we have that I(X;θ(Y )) ≤ I(X;Y ). We have equality if and only if θ is an injective map.

Lemma 4 (Strict Conditional Data Processing Inequality (Schoenebeck and Yu, 2020)). If (W,X,Y ) on a ﬁnite space W ×X ×Y is second-order stochastic relevant (i.e., for any distinct x,x ∈ X , there is some w ∈ W such that P (Y |w,x) = P (Y |w,x )) and has full support, then for any random function θ : Y → Y where the randomness is independent of (W, X, Y), we have that I(X;θ(Y )|W ) ≤ I(X;Y |W ). We have equality if and only if θ is an injective map.

D Relationship to HMIM and TDPP
D.1 Relationship to HMIM
Here we discuss how our mechanism differs from the single-HMIM proposal under the HMIP framework (Kong and Schoenebeck, 2018).
• Compulsory Reports: Unlike single-HMIM, reviewers must provide scores for all criteria, not just the ones they have completed. This is currently the norm in peer review, and we would like to design the mechanism around this expectation. We model reviewers’ alternative to effortful reports as ‘best guess’ reports, as opposed to HMIM where a reviewer who does not complete a criterion does not report any signal and simply forgoes any payment on it. Realistically, a reviewer’s signals on completed criteria may enable them to make reasonable guesses on higher level criteria, and we account for this in computing reviewers’ marginal payments and setting hyperparameters.
• Partial Vs Total Order: Kong and Schoenebeck (2018) only impose a partial order on their criteria hierarchy to keep it general; we impose a total order for ease of exposition.
• Different Payment Scheme: While we adopt the hierarchical criteria structure from HMIP, the actual payments we provide are adopted from TDPP. However, at equilibrium, a reviewer’s expected target payment is almost the mutual information payment scheme proposed in HMIP, under our conditional independence assumption (Assumption 2). The difference is that while HMIP (under the Conditional Independence assumption) would compute the mutual information of the target’s and source’s signals on criterion t conditioned on the source’s signals on lower level criteria, our mechanism also conditions this on the expert’s private information.

24

• Strategy and Equilibria: Our mechanism, like single-HMIM, is strictly truthful. The ‘idealized’ HMIP has the desirable property that truthful reporting is a dominant strategy, but the practical single-HMIM algorithm does not have this property. Truthful reports are not a dominant strategy in our H-DIPP mechanism either, since target payments depend on the prediction strategy of experts. However, it is the case that a reviewer is paid greater target payments in truth-telling equilibria, and the use of proper scoring rules incentivizes experts to predict truthfully if targets deviate to truth-telling.
D.2 Relationship to TDPP
Here we discuss how our mechanism differs from the TDPP proposal (Schoenebeck and Yu, 2020).
• Playing All Roles: TDPP assigns each agent a single role (expert, target, or source). In our mechanism, every agent plays all three roles.
• Effort Model: TDPP assumes that signals are revealed to agents at no cost. We assume that the effort level determines the signals that are revealed to agents, so we adopt the hierarchical approach of HMIP to incentivize the effort needed to respond to higher level criteria.
• Knowledge of Common Prior: TDPP and other ‘detail-free’ mechanisms assume no knowledge of the common prior. While we do not strictly need to know a reviewer triplet’s common-prior to run the mechanism, we would like to know certain properties (the marginal payments from Lemma 1) of the common prior (as in HMIP) so that we can set the mechanism hyperparameters to incentivize the desired fully informative equilibrium.
• Multi-criteria Predictions and No Source Payment: TDPP only elicits source and target responses on a single question, while H-DIPP elicits responses to a hierarchy of criteria. H-DIPP reveals the source’s signal after each criterion (as opposed to revealing it all at once, or revealing target signals, etc.) as this brings the expected target payment close to the HMIP recommendation. However, this does mean that unlike TDPP, we do not provide any source payment for the following reason. The TDPP expected source payment was based on the expert k’s ﬁrst prediction of target k ’s signals on a criterion t (made without the source’s information). This term did not change the source’s incentives in the original TDPP (as their actions could not affect the expert’s ﬁrst prediction), but it was a convenient accounting trick to make the ex ante agent welfare equal to the conditional mutual information of the source and target’s reports. However, we cannot incorporate such a source payment in H-DIPP: the expert makes predictions with the source’s signals on lower level criteria, so the source can inﬂuence the expert’s ﬁrst prediction on every criterion t > 1. Thus, including such a source payment could lead the source to strategically misreport so that the expert’s ﬁrst predictions are poor. Naturally, leaving out source payments preserves source’s incentives to report truthfully, but the ex ante reviewer welfare is no longer the conditional mutual information. This is why there are ‘extra’ entropy terms in Lemma 2.
• Strategy and Equilibria: Like TDPP, our H-DIPP mechanism is strictly truthful. However, TDPP has the additional very desirable property of being strongly truthful, i.e., the truth-telling equilibrium pays more than all other equilibria, so agents have no reason to coordinate into other equilibria. Owing to the multi-round structure of H-DIPP, we cannot quite make the same guarantee; it may be possible to have equilibria where reviewers strategically misreport to increase a peer’s target payment by more than the loss of their target payment thereby increasing aggregate payment, although there is no individual incentive to do so. It may be possible for all reviewers to do this so their individual and aggregate payments are higher than the fully informative equilibrium, although accepting the increased target payment bestowed by a source’s strategic misreports and not reciprocating by reporting truthfully would lead to a higher-paying equilibrium for the target. Investigating such equilibria is an important direction for future work. Nevertheless, we can make the weaker claim that the fully informative equilibrium pays more than an effortless uninformative equilibrium (Theorem 3).
E Proofs
Theorem 1 (Strictly Truthful) Suppose reviewers A, B, C complete [tA], [tB], and [tC ] criteria respectively. Then, on shared complete criteria [t ] = [min(tA,tB,tC )], the H-DIPP mechanism is
25

strictly truthful, i.e., all reviewers honestly reporting their true predictions and private review scores on shared complete criteria [t ] is a strict Bayesian Nash equilibrium.
Proof of Theorem 1. We adapt the proof of strict truthfulness of TDPP by Schoenebeck and Yu (2020) for H-DIPP. We show that agents reporting their honest predictions and scores on shared complete criteria is a strict Bayesian Nash equilibrium; importantly, payments for each criterion are independent and can be strategized about independently, hence we can reason strictly about strategies on the shared complete criteria x[kt ] ⊆ x[ktk]. Reviewer A’s private signals on incomplete criteria t > tA are xtA = gAt|tA (x[AtA]) for some best guess function g, and similarly for reviewers B and C. Since all reviewers play the role of expert, target, and source, we only analyze reviewer A’s incentives when peers B and C report truthfully; these incentives are symmetrical for all reviewers. Additionally, we hold the number of completed criteria and hence effort cost ek(tk) constant, and look to optimize payoff under this condition.
Suppose reviewers B and C report their private signals truthfully, i.e., xˆtB = xtB and xˆtC = xtC , and reviewer C as expert truthfully predicts reviewer A’s score as the posterior from updating the common prior pˆAt←C = P(XAt |x[Ct ],x[Bt−1]), and pˆ+At←C = P(XAt |xC[t ],x[Bt]).
Now, reviewer A’s expected payment (taken with respect to the joint distribution Q(Xˆ [AT ], X[At ], X[Bt ], X[Ct ]) of true signals and A’s potentially strategic reports) on the shared complete criteria can be written as:









EQ[rAt ] = αAt E[Slog(xtB ,pˆBt←A)+Slog(xtB ,pˆ+Bt←A)+βAt ESlog(xˆtA,P(XAt |x[Ct ],x[Bt]))−Slog(xˆtA,P(XAt |xC[t ],xB[t−1]))

expert payment

target payment (11)

Now, reviewer A’s payment on a given criterion t is independent of their reported signals and predictions on other criteria, so the sum of payments on complete criteria can be maximized by maximizing the expert payment and target payment on each criterion:
Expert Payment The expected expert payment consists of reviewer A’s predictions of their target B’s signal on each criterion t ﬁrst with the source C’s signals on criteria less than t and then with C’s signal on criterion t as well. Predictions on each criterion are scored independently, so a strategic prediction on one criterion cannot impact future payments on other criteria. Since we use a strictly proper scoring rule to score both predictions on each criterion, reviewer A’s expected payoff for each criterion is strictly maximized when they report their true posterior belief over their target B’s signals under the common prior, i.e., pˆ+Bt←A = P(XBt |xA[tA],x[Ct]) and pˆBt←A = P(XBt |x[AtA],xC[t−1]).
Target Payment Now, consider the reviewer A’s expected target payment on criteria t ≤ t . We observe that we can model the strategies for responses to different criteria independently since the target payments are also independent for different criteria. Let xˆtA = θAt (x[AtA]) be reviewer A’s deterministic best response report (on criterion t) to reviewer B’s honest signal reports xˆ[BT ] and reviewer C’s honest (posterior) predictions pˆ+At←C = P(XAt |x[CtC],x[Bt]), pˆAt←C = P(XAt |xC[tC],xB[t−1]).
For t ≤ t , reviewer A’s expected target payment for playing this strategy xˆtA = θAt (x[At ]) is:

ut(θ) : = E Slog(xˆtA,P(XAt |x[CtC ],x[Bt]))−Slog(xˆtA,P(XAt |x[CtC ],x[Bt−1]))

= EQ = EQ = EP

log P(xˆtA|x[CtC ],x[Bt]) P(xˆtA|x[CtC ],x[Bt−1])
log P(xˆtA,xtB |x[CtC ],x[Bt−1]) P(xˆtA|x[CtC ],x[Bt−1])P(xtB |x[Ct ],x[Bt−1])
log P(xtB |x[CtC ],x[Bt−1],θAt (x[AtA])) P(xtB |x[CtC ],x[Bt−1])

26

(12)

By a similar calculation, we have that reviewer A’s expected payment under a truth-telling strategy

is ut(τ ) := EP log P(Px(tBx|tx|[CxtC[t]C,x],[Bxt−[t1−],1x])tA) . Then, the difference between truth-telling payment and

BC

B

the best response payment is:

P(xtB |x[CtC ],xB[t−1],xtA) ut(τ )−ut(θ) = EP log P(xtB|x[CtC],x[Bt−1],θAt (x[AtA]))

= EX[AtA],X[Bt−1],X[CtC ]

EX t

log P(xtB |x[CtC ],xB[t−1],xtA) P(xtB |xC[tC ],xB[t−1],θAt (x[AtA]))

X[AtA] = x[AtA],XB[t−1] = x[Bt−1],X[CtC ] = x[CtC ]

= EX[AtA],XB [t−1],X[CtC ] KL P(xtB |x[CtC ],xB[t−1],xtA)||P(xtB |xC[tC ],x[Bt−1],θAt (x[AtA])) (13)

Since KL is a divergence, it is greater than or equal to zero, so we must have ut(τ )−ut(θ) ≥ 0. Finally,

observe

that

if

the

strategy

θ

is

not

truthful,

i.e.,

θA

t

(x

[tA A

]

)

=

x

t A

,

then

by

the

second-order

stochastic

rel-

evance there is some xtC ⊂ x[CtC] for which P(xtB|x[CtC],x[Bt−1],xtA) = P(xtB|x[CtC],x[Bt−1],xˆtA), so the KL

divergence is not identically zero over all signals in the expectation. Consequently, the inequality is strict,

implying u(τ ) > u(θ). Thus, the best response strategy is truthtelling (θ = τ ), and reviewer A strictly

maximizes their target payment by truthfully reporting their signal xˆtA = xtA on all complete criteria.

We have shown that when peers report their predictions and private review scores truthfully, a reviewer should truthfully report their prediction to strictly maximize their expert payment and truthfully report their private review score to strictly maximize their target payment. These incentives are symmetrical for reviewers B and C, so truthful reporting is a strict Bayesian Nash Equilibrium.

Lemma 1 Suppose reviewers B, C complete all criteria and truthfully report their private review scores and predictions. Then, reviewer A’s marginal expected payment for completing criterion tA and honestly reporting private review scores and predictions is ∆RA(tA) := RA(tA)−RA(tA −1) = ∆RAexpert(tA)+∆RAtarget(tA), where:

1. ∆RAexpert(tA) := RAexpert(tA)−RAexpert(tA −1) = 2. ∆RAtarget(tA) := RAtarget(tA)−RAtarget(tA −1) =

Tt=1αAt ∆RAextpert(tA) Tt=1βAt ∆RAtartget(tA)

3. ∆RAextpert(tA) := I XBt ;XAtA |X[AtA−1],X[Ct] +I XBt ;XAtA |XA[tA−1],X[Ct−1]

 0, t < tA



4. ∆RAtartget(tA) := I XBtA ;XAtA |X[CT ],X[BtA−1] −I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1] ,

t = tA

 E

KL

P(XBt |gAt|tA (X[AtA]),X[CT ],X[Bt−1])||P(XBt |gAt|tA−1(X[AtA−1]),X[CT ],X[Bt−1])

,

t > tA

Furthermore, the marginal expected expert and target payments on each criterion are always nonnegative, i.e., ∆RAextpert(tA),∆RAtartget(tA) ≥ 0 for all t,tA ∈ [T ] and the inequality is strict when t = tA. Similar results hold for reviewers B and C when peers complete all criteria and report truthfully.

Proof. Assuming peers B, C complete all T criteria and report truthfully, we can

write reviewer A’s strategy proﬁle of effort eA(tA) and truthful reports as sA =

eA(tA),x[ktA] ∪{gAt |tA (x[AtA])}t

∈

[

tA

+1,T

]

,{P

(XBt

|

X

[tA A

]

,X

[t] C

),P

(XBt

|

X

[tA A

]

,X

[t− C

1]

)}t

∈

[T

]

,

and the expected payment is:

T
RA = αAt EX[AtA],XBt ,X[Ct] log P(XBt |X[AtA],X[Ct]) +log P(XBt |X[AtA],X[Ct−1]) t=1

T
+ βAt E t [t] [T ] log XA,XB ,XC t=1

RA expert
P(XAt |X[CT ],X[Bt]) P(XAt |X[CT ],X[Bt−1])

RA target

(14)

27

The expected payment RA is technically a function of all reviewers’ completed criteria RA(tA,tB,tC ),
but we simply write it as RA(tA) as other reviewers’ are assumed to have completed T criteria.
The marginal payment for completing criterion tA is RA(tA) − RA(tA − 1) = ∆RA(tA) = ∆RAexpert(tA)+∆RAtarget(tA). The marginal expert payment ∆RAexpert(tA) is:

∆RAexpert(tA) = RAexpert(tA)−RAexpert(tA −1)

T
= αAt
t=1

EX[AtA],XBt ,X[Ct] log P(XBt |X[AtA],X[Ct])

+log

T
− αAt
t=1

EX[AtA−1],XBt ,X[Ct] log P(XBt |X[AtA−1],X[Ct])

T
= αAt
t=1

EX[AtA],XBt ,X[Ct] log P(XBt |X[AtA],X[Ct])

−log

P(XBt |XA[tA],X[Ct−1]) +log P(XBt |X[AtA−1],X[Ct−1]) P(XBt |XA[tA−1],X[Ct])

+EX[AtA],XBt ,X[Ct−1] log P(XBt |X[AtA],X[Ct−1]) −log P(XBt |X[AtA−1],X[Ct−1])

T
= αAt
t=1

EX[AtA],XBt ,X[Ct] log

P(XBt |X[AtA],X[Ct]) P(XBt |X[AtA−1],X[Ct])

+EX[AtA],XBt ,XC[t−1] log

T
= αAt I XBt ;XAtA |X[AtA−1],X[Ct] +I XBt ;XAtA |X[AtA−1],X[Ct−1]
t=1

P(XBt |X[AtA],X[Ct−1]) P(XBt |X[AtA−1],XC[t−1])
(15)

Deﬁning ∆RAextpert(tA) := I

XBt

;XAtA

|X

[tA A

−1]

,X

[t] C

+I

XBt

;XAtA

|X

[tA A

−1]

,X

[t− C

1]

, we have

∆RAexpert(tA) =

T t=1

αAt

∆

RAextpert

(t

A

).

Next,

we compute the marginal target payment as follows

(splitting the sum over T criteria into three: marginal target payment on criteria less than tA, criterion

tA, and criteria greater than tA):

∆RAtarget(tA) = RAtarget(tA)−RAtarget(tA −1)

tA −1

=

βAt

t=1

E log

P(XAt |X[CT ],X[Bt]) P(XAt |X[CT ],X[Bt−1])

−E log

P(XAt |X[CT ],X[Bt]) P(XAt |X[CT ],X[Bt−1])

+ βAtA

E log

P(XAtA |X[CT ],X[BtA]) P(XAtA |X[CT ],X[BtA−1])

−E log

P(gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA]) P(gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1])

T

+

βAt

t=tA +1

E log

P(gAt|tA (X[AtA])|X[CT ],X[Bt]) P(gAt|tA (X[AtA])|X[CT ],X[Bt−1])

−E log

P(gAt|tA−1(X[AtA−1])|X[CT ],X[Bt]) P(gAt|tA−1(X[AtA−1])|X[CT ],X[Bt−1])

= βAtA I XBtA ;XAtA |X[CT ],X[BtA−1] −I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1]

T

+

βAt

t=tA +1

E log P(XBt |gAt|tA (X[AtA]),X[CT ],X[Bt−1]) P(XBt |X[CT ],X[Bt−1])

−E log P(XBt |gAt|tA−1(X[AtA−1]),X[CT ],XB[t−1]) P(XBt |X[CT ],X[Bt−1])

= βAtA I XBtA ;XAtA |X[CT ],XB[tA−1] −I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1]

T

+

βAt

t=tA +1

EX[tA],X[t],X[T ]

A

BC

log P(XBt |gAt|tA (X[AtA]),X[CT ],X[Bt−1]) P(XBt |gAt|tA−1(X[AtA−1]),X[CT ],X[Bt−1])

= βAtA I XBtA ;XAtA |X[CT ],XB[tA−1] −I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1]

T

+

βAt

t=tA +1

EX[AtA],X[Bt−1],X[CT ] KL P(XBt |gAt|tA (X[AtA]),X[CT ],X[Bt−1])||P(XBt |gAt|tA−1(X[AtA−1]),X[CT ],XB[t−1]) (16)

28

Deﬁning ∆RAtatrget(tA) = 0 for t < tA, ∆RAtatrgAet(tA) = I XBtA ;XAtA |X[CT ],XB[tA−1] −

I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1] , and for t > tA as ∆RAtartget =

E

KL

P(

XBt

|gAt|t

A

(X

[tA A

]

)

,X

[T C

]

,X

[t− B

1]

)||P

(XBt

|gAt|t

A

−

1

(

X

[tA A

−

1]

),X

[T C

]

,

X

[t−1] B

)

,

we get

∆RAtarget(tA) = Tt=1βAt ∆RAtartget(tA). Thus, we see that the marginal expected payment is:

RA(tA)−RA(tA −1) = ∆RA(tA)
= ∆RAexpert(tA)+∆RAtarget(tA)
T
= αAt ∆RAextpert(tA)+βAt ∆RAtartget(tA)
t=1

(17)

Finally

we

show

non-negativity

of

marginal

expected

expert

and

target

payments

∆

R

expert k

,∆

R

target k

≥

0,

and positivity on criterion tA when the marginal criterion completed is tA, i.e., ∆RAextpAert(tA) > 0 and

∆RAtartgAet(tA) > 0.

We easily get the non-negativity of the marginal expected expert payment ∆RAextpert(tA) ≥ 0 from the non-negativity of mutual information. Now consider the marginal expected

expert payment on criterion tA in particular (∆RAextpAert(tA)). By second order stochas-

tic relevance (Assumption 3), for any xtAA = x˜tAA there is some xtCA ⊂ x[CtA] such that

P

(XBtA

|x

tA A

,

xA[tA−1],

x[CtA])

=

P

(XBtA

|x˜

tA A

,

xA[tA−1],

xC[tA])

⇒

XBtA

⊥⊥

XAtA

|

XA[tA−1], X[CtA].

Thus, I XBtA ;XAtA |X[AtA−1],X[CtA] > 0. This gives us that ∆RAextpAert(tA) > 0 for all tA ∈ [T ].

Next, we consider the marginal expected target payment. By the non-negativity of mutual information and KL divergence, we have that ∆RAtartget(tA) ≥ 0. We show that the inequality is strict when t = tA, i.e., ∆RAtartgAet(tA) = I XBtA ;XAtA |X[CT ],XB[tA−1] −I XBtA ;gAtA|tA−1(XA[tA−1])|X[CT ],XB[tA−1] > 0 as
follows:

I XBtA ;gAtA|tA−1(X[AtA−1])|X[CT ],X[BtA−1] ≤ I XBtA ;X[AtA−1]|X[CT ],X[BtA−1]

= I XBtA ;X[AT ]|X[CT ],X[BtA−1] −I XBtA ;X[AtA,T ]|X[AtA−1],X[CT ],X[BtA−1]

= I XBtA ;XAtA |X[CT ],X[BtA−1] −I XBtA ;X[AtA,T ]|X[AtA−1],X[CT ],X[BtA−1]

< I XBtA ;XAtA |X[CT ],X[BtA−1]

(18)

For the ﬁrst inequality, we used the information monotonicity of mutual information. For the equality in the second step, we used the chain rule of mutual information. For the equality in the third step, we use the conditional independence assumption (Assumption 2). For the ﬁnal inequality, we used the fact that second-order stochastic relevance (Assumption 3) implies that for any xtAA = x˜tAA there is some xtCA ⊂ x[CT ] such that P(XBtA |xtAA ,x[AtA−1],x[CT ],x[BtA−1]) = P(XBtA |x˜tAA ,x[AtA−1],x[CT ],x[BtA−1]) ⇒ XBtA ⊥⊥ XAtA | X[AtA−1],X[CT ],X[BtA−1] ⇒ I XBtA ;XA[tA,T ]|X[AtA−1],X[CT ],X[BtA−a] > 0.
The above derivation applies similarly for reviewer B when peers A,C complete all criteria report truthfully, and for reviewer C when peers A,B complete all criteria and report truthfully.
29

Theorem 2 Suppose mechanism hyperparameters {α[AT ],β[AT ],α[BT ],β[BT ],α[CT ],β[CT ]} are solutions to the following linear program:

T

min

αkt Rkextpert(T )+βkt Rktatrget(T )

k∈{A,B,C} t=1

T
s.t. αAt ∆RAextpert(tA)+βAt ∆RAtartget(tA) > ∆eA(tA) for 1 ≤ tA ≤ T
t=1
T
αBt ∆RBexptert(tB )+βBt ∆RBtartget(tB ) > ∆eB (tB ) for 1 ≤ tB ≤ T
t=1
T
αCt ∆RCexptert(tC )+βCt ∆RCtartget(tC ) > ∆eC (tC ) for 1 ≤ tC ≤ T
t=1
αkt ,βkt ≥ 0

(19)

Then, for any reviewer k, if their peers complete all criteria and report predictions and review scores truthfully, reviewer k strictly maximizes their expected utility by also completing all criteria and reporting predictions and review scores truthfully. In other words, completing all criteria and reporting truthfully is a strict Bayesian-Nash equilibrium. We refer to this equilibrium as the fully-informative equilibrium.

Proof. We require the hyperparameters to be non-negative so reviewers are paymented and not penalized for maximizing their expert and target payments. When reviewer k’s peers are completing all criteria and reporting truthfully, we want the mechanism to pay enough to cover reviewer k’s cost of effort and incentivize them to complete all criteria and report truthfully as well. By Theorem 1, we know that truthful reporting on shared completed criteria is a strict BNE, so when a reviewer k’s peers complete all criteria and report truthfully, every additional criterion completed by reviewer k becomes a shared complete criterion and their payoff is maximized by truthfully reporting the review score on the marginal criterion. Thus, the question is simply how to set the hyperparameters to incentivize completion of all criteria (when peers complete all criteria and report truthfully). We analyse reviewer A’s incentives when reviewers B,C complete all criteria and report truthfully, and this applies symmetrically to reviewers B,C.

Reviewer A’s expected utility for completing tA criteria and reporting signals truthfully is

UA = RA(tA) − eA(tA), so to ensure that the mechanism always covers the cost of effort, we want

the marginal utility of completing an additional criterion tA to be positive, i.e., RA(tA)−eA(tA) >

RA(tA −1)−eA(tA −1) for all 1 ≤ tA ≤ T . Rearranging this, we can write RA(tA)−RA(tA −1) >

eA(tA)−eA(tA −1) ⇒ ∆RA(tA) > ∆eA(tA). ∆eA(tA) is the marginal cost of completing criterion

tA, and by Equation 17, the marginal expected payment for completing criterion tA when peers com-

plete all criteria is ∆RA(tA) = Tt=1αAt ∆RAextpert(tA)+βAt ∆RAtartget(tA). Thus, if the hyperparameters

{α[AT ],β[AT ]} satisfy

T t=1

αAt ∆RAextpert(tA)

+

βAt ∆RAtartget(tA)

>

∆eA(tA)

for

all

1

≤

tA

≤

T

,

then

reviewer A maximizes their utility by completing all criteria and reporting truthfully. Satisfying these

constraints is feasible since ∆RAextpert(tA),RAtartget(tA) > 0 for t = tA as shown in Lemma 1.

By the same argument, we desire {α[BT ], β[BT ]} satisfy ∆RB(tB) > ∆eB(tB), and {α[CT ], β[CT ]} satisfy ∆RC (tC ) > ∆eC (tC ). Then, for any reviewer k, if their peers complete all criteria and report truthfully, their marginal utility of completing an additional criterion is always positive, so reviewer k strictly maximizes their expected utility by also completing all criteria and reporting truthfully. Hence, completing all criteria and reporting truthfully is a strict Bayesian-Nash equilibrium.

In setting the hyperparameters, we also wish to minimize the mechanism’s cost for having a paper reviewed, so we wish to minimize the total payout RA(T ) + RB(T ) + RC (T ) = min k∈{A,B,C} Tt=1αkt Rkextpert(T )+βkt Rktatrget(T ). The optimal choice of hyperparameters is found by minimizing the mechanism’s cost under the aforementioned constraints by solving the given linear
program where the constraints are modiﬁed to a weak inequality, and adding an to the solution of
the LP to strictly satisfy the constraints.

30

Lemma 2 Let sk = ek(tk),xˆ[kT ],pˆ+k [T]←k,pˆk [T]←k be reviewer k’s strategy, where k is reviewer

k’s target. If s = (sA,sB,sC ) is an equilibrium strategy proﬁle, reviewer A’s expected payment is:

T
E[rA] = αAt −E H Q XˆBt |x[AtA],xˆ[Ct]
t=1

−E H Q XˆBt |x[AtA],xˆ[Ct−1]

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ [Bt−1] (20)

(expected payment for peers B,C computed analogously). Reviewers’ total expected payment is:

T
E[rA +rB +rC ] = αAt −E H Q XˆBt |x[AtA],xˆ[Ct]
t=1

−E H Q XˆBt |x[AtA],xˆC[t−1]

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ [Bt−1]

+αBt −E H Q XˆCt |xB[tB],xˆ[At] −E H Q XˆCt |x[BtB],xˆ[At−1]

+βBt I XˆCt ;XˆBt |X[AtA],Xˆ [Ct−1]

+αCt −E H Q XˆAt |x[CtC ],xˆ[Bt] −E H Q XˆAt |x[CtA],xˆ[Bt−1]

+βCt I XˆAt ;XˆCt |X[BtB ],Xˆ [At−1] (21)

Proof of Lemma 2. First, we observe that since s is an equilibrium strategy proﬁle, all reviewers’ predictions are truthful, i.e., pˆ+Bt←A = Q XˆBt |X[AtA],Xˆ [Ct] and pˆBt←A = Q XˆBt |X[AtA],Xˆ [Ct−1] , and so on for the other reviewers’ expert predictions. Predictions are truthful in any equilibrium as otherwise reviewers could unilaterally improve their expert payment under the strictly proper log scoring rule by deviating to a truthful prediction. The expected payment for reviewers (expectation of Equation 5) for criterion t is: EQ[rAt ] = E[αAt Slog(xˆtB ,pˆ+Bt←A)+Slog(xˆtB ,pˆBt←A) +βAt Slog(xˆtA,pˆ+At←C )−Slog(xˆtA,pˆAt←C ) ] EQ[rBt ] = E[αAt Slog(xˆtC ,pˆ+Ct←B )+Slog(xˆtC ,pˆCt←B ) +βBt Slog(xˆtB ,pˆ+Bt←A)−Slog(xˆtB ,pˆBt←A) ] EQ[rCt ] = E[αCt Slog(xˆtA,pˆ+At←C )+Slog(xˆtA,pˆAt←C ) +βCt Slog(xˆtC ,pˆ+Ct←B )−Slog(xˆtC ,pˆCt←B ) ]
(22)
Now, observe that reviewer A’s expected expert payment (ﬁrst two terms in expectation) on some criterion t can be written as follows:

αAt

E[

Slog

(

xˆtB

,

pˆ +B

t

←

A

)

+

Slog

(xˆ

t B

,pˆ

B

t

←

A

)]

=

αAt

E

Q

XˆBt |X[AtA],Xˆ [Ct]

+Q

XˆBt |X[AtA],Xˆ [Ct−1]

= −E [tA] ˆ [t] H Q XˆBt |x[AtA],xˆ[Ct] XA ,XC

−EX[tA],Xˆ [t−1]

A

C

HQ (23)

XˆBt |x[AtA],xˆ[Ct−1]

where H(·) is the Shannon entropy. For reviewer A’s target payment (last two terms in expectation), we can use Equation 10 to write:

E

Q

[

Slog

(

xˆ

t A

,

pˆ +At

←

C

)

−

S

log

(

xˆtA

,pˆ A

t

←C

)]

=

I

XˆBt ;XˆAt |X[CtC ],Xˆ B[t−1]

(24)

Remark 2. Each reviewer’s target payment on criterion t is exactly the HMIP payment for that criterion under the conditional independence assumption (Assumption 4.3 in Kong and Schoenebeck (2018)).

We can then write reviewer A’s total expected payment on criterion t as follows (and the same applies to B and C):

EQ[rAt ] = αAt −E [tA] ˆ [t] H Q XˆBt |x[AtA],xˆ[Ct] XA ,XC

−E [tA] ˆ [t−1] H Q XˆBt |x[AtA],xˆ[Ct−1] XA ,XC

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ [Bt−1] (25)

Making the analogous computation for reviewers B and C, we can compute the total expected payment for the reviewers on a single criterion t:

EQ[rAt +rBt +rCt ] = αAt −E H Q XˆBt |x[AtA],xˆ[Ct] −E H Q XˆBt |x[AtA],xˆ[Ct−1]

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ [Bt−1]

+αBt −E H Q XˆCt |x[BtB],xˆ[At] −E H Q XˆCt |x[BtB],xˆ[At−1]

+βBt I XˆCt ;XˆBt |X[AtA],Xˆ [Ct−1]

+αCt −E H Q XˆAt |x[CtC ],xˆ[Bt] −E H Q XˆAt |x[CtA],xˆ[Bt−1]

+βCt I XˆAt ;XˆCt |X[BtB ],Xˆ [At−1] (26)

31

Thus, the total expected payment for the reviewers for all criteria is:





T

EQ[rA +rB +rC ] = E

rkt 

k∈{A,B,C} t=1

T
= αAt −E H Q XˆBt |x[AtA],xˆ[Ct]
t=1
+αBt −E H Q XˆCt |x[BtB],xˆ[At]

−E H Q XˆBt |x[AtA],xˆ[Ct−1] −E H Q XˆCt |x[BtB],xˆ[At−1]

+αCt −E H Q XˆAt |x[CtC ],xˆ[Bt] −E H Q XˆAt |x[CtA],xˆ[Bt−1]

+βAt I XˆBt ;XˆAt |X[CtC ],Xˆ B[t−1]
+βBt I XˆCt ;XˆBt |X[AtA],Xˆ [Ct−1] +βCt I XˆAt ;XˆCt |X[BtB ],Xˆ A[t−1]
(27)

Theorem 3

(a) In the highest paying uninformative equilibrium (where all reviewers exert zero effort), every reviewer’s expected payment (and hence aggregate reviewer welfare) is zero.

(b) Suppose that in addition to the constraints speciﬁed in Theorem 2, the mechanism hyperpa-

rameters also satisfy Rk(T )−ek(T ) > 0 ⇔

T t=1

αkt

Rkextpert(T

)

+

βkt

Rktatrget(T

)

>

ek

(T

)

for

k ∈ {A,B,C}, i.e., reviewers’ expected utility in the fully-informative equilibrium is positive.

Then, it is individually rational to participate in the mechanism and complete all criteria

and report truthfully, given peers are doing the same. Additionally, the fully-informative

equilibrium has higher individual and aggregate utility than the uninformed equilibrium.

Proof of Theorem 3.

(a) When all reviewers exert zero effort, their reports are not sampled from the common prior. Instead, they may report some ﬁxed a priori most likely sequence of signals. In such an equilibrium, expert predictions without the source will be perfect and the updated predictions will be the same, as the expert can simply predict the constant signals with complete certainty. Thus, the entropy of expert predictions is minimized and expert payments will be 0. Since expert predictions will not change, all target payments will also be 0 in expectation. Intuitively, this is because the signals do not come from the common prior, and do not contain information about other reviewers’ signals.
In any other uninformative equilibrium, expected target payments will still be zero since mutual information of uninformed signals not drawn from the common prior will be zero. However, the expert payment can only decrease (the maximum possible expert payment is zero). Thus, in the highest paying uninformative equilibrium, reviewers individually and in aggregate are paid nothing in expectation.

(b) It is individually rational for a reviewer to participate in the mechanism when their expected

utility is positive. We want individual rationality under our desired fully informative

equilibria, so we achieve positive expected utility by requiring Rk(T ) − ek(T ) > 0 or

equivalently

T t=1

αkt

Rkextpert

(T

)

+

β

t k

R

target kt

(

T

)

>

e

k

(T

).

Denoting the expected utility of some arbitrary uninformative equilibrium as Rk(0,0,0), we observe that Rk(T ) > ek(T ) > 0 ≥ Rk(0, 0, 0), thus the individual utility of the fully-informative equilibrium is higher than any uninformative equilibrium. This holds for all reviewers, so the aggregate utility of the fully-informative equilibrium is also higher than the uninformative equilibrium.

32

