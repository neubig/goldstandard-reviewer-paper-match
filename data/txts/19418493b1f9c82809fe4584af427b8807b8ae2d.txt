Automatic Extraction of Commonsense LocatedNear Knowledge
Frank F. Xu‚àó Bill Yuchen Lin‚àó Kenny Q. Zhu Department of Computer Science and Engineering
Shanghai Jiao Tong University Shanghai, China
{frankxu, yuchenlin}@sjtu.edu.cn, kzhu@cs.sjtu.edu.cn

arXiv:1711.04204v3 [cs.CL] 13 May 2018

Abstract
LOCATEDNEAR relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classiÔ¨Åer and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.
1 Introduction
ArtiÔ¨Åcial intelligence systems can beneÔ¨Åt from incorporating commonsense knowledge as background, such as ice is cold (HASPROPERTY), chewing is a sub-event of eating (HASSUBEVENT), chair and table are typically found near each other (LOCATEDNEAR), etc. These kinds of commonsense facts have been used in many downstream tasks, such as textual entailment (Dagan et al., 2009; Bowman et al., 2015) and visual recognition tasks (Zhu et al., 2014). The commonsense knowledge is often represented as relation triples in commonsense knowledge bases, such as ConceptNet (Speer and Havasi, 2012), one of the largest commonsense knowledge graphs available today. However, most commonsense knowledge bases are manually curated or crowd-sourced by community efforts and thus do not scale well.
This paper aims to automatically extract the commonsense LOCATEDNEAR relation between physical objects from textual corpora. LOCATEDNEAR is deÔ¨Åned as the relationship between two objects typically found near each other in real life. We focus on LOCATEDNEAR relation for these reasons:
1. LOCATEDNEAR facts provide helpful prior knowledge to object detection tasks in com-
‚àóBoth authors contributed equally.

Figure 1: LOCATEDNEAR facts assist the detection of vague objects: if a set of knife, fork and plate is on the table, one may believe there is a glass beside based on the commonsense, even though these objects are hardly visible due to low light.
plex image scenes (Yatskar et al., 2016). See Figure 1 for an example. 2. This commonsense knowledge can beneÔ¨Åt reasoning related to spatial facts and physical scenes in reading comprehension, question answering, etc. (Li et al., 2016) 3. Existing knowledge bases have very few facts for this relation (ConceptNet 5.5 has only 49 triples of LOCATEDNEAR relation). We propose two novel tasks in extracting LOCATEDNEAR relation from textual corpora. One is a sentence-level relation classiÔ¨Åcation problem which judges whether or not a sentence describes two objects (mentioned in the sentence) being physically close by. The other task is to produce a ranked list of LOCATEDNEAR facts with the given classiÔ¨Åed results of large number of sentences. We believe both two tasks can be used to automatically populate and complete existing commonsense knowledge bases. Additionally, we create two benchmark datasets for evaluating LOCATEDNEAR relation extraction

systems on the two tasks: one is 5,000 sentences each describing a scene of two physical objects and with a label indicating if the two objects are co-located in the scene; the other consists of 500 pairs of objects with human-annotated scores indicating conÔ¨Ådences that a certain pair of objects are commonly located near in real life.1
We propose several methods to solve the tasks including feature-based models and LSTM-based neural architectures. The proposed neural architecture compares favorably with the current state-ofthe-art method for general-purpose relation classiÔ¨Åcation problem. From our relatively smaller proposed datasets, we extract in total 2,067 new LOCATEDNEAR triples that are not in ConceptNet.
2 Sentence-level LOCATEDNEAR Relation ClassiÔ¨Åcation
Problem Statement Given a sentence s mentioning a pair of physical objects <ei, ej>, we call <s, ei, ej> an instance. For each instance, the problem is to determine whether ei and ej are located near each other in the physical scene described in the sentence s. For example, suppose ei is ‚Äúdog‚Äù, ej is ‚Äúcat‚Äù, and s = ‚ÄúThe King puts his dog and cat on the table.‚Äù. As it is true that the two objects are located near in this sentence, a successful classiÔ¨Åcation model is expected to label this instance as True. However, if s2 = ‚ÄúMy dog is older than her cat.‚Äù, then the label of the instance <s2, ei, ej> is False, because s2 just talks about a comparison in age. In the following subsections, we present two different kinds of baseline methods for this binary classiÔ¨Åcation task: feature-based methods and LSTM-based neural architectures.
2.1 Feature-based Methods
Our Ô¨Årst baseline method is an SVM classiÔ¨Åer based on following features commonly used in many relation extraction models (Xu et al., 2015):
1. Bag of Words (BW): the set of words that ever appeared in the sentence.
2. Bag of Path Words (BPW): the set of words that appeared on the shortest dependency path between objects ei and ej in the dependency tree of the sentence s, plus the words in the two subtrees rooted at ei and ej in the tree.
3. Bag of Adverbs and Prepositions (BAP): the existence of adverbs and prepositions in the
1https://github.com/adapt-sjtu/ commonsense-locatednear

sentence as binary features. 4. Global Features (GF): the length of the sen-
tence, the number of nouns, verbs, adverbs, adjectives, determiners, prepositions and punctuations in the whole sentence. 5. Shortest Dependency Path features (SDP): the same features as with GF but in dependency parse trees of the sentence and the shortest path between ei and ej, respectively. 6. Semantic Similarity features (SS): the cosine similarities between the pre-trained GloVe word embeddings (Pennington et al., 2014) of the two object words. We evaluate linear and RBF kernels with different parameter settings, and Ô¨Ånd the RBF kernel with {C = 100, Œ≥ = 10‚àí3} performs the best overall.
2.2 LSTM-based Neural Architectures
We observe that the existence of LOCATEDNEAR relation in an instance <s,e1,e2> depends on two major information sources: one is from the semantic and syntactical features of sentence s and the other is from the object pair <e1,e2>. By this intuition, we design our LSTM-based model with two parts, shown in lower part of Figure 2. The left part is for encoding the syntactical and semantic information of the sentence s, while the right part is encoding the semantic similarity between the pre-trained word embeddings of e1 and e2.
Solely relying on the original word sequence of a sentence s has two problems: (i) the irrelevant words in the sentence can introduce noise into the model; (ii) the large vocabulary of original sentences induce too many parameters, which may cause over-Ô¨Åtting. For example, given two sentences ‚ÄúThe king led the dog into his nice garden.‚Äù and ‚ÄúA criminal led the dog into a poor garden.‚Äù. The object pair is <dog, garden> in both sentences. The two words ‚Äúlead‚Äù and ‚Äúinto‚Äù are essential for determining whether the object pair is located near, but they are not attached with due importance. Also, the semantic differences between irrelevant words, such as ‚Äúking‚Äù and ‚Äúcriminal‚Äù, ‚Äúbeautiful‚Äù and ‚Äúpoor‚Äù, are not useful to the colocation relation between the ‚Äúdog‚Äù and ‚Äúgarden‚Äù, and thus tend to act as noise.
To address the above issues, we propose a normalized sentence representation method merging the three most important and relevant kinds of information about each instance: lemmatized forms, POS (Part-of-Speech) tags and dependency roles.

LocatedNear Relation Extraction

.. .. .. ..

<, >
Object Pairs

. . .
< ,, >
. . .
< Ì±ö, , >
. . .
Corpus

Sentence-Level Relation Classifier

Sentence-Level Relation Classifier

LSTM

. . .
Ì±ê < ,, >
. . . Ì±ê <. Ì±ö. ., , >
Classification Confidence

Ì±ê <, >
LocatedNear Relation Scores

œÉ
MLP

token

distance

embeddings

features

DT lead#s lead DT Ì∞∏

into PR JJ

Ì∞∏

-4

-3

-2

-1

0

1

2

3

4

entity

distances

-8

-7

-6

-5

-4 -3 -2 -1

0

Ì≤Ü

Ì≤îÌ≤é: The king led the dog into his nice garden . dog

Figure 2: Framework with a LSTM-based classiÔ¨Åer

Ì≤Ü
garden

Level Objects Lemma Dependency Role POS Tag

Examples E1, E2 open, lead, into, ... open#s, open#o, into#o, ... DT, PR, CC, JJ, ...

Table 1: Examples of four types of tokens during sentence normalization. (#s stands for subjects and #o for objects)

We Ô¨Årst replace the two nouns in the object pair as ‚ÄúE1‚Äù and ‚ÄúE2‚Äù, and keep the lemmatized form of the original words for all the verbs, adverbs and prepositions, which are highly relevant to describing physical scenes. Then, we replace the subjects and direct objects of the verbs and prepositions (nsubj, dobj for verbs and case for prepositions in dependency parse trees) with special tokens indicating their dependency roles. For the remaining words, we simply use their POS tags to replace the originals. The four kinds of tokens are illustrated in Table 1. Figure 2 shows a real example of our normalized sentence representation, where the object pair of interest is <dog, garden>.
Apart from the normalized tokens of the original sequence, to capture more structural information, we also encode the distances from each token to E1 and E2 respectively. Such position embeddings (position/distance features) are proposed by (Zeng et al., 2014) with the intuition that information

needed to determine the relation between two target nouns normally comes from the words which are close to the target nouns.
Then, we leverage LSTM to encode the whole sequence of the tokens of normalized representation plus position embedding. In the meantime, two pretrained GloVe word embeddings (Pennington et al., 2014) of the original two physical object words are fed into a hidden dense layer.
Finally, we concatenate both outputs and then use sigmoid activation function to obtain the Ô¨Ånal prediction. We choose to use the popular binary cross-entropy as our loss function, and RMSProp as the optimizer. We apply a dropout rate (Zaremba et al., 2014) of 0.5 in the LSTM and embedding layer to prevent overÔ¨Åtting.
3 LOCATEDNEAR Relation Extraction
The upper part of Figure 2 shows the overall workÔ¨Çow of our automatic framework to mine LocatedNear relations from raw text. We Ô¨Årst construct a vocabulary of physical objects and generate all candidate instances. For each sentence in the corpus, if a pair of physical objects ei and ej appear as nouns in a sentence s, then we apply our sentence-level relation classiÔ¨Åer on this instance. The relation classiÔ¨Åer yields a probabilistic score s indicating the conÔ¨Ådence of the instance in the existence of LOCATEDNEAR relation. Finally, all scores of the instances from the corpus are grouped by the ob-

ject pairs and aggregated, where each object pair is associated with a Ô¨Ånal score. These mined physical pairs with scores can easily be integrated into existing commonsense knowledge base.
More speciÔ¨Åcally, for each object pair <ei, ej>, we Ô¨Ånd all the m sentences in our corpus mentioning both objects. We classify the m instances with the sentence-level relation classiÔ¨Åer and obtain conÔ¨Ådence scores for each instance, then feed them into a heuristic scoring function f to obtain the Ô¨Ånal aggregated score for the given object pair. We propose the following 5 choices of f considering accumulation and threshold:

f0 = m

(1)

m

f1 = conf(sk, ei, ej)

(2)

k=1
1m f2 = m conf(sk, ei, ej) (3)
k=1 m

f3 =

1{conf(sk,ei,ej )>0.5}

(4)

k=1

1m

f4 = m

1{conf(sk,ei,ej )>0.5}

(5)

k=1

4 Datasets

Our proposed vocabulary of single-word physical objects is constructed by the intersection of all ConceptNet concepts and all entities that belong to ‚Äúphysical object‚Äù class in Wikidata (VrandecÀáic¬¥ and Kro¬®tzsch, 2014). We manually Ô¨Ålter out some words that have the meaning of an abstract concept, which results in 1,169 physical objects in total.
Afterwards, we utilize a cleaned subset of the Project Gutenberg corpus (Lahiri, 2014), which contains 3,036 English books written by 142 authors. An assumption here is that sentences in Ô¨Åctions are more likely to describe real life scenes. We sample and investigate the density of LOCATEDNEAR relations in Gutenberg with other widely used corpora, namely Wikipedia, used by Mintz et al. (2009) and New York Times corpus (Riedel et al., 2010). In the English Wikipedia dump, out of all sentences which mentions at least two physical objects, 32.4% turn out to be positive. In the New York Times corpus, the percentage of positive sentences is only 25.1%. In contrast, that percentage in the Gutenberg corpus is 55.1%, much higher than the other two corpora, making it a good choice for LOCATEDNEAR relation extraction.

From this corpus, we identify 15,193 pairs that co-occur in more than 10 sentences. Among these pairs, we randomly select 500 object pairs and 10 sentences with respect to each pair for annotators to label their commonsense LOCATEDNEAR. Each instance is labeled by at least three annotators who are college students and proÔ¨Åcient with English. The Ô¨Ånal truth labels are decided by majority voting. The Cohen‚Äôs Kappa among the three annotators is 0.711 which suggests substantial agreement (Landis and Koch, 1977). This dataset has almost double the size of those most popular relations in the SemEval task (Hendrickx et al., 2010), and the sentences in our data set tend to be longer. We randomly choose 4,000 instances as the training set and 1,000 as the test set for evaluating the sentence-level relation classiÔ¨Åcation task. For the second task, we further ask the annotators to label whether each pair of objects are likely to locate near each other in the real world. Majority votes determine the Ô¨Ånal truth labels. The inter-annotator agreement here is 0.703 (substantial agreement).
5 Evaluation
In this section, we Ô¨Årst present our evaluation of our proposed methods and the state-of-the-art general relation classiÔ¨Åcation model on the Ô¨Årst task. Then, we evaluate the quality of the new LOCATEDNEAR triples we extracted.
5.1 Sentence-level LOCATEDNEAR Relation ClassiÔ¨Åcation
We evaluate the proposed methods against the stateof-the-art general domain relation classiÔ¨Åcation model (DRNN) (Xu et al., 2016). The results are shown in Table 2. For feature-based SVM, we do feature ablation on each of the 6 feature types. For LSTM-based model, we experiment on variants of input sequence of original sentence: ‚ÄúLSTM+Word‚Äù uses the original words as the input tokens; ‚ÄúLSTM+POS‚Äù uses only POS tags as the input tokens; ‚ÄúLSTM+Norm‚Äù uses the tokens of sequence after sentence normalization. Besides, we add two naive baselines: ‚ÄúRandom‚Äù baseline method classiÔ¨Åes the instances into two classes with equal probability. ‚ÄúMajority‚Äù baseline method considers all the instances to be positive.
From the results, we Ô¨Ånd that the SVM model without the Global Features performs best, which indicates that bag-of-word features beneÔ¨Åt more in shortest dependency paths than on the whole sen-

Random

Acc.

0.500

P

0.551

R

0.500

F1

0.524

Majority 0.551 0.551 1.000 0.710

SVM 0.584 0.606 0.702 0.650

SVM(-BW) 0.577 0.579 0.675 0.623

SVM(-BPW) 0.556 0.567 0.681 0.619

SVM(-BAP) 0.563 0.573 0.811 0.672

SVM(-GF) 0.605 0.616 0.751 0.677

SVM(-SDP) SVM(-SS) DRNN LSTM+Word LSTM+POS LSTM+Norm

Acc.

0.579

0.584

0.635

0.637

0.641

0.653

P

0.597

0.605

0.658

0.635

0.650

0.654

R

0.728

0.708

0.702

0.800

0.751

0.784

F1

0.656

0.652

0.679

0.708

0.697

0.713

Table 2: Performance of baselines on co-location classiÔ¨Åcation task with ablation. (Acc.=Accuracy, P=Precision, R=Recall, ‚Äú-‚Äù means without certain feature)

f MAP P@50 P@100 P@200 P@300

f0 0.42 0.40 0.44

0.42

0.38

f1 0.58 0.70 0.60

0.53

0.44

f2 0.48 0.56 0.52

0.49

0.42

f3 0.59 0.68 0.63

0.55

0.44

f4 0.56 0.40 0.48

0.50

0.42

Table 3: Ranking results of scoring functions.

(door, room) (ship, sea) (Ô¨Åre, wood) (Ô¨Åre, smoke) (book, table)

(boy, girl) (house, garden)
(house, Ô¨Åre) (door, hall) (fruit, tree)

(cup, tea) (arm, leg) (horse, saddle) (door, street) (table, chair)

Table 4: Top object pairs returned by best performing scoring function f3

tence. Also, we notice that DRNN performs best (0.658) on precision but not signiÔ¨Åcantly higher than LSTM+Norm (0.654). The experiment shows that LSTM+Word enjoys the highest recall score, while LSTM+Norm is the best one in terms of the overall performance. One reason is that the normalization representation reduces the vocabulary of input sequences, while also preserving important syntactical and semantic information. Another reason is that the LOCATEDNEAR relation are described in sentences decorated with prepositions/adverbs. These words are usually descendants of the object word in the dependency tree, outside of the shortest dependency paths. Thus, DRNN cannot capture the information from the words belonging to the descendants of the two object words in the tree, but this information is well captured by LSTM+Norm.
5.2 LOCATEDNEAR Relation Extraction
Once we have obtained the probability score for each instance using LSTM+Norm, we can extract LOCATEDNEAR relation using the scoring function f . We compare the performance of 5 different heuristic choices of f , by quantitative results. We rank 500 commonsense LOCATEDNEAR object pairs described in Section 3. Table 3 shows the ranking results using Mean Average Precision (MAP) and Precision at K as the metrics. Accumulative scores (f1 and f3) generally do better. Thus, we choose f = f3 with a MAP score of 0.59 as the scoring function.

Qualitatively, we show 15 object pairs with some of the highest f3 scores in Table 4. Setting a threshold of 40.0 for f3, which is the minimum non-zero f3 score for all true object pairs in the LOCATEDNEAR object pairs data set (500 pairs), we obtain a total of 2,067 LOCATEDNEAR relations, with a precision of 68% by human inspection.
6 Conclusion
In this paper, we present a novel study on enriching LOCATEDNEAR relationship from textual corpora. Based on our two newly-collected benchmark datasets, we propose several methods to solve the sentence-level relation classiÔ¨Åcation problem. We show that existing methods do not work as well on this task and discovered that LSTM-based model does not have signiÔ¨Åcant edge over simpler featurebased model. Whereas, our multi-level sentence normalization turns out to be useful.
Future directions include: 1) better leveraging distant supervision to reduce human efforts, 2) incorporating knowledge graph embedding techniques, 3) applying the LOCATEDNEAR knowledge into downstream applications in computer vision and natural language processing.
Acknowledgment
Kenny Q. Zhu is the contact author and was supported by NSFC grants 91646205 and 61373031. Thanks to the annotators for manual labeling, and the anonymous reviewers for valuable comments.

References
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632‚Äì642. Association for Computational Linguistics.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 15(4):i‚Äìxvii.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O¬¥ Se¬¥aghdha, Sebastian Pado¬¥, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classiÔ¨Åcation of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33‚Äì38. Association for Computational Linguistics.
Shibamouli Lahiri. 2014. Complexity of word collocation networks: A preliminary structural analysis. In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 96‚Äì105. Association for Computational Linguistics.
J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33 1:159‚Äì74.
Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel. 2016. Commonsense knowledge base completion. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin, Germany, August. Association for Computational Linguistics, pages 1445‚Äì1455.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003‚Äì1011. Association for Computational Linguistics.
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532‚Äì1543. Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. Machine learning and knowledge discovery in databases, pages 148‚Äì163.

Robert Speer and Catherine Havasi. 2012. Representing general relational knowledge in conceptnet 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012). European Language Resources Association (ELRA).
Denny VrandecÀáic¬¥ and Markus Kro¬®tzsch. 2014. Wikidata: A free collaborative knowledgebase. Communications of ACM, 57:78‚Äì85.
Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. 2016. Improved relation classiÔ¨Åcation by deep recurrent neural networks with data augmentation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1461‚Äì 1470. The COLING 2016 Organizing Committee.
Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1785‚Äì1794. Association for Computational Linguistics.
Mark Yatskar, Vicente Ordonez, and Ali Farhadi. 2016. Stating the obvious: Extracting visual common sense knowledge. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 193‚Äì198. Association for Computational Linguistics.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classiÔ¨Åcation via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335‚Äì2344. Dublin City University and Association for Computational Linguistics.
Yuke Zhu, Alireza Fathi, and Li Fei-Fei. 2014. Reasoning about object affordances in a knowledge base representation. In European conference on computer vision, pages 408‚Äì424. Springer.

