MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION
Xuankai Chang1,2, Wangyou Zhang2, Yanmin Qian2†, Jonathan Le Roux3, Shinji Watanabe1†
1Center for Language and Speech Processing, Johns Hopkins University, USA 2SpeechLab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China
3Mitsubishi Electric Research Laboratories (MERL), USA

arXiv:1910.06522v2 [eess.AS] 16 Oct 2019

ABSTRACT
Recently, the end-to-end approach has proven its efﬁcacy in monaural multi-speaker speech recognition. However, high word error rates (WERs) still prevent these systems from being used in practical applications. On the other hand, the spatial information in multi-channel signals has proven helpful in far-ﬁeld speech recognition tasks. In this work, we propose a novel neural sequence-tosequence (seq2seq) architecture, MIMO-Speech, which extends the original seq2seq to deal with multi-channel input and multi-channel output so that it can fully model multi-channel multi-speaker speech separation and recognition. MIMO-Speech is a fully neural end-toend framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model. With this processing, the input overlapped speech is directly mapped to text sequences. We further adopted a curriculum learning strategy, making the best use of the training set to improve the performance. The experiments on the spatialized wsj1-2mix corpus show that our model can achieve more than 60% WER reduction compared to the single-channel system with high quality enhanced signals (SI-SDR = 23.1 dB) obtained by the above separation function.
Index Terms— Overlapped speech recognition, end-to-end, neural beamforming, speech separation, curriculum learning.
1. INTRODUCTION
The cocktail party problem, where the speech of a target speaker is entangled with noise or speech of interfering speakers, has been a challenging problem in speech processing for more than 60 years [1]. In recent years, there have been many research efforts based on deep learning addressing the multi-speaker speech separation and recognition problems. These works can be categorized into two classes depending on the type of input signals, namely single-channel and multi-channel.
In the single-channel multi-speaker speech separation and recognition tasks, several techniques have been proposed, achieving signiﬁcant progress. One such technique is deep clustering (DPCL) [2–4]. In DPCL, a neural network is trained to map each time-frequency unit to an embedding vector, which is used to assign each unit to a source by a clustering algorithm afterwards. DPCL was then integrated into a joint training framework with end-to-end speech recognition in [5], showing promising performance. Another approach called permutation-free training [2, 3] or permutationinvariant training (PIT) [6, 7] relies on training a neural network to
† Yanmin Qian and Shinji Watanabe are the corresponding authors.

estimate a mask for every speaker with a permutation-free objective function that minimizes the reconstruction loss. PIT was later applied to multi-speaker automatic speech recognition (ASR) by directly optimizing a speech recognition loss [8, 9] within a DNNHMM hybrid ASR framework. In recent years, end-to-end models have drawn a lot of attention in single-speaker ASR systems and shown great success [10–13]. These models have simpliﬁed the ASR paradigm by unifying acoustic, language, and phonetic models into a single neural network. In [14, 15], joint CTC/attention-based encoder-decoder [12] end-to-end models were developed to solve the single-channel multi-speaker speech recognition problem, where the encoder separates the mixed speech features and the attentionbased decoder generates the output sequences. Although signiﬁcant performance improvements have been achieved in the monaural case, there is still a large performance gap compared with that of single-speaker speech recognition systems, making such models not yet ready for widespread application in real scenarios.
The other important case is that of multi-channel multi-speaker speech separation and recognition, where the input signals are collected by microphone arrays. Acquiring multi-channel data is not so limiting nowadays, where microphone arrays are widely deployed in many devices. When multi-channel data is available, the spatial information can be exploited to determine the speaker location and to separate the speech with higher accuracy. Yoshioka et al [16] proposed a method for performing multi-channel speech separation under the PIT framework. A mask-based beamformer called the unmixing transducer was used to separate the overlapped speech. Another method proposed by Wang et al [17] leverages the interchannel differences as spatial features combined with the singlechannel spectral features as the input, to separate the multi-channel data using the DPCL technique.
Previous works based on multi-channel multi-speaker input mainly focus on separation. In this paper, we propose an end-toend multi-channel multi-speaker speech recognition system. Such a sequence-to-sequence model is trained to directly map multi-channel input (MI) speech signals where multiple speakers speak simultaneously, to multiple output (MO) text sequences, one for each speaker. We refer to this system as MIMO-Speech. The recent research on single-speaker far-ﬁeld speech recognition has shown that neural beamforming techniques for denoising [18, 19] can achieve stateof-the art results in robust ASR tasks [20–22]. Several works have shown that it is feasible to design a totally differentiable end-to-end model by integrating the neural beamforming mechanism and the sequence-to-sequence speech recognition together [23–26]. [27] further shows that the neural beamforming function in a multi-channel end-to-end system can enhance the signals. In light of this success, we redesigned the neural-beamformer front-end to allow it to attend

to multiple beams at different directions. After getting the separated signals, the log ﬁlter bank features are extracted inside the neural network. Finally, a joint CTC/attention-based encoder-decoder recognizes each feature stream. With this framework, the outputs of the beamformer in the middle of the model can also be used as speech separation signals. During the training, a data scheduling strategy using curriculum learning is specially designed and leads to an additional performance boost. To prove the basic concept of our method, we ﬁrst evaluated our proposed method in the anechoic scenario. From the results, we ﬁnd that even without explicitly optimizing for separation, the intermediate signals after the beamformer still show very good quality in terms of audibility. Then we also tested the model on the reverberant case to give a preliminary result.

2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR
In this section, we ﬁrst present the proposed end-to-end multichannel multi-speaker speech recognition model, which is shown in Fig. 1. We then describe the techniques applied in scheduling the training data, which have an important role in improving the performance.

CTC Loss

L c tc < l a t e x i t s h a 1 _ b a s e 6 4 = " a Q 0 O f 8 N B 8 F p e U W z 3 E u h B o Z M R z f Q = " > A A A C A X i c b V D L S s N A F J 3 4 r P U V d S O 4 G S y C q 5 J U Q Z c F N y 5 c V L A P a E K Y T C f t 0 M m D m R u x h L j x V 9 y 4 U M S t f + H O v 3 H S Z q G t B y 4 c z r m X e + / x E 8 E V W N a 3 s b S 8 s r q 2 X t m o b m 5 t 7 + y a e / s d F a e S s j a N R S x 7 P l F M 8 I i 1 g Y N g v U Q y E v q C d f 3 x V e F 3 7 5 l U P I 7 u Y J I w N y T D i A e c E t C S Z x 4 6 I Y E R J S K 7 y b 3 M A f Y A G Q W a 5 5 5 Z s + r W F H i R 2 C W p o R I t z / x y B j F N Q x Y B F U S p v m 0 l 4 G Z E A q e C 5 V U n V S w h d E y G r K 9 p R E K m 3 G z 6 Q Y 5 P t D L A Q S x 1 R Y C n 6 u + J j I R K T U J f d x b 3 q n m v E P / z + i k E l 2 7 G o y Q F F t H Z o i A V G G J c x I E H X D I K Y q I J o Z L r W z E d E U k o 6 N C q O g R 7 / u V F 0 m n U 7 b N 6 4 / a 8 1 m y U c V T Q E T p G p 8 h G F 6 i J r l E L t R F F j + g Z v a I 3 4 8 l 4 M d 6 N j 1 n r k l H O H K A / M D 5 / A M k l l 7 M = < / l a t e x i t >

Permutation Invariant Training

Permutation

⇡ˆ< l a t e x i ts h a 1 _ b a s e 6 4 = " Q A B l M 8 O 3 E j q t B n q V t P T 2 t w Z R 8 V c = " > A A A B 8 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y B E 8 l a Q K e i x 4 8 V j B f k g T y m a 7 a Z f u b s L u R C i h v 8 K L B 0 W 8 + n O 8 + W / c t j l o 6 4 O B x 3 s z z M y L U s E N e t 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J s k 0 Z S 2 a i E R 3 I 2 K Y 4 I q 1 k K N g 3 V Q z I i P B O t H 4 d u Z 3 n p g 2 P F E P O E l Z K M l Q 8 Z h T g l Z 6 D E Y E 8 y D l 0 3 6 l 6 t W 8 O d x V 4 h e k C g W a / c p X M E h o J p l C K o g x P d 9 L M c y J R k 4 F m 5 a D z L C U 0 D E Z s p 6 l i k h m w n x + 8 N Q 9 t 8 r A j R N t S 6 E 7 V 3 9 P 5 E Q a M 5 G R 7 Z Q E R 2 b Z m 4 n / e b 0 M 4 5 s w 5 y r N k C m 6 W B R n w s X E n X 3 v D r h m F M X E E k I 1 t 7 e 6 d E Q 0 o W g z K t s Q / O W X V 0 m 7 X v M v a / X 7 q 2 r D K + I o w S m c w Q X 4 c A 0 N u I M m t I C C h G d 4 h T d H O y / O u / O x a F 1 z i p k T + A P n 8 w c a o Z C M < / l a t e x i t >

Attention Decoder

ASR Encoder
O< l a t e x i t s h a 1 _ b a s e 6 4 = " 0 L 5 / 9 i X A 9 w i 4 E i Q A w l v N F s c d 4 2 A = " > A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z c F N + 6 s Y B / Q G U s m z b S h m U x I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 6 c U H K m j e t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 U m q C G 2 T h C e q F 2 J N O R O 0 b Z j h t C c V x X H I a T e c 3 O R + 9 4 k q z R L x Y K a S B j E e C R Y x g o 2 V f D / G Z h x G 2 d 3 s 0 R t U a 2 7 d n Q O t E q 8 g N S j Q G l S / / G F C 0 p g K Q z j W u u + 5 0 g Q Z V o Y R T m c V P 9 V U Y j L B I 9 q 3 V O C Y 6 i C b Z 5 6 h M 6 s M U Z Q o + 4 R B c / X 3 R o Z j r a d x a C f z j H r Z y 8 X / v H 5 q o u s g Y 0 K m h g q y O B S l H J k E 5 Q W g I V O U G D 6 1 B B P F b F Z E x l h h Y m x N F V u C t / z l V d J p 1 L 2 L e u P + s t Z 0 i z r K c A K n c A 4 e X E E T b q E F b S A g 4 R l e 4 c 1 J n R f n 3 f l Y j J a c Y u c Y / s D 5 / A H m E Z G I < / l a t e x i t > 1 Feature Extractor
xet,nfh,1
< l a t e x i t s h a 1 _ b a s e 6 4 = " r e 4 p G N W 8 7 O v X 0 z 6 g Q l s q L L K K g F U = " > A A A C C H i c b V A 9 S w N B E N 2 L X z F + R S 0 t P A y C R Q h 3 U d A y Y G M Z w X x A E s P e Z i 5 Z s r d 3 7 M 5 J w n G l j X / F x k I R W 3 + C n f / G z U e h i Q 8 G H u / N M D P P i w T X 6 D j f V m Z l d W 1 9 I 7 u Z 2 9 r e 2 d 3 L 7 x / U d R g r B j U W i l A 1 P a p B c A k 1 5 C i g G S m g g S e g 4 Q 2 v J 3 7 j A Z T m o b z D c Q S d g P Y l 9 z m j a K R u / r g d U B x 4 f j J K 7 5 M 2 w g g T k I O 0 6 K b d B I t + 2 s 0 X n J I z h b 1 M 3 D k p k D m q 3 f x X u x e y O A C J T F C t W 6 4 T Y S e h C j k T k O b a s Y a I s i H t Q 8 t Q S Q P Q n W T 6 S G q f G q V n + 6 E y J d G e q r 8 n E h p o P Q 4 8 0 z k 5 W y 9 6 E / E / r x W j f 9 V J u I x i B M l m i / x Y 2 B j a k 1 T s H l f A U I w N o U x x c 6 v N B l R R h i a 7 n A n B X X x 5 m d T L J f e 8 V L 6 9 K F S c e R x Z c k R O y B l x y S W p k B t S J T X C y C N 5 J q / k z X q y X q x 3 6 2 P W m r H m M 4 f k D 6 z P H + Q + m n U = < / l a t e x i t >

ASR Encoder
O< l a t e x i t s h a 1 _ b a s e 6 4 = " q z s R N A 3 d o l 5 x h + L B E H 8 W m K X 4 z + k = " > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C G 3 d W s A / o j C W T Z t r Q T G Z I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 6 c I B F c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 X G q K G v T W M S q F x D N B J e s b b g R r J c o R q J A s G 4 w u c n 9 7 h N T m s f y w U w T 5 k d k J H n I K T F W 8 r y I m H E Q Z n e z x 8 a g W n P q z h x 4 l b g F q U G B 1 q D 6 5 Q 1 j m k Z M G i q I 1 n 3 X S Y y f E W U 4 F W x W 8 V L N E k I n Z M T 6 l k o S M e 1 n 8 8 w z f G a V I Q 5 j Z Z 8 0 e K 7 + 3 s h I p P U 0 C u x k n l E v e 7 n 4 n 9 d P T X j t Z 1 w m q W G S L g 6 F q c A m x n k B e M g V o 0 Z M L S F U c Z s V 0 z F R h B p b U 8 W W 4 C 5 / e Z V 0 G n X 3 o t 6 4 v 6 w 1 n a K O M p z A K Z y D C 1 f Q h F t o Q R s o J P A M r / C G U v S C 3 t H H Y r S E i p 1 j + A P 0 + Q P n l Z G J < / l a t e x i t > 2 Feature Extractor
xet,nfh,2
< l a t e x i t s h a 1 _ b a s e 6 4 = " V q v v r 3 3 d q F R Q 4 x k i 6 p m j N p M h Y X w = " > A A A C C H i c b V A 9 S w N B E N 2 L X z F + R S 0 t P A y C R Q h 3 U d A y Y G M Z w X x A E s P e Z i 5 Z s r d 3 7 M 5 J w n G l j X / F x k I R W 3 + C n f / G z U e h i Q 8 G H u / N M D P P i w T X 6 D j f V m Z l d W 1 9 I 7 u Z 2 9 r e 2 d 3 L 7 x / U d R g r B j U W i l A 1 P a p B c A k 1 5 C i g G S m g g S e g 4 Q 2 v J 3 7 j A Z T m o b z D c Q S d g P Y l 9 z m j a K R u / r g d U B x 4 f j J K 7 5 M 2 w g g T k I O 0 W E 6 7 C R b 9 t J s v O C V n C n u Z u H N S I H N U u / m v d i 9 k c Q A S m a B a t 1 w n w k 5 C F X I m I M 2 1 Y w 0 R Z U P a h 5 a h k g a g O 8 n 0 k d Q + N U r P 9 k N l S q I 9 V X 9 P J D T Q e h x 4 p n N y t l 7 0 J u J / X i t G / 6 q T c B n F C J L N F v m x s D G 0 J 6 n Y P a 6 A o R g b Q p n i 5 l a b D a i i D E 1 2 O R O C u / j y M q m X S + 5 5 q X x 7 U a g 4 8 z i y 5 I i c k D P i k k t S I T e k S m q E k U f y T F 7 J m / V k v V j v 1 s e s N W P N Z w 7 J H 1 i f P + X J m n Y = < / l a t e x i t >

ISTFT

g1
< l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 q m T + D G 4 M 3 j 9 H c W b d 0 h j e D y 1 y I = " > A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 i E u i l J K + i y 4 M Z l B f u A N p b J d N I O n U z C z E S o o V / i x o U i b v 0 U d / 6 N k z Y L b T 0 w c D j n X u 6 Z 4 8 e c K e 0 4 3 1 Z h Y 3 N r e 6 e 4 W 9 r b P z g s 2 0 f H H R U l k t A 2 i X g k e z 5 W l D N B 2 5 p p T n u x p D j 0 O e 3 6 0 5 v M 7 z 5 S q V g k 7 v U s p l 6 I x 4 I F j G B t p K F d H o R Y T / w g H c 8 f 3 G p w M b Q r T s 1 Z A K 0 T N y c V y N E a 2 l + D U U S S k A p N O F a q 7 z q x 9 l I s N S O c z k u D R N E Y k y k e 0 7 6 h A o d U e e k i + B y d G 2 W E g k i a J z R a q L 8 3 U h w q N Q t 9 M 5 n F V K t e J v 7 n 9 R M d X H s p E 3 G i q S D L Q 0 H C k Y 5 Q 1 g I a M U m J 5 j N D M J H M Z E V k g i U m 2 n R V M i W 4 q 1 9 e J 5 1 6 z W 3 U 6 n e X l a a T 1 1 G E U z i D K r h w B U 2 4 h R a 0 g U A C z / A K b 9 a T 9 W K 9 W x / L 0 Y K V 7 5 z A H 1 i f P x X V k q Y = < / l a t e x i t >

(f

)

m0t,f < l a t e x i t s h a 1 _ b a s e 6 4 = " q o / r 3 f b M h 0 I l 1 v M 5 y m a Q 4 P s w + y c = " > A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w I S W p g i 4 L b l x W s A 9 o a p h M J + 3 Q y Y O Z G 7 E M W b j x V 9 y 4 U M S t H + H O v 3 H S Z q G t B y 4 c z r m X e + / x E 8 4 k 2 P a 3 U V p Z X V v f K G 9 W t r Z 3 d v f M / Y O O j F N B a J v E P B Y 9 H 0 v K W U T b w I D T X i I o D n 1 O u / 7 k K v e 7 9 1 R I F k e 3 M E 3 o I M S j i A W M Y N C S Z 1 b d E M P Y D 1 S Y e Q p O g + x O u U A f Q N l Z 5 p k 1 u 2 7 P Y C 0 T p y A 1 V K D l m V / u M C Z p S C M g H E v Z d + w E B g o L Y I T T r O K m k i a Y T P C I 9 j W N c E j l Q M 2 e y K x j r Q y t I B a 6 I r B m 6 u 8 J h U M p p 6 G v O / O T 5 a K X i / 9 5 / R S C y 4 F i U Z I C j c h 8 U Z B y C 2 I r T 8 Q a M k E J 8 K k m m A i m b 7 X I G A t M Q O d W 0 S E 4 i y 8 v k 0 6 j 7 p z V G z f n t a Z d x F F G V X S E T p C D L l A T X a M W a i O C H t E z e k V v x p P x Y r w b H / P W k l H M H K I / M D 5 / A O v Z m N o = < / l a t e x i t > (noise)

g2
< l a t e x i t s h a 1 _ b a s e 6 4 = " g J C Y 6 X z C V t I G j 9 9 Z v E K 9 6 g t h T c g = " > A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 i E u i l J K + i y 4 M Z l B f u A N p b J d N I O n U z C z E S o o V / i x o U i b v 0 U d / 6 N k z Y L b T 0 w c D j n X u 6 Z 4 8 e c K e 0 4 3 1 Z h Y 3 N r e 6 e 4 W 9 r b P z g s 2 0 f H H R U l k t A 2 i X g k e z 5 W l D N B 2 5 p p T n u x p D j 0 O e 3 6 0 5 v M 7 z 5 S q V g k 7 v U s p l 6 I x 4 I F j G B t p K F d H o R Y T / w g H c 8 f 6 t X g Y m h X n J q z A F o n b k 4 q k K M 1 t L 8 G o 4 g k I R W a c K x U 3 3 V i 7 a V Y a k Y 4 n Z c G i a I x J l M 8 p n 1 D B Q 6 p 8 t J F 8 D k 6 N 8 o I B Z E 0 T 2 i 0 U H 9 v p D h U a h b 6 Z j K L q V a 9 T P z P 6 y c 6 u P Z S J u J E U 0 G W h 4 K E I x 2 h r A U 0 Y p I S z W e G Y C K Z y Y r I B E t M t O m q Z E p w V 7 + 8 T j r 1 m t u o 1 e 8 u K 0 0 n r 6 M I p 3 A G V X D h C p p w C y 1 o A 4 E E n u E V 3 q w n 6 8 V 6 t z 6 W o w U r 3 z m B P 7 A + f w A X X J K n < / l a t e x i t >

(f

)

MVDR Filtering

m1t,f < l a t e x i t s h a 1 _ b a s e 6 4 = " Z S 9 n O T 7 x a G 0 i G a 1 p b 9 3 C s o u a d r A = " > A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w I S W p g i 4 L b l x W s A 9 o a p h M J + 3 Q y Y O Z G 7 E M W b j x V 9 y 4 U M S t H + H O v 3 H S Z q G t B y 4 c z r m X e + / x E 8 4 k 2 P a 3 U V p Z X V v f K G 9 W t r Z 3 d v f M / Y O O j F N B a J v E P B Y 9 H 0 v K W U T b w I D T X i I o D n 1 O u / 7 k K v e 7 9 1 R I F k e 3 M E 3 o I M S j i A W M Y N C S Z 1 b d E M P Y D 1 S Y e Q p O g + x O u U A f Q D l Z 5 p k 1 u 2 7 P Y C 0 T p y A 1 V K D l m V / u M C Z p S C M g H E v Z d + w E B g o L Y I T T r O K m k i a Y T P C I 9 j W N c E j l Q M 2 e y K x j r Q y t I B a 6 I r B m 6 u 8 J h U M p p 6 G v O / O T 5 a K X i / 9 5 / R S C y 4 F i U Z I C j c h 8 U Z B y C 2 I r T 8 Q a M k E J 8 K k m m A i m b 7 X I G A t M Q O d W 0 S E 4 i y 8 v k 0 6 j 7 p z V G z f n t a Z d x F F G V X S E T p C D L l A T X a M W a i O C H t E z e k V v x p P x Y r w b H / P W k l H M H K I / M D 5 / A O 1 f m N s = < / l a t e x i t > (speaker 1)

m2t,f < l a t e x i t s h a 1 _ b a s e 6 4 = " w P B A 0 Q E u R m b M v j t L 5 2 p f 3 T D + P i Q = " > A A A C B H i c b V B N S 8 N A E N 3 U r 1 q / o h 5 7 C R b B g 5 S k C n o s e P F Y w X 5 A U 8 N m u 2 m X 7 i Z h d y K W k I M X / 4 o X D 4 p 4 9 U d 4 8 9 + 4 a X P Q 1 g c D j / d m m J n n x 5 w p s O 1 v o 7 S y u r a + U d 6 s b G 3 v 7 O 6 Z + w c d F S W S 0 D a J e C R 7 P l a U s 5 C 2 g Q G n v V h S L H x O u / 7 k K v e 7 9 1 Q q F o W 3 M I 3 p Q O B R y A J G M G j J M 6 u u w D D 2 g 1 R k X g q n Q X a X u k A f I G 1 k m W f W 7 L o 9 g 7 V M n I L U U I G W Z 3 6 5 w 4 g k g o Z A O F a q 7 9 g x D F I s g R F O s 4 q b K B p j M s E j 2 t c 0 x I K q Q T p 7 I r O O t T K 0 g k j q C s G a q b 8 n U i y U m g p f d + Y n q 0 U v F / / z + g k E l 4 O U h X E C N C T z R U H C L Y i s P B F r y C Q l w K e a Y C K Z v t U i Y y w x A Z 1 b R Y f g L L 6 8 T D q N u n N W b 9 y c 1 5 p 2 E U c Z V d E R O k E O u k B N d I 1 a q I 0 I e k T P 6 B W 9 G U / G i / F u f M x b S 0 Y x c 4 j + w P j 8 A e 7 l m N w = < / l a t e x i t > (speaker 2)

Masking Network

{X}Cc=1 < l a t e x i t s h a 1 _ b a s e 6 4 = " U Z L 2 K / Q K W f m N A t r n g M x x N z E j J N 8 = " > A A A C A X i c b V D L S s N A F J 3 U V 6 2 v q B v B z W A R X J W k C r o R C t 2 4 r G A f 0 M Q y m U 7 a o Z N J m J k I Z Y g b f 8 W N C 0 X c + h f u / B s n b R b a e u D C 4 Z x 7 u f e e I G F U K s f 5 t k o r q 2 v r G + X N y t b 2 z u 6 e v X / Q k X E q M G n j m M W i F y B J G O W k r a h i p J c I g q K A k W 4 w a e Z + 9 4 E I S W N + p 6 Y J 8 S M 0 4 j S k G C k j D e w j T 3 s R U u M g 1 L 3 M y w Y a X 7 v Z v W 5 m A 7 v q 1 J w Z 4 D J x C 1 I F B V o D + 8 s b x j i N C F e Y I S n 7 r p M o X y O h K G Y k q 3 i p J A n C E z Q i f U M 5 i o j 0 9 e y D D J 4 a Z Q j D W J j i C s 7 U 3 x M a R V J O o 8 B 0 5 t f K R S 8 X / / P 6 q Q q v f E 1 5 k i r C 8 X x R m D K o Y p j H A Y d U E K z Y 1 B C E B T W 3 Q j x G A m F l Q q u Y E N z F l 5 d J p 1 5 z z 2 v 1 2 4 t q o 1 7 E U Q b H 4 A S c A R d c g g a 4 A S 3 Q B h g 8 g m f w C t 6 s J + v F e r c + 5 q 0 l q 5 g 5 B H 9 g f f 4 A 5 Z i X H g = = < / l a t e x i t > Masking Network and Neural Beamformer
Fig. 1. End-to-End Multi-channel Multi-speaker Model

2.1. Model Architecture
By using the differences in the signals recorded at each sensor, distributed sensors can exploit spatial information. They are thus particularly useful for separating sources that are spatially partitioned. In this work, we present a sequence-to-sequence architecture with multi-channel input and multi-channel output to model the multichannel multi-speaker speech recognition, shown in Fig. 1 for the case of two speakers. The proposed end-to-end multi-channel multispeaker ASR model can be divided into three stages. The ﬁrst stage is a single-channel masking network to perform pre-separation by predicting multiple speaker and noise masks for each channel. Then a multi-source neural beamformer is used to spatially separate multiple speaker sources. In the last stage, an end-to-end ASR module with permutation-free training is used to perform the multi-output speech recognition.

We used a similar architecture as in [23], where the masking network and the neural beamformer are integrated into an attentionbased encoder-decoder neural network, and the whole model is jointly optimized solely via a speech recognition objective. The input of the model can consist of an arbitrary number of channels C, and its output is the text sequence for each speaker directly. We denote by J the number of speakers in the mixed utterances, and for simplicity of notation, we shall consider the noise component as the 0-th source.

2.1.1. Monaural Masking Network

The monaural masking network, shown at the bottom of Fig. 1, esti-
mates the masks of each channel for every speaker and an extra noise component. Let us denote by Xc = (xt,f,c)t,f ∈ CT ×F the complex STFT of the c-th channel of the observed multi-channel multispeaker speech, where 1 ≤ t ≤ T, 1 ≤ f ≤ F, 1 ≤ c ≤ C denote
time, frequency, and channel indices, respectively. The mask estimation module produces time-frequency masks Mic = (mit,f,c)t,f ∈ [0, 1]T ×F , with i ∈ {1, . . . , J} for each of the J speakers, and i = 0 for the noise, using the complex STFT of the c-th channel of the ob-
served multi-channel multi-speaker speech as input. The computa-
tion is performed independently on each of the input channels:

Mc = MaskNet(Xc),

(1)

where Mc = (Mic)i ∈ [0, 1]T ×F ×J is the set of estimated masks for the c-th channel.

2.1.2. Multi-source Neural Beamformer

The multi-source neural beamformer is a key component in the proposed model, which produces the separated speech of each speaker. The masks obtained on each channel for each speaker and the noise are used in the computation of the power spectral density (PSD) matrices of each source as follows [18, 28]:

i

1

T i

H

C×C

Φ (f ) = T mi

mt,f xt,f xt,f ∈ C , (2)

t=1 t,f t=1

where i ∈ {0, . . . , J }, xt,f = {xt,f,c}Cc=1, mit,f = {mit,f,c}Cc=1, and H represents the conjugate transpose.
After getting the PSD matrices of every speaker and the noise, we estimate the beamformer’s time-invariant ﬁlter coefﬁcients gi(f )
at frequency f for each speaker i ∈ {1, · · · , J} via the MVDR
formalization [29] as follows:

(

Φj (f ))−1Φi(f )

gi(f ) =

j=i

u ∈ CC ,

(3)

Tr(( j=i Φj (f ))−1Φi(f ))

where u ∈ RC is a vector representing the reference microphone that is derived from an attention mechanism [23], and Tr(·) denotes
the trace operation. Notice that in Eq. 3, the formula to derive the ﬁl-
ter coefﬁcient is different from that in [23] in the way that the noise PSD is replaced by j=i Φj(f ). This is because both noise and other speakers are considered as interference when focusing on a
given speaker. This is akin to the speech-speech-noise (SSN) model
in [16]. Such a method is employed to make more accurate estima-
tions of the PSD matrices, in which the traditional PSD matrix is
expressed using the PSD matrix of interfering speaker and that of
the background noise. Finally, the beamforming ﬁlters gi(f ) obtained in Eq. 3 are used
to separate and denoise the input overlapped multi-channel signals

xt,f ∈ CC to obtain a single-channel estimate of the enhanced STFT sˆit,f for speaker i:

sˆit,f = (gi(f ))H xt,f ∈ C.

(4)

Each separated speech signal waveform can be obtained by inverse STFT for listening, as iSTFT(Sˆi), i = 1, . . . , J.

2.1.3. End-to-End Speech Recognition

The outputs of the neural beamformer are estimates of the separated
speech signals for each speaker. Before feeding these streams to the
end-to-end speech recognition submodule, we need to convert the
STFT features to normalized log ﬁlterbank features. A log mel ﬁl-
terbank transformation is ﬁrst applied on the magnitude of the beamformed STFT signal Sˆi = (Sˆti,f )t,f for each speaker i, and a global mean-variance normalization is then performed on the log-ﬁlterbank feature to produce a proper input Oi for the speech recognition sub-
module:

FBanki = MelFilterBank(|Sˆi|),

(5)

Oi = GlobalMVN(log(FBanki)).

(6)

We brieﬂy introduce the end-to-end speech recognition sub-
module used here, which is similar to the joint CTC/attention-based encoder-decoder architecture [12]. The feature vectors Oi are ﬁrst transformed to a hidden representation Hi by an encoder network.
A decoder then generates the output token sequences based on the history information y and a weighted sum vector c obtained
with an attention mechanism. The end-to-end speech recognition is
computed as follows:

Hi = Encoder(Oi)

(7)

cin, αni = Attention(αni −1, ein−1, Hi)

(8)

ein = Update(ein−1, cin−1, yni −1)

(9)

yni ∼ Decoder(cin, yni −1),

(10)

where i denotes the index of the source stream and n an output label sequence index.
Typically, the history information y is replaced by the reference labels R = (r1, · · · , rN ) in a teacher-forcing fashion at training time. However, since there are multiple possible assignments between the inputs and the references, it is necessary to used permutation invariant training (PIT) in the end-to-end speech recognition [14, 15]. The best permutation of the input sequences and the references is determined by the connectionist temporal classiﬁcation (CTC) loss Lossctc:

πˆ = argmin Lossctc(Zi, Rπ(i)), i = 1, . . . , J,

(11)

π∈P i

where Zi denotes the output sequence computed from the encoder output Hi for the CTC loss, P is the set of all permutations on
{1, . . . , J}, and π(i) is the i-th element for permutation π.
The ﬁnal ASR loss of the model is obtained as:

L = λLctc + (1 − λ)Latt,

(12)

Lctc = Lossctc(Zi, Rπˆ(i)),

(13)

i

Latt = Lossatt(Yi, Rπˆ(i)),

(14)

i

where 0 ≤ λ ≤ 1 is an interpolation factor, and Lossatt is the crossentropy loss to train the attention-based encoder-decoder networks.

2.2. Data Scheduling and Curriculum Learning
From preliminary empirical results, we ﬁnd that it is relatively difﬁcult to perform straightforward end-to-end training of such a multistage model, especially without an intermediate criterion to guide the training. In our model, the speech recognition submodule has the same architecture as the typical end-to-end speech recognition model, and the input is expected to be similar to the log ﬁlterbank of single-speaker speech. Thus, in order to train the model properly, we did not only use the spatialized utterances of the multi-speaker corpus but also the single-speaker utterances from the original WSJ training set. During training, every batch is randomly chosen either from the multi-channel multi-speaker set or from the single-channel single-speaker set. For single-speaker batches, the masking network and neural beamformer stages are bypassed, and the input is directly fed to the recognition submodule. Furthermore, the loss is calculated without considering permutations, as there is only a single speaker per input.
With this data scheduling scheme, the model can achieve a decent performance from random initialization. For multi-channel multi-speaker data batches, the loss of the ASR objective function is back-propagated down through the model to the masking network. For data batches consisting of single-speaker utterances, only the speech recognition part is optimized, which leads to more accurate loss computation in the future. The single-speaker data batches rectify the behavior of the ASR model as it performs regularization during the training.
According to previous researches, starting from easier subtasks can lead the model to learn better, an approach called curriculum learning [30, 31]. To further exploit the data scheduling scheme, we introduce more constraints on the order of the data batches of the training set. As was observed in prior research by [9], the signal-tonoise ratio (SNR, the energy ratio between the target speech and the interfering sources) has a great inﬂuence on the ﬁnal recognition performance. When the speech energy levels of the target speaker and the interfering sources are obviously different, the recognition accuracy of the interfering source speech is very poor. Thus, we sort the multi-speaker data in ascending order of SNR between the loudest and quietest speaker, thus starting with mixtures where both speakers are at similar levels. Furthermore, we sort the single-speaker data from short to long, as short sequences tend to be easier to learn in seq2seq learning. The strategy is formally depicted in Algorithm 1. We applied such a curriculum learning strategy in order to make the model learn step by step and expect it to improve the training.
3. EXPERIMENT
To check the effectiveness of our proposed end-to-end model, we evaluated it on the remixed WSJ1 data used in [14], which we here refer to as the wsj1-2mix dataset. The multi-speaker speech training set was generated by randomly selecting two utterances from the WSJ SI284 corpus, resulting in a 98.5 h dataset. The signal-to-noise ratio (SNR) of one source against the other was randomly chosen from a uniform distribution in the range of [−5, 5] dB. The validation and evaluation sets were generated in a similar way by selecting source utterances from the WSJ Dev93 and Eval92 respectively, and the durations are 1.3 h and 0.8 h. We then create a new spatialized version of the wsj1-2mix dataset following the process applied to the wsj0-2mix dataset in [17], using a room impulse response (RIR) generator1, where the characteristics of each two-speaker mixture
1Available online at https://github.com/ehabets/ RIR-Generator

Algorithm 1: Curriculum learning strategy

1 Load the training dataset X;

2 Categorize the training data X into single-channel

single-speaker data Xclean and multi-channel multi-speaker

data Xnoisy;

3 Sort the single-channel single-speaker training data in Xclean

in ascending order of the utterance lengths, leading to

Xclean ; 4 Sort the multi-channel multi-speaker training data in Xnoisy in

ascending order of the SNR level, leading to Xnoisy ; 5 Divide Xclean and Xnoisy into minibatch sets Bclean and Bnoisy; 6 Sort batches to alternate between batches from Bclean and

Bnoisy ;

7 while model is not converged do

8 for each b in all minibatches do

9

Feed minibatch b into the model, update the model;

10

end

11 end

12 while model is not converged do

13 Shufﬂe the training data in Xclean and Xnoisy randomly

and divide them into minibatch sets Bclean and Bnoisy; 14 Select each minibatch randomly from Bclean and Bnoisy
and feed it into the model iteratively to update the

model;

15 end

are randomly generated including room dimensions, speaker locations, and microphone geometry2.
To train the model, we used the spatialized wsj1-2mix data with J = 2 speakers as well as the train si284 training set from the WSJ1 dataset to regularize the training procedure. All input data are raw waveform audio signals. The STFT was computed using 25 mswidth Hann window with 10 ms shift, with zero-padding resulting in a spectral dimension F = 257. In our experiments, we only report results in the case of C = 2 channels, but our model is ﬂexible and can be used with an arbitrary number of channels. We ﬁrst report recognition and separation results in the anechoic scenario in Sections 3.2 and 3.3. Then we show preliminary results in the reverberant scenario in Section 3.4.
3.1. Conﬁgurations
Our end-to-end multi-channel multi-speaker model is completely built based on the ESPnet framework [32] with Pytorch backend. All the network parameters were initialized randomly from uniform distribution in the range [−0.1, 0.1]. We used AdaDelta with ρ = 0.95 and = 1e−8 as optimization method. The maximum number of epochs for training is set to 15 but the training process is stopped early if performance does not increase for 3 consecutive epochs. For decoding, a word-based language model [33] was trained on the transcripts of the WSJ corpus.
3.1.1. Neural Beamformer
The mask estimation network is a 3-layer bidirectional long-short term memory with projection (BLSTMP) network with 512 cells in
2The spatialization toolkit is available at http://www.merl.com/ demos/deep-clustering/spatialize_wsj0-mix.zip

each direction. The computation of the reference microphone vector has the same parameters as in [23] except the vector dimension which is here set to 512. In the MVDR formula of Eq. 3, a small value is added to the PSD matrix to guarantee that an inverse exists.
3.1.2. Encoder-Decoder Network
The encoder network consists of two VGG-motivated CNN blocks and three BLSTMP layers. The CNN layers have a kernel size of 3 × 3 and the number of feature maps is 64 and 128 in the ﬁrst and second block, respectively. Every BLSTMP layer has 1024 memory cells in each direction with projection size 1024. 80 dimensional log ﬁlterbank features are extracted for each separated speech signals and global mean-variance normalization is applied, using the statistics of the single-speaker WSJ1 training set. In the decoder network, there is only a single layer of unidirectional long-short term memory network (LSTM) and the number of cells is 300. The interpolation factor λ of the loss function in Eq. 12 is set to 0.2.
3.2. Performance of Multi-Speaker Speech Recognition
In this subsection, we describe the speech recognition performance on the spatialized anechoic wsj1-2mix data, which only modiﬁes the signals via delays and decays due to the propagation. Note that although beamforming algorithms can address the anechoic case without too much effort, it is still necessary to show that our proposed end-to-end method can address the multi-channel multispeaker speech recognition problem and both the speech recognition submodule and the neural beamforming separation submodule perform well as they are designed. We shall also note that the whole system is trained solely through an ASR objective, and it is thus not trivial for the system to learn how to properly separate the signals even in the anechoic case.
The multi-speaker speech recognition performance is shown in Table 1. There are three single-channel end-to-end speech recognition baseline systems. The ﬁrst one is a single-channel multi-speaker ASR model trained on the ﬁrst channel of the spatialized corpus, where the model is the same as the one proposed in [15]. The second is a single-channel multi-speaker ASR model trained with speech that is enhanced by BeamformIt [34], which is a well-known delayand-sum beamformer. And the third one is to use BeamformIt to ﬁrst separate the speech by choosing its best and second-best output streams, and then to recognize them with a normal single-speaker end-to-end ASR model. The spatialization of the corpus results in a degradation of the performance: the multi-speaker ASR model trained with the 1st channel has a word error rate (WER) of 29.43% on the evaluation set, compared to only 20.43% obtained on the original unspatialized wsj1-2mix data in [15]. Using the BeamformIt tool to enhance the spatialized signal can improve the recognition accuracy of a multi-speaker model, leading to a WER of 21.75% on the evaluation set. However, traditional beamforming algorithms such as BeamformIt can not perfectly separate the overlapped speech signals, and the performance of the single-speaker model in terms of WER is very poor, 98.00%.
The performance of our proposed end-to-end multi-channel multi-speaker model (MIMO-Speech) is shown at the bottom of the table. The curriculum learning strategy described in Section 2.2 is used to further improve performance. From the table, it can be observed that MIMO-Speech is signiﬁcantly better than traditional methods, achieving 4.51% character error rate (CER) and 8.62% word error rate (WER). Compared against the best baseline model,

the relative improvement is over 60% in terms of both CER and WER. When applying our data scheduling scheme by sorting the multi-speaker speech in ascending order of SNRs, an additional performance boost can be realized. The ﬁnal CER and WER on the evaluation set are 3.75% and 7.55% respectively, with over 12% relative improvement against MIMO-Speech without curriculum learning. Overall, our proposed MIMO-Speech network can achieve good recognition performance on the spatialized anechoic wsj1-2mix corpus.

Table 1. Performance in terms of average CER and WER [%] on the spatialized anechoic wsj1-2mix corpus.

Model 2-spkr ASR (1st channel) BeamformIt Enhancement (2-spkr ASR) BeamformIt Separation (1-spkr ASR) MIMO-Speech + Curriculum Learning (SNRs)
Model 2-spkr ASR (1st channel) BeamformIt Enhancement (2-spkr ASR) BeamformIt Separation (1-spkr ASR) MIMO-Speech + Curriculum Learning (SNRs)

dev CER 22.65 15.23 77.30 7.29 6.34
dev WER 34.98 26.61 98.60 13.54 12.59

eval CER 19.07 12.45 77.10 4.51 3.75
eval WER 29.43 21.75 98.00 8.62 7.55

(a) Mask for Speaker 1

(b) Mask for Speaker 2

(c) Separated Speech for Speaker 1 (d) Separated Speech for Speaker 2

3.3. Performance of Multi-Speaker Speech Separation
One question regarding our model is whether the front-end of MIMO-Speech, the neural beamformer, learns a proper beamforming behavior as other algorithms do since there is no explicit speech separation criterion to optimize the network. To investigate the role of the neural beamformer, we consider the masks mi that are used to compute the PSD matrices and the enhanced separated STFT signals ˆsi, i = 1, . . . , J obtained at the output of the beamformer. Example results are shown in Fig. 2. Note that in our model, the masks are not constrained to sum to 1 at each time-frequency unit, resulting in a scaling indeterminacy within each frequency. For better readability in the ﬁgures, we here renormalize each mask using its median within each frequency. In the ﬁgure, the difference between the masks from each speaker is clear. And from the spectrogram, it is also observed that for each separated stream, the signals are less overlapped compared with the input multi-speaker speech signal. The mask and spectrogram examples suggest that MIMO-Speech can separate the speech to some level.
To evaluate the separation quality, we reconstruct the separated waveforms for each speaker from the outputs of the beamformer via inverse STFT, and compare them with the reference signals in terms of PESQ and scale-invariant signal-to-distortion ratio (SI-SDR) [35]. The results are shown in Table 2. As we can see, the separated audios have very good quality. The separated signals from the MIMOSpeech model 3 have an average PESQ value of 3.6 and an average SI-SDR of 23.1 dB. When using curriculum learning, PESQ and SISDR degrade slightly, but the quality is still very high. This result suggests that our proposed MIMO-Speech model is capable of learning to separate overlapped speech via beamforming, based solely on an ASR objective.
3Audio samples are available online at https://simpleoier. github.io/MIMO-Speech/index.html

(e) Overlapped Speech
Fig. 2. Example of masks output by the masking network and separated speech log spectrograms output by the MVDR beamformer.
In order to further explore the neural beamformer’s effect, we show an example of estimated beam pattern [36] for the separated sources. Figure 3 shows the beam pattern of two separated signals at frequencies {500 Hz, 1000 Hz, 2000 Hz, 4000 Hz}. The value of the beam at different degrees quantiﬁes the reduction of the speech signals received. As we can see from the ﬁgures, the crests and troughs of the beams are different for the two speakers, which shows the neural beamformer is trained properly and can tell the difference between the sources.
3.4. Evaluation on the spatialized reverberant data
To give a comprehensive analysis of the MIMO-Speech model, we investigated how the model performs in a more realistic case, using the spatialized reverberant wsj1-2mix data. As a comparison, we ﬁrst trained a normal single-speaker end-to-end speech recognition system. The model is trained with the spatialized reverberant speech from each single speaker. The performance is shown in Table 3. For the MIMO-Speech model, the spatialized reverberant wsj1-2mix training dataset was added to the training set for

Table 2. Performance in terms of average PESQ and SI-SDR [dB] on the spatialized anechoic wsj1-2mix corpus.

Model MIMO-Speech + Curriculum Learning (SNRs)
Model MIMO-Speech + Curriculum Learning (SNRs)

dev PESQ 3.6 3.7
dev SI-SDR 22.1 21.1

eval PESQ 3.6 3.6
eval SI-SDR 23.1 21.8

Table 4. Performance in terms of average CER and WER [%] on the spatialized wsj1-2mix corpus of MIMO-Speech trained on either anechoic (A) or reverberant (R) and evaluated on either the anechoic (A) or reverberant (R) evaluation set.

Model MIMO-Speech (A) MIMO-Speech (R)
Model MIMO-Speech (A) MIMO-Speech (R)

eval CER (A) 4.51 4.08
eval WER (A) 8.62 8.72

eval CER (R) 62.32 18.15
eval WER (R) 81.30 29.99

(a)Speaker 1

Table 5. Performance in terms of average CER and WER [%] on the spatialized wsj1-2mix corpus of MIMO-Speech trained on either anechoic (A) or reverberant (R) and evaluated on the reverberant data after Nara-WPE dereverberation (D).

Model MIMO-Speech (A) MIMO-Speech (R)
Model MIMO-Speech (A) MIMO-Speech (R)

dev CER (D) 51.00 20.09
dev WER (D) 69.08 33.09

eval CER (D) 52.02 15.04
dev WER (D) 69.42 25.28

(b) Speaker 2 Fig. 3. Example of beam patterns of the separated speech.

the multi-conditioned training. The results on the speech recognition task are shown in Table 4. The reverberant speech is difﬁcult to recognize as the performance shows severe degradation when we tried to infer the reverberant speech using the anechoic multi-speaker model. The multi-conditioned training can release such degradation, improving the WER by over 60%. The results suggest that the proposed MIMO-Speech also has potential for application in complex scenarios. As a complementary experiment, we used Nara-WPE [37] to perform speech dereverberation only for the development and evaluation data. The speech recognition results are shown in Table.5 which suggests that the speech dereverberation techniques only in the inference stage can lead to further improvement. Note that the results here are just a preliminary study. The main drawback here is that we did not consider any dereverberation techniques in designing our model.

Table 3. Performance in terms of average CER and WER [%] of the baseline single-speaker end-to-end speech recognition model trained on reverberant (R) single-speaker speech and evaluated on reverberant (R) multi-speaker speech.

Model End-to-End Model (R)
Model End-to-End Model (R)

dev CER (R) 81.6
dev WER (R) 103.9

eval CER (R) 82.7
eval WER (R) 104.2

4. CONCLUSION
In this paper, we have proposed an end-to-end multi-channel multispeaker speech recognition model called MIMO-Speech. More speciﬁcally, the model takes multi-speaker speech recorded by a microphone array as input and outputs text sequences for each speaker. Furthermore, the front-end of the model, involving a neural beamformer, learns to perform speech separation even though no explicit signal reconstruction criterion is used. The main advantage of the proposed approach is that the whole model is differentiable and can be optimized with an ASR loss as target. In order to make the training easier, we utilized single-channel single-speaker speech as well. We also designed an effective curriculum learning strategy to improve the performance. Experiments on a spatialized version of the wsj1-2mix corpus show that the proposed framework has fairly good performance. However, performance on reverberant data still suffers from a large gap against the anechoic case. In future work, we will continue to improve this system by incorporating dereverberation strategies as well as by integrating further the masking and beamforming approaches.
5. ACKNOWLEDGEMENT
Wangyou Zhang and Yanmin Qian were supported by the China NSFC projects (No. 61603252 and No. U1736202).
We are grateful to Xiaofei Wang for sharing his code for beam pattern visualization.

6. REFERENCES
[1] E. C. Cherry, “Some experiments on the recognition of speech, with one and with two ears,” The Journal of the Acoustical Society of America, vol. 25, no. 5, 1953.
[2] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2016.
[3] Y. Isik, J. Le Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-channel multi-speaker separation using deep clustering,” in Proc. ISCA Interspeech, Sep. 2016.
[4] T. Menne, I. Sklyar, R. Schlu¨ter, and H. Ney, “Analysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech,” arXiv preprint arXiv:1905.03500, 2019.
[5] S. Settle, J. Le Roux, T. Hori, S. Watanabe, and J. R. Hershey, “End-to-end multi-speaker speech recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2018.
[6] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multitalker speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2017.
[7] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 25, no. 10, 2017.
[8] D. Yu, X. Chang, and Y. Qian, “Recognizing multi-talker speech with permutation invariant training,” in Proc. ISCA Interspeech, Aug. 2017.
[9] Y. Qian, X. Chang, and D. Yu, “Single-channel multitalker speech recognition with permutation invariant training,” Speech Communication, vol. 104, 2018.
[10] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in Proc. International Conference on Machine Learning (ICML), Jun. 2014.
[11] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2016.
[12] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2017.
[13] T. Hori, S. Watanabe, and J. Hershey, “Joint CTC/attention decoding for end-to-end speech recognition,” in Proc. Annual Meeting of the Association for Computational Linguistics (ACL), vol. 1, Jul. 2017.
[14] H. Seki, T. Hori, S. Watanabe, J. Le Roux, and J. R. Hershey, “A purely end-to-end system for multi-speaker speech recognition,” in Proc. Annual Meeting of the Association for Computational Linguistics (ACL), Jul. 2018.

[15] X. Chang, Y. Qian, K. Yu, and S. Watanabe, “End-to-end monaural multi-speaker ASR system without pretraining,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2019.
[16] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, and F. Alleva, “Recognizing overlapped speech in meetings: A multichannel separation approach using neural networks,” in Proc. ISCA Interspeech, Sep. 2018.
[17] Z.-Q. Wang, J. Le Roux, and J. R. Hershey, “Multi-channel deep clustering: Discriminative spectral and spatial embeddings for speaker-independent speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2018.
[18] J. Heymann, L. Drude, and R. Haeb-Umbach, “Neural network based spectral mask estimation for acoustic beamforming,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2016.
[19] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, and J. Le Roux, “Improved MVDR beamforming using singlechannel mask prediction networks,” in Proc. ISCA Interspeech, Sep. 2016.
[20] T. Menne, J. Heymann, A. Alexandridis, K. Irie, A. Zeyer, M. Kitza, P. Golik, I. Kulikov, L. Drude, R. Schlu¨ter et al., “The RWTH/UPB/FORTH system combination for the 4th CHiME challenge evaluation,” in Proc. CHiME workshop, Sep. 2016.
[21] J. Heymann, L. Drude, C. Boeddeker, P. Hanebrink, and R. Haeb-Umbach, “Beamnet: End-to-end training of a beamformer-supported multi-channel ASR system,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2017.
[22] W. Minhua, K. Kumatani, S. Sundaram, N. Stro¨m, and B. Hoffmeister, “Frequency domain multi-channel acoustic modeling for distant speech recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2019.
[23] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, “Multichannel end-to-end speech recognition,” in Proc. International Conference on Machine Learning (ICML), 2017.
[24] S. Braun, D. Neil, J. Anumula, E. Ceolini, and S.-C. Liu, “Multi-channel attention for end-to-end speech recognition.” in Proc. ISCA Interspeech, Sep. 2018.
[25] X. Wang, R. Li, S. H. Mallidi, T. Hori, S. Watanabe, and H. Hermansky, “Stream attention-based multi-array end-to-end speech recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2019.
[26] A. Shanmugam Subramanian, X. Wang, S. Watanabe, T. Taniguchi, D. Tran, and Y. Fujita, “An investigation of endto-end multichannel speech recognition for reverberant and mismatch conditions,” arXiv preprint arXiv:1904.09049, Apr. 2019.
[27] T. Ochiai, S. Watanabe, and S. Katagiri, “Does speech enhancement work with end-to-end ASR objectives?: Experimental analysis of multichannel end-to-end ASR,” in Proc. International Workshop on Machine Learning for Signal Processing (MLSP), Sep. 2017.

[28] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, C. Yu, W. J. Fabian, M. Espi, T. Higuchi et al., “The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices,” in Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Dec. 2015.
[29] M. Souden, J. Benesty, and S. Affes, “On optimal frequencydomain multichannel linear ﬁltering for noise reduction,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 2, 2009.
[30] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,” in Proc. International Conference on Machine Learning (ICML), Jun. 2009.
[31] D. Amodei et al., “Deep Speech 2: End-to-end speech recognition in English and Mandarin,” in Proc. International Conference on Machine Learning (ICML), Jun. 2016.
[32] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen et al., “ESPnet: End-to-end speech processing toolkit,” arXiv preprint arXiv:1804.00015, 2018.

[33] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recognition with word-based RNN language models,” in Proc. IEEE Spoken Language Technology Workshop (SLT), Dec. 2018.
[34] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 7, Sep. 2007.
[35] J. Le Roux, S. T. Wisdom, H. Erdogan, and J. R. Hershey, “SDR – half-baked or well done?” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2019.
[36] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A consolidated perspective on multimicrophone speech enhancement and source separation,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 25, no. 4, 2017.
[37] L. Drude, J. Heymann, C. Boeddeker, and R. Haeb-Umbach, “NARA-WPE: A Python package for weighted prediction error dereverberation in Numpy and Tensorﬂow for online and ofﬂine processing,” in ITG Fachtagung Sprachkommunikation (ITG), Oct. 2018.

