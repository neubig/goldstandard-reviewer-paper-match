arXiv:2204.06457v1 [cs.CL] 13 Apr 2022

The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer
Pavel Eﬁmov1, Leonid Boytsov2, Elena Arslanova3, Pavel Braslavski3,4 1ITMO University, Saint Petersburg, Russia
2Bosch Center for Artiﬁcial Intelligence, Pittsburgh, USA 3Ural Federal University, Yekaterinburg, Russia 4HSE University, Moscow, Russia
pavel.vl.efimov@gmail.com, leo@boytsov.info contilen@gmail.com, pbras@yandex.ru
Abstract
Large pre-trained multilingual models such as mBERT and XLM-R enabled effective crosslingual zero-shot transfer in many NLP tasks. A cross-lingual adjustment of these models using a small parallel corpus can potentially further improve results. This is a more data efﬁcient method compared to training a machine-translation system or a multi-lingual model from scratch using only parallel data. In this study, we experiment with zero-shot transfer of English models to four typologically different languages (Spanish, Russian, Vietnamese, and Hindi) and three NLP tasks (QA, NLI, and NER). We carry out a cross-lingual adjustment of an off-the-shelf mBERT model. We conﬁrm prior ﬁnding that this adjustment makes embeddings of semantically similar words from different languages closer to each other, while keeping unrelated words apart. However, from the paired-differences histograms introduced in our work we can see that the adjustment only modestly affects the relative distances between related and unrelated words. In contrast, ﬁne-tuning of mBERT on English data (for a speciﬁc task such as NER) draws embeddings of both related and unrelated words closer to each other. The cross-lingual adjustment of mBERT improves NLI in four languages and NER in two languages, while QA performance never improves and sometimes degrades. When we ﬁne-tune a cross-lingual adjusted mBERT for a speciﬁc task (e.g., NLI), the cross-lingual adjustment of mBERT may still improve the separation between related and related words, but this works consistently only for the XNLI task. Our study contributes to a better understanding of cross-lingual transfer capabilities of large multilingual language models and of effectiveness of their cross-lingual adjustment in various NLP tasks.
1 Introduction
Natural disasters, military operations, or disease outbreaks require a quick launch of information systems relying on human language technologies. Such systems need to provide instant situational awareness based on sentiment analysis, named entity recognition (NER), information retrieval, and question answering (QA) (Roussinov et al., 2008; Voorhees et al., 2020; Chan and Tsai, 2019; Strassel and Tracey, 2016). The quality of these techniques heavily depends on the existence of annotated data, which is particularly challenging in low-resource languages. Large language models pre-trained on a large multilingual corpus such as mBERT or XLM-R enable a zero-shot cross-lingual transfer by learning to produce contextualized word representations, which are (to some degree) language-independent (Libovicky` et al., 2019; Pires et al., 2019). These representations can be further aligned using a modest amount of parallel data, which was shown to improve zero-shot transfer for syntax parsing, natural language inference (NLI), and NER (Kulshreshtha et al., 2020; Wang et al., 2019b; Wang et al., 2019a). This approach requires less data and is a more computationally efﬁcient alternative to training a machine translation system or a pre-training a large multilingual model on a large parallel corpus.
The most common approach is to ﬁnd a rotation matrix using a bilingual dictionary or a parallel corpus that brings vector representation of related words in different languages closer to each other. Different from post hoc rotation-based alignment, Cao et al. (2020) employed parallel data for direct cross-lingual adjustment of the mBERT model. They showed it to be more effective than rotation in cross-lingual NLI and parallel sentence retrieval tasks in ﬁve European languages.

However, we are not aware of any systematic study of the effectiveness of this procedure across typologically diverse languages and different NLP tasks. To ﬁll this gap, we ﬁrst adjust mBERT using parallel data (English vs. Spanish, Russian, Vietnamese, and Hindi) with an objective to make embeddings of semantically similar words (in different languages) to be closer to each other. Then, we ﬁne-tune crosslingually adjusted mBERT models for three NLP tasks (NLI, NER, and QA) using English data. Finally, we apply the trained models to the test data in four target languages in a zero-shot fashion (i.e., without ﬁne-tuning in the target language). We perform each experiment with ﬁve seeds and assess statistical signiﬁcance of the difference from a baseline. In our study we ask the following research questions:
R1 How does cross-lingually adjusted mBERT subsequently ﬁne-tuned on English data and zero-shot transferred to a target language perform on various NLP tasks and target languages?
R2 How do the size of the parallel corpus used for adjustment and the choice of a subword aggregation approach affect outcomes?
R3 How do adjustment of mBERT on parallel data and ﬁne-tuning for a speciﬁc task affect similarity of contextualized embeddings of semantically related and unrelated words across languages?
Our experiments demonstrate the following:
• The cross-lingual adjustment of mBERT improves NLI in four languages (by one point) and NER in two languages (by 1.5-2.5 points). Yet, there is no statistically signiﬁcant improvement for QA and a statistically signiﬁcant deterioration on three out of eight QA datasets.
• Although a choice of subword aggregation approach slightly affects outcomes, there is no single pattern across all tasks and languages.
• As the amount of parallel data increases, this clearly beneﬁts both NLI and NER, whereas QA performance peaks at roughly 5K parallel sentences and further decreases as the number of parallel sentences increases.
• When comparing similarity of contextualized-embeddings of words across languages (Fig. 1 and 3), we see that the cross-lingual adjustment of mBERT increases the cosine similarity between related words while keeping unrelated words apart. However from the paired-differences histograms (Fig. 2 and 4) introduced in our work, we can see that the adjustment only modestly affects the relative distances between related and unrelated words.
• Prior work did not inspect histograms obtained after ﬁne-tuning. Quite surprisingly, we ﬁnd that ﬁne-tuning of mBERT for a speciﬁc task draws embeddings of both related and unrelated words much closer to each other (Fig. 1c). When we ﬁne-tune a cross-lingual adjusted mBERT for a speciﬁc task (e.g., NLI), the cross-lingual adjustment of mBERT may still improve the separation between related and unrelated words, but this works consistently only for the NLI task.
In summary, our study contributes to a better understanding of (1) cross-lingual transfer capabilities of large multilingual language models and of (2) effectiveness of their cross-lingual adjustment in various NLP tasks.
2 Related Work
2.1 Cross-Lingual Zero-Shot Transfer with Multilingual Models The success of mBERT in cross-language zero-shot regime on many tasks inspired many papers that attempted to explain its cross-lingual abilities and limitations (Wu and Dredze, 2019; Conneau et al., 2020; K et al., 2020; Libovicky` et al., 2019; Dufter and Schu¨tze, 2020; Chi et al., 2020; Pires et al., 2019; Artetxe et al., 2020; Chi et al., 2020). These studies showed that the multilingual models learn high-level abstractions common to all languages. As a result, transfer is possible even when languages share no vocabulary. However, the gap between performance on English and a target language is smaller

if the languages are cognate, i.e. share a substantial portion of model’s vocabulary, have similar syntactic structures, and are from the same language family (Wu and Dredze, 2019; Lauscher et al., 2020). Moreover, the size of target language data used for pre-training and the size of the model vocabulary allocated to the language also positively impacts cross-lingual learning performance (Lauscher et al., 2020; Artetxe et al., 2020).
Zero-shot transfer of mBERT or other multilingual transformer-based models from English to a different language was applied inter alia to POS tagging, cross-lingual information retrieval, dependency parsing, NER, NLI, and QA (Wu and Dredze, 2019; Wang et al., 2019b; Pires et al., 2019; Hsu et al., 2019; Litschko et al., 2021). XTREME data suite (Hu et al., 2020) and its successor XTREME-R (Ruder et al., 2021) are a collection of tasks and corresponding datasets for evaluation of zero-shot transfer capabilities of large multilingual models from English to tens of languages. XTREME includes NLI, NER, and QA datsets used in the current study. Authors state that performance on question answering on XTREME has improved only slightly since its inception in contrast to a more impressive progress in e.g. classiﬁcation and retrieval tasks. Although transfer from English is not always an optimal choice (Lin et al., 2019; Turc et al., 2021), English still remains the most popular source language. Furthermore, despite there have been developed quite a few new models that differ in architectures, supported languages, and training data (Doddapaneni et al., 2021), mBERT remains the most popular cross-lingual model.
2.2 Cross-lingual Alignment of Embeddings
Mikolov et al. demonstrated that vector spaces can encode semantic relationships between words and that there are similarities in the geometry of these vectors spaces across languages (Mikolov et al., 2013). A variety of approaches have been proposed for aligning monolingual representations based on bilingual dictionaries and parallel sentences. The most widely used approach—which requires only a bilingual dictionary—consists in ﬁnding a rotation matrix that aligns vectors of two monolingual models (Mikolov et al., 2013). Lample et al. (2018) proposed an alignment method based on adversarial training, which does not require parallel data. A comprehensive overview of alignment methods for pre-Transformer models can be found in (Ruder et al., 2019).
Schuster et al. (2019) applied rotation method to align contextualized ELMo embeddings (Peters et al., 2018) using anchors (averaged vectors of tokens in different contexts) and bilingual dictionaries. They showed improved results of cross-lingual dependency parsing using English as source and several European languages as target languages. Wang et al. (2019a) aligned English BERT and mBERT representations using rotation method and Europarl parallel data (Koehn, 2005). They employed the resulting embeddings in a cross-lingual dependency parsing model. The parser with aligned embeddings consistently outperformed zero-shot mBERT on 15 out of 17 target languages.
Instead of aligning on a word level, Aldarmaki and Diab (2019) performed a sentence-level alignment of ELMo embeddings and evaluated this approach on the parallel sentence retrieval task.
Cao et al. (2020) proposed to directly modify the mBERT model by making the representations of semantically related words in different languages to be closer to each other. This work was motivated by the observation that embedding spaces of different languages are not always isometric (Søgaard et al., 2018) and, hence, are not always amenable to alignment via rotation. Cao et al. (2020) showed that mBERT simultaneously adjusted on ﬁve European languages consistently outperformed other alignment approaches on XNLI data (Conneau et al., 2018). In the current study, we implement the approach of Cao et al. (2020) with some modiﬁcations.
Kulshreshtha et al. (2020) compared different alignment methods (rotation vs. adjustment). They evaluated the modiﬁed embeddings on NER and slot ﬁlling tasks. According to their results, rotation-based alignment performs better on the NER task, while model adjustment performs better on slot ﬁlling. Zhao et al. (2021) continued this line of research and proposed several improvements of the model alignment method: 1) z-normalization of vectors and 2) text normalization to make the input more structurally ‘similar’ to English training data. Experiments on XNLI dataset and translated sentence retrieval showed that vector normalization leads to more consistent improvements over zero-shot baseline compared to text normalization.

3 Methods

In this study, we use a multilingual BERT (mBERT) as the main model (Devlin et al., 2019). mBERT is a case-sensitive “base” 12-layer Transformer model (Vaswani et al., 2017) with 178M parameters.1 It

was trained with a masked language model objective on 104 languages with a shared WordPiece (Wu et

al., 2016) vocabulary (using 104 Wikipedias). To balance the distribution of languages, high-resource languages were under-sampled and low-resource languages were over-sampled.2 For a number of NLP

tasks, cross-lingual transfer of mBERT can be competitive with training a monolingual model using the

training data in the target language (see Section 2).

We align cross-lingual embeddings by directly modifying/adjusting the language model itself, follow-

ing the approach proposed recently by Cao et al. (2020). The approach—which differs from ﬁnding a

rotation matrix—proved to be effective in the XNLI task. However, there are some differences in our im-

plementation. In all cases, we work with one pair of languages at a time while Cao et al. (2020) adjusted

mBERT for ﬁve languages at once. Our approach allows us to carry out a parameter-sensitivity analysis

individually for each of the target languages.

BERT uses WordPiece tokenization (Wu et al., 2016), which splits sufﬁciently long words into sub-

word tokens. We ﬁrst word-align parallel data with fast align (Dyer et al., 2013) and then employ three

common approaches to aggregate subword token vectors into a single vector representing a word: 1) using the vector of the ﬁrst token3; 2) using the vector of the last token (Cao et al., 2020); 3) averaging of all

subword tokens. We also explored another variant: applying fast align directly to subword tokens pro-

duced by BERT. We use the averaging approach for our main experiments. Additionally, we assess how

the choice of the subword aggregation approach affects performance on Hindi datam see Section 5.4.

Based on alignments in parallel data, we obtain a collection of word pairs (si, ti): si from the source

language, ti from the target one. From these alignments we can obtain their mBERT vector representa-

tions f (si) and f (ti). Then, we adjust the mBERT model on aligned pairs’ vectors using the following

loss function:

L=

f (si) − f (ti) 2 +

f (sj) − f 0(sj) 2,

(1)

(si,ti)

sj

where the ﬁrst term “pulls” the embeddings in the source and target language together, while the second (regularization) term prevents source (English) representations from deviating far from their initial values in the ‘original’ mBERT f 0. Finally, a cross-lingually adjusted the mBERT model is ﬁne-tuned for a speciﬁc task.

4 Tasks and Data

4.1 Languages and Parallel Data

In our experiments we transfer models trained

on English to four languages: Spanish, Russian, Vietnamese, and Hindi. This set represents four different families (including one non-IndoEuropean language), three scripts, and two different prevalent word orders (see Table 1). All the languages are among languages that were used to train mBERT.4
We use a parallel corpus (i.e., a bitext) Wiki-

Lang
en es ru vi hi

Family
IE/Germanic IE/Romance IE/Slavic Austroasiatic IE/Indo-Aryan

Script
Latin Latin Cyrillic Latin Devanagari

Word order SVO SVO SVO SVO SOV

Number of English Wiki pages 6.3M 1.7M 1.7M 1.3M 150K

IE : Indo-European; Prevalent word order: SVO – subject-verb-object, SOV – subject-object-verb;

Table 1: Language information.

Matrix (Schwenk et al., 2021) to align embed-

dings. WikiMatrix is a large collection of aligned sentences in 1,620 different language pairs mined

1https://huggingface.co/bert-base-multilingual-cased 2https://github.com/google-research/bert/blob/master/multilingual.md 3Wang et al. (2019b) used this variant in their experiments and report that other options don’t induce much difference. 4However, Hindi Wikipedia is an order of magnitude smaller compared to other Wikipedias, which may have led to somewhat inferior contextualized embeddings.

from Wikipedia. The dataset is distributed under CC-BY-SA license. Following (Wang et al., 2019b; Kulshreshtha et al., 2020), we take 30K sentence pairs for each language pair as a ‘basic’ size.5
4.2 Natural Language Inference
Natural language inference (NLI) is task of determining the relation between two ordered sentences and classifying them into: entailment, contradiction, or “no relation”. English MultiNLI collection (Williams et al., 2018) consists of 433K sentence pairs originating from multiple genres. The XNLI dataset (Conneau et al., 2018) complements the MultiNLI training set with 2,500 development and 5,000 test examples in 15 languages, including all four target languages of the current study. Performance on XNLI is evaluated using classiﬁcation accuracy. XNLI is distributed under the CC BY-NC license.
4.3 Named Entity Recognition
Named entity recognition (NER) is a task of locating named entities in unstructured text and classifying them into predeﬁned categories such as persons, organizations, locations, etc. In our experiments, we employ the Wikiann NER corpus (Rahimi et al., 2019) that is derived from a larger “silver-standard” collection that was created fully automatically (Pan et al., 2017). Wikiann NER has data for 41 language, including all languages in the current study. The dataset is distributed under the Apache-2.0 license. The named entity types include location (LOC), person (PER), and organization (ORG). The English training set contains 20K sentences. Test sets for Spanish, Vietnamese, and Russian have 10K sentences each; for Hindi – 1K sentences. Performance is evaluated using the token-level micro-averaged F1.
4.4 Question Answering
Machine reading comprehension (MRC) is a variant of QA task. Given a question and a text paragraph, the system needs to return a continuous span of paragraph tokens as an answer. The ﬁrst large-scale MRC dataset is the English Wikipedia-based dataset SQuAD (Rajpurkar et al., 2016), which contains about 100K paragraph-question-answer triples. To create SQuAD, crowd workers were shown a Wikipedia paragraph; the task was to formulate several questions to the paragraph content and select a text span as an answer. SQuAD is available under the CC BY-SA license. SQuAD has become a de facto standard and inspired creation of analogous resources in other languages (Rogers et al., 2021).
We use SQuAD as the source dataset to train MRC models. To test the models, we use XQuAD, MLQA, and TyDi QA datasets. XQuAD (Artetxe et al., 2020) is a professional translation of 240 SQuAD paragraphs and 1,190 questions-answer pairs into 10 languages (including four languages of our study). MLQA (Lewis et al., 2020) data is available for six languages including Spanish, Vietnamese, and Hindi (but it does not have Russian). There are about 5K questions for each of our languages. TyDi QA (Clark and others, 2020) includes 11 typologically diverse languages of which we use only Russian (812 test items). SQuAD, XQuAD, and MLQA are distributed under the CC BY-SA license; TyDi QA – under the Apache-2.0 license.
Standard evaluation metrics for SQuAD-like datasets are EM (exact answer-span match) and tokenlevel F1-score. We report F1-scores because they are considered to be more robust.
5 Experimental Results and Analysis
5.1 Setup
All experiments were conducted on a single Tesla V100 16GB. For cross-lingual model adjustment we use the Adam optimizer and hyperparameters provided by Cao et al. (2020). To obtain reliable results we run ﬁve iterations (using different seeds) of model adjustment (for each conﬁguration) followed by ﬁne-tuning on down-stream tasks. For each run we sample a required number of sentences from a set of 250K parallel (WikiMatrix) sentences word-aligned with fast align. One run of model adjustment on 30K parallel sentences takes about 15 minutes.
5Cao et al. (2020) use the same magnitude of data – 50K sentence pairs for each out of ﬁve languages.

mBERT
Original Adjusted
Original Adjusted

es

ru

vi

Original XNLI

74.59 68.26 70.29

75.52* 69.39* 71.21*

Mixed-language NLI

71.22 65.02 63.12

73.10* 67.47* 67.29*

hi
59.64 60.77*
54.16 57.51*

Statistically signiﬁcant differences between original and adjusted mBERT are marked with * (p-value threshold 0.05).

Table 2: NLI results (accuracy).

mBERT Original Adjusted

es 73.33 72.02*

ru 64.53 66.80*

vi 71.71 71.08*

hi 65.54 68.11*

Statistically signiﬁcant differences between original and adjusted mBERT are marked with * (p-value threshold 0.05).

Table 3: NER results (token-level F1).

mBERT Original Adjusted
Original Adjusted
Original Adjusted

Spanish

Russian

Vietnamese

MLQA XQuAD TyDi QA XQuAD MLQA XQuAD

65.07 75.62

66.74

71.14 60.18 69.42

63.96* 74.59* 66.68

70.57 59.34* 69.97

Question in target language, paragraph in English

68.17 76.36

–

72.10 55.79 64.56

67.84 76.22

–

72.17 56.72* 66.73*

Question in English, paragraph in target language

67.38 76.52

–

67.70 64.27 68.69

66.99* 76.71

–

68.05 65.04* 68.86

Hindi MLQA XQuAD 49.05 57.63 48.81 57.64

43.28 44.36*

47.74 50.28*

55.45 55.41

58.44 58.10

Statistically signiﬁcant differences between original and adjusted mBERT models are marked with * (p-value threshold 0.05).

Table 4: Effectiveness of QA systems (F1-score).

For ﬁne-tuning on XNLI, SQuAD, and Wikiann we use parameters and scripts provided by HuggingFace.6 These scripts use a basic architecture consisting of a BERT model and a task-speciﬁc linear layer. We freeze the embedding vectors during ﬁne-tuning on down-stream tasks because during the training on English data the model ignores vectors in other languages. Fine-tuning on XNLI, SQuAD and Wikiann takes about 100 minutes, 60 minutes, and 3 minutes respectively. Including all preliminary and exploratory experiments the total computational budget was approximately 450 hours.
All reported results are averages over ﬁve runs with different seeds. We further assess signiﬁcance of differences between results for the original and adjusted mBERT using paired statistical tests. For QA and XNLI we ﬁrst average metric values for each example over different runs and then carry out a paired t-test using averaged values. For NER we concatenate example-speciﬁc predictions for all seeds and run 1,000 iterations of a permutation test for concatenated sequences (Pitman, 1937; Efron and Tibshirani, 1993).
5.2 Main Results
Results for NLI, NER, and QA tasks are summarized in Tables 2, 3, and 4, respectively. We can observe consistent and statistically signiﬁcant improvements of aligned models over zero-shot
transfer on XNLI for all languages. All gains are around one accuracy point, which is in line with Cao et al. (2020) even though we used a set of more diverse languages, presumably noisier parallel data, and a slightly different learning scheme. Results conﬁrm previous ﬁndings about cross-lingual zero-short transfer: we observe better results for the cognate target language (Spanish) and worse for less related ones (especially Hindi). We also constructed a mixed bilingual variant of XNLI test data: in each pair, we randomly swapped one of the sentences with its English counterpart. Classiﬁcation results for this “mixed” dataset are shown in the bottom part of Table 2. For all four languages, using the adjusted mBERT instead of the original one results in higher accuracy gains compared to the original mBERT.
NER results are mixed: we observe signiﬁcant gains for Russian (+2.3 F1 points) and Hindi (+2.6 F1 points) when using a cross-lingually adjusted model, while the results for Spanish and Vietnamese are worse compared to the original mBERT. At the same time, the baseline scores for Russian and Hindi are lower compared to English and Vietnamese.
When we ﬁne-tune a cross-lingually adjusted mBERT on QA tasks, we observe a statistically signif-
6https://github.com/huggingface/transformers/tree/master/examples/pytorch

icant performance degradation for both Spanish datasets as well as for Vietnamese MLQA. Again, we observe better results (on both parallel datasets MLQA and XQuAD) for the cognate target language (Spanish) and worse for less related ones (especially Hindi). It is also noteworthy that models are noticeably more accurate on XQuAD compared to MLQA, which can be due to XQuAD being a translation of SQuAD, which is, in turn, is used to train our QA models.
We also conducted experiments on cross-lingual QA using two parallel datasets: MLQA and XQuAD; results are shown in the lower part of Table 4. We explored two directions: (1) question is in a target language, but paragraph is in English; (2) a question is in English, but a paragraph is in a target language. Again, results are not consistent across languages and cross-lingual “directions”. In most cases the differences between results obtained using the original and adjusted mBERT are not statistically signiﬁcant.
K et al. (2020) showed that the quality of cross-lingual transfer was higher in the case of languages with similar word order. Hsu et al. (2019) and Zhao et al. (2021) experimented with word rearrangements for cross-lingual QA and NLI, respectively, and obtained some improvements. We trained a QA model using an English-Hindi adjusted mBERT on the SQuAD-SOV dataset released by Hsu et al. (2019), where sentences were re-arranged to Subject-Object-Verb order. This combination led to a degraded quality.7

5.3 Analysis of the Adjusted mBERT
Liu et al. (2021) observed that after ﬁne-tuning on a speciﬁc task (POS-tagging and NER) large multilingual models became worse at the tasks they were initially trained for (e.g., predicting a masked word). They also became worse at cross-lingual sentence retrieval. This result motivated us to study how cross-lingual adjustment of mBERT affected the ability of the model to capture semantic similarity in a mono-lingual and cross-lingual settings.

Mono-lingual evaluation. For the mono-

lingual, English, evaluation, we used the Stanford Contextual Word Similarity (SCWS)

orig. mBERT en-es en-ru en-vi en-hi

60.72

60.03 59.45 58.72 60.22

dataset (Huang et al., 2012). It contains contexts for around 2K word pairs along with crowdsourced (ground-truth) similarity ratings

Table 5: SCWS correlation scores of mBERT models: original vs. adjusted on 4 language pairs.

for each pair, which allows us to rank them.

We compared these ground-truth rankings of word pairs with rankings produced by an original or

adjusted mBERT. To this end, a similarity score of two words was computed using the cosine similarity

between words’ contextualized embeddings. Agreement with ground-truth data was computed using the

Spearman’s rank correlation.

According to results in Table 5, the cross-lingual adjustment leads to a modest deterioration of mono-

lingual performance of mBERT, which is in line with Liu et al. (2021). The degradation is smallest for

Spanish and Hindi.

Cross-lingual evaluation. In the cross-lingual evaluation we compared cosine similarities between contextualized embeddings in English and other languages. To this end we sampled from WikiMatrix (Schwenk et al., 2021) using two scenarios: semantically related words from parallel sentences (matched via fast align) and unrelated words sampled from unpaired sentences (nearly always unrelated). For each pair of languages and each NLP task, the sampling processed is carried out for: (1) the original mBERT, (2) an adjusted mBERT, (3) the original mBERT ﬁne-tuned for the target NLP task, (4) the adjusted mBERT ﬁne-tuned for the target NLP task.
There are two principal ways to visualize cosine similarities between related and unrelated words’ embeddings. In the unpaired approach, which was used by (Zhao et al., 2021), one independently plots histograms for cosine similarities for related and unrelated words. A plot of unpaired cosine similarities for the Hindi NLI task is shown in Figure 1. Histograms for other languages and tasks can be found in
7Manual inspection of the data revealed that all SQuAD data is lowercased, which may negatively impact QA training. Moreover, the quality of rearrangements is rather low, most obvious problem is incorrect processing of passive voice constructions.

unrelated

related

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.00.00 0.25 0(.a5)0 0.75 1.00 0.00.00 0.25 0(.b5)0 0.75 1.00 0.00.00 0.25 0(.c5)0 0.75 1.00 0.00.00 0.25 0(.d5)0 0.75 1.00

Figure 1: Histograms of cosine similarities between pairs of contextualized representations (produced by mBERT) for randomly sampled related (i.e., aligned) and unrelated word pairs from WikiMatrix (HiEn): (a) original, (b) after cross-lingual adjustment, (c) after ﬁne-tuning on English NLI data, (d) after cross-lingual adjustment and subsequent ﬁne-tuning on English NLI data.

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 (0a.0) 0.5 1.0 0.0 1.0 0.5 (0b.0) 0.5 1.0 0.0 1.0 0.5 (0c.0) 0.5 1.0 0.0 1.0 0.5 (0d.0) 0.5 1.0
Figure 2: Histograms of differences between cosine similarities of randomly sampled related and unrelated word representations produced by mBERT from WikiMatrix (Hi-En): (a) original, (b) after crosslingual adjustment, (c) after ﬁne-tuning on English NLI data, (d) after cross-lingual adjustment and subsequent ﬁne-tuning on English NLI data.

A.1 (Fig. 3). However, unpaired histograms are incomplete and potentially misleading. For example, the Figure 1a, which represents the original mBERT, shows that related words in two languages are typically closer to each other than to randomly selected unrelated words, but the histograms overlap substantially. This can lead to an incorrect conclusion that in a large number of cases a word can be closer to unrelated words than to related ones.
To address this issue, we propose to complement the unpaired histogram with a paired-differences histogram. To this end, given a pair of related words ti (a target language) and sirel (a source language) we compute a difference between a cos(ti, sirel) and an average cosine similarity between ti and the set of words {siuknrel} that are unrelated to ti (for simplicity of presentation we denote word embeddings using the same symbols as words themselves):

ii

1n

i ik

D(ti) = cos(t , srel) − n cos(t , sunrel)

k=1

Then, we estimate the distribution of differences using a sample of ti. Because nearly all words in the sample are unrelated, the average cosine similarity with an unrelated word can be accurately approximated via the average cosine similarity with all sample words. In turn, using the linearity of the inner product, this can be shown to be equal to a scaled cosine similarity to the average embedding (see A.2):

ii

1n

ik

ii

1 n sk

n i

sk

D(ti) ≈ cos(t , srel) − n cos(t , s ) = cos(t , srel) − n

cos t , ||sk||

||sk||

(2)

k=1

k=1

k=1

As we can see from both unpaired and paired histograms (Fig. 1b and 2), the cross-lingual adjustment makes embeddings of semantically similar words from different languages closer to each other while

keeping unrelated words apart, which is in line with Zhao et al. (2021). However, from the paired histograms (Fig. 2a and 2b), we can see that the adjustment only modestly affects the relative distances between related and unrelated words. Thus, inspection of only unpaired histograms can make one overestimate the degree of improvement due to the cross-lingual adjustment.
Prior work did not inspect histograms obtained after ﬁne-tuning. Yet, quite surprisingly, ﬁne-tuning the original, i.e., unadjusted, mBERT on the English NLI data (Figure 1c and 2c) makes distributions of related and unrelated words almost fully overlap, i.e. all embeddings become close to each other. In that, if we ﬁne-tune the adjusted mBERT (Fig. 1d and 2d), this procedure does reduce the gap between related and unrelated words, but the gap remains larger compared to ﬁne-tuning of the unadjusted mBERT. Thus, unlike English-only ﬁne-tuning, the cross-lingual adjustment reduces the similarity gap between related words (from different languages) while keeping unrelated words largely apart.
Remarkably, achieving this objective does not seem to be sufﬁcient for improving zero-shot transfer. For example, judging from paired-differences histograms for NER (see Figure 4 in A.1), there is little difference between ﬁnetuning adjusted and non-adjusted mBERT. Yet, we observe large improvements for Russian and Hindi, but substantial degradation for Spanish and Vietnamese (Table 3). Furthermore, unpaired histograms (Figure 3) present a less accurate picture compared to paired-differences histograms: They make us think that alignment should substantially improve Spanish results, while Spanish results are actually worse when using the adjusted mBERT.
We can see that the cross-lingual adjustment does not consistently improve QA and NER tasks. It is quite possible that degradation in monolingual performance (as shown using SCWS data, Table 5) is partially responsible for this underwhelming performance, especially for QA, whose quality degrades as the amount of parallel data used for adjustment increases (see Table 6).
Furthermore, Muttenthaler et al. (2020) and van Aken et al. (2019) showed that QA models essentially clustered answer token vectors and separated them from the rest of the paragraph token vectors using a vector representation of the question. Thus, to solve the QA task, the model learns to rely on mutual similarities among question and answer tokens (on English QA data) rather than on their actual vector representations. We, thus, hypothesize that the cross-lingual adjustment is not successful, because it aims to improve these representations rather than answer-question token similarities.

5.4 Ablations
An objective of this section analysis is to assess the impact of most important hyper-parameters, which might have substantially affected outcomes. We focus on the size of the parallel corpus, on the approach to aggregating subword embedding, and a choice of aligned tokens (is using [CLS] and/or [SEP] beneﬁcial?). In all cases we measure a performance gain/loss compared to the original, i.e., unadjusted mBERT.

Size of the parallel corpus. Because zero-

Size XNLI NER TyDi QA XQuAD

shot transfer is typically more challenging for languages with non-Latin script (see Tables 2 and 3), we initially considered experimenting

5K 10K 30K 100K

+0.48 +1.12 +1.13 +2.09

+1.48 +1.99 +2.27 +2.27

+0.59 +0.81 –0.06 –0.15

+0.16 –0.48 –0.56 –1.18

with either Russian or Hindi. Eventually, we chose Russian, because the Russian Wikipedia is much larger compared to Hindi. As a result it has a better alignment quality as indicated by

Table 6: Performance gains (compared to the original mBERT) of models aligned on En-Ru data depending on the number of sentence pairs.

higher margin scores (Schwenk et al., 2021).

We adjusted mBERT on 5K/10K/30k/100K sentence pairs and subsequently ﬁne-tuned the model on

respective tasks. As in all other experiments, we train the models with ﬁve seeds and report averaged

results. Table 6 shows that XNLI accuracy improves monotonically as the size of the parallel corpus

increases. NER scores reach a plateau after 10K sentence pairs. QA models beneﬁt from adjustment

using only a small amount of parallel data (and even slightly outperform the original mBERT baseline

when adjusted using 5K sentence pairs). QA performance peaks at roughly 5K parallel sentences and

further decreases as the number of parallel sentences increases.

An approach to subword embedding aggregation. In our main experiments, we align words by using their averaged (avg) subword embeddings, which performed best in preliminary QA experiments. However, as Table 7 shows this is not an optimal approach across all tasks and languages. For example, in the case of Hindi, we get better results using the ﬁrst token (start) on NLI and NER tasks (though differences are small for NER).
Interestingly, when we apply fast align to original WordPiece tokens (subword alignment), we obtain much worse results on all tasks except NLI. We hypothesize that a lower quality of the subword alignment approach is likely due to a small mBERT vocabulary allocated for Hindi. This leads to excessive word splitting and, consequently, to a worse alignment.

A choice of aligned tokens. In our main experiments, the mBERT adjustment procedure uses both regular word and special-word tokens [CLS] and [SEP]. In Table 8 we show ablation experiments where we exclude some of the tokens from the alignment procedure. We conjectured that in the NLI task the model relies more on the sentence-level representation through a [CLS] token. However, an almost one-point gain is achieved by aligning only words, which is reduced when the alignment additionally uses [CLS]. The same is true for the NER task. In the case of QA, aligning only the [CLS] token is suboptimal, but combining regular words with special tokens is beneﬁcial.

Mode start end avg subword alignment

XNLI +1.94 +1.59 +1.13 +1.88

NER +2.87 +2.55 +2.57 –1.00

MLQA –0.57 –1.55 –0.25 –4.71

XQuAD –0.47 –1.18 +0.01 –4.19

Adjustment by [CLS]
[CLS] [SEP] words all

XNLI +0.69 +0.74 +1.33 +1.13

NER +1.88 +1.61 +2.58 +2.57

MLQA –0.55 –0.89 –0.67 –0.25

XQuAD –0.18 –0.66 –0.30 +0.01

Table 7: Impact of subword aggregation approach (Hindi): Performance gains compared to the original mBERT.

Table 8: Impact of special and word tokens (Hindi): Performance gains compared to the original mBERT.

6 Conclusion
In this study, we experiment with zero-shot transfer of English models to four typologically different languages and three NLP tasks. The original mBERT is being compared to mBERT “adjusted” with a help of a small parallel corpus. The cross-lingual adjustment of mBERT improves NLI in four languages and NER in two languages. However, in the case of QA performance never improves and sometimes degrades.
Similar to prior work we analyze the effectiveness of the adjustment with a help of histograms for cosine similarities for contextualized word embeddings of related and unrelated words across languages. We propose to use a new type of the histogram (which plots paired differences) and demonstrate it more accurately explains effectiveness of the adjustment. Our study contributes to a better understanding of cross-lingual transfer capabilities of large multilingual language models. It also identiﬁes limitations of their cross-lingual adjustment in various NLP tasks.

References
Hanan Aldarmaki and Mona Diab. 2019. Context-aware cross-lingual mapping. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3906–3911.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In ACL, pages 4623–4637.
Steven Cao, Nikita Kitaev, and Dan Klein. 2020. Multilingual alignment of contextual word representations. In 8th International Conference on Learning Representations, ICLR 2020.
Hao-Yung Chan and Meng-Han Tsai. 2019. Question-answering dialogue system for emergency operations. International Journal of Disaster Risk Reduction, 41:101313.

Ethan A Chi, John Hewitt, and Christopher D Manning. 2020. Finding universal grammatical relations in multilingual bert. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564–5577.
Jonathan H Clark et al. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. TACL, 8:454–470.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475–2485.
Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Emerging cross-lingual structure in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6022–6034.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, June.
Sumanth Doddapaneni, Gowtham Ramesh, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh M Khapra. 2021. A primer on pretrained multilingual language models. arXiv preprint arXiv:2107.00676.
Philipp Dufter and Hinrich Schu¨tze. 2020. Identifying elements essential for bert’s multilinguality. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4423–4437.
Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644–648.
Bradley Efron and Robert Tibshirani. 1993. An Introduction to the Bootstrap. Springer.
Tsung-Yuan Hsu, Chi-Liang Liu, and Hung-yi Lee. 2019. Zero-shot reading comprehension by cross-lingual transfer learning with multi-lingual language representation model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5933–5940, November.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning, pages 4411–4421.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882.
Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth. 2020. Cross-lingual ability of multilingual BERT: an empirical study. In ICLR.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, pages 79–86.
Saurabh Kulshreshtha, Jose Luis Redondo Garcia, and Ching-Yun Chang. 2020. Cross-lingual alignment methods for multilingual BERT: A comparative study. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 933–942, November.
Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve´ Je´gou. 2018. Word translation without parallel data. In International Conference on Learning Representations.
Anne Lauscher, Vinit Ravishankar, Ivan Vulic´, and Goran Glavasˇ. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483–4499.
Patrick Lewis, Barlas Og˘uz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. MLQA: Evaluating cross-lingual extractive question answering. In ACL, pages 7315–7330.
Jindˇrich Libovicky`, Rudolf Rosa, and Alexander Fraser. 2019. How language-neutral is multilingual bert? arXiv preprint arXiv:1911.03310.

Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. 2019. Choosing transfer languages for cross-lingual learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3125–3135, July.
Robert Litschko, Ivan Vulic´, Simone Paolo Ponzetto, and Goran Glavasˇ. 2021. Evaluating multilingual text encoders for unsupervised cross-lingual retrieval. arXiv preprint arXiv:2101.08370.
Zihan Liu, Genta Indra Winata, Andrea Madotto, and Pascale Fung. 2021. Preserving cross-linguality of pretrained models via continual learning. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 64–71.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.
Lukas Muttenthaler, Isabelle Augenstein, and Johannes Bjerva. 2020. Unsupervised evaluation for question answering with transformers. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 83–90.
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946–1958.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001.
Edwin JG Pitman. 1937. Signiﬁcance tests which may be applied to samples from any populations. Supplement to the Journal of the Royal Statistical Society, 4(1):119–130.
Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Massively multilingual transfer for ner. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.
Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2021. QA dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension. arXiv preprint arXiv:2107.12708.
Dmitri Roussinov, Weiguo Fan, and Jose´ Robles-Flores. 2008. Beyond keywords: Automated question answering on the web. Communications of the ACM, 51(9):60–65.
Sebastian Ruder, Ivan Vulic´, and Anders Søgaard. 2019. A survey of cross-lingual word embedding models. Journal of Artiﬁcial Intelligence Research, 65:569–631.
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. 2021. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215–10245.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. 2019. Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1599–1613.
Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzma´n. 2021. WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351–1361.
Anders Søgaard, Sebastian Ruder, and Ivan Vulic´. 2018. On the limitations of unsupervised bilingual dictionary induction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 778–788.

Stephanie Strassel and Jennifer Tracey. 2016. LORELEI language packs: Data, tools, and resources for technology development in low resource languages. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 3273–3280.
Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, and Kristina Toutanova. 2021. Revisiting the primacy of english in zero-shot cross-lingual transfer. arXiv preprint arXiv:2106.16171.
Betty van Aken, Benjamin Winter, Alexander Lo¨ser, and Felix A Gers. 2019. How does BERT answer questions? A layer-wise analysis of transformer representations. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 1823–1832.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998–6008.
Ellen M. Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R. Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2020. TREC-COVID: constructing a pandemic information retrieval test collection. SIGIR Forum, 54(1):1:1–1:12.
Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. 2019a. Cross-lingual BERT transformation for zero-shot dependency parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5721–5727, Hong Kong, China.
Zirui Wang, Jiateng Xie, Ruochen Xu, Yiming Yang, Graham Neubig, and Jaime Carbonell. 2019b. Crosslingual alignment vs joint training: A comparative study and a simple uniﬁed framework. arXiv preprint arXiv:1910.04708.
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122.
Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833–844.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.
Wei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle Augenstein. 2021. Inducing language-agnostic multilingual representations. In Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pages 229–240.

A Appendix

A.1 Histograms of Cosine Similarities and Their Differences

unrelated

related

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

mBERT

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

Adjusted mBERT

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

mBERT for QA

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

Adjusted mBERT for QA

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

mBERT for NER

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

Adjusted mBERT for NER

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

mBERT for XNLI

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0 0.0 0.5 0.0 0.5 1.0

Adjusted mBERT for XNLI

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 0.5 0.0 Es-En 0.5 1.0 0.0 0.5 0.0 Ru-En 0.5 1.0 0.0 0.5 0.0 Vi-En 0.5 1.0 0.0 0.5 0.0 Hi-En 0.5 1.0

Figure 3: Histograms of cosine similarities between pairs of contextualized representations (produced by mBERT) for randomly sampled related (i.e., aligned) and unrelated word pairs from WikiMatrix. Columns correspond to language pairs. Rows depict histograms of the original mBERT model, its crosslingual adjustments, as well as their variants ﬁne-tuned on QA, NER, and NLI tasks.

mBERT

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

Adjusted mBERT

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

mBERT for QA

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

Adjusted mBERT for QA

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

mBERT for NER

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

Adjusted mBERT for NER

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

mBERT for XNLI

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0 0.0 1.0 0.5 0.0 0.5 1.0

Adjusted mBERT for XNLI

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0 1.0 0.5 Es0-.E0n 0.5 1.0 0.0 1.0 0.5 Ru0.-0En 0.5 1.0 0.0 1.0 0.5 V0i-.E0n 0.5 1.0 0.0 1.0 0.5 H0i-.E0n 0.5 1.0

Figure 4: Histograms of differences between cosine similarities of randomly sampled related and unrelated word representations produced by mBERT from WikiMatrix. Columns correspond to language pairs. Rows depict histograms of the original mBERT model, its cross-lingual adjustments, as well as their variants ﬁne-tuned on QA, NER, and NLI tasks.

A.2 Average Distance to Unrelated Vectors
Let {(ti, sirel)} be a set of pairs of related words, ti be a word in the target language and sirel be a related word in the source language. Let siuknrel be a set of unrelated words in the source language. For simplicity of representation we also use the same notation to represent respective word embeddings. Then, the average cosine between ti and an unrelated word vectors can be written as follows:

1n

i ik

n cos(t , sunrel)

k=1

Because nearly all the words are unrelated, this sum is nearly equal to the sum over all word contexts in

the source language. This sum can, in turn, be further rewritten using the linearity of the inner product

as follows:

1n

i k 1 n ti, sk

1

n i

sk

n

cos(t , s ) = n

||ti|| · ||sk|| = ||ti||n t ,

||sk|| =

k=1

k=1

k=1

1 n sk

n i

sk

= n

||sk|| cos t , ||sk||

(3)

k=1

k=1

According Eq. 3, the cosine similarity to an unrelated word vector averaged overall unrelated words is equal to a scaled cosine similarity to the average word vector.

