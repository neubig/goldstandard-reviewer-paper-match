INTERSPEECH 2021 ConferencingSpeech Challenge: Towards Far-ﬁeld Multi-Channel Speech Enhancement for Video Conferencing
Wei Rao1, Yihui Fu2, Yanxin Hu2, Xin Xu3, Yvkai Jv2, Jiangyu Han1, Zhongjie Jiang1, Lei Xie2, Yannan Wang1, Shinji Watanabe4,5, Zheng-Hua Tan6, Hui Bu3, Tao Yu7, Shidong Shang1
1Tencent Ethereal Audio Lab, China, 2Northwestern Polytechnical University, China, 3Beijing Shell Shell Technology Co., LTD., China, 4Carnegie Mellon University, USA, 5Johns Hopkins University, USA, 6Aalborg University, Denmark, 7Tencent Ethereal Audio Lab, USA
ellenwrao@tencent.com

arXiv:2104.00960v1 [eess.AS] 2 Apr 2021

Abstract
The ConferencingSpeech 2021 challenge is proposed to stimulate research on far-ﬁeld multi-channel speech enhancement for video conferencing. The challenge consists of two separate tasks: 1) Task 1 is multi-channel speech enhancement with single microphone array and focusing on practical application with real-time requirement and 2) Task 2 is multi-channel speech enhancement with multiple distributed microphone arrays, which is a non-real-time track and does not have any constraints so that participants could explore any algorithms to obtain high speech quality. Targeting the real video conferencing room application, the challenge database was recorded from real speakers and all recording facilities were located by following the real setup of conferencing room. In this challenge, we opensourced the list of open source clean speech and noise datasets, simulation scripts, and a baseline system for participants to develop their own system. The ﬁnal ranking of the challenge will be decided by the subjective evaluation which is performed using Absolute Category Ratings (ACR) to estimate Mean Opinion Score (MOS), speech MOS (S-MOS), and noise MOS (NMOS). This paper describes the challenge, tasks, datasets, and subjective evaluation. The baseline system which is a complex ratio mask based neural network and its experimental results are also presented. Index Terms: ConferencingSpeech challenge, multi-channel speech enhancement, multiple distributed microphone arrays, casual system, subjective evaluation.
1. Introduction
In recent years, video conferencing becomes increasingly important. It helps us to seamlessly connect with people of our choice anytime anywhere in the world and break barriers of distance among people. However, during video conference, the speech quality will be signiﬁcantly affected by background noise, reverberation, the number of recording microphones, the layout of microphone array, the acoustic and circuit design of microphone arrays, interference speakers, and so on. Effective speech enhancement plays an important role in the video conferencing system. Although the performance of speech enhancement has been improved dramatically in the past several decades [1–12], there are still a set of challenging problems that should be further addressed in the far-ﬁeld complex meeting room environments.
ConferencingSpeech 2021 challenge is proposed to stimulate research on processing the far-ﬁeld speech recorded from microphone arrays in video conferencing rooms and has the following features: 1) Focusing on the far-ﬁeld multi-channel

speech enhancement problem using multiple distributed microphone arrays in the real meeting room scenario; 2) Exploring real-time multi-channel speech enhancement methods to achieve superior perceptual quality and intelligibility of enhanced speech with low latency and no future “frame” information; 3) Targeting the real video conferencing room application, in which the challenge database is recorded from real speakers in real conference rooms. Multiple microphone arrays with 3 different geometric topologies are allocated in each recording room. The number of speakers and the distances between speakers and microphone arrays vary particularly according to the sizes of meeting rooms. Twelve different sizes and decorated materials of rooms with the presence of common meeting room noises are used for recording, which makes the reverberation and noises as the dominating factors affecting the speech quality; 4) Focusing on the development of algorithms, the challenge requires the close training condition. In other words, only provided list of open source clean speech datasets and noise dataset could be used for training; 5) The ﬁnal ranking of the challenge will be decided by subjective evaluation. The subjective evaluation will be performed using Absolute Category Ratings (ACR) to estimate Mean Opinion Score (MOS), speech MOS (S-MOS), and noise MOS (N-MOS) by the subjective evaluation platform.
1.1. Related Works
Some corpora were released to promote the research on the farﬁeld scenario. Earlier meeting corpora include ICSI [13], AMI [14], CHIL [15] and so on. The VOiCES corpus [16] is an opensourced corpus focusing on distant speech under real room conditions. But the recordings were collected from distant microphones, not from microphone arrays. The LibriCSS [17] corpus was proposed for speech separation task and consisted of multichannel audio recordings recorded in the real room instead of being generated by simulation. However, the utterances were taken from LibriSpeech and played back from a loud-speaker placed in the room, not from real speakers. The CHiME-5 database [18] simulated a dinner party scenario and collected distant multi-microphone speech recordings in everyday home environments. The DiPCo corpus [19] also imitated the dinner party scenario and collected the recordings by a single-channel close-talk microphone and ﬁve far-ﬁeld 7-microphone array devices positioned at different locations in the recording room.
Our ConferencingSpeech Challenge dataset is specially designed for the real video conferencing room and recorded from real speakers by multiple different types of microphone arrays. 12 meeting rooms with different sizes and decorative materials were used for recording. In addition, different from the recent

Room

Table 5-1 3 5-2

4-2

4-1

Microphone Array (MA) Information

1 Linear MA with non-uniformly distributed 8 microphones

2 Circular MA with 16 microphones

2

3 Circular MA with 16 microphones

1

4-1 4-2 Linear MA with uniformly distributed 8 microphones

5-1 5-2 Linear MA with uniformly distributed 8 microphones

Figure 1: The setup of microphone arrays in the meeting rooms. The red arrow points at the position of ﬁrst microphone of each MA.

challenges DNS [20, 21] focusing on single-channel, CHiME6 [22] focusing on multi-channel automatic speech recognition, and FFSVC2020 [23] focusing on multi-channel speaker veriﬁcation, the ConferencingSpeech 2021 Challenge is proposed to explore far-ﬁeld multi-channel speech enhancement methods to achieve superior perceptual quality and intelligibility of enhanced speech.

Mic 13

Mic 9

5cm

Mic 1

2. Tasks and Rules
The challenge consists of two tasks. There is no limitation on the system architecture, models, and training techniques.
Task1: Multi-channel speech enhancement with single microphone array. This task focuses on processing the speech from single linear microphone array with non-uniform distributed microphones and considering practical application with real-time requirement. No future “frame” information could be used in Task 1. Frame length should be less than or equal to 40ms. The real time factor of algorithms must be less than or equal to one on the single thread of an Intel Core i5 machine clocked at 2.4GHz or equivalent processors. The real time factor Frt is formulated as follows:

Mic 5
(a) Circular microphone array
1.1cm

Mic 1

Mic 8

(b) Linear microphone array with uniformly distributed 8 microphones

15 cm

10 cm 5 cm

20 cm 5 cm 10 cm

15 cm

Mic 1

Mic 2 Mic 3 Mic 4

Mic 5 Mic 6 Mic 7

Mic 8

(c) Linear microphone array with non-uniformly distributed 8 microphones

Figure 2: Conﬁguration of three types of microphone arrays.

Frt = Tp (1) Tt
where Tp is the processing time of given test clip; Tt is the length of test audio.
Task2: Multi-channel speech enhancement with multiple distributed microphone arrays. This task focuses on processing the speech from multiple distributed microphone arrays. There are ﬁve microphone arrays from three different geometric typologies. All speech signals from these ﬁve microphone arrays are synchronized. This task is a non-real-time track and does not have any constraints so that participants could explore any algorithms to obtain high speech quality.
3. Data Description
3.1. ConferencingSpeech 2021 Challenge Database
Aiming at the real video conferencing room scenario, the ConferencingSpeech 2021 Challenge Database was recorded with real speakers and all recording facilities are located by following the real setup of video conferencing room. To simulate most of video conferencing room scenarios, 12 rooms with different sizes and decorative materials were used for recording. The decorative materials of rooms could be categorized into four type: without glass wall, with 1 glass walls, with 2 glass walls, and with 3 glass walls. The database contains two languages: English and Chinese. The sampling rate of recordings is 16kHz.
Multiple microphone arrays from three different geometry topologies were distributed in the recording rooms to investigate

the impact of layout, acoustic and circuit design of microphone arrays on the speech quality. Figure 1 shows the recording setup of ConferencingSpeech 2021 Challenge database. The recording devices included 5 microphone arrays from 3 different geometric topologies. The red arrow in Figure 1 points to the ﬁrst channel of microphone arrays. The microphone arrays in each meeting room followed the allocation of Figure 1, but the distances among microphone arrays (MAs) varied according to the sizes of meeting room. All recordings from these 5 MAs were synchronized. The information of MAs in Figure 1 is summarized as follows:
• No.1 is a linear MA with non-uniformly distributed 8 microphones. The interval among microphones are illustrated in Figure 2(c).
• No.2 and No.3 are circular MA with 16 microphones. The radius of circular MA is 5cm as shown in Figure 2(a).
• No.4 and No.5 are linear MA. Each MA is composed of two small linear MA with uniformly distributed 8 microphones. The interval among microphones in the small linear MA is 1.1cm as shown in Figure 2(b).
The recording conditions of this database could be categorized into the following two parts:
• Semi-real recording: the data was recorded in the real meeting room scenario, but was not in the real meeting. The speech and noise data were separately recorded in all

12 quiet meeting rooms. Both were collected from playback and real speakers. Then, these speech and noise data were used for simulation. It contained two language: English and Chinese. The beneﬁt of semi-real recording is that more variations of meeting scenarios could be considered.
• Real recording: the data was recorded during the real meeting in the real meeting room. All utterances were recorded from real speakers under real noise conditions. The recording language was Chinese.
3.2. Training Set
To focus on the development of algorithms, we designed the challenge with the close training condition. In other words, only the provided list of open source clean speech datasets and noise dataset could be used for training.
3.2.1. Clean Speech
Clean training speech set signals were chosen from four open source speech databases: AISHELL-1 [24], AISHELL-3 [25], VCTK [26], and Librispeech (train-clean-360) [27]. The speech utterances with SNR higher than 15 dB were selected for training. The total duration of clean training speech is around 550 hours.
3.2.2. Noise Set
The Noise set is composed of two parts. Part I was selected from MUSAN [28] and Audioset1. The total duration is around 120 hours. Part II is the real meeting room noises recorded by high ﬁdelity devices. The total number of clips is 98.
3.2.3. Room Impulse Responses (RIR)
We used an image method to simulate RIRs for three microphone arrays introduced in Figure 2. The room size ranged from 3×3×3 m3 to 8×8×3 m3, containing more than 2500 rooms. The microphone array was randomly placed in the room with height ranging from 1.0 to 1.5 m. The sound source, including speech and noise, came from any possible position in the room with height ranging from 1.2 to 1.9 m. The angle between two sources was wider than 20°. The distance between sound source and microphone array were ranged from 0.5 to 5.0 m. The total number of RIRs was more than 10,000 for each microphone array.
3.3. Development set
The development set was categorized into three parts: Simulation clips, Semi-real recordings, and Real recordings. Semi-real and real recordings are selected from the recordings in the ConferencingSpeech 2021 Challenge Database.
3.3.1. Simulation clips
The simulation set was provided for participants to develop the systems and estimate the objective scores, which contains two sets: (1) a single MA set and (2) a multiple MA set.
For the single MA set, we simulated 1,588 clips for three types of MA. The details can be found in Section 3.1. Similar to the single MA set, multiple MA set also consistes of simulation clips from these three MAs. The only difference is that these three MAs are assumed in the same room during simulation.
1https://research.google.com/audioset/

1,624 clean speech selected from AISHELL-1, AISHELL3, and VCTK and 800 noise clips selected from MUSAN were used for the simulation of both sets. The simulated SNR ranged from 0 to 30 dB and the duration of clips was 6 seconds.
3.3.2. Semi-real recordings
As mentioned in Section 3.2, the speech sources could be divided into playback and real speakers. The Semi-real recordings consisted of 2.35 hours of playback English speech segments and 2.31 hours of real speaker’s Chinese speech segments. Each audio clip contained multiple channel information. All ﬁve MAs’ recordings were provided for participants to develop their systems.
3.3.3. Real recordings
More than 200 real recording clips were provided, which are from 12 real speakers and their ages range from 18 to 50 years old. Similar to semi-real recordings, each audio clip contained multiple channel information and all ﬁve MAs’ recordings were provided.
3.4. Evaluation set
The evaluation set consists of two task sets. We selected Audio clips from other 9 rooms’ recordings of ConferencingSpeech 2021 Challenge Database which were unseen in development test set. No. 1 MA in Figure 1 was selected for Task 1. The recordings from ﬁve MAs in Figure 1 were provided for Task 2.
Different from the development set, the evaluation set only contains the semi-real and real recordings. The evaluation set for each task is composed of three parts: semi-real recordings from playback, semi-real recordings from real speakers, and real meeting recordings. Speciﬁcally, in each task, 135 semireal recordings from playback and real speakers were selected for each MA, respectively. And 165 real recordings were selected for each MA.
4. Subjective Evaluation
The performance of each participated team was decided by subjective evaluation. Inspired from ITU-T P.835, the subjective evaluation was performed according to overall quality rating, speech signal rating, and background noise rating, for which we used Absolute Category Ratings (ACR) to estimate global Mean Opinion Score (MOS), Speech MOS (S-MOS), and Noise MOS (N-MOS), respectively. The details of evaluation metrics are as follows:
• MOS: Determination of subjective global MOS. The rater will select the category which best describes the overall quality of heard sample for the purpose of everyday speech communication. The categories of overall speech sample are 5-Excellent / 4-Good / 3-Fair / 2-Poor / 1-Bad.
• S-MOS: Determination of subjective speech MOS. The rater will only attend to the quality of speech signal. The categories of speech signal in this sample are 5-Not Distorted / 4-Slightly Distorted / 3-Somewhat Distorted / 2Fairly Distorted / 1-Very Distorted.
• N-MOS: Determination of subjective noise MOS. The rater will only attend to the background. The categories of background in this sample are 5-Not Noticeable / 4Slightly Noticeable / 3- Noticeable But Not Intrusive / 2-Somewhat Intrusive / 1-Very Intrusive.

Table 1: Results of baseline system on the development simulation set. “Dev. Simu. Set” represents development simulation set; “MA” represents microphone array; “Noisy” represents the speech utterances of development simulation set; “Enhanced” represents the enhanced speech utterances by the baseline system.

Dev. Simu. Set Single MA
Multiple MA

MA
Circular
Linear Uniform Linear Nonuniform
Circular
Linear Uniform Linear Nonuniform
SNR Selection

Noisy Enhanced
Noisy Enhanced
Noisy Enhanced
Noisy Enhanced
Noisy Enhanced
Noisy Enhanced
Noisy Enhanced

PESQ 1.514 1.990 1.534 2.035 1.515 1.999 1.513 1.997 1.514 2.007 1.506 1.983 1.524 2.023

STOI 0.824 0.888 0.829 0.893 0.823 0.888 0.826 0.890 0.826 0.891 0.824 0.887 0.830 0.893

E-STOI 0.693 0.783 0.700 0.790 0.690 0.780 0.696 0.785 0.696 0.786 0.693 0.780 0.702 0.788

Si-SNR 4.566 9.248 4.720 9.445 4.474 9.159 4.596 9.271 4.618 9.260 4.504 9.228 4.989 9.526

• dMOS/dS-MOS/dN-MOS: Difference between the MOS/S-MOS/N-MOS after enhancement and MOS/SMOS/N-MOS of the noisy evaluation set before enhancement.
• CI: Conﬁdence Interval of MOS score.
Each rater would determine MOS, S-MOS, and N-MOS scores for each evaluation audio ﬁle. And each evaluation audio ﬁle would be rated by more than 20 qualiﬁed raters. Two rounds of subjective evaluation were performed: 1) the ﬁrst round included submissions from all teams and 2) the second round included several top-scoring teams for further evaluation. The subjective evaluation results of each task were based on the combined results from both rounds. The ﬁnal ranking will be determined by MOS and released in June.

5. Baseline System

The baseline system we proposed is a complex ratio mask

(CRM) [29] based neural network. The input of the baseline

system is the multi-channel speech signals of one MA and the

output is the single-channel enhanced speech. After performing

Short Time Fourier Transform (STFT), magnitude and phase of

input signals will be obtained and then used for estimating the

inter-channel phase difference (IPD) of speciﬁed microphone

pairs:

IPDi,j = ∠ej(∠Oi−∠Oj ),

(2)

where Oi and Oj denotes the spectrum of observed signal of microphone i and j. We concatenate the real and imaginary part of the ﬁrst microphone’s observed signal, as well as the cosIPD of four microphone pairs along the frequency axis to generate X ∈ R6F ×T as the input of the neural network, where F and T denotes the number of frequency bins and frames, respectively.
A 3-layer real-valued LSTM is used to capture the temporal information of input features. Then a real-valued fully connection (FC) layer is adopted to map the output of LSTM into real and imaginary masks, respectively. CRM is applied to the spectrum of the ﬁrst microphone channel of observed signal X0 to derive the enhanced speech:

Yr = MrX0r − MiX0i,

(3)

Yi = MrX0i + MiX0r,

(4)

where Mr and Mi denote the real and imaginary part of CRM, respectively. The setup and code scripts of this baseline system are available on the github page. 2
For training, the provided ofﬁcial training data was used to simulate multi-channel far-ﬁeld noisy data by convolving single channel signal with RIR. The SNR of simulation training data ranges from 0 to 30 dB. To compress the input feature dimension, for circular MA, 8 channels signals were selected intermittently from all 16 channels signal to calculate the input features, while for linear uniform MA and linear nonuniform MA, ﬁrst 8 channels signals were selected to calculate the input features. For linear MA, IPDs were calculated among four microphone pairs: (1,5), (2,6), (3,7) and (4,8), while for circular MA, IPDs were calculated among four microphone pairs: (1,9), (3,11), (5,13) and (7,15). The frame length and frame hop of STFT/iSTFT were set to 20ms and 10 ms, respectively. The hidden layer size of the 3-layer LSTM and FC layer were set to 512 and 514, respectively. We trained the model for 18 epochs with Adam optimizer using PyTorch. The initial learning rate was set to 0.001 and would be halved if no improvement on the development set for 2 epochs. The real time factor (RTF) of the baseline system is 0.0425 on Intel Xeon clocked at 2.5 GHz.
For single MA of the development simulation set, three models for circular MA, linear uniform MA and linear nonuniform MA were trained separately, while for multiple MAs, the models trained for single MA were reused and the output with the highest estimated SNR within three models was chosen as the ﬁnal output. The SNR is calculated by:

max{10log RMS(yn) },

(5)

n∈ψ

10 RMS(yn − xn)

where RMS represents root mean square; ψ denotes the all possible microphone array and xn and yn denotes the time domain noisy and enhanced signal of microphone array n, respectively.
The results of the baseline system on the development simulation set are shown in Table 1 and they demonstrate that (1) the baseline system effectively enhanced the noisy speech of the development simulation set and (2) the performance of SNR selection in multiple MAs is slightly better than that of single MA model.

6. Conclusion
The ConferencingSpeech challenge is intended to promote farﬁeld multi-channel noise suppression and dereverberation for achieving superior subjective speech quality in real video conferencing scenarios. Two tasks are specially designed for different purposes. Task 1 is on exploring real-time multi-channel speech enhancement methods to achieve excellent perceptual quality and intelligibility of enhanced speech with low latency and no future “frame” information. Task 2 is on how to utilize the multiple distributed microphone arrays to improve the performance of speech enhancement.
Twenty one submissions have been received from seventeen teams. Five teams participated in both tasks. The ﬁnal ranking, challenge results and related analysis will be announced in June 2021. We believe that this challenge and the published datasets will promote the research and development in far-ﬁeld multichannel speech enhancement.

2https://github.com/ConferencingSpeech/ ConferencingSpeech2021

7. References
[1] P. C. Loizou, Speech enhancement: theory and practice. CRC press, 2013.
[2] T. Gerkmann, M. Krawczyk-Becker, and J. Le Roux, “Phase processing for single-channel speech enhancement: History and recent advances,” IEEE Signal Processing Magazine, vol. 32, no. 2, pp. 55–66, 2015.
[3] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A consolidated perspective on multimicrophone speech enhancement and source separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 4, pp. 692–730, 2017.
[4] D. Michelsanti, Z. H. Tan, S. X. Zhang, Y. Xu, M. Yu, D. Yu, and J. Jensen, “An overview of deep-learning-based audio-visual speech enhancement and separation,” arXiv preprint arXiv:2008.09586, 2020.
[5] C. Xu, X. Xiao, S. Sun, W. Rao, E. S. Chng, and H. Li, “Weighted spatial covariance matrix estimation for music based tdoa estimation of speech source.” in Proc. Interspeech 2017, 2017, pp. 1894– 1898.
[6] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le Roux, J. R. Hershey, and B. Schuller, “Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,” in International conference on latent variable analysis and signal separation. Springer, 2015, pp. 91–99.
[7] Z. Chen, S. Watanabe, H. Erdogan, and J. R. Hershey, “Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks,” in Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[8] D. Michelsanti and Z. H. Tan, “Conditional generative adversarial networks for speech enhancement and noise-robust speaker veriﬁcation,” arXiv preprint arXiv:1709.01703, 2017.
[9] M. Kolbæk, Z. H. Tan, and J. Jensen, “Speech intelligibility potential of general and specialized deep neural network based speech enhancement systems,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 1, pp. 153– 167, 2016.
[10] L. Chai, J. Du, and Y. Wang, “Gaussian density guided deep neural network for single-channel speech enhancement,” in 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2017, pp. 1–6.
[11] X. Hao, C. Shan, Y. Xu, S. Sun, and L. Xie, “An attention-based neural network approach for single channel speech enhancement,” in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6895–6899.
[12] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, “DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement,” Proc. Interspeech 2020, pp. 2472–2476, Oct. 2020.
[13] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke et al., “The icsi meeting corpus,” in 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03)., vol. 1. IEEE, 2003, pp. I–I.
[14] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal et al., “The AMI meeting corpus: A pre-announcement,” in International workshop on machine learning for multimodal interaction. Springer, 2005, pp. 28–39.
[15] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia et al., “The chil audiovisual corpus for lecture and meeting analysis inside smart rooms,” Language resources and evaluation, vol. 41, no. 3, pp. 389–407, 2007.

[16] C. Richey, M. A. Barrios, Z. Armstrong, C. Bartels, H. Franco, M. Graciarena, A. Lawson, M. K. Nandwana, A. Stauffer, J. van Hout et al., “Voices obscured in complex environmental settings (VOiCES) corpus,” in Proc. INTERSPEECH 2018, Hyderabad, India, Sep. 2018, p. 1566–1570.
[17] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, “Continuous speech separation: dataset and analysis,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7284–7288.
[18] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁfth ’CHiME’ speech separation and recognition challenge: dataset, task and baselines,” in Proc. Interspeech 2018, Hyderabad, India, Sep. 2018, pp. 1561––1565.
[19] M. Van Segbroeck, A. Zaid, K. Kutsenko, C. Huerta, T. Nguyen, X. Luo, B. Hoffmeister, J. Trmal, M. Omologo, and R. Maas, “DiPCo–Dinner Party Corpus,” in Proc. INTERSPEECH 2020, Shanghai, China, Oct. 2020, pp. 2492–2496.
[20] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun et al., “The INTERSPEECH 2020 deep noise suppression challenge: datasets, subjective testing framework, and challenge results,” in Proc. INTERSPEECH 2020, Shanghai, China, Oct. 2020, pp. 2492–2496.
[21] C. K. Reddy, H. Dubey, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, “ICASSP 2021 deep noise suppression challenge,” arXiv preprint arXiv:2009.06122, 2020.
[22] S. Watanabe, M. Mandel, J. Barker, and E. Vincent, “CHIME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” arXiv preprint arXiv:2004.09249, 2020.
[23] X. Qin, M. Li, H. Bu, W. Rao, R. K. Das, S. Narayanan, and H. Li, “The INTERSPEECH 2020 Far-Field Speaker Veriﬁcation Challenge,” in Proc. INTERSPEECH 2020, Shanghai, China, Oct. 2020.
[24] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “AISHELL-1: An open-source mandarin speech corpus and a speech recognition baseline,” in 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1–5.
[25] Y. Shi, H. Bu, X. Xu, S. Zhang, and M. Li, “AISHELL-3: A multi-speaker mandarin TTS corpus and the baselines,” 2020. [Online]. Available: https://arxiv.org/abs/2010.11567
[26] V. Christophe, Y. Junichi, and M. Kirsten, “CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit,” The Centre for Speech Technology Research (CSTR), 2016.
[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an ASR corpus based on public domain audio books,” in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206–5210.
[28] D. Snyder, G. Chen, and D. Povey, “MUSAN: A Music, Speech, and Noise Corpus,” 2015, arXiv:1510.08484v1.
[29] D. S. Williamson, Y. Wang, and D. Wang, “Complex ratio masking for monaural speech separation,” IEEE/ACM transactions on audio, speech, and language processing, vol. 24, no. 3, pp. 483– 492, 2015.

