SPEAKER DIARIZATION WITH REGION PROPOSAL NETWORK
Zili Huang1, Shinji Watanabe1, Yusuke Fujita2, Paola Garc´ıa1, Yiwen Shao1, Daniel Povey1, Sanjeev Khudanpur1
1 Center for Language and Speech Processing, Johns Hopkins University, USA 2 Hitachi, Ltd. Research & Development Group, Japan

arXiv:2002.06220v1 [eess.AS] 14 Feb 2020

ABSTRACT Speaker diarization is an important pre-processing step for many speech applications, and it aims to solve the “who spoke when” problem. Although the standard diarization systems can achieve satisfactory results in various scenarios, they are composed of several independently-optimized modules and cannot deal with the overlapped speech. In this paper, we propose a novel speaker diarization method: Region Proposal Network based Speaker Diarization (RPNSD). In this method, a neural network generates overlapped speech segment proposals, and compute their speaker embeddings at the same time. Compared with standard diarization systems, RPNSD has a shorter pipeline and can handle the overlapped speech. Experimental results on three diarization datasets reveal that RPNSD achieves remarkable improvements over the state-of-the-art x-vector baseline. Index Terms— speaker diarization, neural network, end-to-end, region proposal network, Faster R-CNN
1. INTRODUCTION
Speaker diarization, the process of partitioning an input audio stream into homogeneous segments according to the speaker identity [1–4] (often referred as “who spoke when”), is an important pre-processing step for many speech applications.
Fig. 1: Pipelines of the standard diarization system (left) and the RPNSD system (right)
As shown in Figure 1 left, a standard diarization system [5–8] consists of four steps. (1) Segmentation: this step removes the nonspeech portion of the audio with speech activity detection (SAD), and the speech regions are further cut into short segments. (2) Embedding extraction: in this step, a speaker embedding is extracted for each short segment. Typical speaker embeddings include i-vector

[9–12] and deep speaker embeddings [5, 13–22]. (3) Clustering: after the speaker embedding is extracted for each short segment, the segments are grouped into different clusters. Each cluster corresponds to one speaker identity. (4) Re-segmentation: this is an optional step that further reﬁnes the diarization prediction. Among the re-segmentation methods, VB re-segmentation [12, 23, 24] is the most famous one.
Despite the successful applications in many scenarios, standard diarization systems have two major problems. (1) Many individual modules: to build a diarization system, you need a SAD model, a speaker embedding extractor, a clustering module and a re-segmentation module, all of which are optimized individually. (2) Overlap: the standard diarization system cannot handle the overlapped speech. To deal with the overlapped speech, some new modules are needed to detect and classify the overlaps, which makes the procedure even more complicated. The overlapped speech will also hurt the performance of clustering, which is the main reason standard diarization systems cannot perform well in highly overlapped scenarios [25] [26].
Inspired by Faster R-CNN [27], one of the best-known frameworks in object detection, we propose Region Proposal Network based Speaker Diarization (RPNSD). As shown in Figure 1 right, in this method, we combine the segmentation, embedding extraction and re-segmentation into one stage. The segment boundaries and speaker embeddings are jointly optimized in one neural network. After the speech segments and corresponding speaker embeddings are extracted, we only need to cluster the segments and apply non-maximum suppression (NMS) to get the diarization prediction, which is much more convenient than the standard diarization system. In addition to that, since the speech segment proposals overlap with each other, our framework solves the overlap problem in a natural and elegant way.
The experimental results on Switchboard, CALLHOME and simulated mixtures reveal that our framework achieves signiﬁcant and consistent improvements over the state-of-the-art x-vector baseline, and a great portion of the improvements come from successfully detecting the overlapped speech regions. Our code is available at https://github.com/HuangZiliAndy/RPNSD.
2. METHODOLOGY
In this section, we will introduce our framework in details. Our framework aims to solve the speaker diarization problem and it consists of two steps. (1) Joint speech segment proposal and speaker embedding extraction. (2) Post-processing. In the ﬁrst step, we predict the boundary of speech segments and extract speaker embeddings with one neural network. In the second step, we perform clustering and apply NMS to get diarization predictions.

2.1. Joint Speech Segment Proposal and Embedding Extraction
Fig. 2: Procedure of the ﬁrst step: joint speech segment proposal and speaker embedding extraction
The overall procedure of the ﬁrst step is shown in Figure 2. Given an audio input, we ﬁrst extract acoustic features1 and feed them into convolution layers to obtain the feature maps. Then a Region Proposal Network (RPN) will generate many overlapped speech segment proposals [28] and predict their conﬁdence scores. After that, the deep features corresponding to the speech segment proposals are pooled into ﬁxed-size representations. Finally, we perform speaker classiﬁcation and boundary reﬁnement on the top of the representations.
2.1.1. Region Proposal Network (RPN)
The RPN [27] is the key component of our framework. It takes the feature maps as the input and outputs the region proposals. The original RPN generates 2-d region proposals while our RPN generates 1-d speech segment proposals. In our framework, the RPN takes the feature maps as the input2 and predicts speech segment proposals. Similar to brute-force search, the RPN will consider every timestep as a possible center and expand several anchors with pre-deﬁned sizes from it. In our system, we use 9 anchors with the size of {1, 2, 4, 8, 16, 24, 32, 48, 64}, which covers the speech segments from 16 to 1024 frames. Meanwhile, the RPN will also predict scores and reﬁne boundaries for each speech segment proposal with convolution layers. Among the 63×9 = 567 (63 timesteps and 9 anchors per timestep) speech segment proposals, we ﬁrst ﬁlter out the speech segment proposals with low conﬁdence scores and then further remove highly overlapped segments with NMS. In the end, we keep 100 high-quality speech segment proposals after NMS during training and 50 during evaluation.
2.1.2. RoIAlign
After the RPN predicts the speech segment proposals, we extract corresponding regions from the feature maps as the deep features for each segment. Since the sizes of speech segment proposals vary a lot, we need RoIAlign [29] to pool the features into ﬁxed dimension. Suppose we want to pool the D × T speech segment proposal (D is the feature dimension and T is the unﬁxed timestep) into a ﬁxed representation, the proposed region is ﬁrst divided into N ×N (N = 7) RoI bins. Then we uniformly sample four locations in each RoI bin and use bilinear interpolation to compute the values of them. The result is aggregated using average pooling. With the pooled feature of ﬁxed dimension, we can perform speaker classiﬁcation and boundary reﬁnement for each speech segment proposal.
1We experiment on 8kHz telephone data and we choose the STFT feature with frame size 512 and frame shift 80. During training we segment the audios into 10s chunks, so the feature shape of each chunk is (257, 1000).
2The size of the feature maps is (1024, 16, 63). There are 63 timesteps and each timestep corresponds 16 frames of speech.

2.1.3. Loss Function

The training loss consists of ﬁve parts and is formulated as

L = Lrpn cls + Lrpn reg + Lrcnn cls + Lrcnn reg + α · Lspk cls (1)

In equation 1, Lrpn cls and Lrcnn cls are binary cross-entropy loss to classify foreground/background (fg/bg), which is formulated as
Lcls(pi, p∗i ) = −(p∗i log(pi) + (1 − p∗i ) log(1 − pi)) (2)

where pi is the probability that the speech segment i is foreground and p∗i is the ground truth label. Whether a segment is fg or bg is determined by the Intersection-over-Union (IoU) overlap with the
ground-truth segments. Lrpn reg and Lrcnn reg are regression loss to
reﬁne the speech segment boundaries, which are formulated as

Lreg(ti, t∗i ) = R(ti − t∗i )

(3)

where ti and t∗i are the coordinates of predicted segments and
ground truth segments respectively, and R is the smooth L1 loss function in [30]. The coordinates ti and t∗i are deﬁned as follows.

ti = [(x − xa)/wa, log(w/wa)]

(4)

t∗i = [(x∗ − xa)/wa, log(w∗/wa)]

(5)

where x and w denote the center position and length of the segment. x, xa and x∗ represent the center positions for the predicted
segment, anchor and ground truth segment respectively (likewise for w). Lspk cls is the cross-entropy loss to classify the segments speaker identity, which is deﬁned as

Lspk cls(si, s∗i ) = −s∗i · log (si)

(6)

where si is the predicted probability distribution over all speakers in the training set and s∗i is the ground truth one-hot speaker label. Lspk cls is scaled with a weight factor α.
Among the loss components, Lrpn cls and Lrpn reg are used to train the RPN. We adopt the same strategy as [27], and sample 128 from 567 initial speech segment proposals to compute Lrpn cls and Lrpn reg. The segment proposals having an IoU overlap higher than 0.7 with any ground-truth segments are labeled as fg while the segment proposals with an IoU overlap lower than 0.3 for all ground-truth segments are labeled as bg. Lrpn reg is calculated only for the fg. Lrcnn cls and Lrcnn reg have the exactly same form but are calculated with different samples. We sample 64 from the 100 high-quality speech segments mentioned in section 2.1.1 to compute Lrcnn cls and Lrcnn reg. Lspk cls is also calculated with the 64 samples, and it ensures that we extract discriminative embeddings from the model.

2.2. Post-processing
In RPNSD, the input of the ﬁrst step is an audio and the output includes: (1) the speech segment proposals, (2) the probability of fg/bg and (3) the speaker embedding for each segment proposal. In the second step, we perform post-processing to get the diarization prediction. The whole process contains three steps.
1. Remove the speech segment proposals whose fg probability is lower than a threshold γ. (γ = 0.5 in our experiment)
2. Clustering: Group the remaining speech segment proposals into clusters. (We use K-means in our experiment)
3. Apply NMS (NMS threshold = 0.3) for segments in the same cluster to remove the highly overlapped segment proposals.

3. EXPERIMENTS
3.1. Datasets and Evaluation Metrics
3.1.1. Datasets
We train our systems on two datasets (Mixer 6 + SRE + SWBD and Simulated TRAIN) and evaluate on three datasets (Switchboard, CALLHOME and Simulated DEV) to verify the effectiveness of our framework. The dataset statistics are shown in Table 1. The overlap ratio is deﬁned as overlap ratio = ttssppkk≥ ≥21 , where tspk≥n denotes the total time of speech regions with more than n speakers. Since end-to-end systems are usually data hungry and require massive training data to generalize better, we come up with two methods to create huge amount of diarization data. (1) Use public telephone conversation datasets (Mixer 6 + SRE + SWBD). (2) Use speech data of different speakers to create synthetic diarization datasets (Simulated TRAIN). Detailed introductions for each dataset are as follows.

Train sets Mixer 6 + SRE + SWBD Simulated TRAIN(β = 2)
Test sets CALLHOME SWBD DEV SWBD TEST Simulated DEV(β = 2) Simulated DEV(β = 3) Simulated DEV(β = 5)

# utts
29,697 100,000
499 99 100 500 500 500

avg. dur (sec)
348.1 87.6
124.5 304.6 312.0 87.3 103.8 137.1

overlap ratio (%)
5.0 34.4
16.9 5.2 5.8 34.4 27.2 19.5

Table 1: Dataset statistics

The Mixer 6 + SRE + SWBD dataset includes Mixer 6, SRE0410, Switchboard-2 Phase I-III and Switchboard Cellular Part 1, 2, and the majority of the dataset are 8kHz telephone conversations. For speaker recognition, we usually use single channel audios that contain only one person. While in our experiment, we sum up both channels to create a large diarization dataset. The ground truth diarization label is generated by applying SAD on single channels.3 We also used the same data augmentation technique as [21] and the train sets are augmented with music, noise and reverberation from the MUSAN [31] and the RIR [32] dataset. The augmented train set contains 10,574 hours of speech.
SWBD DEV and SWBD TEST are sampled from the SWBD dataset (We exclude these audios from Mixer6 + SRE + SWBD). They contain around 100 5-minute audios and share no common speaker with the train set. Like Mixer 6 + SRE + SWBD, the overlap ratio of SWBD DEV and SWBD TEST is quite low. We create these two datasets to evaluate the system performance on similar data.
The CALLHOME dataset is one of the best-known benchmarks for speaker diarization. As one part of 2000 NIST Speaker Recognition Evaluation (LDC2001S97), the CALLHOME dataset contains 500 audios in 6 languages including Arabic, English, German, Japanese, Mandarin, and Spanish. The number of speakers in each audio ranges from 2 to 7.
We also use synthetic datasets (same as [25,26]) to evaluate RPNSD’s performance on highly overlapped speech. The simulated mixtures are made by placing two speakers’ speech segments in a
3In all experiments of this paper, we use the TDNN SAD model (http: //kaldi-asr.org/models/m4) trained on the Fisher corpus.

single audio ﬁle. The human voices are taken from SRE and SWBD, and we use the same data augmentation technique as [21]. The parameter β is the average length of silence intervals between segments of a single speaker, and a larger β results in less overlap. In our experiment, we generate a large dataset with β = 2 for training and three datasets with β = 2, 3, 5 for evaluation. The training set and test set share no common speaker.
3.1.2. Evaluation Metrics
We evaluate different systems with Diarization Error Rate (DER). The DER includes Miss Error (speech predicted as non-speech or two speaker mixture predicted as one speaker etc.), False Alarm Error (non-speech predicted as speech or single speaker speech predicted as multiple speaker etc.) and Confusion Error (one speaker predicted as another). Many previous studies [20, 33] ignore the overlapped regions and use 0.25s collar for evaluation. While in our study, we score the overlapped regions and report the DER with different collars.
3.2. Baseline
We follow Kaldi’s CALLHOME diarization V2 recipe [34] to build baselines. The recipe uses oracle SAD labels which are not available in real situations, so we ﬁrst use a TDNN SAD model to detect the speech segments. Then the speech segments are cut into 1.5s chunks with 0.75s overlap, and x-vectors are extracted for each segment. After that, we apply Agglomerative Hierarchical Clustering (AHC) to group segments into different clusters, and the similarity matrix is based on PLDA [35] scoring. We also apply VB re-segmentation for CALLHOME experiments.
3.3. Experimental Settings
We use ResNet-101 as the network architecture and Stochastic Gradient Descent (SGD) as the optimizer 4. We start training with a learning rate of 0.01 and it decays twice to 0.0001. The batch size is set as 8 and we train our model on NVidia GTX 1080 Ti for around 4 days. The scaling factor α in equation 1 is set to 1.0 for training. During adaptation, we use a learning rate of 4 · 10−5 and α is set to 0.1. The speaker embedding dimension is 128.
3.4. Experimental Results
3.4.1. Experiments on Switchboard

Dataset
SWBD DEV SWBD TEST

System
x-vector RPNSD x-vector RPNSD

DER(%) c=0s
15.39 9.18 15.08 9.09

DER(%) c=0.1s
9.51 4.09 9.36 4.14

DER(%) c=0.25s
4.66 2.50 4.42 2.55

Table 2: DERs (%) on SWBD DEV and SWBD TEST with different collars, the overlapped speech is also scored.

In this experiment, we train RPNSD on Mixer 6 + SRE + SWBD and use Kaldi’s x-vector model for CALLHOME as the baseline.5
4We refer the PyTorch implementation of Faster R-CNN in [36]. 5We also train a x-vector model on single channel data of Mixer 6 + SRE + SWBD as a fair comparison but the performance is slightly worse than Kaldi’s diarization model (http://kaldi-asr.org/models/m6).

System
x-vector x-vector x-vector (+VB) x-vector (+VB) x-vector x-vector x-vector (+VB) x-vector (+VB) RPNSD

SAD
oracle oracle oracle oracle TDNN SAD TDNN SAD TDNN SAD TDNN SAD
/

Cluster
AHC with threshold AHC with oracle # spk
AHC with threshold AHC with oracle # spk
AHC with threshold AHC with oracle # spk
AHC with threshold AHC with oracle # spk K-means with oracle # spk

DER(%) Score Overlap c=0s c=0.1s c=0.25s

25.07 24.13 23.47 22.12 32.63 32.20 30.44 29.54 25.46

21.75 20.76 19.89 18.47 26.62 26.13 24.69 23.77 20.41

17.57 16.54 16.38 14.91 20.71 20.14 19.51 18.61 17.06

DER(%) Not Score Overlap c=0s c=0.1s c=0.25s

12.88 11.63 10.68 9.11 23.23 22.53 20.06 19.06 21.39

10.60 9.33 8.15 6.53 16.85 16.10 14.17 13.18 15.35

8.02 6.73 6.51 4.90 11.70 10.90 10.09 9.14 11.81

Table 3: DERs (%) on CALLHOME with different scoring options

As shown in Table 2, RPNSD signiﬁcantly reduces the DER from 15.39% to 9.18% on SWBD DEV and 15.08% to 9.09% on SWBD TEST. On SWBD TEST, the DER composition of the x-vector baseline is 8.9% (Miss) + 1.1% (False Alarm) + 5.0% (Speaker Confusion) = 15.08% (with 2.8% Miss and 0.9% False Alarm for SAD). For RPNSD, the DER composition is 4.0% (Miss) + 4.8% (False Alarm) + 0.3% (Speaker Confusion) = 9.09% (with 2.0% Miss and 2.2% False Alarm for SAD).
Since RPNSD can handle the overlapped speech, the miss error decreases from 8.9% to 4.0%. As a cost, the false alarm error increases from 1.1% to 4.8%. Surprisingly, the speaker confusion decreases largely from 5.0% to 0.3%. There might be two reasons for this. (1) Instead of making decisions on short segments, RPNSD makes use of longer context and extracts more discriminative speaker embeddings. (2) The training and testing condition are more matched for RPNSD. Instead of training on single speaker data, we are training on “diarization data” and testing on “diarization data”.
3.4.2. Experiments on CALLHOME
The CALLHOME corpus is one of the best-known benchmarks for speaker diarization. Since the CALLHOME corpus is quite small (with 17 hours of speech) and doesn’t specify dev/test splits, we follow the “pre-train and adapt” procedure and perform a 5-fold cross validation on this dataset. We use the model in section 3.4.1 as the pre-trained model, adapt it on 4/5 of CALLHOME data and evaluate on the rest 1/5. Since our model does not use any segment boundary information, it is unfair to compare it with x-vector systems using the oracle SAD label. Therefore we compare it with x-vector systems using TDNN SAD. As shown in Table 3, our system achieves better results than x-vector systems with and w/o VB re-segmentation. It largely reduces the DER from 32.30% (or 29.54% after VB resegmentation) to 25.46%. The detailed DER breakdown is shown in Table 4. Due to the ability to handle overlapped speech, RPNSD largely reduces the Miss Error from 18.6% to 12.8%. As a cost, the False Alarm Error increases from 5.1% to 7.5%. The Confusion Error of RPNSD is also lower than x-vector and x-vector (+VB).
The DER result of RPNSD (25.46%) is even close to the xvector system using the oracle SAD label (24.13%). If the oracle SAD label is used, the DER of RPNSD system must be lower than 25.46 − 3.2 = 22.26%6, which is better than the x-vector system (24.13%) and quite close to x-vector (+VB) (22.12%).
6This is because we can easily remove the False Alarm SAD error by labeling them as silence. It is more difﬁcult to handle the Miss SAD error in this framework, but we can further reduce the DER for sure.

System
x-vector x-vector (+VB)
RPNSD

DER
32.20 29.54 25.46

DER breakdown MI FA CF
18.6 5.1 8.6 18.6 5.1 5.9 12.8 7.5 5.2

SAD error MI FA
4.2 5.3 4.2 5.3 5.2 3.2

Table 4: The DER composition of different diarization systems on CALLHOME dataset. The DER includes Miss Error (MI), False Alarm Error (FA), and Confusion Error (CF). The SAD error includes Miss (MI) and False Alarm (FA).

3.4.3. Experiments on Simulated Mixtures
According to our experience, standard diarization systems fail to perform well on highly overlapped speech. Therefore we design experiments on simulated mixtures to evaluate the system performance on overlapped scenarios. As shown in Table 5, RPNSD achieves much lower DER than i-vector and x-vector systems. Compared with permutation-free loss based end-to-end systems [25, 26], the performance of RPNSD is better than BLSTM-EEND but worse than SA-EEND. However, unlike these two systems, RPNSD does not have any constraint on the number of speakers.

System
i-vector x-vector BLSTM-EEND SA-EEND RPNSD

Simulated β=2 β=3 β=5

33.74 28.77 12.28 7.91 9.30

30.93 24.46 14.36 8.51 11.57

25.96 19.78 19.69 9.51 14.55

Table 5: DERs (%) on simulated mixtures with 0.25s collar, the overlapped speech is also scored.

4. CONCLUSION
In this paper, we propose a novel speaker diarization system RPNSD. Taken an audio as the input, the model predicts speech segment proposals and speaker embeddings at the same time. With some simple post-processing (clustering and NMS), we can get the diarization prediction, which is much more convenient than the standard process. In addition to that, the RPNSD system solves the overlapping problem in an elegant way. Our experimental results on Switchboard, CALLHOME and synthetic mixtures reveal that the improvements of the RPNSD system are obvious and consistent.

5. REFERENCES
[1] Douglas A Reynolds and P Torres-Carrasquillo, “Approaches and applications of audio diarization,” in ICASSP. IEEE, 2005, vol. 5, pp. v–953.
[2] Sue E Tranter and Douglas A Reynolds, “An overview of automatic speaker diarization systems,” IEEE Transactions on audio, speech, and language processing, vol. 14, no. 5, pp. 1557–1565, 2006.
[3] Chuck Wooters and Marijn Huijbregts, “The ICSI RT07s speaker diarization system,” in Multimodal Technologies for Perception of Humans, pp. 509–519. Springer, 2007.
[4] Xavier Anguera, Simon Bozonnet, Nicholas Evans, Corinne Fredouille, Gerald Friedland, and Oriol Vinyals, “Speaker diarization: A review of recent research,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 356–370, 2012.
[5] Gregory Sell et al., “Diarization is hard: Some experiences and lessons learned for the jhu team in the inaugural dihard challenge.,” in Interspeech, 2018, pp. 2808–2812.
[6] Mireia D´ıez et al., “BUT System for DIHARD Speech Diarization Challenge 2018.,” in Interspeech, 2018, pp. 2798–2802.
[7] Lei Sun et al., “Speaker diarization with enhancing speech for the ﬁrst dihard challenge.,” in Interspeech, 2018, pp. 2793– 2797.
[8] Ignacio Vin˜als et al., “Estimation of the Number of Speakers with Variational Bayesian PLDA in the DIHARD Diarization Challenge.,” in Interspeech, 2018, pp. 2803–2807.
[9] Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and Pierre Dumouchel, “Joint factor analysis versus eigenchannels in speaker recognition,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4, pp. 1435–1447, 2007.
[10] Najim Dehak, Patrick J Kenny, Re´da Dehak, Pierre Dumouchel, and Pierre Ouellet, “Front-end factor analysis for speaker veriﬁcation,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788–798, 2010.
[11] Gregory Sell and Daniel Garcia-Romero, “Speaker diarization with plda i-vector scoring and unsupervised calibration,” in 2014 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 413–417.
[12] Gregory Sell and Daniel Garcia-Romero, “Diarization resegmentation in the factor analysis subspace,” in ICASSP. IEEE, 2015, pp. 4794–4798.
[13] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier Gonzalez-Dominguez, “Deep neural networks for small footprint text-dependent speaker veriﬁcation,” in ICASSP. IEEE, 2014, pp. 4052–4056.
[14] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer, “End-to-end text-dependent speaker veriﬁcation,” in ICASSP. IEEE, 2016, pp. 5115–5119.
[15] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno, “Generalized end-to-end loss for speaker veriﬁcation,” in ICASSP. IEEE, 2018, pp. 4879–4883.
[16] Quan Wang et al., “Speaker diarization with lstm,” in ICASSP. IEEE, 2018, pp. 5239–5243.
[17] Chao Li et al., “Deep speaker: an end-to-end neural speaker embedding system,” arXiv preprint arXiv:1705.02304, 2017.

[18] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman, “Voxceleb: A large-scale speaker identiﬁcation dataset,” Proc. Interspeech 2017, pp. 2616–2620, 2017.
[19] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman, “Voxceleb2: Deep speaker recognition,” Proc. Interspeech 2018, pp. 1086–1090, 2018.
[20] David Snyder et al., “Deep neural network embeddings for text-independent speaker veriﬁcation.,” in Interspeech, 2017, pp. 999–1003.
[21] David Snyder et al., “X-vectors: Robust dnn embeddings for speaker recognition,” in ICASSP. IEEE, 2018, pp. 5329–5333.
[22] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan McCree, “Speaker diarization using deep neural network embeddings,” in ICASSP. IEEE, 2017, pp. 4930– 4934.
[23] Patrick Kenny, “Bayesian analysis of speaker diarization with eigenvoice priors,” CRIM, Montreal, Technical Report, 2008.
[24] Mireia Diez, Luka´s Burget, and Pavel Matejka, “Speaker diarization based on bayesian HMM with eigenvoice priors.,” in Odyssey, 2018, pp. 147–154.
[25] Yusuke Fujita et al., “End-to-end neural speaker diarization with permutation-free objectives,” in Proc. Interspeech, 2019.
[26] Yusuke Fujita et al., “End-to-end neural speaker diarization with self-attention,” in Proc. ASRU, 2019 (to appear).
[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99.
[28] Chieh-Chi Kao, Weiran Wang, Ming Sun, and Chao Wang, “R-crnn: Region-based convolutional recurrent neural network for audio event detection,” Proc. Interspeech 2018, pp. 1358– 1362, 2018.
[29] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick, “Mask r-cnn,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961–2969.
[30] Ross Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1440–1448.
[31] David Snyder, Guoguo Chen, and Daniel Povey, “Musan: A music, speech, and noise corpus,” arXiv preprint arXiv:1510.08484, 2015.
[32] Tom Ko et al., “A study on data augmentation of reverberant speech for robust speech recognition,” in ICASSP. IEEE, 2017, pp. 5220–5224.
[33] Aonan Zhang, Quan Wang, Zhenyao Zhu, John Paisley, and Chong Wang, “Fully supervised speaker diarization,” in ICASSP. IEEE, 2019, pp. 6301–6305.
[34] Daniel Povey et al., “The kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number CONF.
[35] Simon JD Prince and James H Elder, “Probabilistic linear discriminant analysis for inferences about identity,” in 2007 IEEE 11th International Conference on Computer Vision. IEEE, 2007, pp. 1–8.
[36] Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh, “A faster pytorch implementation of faster r-cnn,” https://github.com/jwyang/faster-rcnn.pytorch, 2017.

