MARINA: Faster Non-Convex Distributed Learning with Compression

arXiv:2102.07845v3 [cs.LG] 7 Jan 2022

Eduard Gorbunov 1 2 3 Konstantin Burlachenko 3 Zhize Li 3 Peter Richta´rik 3

Abstract
We develop and analyze MARINA: a new communication efﬁcient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences that is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al. (2019). Unlike virtually all competing distributed ﬁrst-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. The communication complexity bounds we prove for MARINA are evidently better than those of all previous ﬁrst-order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The ﬁrst method is designed for the case when the local loss functions owned by clients are either of a ﬁnite sum or of an expectation form, and the second method allows for a partial participation of clients – a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of oracle/communication complexity. Finally, we provide a convergence analysis of all methods for problems satisfying the Polyak-Łojasiewicz condition.
1. Introduction
Non-convex optimization problems appear in various applications of machine learning, such as training deep neural networks (Goodfellow et al., 2016) and matrix completion and recovery (Ma et al., 2018; Bhojanapalli et al., 2016). Because of their practical importance, these problems gained much attention in recent years, which led to a rapid develop-
1Moscow Institute of Physics and Technology, Moscow, Russia 2Yandex, Moscow, Russia 3King Abdullah University of Science and Technology, Thuwal, Saudi Arabia. Correspondence to: Eduard Gorbunov <eduard.gorbunov@phystech.edu>, Peter Richta´rik <peter.richtarik@kaust.edu.sa>.
Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

ment of new efﬁcient methods for non-convex optimization problems (Danilova et al., 2020), and especially the training of deep learning models (Sun, 2019).
Training deep neural networks is notoriously computationally challenging and time-consuming. In the quest to improve the generalization performance of modern deep learning models, practitioners resort to using increasingly larger datasets in the training process, and to support such workloads, it is imperative to use advanced parallel and distributed hardware, systems, and algorithms. Distributed computing is often necessitated by the desire to train models from data naturally distributed across several edge devices, as is the case in federated learning (Konecˇny´ et al., 2016; McMahan et al., 2017). However, even when this is not the case, distributed methods are often very efﬁcient at reducing the training time (Goyal et al., 2017; You et al., 2020). Due to these and other reasons, distributed optimization has gained immense popularity in recent years.
However, distributed methods almost invariably suffer from the so-called communication bottleneck: the communication cost of information necessary for the workers to jointly solve the problem at hand is often very high, and depending on the particular compute architecture, workload, and algorithm used, it can be orders of magnitude higher than the computation cost. A popular technique for resolving this issue is communication compression (Seide et al., 2014; Konecˇny´ et al., 2016; Suresh et al., 2017), which is based on applying a lossy transformation/compression to the models, gradients, or tensors to be sent over the network to save on communication. Since applying a lossy compression generally decreases the utility of the exchanged messages, such an approach will typically lead to an increase in the number of communications, and the overall usefulness of this technique manifests itself in situations where the communication savings are larger compared to the increased need for the number of communication rounds (Horva´th et al., 2019).
The optimization and machine learning communities have exerted considerable effort in recent years to design distributed methods supporting compressed communication. From many methods proposed, we emphasize VR-DIANA (Horva´th et al., 2019), FedCOMGATE (Haddadpour et al., 2020), and FedSTEPH (Das et al., 2020) because these pa-

MARINA: Faster Non-Convex Distributed Learning with Compression

pers contain the state-of-the-art results in the setup when the local loss functions can be arbitrary heterogeneous.

1.1. Contributions

We propose several new distributed optimization methods supporting compressed communication, speciﬁcally focusing on smooth but nonconvex problems of the form

n

min f (x) = n1 fi(x) ,

(1)

x∈Rd

i=1

where n workers/devices/clients/peers are connected in a centralized way with a parameter-server, and client i has an access to the local loss function fi only. We establish strong complexity rates for them and show that they are better than previous state-of-the-art results.

• MARINA. The main contribution of our paper is a new distributed method supporting communication compression called MARINA (Alg 1). In this algorithm, workers apply an unbiased compression operator to the gradient differences at each iteration with some probability and send them to the server that performs aggregation by averaging. Unlike all known methods operating with unbiased compression operators, this procedure leads to a biased gradient estimator. We prove convergence guarantees for MARINA, which are strictly better than previous state-of-the-art me√thods (see√Table 1). For example, MARINA’s rate O( 1+ωε/2 n ) is O( ω) times better than that of the state-of-the-art method DIANA (Mishchenko et al., 2019), where ω is the variance parameter associated with the deployed compressor. For example, in the case of the Rand1 sparsiﬁcation compressor, we have ω = d √− 1, and hence we get an improvement by the factor O( d). Since the number d of features can be truly very large when training modern models, this is a substantial improvement that can even amount to several orders of magnitude.

• Variance Reduction on Nodes. We generalize MARINA

to VR-MARINA, which can handle the situation when the

local functions fi have either a ﬁnite-sum (each fi is an

average of m functions) or an expectation form, and when it

is more efﬁcient to rely on local stochastic gradients rather

than on local gradients. When compared with MARINA, VR-

MARINA additionally performs local variance reduction on

all nodes, progressively removing the variance coming from

the stochastic approximation, leading to a better oracle com-

plexity than previous state-of-the-art results (see Table 1).

When no compr√ession is used (i.e., ω = 0), the rate of VRMARINA is O( √nmε2 ), while the rate of the state-of-the-art

method VR-DIANA is O( m2/3 ). This is an improvement

√

ε2

by the factor O( nm1/6). When much compression is

applied, and ω is large, our method is faster by the factor O( mm1/22/+3+ω1ω/2 ). In the special case, when there is just a sin-

gle node (n = 1), and no compression is used, VR-MARINA reduces to the PAGE method of Li et al. (2020); this is an optimal ﬁrst-order algorithm for smooth non-convex ﬁnitesum/online optimization problems.
• Partial Participation. We develop a modiﬁcation of MARINA allowing for partial participation of the clients, which is a feature critical in federated learning. The resulting method, PP-MARINA, has superior communication complexity to the existing methods developed for this settings (see Table 1).
• Convergence Under the Polyak-Łojasiewicz Condition. We analyze all proposed methods for problems satisfying the Polyak-Łojasiewicz condition (Polyak, 1963; Łojasiewicz, 1963). Again, the obtained results are strictly better than previous ones (see Table 2). Statements and proofs of all these results are in the Appendix.
• Simple Analysis. The simplicity and ﬂexibility of our analysis offer several extensions. For example, one can easily generalize our analysis to the case of different quantization operators and different batch sizes used by clients. Moreover, one can combine the ideas of VR-MARINA and PP-MARINA and obtain a single distributed algorithm with compressed communications, variance reduction on nodes, and clients’ sampling. We did not do this to keep the exposition simpler.
1.2. Related Work
Non-Convex Optimization. Since ﬁnding a global minimum of a non-convex function is, in general, an NP-hard problem (Murty & Kabadi, 1987), many researchers in nonconvex optimization focus on relaxed goals such as ﬁnding an ε-stationary point. The theory of stochastic ﬁrst-order methods for ﬁnding ε-stationary points is well-developed: it contains lower bounds for expectation minimization without smoothness of stochastic realizations (Arjevani et al., 2019) and for ﬁnite-sum/expectation minimization (Fang et al., 2018; Li et al., 2020) as well as optimal methods matching the lower bounds (see (Danilova et al., 2020; Li et al., 2020) for the overview). Recently, distributed variants of such methods were proposed (Sun et al., 2020; Sharma et al., 2019; Khanduri et al., 2020).
Compressed Communications. Works on distributed methods supporting communication compression can be roughly split into two large groups: the ﬁrst group focuses on methods using unbiased compression operators (which refer to as quantizations in this paper), such as RandK, and the second one studies methods using biased compressors such as TopK. One can ﬁnd a detailed summary of the most popular compression operators in (Safaryan et al., 2020; Beznosikov et al., 2020).
Unbiased Compression. In this line of work, the ﬁrst con-

MARINA: Faster Non-Convex Distributed Learning with Compression

Table 1: Summary of the state-of-the-art results for ﬁnding an ε-stationary point for the problem (1), i.e., such a point xˆ that E ∇f (xˆ) 2 ≤ ε2. Dependences on the numerical constants, “quality” of the starting point, and smoothness constants are omitted in the complexity bounds. Abbreviations: “PP” = partial participation; “Communication complexity” = the number of communications rounds needed to ﬁnd an ε-stationary point; “Oracle complexity” = the number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd an ε-stationary point. Notation: ω = the quantization parameter (see Def. 1.1); n = the number of nodes; m = the size of the local dataset; r = (expected) number of clients sampled at each iteration; b = the batchsize for VR-MARINA at the iterations with compressed communication. To simplify the bounds, we assume that the expected density ζQ of the quantization operator Q (see Def. 1.1) satisﬁes ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and 2-quantization, see (Beznosikov et al., 2020)). We notice that (Haddadpour et al., 2020) and (Das et al., 2020) contain also better rates under different assumptions on clients’ similarity.

Setup

Method

Citation

Communication Complexity

Oracle Complexity

(1) (1)+(5) (1)+(6)

DIANA
FedCOMGATE (1) FedSTEPH, r = n
MARINA (Alg. 1)
DIANA
VR-DIANA VR-MARINA (Alg. 2), b = 1(2)
DIANA (3) FedCOMGATE (3) VR-MARINA (Alg. 2), b = 1 VR-MARINA (Alg. 2), b = Θ n1ε2

(Mishchenko et al., 2019) (Horva´th et al., 2019) (Li & Richta´rik, 2020)
(Haddadpour et al., 2020) (Das et al., 2020)
Thm. 2.1 & Cor. 2.1 (NEW)
(Li & Richta´rik, 2020)
(Horva´th et al., 2019)
Thm. 3.1 & Cor. 3.1 (NEW) (Mishchenko et al., 2019)
(Li & Richta´rik, 2020) (Haddadpour et al., 2020) Thm. 3.2 & Cor. 3.2 (NEW) Thm. 3.2 & Cor. 3.2 (NEW)

√
1+(1+ω) ω/n ε2

1+ω

ε2 1+ω/n

ε4 √ 1+ω/ n

√ε2

1+(1+ω) ε2

ω/√n + 1n+ε4ω

m2/3+ω 1+ω/n

√ε2

√

1+max ω, (1+ω)m / n

√ε2

1+(1+ω) ε2

ω/n + 1n+ε4ω

1+ω

√ε2

√

ω

+

1+ω/ 2

n+

1+ω 3

ε

√ nε

ω + 1+ωε/2 n

√
1+(1+ω) ω/n ε2

1+ω

nε4 1+ω/n

ε4 √ 1+ω/ n

√ε2

1+(1+ω) ε2

ω/√n + 1n+ε4ω

m2/3+ω 1+ω/n

√ε2

√

1+max ω, (1+ω)m / n

√ε2

1+(1+ω) ε2

ω/n + 1n+ε4ω

1+ω

n√ε4

√

ω + 1+ωε/2 n + √n1ε+3ω

nωε2 + 1+nωε/4 n

PP, (1)

FedSTEPH PP-MARINA (Alg. 4)

(Das et al., 2020) Thm. 4.1 & Cor. 4.1 (NEW)

1+rεω4/n + (1r+(√nω−)(1n)−ε4r)
1+(1+ω) n/r ε2

1+rεω4/n + (1r+(√nω−)(1n)−ε4r)
1+(1+ω) n/r ε2

(1) The results for FedCOMGATE are derived under assumption that for all vectors x1, . . . , xn ∈ Rd the quantization operator Q satisﬁes

E

1 n

n i=1

Q(xj

)

2−

Q

1 n

n i=1

xj

2

≤ G for some constant G ≥ 0. In fact, this assumption does not hold for classical quantization

operators like RandK and 2-quantization on Rd. The counterexample: n = 2 and x1 = −x2 = (t, t, . . . , t) with arbitrary large t > 0.

(2) One can even further improve the communication complexity by increasing b . (3) No assumptions on the smoothness of the stochastic realizations fξ(x) are used.

vergence result in the non-convex case was obtained by Alistarh et al. (2017) for QSGD, under assumptions that the local loss functions are the same for all workers, and the stochastic gradient has uniformly bounded second moment. After that, Mishchenko et al. (2019) proposed DIANA (and its momentum version) and proved its convergence rate for non-convex problems without any assumption on the boundedness of the second moment of the stochastic gradient, but under the assumption that the dissimilarity between local loss functions is bounded. This restriction was later eliminated by Horva´th et al. (2019) for the variance reduced version of DIANA called VR-DIANA, and the analysis was extended to a large class of unbiased compressors. Finally, the results for QSGD and DIANA were recently generalized and tightened by Li & Richta´rik (2020) in a unifying framework that included many other methods as well.
Biased Compression. Biased compression operators are less “optimization-friendly” than unbiased ones. Indeed, one can construct a simple convex quadratic problem for which distributed SGD with Top1 compression diverges exponentially fast (Beznosikov et al., 2020). However, this issue can be resolved using error compensation (Seide et al., 2014). The ﬁrst analysis of error-compensated SGD (EC-

SGD) for non-convex problems was obtained by Karimireddy et al. (2019) for homogeneous problems under the assumption that the second moment of the stochastic gradient is uniformly bounded. The last assumption was recently removed from the analysis of EC-SGD by Stich & Karimireddy (2020); Beznosikov et al. (2020), while the ﬁrst results without the homogeneity assumption were obtained by Koloskova et al. (2020a) for Choco-SGD, but still under the assumption that the second moment of the stochastic gradient is uniformly bounded. This issue was resolved by Beznosikov et al. (2020). In general, the current understanding of optimization methods with biased compressors is far from complete: even in the strongly convex case, the ﬁrst linearly converging (Gorbunov et al., 2020) and accelerated (Qian et al., 2020) error-compensated stochastic methods were proposed just recently.
Other Approaches. Besides communication compression, there are also different techniques aiming to reduce the overall communication cost of distributed methods. The most popular ones are based on decentralized communications and multiple local steps between communication rounds, where the second technique is very popular in federated learning (Konecˇny´ et al., 2016; Kairouz et al., 2019).

MARINA: Faster Non-Convex Distributed Learning with Compression

Table 2: Summary of the state-of-the-art results for ﬁnding an ε-solution for the problem (1) satifying Polyak-Łojasiewicz condition (see As. 2.1), i.e., such a point xˆ that E [f (xˆ) − f (x∗)] ≤ ε. Dependences on the numerical constants and log(1/ε) factors are omitted and all smoothness constanst are denoted by L in the complexity bounds. Abbreviations: “PP” = partial participation; “Communication complexity” = the number of communications rounds needed to ﬁnd an ε-stationary point; “Oracle complexity” = the number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd an ε-stationary point. Notation: ω = the quantization parameter (see Def. 1.1); n = the number of nodes; m = the size of the local dataset; r = (expected) number of clients sampled at each iteration; b = the batchsize for VR-MARINA at the iterations with compressed communication. To simplify the bounds, we assume that the expected density ζQ of the quantization operator Q (see Def. 1.1) satisﬁes ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and 2-quantization, see (Beznosikov et al., 2020)). We notice that (Haddadpour et al., 2020) and (Das et al., 2020) contain also better rates under different assumptions on clients’ similarity.

Setup (1)

Method
DIANA FedCOMGATE (1) MARINA (Alg. 1)

DIANA

(1)+(5)

VR-DIANA VR-MARINA (Alg. 2), b = 1(2)

(1)+(6)

DIANA (3) FedCOMGATE (3)

VR-MARINA (Alg. 2), b = 1

VR-MARINA (Alg. 2), b = Θ

1 nµε

Citation (Li & Richta´rik, 2020) (Haddadpour et al., 2020) Thm. 2.2 & Cor. C.2 (NEW)
(Li & Richta´rik, 2020)
(Li & Richta´rik, 2020) Thm. D.2 & Cor. D.2 (NEW)
(Mishchenko et al., 2019) (Li & Richta´rik, 2020)
(Haddadpour et al., 2020)
Thm. D.4 & Cor. D.4 (NEW)
Thm. D.4 & Cor. D.4 (NEW)

Communication Complexity √
L(1+(1+ω) ω/n)
µ L(1+ω)
µ√
ω + L(1+µω/ n) √
L(1+(1+µω) ω/n) + + L(n1+µω√) Lµ + 1ε
L m2/3+ω 1+ω/n

µ

ω + m+

√

√

L(1+max ω, (1+ω)m / n)

+

µ

√

1+(1+ω) ε2

ω/n + 1n+ε4ω

L(1+ω)

µ
ω + n1µε +

+ Lµ

1 + √ω +
n

ω n2 µε

√

ω + L(1+µω/ n)

Oracle Complexity √
L(1+(1+ω) ω/n)
µ L(1+ω)
nµε √
ω + L(1+µω/ n) √
L(1+(1+µω) ω/n) + + L(n1+µω√) Lµ + 1ε
L m2/3+ω 1+ω/n

µ

ω + m+

√

√

L(1+max ω, (1+ω)m / n)

+

µ

√

1+(1+ω) ε2

ω/n + 1n+ε4ω

L(1+ω)

ω + n1µε + nµε

+ Lµ 1 + √ωn +

ω n2 µε

√

nωµε + L(1n+µω2/ε n)

PP, (1)

FedSTEPH (4) PP-MARINA (Alg. 4)

(1) The results for FedCOMGATE

E

1 n

n i=1

Q(xj

)

2−

Q

1 n

are derived

n i=1

xj

2

(Das et al., 2020) Thm. E.2 & Cor. E.2 (NEW)

L 3/2

µ

√

(ω+r1)n + L(1+(1+µω) n/r)

L 3/2

µ

√

(ω+r1)n + L(1+(1+µω) n/r)

under assumption that for all vectors x1, . . . , xn ∈ Rd the quantization operator Q satisﬁes

≤ G for some constant G ≥ 0. In fact, this assumption does not hold for classical quantization operators like

RandK and 2-quantization on Rd. The counterexample: n = 2 and x1 = −x2 = (t, t, . . . , t) (2) One can even further improve the communication complexity by increasing b . (3) No assumptions on the smoothness of the stochastic realizations fξ(x) are used.
(4) The rate is derived under assumption that r = Ω((1 + ω) L/µ log(1/ε)).

with arbitrary large t > 0.

One can ﬁnd the state-of-the-art distributed optimization methods using these techniques and their combinations in (Lian et al., 2017; Karimireddy et al., 2020; Li et al., 2019; Koloskova et al., 2020b). Moreover, there exist results based on the combinations of communication compression with either decentralized communication, e.g., Choco-SGD (Koloskova et al., 2020a), or local updates, e.g., QsparseLocal-SGD (Basu et al., 2019), FedCOMGATE (Haddadpour et al., 2020), FedSTEPH (Das et al., 2020), where in (Basu et al., 2019) the convergence rates were derived under an assumption that the stochastic gradient has uniformly bounded second moment and the results for Choco-SGD, FedCOMGATE, FedSTEPH were described either earlier in the text, or in Table 1.
1.3. Preliminaries
We will rely on two key assumptions thrughout the text.
Assumption 1.1 (Uniform lower bound). There exists f∗ ∈ R such that f (x) ≥ f∗ for all x ∈ Rd. Assumption 1.2 (L-smoothness). We assume that fi is Li-

smooth for all i ∈ [n] = {1, 2, . . . , n} meaning that the following inequality holds ∀x, y ∈ Rd, ∀i ∈ [n]:

∇fi(x) − ∇fi(y) ≤ Li x − y .

(2)

This assumption implies that f is Lf -smooth with L2f ≤

L2 = n1

n i=1

L2i .

Finally, we describe a large class of unbiased compression operators satisfying a certain variance bound, which we will refer to, in this paper, by the name quantization.

Deﬁnition 1.1 (Quantization). We say that a stochastic mapping Q : Rd → Rd is a quantization operator/quantization if there exists ω > 0 such that for any x ∈ Rd , we have

E [Q(x)] = x, E Q(x) − x 2 ≤ ω x 2. (3)

For the given quantization operator Q(x), we deﬁne the the expected density as ζQ = supx∈Rd E [ Q(x) 0] , where
y 0 is the number of non-zero components of y ∈ Rd.

Notice that the expected density is well-deﬁned for any quantization operator since Q(x) 0 ≤ d.

MARINA: Faster Non-Convex Distributed Learning with Compression

2. MARINA
In this section, we describe the main algorithm of this work: MARINA (see Algorithm 1). At each iteration of MARINA, each worker i either sends to the server the dense vector ∇fi(xk+1) with probability p, or it sends the quantized gradient difference Q ∇fi(xk+1) − ∇fi(xk)) with probability 1−p. In the ﬁrst situation, the server just averages the vectors received from workers and gets gk+1 = ∇f (xk+1), whereas in the second case, the server averages the quantized differences from all workers and then adds the result to gk to get gk+1. Moreover, if Q is identity quantization, i.e., Q(x) = x, then MARINA reduces to Gradient Descent (GD).

One can ﬁnd the full statement of the theorem together with its proof in Section C.1 of the Appendix.
The following corollary provides the bounds on the number of iterations/communication rounds and estimates the total communication cost needed to achieve an ε-stationary point in expectation. Moreover, for simplicity, throughout the paper we assume that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.
Corollary 2.1. Let the assumptions of Theorem 2.1 hold
−1
and p = ζQ/d. If γ ≤ L−1 1 + ω(d−ζQ)/(nζQ) , then
MARINA requires

Algorithm 1 MARINA

1: Input: starting point x0, stepsize γ, probability p ∈

(0, 1], number of iterations K

2: Initialize g0 = ∇f (x0)

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p) 5: Broadcast gk to all workers

6: for i = 1, . . . , n in parallel do

7:

xk+1 = xk − γgk

8:

Set gik+1 = ∇fi(xk+1) if ck = 1, and gik+1 =

gk + Q ∇fi(xk+1) − ∇fi(xk)) otherwise

9: end for
10: gk+1 = n1 11: end for

n i=1

gik+1

12: Return: xˆK chosen uniformly at random from {xk }Kk=−01

However, for non-trivial quantizations, we have E[gk+1 | xk+1] = ∇f (xk+1) unlike all other distributed methods
using exclusively unbiased compressors we know of. That is, gk+1 is a biased stochastic estimator of ∇f (xk+1). How-
ever, MARINA is an example of a rare phenomenon in stochastic optimization when the bias of the stochastic gradient helps to achieve better complexity.

2.1. Convergence Results for Generally Non-Convex Problems
We start with the following result.
Theorem 2.1. Let Assumptions 1.1 and 1.2 be satisﬁed. Then, after

K =O

∆0 L ε2

1+

(1−p)ω pn

iterations with ∆0

=

f (x0) − f∗,

L2

=

1 n

n i=1

L2i

and

−1

the stepsize γ ≤ L−1 1 + (1−p)ω/(pn) , MARINA pro-

duces point xˆK for which E[ ∇f (xˆK ) 2] ≤ ε2.

O

∆0 L ε2

1+

ωn ζdQ − 1

iterations/communication rounds in order to achieve E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total communication cost per worker is O(d + ζQK).
Let us clarify the obtained result. First of all, if ω = 0 (no quantization), then ζQ = 0 and the rate coincides with the rate of Gradient Descent (GD). Since GD is optimal among ﬁrst-order methods in terms of reducing the norm of the gradient (Carmon et al., 2019), the dependence on ε in our bound cannot be improved in general. Next, if n is large enough, i.e., n ≥ ω(d/ζQ − 1), then1 the iteration complexity of MARINA (method with compressed communications) and GD (method with dense communications) coincide. This means that in this regime, MARINA is able to reach a provably better communication complexity than GD!

2.2. Convergence Results Under Polyak-Łojasiewicz condition

In this section, we provide a complexity bounds for MARINA under the Polyak-Łojasiewicz (PŁ) condition.
Assumption 2.1 (PŁ condition). Function f satisﬁes Polyak-Łojasiewicz (PŁ) condition with parameter µ, i.e.,

∇f (x) 2 ≥ 2µ (f (x) − f (x∗)) .

(4)

holds for x∗ = arg minx∈Rd f (x) and for all x ∈ Rd.

Under this and previously introduced assumptions, we derive the following result.
Theorem 2.2. Let Assumptions 1.1, 1.2 and 2.1 be satisﬁed. Then, after

K = O max p1 , Lµ 1 + (1−pnp)ω log ∆ε0 1For 2-quantization this requirement is satisﬁed when n ≥ d.

MARINA: Faster Non-Convex Distributed Learning with Compression

iterations L2 =
min L−1

with ∆0

1 n

n i=1

L2i

=

f (x0) − f (x∗),

and the stepsize γ ≤

−1
1 + 2(1−p)ω/(pn) , p(2µ)−1 , MARINA

produces a point xK for which E[f (xK ) − f (x∗)] ≤ ε.

One can ﬁnd the full statement of the theorem together with its proof in Section C.2 of the Appendix.

3. Variance Reduction

Throughout this section, we assume that the local loss on each node has either a ﬁnite-sum form (ﬁnite sum case),

m

fi(x)

=

1 m

fij (x),

(5)

j=1

or an expectation form (online case),

fi(x) = Eξi∼Di [fξi (x)].

(6)

3.1. Finite Sum Case
In this section, we generalize MARINA to problems of the form (1)+(5), obtaining VR-MARINA (see Algorithm 2). At

Algorithm 2 VR-MARINA: ﬁnite sum case

1: Input: starting point x0, stepsize γ, minibatch size b ,

probability p ∈ (0, 1], number of iterations K

2: Initialize g0 = ∇f (x0)

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p) 5: Broadcast gk to all workers

6: for i = 1, . . . , n in parallel do

7:

xk+1 = xk − γgk

8:

Set gik+1 = ∇fi(xk+1) if ck = 1, and gik+1 =

gk + Q b1

j∈I (∇fij (xk+1) − ∇fij (xk)) i,k

otherwise, where Ii,k is the set of the indices in

the minibatch, |Ii,k| = b

9: end for

10:

gk+1 = n1

n i=1

gik+1

11: end for

12: Return: xˆK chosen uniformly at random from {xk }Kk=−01

n = 1, then MARINA reduces to the optimal method PAGE (Li et al., 2020).

In this part, we will rely on the following average smoothness assumption.

Assumption 3.1 (Average L-smoothness). For all k ≥ 0

and i ∈ [n] the minibatch stochastic gradients difference

∆ki

=

1 b

j∈I (∇fij (xk+1) − ∇fij (xk)) computed on

i,k

the i-th worker satisﬁes E ∆ki | xk, xk+1 = ∆ki and

E ∆ki − ∆ki 2 | xk, xk+1 ≤ Lb2i xk+1 − xk 2 (7)

with some Li ≥ 0, where ∆ki = ∇fi(xk+1) − ∇fi(xk).
This assumption is satisﬁed in many standard minibatch regimes. In particular, if Ii,k = {1, . . . , m}, then Li = 0, and if Ii,k consists of b i.i.d. samples from the uniform distributions on {1, . . . , m} and fij are Lij-smooth, then Li ≤ maxj∈[m] Lij .
Under this and the previously introduced assumptions, we derive the following result.
Theorem 3.1. Consider the ﬁnite sum case (1)+(5). Let Assumptions 1.1, 1.2 and 3.1 be satisﬁed. Then, after

K =O

∆0 ε2

L+

1p−np ωL2 + (1+bω)L2

iterations with ∆0

=

f (x0) − f∗, L2

=

1 n

L2

=

1 n

n i=1

L2i

and

the

stepsize

−1

n i=1

L2i ,

γ≤

L + ωL2 + (1+ω)L2/b (1−p)/(pn) , VR-MARINA

produces such a point xˆK that E[ ∇f (xˆK ) 2] ≤ ε2.

One can ﬁnd the full statement of the theorem together with its proof in Section D.1.1 of the Appendix.

Corollary 3.1. Let the assumptions of Theorem 3.1 hold

and p = min {ζQ/d, b /(m+b )}, where b ≤ m. If γ ≤

−1

L + ωL2 + (1+ω)L2/b { max d/ζQ−1,m/b }/n

then

VR-MARINA requires

O

∆0 ε2

L 1+

ω { max d/ζQ−1,m/b }
n

each iteration of VR-MARINA, devices are to compute the full gradients ∇fi(xk+1) and send them to the server with probability p. Typically, p ≤ 1/m and m is large, meaning that workers compute full gradients rarely (once per ≥ m
iterations in expectation). At other iterations, workers com-
pute minibatch stochastic gradients evaluated at the current
and previous points, compress them using an unbiased com-
pression operator, i.e., quantization/quantization operator, and send the resulting vectors gik+1 −gk to the server. Moreover, if Q is the identity quantization, i.e., Q(x) = x, and

+L

(1+ω) { max d/ζQ−1,m/b }
nb

iterations/communication rounds and O (m + b K) stochastic oracle calls per node in expectation in order to achieve E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total communication cost per worker is O(d + ζQK).
First of all, when workers quatize differences of the full gradients, then Ii,k = {1, . . . , m} for all i ∈ [n] and k ≥ 0,

MARINA: Faster Non-Convex Distributed Learning with Compression

implying L = 0. In this case, the complexity bounds for VRMARINA recover the ones for MARINA. Next, when ω = 0 (no quantization) and n = 1, our bounds for iteration and oracle complexities for VR-MARINA recover the bounds for PAGE (Li & Richta´rik, 2020), which is optimal for ﬁnitesum smooth non-convex optimization. This observation implies that the dependence on ε and m in the complexity bounds for VR-MARINA cannot be improved in the class of ﬁrst-order stochastic methods. Next, we notice that up to the differences in smoothness constants, the iteration and oracle complexities for VR-MARINA beneﬁt from the number of workers n. Finally, as Table 1 shows, the rates for VR-MARINA are strictly better than ones for the previous state-of-the-art method VR-DIANA (Horva´th et al., 2019).
We provide the convergence results for VR-MARINA in the ﬁnite-sum case under the Polyak-Łojasiewicz condition, together with complete proofs, in Section D.1.2 of the Appendix.

3.2. Online Case

In this section, we focus on problems of type (1)+(6).

For this type of problems, we consider a slightly

modiﬁed version of VR-MARINA. That is, we replace

line 8 in Algorithm 2 with the following update rule:

gik+1

=

1 b

j∈Ii,k ∇fξikj (xk+1) if ck = 1, and gik+1 =

gk + Q b1

j∈I (∇fξk (xk+1) − ∇fξk (xk)) other-

i,k

ij

ij

wise, where Ii,k, Ii,k are the sets of the indices in the mini-

batches, |Ii,k| = b, |Ii,k| = b , and ξikj is independently

sampled from Di for i ∈ [n], j ∈ [m] (see Algorithm 3 in

the Appendix).

Before we provide our convergence results in this setup, we reformulate Assumption 3.1 for the online case.

Assumption 3.2 (Average L-smoothness). For all k ≥ 0

and i ∈ [n] the minibatch stochastic gradients difference

∆ki

=

1 b

j∈I (∇fξk (xk+1) − ∇fξk (xk)) computed on

i,k

ij

ij

the i-th worker satisﬁes E ∆ki | xk, xk+1 = ∆ki and

E ∆ki − ∆ki 2 | xk, xk+1 ≤ Lb2i xk+1 − xk 2 (8)

with some Li ≥ 0, where ∆ki = ∇fi(xk+1) − ∇fi(xk).
Moreover, we assume that the variance of the stochastic gradients on all nodes is uniformly upper bounded.
Assumption 3.3. We assume that for all i ∈ [n] there exists such constant σi ∈ [0, +∞) that for all x ∈ Rd

Eξi ∼Di

Eξi∼Di [∇fξi (x)] = ∇fi(x), (9)

∇fξi (x) − ∇fi(x) 2 ≤ σi2.

(10)

Under these and previously introduced assumptions, we derive the following result.
Theorem 3.2. Consider the online case (1)+(6). Let Assumptions 1.1, 1.2, 3.2 and 3.3 be satisﬁed. Then, after

K =O

∆0 ε2

L+

1p−np ωL2 + (1+bω)L2

iterations with ∆0 = f (x0) − f∗, L2 =

1 n

n i=1

L2i ,

L2

=

1 n

n i=1

L2i ,

the stepsize

−1

γ ≤ L + ωL2 + (1+ω)L2/b (1−p)/(pn) , and

b = Θ σ2/(nε2) , σ2 = n1

n i=1

σi2,

VR-MARINA

produces a point xˆK for which E[ ∇f (xˆK ) 2] ≤ ε2.

One can ﬁnd the full statement of the theorem, together with its proof, in Section D.2.1 of the Appendix.

Corollary 3.2. Let the assumptions of Theo-

rem 3.2 hold and choose p = min {ζQ/d, b /(b+b )},

where b ≤ b, b = Θ σ2/(nε2) . If γ ≤

−1

L + ωL2 + (1+ω)L2/b { max d/ζQ−1,b/b }/n

,

then VR-MARINA requires

O

∆0 ε2

L 1+

ωn max ζdQ − 1, nbσ2ε2

+L (1n+bω) max ζdQ − 1, nbσ2ε2
iterations/communication rounds and O(ζQK + σ2/(nε2)) stochastic oracle calls per node in expectation to achieve E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total communication cost per worker is O(d + ζQK).
Similarly to the ﬁnite-sum case, when ω = 0 (no quantization) and n = 1, our bounds for iteration and oracle complexities for VR-MARINA recover the bounds for PAGE (Li & Richta´rik, 2020), which is optimal for online smooth non-convex optimization as well. That is, the dependence on ε in the complexity bound for VR-MARINA cannot be improved in the class of ﬁrst-order stochastic methods. As previously, up to the differences in smoothness constants, the iteration and oracle complexities for VR-MARINA beneﬁt from an increase in the number of workers n.
We provide the convergence results for VR-MARINA in the online case under the Polyak-Łojasiewicz condition, together with complete proofs, in Section D.2.2 of the Appendix.

4. Partial Participation
Finally, we propose another modiﬁcation of MARINA. In particular, we prove an option for partial participation of

jjrf(xk)jj2 jjrf(xk)jj2

MARINA: Faster Non-Convex Distributed Learning with Compression

mushrooms

10-1

10-2 10-3
0

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds

10-1 10-2

jjrf(xk)jj2

10-1 10-2 10-3

mushrooms
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

mushrooms

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

jjrf(xk)jj2

10-1 10-2 10-3

w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

jjrf(xk)jj2

10-1 10-2 10-3

w8a

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

10-1 10-2

mushrooms
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

jjrf(xk)jj2

10-3
0

100 200 300 400 500 600
# of Epochs

10-3 0

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n

Figure 1: Comparison of MARINA with DIANA, and of VR-MARINA with VR-DIANA, on binary classiﬁcation problem involving non-convex loss (11) with LibSVM data (Chang & Lin, 2011). Parameter n is chosen as per Tbl. 3 in the Appendix. Stepsizes for the methods are chosen according to the theory and the batchsizes for VR-MARINA and VR-DIANA are ∼ m/100. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

the clients - a feature important in federated learning. The

resulting method is called PP-MARINA (see Algorithm 4

in the Appendix). At each iteration of PP-MARINA, the

server receives the quantized gradient differences from r

clients with probability 1 − p, and aggregates full gradi-

ents from all clients with probability p, i.e., PP-MARINA

coincides with MARINA up to the following difference:

gik+1

=

∇fi(xk+1), gk+1

=

1 n

n i=1

gik+1

if

ck

=

1,

and gik+1 = gk + Q ∇fi(xk+1) − ∇fi(xk)) , gk+1 =

1r ik∈Ik gikk+1 otherwise, where Ik is the set of r i.i.d. sam-

ples from the uniform distribution over {1, . . . , n}. That is,

if the probability p is chosen to be small enough, then with

high probability the server receives only quantized vectors

from a subset of clients at each iteration.

Below, we provide a convergence result for PP-MARINA for smooth non-convex problems.
Theorem 4.1. Let Assumptions 1.1 and 1.2 be satisﬁed. Then, after

K =O

∆0 L ε2

1+

(1−p)(1+ω) pr

iterations

with

∆0

=

f (x0) − f∗,

L2

=

1 n

n i=1

L2i

and

the

−1

stepsize γ ≤ L−1 1 + (1−p)(1+ω)/(pr) , PP-MARINA

produces a point xˆK for which E[ ∇f (xˆK ) 2] ≤ ε2.

One can ﬁnd the full statement of the theorem together with its proof in Section E.1 of the appendix.
Corollary 4.1. Let the assumptions of Theorem 4.1 hold and choose p = ζQr/(dn), where r ≤ n. If γ ≤

L−1 1 + requires

−1
(1+ω)(dn−ζQr)/(b ζQr) , then PP-MARINA

O

∆0 L ε2

1+

1+rω ζdQnr − 1

iterations/communication rounds to achieve E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total com-
munication cost is O (dn + ζQrK).

When r = n, i.e., all clients participate in communication with the server at each iteration, the rate for PP-MARINA recovers the rate for MARINA under the assumption that (1 + ω)(d/ζQ − 1) = O(ω(d/ζQ − 1)), which holds for a wide class of quantization operators, e.g., for identical quantization, RandK, and p-quantization. In general, the derived complexity is strictly better than previous state-ofthe-art one (see Table 1).
We provide the convergence results for PP-MARINA under the Polyak-Łojasiewicz condition, together with complete proofs, in Section E.2 of the Appendix.

5. Numerical Experiments
5.1. Binary Classiﬁcation with Non-Convex Loss
We conduct several numerical experiments2 on binary classiﬁcation problem involving non-convex loss (Zhao et al., 2010) (used for two-layer neural networks) with LibSVM
2Our code is available at https://github.com/ burlachenkok/marina.

MARINA: Faster Non-Convex Distributed Learning with Compression

data (Chang & Lin, 2011) to justify the theoretical claims of the paper. That is, we consider the following optimization problem:

1N xm∈iRnd f (x) = N t=1 (at x, yi) , (11)

where {at} ∈ Rd, yi ∈ {−1, 1} for all t = 1, . . . , N , and the function : Rd → R is deﬁned as

1

2

(b, c) = 1 −

.

1 + exp(−bc)

The distributed environment is simulated in Python 3.8 using MPI4PY and other standard libraries. Additional details about the experimental setup together with extra experiments are deferred to Section A of the Appendix.

In our experiments, we compare MARINA with the full-batch version of DIANA, and then VR-MARINA with VR-DIANA. We exclude FedCOMGATE and FedPATH from this comparison since they have signiﬁcantly worse oracle complexities (see Table 1). The results are presented in Fig. 1. As our theory predicts, the ﬁrst row shows the superiority of MARINA to DIANA both in terms of iteration/communication complexity and the total number of transmitted bits to achieve the given accuracy. Next, to study the oracle complexity as well, we consider non-full-batched methods – VR-MARINA and VR-DIANA – since they have better oracle complexity than the full-batched methods in the ﬁnite-sum case. Again, the results presented in the second row justify that VR-MARINA outperforms VR-DIANA in terms of oracle complexity and the total number of transmitted bits to achieve the given accuracy.

5.2. Image Classiﬁcation

We also compared the performance of VR-MARINA and VRDIANA on the training ResNet-18 (He et al., 2016) at CIFAR100 (Krizhevsky et al., 2009) dataset. Formally, the optimization problem is

1N

min f (x) =

(p(f (ai, x)), yi) , (12)

x∈Rd

N

i=1

where

{

(ai

,

yi

)}

N i=1

encode

images

and

labels

from

CIFAR100 dataset, f (ai, x) is the output of ResNet-18

on image ai with weights x, p is softmax function, and (·, ·)

is cross-entropy loss. The code is wrtitten in Python 3.9

using PyTorch 1.7, and the distributed environment is

simulated.

The results are presented in Fig. 2. Again, VR-MARINA converges signiﬁcantly faster than VR-DIANA both in terms of the oracle complexity and the total number of transmitted bits to achieve the given accuracy. See other details and observations in Section A of the Appendix.

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-2 10-3

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

jjrf(x)jj2

f(x)

5 × 100

10-4

7 × 100 6 × 100

0 500 1000 1500 2000 2500 3000 3500
Communication Rounds
Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-5

0 500 1000 1500 2000 2500 3000 3500
Communication Rounds

10-2 10-3

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

jjrf(x)jj2

f(x)

5 × 100

10-4

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4

#bits/n

1e11

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-5

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4

#bits/n

1e11

10-2 10-3

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

jjrf(x)jj2

f(x)

5 × 100

10-4

0

50 100 150 200 250

Epochs

10-5 0

50

100 150 200 250

Epochs

Figure 2: Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset. Number of workers equals 5. Stepsizes for the methods were tuned and the batchsizes are ∼ m/50. In all cases, we used the RandK sparsiﬁcation operator, the approximate values of K are given in the legends (d is dimension of the problem).

Acknowledgements
The work of Peter Richta´rik, Eduard Gorbunov, Konstantin Burlachenko and Zhize Li was supported by KAUST Baseline Research Fund. The paper was written while E. Gorbunov was a research intern at KAUST. The work of E. Gorbunov in Sections 1, 2, and C was also partially supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) 075-00337-20-03, project No. 0714-2020-0005, and in Sections 3, 4, D, E – by RFBR, project number 19-31-51001. We thank Konstantin Mishchenko (KAUST) for a suggestion related to the experiments, Elena Bazanova (MIPT) for the suggestions about improving the text, and Slavom´ır Hanzely (KAUST), Egor Shulgin (KAUST), and Alexander Tyurin (KAUST) for spotting the typos.

MARINA: Faster Non-Convex Distributed Learning with Compression

References
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. QSGD: Communication-efﬁcient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pp. 1709–1720, 2017.
Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Basu, D., Data, D., Karakus, C., and Diggavi, S. Qsparselocal-SGD: Distributed SGD with quantization, sparsiﬁcation and local computations. In Advances in Neural Information Processing Systems, pp. 14668–14679, 2019.
Beznosikov, A., Horva´th, S., Richta´rik, P., and Safaryan, M. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Bhojanapalli, S., Kyrillidis, A., and Sanghavi, S. Dropping convexity for faster semi-deﬁnite optimization. In Conference on Learning Theory, pp. 530–582. PMLR, 2016.
Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. Lower bounds for ﬁnding stationary points i. Mathematical Programming, pp. 1–50, 2019.
Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
Danilova, M., Dvurechensky, P., Gasnikov, A., Gorbunov, E., Guminov, S., Kamzolov, D., and Shibaev, I. Recent theoretical advances in non-convex optimization. arXiv preprint arXiv:2012.06188, 2020.
Das, R., Hashemi, A., Sanghavi, S., and Dhillon, I. S. Improved convergence rates for non-convex federated learning with compression. arXiv preprint arXiv:2012.04061, 2020.
Fang, C., Li, C., Lin, Z., and Zhang, T. Near-optimal non-convex optimization via stochastic path integrated differential estimator. Advances in Neural Information Processing Systems, 31:689, 2018.
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. Deep learning, volume 1. MIT press Cambridge, 2016.
Gorbunov, E., Kovalev, D., Makarenko, D., and Richta´rik, P. Linearly converging error compensated sgd. Advances in Neural Information Processing Systems, 33, 2020.
Goyal, P., Dolla´r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

Haddadpour, F., Kamani, M. M., Mokhtari, A., and Mahdavi, M. Federated learning with compression: Uniﬁed analysis and sharp guarantees. arXiv preprint arXiv:2007.01154, 2020.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
Horva´th, S., Ho, C.-Y., Lˇ udov´ıt Horva´th, Sahu, A. N., Canini, M., and Richta´rik, P. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019.
Horva´th, S., Kovalev, D., Mishchenko, K., Stich, S., and Richta´rik, P. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. Error feedback ﬁxes signSGD and other gradient compression schemes. In International Conference on Machine Learning, pp. 3252–3261, 2019.
Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.
Khanduri, P., Sharma, P., Kaﬂe, S., Bulusu, S., Rajawat, K., and Varshney, P. K. Distributed stochastic non-convex optimization: Momentum-based variance reduction. arXiv preprint arXiv:2005.00224, 2020.
Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. Decentralized deep learning with arbitrary communication compression. ICLR, pp. arXiv:1907.09356, 2020a. URL https://arxiv.org/abs/1907.09356.
Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S. A uniﬁed theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pp. 5381–5393. PMLR, 2020b.
Konecˇny´, J., McMahan, H. B., Yu, F. X., Richta´rik, P., Suresh, A. T., and Bacon, D. Federated learning: Strategies for improving communication efﬁciency. arXiv preprint arXiv:1610.05492, 2016.
Konecˇny´, J., McMahan, H. B., Yu, F., Richta´rik, P., Suresh, A. T., and Bacon, D. Federated learning: strategies for

MARINA: Faster Non-Convex Distributed Learning with Compression

improving communication efﬁciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.
Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.
Li, X., Yang, W., Wang, S., and Zhang, Z. Communication efﬁcient decentralized training with multiple local updates. arXiv preprint arXiv:1910.09126, 5, 2019.
Li, Z. and Richta´rik, P. A uniﬁed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020.
Li, Z., Bao, H., Zhang, X., and Richta´rik, P. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. arXiv preprint arXiv:2008.10898, 2020.
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pp. 5330–5340, 2017.
Łojasiewicz, S. A topological property of real analytic subsets. Coll. du CNRS, Les e´quations aux de´rive´es partielles, 117:87–89, 1963.
Ma, C., Wang, K., Chi, Y., and Chen, Y. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion. In International Conference on Machine Learning, pp. 3345–3354. PMLR, 2018.
McMahan, H. B., Moore, E., Ramage, D., Hampson, S., and Agu¨era y Arcas, B. Communication-efﬁcient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2017.
Mishchenko, K., Gorbunov, E., Taka´cˇ, M., and Richta´rik, P. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Murty, K. and Kabadi, S. Some np-complete problems in quadratic and nonlinear programming. Mathematical programming, 39(2):117–129, 1987.

Safaryan, M., Shulgin, E., and Richta´rik, P. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. arXiv preprint arXiv:2002.08958, 2020.
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
Sharma, P., Kaﬂe, S., Khanduri, P., Bulusu, S., Rajawat, K., and Varshney, P. K. Parallel restarted spider– communication efﬁcient distributed nonconvex optimization with optimal computation complexity. arXiv preprint arXiv:1912.06036, 2019.
Stich, S. U. and Karimireddy, S. P. The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates. Journal of Machine Learning Research, 21:1–36, 2020.
Sun, H., Lu, S., and Hong, M. Improving the sample and communication complexity for decentralized non-convex optimization: Joint gradient estimation and tracking. In International Conference on Machine Learning, pp. 9217– 9228. PMLR, 2020.
Sun, R. Optimization for deep learning: theory and algorithms. arXiv preprint arXiv:1912.08957, 2019.
Suresh, A. T., Yu, F. X., Kumar, S., and McMahan, H. B. Distributed mean estimation with limited communication. In Proceedings of the 34th International Conference on Machine Learning, 2017.
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=Syx4wnEtvH.
Zhao, L., Mammadov, M., and Yearwood, J. From convex to nonconvex: a loss function analysis for binary classiﬁcation. In 2010 IEEE International Conference on Data Mining Workshops, pp. 1281–1288. IEEE, 2010.

Polyak, B. T. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864–878, 1963.

Qian, X., Richta´rik, P., and Zhang, T. Error compensated distributed sgd can be accelerated. arXiv preprint arXiv:2010.00091, 2020.

Appendix MARINA: Faster Non-Convex Distributed Learning with Compression

A. Extra Experiments
A.1. Binary Classiﬁcation with Non-Convex Loss
A.1.1. SETUP
In Section 5.1, we present the behavior of MARINA, VR-MARINA, DIANA, and VR-DIANA on the binary classiﬁcation problem involving non-convex loss (Zhao et al., 2010). The datasets were taken from LibSVM (Chang & Lin, 2011) and split into ﬁve equal parts among ﬁve clients (we excluded N − 5 · N/5 last datapoints from each dataset), see the summary in Table 3.
Table 3: Summary of the datasets and splitting of the data among clients (Figure 1).

Dataset n N (# of datapoints) d (# of features)

mushrooms 5

8 120

112

w8a

5

49 745

300

phishing 5

11 055

69

a9a

5

32 560

124

The code was written in Python 3.8 using MPI4PY to emulate the distributed environment and then was executed on a machine with 48 cores, each is Intel(R) Xeon(R) Gold 6246 CPU 3.30GHz.
A.1.2. EXTRA EXPERIMENTS
In this section, we provide additional numerical results on the comparison of MARINA, VR-MARINA, DIANA, and VR-DIANA on the problem (11). Since one of the main goals of our experiments is to justify the theoretical ﬁndings of the paper, in the experiments, we used the stepsizes from the corresponding theoretical results for the methods (for DIANA and VR-DIANA the stepsizes were chosen according to (Horva´th et al., 2019; Li & Richta´rik, 2020)). Next, to compute the stochastic gradients, we use batchsizes = max{1, m/100} for VR-MARINA and VR-DIANA.
The results for the full-batched methods are reported in Figure 3, and the comparison of VR-MARINA and VR-DIANA is given in Figure 4. Clearly, in both cases, MARINA and VR-MARINA show faster convergence than the previous state-of-the-art methods, DIANA and VR-DIANA, for distributed non-convex optimization with compression in terms of ∇f (xk) 2 and f (xk) decrease w.r.t. the number of communication rounds, oracle calls per node and the total number of transferred bits from workers to the master.
We also tested MARINA and DIANA on mushrooms dataset with a bigger number of workers (n = 20). The results are reported in Figure 5. Similarly to the previous numerical tests, MARINA shows its superiority to DIANA with n = 20 as well.
A.2. Image Classiﬁcation
A.2.1. SETUP
In Section 5.2, we demonstrate the performance of VR-MARINA and VR-DIANA on training ResNet-18 at CIFAR100 dataset. ResNet-18 has d = 11 689 512 parameters to train and CIFAR100 contains N = 50 000 colored images. The dataset is split into 5 parts among 5 workers in such a way that the ﬁrst four workers get 10 112 samples and the ﬁfth one get 9 552 samples. The code was written in Python 3.9 using PYTORCH 1.7 and then was executed on a machine with NVIDIA GPU Geforce RTX 2080 Ti with 11 GByte onboard global GPU memory.
In all experiments, we use batchsize = 256 on each worker and tune the stepsizes for each method separately. That is, for each method and for each choice of K for RandK operator we run the method with stepsize γ ∈ {10−6, 0.1, 0.2, 0.5, 1.0, 5.0} to ﬁnd the interval containing the best stepsize. Next, the obtained interal is split into ∼ 10 equal parts and the method is run with corresponding stepsizes. Other parameters of the methods are chosen according to the theory. The summary of used

MARINA: Faster Non-Convex Distributed Learning with Compression

f(xk)

jjrf(xk)jj2

mushrooms

10-1

10-2
0

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds mushrooms

10-1

10-2 10-3
0
10-1

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

mushrooms

10-1 10-2

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

10-1

w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0
10-1 10-2 10-3

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0
10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

w8a

10-1

MARINA (K=1) MARINA (K=5)

MARINA (K=10)

10-2

DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

2 × 10-1
10-1
6 × 10-2

phishing
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds phishing

10-3 10-4

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-5
0
2 × 10-1
10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
phishing
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

6 × 10-2

0.0 10-3 10-4

0.2

0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

phishing

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-5

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1
1.4 × 10-1

a9a

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

1.2 × 10-1

10-1 0
10-1 10-2 10-3

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
a9a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-4

0
2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1 1.4 × 10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
a9a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

1.2 × 10-1

10-1 0.0
10-1 10-2 10-3

0.2

0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

a9a

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-4

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

f(xk)

jjrf(xk)jj2

Figure 3: Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-convex loss (11) with LibSVM data (Chang & Lin, 2011). Parameter n is chosen as per Tbl. 3 (n = 5). Stepsizes for the methods are chosen according to the theory. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

parameters is given in Table 4.

Table 4: Summary of the parameters used in the experiments presented in Fig. 2 and Fig. 6. Stepsizes were tuned, batchsize = 256 on each worker, other parameters were picked according to the theory, except the last line, where p for VR-MARINA without compression was picked as for VR-MARINA with RandK, K = 100 000 compression operator.

Method

RandK, K =

γ

p

VR-MARINA

100 000

0.95 0.008554

VR-MARINA

500 000

0.95 0.024691

VR-MARINA

1 000 000

0.95 0.024691

VR-DIANA

100 000

0.15 0.025316

VR-DIANA

500 000

0.35 0.025316

VR-DIANA

1 000 000

0.35 0.025316

VR-MARINA 11 689 512 (K = d) 3.5 0.024691

VR-DIANA 11 689 512 (K = d) 2.5 0.025316

VR-MARINA 11 689 512 (K = d) 3.5 0.008554

MARINA: Faster Non-Convex Distributed Learning with Compression

f(xk)

jjrf(xk)jj2

f(xk)

mushrooms

10-1 10-2 0

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds mushrooms

10-1 10-2

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-3 0
10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-2 0
10-1 10-2

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n
mushrooms
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-3 0

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n
mushrooms

10-1 10-2 0

100

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
200 300 400 500 600 # of Epochs mushrooms

10-1 10-2

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-3 0

100 200 300 400 500 600 # of Epochs

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

w8a

2 × 10-1

10-1
6 × 10-2 4 × 10-2 3 × 10-2
0
10-1
10-2
10-3

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds w8a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds w8a

2 × 10-1

10-1
6 × 10-2 4 × 10-2 3 × 10-2
0
10-1
10-2
10-3

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

2000000

4000000 6000000
#bits/n w8a

8000000

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
2 × 10-1
10-1
6 × 10-2 4 × 10-2 3 × 10-2
0
10-1 10-2 10-3

2000000

4000000 6000000
#bits/n

8000000

w8a

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

100 200 300 400 500 600
# of Epochs w8a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0

100 200 300 400 500 600

# of Epochs

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

10-1

phishing
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds phishing

10-3 10-4 10-5
0

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds

phishing

10-1

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2 10-3 10-4

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n
phishing
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-5
0

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n

phishing

10-1

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2 10-3 10-4 10-5
0

100 200 300 400 500 600
# of Epochs phishing
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
100 200 300 400 500 600
# of Epochs

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1
1.4 × 10-1
1.2 × 10-1

a9a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds a9a

10-2

10-4 10-6 10-8
0
2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1 1.4 × 10-1
1.2 × 10-1

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds a9a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2 10-4 10-6

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n

a9a

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-8
0
2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1 1.4 × 10-1
1.2 × 10-1

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n a9a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0 100 200 300 400 500 600 # of Epochs a9a

10-2

10-4 10-6 10-8
0

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
100 200 300 400 # of Epochs

500

600

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

Figure 4: Comparison of VR-MARINA with VR-DIANA on binary classiﬁcation problem involving non-convex loss (11) with LibSVM data (Chang & Lin, 2011). Parameter n is chosen as per Tbl. 3 (n = 5). Stepsizes for the methods are chosen according to the theory and the batchsizes are ∼ m/100. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

A.2.2. EXTRA EXPERIMENTS
Results presented in Fig. 2 show the superiority of VR-MARINA to VR-DIANA in training ResNet-18 at CIFAR100. To emphasize the effect of compression we also run VR-MARINA and VR-DIANA without compression, see the results in Fig. 6. First of all, one con notice that the methods do beneﬁt from compression: VR-MARINA and VR-DIANA with compression converge much faster than their non-comressed versions in terms of the total number of transmitted bits to achieve given

MARINA: Faster Non-Convex Distributed Learning with Compression

f(xk)

mushrooms,n=20

10-1

10-2
0
10-1

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds mushrooms,n=20
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

jjrf(xk)jj2

mushrooms,n=20

10-1

10-2 10-3
0
10-1 10-2

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds mushrooms,n=20
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

jjrf(xk)jj2

f(xk)

10-2

10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

Figure 5: Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-convex loss (11) with mushrooms dataset and n = 20 workers. Stepsizes for the methods are chosen according to the theory. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

accuracy.
Moreover, as Fig. 2 shows, VR-MARINA with K = 100 000 converges faster than VR-MARINA with larger K in terms of the epochs. That is, the method with more aggresive compression requires less oracle calls to achieve the same accuracy. The reason of such an unusual behavior is the choice of p: when K = 100 000 the theoretical choice of p is much smaller than for K = 500 000 and K = 1 000 000. Therefore, in VR-MARINA with K = 100 000, the workers compute the full gradients more rarely than in the case of larger K. As the result, it turns out, that the total number of oracle calls needed to achieve given accuracy also smaller for K = 100 000 than for larger K. Moreover, we see this phenomenon even without applying compression: VR-MARINA without compression and with p as in the experiment with VR-MARINA with K = 100 000 converges faster than VR-MARINA without compression and with theoretical choice of p, which is the same as in the case when K = 500 000, 1 000 000, see Table 4.

MARINA: Faster Non-Convex Distributed Learning with Compression

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

f(x)

f(x)

f(x)

5 × 100

5 × 100

5 × 100

0 10-2
10-3

500

1000

1500

2000

2500

3000

3500

0.0

Communication Rounds

Training ResNet-18 @ CIFAR100

10-2

10-3

0.2

0.4

0.6

0.8

1.0

1.2

0

#bits/n

1e12

Training ResNet-18 @ CIFAR100

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

10-2 10-3

50

100

150

200

250

Epochs

Training ResNet-18 @ CIFAR100

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

10-4

10-4

10-4

jjrf(x)jj2

jjrf(x)jj2

jjrf(x)jj2

10-5 10-6

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

10-5 10-6

10-5 10-6

0

500

1000

1500

2000

2500

3000

3500

0.0

0.2

0.4

0.6

0.8

1.0

1.2

0

Communication Rounds

#bits/n

1e12

50

100

150

200

250

Epochs

Figure 6: Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset. Number of workers equals 5. Stepsizes for the methods were tuned and the batchsizes are ∼ m/50. We used the RandK sparsiﬁcation operator, the approximate values of K are given in the legends (d is dimension of the problem). We also show the performance of VR-MARINA and VR-DIANA without compression.

MARINA: Faster Non-Convex Distributed Learning with Compression

B. Basic Facts and Auxiliary Results

B.1. Useful Properties of Expectations

Variance decomposition. For a random vector ξ ∈ Rd and any deterministic vector x ∈ Rd, the variance can be

decomposed as

E ξ − Eξ 2 = E ξ − x 2 − Eξ − x 2

(13)

Tower property of mathematical expectation. For random variables ξ, η ∈ Rd, we have

E [ξ] = E [E [ξ | η]]

(14)

under an assumption that all expectations in the expression above are well-deﬁned.

B.2. One Lemma

In this section, we formulate a lemma from (Li et al., 2020), which holds in our settings as well. We omit the proof of this lemmas since it is identical to the one from (Li et al., 2020). Lemma B.1 (Lemma 2 from (Li et al., 2020)). Assume that function f is L-smooth and xk+1 = xk − γgk. Then

f (xk+1) ≤ f (xk) − γ ∇f (xk) 2 −

1L −

xk+1 − xk 2 + γ gk − ∇f (xk) 2.

(15)

2

2γ 2

2

MARINA: Faster Non-Convex Distributed Learning with Compression

C. Missing Proofs for MARINA
C.1. Generally Non-Convex Problems In this section, we provide the full statement of Theorem 2.1 together with the proof of this result. Theorem C.1 (Theorem 2.1). Let Assumptions 1.1 and 1.2 be satisﬁed and

1

γ≤

,

(16)

L 1+

(1−p)ω pn

where L2 = n1

n i=1

L2i .

Then

after

K

iterations

of

MARINA

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 ,

(17)

γK

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

∆0L

(1 − p)ω

K = O ε2 1 + pn

(18)

iterations MARINA produces such a point xˆK that E[ ∇f (xˆK ) 2] ≤ ε2. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

∆0L

(1 − p)ω

d + K(pd + (1 − p)ζQ) = O d + ε2 1 +

pn (pd + (1 − p)ζQ) ,

(19)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 2.1. The scheme of the proof is similar to the proof of Theorem 1 from (Li et al., 2020). From Lemma B.1, we have

E[f (xk+1)] ≤ E[f (xk)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

(20)

2

2γ 2

2

Next, we need to derive an upper bound for E gk+1 − ∇f (xk+1) 2 . By deﬁnition of gk+1, we have

∇f (xk+1) 

gk+1 =

n
gk + 1 Q



n

i=1

∇fi(xk+1) − ∇fi(xk)

with probability p, with probability 1 − p.

Using this, variance decomposition (13) and tower property (14), we derive:

E gk+1 − ∇f (xk+1) 2

(=14) (14=),(13)


n

2

(1 − p)E  gk + 1 Q ∇fi(xk+1) − ∇fi(xk) − ∇f (xk+1) 

n

i=1


n

2

1 (1 − p)E 

Q ∇fi(xk+1) − ∇fi(xk) − ∇f (xk+1) + ∇f (xk) 

n

i=1

+(1 − p)E gk − ∇f (xk) 2 .

MARINA: Faster Non-Convex Distributed Learning with Compression

Since Q ∇f1(xk+1) − ∇f1(xk) , . . . , Q ∇fn(xk+1) − ∇fn(xk) are independent random vectors for ﬁxed xk and xk+1 we have

E gk+1 − ∇f (xk+1) 2


n

2

1 = (1 − p)E 

Q ∇fi(xk+1) − ∇fi(xk) − ∇fi(xk+1) + ∇fi(xk) 

n

i=1

+(1 − p)E gk − ∇f (xk) 2

1−p n

2

=

E Q ∇fi(xk+1) − ∇fi(xk) − ∇fi(xk+1) + ∇fi(xk)

n2

i=1

+(1 − p)E gk − ∇f (xk) 2

(3) (1 − p)ω n

2

2

≤

E ∇fi(xk+1) − ∇fi(xk) + (1 − p)E gk − ∇f (xk) .

n2

i=1

Using L-smoothness (2) of fi together with the tower property (14), we obtain

E gk+1 − ∇f (xk+1) 2

(1 − p)ω n

2

≤ n2

L2i E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk)

i=1

= (1 − p)ωL2 E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk) 2 . (21) n

Next,

we

introduce

a

new

notation:

Φk

=

f (xk)

−

f∗

+

γ 2p

gk − ∇f (xk)

2. Using this and inequalities (20) and (21), we

establish the following inequality:

E [Φk+1]

≤

E f (xk) − f∗ − γ ∇f (xk) 2 −

1L −

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E (1 − p)ωL2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

2p

n

= E [Φk] − γ E ∇f (xk) 2 + γ(1 − p)ωL2 − 1 + L E xk+1 − xk 2

2

2pn

2γ 2

(16)
≤

γ E [Φk] − E

∇f (xk) 2 ,

(22)

2

where in the last inequality, we use γ(1−2ppn)ωL2 − 21γ + L2 ≤ 0 following from (16). Summing up inequalities (22) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 E
K k=0

∇f (xk) 2

2 K−1

2 (E[Φ0] − E[ΦK ]) 2∆0

≤ γK (E[Φk] − E[Φk+1]) = γK = γK ,

k=0

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (14) and the deﬁnition of xˆK , we obtain (17) that implies (18) and (19).

Corollary C.1 (Corollary 2.1).

Let the assumptions of Theorem 2.1 hold and p =

ζQ d

,

where

ζQ

is

the

expected

density

of

the quantization (see Def. 1.1). If

1

γ≤

,

L 1 + ωn ζdQ − 1

then MARINA requires

K = O ∆0L 1 + ω d − 1

ε2

n ζQ

MARINA: Faster Non-Convex Distributed Learning with Compression iterations/communication rounds to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

∆0L O d + ε2 ζQ +

ωζQ (d − ζQ) n

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof of Corollary 2.1. The choice of p = ζdQ implies 1−p = p
pd + (1 − p)ζQ ≤

d − 1,
ζQ ζQ + 1 − ζQ
d

· ζQ ≤ 2ζQ.

Plugging these relations in (16), (18), and (19), we get that if

1

γ≤

,

L 1 + ωn ζdQ − 1

then MARINA requires

K = O ∆0L 1 + ε2
= O ∆0L 1 + ε2

(1 − p)ω pn
ωd −1
n ζQ

iterations/communication rounds in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

∆0L d + K(pd + (1 − p)ζQ) = O d + ε2 1 +

(1 − p)ω pn (pd + (1 − p)ζQ)

∆0L = O d + ε2 ζQ +

ωζQ (d − ζQ) n

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

C.2. Convergence Results Under Polyak-Łojasiewicz condition

In this section, we provide the full statement of Theorem 2.2 together with the proof of this result. Theorem C.2 (Theorem 2.2). Let Assumptions 1.1, 1.2 and 2.1 be satisﬁed and









 

1

p

 

γ ≤ min

,

,

(23)

 L 1 + 2(1−p)ω

2µ  



pn



where L2 = n1

n i=1

L2i .

Then

after

K

iterations

of

MARINA

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0,

(24)

MARINA: Faster Non-Convex Distributed Learning with Compression where ∆0 = f (x0) − f (x∗). That is, after

K = O max 1 , L 1 + (1 − p)ω log ∆0 (25)

pµ

pn

ε

iterations MARINA produces such a point xK that E[f (xK ) − f (x∗)] ≤ ε. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O d + max 1 , L 1 + (1 − p)ω (pd + (1 − p)ζQ) log ∆0 , (26)

pµ

pn

ε

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 2.2. The proof is very similar to the proof of Theorem 2.1. From Lemma B.1 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk 2

2

2γ 2

γ +E

gk − ∇f (xk) 2

2

(4)
≤ (1 − γµ)E f (xk) − f (x∗) −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

2γ 2

2

Using the same arguments as in the proof of (21), we obtain

E gk+1 − ∇f (xk+1) 2

≤ (1 − p)ωL2 E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk) 2 . n

Putting

all

together,

we

derive

that

the

sequence

Φk

=

f (xk)

−

f (x∗)

+

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤

E (1 − γµ)(f (xk) − f (x∗)) −

1L −

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E (1 − p)ωL2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

p

n

=

E (1 − γµ)(f (xk) − f (x∗)) +

γγ + (1 − p)

2p

gk − ∇f (xk) 2

+ γ(1 − p)ωL2 − 1 + L E xk+1 − xk 2

pn

2γ 2

(23)
≤ (1 − γµ)E[Φk],

where in the last inequality, we use γ(1−ppn)ωL2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤ (1 − γµ) γp following from (23). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

E f (xK ) − f (x∗) ≤ E[ΦK ] ≤ (1 − γµ)K Φ0 = (1 − γµ)K (f (x0) − f (x∗))

that implies (25) and (26).

Corollary C.2.

Let the assumptions of Theorem 2.2 hold and p =

ζQ d

,

where

ζQ

is

the

expected

density

of

the

quantization

(see Def. 1.1). If





  
γ ≤ min
 L 1 +

1 2nω ζdQ − 1



p

 

,

,

2µ 





MARINA: Faster Non-Convex Distributed Learning with Compression

then MARINA requires

dL

ωd

K = O max , 1 +

−1

ζQ µ

n ζQ

log ∆0 ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker

is

L O d + max d, µ ζQ +

ωζQ (d − ζQ) n

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = ζdQ implies

1−p

d

=

− 1,

p

ζQ

pd + (1 − p)ζQ ≤ ζQ + 1 − ζQ · ζQ ≤ 2ζQ. d

Plugging these relations in (23), (25), and (26), we get that if


   γ ≤ min
 L 1 +

1 2nω ζdQ − 1





p

 

,

,

2µ 





then MARINA requires

1L K = O max , 1 +
pµ

(1 − p)ω pn

log ∆0 ε

dL

ωd

= O max , 1 +

−1

ζQ µ

n ζQ

log ∆0 ε

iterations/communication rounds in order to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is

1L d + K(pd + (1 − p)ζQ) = O d + max p , µ 1 +

(1 − p)ω pn

(pd + (1 − p)ζQ) log ∆0 ε

L = O d + max d, µ ζQ +

ωζQ (d − ζQ) n

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression

D. Missing Proofs for VR-MARINA
D.1. Finite Sum Case D.1.1. GENERALLY NON-CONVEX PROBLEMS In this section, we provide the full statement of Theorem 3.1 together with the proof of this result. Theorem D.1 (Theorem 3.1). Consider the ﬁnite sum case (1)+(5). Let Assumptions 1.1, 1.2 and 3.1 be satisﬁed and

1

γ≤

,

(27)

L + 1p−np ωL2 + (1+bω)L2

where L2 = n1

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 ,

(28)

γK

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

K = O ∆0 L + 1 − p ωL2 + (1 + ω)L2

(29)

ε2

pn

b

iterations VR-MARINA produces such a point xˆK that E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total number of stochastic oracle calls per node equals

m + K(pm + 2(1 − p)b ) = O m + ∆0 L + 1 − p ωL2 + (1 + ω)L2 (pm + (1 − p)b ) .

(30)

ε2

pn

b

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O d + ∆ε20 L + 1p−np ωL2 + (1 +bω)L2 (pd + (1 − p)ζQ) , (31)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 3.1. The proof of this theorem is a generalization of the proof of Theorem 2.1. From Lemma B.1, we have

E[f (xk+1)] ≤ E[f (xk)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

(32)

2

2γ 2

2

Next, we need to derive an upper bound for E representation of gk+1:

n
gk+1 − ∇f (xk+1) 2 . Since gk+1 = n1 gik+1, we get the following
i=1

∇f (xk+1) 



gk+1 =

n
gk + 1 Q

 

n

i=1

1 b

(∇fij (xk+1) − ∇fij (xk))

j∈Ii,k

with probability p, with probability 1 − p.

MARINA: Faster Non-Convex Distributed Learning with Compression Using this, variance decomposition (13) and tower property (14), we derive:

E gk+1 − ∇f (xk+1) 2

(=14) (14=),(13)

 



2

1n

1

(1 − p)E  gk +

Q

(∇fij (xk+1) − ∇fij (xk)) − ∇f (xk+1) 



n

b



i=1

j∈Ii,k

 



2

1n

1

(1 − p)E 

Q

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) + ∇f (xk) 

n

b



i=1

j∈Ii,k

+(1 − p)E gk − ∇f (xk) 2 .

Next,

we

use

the

notation:

∆ki

=

1 b

(∇fij(xk+1) − ∇fij(xk)) and ∆ki = ∇fi(xk+1) − ∇fi(xk). These vectors

j∈Ii,k

satisfy E ∆ki | xk, xk+1 = ∆ki for all i ∈ [n]. Moreover, Q(∆k1), . . . , Q(∆kn) are independent random vectors for ﬁxed

xk and xk+1. These observations imply

E gk+1 − ∇f (xk+1) 2

= = (14=),(13)
(14=),(3) (14=),(13)

 (1 − p)E 

1n n i=1

Q(∆ki ) − ∆ki

2  + (1 − p)E

gk − ∇f (xk) 2

1−p n2
1−p n2

n
E
i=1 n
E
i=1

Q(∆ki ) − ∆ki + ∆ki − ∆ki
2
Q(∆ki ) − ∆ki + E

2
+ (1 − p)E
2
∆ki − ∆ki

gk − ∇f (xk) 2

+(1 − p)E gk − ∇f (xk) 2

1−p n n2 ωE
i=1

2
∆ki + E

2
∆ki − ∆ki

+ (1 − p)E

1−p n n2 ωE
i=1

∆ki 2 + (1 + ω)E

2
∆ki − ∆ki

+(1 − p)E gk − ∇f (xk) 2 .

gk − ∇f (xk) 2

Using L-smoothness (2) and average L-smoothness (7) of fi together with the tower property (14), we get

E gk+1 − ∇f (xk+1) 2

1−p n

2 (1 + ω)L2i

k+1

k2

≤ n2

ωLi + b

E x −x

i=1

+(1 − p)E gk − ∇f (xk) 2

= 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E gk − ∇f (xk) 2 .

(33)

Next,

we

introduce

new

notation:

Φk

=

f (xk)

−

f∗

+

γ 2p

gk − ∇f (xk)

2. Using this and inequalities (32) and (33), we

MARINA: Faster Non-Convex Distributed Learning with Compression

establish the following inequality:

E [Φk+1]

≤

E f (xk) − f∗ − γ ∇f (xk) 2 −

1L −

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

2p

n

b

xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

= E [Φk] − γ E ∇f (xk) 2 + γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L E xk+1 − xk 2

2

2pn

b

2γ 2

(27)
≤

γ E [Φk] − E

∇f (xk) 2 ,

(34)

2

where in the last inequality, we use γ(21p−np) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 following from (27). Summing up inequalities (34) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 E
K k=0

∇f (xk) 2

2 K−1

2 (E[Φ0] − E[ΦK ]) 2∆0

≤ γK (E[Φk] − E[Φk+1]) = γK = γK ,

k=0

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (14) and the deﬁnition of xˆK , we obtain (28) that implies (29), (30), and (31).

Remark D.1 (About batchsizes dissimilarity). We notice that our analysis can be easily extended to handle the version of VR-

MARINA with different batchsizes b1, . . . , bn

on different workers,

i.e.,

when |Ii,k|

=

bi

and ∆ki

=

1 b

j∈I (∇fij (xk+1)−

i

i,k

∇fij(xk)). In this case, the statement of Theorem 3.1 remains the same with the small modiﬁciation: instead of Lb2 the complexity bounds will have 1 n L2i .
n i=1 bi

Corollary D.1 (Corollary 3.1). Let the assumptions of Theorem 3.1 hold and p = min ζdQ , mb+b , where b ≤ m and ζQ is the expected density of the quantization (see Def. 1.1). If

γ≤ L+

1 ,

{ max d/ζQ−1,m/b }
n

ωL2 + (1+bω)L2

then VR-MARINA requires

O ∆0 L 1 + ε2

ω max {d/ζQ − 1, m/b }

(1 + ω) max {d/ζQ − 1, m/b }

+L

n

nb

iterations/communication rounds,

O m + ∆0 L b + ε2

ω max {(d/ζQ − 1)(b )2, mb }

(1 + ω) max {(d/ζQ − 1)b , m}

+L

n

n

stochastic oracle calls per node in expectation in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

O d + ∆0ζQ L 1 + ε2

ω max {d/ζQ − 1, m/b }

(1 + ω) max {d/ζQ − 1, m/b }

+L

n

nb

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression

Proof of Corollary 3.1. The choice of p = min ζdQ , mb+b implies

1−p

d

m

= max

− 1, ,

p

ζQ

b

2mb

pm + (1 − p)b ≤

≤ 2b ,

m+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

√

√√

Plugging these relations in (27), (29), (30) and (31) and using a + b ≤ a + b, we get that if

γ≤ L+

1 ,

{ max d/ζQ−1,m/b }
n

ωL2 + (1+bω)L2

then VR-MARINA requires

K = O ∆0 L + ε2

1 − p ωL2 + (1 + ω)L2

pn

b

= O ∆0 L + L2 ω max {d/ζQ − 1, m/b } + L2 (1 + ω) max {d/ζQ − 1, m/b }

ε2

n

nb

= O ∆0 L 1 + ε2

ω max {d/ζQ − 1, m/b }

(1 + ω) max {d/ζQ − 1, m/b }

+L

n

nb

iterations/communication rounds and m + K(pm + 2(1 − p)b ) = O m + ∆0 L + ε2

1 − p ωL2 + (1 + ω)L2

pn

b

(pm + (1 − p)b )

= O m + ∆0 L 1 + ε2

ω max {d/ζQ − 1, m/b } n

= O m + ∆0 L b + ε2

(1 + ω) max {d/ζQ − 1, m/b }

+L

b

nb

ω max {(d/ζQ − 1)(b )2, mb } n

(1 + ω) max {(d/ζQ − 1)b , m} +L
n

stochastic oracle calls per node in expectation in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

∆0 d + K(pd + (1 − p)ζQ) = O d + ε2 L +

1 − p ωL2 + (1 + ω)L2

pn

b

(pd + (1 − p)ζQ)

= O d + ∆0ζQ L 1 + ε2

ω max {d/ζQ − 1, m/b } n

(1 + ω) max {d/ζQ − 1, m/b } +L
nb

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression

D.1.2. CONVERGENCE RESULTS UNDER POLYAK-ŁOJASIEWICZ CONDITION In this section, we provide an analysis of VR-MARINA under the Polyak-Łojasiewicz condition in the ﬁnite sum case. Theorem D.2. Consider the ﬁnite sum case (1)+(5). Let Assumptions 1.1, 1.2, 3.1 and 2.1 be satisﬁed and









 

1

p

 

γ ≤ min

,

,

(35)

  

L

+

2(1p−np) ωL2 + (1+bω)L2

2µ   

where L2 = n1

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA,

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0,

(36)

where ∆0 = f (x0) − f (x∗). That is, after







 1 L + 1p−np ωL2 + (1+bω)L2  ∆0 

 K = O max ,

log 

(37)

 p

µ  ε









iterations VR-MARINA produces such a point xK that E f (xK ) − f (x∗) ≤ ε, and the expected total number of stochastic oracle calls per node equals





 

L+



1

m + K(pm + 2(1 − p)b ) = O m + max ,

 p









1p−np ωL2 + (1+bω)L2 

∆0 

(pm + (1 − p)b ) log  . (38)

µ  ε





Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server we have that the expected total communication cost per worker equals









 1 L + 1p−np ωL2 + (1+bω)L2 

∆0 

 d + K(pd + (1 − p)ζQ) = O d + max ,

(pd + (1 − p)ζQ) log  , (39)

 p µ  ε 









where ζQ is the expected density of the quantization (see Def. 1.1).

Proof. The proof is very similar to the proof of Theorem 3.1. From Lemma B.1 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk 2

2

2γ 2

γ +E

gk − ∇f (xk) 2

2

(4)
≤ (1 − γµ)E f (xk) − f (x∗) −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

2γ 2

2

Using the same arguments as in the proof of (33), we obtain

E gk+1 − ∇f (xk+1) 2

≤ 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk) 2 .

n

b

MARINA: Faster Non-Convex Distributed Learning with Compression

Putting

all

together

we

derive

that

the

sequence

Φk

=

f (xk)

−

f (x∗)

+

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤

E (1 − γµ)(f (xk) − f (x∗)) −

1L −

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

p

n

b

xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

=

E (1 − γµ)(f (xk) − f (x∗)) +

γγ + (1 − p)

2p

gk − ∇f (xk) 2

+ γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L E xk+1 − xk 2

pn

b

2γ 2

(35)
≤ (1 − γµ)E[Φk],

where in the last inequality we use γ(1p−np) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤ (1 − γµ) γp following from (35). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

E f (xk+1) − f (x∗) ≤ E[Φk+1] ≤ (1 − γµ)k+1Φ0 = (1 − γµ)k+1(f (x0) − f (x∗))

that implies (37), (38), and (39).

Corollary D.2. Let the assumptions of Theorem D.2 hold and p = min ζdQ , mb+b , where b ≤ m and ζQ is the expected density of the quantization (see Def. 1.1). If



  
γ ≤ min



 

L

+

1
2 { max d/ζQ−1,m/b }
n

ωL2 + (1+bω)L2





p

 

,

,

2µ 





then VR-MARINA requires

1L O max , 1 +
pµ

ω max {d/ζQ − 1, m/b }

L

+

n

µ

(1 + ω) max {d/ζQ − 1, m/b } log ∆0

nb

ε

iterations/communication rounds,

bL O m + max , b +
pµ

ω max {(d/ζQ − 1)(b )2, mb } L

+

n

µ

(1 + ω) max {(d/ζQ − 1)b , m} log ∆0

n

ε

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is

1L O d + ζQ max p , µ 1 +

ω max {d/ζQ − 1, m/b }

L

+

n

µ

(1 + ω) max {d/ζQ − 1, m/b } log ∆0

nb

ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = min ζdQ , mb+b implies

1−p

d

m

= max

− 1, ,

p

ζQ

b

2mb

pm + (1 − p)b ≤

≤ 2b ,

m+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

MARINA: Faster Non-Convex Distributed Learning with Compression

√

√√

Plugging these relations in (35), (37), (38) and (39) and using a + b ≤ a + b, we get that if



  
γ ≤ min



 

L

+

1
2 { max d/ζQ−1,m/b }
n

ωL2 + (1+bω)L2





p

 

,

,

2µ 





then VR-MARINA requires



 

L+

 1

K = O max ,

 p









1p−np ωL2 + (1+bω)L2  ∆0  log 
µ  ε





 1 L +
= O max , p

L2 ω { max d/ζQ−1,m/b

}

+

L2 (1+ω) { max d/ζQ−1,m/b

}

 

 ∆

n

nb

log

0


µ ε

1L = O max , 1 +
pµ

ω max {d/ζQ − 1, m/b }

L

+

n

µ

(1 + ω) max {d/ζQ − 1, m/b } log ∆0

nb

ε

iterations/communication rounds and





 

L+



1

m + K(pm + 2(1 − p)b ) = O m + max ,

 p









1p−np ωL2 + (1+bω)L2 

∆0 

(pm + (1 − p)b ) log 

µ  ε





1L = O m + max , 1 +
pµ

ω max {d/ζQ − 1, m/b } n

bL = O m + max , b +
pµ

+ L (1 + ω) max {d/ζQ − 1, m/b } b log ∆0

µ

nb

ε

ω max {(d/ζQ − 1)(b )2, mb } n

+ L (1 + ω) max {(d/ζQ − 1)b , m} log ∆0

µ

n

ε

stochastic oracle calls per node in expectation in order to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is





 

L+



1

d + K(pd + (1 − p)ζQ) = O d + max p ,

 









1p−np ωL2 + (1+bω)L2 

∆0 

(pd + (1 − p)ζQ) log 

µ

ε







1L = O d + ζQ max p , µ 1 +

ω max {d/ζQ − 1, m/b } n

+ L (1 + ω) max {d/ζQ − 1, m/b } log ∆0

µ

nb

ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.2. Online Case

MARINA: Faster Non-Convex Distributed Learning with Compression

Algorithm 3 VR-MARINA: online case

1: Input: starting point x0, stepsize γ, minibatch sizes b, b < b, probability p ∈ (0, 1], number of iterations K 2: Initialize g0 = n1b ni=1 j∈Ii,0 ∇fξi0j (x0), where Ii,0 is the set of the indices in the minibatch, |Ii,0| = b, and ξi0j is

independently sampled from Di for i ∈ [n], j ∈ [m]

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p) 5: Broadcast gk to all workers

6: for i = 1, . . . , n in parallel do

7:

xk+1 = xk − γgk



k+1

1
b

j∈Ii,k ∇fξikj (xk+1),

if ck = 1,

8: Set gi = gk + Q b1 j∈Ii,k (∇fξikj (xk+1) − ∇fξikj (xk)) , if ck = 0, where Ii,k, Ii,k are the sets of the

indices in the minibatches, |Ii,k| = b, |Ii,k| = b , and ξikj is independently sampled from Di for i ∈ [n], j ∈ [m]

9: end for

10:

gk+1 = n1

n i=1

gik+1

11: end for

12: Return: xˆK chosen uniformly at random from {xk}Kk=−01

D.2.1. GENERALLY NON-CONVEX PROBLEMS

In this section, we provide the full statement of Theorem 3.2 together with the proof of this result. Theorem D.3 (Theorem 3.2). Consider the online case (1)+(6). Let Assumptions 1.1, 1.2, 3.2 and 3.3 be satisﬁed and

1

γ≤

,

(40)

L + 1p−np ωL2 + (1+bω)L2

where L2 = n1

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA,

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 + σ2 1 + 1 , (41)

γK nb

pK

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

K = O 1 + ∆0 L + 1 − p ωL2 + (1 + ω)L2

(42)

p ε2

pn

b

iterations with b = Θ( nσε22 ) VR-MARINA produces such a point xˆK that E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total number of stochastic oracle calls per node equals

b+K(pb+2(1−p)b ) = O σ2 + 1 + ∆0 L +

nε2

p ε2

1 − p ωL2 + (1 + ω)L2

pn

b

σ2 p nε2 + (1 − p)b

. (43)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server we have that the expected total communication cost per worker equals

1 ∆0 d + K(pd + (1 − p)ζQ) = O d + p + ε2 L +

1 − p ωL2 + (1 + ω)L2

pn

b

(pd + (1 − p)ζQ) , (44)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 3.2. The proof follows the same steps as the proof of Theorem 3.1. From Lemma B.1, we have

E[f (xk+1)] ≤ E[f (xk)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

(45)

2

2γ 2

2

MARINA: Faster Non-Convex Distributed Learning with Compression

Next, we need to derive an upper bound for E representation of gk+1:

n
gk+1 − ∇f (xk+1) 2 . Since gk+1 = n1 gik+1, we get the following
i=1

n

1  nb

∇fξk (xk+1)

 

i=1 j∈Ii,k

ij

gk+1 =

n

gk + n1 Q



i=1

1 b
j∈I

(∇fξk (xk+1) − ∇fξk (xk))

ij

ij

i,k

with probability p, with probability 1 − p.

Using this, variance decomposition (13), tower property (14), and independence of ξikj for i ∈ [n], j ∈ Ii,k, we derive:

E gk+1 − ∇f (xk+1) 2

 



2

(14)

k 1n 1

= (1 − p)E  g +

Q

(∇fij (xk+1) − ∇fij (xk)) − ∇f (xk+1) 



n

b



i=1

j∈Ii,k



2

p

n

+ n2b2 E 

∇fξikj (xk+1) − ∇f (xk+1) 

i=1 j∈Ii,k

(14=),(13)

 



2

1n

1

(1 − p)E 

Q

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) + ∇f (xk) 

n

b



i=1

j∈Ii,k

(14=),(10)

+(1 − p)E

gk − ∇f (xk) 2

pn + n2b2 E
i=1 j∈Ii,k

2
∇fξk (xk+1) − ∇f (xk+1) ij

 



2

1n

1

(1 − p)E 

Q

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) + ∇f (xk) 

n

b



i=1

j∈Ii,k

+(1 − p)E

gk − ∇f (xk) 2 + pσ2 , nb

where σ2 = n1

n i=1

σi2.

Applying

the

same

arguments

as

in

the

proof

of

inequality

(33),

we

obtain

E gk+1 − ∇f (xk+1) 2

≤ 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E gk − ∇f (xk) 2 + pσ2 . (46) nb

Next,

we

introduce

new

notation:

Φk

=

f (xk)

−

f∗

+

γ 2p

gk − ∇f (xk)

2. Using this and inequalities (45) and (46), we

establish the following inequality:

E [Φk+1]

≤

E f (xk) − f∗ − γ ∇f (xk) 2 −

1L −

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

2p

n

b

xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2 + pσ2 nb

= E [Φk] − γ E ∇f (xk) 2 + γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L E xk+1 − xk 2 + γσ2

2

2pn

b

2γ 2

2nb

(≤40) E [Φk] − γ E ∇f (xk) 2 + γσ2 , (47)

2

2nb

where in the last inequality, we use γ(21p−np) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 following from (40). Summing up inequalities (47) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 E
K k=0

∇f (xk) 2

2 K−1

σ2 2 (E[Φ0] − E[ΦK ]) σ2 2∆0 σ2

1

≤ γK (E[Φk] − E[Φk+1]) + nb = γK + nb ≤ γK + nb 1 + pK ,

k=0

MARINA: Faster Non-Convex Distributed Learning with Compression
since g0 = n1b ni=1 j∈Ii,0 ∇fξi0j (x0) and ΦK ≥ 0. Finally, using the tower property (14) and the deﬁnition of xˆK , we obtain (41) that implies (42), (43), and (44).

Remark D.2 (About batchsizes dissimilarity). Similarly to the ﬁnite sum case, our analysis can be easily extended to handle

the version of VR-MARINA with different batchsizes b1, . . . , bn and b1, . . . , bn on different workers, i.e., when |Ii,k| = bi,

|Ii,k |

=

bi

for

i

∈

[n].

In

this

case,

the

statement

of

Theorem

3.2

remains

the

same

with

the

small

modiﬁciation:

instead

of

L2 b

the complexity bounds will have n1

n L2i , and instead of the requirement b = Θ σ2 it will have 1

i=1 bi

nε

n2

ni=1 σbii2 = Θ(ε2).

Corollary D.3 (Corollary 3.2). Let the assumptions of Theorem 3.2 hold and p = min b = Θ σ2/(nε2) and ζQ is the expected density of the quantization (see Def. 1.1). If

ζdQ , b+b b

, where b ≤ b,

γ≤ L+

1 ,

{ max d/ζQ−1,b/b }
n

ωL2 + (1+bω)L2

then VR-MARINA requires

d σ2 O max ,

+ ∆0 L 1 +

ζQ nb ε2

ε2

ω

d

σ2

n max ζQ − 1, nb ε2

(1 + ω)

d

σ2

+ L nb max ζQ − 1, nb ε2

iterations/communication rounds and

db σ2 O max ζQ , nε2

+ ∆0Lb + ∆0L

ε2

ε2

ωb max
n

d

σ2

ζQ − 1 b , nε2

+ ∆0L ε2

1+ω max
n

d

σ2

ζQ − 1 b , nε2

stochastic oracle calls per node in expectation to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

O max d, σ2ζQ + ∆0ζQ L 1 +

nb ε2

ε2

ω

d

σ2

n max ζQ − 1, nb ε2

1+ω

d

σ2

+ L nb max ζQ − 1, nb ε2

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof of Corollary 3.1. The choice of p = min ζdQ , b+b b implies

1−p

d

b

= max

− 1, ,

p

ζQ

b

2bb

pb + (1 − p)b ≤

≤ 2b ,

b+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

√

√√

Plugging these relations in (40), (42), (43) and (44) and using a + b ≤ a + b, we get that if

γ≤ L+

1 ,

{ max d/ζQ−1,b/b }
n

ωL2 + (1+bω)L2

MARINA: Faster Non-Convex Distributed Learning with Compression

then VR-MARINA requires

K = O 1 + ∆0 L + p ε2

1 − p ωL2 + (1 + ω)L2

pn

b

d σ2 = O max ,

+ ∆0 L + L2 ω max {d/ζQ − 1, b/b } + L2 (1 + ω) max {d/ζQ − 1, b/b }

ζQ nb ε2

ε2

n

nb

d σ2 = O max ,

+ ∆0 L 1 +

ζQ nb ε2

ε2

ω

d

σ2

n max ζQ − 1, nb ε2

(1 + ω)

d

σ2

+L nb max ζQ − 1, nb ε2

iterations/communication rounds and

b + K(pb + 2(1 − p)b ) = O b + (1 − p)b + ∆0 L +

p

ε2

1 − p ωL2 + (1 + ω)L2

pn

b

(pb + (1 − p)b )

= O max db , b + ∆0 L 1 +

ζQ

ε2

ω max {d/ζQ − 1, b/b } n

(1 + ω) max {d/ζQ − 1, b/b }

+L

b

nb

= O max db , σ2 + ∆0 L b +

ζQ nε2

ε2

ωb max
n

d

σ2

ζQ − 1 b , nε2

1+ω

+L

max

n

d

σ2

ζQ − 1 b , nε2

stochastic oracle calls per node in expectation to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

(1 − p)ζQ ∆0 d + K(pd + (1 − p)ζQ) = O d + p + ε2 L +

1 − p ωL2 + (1 + ω)L2

pn

b

(pd + (1 − p)ζQ)

= O max d, σ2ζQ + ∆0ζQ L 1 +

nb ε2

ε2

ω

d

σ2

n max ζQ − 1, nb ε2

1+ω

d

σ2

+L nb max ζQ − 1, nb ε2

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.2.2. CONVERGENCE RESULTS UNDER POLYAK-ŁOJASIEWICZ CONDITION

In this section, we provide an analysis of VR-MARINA under Polyak-Łojasiewicz condition in the online case. Theorem D.4. Consider the ﬁnite sum case (1)+(6). Let Assumptions 1.1, 1.2, 3.2, 2.1 and 3.3 be satisﬁed and









 

1

p

 

γ ≤ min

,

,

(48)

  

L

+

2(1p−np) ωL2 + (1+bω)L2

2µ   

MARINA: Faster Non-Convex Distributed Learning with Compression

where L2 = n1

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA,

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0 + σ2 , (49) nbµ

where ∆0 = f (x0) − f (x∗). That is, after







 1 L + 1p−np ωL2 + (1+bω)L2  ∆0 

 K = O max ,

log 

(50)

 p

µ  ε









iterations with b = Θ nσµ2ε VR-MARINA produces such a point xK that E f (xK ) − f (x∗) ≤ ε, and the expected total number of stochastic oracle calls per node equals









 1 L + 1p−np ωL2 + (1+bω)L2 

∆0 

 b + K(pb + 2(1 − p)b ) = O m + max ,

(pb + (1 − p)b ) log  . (51)

 p µ  ε 









Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals









 1 L + 1p−np ωL2 + (1+bω)L2 

∆0 

 d + K(pd + (1 − p)ζQ) = O d + max ,

(pd + (1 − p)ζQ) log  , (52)

 p µ  ε 









where ζQ is the expected density of the quantization (see Def. 1.1).

Proof. The proof is very similar to the proof of Theorem 3.2. From Lemma B.1 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk 2

2

2γ 2

γ +E

gk − ∇f (xk) 2

2

(4)
≤ (1 − γµ)E f (xk) − f (x∗) −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

2γ 2

2

Using the same arguments as in the proof of (46), we obtain

E gk+1 − ∇f (xk+1) 2

≤ 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E gk − ∇f (xk) 2 + pσ2 . (53) nb

Putting

all

together,

we

derive

that

the

sequence

Φk

=

f (xk)

−

f (x∗)

+

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤

E (1 − γµ)(f (xk) − f (x∗)) −

1L −

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

p

n

b

xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2 + pσ2 nb

=

E (1 − γµ)(f (xk) − f (x∗)) +

γγ + (1 − p)

2p

gk − ∇f (xk) 2 + γσ2 nb

+ γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L E xk+1 − xk 2

pn

b

2γ 2

(35)

γσ2

≤ (1 − γµ)E[Φk] + nb ,

MARINA: Faster Non-Convex Distributed Learning with Compression

where in the last inequality we use γ(1p−np) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤ (1 − γµ) γp following from (48). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

γσ2 K−1

E f (xK ) − f (x∗) ≤ E[ΦK ] ≤ (1 − γµ)K Φ0 +

(1 − γµ)k

nb

k=0

≤ (1 − γµ)K (f (x0) − f (x∗)) + γσ2 ∞ (1 − γµ)k nb
k=0

≤ (1 − γµ)K (f (x0) − f (x∗)) + σ2 . nbµ

Together with b = Θ nσµ2ε it implies (50), (51), and (52).

Corollary D.4. Let the assumptions of Theorem D.4 hold and p = min ζdQ , b+b b , where b ≤ b and ζQ is the expected density of the quantization (see Def. 1.1). If





  

γ ≤ min



 

L

+

1
2 { max d/ζQ−1,b/b }
n

ωL2 + (1+bω)L2



p

 

,

2µ 





and then VR-MARINA requires

σ2

1n

b=Θ

, σ2 =

σi2,

nµε

n

i=1

d σ2 L

O max ,

, 1+

ζQ nb µε µ

ω

d

σ2

max

− 1,

n

ζQ

nb µε

L 1+ω

d

σ2

+

max

− 1,

µ nb

ζQ

nb µε

log ∆0 ε

iterations/communication rounds,

b d σ2 L O max , , b +
ζQ nµε µ

ωb max
n

d

σ2

−1 b,

ζQ

nµε

+ Lµ 1+nω max ζdQ − 1 b , nσµ2ε log ∆ε0

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is

d σ2 L O ζQ max ζQ , nb µε , µ 1 +

ω

d

σ2

max

− 1,

n

ζQ

nb µ

L 1+ω

d

σ2

+

max

− 1,

µ nb

ζQ

nb µ

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = min ζdQ , b+b b implies

1−p

d

b

= max

− 1, ,

p

ζQ

b

2bb

pb + (1 − p)b ≤

≤ 2b ,

b+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

MARINA: Faster Non-Convex Distributed Learning with Compression

√

√√

Plugging these relations in (48), (50), (51) and (52) and using a + b ≤ a + b, we get that if



  
γ ≤ min



 

L

+

1
2 { max d/ζQ−1,b/b }
n

ωL2 + (1+bω)L2





p

 

,

,

2µ 





then VR-MARINA requires



 

L+

 1

K = O max ,

 p









1p−np ωL2 + (1+bω)L2  ∆0  log 
µ  ε





  d b L+
= O max , ,  ζQ b

L2 ω { max d/ζQ−1,b/b

}

+

L2 (1+ω) { max d/ζQ−1,b/b

}

 

 ∆

n

nb

log

0


µ ε

d σ2 L

= O max ,

, 1+

ζQ nb µε µ

ω

d

σ2

max

− 1,

n

ζQ

nb µε

L 1+ω

d

σ2

+

max

− 1,

µ nb

ζQ

nb µε

log ∆0 ε

iterations/communication rounds and





 

L+



1

b + K(pb + 2(1 − p)b ) = O b + max ,

 p









1p−np ωL2 + (1+bω)L2 

∆0 

(pb + (1 − p)b ) log 

µ  ε





d bL = O b + max , , 1 +
ζQ b µ

ω max {d/ζQ − 1, b/b } n

+ L (1 + ω) max {d/ζQ − 1, b/b } b log ∆0

µ

nb

ε

b d σ2 L = O max , , b +
ζQ nµε µ

ωb max
n

d

σ2

−1 b,

ζQ

nµε

L 1+ω

+

max

µn

d

σ2

−1 b,

ζQ

nµε

log ∆0 ε

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is





 

L+



1

d + K(pd + (1 − p)ζQ) = O d + max p ,

 









1p−np ωL2 + (1+bω)L2 

∆0 

(pd + (1 − p)ζQ) log 

µ

ε







d σ2 L = O ζQ max ζQ , nb µε , µ 1 +

ω

d

σ2

max

− 1,

n

ζQ

nb µε

L 1+ω

d

σ2

+

max

− 1,

µ nb

ζQ

nb µε

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression
E. Missing Proofs for PP-MARINA

Algorithm 4 PP-MARINA

1: Input: starting point x0, stepsize γ, probability p ∈ (0, 1], number of iterations K, clients-batchsize r ≤ n

2: Initialize g0 = ∇f (x0)

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p)

5: Choose Ik = {1, . . . , n} if ck = 1, and choose Ik as the set of r i.i.d. samples from the uniform distribution over {1, . . . , n} otherwise

6: Broadcast gk to all workers

7: for i = 1, . . . , n in parallel do

8:

xk+1 = xk − γgk

k+1

∇fi(xk+1)

if ck = 1,

9: Set gi = gk + Q ∇fi(xk+1) − ∇fi(xk) if ck = 0.

10: end for

∇f (xk+1) 

11: Set gk+1 = gk + 1



r

Q ∇fik (xk+1) − ∇fik (xk)

ik ∈Ik

12: end for

13: Return: xˆK chosen uniformly at random from {xk}kK=−01

if ck = 1, if ck = 0.

E.1. Generally Non-Convex Problems

In this section, we provide the full statement of Theorem 4.1 together with the proof of this result. Theorem E.1 (Theorem 4.1). Let Assumptions 1.1 and 1.2 be satisﬁed and

1

γ≤

,

(54)

L 1+

(1−p)(1+ω) pr

where L2 = n1

n i=1

L2i .

Then

after

K

iterations

of

PP-MARINA,

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 ,

(55)

γK

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

∆0L

(1 − p)(1 + ω)

K = O ε2 1 + pr

(56)

iterations PP-MARINA produces such a point xˆK that E[ ∇f (xˆK ) 2] ≤ ε2. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost (for all workers) equals

∆0L

(1 − p)(1 + ω)

dn + K(pdn + (1 − p)ζQr) = O dn + ε2 1 +

pr (pdn + (1 − p)ζQr) , (57)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 4.1. The proof is very similar to the proof of Theorem 3.1. From Lemma B.1, we have

E[f (xk+1)] ≤ E[f (xk)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

(58)

2

2γ 2

2

MARINA: Faster Non-Convex Distributed Learning with Compression Next, we need to derive an upper bound for E gk+1 − ∇f (xk+1) 2 . By deﬁnition of gk+1, we have

∇f (xk+1) 

gk+1 = gk + 1

Q



r

ik ∈Ik

∇fik (xk+1) − ∇fik (xk)

with probability p, with probability 1 − p.

Using this, variance decomposition (13) and tower property (14), we derive:

E gk+1 − ∇f (xk+1) 2

(=14) (14=),(13)



2

(1 − p)E  gk + 1

Q ∇fi (xk+1) − ∇fi (xk) − ∇f (xk+1) 



r

k

k



ik ∈Ik



2

1 (1 − p)E 

Q ∇fi (xk+1) − ∇fi (xk) − ∇f (xk+1) + ∇f (xk) 

r

k

k



ik ∈Ik

+(1 − p)E gk − ∇f (xk) 2 .

Next, we use the notation: ∆ki = ∇fi(xk+1) − ∇fi(xk) for i ∈ [n] and ∆k = ∇f (xk+1) − ∇f (xk). These vectors satisfy E ∆kik | xk, xk+1 = ∆k for all ik ∈ Ik. Moreover, Q(∆kik ) for ik ∈ Ik are independent random vectors for ﬁxed xk and xk+1. These observations imply

E gk+1 − ∇f (xk+1) 2

= = (14=),(13)
(14=),(3) (14=),(13)



1 (1 − p)E 
r

Q(∆kik ) − ∆k

ik ∈Ik

2  + (1 − p)E 

gk − ∇f (xk) 2

 1−p
r2 E 
ik ∈Ik
1−p n E
rn i=1
+(1 − p)E

 Q(∆kik ) − ∆kik + ∆kik − ∆k 2 + (1 − p)E
Q(∆ki ) − ∆ki 2 + E ∆ki − ∆k 2 gk − ∇f (xk) 2

gk − ∇f (xk) 2

1−p n ωE
rn i=1

∆ki 2 + E

(1 − p)(1 + ω) n E
rn i=1

∆ki 2

∆ki − ∆k 2 + (1 − p)E

+ (1 − p)E gk − ∇f (xk) 2 gk − ∇f (xk) 2 .

Using L-smoothness (2) of fi together with the tower property (14), we get

E gk+1 − ∇f (xk+1) 2

(1 − p)(1 + ω) n

2

≤

L2i E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk)

nr

i=1

= (1 − p)(1 + ω)L2 E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk) 2 . (59) r

Next,

we

introduce

new

notation:

Φk

=

f (xk)

−

f∗

+

γ 2p

gk − ∇f (xk)

2. Using this and inequalities (58) and (59), we

MARINA: Faster Non-Convex Distributed Learning with Compression

establish the following inequality:

E [Φk+1]

≤

E f (xk) − f∗ − γ ∇f (xk) 2 −

1L −

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E (1 − p)(1 + ω)L2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

2p

r

= E [Φk] − γ E ∇f (xk) 2 + γ(1 − p)(1 + ω)L2 − 1 + L E xk+1 − xk 2

2

2pr

2γ 2

(54)
≤

γ E [Φk] − E

∇f (xk) 2 ,

(60)

2

where in the last inequality we use γ(1−p)2(p1r+ω)L2 − 21γ + L2 ≤ 0 following from (54). Summing up inequalities (34) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 E
K k=0

∇f (xk) 2

2 K−1

2 (E[Φ0] − E[ΦK ]) 2∆0

≤ γK (E[Φk] − E[Φk+1]) = γK = γK ,

k=0

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (14) and the deﬁnition of xˆK , we obtain (55) that implies (56) and (57).

Corollary E.1 (Corollary 4.1). Let the assumptions of Theorem 4.1 hold and p = ζdQnr , where r ≤ n and ζQ is the expected density of the quantization (see Def. 1.1). If

1

γ≤

,

L 1 + 1+rω ζdQnr − 1

then PP-MARINA requires

K = O ∆0L 1 + ε2

1+ω r

dn −1
ζQr

iterations/communication rounds to achieve E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total communication cost is

∆0L O dn + ε2 ζQr +

(1 + ω)ζQ (dn − ζQr)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof of Corollary 4.1. The choice of p = ζdQnr implies

1−p

dn

=

− 1,

p

ζQr

pdn + (1 − p)ζQr ≤ ζQr + 1 − ζQr dn

Plugging these relations in (54), (56), and (57), we get that if

γ≤ L 1+

1 1+rω ζdQnr − 1

· ζQr ≤ 2ζQr. ,

then PP-MARINA requires

K = O ∆0L 1 + ε2

(1 − p)(1 + ω) pr

= O ∆0L 1 + ε2

1+ω r

dn −1
ζQr

MARINA: Faster Non-Convex Distributed Learning with Compression iterations/communication rounds in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost is

∆0L dn + K(pdn + (1 − p)ζQr) = O dn + ε2 1 +

(1 − p)(1 + ω) pr (pdn + (1 − p)ζQr)

∆0L = O dn + ε2 ζQr +

(1 + ω)ζQ (dn − ζQr)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

E.2. Convergence Results Under Polyak-Łojasiewicz condition In this section, we provide an analysis of PP-MARINA under Polyak-Łojasiewicz condition. Theorem E.2. Let Assumptions 1.1, 1.2 and 2.1 be satisﬁed and









 

1

p

 

γ ≤ min

,

,

(61)

 L 1 + 2(1−p)(1+ω)

2µ  



pr



where L2 = n1

n i=1

L2i .

Then

after

K

iterations

of

PP-MARINA,

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0,

(62)

where ∆0 = f (x0) − f (x∗). That is, after

K = O max 1 , L 1 + (1 − p)(1 + ω) log ∆0 (63)

pµ

pr

ε

iterations PP-MARINA produces such a point xK that E[f (xK ) − f (x∗)] ≤ ε. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost (for all workers) equals

1L dn + K(pdn + (1 − p)ζQr) = O dn + max p , µ 1 +

(1 − p)(1 + ω) pr

where ζQ is the expected density of the quantization (see Def. 1.1).

(pdn + (1 − p)ζQr) log ∆0 , (64) ε

Proof. The proof is very similar to the proof of Theorem 4.1. From Lemma B.1 and PŁ condition we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 −

1L −

E

xk+1 − xk 2

2

2γ 2

γ +E

gk − ∇f (xk) 2

2

(4)
≤ (1 − γµ)E f (xk) − f (x∗) −

1L −

E

xk+1 − xk

2

γ +E

gk − ∇f (xk) 2 .

2γ 2

2

Using the same arguments as in the proof of (59), we obtain

E gk+1 − ∇f (xk+1) 2

≤ (1 − p)(1 + ω)L2 E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk) 2 . r

MARINA: Faster Non-Convex Distributed Learning with Compression

Putting

all

together,

we

derive

that

the

sequence

Φk

=

f (xk)

−

f (x∗)

+

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤

E (1 − γµ)(f (xk) − f (x∗)) −

1L −

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E (1 − p)(1 + ω)L2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

p

r

=

E (1 − γµ)(f (xk) − f (x∗)) +

γγ + (1 − p)

2p

gk − ∇f (xk) 2

+ γ(1 − p)(1 + ω)L2 − 1 + L E xk+1 − xk 2

pr

2γ 2

(61)
≤ (1 − γµ)E[Φk],

where in the last inequality we use γ(1−p)p(r1+ω)L2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤ (1 − γµ) γp following from (61). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

E f (xK ) − f (x∗) ≤ E[ΦK ] ≤ (1 − γµ)K Φ0 = (1 − γµ)K (f (x0) − f (x∗))

that implies (63) and (64).

Corollary E.2. Let the assumptions of Theorem E.2 hold and p = ζdQnr , where r ≤ n and ζQ is the expected density of the

quantization (see Def. 1.1). If





  
γ ≤ min
 L 1 +

1
2(1+ω) r

ζdQnr − 1



p

 

,

,

2µ 





then PP-MARINA requires

dn L

K = O max

1+

ζQr µ

1+ω r

dn −1
ζQr

log ∆0 ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost is

L O dn + max dn, µ ζQr + (1 + ω)ζQ (dn − ζQr)

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = ζdQnr implies

1−p

dn

=

− 1,

p

ζQr

pdn + (1 − p)ζQr ≤ ζQr + 1 − ζQr · ζQr ≤ 2ζQr. dn

Plugging these relations in (61), (63), and (64), we get that if


   γ ≤ min
 L 1 +

1
2(1+ω) r

ζdQnr − 1





p

 

,

,

2µ 





MARINA: Faster Non-Convex Distributed Learning with Compression then PP-MARINA requires

1L K = O max , 1 +
pµ

(1 − p)(1 + ω) pr

log ∆0 ε

dn L

= O max

1+

ζQr µ

1+ω r

dn −1
ζQr

log ∆0 ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost is

1L dn + K(pdn + (1 − p)ζQr) = O dn + max p , µ 1 +

(1 − p)(1 + ω) pr

(pdn + (1 − p)ζQr) log ∆0 ε

L = O dn + max dn, µ ζQr + (1 + ω)ζQ (dn − ζQr)

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

