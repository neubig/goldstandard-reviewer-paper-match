Scalable Gradients for Stochastic Diﬀerential Equations

arXiv:2001.01328v6 [cs.LG] 18 Oct 2020

Xuechen Li∗ Google Research

Ting-Kam Leonard Wong University of Toronto

Ricky T. Q. Chen University of Toronto
Vector Institute

David Duvenaud University of Toronto
Vector Institute

Abstract
The adjoint sensitivity method scalably computes gradients of solutions to ordinary diﬀerential equations. We generalize this method to stochastic diﬀerential equations, allowing time-eﬃcient and constant-memory computation of gradients with high-order adaptive solvers. Speciﬁcally, we derive a stochastic differential equation whose solution is the gradient, a memory-eﬃcient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to ﬁt stochastic dynamics deﬁned by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset.
1 Introduction
Deterministic dynamical systems can often be modeled by ordinary diﬀerential equations (ODEs). The adjoint sensitivity method can eﬃciently compute gradients of ODE solutions with constant memory cost. This method was well-known in the physics, numerical analysis, and control communities for decades [3, 4, 60, 65]. Recently, it was combined with modern reverse-mode automatic diﬀerentiation packages, enabling ODEs with millions of parameters to be ﬁt to data [12] and allowing more ﬂexible density estimation and time series models [23, 32, 72].
Stochastic diﬀerential equations (SDEs) generalize ODEs, adding instantaneous noise to their dynamics [55, 77, 78]. They are a natural model for phenomena governed by many small and unobserved interactions, such as motion of molecules in a liquid [8],
∗Member of AI Residency program. Proceedings of the 23rdInternational Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s).

allele frequencies in a gene pool [15], or prices in a market [79]. Previous attempts on ﬁtting SDEs mostly relied on methods with poor scaling properties. The pathwise approach [22, 89], a form of forward-mode automatic diﬀerentiation, scales poorly in time with the number of parameters and states in the model. On the other hand, simply diﬀerentiating through the operations of an SDE solver [19] scales poorly in memory.
In this work, we generalize the adjoint method to stochastic dynamics deﬁned by SDEs. We give a simple and practical algorithm for ﬁtting SDEs with tens of thousands of parameters, while allowing the use of high-order adaptive time-stepping SDE solvers. We call this approach the stochastic adjoint sensitivity method.

Method
Forward pathwise [22, 89] Backprop through solver [19] Stochastic adjoint (ours)

Memory
O(1) O(L) O(1)

Time
O(LD) O(L) O(L log L)

Table 1: Asymptotic complexity comparison. L is the number of steps used in a ﬁxed-step solve, and D is the number of state and parameters. Both memory and time are expressed in units of the cost of evaluating the drift and diﬀusion functions once each.

There are two main diﬃculties in generalizing the adjoint formulation for ODEs to SDEs. The ﬁrst is mathematical: SDEs are deﬁned using nonstandard integrals that usually rely on Itô calculus. The adjoint method requires solving the dynamics backwards in time from the end state. However, it is not clear exactly what “running the SDE backwards” means in the context of stochastic calculus, and when it correctly reconstructs the forward trajectory. We address this problem in Section 3, deriving a backward Stratonovich SDE whose dynamics compute the necessary gradient.
The second diﬃculty is computational: To retrace the steps, one needs to reconstruct the noise sampled on the forward pass, ideally without storing it. In Section 4, we give an algorithm that allows querying a Brownian motion sample at any time point arbitrarily-precisely,

Scalable Gradients for Stochastic Diﬀerential Equations

while only storing a single random seed.
We combine our adjoint approach with a gradientbased stochastic variational inference scheme for efﬁciently marginalizing over latent SDE models with arbitrary diﬀerentiable likelihoods. This model family generalizes several existing families such as latent ODEs [12, 72], Gaussian state-space models [36, 81], and deep Kalman ﬁlters [40], and can naturally handle irregularly-sampled times series and missing observations. We train latent SDEs on toy and real datasets, demonstrating competitive performance compared to existing approaches for dynamics modeling.
2 Background: Stochastic Flows
2.1 Adjoint Sensitivity Method
The adjoint sensitivity method is an eﬃcient approach to solve control problems relying on the adjoint (costate) system [65]. Chen et al. [12] used this method to compute the gradient with respect to parameters of a neural ODE, which is a particular model among many others inspired by the theory of dynamical systems [10, 11, 26, 44, 46, 74, 86]. The method, shown in Algorithm 1, is scalable, since the most costly computation is a vector-Jacobian product deﬁning its backwards dynamics. In addition, since the gradient is obtained by solving another ODE, no intermediate computation is stored as in the case of regular backpropagation [73].

2.2 Stochastic Diﬀerential Equations

Consider a ﬁltered probability space (Ω, F , {Ft}t∈T, P ) on which an m-dimensional adapted Wiener process
(or Brownian motion) {Wt}t∈T is deﬁned. For a ﬁxed terminal time T > 0, we denote by T = [0, T ] the
time horizon. We denote the ith component of Wt by Wt(i). Due to space constraint, we refer the read to Appendix 9.1 for more on notation.

A stochastic process {Zt}t∈T can be deﬁned by an Itô SDE

T

m

ZT = z0 + b(Zt, t) dt+

T
σi(Zt, t) dWt(i), (1)

0

i=1 0

where z0 ∈ Rd is the starting state, and b : Rd×R → Rd and σi : Rd × R → Rd are the drift and diﬀusion functions, respectively. For ease of presentation, we let m = 1 in the following unless otherwise stated. Our contributions can be easily generalized to cases where m > 1. Here, the second integral on the right hand side of (1) is the Itô stochastic integral [55]. When the coeﬃcients are globally Lipschitz in both the state and time, there exists a unique strong solution to the SDE [55].

2.3 Neural Stochastic Diﬀerential Equations
Similar to neural ODEs, one can consider drift and diﬀusion functions deﬁned by neural networks, a model known as the neural SDE [32, 45, 82, 83].
Among works on neural SDEs, none has enabled an eﬃcient training framework. In particular, Tzen and Raginsky [82] and Liu et al. [45] considered computing the gradient by simulating the forward dynamics of an explicit Jacobian matrix. This Jacobian has size of either the square of the number of parameters, or the number of parameters times the number of states, building on the pathwise approach [22, 89]. In contrast, our approach only requires a small number of cheap vectorJacobian products, independent of the dimension of the parameter and state vectors. These vector-Jacobian products have the same asymptotic time cost as evaluating the drift and diﬀusion functions, and can be easily computed by modern automatic diﬀerentiation libraries [1, 16, 49, 59].

2.4 Backward Stratonovich Integral
Our stochastic adjoint sensitivity method involves stochastic processes running both forward and backward in time. The Stratonovich stochastic integral, due to its symmetry, gives nice expressions for the backward dynamics and is more convenient for our purpose. Our results can also be straightforwardly applied to Itô SDEs, relying on a simple conversion rule (see e.g. [64, Sec. 2]).
Following the treatment of Kunita [41], we introduce the forward and backward Stratonovich integrals. Let {Fs,t}s≤t;s,t∈T be a two-sided ﬁltration, where Fs,t is the σ-algebra generated by {Wv − Wu : s ≤ u ≤ v ≤ t} for s, t ∈ T such that s ≤ t. For a continuous semimartingale {Yt}t∈T adapted to the forward ﬁltration {F0,t}t∈T, the Stratonovich stochastic integral is

N

0T Yt ◦ dWt =|Πli|m→0

(Ytk +2Ytk−1 ) Wtk − Wtk−1 ,

k=1

where Π = {0 = t0 < · · · < tN = T } is a partition of the interval T = [0, T ], |Π| = maxk tk−tk−1 denotes the size of largest segment of the partition, and the limit is to be interpreted in the L2 sense. The Itô integral uses instead the left endpoint Ytk rather than the average. In general, the Itô and Stratonovich integrals diﬀer by a term of ﬁnite variation.
To deﬁne the backward Stratonovich integral, we con-
sider the backward Wiener process {W t}t∈T deﬁned
as W t = Wt − WT for all t ∈ T that is adapted to the backward ﬁltration {Ft,T }t∈T. For a continuous
semimartingale Y t adapted to the backward ﬁltration,

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

Algorithm 1 ODE Adjoint Sensitivity
Input: Parameters θ, start time t0, stop time t1, ﬁnal state zt1 , loss gradient ∂L/zt1 , dynamics f (z, t, θ).

Algorithm 2 SDE Adjoint Sensitivity (Ours)
Input: Parameters θ, start time t0, stop time t1, ﬁnal state zt1 , loss gradient ∂L/zt1 , drift f (z, t, θ), diﬀusion σ(z, t, θ), Wiener process sample w(t).

def f ([zt, at, ·], t, θ):

Augmented dynamics

v = f (zt, −t, θ)

return [−v, at∂v/∂z, at∂v/∂θ ]

def f ([zt, at, ·], t, θ):

Augmented drift

v = f (zt, −t, θ)

return [−v, at∂v/∂z, at∂v/∂θ]

def σ([zt, at, ·], t, θ):

Augmented diﬀusion

v = σ(zt, −t, θ)

return [−v, at∂v/∂z, at∂v/∂θ]

 zt0 

 zt1 



∂L/∂zt0  = odeint∂L/∂zt1 , f , −t1, −t0

∂L/∂θ

0p

return ∂L/∂zt0 , ∂L/∂θ

def w(t):

Replicated noise

return [−w(−t), −w(−t), −w(−t)]

 zt0 

 zt1 



∂L/∂zt0  = sdeint∂L/∂zt1 , f , σ, w, −t1, −t0

∂L/∂θ

0p

return ∂L/∂zt0 , ∂L/∂θ

Figure 1: Pseudocode of the (ODE) adjoint sensitivity method (left), and our generalization to Stratonovich SDEs (right). Diﬀerences are highlighted in blue. Square brackets denote vector concatenation.

the backward Stratonovich integral is

N
sT Y t ◦ dW t =|Πli|m→0
k=1

Y tk +Y tk−1 2

W tk−1 − W tk ,

where Π = {0 = tN < · · · < t0 = T } is the partition.

2.5 Stochastic Flow of Diﬀeomorphisms

It is well known that an ODE deﬁnes a ﬂow of diﬀeomorphisms [6]. Here we consider the stochastic analog for the Stratonovich SDE

T

T

ZT = z0 + b(Zt, t) dt + σ(Zt, t) ◦ dWt. (2)

0

0

Throughout the paper, we assume that both b and
σ have inﬁnitely many bounded derivatives w.r.t. the
state, and bounded ﬁrst derivatives w.r.t. time, i.e. b, σ ∈ Cb∞,1, and thus the SDE has a unique strong solution. Let Φs,t(z) := Zts,z be the solution at time t when the process is started at z at time s. Given a
realization of the Wiener process, this deﬁnes a collection of continuous maps S = {Φs,t}s≤t;s,t∈T from Rd to itself.

The following theorem shows that these maps are diffeomorphisms (after choosing a suitable modiﬁcation) and that they satisfy backward SDEs.
Theorem 2.1 ([41, Theorem 3.7.1]). (a) With probability 1, the collection S = {Φs,t}s≤t;s,t∈T satisﬁes the ﬂow property

Φs,t(z) = Φu,t(Φs,u(z)), s ≤ u ≤ t, z ∈ Rd.

Moreover, each Φs,t is a smooth diﬀeomorphism from Rd to itself. We thus call S the stochastic ﬂow of diﬀeomorphisms generated by the SDE (2).
(b) The backward ﬂow Ψs,t := Φ−s,t1 satisﬁes the backward SDE:
t
Ψs,t(z) = z− b(Ψu,t(z), u) du−
s t
σ(Ψu,t(z), u) ◦ dW u, (3)
s
for all z ∈ Rd and s, t ∈ T such that s ≤ t.
The coeﬃcients in (2) and (3) diﬀer by only a negative sign. This symmetry is due to our use of the Stratonovich integral (see Figure 2).

Zt

True solution Itoˆ Reverse Strat Reverse

0

1

t

Figure 2: Negating the drift and diﬀusion functions for an Itô SDE and simulating backwards from the end state gives the wrong reconstruction. Negating the drift and diﬀusion functions for the converted Stratonovich SDE gives the same path when simulated backwards.

Scalable Gradients for Stochastic Diﬀerential Equations

3 Sensitivity via Stochastic Adjoint
We present our main contribution: a stochastic analog of the adjoint sensitivity method for SDEs. We use (3) to derive another backward Stratonovich SDE, which we call the stochastic adjoint process. The direct implication is a gradient computation algorithm that works by solving a set of dynamics in reverse time, and relies on cheap vector-Jacobian products without storing any intermediate quantities.
3.1 Stochastic Adjoint Process
The goal is to derive a stochastic adjoint process {∂L/∂Zt}t∈T that can be simulated by evaluating only vector-Jacobian products, where L = L(ZT ) is a scalar loss of the terminal state from the forward ﬂow ZT = Φ0,T (z0).
We ﬁrst derive a backward SDE for the process {∂ZT /∂Zt}t∈T, assuming that Zt = Ψt,T (ZT ) follows the inverse ﬂow from a deterministic end state ZT ∈ Rd that does not depend on the realized Wiener process (Lemma 3.1). We then extend to the case where ZT = Φ0,T (z0) is obtained by the forward ﬂow starting from a deterministic initial state z0 ∈ Rd (Theorem 3.2). This latter part is unconventional, and the resulting value cannot be interpreted as the solution to a backward SDE anymore due to loss of adaptedness. Instead, we will formulate the result with the Itô map [69]. Finally, it is straightforward to extend the state Zt to include parameters of the drift and diﬀusion functions such that the desired gradient can be obtained for stochastic optimization; we comment on this step in Section 3.3.
We ﬁrst present the SDE for the Jacobian matrix of the backward ﬂow.
Lemma 3.1 (Dynamics of ∂ZT /∂Zt). Consider the stochastic ﬂow generated by the backward SDE (3) as in
Theorem 2.1(b). Letting Js,t(z) := ∇Ψs,t(z), we have
t
Js,t(z) = Id− ∇b(Ψr,t(z), r)Jr,t(z) dr−
s t
∇σ(Ψr,t(z), r)Jr,t(z) ◦ dW r, (4)
s
for all s ≤ t and z ∈ Rd. Furthermore, letting Ks,t(z) = [Js,t(z)]−1, we have
t
Ks,t(z) = Id+ Kr,t(z)∇b(Ψr,t(z), r) dr+
s t
Kr,t(z)∇σ(Ψr,t(z), r) ◦ dW r, (5)
s
for all s ≤ t and z ∈ Rd.

The proof included in Appendix 9.2 relies on Itô’s lemma in the Stratonovich form [41, Theorem 2.4.1]. We stress that this lemma considers only the case where the endpoint z is ﬁxed and deterministic.
Now, we extend to the case where the endpoint is not deterministic, but rather computed from the forward ﬂow. To achieve this, we compose the state process and the loss function. Consider As,t(z) = ∂L(Φs,t(z))/∂z. The chain rule gives As,t(z) = ∇L(Φs,t(z))∇Φs,t(z). Let

As,t(z) :=As,t(Ψs,t(z)) =

(6)

∇L(z)∇Φs,t(Ψs,t(z)) = ∇L(z)Ks,t(z).

Note that As,t(z) = As,t(Φs,t(z)). Since ∇L(z) is
a constant, (As,t(z), Ψs,t(z)) satisﬁes the augmented backward SDE system
t
As,t(z) =∇L(z) + Ar,t(z)∇b(Ψr,t(z), r) dr+
s t
Ar,t(z)∇σ(Ψr,t(z), r) ◦ dW r, t s (7) Ψs,t(z) =z − b(Ψr,t(z), r) dr−
s t
σ(Ψr,t(z), r) ◦ dW r.
s
Since the drift and diﬀusion functions of this augmented system are Cb∞,1, the system has a unique strong solution. Let s = 0 and t = T . Since (7) admits a strong solution, we may write

A0,T (z) = F(z, W·),

(8)

where W· = {Wt}0≤t≤T denotes the path of the Wiener process and

F : Rd × C([0, 1], Rm) → Rd

is a deterministic measurable function (the Itô map) [69, Chapter V, Deﬁnition 10.9]. Intuitively, F can be thought as a black box that computes the solution to the backward SDE system (7) given the position z at time T and the realized Wiener process sample. Similarly, we let G be the solution map for the forward ﬂow (2). The next theorem follows immediately from (6) and the deﬁnition of F.
Theorem 3.2. For P -almost all ω ∈ Ω, we have

A0,T (z) = A0,T (G(z, W·)) = F(G(z, W·), W·), where G(z, W·) = Φ0,T (z).

Proof. This is a consequence of composing A0,T (z) = A0,T (Φ0,T (z)) and (8).

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

This shows that one can obtain the gradient by “composing” the backward SDE system (7) with the original forward SDE (2) and ends our continuous-time analysis.
3.2 Numerical Approximation
In practice, we compute solutions to SDEs with numerical solvers Fh and Gh, where h = T /L denotes the mesh size of a ﬁxed grid. The approximate algorithm thus outputs Fh(Gh(z, W·), W·). The following theorem provides suﬃcient conditions for convergence.
Theorem 3.3. Suppose the schemes Fh and Gh satisfy the following conditions: (i) Fh(z, W·) → F(z, W·) and Gh(z, W·) → G(z, W·) in probability as h → 0, and (ii) for any M > 0, we have sup|z|≤M |Fh(z, W·) − F(z, W·)| → 0 in probability as h → 0. Then, for any starting point z of the forward ﬂow, we have
Fh(Gh(z, W·), W·) → F(G(z, W·), W·) = A0,T (z)
in probability as h → 0.
See Appendix 9.3 for the proof. Usual schemes such as the Euler-Maruyama scheme (more generally Itô-Taylor schemes) converge pathwise (i.e. almost surely) from any ﬁxed starting point [38] and satisﬁes (i). While (ii) is strong, we note that the SDEs considered here have smooth coeﬃcients, and thus their solutions enjoy nice regularity properties in the starting position. Therefore, it is reasonable to expect that the corresponding numerical schemes to also behave nicely as a function of both the mesh size and the starting position. To the best of our knowledge, this property is not considered at all in the literature on numerical methods for SDEs (where the initial position is ﬁxed), but is crucial in the proof of Theorem 3.3. In Appendix 9.4, we prove that condition (ii) holds for the Euler-Maruyama scheme. Detailed analysis for other schemes is beyond the scope of this paper.
3.3 The Algorithm
So far we have derived the gradient of the loss with respect to the initial state. We can extend these results to give gradients with respect to parameters of the drift and diﬀusion functions by treating them as an additional part of the state whose dynamics has zero drift and diﬀusion. We summarize this in Algorithm 2, assuming access only to a black-box solver sdeint. All terms in the augmented dynamics, such as at∂f /∂θ and at∂σ/∂θ can be cheaply evaluated by calling vjp(at, f, θ) and vjp(at, σ, θ), respectively.
Diﬃculties with non-diagonal diﬀusion. In principle, we can simulate the forward and backward adjoint dynamics with any high-order solver of choice. However,

for general matrix-valued diﬀusion functions σ, to obtain a numerical solution with strong order1 beyond 1/2, we need to simulate multiple integrals of the Wiener process such as 0t 0s dWu(i) dWs(j), i, j ∈ [m], i = j. These random variables are diﬃcult to simulate and costly to approximate [87].
Fortunately, if we restrict our SDE to have diagonal noise, then even though the backward SDE for the stochastic adjoint will not in general have diagonal noise, it will satisfy a commutativity property [70]. In that case, we can safely adopt certain numerical schemes of strong order 1.0 (e.g. Milstein [52] and stochastic Runge-Kutta [71]) without approximating multiple integrals or the Lévy area during simulation. We formally show this in Appendix 9.5.
One may also consider numerical schemes with high weak order [39]. However, analysis of this scenario is beyond the current scope.
3.4 Software and Implementation
We have implemented several common SDE solvers in PyTorch [59] with adaptive timestepping using a PI controller [9, 30]. Following torchdiffeq [12], we have created a user-friendly subclass of torch.autograd.Function that facilitates gradient computation using our stochastic adjoint framework for SDEs that are subclasses of torch.nn.Module. We include a short code snippet covering the main idea of the stochastic adjoint in Appendix 9.13. The complete codebase can be found at https://github.com/google-research/torchsde.
4 Virtual Brownian Tree
Our formulation of the adjoint can be numerically integrated eﬃciently, since simulating its dynamics only requires evaluating cheap vector-Jacobian products, as opposed to whole Jacobians. However, the backward-in-time nature introduces a new diﬃculty: The same Wiener process sample path used in the forward pass must be queried again during the backward pass. Naïvely storing Brownian motion increments implies a large memory consumption and complicates the usage of adaptive time-stepping integrators, where the evaluation times in the backward pass may be diﬀerent from those in the forward pass.
To overcome this issue, we combine Brownian trees with splittable pseudorandom number generators (PRNGs) to give an algorithm that can query values of a Wiener
1A numerical scheme is of strong order p if E [|XT − XNη|] ≤ Cηp for all T > 0, where Xt and XNη are respectively the coupled true solution and numerical solution, N and η are respectively the iteration index and step size such that N η = T , and C is independent of η.

Scalable Gradients for Stochastic Diﬀerential Equations

process sample path at arbitrary times. This algorithm, which we call the virtual Brownian tree, has O(1) memory cost, and time cost logarithmic with respect to the inverse error tolerance.

Figure 3: Evaluating a Brownian motion sample at time tq using a virtual Brownian tree. Our algorithm repeatedly bisects the interval, sampling from a Brownian bridge at each halving to determine intermediate values. Each call to the random number generator uses a unique key whose value depends on the path taken to reach it.

4.1 Brownian Bridges and Brownian Trees

Lévy’s Brownian bridge [67] states that given a start time ts and end time te along with their respective Wiener process values ws and we, the marginal of the process at time t ∈ (ts, te) is a normal distribution:

N (te − t)ws + (t − ts)we , (te − t)(t − ts) Id . (9)

te − ts

te − ts

We can recursively apply this formula to evaluate the process at the midpoint of any two distinct timestamps where the values are already known. Constructing the whole sample path of a Wiener process in this manner results in what is known as the Brownian tree [17]. Storing this tree would be memory-intensive, but we show how to reconstruct any node in this tree as desired.

4.2 Brownian Trees using Splittable Seeds
We assume access to a splittable PRNG [14], which has an operation split that deterministically generates two keys from an existing key. Given a key, the function BrownianBridge samples deterministically from (9). To obtain the Wiener process value at a speciﬁc time, we must ﬁrst know or sample the values at the initial and terminal times. Then, the virtual Brownian tree recursively samples from the midpoint of Brownian bridges, each sample using a key split from that of its parent node. The algorithm terminates when the most recently sampled time is close enough to the desired time. We outline the full procedure in Algorithm 3. This algorithm has constant memory cost. For a ﬁxed-

Algorithm 3 Virtual Brownian Tree Input: Seed s, query time t, error tolerance , start
time ts, start state ws, end time te, end state we.
tm = (ts + te)/2 sm, sl, sr = split(s, children=3) wm = BrownianBridge(ts, ws, te, we, tm, sm) while |t − tm| > do
if t < tm then te, xe, s = tm, wm, sl else ts, xs, s = tm, wm, sr end if tm = (ts + te)/2 sm, sl, sr = split(s, children=3) wm = BrownianBridge(ts, ws, te, we, tm, sm) end while return wm

step-size solver taking L steps, the tolerance that the tree will need to be queried at scales as 1/L. Thus the per-step time complexity scales as log L. Our implementation uses an eﬃcient count-based PRNG [76] which avoids passing large random states, and instead simply passes integers. Table 1 compares the asymptotic time complexity of this approach against existing alternatives.

5 Latent Stochastic Diﬀerential Equations

The algorithms presented in Sections 3 and 4 allow us to eﬃciently compute gradients of scalar objectives with respect to SDE parameters, letting us ﬁt SDEs to data. This raises the question: Which loss to optimize?
Simply ﬁtting SDE parameters to maximize likelihood will in general cause overﬁtting, and will result in the diﬀusion function going to zero. In this section, we show how to do eﬃcient variational inference in SDE models, and optimize the marginal log-likelihood to ﬁt both prior (hyper-)parameters and the parameters of a tractable approximate posterior over functions.
In particular, we can parameterize both a prior over functions and an approximate posterior using SDEs:

dZ˜t = hθ(Z˜t, t) dt + σ(Z˜t, t) dWt,

(prior)

dZt = hφ(Zt, t) dt + σ(Zt, t) dWt, (approx. post.)

where hθ, hφ, and σ are Lipschitz in both arguments, and both processes have the same starting value: Z˜0 = Z0 = z0 ∈ Rd.
If both processes share the same diﬀusion function σ, then the KL divergence between them is ﬁnite (under additional mild regularity conditions; see Appendix 9.7), and can be estimated by sampling paths from the approximate posterior process. Then, the

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

w(·)

z0

zt1

SDESolve
zt2 . . . ztn

w(·)

z0

zt1

SDESolve
zt2 . . . ztn

θ

xt1

xt2 . . . xtn

φ

xt1

xt2 . . . xtn

(a) Generation

(b) Recognition

Figure 4: Graphical models for the generative process (decoder) and recognition network (encoder) of the latent stochastic diﬀerential equation model. This model can be viewed as a variational autoencoder with inﬁnite-dimensional noise. Red circles represent entire function draws from Brownian motion. Given the initial state z0 and a Brownian motion sample path w(·), the intermediate states zt1 , . . . , ztn are deterministically approximated by a numerical SDE solver.

evidence lower bound (ELBO) can be written as:

6 Related Work

log p(x1, x2, . . . , xN |θ) ≥

N

EZt

log p(xti |zti ) −

i=1

(10) T 1 |u(zt, t)|2 dt , 02

where u : Rd × [0, T ] → Rm satisﬁes

σ(z, t)u(z, t) = hφ(z, t) − hθ(z, t),

and the expectation is taken over the approximate posterior process deﬁned by (approx. post.). The likelihoods of observations x1, . . . , xN at times t1, . . . , tN depend only on latent states zt at corresponding times.

To compute the gradient with respect to prior param-

eters θ and variational parameters φ, we need only

augment the forward SDE with an extra scalar vari-

able

whose

drift

is

1 2

|u

(Z

t

,

t)

|

2

and

diﬀusion

is

zero.

The backward dynamics can be derived analogously

using (7). We include a detailed derivation in Ap-

pendix 9.7. Thus, a stochastic estimate of the gradients

of the loss w.r.t. all parameters can be computed in a

single pair of forward and backward SDE solves.

The variational parameters φ can either be optimized individually for each sequence, or if multiple time series are sharing parameters, then an encoder network can be trained to input the observations and output φ. This architecture, shown in Figure 4, can be viewed as an inﬁnite-dimensional Variational AutoEncoder (VAE) [35, 68], whose latent is an SDE-induced stochastic process. We may generalize the above to cases where the diﬀusion is parameterized, which is then analogous to learning the prior of the latent code in VAEs.

Sensitivity Analysis for SDEs. Gradient computation is closely related to sensitivity analysis. Computing gradients with respect to parameters of vector ﬁelds of an SDE has been extensively studied in the stochastic control literature [42]. In particular, for low dimensional problems, this is done eﬀectively using dynamic programming [7] and ﬁnite diﬀerences [20, 43]. However, both approaches scale poorly with the dimensionality of the parameter vector.
Analogous to REINFORCE (or the score-function estimator) [21, 37, 88], Yang and Kushner [89] considered deriving the gradient as ∇E [L(ZT )] = E [L(ZT )H] for some random variable H. However, H usually depends on the density of ZT with respect to the Lebesgue measure which can be diﬃcult to compute. Gobet and Munos [22] extended this approach by weakening a nondegeneracy condition using Mallianvin calculus [53].
Closely related to the current approach is the pathwise method [89], which is also a continuous-time analog of the reparameterization trick [35, 68]. Existing methods in this regime [22, 45, 82] all require simulating a (forward) SDE where each step requires computing entire Jacobian matrices. This computational cost is prohibitive for high-dimensional systems with a large number of parameters.
Based on the Euler discretization, Giles and Glasserman [19] considered simply performing reverse-mode automatic diﬀerentiation through all intermediate steps. They named this method the adjoint approach, which, by modern standards, is a form of “backpropagation through the operations of a numerical solver”. This approach, widely adopted in the ﬁeld of ﬁnance for

Scalable Gradients for Stochastic Diﬀerential Equations

Error Error Error

10 3 10 5 10 7 10 9
1e-1 1e-S2tep Siz1ee-3 1e-4
(a) Fixed Step Size vs Error

10 1
100

10 1

10 2

10 2

10 3

10 3

10 4

10 4

10 5

10 6

10 5

10 7

0.0

0.5 NF1E.0Total1.5

2.0

10 6

1e4

(b) Forward NFE vs Error

0.012

Euler backprop (dt=0.001) Milstein backprop (dt=0.01)

0.010

Milstein adjoint (dt=0.01)

0.008

0.006

0.004

0.002

0.0000.3 0.4 0.5Rela0t.6ive W0.a7lltim0e.8 0.9 1.0
(c) Eﬃciency Comparison

Figure 5: (a) Same ﬁxed step size used in both forward and reverse simulation. Boxplot generated by repeating the experiment with diﬀerent Brownian motion sample paths 64 times. (b) Colors of dots represent tolerance levels and correspond to the colorbar on the right. Only atol was varied and rtol was set to 0.

calibrating market models [19], has high memory cost, and relies on a ﬁxed Euler-Maruyama discretization. Recently, this approach was also used by Hegde et al. [27] to learn parameterized drift and diﬀusion functions of an SDE. In scientiﬁc computing, Innes et al. [31] considered backpropagating through high-order implicit SDE solvers.
Ryder et al. [75] perform variational inference over the state and parameters for Euler-discretized latent SDEs and optimize the model with backpropagation. This approach should not be confused with the formulation of variational inference for non-discretized SDEs presented in previous works [25, 57, 82] and our work, as it is unclear whether the limit of their discretization corresponds to that obtained by operating with continuous-time SDEs using Girsanov’s theorem.
Backward SDEs. Our stochastic adjoint process relies on the notion of backward SDEs devised by Kunita [41], which is based on two-sided ﬁltrations. This is diﬀerent from the more traditional notion of backward SDEs where only a single ﬁltration is deﬁned [58, 62]. Based on the latter notion, forward-backward SDEs (FBSDEs) have been proposed to solve stochastic optimal control problems [63]. However, simulating FBSDEs is costly due to the need to estimate conditional expectations in the backward pass [58].
Bayesian Learning of SDEs. Recent works considered the problem of inferring an approximate posterior SDE given observed data under a prior SDE with the same diﬀusion coeﬃcient [25, 57, 82]. The special case with constant diﬀusion coeﬃcients was considered more than a decade ago [5]. Notably, computing the KL divergence between two SDEs over a ﬁnite time horizon was well-explored in the control literature [33, 80]. We include background on this topic in Appendix 9.6.
Bayesian learning and parameter estimation for SDEs

have a long history [24]. Techniques which don’t require positing a variational family such as the extended Kalman ﬁlter and Markov chain Monte Carlo have been considered in the literature [50].
7 Experiments
The aim of this section is threefold. We ﬁrst empirically verify our theory by comparing the gradients obtained by our stochastic adjoint framework against analytically derived gradients for problems having closed-form solutions. We then ﬁt latent SDE models with our framework on two synthetic datasets, verifying that the variational inference framework allows learning a generative model of time series. Finally, we learn dynamics parameterized by neural networks with a latent SDE from a motion capture dataset, demonstrating competitive performance compared to existing approaches.
We report results based on an implementation of Brownian motion that stores all intermediate queries. The virtual Brownian tree allowed training with much larger batch sizes on GPUs, but was not necessary for our small-scale experiments. Notably, our adjoint approach, even when combined with the Brownian motion implementation that stores noise, was able to reduce the memory usage by 1/2-1/3 compared to directly backpropagating through solver operations on the tasks we considered.
7.1 Numerical Studies
We consider three test problems (examples 1-3 from [66]; details in Appendix 9.8), all of which have closedform solutions. We compare the gradient computed from simulating our stochastic adjoint process using the Milstein scheme against the exact gradient. Figure 5(a) shows that for test example 2, the error between the adjoint gradient and analytical gradient decreases with step size.

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

For all three test problems, the mean squared error across dimensions tends to be smaller as the absolute tolerance of the adaptive solver is reduced (e.g. see Fig. 5 (b)). However, the Number of Function Evaluations (NFEs) tends to be much larger than that in the ODE case [12].

Additionally, for two out of three test problems, we found that our adjoint approach with the Milstein scheme and ﬁxed step size can be much more timeeﬃcient than regular backpropagation through operations of the Milstein and Euler schemes (see e.g. Fig. 5(c)). Backpropagating through the Euler scheme gives gradients of higher error compared to the Milstein method. On the other hand, directly backpropagating through the Milstein solve requires evaluating highorder derivatives and can be costly.

Results for examples 1 and 3 are in Appendix 9.9.

Training data

Posterior sample

y3

y3

y1

y2

Samples from learned prior

Reconstruction Datay1

y2

Latent dimension 3

y3

yt

y1

y2

t

Figure 6: Learned posterior and prior dynamics on data from a stochastic Lorenz attractor. All samples from our model are continuous-time paths, and form a multi-modal, non-Gaussian distribution.

7.2 Synthetic Datasets
We trained latent SDEs with our adjoint framework to recover (1) a 1D Geometric Brownian motion, and (2) a 3D stochastic Lorenz attractor process. The main objective is to verify that the learned posterior can reconstruct the training data, and that the learned priors are not deterministic. We jointly optimize the evidence lower bound (10) with respect to parameters of the prior and posterior distributions at the initial latent state z0, the prior and posterior drift, the diﬀusion function, the encoder, and the decoder. We include the details of datasets and architectures in Appendix 9.10.
For the stochastic Lorenz attractor, not only is the model able to reconstruct the data well, but also the

learned prior process can produce bimodal samples in both data and latent space. This is showcased in the last row of Figure 6 where the latent and data space samples cluster around two modes. This is hard to achieve using a latent ODE with a unimodal Gaussian initial approximate posterior. We include additional visualizations in Appendix 9.11.
7.3 Motion Capture Dataset
To demonstrate that latent SDEs can learn complex dynamics from real-world datasets, we evaluated their predictive performance on a 50-dimensional motion capture dataset. The dataset, from Gan et al. [18], consists of 23 walking sequences of subject 35 partitioned into 16 training, 3 validation, and 4 test sequences. We follow the preprocessing of Wang et al. [85].
In designing the recognition network, we follow Yıldız et al. [90] and use a fully connected network to encode the ﬁrst three observations of each sequence and thereafter predicted the remaining sequence. This encoder is chosen for fair comparison to existing models, and could be extended to a recurrent or attention model [84]. The overall architecture is described in Appendix 9.12 and is similar to that of ODE2VAE [90], with a similar number of parameters. We also use a ﬁxed step size 1/5 of smallest interval between any two observations [90].
We train latent ODE and latent SDE models with the Adam optimizer [34] and its default hyperparameter settings, with an initial learning rate of 0.01 that is exponentially decayed with rate 0.999 during each iteration. We perform validation over the number of training iterations, KL penalty [29], and KL annealing schedule. All models were trained for at most 400 iterations, where we start to observe severe overﬁtting for most model instances. We report the test MSE on future observations following Yıldız et al. [90]. We believe that the improved performance is due to the strong regularization in path space, as removing the KL penalty improve training error but caused validation error to deteriorate.

Table 2: Test MSE on 297 future frames averaged over
50 samples. 95% conﬁdence interval reported based on t-statistic. †results from [90].

Method

Test MSE

DTSBN-S [18]
npODE [28]
NeuralODE [12] ODE2VAE [90] ODE2VAE-KL [90] Latent ODE [12, 72] Latent SDE (this work)

34.86 ± 0.02† 22.96† 22.49 ± 0.88† 10.06 ± 1.4† 8.09 ± 1.95†
5.98 ± 0.28
4.03 ± 0.20

Scalable Gradients for Stochastic Diﬀerential Equations

8 Discussion
We presented a generalization of the adjoint sensitivity method to compute gradients through solutions of SDEs. In contrast to existing approaches, this method has nearly the same time and memory complexity as simply solving the SDE. We showed how our stochastic adjoint framework can be combined with a gradientbased stochastic variational inference scheme for training latent SDEs.
It is worthwhile to mention that SDEs and the commonly used GP models deﬁne two distinct classes of stochastic processes, albeit having a nonempty intersection (e.g. Ornstein-Uhlenbeck processes fall under both). Computationally, the cost of ﬁtting GPs lies in the matrix inversion, whereas the computational bottleneck of training SDEs is the sequential numerical solve. Empirically, another avenue of research is to reduce the variance of gradient estimates. In the future, we may adopt techniques such as control variates or antithetic paths.
On the application side, our method opens up a broad set of opportunities for ﬁtting any diﬀerentiable SDE model, such as Wright-Fisher models with selection and mutation parameters [15], derivative pricing models in ﬁnance, or inﬁnitely-deep Bayesian neural networks [61]. In addition, the latent SDE model enabled by our framework can be extended to include domain knowledge and structural or stationarity constraints [48] in the prior process for speciﬁc applications.
On the theory side, there remain fundamental questions to be answered. Convergence rates of numerical gradients estimated with general schemes are unknown. Additionally, since our analyses are based on strong orders of schemes, it is natural to question whether convergence results still hold when we consider weak errors, and moreover if the method could be reformulated more coherently with rough paths theory [47].
Acknowledgements
We thank Yulia Rubanova, Danijar Hafner, Mufan Li, Shengyang Sun, Kenneth R. Jackson, Simo Särkkä, Daniel Lacker, and Philippe Casgrain for helpful discussions. We thank Çağatay Yıldız for helpful discussions regarding evaluation settings of the mocap task. We also thank Guodong Zhang, Kevin Swersky, Chris Rackauckas, and members of the Vector Institute for helpful comments on an early draft of this paper.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale

machine learning. In 12th Symposium on Operating Systems Design and Implementation, pages 265–283, 2016.
[2] R Adams. Sobolev Spaces. Academic Press, 1975.
[3] Joel Andersson. A general-purpose software framework for dynamic optimization. PhD thesis, Arenberg Doctoral School, KU Leuven, 2013.
[4] Joel Andersson, Joris Gillis, Greg Horn, James B Rawlings, and Moritz Diehl. CasADi: a software framework for nonlinear optimization and optimal control. Mathematical Programming Computation, 11(1):1–36, 2019.
[5] Cédric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John S Shawe-Taylor. Variational inference for diﬀusion processes. In Advances in Neural Information Processing Systems, pages 17–24, 2008.
[6] VI Arnold. Ordinary Diﬀerential Equations. The MIT Press, 1978.
[7] Jonathan Baxter and Peter L Bartlett. Inﬁnitehorizon gradient-based policy search. 2001.
[8] Robert Brown. ... microscopical observations ... on the particles contained in the pollen of plants. The Philosophical Magazine, 4(21):161–173, 1828.
[9] Pamela M Burrage, R Herdiana, and Kevin Burrage. Adaptive stepsize based on control theory for stochastic diﬀerential equations. Journal of Computational and Applied Mathematics, 170(2): 317–336, 2004.
[10] Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks from dynamical systems view. arXiv preprint arXiv:1710.10348, 2017.
[11] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural networks. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[12] Ricky Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary diﬀerential equations. In Advances in neural information processing systems, pages 6571–6583, 2018.
[13] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[14] Koen Claessen and Michał H Pałka. Splittable pseudorandom number generators using crypto-

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

graphic hashing. In ACM SIGPLAN Notices, volume 48, pages 47–58. ACM, 2013.
[15] Warren J Ewens. Mathematical population genetics 1: theoretical introduction, volume 27. Springer Science & Business Media, 2012.
[16] Roy Frostig, Matthew James Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing, 2018.
[17] Jessica G Gaines and Terry J Lyons. Variable step size control in the numerical solution of stochastic diﬀerential equations. SIAM Journal on Applied Mathematics, 57(5):1455–1484, 1997.
[18] Zhe Gan, Chunyuan Li, Ricardo Henao, David E Carlson, and Lawrence Carin. Deep temporal sigmoid belief networks for sequence modeling. In Advances in Neural Information Processing Systems, pages 2467–2475, 2015.
[19] Mike Giles and Paul Glasserman. Smoking adjoints: Fast Monte Carlo greeks. Risk, 19(1):88–92, 2006.
[20] Paul Glasserman and David D Yao. Some guidelines and guarantees for common random numbers. Management Science, 38(6):884–908, 1992.
[21] Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75–84, 1990.
[22] Emmanuel Gobet and Rémi Munos. Sensitivity analysis using Itô–Malliavin calculus and martingales, and application to stochastic optimal control. SIAM Journal on control and optimization, 43(5): 1676–1713, 2005.
[23] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. International Conference on Learning Representations, 2019.
[24] Narendra Gupta and Raman Mehra. Computational aspects of maximum likelihood estimation and reduction in sensitivity function calculations. IEEE transactions on automatic control, 19(6): 774–783, 1974.
[25] Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, and Han-Lim Choi. Adaptive pathintegral autoencoders: Representation learning and planning for dynamical systems. In Advances in Neural Information Processing Systems, pages 8927–8938, 2018.
[26] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004, 2017.
[27] Pashupati Hegde, Markus Heinonen, Harri Lähdesmäki, and Samuel Kaski. Deep learning

with diﬀerential gaussian process ﬂows. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1812–1821, 2019.
[28] Markus Heinonen, Cagatay Yildiz, Henrik Mannerström, Jukka Intosalmi, and Harri Lähdesmäki. Learning unknown ode models with gaussian processes. arXiv preprint arXiv:1803.04303, 2018.
[29] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. betavae: Learning basic visual concepts with a constrained variational framework. ICLR, 2(5):6, 2017.
[30] Silvana Ilie, Kenneth R Jackson, and Wayne H Enright. Adaptive time-stepping for the strong numerical solution of stochastic diﬀerential equations. Numerical Algorithms, 68(4):791–812, 2015.
[31] Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckus, Elliot Saba, Viral B Shah, and Will Tebbutt. Zygote: A diﬀerentiable programming system to bridge machine learning and scientiﬁc computing. arXiv preprint arXiv:1907.07587, 2019.
[32] Junteng Jia and Austin R. Benson. Neural Jump Stochastic Diﬀerential Equations. arXiv e-prints, art. arXiv:1905.10403, May 2019.
[33] Hilbert Johan Kappen and Hans Christian Ruiz. Adaptive importance sampling for control and inference. Journal of Statistical Physics, 162(5): 1244–1266, 2016.
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[35] Diederik P Kingma and Max Welling. Autoencoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[36] Genshiro Kitagawa and Will Gersch. Linear gaussian state space modeling. In Smoothness Priors Analysis of Time Series, pages 55–65. Springer, 1996.
[37] Jack PC Kleijnen and Reuven Y Rubinstein. Optimization and sensitivity analysis of computer simulation models by the score function method. European Journal of Operational Research, 88(3): 413–427, 1996.
[38] Peter E Kloeden and Andreas Neuenkirch. The pathwise convergence of approximation schemes for stochastic diﬀerential equations. LMS journal of Computation and Mathematics, 10:235–253, 2007.
[39] Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic diﬀerential equations,

Scalable Gradients for Stochastic Diﬀerential Equations

volume 23. Springer Science & Business Media, 2013.
[40] Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.
[41] Hiroshi Kunita. Stochastic Flows and JumpDiﬀusions. Springer, 2019.
[42] Harold Kushner and Paul G Dupuis. Numerical methods for stochastic control problems in continuous time, volume 24. Springer Science & Business Media, 2013.
[43] Pierre L’Ecuyer and Gaétan Perron. On the convergence rates of ipa and fdc derivative estimators. Operations Research, 42(4):643–656, 1994.
[44] Qianxiao Li, Long Chen, Cheng Tai, and E Weinan. Maximum principle based algorithms for deep learning. The Journal of Machine Learning Research, 18(1):5998–6026, 2017.
[45] Xuanqing Liu, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde: Stabilizing neural ode networks with stochastic noise. arXiv preprint arXiv:1906.02355, 2019.
[46] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ﬁnite layer neural networks: Bridging deep architectures and numerical diﬀerential equations. arXiv preprint arXiv:1710.10121, 2017.
[47] Terry J Lyons. Diﬀerential equations driven by rough signals. Revista Matemática Iberoamericana, 14(2):215–310, 1998.
[48] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. In Advances in Neural Information Processing Systems, pages 2917–2925, 2015.
[49] Dougal Maclaurin, David Duvenaud, M Johnson, and RP Adams. Autograd: Reverse-mode diﬀerentiation of native python. In ICML workshop on Automatic Machine Learning, 2015.
[50] Isambi S Mbalawata, Simo Särkkä, and Heikki Haario. Parameter estimation in stochastic diﬀerential equations with markov chain monte carlo and non-linear kalman ﬁltering. Computational Statistics, 28(3):1195–1223, 2013.
[51] Grigori Noah Milstein and Michael V Tretyakov. Stochastic Numerics for Mathematical Physics. Springer Science & Business Media, 2013.
[52] Grigorii Noikhovich Milstein. Numerical integration of stochastic diﬀerential equations, volume 313. Springer Science & Business Media, 1994.
[53] Ivan Nourdin and Giovanni Peccati. Normal approximations with Malliavin calculus: from Stein’s

method to universality, volume 192. Cambridge University Press, 2012.
[54] Daniel Ocone and Étienne Pardoux. A generalized itô-ventzell formula. application to a class of anticipating stochastic diﬀerential equations. 25 (1):39–71, 1989.
[55] Bernt Øksendal. Stochastic Diﬀerential Equations. Springer, 2003.
[56] Bernt Oksendal. Stochastic diﬀerential equations: an introduction with applications. Springer Science & Business Media, 2013.
[57] Manfred Opper. Variational inference for stochastic diﬀerential equations. Annalen der Physik, 531 (3):1800233, 2019.
[58] Etienne Pardoux and Shige Peng. Backward stochastic diﬀerential equations and quasilinear parabolic partial diﬀerential equations. In Stochastic Partial Diﬀerential Equations and Their Applications, pages 200–217. Springer, 1992.
[59] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic diﬀerentiation in pytorch. 2017.
[60] Barak A Pearlmutter. Gradient calculations for dynamic recurrent neural networks: A survey. IEEE Transactions on Neural networks, 6(5):1212–1228, 1995.
[61] Stefano Peluchetti and Stefano Favaro. Neural stochastic diﬀerential equations. arXiv preprint arXiv:1904.01681, 2019.
[62] Shige Peng. A general stochastic maximum principle for optimal control problems. SIAM Journal on Control and Optimization, 28(4):966–979, 1990.
[63] Shige Peng and Zhen Wu. Fully coupled forwardbackward stochastic diﬀerential equations and applications to optimal control. SIAM Journal on Control and Optimization, 37(3):825–843, 1999.
[64] Eckhard Platen. An introduction to numerical methods for stochastic diﬀerential equations. Acta numerica, 8:197–246, 1999.
[65] Lev Semenovich Pontryagin. Mathematical Theory of Optimal Processes. Routledge, 2018.
[66] Christopher Rackauckas and Qing Nie. Adaptive methods for stochastic diﬀerential equations via natural embeddings and rejection sampling with memory. Discrete and Continuous Dynamical Systems. Series B, 22(7):2731, 2017.
[67] Daniel Revuz and Marc Yor. Continuous martingales and Brownian motion, volume 293. Springer Science & Business Media, 2013.

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

[68] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
[69] L Chris G Rogers and David Williams. Diﬀusions, Markov Processes and Martingales: Volume 2, Itô Calculus, volume 2. Cambridge University Press, 2000.
[70] Andreas Rößler. Runge–Kutta methods for stratonovich stochastic diﬀerential equation systems with commutative noise. Journal of Computational and Applied mathematics, 164:613–627, 2004.
[71] Andreas Rößler. Runge–Kutta methods for the strong approximation of solutions of stochastic diﬀerential equations. SIAM Journal on Numerical Analysis, 48(3):922–952, 2010.
[72] Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent odes for irregularly-sampled time series. Neural Information Processing Systems, 2019.
[73] David E Rumelhart, Geoﬀrey E Hinton, Ronald J Williams, et al. Learning representations by backpropagating errors. Cognitive Modeling, 5(3):1, 1988.
[74] Lars Ruthotto and Eldad Haber. Deep neural networks motivated by partial diﬀerential equations. arXiv preprint arXiv:1804.04272, 2018.
[75] Thomas Ryder, Andrew Golightly, A Stephen McGough, and Dennis Prangle. Black-box variational inference for stochastic diﬀerential equations. arXiv preprint arXiv:1802.03335, 2018.
[76] John K Salmon, Mark A Moraes, Ron O Dror, and David E Shaw. Parallel random numbers: as easy as 1, 2, 3. In Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, page 16. ACM, 2011.
[77] Simo Särkkä. Bayesian ﬁltering and smoothing, volume 3. Cambridge University Press, 2013.
[78] Simo Särkkä and Arno Solin. Applied stochastic diﬀerential equations, volume 10. Cambridge University Press, 2019.
[79] Steven E Shreve. Stochastic calculus for ﬁnance II: Continuous-time models, volume 11. Springer Science & Business Media, 2004.
[80] Evangelos Theodorou. Nonlinear stochastic control and information theoretic dualities: Connections, interdependencies and thermodynamic interpretations. Entropy, 17(5):3352–3375, 2015.
[81] Ryan Turner, Marc Deisenroth, and Carl Rasmussen. State-space inference and learning with

gaussian processes. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 868–875, 2010.
[82] Belinda Tzen and Maxim Raginsky. Neural stochastic diﬀerential equations: Deep latent gaussian models in the diﬀusion limit. arXiv preprint arXiv:1905.09883, 2019.
[83] Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diﬀusions. Proceeings of the Conference on Learning Theory, 2019.
[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
[85] Jack M Wang, David J Fleet, and Aaron Hertzmann. Gaussian process dynamical models for human motion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(2):283–298, 2007.
[86] E Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics and Statistics, 5(1):1–11, 2017.
[87] Magnus Wiktorsson et al. Joint characteristic function and simultaneous simulation of iterated itô integrals for multiple independent brownian motions. The Annals of Applied Probability, 11(2): 470–487, 2001.
[88] Ronald J Williams. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229–256, 1992.
[89] Jichuan Yang and Harold J Kushner. A monte carlo method for sensitivity analysis and parametric optimization of nonlinear stochastic systems. SIAM Journal on Control and Optimization, 29 (5):1216–1249, 1991.
[90] Çağatay Yıldız, Markus Heinonen, and Harri Lähdesmäki. Ode2vae: Deep generative second order odes with bayesian neural networks. arXiv preprint arXiv:1905.10994, 2019.

Scalable Gradients for Stochastic Diﬀerential Equations

9 Appendix

9.1 Notation
For a ﬁxed terminal time T > 0, we denote by T = [0, T ] ⊆ R the time horizon. Let C∞ be the class of inﬁnitely diﬀerentiable functions from Rd to itself. Let Cp,q be the class of functions from Rd × T to Rd that are p and q times continuously diﬀerentiable in the ﬁrst and second input, respectively. Let Cbp,q ⊆ Cp,q be the subclass with bounded derivatives of all possible orders. For a positive integer m, we adopt the shorthand [m] = {1, 2, . . . , m}. We denote the Euclidean norm of a vector v by |v|. For f ∈ Cp,q, we denote its Jacobian with respect to the ﬁrst input by ∇f . We denote the concatenation of two vectors u ∈ Rd1 and v ∈ Rd2 by the simpliﬁed notation (u, v), as opposed to the slightly lengthy notation (u , v ) .

9.2 Proof of Theorem 3.1
Proof of Theorem 3.1. We have Js,t(z) = ∇Ψs,t(z), where Ψs,t(z) is deﬁned in (3). Now we take the gradient with respect to z on both sides. The solution is diﬀerentiable with respect to z and we may diﬀerentiate under the stochastic integral [41, Proposition 2.4.3]. Theorem 3.4.3 [41] is suﬃcient for the regularity conditions required. Since Ks,t(z) = Js,t(z)−1, applying the Stratonovich version of Itô’s formula to (4), we have (5).

9.3 Proof of Theorem 3.3

Proof of Theorem 3.3. By the triangle inequality,

|F(G(z, W·), W·) − Fh(Gh(z, W·), W·)| ≤ |F(G(z, W·), W·) − F(Gh(z, W·), W·)| + |F(Gh(z, W·), W·) − Fh(Gh(z, W·), W·)| .

Ih(1)

Ih(2)

We show that both Ih(1) and Ih(2) converge to 0 in probability as h → 0. For simplicity, we suppress z and W·. Bounding Ih(1). Let > 0 be given. Since Gh → G in probability, there exist M1 > 0 and h0 > 0 such that

P(|G| > M1) < , P(|Gh| > 2M1) < , for all h ≤ h0.

By Lemma 2.1 (iv) of Ocone and Pardoux [54], which can be easily adapted to our context, there exists a positive
random variable C1, ﬁnite almost surely, such that sup|z|≤2M1 |∇zF| ≤ C1, and there exists M2 > 0 such that P(|C1| > M2) < . Given M2, there exists h1 > 0 such that

P |G − Gh| > M2 < , for all h ≤ h1. Now, suppose h ≤ min{h0, h1}. Then, by the union bound, with probability at least 1 − 4 , we have

|G| ≤ M1, |Gh| ≤ 2M1, |C1| ≤ M2, |G − Gh| ≤ M2 .

On this event, we have

Ih(1) = |F(G) − F(Gh)| ≤ C1|G − Gh| ≤ M2 M2 = .

Thus, we have shown that Ih(1) converges to 0 in probability as h → 0.

Bounding Ih(2). The idea is similar. By condition (ii), we have

lim sup |Fh(zT ) − F(zT )| = 0
h→0 |zT |≤M

in probability. Using this and condition (i), for given > 0, there exist M > 0 and h2 > 0 such that for all h ≤ h2, we have
|Gh| ≤ M and sup |Fh(zT ) − F(zT )| <
|zT |≤M

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

with probability at least 1 − . On this event, we have
|F(Gh) − Fh(Gh)| ≤ sup |Fh(zT ) − F(zT )| < .
|zT |≤M
Thus Ih(2) also converges to 0 in probability as h → 0.
9.4 Euler-Maruyama Scheme Satisﬁes Local Uniform Convergence
Here we verify that the Euler-Maruyama scheme satisﬁes condition (ii) when d = 1. Our proof can be extended to the case where d > 1 assuming an Lp estimate of the error; see the discussion after the proof of Proposition 9.1. Proposition 9.1. Let Fh(z) be the Euler-Maruyama discretization of a 1-dimensional SDE with mesh size h of F(z). Then, for any compact A ⊂ R, we have
plimh→0 sup |Fh(z) − F(z)| = 0.
z∈A

Usual convergence results in stochastic numerics only control the error for a single ﬁxed starting point. Here, we strengthen the result to local uniform convergence. Our main idea is to apply a Sobolev inequality argument [54, Part II]. To do so, we need some preliminary results about the Euler-Maruyama discretization of the original SDE and its derivative. We ﬁrst recall a theorem characterizing the expected squared error for general schemes.
Theorem 9.2 (Mean-square order of convergence [51, Theorem 1.1]). Let {Ztz}t≥0 be the solution to an Itô SDE, and {Z˜kz}k∈N be a numerical discretization with ﬁxed step size h, both of which are started at z ∈ Rd and deﬁned on the same probability space. Let the coeﬃcients of the SDE be Cb1,∞. Furthermore, suppose that the numerical scheme has order of accuracy p1 for the expectation of deviation and order of accuracy p2 for the mean-square deviation. If p1 ≥ p2 + 1/2 and p2 ≥ 1/2, then, for any N ∈ N, k ∈ [N ], and z ∈ Rd
E |Ztzk − Z˜kz|2 ≤ C 1 + |z|2 h2p2−1,

for a constant C that does not depend on h or z.

We refer the reader to [51] for the precise deﬁnitions of orders of accuracy and the proof. Given this theorem, we establish an estimate regarding errors of the discretization and its derivative with respect to the initial position.
Lemma 9.3. We have

E |F(z) − Fh(z)|2 + |∇zF(z) − ∇zFh(z)|2 ≤C1(1 + |z|2)h,

where C1 is a constant independent of z and h.

Proof of Lemma 9.3. Since the coeﬃcients of the SDE are of class Cb∞,1, we may diﬀerentiate the SDE in z to get the SDE for the derivative ∇zZtz [41]. Speciﬁcally, letting Ytz = ∇zZtz, we have

t

t

Ytz = Id +

∇

b(

Z

z s

,

s)

Ysz

ds +

∇

σ

(

Z

z s

,

s)

Ysz

dWs.

0

0

Note that the augmented process (F(z), ∇zF(z)) satisﬁes an SDE with Cb∞,1 coeﬃcients. By the chain rule, one can easily show that the derivative of the Euler-Maruyama discretization Fh(z) is the discretization of the derivative process Ytz. Thus, (Fh(z), ∇zFh(z)) is simply the discretization of (F(z), ∇zF(z)). Since the Euler-Maruyama scheme has orders of accuracy (p1, p2) = (1.5, 1.0) [51, Section 1.1.5], by Theorem 9.2, we have
E |F(z) − Fh(z)|2 + |∇zF(z) − ∇zFh(z)|2 ≤ C1(1 + |z|2)h, z ∈ Rd
for some constant C1 that does not depend on z or h.

We also recall a variant of the Sobolev inequality which we will apply for d = 1.

Scalable Gradients for Stochastic Diﬀerential Equations

Theorem 9.4 (Sobolev inequality [2, Theorem 5.4.1.c]). For any p > d, there exists a universal constant cp such that

where

sup |f (x)| ≤ cp f 1,p ,
x∈Rd

f p1,p := |f (x)|p dx + |∇xf (x)|p dx,

Rd

Rd

for all continuously diﬀerentiable f : Rd → R.

Proof of Proposition 9.1. Deﬁne Hαh : Ω × R → R, regarded as a random function Hαh (ω) : R → R, by

Hαh (z) = (1F(+z)|z−|2F)1h/(2z+)α ,

where α > 1/2 is a ﬁxed constant. Since Hαh is continuously diﬀerentiable a.s., by Theorem 9.4,

|F(z) − Fh(z)| ≤ c2(1 + |z|2)1/2+α Hαh 1,2 , for all z ∈ R a.s.

Without loss of generality, we may let the compact set be A = {z : |z| ≤ M } where M > 0. Then,

sup |F(z) − Fh(z)| ≤ c2(1 + M 2)1/2+α Hαh 1,2 , a.s.

(11)

|z|≤M

It remains to estimate Hαh 1,2. Starting from the deﬁnition of · 1,p, a standard estimation yields

Hαh

2 1,2

≤C2

|F(z) − Fh(z)|2 + |∇zF(z) − ∇zFh(z)|2 dz, (1 + |z|2)1+2α

R

where C2 is a deterministic constant depending only on α (but not z and h). Now we take expectation on both sides. By Lemma 9.3, we have

E

Hαh

2 1,2

≤C2

E[|F(z) − Fh(z)|2 + |∇zF(z) − ∇zFh(z)|2] dz, (1 + |z|2)1+2α

R

1 ≤C1C2h (1 + |z|2)2α dz,
R

where the last integral is ﬁnite since α > 1/2.

We have shown that E

Hαh

2 1,2

= O(h). Thus

Hαh 1,2 → 0 in L2, and hence also in probability, as h → 0. From

equation 11, we have that supz∈A |Fh(z) − F(z)| converges to 0 in probability as h → 0.

It is clear from the above proof that we may generalize to the case where d > 1 and other numerical schemes if we can bound the expected W 1,p-norm of Fh − F in terms of z and h, for p > d, where W 1,p here denotes the Sobolev space consisting of all real-valued functions on Rd whose weak derivatives are functions in Lp. For the Euler scheme and d > 1, we need only bound the Lp norm of the discretization error in terms of z and h for general p. To achieve this, we would need to make explicit the dependence on z for existing estimates (see e.g. [39, Chapter 10]).
Generically extending the argument to other numerical schemes, however, is technically non-trivial. We plan to address this question in future research.

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

9.5 Stochastic Adjoint has Commutative Noise when Original SDE has Diagonal Noise

Recall the Stratonovich SDE (2) with drift and diﬀusion functions b, σ1, . . . , σm ∈ Rd × R → Rd being governed by a set of parameters θ ∈ Rp. Consider the augmented state composed of the state and parameters Yt = (Zt, θ). The augmented state satisﬁes a Stratonovich SDE with the drift function f (y, t) = (b(z, t), 0p) and diﬀusion functions gi(y, t) = (σi(z, t), 0p) for i ∈ [m]. We have omitted the dependence on parameter θ to reduce notational clutter. By (5) and (6), the adjoint process of the augmented state follows the backward Stratonovich SDE:

y

y

At = AT +

Ty

m

As ∇f (Y s, s) ds +

t

i=1

Ty

(i)

As ∇gi(Y s, s) ◦ dW s .

t

By deﬁnitions of f and gi, the Jacobian matrices ∇f (x, s) and ∇gi(x, s) have the forms

∇f (y, s) =

∇b(z, s) 0p×d

0d×p 0p×p

∈ R(d+p)×(d+p),

∇gi(y, s) =

∇σi(z, s) 0p×d

0d×p 0p×p

∈ R(d+p)×(d+p).

Thus, the backward Stratonovich SDEs for the adjoint processes of the state and parameters have the forms

z

z

At =AT +

T z ∂b(z, s)

m

t As ∂z

ds +

z =Z s

i=1

T z ∂σi(z, s)

(i)

t As ∂z

◦ dW s ,
z =Z s

(12)

θ

θ

At =AT +

T z ∂b(z, s)

m

t As ∂θ

ds +

z =Z s

i=1

T z ∂σi(z, s)

(i)

t As ∂θ

◦ dW s .
z =Z s

Now assume the original SDE has diagonal noise. Then, m = d and Jacobian matrix ∇σi(z) has the form

0 ... 0 0 0 ... 0

∇σi(z) = 0 ... 0 ∂σ∂i,zii(z) 0 ... 0 .

(13)

0 ... 0 0 0 ... 0

Consider the adjoint process for the augmented state along with the backward ﬂow of the backward Stratonovich
zθ
SDE (3), whose overall state we denote by Xt = (Zt, At , At ). By (12) and (13), {Xt}t∈T satisﬁes a backward Stratonovich SDE with a diﬀusion of the form

 −σ1,1(z1)
 

0 ... 0 ...

0



 

 

0

0 ... 0

−σd,d(zd)

 

 

∂

σ1,1 ∂z

(z

1

)

az1

0

...

0



1

 G(x) = 

...





0





 

∈

R(2d+p)×d,



 

0

 

∂σ1,1(z1) az

 ∂θ1 1

0 ... 0 ... ... ...

∂σd∂,zdd(zd) azd  ∂σd∂,dθ1(zd) azd 

 ...

... ... ...

... 





a ∂σ1,1(z1) z

∂θp

1

... ... ...

a ∂σd,d(zd) z

∂θp

d

(14)

where x = (z, az, aθ), and the subscript indexes the dimension. Recall, for an SDE with diﬀusion function Σ(x) ∈ Rd×m, it is said to satisfy the commutativity property [70] if

d

∂Σk,j (x) d

∂Σk,j (x)

Σi,j2 (x)

1 = Σi,j1 (x)

2,

i=1 ∂xi i=1 ∂xi

(15)

for all j1, j2 ∈ [m] and k ∈ [d]. When an SDE has commutative noise, the computationally intensive double Itô integrals (and the Lévy areas) need not be simulated by having the numerical scheme take advantage of the following property of iterated integrals [30]:

tu

tu

dWr(i) dWu(j) +

dWr(j) dWu(i) = ∆W (i)∆W (j),

ss

ss

where the Brownian motion increment ∆W (i) = Wt(i) − Ws(i) for i ∈ [m] can be easily sampled.

We show the diﬀusion function (14) satisﬁes the commutativity condition (15) with a proof by exhaustion:

Scalable Gradients for Stochastic Diﬀerential Equations

Case 1: k = 1, . . . , d. Both LHS and RHS are zero unless j1 = j2 = k, since for Σi,j2 (x) ∂Σk∂,xj1i(x) to be non-zero, i = j1 = j2 = k.
Case 2: k = d + 1 . . . , 2d. Similar to the case above.
Case 3: k = 2d + 1 . . . , 2d + p. Write k = 2d + l, where l ∈ [p]. Both LHS and RHS are zero unless j1 = j2 = l, since for Σi,j2 (x) ∂Σk∂,xj1i(x) to be non-zero i = l or i = d + l and j1 = j2 = l.
This concludes that the commutativity condition holds. Finally, we comment that the Milstein scheme for the stochastic adjoint of diagonal noise SDEs can be implemented such that during each iteration of the backward solve, vjp is only called a number of times independent of the dimensionality of the original SDE.

9.6 Background on Latent SDE

Consider a ﬁltered probability space (Ω, F , {Ft}0≤t≤T , P ), where T = [0, T ] is a ﬁnite time horizon.

Recall the approximate posterior process that we intend to learn is governed by the SDE:

dZt = hφ(Zt, t) dt + σ(Zt, t) dWt, Z0 = z0 ∈ Rd.

(16)

Suppose there exists a measurable function u(z, t) such that

• σ(z, t)u(z, t) = hφ(z, t) − hθ(z, t), and • u(Zt, t) satisﬁes Novikov’s condition, i.e. E exp

0T 12 |u(Zt, t)|2 dt

< ∞.

Novikov’s condition ensures that the process

Mt = exp − t 1 |u(Zs, s)|2 ds − t u(Zs, s)

02

0

dWs ,

0 ≤ t ≤ T,

is a P -martingale. By Girsanov Theorem II [56, Theorem 8.6.4], the process Wt = 0t u(Zs, s) ds + Wt, 0 ≤ t ≤ T is a Wiener process under the probability measure Q deﬁned by

dQ = MT dP,

Moreover, since a simple rewrite shows that

dZt = hθ(Zt, t) dt + σ(Zt, t) dWt, Z0 = z0,

(17)

we conclude that the Q-law of (17) (or equivalently (16)) is the same as the P -law of the prior process.

9.6.1 Deriving the Variational Bound

Let xt1 , . . . , xtN be observed data at times t1, . . . , tN , whose conditionals only depend on the respective latent states zt1 , . . . , ztN . Since the Q-law of the approximate posterior is the same as the P -law of the prior,

log p(xt1 , . . . , xtN ) = log EP

N
p(xti |z˜ti )
i=1

= log EQ

N
p(xti |zti )
i=1

= log EP

N
p(xti |zti )MT
i=1

≥EP

N
log p(xti |zti ) + log MT
i=1

=EP

N
log p(xti |zti ) −
i=1

T 1 |u(Zt, t)|2 dt − 02

T
u(Zt)
0

dWt

=EP

N

T1

log p(xt |zt ) −

|u(Zt, t)|2 dt ,

i=1 i i 0 2

where the second line follows from the deﬁnition of Q and third line follows from Jensen’s inequality. In the last equality we used the fact that the Itô integral 0· u(Zt) dWt is a martingale.

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

9.7 Stochastic Adjoint for Latent SDE

To simulate the variational lower bound (10) with Monte Carlo in the forward pass along with the original dynamics, we need only extend the original augmented state with an extra variable Lt such that the new drift and diﬀusion functions for the new augmented state Yt = (Zt, θ, Lt) are

 b(z, t) 

f (x, t) =  0p  ∈ Rd+p+1,

1 2

|

u(z

,

t)|

2 2

σi(z, t) gi(x, t) =  0p  ∈ Rd+p+1,
0

i ∈ [m].

By (7), the backward SDEs of the adjoint processes become

z

z

T

At =AT +

t

θ

θ

T

At =AT +

t

l

l

At =AT .

z ∂b(z, s)

1 l ∂|u(z, s)|22

As ∂z

+ 2 As ∂z

z =Z s

z =Z s

z ∂b(z, s)

1 l ∂|u(z, s)|22

As ∂θ

+ 2 As ∂θ

z =Z s

z =Z s

m
ds +
i=1 m
ds +
i=1

T z ∂σi(z, s)

(i)

t As ∂z

◦ dW s ,
z =Z s

T z ∂σi(z, s)

(i)

t As ∂θ

◦ dW s ,
z =Z s

(18)

In this case, neither does one need to simulate the backward SDE of the extra variable nor does one need to simulate its adjoint. Moreover, when considered as a single system for the augmented adjoint state, the diﬀusion function of the backward SDE (18) satisﬁes the commutativity property (15).

9.8 Test Problems In the following, α, β, and p are parameters of SDEs, and x0 is a ﬁxed initial value.

Example 1.

dXt = αXt dt + βXt dWt, X0 = x0.

Analytical solution:

β− α2 t+αWt

Xt = X0e 2

.

Example 2. Analytical solution:

dXt = − p2 2 sin (Xt) cos3 (Xt) dt + p cos2 (Xt) dWt, X0 = x0. Xt = arctan (pWt + tan (X0)) .

Example 3.

β

dXt = √

−

1

αβ

Xt dt + √

dWt, X0 = x0.

1 + t 2(1 + t)

1+t

Analytical solution:

1

β

Xt = √

X0 + √

(t + αWt) .

1+t

1+t

In each numerical experiment, we duplicate the equation 10 times to obtain a system of SDEs where each dimension had their own parameter values sampled from the standard Gaussian distribution and then passed through a sigmoid to ensure positivity. Moreover, we also sample the initial value for each dimension from a Gaussian distribution.

Error

Scalable Gradients for Stochastic Diﬀerential Equations

10 1 10 3 10 5 10 7
1e-1 1e-S2tep Siz1ee-3 1e-4
(a) Fixed Step Size vs Error
10 3 10 5 10 7 10 9
1e-1 1e-S2tep Siz1ee-3 1e-4
(d) Fixed Step Size vs Error

Error

Error

10 1

101

10 2

10 1

10 3

10 3

10 4

10 5

10 5

0 1 2 NF3E To4tal 5 6 17e4 10 6 (b) Total NFE vs Error

10 1
100

10 1

10 2

10 2

10 3

10 3

10 4

10 4

10 5

10 6

10 5

10 7

0.0

0.5 NF1E.0Total1.5

2.0

10 6

1e4

(e) Total NFE vs Error

Error

Error

Euler backprop (dt=0.001)

0.25

Milstein backprop (dt=0.01) Milstein adjoint (dt=0.01)

0.20

0.15

0.10

0.05

0.00 0.5 0.R6elativ0e.7Wall0ti.m8 e 0.9 1.0
(c) Eﬃciency Comparison

0.008

0.007

0.006

0.005

Euler backprop (dt=0.001)

0.004

Milstein backprop (dt=0.01) Milstein adjoint (dt=0.01)

0.003

0.002

0.001

0.000 0.4 0.5 R0e.6lative0.W7 allti0m.8e 0.9 1.0
(f) Eﬃciency Comparison

Figure 7: (a-c) Example 1. (d-f) Example 3.

Error

9.9 Results for Example 1 and 3
9.10 Toy Datasets Conﬁguration
9.10.1 Geometric Brownian Motion
Consider a geometric Brownian motion SDE:
dXt = µXt dt + σXt dWt, X0 = x0.
We use µ = 1, σ = 0.5, and x0 = 0.1 + as the ground-truth model, where ∼ N (0, 0.032). We sample 1024 time series, each of which is observed at intervals of 0.02 from time 0 to time 1. We corrupt this data using Gaussian noise with mean zero and standard deviation 0.01. To recover the dynamics, we use a GRU-based [13] latent SDE model where the GRU has 1 layer and 100 hidden units, the prior and posterior drift functions are MLPs with 1 hidden layer of 100 units, and the diﬀusion function is an MLP with 1 hidden layer of 100 hidden units and the sigmoid activation applied at the end. The drift function in the posterior is time-inhomogenous in the sense that it takes in a context vector of size 1 at each observation that is output by the GRU from running backwards after processing all future observations. The decoder is a linear mapping from a 4 dimensional latent space to observation space. For all nonlinearities, we use the softplus function. We ﬁx the observation model to be Gaussian with noise standard deviation 0.01. We optimize the model jointly with respect to the parameters of a Gaussian distribution for initial latent state distribution, the prior and posterior drift functions, the diﬀusion function, the GRU encoder, and the decoder. We use a ﬁxed discretization with step size of 0.01 in both the forward and backward pass. We use the Adam optimizer [34] with an initial learning rate of 0.01 that is decay by a factor of 0.999 after each iteration. We use a linear KL annealing schedule over the ﬁrst 50 iterations.

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

9.10.2 Stochastic Lorenz Attractor
Consider a stochastic Lorenz attractor SDE with diagonal noise:
dXt =σ (Yt − Xt) dt + αx dWt, X0 = x0, dYt = (Xt (ρ − Zt) − Yt) dt + αy dWt, Y0 = y0, dZt = (XtYt − βZt) dt + αz dWt, Z0 = z0.
We use σ = 10, ρ = 28, β = 8/3, (αx, αy, αz) = (.15, .15., .15), and (x0, y0, z0) sampled from the standard Gaussian distribution as the ground-truth model. We sample 1024 time series, each of which is observed at intervals of 0.025 from time 0 to time 1. We normalize these samples by their mean and standard deviation across each dimension and corrupt this data by Gaussian noise with mean zero and standard deviation 0.01. We use the same architecture and training procedure for the latent SDE model as in the geometric Brownian motion section, except that the diﬀusion function consists of four small neural networks, each for a single dimension of the latent SDE.

9.11 Additional Visualization

Training data

Posterior sample 1

Posterior sample 2

Posterior sample 3

Posterior sample 4

y3

y3

y3

y3

y3

y1

y2

Samples from learned prior

Reconstruction Data y1

y2

Latent dimension 1

Reconstruction Data y1

y2

Latent dimension 2

Reconstruction Data y1

y2

Latent dimension 3

Reconstruction Data y1

y2

Latent dimension 4

y3

yt

yt

yt

yt

y1

y2

Samples from learned prior

Latent dimt ension 1

Latent dimt ension 2

Latent dimt ension 3

Latent dimt ension 4

y3

yt

yt

yt

yt

y1

y2

t

t

t

t

Figure 8: Additional visualizations of learned posterior and prior dynamics on the synthetic stochastic Lorenz attractor dataset. First row displays the true data and posterior reconstructions. Second row displays samples with initial latent state for each trajectory is sampled independently. Third row displays samples with initial latent state sampled and ﬁxed to be the same for diﬀerent trajectories.

See Figure 8 for additional visualization on the synthetic Lorenz attractor dataset. See Figure 9 for visualization on the synthetic geometric Brownian motion dataset. We comment that for the second example, the posterior reconstructs the data well, and the prior process exhibit behavior of the data. However, from the third row, we can observe that the prior process is learned such that most of the uncertainty is account for in the initial latent state. We leave the investigation of more interpretable prior process for future work.

9.12 Model Architecture for Learning from Motion Capture Dataset
We use a latent SDE model with an MLP encoder which takes in the ﬁrst three frames and outputs the mean and log-variance of the variational distribution of the initial latent state and a context vector. The decoder has a similar architecture as that for the ODE2VAE model [90] and projects the 6-dimensional latent state into the 50-dimensional observation space. The posterior drift function takes in a 3-dimensional context vector output by the encoder and the current state and time, whereas the prior drift only takes in the current state and time. The diﬀusion function is composed of multiple small neural nets, each producing a scalar for the corresponding

Scalable Gradients for Stochastic Diﬀerential Equations
Figure 9: Visualizations of learned posterior and prior dynamics on the synthetic geometric Brownian motion dataset. First row displays the true data and posterior reconstructions. Orange contour covers 95% of 512 samples. Second row displays samples with initial latent state for each trajectory is sampled independently. Third row displays samples with initial latent state sampled and ﬁxed to be the same for diﬀerent trajectories. dimension such that the posterior SDE has diagonal noise. We use the same observation likelihood as that of the ODE2VAE model [90]. We comment that the overall parameter count of our model (11605) is smaller than that of ODE2VAE for the same task (12157). The latent ODE baseline was implemented with a similar architecture, except is does not have the diﬀusion and prior drift components, and its vector ﬁeld deﬁning the ODE does not take in a context vector. Therefore, the model has slightly fewer parameters (10573) than the latent SDE model. See Figure 10 for overall details of the architecture. The main hyperparameter we tuned was the coeﬃcient for reweighting the KL. For both the latent ODE and SDE, we considered training the model with a reweighting coeﬃcient in {1, 0.1, 0.01, 0.001}, either with or without a linear KL annealing schedule that increased from 0 to the prescribed value over the ﬁrst 200 iterations of training.

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud
Figure 10: Architecture speciﬁcs for the latent SDE model used to train on the mocap dataset. First row from left to right are the encoder and decoder. Second row from left to right are the prior drift, posterior drift, and diﬀusion functions.

Scalable Gradients for Stochastic Diﬀerential Equations

9.13 Stochastic Adjoint Implementation
We include the core implementation of the stochastic adjoint, assuming access to a callable Brownian motion bm, an Euler-Maruyama integrator ito_int_diag for diagonal noise SDEs, and several helper functions whose purposes can be inferred from their names.

class _SdeintAdjointMethod(torch.autograd.Function):

@staticmethod def forward(ctx, *args):
(y0, f, g, ts, flat_params_f, flat_params_g, dt, bm) = ( args[:-8], args[-7], args[-6], args[-5], args[-4], args[-3], args[-2], args[-1])
ctx.f, ctx.g, ctx.dt, ctx.bm = f, g, dt, bm

def g_prod(t, y, noise): g_eval = g(t=t, y=y) g_prod_eval = tuple( g_eval_i * noise_i for g_eval_i, noise_i in _zip(g_eval, noise)) return g_prod_eval

with torch.no_grad(): ans = ito_int_diag(f, g_prod, y0, ts, dt, bm)
ctx.save_for_backward(ts, flat_params_f, flat_params_g, *ans) return ans

@staticmethod def backward(ctx, *grad_outputs):
ts, flat_params_f, flat_params_g, *ans = ctx.saved_tensors f, g, dt, bm = ctx.f, ctx.g, ctx.dt, ctx.bm f_params, g_params = tuple(f.parameters()), tuple(g.parameters()) n_tensors = len(ans)

def aug_f(t, y_aug): y, adj_y = y_aug[:n_tensors], y_aug[n_tensors:2 * n_tensors]

with torch.enable_grad(): y = tuple(y_.detach().requires_grad_(True) for y_ in y) adj_y = tuple(adj_y_.detach() for adj_y_ in adj_y)

g_eval = g(t=-t, y=y) gdg = torch.autograd.grad(
outputs=g_eval, inputs=y, grad_outputs=g_eval, create_graph=True) f_eval = f(t=-t, y=y) f_eval = _sequence_subtract(gdg,

f_eval)

# -f + gdg.

vjp_y_and_params = torch.autograd.grad( outputs=f_eval, inputs=y + f_params + g_params, grad_outputs=tuple(-adj_y_ for adj_y_ in adj_y), retain_graph=True, allow_unused=True)
vjp_y = vjp_y_and_params[:n_tensors] vjp_f = vjp_y_and_params[-len(f_params + g_params):-len(g_params)] vjp_g = vjp_y_and_params[-len(g_params):]

vjp_y = tuple(torch.zeros_like(y_) if vjp_y_ is None else vjp_y_ for vjp_y_, y_ in zip(vjp_y, y))

adj_times_dgdx = torch.autograd.grad( outputs=g_eval, inputs=y, grad_outputs=adj_y, create_graph=True)
extra_vjp_y_and_params = torch.autograd.grad( outputs=g_eval, inputs=y + f_params + g_params, grad_outputs=adj_times_dgdx, allow_unused=True)

Xuechen Li∗, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud

extra_vjp_y = extra_vjp_y_and_params[:n_tensors] extra_vjp_f = extra_vjp_y_and_params[-len(f_params + g_params):-len(g_params)] extra_vjp_g = extra_vjp_y_and_params[-len(g_params):]

extra_vjp_y = tuple( torch.zeros_like(y_) if extra_vjp_y_ is None else extra_vjp_y_ for extra_vjp_y_, y_ in zip(extra_vjp_y, y))

vjp_y = _sequence_add(vjp_y, extra_vjp_y) vjp_f = vjp_f + extra_vjp_f vjp_g = vjp_g + extra_vjp_g

return (*f_eval, *vjp_y, vjp_f, vjp_g)

def aug_g_prod(t, y_aug, noise): y, adj_y = y_aug[:n_tensors], y_aug[n_tensors:2 * n_tensors]

with torch.enable_grad(): y = tuple(y_.detach().requires_grad_(True) for y_ in y) adj_y = tuple(adj_y_.detach() for adj_y_ in adj_y)

g_eval = tuple(-g_ for g_ in g(t=-t, y=y)) vjp_y_and_params = torch.autograd.grad(
outputs=g_eval, inputs=y + f_params + g_params, grad_outputs=tuple(-noise_ * adj_y_ for noise_, adj_y_ in zip(noise, allow_unused=True) vjp_y = vjp_y_and_params[:n_tensors] vjp_f = vjp_y_and_params[-len(f_params + g_params):-len(g_params)] vjp_g = vjp_y_and_params[-len(g_params):]

adj_y)),

vjp_y = tuple( torch.zeros_like(y_) if vjp_y_ is None else vjp_y_ for vjp_y_, y_ in zip(vjp_y, y)
) g_prod_eval = _sequence_multiply(g_eval, noise)

return (*g_prod_eval, *vjp_y, vjp_f, vjp_g)

def aug_bm(t): return tuple(-bmi for bmi in bm(-t))

T = ans[0].size(0) with torch.no_grad():
adj_y = tuple(grad_outputs_[-1] for grad_outputs_ in grad_outputs) adj_params_f = torch.zeros_like(flat_params_f) adj_params_g = torch.zeros_like(flat_params_g)

for i in range(T - 1, 0, -1): ans_i = tuple(ans_[i] for ans_ in ans) aug_y0 = (*ans_i, *adj_y, adj_params_f, adj_params_g) aug_ans = ito_int_diag( f=aug_f, g_prod=aug_g_prod, y0=aug_y0, ts=torch.tensor([-ts[i], -ts[i - 1]]).to(ts), dt=dt, bm=aug_bm) adj_y = aug_ans[n_tensors:2 * n_tensors] adj_params_f, adj_params_g = aug_ans[-2], aug_ans[-1]

# Take the result at the end time. adj_y = tuple(adj_y_[1] for adj_y_ in adj_y) adj_params_f, adj_params_g = adj_params_f[1], adj_params_g[1]

# Accumulate gradients at intermediate points. adj_y = _sequence_add(
adj_y, tuple(grad_outputs_[i - 1] for grad_outputs_ in grad_outputs) ) return (*adj_y, None, None, None, adj_params_f, adj_params_g, None, None)

