Which Training Methods for GANs do actually Converge?

arXiv:1801.04406v4 [cs.LG] 31 Jul 2018

Lars Mescheder 1 Andreas Geiger 1 2 Sebastian Nowozin 3

Abstract
Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zerocentered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a ﬁnite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simpliﬁed gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We ﬁnd these penalties to work well in practice and use them to learn highresolution generative image models for a variety of datasets with little hyperparameter tuning.
1. Introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are powerful latent variable models that can be used to learn complex real-world distributions. Especially for images, GANs have emerged as one of the dominant approaches for generating new realistically looking samples after the model has been trained on some dataset.
1MPI Tübingen, Germany 2ETH Zürich, Switzerland 3Microsoft Research, Cambridge, UK. Correspondence to: Lars Mescheder <lars.mescheder@tue.mpg.de>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).

Method
unregularized (Goodfellow et al., 2014) WGAN (Arjovsky et al., 2017) WGAN-GP (Gulrajani et al., 2017) DRAGAN (Kodali et al., 2017) Instance noise (Sønderby et al., 2016) ConOpt (Mescheder et al., 2017) Gradient penalties (Roth et al., 2017) Gradient penalty on real data only Gradient penalty on fake data only

Local convergence (a.c. case)
        

Local convergence (general case)
        

Table 1. Convergence properties of different GAN training algorithms for general GAN-architectures. Here, we distinguish between the case where both the data and generator distributions are absolutely continuous (a.c.) and the general case where they may lie on lower dimensional manifolds.

However, while very powerful, GANs can be hard to train and in practice it is often observed that gradient descent based GAN optimization does not lead to convergence. As a result, a lot of recent research has focused on ﬁnding better training algorithms (Arjovsky et al., 2017; Gulrajani et al., 2017; Kodali et al., 2017; Sønderby et al., 2016; Roth et al., 2017) for GANs as well as gaining better theoretically understanding of their training dynamics (Arjovsky et al., 2017; Arjovsky & Bottou, 2017; Mescheder et al., 2017; Nagarajan & Kolter, 2017; Heusel et al., 2017).
Despite practical advances, the training dynamics of GANs are still not completely understood. Recently, Mescheder et al. (2017) and Nagarajan & Kolter (2017) showed that local convergence and stability properties of GAN training can be analyzed by examining the eigenvalues of the Jacobian of the the associated gradient vector ﬁeld: if the Jacobian has only eigenvalues with negative real-part at the equilibrium point, GAN training converges locally for small enough learning rates. On the other hand, if the Jacobian has eigenvalues on the imaginary axis, it is generally not locally convergent. Moreover, Mescheder et al. (2017) showed that if there are eigenvalues close but not on the imaginary axis, the training algorithm can require intractably small learning rates to achieve convergence. While Mescheder et al. (2017) observed eigenvalues close to the imaginary axis in practice, this observation does not answer the question if eigenvalues close to the imaginary axis are a general phenomenon and if yes, whether they are indeed the root cause for the training

Which Training Methods for GANs do actually Converge?

instabilities that people observe in practice.
A partial answer to this question was given by Nagarajan & Kolter (2017), who showed that for absolutely continuous data and generator distributions1 all eigenvalues of the Jacobian have negative real-part. As a result, GANs are locally convergent for small enough learning rates in this case. However, the assumption of absolute continuity is not true for common use cases of GANs, where both distributions may lie on lower dimensional manifolds (Sønderby et al., 2016; Arjovsky & Bottou, 2017).
In this paper we show that this assumption is indeed necessary: by considering a simple yet prototypical example of GAN training we analytically show that (unregularized) GAN training is not always locally convergent. We also discuss how recent techniques for stabilizing GAN training affect local convergence on our example problem. Our ﬁndings show that neither Wasserstein GANs (WGANs) (Arjovsky et al., 2017) nor Wasserstein GANs with Gradient Penalty (WGAN-GP) (Gulrajani et al., 2017) nor DRAGAN (Kodali et al., 2017) converge on this simple example for a ﬁxed number of discriminator updates per generator update. On the other hand, we show that instance noise (Sønderby et al., 2016; Arjovsky & Bottou, 2017), zero-centered gradient penalties (Roth et al., 2017) and consensus optimization (Mescheder et al., 2017) lead to local convergence.
Based on our analysis, we give a new explanation for the instabilities commonly observed when training GANs based on discriminator gradients orthogonal to the tangent space of the data manifold. We also introduce simpliﬁed gradient penalties for which we prove local convergence. We ﬁnd that these gradient penalties work well in practice, allowing us to learn high-resolution image based generative models for a variety of datasets with little hyperparameter tuning.
In summary, our contributions are as follows:
• We identify a simple yet prototypical counterexample showing that (unregularized) gradient descent based GAN optimization is not always locally convergent
• We discuss if and how recently introduced regularization techniques stabilize the training
• We introduce simpliﬁed gradient penalties and prove local convergence for the regularized GAN training dynamics
All proofs can be found in the supplementary material.
1Nagarajan & Kolter (2017) also proved local convergence for a slightly more general family of probability distributions where the support of the generator distribution is equal to the support of the true data distribution near the equilibrium point. Alternatively, they showed that their results also hold when the discriminator satisﬁes certain (strong) smoothness conditions. However, these conditions are usually hard to satisfy in practice without prior knowledge about the support of the true data distribution.

2. Instabilities in GAN training
2.1. Background
GANs are deﬁned by a min-max two-player game between a discriminative network Dψ(x) and generative network Gθ(z). While the discriminator tries to distinguish between real data point and data points produced by the generator, the generator tries to fool the discriminator. It can be shown (Goodfellow et al., 2014) that if both the generator and discriminator are powerful enough to approximate any realvalued function, the unique Nash-equilibrium of this two player game is given by a generator that produces the true data distribution and a discriminator which is 0 everywhere on the data distribution.
Following the notation of Nagarajan & Kolter (2017), the training objective for the two players can be described by an objective function of the form

L(θ, ψ) = Ep(z) [f (Dψ(Gθ(z)))] + EpD(x) [f (−Dψ(x))] (1)

for some real-valued function f . The common choice f (t) = − log(1 + exp(−t)) leads to the loss function considered in the original GAN paper (Goodfellow et al., 2014). For technical reasons we assume that f is continuously differentiable and satisﬁes f (t) = 0 for all t ∈ R.

The goal of the generator is to minimize this loss whereas the discriminator tries to maximize it. Our goal when training GANs is to ﬁnd a Nash-equilibrium, i.e. a parameter assignment (θ∗, ψ∗) where neither the discriminator nor the generator can improve their utilities unilaterally.

GANs are usually trained using Simultaneous or Alternating Gradient Descent (SimGD and AltGD). Both algorithms can be described as ﬁxed point algorithms (Mescheder et al., 2017) that apply some operator Fh(θ, ψ) to the parameter values (θ, ψ) of the generator and discriminator, respectively. For example, simultaneous gradient descent corresponds to the operator Fh(θ, ψ) = (θ, ψ) + h v(θ, ψ), where v(θ, ψ) denotes the gradient vector ﬁeld

v(θ, ψ) := −∇θL(θ, ψ) .

(2)

∇ψL(θ, ψ)

Similarly, alternating gradient descent can be described by an operator Fh = F2,h ◦ F1,h where F1,h and F2,h perform an update for the generator and discriminator, respectively.

Recently, it was shown (Mescheder et al., 2017) that local
convergence of GAN training near an equilibrium point (θ∗, ψ∗) can be analyzed by looking at the spectrum of the Jacobian Fh(θ∗, ψ∗) at the equilibrium: if Fh(θ∗, ψ∗) has eigenvalues with absolute value bigger than 1 , the training algorithm will generally not converge to (θ∗, ψ∗). On the
other hand, if all eigenvalues have absolute value smaller

Which Training Methods for GANs do actually Converge?

y pD = δ0

pθ = δθ Dψ (x) x

y

pD = δ0

pθ = δθ

Dψ (x)

x

(a) t = t0

(b) t = t1

Figure 1. Visualization of the counterexample showing that gradient descent based GAN optimization is not always convergent: (a) In the beginning, the discriminator pushes the generator towards the true data distribution and the discriminator’s slope increases. (b) When the generator reaches the target distribution, the slope of the discriminator is largest, pushing the generator away from the target distribution. This results in oscillatory training dynamics that never converge.
than 1, the training algorithm will converge to (θ∗, ψ∗) with linear rate O(|λmax|k) where λmax is the eigenvalue of F (θ∗, ψ∗) with the biggest absolute value. If all eigenvalues of F (θ∗, ψ∗) are on the unit circle, the algorithm can be convergent, divergent or neither, but if it is convergent it will generally converge with a sublinear rate. A similar result (Khalil, 1996; Nagarajan & Kolter, 2017) also holds for the (idealized) continuous system

θ˙(t)

−∇ψL(θ, ψ)

ψ˙(t) = ∇θL(θ, ψ)

(3)

which corresponds to training the GAN with inﬁnitely small learning rate: if all eigenvalues of the Jacobian v (θ∗, ψ∗) at a stationary point (θ∗, ψ∗) have negative real-part, the continuous system converges locally to (θ∗, ψ∗) with linear convergence rate. On the other hand, if v (θ∗, ψ∗) has eigenvalues with positive real-part, the continuous system is not locally convergent. If all eigenvalues have zero realpart, it can be convergent, divergent or neither, but if it is convergent, it will generally converge with a sublinear rate.
For simultaneous gradient descent linear convergence can be achieved if and only if all eigenvalues of the Jacobian of the gradient vector ﬁeld v(θ, ψ) have negative real part (Mescheder et al., 2017). This situation was also considered by Nagarajan & Kolter (2017) who examined the asymptotic case of step sizes h that go to 0 and proved local convergence for absolutely continuous generator and data distributions under certain regularity assumptions.

2.2. The Dirac-GAN
Simple experiments, simple theorems are the building blocks that help us understand more complicated systems.
Ali Rahimi - Test of Time Award speech, NIPS 2017

In this section, we describe a simple yet prototypical counterexample which shows that in the general case unregularized GAN training is neither locally nor globally convergent.

Deﬁnition 2.1. The Dirac-GAN consists of a (univariate) generator distribution pθ = δθ and a linear discriminator Dψ(x) = ψ · x. The true data distribution pD is given by a Dirac-distribution concentrated at 0.

Note that for the Dirac-GAN, both the generator and the discriminator have exactly one parameter. This situation is visualized in Figure 1. In this setup, the GAN training objective (1) is given by

L(θ, ψ) = f (ψθ) + f (0)

(4)

While using linear discriminators might appear restrictive, the class of linear discriminators is in fact as powerful as the class of all real-valued functions for this example: when we use f (t) = − log(1 + exp(−t)) and we take the supremum over ψ in (4), we obtain (up to scalar and additive constants) the Jensen-Shannon divergence between pθ and pD. The same holds true for the Wasserstein-divergence, when we use f (t) = t and put a Lipschitz constraint on the discriminator (see Section 3.1).
We show that the training dynamics of GANs do not converge in this simple setup.
Lemma 2.2. The unique equilibrium point of the training objective in (4) is given by θ = ψ = 0. Moreover, the Jacobian of the gradient vector ﬁeld at the equilibrium point has the two eigenvalues ±f (0) i which are both on the imaginary axis.

We now take a closer look at the training dynamics produced by various algorithms for training the Dirac-GAN. First, we consider the (idealized) continuous system in (3): while Lemma 2.2 shows that the continuous system is generally not linearly convergent to the equilibrium point, it could in principle converge with a sublinear convergence rate. However, this is not the case as the next lemma shows:
Lemma 2.3. The integral curves of the gradient vector ﬁeld v(θ, ψ) do not converge to the Nash-equilibrium. More speciﬁcally, every integral curve (θ(t), ψ(t)) of the gradient vector ﬁeld v(θ, ψ) satisﬁes θ(t)2 + ψ(t)2 = const for all t ∈ [0, ∞).

Note that our results do not contradict the results of Nagarajan & Kolter (2017) and Heusel et al. (2017): our example violates Assumption IV in Nagarajan & Kolter (2017) that the support of the generator distribution is equal to the support of the true data distribution near the equilibrium. It also violates the assumption2 in Heusel et al. (2017) that the optimal discriminator parameter vector is a continuous function of the current generator parameters. In fact, unless
2This assumption is usually even violated by WassersteinGANs, as the optimal discriminator parameter vector as a function of the current generator parameters can have discontinuities near the Nash-equilibrium. See Section 3.1 for details.

Which Training Methods for GANs do actually Converge?

(a) SimGD

(b) AltGD

Figure 2. Training behavior of the Dirac-GAN. The starting iterate is marked in red.
θ = 0, there is not even an optimal discriminator parameter for the Dirac-GAN. Indeed, we found that two-time scale updates as suggested by Heusel et al. (2017) do not help convergence towards the Nash-equilibrium (see Figure 22 in the supplementary material). However, our example seems to be a prototypical situation for (unregularized) GAN training which usually deals with distributions that are concentrated on lower dimensional manifolds (Arjovsky & Bottou, 2017).
We now take a closer look at the discretized system.
Lemma 2.4. For simultaneous gradient descent, the Jacobian of the update operator Fh(θ, ψ) has eigenvalues λ1/2 = 1 ± hf (0)i with absolute values 1 + h2f (0)2 at the Nash-equilibrium. Independently of the learning rate, simultaneous gradient descent is therefore not stable near the equilibrium. Even stronger, for every initial condition and learning rate h > 0, the norm of the iterates (θk, ψk) obtained by simultaneous gradient descent is monotonically increasing.

The behavior of simultaneous gradient descent for our example problem is visualized in Figure 2a.
Similarly, for alternating gradient descent we have
Lemma 2.5. For alternating gradient descent with ng generator and nd discriminator updates, the Jacobian of the update operator Fh(θ, ψ) has eigenvalues

α2

α2 2

λ1/2 = 1 − 2 ± 1 − 2 − 1. (5)

with α := √ngndhf (0). For α ≤ 2, all eigenvalues are hence on the unit circle. Moreover for α > 2, there are eigenvalues outside the unit circle.

Even though Lemma 2.5 shows that alternating gradient descent does not converge linearly to the Nash-equilibrium, it could in principle converge with a sublinear convergence rate. However, this is very unlikely because – as Lemma 2.3 shows – even the continuous system does not converge. Indeed, we empirically found that alternating gradient descent oscillates in stable cycles around the equilibrium and shows no sign of convergence (Figure 2b).

2.3. Where do instabilities come from?
Our simple example shows that naive gradient based GAN optimization does not always converge to the equilibrium point. To get a better understanding of what can go wrong for more complicated GANs, it is instructive to analyze these instabilities in depth for this simple example problem.
To understand the instabilities, we have to take a closer look at the oscillatory behavior that GANs exhibit both for the Dirac-GAN and for more complex systems. An intuitive explanation for the oscillations is given in Figure 1: when the generator is far from the true data distribution, the discriminator pushes the generator towards the true data distribution. At the same time, the discriminator becomes more certain, which increases the discriminator’s slope (Figure 1a). Now, when the generator reaches the target distribution (Figure 1b), the slope of the discriminator is largest, pushing the generator away from the target distribution. As a result, the generator moves away again from the true data distribution and the discriminator has to change its slope from positive to negative. After a while, we end up with a similar situation as in the beginning of training, only on the other side of the true data distribution. This process repeats indeﬁnitely and does not converge.
Another way to look at this is to consider the local behavior of the training algorithm near the Nash-equilibrium. Indeed, near the Nash-equilibrium, there is nothing that pushes the discriminator towards having zero slope on the true data distribution. Even if the generator is initialized exactly on the target distribution, there is no incentive for the discriminator to move to the equilibrium discriminator. As a result, training is unstable near the equilibrium point.
This phenomenon of discriminator gradients orthogonal to the data distribution can also arise for more complex examples: as long as the data distribution is concentrated on a low dimensional manifold and the class of discriminators is big enough, there is no incentive for the discriminator to produce zero gradients orthogonal to the tangent space of the data manifold and hence converge to the equilibrium discriminator. Even if the generator produces exactly the true data distribution, there is no incentive for the discriminator to produce zero gradients orthogonal to the tangent space. When this happens, the discriminator does not provide useful gradients for the generator orthogonal to the data distribution and the generator does not converge.
Note that these instabilities can only arise if the true data distribution is concentrated on a lower dimensional manifold. Indeed, Nagarajan & Kolter (2017) showed that under some suitable assumptions - gradient descent based GAN optimization is locally convergent for absolutely continuous distributions. Unfortunately, this assumption may not be satisﬁed for data distributions like natural images to

Which Training Methods for GANs do actually Converge?

posed in the original GAN paper (Goodfellow et al., 2014) leads to convergence of the continuous system, albeit with an extremely slow convergence rate. A more detailed discussion and an analysis of consensus optimization (Mescheder et al., 2017) can be found in the supplementary material.

(a) Standard GAN

(b) Non-saturating GAN

(c) WGAN (nd = 5)

(d) WGAN-GP (nd = 5)

(e) Consensus optimization

(f) Instance noise

(g) Gradient penalty

(h) Gradient penalty (CR)

Figure 3. Convergence properties of different GAN training algorithms using alternating gradient descent with recommended number of discriminator updates per generator update (nd = 1 if not noted otherwise). The shaded area in Figure 3c visualizes the set of forbidden values for the discriminator parameter ψ. The starting iterate is marked in red.

which GANs are commonly applied (Arjovsky & Bottou, 2017). Moreover, even if the data distribution is absolutely continuous but concentrated along some lower dimensional manifold, the eigenvalues of the Jacobian of the gradient vector ﬁeld will be very close to the imaginary axis, resulting in a highly ill-conditioned problem. This was observed by Mescheder et al. (2017) who examined the spectrum of the Jacobian for a data distribution given by a circular mixture of Gaussians with small variance.

3. Regularization strategies
As we have seen in Section 2, unregularized GAN training does not always converge to the Nash-equilibrium. In this section, we discuss how several regularization techniques that have recently been proposed, inﬂuence convergence of the Dirac-GAN.
Interestingly, we also ﬁnd that the non-saturating loss pro-

3.1. Wasserstein GAN
The two-player GAN game can be interpreted as minimizing a probabilistic divergence between the true data distribution and the distribution produced by the generator (Nowozin et al., 2016; Goodfellow et al., 2014). This divergence is obtained by considering the best-response strategy for the discriminator, resulting in an objective function that only contains the generator parameters. Many recent regularization techniques for GANs are based on the observation (Arjovsky & Bottou, 2017) that this divergence may be discontinuous with respect to the parameters of the generator or may even take on inﬁnite values if the support of the data distribution and the generator distribution do not match.
To make the divergence continuous with respect to the parameters of the generator, Wasserstein GANs (WGANs) Arjovsky et al. (2017) replace the Jensen-Shannon divergence used in the original derivation of GANs (Goodfellow et al., 2014) with the Wasserstein-divergence. As a result, Arjovsky et al. (2017) propose to use f (t) = t and restrict the class of discriminators to Lipschitz continuous functions with Lipschitz constant equal to some g0 > 0. While a WGAN converges if the discriminator is always trained until convergence, in practice WGANs are usually trained by running only a ﬁxed ﬁnite number of discriminator updates per generator update. However, near the Nash-equilibrium the optimal discriminator parameters can have a discontinuity as a function of the current generator parameters: for the Dirac-GAN, the optimal discriminator has to move from ψ = −1 to ψ = 1 when θ changes signs. As the gradients get smaller near the equilibrium point, the gradient updates do not lead to convergence for the discriminator. Overall, the training dynamics are again determined by the Jacobian of the gradient vector ﬁeld near the Nash-equilibrium:
Lemma 3.1. A WGAN trained with simultaneous or alternating gradient descent with a ﬁxed number of discriminator updates per generator update and a ﬁxed learning rate h > 0 does generally not converge to the Nash equilibrium for the Dirac-GAN.
The training behavior of the WGAN is visualized in Figure 3c. We stress that this analysis only holds if the discriminator is trained with a ﬁxed number of discriminator updates (as it is usually done in practice). More careful training that ensures that the discriminator is kept exactly optimal or two-timescale training (Heusel et al., 2017) might be able to ensure convergence for WGANs.

Which Training Methods for GANs do actually Converge?

y Dψ (x)
x

(a) Example with instance noise

(b) Eigenvalues

Figure 4. Dirac-GAN with instance noise. While unregularized GAN training is inherently unstable, instance noise can stabilize it: (a) Near the Nash-equilibrium, the discriminator is pushed towards the zero discriminator. (b) As we increase the noise level σ from 0 to σcritical, the real part of the eigenvalues at the equilibrium point becomes negative and the absolute value of the imaginary part becomes smaller. For noise levels bigger than σcritical all eigenvalues are real-valued and GAN training hence behaves like a normal optimization problem.
The convergence properties of WGANs were also considered by Nagarajan & Kolter (2017) who showed that even for absolutely continuous densities and inﬁnitesimal learning rates, WGANs are not always locally convergent.
We also found that WGAN-GP (Gulrajani et al., 2017) does not converge for the Dirac-GAN (Figure 3d). Please see the supplementary material for details.3

3.2. Instance noise
A common technique to stabilize GANs is to add instance noise (Sønderby et al., 2016; Arjovsky & Bottou, 2017), i.e. independent Gaussian noise, to the data points. While the original motivation was to make the probabilistic divergence between data and generator distribution well-deﬁned for distributions that do not have common support, this does not clarify the effects of instance noise on the training algorithm itself and its ability to ﬁnd a Nash-equilibrium. Interestingly, however, it was recently shown (Nagarajan & Kolter, 2017) that in the case of absolutely continuous distributions, gradient descent based GAN optimization is - under suitable assumptions - locally convergent.
Indeed, for the Dirac-GAN we have:
Lemma 3.2. When using Gaussian instance noise with standard deviation σ, the eigenvalues of the Jacobian of the gradient vector ﬁeld are given by
λ1/2 = f (0)σ2 ± f (0)2σ4 − f (0)2. (6)
In particular, all eigenvalues of the Jacobian have negative real-part at the Nash-equilibrium if f (0) < 0 and σ > 0. Hence, simultaneous and alternating gradient descent are both locally convergent for small enough learning rates.
3Despite these negative results, WGAN-GP has been successfully applied in practice (Gulrajani et al., 2017; Karras et al., 2017) and we leave a theoretical analysis of these empirical results to future research.

Interestingly, Lemma 3.2 shows that there is a critical noise level given by σc2ritical = |f (0)|/|f (0)|. If the noise level is smaller than the critical noise level, the eigenvalues of the Jacobian have non-zero imaginary part which results in a rotational component in the gradient vector ﬁeld near the equilibrium point. If the noise level is larger than the critical noise level, all eigenvalues of the Jacobian become real-valued and the rotational component in the gradient vector ﬁeld disappears. The optimization problem is best behaved when we select σ = σcritical: in this case we can even achieve quadratic convergence for h = |f (0)|−1. The effect of instance noise on the eigenvalues is visualized in Figure 4b, which shows the traces of the two eigenvalues as we increase σ from 0 to 2σcritical.
Figure 3f shows the training behavior of the GAN with instance noise, showing that instance noise indeed creates a strong radial component in the gradient vector ﬁeld which makes the training algorithm converge.

3.3. Zero-centered gradient penalties

Motivated by the success of instance noise to make the f divergence between two distributions well-deﬁned, Roth et al. (2017) derived a local approximation to instance noise that results in a zero-centered4 gradient penalty for the discriminator.

For the Dirac-GAN, a penalty on the squared norm of the

gradients of the discriminator (no matter where) results in

the regularizer

R(ψ) = γ ψ2.

(7)

2

This regularizer does not include the weighting terms considered by Roth et al. (2017). However, the same analysis can also be applied to the regularizer with the additional weighting, yielding almost exactly the same results (see Section D.2 of the supplementary material).

Lemma 3.3. The eigenvalues of the Jacobian of the gradient vector ﬁeld for the gradient-regularized Dirac-GAN at the equilibrium point are given by

γ

γ2

λ1/2 = − ±

− f (0)2.

(8)

2

4

In particular, for γ > 0 all eigenvalues have negative real part. Hence, simultaneous and alternating gradient descent are both locally convergent for small enough learning rates.

Like for instance noise, there is a critical regularization parameter γcritical = 2|f (0)| that results in a locally rotation free vector ﬁeld. A visualization of the training behavior of the Dirac-GAN with gradient penalty is shown in Figure 3g. Figure 3h illustrates the training behavior of the GAN with
4In contrast to the gradient regularizers used in WGAN-GP and DRAGAN which are not zero-centered.

Which Training Methods for GANs do actually Converge?

gradient penalty and critical regularization (CR). In particular, we see that near the Nash-equilibrium the vector ﬁeld does not have a rotational component anymore and hence behaves like a normal optimization problem.

4. General convergence results
In Section 3 we analyzed the convergence properties of various regularization strategies for the Dirac-GAN. In this section, we consider general GANs. First, we introduce two simpliﬁed versions of the zero-centered gradient penalty proposed by Roth et al. (2017). We then show that these gradient penalties allow us to extend the convergence proof by Nagarajan & Kolter (2017) to the case where the generator and data distribution do not locally have the same support.5 As a result, our convergence proof for the regularized training dynamics also holds for the more realistic case where both the generator and data distributions may lie on lower dimensional manifolds.

4.1. Simpliﬁed gradient penalties
Our analysis suggests that the main effect of the zerocentered gradient penalties proposed by Roth et al. (2017) on local stability is to penalize the discriminator for deviating from the Nash-equilibrium. The simplest way to achieve this is to penalize the gradient on real data alone: when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.
This leads to the following regularization term:

γ R1(ψ) := Ep (x)

∇Dψ(x) 2 .

(9)

2D

Note that this regularizer is a simpliﬁed version of to the regularizer derived by Roth et al. (2017). However, our regularizer does not contain the additional weighting terms and penalizes the discriminator gradients only on the true data distribution.
We also consider a similar regularization term given by

γ R2(θ, ψ) := Ep (x)

∇Dψ(x) 2

(10)

2θ

where we penalize the discriminator gradients on the current generator distribution instead of the true data distribution.
Note that for the Dirac-GAN from Section 2, both regularizers reduce to the gradient penalty from Section 3.3 whose behavior is visualized in Figure 3g and Figure 3h.
5Assumption IV in Nagarajan & Kolter (2017)

4.2. Convergence
In this section we present convergence results for the regularized GAN-training dynamics for both regularization terms R1(ψ) and R2(ψ) under some suitable assumptions.6
Let (θ∗, ψ∗) denote an equilibrium point of the regularized training dynamics. In our convergence analysis, we consider the realizable case, i.e. we assume that there are generator parameters that make the generator produce the true data distribution:
Assumption I. We have pθ∗ = pD and Dψ∗ (x) = 0 in some local neighborhood of supp pD.

Like Nagarajan & Kolter (2017), we assume that f satisﬁes the following property:
Assumption II. We have f (0) = 0 and f (0) < 0.

An extension of our convergence proof to f (t) = t (as in WGANs) can be found in the supplementary material.
The convergence proof is complicated by the fact that for neural networks, there generally is not a single equilibrium point (θ∗, ψ∗), but a submanifold of equivalent equilibria corresponding to different parameterizations of the same function. We therefore deﬁne the reparameterization manifolds MG and MD. To this end, let
h(ψ) := EpD(x) |Dψ(x)|2 + ∇xDψ(x) 2 . (11)

The reparameterization manifolds are then deﬁned as

MG := {θ | pθ = pD} MD := {ψ | h(ψ) = 0}. (12)

To prove local convergence, we have to assume some regularity properties for MG and MD near the equilibrium point. To state these assumptions, we need

g(θ) := Epθ(x) [∇ψDψ(x)|ψ=ψ∗ ] .

(13)

Assumption III. There are -balls B (θ∗) and B (ψ∗) around θ∗ and ψ∗ so that MG ∩ B (θ∗) and MD ∩ B (ψ∗) deﬁne C1- manifolds. Moreover, the following holds:
(i) if v ∈ Rn is not in the tangent space of MD at ψ∗, then ∂v2h(ψ∗) = 0.
(ii) if w ∈ Rm is not in the tangent space of MG at θ∗, then ∂wg(θ∗) = 0.

While formally similar, the two conditions in Assumption III have very different meanings: the ﬁrst condition is a simple regularity property that means that the geometry of MD can be locally described by the second derivative of h. The second condition implies that the discriminator is strong
6Our results also hold for any convex combination of R1 and R2 and the regularizer with the additional weighting terms derived by Roth et al. (2017). See the supplementary material for details.

Which Training Methods for GANs do actually Converge?

enough so that it can detect any deviation from the equilibrium generator distribution. Indeed, this is the only point where we assume that the class of representable discriminators is sufﬁciently expressive (and excludes, for example, the trivial case Dψ = 0 for all ψ).

We are now ready to state our main convergence result. To this end, consider the regularized gradient vector ﬁeld

v˜i(θ, ψ) :=

−∇θL(θ, ψ)

. (14)

∇ψL(θ, ψ) − ∇ψRi(θ, ψ)

Theorem 4.1. Assume Assumption I, II and III hold for (θ∗, ψ∗). For small enough learning rates, simultaneous
and alternating gradient descent for v˜1 and v˜2 are both convergent to MG × MD in a neighborhood of (θ∗, ψ∗). Moreover, the rate of convergence is at least linear.

Theorem 4.1 shows that GAN training with our gradient penalties is convergent when initialized sufﬁciently close to the equilibrium point. While this does not show that the method is globally convergent, it at least shows that near the equilibrium the method is well-behaved.

4.3. Stable equilibria for unregularized GAN training
As we have seen in Section 2, unregularized GAN training does not always converge to the Nash-equilibrium. However, this does not rule out the existence of stable equilibria for every GAN architecture. In Section E of the supplementary material, we identify two forms of stable equilibria that may exist for unregularized GAN training (energy solutions and full-rank solutions). However, it is not yet clear under what conditions such solutions exist for high dimensional data distributions.

5. Experiments
2D-Problems Measuring convergence for GANs is hard for high dimensional problems, because we lack a metric that can reliably detect non-convergent behavior. We therefore ﬁrst examine the behavior of the different regularizers on simple 2D examples where we can assess convergence using an estimate of the Wasserstein-1-distance.
To this end, we run 5 different training algorithms on 4 different 2D-examples for 6 different GAN architectures. For each method, we try both stochastic gradient descent and RMS-Prop with 4 different learning rates. For the R1-, R2and WGAN-GP-regularizers we try 3 different regularization parameters. We train all methods for 50k iterations and report the results for the best hyperparameter setup. Please see the supplementary material for details.
The results are shown in Figure 5. We see that the R1- and R2-regularizers perform similarly and they achieve slightly better results than unregularized training or training with

(a) 2D Gaussian

(b) Line segment

(c) Circle

(d) Four line segments

Figure 5. Wasserstein-1-distance to true data distribution for 4 different 2D-data-distributions, 6 different architectures (small bars) and 5 different training methods. Here, we abbreviate WGANGP with 1 and 5 discriminator update(s) per generator update as WGP-1 and WGP-5.

WGAN-GP. In the supplementary material we show that the R1- and R2-regularizers ﬁnd solutions where the discriminator is 0 in a neighborhood of the true data distribution, whereas unregularized training and WGAN-GP converge to energy solutions.

Images To test how well the gradient penalties from Section 4.1 perform on more complicated tasks, we train convolutional GANs on a variety of datasets, including a generative model for all 1000 Imagenet classes and a generative model for the celebA-HQ dataset (Karras et al., 2017) at resolution 1024×1024. While we ﬁnd that unregularized GAN training quickly leads to mode-collapse for these problems, our simple R1-regularizer enables stable training. Random samples from the models and more details on the experimental setup can be found in the supplementary material.

6. Conclusion
In this paper, we analyzed the stability of GAN training on a simple yet prototypical example. Due to the simplicity of the example, we were able to analyze the convergence properties of the training dynamics analytically and we showed that (unregularized) gradient based GAN optimization is not always locally convergent. Our ﬁndings also show that WGANs and WGAN-GP do not always lead to local convergence whereas instance noise and zero-centered gradient penalties do. Based on our analysis, we extended our results to more general GANs and we proved local convergence for simpliﬁed zero-centered gradient penalties under suitable assumptions. In the future, we would like to extend our theory to the non-realizable case and examine the effect of ﬁnite sampling sizes on the GAN training dynamics.

Which Training Methods for GANs do actually Converge?

Acknowledgements
We would like to thank Vaishnavh Nagarajan and Kevin Roth for insightful discussions. We also thank Vaishnavh Nagarajan for giving helpful feedback on an early draft of this manuscript. We thank NVIDIA for donating the GPUs for the experiments presented in the supplementary material. This work was supported by Microsoft Research through its PhD Scholarship Programme.
References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P. A., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., and Zheng, X. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016., pp. 265–283, 2016.
Arjovsky, M. and Bottou, L. Towards principled methods for training generative adversarial networks. CoRR, abs/1701.04862, 2017.
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein GAN. CoRR, abs/1701.07875, 2017.
Barratt, S. and Sharma, R. A note on the inception score. CoRR, abs/1801.01973, 2018.
Berthelot, D., Schumm, T., and Metz, L. BEGAN: boundary equilibrium generative adversarial networks. CoRR, abs/1703.10717, 2017.
Bertsekas, D. P. Nonlinear programming. Athena scientiﬁc Belmont, 1999.
Gidel, G., Berard, H., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial nets. CoRR, abs/1802.10551, 2018.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680, 2014.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 6629–6640, 2017.
Hjelm, R. D., Jacob, A. P., Che, T., Cho, K., and Bengio, Y. Boundary-seeking generative adversarial networks. CoRR, abs/1702.08431, 2017.
Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.
Khalil, H. K. Nonlinear systems. Prentice-Hall, New Jersey, 2(5):5–1, 1996.
Kodali, N., Abernethy, J. D., Hays, J., and Kira, Z. How to train your DRAGAN. CoRR, abs/1705.07215, 2017.
Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.
Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Mescheder, L. M., Nowozin, S., and Geiger, A. The numerics of gans. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 1823–1833, 2017.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018.
Nagarajan, V. and Kolter, J. Z. Gradient descent GAN optimization is locally stable. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5591–5600, 2017.

Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5769–5779, 2017.

Nowozin, S., Cseke, B., and Tomioka, R. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 271–279, 2016.

Which Training Methods for GANs do actually Converge?
Odena, A., Olah, C., and Shlens, J. Conditional image synthesis with auxiliary classiﬁer gans. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2642–2651, 2017.
Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015.
Roth, K., Lucchi, A., Nowozin, S., and Hofmann, T. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 2015–2025, 2017.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.
Salimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2226–2234, 2016.
Sønderby, C. K., Caballero, J., Theis, L., Shi, W., and Huszár, F. Amortised MAP inference for image superresolution. CoRR, abs/1610.04490, 2016.
Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude, 2012.
Yazici, Y., Foo, C. S., Winkler, S., Yap, K., Piliouras, G., and Chandrasekhar, V. The unusual effectiveness of averaging in GAN training. CoRR, abs/1806.04498, 2018.
Yu, F., Zhang, Y., Song, S., Seff, A., and Xiao, J. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015.
Zhao, J. J., Mathieu, M., and LeCun, Y. Energy-based generative adversarial network. CoRR, abs/1609.03126, 2016.

Which Training Methods for GANs do actually Converge? Supplementary Material

A. Preliminaries
In this section we ﬁrst summarize some results from the theory of discrete dynamical systems. We also prove a discrete version of a basic convergence theorem for continuous dynamical systems from Nagarajan & Kolter (2017) which allows us to make statements about training algorithms for GANs for ﬁnite learning rates. Afterwards, we summarize some results from Mescheder et al. (2017) about the convergence properties of simultaneous and alternating gradient descent. Moreover, we state some eigenvalue bounds that were derived by Nagarajan & Kolter (2017) which we need to prove Theorem 4.1 on the convergence of the regularized GAN training dynamics.

A.1. Discrete dynamical systems

In this section, we recall some basic deﬁnitions from the theory of discrete nonlinear dynamical systems. For a similar description of the theory of continuous nonlinear dynamical systems see for example Khalil (1996) and Nagarajan & Kolter (2017).

In this paper, we consider continuously differentiable operators F : Ω → Ω acting on an open set Ω ⊂ Rn. A ﬁxed point of F is a point x¯ ∈ Ω such that F (x¯) = x¯. We are interested in stability and convergence of the ﬁxed point iteration F (k)(x) near the ﬁxed point. To this end, we ﬁrst have to deﬁne what we mean by stability and local convergence:
Deﬁnition A.1. Let x¯ ∈ Ω be a ﬁxed point of a continuously differentiable operator F : Ω → Ω. We call x¯

• stable if for every > 0 there is δ > 0 such that x − x¯ < δ implies F (k)(x) − x¯ < for all k ∈ N.
• asymptotically stable if it is stable and there is δ > 0 such that x − x¯ < δ implies that F (k)(x) converges
to x¯
• exponentially stable if there is λ ∈ [0, 1), δ > 0 and C > 0 such that x − x¯ < δ implies

F (k)(x) − x¯ < C x − x¯ λk

(15)

for all k ∈ N.

If x¯ is asymptotically stable ﬁxed point of F , we call the algorithm obtained by iteratively applying F locally convergent to x¯. If x¯ is exponentially stable, we call the cor-

responding algorithm linearly convergent. Moreover, if x¯ is exponentially stable, we call the inﬁmum of all λ so that (15) holds for some C > 0 the convergence rate of the ﬁxed point iteration.
As it turns out, local convergence of ﬁxed point iterations can be analyzed by examining the spectrum of the Jacobian of the ﬁxed point operator. We have the following central Theorem:
Theorem A.2. Let F : Ω → Ω be a C1-mapping on an open subset Ω of Rn and x¯ ∈ Ω be a ﬁxed point of F . Assume that the absolute values of the eigenvalues of the Jacobian F (x¯) are all smaller than 1. Then the ﬁxed point iteration F (k)(x) is locally convergent to x¯. Moreover, the rate of convergence is at least linear with convergence rate |λmax| where λmax denotes the eigenvalue of F (x¯) with the largest absolute value.

Proof. See Bertsekas (1999), Proposition 4.4.1.

For the proof of Theorem 4.1 in Section D, we need a generalization of Theorem A.2 that takes into account submanifolds of ﬁxed points. The next theorem is a discrete version of Theorem A.4 from Nagarajan & Kolter (2017) and we prove it in a similar way:
Theorem A.3. Let F (α, γ) deﬁne a C1-mapping that maps some domain Ω to itself. Assume that there is a local neighborhood U of 0 such that F (0, γ) = (0, γ) for γ ∈ U . Moreover, assume that all eigenvalues of J := ∇αF (α, 0) |α=0 have absolute value smaller than 1. Then the ﬁxed point iteration deﬁned by F is locally convergent to M := {(0, γ) | γ ∈ U } with linear convergence rate in a neighborhood of (0, 0). Moreover, the convergence rate is |λmax| with λmax the eigenvalue of J with largest absolute value.

Proof. In the following, we write F (α, γ) = (F1(α, γ), F2(α, γ)), so that the ﬁxed point iteration can be written as

αk+1 = F1(αk, γk) γk+1 = F2(αk, γk).

(16)

We ﬁrst examine the behavior of F1 near (0, 0). To this end, we develop F1 into a Taylor-series

F1(α, γ) = Jα + g1(α, γ)

(17)

Which Training Methods for GANs do actually Converge?

We ﬁrst show that for any c > 0 we have g1(α, γ) ≤ c α sufﬁciently close to (0, 0): because F1(0, γ) = 0 for all γ close to 0, g1(α, γ) must be of the form g1(α, γ) = h1(α, γ)α with h1(0, 0) = 0. This shows that for any c > 0 there is indeed an open neighborhood V of (0, 0) so that |g1(α, γ)| ≤ c α for all (α, γ) ∈ V .

According to Bertsekas (1999), Proposition A 15, we can select for every > 0 a norm · Q on Rn such that

J α Q < (|λmax| + ) α Q

(18)

for α ∈ Rn where |λmax| denotes the eigenvalue of J with the largest absolute value.

Hence, for (α, γ) ∈ V ,

F1(α, γ) Q ≤ J α Q + g1(α, γ) Q < (|λmax| + + c) α Q (19)

Because we can make c + as small as we want, this shows that αk ≤ Cλk α0 for some C > 0 and λ ∈ [0, 1), if α0 and all γl for l = 0, . . . , k − 1 are sufﬁciently close to 0 . We therefore have to show that the iterates γk stay in a given local neighborhood of 0, i.e. γk < δ for some δ > 0, when α0 and γ0 are initialized sufﬁciently close to 0.
To show this, we develop F2 into a Taylor-series around 0:

F2(α, γ) = γ + g2(α, γ).

(20)

Again, we see that g2 must be of the form g2(α, γ) = h2(α, γ)α, showing that g2(α, γ) ≤ c α Q for some ﬁxed constant c > 0 (note that in general h2(0, 0) = 0). We therefore have

k−1

k−1

γk − γ0 ≤

g2(αl, γl) ≤ c αl Q

l=0

l=0

k−1

Cc

≤ Cc λl α0 Q ≤

α0 Q (21)

1−λ

l=0

Hence, if we initialize α0 within α0 Q < 21C−Cλ δ and γ0 within γ0 < 2δ , we have γk < δ for all k ∈ N, concluding the proof.

A.2. Simultaneous and Alternating Gradient Descent

In this section, we recall some results from Mescheder et al. (2017) about the convergence properties of simultaneous and alternating gradient descent as algorithms for training generative adversarial networks.

Recall that simultaneous gradient descent can be described by an update operator of the form

Fh(θ, ψ) = θ − h∇θL(θ, ψ)

(22)

ψ + h∇ψL(θ, ψ)

where L(θ, ψ) is the GAN training objective deﬁned in (1).

Similarly, alternating gradient descent can be described by an update operator of the form Fh = F2,h ◦ F1,h where F1,h and F2,h are given by

F1,h(θ, ψ) = θ − h∇θL(θ, ψ)

(23)

ψ

θ

F2,h(θ, ψ) = ψ + h∇ψL(θ, ψ) .

(24)

Moreover, we deﬁned the gradient vector ﬁeld

v(θ, ψ) = −∇θL(θ, ψ) .

(25)

∇ψL(θ, ψ)

To understand convergence of simultaneous and alternating gradient descent, we have to understand when the Jacobian of the corresponding update operator has only eigenvalues with absolute value smaller than 1.
Lemma A.4. The eigenvalues of the Jacobian of the update operator for simultaneous gradient descent are given by λ = 1 + hµ with µ the eigenvalues of v (θ∗, ψ∗). Assume that v (θ∗, ψ∗) has only eigenvalues with negative real part. The eigenvalues of the Jacobian of the update operator Fh for simultaneous gradient descent are then all in the unit circle if and only if

1

2

h< | Re(λ)|

Im(λ) 2

(26)

1 + Re(λ)

for all eigenvalues λ of v (θ∗, ψ∗).

Proof. For simultaneous gradient descent we have

Fh(θ, ψ) = (θ, ψ) + hv(θ, ψ)

(27)

and hence Fh(θ∗, ψ∗) = I + hv (θ∗, ψ∗). Therefore the eigenvalues are given by λ = 1 + hµ with µ the eigenvalues of v (θ∗, ψ∗).

To see when |λ| < 1, we write µ = −a + ib with a, b ∈ R and a > 0. Then

|λ|2 = (1 − ha)2 + h2b2

(28)

which is smaller than 1 if and only if

2a

h < a2 + b2 .

(29)

Dividing both the numerator and denominator by a2 shows the assertion.

Lemma A.5. Assume that v (θ∗, ψ∗) has only eigenvalues with negative real part. For h > 0 small enough, the eigenvalues of the Jacobian of the update operator Fh for alternating gradient descent are then all in the unit circle.

Which Training Methods for GANs do actually Converge?

Proof. The Jacobian of the update operator Fh = Fh,2◦Fh,1 at an equilibrium point (θ∗, ψ∗) is

Fh(θ∗, ψ∗) = Fh,2(θ∗, ψ∗) · Fh,1(θ∗, ψ∗).

(30)

However, we have

Fh,i(θ∗, ψ∗) = I + hvi(θ∗, ψ∗)

(31)

for i ∈ {1, 2} where

v1(θ, ψ) = −∇θL(θ, ψ)

(32)

0

0

v2(θ, ψ) = ∇ψL(θ, ψ)

(33)

denote the components of the gradient vector ﬁeld. Hence

Fh(θ∗, ψ∗) = I + h(v1(θ∗, ψ∗) + v2(θ∗, ψ∗)) + h2v2(θ∗, ψ∗)v1(θ∗, ψ∗) = I + h(v (θ∗, ψ∗) + h R(θ∗, ψ∗)). (34)
with R(θ∗, ψ∗) := v2(θ∗, ψ∗)v1(θ∗, ψ∗). For h > 0 small enough, all eigenvalues of v (θ∗, ψ∗) + h R(θ∗, ψ∗) will be arbitrarily close to the eigenvalues of v (θ∗, ψ∗). Because all eigenvalues of v (θ∗, ψ∗) have negative real-part, all eigenvalues of Fh(θ∗, ψ∗) will hence lie inside the unit circle for h > 0 small enough.

In the proof of Theorem 4.1 we will use local coordinates, i.e. a diffeomorphism φ that maps a local neighborhood of (θ∗, ψ∗) to an open subset of Rn+m. The vector ﬁeld v and the update operator Fh then have the following representation in the local coordinates:

Fhφ(α) := φ ◦ Fh ◦ φ−1(α)

(35)

vφ(α) = φ (θ, ψ) · (v ◦ φ−1(α))

(36)

While in local coordinates, the simple relationships between Fhφ(α) and vφ(α) needed to prove Lemma A.4 and Lemma A.5 do not hold anymore, the spectrum can be de-
scribed in the same way:
Remark A.6. Assume (θ∗, ψ∗) is a ﬁxed point of Fh and a stationary point of v. Let α∗ = φ(θ∗, ψ∗). Then

(Fhφ) (α∗) = φ (θ∗, ψ∗)Fh(θ∗, ψ∗)φ (θ∗, ψ∗)−1 (37) (vφ) (α∗) = φ (θ∗, ψ∗)v (θ∗, ψ∗)φ (θ∗, ψ∗)−1 (38)

Hence, (Fhφ) (α∗) and Fh(θ∗, ψ∗) have the same spectrum. The same also holds for (vφ) (α∗) and v (θ∗, ψ∗).

Proof. This follows from the chain and product rules by using the fact that Fh(θ∗, ψ∗) = (θ∗, ψ∗) and v(θ∗, ψ∗) = 0.

As we will see in the proof of Theorem 4.1, Remark A.6 allows us to apply Theorem A.3 to situations where the stationary points lie on a lower dimensional manifold instead of a space of the form {0}k × Rn+m−k.

A.3. Eigenvalue bounds

When analyzing the convergence properties of GANs, we

have to analyze the spectrum of real-valued matrices of the

from

0 −BT

B −Q

(39)

with Q symmetric positive deﬁnite. To this end, we need the following important theorem from Nagarajan & Kolter (2017) which gives explicit bounds on the real part of the eigenvalues:

Theorem A.7. Assume J ∈ R(n+m)×(n+m) is of the fol-

lowing form:

0 −BT

J = B −Q

(40)

where Q ∈ Rm×m is a symmetric positive deﬁnite matrix and B ∈ Rm×n has full column rank. Then all eigenvalues
λ of J satisfy Re(λ) < 0. More precisely

• if Im(λ) = 0

Re(λ) ≤ − λmin(Q)λmin(BTB) (41) λmax(Q)λmin(Q) + λmin(BTB)

• if Im(λ) = 0

Re(λ) ≤ − λmin(Q) (42) 2

Proof. See Nagarajan & Kolter (2017), Lemma G.2.

In Section E.1, we need a generalization of Theorem A.7. Using almost exactly the same proof as for Theorem A.7, we obtain

Theorem A.8. Assume J ∈ R(n+m)×(n+m) is of the fol-

lowing form:

−P −BT

J = B −Q

(43)

where P ∈ Rn×n is a symmetric positive semi-deﬁnite matrix, Q ∈ Rm×m is a symmetric positive deﬁnite matrix and B ∈ Rm×n has full column rank. Then all eigenvalues
λ of J satisfy Re(λ) < 0.

Proof. Let vT = (aT, bT) denote some eigenvector of J with corresponding eigenvalues λ = λr + iλi, where λr, λi ∈ R. Then
λr = 1 v¯T(J + J T)v = −a¯TP a − ¯bTQb. (44) 2

Which Training Methods for GANs do actually Converge?

Because both P and Q are positive semi-deﬁnite, we have λr ≤ 0. Because Q is positive deﬁnite, it sufﬁces to show that b = 0 to prove λr < 0.
Assume that b = 0. Because v is an eigenvector of J, we have Ba − Qb = λb and therefore Ba = 0. Because B has full-column rank, this shows a = 0 and hence v = 0. However, this contradicts the fact that v is an eigenvector of J. All in all, this show that b = 0 and thus λr ≤ −¯bTQb < 0 as required.

For applying Theorems A.2, we have to show that the Jacobian of the update operator Fh only has eigenvalues with absolute value smaller than 1. For simultaneous and alternating gradient descent this can be achieved (Lemma A.4 and A.5), if the Jacobian of the gradient vector ﬁeld v only has eigenvalues with negative real-part. While this condition sufﬁces to prove convergence for small learning rates, Mescheder et al. (2017) showed that simultaneous and alternating gradient descent might still require intractably small learning rates if the imaginary part of the eigenvalues is large. However, in our case we have the following simple bound on the imaginary part of the eigenvalues:
Lemma A.9. Let

−P −BT

J = B −Q

(45)

where P ∈ Rn×n and Q ∈ Rm×m are symmetric. All eigenvalues λ of J then satisfy

| Im(λ)| ≤ λmax(BT B).

(46)

Note that this bound is independent from P and Q.

Proof. Assume v, v = 1, is an eigenvector of J with eigenvalue λ. Then

Im(λ) = v¯TJav.

(47)

with Ja := 21i (J − JT). Hence, by the Cauchy-Schwarz inequality

| Im(λ)| ≤ v Jav = Jav .

(48)

But, if vT = (aT, bT), Jav 2 = bBBTb + aBTBa ≤ λmax(BTB). (49)

B. Proofs for the Dirac-GAN
This section contains the proofs for our results from Section 2 and Section 3 on the properties of the Dirac-GAN.
Lemma 2.2. The unique equilibrium point of the training objective in (4) is given by θ = ψ = 0. Moreover, the Jacobian of the gradient vector ﬁeld at the equilibrium point has the two eigenvalues ±f (0) i which are both on the imaginary axis.

Proof. The loss in (4) can be rewritten as

L(θ, ψ) = f (θψ) + const

(51)

It is easy to check that the gradient vector ﬁeld is given by

−f (θψ)ψ

v(θ, ψ) = f (θψ)θ .

(52)

Because L(θ, 0) = L(0, ψ) = const for all θ, ψ ∈ R, (θ, ψ) = (0, 0) is indeed a Nash-equilibrium for the game deﬁned by (51). Because we assume f (t) = 0 for all t ∈ R, we have v(θ, ψ) = 0 if and only if (θ, ψ) = (0, 0), showing that (0, 0) is indeed the unique Nash-equilibrium.
Moreover, the Jacobian v (θ, ψ) of v is given by

−f (θψ)ψ2

−f (θψ) − f (θψ)θψ

f (θψ) + f (θψ)θψ

f (θψ)θ2

. (53)

Evaluating it at the Nash equilibrium θ = ψ = 0, we obtain

0 −f (0)

v (0, 0) = f (0) 0

(54)

which has the eigenvalues ±f (0)i.

Lemma 2.3. The integral curves of the gradient vector ﬁeld v(θ, ψ) do not converge to the Nash-equilibrium. More speciﬁcally, every integral curve (θ(t), ψ(t)) of the gradient vector ﬁeld v(θ, ψ) satisﬁes θ(t)2 + ψ(t)2 = const for all t ∈ [0, ∞).

Proof. Let R(θ, ψ) := 12 (θ2 + ψ2). Then

d R(θ(t), ψ(t))
dt = θ(t)v1(θ(t), ψ(t)) + ψ(t)v2(θ(t), ψ(t)) = 0, (55)
showing that R(θ, ψ) is indeed constant for all t ∈ [0, ∞).

This shows | Im(λ)| ≤ λmax(BT B).

Lemma 2.4. For simultaneous gradient descent, the Jacobian of the update operator Fh(θ, ψ) has eigenvalues (50) λ1/2 = 1 ± hf (0)i with absolute values 1 + h2f (0)2 at the Nash-equilibrium. Independently of the learning rate,
simultaneous gradient descent is therefore not stable near

Which Training Methods for GANs do actually Converge?

the equilibrium. Even stronger, for every initial condition and learning rate h > 0, the norm of the iterates (θk, ψk) obtained by simultaneous gradient descent is monotonically increasing.
Proof. The ﬁrst part is a direct consequence of Lemma A.4 and Lemma 2.2.
To see the the norms of the iterates (θk, ψk) is monotonically increasing, we calculate

with α = √ngndhf (0) which are on the unit circle if and only if α ≤ 2.
Lemma 3.1. A WGAN trained with simultaneous or alternating gradient descent with a ﬁxed number of discriminator updates per generator update and a ﬁxed learning rate h > 0 does generally not converge to the Nash equilibrium for the Dirac-GAN.

θk2+1 + ψk2+1 = (θk − hf (θkψk)ψk)2 + (ψk + hf (θkψk)θk)2 = θk2 + ψk2 + h2f (θkψk)2(θk2 + ψk2) ≥ θk2 + ψk2. (56)

Lemma 2.5. For alternating gradient descent with ng generator and nd discriminator updates, the Jacobian of the update operator Fh(θ, ψ) has eigenvalues

α2

α2 2

λ1/2 = 1 − 2 ± 1 − 2 − 1. (5)

with α := √ngndhf (0). For α ≤ 2, all eigenvalues are hence on the unit circle. Moreover for α > 2, there are eigenvalues outside the unit circle.

Proof. The update operators for alternating gradient descent are given by

θ − hf (θψ)ψ

F1(θ, ψ) =

ψ

(57)

θ

F2(θ, ψ) = ψ + hf (θψ)θ .

(58)

Hence, the Jacobians of these operators at 0 are given by

1 −hf (0)

F1(0, 0) = 0

1

(59)

10

F2(0, 0) = hf (0) 1 .

(60)

As a result, the Jacobian of the combined update operator is

(F2nd

◦

F

n 1

g

)

(0,

0)

=

F2(0,

0)nd

·

F1(0,

0)ng

=1

−nghf (0)

. (61)

ndhf (0) −ngndh2f (0)2 + 1

An easy calculation shows that the eigenvalues of this matrix

are

α2

α2 2

λ1/2 = 1 − 2 ± 1 − 2 − 1 (62)

Proof. First, consider simultaneous gradient descent. Assume that the iterates (θk, ψk) converge towards the equilibrium point (0, 0). Note that (θk+1, ψk+1) = 0 if (θk, ψk) = 0. We can therefore assume without loss of generality that (θk, ψk) = 0 for all k ∈ N.
Because limk→∞ ψk = 0, there exists k0 such that for all k ≥ k0 we have |ψk| < 1. For k ≥ k0 we therefore have

θk+1 = 1 −h θk .

(63)

ψk+1

h 1 ψk

For k ≥ k0, the iterates are therefore given by

θk = Ak−k0 θk0

ψk

ψk0

1 −h with A = h 1 . (64)

However, the eigenvalues of A are g√iven by λ1/2 = 1 ± hi which both have absolute value 1 + h2 > 1. This contradicts the assumption that (θk, ψk) converges to (0, 0).

A similar argument also hold for alternating gradient descent. In this case, A is given by

1 0 nd 1 −h ng

1

−hng

h1

01

= hnd 1 − h2ngnd . (65)

The eigenvalues of A as in (65) are given by

h2 ng nh

h2ngnh 2

1−

± 1−

− 1. (66)

2

2

At least one of these eigenvalues has absolute value greater or equal to 1. Note that for almost all initial conditions (θ0, ψ0), the the inner product between the eigenvector corresponding to the eigenvalue with modulus bigger than 1 will be nonzero for all k ∈ N. Since the recursion in (63) is linear, this contradicts the fact that (θk, ψk) → (0, 0), showing that alternating gradient descent generally does not converge to the Nash-equilibrium either.
Lemma 3.2. When using Gaussian instance noise with standard deviation σ, the eigenvalues of the Jacobian of the gradient vector ﬁeld are given by
λ1/2 = f (0)σ2 ± f (0)2σ4 − f (0)2. (6)

Which Training Methods for GANs do actually Converge?

In particular, all eigenvalues of the Jacobian have negative real-part at the Nash-equilibrium if f (0) < 0 and σ > 0. Hence, simultaneous and alternating gradient descent are both locally convergent for small enough learning rates.

Proof. When using instance noise, the GAN training objective (1) is given by

Eθ˜∼N (θ,σ2) f (θ˜ψ) + Ex∼N (0,σ2) [f (−xψ)] . (67)

The corresponding gradient vector ﬁeld is hence given by

−ψf (θ˜ψ)

v˜(θ, ψ) = Eθ˜,x θ˜f (θ˜ψ) − xf (−xψ) .

(68)

The Jacobian v˜ (θ, ψ) is therefore

−f (θ˜ψ)ψ2

−f (θ˜ψ) − f (θ˜ψ)θ˜ψ

Eθ˜,x f (θ˜ψ) + f (θ˜ψ)θ˜ψ f (θ˜ψ)θ˜2 + x2f (−xψ)

(69)

Evaluating it at θ = ψ = 0 yields

0 −f (0)

v˜ (0, 0) = f (0) 2f (0)σ2

(70)

whose eigenvalues are given by

λ1/2 = f (0)σ2 ± f (0)2σ4 − f (0)2.

(71)

Lemma 3.3. The eigenvalues of the Jacobian of the gradient vector ﬁeld for the gradient-regularized Dirac-GAN at the equilibrium point are given by

γ

γ2

λ1/2 = − ±

− f (0)2.

(8)

2

4

In particular, for γ > 0 all eigenvalues have negative real part. Hence, simultaneous and alternating gradient descent are both locally convergent for small enough learning rates.

Proof. The regularized gradient vector ﬁeld becomes

−f (θψ)ψ

v˜(θ, ψ) = f (θψ)θ − γψ .

(72)

The Jacobian v˜ (θ, ψ) is therefore given by

−f (θψ)ψ2

−f (θψ) − f (θψ)θψ

f (θψ) + f (θψ)θψ

f (θψ)θ2 − γ

. (73)

Evaluating it at θ = ψ = 0 yields

0 −f (0)

v˜ (0, 0) = f (0) −γ

(74)

whose eigenvalues are given by

γ

γ2

λ1/2 = − ±

− f (0)2.

(75)

2

4

C. Other regularization strategies
In this section we discuss further regularization techniques for GANs on our example problem that were omitted in the main text due to space constraints.

C.1. Nonsaturating GAN

Especially in the beginning of training, the discriminator can reject samples produced by the generator with high conﬁdence (Goodfellow et al., 2014). When this happens, the loss for the generator may saturate so that the generator receives almost no gradient information anymore.

To circumvent this problem Goodfellow et al. (2014) in-

troduced a nonsaturating objective for the generator. In

nonsaturating GANs, the generator objective is replaced

with7

max Epθ(x)f (−Dψ(x)).

(76)

θ

In our example, this is maxθ f (−ψθ).

While the nonsaturating generator objective was originally motivated by global stability considerations, we investigate its effect on local convergence. A linear analysis similar to normal GANs yields
Lemma C.1. The unique Nash-equilibrium for the nonsaturating GAN on the example problem is given by θ = ψ = 0. The eigenvalues of the Jacobian of the gradient vector ﬁeld at the equilibrium are ±f (0)i which are both on the imaginary axis.

Proof. The gradient vector ﬁeld for the nonsaturating GAN is given by

−f (−θψ)ψ

v(θ, ψ) = f (θψ)θ .

(77)

As in the proof of Lemma 2.2, we see that (ψ, θ) = (0, 0) deﬁnes the unique Nash-equilibrium for the nonsaturating GAN.

Moreover, the Jacobian v (θ, ψ) is

f (−θψ)ψ2

−f (−θψ) + f (−θψ)θψ

f (θψ) + f (θψ)θψ

f (θψ)θ2

.

(78)

At θ = ψ = 0 we therefore have

0 −f (0)

v (0, 0) = f (0) 0 .

(79)

with eigenvalues λ1/2 = ±f (0)i.

Lemma C.1 implies that simultaneous gradient descent is not locally convergent for a nonsaturating GAN and any
7Goodfellow et al. (2014) used f (t) = − log(1 + exp(−t)).

Which Training Methods for GANs do actually Converge?

learning rate h > 0, because the eigenvalues of the Jacobian of the corresponding update operator Fh all have absolute value larger than 1 (Lemma A.4). While Lemma C.1 also rules out linear convergence towards the Nash-equilibrium in the continuous case (i.e. for h → 0), the continuous training dynamics could in principle still converge with a sublinear convergence rate. Indeed, we ﬁnd this to be the case for the Dirac-GAN. We have
Lemma C.2. For every integral curve of the gradient vector ﬁeld of the nonsaturating Dirac-GAN we have
d (θ(t)2 + ψ(t)2) = 2 [f (θψ) − f (−θψ)] θψ. (80) dt
For concave f this is nonpositive. Moreover, for f (0) < 0, the continuous training dynamics of the nonsaturating Dirac-GAN converge with logarithmic convergence rate.

Proof. The gradient vector ﬁeld for the nonsaturating DiracGAN is given by

−f (−θψ)ψ

v(θ, ψ) = f (θψ)θ .

(81)

Hence, we have

d (θ(t)2 + ψ(t)2) = v1(θ, ψ)θ + v2(θ, ψ)ψ dt
= 2θψ [f (θψ) − f (−θψ)] . (82)

For concave f , we have

f (θψ) − f (−θψ)

≤0

(83)

2θψ

and hence

d (θ(t)2 + ψ(t)2) ≤ 0.

(84)

dt

Now assume that f (0) = 0 and f (0) < 0.
To intuitively understand why the continuous system converges with logarithmic convergence rate, note that near the equilibrium poin√t we asymp√totically have in polar coordinates (θ, ψ) = ( w cos(φ), w sin(φ)):

φ˙ = f (0) + O(|w|1/2)

(85)

w˙ = 4f (0)θ2ψ2 + O(|θψ|4)

(86)

= f (0)w2 sin2(2φ) + O(|w|4).

(87)

When we ignore higher order terms, we can solve this sys-

tem analytically8 for φ and w:

φ(t) = f (0)(t − t0)

(89)

2

w(t) =

(90)

−f

(0)t

+

f 4f

(0) (0)

sin(4f

(0)(t

−

t0))

+

c

The training dynamics are hence convergent with logarithmic convergence rate O √1 .
t

For a more formal proof, ﬁrst note that w is nonincreasing by the ﬁrst part of the proof. Moreover, for every > 0 there is δ > 0 such that for w < δ:

f (0) − ≤ φ˙ ≤ f (0) +

(91)

w˙ ≤ (f (0) sin2(2φ) + )w2.

(92)

This implies that for every time interval [0, T ], φ(t) is in

π π 3π π

+k , +k

(93)

8 28 2

k∈Z

for t in a union of intervals QT ⊆ [0, T ] with total length at least β αT with some constants α, β > 0 which are independent of T .
For these t ∈ QT we have sin2(2φ(t)) ≥ 12 . Because f (0) < 0, this shows

1 w˙ (t) ≤ f (0) +

w(t)2

(94)

2

for t ∈ QT and small enough. Solving the right hand formally yields

1

w(t) ≤ −( 1 f

(0) +

. )t + c

(95)

2

As w(t) is nonincreasing for t ∈/ QT and the total length of QT is at least β αT this shows that

1

w(T ) ≤ −( 1 f

(0) +

)β αT

. +c

(96)

2

The training dynamics hence converge with logarithmic convergence rate O √1 .
t

Note that the standard choice f (t) = − log(1 + exp(−t)) is concave and satisﬁes f (0) = − 14 < 0. Lemma C.1 is hence applicable and shows that the GAN training dynamics for the standard choice of f converge with logarithmic
convergence rate in the continuous case. The training be-
havior of the nonsaturating GAN on our example problem
is visualized in Figure 3b.

8For solving the ODE we use the separation of variablestechnique and the identity

2 sin2(ax)dx = x − sin(2ax) .

(88)

2a

Which Training Methods for GANs do actually Converge?

C.2. Wasserstein GAN-GP

In practice, it can be hard to enforce the Lipschitz-constraint for WGANs. A practical solution to this problem was given by Gulrajani et al. (2017), who derived a simple gradient penalty with a similar effect as the Lipschitz-constraint. The resulting training objective is commonly referred to as WGAN-GP.

Similarly to WGANs, we ﬁnd that WGAN-GP does not converge for the Dirac-GAN. A similar analysis also applies to the DRAGAN-regularizer proposed by Kodali et al. (2017).

The regularizer proposed by Gulrajani et al. (2017) is given by
R(ψ) = γ Exˆ ( ∇xDψ(xˆ) − g0)2 (97) 2
where xˆ is sampled uniformly on the line segment between two random points x1 ∼ pθ(x1), x2 ∼ pD(x2).

For the Dirac-GAN, it simpliﬁes to

R(ψ)

=

γ (|ψ|

−

g0)2

(98)

2

The corresponding gradient vector ﬁeld is given by
−ψ v˜(θ, ψ) = θ − sign(ψ)γ(|ψ| − g0) . (99)
Note that the gradient vector ﬁeld has a discontinuity at the equilibrium point, as the gradient vector ﬁeld takes on values with norm bigger than some ﬁxed constant in every neighborhood of the equilibrium point. As a result, we have Lemma C.3. WGAN-GP trained with simultaneous or alternating gradient descent with a ﬁxed number of generator and discriminator updates and a ﬁxed learning rate h > 0 does not converge locally to the Nash equilibrium for the Dirac-GAN.

Proof. First, consider simultaneous gradient descent. Assume that the iterates (θk, ψk) converge towards the equilibrium point (0, 0). For almost all initial conditions9 we have (θk, ψk) = (0, 0) for all k ∈ N. This implies
|ψk+1 − ψk| = h|θk − γψk − sign(ψk)g0| (100)
and hence limk→∞ |ψk+1 − ψk| = h|g0| = 0, showing that (θk, ψk) is not a Cauchy sequence. This contradicts the assumption that (θk, ψk) converges to the equilibrium point (0, 0).
A similar argument also holds for alternating gradient descent.

The training behavior of WGAN-GP on our example problem is visualized in Figure 3d.
9Depending on γ, h and g0 modulo a set of measure 0.

As for WGANs, we stress that this analysis only holds if the discriminator is trained with a ﬁxed number of discriminator updates per generator update. Again, more careful training that ensures that the discriminator is kept exactly optimal or two-timescale training (Heusel et al., 2017) might be able to ensure convergence for WGAN-GP.

C.3. Consensus optimization
Consensus optimization (Mescheder et al., 2017) is an algorithm that attempts to solve the problem of eigenvalues with zero real-part by introducing a regularization term that explicitly moves the eigenvalues to the left. The regularization term in consensus optimization is given by

γ R(θ, ψ) =

v(θ, ψ)

2

2

=

γ (

∇θL(θ, ψ)

2+

2

∇ψL(θ, ψ) 2).

(101)

As was proved by Mescheder et al. (2017), consensus op-
timization converges locally for small learning rates h > 0 provided that the Jacobian v (θ∗, ψ∗) is invertible.10

Indeed, for the Dirac-GAN we have
Lemma C.4. The eigenvalues of the Jacobian of the gradient vector ﬁeld for consensus optimization at the equilibrium point are given by

λ1/2 = −γf (0)2 ± if (0)

(102)

In particular, all eigenvalues have a negative real part −γf (0)2. Hence, simultaneous and alternating gradient descent are both locally convergent using consensus optimization for small enough learning rates.

Proof. As was shown by Mescheder et al. (2017), the Jacobian of the modiﬁed vector ﬁeld v˜ at the equilibrium point is

v˜ (0, 0) = v (0, 0) − γv (0, 0) v (0, 0).

(103)

In our case, this is
−γf (0)2 f (0)

−f (0) −γf (0)2.

(104)

A simple calculation shows that the eigenvalues of v˜ (0, 0) are given by

λ1/2 = −γf (0)2 ± if (0).

(105)

This concludes the proof.
10Mescheder et al. (2017) considered only the case of isolated equilibrium points. However, by applying Theorem A.3, it is straightforward to generalize their proof to the case where we are confronted with a submanifold of equivalent equilibrium points.

Which Training Methods for GANs do actually Converge?

A visualization of consensus optimization for the DiracGAN is given in Figure 3e.
Unfortunately, consensus optimization has the drawback that it can introduce new spurious points of attraction to the GAN training dynamics. While this is usually not a problem for simple examples, it can be a problem for more complex ones like deep neural networks.
A similar regularization term as in consensus optimization was also independently proposed by Nagarajan & Kolter (2017). However, Nagarajan & Kolter (2017) proposed to only regularize the component ∇ψL(θ, ψ) of the gradient vector ﬁeld corresponding to the discriminator parameters. Moreover, the regularization term is only added to the generator objective to give the generator more foresight. It can be shown (Nagarajan & Kolter, 2017) that this simpliﬁed regularization term can in certain situations also make the training dynamics locally convergent, but might be better behaved at stationary points of the GAN training dynamics that do not correspond to a local Nash-equilibrium. Indeed, a more detailed analysis shows that this simpliﬁed regularization term behaves similarly to instance noise and gradient penalties (which we discussed in Section 3.2 and Section 3.3) for the Dirac-GAN.

Similarly, the gradient of L(θ, ψ) with respect to ψ is given by

∇ψL(θ, ψ) = Epθ(x) [f (Dψ(x))∇ψDψ(x)] − EpD(x) [f (−Dψ(x))∇ψDψ(x)] .

(109)

Proof. This is just the chain rule.

Lemma D.2. Assume that (θ∗, ψ∗) satisﬁes Assumption I.

The Jacobian of the gradient vector ﬁeld v(θ, ψ) at (θ∗, ψ∗)

is then

v (θ∗, ψ∗) = 0 −KDT G . KDG KDD

(110)

The terms KDD and KDG are given by

KDD = 2f (0) EpD(x) [∇ψDψ∗ (x)∇ψDψ∗ (x)T] (111)

KDG = f (0)∇θ Epθ(x) [∇ψDψ∗ (x)] |θ=θ∗

(112)

Proof. First note that by the deﬁnition of v(θ, ψ) in (106), the Jacobian v (θ∗, ψ∗) of v(θ, ψ) is given by

−∇

2 θ

L(θ

∗

,

ψ

∗

)

−∇

2 θ

,ψ

L

(θ

∗

,

ψ

∗

)

.

∇

2 θ

,ψ

L(θ

∗

,

ψ

∗

)

∇

2 ψ

L

(θ

∗

,

ψ

∗

)

(113)

D. General convergence results
In this section, we prove Theorem 4.1. To this end, we extend the convergence proof by Nagarajan & Kolter (2017) to our setting. We show that by introducing the gradient penalty terms Ri(θ, ψ), we can get rid of the assumption that the generator and data distributions locally have the same support. As we have seen, this makes the theory applicable to more realistic cases, where both the generator and data distributions typically lie on lower dimensional manifolds.

D.1. Convergence proof

To prove Theorem 4.1, we ﬁrst need to understand the local structure of the gradient vector ﬁeld v(θ, ψ). Recall that the gradient vector ﬁeld v(θ, ψ) is deﬁned as

v(θ, ψ) := −∇θL(θ, ψ) ∇ψL(θ, ψ)

(106)

with

L(θ, ψ) = Ep(z) [f (Dψ(Gθ(z)))] + EpD(x) [f (−Dψ(x))] . (107)
Lemma D.1. The gradient of L(θ, ψ) with respect to θ is given by

∇θL(θ, ψ) = Ep(z) f (Dψ(Gθ(z)) [∇θGθ(z)]T · ∇xDψ(Gθ(z)) .

(108)

By Assumption I, Dψ∗ (x) = 0 in some neighborhood

of supp pD. Hence, we also have ∇xDψ∗ (x) = 0 and

∇

2 x

D

ψ

∗

(x

)

=

0 for x

∈

supp pD.

By taking the deriva-

tive of (108) with respect to θ and using ∇xDψ∗ (x) =

0

and

∇

2 x

Dψ

∗

(

x

)

=

0 for x

∈

supp pD

we see that

∇2θL(θ∗, ψ∗) = 0.

To show (111) and (112), simply take the derivative of
(109) with respect to θ and ψ and evaluate at it at (θ, ψ) = (θ∗, ψ∗).

We now take a closer look at the regularized vector ﬁeld. Recall that we consider the two regularization terms

γ R1(θ, ψ) := 2 EpD(x)
γ R2(θ, ψ) := 2 Epθ(x)

∇xDψ(x) 2 ∇xDψ(x) 2 .

(114) (115)

As discussed in Section 4.1, the regularization is only applied to the discriminator. The regularized vector ﬁeld is hence given by

v˜(θ, ψ) :=

−∇θL(θ, ψ)

.

∇ψL(θ, ψ) − ∇ψRi(θ, ψ)

(116)

Lemma D.3. The gradient ∇ψRi(θ, ψ) of the regularization terms Ri, i ∈ {1, 2}, with respect to ψ are

∇ψR1(θ, ψ) = γ EpD(x) [∇ψ,xDψ(x)∇xDψ(x)] (117) ∇ψR2(θ, ψ) = γ Epθ(x) [∇ψ,xDψ(x)∇xDψ(x)] . (118)

Which Training Methods for GANs do actually Converge?

Proof. These equations can be derived by taking the derivative of (114) and (115) with respect to ψ.
Lemma D.4. The second derivatives ∇2ψRi(θ∗, ψ∗) of the regularization terms Ri, i ∈ {1, 2}, with respect to ψ at (θ∗, ψ∗) are both given by

Using the fact that Dψ(x) = 0 and ∇xDψ(x) = 0 for x ∈ supp pD, we see that the Hessian of h(ψ) at ψ∗ is

∇2ψh(ψ∗) = 2 EpD(x) ∇ψDψ(x)∇ψDψ(x)T + ∇ψ,xDψ(x)∇ψ,xDψ(x)T

(125)

LDD := γ EpD(x) [∇ψ,xDψ∗ (x)∇ψ,xDψ∗ (x)T] . (119)

Moreover, both regularization ∇θ,ψRi(θ∗, ψ∗) = 0.

terms

satisfy

Proof. ∇2ψRi(θ∗, ψ∗), i ∈ {1, 2}, can be computed by taking the derivative of (117) and (118) with respect to ψ and
using the fact that ∇xDψ∗ (x) = 0 in a neighborhood of supp pD.

Moreover, we clearly have ∇θ,ψR1(θ∗, ψ∗) = 0, because

R1 does not depend on θ. To see that ∇θ,ψR2(θ∗, ψ∗) = 0,

take the derivative of (118) with respect to θ and use the

fact

that

∇xDψ∗ (x)

=

0

and

∇

2 x

Dψ

∗

(x

)

=

0

for

x

∈

supp pD.

As a result, the Jacobian v˜ (θ∗, ψ∗) of the regularized gradient vector ﬁeld at the equilibrium point is given by

v˜ (θ∗, ψ∗) = 0

−KDT G

.

KDG KDD − LDD

(120)

For brevity, we deﬁne MDD := KDD − LDD.
To prove Theorem 4.1, we have to show that v˜ (θ∗, ψ∗) is well behaved when restricting it to the space orthogonal to the tangent space of MG × MD at (θ∗, ψ∗):
Lemma D.5. Assume that Assumptions II and III hold. If v = 0 is not in the tangent space of MD at ψ∗, then v¯TMDDv < 0.

Proof. By Lemma D.2, we have

vTKDDv = 2f (0) EpD(x) (∇ψDψ∗ (x)Tv)2 (121)

and by Lemma D.4

vTLDDv = γ EpD(x) ∇x,ψDψ∗ (x)v 2 . (122)

By Assumption II, we have f (0) < 0. Hence, vTMDDv ≤ 0 and vTMDDv = 0 implies
∇ψDψ∗ (x)Tv = 0 and ∇x,ψDψ∗ (x)v = 0 (123)

for all x ∈ supp pD. Let
h(ψ) := EpD(x) |Dψ(x)|2 + ∇xDψ(x) 2 .

(124)

The second directional derivate ∂v2h(ψ) is therefore

∂v2h(ψ) = 2 EpD(x) |∇ψDψ(x)Tv|2 + ∇x,ψDψ(x)v 2 = 0.

(126)

By Assumption III, this can only hold if v is in the tangent space of MD at ψ∗.
Lemma D.6. Assume that Assumption III holds. If w = 0 is not in the tangent space of MG at θ∗, then KDGw = 0.

Proof. By Lemma D.2, we have

KDGw = f (0) ∇θ Epθ(x) [∇ψDψ∗ (x)] |θ=θ∗ w = f (0)∂wg(θ). (127)

for

g(θ) := Epθ(x) [∇ψDψ∗ (x)] .

(128)

By Assumption III, this implies KDGw = 0 if w is not in the tangent space of MG at θ∗.

We are now ready to prove Theorem 4.1:
Theorem 4.1. Assume Assumption I, II and III hold for (θ∗, ψ∗). For small enough learning rates, simultaneous and alternating gradient descent for v˜1 and v˜2 are both convergent to MG × MD in a neighborhood of (θ∗, ψ∗). Moreover, the rate of convergence is at least linear.

Proof. First note that by Lemma D.1 and Lemma D.3 v(θ, ψ) = 0 for all points (θ, ψ) ∈ MG × MD, because Dψ(x) = 0 and ∇xDψ(x) = 0 for all x ∈ supp pD and ψ ∈ MD. Hence, MG × MD consists only of equilibrium points of the regularized gradient vector ﬁelds.

Let Tθ∗ MG and Tψ∗ MD denote the tangent spaces of MG and MD at θ∗ and ψ∗.

We now want to show that both simultaneous and alternating gradient descent are locally convergent to MG × MD for the regularized gradient vector ﬁeld v˜(θ, ψ). To this end,
we want to apply Theorem A.3. By choosing local coordinates θ(α, γG) and ψ(β, γD) for MG and MD and using Remark A.6, we can assume without loss of generality that θ∗ = 0, ψ∗ = 0 as well as

MG = Tθ∗ MG = {0}k × Rn−k MD = Tψ∗ MD = {0}l × Rm−l.

(129) (130)

Which Training Methods for GANs do actually Converge?

This allows us to write11 v˜(θ, ψ) = v˜(α, γG, β, γD) In order to apply Theorem A.3, we have to show that ∇(α,β)v˜(θ∗, ψ∗) only has eigenvalues with negative realpart.

By Lemma D.2, ∇(α,β)v˜(θ∗, ψ∗) is of the form

0 K˜ DG

−K˜DT G K˜DD − L˜DD

(131)

where K˜DD, K˜DG and L˜DD denote the submatrices of KDD, KDG and LDD corresponding to the (α, β) coordinates.
We now show that M˜ DD := K˜DD − L˜DD is negative deﬁnite and K˜DG has full column rank.

To this end, ﬁrst note that

v˜TM˜ DDv˜ = vTMDDv

(132)

with vT := (v˜T, 0). Note that v ∈/ Tψ∗ MD for v˜ = 0. Hence, by Lemma D.5 we have that v˜TM˜DDv˜ < 0 if v˜ = 0. As a result, we see that M˜DD is symmetric negative deﬁnite.

Similarly, for wT := (w˜T, 0), the components of KDGw corresponding to the β-coordinates are given by K˜DGw˜. Again, we have w ∈/ Tθ∗ MG for w˜ = 0. Hence, by Lemma D.6 we have that KDGw = 0 if w˜ = 0. Because the components of KDGw corresponding to the γD coordinates are 0, this shows that K˜DGw˜ = 0. K˜DG therefore has full column rank.

Theorem A.7 now implies that all eigenvalues of ∇(α,β)v˜(θ∗, ψ∗) have negative real part. By Lemma A.4, Lemma A.5 and Theorem A.3, simultaneous and alter-
nating gradient descent are therefore both convergent to MG × MD near (θ∗, ψ∗) for small enough learning rates. Moreover, the rate of convergence is at least linear.

D.2. Extensions

In the proof of Theorem 4.1 we have assumed that f (0) < 0. This excludes the function f (t) = t which is used in Wasserstein-GANs. We now show that our convergence proof extends to the case where f (t) = t when we modify Assumption III as little bit:
Remark D.7. When we replace h(ψ) with

h˜(ψ) := EpD(x) ∇xDψ(x) 2

(133)

and MD with M˜ D := {ψ | h˜(ψ) = 0} the results of Theorem 4.1 still hold for f (t) = t.

Proof. Almost everything in the proof of Theorem 4.1 still holds for these modiﬁed assumptions. The only thing that
11By abuse of notation, we simply write θ = (α, γG) and ψ = (β, γD).

we have to show is that MG × MD still consists only of equilibrium points and that Lemma D.5 still holds in this setting.
To see the former, note that by Lemma D.1 we still have ∇θL(θ, ψ) = 0 for (θ, ψ) ∈ MG × MD, because we have ∇xDψ(x) = 0 for ψ ∈ MD and x ∈ supp pD. On the other hand, for f (t) = t we also have ∇ψL(θ, ψ) = 0 if θ ∈ MG, because for θ ∈ MG the deﬁnition of MG implies that pθ = pD and hence, by Lemma D.1,

∇ψL(θ, ψ) = Ex∼pD [∇ψDψ(x)] − Ex∼pD [∇ψDψ(x)] = 0.

(134)

To see why Lemma D.5 still holds, ﬁrst note that for f (t) =

t, we have f (0) = 0, so that by Lemma D.2 KDD = 0.

Hence,

vTMDDv = −vTLDDv.

(135)

We therefore have to show that vTLDDv = 0 if v is not in the tangent space of MD.

However, we have seen in the proof of Lemma D.5 that

vTLDDv = γ EpD(x) ∇x,ψDψ∗ (x)v 2 . (136)

Hence vTLDDv = 0 implies ∇x,ψDψ∗ (x)v = 0 for x ∈ supp pD and thus
∂v2h(ψ) = 2 EpD(x) ∇x,ψDψ(x)v 2 = 0. (137)
By Assumption III, this can only be the case if v is in the tangent space of MD. This concludes the proof.

In Section D.1, we showed that both regularizers R1 and R2 from Section 4.1 make the GAN training dynamics locally convergent. A similar, but slightly more complex regularizer was also proposed by Roth et al. (2017) who tried to ﬁnd a computationally efﬁcient approximation to instance noise. The regularizer proposed by Roth et al. (2017) is given by a linear combination of R1 and R2 where the weighting is adaptively chosen depending on the logits of Dψ(x) of the current discriminator at a data point x:

RRoth(θ, ψ) = Epθ(x) (1 − σ(Dψ(x)))2 ∇xDψ(x) 2 + EpD(x) σ(Dψ(x)))2 ∇xDψ(x) 2 (138)
Indeed, we can show that our convergence proof extends to this regularizer (and a slightly more general class of regularizers):
Remark D.8. When we replace the regularization terms R1 and R2 with

R3(θ, ψ) = Epθ(x) w1(Dψ(x)) ∇xDψ(x) 2 + EpD(x) w2(Dψ(x)) ∇xDψ(x) 2

(139)

Which Training Methods for GANs do actually Converge?

so that w1(0) > 0 and w2(0) > 0, the results of Theorem 4.1 still hold.
Proof. Again, we have to show that MG × MD still consists only of equilibrium points and that Lemma D.5 still holds in this setting.
However, by using ∇xDψ(x) = 0 for x ∈ supp pD and ψ ∈ MD, it is easy to see that ∇ψR3(θ, ψ) = 0 for all (θ, ψ) ∈ MG × MD, which implies that MG × MD still consists only of equilibrium points.
To see why Lemma D.5 still holds in this setting, note that (after a little bit of algebra) we still have ∇θ,ψR3(θ∗, ψ∗) = 0 and
∇2ψR3(θ∗, ψ∗) = γ1 (w1(0) + w2(0))LDD. (140)
The proof of Lemma D.5 therefore still applies in this setting.
E. Stable equilibria for unregularized GAN training
In Section 2, we have seen that unregularized GAN training is not always locally convergent to the equilibrium point. Moreover, in Section 4, we have shown that zero-centered gradient penalties make general GANs locally convergent under some suitable assumptions.
While our results demonstrate that we cannot expect unregularized GAN training to lead to local convergence for general GAN architectures, there can be situations where unregularized GAN training has stable equilibria. Such equilibria usually require additional assumptions on the class of representable discriminators.
In this section, we identify two types of stable equilibria. For the ﬁrst class of stable equilibria, which we call energy solutions, the equilibrium discriminator forms an energy function for the true data distributions and might be a partial explanation for the success of autoencoder-based discriminators (Zhao et al., 2016; Berthelot et al., 2017). For the second class, which we call full-rank solutions, the discriminator learns a representation of the data distribution with certain properties and might be a partial explanation for the success of batch-normalization for training GANs (Radford et al., 2015).
E.1. Energy Solutions
For technical reasons, we assume that supp pD deﬁnes a C1-manifold in this section.
Energy solutions are solutions where the discriminator forms a potential function for the true data distribution. Such solutions (θ∗, ψ∗) satisfy the following property:

Assumption I . We have pθ∗ = pD, Dψ∗ (x) = 0,

∇xDψ∗ (x)

=

0

and

v

T

∇

2 x

Dψ

∗

(

x

)v

>

0 for all x

∈

supp pD and v not in the tangent space of supp pD at x.

We also need a modiﬁed version of Assumption III which
ensures certain regularity properties of the reparameterization manifolds MG and MD near the equilibrium (θ∗, ψ∗). To formulate Assumption III , we need

g˜(ψ) := ∇θ Epθ(x) [Dψ(x)] θ=θ∗ .

(141)

Assumption III . There are -balls B (θ∗) and B (ψ∗) around θ∗ and ψ∗ so that MG ∩ B (θ∗) and MD ∩ B (ψ∗) deﬁne C1- manifolds. Moreover, the following holds:
(i) if v is not in the tangent space of MD at ψ∗, then ∂vg˜(ψ∗) = 0.
(ii) if w is not in the tangent space of MG at θ∗, then there is a latent code z ∈ Rk so that ∇θGθ∗ (z)w is not in the tangent space of supp pD at Gθ∗ (z) ∈ supp pD.
The ﬁrst part of Assumption III implies that the generator gradients become nonzero whenever the discriminator moves away from an equilibrium discriminator. The second part of Assumption III means that every time the generator leaves the equilibrium, it pushes some data point aways from supp pD, i.e. the generator is not simply redistributing mass on supp pD.
In Theorem E.2 we show that energy solutions lead to local convergence of the unregularized GAN training dynamics. For the proof, we ﬁrst need a generalization of Lemma D.2:

Lemma E.1. Assume that (θ∗, ψ∗) satisﬁes Assumption I . The Jacobian of the gradient vector ﬁeld v(θ, ψ) at (θ∗, ψ∗)
is then given by

v (θ∗, ψ∗) = KGG −KDT G . KDG KDD

(142)

The terms KDD and KDG are given by

KGG = − f (0) Ep(z) [∇θGθ∗ (z)]T

∇

2 x

Dψ

∗

(

Gθ

∗

(

z

))

∇

θ

Gθ

∗

(z

)

KDD = 2f (0) EpD(x) [∇ψDψ∗ (x)∇ψDψ∗ (x)T]

KDG = f (0) ∇θ Epθ(x) [∇ψDψ∗ (x)] |θ=θ∗ T

(143)
(144) (145)

Proof. Almost all parts of the proof of Lemma D.2 are
still valid. The only thing that remains to show is that ∇2θL(θ∗, ψ∗) = −KGG. To see this, just take the derivative of (108) with respect to θ and use the fact that ∇xDψ(x) = 0 for x ∈ supp pD.

We are now ready to formulate our convergence result for energy solutions:

Which Training Methods for GANs do actually Converge?

Theorem E.2. Assume Assumption I , II and III hold for (θ∗, ψ∗). Moreover, assume that f (0) > 0. For small
enough learning rates, simultaneous and alternating gra-
dient descent for the (unregularized) gradient vector ﬁeld v are both convergent to MG × MD in a neighborhood of (θ∗, ψ∗). Moreover, the rate of convergence is at least
linear.

Proof (Sketch). The proof is similar to the proof of Theorem 4.1.
First, note that MG × MD still only consists of equilibrium points. Next, we introduce local coordinates and show that for v not in the tangent space of MG at θ∗, we have vTKGGv < 0. This can be shown using Lemma E.1, Assumption I and the second part of Assumption III .
Moreover, we need to show that for w not in the tangent space of MD at ψ∗, we have KDT Gw = 0. This can be shown by applying the ﬁrst part of Assumption III .
The rest of the proof is the same as the proof of Theorem 4.1, except that we have to apply Theorem A.8 instead of Theorem A.7.

Note that energy solutions are only possible, if the discriminator is able to satisfy Assumption I . This is not the case for the Dirac-GAN from Section 2. However, if we use a quadratic discriminator instead, there are also energy solutions to the unregularized GAN training dynamics for the Dirac-GAN. To see this, we can parameterize Dψ(x) as

Dψ(x) := ψ1x2 + ψ2x.

(146)

It is easy to check that the Dirac-GAN with a discrimina-

tor as in (146) indeed has energy solutions: every (θ, ψ)

with θ = 0 and ψ2 = 0 deﬁnes an equilibrium point of

the Dirac-GAN and the GAN-training dynamics are locally

convergent near this point if ψ1 > 0. Note however, that

even though all equilbria with ψ1 > 0 are points of attrac-

tion for the continuous GAN training dynamics, they may

not be attractors for the discretized system when ψ1 is large

and the learning rate h is ﬁxed. In general, the conditioning

of energy solutions depends on the condition numbers of

the

Hessians

∇

2 x

D

ψ

∗

(x

)

at

all

x

∈

supp pD.

Indeed,

the

presence of ill-conditioned energy solutions might be one

possible explanation why WGAN-GP often works well in

practice although it is not even locally convergent for the

Dirac-GAN.

E.2. Full-Rank Solutions

In practice, Dψ(x) is usually implemented by a deep neural

network. Such discriminators can be described by functions

of the form

Dψ(x) = ψ1Tηψ2 (x)

(147)

with a vector-valued C1-functions ηψ2 and ψ = (ψ1, ψ2). ηψ2 can be regarded as a feature-representation of the data point x.
We now state several assumptions that lead to local convergence in this situation.
The ﬁrst assumption can be seen as a variant of Assumption I adapted to this speciﬁc situation: Assumption I . We have pθ∗ = pD and ψ1∗ = 0.
We again consider reparameterization manifolds, which we deﬁne as follows in this section:
MG := {θ | pθ = pD} MD := {ψ | ψ1 = 0}. (148)

Moreover, let g(θ) = Epθ(x) ηψ2∗ (x) .

(149)

Assumption III now becomes:
Assumption III . There is an -ball B (θ∗) around θ∗ so that MG deﬁnes a C1- manifold12. Moreover, the following holds:
(i) The matrix EpD(x) ηψ2∗ (x)ηψ2∗ (x)T has full rank. (ii) if w is not in the tangent space of MG at θ∗, then
∂wg(θ∗) = 0.
We call a function ηψ2∗ that satisﬁes the ﬁrst part of Assumption III a full-rank representation of pD. Moreover, if ηψ2∗ satisﬁes the second part of Assumption III , we call ηψ2∗ a complete representation, because the second part of Assumption III implies that every deviation from the Nashequilibrium pθ∗ = pD is detectable using ηψ2∗ .
In practice, complete full-rank representations might only exist if the class of discriminators is very powerful or the class of generators is limited. Especially the second part of Assumption III might be hard to satisfy in practice. Moreover, ﬁnding such representations might be much harder than ﬁnding equilibria for the regularized GAN-training dynamics from Section 4.
Nonetheless, we have the following convergence result for GANs that allow for complete full-rank representations:
Theorem E.3. Assume Assumption I , Assumption II and III hold for (θ∗, ψ∗). For small enough learning rates, simultaneous and alternating gradient descent for the (unregularized) gradient vector ﬁeld v are both convergent to MG × MD in a neighborhood of (θ∗, ψ∗). Moreover, the rate of convergence is at least linear.

Proof (Sketch). The proof is again similar to the proof of Theorem 4.1. We again introduce local coordinates and
12Note that MD is a C1-manifold by deﬁnition in this setup.

Which Training Methods for GANs do actually Converge?

show that for w not in the tangent space of MD at ψ∗, we have wTKDDw < 0. To see this, note that w must have a nonzero ψ1 component if it is not in the tangent space of MD at ψ∗. However, using (111), we see that the submatrix of KDD corresponding to the ψ1 coordinates is given by
K˜DD = 2f (0) EpD(x) ηψ2∗ (x)ηψ2∗ (x)T . (150)
This matrix is negative deﬁnite by Assumption II and the ﬁrst part of Assumption III .
Moreover, by applying (112), we see that the component of KDGw, w ∈ Rn, corresponding to the ψ1 coordinates is given by
∂wg(θ∗) = f (0)∇θ Epθ(x) ηψ2∗ (x) θ=θ∗ w. (151)
Using the second part of Assumption III , we therefore see that for w not in the tangent space of MG at θ∗, we have KDGw = 0.
The rest of the proof is the same as the proof of Theorem 4.1.
For the Dirac-GAN from Section 2, we can obtain a complete full-rank representation, when we parameterize the discriminator Dψ as Dψ(x) = ψ exp(x), i.e. if we set ψ1 := ψ and ηψ2 (x) := exp(x). It is easy to check that ηψ2 indeed deﬁnes a complete full-rank representation and that the Dirac-GAN is locally convergent to (θ∗, ψ∗) = (0, 0) for this parameterization of Dψ(x).
F. Experiments
In this section, we describe additional experiments and give more details on our experimental setup. If not noted otherwise, we always use the nonsaturating GAN-objective introduced by Goodfellow et al. (2014) for training the generator. For WGAN-GP we use the generator and discriminator objectives introduced by Gulrajani et al. (2017).13
2D-Problems For the 2D-problems, we run unregularized GAN training, R1-regularized and R2-regularized GAN training as well WGAN-GP with 1 and 5 discriminator update per generator update. We run each method on 4 different 2D-examples for 6 different GAN architectures. The 4 data-distributions are visualized in Figure 8. All 6 GAN architectures consist of 4-layer fully connected neural networks for both the generator and discriminator, where we select the number of hidden units from {8, 16, 32} and use select either leaky RELUs (i.e. ϕ(t) = max(t, 0.2t)) or Tanh-activation functions.
13The code to reproduce the experiments presented in this section can be found under https://github.com/ LMescheder/GAN_stability.

For each method, we try both Stochastic Gradient Descent (SGD) and RMS-Prop with 4 different learning rates: for SGD, we select the learning rate from {5 · 10−3, 10−2, 2 · 10−2, 5 · 10−2}. For RMSProp, we select it from {5 · 10−5, 10−4, 2 · 10−4, 5 · 10−4}. For the R1-, R2- and WGAN-GP-regularizers we try the regularization parameters γ = 1, γ = 3 and γ = 10. For each method and architecture, we pick the hyperparameter setting which achieves the lowest Wasserstein-1-distance to the true data distribution. We train all methods for 50k iterations and we report the Wasserstein-1-distance averaged over the last 10k iterations. We estimate the Wasserstein-1-distance using the Python Optimal Transport package14 by drawing 2048 samples from both the generator distribution and the true data distribution.
The best solution found by each method for the “Circle”distribution is shown in Figure 9. We see that the R1- and R2-regularizers converge to solutions for which the discriminator is 0 in a neighborhood of the true data distribution. On the other hand, unregularized training and WGAN-GP converge to energy solutions where the discriminator forms a potential function for the true data distribution. Please see Section E.1 for details.
CIFAR-10 To test our theory on real-world tasks, we train a DC-GAN architecture (Radford et al., 2015) with 3 convolutional layers and no batch-normalization on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). We apply different regularization strategies to stabilize the training. To compare the different regularization strategies, we measure the inception score (Salimans et al., 2016) over Wall-clock-time. We implemented the network in the Tensorﬂow framework (Abadi et al., 2016). For all regularization techniques, we use the RMSProp optimizer (Tieleman & Hinton, 2012) with α = 0.9 and a learning rate of 10−4.
For the R1 and R2 regularizers from Section 4.1 we use a regularization parameter of γ = 10. For the WGAN-GP regularizer we also use a regularization parameter of γ = 10 as suggested by Gulrajani et al. (2017). We train all methods using 1 discriminator update per generator update except for WGAN-GP, for which we try both 1 and 5 discriminator updates
The inception score over time for the different regularization strategies is shown in Figure 6. As predicted by our theory, we see that the R1- and R2-regularizers from Section 4.1 lead to stable training whereas unregularized GAN training is not stable. We also see that WGAN-GP with 1 or 5 discriminator updates per generator update lead to similar ﬁnal inception scores on this architecture. The good behavior of WGAN-GP is surprising considering the fact that it does not even converge locally for the Dirac-GAN. One possible
14http://pot.readthedocs.io

Which Training Methods for GANs do actually Converge?

explanation is that WGAN-GP oscillates in narrow circles around the equilibrium which might be enough to produce images of sufﬁciently high quality. Another possible explanation is that WGAN-GP converges to an energy or full-rank solution (Section E) for this example.
Imagenet In this experiment, we use the R1-regularizer to learn a generative model of all 1000 Imagenet (Russakovsky et al., 2015) classes at resolution 128 × 128 in a single GAN. Because of the high variability of this dataset, this is known to be a challenging task and only few prior works have managed to obtain recognizable samples for this dataset. Prior works that report results for this dataset either show a high amount of mode collapse (Salimans et al., 2016; Odena et al., 2017), report results only at a lower resolution (Hjelm et al., 2017) or use advanced normalization layers to stabilize the training (Miyato et al., 2018).
For the Imagenet experiment, we use ResNet-architectures15 for the generator and discriminator, both having 26 layers in total. Both the generator and discriminator are conditioned on the labels of the input data. The architectures for the generator and discriminator are shown in Table 3. We use preactivation ResNet-blocks and Leaky RELU-nonlinearities everywhere. We also multiply the output of the ResNet blocks with 0.1. For the generator, we sample a latent variable z from a 256-dimensional unit Gaussian distribution and concatenate it with a 256 dimensional embedding of the labels, which we normalize to the unit sphere. The resulting 512-dimensional vector is then fed into the ﬁrst fully connected layer of the generator. The discriminator takes as input an image and outputs a 1000 dimensional vector. Depending on the label of the input, we select the corresponding index in this vector and use it as the logits for the GAN-objective.
For training, we use the RMSProp optimizer with α = 0.99, = 10−8 and an initial learning rate of 10−4. We use a
batch size of 128 and we train the networks on 4 GeForce GTX 1080 Ti GPUs for 500.000 iterations. Similarly to prior work (Karras et al., 2017; Yazici et al., 2018; Gidel et al., 2018), we use an exponential moving average16 with decay 0.999 over the weights to produce the ﬁnal model.
We ﬁnd that while training this GAN without any regularization quickly leads to mode collapse, using the R1regularizers from Section 4.1 leads to stable training.
15We used more complicated architectures for the generator and discriminator in an earlier version of this manuscript. The simpliﬁed architectures presented in this version are more efﬁcient and perform slightly better.
16In an earlier version of this manuscript, we instead annealed the learning rate which has a similar effect. However, taking a moving average introduces fewer hyperparameters and led to higher inception scores in our experiments.

Some random (unconditional) samples can be seen in Figure 10. Moreover, Figure 11 and Figure 12 show conditional samples for some selected Imagenet classes. While not completely photorealistic, we ﬁnd that our model can produce convincing samples from all 1000 Imagenet classes.
We also compare the R1-regularizer with WGAN-GP (with 1 discriminator update per generator update). For this comparison, we did not use the exponential moving average over the weights and also did not normalize the embeddings of the labels to the unit sphere. The resulting inception score17 over the number of iterations is visualized in Figure 7. We ﬁnd that for this dataset and architecture we can achieve higher inception scores when using the R1-regularizer in place of the WGAN-GP regularizer.
celebA and LSUN To see if the R1-regularizers helps to train GANs for high-resolution image distributions, we apply our method to the celebA dataset (Liu et al., 2015) and to 4 subsets of the LSUN dataset (Yu et al., 2015) with resolution 256×256. We use a similar training setup as for the Imagenet experiment, but we use a slightly different architecture (Table 4). As in the Imagenet-experiment, we use preactivation ResNet-blocks and Leaky RELU-nonlinearities everywhere and we multiply the output of the ResNet-blocks with 0.1. We implemented the network in the PyTorch framework and use the RMSProp optimizer with α = 0.99 and a learning rate of 10−4. We again found that results can be (slightly) improved by using an exponential moving average with decay 0.999 over the weights to produce the ﬁnal models. As a regularization term, we use the R1-regularizer with γ = 10. For the latent code z, we use a 256 dimensional Gaussian distribution. The batch size is 64. We trained each model for about 300.000 iterations on 2 GeForce GTX 1080 Ti GPUs.
We ﬁnd that the R1-regularizer successfully stabilizes training of this architecture. Some random samples can be seen in Figures 13, 14, 15, 16 and 17.
celebA-HQ In addition to the generative model for celebA with resolution 256 × 256, we train a GAN on the celebAHQ dataset (Karras et al., 2017) with resolution 1024×1024. We use almost the same architecture as for celebA (Table 4), but add two more levels to increase the resolution from 256 × 256 to 1024 × 1024 and decrease the number of features from 64 to 16. Because of memory constraints, we also decrease the batch size to 24. As in the previous experiments, we use an exponential moving average with decay 0.999 over the weights to produce the ﬁnal model. In contrast to Karras et al. (2017), we train our model endto-end during the whole course of training, i.e. we do not
17For measuring the inception score, we use the public implementation from http://github.com/sbarratt/ inception-score-pytorch.

Which Training Methods for GANs do actually Converge?
use progressively growing GAN-architectures (nor any of the other techniques used by Karras et al. (2017) to stabilize the training). We trained the model for about 300.000 iterations on 4 GeForce GTX 1080 Ti GPUs. We ﬁnd that the simple R1-regularizer stabilizes the training, allowing our model to converge to a good solution without using a progressively growing GAN. Some random samples are shown in Figure 18.
Figure 6. Inception score over time for various regularization strategies when training on CIFAR-10. While the inception score can be problematic for evaluating probabilistic models (Barratt & Sharma, 2018), it still gives a rough idea about the convergence and stability properties of different training methods.

Figure 7. Inception score over the number of iterations for GAN training with R1- and WGAN-GP-regularization when training on Imagenet. We ﬁnd that R1-regularization leads to higher inception scores for this dataset and GAN-architecture.

Layer
Fully Connected Reshape TransposedConv2D TransposedConv2D TransposedConv2D

output size
256 · 4 · 4 256 × 4 × 4 128 × 8 × 8 64 × 16 × 16 3 × 32 × 32

ﬁlter
256 → 256 · 4 · 4 -
256 → 128 128 → 64 64 → 3

(a) Generator architecture

Layer

output size

ﬁlter

Conv2D Conv2D Conv2D Reshape Fully Connected

64 × 16 × 16 128 × 8 × 8 256 × 4 × 4
256 · 4 · 4 256 · 4 · 4

3 → 64 64 → 128 128 → 256
256 · 4 · 4 → 1

(b) Discriminator architecture

Table 2. Architectures for CIFAR-10-experiment.

Which Training Methods for GANs do actually Converge?

(a) 2D-Gaussian

(b) Line

(c) Circle

(d) Four lines

Figure 8. The four 2D-data distributions on which we test the different algorithms.

(a) unregularized

(b) R1

(c) R2

(d) WGAN-GP-1

(e) WGAN-GP-5

Figure 9. Best solutions found by the different algorithms for learning a circle. The blue points are samples from the true data distribution, the orange points are samples from the generator distribution. The colored areas visualize the gradient magnitude of the equilibrium discriminator. We ﬁnd that while the R1- and R2-regularizers converge to equilibrium discriminators that are 0 in a neighborhood of the true data distribution, unregularized training and WGAN-GP converge to energy solutions (Section E.1).

(a) Training distribution

(b) Random samples

Figure 10. Unconditional random samples for a GAN trained on the ILSVRC dataset (Russakovsky et al., 2015) with resolution 128 × 128. The ﬁnal inception score is 30.2 ± 0.5.

Which Training Methods for GANs do actually Converge?

(a) tench

(b) papillon

(c) weevil

(d) admiral

(e) lighthouse

(f) bell cote

(g) castle

(h) dam

(i) dock

Figure 11. Class conditional random samples for a GAN trained on the Imagenet dataset.

Which Training Methods for GANs do actually Converge?

(a) home theater

(b) jack-o’-lantern

(c) passenger car

(d) police van

(e) rugby ball

(f) ski

(g) triﬂe

(h) pizza

(i) valley

Figure 12. Class conditional random samples for a GAN trained on the Imagenet dataset.

Which Training Methods for GANs do actually Converge?
Figure 13. Random samples for a GAN trained on the celebA dataset (Liu et al., 2015) (256 × 256) for a DC-GAN (Radford et al., 2015) based architecture with additional residual connections (He et al., 2016). For both the generator and the discriminator, we do not use batch normalization.

Which Training Methods for GANs do actually Converge?
f Figure 14. Random samples for a GAN trained on the LSUN-bedroom dataset (Yu et al., 2015) (256 × 256) for a DC-GAN (Radford et al., 2015) based architecture with additional residual connections (He et al., 2016). For both the generator and the discriminator, we do not use batch normalization.

Which Training Methods for GANs do actually Converge?
Figure 15. Random samples for a GAN trained on the LSUN-church dataset (Yu et al., 2015) (256 × 256) for a DC-GAN (Radford et al., 2015) based architecture with additional residual connections (He et al., 2016). For both the generator and the discriminator, we do not use batch normalization.

Which Training Methods for GANs do actually Converge?
Figure 16. Random samples for a GAN trained on the LSUN-bridge dataset (Yu et al., 2015) (256 × 256) for a DC-GAN (Radford et al., 2015) based architecture with additional residual connections (He et al., 2016). For both the generator and the discriminator, we do not use batch normalization.

Which Training Methods for GANs do actually Converge?
Figure 17. Random samples for a GAN trained on the LSUN-tower dataset (Yu et al., 2015) (256 × 256) for a DC-GAN (Radford et al., 2015) based architecture with additional residual connections (He et al., 2016). For both the generator and the discriminator, we do not use batch normalization.

Which Training Methods for GANs do actually Converge?
Figure 18. Random samples for a GAN trained on the celebA-HQ dataset (Karras et al., 2017) (1024 × 1024) for a DC-GAN (Radford et al., 2015) based architecture with additional residual connections (He et al., 2016). During the whole course of training, we directly train the full-resolution generator and discriminator end-to-end, i.e. we do not use any of the techniques described in Karras et al. (2017) to stabilize the training.

Which Training Methods for GANs do actually Converge?

Layer
Fully Connected Reshape
Resnet-Block Resnet-Block NN-Upsampling
Resnet-Block Resnet-Block NN-Upsampling
Resnet-Block Resnet-Block NN-Upsampling
Resnet-Block Resnet-Block NN-Upsampling
Resnet-Block Resnet-Block NN-Upsampling
Resnet-Block Resnet-Block Conv2D

output size
1024 · 4 · 4 1024 × 4 × 4
1024 × 4 × 4 1024 × 4 × 4 1024 × 8 × 8
1024 × 8 × 8 1024 × 8 × 8 1024 × 16 × 16
512 × 16 × 16 512 × 16 × 16 512 × 32 × 32
256 × 32 × 32 256 × 32 × 32 256 × 64 × 64
128 × 64 × 64 128 × 64 × 64 128 × 128 × 128
64 × 128 × 128 64 × 128 × 128 3 × 128 × 128

ﬁlter
512 → 1024 · 4 · 4 -
1024 → 1024 → 1024 1024 → 1024 → 1024
-
1024 → 1024 → 1024 1024 → 1024 → 1024
-
1024 → 512 → 512 512 → 512 → 512
-
512 → 256 → 256 256 → 256 → 256
-
256 → 128 → 128 128 → 128 → 128
-
128 → 64 → 64 64 → 64 → 64
64 → 3

(a) Generator architecture

Layer

output size

ﬁlter

Conv2D

64 × 128 × 128

3 → 64

Resnet-Block Resnet-Block Avg-Pool2D

64 × 128 × 128 128 × 128 × 128 128 × 64 × 64

64 → 64 → 64 64 → 64 → 128
-

Resnet-Block Resnet-Block Avg-Pool2D

128 × 64 × 64 256 × 64 × 64 256 × 32 × 32

128 → 128 → 128 128 → 128 → 256
-

Resnet-Block Resnet-Block Avg-Pool2D

256 × 32 × 32 512 × 32 × 32 512 × 16 × 16

256 → 256 → 256 256 → 256 → 512
-

Resnet-Block Resnet-Block Avg-Pool2D

512 × 16 × 16 1024 × 16 × 16 1024 × 8 × 8

512 → 512 → 512 512 → 512 → 1024
-

Resnet-Block Resnet-Block Avg-Pool2D

1024 × 8 × 8 1024 × 8 × 8 1024 × 4 × 4

1024 → 1024 → 1024 1024 → 1024 → 1024
-

Resnet-Block Resnet-Block Fully Connected

1024 × 4 × 4 1024 × 4 × 4
1024 · 4 · 4

1024 → 1024 → 1024 1024 → 1024 → 1024
1024 · 4 · 4 → 1000

(b) Discriminator architecture

Layer
Fully Connected Reshape
Resnet-Block NN-Upsampling
Resnet-Block NN-Upsampling
Resnet-Block NN-Upsampling
Resnet-Block NN-Upsampling
Resnet-Block NN-Upsampling
Resnet-Block NN-Upsampling
Resnet-Block Conv2D

output size
1024 · 4 · 4 1024 × 4 × 4
1024 × 4 × 4 1024 × 8 × 8
1024 × 8 × 8 1024 × 16 × 16
512 × 16 × 16 512 × 32 × 32
256 × 32 × 32 256 × 64 × 64
128 × 64 × 64 128 × 128 × 128
64 × 128 × 128 64 × 256 × 256
64 × 256 × 256 3 × 256 × 256

ﬁlter
512 → 1024 · 4 · 4 -
1024 → 1024 → 1024 -
1024 → 1024 → 1024 -
1024 → 512 → 512 -
512 → 256 → 256 -
256 → 128 → 128 -
128 → 64 → 64 -
64 → 64 → 64 64 → 3

(a) Generator architecture

Layer

output size

ﬁlter

Conv2D

64 × 256 × 256

3 → 64

Resnet-Block Avg-Pool2D

64 × 256 × 256 64 × 128 × 128

64 → 64 → 64 -

Resnet-Block Avg-Pool2D

128 × 128 × 128 128 × 64 × 64

64 → 64 → 128 -

Resnet-Block Avg-Pool2D

256 × 64 × 64 256 × 32 × 32

128 → 128 → 256 -

Resnet-Block Avg-Pool2D

512 × 32 × 32 512 × 16 × 16

256 → 256 → 512 -

Resnet-Block Avg-Pool2D

1024 × 16 × 16 1024 × 8 × 8

512 → 512 → 1024 -

Resnet-Block Avg-Pool2D

1024 × 8 × 8 1024 × 4 × 4

1024 → 1024 → 1024 -

Fully Connected 1024 · 4 · 4

1024 · 4 · 4 → 1

(b) Discriminator architecture

Table 4. Architectures for LSUN- and celebA-experiments.

Table 3. Architectures for Imagenet-experiment.

Which Training Methods for GANs do actually Converge?

(a) Standard GAN

(b) Non-saturating GAN

(a) Standard GAN

(b) Non-saturating GAN

(c) WGAN

(d) WGAN-GP

(c) WGAN

(d) WGAN-GP

(e) Consensus optimization

(f) Instance noise

(e) Consensus optimization

(f) Instance noise

(g) Gradient penalty

(h) Gradient penalty (CR)

Figure 19. Convergence properties of different GAN training algorithms using simultaneous gradient descent. The shaded area in Figure 19c visualizes the set of forbidden values for the discriminator parameter ψ. The starting iterate is marked in red.

(g) Gradient penalty

(h) Gradient penalty (CR)

Figure 20. Convergence properties of different GAN training algorithms using alternating gradient descent with 1 discriminator update per generator update The shaded area in Figure 20c visualizes the set of forbidden values for the discriminator parameter ψ. The starting iterate is marked in red.

Which Training Methods for GANs do actually Converge?

(a) Standard GAN

(b) Non-saturating GAN

(c) WGAN

(d) WGAN-GP

(e) Consensus optimization

(f) Instance noise

(g) Gradient penalty

(h) Gradient penalty (CR)

Figure 21. Convergence properties of different GAN training algorithms using alternating gradient descent with 5 discriminator updates per generator update. The shaded area in Figure 21c visualizes the set of forbidden values for the discriminator parameter ψ. The starting iterate is marked in red.

Which Training Methods for GANs do actually Converge?

(a) SimGD

(b) AltGD (nd = 1)

(c) AltGD (nd = 5)

Figure 22. Convergence properties of our GAN using two time-scale training as proposed by Heusel et al. (2017). For the Dirac-GAN we do not see any sign of convergence when training with two time-scales. The starting iterate is marked in red.

