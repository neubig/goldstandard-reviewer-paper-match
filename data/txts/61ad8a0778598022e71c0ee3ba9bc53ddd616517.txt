Answering Complicated Question Intents Expressed in Decomposed Question Sequences

Mohit Iyyer∗

Wen-tau Yih, Ming-Wei Chang

Department of Computer Science and UMIACS

Microsoft Research

University of Maryland, College Park

Redmond, WA 98052

miyyer@umd.edu

{scottyih,minchang}@microsoft.com

arXiv:1611.01242v1 [cs.CL] 4 Nov 2016

Abstract
Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. Existing QA systems face two major problems when evaluated on our dataset: (1) handling questions that contain coreferences to previous questions or answers, and (2) matching words or phrases in a question to corresponding entries in the associated table. We conclude by proposing strategies to handle both of these issues.
1 Introduction
Semantic parsing, which maps natural language text to meaning representations in formal logic, has emerged as a key technical component for building question answering systems (Liang, 2016). Once a natural language question has been mapped to a formal query, its answer can be retrieved simply by executing the query on a back-end structured database.
One of the main focuses of semantic parsing research is how to address compositionality in language. Extremely complicated questions have been used to
∗Work done during an internship at Microsoft Research

demonstrate the sophistication of semantic parsers,1 and such questions have been speciﬁcally targeted in the design of a recently-released QA dataset (Pasupat and Liang, 2015). Take for example the following question: “of those actresses who won a Tony after 1960, which one took the most amount of years after winning the Tony to win an Oscar?” The corresponding logical form is highly compositional; in order to answer it, many sub-questions must be implicitly answered in the process (e.g., “who won a Tony after 1960?”).
While we agree that semantic parsers should be able to answer very complicated questions, in reality these questions are rarely issued by human users of QA systems.2 Because users can interact with a QA system repeatedly, there is no need to assume a singleturn QA setting where the exact question intent has to be captured with just one complex question. The same intent can be more naturally expressed through a sequence of simpler questions, as shown below:
1. What actresses won a Tony after 1960? 2. Of those, who later won an Oscar? 3. Who had the biggest gap between their two
award wins?
Decomposing complicated intents into multiple related but simpler questions is arguably a more effective strategy to explore a topic of interest, and it reduces the cognitive burden on both the person who
1For example, “will it be warmer than 70 degrees near the Golden Gate Bridge after 5PM the day after tomorrow?” (Dye, 2016)
2As indirect evidence, the percentage of questions with more than 15 words is only 3.75% in the WikiAnswers questions dataset (Fader et al., 2014).

asks the question as well as the one who answers it.3 In this work, we study the semantic parsing
problem for answering sequences of simple related questions. We collect a dataset of question sequences that we call SequentialQA (SQA)4 by asking crowdsourced workers to decompose complicated questions sampled from the WikiTableQuestions dataset (Pasupat and Liang, 2015) into multiple easier ones. In addition, each question is associated with answers selected by workers from a corresponding Wikipedia HTML table. Using the SQA dataset, we investigate experimentally how we should modify traditional semantic parser design to address different properties in this new, multi-turn QA setting, such as inter-question coreferences.
Our contributions are twofold. First, to the best of our knowledge, SQA is the ﬁrst semantic parsing dataset for sequential question answering. We believe this dataset will be valuable to future research on both semantic parsing and question answering in the more natural interactive setting. Second, after evaluating existing question answering systems on SQA, we ﬁnd that none of them performs adequately, despite the relative lack of compositionality. We provide a detailed error analysis, which suggests that improperly-resolved references and mismatches between question text and table entries are the main sources of errors.
The rest of the paper is structured as follows. Sec. 2 contrasts the existing tasks and datasets to SQA. Sec. 3 describes how we collect the data in detail. Sec. 4 presents our experimental study, followed by the discussion in Sec. 5.2. Finally, Sec. 6 concludes the paper.
2 Related Work
Our work is related to existing research on conversational (or contextual) semantic parsing, as well as more generally to interactive question-answering systems that operate on semi-structured data.
Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence
3While cognitive load has not been measured speciﬁcally for complicated questions, there have been many studies linking increased sentence complexity to longer reading times (Hale, 2006; Levy, 2008; Frank, 2013).
4To be released at http://aka.ms/sqa

analysis described in Zettlemoyer and Collins (2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clariﬁcations. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and actions. We differentiate ourselves from these prior works in two signiﬁcant ways: ﬁrst, our dataset is not restricted to a particular domain, which results in major challenges further detailed in Section 5.2; and second, a major goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent.
Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. A follow-up work (Lacatusu et al., 2006) proposes syntax-based heuristics to automatically decompose complex questions into simpler ones. Both works rely on proprietary limited-domain datasets; it is unlikely that the proposed heuristics would scale across arbitrary domains.
3 A Dataset of Question Sequences
Since there are no previous publicly-available datasets for our task, we collect the SequentialQA (SQA) dataset via crowdsourcing. We leverage WikiTableQuestions (Pasupat and Liang, 2015, henceforth WTQ), which contains highly compositional questions associated with HTML tables from Wikipedia. Each crowdsourcing task contains a long, complex question originally from WTQ as the question intent. The workers are asked to compose a sequence of simpler questions that lead to the ﬁnal intent; an example of this process is shown in Figure 1.
To simplify the task for workers, we only select certain types of questions from WTQ. In particular, we only use questions from WTQ whose answers

ORIGINAL INTENT:
Of those actresses who won a Tony after 1960, which took the most amount of years to get their EGOT completed?
DECOMPOSED SEQUENCE:
What actresses have completed an EGOT?
Which of them won a Tony after 1960?
Of those, who took the most years to complete the EGOT?

List of people who have won Academy, Emmy, Grammy, and Tony Awards

Name

EGOT Years to completed complete

Emmy

Grammy

Oscar

Tony

Richard 1962

17

1962

1960

1945

1951

Rodgers

Helen 1977

45

1953

1977

1932

1941

Hayes

Rita

1977

16

1977

1972

1961

1975

Moreno

John

1991

30

1991

1979

1981

1961

Gielgud

Audrey 1994

41

1993

1994

1953

1954

Hepburn

Marvin 1995

23

1995

1974

1973

1976

Hamlisch

Jonathan 1997

20

1982

1988

1977

1997

Tunick

Mel

2001

34

1967

1998

1968

2001

Brooks

Mike

2001

40

2001

1961

1967

1964

Nichols

Whoopi 2002

17

2002

1985

1990

2002

Goldberg

Scott

2012

28

1984

2012

2007

1994

Rudin

Robert 2014

10

2008

2012

2014

2004

Lopez

Figure 1: An example decomposition of a complicated intent from WTQ. Workers must create a sequence of decomposed questions where the answer to each question is a subset of cells in the table.

are cells in the table, which excludes those involving arithmetic and counting. We likewise also restrict the questions our workers can write to those that are answerable by only table cells. These restrictions speed the annotation process because, instead of typing their answers, workers can just click on the table to answer their question. They also allow us to collect answer coordinates (row and column in the table) as opposed to answer text, which removes many normalization issues for answer string matching that are present in the original WTQ dataset. Finally, we only use intents that contain nine or more words; we ﬁnd that shorter questions tend to be simpler and are thus less amenable to decomposition.
After iterating on the task design with many pilot tasks, we found that the following constraints are necessary for workers to produce good sequences:
Minimum sequence length: Workers must create sequences that contain at least two questions. If the intent is not easily decomposed into multiple questions, we instruct workers to create an alternate intent whose answer is the same as that of the original. We also encourage workers to write longer sequences if possible.
Final answer same as original answer: The ﬁnal question of a sequence must have the same answer as that of the original intent. Without this constraint, some workers were writing sequences that diverged

drastically from the intent.
No copying the intent: After adding the previous constraint, we found that many workers were just copying the intent as the ﬁnal question of their sequence, which resulted in unnatural-sounding sequences. After we disallowed copying, the workers’ ﬁnal questions contained many more references to previous questions and answers.
We also encouraged (but did not enforce) the following:
Simplicity: When decomposing a complicated intent into a sequence of questions, we expect that each question in the sequence should be simpler than the intent itself. However, deﬁning “simple” is difﬁcult, and enforcing any deﬁnition is even harder. Instead, we told workers to try to limit their questions to those that can be answered with just a single primitive operation (e.g., column selection, argmax/argmin, ﬁltering with a single condition) and provided them with examples of each primitive. Following this definition too closely, however, can result in unnatural sequences, so we do not make any UI changes to limit questions to single primitives.
Inter-question coreferences: Take the following two sequences generated from the same question intent:
1. What country won the World Cup in 2014? Of the players on the team that won the World Cup in 2014, which ones were midﬁelders?
2. What country won the World Cup in 2014? Of the players on that team, which ones were midﬁelders?
The second question of the ﬁrst sequence clumsily repeats information found in the preceding question, while the second sequence avoids this repetition with the referring expression “that team”. To encourage more coreferences between questions, we showed workers example sequences like these and stated that the second one is preferred.
3.1 Properties of SQA
In total, we used 2,022 question intents from the train and test folds of the WTQ for decomposition. We had three workers decompose each intent, resulting in 6,066 unique questions sequences containing 17,553

total question-answer pairs (for an average of 2.9 questions per sequence). We divide the dataset into train and test using the original WTQ folds, resulting in an 83/17 train/test split. Importantly, just like in WTQ, none of the tables in the test set are seen in the training set.
We identify three frequently-occurring question classes: select column, select subset, and select row. In select column questions, the answer is an entire column of the table; these questions account for 23% of all questions in SQA. Subset and row selection are more complicated than column selection, particularly because they usually contain coreferences to the previous question’s answer. In select subset questions, the answer is a subset of the previous question’s answer; similarly, the answers to select row questions occur in the same row(s) as the previous answer but in a different column. Select subset questions make up 27% of SQA, while select row is 19%. The remaining 31% of SQA is comprised of more complex questions that are combinations of these three types. In the sequence “what are all of the tournaments? in which one did he score the least points? on what date was that?”, the ﬁrst question is a column selection, the second question is a subset selection, and the ﬁnal question is a row selection.
We also observe dramatic differences in the types of questions that are asked at each position of the sequence. For example, looking at just the ﬁrst question of each sequence, 51% of them are of the select column variety (e.g., “what are all of the teams?”). This number dwindles to just 18% when we look at the second question of each sequence, which indicates that the collected sequences start with general questions and progress to more speciﬁc ones. By definition, select subset and select row questions cannot be the ﬁrst question in a sequence.
4 Baseline Experiments
We evaluate two existing QA systems on SQA, a semantic parsing system called ﬂoating parser and an end-to-end neural network. The ﬂoating parser considers each question in a sequence independently of the previous questions, while the neural network leverages contextual information from the sequence. Our goals with these experiments are (1) to measure the difﬁculty of SQAand (2) to better understand the

behaviors of existing state-of-the-art systems.
4.1 Floating parser
An obvious baseline is the ﬂoating parser (FP) developed by Pasupat and Liang (2015), which FP maps questions to logical forms and then executes them on the table to retrieve the answers. It achieves 37.0% accuracy on the WTQ test set. One of the key challenges in semantic parsing is the “semantic matching problem”, where question text cannot be matched to the corresponding answer column or cell verbatim. Without external knowledge, it is often hard to map words or phrases in a question to predicates in its corresponding logical form. Further compounding this problem is that the train and test tables are disjoint, which renders lexicon induction futile. Therefore, FP does not anchor predicates to tokens in the question, relying instead on typing constraints to reduce the search space.5
Using FP as-is results in poor performance on SQA. The main reason is that the system is conﬁgured for questions with single answers, while SQA contains a high percentage of questions with multiplecell answers. We address this issue by removing a pruning hyperparameter (tooManyValues that eliminates all candidate parses with more than ten items in their denotations, as well as by removing features that add bias on the denotation size.
4.2 End-to-end neural network
Recently, two different end-to-end neural network architectures for question-answering on tables have been proposed (Neelakantan et al., 2015; Yin et al., 2016). Both models show promising results on synthetic datasets, but neither has been evaluated on real data. We implement our own end-to-end neural model (NEURAL) by generally following both models but deviating when necessary to account for our dataset characteristics.
As a brief description, we encode the question, each column header, and each cell in the table with a character-level LSTM. We identify three high-level operations based on our dataset characteristics (select column, select row, and select cell) and design modules that perform each of these functions. A module-level soft attention mechanism, effectively a
5See Pasupat and Liang (2015) for more details.

weighted sum of the module scores, decides which module to use given a question.6 We also place an additional LSTM over the question sequence in order to pass information about previous answers and questions to the current time step. Finally, the output of the attention mechanism and the question sequence LSTM is combined and fed to a binary classiﬁer that, given each cell of the table, decides if the cell is part of the answer to the current question or not.
Fig. 2 shows an example of how the modules in NEURAL work together to answer a given question. In particular, since the question “which of them won a Tony after 1960?” is asking for the names of the actresses, the column selection module places most of its weight on the “Name” column, while the row selection module highly weights rows that satisfy the condition “Tony after 1960”. The modules, which take the question and table as input, are merged with an attention mechanism a that also considers the answer to the previous question. A full speciﬁcation of NEURAL can be found in Appendix A.
In contrast to both the neural programmer of Neelakantan et al. (2015) and the neural enquirer of Yin et al. (2016), we make the simplifying assumption that each question in a sequence can be solved with just a single operation. Another major difference is that we use a character-level LSTM, as the training and test vocabulary are radically different.7
4.3 Results
Table 1 shows the results of both FP and NEURAL on the test set of SQA. We present both the overall accuracy and the accuracy of answers to questions at each position. Although the accuracy of FP on position-1 questions (48.7%) is much higher than its performance on WTQ (37.0%), the overall accuracy (32.8%) is still lower, which indicates that our SQA dataset remains difﬁcult. In addition, the NEURAL model signiﬁcantly underperforms FP, suggesting that it requires more data or more sophisticated archi-
6We did not design more speciﬁc modules to handle arithmetic or aggregation like those of Neelakantan et al. (2015), although this is a potentially interesting direction for larger datasets.
7Due to the fact that much of our vocabulary (e.g., numbers, entities) is not included in a regular corpus, we suspect that the alternative of leveraging publicly-available word embeddings will not be effective.

answer to previous question: What actresses have completed an EGOT?

Which of them won a Tony after 1960?

select_col
a

select_cell

select_row

0.71

0.14

0.03

0.02

0.03

0.06

Name

EGOT Years to complete complete

Emmy

Grammy

Oscar

0.01 Tony

0.05

Helen

Hayes 1977

45

1953

1977

1932

1941

0.87

Rita

Moreno 1977

16

1977

1972

1961

1975

0.13

Audrey

Hepburn 1994

41

1993

1994

1953

1954

0.94

Whoopi

Goldberg 2002

17

2002

1985

1990

2002

Figure 2: Diagram of NEURAL architecture. Small colored rectangles represent the output of the character-level LSTM decoder. The question, column header, and cell representations are passed to three attentional modules. The output of these modules is combined with the answer predictions for the previous question to yield a ﬁnal answer prediction for each cell.

Model All Pos 1 Pos 2 Pos 3 Pos 4
FP 32.8 48.7 25.8 26.2 17.5 NEURAL 17.4 27.6 13.4 11.8 12.2
Table 1: Accuracy of existing systems on our datasets on all questions and questions at all positions within the sequence.

tectural design to generalize to all of SQA’s complexities.
5 Directions for Improving Sequential Question Answering
In this section, we explore possible directions for improving the system performance in the sequential question answering setting. We start from investigating different strategies for handling the coreference issues of questions, and then revisited the semantic matching issue by conducting some error analysis.
5.1 Adapting Existing Semantic Parsers
As we observed in Sec. 4, existing semantic parsers perform suboptimally on SQA. One possible explanation for the suboptimal performance of existing semantic parsers, shown in Table 1, is that questions that contain references to previous questions or answers are not handled properly. By leveraging FP we propose two ways to deal with this issue: question

rewriting and table rewriting.
Question rewriting: Take for example the partial sequence “what are all the countries that participated in the olympics? which ones won more than two gold medals?” Any system that treats these two questions independently of each other has a high likelihood of failing on the second question because “ones” is not resolved to “countries”. The obvious solution is to apply coreference resolution. However, existing coreference resolution systems struggle at identifying coreferences across two questions, potentially due to the fact that their training data came from newswire text with few questions.
An alternative approach is to create a set of common referential expressions (e.g., “ones”, “them”, “those”) and replace them with noun phrases from the previous question. As we do not have ground-truth coreference annotations, we compute upper-bound improvements on question rewriting instead. That is, we rewrite a reference in a question with all possible noun phrases in the previous question and count the question as correct if any of the rewritten questions are answered correctly. Interestingly, we observe an upper bound improvement of only ≈2% accuracy.
Why is the upper bound so low? An error analysis ﬁnds that in many cases, the logical form predicted by FP is wrong even when the referential expression is correctly resolved. We will discuss this phenomenon more in Sec. 5.2, but here we concentrate on another common scenario: the question contains a coreference to the answer of the previous question. If we modify our example sequence to “what are all the countries that participated in the olympics in 2012? which ones won more than two gold medals?”, then simply replacing “ones” with “countries” does not resolve the reference.
Table rewriting: Instead of building a model that can learn to rewrite the second question to “which countries won more than two gold medals in 2012”, or training a semantic parser that can incrementally update the logical form from the previous question as in Zettlemoyer and Collins (2009), we propose to simply rewrite the table based on the ﬁrst question’s answer. Speciﬁcally, if we know that a particular question is a row or subset selection type, then we also know that its answer must be located in the rows that contain the previous answer. For example, take

the second question of the decomposed sequence in Fig. 1, which contains a coreference to the answer of the ﬁrst question (“which of them won a Tony after 1960”) that refers to four actresses. The smallest possible table from which we can still answer this question is one that has four rows (for each of the four actresses) and two columns (“Name” and “Tony”). However, identifying the columns necessary to answer each question is often difﬁcult, so we leave this task to the semantic parser and remove only rows (not columns) that do not contain the previous question’s answers (see the rewritten table for this example in Fig. 2). In this way, we implicitly resolve the coreference “of them”, as any rows that do not correspond to actresses are excluded.
Before rewriting the table, we have to ﬁrst decide whether the question contains a coreference to the answer or not. We know that we should only rewrite the table for subset and row selection questions. Since we can identify the question type in our dataset based on the coordinates of the answers, we assume that we know which questions should and should not be rewritten and use this information to compute upper bounds for semantic parser improvement with table rewriting. We evaluate ﬁve different rewriting policies which vary in their knowledge of both the question type and the correctness of the previous predicted answer:
1. never rewrite the table 2. always rewrite the table based on the previous
predicted answer, regardless of whether table rewriting is applicable to the question 3. rewrite row/subset: rewrite the table based on the previous predicted answer only when table rewriting is applicable (i.e., the question is subset or row selection) 4. reference: same as rewrite row/subset, except we only rewrite when we know the previous predicted answer is correct 5. upper bound: same as rewrite row/subset, except we rewrite using the previous ground-truth answer instead of the previous predicted answer
Table 2 shows the results of running these different rewriting policies on our dev set. The oracle score represents the percentage of questions for which at least one candidate logical form generated by the

Policy

Dev Acc Dev Oracle

Never rewrite

27.7

66.6

Always rewrite

26.9

55.3

Rewrite row/subset 28.2

59.8

Reference

29.2

67.3

Upper bound

37.0

71.9

Table 2: Dev accuracy of different table rewriting policies; the upper bound represents an almost 10% absolute improvement that the other policies do not come close to reaching due to the poor baseline performance of FP.

parser8 evaluates to the correct answer. The most important takeaway is that accuracy improvements are very small when we rewrite based on the previous predictions. Intuitively, this makes sense: if the parser only gets 30% accuracy, then 70% of the time it will be incorrect on the previous question, and rewriting the table based on a wrong answer could make it impossible for the parser to get the right answer (see the lower oracle scores for always rewrite and rewrite row/subset). Based on these results, table rewriting will only be useful if the base parser’s accuracy is high.
5.2 The semantic matching problem
The underwhelming improvements from question and table rewriting force us to re-evaluate our original hypothesis that reference resolution is the main source of complexity in our dataset. We take 70 questions from our dev set and manually annotate them with reasons why FP answered them incorrectly. Somewhat surprisingly, we ﬁnd that only 15 of these errors are due solely to coreferences! The majority of errors are due to wrong logical forms that cannot be corrected by simply resolving a coreference (e.g., the wrong operations are used, or the order of the operations is incorrect).
When checking these questions in detail, we ﬁnd that the majority of the errors are due to the semantic matching problem – mismatches between question text and table text. The error analysis in (Pasupat and Liang, 2015) on the more complicated WTQ dataset shows that 25% of errors are due to these mismatches and an additional 29% to normalization issues (e.g.,
8The number of candidate parses considered by FP varies and could sometimes be hundreds.

what historic sites are located near a highway?

Name

Location

City

Bronson Public Library

207 Matteson Street

Bronson

City of Coldwater Informational Designation

City Park at intersection of US-12 and US-27

Goldwater

on what dates did the games end in a tie?

Date

Opponents

Result F-A

25 August 1984

Watford

1-1

8 September 1984 Newcastle United

5-0

what stations play rock and jazz?

Location

Call sign

Network

Beach

KDPR

News & Classical

Grand Forks Lisbon

KFJM KDSU

Roots, Rock, and Jazz
News, Classical, Rock, and Jazz

Figure 3: Example mismatches between question and table from SQA. Resolving the mismatches requires world knowledge such as highway naming conventions and sports terminology that must be provided externally or learned from a larger corpus.

an answer cell may contain “Beijing, China” but the crowdsourced answer is just “Beijing”). Because all answers in SQA are the exact text of cells in the table, we avoid these normalization issues; however, the results in Table 1 show that the sequential nature of SQA makes it equally as difﬁcult as WTQ for machines. The examples in Figure 3 suggest that without solving the semantic matching problem, we will not be able to properly take advantage of our question or table rewriting adaptations.
6 Conclusion
While most current QA systems assume a singleturn setting, in this work we move towards a more conversational, multi-turn scenario in which systems must rely on prior context to answer the user’s current question. To this end, we introduce SQA, a dataset that consists of 6,066 unique sequences of inter-related questions about Wikipedia tables, with 17,553 questions-answer pairs in total. To the best of our knowledge, SQA is the ﬁrst semantic parsing dataset that addresses sequential question answering, which is a more natural interface for information access.
The unique setting and task scenario deﬁned in SQA immediately triggers several interesting research questions, such as whether the simpler questions make the semantic parsing problem easier and

how should a system address the coreferences among questions and answers. Our preliminary experimental study found that existing systems do not perform well on SQA. Moreover, the potential of various kinds of question and table rewriting strategies for handling coreferences is hindered by semantic matching errors between question text and cells or column headers in the table. In the near future, we plan to resolve such errors by incorporating large external knowledge sources into semantic parsers. Longer-term, we hope that research on SQA will push towards more interactive settings where systems can ask users for clariﬁcations and incorporate user feedback into future predictions.
References
[Artzi and Zettlemoyer2011] Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping semantic parsers from conversations. In Proceedings of Empirical Methods in Natural Language Processing.
[Dye2016] John Dye. 2016. The creator of Siri showcases Viv, an impressive AI personal assistant. http://www.androidauthority.com/ dag-kittlaus-showcase-viv-691539/, May.
[Fader et al.2014] Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156– 1165. ACM.
[Frank2013] Stefan L Frank. 2013. Uncertainty reduction as a measure of cognitive load in sentence comprehension. Topics in Cognitive Science, 5(3).
[Hale2006] John Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4).
[Harabagiu et al.2005] Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005. Experiments with interactive question-answering. In Proceedings of the Association for Computational Linguistics.
[Kelly and Lin2007] Diane Kelly and Jimmy Lin. 2007. Overview of the trec 2006 ciqa task. In ACM SIGIR Forum, volume 41, pages 107–116. ACM.
[Kingma and Ba2014] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations.
[Lacatusu et al.2006] Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu. 2006. Impact of question decomposition on the quality of answer summaries. In International Language Resources and Evaluation.

[Levy2008] Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3).
[Liang2016] Percy Liang. 2016. Learning executable semantic parsers for natural language understanding. Commun. ACM, 59(9):68–76, August.
[Long et al.2016] Reginald Long, Panupong Pasupat, and Percy Liang. 2016. Simpler context-dependent logical forms via model projections. In Proceedings of the Association for Computational Linguistics.
[Neelakantan et al.2015] Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. 2015. Neural programmer: Inducing latent programs with gradient descent. In Proceedings of the International Conference on Learning Representations.
[Pasupat and Liang2015] Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semistructured tables. In Proceedings of the Association for Computational Linguistics.
[Yin et al.2016] Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao. 2016. Neural enquirer: Learning to query tables with natural language. In International Joint Conference on Artiﬁcial Intelligence.
[Zettlemoyer and Collins2009] Luke S Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Association for Computational Linguistics.
A Implementation Detail in NEURAL
We implement each of the three modules in NEURAL with soft attention mechanisms over columns, rows, and cells (mcol, mrow, and mcell, respectively). While they are functionally similar, each module differs from the others in both inputs and outputs. Before we present the equations deﬁning each module, we introduce some notation: say we have a r × c-dimensional table and an LSTM that encodes the question into a d-dimensional vector q. We use the same LSTM to encode the column headers into a c × d matrix h and the table cell entries into an r × c × d tensor t1. Similar to the neural enquirer, we add type information to the cell representations by computing a bilinear product with the column headers, ti,j = ReLu(hj W1t1i,j ). Before we can implement our modules, we also have to integrate the previous answer predictions (p1 of dimensionality r × c). We use a feedforward layer to determine how relevant the previous answers are to the current question: p = ReLu(W3p1 + W4q). Then, the table representation is updated with the ground-truth previous answers in a simple additive fashion: t = t + p ∗ t.
Our modules are deﬁned as follows:

mcol = softmax(hW5q),

mrow = σ(( ti,j )W6q),

(1)

j

mcell = σ(tW7q)

Note that mcol uses a softmax instead of a sigmoid; most of the questions in SQA have answers that come from just a single

column of the table, so the softmax function’s predisposition to select a single input is desirable here. Finally, we compute the ﬁnal answer predictions a by merging the module outputs with a soft attention mechanism that looks at the question to generate a three-dimensional vector, matt, where each dimension corresponds to the weight for one module.

matt = softmax(W8q),

(2)

ai,j =

matt ∗ [mcolj ; mrowi ; mcelli,j ]

The model parameters are optimized using Adam (Kingma and Ba, 2014); we train for 100 epochs and select the bestperforming model on the dev set. We set the dimensionality of our LSTM hidden state to d = 256 and the character embedding dimensionality to 100.

