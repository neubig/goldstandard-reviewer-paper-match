Practical Beneﬁts of Feature Feedback Under Distribution Shift
Anurag Katakkar*, Weiqin Wang*, Clay H. Yoo*, Zachary C. Lipton, Divyansh Kaushik Carnegie Mellon University
akatakka,weiqinw,hyungony,zlipton,dkaushik@cmu.edu

arXiv:2110.07566v1 [cs.CL] 14 Oct 2021

Abstract
In attempts to develop sample-efﬁcient algorithms, researcher have explored myriad mechanisms for collecting and exploiting feature feedback, auxiliary annotations provided for training (but not test) instances that highlight salient evidence. Examples include bounding boxes around objects and salient spans in text. Despite its intuitive appeal, feature feedback has not delivered signiﬁcant gains in practical problems as assessed on iid holdout sets. However, recent works on counterfactually augmented data suggest an alternative beneﬁt of supplemental annotations: lessening sensitivity to spurious patterns and consequently delivering gains in out-of-domain evaluations. Inspired by these ﬁndings, we hypothesize that while the numerous existing methods for incorporating feature feedback have delivered negligible in-sample gains, they may nevertheless generalize better out-of-domain. In experiments addressing sentiment analysis, we show that feature feedback methods perform signiﬁcantly better on various natural out-of-domain datasets even absent differences on in-domain evaluation. By contrast, on natural language inference tasks, performance remains comparable. Finally, we compare those tasks where feature feedback does (and does not) help.1
1 Introduction
Addressing various classiﬁcation tasks in natural language processing (NLP), including sentiment analysis (Zaidan et al., 2007), natural language inference (NLI) (DeYoung et al., 2020), and propaganda detection (Pruthi et al., 2020), researchers have introduced resources containing additional side information by tasking humans with marking spans in the input text (called rationales or feature feedback) that provide supporting evidence for the label. For example, spans like “underwhelming”, “horrible”, or “worst ﬁlm since Johnny English”
1AK, WW, and CY had equal contribution.

might indicate negative sentiment in a movie review. Conversely, spans like “exciting”, “amazing”, or “I never thought Vin Diesel would make me cry” might indicate positive sentiment.
These works have proposed a variety of strategies for incorporating feature feedback as additional supervision (Lei et al., 2016; Zhang et al., 2016; Lehman et al., 2019; Chen et al., 2019; Jain et al., 2020; DeYoung et al., 2020; Pruthi et al., 2020). Other researchers have studied the learningtheoretic properties of idealized forms of feature feedback (Poulis and Dasgupta, 2017; Dasgupta et al., 2018; Dasgupta and Sabato, 2020). We focus our study here on the resources and practical methods developed for NLP.
Some have used this feedback to perturb instances for data augmentation (Zaidan et al., 2007), while others have explored multitask objectives for simultaneously classifying documents and extracting rationales (Pruthi et al., 2020). A number of papers exploit feature feedback as intermediate supervision for building extract-then-classify pipelines (Chen et al., 2019; Lehman et al., 2019; Jain et al., 2020). One common assumption is that resulting models would learn to identify and rely more on spans relevant to the target labels, which would in turn lead to more accurate predictions.
However, despite their intuitive appeal, feature feedback methods have thus far yielded underwhelming results on independent drawn and identically distributed (iid) test sets in applications involving modern deep nets. While Zaidan et al. (2007) demonstrated signiﬁcant gains when incorporating rationales into their SVM learning scheme, beneﬁts have been negligible in the BERT era. For example, although Pruthi et al. (2020) and Jain et al. (2020) address a different aim—to improve extraction accuracy—their experiments show no improvement in classiﬁcation accuracy by virtue of incorporating rationales.
On the other hand, Kaushik et al. (2020), in-

troduced counterfactually augmented data (CAD) with the primary aim of showing how supplementary annotations can be incorporated to make models less sensitive to spurious patterns, and additionally demonstrated that models trained on CAD degraded less in a collection of out-of-domain tests than their vanilla counterparts. In followup work, they showed that for both CAD and feature feedback, although corruptions to evidence spans via random word ﬂips result in performance degradation both in- and out-of-domain, when nonevidence spans are corrupted, out-of-domain performance often improves (Kaushik et al., 2021). These ﬁndings echo earlier results in computer vision (Ross et al., 2017; Ross and Doshi-Velez, 2018) where regularizing input gradients (so-called local explanations) to accord with expert attributions led to improved out-of-domain performance.
In this paper, we conduct an empirical study of the out-of-domain beneﬁts of incorporating feature feedback in NLP. We seek to address two primary research questions: (i) do models that rely on feature feedback generalize better out of domain compared to classify-only models (i.e., models trained without feature feedback)? and (ii) do we need to solicit feature feedback for an entire dataset or can signiﬁcant beneﬁts be realized with a modest fraction of examples annotated? Our experiments on sentiment analysis (Zaidan et al., 2007) and NLI (DeYoung et al., 2020) use both linear, BERT (Devlin et al., 2019), and ELECTRA (Clark et al., 2020) models, using two feature feedback techniques (Pruthi et al., 2020; Jain et al., 2020).
We ﬁnd that sentiment analysis models ﬁnetuned with feature feedback on IMDb data see no improvement on in-domain accuracy. However, out-of-domain, sentiment analysis models beneﬁt signiﬁcantly from feature feedback. For example, ELECTRA and BERT models both see gains of ≈ 6% on both Amazon (Ni et al., 2019) and Yelp reviews (Kaushik et al., 2021) even when feature feedback is available for just 25% of instances. However, on NLI, we ﬁnd that both iid and out-ofdomain performance are comparable with or without feature feedback. We further ﬁnd that while for sentiment analysis, rationales constitute only ≈ 21% of all unique tokens in the training set, for NLI they constitute ≈ 80%, potentially helping to explain why feature feedback appears less useful there.

2 Methods and Datasets
Our study centers on two techniques, applied to two pretrained models, addressing one sentiment analysis dataset and one NLI dataset equipped with feature feedback.
Techniques for Incorporating Feature Feedback Our experiments employ a classify-andextract model (Pruthi et al., 2020) and an extractthen-classify model (Jain et al., 2020). Both these models have roughly achieved state-of-the-art classiﬁcation performance, with (Pruthi et al., 2020) achieving signiﬁcantly better extraction performance. In both cases, feature feedback annotations constitute the supervision for the extractive component. The classify-and-extract model jointly predicts the categorical target variable and performs sequence-tagging to extract rationales. The parameters of a pretrained language model such as BERT are shared between a classiﬁcation head and a linear chain CRF (Lafferty et al., 2001). The extractthen-classify method (Jain et al., 2020) ﬁrst trains a classiﬁer on complete examples to predict the categorical target variable. The top k% of input tokens are extracted from each sample, based on attention scores obtained from this classiﬁer.2 A second classiﬁer, called the extractor, is trained on continuous representations from the ﬁrst classiﬁer and makes token-level binary predictions to identify rationale tokens in the input. An additional binary cross-entropy term in the objective of the extractor is used to maximise agreement of the extracted tokens with human rationales. Finally, a third classiﬁer is trained to predict the target (sentiment or entailment) label based only on these extracted tokens.
For both approaches, we conduct experiments with two pretrained language models (BERT and ELECTRA). We limit the maximum sequence length to 512 tokens. All models are trained for 10 epochs using an AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 2e − 5 and a batch size of 8. Early stopping is based on mean of classiﬁcation and extraction F1 scores on the validation set. We replicate all experiments on 5 seeds and report mean performance along with standard deviation.
To verify that our hypothesis holds across differ-
2For both sentiment analysis and NLI, we choose k corresponding to the median fraction of tokens marked as rationales by humans in the respective in-sample datasets (≈ 8% and ≈ 43% respectively).

ent model architectures, we also use a linear SVM (Zaidan et al., 2007) with a modiﬁed objective function on top of the ordinary soft-margin SVM, i.e.,
1 ||w||2 + C( δi) + Ccontrast( ξij) 2 i i,j

subject to the constraints

w · xij · yi ≥ 1 − ξij ∀i, j

where xij

:=

xi −vij µ

are psuedoexamples con-

structed by subtracting contrast examples (vij), in-

put sentence void of randomly chosen rationales,

from the original input (xi). We use term-frequency

embeddings with unigrams appearing in at least 10

reviews as features and set C = Ccontrast = µ = 1.

For each sample in the train data, we generate 5

psuedoexamples.

Datasets For sentiment analysis, we use the IMDb movie reviews dataset introduced by Zaidan et al. (2007). Reviews in this dataset are labeled as having either positive or negative sentiment. Zaidan et al. (2007) also tasked annotators to mark spans in each review that were indicative of the overall sentiment. We use these spans as feature feedback. Overall, the dataset has 1800 reviews in the training set (with feature feedback) and 200 in test (without feature feedback). Since the test set does not include ground truth labels for evidence extraction, we construct a test set out of the 1800 examples in the original training set. This leaves 1200 reviews for a new training set, 300 for validation, and 300 for test. For NLI, we use a subsample of the E-SNLI dataset (DeYoung et al., 2020) used in Kaushik et al. (2021). In this dataset, there are 6318 premise-hypothesis pairs, equally divided across entailment and contradiction categories. Each pair is accompanied with marked tokens deemed relevant to the label’s applicability.

3 Experiments
We ﬁrst ﬁne-tune BERT and ELECTRA on a sentiment analysis dataset (Zaidan et al., 2007) following both classify-and-extract and extract-thenclassify approaches. We evaluate resulting models on both iid test set as well as various naturally occurring out-of-domain datasets for sentiment analysis and compare resulting performance with classify-only models (Table 3). We ﬁnd that both approaches lead to signiﬁcant gains in out-ofdomain performance compared to the classify-only

Test set
In-domain CRD SST2 Amazon Semeval Yelp
In-domain CRD SST2 Amazon Semeval Yelp

Classify-only Pruthi et al.

BERT

85.90.7 89.30.7 77.64.1 78.14.9 70.65.7 86.81.7

89.92.3 91.60.7 79.33.6 83.53.1 73.22.6 85.71.6

ELECTRA

93.20.3 91.60.4 73.21.3 72.82.0 67.54.5 79.03.6

91.81.4 93.70.9 74.01.2 75.52.1 72.51.8 84.61.8

Jain et al.
90.40.3 87.50.8 75.61.2 92.31.2 68.62.2 91.60.1
93.10.3 91.50.7 77.21.4 84.21.6 66.73.0 94.70.2

Table 1: Mean and standard deviation (in subscript) of accuracy scores of classify-only models, and models proposed by Pruthi et al. (2020) and Jain et al. (2020), ﬁned-tuned for sentiment analysis. Results highlighted in bold are signiﬁcant difference with p < 0.05.

method. For instance, ELECTRA ﬁne-tuned using the extract-then-classify framework leads to ≈ 15.7% gain in accuracy when evaluated on Yelp.
Since Pruthi et al. (2020) demonstrate better performance on evidence extraction for sentiment analysis compared to Jain et al. (2020), we rely on their method to conduct additional analysis. For both sentiment analysis and NLI, we ﬁne-tune models with varying proportion of samples with rationales and report iid and out-of-domain performance (Tables 9 and 10). We compare models trained with rationales against models trained without rationales and boldface the signiﬁcant differences with p-value < 0.05. Training with no feature feedback recovers the classify-only baseline. We evaluate on CRD (Kaushik et al., 2020), SST-2 (Socher et al., 2013), Amazon reviews (Ni et al., 2019), Semeval Tweets (Rosenthal et al., 2017) and Yelp reviews (Kaushik et al., 2021) for sentiment analysis, and Revised Premise (RP), Revised Hypothesis (RH) (Kaushik et al., 2020), MNLI matched (MNLI-M) and mismatched (MNLI-MM) (Williams et al., 2018) for NLI.
On sentiment analysis, we ﬁnd feature feedback to improve BERT’s iid performance but ﬁnd ELECTRA’s performance comparable with and without feature feedback. However, feature feedback leads to an increase in performance out-ofdomain on both BERT and ELECTRA. For instance, with feature feedback for all training examples, ELECTRA’s classiﬁcation accuracy increases from 91.6% to 93.7% on CRD and 79% to 84.6%

Evaluation Set
In-domain CRD SST-2 Amazon Semeval Yelp

300
77.03.9/77.62.2 48.02.9/56.41.3 52.21.6/62.91.0 51.81.5/65.91.9 50.31.4/56.71.1 60.24.0/72.02.4

Dataset size 600
78.53.2/82.32.0 48.32.5/58.02.7 50.93.0/64.00.9 52.42.0/66.51.2 50.31.2/56.40.8 57.37.1/74.51.5

900
80.51.7/84.91.6 48.42.3/58.71.8 51.33.1/64.90.9 52.02.9/69.90.4 50.10.5/58.81.3 61.24.6/74.82.5

1200
75.23.5/79.13.4 48.32.0/58.22.4 49.70.3/65.61.5 50.90.3/68.73.1 49.80.1/58.01.5 55.72.8/74.82.7

Table 2: Mean and standard deviation (in subscript) of accuracy scores of classify-only SVM models (left) presented alongside accuracy scores of models trained with feature feedback (right), with increasing number of training-samples for sentiment analysis using the method proposed by Zaidan et al. (2007). Results highlighted in bold show statistically signiﬁcant difference with p < 0.05.

Test set
In-domain RP RH MNLI-M MNLI-MM
In-domain RP RH MNLI-M MNLI-MM

Classify-only Pruthi et al.

BERT

88.72.0 62.93.9 76.93.5 69.72.6 71.52.7

89.80.8 66.60.6 80.51.9 68.11.9 69.22.3

ELECTRA

96.00.2 80.81.0 88.91.0 86.50.9 86.60.8

95.00.3 78.00.6 88.70.9 81.92.1 82.12.0

Jain et al.
77.70.1 57.90.1 70.70.2 69.80.1 66.20.1
85.40.04 72.20.1 79.70.1 77.10.1 75.70.1

Table 3: Mean and standard deviation (in subscript) of F1 scores of models ﬁne-tuned for NLI with increasing number of examples with feature feedback. Results highlighted in bold are signiﬁcantly better than classifyonly performance (p < 0.05).

on Yelp. Similar trends are also observed when we ﬁne-tune BERT with feature feedback. Interestingly, when evaluated on the SemEval dataset (Tweets), we observe that BERT ﬁne-tuned with feature feedback on all training examples achieves comparable performance to ﬁne-tuning without feature feedback. However, ﬁne-tuning with feature feedback on just 25% of training examples leads to a signiﬁcant improvement in classiﬁcation accuracy. We speculate that this might be a result of implicit hyperparameter tuning when combining prediction and extraction losses, and a more extensive hyperparameter search could provide comparable (if not better) gains with 100% data. For both BERT and ELECTRA, we also ﬁnd that ﬁne-tuning with feature feedback leads to a lower variance (across different random initializations) in model accuracy when evaluated out-of-domain. Similarly,

SVM trained with feature feedback (Zaidan et al., 2007) consistently outperformed an SVM classiﬁer trained without feature feedback when evaluated out-of-domain despite obtaining similar accuracy on the in-domain test set (Table 2). For instance, an SVM classiﬁer trained on just label information from 1200 training examples achieved 75.2% ± 3.5% accuracy on the in-domain test set, and SVM trained with label information along with feature feedback achieved comparable accuracy of 79.1% ± 3.4%. But the classiﬁer trained with feature feedback led to ≈ 19% improvement in classiﬁcation accuracy on Yelp reviews, and ≈ 18% improvement on Amazon Product reviews compared to the classiﬁer trained without feature feedback.
For NLI, it appears that feature feedback provides no added beneﬁt compared to a classify-only BERT model, whereas, ELECTRA’s iid performance decreases with feature feedback. Furthermore, models ﬁne-tuned with feature feedback generally perform no better than classify-only models when trained with varying proportions of rationales (Table 10) while classify-only models perform signiﬁcantly better than the models trained with rationales when trained with varying dataset size. (Appendix Table 11). These results are in line with observations in prior work on counterfactually augmented data (Huang et al., 2020).
4 Discussion and Analysis
To further analyze the different trends on sentiment analysis versus NLI, we analyze the marked feature feedback in both datasets. We ﬁnd that 21.37% of tokens in the vocabulary of Zaidan et al. (2007) are marked as rationales in at least one movie review. Interestingly, this fraction is 79.54% in the NLI training set (Table 4). Movie reviews might con-

Task
Sentiment Analysis NLI

Unigram
21.37 79.54

Bigram
11.20 35.49

Table 4: Percentage of unigram and bigram vocabularies that are marked as feature feedback at least once.

Dall Drationale

Entailment
0.25 0.30

Contradiction
0.16 0.09

Table 5: Mean Jaccard index of premise-hypothesis word overlap (Dall) and rationale overlap (Drationale) in the training set.

tain certain words or phrases that generally denote positive or negative sentiment (such as “amazing movie” or “waste of my time”). Whereas, for NLI tasks, there is no notion of such words or phrases that would suggest entailment or contradiction generally. Hence, in NLI datasets, it is likely that a word or a phrase might be marked as indicating entailment in one example but as contradiction in another.
Additionally, for sentiment analysis, we construct a vocabulary of unigrams and bigrams from phrases marked as feature feedback in examples from the training set (V rationale). We compute the fraction of unigrams (and bigrams) that occur in this vocabulary and also occur in unigram (and bigram) vocabularies of each out-of-domain dataset for sentiment analysis. We ﬁnd that a large fraction of unigrams from V rationale also exist in CRD (≈ 60%), SST2 (≈ 64%), and Yelp (≈ 78%) datasets (movie and restaurant reviews). However, this overlap is much smaller for Semeval (≈ 30%) and Amazon (≈ 45%) datasets, which consist of tweets and product reviews, respectively. For these overlapping unigrams, we observe a relatively large percentage (≈ 50–65%) preserve their associated training set label (the majority label) in the out-ofdomain dataset. Similar trends hold for bigrams as well, though fewer bigrams from V rationale are present in out-of-domain datasets (Appendix Table 12). A model that pays more attention to these spans might perform better out of domain.
Furthermore, for each pair in the NLI training set, we compute Jaccard similarity between the premise and hypothesis sentence (Table 5). We compute the mean of these example-level Jaccard indices

over the entire dataset, ﬁnding that it is common for examples in our training set to have overlap between premise and hypothesis sentences, regardless of the label. However, when we compute mean Jaccard similarity between premise and hypothesis rationales, we ﬁnd higher overlap for examples labeled as entailment (Jaccard index of 0.3) and a signiﬁcantly lower overlap for contradictory pairs (Jaccard index of 0.09). Thus, models trained with feature feedback might learn to identify word overlap as predictive of entailment even when the true label is contradiction. While this may not improve an NLI model’s performance, it could be useful in tasks like Question Answering, where answers often lie in sentences that have high word overlap with the question, as evidenced by prior work (Lamm et al., 2020; Majumder et al., 2021). Interestingly, our results on NLI are in conﬂict with recent ﬁndings where models trained with rationales showed signiﬁcant improvement over classify-only models in both iid and out-of-domain (MNLI-M and MNLI-MM) settings (Stacey et al., 2021). One explanation for these different ﬁndings could be the different modeling strategies employed in this paper and Stacey et al. (2021). While we guide models by extracting rationales, Stacey et al. (2021) do not build an extractor model, and the rationales are used to guide the training of the classiﬁer’s attention module. Investigating this difference is left for future work.
References
Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019. Seeing things from a different angle: Discovering diverse perspectives about claims. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (NAACL-HLT).
Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations (ICLR).
Sanjoy Dasgupta, Akansha Dey, Nicholas Roberts, and Sivan Sabato. 2018. Learning from discriminative feature feedback. In International Conference on Neural Information Processing Systems (NeurIPS).
Sanjoy Dasgupta and Sivan Sabato. 2020. Robust learning from discriminative feature feedback. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C Wallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. In Association for Computational Linguistics (ACL).
William Huang, Haokun Liu, and Samuel Bowman. 2020. Counterfactually-augmented snli training data does not yield better generalization than unaugmented data. In First Workshop on Insights from Negative Results in NLP.
Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C Wallace. 2020. Learning to faithfully rationalize by construction. In Association for Computational Linguistics (ACL).
Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2020. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations (ICLR).
Divyansh Kaushik, Amrith Setlur, Eduard Hovy, and Zachary C Lipton. 2021. Explaining the efﬁcacy of counterfactually-augmented data. International Conference on Learning Representations (ICLR).
John D Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML).
Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. 2020. Qed: A framework and dataset for explanations in question answering. arXiv preprint arXiv:2009.06354.
Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C Wallace. 2019. Inferring which medical treatments work from reports of clinical trials. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Empirical Methods in Natural Language Processing (EMNLP).
Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR).
Sagnik Majumder, Chinmoy Samant, and Greg Durrett. 2021. Model agnostic answer reranking system for adversarial question answering. In European Chapter of the Association for Computational Linguistics: Student Research Workshop (EACL SRW).

Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and ﬁne-grained aspects. In Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Stefanos Poulis and Sanjoy Dasgupta. 2017. Learning with feature feedback: from theory to practice. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS).
Danish Pruthi, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton. 2020. Weakly-and semisupervised evidence extraction. In Empirical Methods in Natural Language Processing (EMNLP).
Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. Semeval-2017 task 4: Sentiment analysis in twitter. In International Workshop on Semantic Evaluation (SemEval).
Andrew Ross and Finale Doshi-Velez. 2018. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In AAAI Conference on Artiﬁcial Intelligence.
Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. 2017. Right for the right reasons: training differentiable models by constraining their explanations. In International Joint Conference on Artiﬁcial Intelligence (IJCAI).
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP).
Joe Stacey, Yonatan Belinkov, and Marek Rei. 2021. Natural language inference with a human touch: Using human explanations to guide model attention. arXiv preprint arXiv:2104.08142.
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In Human language technologies: North American chapter of the association for computational linguistics (NAACL-HLT).
Ye Zhang, Iain Marshall, and Byron C Wallace. 2016. Rationale-augmented convolutional neural networks for text classiﬁcation. In Empirical Methods in Natural Language Processing (EMNLP).
A Appendix

Task Sentiment Analysis (Positive)

Examples
. . . characters are portrayed with such saddening realism that you can’t help but love them , as pathetic as they really are . although levy stands out , guest , willard , o’hara , and posey are all wonderful and definitely should be commended for their performances ! if there was an oscar for an ensemble performance , this is the group that should sweep it . . .

Sentiment Analysis (Negative)

. . . then , as it’s been threatening all along , the ﬁlm explodes into violence . and just when you think it’s ﬁnally over , schumacher tags on a ridiculous self-righteous ﬁnale that drags the whole unpleasant experience down even further . trust me . there are better ways to waste two hours of your life . . .

NLI (Entailment)

P: a white dog drinks water on a mountainside. H: there is a dog drinking water right now.

NLI (Contradiction)

P: a dog leaping off a boat H: dogs drinking water from pond

Table 6: Examples of documents (and true label) with feature feedback (highlighted in yellow).

Task Sentiment Analysis (Positive, Correct)

Examples
everyone should adapt a tom robbins book for screen . while the movie is ﬁne and the performances are good , the dialogue , which works well reading it , is beautiful when spoken .

Sentiment Analysis (Positive, Wrong)

... very uncaptivating yet one gets the feeling that their is some serious exploitation going on here ...

Sentiment Analysis (Negative, Correct) ... using quicken is a frustrating experience each time i ﬁre it up ...

Sentiment Analysis (Negative, Wrong)

... with many cringe-worthy ‘surprises’, which happen around 10 minutes after you see exactly what’s going to happen ...

NLI (Entailment, Correct)

P: a woman cook in an apron is smiling at the camera with two other cooks in the background . H: a woman looking at the camera .

NLI (Entailment, Wrong)

P: a woman in a brown dress looking at papers in front of a class . H: a woman looking at papers in front of a class is not wearing a blue dress .

NLI (Contradiction, Correct)

P: the woman in the white dress looks very uncomfortable in the busy surroundings H: the dress is black .

NLI (Contradiction, Wrong)

P: a man , wearing a cap , is pushing a cart , on which large display boards are kept , on a road . H: the person is pulling large display boards on a cart .

Table 7: Examples (from out-of-domain evaluation sets; with true label and model prediction) of explanations highlighted by feature feedback models (highlighted in yellow).

Task
Sentiment Analysis NLI

BERT
45.82.8 56.50.4

ELECTRA
51.70.7 59.20.9

Table 8: Rationale extraction F1 scores of BERT and ELECTRA models trained with 100% rationales. NLI results are for models trained with 6318 training samples.

Evaluation set
In-domain CRD SST2 Amazon Semeval Yelp
In-domain CRD SST2 Amazon Semeval Yelp

Fraction of Training Data with Rationales No rationales 25% 50% 75%

BERT

85.90.7 89.30.7 77.64.1 78.14.9 70.65.7 86.81.7

87.71.1 91.70.6 81.20.6 85.31.2 77.81.0 86.91.1

88.12.4 92.30.9 81.30.7 84.61.7 75.50.8 85.81.5

90.21.5 92.30.3 81.80.6 84.00.5 74.90.8 85.40.7

ELECTRA

93.20.3 91.60.4 73.21.3 72.82.0 67.54.5 79.03.6

92.40.9 92.10.8 73.11.8 79.01.8 70.51.5 84.51.1

92.81.2 93.00.6 72.31.6 75.71.2 66.21.5 84.21.7

93.71.9 93.10.3 72.31.1 76.61.8 67.12.2 84.31.2

100%
89.92.3 91.60.7 79.33.6 83.53.1 73.22.6 85.71.6
91.81.4 93.70.9 74.01.2 75.52.1 72.51.8 84.61.8

Table 9: Mean and standard deviation (in subscript) of accuracy scores of models ﬁne-tuned for sentiment analysis using the method proposed by Pruthi et al. (2020) with different base models (BERT and ELECTRA) and increasing proportion of examples with feature feedback. Results highlighted in bold are signiﬁcant difference with p < 0.05.

Evaluation set
In-domain RP RH MNLI-M MNLI-MM
In-domain RP RH MNLI-M MNLI-MM

Fraction of Training Data with Rationales No rationales 25% 50% 75%

BERT

88.72.0 62.93.9 76.93.5 69.72.6 71.52.7

89.60.4 67.62.0 80.41.1 67.63.4 68.84.5

89.90.4 67.41.2 81.71.6 68.14.6 69.25.9

89.70.4 68.60.6 81.40.7 68.82.0 69.82.7

ELECTRA

96.00.2 80.81.0 88.91.0 86.50.9 86.60.8

95.10.3 78.21.3 88.01.2 82.02.8 82.62.8

95.00.3 79.21.1 88.40.3 82.41.6 83.51.4

95.00.3 77.21.3 87.90.4 82.30.9 82.60.8

100%
89.80.8 66.60.6 80.51.9 68.11.9 69.22.3
95.00.3 78.00.6 88.70.9 81.92.1 82.12.0

Table 10: Mean and standard deviation (in subscript) of F-1 scores of models ﬁne-tuned for NLI using the method proposed by Pruthi et al. (2020) with different base models (BERT and ELECTRA) and increasing proportion of examples with feature feedback. Results highlighted in bold are signiﬁcant difference with p < 0.05.

Evaluation Set
In-domain RP RH MNLI-M MNLI-MM
In-domain RP RH MNLI-M MNLI-MM

1500
85.96.0/84.52.0 61.80.9/62.81.8 74.51.6/71.83.4 63.73.1/60.83.2 64.84.3/61.84.3
94.60.2/92.70.5 78.41.2/75.22.5 87.70.7/85.21.4 82.82.2/77.01.8 83.62.5/77.92.1

Dataset size 3000
BERT
87.90.4/87.71.0 63.31.6/64.21.8 77.01.4/77.32.1 69.21.8/66.32.2 71.32.3/67.52.8
ELECTRA
95.10.4/94.20.3 78.51.8/77.20.9 88.11.3/87.30.6 85.41.8/78.91.7 86.22.1/79.91.9

4500
89.10.4/89.20.2 63.71.8/66.81.4 78.31.1/80.41.8 70.20.9/67.53.1 72.11.2/68.94.2
95.70.2/94.40.2 81.20.6/76.21.2 89.40.6/87.11.0 86.01.6/80.42.1 86.11.8/80.82.2

6318
88.72.0/89.80.8 62.93.9/66.41.7 76.93.5/80.51.9 69.72.6/68.11.9 73.11.9/71.41.1
96.00.2/95.10.3 80.81.0/78.00.6 88.91.0/88.70.9 86.50.9/81.92.1 86.60.8/82.12.0

Table 11: Mean and standard deviation (in subscript) of F-1 scores of classify-only models/models trained with feature feedback, with increasing number of training-samples for NLI using the method proposed by Pruthi et al. (2020). Results highlighted in bold are statistically signiﬁcant difference with p < 0.05.

Dataset
CRD SST2 Amazon Semeval Yelp
CRD SST2 Amazon Semeval Yelp

% Overlap Label Agreement

Unigram

60.3

51.3

64.6

66.5

45.6

47.6

30.9

60.3

78.3

65.1

Bigram

28.2

51.9

28.5

64.5

19.6

49.9

10.2

58.5

46.8

65.3

Table 12: Rationale vocabulary overlap and label agreement between in-sample and OOD datasets.

