arXiv:2109.12286v1 [cs.LG] 25 Sep 2021

Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms
Liyuan Zheng1, Tanner Fiez1, Zane Alumbaugh2, Benjamin Chasnov1, and Lillian J. Ratliﬀ1
1University of Washington 2University of California, Santa Cruz
Abstract
The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation. We adopt this viewpoint and model the actor and critic interaction as a two-player general-sum game with a leader-follower structure known as a Stackelberg game. Given this abstraction, we propose a meta-framework for Stackelberg actor-critic algorithms where the leader player follows the total derivative of its objective instead of the usual individual gradient. From a theoretical standpoint, we develop a policy gradient theorem for the reﬁned update and provide a local convergence guarantee for the Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an empirical standpoint, we demonstrate via simple examples that the learning dynamics we study mitigate cycling and accelerate convergence compared to the usual gradient dynamics given cost structures induced by actor-critic formulations. Finally, experiments on OpenAI gym environments show that Stackelberg actor-critic algorithms always perform at least as well and often signiﬁcantly outperform the standard actor-critic algorithm counterparts.
1 Introduction
The algorithmic techniques for reinforcement learning can be classiﬁed into policy-based, value-based, and actor-critic methods (Sutton and Barto, 2018). Policy-based methods directly optimize a parameterized policy to maximize the expected return, while value-based methods estimate the expected return and then infer an optimal policy from the value-function by selecting the maximizing actions. Actor-critic methods bridge policy-based and value-based methods by learning the parameterized policy (actor) and the value-function (critic) together. In particular, actor-critic methods learn a critic that approximates the expected return of the actor while concurrently learning an actor to optimize the expected return based on the critic’s estimation.
In this paper, we adopt a game-theoretic perspective of actor-critic reinforcement learning algorithms. To provide some relevant background from game theory, recall that Stackelberg games are a class of games that describe interactions between a leader and a follower (Ba¸sar and Olsder, 1998). In a Stackelberg game, the leader is distinguished by the ability to act before the follower. As a result of this structure, the leader optimizes its objective accounting for the anticipated response of the follower, while the follower selects a best response to the leader’s action to optimize its own objective. The interaction between the actor and critic in reinforcement learning has an intrinsic hierarchical structure reminiscent of a Stackelberg game, which motivates our work to contribute a novel game-theoretic modeling framework along with theoretical and empirical results.
Modeling Contributions. We explicitly cast the interaction between the actor and critic as a two-player general-sum Stackelberg game toward solving reinforcement learning problems. Notably, this perspective deviates from the majority of work on actor-critic reinforcement learning algorithms, which implicitly neglect the interaction structure by independently optimizing the actor and critic objectives using individual gradient dynamics. In order to solve the game iteratively in a manner that reﬂects the interaction structure, we study
1

learning dynamics in which the player deemed the leader updates its parameters using the total derivative of its objective deﬁned using the implicit function theorem and the player deemed the follower updates using the typical individual gradient dynamics. We refer to this gradient-based learning method as the Stackelberg gradient dynamics. The designations of leader and follower between the actor and critic can result in distinct game-theoretic outcomes and we explore both choices and explain how the proper roles depend on the respective objective functions.
Theoretical Contributions. The Stackelberg gradient dynamics were previously studied in general nonconvex games and enjoy a number of theoretical guarantees (Fiez et al., 2020). In this paper we tailor the analysis of this learning dynamic to the reinforcement learning problem. To do this, we begin by developing a policy gradient theorem for the total derivative update (Theorem 1). Then, building oﬀ of this result, we develop a meta-framework of Stackelberg actor-critic algorithms. Speciﬁcally, this framework adapts the standard actor-critic, deep deterministic policy gradient, and soft-actor critic algorithms to be optimized using the Stackelberg gradient dynamics in place of the usual individual gradient dynamics. For the Stackelberg actor-critic algorithms this meta-framework admits, we prove local convergence (Theorem 2) to local Stackelberg equilibrium.
Experimental Contributions. From an empirical standpoint, we begin by pointing out in Section 3 that the objective functions in actor-critic algorithms commonly exhibit a type of hidden structure in terms of the parameters. Given this observation, we develop simple, yet illustrative examples comparing the behavior of Stackelberg actor-critic algorithms with standard actor-critic algorithms. In particular, we observe that the Stackelberg dynamics mitigate cycling in the parameter space and accelerate convergence. We discover from extensive experiments on OpenAI gym environments that similar observations carry over to complex problems and that our Stackelberg actor-critic algorithms always perform at least as well and often signiﬁcantly outperform the standard actor-critic algorithm counterparts.
2 Related Work
Game-theoretic frameworks have been studied extensively in reinforcement learning but mostly in multi-agent setting (Yang and Wang, 2020). In multi-agent reinforcement learning, the decentralized learning scheme is mostly adopted in practice (Zhang et al., 2021), where agents typically behave independently and optimize their own objective with no explicit information exchange. A shortcoming of this method is that agents fail to consider the learning process of other agents and simply treat them as a static component of the environment (Hernandez-Leal et al., 2017). To resolve this, several works design learning algorithms that explicitly account for the learning behavior of other agents (Zhang and Lesser, 2010; Foerster et al., 2018; Letcher et al., 2018), which is shown to improve learning stability and induce cooperation. In contrast, Prajapat et al. (2021) study a competitive policy optimization method for multi-agent reinforcement learning which performs recursive reasoning about the behavior of opponents to exploit them in two-player zero-sum games. Zhang et al. (2020) study multi-agent reinforcement learning problems, where each agent is using a typical actor-critic algorithm, with the twist that the follower’s policy takes the leader’s action as an input, which is used to approximate the potential best response. However, the procedure reduces to the usual actor-critic algorithm when applied to a single-agent reinforcement learning problem.
The past research taking a game-theoretic viewpoint of single-agent reinforcement learning is limited despite the fact that there is often implicitly multiple players in reinforcement learning algorithms. Rajeswaran et al. (2020) propose a framework that casts model-based reinforcement learning as a two-player general-sum Stackelberg game between a policy player and a model player. However, they only consider optimizing the objective of each player using the typical individual gradient dynamics with timescale separation as an approximation to Stackelberg gradient dynamics. Concurrent with this work, Wen et al. (2021) show that Stackelberg policy gradient recovers the standard policy gradient under certain strong assumptions, including that the critic is directly parameterized by the Q-value function. Hong et al. (2020) analyze the Stackelberg gradient dynamics with timescale separation for bilevel optimization with application to reinforcement learning. For reinforcement learning, they give a convergence guarantee for an actor-critic algorithm under assumptions such as exact linear function approximation which result in the total derivative being equivalent to the
2

individual gradient. We provide a complimentary study by developing a general framework for Stackelberg actor-critic algorithms that we analyze without such assumptions and also extensively evaluate empirically on reinforcement learning tasks.

3 Motivation & Preliminaries
In this section, we begin by presenting background on Stackelberg games and the relevant equilibrium concept. Then, to motivate and illustrate the utility of Stackelberg-based actor-critic algorithms, we highlight a key hidden structure that exists in actor-critic objective formulations and explore the behavior of Stackelberg gradient dynamics in comparison to individual gradient dynamics given this design. Finally, we provide the necessary mathematical background and formalism for actor-critic reinforcement learning algorithms.

3.1 Game-Theoretic Preliminaries
A Stackelberg game is a game between two agents where one agent is deemed the leader and the other the follower. Each agent has an objective they want to optimize that depends on not only their own actions but also on the actions of the other agent. Speciﬁcally, the leader optimizes its objective under the assumption that the follower will play a best response. Let f1(x1, x2) and f2(x1, x2) be the objective functions that the leader and follower want to minimize, respectively, where x1 ∈ X1 ⊆ Rd1 and x2 ∈ X2 ⊆ Rd2 are their decision variables or strategies and x = (x1, x2) ∈ X1 × X2 is their joint strategy. The leader and follower aim to solve the following problems:

minx1∈X1 {f1(x1, x2) x2 ∈ arg miny∈X2 f2(x1, y)}, minx2∈X2 f2(x1, x2).

( L) ( F)

Since the leader assumes the follower chooses a best response x∗2(x1) = arg miny f2(x1, y),1 the follower’s decision variables are implicitly a function of the leader’s. In deriving suﬃcient conditions for the optimization
problem in (L), the leader utilizes this information by the total derivative of its cost function which is given
by ∇f1(x1, x∗2(x1)) = ∇1f1(x) + (∇x∗2(x1)) ∇2f1(x).

where ∇x∗2(x1) = −(∇22f2(x))−1∇21f2(x). 2

Hence, a point x = (x1, x2) is a local solution to (L) if ∇f1(x1, x∗2(x1)) = 0 and ∇2f1(x1, x∗2(x1)) > 0. For

the

follower’s

problem,

suﬃcient

conditions

for

optimality

are

∇2f2(x1, x2)

=

0

and

∇

2 2

f

2

(

x

1

,

x2

)

>

0.

This

gives rise to the following equilibrium concept which characterizes suﬃcient conditions for a local Stackelberg

equilibrium.

Deﬁnition

1

(Diﬀerential

Stackelberg

Equilibrium,

Fiez

et

al.

2020).

The

joint

strategy

x∗

=

(x

∗ 1

,

x∗2

)

∈

X1 × X2 is a diﬀerential Stackelberg equilibrium if ∇f1(x∗) = 0, ∇2f2(x∗) = 0, ∇2f1(x∗) > 0, and

∇22f2(x∗) > 0.

The Stackelberg learning dynamics derive from the ﬁrst-order gradient-based suﬃcient conditions and are given by

x1,k+1 = x1,k − α1∇f1(x1,k, x2,k) x2,k+1 = x2,k − α2∇2f2(x1,k, x2,k)

where αi, i = 1, 2 are the leader and follower learning rates.
1Under suﬃcient regularity conditions on the follower’s optimization problem, the best response map is a singleton. This is a generic condition in games (Ratliﬀ et al., 2014; Fiez et al., 2020).
2The partial derivative of f (x1, x2) with respect to the xi is denoted by ∇if (x1, x2) and the total derivative of f (x1, h(x1)) for some function h, is denoted ∇f where ∇f (x1, h(x1) = ∇1f (x1, h(x1)) + (∇h(x1)) ∇2f (x1, h(x1)).

3

w w
w w

1.0

0.5

0.0

0.5

1.0 1

0

1

(a) Individual gradient

1.0

0.5

0.0

0.5

1.0 1

0

1

(b) Stackelberg gradient

100

Individual grad

Stackelberg grad

Stackelberg w/ reg 10 1

10 2

0 10000 20000 30000 40000 50000
Iterations
(c) Error to equilibrium

Figure 1: (a)–(b) Vector ﬁelds and trajectories of the actor and critic updates using individual gradient and Stackelberg gradient. (c) Error w − w∗ 2 + θ − θ∗ 2 for individual gradient, Stackelberg gradient, and Stackelberg gradient with regularization, where (θ∗, w∗) = (0, 0).

1.0

1.0

0.5

0.5

0.0

0.0

0.5

0.5

1.0 1

0

1 1.0 1

0

1

(a) Individual gradient

(b) Stackelberg gradient

Figure 2: (a)–(b) Individual gradient and Stackelberg gradient with entropic regularization in actor objective.

3.2 Motivating Examples
In the next section we present several common actor-critic formulations including the “vanilla” actor-critic, deep deterministic policy gradient, and soft actor-critic. A common theme among them is that the actor and critic objectives exhibit a simple hidden structure in the parameters. In particular, the actor objective typically has a hidden linear structure in terms of the parameters θ which is abstractly of the form Qw(θ) = w µ(θ). Analogously, the critic objective usually has a hidden quadratic structure in the parameters w which is abstractly of the form or (R(θ) − Qw(θ))2. The terminology of hidden structure in this context refers to the fact that the speciﬁed structure appears when the functions transforming the parameters are removed.3 Interestingly, similar observations have been made regarding generative adversarial network formulations and exploited to gain insights into gradient learning dynamics for optimizing them (Vlatakis-Gkaragkounis et al., 2019; Flokas et al., 2021).
Based on this observation, we investigate simple, yet illustrative reinforcement learning problems with the aforementioned structure and compare and contrast the behavior of the Stackelberg gradient dynamics with the usual individual gradient dynamics. As we demonstrate later in Section 5, the insights we uncover from this study generally carry over to complex reinforcement learning problems.
Example. Consider a single step Markov decision process where the reward function is given by R(θ) = − 15 θ2 and θ ∈ [−1, 1] is the decision variable of actor. Suppose that the critic is designed using the most basic linear function approximation Qw(θ) = wθ with w ∈ [−1, 1]. The actor seeks to ﬁnd the action that maximizes the
3The actor and critic functions could be approximated by neural nets in practice but we consider the simplest linear case, which captures the hidden structure and gives insights for general cases.

4

value indicated by the critic and the critic approximates the rewards of actions generated by the actor. Thus, the actor has objective J(θ, w) = Qw(θ) = wθ and the critic has objective L(θ, w) = Eθ∼ρ[(R(θ) − Qw(θ))2]. For simplicity, we assume the critic only minimizes the mean square error of the sample action generated by current actor θ. The critic objective is then L(θ, w) = (R(θ) − Qw(θ))2 = (w · θ + 15 θ2)2.
Actor-Critic & Deep Deterministic Policy Gradient. The structure of this example closely mirrors the hidden structure of both the “vanilla” actor-critic and deep deterministic policy gradient formulations as described in the next section. The typical way to optimize the objectives is by performing individual gradient dynamics (gradient descent on each cost) on the actor and critic parameters. Figure 1(a) shows the gradient vector ﬁeld and the parameter trajectories under the individual gradient dynamics. We observe that although the trajectory eventually converges to the equilibrium point (θ∗, w∗) = (0, 0), it cycles signiﬁcantly. Figure 1(b) shows the vector ﬁeld and parameter trajectories under the Stackelberg gradient dynamics, the details of which will be introduced in Section 4. We observe that the cycling behavior is completely eliminated as a result of the consideration given to the interaction structure. Figure 1(c) shows the error to equilibrium w − w∗ 2 + θ − θ∗ 2 for the individual gradient dynamics and the Stackelberg gradient dynamics along with a regularized version introduced in Section 4.5. This highlights that cycling is mitigated and convergence accelerated by optimizing using the Stackelberg gradient.

Soft Actor-Critic. The soft actor-critic algorithm also exhibits a similar structure, but with entropic regularization included in the actor objective. We show the vector ﬁelds along with the parameter trajectories for the individual gradient dynamics and the Stackelberg gradient dynamics in Figure 2(a) and Figure 2(b), respectively. Given the entropic regularization, both learning algorithms behave similarly. This perhaps indicates that the individual gradient dynamics are more well-suited to optimize this form of objectives and highlights the importance of considering how game dynamics perform on types of hidden structures when optimizing actor-critic algorithms in reinforcement learning.
Further details on the examples in this section are provided in Appendix A. Importantly, regardless of the objective function structure, the Stackelberg gradient dynamics tend to converge rather directly to the equilibrium and for some hidden structures they signiﬁcantly mitigate oscillations and stabilize training. It is well-known that this is a desirable property of the reinforcement learning algorithms owing to the implications for both evaluation and real-world applications (Chan et al., 2019). Together, this motivating section suggests that introducing the Stackelberg dynamics as a “meta-algorithm” on existing actor-critic methods is likely to lead to more favorable convergence properties. We demonstrate this empirically in Section 5, while now we introduce actor-critic algorithms.

3.3 Actor-Critic Algorithms

We consider discrete-time Markov decision processes (MDPs) with continuous state space S and continuous

action space A. We denote the state and action at time step t by st and at, respectively. The initial state s0

is determined by the initial state density s0 ∼ ρ(s). At time step t, the agent in state st takes an action at

according to a policy at ∼ π(·|st) and obtains a reward rt = r(st, at). The agent then transitions to state

st+1 determined by the transition function st+1 ∼ P (s |st, at). A trajectory τ = (s0, a0, . . . , sT , aT ) gives the

cumulative rewards or return deﬁned as R(τ ) =

T t=0

γtr(st,

at),

where

the

discount

factor

0

<

γ

≤

1

assigns

weights to rewards received at diﬀerent time steps. The expected return of π after executing at in state st

can be expressed by the Q function

Qπ(st, at) = Eτ∼π

T t =t

γt

−tr(st

, at

)|st, at

.

Correspondingly, the expected return of π in state st can be expressed by the value function V deﬁned as

V π(st) = Eτ∼π

T t =t

γt

−tr(st

, at

)|st

.

5

The goal of reinforcement learning is to ﬁnd an optimal policy that maximizes the expected return which is given by

J (π) = Eτ∼π

T t=0

γ

t

r(st

,

at

)

=

τ p(τ |π)R(τ )dτ

= Es∼ρ,a∼π(·|s) Qπ(s, a) ,

where p(τ |π) = ρ(s0)

T t=0

π(at

|st

)P

(st+1

|st

,

at

).

The policy-based approach (Williams, 1992) parameterizes the policy π by the parameter θ and ﬁnds the

optimal parameter choice θ∗ by maximizing the expected return

J (θ) = Es∼ρ,a∼πθ(·|s) Qπ(s, a) . (1)

This optimization problem can be solved by gradient ascent. By the policy gradient theorem (Sutton et al.,

2000),

∇θJ (θ) = Es∼ρ,a∼πθ(·|s) [∇θ log πθ(a|s)Qπ(s, a)] ,

where ∇θ denotes the derivative with respect to θ. A common method to approximate Qπ(s, a) in the policy

gradient is by sampling trajectories and averaging returns, which is known as REINFORCE (Williams, 1992).

“Vanilla” Actor-Critic (AC). The actor-critic method (Konda and Tsitsiklis, 2000; Grondman et al.,
2012) relies on a critic function Qw(s, a) parameterized by w to approximate Qπ(s, a). By replacing Qw(s, a) with Qπ(s, a) in (1), the actor which is parameterized by θ has the objective

J (θ, w) = Es∼ρ,a∼πθ(·|s) Qw(s, a) .

(2)

The objective is optimized using gradient ascent where

∇θJ (θ, w) = Es∼ρ,a∼πθ(·|s)[∇θ log πθ(a|s)Qw(s, a)].

(3)

The critic which is parameterized by w has the objective to minimize the mean square error between the Q-functions
L(θ, w) = Es∼ρ,a∼πθ(·|s)[(Qw(s, a) − Qπ(s, a))2], (4) where the function Qπ(s, a) is approximated by Monte Carlo estimation or bootstrapping (Sutton and Barto, 2018).
The actor-critic method optimizes the objectives with individual gradient dynamics (Peters and Schaal, 2008; Mnih et al., 2016) which gives rise to the updates

θ ← θ + αθ∇θJ (θ, w),

(5)

w ← w − αw∇wL(θ, w),

(6)

where αθ and αw are the learning rates of actor and critic. Clearly, even in this basic actor-critic method, the actor and critic are coupled since J and L depend on both θ and w, which naturally lends to a game-theoretic interpretation.

Deep Deterministic Policy Gradient (DDPG). The DDPG algorithm (Lillicrap et al., 2016) is an oﬀ-policy method with subtly diﬀerent objective functions for the actor and critic. In particular, the formulation has a deterministic actor µθ(s) : S → A with the objective

J (θ, w) = Eξ∼D [Qw(s, µθ(s))] .

(7)

The critic objective is the mean square Bellman error

L(θ, w) = E [(Qw(s, a) − (r + γQ0(s , µθ(s ))))2],

(8)

ξ∼D

where ξ = (s, a, r, s ), D is a replay buﬀer, and Q0 is a target Q network.4

4In the DDPG algorithm, the next-state actions used in the target network come from the target policy instead of the current policy. To be consistent with SAC, we use the current policy.

6

Algorithm 1: Stackelberg Actor-Critic Framework
Input: actor-critic algorithm ALG, player designations, and learning rate sequences αθ,k, αw,k. if actor is leader, update actor and critic in ALG with:

θk+1 = θk + αθ,k∇J (θk, wk)

(11)

wk+1 = wk − αw,k∇wL(θk, wk)

(12)

if critic is leader, update actor and critic in ALG with:

θk+1 = θk + αθ,k∇θJ (θk, wk) wk+1 = wk − αw,k∇L(θk, wk)

(13) (14)

Soft Actor-Critic (SAC). The SAC algorithm (Haarnoja et al., 2018) exploits the double Q-learning trick (Van Hasselt et al., 2016) and employs entropic regularization to encourage exploration. The actor’s objective J(θ, w) is

Eξ∼D min Qwi (s, aθ(s)) − η log(πθ(aθ(s)|s)) ,

(9)

i=1,2

where aθ(s) is a sample from πθ(·|s) and η is entropy regularization coeﬃcient. The parameter of the critic is the union of both Q networks parameters w = {w1, w2} and the critic objective is deﬁned correspondingly by

L(θ, w) = Eξ∼D i=1,2 (Qwi (s, a) − y(r, s ))2 ,

(10)

where

y(r, s ) = r + γ( min Q0,i(s , aθ(s )) − η log(πθ(aθ(s )|s ))).
i=1,2

The target networks in DDPG and SAC are updated by taking the Polyak average of the network parameters over the course of training, and the actor and critic networks are updated by individual gradient dynamics identical to (5)–(6).

4 Stackelberg Framework
In this section, we begin by formulating the actor-critic interaction as two-player general-sum Stackelberg game and introduce a Stackelberg framework for actor-critic algorithms, under which we develop novel Stackelberg versions of existing algorithms: Stackelberg actor-critic (STAC), Stackelberg deep deterministic policy gradient (STDDPG), and Stackelberg soft actor-critic (STSAC). Following this, we give a local convergence guarantee for the algorithms to a local Stackelberg equilibrium. Finally, a regularization method for practical usage of the algorithms is discussed.

4.1 Meta-Algorithm

Given an actor-critic formulation, in particular, the objectives of the actor and critic deﬁned by J(θ, w) and L(θ, w), we can interpret the problem as a two-player general-sum Stackelberg game. If we view the actor as the leader and the critic as a follower, then the players aim to solve the following optimization problems, respectively:

maxθ{J(θ, w∗(θ)) w∗(θ) = arg minw L(θ, w )} minw L(θ, w).

(AL) (CF)

On the other hand, if we view the critic as the leader and the actor as the follower, then the players aim to solve the following optimization problems, respectively:

minw{L(θ∗(w), w) θ∗(w) = arg maxθ J(θ , w)} maxθ J(θ, w).

(CL) (AF)

7

As described in Section 3.1, we propose to optimize the objectives using a learning algorithm that accounts

for the structure of the problems. Speciﬁcally, since the leader assumes the follower selects a best response, it

is natural to optimize the leader objective by following the total derivative given that the follower’s decision is

implicitly a function of the leader’s. The meta-framework we adopt for Stackelberg reﬁnements of actor-critic

methods is in Algorithm 1. The distinction compared to the usual actor-critic methods is that in the updates

we replace the individual gradient for the leader by the implicitly deﬁned total derivative which accounts for

the interaction structure whereas the rest of the actor-critic method remains identical.

The dynamics with the actor as the leader are given by (11)–(12) where the actor’s total derivative J(θ, w)

is

∇θJ (θ, w) − ∇wθL(θ, w)(∇2wL(θ, w))−1∇wJ (θ, w).

(15)

When the critic is the leader the dynamics are given by (13)–(14) where the critic’s total derivative ∇L(θ, w)

is

∇wL(θ, w) − ∇θwJ (θ, w)(∇2θJ (θ, w))−1∇θL(θ, w).

(16)

We now consider instantiations of this framework and explain how the total derivative can be obtained from sampling along with natural choices of leader and follower.

4.2 Stackelberg “Vanilla” Actor-Critic
We start by instantiating the Stackelberg meta-algorithm for the “vanilla” actor-critic (AC) algorithm for which the actor and critic objectives are given in (2) and (4), respectively.5 In this on-policy formulation, the critic assists the actor in learning the optimal policy by approximating the value function of the current policy. To give an accurate approximation, the critic aims to be selecting a best response w∗(θ) = arg minw L(θ, w ). Thus, the actor naturally plays the role of leader and the critic the follower.
However, estimating the total derivative ∇J(θ, w) as deﬁned in (15) is not straightforward and we analyze each component individually. The individual gradient ∇θJ(θ, w) can be computed by policy gradient theorem as given in (3). Moreover, ∇wJ (θ, w) = Es∼ρ,a∼πθ(·|s)[∇wQw(s, a)], which follows by direct computation, and similarly
∇2wL(θ, w) = Es∼ρ,a∼πθ(·|s) 2∇wQw(s, a)∇w Qw(s, a) +2(Qw(s, a) − Qπ(s, a))∇2wQw(s, a) .

To compute ∇wθL(θ, w) in (15), we begin by obtaining ∇θL(θ, w) with the following policy gradient theorem. The proof of Theorem 1 is in Appendix B.
Theorem 1. Given an MDP and actor-critic parameters (θ, w), the gradient of L(θ, w) with respect to θ is given by

∇θL(θ, w) = Eτ∼πθ [∇θ log πθ(a0|s0)

(Qw(s0, a0) − Qπ(s0, a0))2 +

T t=1

γt∇θ

log

πθ

(at|st)

(Qπ(s0, a0) − Qw(s0, a0))Qπ(st, at)].

Theorem 1 allows us to compute ∇θwL(θ, w) directly by ∇w(∇θL(θ, w)) since the distribution of ∇θL(θ, w) does not depend on w and ∇w can be moved into the expectation.
The critic in AC is often designed to approximate the state value function V π(s) which has computational advantages, and the policy gradient can be computed by advantage estimation (Schulman et al., 2015b). In this formulation, J (θ, w) = Eτ∼πθ r(s0, a0) + Vw(s1) and L(θ, w) = Es∼ρ[(Vw(s) − V π(s))2]. Then ∇θL(θ, w) can be computed by the next proposition that is derived in Appendix C.
5We only demonstrate the “vanilla” actor-critic algorithm and its Stackelberg version here and in our experiments, but the framework could be generalized to more on-policy actor-critic algorithms (e.g., A2C, A3C, Mnih et al. 2016).

8

Proposition 1. Given an MDP and actor-critic parameters (θ, w), if the critic has the objective function L(θ, w) = Es∼ρ[(Vw(s) − V π(s))2], then ∇θL(θ, w) is given by
T
E [2 γt∇θ log πθ(at|st)(V π(s0) − Vw(s0))Qπ(st, at)].
τ ∼πθ t=0
Given these derivations, terms in (15) can be estimated by sampled trajectories, and STAC updates using (11)–(12).

4.3 Stackelberg DDPG and SAC
In comparison to on-policy methods where the critic is designed to evaluate the actor using sampled trajectories generated by the current policy, in oﬀ-policy methods the critic minimizes the Bellman error using samples from a replay buﬀer. Thus, the leader and follower designation between the actor and critic in oﬀ-policy methods is not as clear. To this end, we propose variants of STDDPG and STSAC where the leader and follower order can be switched. Given the actor as the leader (AL), the algorithms are similar to policy-based methods, where the critic plays an approximate best response to evaluate the current actor. On the other hand, given the critic as the leader (CL), the actor plays an approximate best response to the critic value, resulting in behavior closely resembling that of the value-based methods.
As shown in (7)–(8) for DDPG and (9)–(10) for SAC, the objective functions of oﬀ-policy methods are deﬁned in expectation over an arbitrary distribution from a replay buﬀer instead of the distribution induced by the current policy. Thus, each terms in the total derivatives updates in (15) and (16) can be computed directly and estimated by samples. Then, STDDPG and STSAC update using (11)–(12) or (13)–(14) depending on the choices of leader and follower.

4.4 Convergence Guarantee
Consider, without loss of generality, the actor is designated as the leader and the critic the follower. Then, the actor and critic updates with the Stackelberg gradient dynamics and learning rates sequences {αθ,k}, {αw,k} are of the form

θk+1 = θk + αθ,k(∇J (θ, w) + θ,k+1), wk+1 = wk − αw,k(∇wL(θ, w) + w,k+1),

(17) (18)

where { θ,k+1}, { w,k+1} are stochastic processes. The results in this section assume the following.
Assumption 1. The maps ∇J : Rm → Rmθ , ∇wL : Rm → Rmw are Lipschitz, and ∇J < ∞. The learning rate sequences are such that αθ,k = o(αw,k) and k αi,k = ∞, k αi2,k < ∞ for i ∈ I = {θ, w}. The noise processes { i,k} are zero mean, martingale diﬀerence sequences: given the ﬁltration Fk = σ(θs, ws, θ,s, w,s, s ≤ k), { i,k}i∈I are conditionally independent, E[ i,k+1| Fk] = 0 a.s., and E[ i,k+1 | Fk] ≤ ci(1 + (θk, wk) ) a.s. for some constants ci ≥ 0 and i ∈ I.
The following result gives a local convergence guarantee to a local Stackelberg equilibrium under the assumptions and the proof is in Appendix D. For this result, recall that for a continuous-time dynamical system of the form z˙ = −g(z), a stationary point z∗ of the system is said to be locally asymptotically stable or simply stable if the spectrum of the Jacobian denoted by −Dg(z) is in the open left half plane.

Theorem 2. Consider an MDP and actor-critic parameters (θ, w). Given a locally asymptotically stable diﬀerential Stackleberg equilibrium (θ∗, w∗) of the continuous-time limiting system (θ˙, w˙ ) = (∇J(θ, w), −∇wL(θ, w)), under Assumption 1 there exists a neighborhood U for which the iterates (θk, wk) of the discrete-time system in (17)–(18) converge asymptotically almost surely to (θ∗, w∗) for (θ0, w0) ∈ U .
This result is eﬀectively giving the guarantee that the discrete-time dynamics locally converge to a stable, game theoretically meaningful equilibrium of the continuous-time system using stochastic approximation methods given proper learning rates and unbiased gradient estimates (Borkar, 2009).

9

Average Return Average Return

Average Return

200

150

100

20

50

2

4

6

8

Time Steps 1e540

STAC, m = 1
10

STAC, m = 80

20

30

40

50

0.5 1.0 1.5 2.0

Time Steps 1e6

Average Return

AC, m = 1
150

AC, m = 80

100

50

0

0.5 1.0 1.5 2.0

Time Steps 1e6

Average Return

125 100 75 50 25
0 0.5 1.0 1.5 2.0
Time Steps 1e6

Average Return Average Return

Average Return

(a) CartPole
10000

(b) Reacher
12
Time Stp1ees6 STDDPG-AL
125

(c) Hopper

STDDPG-CL
2500

DDPG

Average Return

7500

10010000

5000

75

2500

50

0

1

2

3

Time Steps 1e5

500250

1

2

3

Time Steps 1e5

2000

1500

1000

500

0

1

2

3

Time Steps 1e5

Average Return

(d) Walker2d

1500

1000

500

0

1

2

3

Time Steps 1e5

(e) HalfCheetah

0 (f) Swimmer

(g) Hopper

(h) Walker2d

Average Return Average Return

Average Return

Time Stpes 1

2

STSAC-AL

3 STSAC-C1Le5 SAC

10000

40

3000

4000

Average Return

Average Return

7500

5000

2500

0

1

2

3

Time Steps 1e5

(i) HalfCheetah

1030000
20

710500

1

2

3

5000 Time Steps 1e5

(j) Swimmer

2000

1000

0

1

2

3

Time Steps 1e5

(k) Hopper

3000

2000

1000

0

1

2

3

Time Steps 1e5

(l) Walker2d

Figure 3: Comparison of AC2,50D0DPG, SAC with their Stackelberg versions on OpenAI gym environments.

0

4.5 Implicit Map RegularTizi1maeti2oSntp3e1se5
The total derivative in the Stackelberg gradient dynamics requires computing the inverse of follower Hessian

∇22f2(x). Since critic networks in practical reinforcement learning problems may be highly non-convex,

(∇22f2(x))−1 can be ill-conditioned. Thus, instead of computing this term directly in the Stackelberg actor-

critic

algorithms,

we

compute

a

regularized

variant

of

the

form

(∇

2 2

f2

(

x)

+

λI

)

−

1

.

This

regularization

method

can

be

interpreted

as

the

leader

viewing

the

follower

as

optimizing

a

regularized

cost

f2(x)

+

λ 2

x2

2, while

the follower actually optimizes f2(x). The regularization λ can interpolate between the Stackelberg and

individual gradient updates for the leader as we now formalize.

Proposition 2. Consider a Stackelberg game where the leader updates using the regularized total derivative ∇λf1(x) = ∇1f1(x) − ∇21f2(x)(∇22f2(x) + λI)−1∇2f1(x). As λ → 0 then ∇λf1(x) → ∇f1(x) and when λ → ∞ then ∇λf1(x) → ∇1f1(x).

5 Experiments
We now show the results of extensive experiments comparing the Stackelberg actor-critic algorithms with the comparable actor-critic algorithms. We ﬁnd that the actor-critic algorithms with the Stackelberg gradient dynamics always perform at least as well and often signiﬁcantly outperform the standard gradient dynamics. Moreover, we provide game-theoretic interpretations of the results.
We run experiments on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics simulator (Todorov et al., 2012). The performance of each algorithm is evaluated by the average episode
10

return versus the number of time steps (state transitions after taking an action according to the policy). For a fair comparison, the hyper-parameters for the actor and critic including the neural network architectures are set equal when comparing the Stackelberg actor-critic algorithms with the stand normal actor-critic algorithms. The implementation details are in Appendix E, and importantly, the Stackelberg actor-critic algorithms are not signiﬁcantly more computationally expensive than the normal algorithms.
Performance. Figures 3(a)–3(d) show the performance of STAC and AC on several tasks. We also experiment with the common heuristic of “unrolling” the critic m steps between actor steps. For each task, STAC with multiple critic unrolling steps performs the best. This is due to the fact when the critic is closer to the best response, then the real response of the critic is closer to what is anticipated by the Stackelberg gradient for the actor. Interestingly, in CartPole, STAC with m = 1 performs even better than AC with m = 80.
Figures 3(e)–3(h) show the performance of STDDPG-AL and STDDPG-CL in comparison to DDPG. We observe that on each task, STDDPG-AL outperforms DDPG by a clear margin, whereas STDDPG-CL has overall better performance than DDPG except on Walker2d. Figures 3(i)–3(l) show the performance of STSAC-AL and STSAC-CL in comparison to SAC.
In all experiments, when the actor is the leader, the Stackelberg versions either outperform or are comparable to the existing actor-critic algorithms, oﬀering compelling evidence that the Stackelberg framework has an empirical advantage in many tasks and settings. We now provide game-theoretic interpretations of the experimental results and connect back to the examples and observations from Section 3.2.
Game-Theoretic Interpretations. SAC is considered the state-of-the-art model-free reinforcement learning algorithm and we observe it signiﬁcantly outperforms DDPG (e.g., on Hopper and Walker2d). The common interpretation of its advantage is that SAC encourages exploration by penalizing low entropy policies. Here we provide another viewpoint.
From a game-theoretic perspective, the objective functions of AC and DDPG take on hidden linear and hidden quadratic structures for the actor and critic. This structure can result in cyclic behavior for individual gradient dynamics as shown in Section 3.2. SAC constructs a more well-conditioned game structure by regularizing the actor objective, which leads to the learning dynamics converging more directly to the equilibrium as seen in Section 3.2. This also explains why we observe improved performance with STAC and STDDPG-AL compared to AC and DDPG, but the performance gap between STSAC-AL and SAC is not as signiﬁcant.
Comparing AL with CL, the actor as the leader always outperforms the critic as the leader in our experiments. As described in Section 3.2, the critic objective is typically a quadratic mean square error objective, which results in a hidden quadratic structure, whereas the actor’s objective typically has a hidden linear structure due to parameterization of the Q network and policy. Thus, the critic cost structure is more well-suited for computing an approximate local best response since it is more likely to be well-conditioned, and so the critic as the follower is the more natural hierarchical game structure. Unrolling the critic for multiple steps to approximate this structure and has been shown to perform well empirically (Schulman et al., 2015a). Algorithm 2 (Appendix E) describes this method for the Stackelberg framework.
6 Conclusion
We revisit the standard actor-critic algorithms from a game-theoretic perspective to capture the hierarchical interaction structure and introduce a Stackelberg framework for actor-critic algorithms. In this framework, we introduce novel Stackelberg versions of existing actor-critic algorithms. In experiments on a number of environments, we show that the Stackelberg actor-critic algorithms always outperform the existing counterparts when the actor plays the leader.
11

References
Tamer Ba¸sar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Stephanie CY Chan, Samuel Fishman, Anoop Korattikara, John Canny, and Sergio Guadarrama. Measuring the reliability of reinforcement learning algorithms. In International Conference on Learning Representations, 2019.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliﬀ. Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study. In International Conference on Machine Learning, 2020.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Solving min-max optimization with hidden structure via gradient descent ascent. arXiv preprint arXiv:2101.05248, 2021.
Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In International Conference on Autonomous Agents and MultiAgent Systems, page 122–130, 2018.
Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actor-critic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307, 2012.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861–1870, 2018.
Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity. Autonomous Agents and Multi-Agent Systems, 2017.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint arXiv:2007.05170, 2020.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pages 1008–1014, 2000.
Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rockta¨schel, and Shimon Whiteson. Stable opponent shaping in diﬀerentiable games. In International Conference on Learning Representations, 2018.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representation, 2016.
James Martens et al. Deep learning via hessian-free optimization. In International Conference on Machine Learning, pages 735–742, 2010.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928–1937, 2016.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147–160, 1994.
12

Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008.
Manish Prajapat, Kamyar Azizzadenesheli, Alexander Liniger, Yisong Yue, and Anima Anandkumar. Competitive policy optimization. In Conference on Uncertainty in Artiﬁcial Intelligence, 2021.
Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 2019.
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model based reinforcement learning. In International Conference on Machine Learning, 2020.
Giorgia Ramponi and Marcello Restelli. Newton optimization on helmholtz decomposition for continuous games. In AAAI Conference on Artiﬁcial Intelligence, pages 11325–11333, 2021.
Lillian J Ratliﬀ, Samuel A Burden, and S Shankar Sastry. Genericity and structural stability of non-degenerate diﬀerential nash equilibria. In American Control Conference, pages 3990–3995. IEEE, 2014.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889–1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, pages 1057–1063, 2000.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033, 2012.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In AAAI Conference on Artiﬁcial Intelligence, 2016.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincar´e recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave zero-sum games. In Advances in Neural Information Processing Systems, 2019.
Junfeng Wen, Saurabh Kumar, Ramki Gummadi, and Dale Schuurmans. Characterizing the gap between actor-critic and policy gradient. arXiv preprint arXiv:2106.06932, 2021.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. Studies in Systems, Decision and Control Handbook on RL and Control, 2020.
Chongjie Zhang and Victor Lesser. Multi-agent learning with policy prediction. In AAAI Conference on Artiﬁcial Intelligence, 2010.
Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In AAAI Conference on Artiﬁcial Intelligence, pages 7325–7332, 2020.
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pages 321–384, 2021.
13

A Motivation Example Details
In this appendix section, we provide more detail for the example in Section 3.

w w w

1.0

0.5

0.0

0.5

1.0 1

0

1.0

1.0

0.5

0.5

0.0

0.0

0.5

0.5

1 1.0 1

0

1 1.0 1

0

1

(a) Individual gradient

(b) Stackelberg gradient

(c) Regularized Stackelberg gradient

Figure 4: Vector ﬁelds and trajectories of the individual gradient, Stackelberg gradient and regularized Stackelberg gradient updates. The Stackelberg updates eliminate cycling by changing the shape of the vector ﬁeld.

100 10 1 10 2
0

10000 20000 30000 40000 50000
Iterations
(a) Error

0.00 0.05 0.10 0.15 0.20
0

Individual grad Stackelberg grad Stackelberg w/ reg
1000 2000 3000 4000 5000
Iterations
(b) Return

Figure 5: (a) Convergence error w − w∗ 2 + θ − θ∗ 2 where (θ∗, w∗) = (0, 0) is the equilibrium. (b) The return R(θ) of the actor. The Stackelberg update eliminates cycling and hence, converges more directly to the equilibrium as can be seen in (a), whereas the individual gradient update oscillates signiﬁcantly. Regularization helps to speed up convergence.

Recall the motivating example in which the actor plays the leader with the objective function J(θ, w) = w·θ, and the critic plays the follower with objective function L(θ, w) = (w · θ + 15 θ2)2. Figure 4 shows the vector ﬁelds and trajectories of each of the updates: individual gradient play6, Stackelberg gradient play, and regularized Stackelberg gradient play. In Figure 4(a), we observe clear cycling behavior. Such cycling behavior may be an indication of reduced reliability along the learning path and is often exacerbated by noise. Generally speaking, it is more desirable to observe smooth, monotonic changes in performance as compared to cycling behavior or noisy ﬂuctuations around a observable trend. The reason for this is that when we go to deploy such algorithms in the real world, it can be extremely costly to have the algorithm perform in oscillatory or even unpredictable ways. This is in particular true when, as is often the case, there are unmodeled exogenous inputs or environmental factors.
6In the learning in games literature, this is also often referred to as simultaneous gradient play or simultaneous gradient descent-ascent.

14

On the other hand, Stackelberg gradient converges more directly to the equilibrium point (θ∗, w∗) = (0, 0) and shown in both Figures 4(b) and 4(c) where the latter are the trajectories of the regularized Stackelberg gradient introduced in Section 4.5. Figure 5(a) shows the error w − w∗ 2 + θ − θ∗ 2 and Figure 5(b) shows the return R(θ) of each of the updates. We can observe that the cycling is mitigated and convergence accelerated by optimizing using the Stackelberg gradient, which leads to more stable returns along the learning.

w w

1.0

1.0

0.5

0.5

0.0

0.0

0.5

0.5

1.0 1

0

1 1.0 1

0

1

(a) SAC

(b) STSAC-AL

Figure 6: Vector ﬁelds and trajectories of the SAC and STSAC-AL updates.

100 10 1 10 2 10 3
0 500 1000 1500 2000
Iterations
(a) Error of SAC and STSAC-AL

0.00

0.05

0.10

0.15
0.20 0

Individual grad Stackelberg grad
500 1000 1500 2000
Iterations

(b) Return of SAC and STSAC-AL

Figure 7: (a) Error for each algorithm, SAC and STSAC-AL, w − w∗ 2 + θ − θ∗ 2 where (θ∗, w∗) = (0, 0) is the equilibrium. (b) Return of the actor R(θ).

In Figures 6 and 7, we show the result of adding entropy regularization to the actor’s objective using the SAC algorithm. Since SAC involves sampling from an stochastic policy, we plot the empirical mean gradient vector ﬁelds in Figure 6(a) and Figure 6(b), where the gradients for update are estimated by samples. With the entropy regularization, both gradient updates converge much faster and the gap between them are less signiﬁcant (Figure 7(a) and 7(b)).

15

B Proof of Theorem 1
Recall that the critic’s objective is given by L(θ, w) = Es∼ρ,a∼πθ(·|s)[(Qw(s, a) − Qπ(s, a))2]. The derivative is computed as follows:

∇θL(θ, w) = ∇θ ρ(s0) πθ(a0|s0) (Qw(s0, a0) − Qπ(s0, a0))2 da0ds0

s0

a0

= ρ(s0) ∇θπθ(a0|s0) (Qw(s0, a0) − Qπ(s0, a0))2 da0ds0

s0

a0

+ ρ(s0) πθ(a0|s0)∇θ (Qw(s0, a0) − Qπ(s0, a0))2 da0ds0

s0

a0

= ρ(s0) πθ(a0|s0)∇θ log πθ(a0|s0) (Qw(s0, a0) − Qπ(s0, a0))2 da0ds0

s0

a0

+ 2 ρ(s0) πθ(a0|s0) (Qπ(s0, a0) − Qw(s0, a0)) ∇θQπ(s0, a0)da0ds0.

s0

a0

From here, it remains to compute ∇θQπ(s0, a0). To do so, recall that Qπ(st, at) and V π(st) are given by

Qπ(st, at) = Eτ∼π

T t =t

γt

−tr(st

,

at

)|st,

at

= r(st, at) + γ

P (s |st, at)V π(s )ds ,

s

and

V π(st) = Eτ∼π

T t =t

γt

−tr(st

,

at

)|st

=

πθ(a|st)Qπ(st, a)da.

a

Hence, ∇θQπ(s0, a0) is computed as follows:

∇θQπ(s0, a0) = γ P (s1|s0, a0)∇θV π(s1)ds1
s1

= γ P (s1|s0, a0) (∇θπθ(a1|s1)Qπ(s1, a1) + πθ(a1|s1)∇θQπ(s1, a1)) da1ds1

s1

a1

= γ P (s1|s0, a0) πθ(a1|s1)∇θ log πθ(a1|s1)Qπ(s1, a1)da1ds1

s1

a1

+ γ2 P (s1|s0, a0) πθ(a1|s1) P (s2|s1, a1)∇θV π(s2)ds2da1ds1

s1

a1

s2

= γ P (s1|s0, a0) πθ(a1|s1)∇θ log πθ(a1|s1)Qπ(s1, a1)da1ds1

s1

a1

+ γ2 P (s1|s0, a0) πθ(a1|s1) P (s2|s1, a1) πθ(a2|s2)∇θ log πθ(a2|s2)Qπ(s2, a2)da2ds2da1ds1

s1

a1

s2

a2

+ γ3 P (s1|s0, a0) πθ(a1|s1) P (s2|s1, a1) πθ(a2|s2) P (s3|s2, a2)∇θV π(s3)ds3da2ds2da1ds1

s1

a1

s2

a2

s3

= γ p(τ1:1|θ)∇θ log πθ(a1|s1)Qπ(s1, a1)dτ1:1
τ

+ γ2 p(τ1:2|θ)∇θ log πθ(a2|s2)Qπ(s2, a2)dτ1:2
τ
+...

T

=

γtp(τ1:t|θ)∇θ log πθ(at|st)Qπ(st, at)dτ.

(19)

τ t=1

16

where the last equality is obtained by unrolling and marginalization for the entire length of the trajectory. Thus, coming back to the computation of ∇θL(θ, w), we have that

∇θL(θ, w) = ρ(s0) πθ(a0|s0)∇θ log πθ(a0|s0) (Qw(s0, a0) − Qπ(s0, a0))2 da0ds0

s0

a0

+ 2 ρ(s0) πθ(a0|s0) (Qπ(s0, a0) − Qw(s0, a0)) ∇θQπ(s0, a0)da0ds0

s0

a0

= p(τ0|θ)∇θ log πθ(a0|s0) (Qw(s0, a0) − Qπ(s0, a0))2
τ
T
+ 2 γtp(τ0:t|θ)∇θ log πθ(at|st) (Qπ(s0, a0) − Qw(s0, a0)) Qπ(st, at)dτ
t=1

= Eτ∼πθ ∇θ log πθ(a0|s0) (Qw(s0, a0) − Qπ(s0, a0))2

which completes the proof.

T
+ γt∇θ log πθ(at|st) (Qπ(s0, a0) − Qw(s0, a0)) Qπ(st, at)
t=1

C Proof of Proposition 1
The critic’s objective is given by L(θ, w) = Es∼ρ (Vw(s) − V π(s))2 . Hence, taking the derivative with respect to θ, we have that

∇θL(θ, w) = ρ(s0)∇θ(Vw(s0) − V π(s0))2ds0
s0

= 2 ρ(s0)(V π(s0) − Vw(s0))∇θV π(s0)ds0.

(20)

s0

Now we compute ∇θV π(s0) in (20). Use the result of (19), we have

∇θV π(s0) = ∇θπθ(a0|s0)Qπ(s0, a0) + πθ(a0|s0)∇θQπ(s0, a0)da0
a0

T

= πθ(a0|s0) ∇θ log πθ(a0|s0)Qπ(s0, a0) + γtp(τ1:t|θ)∇θ log πθ(at|st)Qπ(st, at) dτ. (21)

τ

t=1

Substituting (21) into (20), we have that

T

∇θL(θ, w) = 2

γtp(τ0:t|θ)∇θ log πθ(at|st) (V π(s0) − Vw(s0)) Qπ(st, at)dτ

τ t=0

T
= Eτ∼πθ 2 γt∇θ log πθ(at|st) (V π(s0) − Vw(s0)) Qπ(st, at)

t=0

which completes the proof.

17

D Proof of Theorem 2
Without loss of generality, the actor plays the role of the leader. Consider a diﬀerential Stackelberg equilibrium of the game (θ∗, w∗) which is locally asymptotically stable7 for the continuous time dynamical system
θ˙ = ∇J(θ, w) w˙ −∇wL(θ, w))
where the total derivative of actor in the Stackelberg gradient is given by
∇J (θ, w) = ∇θJ (θ, w) − ∇wθL(θ, w)(∇2wL(θ, w))−1∇wJ (θ, w).
and the individual gradient for the critic is ∇wL(θ, w). The actor and critic employ the discrete time updates given in Algorithm 1 where the actor is the leader. Since the actor and critic have unbiased estimates of their gradients and the learning rates are chosen as stated in Section 4.4, then the result of the theorem follows from Theorem 7 in (Fiez et al., 2020). That is, from an initial point (θ0, w0) ∈ U , the Stackelberg gradient dynamics converge asymptotically to (θ∗, w∗) ∈ U almost surely.
Indeed, the result holds by the following reasoning. Under the assumptions on the noise processes and stepsize sequences, we treat the updates in Algorithm 1 as a stochastic approximation process (θk, wk). Then, we deﬁne asymptotic pseudo-trajectories—i.e., linear interpolations between iterates (θk, wk) and (θk+1, wk+1). Since (θ∗, w∗) is locally asymptotically stable, there exists a neighborhood of (θ∗, w∗) and a local Lyapunov function on that neighborhood. This Lyapunov function can be used to show that the continuous time ﬂow also starting from iterates (θk, wk) and the asymptotic pseudo-trajectories are contracting onto one another asymptotically, for any sequence of iterates starting at (θ0, w0) ∈ U . Hence, the iterates (θk, wk), in turn, converge asymptotically to (θ∗, w∗) almost surely.

Comments on designing gradient estimators. Methods such as REINFORCE (or Monte Carlo method) provide an unbiased estimator of the follower’s individual gradient. Obtaining an unbiased estimate of the total derivative for the leader, on the other hand, is a bit more nuanced. This is because there are multiple gradients being multiplied by one another in the expectation. However, as a heuristic, one way to approximate it is using the expected value of each of the terms that shows up in the total derivative.
Depending on the actor-critic algorithm and objective functions, following either Theorem 1 (Proposition 1) or direct derivatives, each term in the total derivative can be computed as an expectation over a distribution of state and action (generated by current policy in AC and any arbitrary policy in DDPG and SAC). Take DDPG as an example where J (θ, w) = Eξ∼D [Qw(s, µθ(s))], and L(θ, w) = Eξ∼D (Qw(s, a) − (r + γQ0(s , µθ(s ))))2 . The second term in total derivative appears to be a multiplication of several expectations:
∇J (θ, w) = ∇θJ (θ, w) − ∇wθL(θ, w)(∇2wL(θ, w))−1∇wJ (θ, w)
= Eξ∼D [∇θQw(s, µθ(s))] − Eξ∼D ∇wθ (Qw(s, a) − (r + γQ0(s , µθ(s ))))2
∇2w (Qw(s, a) − (r + γQ0(s , µθ(s ))))2 −1 ∇wQw(s, µθ(s))

≈ Eξ∼D [∇θQw(s, µθ(s))] − Eξ∼D ∇wθ (Qw(s, a) − (r + γQ0(s , µθ(s ))))2

Eξ∼D ∇2w (Qw(s, a) − (r + γQ0(s , µθ(s ))))2

−1
Eξ∼D [∇wQw(s, µθ(s))] .

For this approximation, we can obtain an unbiased estimate by resetting the simulator as described in (Sutton et al., 2000, Chapter 11) to estimate each term in the product of expectations. As a result, this is a reasonable heuristic in practice for an approximation to the total derivative. Our policy gradient theorems also provide
7That is, the local linearization of the above dynamics around the point (θ∗, w∗) are in the open left-half complex plane.

18

us a way to derive the estimates of each of these individual terms. Obtaining unbiased estimates as an active area of research (see, e.g., Hong et al. 2020; Ramponi and Restelli 2021). Moreover, from both a theoretical and practical perspective, understanding how the batch size aﬀects the estimate of follower Hessian and the total derivative remains open.

E Implementation Details

This section includes complete details about our experiments. Our implementation is developed based

on public resource Spinning Up8 and our source code is available at https://github.com/LeoZhengZLY/

stackelberg-actor-critic-algos.

We follow the default neural network architecture used in Spinning Up. Particularly, the AC and STAC

use networks of size (64, 32) with tanh units for both the policy and the value function. The DDPG, STDDPG,

SAC, and STSAC use networks of size (256, 256) with relu units. The AC and STAC collected 4000 steps of

agent-environment interaction per batch and use vanilla gradient descent optimizer and the DDPG, STDDPG,

SAC, and STSAC use Adam optimizer with mini-batches of size 100 at each gradient descent step.

The policy gradient terms for AC and STAC are estimated by generalized average estimator (GAE) (Schulman

et al., 2015b) and critics are updated by Monte Carlo method (Sutton and Barto, 2018). In discrete control

task (CartPole), we set the Hessian regularization hyper-parameter λ = 0, and in continuous control tasks

(others), we set the regularization hyper-parameter λ = 500.

The performances for AC and STAC are measured as the average trajectory return across the batch collected

at each epoch. Performances for DDPG, STDDPG, SAC, and STSAC are measured once every 10, 000 steps by

running the deterministic policy (or, in the case of SAC, the mean policy) without action noise for ten

trajectories, and reporting the average return over those test trajectories.

In our Stackelberg framework, the learning rule for the leader involves computing an inverse-Hessian-vector

product

for

the

∇

2 2

f2

(

x

1

,

x2

)

inverse

term

and

Jacobian-vector

product

for

the

∇12f2(x1, x2)

terms.

The

second term can be computed directly by autograd.grad in torch. For the inverse-Hessian-vector term, we

implement the conjugate gradient (CG) method using autograd.grad iteratively. This enable us to compute

and estimate the total derivative on GPU directly and perform Stackelberg gradient update. Each CG

iteration requires a Hessian vector product (HVP). HVPs can be computed in ∼ 1.5 times the cost of a

gradient (Pearlmutter, 1994), so the leader update with k CG iterations only costs ∼ 1.5 · k times a normal

gradient. We run CG with k = 10 so the leader update costs ∼ 15 times a normal gradient. CG has been

applied widely in machine learning (Martens et al., 2010) and recently at scale for meta-learning (Rajeswaran

et al., 2019) and GANs (Fiez et al., 2020). As observed in (Rajeswaran et al., 2019; Fiez et al., 2020), often

k = 5 in CG is suﬃcient to get within numerical precision, so we could have had the leader update cost

∼ 7.5 times a normal gradient. In all our experiments, the Stackelberg versions of actor-critic algorithms

roughly take twice the time to train. This is because the bottleneck in reinforcement learning is sampling

trajectories from the environment rather than gradient computing. This additional time of Stackelberg

algorithms would go down if we used k = 5 in CG. Hence, Stackelberg versions of actor-critic algorithms

training is not signiﬁcantly slower normal actor-critic algorithms.

In Algorithm 2, we provide a more detailed version of our Stackelberg actor-critic algorithm framework

when multiple follower unrolling steps and implicit map regularization are involved.

8Developed by Josh Achiam in 2018: https://spinningup.openai.com/en/latest/
19

Algorithm 2: Stackelberg Actor-Critic Framework with Unrolling Follower Update and Regularization
Input: actor-critic algorithm ALG, player designations, follower unrolling steps m, regularization hyperparameter λ, and learning rate sequences α1,k, α2,k.
for k = 0, 1, 2 . . . do if actor is leader, then update actor and critic in ALG with θk+1 = θk + α1,k(∇θJ (θk, wk,0) − (∇wθL ◦ (∇2wL + λI)−1 ◦ ∇wJ )(θk, wk,0)) wk,l+1 = wk,l − α2,k∇wL(θk, wk,l), l ∈ [0, m − 1] wk+1,0 = wk,m if critic is leader, then update actor and critic in ALG with wk+1 = wk − α1,k(∇wL(θk,0, wk) − (∇θwJ ◦ (∇2θJ + λI)−1 ◦ ∇θL)(θk,0, wk)) θk,l+1 = θk,l + α2,k∇θJ (θk,l, wk), l ∈ [0, m − 1] θk+1,0 = θk,m
end
20

