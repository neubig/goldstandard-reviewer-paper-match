Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings
Kalpesh Krishna‚ô† ‚àó Deepak Nathani‚ô¶ Xavier Garcia‚ô¶ Bidisha Samanta‚ô¶ Partha Talukdar‚ô¶
‚ô†University of Massachusetts Amherst, ‚ô¶Google Research kalpesh@cs.umass.edu
{xgarcia, dnathani, bidishasamanta, partha}@google.com

arXiv:2110.07385v2 [cs.CL] 11 Mar 2022

Abstract
Style transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al., 2021) has attempted ‚Äúfew-shot‚Äù style transfer using just 310 sentences at inference for style extraction. In this work, we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim.
We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages. Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob. We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simpliÔ¨Åcation, gender neutralization, text anonymization) all without retraining the model. Finally, we Ô¨Ånd model evaluation to be difÔ¨Åcult due to the lack of datasets and metrics for many languages. To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.1
1 Introduction
Style transfer is a natural language generation task in which input sentences need to be re-written into a target style, while preserving semantics. It has many applications such as writing assistance (Heidorn, 2000), controlling generation for attributes
1Resources accompanying this project will be added to our project page: https://martiansideofthemoon. github.io/2022/03/03/acl22.html.
*Work done during a Google Research India internship.

honorifics,2,4 Sanskrit3 / Persian1 words for ‚Äújob‚Äù in formal Hindi
‡§Ö‡§™‡§®‡•Ä ‡§µ‡§æ‡§≤‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä1 ‡§Æ‡•Å ‡§ù‡•á ‡§Æ‡§§ ‡§¨‡§§‡§æ‡§ì‡•§ ùúÜ = 0.5
‡§Ü‡§™‡§ï‡•Ä2 ‡§®‡§Ø‡•Å ‡§§3 ‡§ï‡•á ‡§¨‡§æ‡•á‡§∞ ‡§Æ‡•á‡§Ç ‡§Æ‡•Å ‡§ù‡•á ‡§®‡§æ ‡§¨‡§§‡§æ‡§è‡§Ç ‡•§4 ùúÜ = 1.5

-

DiffUR

Style Vector Extractor

Style Vector Extractor

Target (Formal) It is certainly amongst my
favorites.

Source (Informal) Its def one of my favs

‡§Ö‡§™‡§®‡•Ä ‡§µ‡§æ‡§≤‡•Ä ‡§ú‡•â‡§¨ transfer ‡§Æ‡•Å ‡§ù‡•á ‡§Æ‡§§ ‡§¨‡§§‡§æ‡§ì. amount ùúÜ (don‚Äôt tell me about your job)

Figure 1: An illustration of our few-shot style transfer system during inference. Our model extracts style vectors from exemplar English sentences as input (in this case formal/informal sentences) and uses their vector difference to guide style transfer in other languages (Hindi). Œª is used to control the magnitude of transfer: in this example our model produces more high Sanskrit words & honoriÔ¨Åcs (more formal) with higher Œª.

like simplicity, formality or persuasion (Xu et al., 2015; Smith et al., 2020; Niu and Carpuat, 2020), data augmentation (Xie et al., 2020; Lee et al., 2021), and author obfuscation (Shetty et al., 2018).
Most prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to a large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018; Subramanian et al., 2019). Models built are style-speciÔ¨Åc and cannot generalize to new styles during inference, which is needed for applications like real-time adaptation to a user‚Äôs style in a dialog or writing application. Moreover, access to a large unpaired corpus with style labels is a strong assumption. Most standard ‚Äúunpaired‚Äù style transfer datasets have been carefully curated (Shen et al., 2017) or were originally parallel (Xu et al., 2012; Rao and Tetreault, 2018). This is especially relevant in settings outside En-

glish, where NLP tools and labelled datasets are largely underdeveloped (Joshi et al., 2020). In this work, we take the Ô¨Årst steps studying style transfer in seven languages2 with nearly 1.5 billion speakers in total. Since no training data exists for these languages, we analyzed the current state-of-the-art in few-shot multilingual style transfer, the Universal Rewriter (UR) from Garcia et al. (2021). Unfortunately, we Ô¨Ånd it often copies the inputs verbatim (Section 3.1), without changing their style.
We propose a simple inference-time trick of style-controlled translation through English, which improves the UR output diversity (Section 4.1). To further boost performance we propose DIFFUR,3 a novel algorithm using the recent Ô¨Ånding that paraphrasing leads to stylistic changes (Krishna et al., 2020). DIFFUR extracts edit vectors from paraphrase pairs, which are used to condition and train the model (Figure 2). On formality transfer and code-mixing addition, our best performing DIFFUR variant signiÔ¨Åcantly outperforms UR across all languages (by 2-3x) using automatic & human evaluation. Besides better rewriting, our system is better able to control the style transfer magnitude (Figure 1). A scalar knob (Œª) can be adjusted to make the output text reÔ¨Çect the target style (provided by exemplars) more or less. We also observe promising qualitative results in several attribute transfer directions (Section 6.2) including sentiment transfer, simpliÔ¨Åcation, gender neutralization and text anonymization, all without retraining the model and using just 3-10 examples at inference.
Finally, we found it hard to precisely evaluate models due to the lack of evaluation datasets and style classiÔ¨Åers (often used as metrics) for many languages. To facilitate further research in Indic formality transfer, we crowdsource formality annotations for 4000 sentence pairs in four Indic languages (Section 5.1), and use this dataset to design the automatic evaluation suite (Section 5). In summary, our contributions provide an end-toend recipe for developing and evaluating style transfer models and evaluation in a low-resource setting.
2 Related Work
Few-shot methods are a recent development in English style transfer, with prior work using variational autoencoders (Xu et al., 2020), or prompting large pretrained language models at inference (Reif
2Indic (hi,bn,kn,gu,te), Spanish, Swahili. 3‚ÄúDifference Universal Rewriter‚Äù, pronounced as differ.

et al., 2021). Most related is the state-of-the-art TextSETTR model from Riley et al. (2021), who use a neural style encoder to map exemplar sentences to a vector used to guide generation. To train this encoder, they use the idea that adjacent sentences in a document have a similar style. Recently, the Universal Rewriter (Garcia et al., 2021) extended TextSETTR to 101 languages, developing a joint model for translation, few-shot style transfer and stylized translation. This model is the only prior few-shot system we found outside English, and our main baseline. We discuss its shortcomings in Section 3.1, and propose Ô¨Åxes in Section 4. Multilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French. They further introduced XFORMAL, the Ô¨Årst formality transfer evaluation dataset in French, Brazilian Portugese and Italian.4 To the best of our knowledge, we are the Ô¨Årst to study style transfer for the languages we consider. More related work from Hindi linguistics and on style transfer control is provided in Appendix B.
3 The Universal Rewriter (UR) model
We will start by discussing the Universal Rewriter (UR) model from Garcia et al. (2021), upon which our proposed DIFFUR model is built. At a high level, the UR model extracts a style vector s from an exemplar sentence e, which reÔ¨Çects the desired target style. This style vector is used to style transfer an input sentence x. Concretely, consider fenc, fdec to be encoder & decoder Transformers initialized with mT5 (Xue et al., 2021b), which are composed to form the model fur. The UR model extracts the style vector using the encoder representation of a special [CLS] token prepended to e, and adds it to the input x representations for style transfer,
fstyle(e) = s = fenc([CLS] ‚äï e)[0] fur(x, s) = fdec(fenc(x) + s)
where ‚äï is string concatenation, + vector addition. fur is trained using the following objectives,
Learning Style Transfer by Exemplar-driven Denoising: To learn a style extractor, the Universal Rewriter uses the idea that two non-overlapping spans of text in the same document are likely to have the same style. Concretely, let x1 and x2 be
4We do not use this data since it does not cover Indian languages, and due to Yahoo! L6 corpus restrictions for industry researchers (conÔ¨Årmed via author correspondence).

two non-overlapping spans. Style extracted from one span (x1) is used to denoise the other (x2),

x¬Ø2 = fur(noise(x2), fstyle(x1)) Ldenoise = LCE(x¬Ø2, x2)

where LCE is the standard next-word prediction cross entropy loss function and noise(¬∑) refers to 20-60% random token dropping and token replacement. This objective is used on the mC4 dataset (Xue et al., 2021b) with 101 languages. To build a general-purpose rewriter which can do translation as well as style transfer, the model is additionally trained on two objectives: (1) supervised machine translation using the OPUS-100 parallel dataset (Zhang et al., 2020), and (2) a self-supervised objective to learn effective stylecontrolled translation; more details in Appendix C.

During inference (Figure 1), consider an input sentence x and a transformation from style A to B (say informal to formal). Let SA, SB to be exemplar sentences in each of the styles (typically 3-10 sentences). The output y is computed as,

1

sA = |SA|

fstyle(y)

y‚ààSA

1

sB = |SB|

fstyle(y)

y‚ààSB

y = fur(x, Œª(sB ‚àí sA))

where Œª acts as a control knob to determine the
magnitude of style transfer, and the vector subtraction helps remove confounding style information.5

3.1 Shortcomings of the Universal Rewriter
We experimented with the UR model on Hindi formality transfer, and noticed poor performance. We noticed that UR has a strong tendency to copy sentences verbatim ‚Äî 45.5% outputs were copied exactly from the input (and hence not style transferred) for the best performing value of Œª. The copying increase for smaller Œª, making magnitude control harder. We identify the following issues: 1. Random token noise leads to unnatural inputs & transformations: The Universal Rewriter uses 20-60% uniformly random token dropping or replacement to noise inputs, which leads to ungrammatical inputs during training. We hypothesize models tend to learn grammatical error correction, which encourages verbatim copying during
5Garcia et al. (2021) also recommend adding the style vectors from the input sentence x, but we found this increased the amount of verbatim copying and led to poor performance.

inference where Ô¨Çuent inputs are used and no error correction is needed. Moreover, token-level noise does not differentiate between content or function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020). Too much noise could distort semantics and encourage hallucination, whereas too little will encourage copying. 2. Style vectors may not capture the precise style transformation: The Universal Rewriter extracts the style vector from a single sentence during training, which is a mismatch from the inference where a difference between vectors is taken. Without taking vector differences at inference, we observe semantic preservation and overall performance of the UR model is much lower.6 3. mC4 is noisy: On reading training data samples, we noticed noisy samples with severe language identiÔ¨Åcation errors in the Hindi subset of mC4. This has also been observed recently in Kreutzer et al. (2022), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language. 4. No translation data for several languages: We notice worse performance for languages which did not get parallel translation data (for the translation objective in Section 3). In Table 1 we see UR gets a score7 of 30.4 for Hindi and Bengali, languages for which it got translation data. However, the scores are lower for Kannada, Telugu & Gujarati (25.5, 22.8, 23.7), for which no translation data was used. We hypothesize translation data encourages learning language-agnostic semantic representations needed for translation from the given language, which in-turn improves style transfer.
4 Our Models
4.1 Style-Controlled Backtranslation (+ BT)
While the Universal Rewriter model has a strong tendency to exactly copy input sentences while rewriting sentences in the same language (Section 3.1), we found it is an effective style-controlled translation system. This motivates a simple inference-time trick to improve model outputs and reduce copying ‚Äî translate sentences to English (en) in a style-agnostic manner with a zero style
6This difference possibly helps remove confounding information (like semantic properties, other styles) and focus on the speciÔ¨Åc style transformation. Since two spans in the same document will share aspects like article topic / subject along with style, we expect these semantic properties will confound the style vector space obtained after the UR training.
7Using the r-AGG style transfer metric from Section 5.5.

Fix #2: Use difference of output / input vectors to
focus on edits
-

X: This fish is most commonly caught in Arabia mT5 decoder
+

Fix #3: Use cleaner sentences from Samanantar
instead of noisy mC4
Select sentence X from Samanantar

Style Extractor

Style Extractor

X: This fish is noise(X): most commonly Across the
caught... world, Arabian...

mT5 encoder
noise(X): Across the world, Arabian fisherman catch this
fish the most.

Fix #1: Use paraphrases as ‚Äúnoise‚Äù function
instead of random token dropping / replacement

Figure 2: The DIFFUR approach (Section 4.2), with Ô¨Åxes to the shortcomings of the Universal Rewriter approach (Section 3.1) shown. Sentences are noised using paraphrasing, the style vector difference between the paraphrase & original sentence (‚Äúedit vector‚Äù) is used to control denoising. See Figure 1 for the inference-time process.

vector 0, and translate back into the source language (lx) with stylistic control.
xen = fur(en ‚äï x, 0) x¬Ø = fur(lx ‚äï xen, Œª(sB ‚àí sA))
where x is the input sentence, sA, sB are the styles vectors we want to transfer between, en, lx are language codes prepended to indicate the output language (Appendix C). Prior work has shown that backtranslation is effective for paraphrasing (Wieting and Gimpel, 2018; Iyyer et al., 2018) and style transfer (Prabhumoye et al., 2018).
4.2 Using Paraphrase Vector Differences for Style Transfer (DIFFUR)
While style-controlled backtranslation is an effective strategy, it needs two translation steps. This is 2x slower than UR, and semantic errors increase with successive translations. To learn effective style transfer systems needing only a single generation step we develop DIFFUR, a new few-shot style transfer training objective (overview in Figure 2). DIFFUR tackles the issues discussed in Section 3.1 using paraphrases and style vector differences.
Paraphrases as a ‚Äúnoise‚Äù function: Instead of using random token-level noise (Issue #1 in Section 3.1), we paraphrase sentences to ‚Äúnoise‚Äù them during training. Paraphrasing modiÔ¨Åes the lexical & syntactic properties of sentences, while preserving Ô¨Çuency and input semantics. Prior work (Krishna et al., 2020) has shown that paraphrasing leads to stylistic changes, and denoising can be considered a style re-insertion process.

To create paraphrases, we backtranslate sentences from the UR model8 with no style control (zero vectors used as style vectors). To increase diversity, we use random sampling in both translation steps, pooling generations obtained using temperature values [0.4, 0.6, 0.8, 1.0]. Finally, we discard paraphrase pairs from the training data where the semantic similarity score9 is outside the range [0.7, 0.98]. This removes backtransation errors (score < 0.7), and exact copies (score > 0.98). In Appendix K we conÔ¨Årm that our backtranslated paraphrases are lexically diverse from the input.
Using style vector differences for control: To Ô¨Åx the training / inference mismatch for style extraction (Issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control. Concretely, let x be an input sentence and xpara its paraphrase.
sdiff = fstyle(x) ‚àí fstyle(xpara) x¬Ø = fur(xpara, stop-grad(sdiff)) L = LCE(x¬Ø, x)
where stop-grad(¬∑) stops gradient Ô¨Çow through sdiff, preventing the model from learning to copy x exactly. To ensure fstyle extracts meaningful style representations, we Ô¨Åne-tune a trained UR model. Vector differences have many advantages,
1. Subtracting style vectors between a sentence
8SpeciÔ¨Åcally, an Indic variant of the UR model is used, described in Section 4.3. Note it is not necessary to use UR for backtranslation, any good translation model can be used.
9Calculated using LaBSE, discussed in Section 5.3.

and its paraphrase removes confounding features (like semantics) present in the vectors.
2. The vector difference focuses on the precise transformation that is needed to reconstruct the input from its paraphrase.
3. The length of sdiff acts as a proxy for the amount of style transfer, which is controlled using Œª during inference (Section 3).
DIFFUR is related to neural editor models (Guu et al., 2018; He et al., 2020), where language models are decomposed into a probabilistic space of edit vectors over prototype sentences. We justify the DIFFUR design with ablations in Appendix G.1.
4.3 Indic Models (UR-INDIC, DIFFUR-INDIC)
To address the issue of no translation data (Issue #4 in Section 3.1), we train Indic variants of our models. We replace the OPUS translation data used for training the Universal Rewriter (Section 3) with Samanantar (Ramesh et al., 2021), which is the largest publicly available parallel translation corpus for 11 Indic languages. We call these variants UR-INDIC and DIFFUR-INDIC. This process significantly up-samples the parallel data seen between English / Indic languages, and gives us better performance (Table 1) and lower copy rates, especially for languages with no OPUS translation data.
4.4 Multitask Learning (DIFFUR-MLT)
One issue with our DIFFUR-INDIC setup is usage of a stop-grad(¬∑) to avoid verbatim copying from the input. This prevents gradient Ô¨Çow into the style extractor fstyle, and as we see in Appendix H, a degradation of the style vector space. To prevent this we simply multi-task between the exemplardriven denoising UR objective (Section 3) and the DIFFUR objective. We initialize the model with the UR-INDIC checkpoint, and Ô¨Åne-tune it on these two losses together, giving each loss equal weight.
5 Evaluation
Automatic evaluation of style transfer is challenging (Pang, 2019; Mir et al., 2019; Tikhonov et al., 2019), and the lack of resources (such as evaluation datasets, style classiÔ¨Åers) make evaluation trickier for Indic languages. To tackle this issue, we Ô¨Årst collect a small dataset of formality and semantic similarity annotations in four Indic languages (Section 5.1). We use this dataset to guide the design of an evaluation suite (Section 5.2-5.6).

Since automatic metrics in generation are imperfect (Celikyilmaz et al., 2020), we complement our results with human evaluation (Section 5.7).
5.1 Indic Formality Transfer Dataset
Since no public datasets exist for formality transfer in Indic languages, it is hard to measure the extent to which automatic metrics (such as style classiÔ¨Åers) are effective. To tackle this issue, we build a dataset of 1000 sentence pairs in each of four Indic languages (Hindi, Bengali, Kannada, Telugu) with formality and semantic similarity annotations. We Ô¨Årst style transfer held-out Samanantar sentences using our UR-INDIC + BT model (Section 4.1, 4.3) to create sentence pairs with different formality. We then asked three crowdworkers to 1) label the more formal sentence in each pair; 2) rate semantic similarity on a 3-point scale.
Our crowdsourcing is conducted on Task Mate,10 where we hired native speakers from India with at least a high school education and 90% approval rating on the platform. To ensure crowdworkers understood ‚Äúformality‚Äù, we provided instructions following advice from professional Indian linguists, and asked two qualiÔ¨Åcation questions in their native language. More details (agreement, compensation, instructions) are provided in Appendix E.4.
5.2 Transfer Accuracy (r-ACC, a-ACC)
Our Ô¨Årst metric checks whether the output sentence reÔ¨Çects the target style. This is measured by an external classiÔ¨Åer‚Äôs predictions on system outputs. We use two variants of transfer accuracy: (1) Relative Accuracy (r-ACC): does the target style classiÔ¨Åer score the output sentence higher than the input sentence? (2) Absolute Accuracy (a-ACC): does the classiÔ¨Åer score the output higher than 0.5? Building multilingual classiÔ¨Åers: Unfortunately, no large style classiÔ¨Åcation datasets exist for most languages, preventing us from building classiÔ¨Åers from scratch. We resort to zero-shot cross lingual transfer techniques (Conneau and Lample, 2019), where large multilingual pretrained models are Ô¨Årst Ô¨Åne-tuned on English classiÔ¨Åcation data, and then applied to other languages at inference. We experiment with three such techniques, and Ô¨Ånd MAD-X classiÔ¨Åers with language adapters (Pfeiffer et al., 2020b) have the highest accuracy of 81% on our Hindi data from Section 5.1. However, MAD-X classiÔ¨Åers were only available for Hindi, so we use
10https://taskmate.google.com

the next best XLM RoBERTa-base (Conneau et al., 2020) for other languages, which has 75%-82% accuracy on annotated data; details in Appendix E.1.
5.3 Semantic Similarity (SIM)
Our second evaluation criteria is semantic similarity between the input and output. Following recent recommendations (Marie et al., 2021; Krishna et al., 2020), we avoid n-gram overlap metrics like BLEU (Papineni et al., 2002). Instead, we use LaBSE (Feng et al., 2020), a language-agnostic semantic similarity model based on multilingual BERT (Devlin et al., 2019). LaBSE supports 109 languages, and is the only similarity model we found supporting all the Indic languages in this work. We also observed LaBSE had greater correlation with our annotated data (Section 5.1) compared to alternatives; details in Appendix E.2.
Qualitatively, we found that sentence pairs with LaBSE scores lower than 0.6 were almost never paraphrases. To avoid rewarding partial credit for low LaBSE scores, we use a hard threshold11 (L = 0.75) to determine whether pairs are paraphrases,

SIM(x, y ) = 1 if LaBSE(x, y ) > L else 0

5.4 Other Metrics (LANG, COPY, 1-g)
Additionally, we measure whether the input and output sentences are in the same language (LANG), the fraction of outputs copied verbatim from the input (COPY), and the 1-gram overlap between input / output (1-g). High LANG and low COPY / 1-g (more diversity) is better; details in Appendix E.6.
5.5 Aggregated Score (r-AGG, a-AGG)
To get a sense of overall system performance, we combine individual metrics into one score. Similar to Krishna et al. (2020) we aggregate metrics as,

AGG(x, y ) = ACC(x, y ) ¬∑ SIM(x, y ) ¬∑ LANG(y )

1

AGG(D) =

AGG(x, y )

|D|

x,y ‚ààD

Where (x, y ) are input-output pairs, and D is the test corpus. Since each of our individual metrics can only take values 0 or 1 at an instance level, our aggregation acts like a Boolean AND operation. In other words, we are measuring the fraction of outputs which simultaneously transfer style, have
11Roughly 73% pairs annotated as paraphrases (from dataset in Section 5.1) had L > 0.75. We experiment with different values of L in Appendix E.3 and notice similar trends.

a semantic similarity of at least L (our threshold in Section 5.3), and have the same language as the input. Depending on the variant of ACC (relative / absolute), we can derive r-AGG / a-AGG.

5.6 Evaluating Control (CALIB)

An ideal system should not only be able to style transfer sentences, but also control the magnitude of style transfer using the scalar input Œª. To evaluate this, for every system we Ô¨Årst determine a Œªmax value and let [0, Œªmax] be the range of control values. While in our setup Œª is an unbounded scalar, we noticed high values of Œª signiÔ¨Åcantly perturb semantics (also noted in Garcia et al., 2021), with systems outputting style-speciÔ¨Åc n-grams unfaithful to the output. We choose Œªmax to be the largest Œª from the list [0.5, 1.0, 1.5, 2.0, 2.5, 3.0] whose outputs have an average semantic similarity score (SIM, Section 5.3) of at least 0.7512 with the validation set inputs. For each system we take three evenly spaced Œª values in its control range, denoted as Œõ = [ 13 Œªmax, 23 Œªmax, Œªmax]. We then compute the style calibration to Œª (CALIB), or how often does increasing Œª lead to a style score increase? We measure this with a statistic similar to Kendall‚Äôs œÑ (Kendall, 1938), counting concordant pairs in Œõ,

1

CALIB(x) = n

{style(yŒªb) > style(yŒªa)}

Œªb>Œªa

where x is input, CALIB(x) is the average over all possible n (= 3) pairs of Œª values (Œªa, Œªb) in Œõ.

5.7 Human Evaluation
Automatic metrics are usually insufÔ¨Åcient for style transfer evaluation ‚Äî according to Briakou et al. (2021a), 69 / 97 surveyed style transfer papers used human evaluation. We adopt the crowd-sourcing setup from Section 5.1, which was used to build our formality evaluation datasets. We presented 200 generations from each model and the corresponding inputs in a random order, and asked three crowdworkers two questions about each pair of sentences: (1) which sentence is more formal/codemixed? (2) how similar are the two sentences in meaning? This lets us evaluate r-ACC, SIM, r-AGG, CALIB with respect to human annotations instead of classiÔ¨Åer predictions. More experiment details (inter-annotator agreement, compensation, instructions) are provided in Appendix E.4.
12This threshold is identical to the value chosen for paraphrase similarity in Section 5.3. We experiment with more/less conservative thresholds in Appendix E.3.

Model
UR (2021) UR-INDIC UR + BT UR-INDIC + BT
DIFFUR
DIFFUR-INDIC DIFFUR-MLT

Hindi r-AGG a-AGG
30.4 10.4 58.3 18.6
54.2 17.8 60.0 22.2
71.1 22.9 72.6 24.0 78.1 32.2

Bengali r-AGG a-AGG

30.4

7.2

65.5 22.3

55.6 16.9 61.1 22.0

72.7 25.2 75.4 24.3 80.0 35.0

Kannada r-AGG a-AGG

25.5

8.0

61.3 17.8

39.8 11.9 59.2 21.0

69.2 29.1 73.1 29.3 80.4 39.4

Telugu r-AGG a-AGG

22.8

8.4

59.8 19.9

38.4 11.6 56.8 22.2

69.4 27.1 71.0 27.1 79.8 37.9

Gujarati r-AGG a-AGG

23.7

5.0

54.0 10.7

46.3 10.4 57.7 16.8

0.4

0.2

36.0 13.0

75.0 33.1

Table 1: Automatic evaluation of formality transfer in Indic languages. Note each proposed method (*-INDIC, +BT, DIFFUR) improves performance (AGG deÔ¨Åned in Section 5.5), with a combination (DIFFUR-MLT) doing best.

Model

Œª COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

UR (Garcia et al., 2021) 1.5

UR-INDIC

1.0

45.4 77.5 98.0 84.8 45.8 22.9 30.4 10.4 10.4 70.7 95.0 93.8 67.2 23.3 58.3 18.6

UR + BT UR-INDIC + BT

0.5

0.8 44.2 92.9 85.2 72.3 27.8 54.2 17.8

1.0

1.1 49.5 95.9 85.1 76.3 33.1 60.0 22.2

DIFFUR
DIFFUR-INDIC DIFFUR-MLT

1.0

4.7 61.6 97.7 89.7 82.4 31.0 71.1 22.9

1.5

5.3 63.7 98.0 91.9 81.6 30.5 72.5 23.7

2.5

4.4 61.9 97.2 89.7 89.7 34.0 78.1 27.5

Table 2: Performance by individual metrics for Hindi formality transfer. DIFFUR-MLT gives best overall performance (r-AGG / a-AGG), with a good trade-off between style accuracy (ACC), semantic similarity (SIM), langID score (LANG), and low input copy rates (COPY); metrics deÔ¨Åned in Section 5, other language results in Appendix I.

6 Main Experiments
6.1 Experimental Setup
In our experiments, we compare the following models (training details are provided Appendix A):
‚Ä¢ UR: the Universal Rewriter (Garcia et al., 2021), which is our main baseline (Section 3);
‚Ä¢ DIFFUR: our model with paraphrase vector differences (Section 4.2);
‚Ä¢ UR-INDIC, DIFFUR-INDIC: Indic variants of UR and DIFFUR models (Section 4.3);
‚Ä¢ DIFFUR-MLT: Multitask training between URINDIC and DIFFUR-INDIC (Section 4.4);
‚Ä¢ + BT: models with style-controlled backtranslation at inference time (Section 4.1).
Our models are evaluated on (1) formality transfer (Rao and Tetreault, 2018); (2) code-mixing addition, a task where systems attempt to use English words in non-English sentences, while preserving the original script.13 Since we do not have access to any formality evaluation dataset,14 we hold out 22K sentences from Samanantar in each Indic language
13Hinglish is common in India, examples in Figure 5. 14We do not use GYAFC (Rao and Tetreault, 2018) and XFORMAL (Briakou et al., 2021b) due to reasons in footnote 4. Our dataset from Section 5.1 has already been used for classiÔ¨Åer selection, and has machine generated sentences.

for validation / testing. For Swahili / Spanish, we use mC4 / WMT2018 sentences. These sets have similar number of formal / informal sentences, as marked by our formality classiÔ¨Åers (Section 5.2), and are transferred to the opposite formality. We re-use the hi/bn formality transfer splits for codemixing addition, evaluating unidirectional transfer.
Seven languages with varying scripts and morphological richness are used for evaluation (hi,es,sw,bn,kn,te,gu). The UR model only saw translation data for hi,es,bn, whereas UR-INDIC sees translation data for all Indic languages (Section 4.3). To test the generalization capability of the DIFFUR, no Gujarati paraphrase training data for is used. Note that no paired/unpaired data with style labels is used during training: models determine the target style at inference using 3-10 exemplars sentences. For few-shot formality transfer, we use the English exemplars from Garcia et al. (2021). We follow their setup and use English exemplars to guide nonEnglish transfer zero-shot. For code-mixing addition, we use Hindi/English code-mixed exemplars in Devanagari (shown in Appendix D).
6.2 Main Results
Each proposed method improves over prior work, DIFFUR-MLT works best. We present our

performance (r-AGG) style change

0.6 0.8 0.5

0.6

0.4

0.4

0.3

0.2

0.2

0.1

Naive model UR (Garcia 2021) UR-Indic UR + BT UR-Indic + BT DiffUR DiffUR-Indic DiffUR-MLT Ideal system

0.0 0.5 1.0 1.5 2.0 2.5 3.0 transfer amount ( )

0.00.75 0.80 0.85 0.90 0.95 1.00 content similarity

Figure 3: Variation in Kannada formality transfer with Œª. In the left plot, we see DIFFUR-* models have consistently good overall performance with change in Œª. In the right plot, we see the tradeoff between average style change and content similarity as Œª is varied. Plots (such as DIFFUR-*) which stretch the Y-axis range, closer to the ideal system (x = 1) and away from the naive system (x + y = 1, akin to naive model in Krishna et al., 2020) are better.

Model
UR (2021) UR, BT DIFFUR-MLT

Swahili r-AGG / a-AGG
19.9 / 4.8 13.7 / 3.4 32.2 / 7.2

Spanish r-AGG / a-AGG
13.4 / 1.3 33.3 / 5.8 46.5 / 16.5

Table 3: Automatic evaluation of formality transfer in Swahili and Spanish. DIFFUR-MLT performs best.

Model

ACC SIM AGG CALIB C-IN

UR (2021) UR-INDIC

29.5 87.2 23.2 46.5 85.3 40.8

-

-

35.7 43.0

UR + BT

57.5 71.2 42.9

UR-INDIC + BT 65.0 77.8 52.4

-

-

24.0 40.3

DIFFUR
DIFFUR-INDIC DIFFUR-MLT

64.5 80.8 52.0 62.0 83.1 50.4 70.0 80.8 55.6

-

-

48.0 54.5

53.0 54.5

Table 4: Human evaluation on Hindi formality transfer, measuring style accuracy (ACC), input similarity (SIM), overall score (AGG) and control with Œª (CALIB, C-IN). Like Table 1, DIFFUR-MLT performs best.

automatic evaluation results for formality transfer across languages in Table 1, Table 3. Overall we Ô¨Ånd that each of our proposed methods (DIFFUR, *-INDIC, +BT) helps improve performance over the baseline UR model (71.1, 58.3, 54.2 vs 30.4 r-AGG on Hindi). Combining these ideas with multitask learning (DIFFUR-MLT) gives us the best performance across all languages (78.1 on Hindi). On Gujarati, the DIFFUR-INDIC fails to get good performance (36.0 r-AGG) since it did not see Gujarati paraphrase data, but this performance is recovered using DIFFUR-MLT (75.0). In Table 4 we see human evaluations support our automatic evaluation for formality transfer. In Table 5 we perform human evaluation on a subset of models for code-mixing addition and see similar trends, with DIFFUR-MLT signiÔ¨Åcantly outperforming UR, UR-

Model
UR (2021) UR-INDIC,BT DIFFUR-MLT,BT

Hindi ACC / SIM / AGG
4.5 / 93.8 / 3.6 18.5 / 79.2 / 15.3 62.5 / 69.9 / 41.5

Bengali ACC / SIM / AGG
0.0 / 96.4 / 0.0 18.0 / 68.3 / 12.7 79.0 / 57.1 / 43.5

Table 5: Human evaluation on code-mixing addition. DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC). Due to verbatim copying, UR SIM score is nearly 100, but ACC score close to 0.

Model
UR (2021) UR-INDIC UR + BT UR-INDIC + BT

CALIB
29.2 60.7 43.4 38.7

Model
DIFFUR
DIFFUR-INDIC DIFFUR-MLT

CALIB
64.9 69.6 69.0

Table 6: Evaluation of Hindi formality transfer magnitude control using Œª. We Ô¨Ånd that DIFFUR-* are best at calibrating style change (CALIB) to input Œª (metrics details in Section 5.6, more results in Appendix F).

INDIC (41.5 AGG vs 3.6, 15.3 on Hindi).
DIFFUR-MLT and DIFFUR-INDIC are best at controlling magnitude of style transfer: In Table 6, we compare the extent to which models can control the amount of style transfer using Œª. We Ô¨Ånd that all our proposed methods outperform the UR model, which gets only 29.2 CALIB. +BT models are not as effective at control (43.4 CALIB), while DIFFUR-INDIC and DIFFUR-MLT perform best (69.6, 69.0 CALIB). This is graphically illustrated in Figure 3. DIFFUR-MLT performs consistently well across different Œª values (left plot), and gives a high style change without much drop in content similarity to the input as Œª is varied (right plot); more control experiments in Appendix F.
In Table 2 we provide a breakdown by individual metrics. In the baseline Hindi UR model, we notice high COPY rates (45.4%), resulting in lower

Input
Informal ‡§Ö‡§™‡§®‡•Ä ‡§µ‡§æ‡§≤‡•Ä ‡§ú‡•â‡§¨ ‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§§ ‡§¨‡§§‡§æ‡§ì. (don‚Äôt tell me about your job)
Formal ‡§π‡§∏‡§Ç ‡§æ ‡§Æ‡•á‡§Ç ‡§¶‡•ã ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§Æ‡•å‡§§ ‡§π‡•Å‡§à ‡§•‡•Ä ‡§î‡§∞ ‡§≤‡§ó‡§≠‡§ó 150 ‡§ò‡§æ‡§Ø‡§≤ ‡§π‡•Å‡§è ‡§•‡•á‡•§ (two people died in the violence and 150 were injured)
Positive Sentiment ‡§Æ‡•Å‡§ù‡•á ‡§Ø‡§π ‡§´‡§≤‡•ç‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§™‡§∏‡§Ç‡§¶ ‡§Ü‡§à ‡§§‡•Å‡§Æ ‡§§‡•ã‡§π ‡§ï‡§æ‡§´‡•Ä ‡§á‡§Ç‡§ü‡•á‡§≤‡•Ä‡§ú‡§Ç‡•á‡§ü ‡§π‡•ã

Generations

Analysis

Formal
(ùúÜ = 0.5) ‡§Ö‡§™‡§®‡•Ä ‡§µ‡§æ‡§≤‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä ‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§§ ‡§¨‡§§‡§æ‡§ì‡•§ (ùúÜ = 1.0) ‡§Ö‡§™‡§®‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡§Ç‡•á ‡§Æ‡•Å‡§ù‡•á ‡§¨‡§§‡§æ‡§®‡•á ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§®‡§π‡•Ä‡§Ç‡•§ (ùúÜ = 1.5) ‡§Ü‡§™‡§ï‡•Ä ‡§®‡§Ø‡•Å‡§ø ‡§§ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§ù‡•á ‡§®‡§æ ‡§¨‡§§‡§æ‡§è‡§Ç‡•§

As sentences get more formal,
the english word ‚Äújob‚Äù (‡§ú‡•â‡§¨) is converted to Persian (‡§®‡•å‡§ï‡§∞‡•Ä) / high Sanskrit ( ‡§®‡§Ø‡•Å‡§ø ‡§§) and honorifics are used (‡§Ü‡§™‡§ï‡•Ä, ‡§¨‡§§‡§æ‡§è‡§Ç)

Informal
(ùúÜ = 1.0) ‡§π‡§∏‡§Ç ‡§æ ‡§Æ‡•á‡§Ç ‡§¶‡•ã ‡§≤‡•ã‡§ó ‡§Æ‡§æ‡§∞‡•á ‡§ó‡§è ‡§î‡§∞ 150 ‡§ï‡•á ‡§ï‡§∞‡•Ä‡§¨ ‡§≤‡•ã‡§ó ‡§ò‡§æ‡§Ø‡§≤ ‡§π‡•ã ‡§ó‡§è. (ùúÜ = 1.5) ‡§π‡§∏‡§Ç ‡§æ ‡§Æ‡§Ç‡•á 2 ‡§≤‡•ã‡§ó ‡§Æ‡§æ‡§∞‡•á ‡§ó‡§è ‡§•‡•á ‡§µ 150 ‡§≤‡•ã‡§ó ‡§ò‡§æ‡§Ø‡§≤ ‡§π‡•Å‡§è ‡§•‡•á (ùúÜ = 2.0) ‡§π‡§∏‡§Ç ‡§æ ‡§Æ‡§Ç‡•á 2 ‡§≤‡•ã‡§ó ‡§Æ‡§æ‡§∞‡•á ‡§ó‡§è ‡§î‡§∞ 150 ‡§ò‡§æ‡§Ø‡§≤

As sentences get more informal besides lexical changes, sentence shortening is common, while roughly conveying same meaning

Negative Sentiment
‡§á‡§∏ ‡§´‡§≤‡•ç‡§Æ ‡§ï‡•ã ‡§Æ‡§®‡§Ç‡•à ‡•á ‡§ï‡§≠‡•Ä ‡§™‡§∏‡§Ç‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§Ø‡§æ. ‡§§‡•Å‡§Æ ‡§¨‡•á‡§π‡§¶ ‡§Ö‡§®‡§æ‡•ú‡•Ä ‡§π‡•ã.

Negations (‡§®‡§π‡•Ä‡§Ç) and word antonyms (‡§á‡§Ç‡§ü‡•á‡§≤‡•Ä‡§ú‡•á‡§Ç‡§ü, ‡§Ö‡§®‡§æ‡•ú‡•Ä) are
common as sentiment changes

Complex ‡§≠‡§æ‡§ú‡§™‡§æ ‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø ‡§ï‡§∞‡§§‡•Ä ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§ï ‡§†‡§® ‡§™ ‡§∞‡§∂‡•ç‡§∞‡§Æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à.
Monocode 01.2017 ‡§∏‡•á, ‡§Ö‡§•‡§æ‡•ç‡§µ‡§∞‡§ø‡§§ ‡§á‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•á ‡§ö‡§æ‡§≤‡•Ç ‡§π‡•ã‡§®‡•á ‡§ï‡•Ä ‡§§ ‡§• ‡§∏‡•á ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§¨‡•ã‡§≤‡•Ä ‡§≤‡§ó‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§è ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞‡•Ä ‡§∏‡•á‡§µ‡§æ‡§è‡§Ç

Simple ‡§≠‡§æ‡§ú‡§™‡§æ ‡§Æ‡§ú‡§æ‡§ï ‡§ï‡§∞‡§§‡•Ä ‡§¶‡§ñ ‡§∞‡§π‡•Ä ‡§π‡•à‡•§ ‡§ï‡•ú‡•Ä ‡§ö‡•Ä‡§ú‡§Ç‡•á ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
Code-mixed 01.2017, i.e. ‡§â‡§∏ ‡§°‡§ü‡•á ‡§∏‡•á, ‡§ú‡§¨ ‡§∏‡•á ‡§Ø‡§π ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§á‡§Ç‡§ü‡•Ä‡§ó‡•ç‡§∞‡•á‡§ü‡•á‡§° ‡§π‡•à
‡§¨‡•ã‡§≤‡•Ä ‡§≤‡§ó‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§è ‡§ï‡§æ‡§â‡§Ç ‡§∏ ‡§≤‡§ó‡§Ç ‡§∏ ‡§µ‡§ø‡•ç‡§∞‡§µ ‡§∏‡•õ

De-anonymized
‡§´‡§≤‡•ç‡§Æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§•‡§∞‡•ç‡§•‡•Ä ‡§î‡§∞ ‡§Ö ‡§¶ ‡§§ ‡§∞‡§æ‡§µ ‡§π‡•à‡§¶‡§∞‡•Ä ‡§Æ‡§ñ‡•Å ‡•ç‡§Ø ‡§ï‡§∞‡§¶‡§æ‡§∞ ‡§®‡§≠‡§æ‡§§‡•á ‡§π‡•Å‡§è ‡§®‡§ú‡§∞ ‡§Ü ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ ‡§î‡§∞ ‡§á‡§∏‡§Æ‡§æ‡§à‡§≤, ‡§Ö‡§≤‡§Ø‡§∏‡§Ö, ‡§Ø‡•Ç‡§®‡•Å‡§∏ ‡§î‡§∞ ‡§≤‡§§‡•Ç ‡§ï‡•ã ‡§≠‡•Ä‡•§ ‡§á‡§®‡§Æ‡§Ç‡•á ‡§∏‡•á ‡§π‡§∞ ‡§è‡§ï ‡§ï‡•ã ‡§π‡§Æ‡§®‡•á ‡§∏‡§Ç‡§∏‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•ò‡•Å ‡§æ‡§¨‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§†‡§§‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡•Ä

Anonymized ** ‡§´‡§≤‡•ç‡§Æ ‡§Æ‡§Ç‡•á PII ‡§î‡§∞ PII PII ‡§Æ‡§ñ‡•Å ‡•ç‡§Ø ‡§≠‡•Ç ‡§Æ‡§ï‡§æ ‡§®‡§≠‡§æ‡§§‡•á ‡§π‡•Å‡§è ‡§®‡§ú‡§∞ ‡§Ü ‡§∞‡§π‡•á ‡§π‡§Ç‡•à‡•§
‡§î‡§∞ PII, PII, PII ‡§î‡§∞ PII ‡§ï‡•ã ‡§≠‡•Ä‡•§ ‡§á‡§®‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§ï‡•ã ‡§π‡§Æ‡§®‡•á ‡§∏‡§Ç‡§∏‡§æ‡§∞ ‡§ï‡•á ‡§µ‡§∞‡•Å‡§¶‡•ç‡§ß ‡§ä‡§Å ‡§ö‡§æ‡§á‡§Ø‡§æ‡§Å ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡•Ä

Lexical substitutions (‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø ‚Üí ‡§Æ‡§ú‡§æ‡§ï, ‡§ï ‡§†‡§® ‚Üí ‡§ï‡•ú‡•Ä) to use more commonly spoken words
With code-mixing, several english words are introduced ( ‡§§ ‡§• ‚Üí ‡§°‡§ü‡•á / date, ‡§Ö‡§•‡§æ‡§ø‡§∞‡§µ‡•ç‡§§ ‚Üí i.e., ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞‡•Ä ‡§∏‡•á‡§µ‡§æ‡§è‡§Ç ‚Üí ‡§ï‡§æ‡§â‡§Ç ‡§∏ ‡§≤‡§ó‡§Ç ‡§∏ ‡§µ‡§ø‡§∞‡•ç‡§µ ‡§∏‡•õ / counseling services)
Entities (‡§Ö ‡§¶ ‡§§ ‡§∞‡§æ‡§µ ‡§π‡•à‡§¶‡§∞‡•Ä, ‡§á‡§∏‡§Æ‡§æ‡§à‡§≤) are replaced with PII (Personal Identifiable Information) tags, to anonymize text

Gendered
‡§∞‡§Ø‡•ã ‡§ì‡§≤‡§Ç ‡§™‡§ï : ‡§¨‡•à‡§° ‡§Æ‡§ü‡§Ç ‡§® ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§Æ ‡§π‡§≤‡§æ‡§ì‡§Ç ‡§®‡•á ‡§ï‡§Ø‡§æ ‡§®‡§∞‡§æ‡§∂, ‡§π‡§æ‡§∞ ‡§∏‡•á ‡§π‡•Å‡§à ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§

Gender Neutral **
‡§∞‡§Ø‡•ã ‡§ì‡§≤‡§Ç ‡§™‡§ï : ‡§¨‡•à‡§° ‡§Æ‡§ü‡§Ç ‡§® ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§ñ‡§≤‡§æ ‡•ú‡§Ø‡•ã‡§Ç ‡§®‡•á ‡§ï‡§è ‡§®‡§∞‡§æ‡§∂, ‡§π‡§æ‡§∞ ‡§∏‡•á ‡§π‡•Å‡§è ‡§∂‡•Å‡§∞‡•Ç

Gendered words (‡§Æ ‡§π‡§≤‡§æ‡§ì‡§Ç) are replaced with their neutral
equivalents ( ‡§ñ‡§≤‡§æ ‡•ú‡§Ø‡•ã‡§Ç)

Figure 4: Outputs and qualitative analysis of our best performing model for several attribute transfer tasks (Œª is transfer magnitude). We notice lower quality qualitatively for ** marked styles; see Appendix J for more outputs.

ACC scores. COPY reduces in our proposed models (4.4% for DIFFUR-MLT), which boosts overall performance. We Ô¨Ånd the lowest COPY (and lowest 1-g) for models with +BT (1%), which is due to two translation steps. However, this lowers semantic similarity (also seen in Table 4) lowering the overall score (60.0 vs 78.1) compared to DIFFUR-MLT.
In Appendix G we show ablations studies justifying the DIFFUR design, decoding scheme, etc. In Appendix I we show a breakdown by individual metrics for other languages and plot variations with Œª. We also analyze the style encoder fstyle in Appendix H, Ô¨Ånding it is an effective style classiÔ¨Åer.
We analyze several qualitative outputs from DIFFUR-MLT in Figure 4. Besides formality transfer and code-mixing addition, we transfer several

other attributes: sentiment (Li et al., 2018), simplicity (Xu et al., 2015), anonymity (Anandan et al., 2012) and gender neutrality (Reddy and Knight, 2016). More outputs are provided in Appendix J.
7 Conclusion
We present a recipe for building & evaluating controllable few-shot style transfer systems needing only 3-10 style examples at inference, useful in low-resource settings. Our methods outperform prior work in formality transfer & code-mixing for 7 languages, with promising qualitative results for several other attribute transfer tasks. Future work includes further improving systems for some attributes, and studying style transfer for languages where little / no translation data is available.

Acknowledgements
We are very grateful to the Task Mate team (especially Auric Bonifacio Quintana) for their support and helping us crowdsource data and evaluate models on their platform. We thank John Wieting, Timothy Dozat, Manish Gupta, Rajesh Bhatt, Esha Banerjee, Yixiao Song, Marzena Karpinska, Aravindan Raghuveer, Noah Constant, Parker Riley, Andrea Schioppa, Artem Sokolov, Mohit Iyyer and Slav Petrov for several useful discussions during the course of this project. We are also grateful to Rajiv Teja Nagipogu, Shachi Dave, Bhuthesh R, Parth Kothari, Bhanu Teja Gullapalli and Simran Khanuja for helping us annotate model outputs in several Indian languages during pilot experiments. This work was mostly done during Kalpesh Krishna (KK)‚Äôs internship at Google Research India, hosted by Bidisha Samanta and Partha Talukdar. KK was partly supported by a Google PhD Fellowship.
Ethical Considerations
Recent work has highlighted issues of stylistic bias in text generation systems, speciÔ¨Åcally machine translation systems (Hovy et al., 2020). We acknowledge these issues, and consider style transfer and style-controlled generation technology as an opportunity to work towards Ô¨Åxing them (for instance, gender neutralization as presented in Section 6.2). Note that it is important to tread down this path carefully ‚Äî In Chapter 9, Blodgett (2021) argue that style is inseparable from social meaning (as originally noted by Eckert, 2008), and humans may perceive automatically generated text very differently compared to automatic style classiÔ¨Åers.
Our models were trained on 32 Google Cloud TPUs. As discussed in Appendix A, the UR & UR-INDIC model take roughly 18 hours to train. The DIFFUR-* and DIFFUR-MLT models are much cheaper to train (2 hours) since we Ô¨Ånetune the pretrained UR-* models. The Google 2020 environment report mentions,15 ‚ÄúTPUs are highly efÔ¨Åcient chips which have been speciÔ¨Åcally designed for machine learning applications‚Äù. These accelerators run on Google Cloud, which is carbon neutral today, and is aiming to ‚Äúrun on carbon-free energy, 24/7, at all of Google‚Äôs data centers by 2030‚Äù (https://cloud.google. com/sustainability).
15https://www.gstatic.com/ gumdrop/sustainability/ google-2020-environmental-report.pdf

References
Rama Kant Agnihotri. 2013. Hindi: An essential grammar. Routledge.
Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470‚Äì6484, Online. Association for Computational Linguistics.
Balamurugan Anandan, Chris Clifton, Wei Jiang, Mummoorthy Murugesan, Pedro PastranaCamacho, and Luo Si. 2012. t-plausibility: Generalizing words to desensitize text. Transactions on Data Privacy, 5(3):505‚Äì534.
Kalika Bali, Jatin Sharma, Monojit Choudhury, and Yogarshi Vyas. 2014. ‚Äúi am borrowing ya mixing?" an analysis of english-hindi code mixing in facebook. In Proceedings of the First Workshop on Computational Approaches to Code Switching, pages 116‚Äì 126.
Su Lin Blodgett. 2021. Sociolinguistically driven approaches for just natural language processing. UMass Amherst Doctoral Dissertations. 2092.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Github.
Eleftheria Briakou, Sweta Agrawal, Ke Zhang, Joel Tetreault, and Marine Carpuat. 2021a. A review of human evaluation for style transfer. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), Online.
Eleftheria Briakou, Di Lu, Ke Zhang, and Joel Tetreault. 2021b. Ol√°, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3199‚Äì3216, Online. Association for Computational Linguistics.
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799.
Daniel Cer, Mona Diab, Eneko Agirre, I√±igo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1‚Äì14, Vancouver, Canada. Association for Computational Linguistics.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440‚Äì 8451, Online. Association for Computational Linguistics.
Alexis Conneau and Guillaume Lample. 2019. Crosslingual language model pretraining. Proceedings of Advances in Neural Information Processing Systems, 32:7059‚Äì7069.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of Empirical Methods in Natural Language Processing, pages 2475‚Äì2485.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171‚Äì4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Penelope Eckert. 2008. Variation and the indexical Ô¨Åeld 1. Journal of sociolinguistics, 12(4).
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Languageagnostic bert sentence embedding. arXiv preprint arXiv:2007.01852.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.
Xavier Garcia, Noah Constant, Mandy Guo, and Orhan Firat. 2021. Towards universality in multilingual text rewriting. arXiv preprint arXiv:2107.14749.
Tanya Goyal and Greg Durrett. 2020. Neural syntactic preordering for controlled paraphrase generation. Proceedings of the Association for Computational Linguistics.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2018. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics.
Junxian He, Taylor Berg-Kirkpatrick, and Graham Neubig. 2020. Learning sparse prototypes for text generation. Advances in Neural Information Processing Systems, 33.
George Heidorn. 2000. Intelligent writing assistance. Handbook of natural language processing, pages 181‚Äì207.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of the International Conference on Learning Representations.
Dirk Hovy, Federico Bianchi, and Tommaso Fornaciari. 2020. ‚Äúyou sound just like your father‚Äù commercial machine translation systems include stylistic biases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1686‚Äì1690.
Daphne Ippolito, Daniel Duckworth, Chris CallisonBurch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1808‚Äì1822, Online. Association for Computational Linguistics.
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1875‚Äì1885, New Orleans, Louisiana. Association for Computational Linguistics.
Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg. 2017. Shakespearizing modern language using copy-enriched sequence to sequence models. In Proceedings of the Workshop on Stylistic Variation, pages 10‚Äì19.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282‚Äì6293, Online. Association for Computational Linguistics.
Yamuna Kachru. 2006. Hindi, volume 12. John Benjamins Publishing.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81‚Äì93.
Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and Monojit Choudhury. 2020. Gluecos: An evaluation benchmark for codeswitched nlp. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3575‚Äì3585.
Elizaveta Korotkova, Agnes Luhtaru, Maksym Del, Krista Liin, Daiga Deksne, and Mark Fishel. 2019. Grammatical error correction and style transfer via zero-shot monolingual translation. arXiv preprint arXiv:1903.11283.
Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov,

Claytone Sikasote, et al. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50‚Äì72.
Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737‚Äì762, Online. Association for Computational Linguistics.
Ritesh Kumar. 2014. Developing politeness annotated corpus of Hindi blogs. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC‚Äô14), pages 1275‚Äì 1280, Reykjavik, Iceland. European Language Resources Association (ELRA).
Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and Hyung Won Chung. 2021. Neural data augmentation via example extrapolation. arXiv preprint arXiv:2102.01335.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865‚Äì1874, New Orleans, Louisiana. Association for Computational Linguistics.
Benjamin Marie, Atsushi Fujita, and Raphael Rubino. 2021. ScientiÔ¨Åc credibility of machine translation research: A meta-evaluation of 769 papers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7297‚Äì 7306, Online. Association for Computational Linguistics.
Remi Mir, Bjarke Felbo, Nick Obradovich, and Iyad Rahwan. 2019. Evaluating style transfer for text. In Conference of the North American Chapter of the Association for Computational Linguistics.
Shuyo Nakatani. 2010. Language detection library for java.
Xing Niu and Marine Carpuat. 2020. Controlling neural machine translation formality with synthetic supervision. In Association for the Advancement of ArtiÔ¨Åcial Intelligence.
Xing Niu, Sudha Rao, and Marine Carpuat. 2018. Multi-task neural models for translating between styles within and across languages. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1008‚Äì1021.
Richard Yuanzhe Pang. 2019. Towards actual (not operational) textual style transfer auto-evaluation. In Proceedings of the 5th Workshop on Noisy Usergenerated Text (W-NUT 2019).

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.
Jonas Pfeiffer, Andreas R√ºckl√©, Clifton Poth, Aishwarya Kamath, Ivan Vulic¬¥, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020a. Adapterhub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations, Online.
Jonas Pfeiffer, Ivan Vulic¬¥, Iryna Gurevych, and Sebastian Ruder. 2020b. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of Empirical Methods in Natural Language Processing, Online.
Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Adithya Pratapa, Antonios Anastasopoulos, Shruti Rijhwani, Aditi Chaudhary, David R. Mortensen, Graham Neubig, and Yulia Tsvetkov. 2021. Evaluating the morphosyntactic well-formedness of generated texts. In Proceedings of Empirical Methods in Natural Language Processing.
Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan Dandapat, and Kalika Bali. 2018. Language modeling for code-mixing: The role of linguistic theory based synthetic data. In Proceedings of the Association for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of Empirical Methods in Natural Language Processing.
Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra. 2021. Samanantar: The largest publicly available parallel corpora collection for 11 indic languages.
Justus J Randolph. 2005. Free-marginal multirater kappa (multirater k [free]): An alternative to Ô¨Çeiss‚Äô Ô¨Åxed-marginal multirater kappa. Online submission.
Sudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Conference of the North American Chapter of the Association for Computational Linguistics.

Sravana Reddy and Kevin Knight. 2016. Obfuscating gender in social media writing. In Proceedings of the First Workshop on NLP and Computational Social Science, pages 17‚Äì26.
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2021. A recipe for arbitrary text style transfer with large language models. arXiv preprint arXiv:2109.03910.
Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of Empirical Methods in Natural Language Processing.
Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David Uthus, and Zarana Parekh. 2021. TextSETTR: Few-shot text style extraction and tunable targeted restyling. In Proceedings of the Association for Computational Linguistics.
Bidisha Samanta, Mohit Agrawal, and Niloy Ganguly. 2021. A hierarchical vae for calibrating attributes while generating text using normalizing Ô¨Çow. In Proceedings of the Association for Computational Linguistics.
Bidisha Samanta, Sharmila Reddy, Hussain Jagirdar, Niloy Ganguly, and Soumen Chakrabarti. 2019. A deep generative model for code-switched text. arXiv preprint arXiv:1906.08972.
Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao, Shuming Shi, and Rui Yan. 2019. Semi-supervised text style transfer: Cross projection in latent space. In Proceedings of Empirical Methods in Natural Language Processing.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2017. Style transfer from non-parallel text by cross-alignment. In Advances in neural information processing systems, pages 6830‚Äì6841.
Rakshith Shetty, Bernt Schiele, and Mario Fritz. 2018. A4nt: author attribute anonymity by adversarial training of neural machine translation. In 27th {USENIX} Security Symposium ({USENIX} Security 18), pages 1633‚Äì1650.
Eric Michael Smith, Diana Gonzalez-Rico, Emily Dinan, and Y-Lan Boureau. 2020. Controlling style in generated dialogue. arXiv preprint arXiv:2009.10855.
Sandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic Denoyer, Marc‚ÄôAurelio Ranzato, and Y-Lan Boureau. 2019. Multiple-attribute text style transfer. In Proceedings of the International Conference on Learning Representations.
Aleksey Tikhonov and Ivan P Yamshchikov. 2018. Sounds wilde. phonetically extended embeddings for author-stylized poetry generation. In Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology.

Alexey Tikhonov, Viacheslav Shibaev, Aleksander Nagaev, Aigul Nugmanova, and Ivan P Yamshchikov. 2019. Style transfer for texts: Retrain, report errors, compare with rewrites. In Proceedings of Empirical Methods in Natural Language Processing.
Ke Wang, Hang Hua, and Xiaojun Wan. 2019. Controllable unsupervised text attribute transfer via editing entangled latent representation. Advances in Neural Information Processing Systems, 32:11036‚Äì11046.
Matthijs J Warrens. 2010. Inequalities between multirater kappas. Advances in data analysis and classiÔ¨Åcation, 4(4):271‚Äì286.
John Wieting and Kevin Gimpel. 2018. ParaNMT50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In Proceedings of the Association for Computational Linguistics, pages 451‚Äì462, Melbourne, Australia.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33:6256‚Äì6268.
Peng Xu, Jackie Chi Kit Cheung, and Yanshuai Cao. 2020. On variational learning of controllable representations for text without supervision. In International Conference on Machine Learning.
Wei Xu, Chris Callison-Burch, and Courtney Napoles. 2015. Problems in current text simpliÔ¨Åcation research: New data can help. Transactions of the Association for Computational Linguistics, 3:283‚Äì297.
Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. 2012. Paraphrasing for style. In Proceedings of International Conference on Computational Linguistics.
Linting Xue, Aditya Barua, Noah Constant, Rami AlRfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2021a. Byt5: Towards a tokenfree future with pre-trained byte-to-byte models. arXiv preprint arXiv:2105.13626.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021b. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics.
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. 2020. Multilingual universal sentence encoder for semantic retrieval. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations.
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In Association for Computational Linguistics.

Appendices for ‚ÄúFew-shot Controllable Style Transfer for Low-Resource Multilingual Settings‚Äù
A Model training details
To train the UR-INDIC model, we use mC4 (Xue et al., 2021b) for the self-supervised objectives and Samanantar (Ramesh et al., 2021) for the supervised translation. For creating paraphrase data for training our DIFFUR models (Section 4.2), we again leverage Indic language side of Samanantar sentence pairs. Our models are implemented in JAX (Bradbury et al., 2018) using the T5X library.16 We re-use the UR checkpoint from Garcia et al. (2021). To train the UR-INDIC model, we follow the setup in Garcia et al. (2021) and initialize the model with mT5-XL (Xue et al., 2021b), which has 3.7B parameters. We Ô¨Åne-tune the model for 25K steps with a batch size of 512 inputs and a learning rate of 1e-3, using the objectives in Section 3. Training was done on 32 Google Cloud TPUs which took a total of 17.5 hours. To train the DIFFUR and DIFFUR-INDIC models, we further Ô¨Ånetune UR and UR-INDIC for a total of 4K steps using the objective from Section 4.2, taking 2 hours.
B More Related Work
Multilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018). Briakou et al. (2021b) further introduced XFORMAL, the Ô¨Årst formality transfer evaluation dataset in French, Brazilian Portugese and Italian.17 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and codemixing (Bali et al., 2014). Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018; Samanta et al., 2019) and core NLP tasks (Khanuja et al., 2020). To the best of our knowledge, we are the Ô¨Årst to study style transfer for Indic languages. A few prior works build models which can control the degree of style transfer using a scalar input (Wang et al., 2019; Samanta et al., 2021).
16https://github.com/google-research/ t5x
17We do not use this data since it does not cover Indian languages, and due to Yahoo! L6 corpus restrictions for industry researchers (conÔ¨Årmed via authors correspondence).

However, these models are style-speciÔ¨Åc and require large unpaired style corpora during training. We adopt the inference-time control method used by Garcia et al. (2021) and notice much better controllability after our proposed Ô¨Åxes in Section 4.2.
C More details on the translation-speciÔ¨Åc Universal Rewriter objectives
In this section we describe the details of the supervised translation objective and the style-controlled translation objective used in the Universal Rewriter model. See Section 3 for details on the exemplarbased denoising objective. Learning translation via direct supervision: This objective is the standard supervised translation setup, using zero vectors for style. The output language code is prepended to the input. Consider a pair of parallel sentences (x, y) in languages with codes lx, ly (prepended to the input string),
y¬Ø = fur(ly ‚äï x, 0) Ltranslate = LCE(y¬Ø, y)
The Universal Rewriter is trained on Englishcentric translation data from the high-resource languages in OPUS-100 (Zhang et al., 2020).
Learning style-controlled translation: This objective emulates "style-controlled translation" in a self-supervised manner, via backtranslation through English. Consider x1 and x2 to be two non-overlapping spans in mC4 in language lx,
xe2n = fur(en ‚äï x2, ‚àífstyle(x1)) x¬Ø2 = fur(lx ‚äï xe2n, fstyle(x1)) LBT = LCE(x¬Ø2, x2)
D Choice of Exemplars
Formal exemplars 1. This was a remarkably thought-provoking read. 2. It is certainly amongst my favorites. 3. We humbly request your presence at our gala in the coming week. Informal exemplars 1. reading this rly makes u think 2. Its def one of my favs 3. come swing by our bbq next week if ya can make it
Complex exemplars

Codemixed Exemplars 1. ‡§ó‡•Å‡§° ‡§Æ‡•â ‡§®‡§ó‡§®‡§Ç‡•ç‡§ø‡§∞ , ‡§≠‡§æ‡§∞‡§§ 2. ‡§Ö‡§ó‡§∞ ‡§Ü‡§™ ‡§á‡§∏‡•á ‡§´‡•ç‡§∞‡•Ä‡§ú ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§ü‡•á‡§Ç‡§™‡•á‡§∞‡•á‡§ö‡§∞ ‡§ï‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ ‡§π‡§è 3. ‡§π‡§æ‡§Ø ‡§Æ‡•Å‡§ù‡•á ‡§ú‡•â‡§¨ ‡§ö‡§æ ‡§π‡§è 4. ‡§π‡•â‡§≤‡•Ä‡§µ‡•Å‡§° ‡§è‡§ï‡•ç‡§ü‡•ç‡§∞‡•á‡§∏ ‡§è‡§Ç‡§ú‡•á ‡§≤‡§®‡§æ ‡§ú‡•â‡§≤‡•Ä ‡§è‡§ï ‡§è ‡§®‡§Æ‡•á‡§∂‡§® ‡§´‡§≤‡•ç‡§Æ ‡§™‡•ç‡§∞‡•ã‡§°‡•ç‡§Ø‡•Ç‡§∏ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç‡•§ 5. ‡§á‡§∏ ‡§ü‡•Ç ‡§®‡§Æ‡§æ‡§∞‡•ç‡§ß ‡§Ç‡•á‡§ü ‡§Æ‡§Ç‡•á 6 ‡§ü‡•Ä‡§Æ‡§Ç‡•á ‡§ü‡§æ‡§á‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§è ‡§ï‡§Æ‡•ç‡§™‡•Ä‡§ü‡•ç ‡§ï‡§∞‡§Ç‡•á‡§ó‡•Ä‡•§
Monocode Exemplars 1. ‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§, ‡§≠‡§æ‡§∞‡§§ 2. ‡§Ö‡§ó‡§∞ ‡§Ü‡§™ ‡§á‡§∏‡•á ‡§ú‡§Æ‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§ï‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ ‡§π‡§è 3. ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§Æ‡•Å‡§ù‡•á ‡§®‡•å‡§ï‡§∞‡•Ä ‡§ö‡§æ ‡§π‡§è 4. ‡§π‡•â‡§≤‡•Ä‡§µ‡•Å‡§° ‡§Ö ‡§≠‡§®‡•á‡§§‡•ç‡§∞‡•Ä ‡§è‡§Ç‡§ú‡•á ‡§≤‡§®‡§æ ‡§ú‡•ã‡§≤‡•Ä ‡§è‡§ï ‡§ö‡§≤ ‡§ö‡§§‡•ç‡§∞ ‡§ï‡§æ ‡§®‡§Æ‡§æ‡§æ‡§ß‡•ç‡§∞‡§£ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç‡•§ 5. ‡§á‡§∏ ‡§ñ‡•á‡§≤ ‡§™‡•ç‡§∞ ‡§§‡§Ø‡•ã ‡§ó‡§§‡§æ ‡§Æ‡§Ç‡•á ‡§õ‡§π ‡§∏‡§Æ‡•Ç‡§π ‡§ñ‡§§‡§æ‡§¨ ‡§ï‡•á ‡§≤‡§è ‡§™‡•ç‡§∞ ‡§§‡§∏‡•ç‡§™‡§ß‡§æ‡§∞‡§ß‡•ç‡§æ ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á‡•§
Figure 5: Exemplars used for adding code-mixing.
Gendered Exemplars 1. ‡§®‡§∏‡§Æ‡§∞‡•ç ‡§∏‡§æ‡§´ ‡§ï‡§™‡•ú‡•á ‡§™‡§π‡§®‡•Ä ‡§•‡•Ä 2. ‡§π‡§Æ‡§Ç‡•á ‡§î‡§∞ ‡§ú‡§®‡§∂‡§ø‡§ï‡•ç‡§§ ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§π‡•à 3. ‡§Ø‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à
Gender-neutral Exemplars 1. ‡§®‡§∏‡§Æ‡•ç‡§∞ ‡§®‡•á ‡§∏‡§æ‡§´ ‡§ï‡§™‡•ú‡•á ‡§™‡§π‡§®‡•á ‡§•‡•á 2. ‡§π‡§Æ‡§Ç‡•á ‡§î‡§∞ ‡§ï‡§Æ‡§ö‡§∞‡§Æ‡•ç ‡§æ ‡§∞‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§π‡•à 3. ‡§Ø‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§π‡§Ç‡•à
Figure 6: Exemplars used for gender neutralization.
1. The static charges remain on an object until they either bleed off to ground or are quickly neutralized by a discharge. 2. It is particularly famous for the cultivation of kiwifruit. 3. Notably absent from the city are fortiÔ¨Åcations and military structures. Simple exemplars 1. Static charges last until they are grounded or discharged. 2. This area is known for growing kiwifruit. 3. Some things important missing from the city are

De-anonymized Exemplars
1. ‡§Æ‡•á‡§∞‡§æ ‡§´‡•ã‡§® ‡§®‡§Ç‡§¨‡§∞ 091898807646 ‡§π‡•à 2. ‡§ï‡•á ‡§ü ‡§ï‡§æ ‡§Ü‡§ß‡§æ‡§∞ ‡§®‡§Ç‡§¨‡§∞ ‡§π‡•à 4098-7980-8098 3. 18 ‡§∏‡§§‡§Ç‡§¨‡§∞ ‡§ï‡•ã ‡§Æ‡§®‡•à‡§Ç ‡•á microsoft.com ‡§™‡§∞ ‡§µ‡§ø‡•õ‡§ü ‡§ï‡§Ø‡§æ ‡§î‡§∞ IP 192.168.0.1 ‡§∏‡•á test@google.site ‡§™‡§∞ ‡§è‡§ï ‡§à‡§Æ‡•á‡§≤ ‡§≠‡•á‡§ú‡§æ‡•§ 4. ‡§Æ‡•á‡§∞‡§æ ‡§™‡§æ‡§∏‡§™‡•ã‡§ü‡§∞‡•ç‡§ü ‡§®‡§Ç‡§¨‡§∞ 4903-3289-2394 ‡§π‡•à 5. ‡§´‡§≤ Google ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§∞‡§¨‡§∞‡§æ ‡§ï‡•Ä ‡§ü‡•Ä‡§Æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à 6. ‡§¨‡•â‡§¨ 42 ‡§∏‡§æ‡§≤ ‡§ï‡§æ ‡§π‡•à 7. ‡§∂‡§≤‡§ï‡§ü‡•ç‡§∞ 221B ‡§¨‡•á‡§ï‡§∞ ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ü ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•à 8. ‡§Æ‡•á‡§∞‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§™‡§§‡§æ ‡§π‡•à email1@gmail.com
Anonymized Exemplars
1. ‡§Æ‡•á‡§∞‡§æ ‡§´‡•ã‡§® ‡§®‡§Ç‡§¨‡§∞ PII ‡§π‡•à 2. PII ‡§ï‡§æ ‡§Ü‡§ß‡§æ‡§∞ ‡§®‡§Ç‡§¨‡§∞ ‡§π‡•à PII 3. PII ‡§ï‡•ã ‡§Æ‡§®‡§Ç‡•à ‡•á PII ‡§™‡§∞ ‡§µ‡§ø‡•õ‡§ü ‡§ï‡§Ø‡§æ ‡§î‡§∞ IP PII ‡§∏‡•á PII ‡§™‡§∞ ‡§è‡§ï ‡§à‡§Æ‡•á‡§≤ ‡§≠‡•á‡§ú‡§æ‡•§ 4. ‡§Æ‡•á‡§∞‡§æ ‡§™‡§æ‡§∏‡§™‡•ã‡§ü‡§∞‡•ç‡§ü ‡§®‡§Ç‡§¨‡§∞ PII ‡§π‡•à 5. PII PII ‡§Æ‡§Ç‡•á PII ‡§ï‡•Ä ‡§ü‡•Ä‡§Æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à 6. PII PII ‡§∏‡§æ‡§≤ ‡§ï‡§æ ‡§π‡•à 7. PII PII ‡§Æ‡§Ç‡•á ‡§∞‡§π‡§§‡§æ ‡§π‡•à 8. ‡§Æ‡•á‡§∞‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§™‡§§‡§æ ‡§π‡•à PII
Figure 7: Exemplars used for text anonymization. All entities in the deanonymized exemplars are random.
protective buildings and military buildings.
Positive sentiment exemplars 1. The most comfortable bed I‚Äôve ever slept on, I highly recommend it. 2. I loved it. 3. The movie was fantastic. Negative sentiment exemplars 1. The most uncomfortable bed I‚Äôve ever slept on, I would never recommend it. 2. I hated it. 3. The movie was awful.
E Evaluation Appendix
E.1 Multilingual ClassiÔ¨Åer Selection
Due to the absence of a style classiÔ¨Åcation dataset in Indic languages, we built our multilingual classiÔ¨Åer drawing inspiration from recent research in zero-shot cross-lingual transfer (Conneau et al., 2018; Conneau and Lample, 2019; Pfeiffer et al., 2020b). We experimented with three zero-shot transfer techniques while selecting our classiÔ¨Åers for evaluating multilingual style transfer.
TRANSLATE TRAIN: The Ô¨Årst technique uses the hypothesis that style is preserved across translation.

We classify the style of English sentences in the Samanantar translation dataset (Ramesh et al., 2021) using a style classiÔ¨Åer trained on English formality data from Krishna et al. (2020). We use the human translated Indic languages sentences as training data. This training data is used to Ô¨Åne-tune a large-scale multilingual language model.
ZERO-SHOT: The second technique Ô¨Åne-tunes large-scale multilingual language models on a English style transfer dataset, and applies it zero-shot on multilingual data during inference.

ble 7 and other languages in Table 8. Consistent with Pfeiffer et al. (2020b), we Ô¨Ånd MAD-X to be a superior zero-shot cross lingual transfer method compared to baselines. We also Ô¨Ånd XLM-R has better multilingual representations than mBERT. Unfortunately, AdapterHub (Pfeiffer et al., 2020a) has XLM-R language adapters available only for Hindi & Tamil (among Indic languages). For other languages we use the ZERO-SHOT technique on XLM-R, consistent with the recommendations13 provided by Krishna et al. (2020) based on their experiments on XFORMAL (Briakou et al., 2021b).

MAD-X: Introduced by Pfeiffer et al. (2020b), this technique is similar to ZERO-SHOT but additionally uses language-speciÔ¨Åc parameters (‚Äúadapters‚Äù) during inference. These language-speciÔ¨Åc adapters have been originally trained using masked language modeling on the desired language data.
Dataset for evaluating classiÔ¨Åers: We conduct our experiments on Hindi formality classiÔ¨Åcation, leveraging our evaluation datasets from Section 5.1. We removed pairs which did not have full agreement across the three annotators and those pairs which had the consensus rating of ‚ÄúEqual‚Äù formality. This Ô¨Åltering process leaves us with 316 pairs in Hindi (out of 1000). In our experiments, we check whether the classiÔ¨Åers give a higher score to the more formal sentence in the pair.
Models: We leverage the multilingual classiÔ¨Åers open-sourced18 by Krishna et al. (2020). These models have been trained on the English GYAFC formality classiÔ¨Åcation dataset (Rao and Tetreault, 2018), and have been shown to be effective on the XFORMAL dataset (Briakou et al., 2021b) for formality classiÔ¨Åcation in Italian, French and Brazilian Portuguese.13 These classiÔ¨Åers were trained on preprocessed data which had trailing punctuation stripped and English sentences lower-cased, encouraging the models to focus on lexical and syntactic choices. As base multilingual language models, we use (1) mBERT-base from Devlin et al. (2019); (2) XLM-RoBERTabase from Conneau et al. (2020).
Results: Our results on Hindi are presented in Ta-
18https://github.com/ martiansideofthemoon/ style-transfer-paraphrase/blob/master/ README-multilingual.md

Method
TRANSLATE TRAIN
ZERO-SHOT
MAD-X

Model
mBERT mBERT XLM-R XLM-R

Accuracy (‚Üë)
66% 72% 76% 81%

Table 7: Hindi formality classiÔ¨Åcation accuracy on our crowdsourced dataset (Section 5.1) using different cross-lingual transfer methods. Our results indicate that MAD-X is the most effective method, and XLM-R is a better pretrained model than mBERT.

Language
bn kn te

mBERT
65.3% 76.3% 72.6%

XLM-R
82.2% 76.9% 74.6%

Table 8: Formality classiÔ¨Åcation on our crowdsourced Bengali, Kannada and Telugu dataset (Section 5.1) using the ZERO-SHOT technique described in Appendix E.1. Results conÔ¨Årm the efÔ¨Åcacy of the XLM-R classiÔ¨Åer. See Table 7 for Hindi results.

E.2 Semantic Similarity Model Selection We considered three models for evaluating semantic similarity between the input and output:
(1) LaBSE (Feng et al., 2020); (2) m-USE (Yang et al., 2020); (3) multilingual Sentence-BERT (Reimers and Gurevych, 2020), the knowledge-distilled variant paraphrase-xlm-r-multilingual-v1
Among these models, only LaBSE has support for all the Indic languages we were interested in. No Indic language is supported by m-USE, and

multilingual Sentence-BERT has been trained on parallel data only for Hindi, Gujarati and Marathi among our Indic languages. However, in terms of Semantic Textual Similarity (STS) benchmarks (Cer et al., 2017) for English, Arabic & Spanish, m-USE and Sentence-BERT outperform LaBSE (Table 1 in Reimers and Gurevych, 2020).

LaBSE correlates better than Sentence-BERT with our human-annotated formality dataset: We measured the Spearman‚Äôs rank correlation between the semantic similarity annotations on our human-annotated formality datasets (Section 5.1). We discarded 10% sentence pairs which had no agreement among three annotators and took the majority vote for the other sentence pairs. We assigned ‚ÄúDifferent Meaning‚Äù a score of 0, ‚ÄúSlight Difference in Meaning‚Äù a score of 1 and ‚ÄúApproximately Same Meaning‚Äù a score of 2 before measuring Spearman‚Äôs rank correlation. In Table 9 we see a stronger correlation of human annotations with LaBSE compared to Sentence-BERT, especially for languages like Bengali, Kannada for which Sentence-BERT did not see parallel data.

Model

hi bn kn te

LaBSE

0.34 0.49 0.39 0.25

Sentence-BERT 0.33 0.36 0.29 0.18

Table 9: Spearman‚Äôs rank correlation between different semantic similarity models and our semantic similarity human annotations collected along with formality labels. Overall, LaBSE correlates more strongly than Sentence-BERT with our annotated data.

E.3 Evaluation with Different LaBSE thresholds
In Section 6, we set our LaBSE threshold L to 0.75. In this section, we present our evaluations with a more and less conservative value of L.
In Table 18, we present results with L = 0.65, and in Table 19 we set L = 0.85. Compared to Table 1, trends are mostly similar, with DIFFUR models and INDIC variants outperforming counterparts. Note that the absolute values of SIM and AGG metrics differ, with absolute values going down with the stricter threshold of L = 0.85, and up with the relaxed threshold of L = 0.65.
Comparing chosen thresholds with human annotations: To verify these three thresholds are rea-

sonable choices, we measure the LaBSE similarity of the sentence pairs annotated by humans, and compare the LaBSE scores to human semantic similarity annotations. We pool the ‚ÄúApproximately Same Meaning‚Äù and ‚ÄúSlight Difference in Meaning‚Äù categories as ‚Äúsame‚Äù, and consider only sentence pairs with a majority rating of ‚Äúsame‚Äù. In Table 10 we see that the chosen thresholds span the spectrum of LaBSE values for the human annotated semantically similar pairs.

Threshold L
0.65 0.75 0.85

% of sentence pairs > L hi bn kn te
97.4 96.1 94.6 90.6 83.9 76.1 68.4 62.6 75.1 62.7 50.5 45.5

Table 10: Percentage of human annotated semantically similar pairs which have a LaBSE score of at least L. As we increase the threshold L, we see this percentage substantially reduces, indicating our chosen thresholds are within the range of variation in LaBSE scores for semantically similar sentences.

E.4 More Crowdsourcing Details
In Figure 17, we show screenshots of our crowdsourcing interface along with all the instructions shown to crowdworkers. The instructions were written after consulting professional Indian linguists. Each crowdworker was allowed to annotate a maximum of 50 different sentence pairs per language, paying them $0.05 per pair. For formality classiÔ¨Åcation, we showed crowdworkers two sentences and asked them to choose which one is more formal. Crowdworkers were allowed to mark ties using an ‚ÄúEqual‚Äù option. For semantic similarity annotation, we showed crowdworkers the sentence pair and provided three options ‚Äî ‚Äúapproximately same meaning‚Äù, ‚Äúslight difference in meaning‚Äù, ‚Äúdifferent meaning‚Äù, to emulate a 3-point Likert scale. While performing our human evaluation (Section 5.7), we use a 0.5 SIM score for ‚Äúslight difference in meaning‚Äù and a 1.0 SIM score for ‚Äúapproximately same meaning‚Äù annotations. For every system considered, we analyzed the same set of 200 input sentences for style transfer performance, and 100 of those sentences for evaluating controllability. We removed sentences which were exact copies of the input (after removing trailing punctuation) or were in the wrong language to save annotator time and cost. When outputs were exact copies of the

input, we assigned SIM = 100, ACC = 0, AGG = 0. In Table 11 and Table 12 we show the inter-
annotator agreement statistics. We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005; Warrens, 2010), the fraction of sentence pairs with total agreement between the three annotators and the fraction of sentence pairs with no agreement.19 In the table we can see all agreement statistics are well away from a uniform random annotation baseline, indicating good agreement.

F-Œ∫ R-Œ∫ all agree none agree

Random 0.00 0.00

hi

0.21 0.28

bn

0.33 0.40

kn

0.22 0.31

te

0.21 0.31

11.1% 32.8% 43.8% 35.0% 36.0%

22.2% 10.2%
7.2% 7.7% 9.3%

Table 11: Fleiss kappa (F-Œ∫), Randolph kappa (R-Œ∫), and agreement scores of crowdsourcing for formality classiÔ¨Åcation. All Œ∫ scores are well above a random annotation baseline, indicating fair agreement.

F-Œ∫ R-Œ∫ all agree none agree

Random 0.00 0.00

hi

0.10 0.27

bn

0.24 0.34

kn

0.13 0.25

te

0.10 0.31

11.1% 32.6% 38.7% 30.8% 36.1%

22.2% 11.8% 10.2% 11.3%
9.7%

Table 12: Fleiss kappa (F-Œ∫), Randolph kappa (R-Œ∫), and agreement scores of crowdsourcing for semantic similarity. All Œ∫ scores are well above a random annotation baseline, indicating fair agreement.

E.5 Fluency Evaluation
Unlike some prior works, we avoid evaluation of output Ô¨Çuency due to the following reasons: (1) lack of Ô¨Çuency evaluation tools for Indic languages;20 (2) Ô¨Çuency evaluation often discriminates against styles which are out-of-distribution for the Ô¨Çuency classiÔ¨Åer, as discussed in Appendix A.8 of Krishna et al. (2020); (3) several prior works (Pang, 2019; Mir et al., 2019; Krishna et al., 2020) have recommended against using perplexity of style language models for Ô¨Çuency evaluation since it is unbounded and favours unnatural sentences with common words; (4) large language
19The Œ∫ scores are measured using the library https: //github.com/statsmodels/statsmodels.
20A potential tool for Ô¨Çuency evaluation in future work is LAMBRE (Pratapa et al., 2021). However, the original paper does not evaluate performance on Indic languages and the grammars for Indic languages would need to collected / built.

models are known to produce Ô¨Çuent text as perceived by humans (Ippolito et al., 2020; Akoury et al., 2020), reducing the need for this evaluation.
E.6 Details of other individual metrics
Language Consistency (LANG): Since our semantic similarity metric LaBSE is languageagnostic, it tends to ignore accidental translations, which are common errors in large multilingual transformers (Xue et al., 2021a,b), especially the Universal Rewriter (Section 3.1). Hence, we check whether the output sentence is in the same language as the input, using langdetect.21
Output Diversity (COPY, 1-g): As discussed in Section 3.1, the Universal Rewriter has a strong tendency to copy the input verbatim. We build two metrics to measure output diversity compared to the input, which have been previously used for extractive question answering evaluation (Rajpurkar et al., 2016). The Ô¨Årst metric COPY measures the fraction of outputs which were copied verbatim from the input. This is done after removing trailing punctuation, to penalize models generations which solely modify punctuation. A second metric 1-g measures the unigram overlap F1 score between the input and output. A diverse style transfer system should minimize both COPY and 1-g.
F More Controllability Evaluations
We follow the setup in Section 5.6 to Ô¨Årst compute a Œªmax per system. We then compute the following,
1. Style Transfer Performance (r-AGG): An ideal system should have good overall performance (Section 5.5) across different values in the range Œõ. 2. Average Style Score Increase (INCR): As our control value increases, we want the classiÔ¨Åer‚Äôs target style score (compared to the input) to increase. Additionally, we want the style score increase of Œªmax to be as high as possible, indicating the system can span the range of classiÔ¨Åer scores. 3. Style Calibration to Œª (CALIB, C-IN): As deÔ¨Åned in Section 5.6. We additionally also measure calibration by including the input sentence x in the CALIB(x) calculation, treating it as the output for Œª = 0 (no style transfer). Here, calibration is averaged over a total of n = 6 (Œª1, Œª2) pairs. We call this metric C-IN.
21This package is the Python port of Nakatani (2010).

A detailed breakdown of performance by different metrics for every model is shown in Table 15.
G Ablation Studies
G.1 Ablation Study for DIFFUR design
This section describes the ablation experiments conducted for the DIFFUR modeling choices in Section 4.2. We ablate a DIFFUR-INDIC model trained on Hindi paraphrase data only, and present results for Hindi formality transfer in Table 16.
- no paraphrase: We replaced the paraphrase noise function with the random token dropping / replacing noise used in the denoising objective of UR model (Section 3), and continued to use vector differences. As seen in Table 16, this signiÔ¨Åcantly increases the copy rate, which lowers the style transfer performance.
- no paraphrase semantic Ô¨Åltering: We keep a setup identical to Section 4.2, but avoid the LaBSE Ô¨Åltering done (discarding pairs having a LaBSE score outside [0.7, 0.98]) to remove noisy paraphrases or exact copies. As seen in Table 16, this decreases the semantic similarity score of the generations, lowering the overall performance.
- no vector differences: Instead of using vector differences for DIFFUR-INDIC, we simply set sdiff = fstyle(x), or the style of the target sentence. In Table 16, we see this signiÔ¨Åcantly decreases SIM scores, and LANG scores for Œª = 2.0. We hypothesize that this training encourages the model to rely more heavily on the style vectors, ignoring the paraphrase input. This could happen since the style vectors are solely constructed from the output sentence itself, and semantic information / confounding style is not subtracted out. In other words, the model is behaving more like an autoencoder (through the style vector) instead of a denoising autoencoder with stylistic supervision.
- mC4 instead of Samanantar: Instead of creating pseudo-parallel data with Samanantar, we leverage the mC4 dataset itself which was used to train the UR model. We backtranslate spans of text from the Hindi split of mC4 on-the-Ô¨Çy using the UR translation capabilities, and use it as the ‚Äúparaphrase noise function‚Äù. To ensure translation performance does not deteriorate during training, 50% minibatches are supervised translation between Hindi

and English. In Table 16, we see decent overall performance, but the LANG score is 6% lower than DIFFUR-INDIC. Qualitatively we found that the model often translates a few Hindi words to English while making text informal. Due to sparsity of English tokens, it often escapes penalization from LANG. - mC4 + exemplar instead of target: This setting is similar to the previous one, but in addition to the mC4 dataset we utilize the vector difference between the style vector of the exemplar span (instead of target span), and the ‚Äúparaphrase noised‚Äù input. Results in Table 16 show this method is not effective, and it‚Äôs important for the vector difference to model the precise transformation needed.
G.2 Choice of Decoding Scheme
We experiment with Ô¨Åve decoding schemes on the Hindi formality validation set ‚Äî beam search with beam size 1, 4 and top-p sampling (Holtzman et al., 2020) with p = 0.6, 0.75, 0.9.
In Table 17, we present results at a constant style transfer magnitude (Œª = 3.0). Consistent with Krishna et al. (2020), we Ô¨Ånd that top-p decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower semantic similarity (SIM) scores. Overall beam search triumphs since the loss in semantic similarity leads to a worse performing model. In Figure 10, we see a consistent trend across different magnitudes of style transfer (Œª). In all our main experiments, we use beam search with beam size 4 to obtain our generations.
G.3 Number of Training Steps
In Figure 11, we present the variation in style transfer performance with number of training steps for our best model, the DIFFUR-MLT model. We Ô¨Ånd that with more training steps performance generally improves, but improvements saturate after 8k steps. We also see the peak of the graphs (best style transfer performance) shift rightwards, indicating a preference for higher Œª values.
H Analysis Experiments
H.1 Style vectors from fstyle as style classiÔ¨Åers
The Universal Rewriter models succeed in learning an effective style space, useful for few-shot style transfer. But can this metric space also act as a

Model

hi bn kn te

UR

79.1 69.7 66.2 67.1

UR-INDIC

80.7 74.3 68.2 72.2

DIFFUR-INDIC 68.0 73.8 67.0 70.4

DIFFUR-MLT 75.0 81.7 79.8 79.0

Table 13: style vector as a classiÔ¨Åer, measuring the cosine similarity with informal exemplar vectors.

style classiÔ¨Åer? To explore this, we measure the cosine distance between the mean style vector of our informal exemplars,22 and the style vectors derived by passing human-annotated formal/informal pairs (from our dataset of Section 5.1) through fstyle. We only consider pairs which had complete agreement among annotators. In Table 13 we see good agreement (68.2%-80.7%) between human annotations and the classiÔ¨Åer derived from the metric space of the UR-INDIC model. Agreement is lower (67.0%74.3%) for the DIFFUR-INDIC model, likely due to the stop gradient used in Section 4.2. With DIFFUR-MLT, agreement jumps back up to 75%81.7% since gradients Ô¨Çow into the style extractor as well.
H.2 Style Vector Analysis with Formal Exemplars Vectors
In Appendix H.1, we saw that the metric vector space derived from the style encoder fstyle of various models is an effective style classiÔ¨Åer, using the informal exemplar vectors. In Table 14, we present a corresponding analysis using formal exemplar vectors. Most accuracy scores are close to 50%, implying this setup is not a very effective style classiÔ¨Åer.

I Full Breakdown of Results
A full breakdown of results by individual metrics, along with plots showing variation with change in Œª, is provided for ‚Äî Hindi (Table 20, Figure 12), Bengali (Table 21, Figure 13), Kannada (Table 22, Figure 14), Telugu (Table 23, Figure 15), Gujarati (Table 24, Figure 16).
J More Model Outputs
Please refer to Figure 8. In the main body, Figure 4 has a few examples as well with detailed analysis.
K Paraphrase Diversity
In Figure 9 we measure the lexical overlap between paraphrases used in our DIFFUR training strategy for six different languages (Hindi, Bengali, Kannada, Telugu, Swahili and Spanish). The lexical overlap is measured using the unigram F1 score, using the implementation from the SQuAD evaluation script (Rajpurkar et al., 2016). The wide spread of the histogram and sufÔ¨Åcient percentage of low overlap pairs conÔ¨Årm the lexical diversity of the paraphrases used. As shown in prior work (Krishna et al., 2020), high lexical diversity of paraphrases is helpful for changing the input style.

Model

hi bn kn te

UR

56.6 60.0 61.6 57.6

UR-INDIC

59.5 60.6 52.6 44.8

DIFFUR-INDIC 58.5 58.3 59.5 49.7

DIFFUR-MLT 64.9 52.3 47.1 41.8

Table 14: style vector as a classiÔ¨Åer, measuring the cosine similarity with formal exemplar vectors.

22See Appendix D for the exemplar sentences. We found the informal exemplars more effective than formal exemplars for style classiÔ¨Åcation; Appendix H.2 has a comparison.

Input

Generations

Input English Translation

Complex ‡§ï‡•ã‡§ü‡§∞‡§§‡•ç ‡§ï‡•á ‡§Ü‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡•Ä ‡§Ö‡§®‡§¶‡•á‡§ñ‡•Ä ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡§®‡•ç‡§¶‡•Ä ‡§¨‡•ã‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§π‡§Ç‡•à ‡§î‡§∞ ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¶‡•á‡§∂- ‡§µ‡§¶‡•á‡§∂ ‡§Æ‡§Ç‡•á ‡§π‡§®‡•ç‡§¶‡•Ä ‡§ï‡§æ ‡§Æ‡§æ‡§® ‡§¨‡•ù‡§æ‡§Ø‡§æ ‡§π‡•à‡•§
‡§™‡•Å ‡§≤‡§∏ ‡§®‡•á ‡§¶ ‡§≤‡•Ä ‡§∏‡•á ‡§™‡§æ‡§Ç‡§ö ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§ó‡§∞‡§´‡•ç‡§§‡§æ‡§∞ ‡§ï‡§Ø‡§æ ‡§π‡•à‡•§ Simple ‡§µ‡§π ‡§¨‡•â‡§Æ‡•ç‡§¨‡•á ‡§π‡§æ‡§à‡§ï‡•ã‡§ü‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∏‡§¨‡§∏‡•á ‡§∏‡•Ä ‡§®‡§Ø‡§∞ ‡§ú‡§ú ‡§π‡§Ç‡•à‡•§
‡§Æ‡§®‡§Ç‡•à ‡•á ‡§â‡§®‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§π‡•Å‡§§ ‡§ï‡§∞‡•Ä‡§¨ ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§Ø‡§æ ‡§π‡•à‡•§ Informal ‡§´ ‡§Æ ‡§á‡§Ç‡§°‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à ‡§ï‡§æ‡§Æ
‡§Ö‡§∞‡•á ‡§≠‡§à, ‡§π‡§Æ ‡§ï‡•ã‡§à ‡§Æ‡•õ‡§æ‡•ò ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∞‡§π‡•á. ‡§§‡•Å‡§Æ ‡§ø‡§ú‡§Ø‡•ã ‡§Ø‡§æ ‡§Æ‡§∞‡•ã ‡§Æ‡•Å‡§ù‡•á ‡§á‡§∏‡§∏‡•á ‡§ï‡•ã‡§à ‡§Æ‡§§‡§≤‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à.
‡§â‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§§‡•ã ‡§ú‡•à‡§∏‡•á ‡§¨‡§µ‡§æ‡§≤ ‡§Æ‡§ö ‡§ó‡§Ø‡§æ. ‡§î‡§∞ ‡§ú‡•ã‡§∂ ‡§µ ‡•ô‡§∞‡•ã‡§∂ ‡§µ‡§æ‡§≤‡•á ‡§∏‡§Æ‡§®‡•ç‡§¶‡§∞ ‡§ï‡•Ä Formal ‡§Ö ‡§≠‡§≠‡§æ‡§µ‡§ï ‡§≠‡•Ä ‡§Ö‡§™‡§®‡•Ä ‡§≤‡•ú ‡§ï‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§á‡§® ‡§Æ‡§π‡§æ ‡§µ‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø‡•ã‡§Ç ‡§Æ‡§Ç‡•á ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§¶‡§≤‡§µ‡§æ‡§®‡•á ‡§ï‡•á ‡§á‡§ö‡•ç‡§õ‡•Å ‡§ï ‡§π‡§Ç‡•à‡•§
‡§¶‡§∏‡•Ç ‡§∞‡•ã‡§Ç ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§∏‡•á ‡§∏‡•Å‡§®‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ø‡•Ä‡§∂‡•Å ‡§Æ‡§∏‡•Ä‡§π ‡§è‡§ï ‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§® ‡§Æ‡§∏‡§æ‡§≤ ‡§π‡•à‡•§ Positive Sentiment ‡§Ø‡§π ‡§π‡•ã‡§ü‡§≤ ‡§ï‡§æ‡§´‡•Ä ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§•‡§æ Negative Sentiment ‡§™‡§§‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§ö‡§≤‡§§‡§æ, ‡§≤‡•á ‡§ï‡§® ‡§´ ‡§Æ ‡§ï‡•á ‡§™‡•ç‡§∞ ‡§§ ‡§¨‡•á‡§∞‡•Ç‡§ñ‡•Ä ‡§¨‡•ù‡§§‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à
‡§ï‡§æ‡§Ø‡§æ‡§§‡§∞‡•ç‡§≤‡§Ø ‡§ï‡•á ‡§ï‡§Æ‡§ö‡§∞‡•ç‡§§ ‡§æ‡§∞‡•Ä ‡§î‡§∞ ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§® ‡§¨‡§π‡•Å‡§§ ‡§ñ‡§∞‡§æ‡§¨ ‡§π‡•à
Monocode ‡§Ø‡§π‡§æ‡§Ç ‡§ï‡•ã‡§à ‡§Æ‡•Ç‡§≤‡§≠‡•Ç‡§§ ‡§∏‡•Å ‡§µ‡§ß‡§æ‡§è‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡§Ç‡•à‡•§ ‡§ù‡§™‡§ü‡§Æ‡§æ‡§∞‡•Ä ‡§Æ‡§Ç‡•á ‡§∂‡§æ ‡§Æ‡§≤ ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ø‡§ï‡•ç‡§§ ‡§ï‡•ã ‡§™‡§ï‡•ú‡§æ‡•§
‡§á‡§® 11 ‡§Ö ‡§≠‡§Ø‡•Å‡§ï‡•ç‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§ï‡§∏‡•Ä ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à. ‡§Ø‡§π ‡§¨‡§æ ‡§∞‡§∂ ‡§ï‡§à ‡§™‡•ç‡§∞‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡§Ç‡•á ‡§π‡•Å‡§à ‡§π‡•à.
‡§∂‡§µ‡§∏‡•á‡§®‡§æ ‡§î‡§∞ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§Æ‡§Ç‡•á ‡§ï‡•ã‡§à ‡§Ö‡§Ç‡§§‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à
De-anonymized 2019 ‡§≤‡•ã‡§ï‡§∏‡§≠‡§æ ‡§ö‡•Å‡§®‡§æ‡§µ ‡§ï‡•á ‡§≤‡§è ‡§™‡•ç‡§∞‡§∂‡§æ‡§Ç‡§§ ‡§ï‡§∂‡•ã‡§∞ ‡§®‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§Ø‡§æ ‡§ï‡§æ‡§Æ ‡§á‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§Ü‡§ï‡§∞ ‡§á‡§Ç ‡§¶‡§∞‡§æ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§®‡•á ‡§∏‡•ç‡§µ‡§£‡§∞‡•ç‡§§ ‡§Æ‡§Ç ‡§¶‡§∞ ‡§™‡§∞ ‡§π‡§Æ‡§≤‡§æ ‡§ï‡§Ø‡§æ ‡§®‡§∞‡§Ç‡§ú‡§® ‡§è‡§ï ‡§®‡§§‡§ï‡§∞‡•ç‡§§ ‡•Ä, ‡§Æ‡§ø ‡§≤‡§ï‡§æ ‡§î‡§∞ ‡§Ö‡§Æ‡•Ä‡§∞‡§ö‡§Ç‡§¶ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ó‡•Å‡§Æ‡§∞‡§æ‡§π ‡§ï‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§â‡§∏‡§ï‡•á ‡§ß‡§® ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§π‡§Ç‡•à‡•§

Simple ‡§ï‡•ã‡§ü‡§∞‡§§‡•ç ‡§ï‡•á ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§ú‡§∞ ‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡§®‡•ç‡§¶‡•Ä ‡§¨‡•ã‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•Ä‡§è‡§Æ ‡§ø‡§ú‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¶‡•á‡§∂ ‡§î‡§∞ ‡§µ‡§¶‡•á‡§∂ ‡§Æ‡§Ç‡•á ‡§π‡§®‡•ç‡§¶‡•Ä ‡§¨‡•ù‡§æ‡§à ‡§π‡•à‡•§
‡§¶ ‡§≤‡•Ä ‡§∏‡•á ‡§™‡§æ‡§Ç‡§ö ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§ï‡•ú‡§æ Complex ‡§µ‡§π ‡§¨‡•â‡§Æ‡•ç‡§¨‡•á ‡§π‡§æ‡§à‡§ï‡•ã‡§ü‡•ç‡§∞‡§§ ‡§ï‡•á ‡§∏‡§µ‡§æ‡•ç‡§§‡§∞ ‡§ß‡§ï‡•É ‡§§ ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§ß‡•Ä‡§∂ ‡§π‡§Ç‡•à
‡§Æ‡§®‡•à‡§Ç ‡•á ‡§â‡§®‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§π‡•Å‡§§ ‡§®‡§ï‡§ü‡§§‡§æ ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§Ø‡§æ ‡§π‡•à‡•§ Formal ‡§µ‡§π ‡§´ ‡§Æ‡•Ä ‡§ú‡§ó‡§§ ‡§Æ‡§Ç‡•á ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§£‡§∞‡§§‡•ç ‡§≠‡•Ç ‡§Æ‡§ï‡§æ ‡§®‡§≠‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç
‡§™‡•ç‡§∞‡§Ø‡§ú‡§®‡•ã‡§Ç, ‡§π‡§Æ ‡§ï‡•ã‡§à ‡§π‡§Å‡§∏‡•Ä-‡§ñ‡•á‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡§Ç‡•à. ‡§Ü‡§™ ‡§ú‡•Ä‡§§‡•á ‡§Ø‡§æ ‡§Æ‡§∞‡§§‡•á ‡§π‡§Ç‡•à, ‡§á‡§∏‡§∏‡•á ‡§Æ‡•Å‡§ù‡•á ‡§ï‡•ã‡§à ‡§Æ‡§§‡§≤‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à. ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§ú‡•à‡§∏‡•á-‡§ú‡•à‡§∏‡•á ‡§π‡•ú‡§ï‡§Ç ‡§™ ‡§Æ‡§ö ‡§ó‡§Ø‡§æ ‡§î‡§∞ ‡§ú‡•ã‡§∂ ‡§î‡§∞ ‡§Ü‡§∂‡§æ‡§ø‡§®‡•ç‡§µ‡§§ ‡§∏‡§Æ‡•Ç‡§π ‡§ï‡•Ä Informal ‡§Ö ‡§≠‡§≠‡§æ‡§µ‡§ï ‡§≠‡•Ä ‡§Ö‡§™‡§®‡•Ä ‡§≤‡•ú ‡§ï‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§á‡§® ‡§ï‡•â‡§≤‡•á‡§ú‡•ã‡§Ç ‡§Æ‡§Ç‡•á ‡§≠‡•á‡§ú‡§®‡•á ‡§ï‡•á ‡§á‡§ö‡•ç‡§õ‡•Å ‡§ï ‡§π‡§Ç‡•à
‡§¶‡§∏‡•Ç ‡§∞‡•á ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§∏‡•Å‡§®‡§®‡•á ‡§Æ‡§Ç‡•á ‡§Ø‡•Ä‡§∂‡•Å ‡§Æ‡§∏‡•Ä‡§π ‡§¨‡•á‡§∏‡•ç‡§ü ‡§π‡•à
Negative Sentiment ‡§Ø‡§π ‡§π‡•ã‡§ü‡§≤ ‡§¨‡§π‡•Å‡§§ ‡§¨‡•Å‡§∞‡§æ ‡§•‡§æ. Positive Sentiment ‡§™‡§§‡§æ ‡§®‡§π‡•Ä‡§Ç, ‡§≤‡•á ‡§ï‡§® ‡§´ ‡§Æ ‡§ï‡•á ‡§™‡•ç‡§∞ ‡§§ ‡§¶‡§∂‡§ï‡•ç‡§§‡§∞ ‡•ã‡§Ç ‡§ï‡•Ä ‡§∞‡•Å ‡§ö ‡§¨‡•ù‡§§‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à
‡§ï‡§æ‡§Ø‡§æ‡§§‡§∞‡•ç‡§≤‡§Ø ‡§ï‡•á ‡§ï‡§Æ‡§ö‡§§‡•ç‡§∞ ‡§æ‡§∞‡•Ä ‡§î‡§∞ ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏ ‡§®‡§ï ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§® ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§π‡•à‡§Ç Code-mixed ‡§Ø‡§π‡§æ‡§Ç ‡§ï‡•ã‡§à ‡§¨‡•Å ‡§®‡§Ø‡§æ‡§¶‡•Ä ‡§´‡•Ä‡§ö‡§∏‡§§‡§∞‡•ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡§Ç‡•à‡•§ ‡§ó‡§∞‡•ã‡§π ‡§ï‡•á ‡§è‡§ï ‡§∂‡§ñ‡•ç‡§∏ ‡§ï‡•ã ‡§∞‡§Æ‡§æ‡§Ç‡§° ‡§™‡§∞ ‡§≤‡§Ø‡§æ
‡§á‡§® 11 ‡§Ü‡§∞‡•ã ‡§™‡§Ø‡•ã‡§Ç ‡§Æ‡§Ç‡•á ‡§∏‡•á ‡§ï‡§∏‡•Ä ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§≤‡•Ä‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§≤‡•â‡§ï‡§°‡§æ‡§â‡§® ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡•á ‡§ï‡§à ‡§π‡§∏‡•ç‡§∏‡•ã‡§Ç ‡§Æ‡§Ç‡•á ‡§π‡•Å‡§Ü ‡§π‡•à‡•§
‡§∂‡§µ‡§∏‡•á‡§®‡§æ ‡§î‡§∞ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§ó‡•Å‡§° ‡§®‡•ç‡§Ø‡•Ç‡§ú ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§
Anonymized 2019 ‡§≤‡•ã‡§ï‡§∏‡§≠‡§æ ‡§ö‡•Å‡§®‡§æ‡§µ ‡§ï‡•á ‡§≤‡§è PII ‡§®‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§Ø‡§æ ‡§ï‡§æ‡§Æ ‡§á‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ PII ‡§®‡•á PII ‡§™‡§∞ ‡§π‡§Æ‡§≤‡§æ ‡§ï‡§Ø‡§æ
‡§®‡§∞‡§Ç‡§ú‡§® ‡§ï‡•ã ‡§è‡§ï PII, PII ‡§î‡§∞ PII ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ó‡•Å‡§Æ‡§∞‡§æ‡§π ‡§ï‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§â‡§∏‡§ï‡•á ‡§ß‡§® ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§π‡§Ç‡•à‡•§

They ignored the court orders Narendra Modi is a Hindi speaking prime minister who has popularized Hindi across the world The police arrested 5 people in Delhi
He/She is the most senior judge in the Bombay High Court. I‚Äôve worked closely with them.
He/She plays an important role in the film industry. Friends, this is not a joke. I don‚Äôt care whether you live or die!
After this there was a lot of chaos In the sea of energy and passion
Parents also wish to get their daughters admitted in these colleges.
Jesus Christ is the best example of an empathetic listener.
This hotel was very good.
You don‚Äôt realize, but your interest towards the film continually declines as you watch it Office staff and administrative management are very good
This doesn‚Äôt even have basic features. One person involved in the prank was caught. The names of the 11 accused have not been revealed. It rained in several states.
There‚Äôs no difference between Shiv Sena and the BJP.
Prashant Kishore has started working for the 2019 Lok Sabha elections
After this, Indira Gandhi ordered an attack on the Golden Temple Niranjan is misled by a dancer, Mallika & Amirchand, who are after his wealth.

Figure 8: More qualitative examples of generations from our system (see Figure 4 for main table with qualitative analysis). Red and blue colours indicate attribute-speciÔ¨Åc features, while golden text represents model errors.

percent

percent

12

hi lexical change

10

8

6

4

2

0 0.0 12 10

0.2 uni0g.r4am F10s.c6ore 0.8 1.0
kn lexical change

8

6

4

2

0 0.0 0.2 0.4 0.6 0.8 1.0
unigram F1 score

8

sw lexical change

7

6

5

4

3

2

1

0 0.0 0.2 0.4 0.6 0.8 1.0
unigram F1 score

percent

percent

percent

10 8 6 4 2 0 0.0
10 8 6 4 2 0 0.0
10 8 6 4 2 0 0.0

bn lexical change
0.2 uni0g.r4am F10s.c6ore 0.8 1.0
te lexical change
0.2 uni0g.r4am F10s.c6ore 0.8 1.0
es lexical change
0.2 uni0g.r4am F10s.c6ore 0.8 1.0

percent

Figure 9: Lexical overlap between paraphrases used in our DIFFUR training strategy for six different languages (Hindi, Bengali, Kannada, Telugu, Swahili and Spanish). The wide spread of the histogram and sufÔ¨Åcient percentage of low overlap pairs conÔ¨Årm the lexical diversity of the paraphrases used. The lexical overlap is measured using the unigram F1 score, using the implementation from the SQuAD evaluation script (Rajpurkar et al., 2016).

Model

Œªmax/3 Œª r-AGG INCR

2Œªmax/3 Œª r-AGG INCR

Œªmax

Overall

Œª r-AGG INCR CALIB C-IN

UR (2021)

0.5

UR-INDIC

0.5

UR + BT

0.3

UR-INDIC + BT 0.3

DIFFUR

0.5

DIFFUR-INDIC 0.8

DIFFUR-MLT

0.8

22.1 5.2 1.0 53.2 13.4 1.0 53.2 21.4 0.7 57.3 22.9 0.7 65.8 16.6 1.0 67.2 17.9 1.7 56.6 11.3 1.7

26.9 8.9 1.5 58.3 18.8 1.5 53.9 23.5 1.0 59.4 24.6 1.0 71.1 26.0 1.5 72.6 27.3 2.5 72.6 18.1 2.5

30.4 18.7 54.6 26.7 49.1 26.9 60.0 26.7 67.1 21.9 65.0 36.7 78.1 29.9

29.2 31.6 60.7 65.1 43.4 58.8 38.7 56.0 64.9 72.5 69.6 75.5 69.0 71.8

Table 15: Evaluation of extent to which the magnitude of hindi formality transfer can be controlled with Œª. We Ô¨Ånd that DIFFUR-INDIC, DIFFUR-MLT are best at calibrating style change to input Œª (CALIB, C-IN), giving the higher style score increase (INCR) at Œª = Œªmax (details of evaluation setup and metrics in Section 5.6, Appendix F).

Ablation

COPY(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

DIFFUR-INDIC (hindi only) - no paraphrase** - no paraphrase (p, Œª = 0.6, 3) - no paraphrase semantic Ô¨Åltering - no vector differences** - no vector differences (Œª = 0.5) - mC4 instead of Samanantar - mC4 + exemplar instead of target

2.0 97.0 78.4 89.8 39.7 67.3 24.6 21.0 98.3 92.2 60.0 15.7 51.9 10.7 14.2 98.7 81.0 70.9 28.1 51.6 12.5 2.2 97.2 72.2 89.1 38.6 60.7 19.6 0.0 54.3 3.2 99.0 90.0 2.4 1.0 0.9 97.4 66.8 86.4 36.5 53.5 17.3 1.5 91.4 82.0 89.3 39.0 67.7 24.2 5.5 23.8 82.3 77.2 32.3 13.8 3.2

Table 16: Ablation study on Hindi formality transfer validation set using beam size of 4 and Œª = 2.0 unless the optimal hyperparameters were different (marked by **). As shown by the overall a-AGG scores, removing any component of our design leads to an overall performance drop, sometimes signiÔ¨Åcantly. For a detailed description of analysis and results, see Appendix G.1. For detailed metric descriptions, see Section 5.

Decoding COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

beam 4 beam 1

1.8 52.7 95.8 73.3 94.7 51.6 66.2 32.3 1.2 47.4 92.3 61.7 95.7 62.5 55.8 31.4

top-p 0.6 top-p 0.75 top-p 0.9

1.0 45.3 91.5 56.6 96.2 65.9 51.3 29.9 0.9 43.1 90.3 52.4 96.3 69.0 47.3 28.2 0.7 40.4 89.4 46.8 96.6 71.7 42.4 26.5

Table 17: Automatic evaluation of different decoding algorithms (top-p sampling and beam search) on the DIFFURMLT model for Hindi formality transfer (validation set) using Œª = 3.0. As expected, output diversity (1-g, COPY) and style accuracy (r-ACC, a-ACC) improves as we move down the table, but compromise semantic preservation (SIM), bringing the overall performance (r-AGG, a-AGG) down. Also see Figure 10 for a comparison across Œª values, and Section 5 for detailed metric descriptions.

Model
UR (2021) UR + BT
DIFFUR
UR-INDIC UR-INDIC + BT DIFFUR-INDIC DIFFUR-MLT

Hindi r-AGG a-AGG
34.5 13.4 61.6 24.2 79.4 30.3 62.0 23.9 68.0 28.1 80.0 32.4 85.8 45.2

Bengali r-AGG a-AGG

33.8

9.0

65.6 22.8

81.7 36.0

69.3 29.3

73.5 33.3

80.0 32.3

86.0 48.3

Kannada r-AGG a-AGG

26.8

8.8

48.8 16.0

79.0 43.4

64.6 22.2

72.6 29.7

79.9 41.4

86.9 54.4

Telugu r-AGG a-AGG
24.3 10.7 48.7 17.6 79.7 38.0 65.0 25.8 71.6 31.4 78.8 37.0 86.1 51.7

Gujarati r-AGG a-AGG

25.6

5.9

56.3 15.1

0.5

0.2

59.0 13.8

68.4 21.7

38.9 16.2

78.8 41.3

Table 18: Test set performance across languages for a smaller LaBSE semantic similarity threshold of 0.65. Due to the more relaxed threshold, absolute numbers compared to Table 1 are higher. Trends remain similar, with the DIFFUR and INDIC variants outperforming other competing methods.

performance (r-AGG) performance (a-AGG)

0.8

0.7

0.30

0.6

0.25

0.5

0.4

0.20

0.3 0.15

0.2 1.0 1.5 2.0 2.5 3.0 3.5

1.0 1.5 2.0 2.5 3.0 3.5

transfer amount ( )

transfer amount ( )

beam 1 beam 4 topp 0.6 topp 0.75 topp 0.9

Figure 10: Variation in Hindi formality transfer (validation set) performance vs Œª with change in decoding scheme, for the DIFFUR-MLT model. The plots show overall style transfer performance, using the r-AGG (left) and aAGG (right) metrics from Section 5.5. Beam search with beam size 4 performs best, see Table 17 for an individual metric breakdown while keeping Œª = 3.0.

performance (r-AGG)

0.8 0.6 0.4 0.2 0.0
0.5

1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5
0.6

performance (a-AGG)

0.30 0.25 0.20 0.15 0.10 0.05 0.00
0.5

0.5

style change

0.4

0.3

0.2

0.1

0.00.70 0.75 0.80 0.85 0.90 content similarity

1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5

0.95 1.00

Naive model 10k steps 12k steps 14k steps 2k steps 4k steps 6k steps 8k steps

10k steps 12k steps 14k steps 2k steps 4k steps 6k steps 8k steps

Figure 11: Variation in Hindi formality transfer validation set performance with change in number of training steps for the DIFFUR-MLT model. The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. With more training steps performance seems to improve and the peak of the graph shifts towards the right (a preference towards higher scale values). We also see more training steps leads to better controllability (bottom plot, closer to Y-axis is better), but only marginal gains after 6k steps.

Model
UR (2021) UR + BT
DIFFUR
UR-INDIC UR-INDIC + BT DIFFUR-INDIC DIFFUR-MLT

Hindi r-AGG a-AGG

24.2

6.6

40.0 10.7

57.1 13.0

49.6 13.1

43.7 12.9

59.2 14.9

64.8 17.9

Bengali r-AGG a-AGG

24.2

4.8

31.7

8.1

59.6 13.0

54.6 12.7

33.9 10.2

63.8 15.6

69.8 22.0

Kannada r-AGG a-AGG

21.5

6.0

21.2

5.1

54.5 13.8

50.0 11.4

31.9

7.8

58.9 16.1

69.3 23.5

Telugu r-AGG a-AGG

19.1

5.8

19.1

4.8

52.8 12.8

48.1 11.2

29.4

7.8

55.2 14.4

67.5 20.6

Gujarati r-AGG a-AGG

19.4

3.6

26.1

4.4

0.2

0.0

45.9

6.8

34.0

7.4

31.7

8.0

64.0 18.2

performance (r-AGG)

Table 19: Test set performance across languages for a larger LaBSE semantic similarity threshold of 0.85. Due to the stricter threshold, absolute numbers compared to Table 1 are lower, however trends are similar, with the DIFFUR and INDIC variants outperforming other competing methods.

Model

Œª COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

UR (Garcia et al., 2021) 1.5

UR-INDIC

1.0

45.4 77.5 98.0 84.8 45.8 22.9 30.4 10.4 10.4 70.7 95.0 93.8 67.2 23.3 58.3 18.6

UR + BT UR-INDIC + BT

0.5

0.8 44.2 92.9 85.2 72.3 27.8 54.2 17.8

1.0

1.1 49.5 95.9 85.1 76.3 33.1 60.0 22.2

DIFFUR
DIFFUR-INDIC
DIFFUR-MLT

1.0

4.7 61.6 97.7 89.7 82.4 31.0 71.1 22.9

1.5

5.3 63.7 98.0 91.9 81.6 30.5 72.5 23.7

2.0

3.4 57.5 98.3 84.8 86.4 36.8 70.6 24.0

2.5

4.4 61.9 97.2 89.7 89.7 34.0 78.1 27.5

3.0

2.0 52.5 95.9 72.1 94.1 51.9 64.8 32.2

Table 20: Performance breakdown of Hindi formality transfer by individual metrics described in Section 5.

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.5 1.0 1.5 2.0 2.5 3.0 3.5
transfer amount ( )
0.6

performance (a-AGG)

0.30 0.25 0.20 0.15 0.10 0.05 0.00 0.5 1.0 1.5 2.0 2.5 3.0 3.5
transfer amount ( )

UR (Garcia 2021) UR-Indic UR + BT UR-Indic + BT DiffUR DiffUR-Indic DiffUR-MLT

style change

0.5

Naive model UR (Garcia 2021)

0.4

UR-Indic

UR + BT

0.3

UR-Indic + BT

DiffUR

0.2

DiffUR-Indic

0.1

DiffUR-MLT Ideal system

0.00.75 0.80 0.85 0.90 0.95 1.00 content similarity

Figure 12: Variation in Hindi formality transfer test set performance & control for different models (see Table 20 for a individual metric breakdown of the models at the best performing Œª). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. We see the DIFFUR models outperform other systems across the Œª range, and get best performance with the DIFFUR-MLT variant. We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with Œª without loss in semantics (X-axis).

Model

Œª COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

UR (Garcia et al., 2021) 1.5

UR-INDIC

1.0

1.5

21.5 69.1 99.9 87.3 42.4 15.6 30.4

7.2

4.4 58.9 99.0 95.7 69.8 19.5 65.5 17.3

2.4 47.5 97.6 79.8 80.0 37.4 59.6 22.3

UR + BT UR-INDIC + BT

0.5

0.2 30.4 97.8 80.6 71.8 22.3 55.6 15.0

1.0

0.1 27.0 95.4 73.6 77.6 29.6 53.5 16.9

1.0

0.4 34.9 99.8 80.6 78.3 31.4 61.1 22.0

DIFFUR
DIFFUR-INDIC DIFFUR-MLT

1.0

2.1 50.6 99.9 91.6 80.8 25.2 72.7 20.9

1.5

1.1 40.6 99.9 75.8 89.1 39.7 65.8 25.2

1.5

2.0 53.1 99.9 94.2 80.7 24.6 75.4 21.8

2.5

0.9 41.4 99.9 75.6 86.1 36.9 64.6 24.3

2.5

1.8 49.5 99.9 91.9 87.9 39.1 80.0 33.8

3.0

1.0 40.0 99.1 73.0 92.1 56.5 65.3 35.0

Table 21: Performance breakdown of Bengali formality transfer by individual metrics described in Section 5.

performance (r-AGG)

0.8 0.7 0.6 0.5 0.4 0.3 0.2
0.5

performance (a-AGG)

0.35 0.30 0.25 0.20 0.15 0.10 0.05

1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5
0.6

0.5 1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5

style change

0.5

Naive model UR (Garcia 2021)

0.4

UR-Indic

UR + BT

0.3

UR-Indic + BT

DiffUR

0.2

DiffUR-Indic

0.1

DiffUR-MLT Ideal system

0.00.75 0.80 0.85 0.90 0.95 1.00 content similarity

UR (Garcia 2021) UR-Indic UR + BT UR-Indic + BT DiffUR DiffUR-Indic DiffUR-MLT

Figure 13: Variation in Bengali formality transfer test set performance & control for different models (see Table 21 for a individual metric breakdown of the models at the best performing Œª). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. We see the DIFFUR models outperform other systems across the Œª range, and get best performance with the DIFFUR-MLT variant. We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with Œª without loss in semantics (X-axis).

Model

Œª COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

UR (Garcia et al., 2021) 1.5

UR-INDIC

1.0

52.0 86.8 99.9 95.0 29.9 11.2 25.5

8.0

8.6 62.9 98.3 94.5 67.0 20.8 61.3 17.8

UR + BT UR-INDIC + BT

0.5

0.3 26.0 77.8 75.5 67.2 23.3 39.8 11.9

0.5

1.6 40.6 99.9 82.3 73.9 26.8 59.2 19.1

1.0

1.4 37.7 99.8 76.8 78.3 32.8 58.1 21.0

DIFFUR
DIFFUR-INDIC DIFFUR-MLT

1.0

3.0 47.4 99.8 87.9 80.3 30.5 69.2 23.6

2.0

2.2 39.6 99.9 73.0 87.8 48.3 62.1 29.1

1.5

2.9 50.3 99.9 91.5 81.2 32.2 73.1 26.4

2.0

2.3 45.2 99.9 82.7 85.1 42.3 68.5 29.3

2.0

5.4 59.6 100 97.5 82.9 28.9 80.4 27.5

3.0

2.1 42.7 99.1 71.7 92.6 63.4 64.5 39.4

Table 22: Performance breakdown of Kannada formality transfer by individual metrics described in Section 5.

performance (r-AGG)

performance (a-AGG)

0.8

0.4

0.6

0.3

0.4

0.2

0.2

0.1

UR (Garcia 2021) UR-Indic UR + BT UR-Indic + BT DiffUR DiffUR-Indic DiffUR-MLT

0.0 0.5

1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0

transfer amount ( )

transfer amount ( )

0.6

style change

0.5

Naive model UR (Garcia 2021)

0.4

UR-Indic

UR + BT

0.3

UR-Indic + BT

DiffUR

0.2

DiffUR-Indic

0.1

DiffUR-MLT Ideal system

0.00.75 0.80 0.85 0.90 0.95 1.00 content similarity

Figure 14: Variation in Kannada formality transfer test set performance & control for different models (see Table 22 for a individual metric breakdown of the models at the best performing Œª). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. We see the DIFFUR models outperform other systems across the Œª range, and get best performance with the DIFFUR-MLT variant. We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with Œª without loss in semantics (X-axis).

Model

Œª COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

UR (2021) UR-INDIC

1.5

51.3 87.0 100 96.3 26.3 10.1 22.8

7.5

2.0

35.0 68.2 99.9 73.0 45.4 28.6 20.7

8.4

1.0

10.4 64.5 98.8 94.3 65.6 20.2 59.8 16.7

1.5

5.9 53.5 97.3 80.0 74.9 33.1 55.9 19.9

UR + BT

0.5

1.0

UR-INDIC + BT 0.5

1.0

0.2 26.3 82.4 73.4 65.6 23.4 38.4 11.3 0.1 19.8 74.9 64.7 71.2 31.6 33.1 11.6 0.6 39.2 99.9 79.6 73.5 26.2 56.8 17.9 0.5 36.1 99.7 74.0 78.5 35.9 56.0 22.2

DIFFUR

1.0

2.5

DIFFUR-INDIC 1.0

1.5

DIFFUR-MLT

2.0

2.5

1.7 46.0 99.9 87.9 80.5 27.6 69.4 21.5 0.9 36.0 99.8 68.4 90.2 47.2 59.9 27.1 2.4 50.1 99.9 91.7 78.7 28.7 71.0 23.7 1.4 44.6 99.9 83.6 83.6 38.4 68.2 27.1 3.8 55.8 99.9 95.7 84.0 31.2 79.8 28.6 1.8 47.0 99.5 85.8 90.1 48.4 76.0 37.9

Table 23: Performance breakdown of Telugu formality transfer by individual metrics described in Section 5.

performance (r-AGG)

performance (a-AGG)

0.8

0.6

0.3

0.4

0.2

0.2

0.1

UR (Garcia 2021) UR-Indic UR + BT UR-Indic + BT DiffUR DiffUR-Indic DiffUR-MLT

0.0 0.5

1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5
0.6

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 transfer amount ( )

style change

0.5

Naive model UR (Garcia 2021)

0.4

UR-Indic

UR + BT

0.3

UR-Indic + BT

DiffUR

0.2

DiffUR-Indic

0.1

DiffUR-MLT Ideal system

0.00.75 0.80 0.85 0.90 0.95 1.00 content similarity

Figure 15: Variation in Telugu formality transfer test set performance & control for different models (see Table 23 for a individual metric breakdown of the models at the best performing Œª). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. We see the DIFFUR models outperform other systems across the Œª range, and get best performance with the DIFFUR-MLT variant. We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with Œª without loss in semantics (X-axis).

Model

Œª COPY(‚Üì) 1-g(‚Üì) LANG SIM r-ACC a-ACC r-AGG a-AGG

UR (2021) UR-INDIC

1.5

62.6 89.1 99.9 93.1 30.2

9.3 23.7

5.0

1.0

17.5 73.6 98.4 96.8 57.6 11.7 54.0

9.9

1.5

10.9 62.7 96.9 85.4 67.0 19.2 53.0 10.7

UR + BT

0.5

1.0

UR-INDIC + BT 0.5

0.5 34.3 87.3 77.6 69.1 17.8 46.3

9.8

0.3 26.5 78.8 67.6 74.8 27.2 39.1 10.4

1.9 47.4 99.9 87.1 68.1 22.0 57.7 16.8

DIFFUR

0.5

DIFFUR-INDIC 0.5

1.5

DIFFUR-MLT

2.0

2.5

0.0

5.7

1.2 81.3 73.2 25.7

0.4

0.2

1.1 34.7 54.9 95.6 68.6 18.6 37.4

9.0

0.4 24.2 46.0 74.7 78.5 40.0 29.2 13.0

7.7 65.4 98.6 96.2 79.3 25.0 75.0 22.3

4.5 54.6 95.1 85.5 86.0 45.8 69.8 33.1

Table 24: Performance breakdown of Gujarati formality transfer by individual metrics described in Section 5.

performance (r-AGG)

0.6 0.4 0.2 0.0
0.5

1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5
0.6

performance (a-AGG)

0.30 0.25 0.20 0.15 0.10 0.05 0.00
0.5 1.0 tr1a.n5sfer 2a.m0oun2t.(5 ) 3.0 3.5

style change

0.5

Naive model UR (Garcia 2021)

0.4

UR-Indic

UR + BT

0.3

UR-Indic + BT

DiffUR

0.2

DiffUR-Indic

0.1

DiffUR-MLT Ideal system

0.00.75 0.80 0.85 0.90 0.95 1.00 content similarity

UR (Garcia 2021) UR-Indic UR + BT UR-Indic + BT DiffUR DiffUR-Indic DiffUR-MLT

Figure 16: Variation in Gujarati formality transfer test set performance & control for different models (see Table 24 for a individual metric breakdown of the models at the best performing Œª). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. Note that Gujarati is a zero-shot language for DIFFUR models ‚Äî no Gujarati paraphrase data was seen during training. We see that while the vanilla DIFFUR model performs poorly, the DIFFUR-INDIC is competitive with baselines and the DIFFURMLT variant signiÔ¨Åcantly outperforms other systems. We also see that the DIFFUR-MLT variant lead to better style transfer control (bottom plot, closer to x = 1 is better), giving style variation with Œª without loss in semantics (X-axis).

Figure 17: Our crowdsourcing interface on Task Mate, used to build our formality evaluation datasets (Section 5.1) and conduct human evaluations (Section 5.7). The Ô¨Årst row shows our landing page and instruction set derived from our conversations with professional linguists. The second row shows our qualiÔ¨Åcation questions for formality classiÔ¨Åcation, and the third row shows templates for the two questions asked to crowdworkers per pair.

