arXiv:2011.03574v1 [cs.LG] 6 Nov 2020

S I N G L E - N O D E AT TAC K F O R F O O L I N G GRAPH NEURAL NETWORKS
Ben Finkelshtein †∗ Chaim Baskin †∗ Evgenii Zheltonozhskii † Uri Alon † †Technion – Israel Institute of Technology, Haifa, Israel,
{benfin,chaimbaskin,evgeniizh}@campus.technion.ac.il {urialon}@cs.technion.ac.il
ABSTRACT
Graph neural networks (GNNs) have shown broad applicability in a variety of domains. Some of these domains, such as social networks and product recommendations, are fertile ground for malicious users and behavior. In this paper, we show that GNNs are vulnerable to the extremely limited scenario of a single-node adversarial example, where the node cannot be picked by the attacker. That is, an attacker can force the GNN to classify any target node to a chosen label by only slightly perturbing another single arbitrary node in the graph, even when not being able to pick that speciﬁc attacker node. When the adversary is allowed to pick a speciﬁc attacker node, the attack is even more effective. We show that this attack is effective across various GNN types, such as GraphSAGE, GCN, GAT, and GIN, across a variety of real-world datasets, and as a targeted and a non-targeted attack. Our code is available at https://github.com/benfinkelshtein/SINGLE .
1 INTRODUCTION
Graph neural networks (GNNs) (Scarselli et al., 2008; Micheli, 2009) have recently shown sharply increasing popularity due to their generality and computation-efﬁciency (Duvenaud et al., 2015; Li et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Velicˇkovic´ et al., 2018; Xu et al., 2019b). Graph-structured data underlie a plethora of domains such as citation networks (Sen et al., 2008), social networks (Leskovec & Mcauley, 2012; Ribeiro et al., 2017; 2018), knowledge graphs (Wang et al., 2018; Trivedi et al., 2017; Schlichtkrull et al., 2018), and product recommendations (Shchur et al., 2018). Therefore, GNNs are applicable for a variety of real-world structured data.
While most work in this ﬁeld has focused on improving the accuracy of GNNs and applying them to a growing number of domains, only a few past works have explored the vulnerability of GNNs to adversarial examples. Consider the following scenario: a malicious user joins a social network such as Twitter or Facebook. The malicious user mocks the behavior of a benign user, establishes connections with other users, and submits benign posts. After some time, the user submits a new adversarially crafted post, which might seem irregular but overall benign. This new post perturbs the representation of the user as seen by a GNN. As a result, another, speciﬁc benign user gets blocked from the network; alternatively, another malicious user submits a hateful post – but does not get blocked. This scenario is illustrated in Figure 1. In this paper, we show the feasibility of such a troublesome scenario: a single attacker node can perturb its own representation, such that another node will be misclassiﬁed as a label of the attacker’s choice.
Most previous work on adversarial examples in GNNs required the perturbation to span multiple nodes, which in reality requires the cooperation of multiple attackers. For example, the pioneering work of Zu¨gner et al. (2018) perturbed a set of attacker nodes; Bojchevski & Gu¨nnemann (2019a) perturb edges that are covered by a set of nodes. To the best of our knowledge, our work is the ﬁrst to show that a single-node attack is as effective as a multiple-node attack. Further and in contrast with existing work, we show that perturbing a single node is more harmful than perturbing a single edge.
∗Equal contribution.
1

Our vision..

GNN: valid

Our vision.. not racist

GNN: invalid

(a) Before attacking: the target is classiﬁed as valid. (b) After attacking: the target is classiﬁed as invalid.
Figure 1: An partial adversarial example from the test set of the Twitter dataset. An adversariallycrafted post perturbs the representation of the attacker node. This perturbation causes a misclassiﬁcation of the target victim node, although they are not even direct neighbors.

In this paper, we present a ﬁrst a single-node adversarial attack on graph neural networks. If the adversary is allowed to choose the attacker node, for example, by hacking into an existing account, the efﬁciency of the attack signiﬁcantly increases. We present two approaches for choosing the attacker: a white-box gradient-based approach, and a black-box, model-free approach that relies on graph topology. Finally, we perform a comprehensive experimental evaluation of our approach on multiple datasets and GNN architectures.

2 PRELIMINARIES

Let G = {Gi}Ni=G1 be a set of graphs. Each graph G = (V, E, X) ∈ G has a set of nodes V and a set of edges E ⊆ V × V, where (u, v) ∈ E denotes an edge from a node u ∈ V to a node v ∈ V.
X ∈ RN×D is a matrix of D-dimensional node features. The i-th row of X is the feature vector of the node vi ∈ V and is denoted as xi = Xi,: ∈ RD.

Graph neural networks GNNs operate by iteratively propagating neural messages between neighboring nodes. Every GNN layer updates the representation of every node by aggregating its current representation with the current representations of its neighbors.
Formally, each node is associated with an initial representation x(v0) = h(v0) ∈ RD. This representation is considered as the given features of the node. Then, a GNN layer updates each node’s representation given its neighbors, yielding h(v1) ∈ Rd1 for every v ∈ V. In general, the -th layer of a GNN is a function that updates a node’s representation by combining it with its neighbors:

hv( ) = COMBINE h(v −1), {h(u −1) | u ∈ Nv}; θ ,

(1)

where Nv is the set of direct neighbors of v: Nv = {u ∈ V | (u, v) ∈ E}.

The COMBINE function is what mostly distinguishes GNN types. For example, graph convolutional networks (GCN) (Kipf & Welling, 2017) deﬁne a layer as:

h( ) = ReLU

1 W ( )h( −1)

(2)

v

u∈Nv∪{v} cu,v

u

where cu,v is a normalization factor usually set to |Nv| · |Nu|. After such aggregation iterations, every node representation captures information from all nodes within its -hop neighborhood. The total number of layers L is usually determined empirically as a hyperparameter. In the node classiﬁcation scenario, we use the ﬁnal representation hLv to classify v.
For brevity, we focus our deﬁnitions on the semi-supervised transductive node classiﬁcation goal, where the dataset contains a single graph G, and the split into training and test sets is across nodes in the same graph. Nonetheless, these deﬁnitions can be trivially generalized to the inductive setting, where the dataset contains multiple graphs, the split into training and test sets is between graphs, and the test nodes are unseen during training.

We associate each node v ∈ V with a class yv ∈ Y = {1, ..., Y }. The labels of the training nodes are given during training; the test nodes are seen during training – without their labels. The training

2

subset is represented as D = G, {(vi, yi)}Ni=D0 . Given the training set, the goal is to learn a model
fθ : (G, V) → Y that will classify the rest of the nodes correctly. During training, the model fθ thus minimizes the loss over the given labels, using J (·, ·), which typically is the cross-entropy loss:

θ∗ = argmin L (fθ, D) = argmin 1

ND
J (fθ (G, vi) , yi)

(3)

θ

θ ND i=0

3 S I N G L E - N O D E G N N AT TAC K

In this section, we describe our Single-node INdirect Gradient adversariaL Evasion (SINGLE) attack. While our attack is simple, it is the ﬁrst attack that focuses on perturbing nodes (in contrast to edges (Dai et al., 2018)), which works with an arbitrary single attacker node (in contrast to multiple nodes (Zu¨gner et al., 2018)) that is not the node under attack (in contrast to “direct” attacks where the attacker perturbs the node under attack directly (Zu¨gner et al., 2018; Li et al., 2020)).

3.1 PROBLEM DEFINITION
Given a graph G, a trained model fθ, a “victim” node v from the test set along with its classiﬁcation by the model yˆv = fθ (G, v), we assume that an adversary controls another node a in the graph. The goal of the adversary is to modify its own feature vector xa by adding a perturbation vector η ∈ RD of its choice, such that the model’s classiﬁcation of v will change.
We denote by Gxa+η the graph G where the row of X that corresponds to the node a was added with the vector η. In a non-targeted attack, the goal of the attacker is to ﬁnd a perturbation vector η that will change the classiﬁcation to any other class, i.e., fθ (Gxa+η, v) = fθ (G, v). In a targeted attack, the adversary chooses a speciﬁc label yadv ∈ Y and the adversary’s goal is to force fθ (Gxa+η, v) = yadv.
A modiﬁcation of the features of a will affect the classiﬁcation of v only if the distance in the graph between a and v is lower than or equal L – the number of GNN layers. Otherwise, a will not be contained in the receptive ﬁeld of v, and the attack will result in “under-reaching” (Alon & Yahav, 2020) – any perturbation of a will not affect the prediction of v (Barcelo´ et al., 2020). Therefore, we require that distanceG (a, v) ≤ L.
In this work, we focus on gradient-based attacks. These kinds of attacks assume that the attacker can access a similar model to the model under attack and compute gradients. As recently shown by Wallace et al. (2020), this is reasonable assumption: an attacker can query the original model; using these queries, imitate the model under attack by training an imitation model; ﬁnd adversarial examples using the imitation model; and transfer these adversarial examples back to the original model. Under this assumption, these attacks are general and are applicable to any GNN and dataset.

3.2 CHALLENGES

Unnoticeable Perturbations. Our ﬁrst challenge is to ﬁnd an adversarial example that will allow an imperceptible perturbation of the input. This objective is attainable in continuous domains such as images (Szegedy et al., 2013; Goodfellow et al., 2014) and audio (Carlini & Wagner, 2018) if we constrain l∞-norm of the perturbation vector η. It is, however, unclear what imperceptibility means in graphs. In most GNN datasets, a node’s features are a bag-of-words representation of the words that are associated with the node. For example, in Cora (McCallum et al., 2000; Sen et al., 2008), every node is annotated by a many-hot feature vector of words that appear in the paper; in PubMed (Namata et al., 2012), node vectors are TF-IDF word frequencies; in Twitter (Ribeiro et al., 2017), node features are averages of GloVe embeddings, which can be viewed as word frequency vectors multiplied by a (frozen) embedding matrix. We argue that an attack would be unnoticeable in an academic paper or in a set of Tweets if the frequency of some words is slightly modiﬁed. For example, a particular word may be repeated a few times throughout the text or remain unused.
To constrain the η vector, we require that η ∞ ≤ ∞ – the maximal absolute value of the elements in the perturbation vector – is bounded by ∞ ∈ R+.
Perturbing nodes instead of edges. Previous work mostly focused on perturbing graph edges. Zu¨gner et al. (2018) perturb both edges and node features, but conclude that “perturbations in the

3

structure lead to a stronger change in the surrogate loss compared to feature attacks”; Wu et al. (2019) also conclude that “perturbing edges is more effective than modifying the features”. In this paper, we counter these conclusions and show that small node feature perturbations are stronger: (i) ﬁrst, removing all the edges of a particular node is a special case of node feature perturbation. There exists a perturbation η such that W 1 (xa + η) = 0 , i.e., the modiﬁed feature vector xa + η is in the null space of the ﬁrst GNN layer.1 Such a feature perturbation is equivalent to removing all the edges of the node a. (ii) Second, we argue that perturbing the graph structure is not realistic, because a single attacker controls only its own edges, and cannot control the global graph structure as in previous work (Dai et al., 2018; Bojchevski & Gu¨nnemann, 2019b; Zhang & Zitnik, 2020). (iii) Finally, when a successful attack is caused by removing edges, it is unclear whether the misclassiﬁcation is caused by sensitivity to non-robust features in the data (Ilyas et al., 2019), or simply due to smaller amount of information. Similarly, when a successful attack is caused by inserting edges, it is unclear whether this is simply due to incorrect or unrealistic added information.

3 . 3 F I N D I N G T H E P E RT U R BAT I O N V E C TO R

To ﬁnd the perturbation, we iteratively differentiate the desired loss of v with respect to the perturbation vector η, update η according to the gradient, and add it to the feature vector. In non-targeted attacks, we take the positive gradient of the loss of the undesired label to increase the loss; in targeted attacks, we take the negative gradient of the loss of the adversarial label yadv:

ηt+1 = ηt + γ∇ηJ (fθ (Gxa+ηt , v) , yˆv) non-targeted attack

(4)

ηt − γ∇ηJ (fθ (Gxa+ηt , v) , yadv) targeted attack

where γ ∈ R+ is a learning rate. We repeat this process for a predeﬁned number of K iterations, or until the model predicts the desired label.
Enforcing the constraints. We treat the node features as continuous throughout the attack iterations, whether they are discrete or continuous. Once the attack succeeds, we try to reset to zero as many perturbation vector elements as possible. We sort the perturbation vector elements in a decreasing order, according to their absolute value: i1, ..., iD. We start with the index of η whose absolute value is the largest, ηi1 , and reset the rest of the {i2, ..., iD} elements to zero. We then check whether perturbing only the i1 index is sufﬁcient. If the attack succeeds, we stop. If the attack fails (because of the large number of perturbation vector elements set to zero), we continue perturbing the rest of the elements of η. In the worst case, we perturb all the D vector elements of η. In most cases, we stop much earlier, practically perturbing only a small fraction of the vector elements.
Differentiate by frequencies, not by embeddings. When taking the gradient with respect to the perturbation vector ∇η, there is a subtle, but crucial, difference between the way that node representations are given in the dataset: (a) indicative datasets provide initial node representations X = [x1, x2, ...] that are word indicator vectors (many-hot) or frequencies such as (weighted) bagof-words (Sen et al., 2008; Shchur et al., 2018); (b) in encoded datasets, initial node representations are given encoded, e.g., as an average of word2vec vectors (Hamilton et al., 2017; Hu et al., 2020). Indicative datasets can be converted to encoded by multiplying every vector by an embedding matrix; encoded datasets cannot be converted to indicative, without the authors releasing the textual data that was used to create the encoded dataset.
In indicative datasets, a perturbation of a node vector can be realized as a perturbation of the original text from which the indicative vector was derived. That is, adding or removing words in the text can result in the perturbed node vector. In contrast, a few-indices perturbation in encoded datasets might be an effective attack, but will not be realistic because there is no perturbation of the original text that will result in that perturbation of the vector. That is, when perturbing nodes, it is crucial to use indicative datasets, or convert encoded datasets to the indicative representation from which they were derived (as we do in Section 4) using their original text.

1This equation demonstrates GCN, but similar equations hold for other GNN types like GAT and GIN. 4

Clean (no attack) EdgeGrad SINGLE SINGLE-hops

Cora
80.5±0.8 64.7±0.3 60.1±0.1 69.3±0.9

CiteSeer
68.5±0.7 48.15±0.9
34.0±3.6 45.1±5.2

PubMed
78.5±0.6 59.7±0.7 45.5±0.5 48.7±0.9

Twitter
89.1±0.2 82.7±0.0 72.1±7.2 74.5±6.7

Table 1: Test accuracy under different types of attacks, when the attacker node is chosen randomly. Performed using GCN, and ∞ = 1 for the discrete datasets (Cora and CiteSeer), and ∞ = 0.1 for the continuous datasets (PubMed and Twitter).

4 E VA L UAT I O N
We evaluate and analyze the effectiveness of our SINGLE attack. In Section 4.1, we show that SINGLE is more effective than alternatives such as single-edge attacks. In Section 4.2 we show that if we are allowed to choose the attacker node, SINGLE is much more effective.
Setup. Our implementation is based on PyTorch Geometric (Fey & Lenssen, 2019) and its provided datasets. We trained each GNN type with two layers (L = 2), using the Adam optimizer, early stopped according to the validation set, and applied a dropout of 0.5 between layers. We used up to K = 20 attack iterations. All experiments in this section were performed with GCN. Experiments with additional GNN types such as GAT and GraphSAGE are shown in Section 4.5.
Data. We used Cora and CiteSeer (Sen et al., 2008) which are discrete datasets, i.e., the given node feature vectors are many-hot vectors. Thus, we set ∞ = 1, the minimal possible perturbation. We also used PubMed (Sen et al., 2008) and the Twitter-Hateful-Users (Ribeiro et al., 2017) datasets, which are continuous, and node features represent frequencies of words. Continuous datasets allow a much more subtle perturbation, and we set ∞ = 0.1. An analysis of these values is presented in Section 4.5.
The Twitter-Hateful-Users dataset is originally provided as an encoded dataset, where every node is an average of GloVe vectors (Pennington et al., 2014). We reconstructed this dataset using the original text from Ribeiro et al. (2017), to be able to compute gradients with respect to the weighted histogram of words, rather than the embeddings. We took the most frequent 10,000 words as node features, and used GloVe-Twitter embeddings to multiply by the node features. We thus converted this dataset to indicative rather than encoded. Statistics of all dataset are provided in the supplementary material.
Baselines. In SINGLE (Section 3.3) the attacker node is selected randomly for each victim node, and the attack perturbs this node’s features according to ∞. SINGLE-hops is a modiﬁcation of SINGLE where the attacker node is sampled only among nodes that are not neighbors, i.e., the attacker and the victim are not directly connected ((a, v) ∈/ E). We compare to additional approaches from the literature: EdgeGrad follows most previous work (Xu et al., 2019a; Li et al., 2020; Zu¨gner & Gu¨nnemann, 2020): EdgeGrad randomly samples an attacker node as in SINGLE, and either inserts or removes a single edge from or to the attacker node, according to the gradient.2 If both use a randomly selected attacker node, EdgeGrad is strictly stronger than the GradArgmax attack of Dai et al. (2018), which only removes edges. We run each approach 5 times with different random seeds for each dataset, and report the average and standard deviation.
4.1 MAIN RESULTS
Table 1 shows our main results for non-targeted attacks across various datasets. As shown, SINGLE is more effective than EdgeGrad across all datasets. SINGLE-hops, which is more unnoticeable than attacking with a neighbor node, performs almost as good as SINGLE which attacks using a non-neighboring node, and better than EdgeGrad. On Twitter, SINGLE reduces the test accuracy signiﬁcantly better than EdgeGrad: 72.1% compared to 82.7%. Results for targeted attacks are shown in Appendix A.3.
2This can be implemented easily using edge weights: training the GNN with weights of 1 for existing edges, adding all possible edges with weights of 0, and taking the gradient with respect to the vector of weights.
5

GlobalEdgeGrad SINGLE+GradChoice SINGLE+Topology

Cora
29.7±2.4 31.0±1.9 31.1±1.2

CiteSeer
11.9±0.8 19.0±4.2 18.1±3.4

PubMed
15.1±0.8 8.5±1.2 5.2±0.1

Twitter
82.7±0.0 7.0±1.1 6.6±0.5

Table 2: Test accuracy when the adversary can choose the attacker node.

As we explain in Section 3.3, SINGLE tries to ﬁnd a perturbation vector in which the number of perturbed elements is minimal. We measured the number of vector elements that the attack had perturbed in practice. In PubMed, SINGLE used 76 vector elements on average, which are 15% of the elements in the feature vector. In Cora, SINGLE perturbed 717 elements on average, which are 50%. In CiteSeer, SINGLE used 1165 attributes on average, which are 31% of the features. In Twitter, SINGLE used 892 attributes on average, which are 9% of the features.
4 . 2 AT TAC K E R C H O I C E
If the attacker could choose its node, e.g., by hijacking an existing account in a social network, could they increase the effectiveness of the attack? We examine the effectiveness of two approaches for choosing the attacker node.
Gradient Attacker Choice (GradChoice) chooses the attacker node according to the largest gradient with respect to the node representations (for a non-targeted attack): a∗ = argmaxai∈V ∇xi J (fθ (G, v) , yˆv) ∞. The chosen attacker node is never the victim node itself. Topological Attacker Choice (Topology) chooses the attacker node according to topological properties of the graph. As an example, we choose the neighbor of the victim node v with the smallest number of neighbors: a∗ = argmina∈Nv |Na|. The advantage of this approach is that the attacker choice is model-free: if the attacker cannot compute gradients, they can at least choose the most harmful attacker node, and then perform the perturbation itself using other non-gradient approaches such as in Waniek et al. (2018); Chang et al. (2020).
To perform a fair comparison, we compare these approaches with GlobalEdgeGrad, which is similar to EdgeGrad that can insert or remove an edge, with the difference that the chosen edge can be chosen from the entire graph.
Results. Results for these attacker choice approaches are shown in Table 2. The main results are that choosing the attacker node signiﬁcantly increases the effectiveness of the SINGLE attack: for example, in Twitter, from 72.1% (Table 1) to 6.6% test accuracy (Table 2).
In datasets where the given initial node features are continuous (PubMed and Twitter), SINGLE+GradChoice and SINGLE+Topology are much more effective than GlobalEdgeGrad, showing the superiority of node perturbation over edge perturbation in the global view. SINGLE+Topology is comparable SINGLE+GradChoice – SINGLE+Topology reduces test accuracy down to 6.6% on Twitter while SINGLE+GradChoice reduces test accuracy to 7.0%; SINGLE+Topology reduces test accuracy to 5.2% on PubMed while SINGLE+GradChoice achieves 8.5%; although SINGLE+Topology is model-free.
Interestingly, GradChoice and Topology agree on the choice of attacker node for 49.7% of the nodes in Cora, 21.3% of the nodes in CiteSeer, 49.0% of the nodes in PubMed, and on 45.0% of the nodes in Twitter, showing that the node selection can sometimes be performed model-free.
In datasets where the initial node features are discrete (Cora and CiteSeer), i.e., many-hot vectors, GlobalEdgeGrad reduces the test accuracy more than GradChoice and Topology. We believe that the reason is the difﬁculty of two-step optimization in discrete datasets: for example, GradChoice needs to choose the node, and ﬁnd the perturbation afterwards. Finding a perturbation for a discrete vector is more difﬁcult than in continuous datasets, and the choice of the attacker node may not be optimal.
6

SINGLE-two attackers SINGLE-direct SINGLE

Cora
61.5±0.4 21.2±2.5 60.1±0.1

CiteSeer
39.6±3.7 13.8±2.1 18.1±3.4

PubMed
44.7±1.1 0.3±0.1 45.5±0.5

Twitter
72.1±7.5 57.6±8.7 72.1±7.2

Table 3: Scenario ablation: test accuracy under different attacking scenarios.

clean (no attack) SINGLE SINGLE-hops SINGLE-two attackers SINGLE-direct SINGLE+GradChoice SINGLE+Topology

Standard training
78.5±0.6 45.5±0.5 48.7±0.9 44.7±1.1
0.3±0.1 8.5±1.2 5.2±0.1

Adversarial training
76.9±0.6 58.5±2.7 62.1±2.5 58.4±2.7 4.6±1.1 30.6±6.8 21.1±2.1

Table 4: Test accuracy while attacking a model that was adversarially trained on PubMed, with different types of attacks.

4 . 3 S C E NA R I O A B L AT I O N
The main scenario that we focus on in this paper is a SINGLE approach that always perturbs a single node, which is not the victim node (a = v). We now examine our SINGLE attack in other, easier, scenarios: SINGLE-two attackers follows Zu¨gner et al. (2018) and Zang et al. (2020), randomly samples two attacker nodes and perturbs their features using the same approach as SINGLE. SINGLEdirect perturbs the victim node directly (i.e., a=v), an approach that was found to be the most efﬁcient by Zu¨gner et al. (2018), albeit less realistic.
The results are shown in Table 3. Interestingly, perturbing two attacker nodes performs comparably to perturbing a single-node (SINGLE), across most datasets. This motivates our focus on the singleattacker scenario, and questions whether the multi-node attack assumptions of previous work are required. Expectedly, if we allow SINGLE to perturb the victim node directly (i.e., a = v), the attack is much more effective.
4.4 ADVERSARIAL TRAINING
In the previous sections, we studied the effectiveness of the SINGLE attack. In this section, we investigate to what extent can adversarial training (Madry et al., 2018) defend against SINGLE. For each training step and labeled training node, we perform Ktrain adversarial steps to adversarially perturb another randomly sampled node, exactly as in SINGLE, but at training time. The model is then trained to minimize the original cross-entropy loss and the adversarial loss: 2N1D Ni=D0 J (fθ (G, vi) , yi) + J fθ Gxai +ηi , vi , yi . The main difference from Equation (3) is the adversarial term J fθ Gxai +ηi , vi , yi , where ai is the randomly sampled attacker for the node vi. In every training step, we randomly sample a new attacker for each victim node and compute new ηi vectors. After the model is trained, we attack the model with Ktest SINGLE adversarial steps. This is similar to Feng et al. (2019) and Deng et al. (2019), except that they used adversarial training as a regularizer, to improve the accuracy of a model while not under attack. In contrast, we use adversarial training to defend a model against an attack at test time. We used Ktrain = 5, as we found it to be the maximal value for which the model’s accuracy is not signiﬁcantly hurt while not under attack (“clean”), and Ktest = 20 as in the previous experiments.
As shown in Table 4, adversarial training indeed improves the model’s robustness against the different SINGLE attacks. However, the main result of this section is that SINGLE, SINGLE+GradChoice and SINGLE+Topology are still very effective attacks, as they succeed in attacking the adversarially trained model, reducing its test accuracy to 58.5%, 30.6% and 21.1%, respectively.
7

80 70 60 50 Acc 40 30 20 10
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
∞
GCN GAT GIN GraphSage
Figure 2: Effectiveness of the attack compared to the allowed ∞ (performed on PubMed, because its features are continuous).

80

70

60

50

Acc 40

30

20

PubMed

10

Cora

CiteSeer

12345678 distance (a, v)

Figure 3: Test accuracy compared to the distance between the attacker node and the victim node, when the GCN was trained with L = 8 on PubMed.

4.5 SENSITIVITY TO ∞
How does the intensity of the adversarial perturbation affect the performance of the attack? Intuitively, we say that the less we restrict the perturbation (i.e., larger values of ∞), the more powerful the attack. We examine whether this holds in practice.
In our experiments in Sections 4.1 to 4.4, we used ∞ = 0.1 for the continuous datasets (PubMed and Twitter). In this section, we vary the value of ∞ across different GNN types and observe the effectiveness of the attack. Figure 2 shows the results on PubMed. We used this dataset because it is larger than Cora and CiteSeer (Appendix A.1), and most importantly, its features are continuous, thus real-valued perturbations are feasible. As shown in Figure 2, the most signiﬁcant difference is between performing the perturbation ( ∞ = 0.1) and not attacking at all ( ∞ = 0). As we increase the value of ∞, GCN and GraphSage (Hamilton et al., 2017) show a natural descent in test accuracy. Contrarily, GAT (Velicˇkovic´ et al., 2018) and GIN (Xu et al., 2019b) are more robust to increased absolute values of perturbations, while GAT is also the most robust compared to the other GNN types.
4 . 6 D I S TA N C E B E T W E E N AT TAC K E R A N D V I C T I M
In Section 4.1, we found that SINGLE performs similarly to SINGLE-hops, although SINGLE-hops samples an attacker node a whose distance from the victim node v is at least 2. We further question whether the effectiveness of the attack depend on the distance in the graph between the attacker and the victim. We trained a new model for each dataset using L = 8 layers. Then, for each test victim node, we sampled attackers according to their distance to the test node.
As shown in Figure 3, the effectiveness of the attack increases as the distance between the attacker and the victim decreases. At distance of 5, the curve seems to saturate. A possible explanation for this is that apparently – more than few layers (e.g., L = 2 in Kipf & Welling (2017)) are not needed in most datasets. Thus, the rest of the layers can theoretically learn not to pass much of their input starting from the redundant layers, excluding adversarial signals as well.
5 R E L AT E D WO R K
Works on adversarial attacks on GNN differ in several main aspects. In this section, we discuss the main criteria, to clarify the settings that we address.
Single vs. multiple attackers All previous works allowed perturbing multiple nodes, or edges that are covered by multiple nodes: Zu¨gner et al. (2018) perturb features of a set of attacker nodes; Zang et al. (2020) assume “a few bad actors”; other works perturb edges that in realistic settings their perturbation would require controlling multiple nodes (Bojchevski & Gu¨nnemann, 2019a; Sun et al.,
8

2020; Chen et al., 2018). In this paper, we show that the limited scenario of a single independent attacker is as effective as multiple attacker nodes (Table 3).
Node vs. edge perturbations Most adversarial attacks on GNNs perturb the input graph by modifying the graph structure (Zu¨gner & Gu¨nnemann, 2019; Wang et al., 2020; Xu et al., 2019a). For example, Dai et al. (2018) iteratively remove edges, yet their attack manages to reduce the accuracy by about 10% at most when perturbing a single edge. Li et al. (2020) also allow the insertion of edges; Waniek et al. (2018) and Chang et al. (2020) allow insertion and deletion of edges, using attacks that are based on correlations and eigenvalues, and not on gradients. Yefet et al. (2019) perturb one-hot node vectors, in the restricted domain of computer programs. Zu¨gner et al. (2018) and Wu et al. (2019) perturb both edges and nodes; but they concluded that perturbing edges is more effective than perturbing nodes. In this work, we counter these conclusions and show that perturbing node features is more effective than perturbing edges.
Direct vs. inﬂuence attacks Another difference between prior works lies in the difference between direct attacks and inﬂuence attacks. In direct attacks, the attacker perturbs the target node itself. For example, the attack of Zu¨gner et al. (2018) is the most effective when the attacker and the target are the same node. In inﬂuence attacks, the perturbed nodes are at least one hop away from the victim node. In this paper, we show that the strong direct assumption is not required (SINGLE-direct in Section 4.2), and that our attack is effective when the attacker and the target are not even direct neighbors, i.e., they are at least two hops away (SINGLE-hops in Section 4.1).
Poisoning vs. evasion attacks In a related scenario, some work (Zu¨gner & Gu¨nnemann, 2019; Bojchevski & Gu¨nnemann, 2019a; Li et al., 2020; Zhang & Zitnik, 2020) focuses on poisoning attacks that perturb examples before training. Contrarily, we focus on the standard evasion scenario of adversarial examples in neural networks (Szegedy et al., 2013; Goodfellow et al., 2014), where the attack operates at test time, after the model was trained, as Dai et al. (2018).
Attacking vs. certifying Zu¨gner & Gu¨nnemann (2020) focus on certifying the robustness of GNNs against adversarial perturbations; and Bojchevski & Gu¨nnemann (2019b) certiﬁed PageRank-style models. In contrast, we study the effectiveness of the adversarial attack itself.

6 CONCLUSION
We demonstrate that GNNs are susceptible even to the extremely limited scenario of a single-node indirect adversarial example (SINGLE). The practical consequences of these ﬁndings are that a single attacker in a network can force a GNN to classify any other target node as the attacker’s chosen label, by slightly perturbing some of the attacker’s features. We further show that if the attacker can choose its attacker node – the effectiveness of the attack increases signiﬁcantly. We study the effectiveness of these attacks across various GNN types and datasets.
We believe that this work will drive research in this ﬁeld toward exploring novel defense approaches for GNNs. Such defenses can be crucial for real-world systems that are modeled using GNNs. Furthermore, we believe that the surprising results of this work motivate better theoretical understanding of the expressiveness and generalization of GNNs. To these ends, we make all our code publicly available at https://github.com/benfinkelshtein/SINGLE .

A S U P P L E M E N TA RY M AT E R I A L

Table 5: Dataset statistics.

Cora CiteSeer PubMed Twitter

#Training nodes
140 120 60 4474

#Val nodes
500 500 500 248

#Test nodes
1000 1000 1000
249

#Unlabeled Nodes
2708 3327 19717 95415

#Classes
7 6 3 2

9

A . 1 DATA S E T S TAT I S T I C S Statistics of the datasets that we used are shown in Table 5.

A.2 ADDITIONAL GNN TYPES Tables 6 to 8 show the test accuracy of different attacks while attacking different GNN types.

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology
SINGLE-two attackers SINGLE-direct

Cora
66.4±1.2 40.0±12.5 42.0±11.5
67.8±4.9 43.1±4.9 32.2±6.4
38.9±10.8 23.6±1.5

CiteSeer
49.4±1.4 33.2±6.7 41.7±5.8
48.3±5.1 32.4±4.7 25.5±8.0
38.2±7.3 14.8±4.3

PubMed
64.9±1.0 35.7±13.3 35.5±13.6
63.5±4.6 36.4±8.0 27.8±5.7
35.2±13.3 21.8±2.5

Twitter
82.7±0.0 47.3±36.3 46.8±36.7
82.7±0.0 41.3±37.7 42.6±35.9
47.9±36.6 46.3±37.0

Table 6: Test accuracy of GAT under different non-targeted attacks

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology
SINGLE-two attackers SINGLE-direct

Cora
32.9±3.1 27.1±1.3 32.6±0.7
10.7±2.8 15.9±2.0 16.1±1.7
28.7±0.6 5.7±1.4

CiteSeer
18.5±3.0 12.3±2.9 18.5±3.1
4.8±2.1 8.1±1.7 7.6±1.6
15.5±2.8 4.7±1.6

PubMed
33.3±1.7 12.9±1.0 14.0±0.6
10.3±1.0 10.0±1.6
6.3±1.7
13.3±0.8 3.1±3.1

Table 7: Test accuracy of GIN under different non-targeted attacks

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology
SINGLE-two attackers SINGLE-direct

Cora
62.9±1.9 62.7±2.4 70.0±3.3
48.9±2.7 37.3±3.4 37.4±3.6
64.3±3.0 19.6±2.1

CiteSeer
45.9±3.4 32.3±4.3 45.5±4.3
40.4±3.3 18.0±3.2 19.2±4.2
38.3±4.0 13.5±3.9

PubMed
64.2±1.6 57.1±0.8 60.9±0.8
64.7±1.1 8.2±0.7 6.6±0.3
56.4±0.8 0±1.0

Table 8: Test accuracy of GraphSAGE under different non-targeted attacks

A . 3 TA R G E T E D AT TAC K S
Tables 9 to 12 show the success rate of targeted attacks across datasets and approaches. Differently from Table 1 which shows test accuracy, Tables 9 to 12 presents the targeted attack’s success rate (i.e., higher is better), which is the fraction of test examples that the attack managed to force a speciﬁc label prediction.

10

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology

Cora
8.0±0.7 36.6±2.4 33.6±2.3
59.4±0.9 65.8±1.5 57.3±2.1

CiteSeer
14.8±0.5 60.7±2.2 50.0±2.5
78.7±0.9 67.2±2.2 66.3±3.0

PubMed
20.1±0.6 38.2±0.6 35.2±1.6
80.1±0.6 43.5±1.6 90.4±0.3

Twitter
12.6±2.5 14.6±4.7 12.6±3.7
13.0±2.2 42.2±11.6 55.4±9.4

Table 9: Success rate of different targeted attacks on a GCN network.

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology

Cora
6.1±0.4 33.7±8.6 26.6±7.3
6±1.4 25.8±5.3 41.3±5.3

CiteSeer
12.5±1.2 43.5±11.1 29.88±8.6
14.6±2.8 38.6±8.5 52.5±11.3

PubMed
17.9±1.5 50.7±15.8 50.3±15.7
22.3±3.6 50.5±13.5 63.0±10.2

Table 10: Success rate of different targeted attacks on a GAT network.

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology

Cora
16.8±1.2 31.1±1.7 24.5±1.2
44.7±4.7 44.3±5.0 45.1±2.3

CiteSeer
25.6±1.0 49.0±5.4 37.4±4.1
55.0±7.0 59.0±4.5 58.7±5.1

PubMed
37.9±2.6 58.8±7.9 57.8±5.7
64.9±11.8 63.5±9.9 73.2±13.3

Table 11: Success rate of different targeted attacks on a GIN network.

EdgeGrad SINGLE SINGLE-indirect
GlobalGradEdge SINGLE+GradChoice SINGLE+Topology

Cora
7.6±0.3 24.3±1.9 15.4±3.8
9.3±0.9 49.1±3.0 54.1±1.3

CiteSeer
16.3±1.7 50.0±2.5 34.2±2.6
14.7±1.1 63.7±4.0 69.3±3.2

PubMed
19.1±1.4 27.9±1.0 24.1±1.0
19.6±0.8 36.3±2.1 89.8±0.3

Table 12: Success rate of different targeted attacks on a GraphSAGE network.

11

REFERENCES
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. arXiv preprint arXiv:2006.05205, 2020.
Pablo Barcelo´, Egor V. Kostylev, Mikael Monet, Jorge Pe´rez, Juan Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1lZ7AEKvB.
Aleksandar Bojchevski and Stephan Gu¨nnemann. Adversarial attacks on node embeddings via graph poisoning. In International Conference on Machine Learning, pp. 695–704, 2019a.
Aleksandar Bojchevski and Stephan Gu¨nnemann. Certiﬁable robustness to graph perturbations. In Advances in Neural Information Processing Systems, pp. 8319–8330, 2019b.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 1–7. IEEE, 2018.
Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Wenwu Zhu, and Junzhou Huang. A restricted black-box adversarial framework towards attacking graph embedding models. In AAAI, pp. 3389–3396, 2020.
Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, and Qi Xuan. Fast gradient attack on network embedding. arXiv preprint arXiv:1809.02797, 2018.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In International Conference on Machine Learning, pp. 1115–1124, 2018.
Zhijie Deng, Yinpeng Dong, and Jun Zhu. Latent adversarial training of graph convolution networks. In ICML Workshop on Learning and Reasoning with Graph-Structured Representations, 2019. URL https://graphreason.github.io/papers/3.pdf.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Ala´n Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In Advances in neural information processing systems, pp. 2224–2232, 2015.
Fuli Feng, Xiangnan He, Jie Tang, and Tat-Seng Chua. Graph adversarial training: Dynamically regularizing based on graph structure. IEEE Transactions on Knowledge and Data Engineering, 2019.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in neural information processing systems, pp. 1024–1034, 2017.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, pp. 125–136, 2019.
Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017.
Jure Leskovec and Julian J Mcauley. Learning to discover social circles in ego networks. In Advances in neural information processing systems, pp. 539–547, 2012.
Jintang Li, Tau Xie, Liang Chen, Fentang Xie, Xiangnan He, and Zibin Zheng. Adversarial attack on large scale graph. arXiv preprint arXiv:2009.03488, 2020.
12

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3(2):127–163, 2000.
Alessio Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions on Neural Networks, 20(3):498–511, 2009.
Galileo Namata, Ben London, L. Getoor, and Bert Huang. Query-driven active surveying for collective classiﬁcation. 2012.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, 2014. URL http://www.aclweb.org/anthology/D14-1162.
Manoel Horta Ribeiro, Pedro H Calais, Yuri A Santos, Virg´ılio AF Almeida, and Wagner Meira Jr. “Like sheep among wolves”: Characterizing hateful users on twitter. arXiv preprint arXiv:1801.00317, 2017.
Manoel Horta Ribeiro, Pedro H Calais, Yuri A Santos, Virg´ılio AF Almeida, and Wagner Meira Jr. Characterizing and detecting hateful users on twitter. arXiv preprint arXiv:1803.08977, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pp. 593–607. Springer, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gu¨nnemann. Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018, 2018.
Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar. Non-target-speciﬁc node injection attacks on graph neural networks: A hierarchical reinforcement learning approach. In Proc. WWW, volume 3, 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal reasoning for dynamic knowledge graphs. In International Conference on Machine Learning, pp. 3462–3471, 2017.
Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio`, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Eric Wallace, Mitchell Stern, and Dawn Song. Imitation attacks and defenses for black-box machine translation systems. arXiv preprint arXiv:2004.15015, 2020.
Binghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. Certiﬁed robustness of graph neural networks against adversarial structural perturbation. arXiv preprint arXiv:2008.10715, 2020.
13

Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 349–357, 2018.
Marcin Waniek, Tomasz P Michalak, Michael J Wooldridge, and Talal Rahwan. Hiding individuals and communities in a social network. Nature Human Behaviour, 2(2):139–147, 2018.
Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples for graph data: deep insights into attack and defense. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 4816–4823. AAAI Press, 2019.
Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: an optimization perspective. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 3961–3967. AAAI Press, 2019a.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019b. URL https: //openreview.net/forum?id=ryGs6iA5Km.
Noam Yefet, Uri Alon, and Eran Yahav. Adversarial examples for models of code. arXiv preprint arXiv:1910.07517, 2019.
Xiao Zang, Yi Xie, Jie Chen, and Bo Yuan. Graph universal adversarial attacks: A few bad actors ruin graph learning models. arXiv preprint arXiv:2002.04784, 2020.
Xiang Zhang and Marinka Zitnik. Gnnguard: Defending graph neural networks against adversarial attacks. arXiv preprint arXiv:2006.08149, 2020.
Daniel Zu¨gner and Stephan Gu¨nnemann. Certiﬁable robustness of graph convolutional networks under structure perturbations. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1656–1665, 2020.
Daniel Zu¨gner and Stephan Gu¨nnemann. Adversarial attacks on graph neural networks via meta learning. In International Conference on Learning Representations, 2019. URL https:// openreview.net/forum?id=Bylnx209YX.
Daniel Zu¨gner, Amir Akbarnejad, and Stephan Gu¨nnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2847–2856, 2018.
14

