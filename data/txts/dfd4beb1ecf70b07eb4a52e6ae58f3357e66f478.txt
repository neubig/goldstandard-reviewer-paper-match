arXiv:1807.06610v1 [eess.AS] 17 Jul 2018

Learning Noise-Invariant Representations for Robust Speech Recognition
Davis Liang1, Zhiheng Huang1, Zachary Lipton1,2
1Amazon AI 2Carnegie Mellon University liadavis@amazon.com, zhiheng@amazon.com, zlipton@cmu.edu
Abstract
Despite rapid advances in speech recognition, current models remain brittle to superﬁcial perturbations to their inputs. Small amounts of noise can destroy the performance of an otherwise state-of-the-art model. To harden models against background noise, practitioners often perform data augmentation, adding artiﬁcially-noised examples to the training set, carrying over the original label. In this paper, we hypothesize that a clean example and its superﬁcially perturbed counterparts shouldn’t merely map to the same class — they should map to the same representation. We propose invariant-representation-learning (IRL): At each training iteration, for each training example, we sample a noisy counterpart. We then apply a penalty term to coerce matched representations at each layer (above some chosen layer). Our key results, demonstrated on the Librispeech dataset are the following: (i) IRL signiﬁcantly reduces character error rates (CER) on both ‘clean’ (3.3% vs 6.5%) and ‘other’ (11.0% vs 18.1%) test sets; (ii) on several out-of-domain noise settings (different from those seen during training), IRL’s beneﬁts are even more pronounced. Careful ablations conﬁrm that our results are not simply due to shrinking activations at the chosen layers.
1 Introduction
Over the past several years, a series of papers have developed end-to-end deep learning systems for automatic speech recognition (ASR), advancing the state of the art on a variety of benchmarks [1, 2, 3, 4, 5, 6]. Typically, these models consist of either Recurrent Neural Networks (RNNs) with Sequence-to-Sequence (Seq2Seq) architectures [7] and attention mechanisms [8, 9], RNN transducers [10], transformer networks [11, 12], convolutional neural networks paired with transformer networks [13, 14], or RNNs trained with CTC loss [15]. Often, these models act on spectral features, e.g., Mel-Frequency Cepstral Coefﬁcients (MFCC) [16].
While these systems achieve impressive accuracy when trained and evaluated on clean data, they suffer a well-documented sensitivity to changing noise levels and various noise types [17]. Perhaps this vulnerability should not be surprising, given the signiﬁcant impact that background noise can have on MFCC features [17].
One simple strategy to combat the vulnerability of deep nets to background noise is a technique known generally as data augmentation, and as multi-condition training in the speech recognition community. Here, we augment the original data by applying transformations to which we want our models to be invariant and assigning these perturbed data points the same label as the unperturbed originals. While the computer vision literature has long focused on perturbations like random crops, rotations, translations, and Gaussian noise [18, 19, 20, 21, 22], data augmentation papers in the ASR literature commonly sample snippets of additive background noise from datasets such as MUSAN [23], which contains environmental noise (dial tones, thunder, footsteps, animal noises, etc), music (baroque, classical, romantic, jazz, bluegrass, hip-hop, etc.), and speech. ASR models trained with such augmented data have demonstrated lower grapheme error rates on noisy data [24, 25, 26].
1

In this paper, we draw inspiration from the human ability to recognize not only that a clean clip and its noisy counterpart belong to the same category but that they are produced from the same exact recording. Thus, we propose models that map both clean inputs and their noisy counterparts onto the same point in representation space, introducing this inductive bias via regularization terms, penalizing differences between the hidden representations produced from real and noisy data. Throughout training, for each clean example, we synthesize one noisy counterpart, using a custom data augmentation pipeline that ﬁrst selects a random noise snippet and volume level, adding the two raw waveforms and then generating the corresponding MFCC features on the ﬂy. At each iteration, we apply the original cross-entropy loss on the predictions for both clean and perturbed inputs and also penalize the difference in hidden activations encouraging corresponding activations as quantiﬁed by both cosine distance and L2 distance.
Our experiments address the Librispeech dataset [27], building on a Seq2Seq baseline with cross-entropy loss. To keep the empirical study clean, we do not use a language model. We run all experiments both on the standard dev and test sets and also under a variety of out-of-domain noise conditions. First, we show that while data augmentation improves generalization error on both the original task and under out-ofdomain noise, the models still suffer signiﬁcant degradation in performance. Next, we show that InvariantRepresentation Learners (IRLs) improve signiﬁcantly over generic data augmentation models, both on the clean and other (the more challenging dataset with higher word error rate) subsets of the LibriSpeech test set. Comparisons against an adversarial approach proposed by [28] and the logit pairing approach due to [29] demonstrate the signiﬁcant advantage of IRL. We then demonstrate that on a variety of simulated outof-domain noise conditions, the IRL models are considerably more robust than all baselines. Finally, we perform ablation experiments, showing that our models trained with the IRL algorithm outperform wellknown regularization tactics like weight decay applied on the same representations.
1.1 Related Work
A number of proposed models address the goal of noise-robust speech recognition: [30] proposes a method called noise-aware training that introduces information about the environment as additional inputs to DNNbased acoustic models. [24] proposes augmenting training examples with additive noise sampled from the DEMAND noise database training examples. [28] seeks noise-invariant representations in DNN-HMM architectures through an adversarial learning setup. [31] shows the training on multi-modal data leads to noise robust models. [32] demonstrates that modeling speech as a linear combination of exemplars results in noiserobust ASR models. [33] proposes deep recurrent autoencoders to denoise input features. [34] presents an overview of methods for noise-robust ASR, including recursive cepstral mean and variance normalization [35], joint adaptive training [36], and speaker adaptive training [37]. To our knowledge, no prior work in speech recognition employs our simple approach of penalizing distance between the hidden representations corresponding to clean and noisy signals.
In the most similar paper, [28] claimed that with adversarially trained DNN-HMM systems, the best performance gain is achieved when a small number of noise types are available for training. When using 6 noise classes (airport, babble, car, restaurant, street, and train), [28] found that there was no signiﬁcant difference between the adversarial and baseline models. In contrast, our models show a CER improvement over baseline of 3.1% absolute on test-clean and 6.5% absolute on test-other using hundreds of noise classes.
2 Noise-Invariant Representations
To begin, we formally describe our loss function for enforcing noise-invariant representations on the outputs of a given layer. Because our ﬁrst proposed model focuses noise-invariance in the encoding layer, we dub models using such loss functions IRL-E. In other experiments, we apply a cumulative penalty, additionally requiring noise-invariant representations at all subsequent layers, naming this model IRL-C. We begin by describing IRL-E. Subsequently, extension to IRL-C will be straightforward.
2

Figure 1: Diagram demonstrating the various terms in the IRL loss function as applied to a Seq2Seq attention model. Dotted lines represent shared weights.
2.1 IRL-E
The IRL algorithm is simple: First, during training, for each example x, we produce a noisy version by sampling from x ∼ ν(x), where ν is a stochastic function. In our experiments, this function takes a random snippet from a noise database, sets its amplitude by drawing from a normal distribution, and adds it to the original (in sample space), before converting to spectral features. We then incorporate a penalty term in our loss function to penalize the distance between the encodings of the original data point φe(x) and the noisy data point φe(x ), where φl is representation at layer l. In our experiments, we choose φe to be the output of the encoder in our Seq2Seq model. We illustrate the learning setup graphically in Figure 1. In short, our loss function consists of three terms, one to maximize the probability assigned to the the clean example’s label, another to maximize the probability our model assigned to the noisy example’s (identical) label scaled by hyper-parameter α, and a penalty term to induce noise-invariant representations Ld. In the following equations, we express the loss calculated on a single example x and its noisy counterpart x , omitting sums over the dataset for brevity.
L(θ) = Lc(x; θ) + αLc(x ; θ) + Ld(x, x ; θ),
where θ denotes our model parameters. Because our experiments address multiclass classiﬁcation, our primary loss Lc is cross-entropy:
C
Lc(x; θ) = − yk log yˆk(x; θ),
k=1
where C denotes the vocabulary size and yˆ is our model’s softmax output. To induce similar representations for clean and noised data, we apply a penalty consisting of two terms, the ﬁrst penalizes the L2 distance between φe(x) and φe(x ), the second penalizes their negative cosine distance.
Ld(x, x ; θ) =γ (φe(x) − φe(x ))2 − λ φe(x) · φe(x ) ||φe(x)|| · ||φe(x )||
We jointly penalize the L2 and cosine distance for the following reason. It is possible to lower the L2 distance between the two (clean and noisy) hidden representations simply by shrinking the scale of all
3

encoded representations. Trivially, these could then be dilated again simply by setting large weights in the following layer. On the other hand, it is possible to assign high cosine similarity to the two vectors but for their magnitudes to vary signiﬁcantly. By jointly penalizing L2 and cosine distance, we require that both the clean and noisy representations point in the same direction and are close to each other in magnitude.
2.2 Applying IRL Cumulatively Across Layers (IRL-C)
It is possible for representations to be close (but not identical) in the encoder layer, but to subsequently be pushed apart in subsequent decoder layers. Thus, we introduce another model, IRL-C (C for cumulative), that additionally applies the IRL penalty on all subsequent decoder layers. By requiring noise-invariant representations in multiple layers, we ensure that each training example and its randomly-sampled noisy counterpart have similar representations throughout the network. Note that if the encodings of the clean and noisy examples are identical at the encoder layer, then all subsequent layers will also be identical and thus those penalties will go to 0. We can express this loss as a sum over successive representations φl of the clean φl(x) and noisy φl(x ) data:
Ld(x, x ; θ) = L γ(φl(x) − φl(x ))2 − λ φl(x) · φl(x ) l=e ||φl(x)|| · ||φl(x )||
In our experiments, we ﬁnd that IRL-C consistently gives a small improvement over results achieved with IRL-E.
2.3 Application to Recurrent Speech Models
As described to this point, our loss can be applied on any feedforward neural network with any noise process ν. Applying our technique to recurrent neural networks requires just a few additional considerations. Primarily, we must decide how to deal with the sequence structure. Two natural choices are (i) to concatenate the representations for a given layer across time steps, and then to apply our penalty on the concatenated representations and (ii) to apply the penalty separately at each time step and then to sum (or equivalently, up to a scaling factor to average) over the time steps. These approaches are identical for the L2 penalty but not for the cosine distance penalty, owing to the normalizing factor which may be different at each time step. In this work we take approach (i) concatenating the representations across time steps and then calculating the penalty.
All of our models are based off of the sequence-to-sequence due to [9]. The input to the encoder is a sequence of spectral features, here MFCC, which are encoded by several consecutive layers of LSTM units. The encoder output states are then passed through an attention mechanism which computes the similarity between the decoder hidden states and the encoder output states. The output is a softmax over the vocabulary (here, characters) at each decoder time step.
In our experiments with IRL-E (penalty applied on a single layers), we use the output of the encoder to calculate the penalty. Note that there is one output per step in the input sequence and thus we are concatenating across the T1 steps.
To calculate IRL-C, we also start with the encoder output concatenating across all T1 sequence steps to calculate the IRL penalty. However, for all subsequent layers, we are acting upon layers in the decoder, and thus concatenating across the number of decoding sequence steps T2 for calculating these terms in the IRL-C penalty.
3 Datasets
Librispeech We evaluate all models on the LibriSpeech [27] dataset. This dataset consists of roughly 1000 hours of audio split into training, dev and test partitions. The dataset was carefully designed to ensure that
4

no speaker (person) appears in multiple partitions. Within both the dev and the test partitions, the data is further subdivided into “clean” and “other” subsets based on the speakers. The “clean” portion contains those speakers for which a baseline model had the lowest CER, and the “other” portion contains those speakers for whom the error rate was high. Following common practice in the literature on these datasets, we evaluate all models on the dev-clean, dev-other, test-clean, and test-other splits separately.
The MUSAN Noise Dataset For our additive noise, we draw upon samples from the MUSAN noise dataset [23]. MUSAN was released under a ﬂexible Creative Commons license and consists of approximately 109 hours of noise sampled at 16kHz. The dataset contains music from several genres, namely baroque, classical, romantic jazz, bluegrass, and hip-hop, among others, speech from twelve languages, and a wide assortment of technical and non-technical noises. To generate noisy audio, we ﬁrst add MUSAN noise to the training data point at a signal-to-noise ratio drawn from a Gaussian with a mean of 12dB and a variance of 8dB. This aligns roughly with the scale of noise employed in other papers using multi-condition training [2].
4 Experiments
Before presenting our main results, we brieﬂy describe the model architectures, training details, and the various baselines that we compare against. We also present details on our pipeline for synthesizing noisy speech and explain the experimental setup for evaluating on out-of-domain noise.
4.1 Model Architecture
To facilitate reliable comparisons between our methods and various baseline training schemes, we conduct all experiments using identical architectures and tuning schemes. Because we conduct a large number of experiments and because of the computational expense of unrolling of long speech sequences, we struck a balance between performance and speed when choosing the basic architecture. The encoder for our base model consists of 4 layers: 2 encoder BLSTM layers with 320 hidden units each, followed by 2 encoder LSTM layers with 320 hidden units each. Our decoder accesses the encoded representations using dot product attention, and contains 4 decoder LSTM layers, with 320 hidden units each. Notably, our ﬁrst encoder layer halves the sequence length by concatenating adjacent inputs along the temporal axis. Each model across all of our comparisons has the exact same number of trainable parameters. To keep things simple, we do not use an external language model. Instead we decode predictions from all models via beam search with width 10.
To ensure fair comparisons, we perform hyper-parameter searches separately for each model and account for variability due to initialization by training each model 5 times and keeping the best run as determined on the dev-other partition. Speciﬁcally, we tune the weights on our losses by trying each of the scale values (0.001, 0.01, 0.1, 1, 10, and 100). We found that an α of 1 (the weight on the cross-entropy loss of the noised data), a γ of 0.01 (the weight on the L2 distance loss), and a λ of 0.01 (the weight on the cosine distance loss) worked well.
4.2 Training Details
We train all models with the Adam optimizer with an initial learning rate of 0.001. We employ a learning rate schedule similar to NewBob [38] that decreases by a factor of 2 if there is an increase in validation perplexity epoch-over-epoch. We employ a stopping criterion that ends training if validation perplexity does not decrease for three epochs in a row. We limit each models to a maximum of 40 epochs, although our networks generally converge within 20 epochs.
The primary loss function for each model is cross-entropy loss and our primary evaluation metric to evaluate all models is the character error rate. As described above, the additional loss terms for our IRL models are L2 loss and cosine distance between representations of clean and noisy audio.
5

4.3 Baselines
Our baseline models include a model trained on the standard training data, a model trained with noiseaugmented data, a model trained with noise augmented data and weight decay, and a data augmented model supervised with L2 loss to push activations of the encodings to 0. These ablation tests provide evidence that our IRL algorithm isn’t simply penalizing the norm of the encodings.
• Baseline: Our base model trains the baseline sequence-to-sequence model on the original 960 hours of Librispeech training data.
• Data Augmentation: Our data augmentation model trains the sequence-to-sequence model on both the examples from the 960 hour Librispeech training corpus and the randomly generated noisy counterparts.
• Adversarial: The adversarial model consists of an adversarial noise discriminator trained on top of the encoder outputs. The discriminator consists of 2 layers of 256 ReLu units and a single unit sigmoid output. We train the discriminator to classify whether the representation originates from clean or noised inputs. The encoder meanwhile is trained both to minimize the classiﬁcation loss and to fool the discriminator, in a scheme similar to the reverse gradient technique in the domain-adversarial approach due to [39] and applied to speech by [28].
• Logit Pairing: Our ﬁnal baseline consists of the logit pairing model due to [29] which applies L2 loss and cosine distance loss on the ﬁnal decoder layer logits, enforcing noise-invariant representations but only on the output layer.
4.4 Synthesizing Noise
We train all models on the LibriSpeech corpus, generating noisy data by adding randomly selected noise tracks from the MUSAN dataset with a signal to noise ratio drawn from a Gaussian distribution (12dB mean, 8dB standard deviation) and temporal shift drawn from a uniform distribution (with a range of 0 to 1000ms). For the data augmentation model, this result resembles the typical data augmentation (multi-condition training) procedure.
4.5 Out-of-Domain Noise
Next, we evaluate each of our models on a variety of noise conditions that were not seen at training time. In particular, we consider the following out-of-domain noise conditions: (i) augmenting the test-clean split with overlapping out-of-domain speech from the WSJ-0 dataset [40] to simulate multi-speaker environments, (ii) applying additive noise with various SNRdb to simulate varying noise levels, (iii) modulating the volume of the clean signal to simulate different levels of speaker loudness, (iv) convolving the original wave ﬁle with room impulse responses to simulate the effect of room reverberation on speech, and (v) re-sampling to 8kHz to simulate telephoney data. For each setting, we measure CER on the out-of-domain noise-augmented test-clean data.
5 Results
Our IRL-C model achieves the best CER on both test-clean and test-other 3.3% and 11%, respectively (Table 1). This compares baseline scores of 6.5% and 18.1%, respectively. We note that by comparison, conventional data augmentation is only marginally effective. Among the baselines that we consider, logit pairing performs best (5.1% and 14.8%) although the improvements are not comparable to either IRL model.
We found that weight decay slowed down network convergence and did not outperform pure data augmented training. However, [41] showed that weight decay is most effective with separate λrec and λnonrec
6

Baseline Data Aug. Adversarial Logit Pairing IRL-E IRL-C

dev-clean
6.7% 6.4% 6.7% 5.1% 3.6% 3.4%

Evaluation Set
dev-other
17.8% 16.8% 16.7% 14.5% 11.0% 10.7%

test-clean
6.5% 6.4% 6.5% 5.1% 3.5% 3.3%

Test Set
test-other
18.1% 17.5% 17.6% 14.8% 11.2% 11.0%

Table 1: Evaluation and Test Set Character Error Rate on the Librispeech Corpus.

Error on test-clean In-domain (6SNRdB) In-domain (12SNRdB)
Impulse Convolve Speech (6SNRdB) Speech (12SNRdB) Volume (+6 dB) Volume (−6 dB) Telephoney

Base
6.5% 27.8% 13.5%
24.1% 91.5% 77.8% 6.5% 6.5% 14.2%

Data Aug.
6.4% 10.8% 7.8%

CER on Noisy Data

Adv.

Logit

6.5% 16.5% 12.1%

5.1% 8.7% 6.2%

21.0% 32.0% 15.2% 6.4% 6.3% 12.2%

28.3% 67.7% 34.7% 9.8% 9.6% 21.3%

47.6% 33.0% 11.1% 5.1% 5.0% 10.3%

IRL-E
3.5% 6.0% 4.2%
18.0% 16.4% 7.6% 3.6% 3.6% 7.1%

Table 2: Character error rate for test-clean augmented with noise

IRL-C
3.3% 5.7% 4.1%
13.8% 14.1% 6.8% 3.5% 3.5% 6.4%

hyper-parameters for determining the strength of regularization for the recurrent and non-recurrent weight matrices. We have not tried this in our experiments. Additionally, we discovered that applying multicondition training while naively lowering the activations of hidden representations leads to nearly identical performance (on both the original and out-of-domain noise perturbed test data) and convergence trajectory as the base model trained on noise augmented data. These results support our hypothesis that models trained with the IRL algorithm do not trivially decrease the magnitude of intermediate representations.
Our ﬁnal experiments test the effects of various out-of-domain noise on our models. The results are shown in Table 2. We found that our models trained with the IRL procedure had stronger results (and signiﬁcantly less degradation) across all tasks compared to the baseline and the purely data augmented models. When applying various room reverberation on speech, we found that the IRL-C model had a character error rate of 13.8% compared to 21.0% on the data augmented model and 24.1% on the baseline model. Our IRL-C model shows 14.1% character error rate on out-of-domain overlapping speech compared to 91.5% for the baseline and 32.0% on the data augmented model. We found that decreasing the signal-to-noise ratio also effected the baseline models more than the models trained on the IRL algorithm: our IRL-C model received a character error rate of 5.7% compared to 10.8% for baseline and 27.8% for the purely data augmented model. We found that modifying the volume of the speaker did not effect the accuracy of any of the networks. Finally, we found that our models trained with the IRL algorithm performed better for re-sampled telephoney data, achieving a character error rate of 6.4% for IRL-C compared to 14.2% for baseline and 12.2% for the purely data augmented model.
We also executed some empirical analysis to determine the effect of the various approaches on the distances between noisy examples and their clean counterparts in representation space. In general, our IRL

7

L2 Distance Cosine Distance

Average L2 Distance Between Original and Noise Perturbed Data

0.010

IRL-C

IRL-E

Logit Pairing

Adversarial

0.008

Data Augmentation Baseline

0.006

0.004

0.002

0.000 Encoder Output

Decoder 1

Decoder 2 Layer

(a) L2 distance

Decoder 3

Decoder 4

0.16 Average Cosine Distance Between Original and Noise Perturbed Data

IRL-C

IRL-E

0.14

Logit Pairing

Adversarial

0.12

Data Augmentation Baseline

0.10

0.08

0.06

0.04

0.02

0.E0n0coder Output Decoder 1

Decoder 2 Layer

Decoder 3

Decoder 4

(b) Cosine distance

Figure 2: Average distance between original and noised data for various models (distinct lines) and various layers (x-axis). Subplot (a) depicts L2 distance and (b) depicts cosine distance.

models have the lowest L2 and cosine distances between noisy representations and the clean counterparts. In Figure 2, you can see that although the IRL-E and IRL-C model models have similarly close representations at the encoder layer, neither reaches 0 distance. Then for IRL-E over the subsequent layers, the clean and noisy representations diverge again, while for IRL-C they remain close throughout.
6 Conclusions
In this paper, we demonstrated that enforcing noise-invariant representations by penalizing differences between pairs of clean and noisy data can increase model accuracy on the ASR task, produce models that are robust to out-of-domain noise, and improve convergence speed. The performance gains achieved by IRL come without any impact to inference throughput. We note that our core ideas here can be applied broadly to deep networks for any supervised task. While the speech setting is particularly interesting to us, our methods are equally applicable to other machine learning ﬁelds, notably computer vision. One natural extension might be to experiment with various other loss functions such as triplet losses, requiring that noisy data be both close to its clean counterpart and further away from different clean data. Additionally, our approach might be well-suited to conferring greater robustness to adversarial examples. The comparative improvements over requiring invariant hidden representations vs. invariant logits here raises the possibility that we might be able to realize similar gains over logit pairing in the adversarial setting.
7 Acknowledgements
The authors would like to thank Jeremy Cohen, Mukul Kumar, Karishma Malkan, Julian Salazar, Alex Smola, and Jerry Zhang for helpful discussions and suggestions.
References
[1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., “Deep speech: Scaling up end-to-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014.

8

[2] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, and et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” arXiv preprint arXiv:1512.02595, 2015.
[3] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.
[4] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio, “End-toend attention-based large vocabulary speech recognition,” in International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2016.
[5] Albert Zeyer, Kazuki Irie, Ralf Schlu¨ter, and Hermann Ney, “Improved training of end-to-end attention models for speech recognition,” Interspeech, 2018.
[6] Shiyou Zhou, Linhao Dong, Shuang Xu, and Bo Xu, “Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese,” Interspeech, 2018.
[7] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to sequence learning with neural networks,” in Advances in neural information processing systems (NIPS), 2014.
[8] Minh-Thang Luong, Hieu Pham, and Christopher D Manning, “Effective approaches to attention-based neural machine translation,” arXiv preprint arXiv:1508.04025, 2015.
[9] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” 2014.
[10] Alex Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems (NIPS), 2017.
[12] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu, “Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese,” arXiv preprint arXiv:1804.10752, 2018.
[13] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu, “A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese,” arXiv preprint arXiv:1805.06239, 2018.
[14] Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve, “Wav2letter: an end-to-end convnet-based speech recognition system,” arXiv preprint arXiv:1609.03193, 2016.
[15] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2013.
[16] S. Davis and P. Mermelstein, “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics Speech and Signal Processing, 1980.
[17] U. Bhattacharjee, Swapnanil Gogoi, and Rubi Sharma, “A statistical analysis on the impact of noise on mfcc features for speech recognition,” International Conference on Recent Advances and Innovations in Engineering (ICRAIE), 2016.
9

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances in neural information processing systems (NIPS), 2012.
[19] Guozhong An, “The effects of adding noise during backpropagation training on a generalization performance,” Neural computation, 1996.
[20] Yves Grandvalet and Ste´phane Canu, “Comments on” noise injection into inputs in back propagation learning”,” IEEE Transactions on Systems, Man, and Cybernetics, 1995.
[21] Chris M Bishop, “Training with noise is equivalent to tikhonov regularization,” Neural computation, 1995.
[22] Yves Grandvalet, Ste´phane Canu, and Ste´phane Boucheron, “Noise injection: Theoretical prospects,” Neural Computation, 1997.
[23] D. Snyder, G. Chen, and D. Povey, “Musan: A music, speech, and noise corpus,” 2015.
[24] S. Yin, C. Liu, Z. Zhang, Y. Lin, D. Wang, J. Tejedor, T. Zheng, and Y. Li, “Noisy training for deep neural networks in speech recognition,” EURASIP Journal on Audio, Speech, and Music Processing, 2015.
[25] Hong Yu, Achintya Sarkar, Dennis Alexander Lehmann Thomsen, Zheng-Hua Tan, Zhanyu Ma, and Jun Guo, “Effect of multi-condition training and speech enhancement methods on spooﬁng detection,” in First International Workshop on Sensing, Processing and Learning for Intelligent Machines (SPLINE).
[26] J Rajnoha, “Multi-condition training for unknown environment adaptation in robust asr under real conditions,” Acta Polytechnica, 2009.
[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2015.
[28] Dmitriy Serdyuk, Kartik Audhkhasi, Phile´mon Brakel, Bhuvana Ramabhadran, Samuel Thomas, and Yoshua Bengio, “Invariant representations for noisy speech recognition,” arXiv preprint arXiv:1612.01928, 2016.
[29] H. Kannan, A. Kurakin, and I. Goodfellow, “Adversarial logit pairing,” 2018.
[30] Michael L Seltzer, Dong Yu, and Yongqiang Wang, “An investigation of deep neural networks for noise robust speech recognition,” in International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2013.
[31] Jing Huang and Brian Kingsbury, “Audio-visual deep learning for noise robust speech recognition,” in International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2013.
[32] Jort F Gemmeke, Tuomas Virtanen, and Antti Hurmalainen, “Exemplar-based sparse representations for noise robust automatic speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing, 2011.
[33] Andrew L Maas, Quoc V Le, Tyler M O’Neil, Oriol Vinyals, Patrick Nguyen, and Andrew Y Ng, “Recurrent neural networks for noise reduction in robust asr,” in Thirteenth Annual Conference of the International Speech Communication Association, 2012.
[34] Jinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-Umbach, “An overview of noise-robust automatic speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2014.
10

[35] Olli Viikki, David Bye, and Kari Laurila, “A recursive feature vector normalization approach for robust speech recognition in noise,” in IEEE Conference on Acoustics, Speech and Signal Processing. IEEE, 1998.
[36] Hank Liao and MJ F Gales, “Adaptive training with joint uncertainty decoding for robust recognition of noisy data,” in International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2007.
[37] Tasos Anastasakos, John McDonough, Richard Schwartz, and John Makhoul, “A compact model for speaker-adaptive training,” in International Conference on Spoken Language (ICSLP). IEEE, 1996.
[38] ICSI Berkeley, “Quicknet,” Available: http://www1.icsi.berkeley.edu/Speech/qn.html, 2000. [39] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Lavio-
lette, Mario Marchand, and Victor Lempitsky, “Domain-adversarial training of neural networks,” The Journal of Machine Learning Research (JMLR), 2016. [40] et al. Garofolo, John S., “Csr-i (wsj0) complete ldc93s6a.,” Web Download. Philadelphia: Linguistic Data Consortium, 1993. [41] Markus Kliegl, Siddharth Goyal, Kexin Zhao, Kavya Srinet, and Mohammad Shoeybi, “Trace norm regularization and faster inference for embedded speech recognition rnns,” arXiv preprint arXiv:1710.09026, 2017.
11

