Causal Inference with Selectively Deconfounded Data

arXiv:2002.11096v4 [stat.ML] 7 Mar 2021

Kyra Gan

Andrew A. Li

Zachary C. Lipton

Carnegie Mellon University, Pittsburgh, PA 15213

{kyragan,aali1,zlipton,stayur}@cmu.edu

Sridhar Tayur

Abstract
Given only data generated by a standard confounding graph with unobserved confounder, the Average Treatment Eﬀect (ATE) is not identiﬁable. To estimate the ATE, a practitioner must then either (a) collect deconfounded data; (b) run a clinical trial; or (c) elucidate further properties of the causal graph that might render the ATE identiﬁable. In this paper, we consider the beneﬁt of incorporating a large confounded observational dataset (confounder unobserved ) alongside a small deconfounded observational dataset (confounder revealed ) when estimating the ATE. Our theoretical results suggest that the inclusion of confounded data can signiﬁcantly reduce the quantity of deconfounded data required to estimate the ATE to within a desired accuracy level. Moreover, in some cases—say, genetics—we could imagine retrospectively selecting samples to deconfound. We demonstrate that by actively selecting these samples based upon the (already observed) treatment and outcome, we can reduce sample complexity further. Our theoretical and empirical results establish that the worst-case relative performance of our approach (vs. a natural benchmark) is bounded while our best-case gains are unbounded. Finally, we demonstrate the beneﬁts of selective deconfounding using a large real-world dataset related to genetic mutation in cancer.
1 Introduction
The fundamental problem in causal inference is to estimate causal eﬀects using observational data. This
Proceedings of the 24th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2021, San Diego, California, USA. PMLR: Volume 130. Copyright 2021 by the author(s).

task is particularly motivated by scenarios when experiments are infeasible. While the literature typically addresses a rigid setting in which confounders are either always or never observed, in many applications we might observe confounders for a subset of samples. For example, in healthcare, a particular gene might be suspected to confound the relation between a behavior and a health outcome of interest. Due to the high cost of genetic tests, we might only be able to aﬀord to reveal the value of the genetic confounder for a subset of patients. Note that for a variable such as a genetic mutation, we might observe retrospectively, even after the treatment and outcome have been observed. We call this process of revealing the value of an (initially unobserved) confounder deconfounding, and the samples where treatment, outcome, and confounders are all observed deconfounded data.
So motivated, this paper addresses the middle ground along the confounded-deconfounded spectrum. Naively, one could estimate the ATE with standard methods using only the deconfounded data. First, we ask: how much can we improve our ATE estimates by incorporating confounded data over approaches that rely on deconfounded data alone? Second, motivated by the setting in which our confounders are genetic traits that might be retrospectively observed for cases with known treatments and outcomes, we introduce the problem of selective deconfounding—allocating a ﬁxed budget for revealing the confounder based upon observed treatments and outcomes. This prompts our second question: what is the optimal policy for selecting data to deconfound? To our knowledge, this is the ﬁrst paper that focuses on the case where ample (cheaply-acquired) confounded data is available and we can select only few confounded samples to deconfound (expensive).
We address these questions for a standard confounding graph where the treatment and outcome are binary, and the confounder is categorical. First, we propose a simple method for incorporating confounded data that achieves a constant-factor improvement in ATE estimation error. In short, the inclusion of (inﬁnite) confounded data reduces the number of free parameters

Causal Inference with Selectively Deconfounded Data

Z

T

Y

Figure 1: Causal graph with treatment T , outcome Y , and selectively observed confounder Z

to be estimated, improving our estimates of the remaining parameters. Moreover, due to the multiplicative factors in the causal functional, errors in parameter estimates can compound. Thus, our improvements in parameter estimates yield greater beneﬁts in estimating treatment eﬀects. For binary confounders, our numerical results show that on average, over problem instances selected uniformly on the parameter simplex, our method achieves roughly 2.5× improvements in ATE estimation error.
Next, we show that we can reduce error further by actively choosing which samples to deconfound. Our proposed policy for selecting samples dominates reasonable benchmarks. In the worst case, our method requires no more than 2× as many samples as a natural sampling policy and our best-case gains are unbounded. Moreover, our qualitative analysis characterizes those situations most favorable/unfavorable for our method. We extend our work to the scenario where only a ﬁnite amount of confounded samples is present, demonstrating our qualitative insights continue to apply (Appendix C). Additionally, we validate our methods using COSMIC (Tate et al., 2019; Cosmic, 2019), a real-world dataset containing cancer types, genetic mutations, and other patient features, showing the practical beneﬁts of our proposed sampling policy. Throughout the paper, we implicitly assume that the confounded data was sampled i.i.d. from the target population of interest (but our policy for selecting data to deconfound need not be).
2 Related Work
Causal inference has been studied thoroughly under the ignorability assumption, i.e., no unobserved confounding (Neyman, 1923; Rubin, 1974; Holland, 1986). Some approaches for estimating the ATE under ignorability include inverse propensity score weighting (Rosenbaum and Rubin, 1983; Hirano et al., 2003; McCaﬀrey et al., 2004), matching (Dehejia and Wahba, 2002), the backdoor adjustment (Pearl, 1995), and targeted learning (Van der Laan and Rose, 2011). Some related papers look to combine various sources of information, for instance from randomized control trials and observational data to estimate the ATE (Stuart et al., 2011; Hartman et al., 2015). Other papers leverage

machine learning techniques, such as random forests, for estimating causal eﬀects (Alaa and van der Schaar, 2017; Wager and Athey, 2018).
Some papers investigate ATE estimation with confounded data by leveraging mediators (Pearl, 1995) and proxies (Miao et al., 2018). Others investigate combining confounded observational data with experimental data. Kuroki and Pearl (2014) identify graphical structures under which causal eﬀect can be identiﬁed. Miao et al. (2018) propose to use two diﬀerent types of proxies to recover causal eﬀects with one unobserved confounder. Shi et al. (2020) extend the work by Miao et al. (2018) to multiple confounders. However, both methods require knowledge of proxy categories a priori and are not robust under misspeciﬁcation of proxy categories. Louizos et al. (2017) use variational autoencoders to recover the causal eﬀect under the model where when conditioned on the unobserved confounders, the proxies are independent of treatment and outcome. Pearl (1995) introduces the front-door adjustment, expressing the causal eﬀect as a functional that concerns only the (possibly confounded) treatment and outcome, and an (unconfounded) mediator that transmits the entire eﬀect.
In other work, Bareinboim and Pearl (2013) propose to combine observational and experimental data under distribution shift, learning the treatment eﬀect from the experimental data and transporting it to the confounded observational data to obtain a bias-free estimator for the causal eﬀect. Recently, Kallus et al. (2018) propose a two-step process to remove hidden confounding by incorporating experimental data. Lastly, few papers provide ﬁnite sample guarantees for causal inference. Shalit et al. (2017) upper bound the estimation error for a family of algorithms that estimate causal eﬀects under the ignorability assumption.
Unlike most prior work, we (i) address confounded and deconfounded (but not experimental) observational data, (ii) perform ﬁnite sample analysis to quantify the relative beneﬁt of additional confounded and deconfounded data towards improving our estimate of the average treatment eﬀect, and (iii) investigate sampleeﬃcient policies for selective deconfounding.
3 Methods and Theory
Let T and Y be random variables denoting the treatment and outcome. We restrict these to be binary, viewing T as an indicator of whether a particular treatment has occurred and Y as an indicator of whether the outcome was successful. In this work, we assume the existence of a single (possible) confounder, denoted Z, which can take up to k categorical values (Figure 1). In addition, although we only include one unobserved

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

confounder in our model, because our variables are categorical, (as shown in Section 3.1) this subsumes scenarios with multiple categorical confounders. Following Pearl’s nomenclature (Pearl, 2000), let

P (Y = y|do(T = t)) := PY |T,Z (y|t, z)PZ (z).
z∈[k]

Our goal is to estimate the ATE, which can be expressed via the back-door adjustment in terms of the joint distribution PY,T,Z on (Y, T, Z), as:

ATE := P (Y = 1|do(T = 1)) − P (Y = 1|do(T = 0)),

=

PY |T,Z (1|1, z) − PY |T,Z (1|0, z) PZ (z). (1)

z∈[k]

Our key contribution is to analyze and empirically validate methods for estimating the ATE from both confounded and deconfounded observations. In our setup, the confounded data contains n i.i.d. samples from the joint distribution PY,T (marginalized over the hidden confounder Z), and the deconfounded data contains m i.i.d. samples from the full joint distribution PY,T,Z . Thus, the confounded and deconfounded data are (y, t) and (y, t, z) tuples, respectively. Recall that here deconfounding means selecting a confounded data point (y, t) and revealing the value of its confounder z. There are two ways that we can obtain m deconfounded data, one through collecting m deconfounded data directly without using the confounded data, and the other through revealing the value of the confounder for m confounded data points. Note that given this graph, we cannot exactly calculate the ATE unless we intervene or make further assumptions on the structure of the causal graph. Recall that such interventions or graph structures may not be available (e.g., in the case of genetic mutation). Furthermore, when deconfounded data is scarce and confounded data is comparatively plentiful, we hope to improve our ATE estimates.

3.1 Generalizability of Our Model
First, we note that the use of categorical (even binary) data is well-established in both theory (Bareinboim and Pearl, 2013) and application (Knudson, 2001; Rayner et al., 2016), and not merely a simplifying proxy for continuous data. Next, we show that our model subsumes scenarios with multiple categorical confounders. First, absent additional distributional assumptions, our model captures multiple unobserved confounders by simple concatenation (since we impose no limit on the number of classes) without loss. Now, one could make additional assumptions (indeed, a high-dimensional setting might necessitate such assumptions) that could render alternative algorithms applicable. However, there exist many applications where (a) the confounder is of moderate dimension; and (b) a practitioner would be dubious

of any additional assumption (Bates et al., 2020). Second, although in this scenario we implicitly assume that the set of confounders is either never observed or entirely observed, this is also without loss so long as the costs of revealing each confounders are the same (e.g. the genetic example). Intuitively, because we do not impose any independence assumption on the set of confounder, revealing all confounders oﬀers maximal information on the joint distribution of the confounders. We formalize this statement in Appendix A.1. While for simplicity we focus only on the setting in which our confounder can be retroactively observed, as we show in Appendix A.2 our model can be applied straightforwardly to handle a set of additional pretreatment covariates.

3.2 Inﬁnite Confounded Data
In this subsection, we address the setting where we have an inﬁnite amount of confounded data (n = ∞), i.e., the marginal distribution PY,T is known exactly. We leave the analysis on ﬁnite confounded data to Appendix C.

Deconfounded Data Alone We begin with the baseline approach of using only the deconfounded data. Let pzyt = PY,T,Z (y, t, z), and let pˆzyt be empirical estimate of pzyt from the deconfounded data using the
Maximum Likelihood Estimator (MLE). Let ATE be the estimated average treatment eﬀect calculated by plugging pˆzyt’s into Equation (1). In the following theorem, we show a quantity of deconfounded samples m which is suﬃcient to estimate the ATE to within a desired level of accuracy under the estimation process described above. Let C = 12.5k2 ln(8k/δ) −2 throughout.
Theorem 1. (Upper Bound) Using deconfounded data
alone, P ATE − ATE ≥ < δ is satisﬁed if the
deconfounded sample size m is at least

mbase := max C
t,z

−2

pzyt
y

1

= max

C.

t,z PT,Z (t, z)2

The proof of Theorem 1 (Appendix B.2) relies on an additive decomposition of ATE estimation error in terms of the estimation errors on the pzyt’s, along with concentration via Hoeﬀding’s inequality. We will contrast Theorem 1 with counterpart methods that use confounded data.
Incorporating Confounded Data Estimating the ATE requires estimating the entire distribution PY,T,Z . To assess the utility of confounded data, we decompose

Causal Inference with Selectively Deconfounded Data

PY,T,Z into two components: (i) the confounded distribution PY,T ; and (ii) the conditional distributions PZ|Y,T . Given inﬁnite confounded data, the confounded distribution PY,T is known exactly, reducing the number of free parameters in PY,T,Z by three. The deconfounded data can then be used exclusively to estimate the conditional distributions PZ|Y,T . To ease notation, let ayt = PY,T (y, t), and let qyzt = PZ=z|Y,T (y, t). Moreover, let a:= (a00, a01, a10, a11), and let q denote the vector that contains qyzt for all values of Y, T and Z.
Hardness of The Problem We ﬁrst show that for particular choices of the conditional distributions, this estimation problem can be arbitrarily hard for any confounded distribution a. In particular, we show that for every ﬁxed confounded distribution encoded by a and for any ﬁnite amount of deconfounded data m, there exist two conditional distributions encoded by q’s such that we cannot distinguish these two distributions with high probability while their corresponding ATE values are constant away from each other. Let ATEa(q) denote the value of the ATE when evaluated under the distributions a and q. Then, we have
Proposition 1. For every a, there exists some , δ such that for any ﬁxed number of deconfounded samples m, we can always construct a pair of q’s, say q1 and q2, such that no algorithm can distinguish these two conditional distributions with probability more than 1 − δ, and their corresponding ATE values are away: |ATEa(q1) − ATEa(q2)| ≥ .
Here, is a function of the confounded distribution a, and the values of q1 and q2 depend on the variable δ and the number of deconfounded samples, m. The proof of Proposition 1 (Appendix B.3) relies on constructing a pair of q1 and q2 such that the value of |ATEa(q1) − ATEa(q2)| is constant for all confounded distribution a where the entries of a are strictly positive. In particular, this happens when the entries of the conditional distribution q approach to 0.
Unless otherwise mentioned, in the rest of the paper, we assume that each entry of the conditional distribution, qyzt, is bounded within the interval [β, 1 − β], for some small positive constant β. We ﬁrst provide a lower bound on the sample complexity needed for any algorithm and any confounded distribution:
Theorem 2. (Lower Bound) For any estimator and sample selection policy, the number of deconfounded
samples m needed to achieve P ATE − ATE ≥ <
δ is at least Ω( −2 log(δ−1)).
The proof of Theorem 2 (Appendix B.4) proceeds by construction. When comparing this lower bound with the upper bounds that we will present later, we observe that our sample complexities are tight up to a constant.

In the rest of the section, we ﬁrst derive an upper bound of the sample complexity of a natural policy that is analogous to passive sampling (Theorem 3). We then derive the worst-case upper bound over all possible conditional distributions, PZ|Y,T , in Corollary 1. Next, we propose two additional sampling policies, one of which enjoys an instance independent guarantee over the worst-case conditional distribution in PZ|Y,T . We compare these sampling policies by investigating their sample complexity upper bounds (Theorem 5), worstcase upper bounds (Corollary 2), and lower bounds (Theorem 6). Table 1 summarizes our worst-case upper bound and lower bound results. In addition, we derive a worst-case sample complexity guarantee of our proposed sampling policy in Theorem 4.

Let qˆyzt be the empirical estimate of qyzt from the confounded data using the MLE (where m confounded data were deconfounded randomly). Then, we will al-
ways calculate ATE by plugging the ayt’s and qˆyzt’s into Equation (1). The following theorem upper bounds the sample complexity for this estimator (later, we refer to this sampling policy as the natural selection policy):

Theorem 3. (Upper Bound) When incorporating (in-
ﬁnite) confounded data, P (|ATE − ATE| ≥ ) < δ is satisﬁed if the number of deconfounded samples m is at least

mnsp := max
t,z

C y ayt

PT (t)

z

2

= max

C.

t,z PT,Z (t, z)2

(2)

y aytqyt

The proof of Theorem 3 is included in Appendix B.5. Notably, mnsp is less than mbase for any problem instance, highlighting the value of confounded data.
In addition, when qyzt ∈ [β, 1 − β], the maximum of Equation (2) over q is obtained at qyzt = β for some t, z. Let Mnsp be the worst-case mnsp over all possible values of q. Since min maxt 1/( y ayt) is achieved when y ayt = 1/2, maxt 1/( y ayt) ≥ 2. Thus,
Corollary 1. (Worst-Case Upper Bound Guarantee)

C

2C

Mnsp

:=

max
q

mnsp

=

max
t

β2

≥. y ayt β2

Sample Selection Policies One important conse-
quence of our procedure for estimating the ATE is that
the four conditional distributions are estimated sepa-
rately: the deconfounded data is partitioned into four groups, one for each (y, t) ∈ {0, 1}2, and the empirical measures qˆyzt’s are then calculated separately. This means that the procedure does not rely on the fact
that the deconfounded data is drawn from the exact
distribution PY,T,Z , and in particular, the draws might as well have been made directly from the conditional

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

NSP USP OWSP

β−2C1 maxt 4β−2C1 maxt 2β−2C1 maxt

w

, a1t( y ayt¯)2
( y ayt)2

a0t( y ayt¯)2 ( y ayt)2

, a21t( y ayt¯)2
( y ayt)2

a20t( y ayt¯)2 ( y ayt)2

, a1t( y ayt¯)2
y ayt

a0t( y ayt¯)2 y ayt

M

β−2C maxt 4β−2C maxt (

1 y ayt
y a2yt y ayt)2

2β−2C

Table 1: Comparison between the instance-speciﬁc lower bound (w) and the worst-case upper bound (M ).

distributions PZ|Y,T . Suppose now that we can draw directly from these conditional distributions. This situation may arise when the confounder is ﬁxed (like a genetic trait) and can be observed retrospectively. We now ask, given a budget for selective deconfounding samples, how should we allocate our samples among the four groups ((y, t) ∈ {0, 1}2)?
Let x = (x00, x01, x10, x11) denote a selection policy with xyt indicating the proportion of samples allocated to group (y, t), and yt xyt = 1. We consider the following three non-adaptive selection policies:
1. Natural (NSP): xyt = ayt = PY,T (y, t)—this is similar to drawing from PY,T,Z , and is analogous to passive sampling.
2. Uniform (USP): xyt = 1/4. Splits samples evenly across all four conditional distributions.
3. Outcome-weighted (OWSP): xyt = 2 ayytayt = PY |T (y|t)/2. Splits samples evenly across treatment groups (T = 0 vs. 1), and within each treatment group, choosing the number of samples to be proportional to the outcome (Y = 0 vs. 1).
While the particular form of OWSP appears to be the least intuitive, later we show it was in fact the unique policy that provides an instance independent guarantee when considering the worst-case q’s.
For some ﬁxed and δ, let µnsp be the minimum number of samples needed to achieve P (|ATE − ATE| ≥ ) < δ under the natural selection policy over all estimators. We similarly deﬁne µusp and µowsp. Then Theorem 3 provides an upper bound on µnsp by studying the upper bound of a speciﬁc estimator. Before we provide an upper bound of the sample complexity of µusp and µowsp, we ﬁrst establish that µnsp may be signiﬁcantly worse than µowsp, but µowsp is never much worse.
Theorem 4. For any ﬁxed ∈ [0, 0.5 − 2β(1 − β)] and any ﬁxed δ < 1, there exist distributions where µowsp/µnsp is arbitrarily close to zero. In addition, for any estimator and every distribution, µowsp/µnsp ≤ 2.

The proof of Theorem 4 (Appendix B.6) proceeds by construction. Note that the upper bound of in Theorem 4 is not necessary the maximum achievable . Instead it provides a range where Theorem 4 holds. Next, we provide the upper bounds of µusp and µowsp by analyzing our algorithm (analogous to Theorems 1 and 3):

Theorem 5. (Upper Bound) Under the uniform selection policy, with (inﬁnite) confounded data incorporated, P (|ATE − ATE| ≥ ) < δ is satisﬁed if µusp is at least

musp := max
t,z

C y 4a2yt

4

2 = max

y aytqyzt

t,z

y PY,T (y, t)2 PT,Z (t, z)2 C.

Similarly, for the outcome-weighted selection policy:

2

2C y ayt

2

mowsp := max
t,z

2

= max t,z P

(z|t)2 C.

y aytqyzt

Z |T

The proofs of Theorems 3 and 5 (Appendix B.5), which diﬀer from that of Theorem 1, require a modiﬁcation to Hoeﬀding’s inequality (Appendix, Lemma 4), which we derive to bound the sample complexity of the weighted sum of two independent random variables. Theorem 5 points to some additional advantages of OWSP. First, OWSP has the nice property that the suﬃcient number of samples, mowsp, does not depend on PY,T . Second, a comparison of the quantities musp and mowsp suggests that USP is strictly dominated by OWSP, since 4a20t + 4a21t − 2(a0t + a1t)2 = 2(a0t − a1t)2 ≥ 0. We might hope for a similar result by comparing mowsp with mnsp from comparing Theorems 3 and 5, but neither strictly dominates the other. Instead, recall that Theorem 4 shows that µnsp may be signiﬁcantly worse than µowsp, but µowsp is never much worse. Similar to Corollary 1, we now derive an equivalent corollary for Theorem 5 where we consider the worst case over q’s.

Let Musp and Mowsp be the maximum values of musp and mowsp, respectively, over all possible values of q.
Corollary 2. (Worst-Case Upper Bound Guarantee)

4C

Musp

=

max
t

β2(

y a2yt

2C

y ayt)2 ; Mowsp = mtax β2 ≤ Mnsp.

Average Absolute Error Average Absolute Error

Causal Inference with Selectively Deconfounded Data

0.09

Deconf only

0.08

Conf + NSP

0.07

0.06

0.05

0.04

0.03

0.02

0.01

1N00umb3e0r0of D50e0con7fo00unde9d00Sam11p00les

0.040

Conf + NSP

0.035

USP OWSP

0.030

0.025

0.020

0.015

0.010

1N00umb3e0r0of D50e0con7fo00unde9d00Sam11p00les

ErrAovreorfagOeWSP

ErrAovreorfagOeWSP

0.15 0.10 0.05 0.00
0.00
0.20 0.15 0.10 0.05 0.00
0.00

0A.0v5erag0.e10Erro0r.1o5f NS0P.20 0.25 A0v.0e5rage E0.r1r0or of U0.S15P 0.20

Figure 2: Performance of the four policies over 13,000 distributions PY,T,Z , given inﬁnite confounded data. Left and Middle: averaged error over 13,000 distributions for varying numbers of deconfounded samples. Right: error comparison (each point is a single distribution averaged over 100 replications) for 1,200 deconfounded samples.

First, note that Mowsp is independent of the confounded distribution a. Furthermore, from the proof of Theorem 5, we observe that OWSP is the unique policy that makes this upper bound independent of a. When comparing Corollaries 1 and 2, we observe that OWSP always dominates NSP when taking the worst case over q’s. Lastly, we provide the lower bounds of µnsp, µusp, and µowsp that are analogous to Theorem 2:
Theorem 6. (Lower Bound) For every a, there exists a q such that µnsp is at least

C1

a1t( y ayt¯)2 a0t( y ayt¯)2

wnsp := β2 mtax ( y ayt)2 , ( y ayt)2 ;

similarly for uniform selection policy:

C1

a21t( y ayt¯)2 a20t( y ayt¯)2

wusp := β2 mtax 4 ( y ayt)2 , 4 ( y ayt)2 ;

similarly for outcome-weighted sample selection policy:

C1

a1t( y ayt¯)2 a0t( y ayt¯)2

wowsp := β2 mtax 2 y ayt , 2 y ayt ,

where t¯ = 1 − t and C1 ∝ (kβ − 1)2 ln(δ−1) −2.

The proof (Appendix B.7) proceeds by construction. Table 1 summarizes our worst-case upper bounds and instance-speciﬁc lower bounds. When comparing the constants C and C1, we observe that the upper bounds and lower bounds match in k, , and δ, demonstrating the relative tightness of our analysis.
We have shown the advantages of OWSP given an inﬁnite amount of confounded data. However, in practice, the confounded data is ﬁnite. In Appendix C, we analyze the sample complexity upper bound of our

algorithm under ﬁnite confounded data. One new issue that arises with ﬁnite confounded data is that a sampling policy may not be feasible because there are not enough confounded samples to deconfound. In our experiments, when this happens, we approximate the target sampling policy as closely as is feasible (see Appendix E).
4 Experiments
Since the upper bounds that we derived in Section 3 are not necessarily tight, we ﬁrst perform synthetic experiments to assess the tightness of our bounds. For the purpose of illustration, we focus on binary confounders Z throughout this section, and denote qyt = PZ=1|Y,T (y, t). We ﬁrst compare the sampling policies in synthetic experiments on randomly chosen distributions PY,T,Z , measuring both the average and worst-case performance of each sampling policy. We then measure the eﬀect of having ﬁnite (vs. inﬁnite) confounded data. Finally, we test the performance of OWSP on real-world data taken from a genetic database, COSMIC, that includes genetic mutations of cancer patients (Tate et al., 2019; Cosmic, 2019). Because this is (to our knowledge) the ﬁrst paper to investigate the problem of selective deconfounding, the methods in described Section 2 are not directly comparable to ours.
4.1 Inﬁnite Confounded Data
Assuming access to inﬁnite confounded data, we experimentally evaluate all four sampling methods for estimating the ATE: using deconfounded data alone, and using confounded data that has been selected according to NSP, USP, and OWSP. Let a :=

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

Average Absolute Error

0.25

Conf + NSP USP

OWSP

0.20

0.15

0.10

0.05

0.00
5N0umbe1r5o0f Dec2o50nfoun3d5e0d Sa4m5p0les

0.175

Conf + NSP USP OWSP

0.150

0.125

0.100

0.075

0.050

0.025

5N0umbe1r5o0f Dec2o50nfoun3d5e0d Sa4m5p0les

Average Absolute Error

Average Absolute Error

0.018

Conf + NSP

USP

0.016

OWSP

0.014

0.012

0.010

0.008

0.006

0.004

0.002
5N0umbe1r5o0f Dec2o50nfoun3d5e0d Sa4m5p0les

Conf + NSP USP
0.08 OWSP

0.06

0.04

0.02

5N0umbe1r5o0f Dec2o50nfoun3d5e0d Sa4m5p0les

Average Absolute Error

Average Absolute Error

0.040

Conf + NSP USP OWSP

0.035

0.030

0.025

0.020

0.015

0.010
5N0umbe1r5o0f Dec2o50nfoun3d5e0d Sa4m5p0les

Conf + NSP

0.07

USP OWSP

0.06

0.05

0.04

0.03

0.02

0.01
5N0umbe1r5o0f Dec2o50nfoun3d5e0d Sa4m5p0les

Average Absolute Error

Figure 3: Comparison of selection policies for adversarially chosen instances. Top (left) NSP performs the worst: a = (0.9, 0.02, 0.01, 0.07) and q = (0.9, 0.7, 0.01, 0.3). Top (middle) USP performs the worst: a = (0.79, 0.01, 0.02, 0.18) and q = (0.5, 0.01, 0.05, 0.5). Top (right) OWSP performs the worst: a = (0.5, 0.01, 0.19, 0.3) and q = (0.05, 0.5, 0.055, 0.4). Bottom: the same a’s but averaged over 500 q’s drawn uniformly from [0, 1]4.

(a00, a01, a10, a11) , and q := (q00, q01, q10, q11) , encoding the confounded and conditional distributions, respectively. We evaluate the performance of four methods in terms of the absolute error, |ATE − ATE|. Because the variance of our estimators cannot be analyzed in closed form, we report the variance of the absolute error averaged over diﬀerent instances in terms of the error bar in the ﬁgures.
Randomly Generated Instances We ﬁrst evaluate the four methods over a randomly selected set of distributions. Figure 2 was generated by averaging over 13,000 instances, each with the distribution PY,T,Z drawn uniformly from the unit 7-Simplex. Every instance consists of 100 replications, each with a random draw of 1,200 deconfounded samples. The absolute error is measured as a function of the number of deconfounded samples in steps of 100 samples. Figure 2 (left) compares the use of deconfounded data along with the incorporation of confounded data selected naturally (as in the comparison of Theorems 1 and 3). It shows that incorporating confounded data yields a signiﬁcant improvement in estimation error. For example, achieving an absolute error of 0.02 using deconfounded data alone requires more than 1,200 samples on average, while by incorporating confounded data, only 300 samples are required. We observe that the variance of our estimator

has decreased dramatically by incorporating inﬁnite confounded data. Having established the value of confounded data, Figure 2 (middle) compares the three selection policies. We ﬁnd that OWSP outperforms both NSP and USP in terms of both the absolute error and the variance when averaged over joint distributions. To compare the performance of our sampling policies on an instance level, we provide two scatter plots in Figure 2 (right), each containing the 13,000 instances in the left ﬁgures and averaged over 100 replications. The number of deconfounded samples is ﬁxed at 1,200. We observe that OWSP outperforms NSP and USP in the majority of instances.
Worst-Case Instances We evaluate the performance of the three selection policies on joint distributions chosen adversarially against each in Figure 3. The three sub-ﬁgures (the columns) correspond to instances where NSP, USP, and OWSP perform the worst, respectively, from the left to the right. Each sub-ﬁgure is further subdivided: the top contains results for the single adversarial example while the bottom is averaged over 500 q’s sampled uniformly from [0, 1]4. The absolute error is averaged over 10,000 replications in the left ﬁgures and over 500 in the right. In all cases, we draw 500 deconfounded samples and measure the absolute error in steps of 50 samples. Figure 3 (left) validates

Average Absolute Error

Causal Inference with Selectively Deconfounded Data

0.08

NSP USP

0.07

OWSP

0.06

0.05

0.04

0.03
1N0umb21er of46Con1fo00und2e15d S4a6m4pl1e0s00

Figure 4: Experiment on ﬁnite confounded data over 13,000 distributions PY,T,Z , each averaged over 100 replications. The number of deconfounded samples is ﬁxed at 100. Left: averaged over the 13,000 distributions. Middle and Right: error comparison at 681 confounded samples.

Corollary 4. We observe that when the distribution of a is heavily skewed towards (Y = 0, T = 0), OWSP and USP signiﬁcantly outperform NSP. Figure 3 (middle) shows that USP can underperform NSP, but when averaged over all possible values of q, USP performs better than NSP. Figure 3 (right) shows that OWSP can underperform NSP and USP, but, when compared with the left and middle column, the performance of OWSP is close to that of NSP and USP. When averaged over all possible values of q, OWSP outperforms both. Moreover, OWSP’s variance is the lowest across all scenarios. Appendix D provides examples in which each of these joint distributions could appear.
4.2 Finite Confounded Data
Given only n confounded data, we test the performance of the OWSP against NSP and USP. In Figure 4, the absolute error is measured as a function of the number of confounded samples in step sizes that increment in the log scale from 100 to 10,000 while ﬁxing the number of deconfounded samples to 100. Since when we only have 100 confounded samples, the three sampling policies are identical, the error curves corresponding to NSP, USP and OWSP start at the same point on the top left corner in Figure 4 (left). We observe that as the number of confounded samples increases, OWSP quickly outperforms NSP and USP on average, and the gaps between OWSP and the other two selection policies widen. Since we ﬁx the number of deconfounded samples to be 100, 1) all three sampling policies are equivalent when there are only 100 confounded samples in the dataset (i.e., we need to deconfound all 100 confounded samples in all cases), and 2) the average absolute errors of the three selection policies do not converge to 0 in Figure 4. Figure 4 (middle and right) compare the performance of OWSP with that of the NSP and USP, respectively, on an instance level. We observe that OWSP dominates NSP and

USP in the majority of instances by both the absolute error and variance. Note that if we ﬁx the number of confounded samples and increase the number of deconfounded samples (with m ≤ n), we observe that OWSP dominates USP and NSP when the number of deconfounded samples is small. The gap shrinks as the number of deconfounded samples increases. When at m = n, all three methods are equivalent.
4.3 Real-World Experiments: COSMIC
Data Previously, we chose the underlying distribution PY,T,Z uniformly from the unit 7-Simplex. However, real-world problems of interest may not be uniformly distributed. To illustrate the practicality of our methods, we consider a real-world dataset, picking three variables to be the outcome, treatment, and confounder, and artiﬁcially hiding the confounder for some samples. Finally, we evaluate our proposed sampling methods under the assumption that we have access to inﬁnitely many confounded samples. The Catalogue Of Somatic Mutations In Cancer (COSMIC) is a public database of DNA sequences of tumor samples. It consists of targeted gene-screening panels aggregated and manually curated over 25,000 peer reviewed papers. We focus on the variables: primary cancer site and gene. Speciﬁcally, for 1,350,015 cancer patients, we observe their cancer types, and for a subset of genes, whether or not a mutation was observed in each gene.
Causal Models In our experiments, we designate cancer type as the outcome, a particular mutation as the treatment, and another mutation as the confounder—this setup seems reasonable because it is well known that multiple genetic mutations are correlated with individual cancer types (Knudson, 2001), and that mutations can cause both cancer itself and other mutations. As a concrete example, mutations in the genes that code RNA polymerases (responsible for

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

Average Absolute Error Average Error of OWSP Average Error of OWSP

Deconf only

0.04

Conf + NSP Conf + USP

Conf + OWSP

0.03

0.02

0.01

0.00
1N5um30ber45of 6D0ec7o5nfo90un1d0e5d12S0a1m3p5l1e5s0

0.200 0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000
0.00 A0.v0e5rage E0r.1r0or of N0S.1P5 0.20

0.06 0.05 0.04 0.03 0.02 0.01 0.00
0.00 0.0A1 ve0ra.0g2e E0r.0ro3r o0f.0U4SP 0.05 0.06

Figure 5: Performance of the four sampling policies on the COSMIC dataset assuming inﬁnite confounded data. 275 unique (cancer, mutation, mutation) combinations were extracted. Left: averaged over 275 instances, and each averaged over 10,000 replications. Middle and Right: error comparison at 45 deconfounded samples.

ensuring the accuracy of replicating RNA) are found to increase the likelihood of both other mutations and certain cancer types (Rayner et al., 2016). The setting where the treatment mutation and cancer outcome are observed and the confounding mutation is unobserved is plausible because it is common that the majority of patients only have a subset of genes sequenced (e.g. from a commercial panel). The top 6 most commonly mutated genes were selected as treatment and confounder candidates. For each combination of a cancer type and two of these genes, we removed patients for whom these genes was not sequenced, and kept all pairs that had at least 40 patients in each of the four treatment-outcome groups (to ensure our deconfounding policies would have enough samples to deconfound). This procedure gave us 275 unique combinations of a cancer (outcome), mutation (treatment), and another mutation (confounder). Since on average, each {cancer, mutation, mutation} tuple contains around 125,883 patients, we took the estimated empirical distribution as the data-generating distribution and applied the ATE formula described in Section 3 to obtain the true ATE. To model the unobserved confounder, we hid the confounding mutation parameter, only revealing it to a sampling policy when it requested a deconfounded sample. We compared the use of deconfounded data along with the incorporation of confounded data under the three sampling selection polices: NSP, USP, and OWSP.
Results Figure 5 (left) was generated with these 275 instances each repeated for 10,000 replications. The absolute error is measured as a function of the number of deconfounded samples in step sizes of 15. First, similar to Figure 2, we observe that incorporating confounded data reduces both the absolute estimation error and the variance of the estimator by a large margin. Note the improvement of OWSP over NSP is larger in this case

as compared to that seen in Figure 2. Furthermore, when the number of deconfounded samples is small, OWSP outperforms USP. Note that Figure 5 (left) does not start with 0 because absent any deconfounded data, the estimated ATE is the same for all sampling policies. In Figure 5 (middle, right), we ﬁx the number of deconfounded samples to be 45 and compare the performance of OWSP against that of NSP and USP, respectively. Both ﬁgures contain the 275 instances in the left ﬁgure, averaged over 10,000 replications. We observe that under this setup, OWSP dominates NSP in all instances, and outperforms USP in the majority of instances.
5 Conclusion
We propose the problem of causal inference with selectively deconfounded data. This problem is particularly motivated by the scenarios where interventions on treatment is not available. We theoretically analyze the upper bounds and lower bounds on the amount of deconfounded data required under each sample selection policy. In addition, we theoretically demonstrate that the best-case gain of our proposed policy OWSP is unbounded when compared with NSP while the worst-case relative performance is bounded. We point to several promising directions for potential future research. First, we are currently extending our analysis to the adaptive case using ideas from active learning and combinatorial optimization. Second, we plan to extend our results to more general causal problems, including linear and semi-parametric causal models. Finally, we may extend the idea of selective revelation of information beyond confounders to incorporate mediators and proxies.

Causal Inference with Selectively Deconfounded Data

Acknowledgments
We wish to thank Sivaraman Balakrishnan and Uri Shalit for their valuable feedback. We would also like to thank Amazon AI, Facebook, Salesforce, the NSF, UPMC, the PwC Center, and DARPA for their support of our research.
References
Ahmed M Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment eﬀects using multi-task gaussian processes. In Advances in Neural Information Processing Systems, pages 3424–3432, 2017.
Elias Bareinboim and Judea Pearl. A general algorithm for deciding transportability of experimental results. Journal of Causal Inference, 1(1):107–134, 2013.
Stephen Bates, Matteo Sesia, Chiara Sabatti, and Emmanuel Candes. Causal inference in genetic trio studies. arXiv preprint arXiv:2002.09644, 2020.
Cosmic. Cosmic - catalogue of somatic mutations in cancer, Sep 2019. URL https://cancer.sanger. ac.uk/.
Rajeev H Dehejia and Sadek Wahba. Propensity scorematching methods for nonexperimental causal studies. Review of Economics and Statistics, 84(1):151– 161, 2002.
Erin Hartman, Richard Grieve, Roland Ramsahai, and Jasjeet S Sekhon. From sample average treatment eﬀect to population average treatment eﬀect on the treated: combining experimental with observational studies to estimate population treatment eﬀects. Journal of the Royal Statistical Society: Series A (Statistics in Society), 178(3):757–778, 2015.
Keisuke Hirano, Guido W Imbens, and Geert Ridder. Eﬃcient estimation of average treatment eﬀects using the estimated propensity score. Econometrica, 71(4): 1161–1189, 2003.
Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 81(396): 945–960, 1986.
Nathan Kallus, Aahlad Manas Puli, and Uri Shalit. Removing hidden confounding by experimental grounding. In Advances in Neural Information Processing Systems, pages 10888–10897, 2018.
Alfred G Knudson. Two genetic hits (more or less) to cancer. Nature Reviews Cancer, 1(2):157–162, 2001.
Manabu Kuroki and Judea Pearl. Measurement bias and eﬀect restoration in causal inference. Biometrika, 101(2):423–437, 2014.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal

eﬀect inference with deep latent-variable models. In Advances in Neural Information Processing Systems, pages 6446–6456, 2017.
Daniel F McCaﬀrey, Greg Ridgeway, and Andrew R Morral. Propensity score estimation with boosted regression for evaluating causal eﬀects in observational studies. Psychological Methods, 9(4):403, 2004.
Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. Identifying causal eﬀects with proxy variables of an unmeasured confounder. Biometrika, 105(4):987–993, 2018.
Jersey Neyman. Sur les applications de la théorie des probabilités aux experiences agricoles: Essai des principes. Roczniki Nauk Rolniczych, 10:1–51, 1923.
Judea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669–688, 1995.
Judea Pearl. Causality: models, reasoning and inference, volume 29. Springer, 2000.
Emily Rayner, Inge C van Gool, Claire Palles, Stephen E Kearsey, Tjalling Bosse, Ian Tomlinson, and David N Church. A panoply of errors: polymerase proofreading domain mutations in cancer. Nature Reviews Cancer, 2016.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal eﬀects. Biometrika, 70(1):41–55, 1983.
Donald B Rubin. Estimating causal eﬀects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66(5):688, 1974.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment eﬀect: generalization bounds and algorithms. In International Conference on Machine Learning, pages 3076–3085, 2017.
Xu Shi, Wang Miao, Jennifer C Nelson, and Eric J Tchetgen Tchetgen. Multiply robust causal inference with double-negative control adjustment for categorical unmeasured confounding. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2):521–540, 2020.
Elizabeth A Stuart, Stephen R Cole, Catherine P Bradshaw, and Philip J Leaf. The use of propensity scores to assess the generalizability of results from randomized trials. Journal of the Royal Statistical Society: Series A (Statistics in Society), 174(2):369–386, 2011.
John G Tate, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M Beare, Nidhi Bindal, Harry Boutselakis, Charlotte G Cole, Celestino Creatore, Elisabeth Dawson, et al. Cosmic: the catalogue of somatic mutations in cancer. Nucleic Acids Research, 47(D1): D941–D947, 2019.

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur
Mark J Van der Laan and Sherri Rose. Targeted learning: causal inference for observational and experimental data. Springer Science & Business Media, 2011.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment eﬀects using random forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.

Causal Inference with Selectively Deconfounded Data

A The Generalization of Our Models

A.1 Multiple Confounders

In this section, we show that because we do not impose any independence assumption on the set of confounder, revealing the values of all confounders oﬀers maximal information on the joint distribution of the confounders. In particular, we will illustrate through the case where we have two binary confounders. The extension to multiple categorical confounders is straight forward.
In the case where we have two binary confounders Z1 and Z2, we can express the ATE as follows:

ATE =

PY |T,Z1,Z2 (1|1, z1, z2) − PY |T,Z1,Z2 (1|0, z1, z2) PZ1,Z2 (z1, z2).

z1 ,z2

With an inﬁnite amount of confounded data, we are provided with the joint distribution PY,T (y, t). Thus, it remains to estimate the conditional distributions PZ1,Z2|Y,T . In our paper, we consider only the non-adaptive policies, i.e., the number of samples to deconfound in each group (y, t) is ﬁxed a priori. In the case where the costs
of revealing the values of Z1 and Z2 are the same and we do not have any prior knowledge on the distributions of Z1 and Z2, the variables Z1 and Z2 becomes exchangeable. In the case where the sample selection policies are completely non-adaptive (which is the case that we consider in this paper), by the symmetry of the variables Z1 and Z2, we have that sampling from the joint distribution of Z1 and Z2 yields the maximum expected information on the value of the ATE. (Note that if the confounders take categorical values of diﬀerent sizes and we allow
adaptive policies, then we might be able to reduce the total cost of deconfounding to estimate the ATE to within
a desired accuracy level.)

A.2 Pretreatment Covariates

In the case where we have known pretreatment covariates X, our model can be applied in estimating the individual treatment eﬀect where we make the common ignorability assumption on the pretreatment covariates X and the confounder Z: given pretreatment covariates X and the confounder Z, the values of outcome variable, Y = 0 and Y = 1, are independent of treatment assignment. In this case, the distributions PY,T (y, t) and PX (x) are known and the Individual Treatment Eﬀect (ITE):

ITE =

PY |T,Z,X (1|1, z, x) − PY |T,Z,X (1|0, z, x) PZ,X (z, x)

z,x

=

PY |T,Z,X (1|1, z, x) − PY |T,Z,X (1|0, z, x) PZ|X (z|x)PX (x).

(3)

z,x

Note that in Equation (3) the only distributions we need to estimate are the conditional distributions PZ|Y,T,X . The values of PY |T,Z,X and PZ|X can be calculated from PZ|Y,T,X by ﬁrst conditioning the confounded distributions PY,T on the values of the pretreatment covariates X, i.e., we ﬁrst subsample all confounded (outcome, treatment) pairs for a ﬁxed value of X, X = x, and then within each subsample, estimate the conditional distributions
PZ|Y,T,X by applying our methods. To obtain ITE, we weight the estimates we obtain from all subsamples by PX (x).

B Proofs

B.1 Review of Classical Results in Concentration Inequalities

Before embarking on our proofs, we state some classic results that we will use frequently. The following concentration inequalities are part of a family of results collectively referred to as Hoeﬀding’s inequality (e.g., see Vershynin (2018)).

Lemma 1 (Hoeﬀding’s Lemma). Let X be any real-valued random variable with expected value E[X] = 0, such that a ≤ X ≤ b almost surely. Then, for all λ ∈ R, E [exp(λX)] ≤ exp λ2(b8−a)2 .

Theorem 7 (Hoeﬀding’s inequality for general bounded r.v.s). Let X1, ..., XN be independent random variables

such that Xi ∈ [mi, Mi], ∀i. Then, for t > 0, we have P

Ni=1 (Xi − E[Xi]) ≥ t ≤ 2 exp − N i=1(M 2t2i−mi)2 .

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

To begin, recall the notation introduced in Section 3: we model the binary-valued treatment, the binary-valued outcome, and the categorical confounder as the random variables T ∈ {0, 1}, Y ∈ {0, 1}, and Z ∈ {1, . . . , k}, respectively. The underlying joint distribution of these three random variables is represented as PY,T,Z (·, ·, ·). To save on space for terms that are used frequently, we deﬁne the following shorthand notation:
pzyt = PY,T,Z (y, t, z), ayt = PY,T (y, t), qyzt = PZ|Y,T (z|y, t).
These terms appear frequently because, to estimate the entire joint distribution on Y, T, Z (the pzyt’s), it suﬃces to estimate the joint distribution on Y, T (the ayt’s), along with the conditional distribution of Z on Y, T (the qyzt’s):
pzyt = aytqyzt.
Finally, let pˆzyt, aˆzyt, and qˆyzt be the empirical estimates of pzyt, azyt, and qyzt, respectively, using the MLE.

B.2 Proof of Theorem 1

Theorem 1. (Upper Bound) Using deconfounded data alone, P ATE − ATE ≥ deconfounded sample size m is at least

mbase := max C
t,z

−2

pzyt
y

1

= max

C.

t,z PT,Z (t, z)2

< δ is satisﬁed if the

Proof of Theorem 1. This proof proceeds as follows: ﬁrst, we prove a suﬃcient (deterministic) condition, on the errors of our estimates of pzyt’s, under which |ATE − ATE| is small. Second, we show that the errors of our estimates of pzyt’s are indeed small with high probability.

Step 1: First, we can write the ATE in terms of the pzyt’s as follows:

ATE =
z





pz11

pz10

PY |T,Z (1|1, z) − PY |T,Z (1|0, z) PZ (z) = 
z

pz −
y1

 pzy0 

y

y


pzyt  .
y,t

In order for the ATE to be well-deﬁned, we assume |ATE − ATE|:

y pzyt ∈ (0, 1) for all t, z throughout. We can then decompose











|ATE − ATE| =

pˆz11

pˆz10

 
z

pˆz −
y1

 pˆzy0 

y

y

pˆzyt
y,t

pz11

− 

pz −

y1

y

pz10 
pzy0 
y

pzyt 
y,t









pˆz11

pˆz10

z

pz11

pz10

z

≤ 
z

pˆz −
y1

 pˆzy0 

pˆyt − 
y,t

pz −
y1

 pz 

pyt .

y0

y,t

y

y

y

y

Thus, in order to upper bound ATE − ATE by some , it suﬃces to show that









 pˆz11 − pˆz10 

z

pz11

pz10

pˆ − 

−



pz ≤ , ∀z.

(4)

 y pˆzy1 y pˆzy0  y,t yt  y pzy1 y pzy0  y,t yt k

Causal Inference with Selectively Deconfounded Data

Step 2: To bound the above terms, we ﬁrst derive Lemma 2 for bounding the error of the product of two estimates in terms of their two individual errors:
Lemma 2. For any u, uˆ ∈ [−1, 1], and v, vˆ ∈ [0, 1], suppose there exists , θ ∈ (0, 1) such that all of the following conditions hold:

1. |u − uˆ| ≤ (1 − θ) 2. |v − vˆ| ≤ θ 3. u + ≤ 1 4. v + ≤ 1 5. ≤ min(u, v)

Then, |uv − uˆvˆ| ≤ .

Proof of Lemma 2. Since |u − uˆ| ≤ (1 − θ) , we have uˆ ∈ [u − (1 − θ) , u + (1 − θ) ], and similarly, from |v − vˆ| ≤ θ , we have vˆ ∈ [v − θ , v + θ ]. Thus,

|uv − uˆvˆ| ≤ max (|uv − (u + (1 − θ) )(v + θ )|, |uv − (u − (1 − θ) )(v − θ )|) (because v, vˆ ≥ 0)

= max( θu + (1 − θ)v + (1 − θ)θ 2 , θu + (1 − θ)v − (1 − θ)θ 2 )

= θu + (1 − θ)v + (1 − θ)θ 2

(because (1 − θ)θ 2 > 0)

≤ |θ(u + ) + (1 − θ)v |

(because θ 2 > (1 − θ)θ 2)

≤

(because u + ∈ [−1, 1], and v ≤ 1).

We can apply Lemma 2 directly to the terms in (4) by setting

uz = uˆz =

pz11 pz −
y1 y
pˆz11 pˆz −
y1 y

pz10 pz ,
y0 y
pˆz10 pˆz ,
y0 y

vz = pzyt,
y,t

vˆz = pˆzyt,
y,t

and noting that uz, uˆz ∈ [−1, 1], and vz, vˆz ∈ [0, 1]. Lemma 2 implies that the upper bound in (4) holds if, for

some θ ∈ (0, 1), we have

θ |vz − vˆz| < k

1−θ and |uz − uˆz| < k .

While we can apply standard concentration results to the |vz − vˆz| terms, the |uz − uˆz| terms will need to be further decomposed:

|uz − uˆz| =

pz11 pz −
y1 y

pz10 pz −
y0 y

pˆz11 pˆz +
y1 y

pˆz10 pˆzy0
y

pz11

pˆz11

pz10

pˆz10

≤ pz − pˆz + pz − pˆz .

y1

y1

y0

y0

y

y

y

y

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

It will suﬃce to show that for each t and z,

pz1t

pˆz1t

1−θ

pz − pˆz < 2k .

(5)

yt

yt

y

y

Step 3: To bound these terms, we derive Lemma 3. Recall that pz1t + pz0t, pˆz1t + pˆz0t ∈ (0, 1). Lemma 3. For any w + s, wˆ + sˆ ∈ (0, 1), if |w + s − wˆ − sˆ| ≤ (w + s) and |w − wˆ| ≤ (w + s) , then

w

wˆ

−

≤2 .

w + s wˆ + sˆ

Proof of Lemma 3. First, since |w + s − wˆ − sˆ| ≤ (w + s) , we have that

w+s

w+s

−1 ≤

,

wˆ + sˆ

wˆ + sˆ

or equivalently,

w+s w+s

w+s

1−

≤

≤1+

.

wˆ + sˆ wˆ + sˆ

wˆ + sˆ

We can apply this inequality and rearrange terms as follows to conclude the proof:

w

wˆ

−

w + s wˆ + sˆ

1

w+s

=

w − wˆ

w+s

wˆ + sˆ

1

w+s

≤

max w − wˆ 1 −

w+s

wˆ + sˆ

w+s , w − wˆ 1 +
wˆ + sˆ

1

w+s

w+s

=

max w − wˆ +

wˆ , w − wˆ −

wˆ

w+s

wˆ + sˆ

wˆ + sˆ

= max

w − wˆ wˆ

w − wˆ wˆ

+

,

−

w + s wˆ + sˆ w + s wˆ + sˆ

w − wˆ

wˆ

≤

+

w + s wˆ + sˆ

w+s

wˆ

≤

+

w+s

wˆ + sˆ

≤2 .

The second to last inequality follows from the assumption that |w − wˆ| ≤ (w + s) .

Lemma 3 implies that (5) is satisﬁed if

( |pz1t − pˆz1t| <

y pzyt)(1 − θ) 4k

z z z z ( y pzyt)(1 − θ)

and |p1t + p0t − pˆ1t − pˆ0t| <

4k

.

Step 4: We’ve shown above that |ATE − ATE| ≤ is satisﬁed when

θ |vz − vˆz| < k ,

( |pz1t − pˆz1t| <

y pzyt)(1 − θ) ,
4k

and

z z z z ( y pzyt)(1 − θ)

|p1t + p0t − pˆ1t − pˆ0t| <

4k

, ∀t, z.

Note that if ∀t, |pz1t + pz0t − pˆz1t − pˆz0t| =

y pzyt −

pˆz < ( y pzyt)(1−θ) then

y yt

4k

|vz − vˆz| =

pzyt − pˆzyt ≤

y,t

y,t

t

pzyt −

( pˆzyt <

y

y

y,t pzyt)(1 − θ) (1 − θ)

≤

.

4k

4k

Causal Inference with Selectively Deconfounded Data

Thus, to remove the ﬁrst constraint |vz − vˆz| <

θ k

, we set

θ (1 − θ)

=

,

k

4k

and obtain θ = 15 .

Step 5: To summarize so far, Lemmas 2 and 3 allow us to upper bound the error of our estimated ATE in terms of upper bounds on the error of our estimates of its constituent terms:

z

z

y pzyt

z

z

z

z

y pzyt

P |ATE − ATE| < ≥ P

|p1t − pˆ1t| < 5k

|p1t + p0t − pˆ1t − pˆ0t| < 5k

,

t,z

t,z

or equivalently,

z

z

y pzyt

z

z

z

z

y pzyt

P |ATE − ATE| ≥ ≤ P

|p1t − pˆ1t| ≥ 5k

|p1t + p0t − pˆ1t − pˆ0t| ≥ 5k

.

t,z

t,z

Applying a union bound, we have

z

z

y pzyt

z

z

z

z

y pzyt

P |ATE − ATE| ≥ ≤ P |p1t − pˆ1t| ≥ 5k

+ P |p1t + p0t − pˆ1t − pˆ0t| ≥ 5k

. (6)

t,z

Step 6: Finally, we can apply Hoeﬀding’s inequality (Theorem 7) to obtain the upper bound for the inequality above. Let Xyzt be the random variable that maps the event (Y = y, T = t, Z = z) → {0, 1}. Then, Xyzt is a Bernoulli random variable with parameter pzyt. Let m denote the total number of deconfounded samples that we have. Since pˆyt is estimated through the MLE, we have pˆzyt = m i=m1 Xyzt . Applying Theorem 7, we obtain:



2

m i=1

Xyzt

z

y pzyt

y pzyt 2

P

m − pyt ≥ 5k

≤ 2 exp −2m 25k2  , and

(7)



2

m i=1

X1zt

+

X0zt

z

z

y pzyt

y pzyt 2

P

m

− p1t − p0t ≥ 5k

≤ 2 exp −2m 25k2  .

(8)

Combining (6), (7), and (8), we have

P |ATE − ATE| ≥

≤P
t,z

|pz1t − pˆz1t| ≥ 

≤ 4k max 2 exp −2m

t,z 



y pzyt 5k

+ P |pz1t + pz0t − pˆz1t − pˆz0t| ≥

2  y pzyt 2

25k2 



= 8k max exp −2m

t,z



2 y pzyt 2

25k2 

y pzyt 5k

≤ δ,

where the second line follows from the fact that, since t is binary, there are 4k terms in total. Solving the above equation, we conclude that P (|ATE − ATE| ≥ ) < δ is satisﬁed when the sample size m is at least

m ≥ 12.5k2 ln( 8δk ) max

2

t,z

1 2.
y pzyt

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

B.3 Proof of Proposition 1
Proposition 1. For every a, there exists some , δ such that for any ﬁxed number of deconfounded samples m, we can always construct a pair of q’s, say q1 and q2, such that no algorithm can distinguish these two conditional distributions with probability more than 1 − δ, and their corresponding ATE values are away: |ATEa(q1) − ATEa(q2)| ≥ .

Proof of Proposition 1. It suﬃces to show for the case where confounder takes binary value. The extension to categorical confounder is straightforward as illustrated in the proof of Theorem 6 in Appendix B.7. Let qyt = P (Z = 1|Y = y, T = t). To show that Proposition 1 is true, it is suﬃcient to show that there exist a positive constant c (that depends on a) such that for all ﬁxed a, there exists a pair of q and q such that ATEa(q) − ATEa(q ) > c, with q and q close in distribution. We proceed by construction. For ﬁxed a, consider the following q pairs: q = (q00, 0, q10, γ) and q = (q00, γ, q10, 0). Then, we have

ATEa(q) = (a00q00 + a10q10 + a11γ) + a11(1 − γ) (1 − a00q00 − a10q10 − a11γ)− a11(1 − γ) + a01

a10q10

(a00q00 + a10q10 + a11γ) −

a10(1 − q10)

(1 − a00q00 − a10q10 − a11γ),

a10q10 + a00q00

a10(1 − q10) + a00(1 − q00)

and similarly, we have

ATEa(q ) =

a11 (1 − a00q00 − a01γ − a10q10) − a10q10 (a00q00

a11 + a01(1 − γ)

a10q10 + a00q00

+ a01γ + a10q10) −

a10(1 − q10)

(1 − a00q00 − a01γ − a10q10).

a10(1 − q10) + a00(1 − q00)

In particular,

lim ATEa(q) − ATEa(q ) = a00q00 + a10q10 ≤ a00 + a10,

(9)

γ→0

where we can choose q00 and q10 to be 1.

On the other hand, we can show that the number of samples needed to distinguish q from q is at least Ω(1/γ):

since q and q are the same in two of the entries and symmetric on the rest two, to distinguish q and q

is to distinguish a Bernoulli random variable with parameter 0 (denoting this variable B0) from a Bernoulli

random variable with parameter γ (denoting this random variable Bγ). Let f be any estimator of the Bernoulli

random variable, and xi, ..., xm be the sequence of m observations. Then we have |EX∼Bm [f ] − EX∼Bm [f ]| ≤

0

γ

B0m − Bγm 1 ≤

2(ln

2)KL(B

m 0

Bγm) ≤ 2

(ln 2)γm, where the last inequality is because when given m samples,

KL(B0m

Bγm)

≤

(2γ

ln

2

+

(1

−

2γ)

ln

1−2γ 1−γ

)m

≤

2γm.

On

the

other

hand,

any

hypothesis

test

that

takes

n

samples

and distinguishes between H0 : X1, ..., Xn ∼ P0 and H1 : X1, ..., Xn ∼ P1 has probability of error lower bounded

by max(P0(1), P 1(0)) ≥ 14 e−nKL(P0 P1), where P0(1) indicates the probability that we identify class H0 while the true class is H1. Since P0(1) + P1(0) ≤ δ, by contradiction, we can show that m ∼ Ω(ln(δ−1)γ−1).

Note that this lower bound on m can be arbitrarily large by choosing γ to be suﬃciently small. However their ATE values stay constant away as observed in Equation (9). Thus, for every ﬁxed confounded distribution encoded by a and ﬁxed number of deconfounded samples m, we can always construct a pair of conditional distributions encoded by q and q such that their corresponding ATEs are constant away while the probability that we correctly identify the true conditional distribution from q and q is less than 1 − δ. In particular, = c = a00 + a10 in the above example. (Here, we implicitly assume that a00 + a10 is strictly greater than zero, i.e., a00 + a10 > 0.)

B.4 Proof of Theorem 2
Theorem 2. (Lower Bound) For any estimator and sample selection policy, the number of deconfounded samples m needed to achieve P ATE − ATE ≥ < δ is at least Ω( −2 log(δ−1)).

Proof of Theorem 2. Again, it suﬃces to show for the case where the confounder is binary. The extension to categorical confounder is straightforward as illustrated in the proof of Theorem 6 in Appendix B.7. Let

Causal Inference with Selectively Deconfounded Data

qyt = P (Z = 1|Y = y, T = t). We will proceed by construction. Consider q = (q00, q01, β, β + γ) and q = (q00, q01, β + γ, β), for some small γ. Then

ATEa(q) =

a11(β + γ) (a00q00 + a01q01 + a10β + a11(β + γ)) +

a11(1 − β − γ)

a11(β + γ) + a01q01

a11(1 − β − γ) + a01(1 − q01)

(1 − a00q00 − a01q01 − a10β − a11(β + γ)) −

a10β

(a00q00 + a01q01 + a10β + a11(β + γ))−

a10β + a00q00

a10(1 − β)

(1 − a00q00 − a01q01 − a10β − a11(β + γ)),

a10(1 − β) + a00(1 − q00)

and similarly, we have

ATEa(q ) =

a11β (a00q00 + a01q01 + a10(β + γ) + a11β) +

a11(1 − β)

(1 − a00q00−

a11β + a01q01

a11(1 − β) + a01(1 − q01)

a01q01 − a10(β + γ) − a11β) −

a10(β + γ) (a00q00 + a01q01 + a10(β + γ) + a11β)−

a10(β + γ) + a00q00

a10(1 − β − γ)

(1 − a00q00 − a01q01 − a10(β + γ) − a11β).

a10(1 − β − γ) + a00(1 − q00)

Ignoring the γ in the denominator, we have that

ATEa(q) − ATEa(q ) = ( a11 + a10 )(a00q00 + a01q01 + a10β + a11β)γ a11β + a01q01 a10β + a00q00
− ( a11 + a10 )(1 − a00q00 − a01q01 − a10β − a11β)γ a11(1 − β) + a01(1 − q01) a10(1 − β) + a00(1 − q00)

+ a211 − a11a10 βγ −

a211 − a11a10

(1 − β)γ

a11β + a01q01

a11(1 − β) + a01(1 − q01)

+ a210 − a11a10 βγ −

a210 − a11a10

(1 − β)γ

a10β + a00q00

a10(1 − β) + a00(1 − q00)

+ a211 γ2 +

a211 γ2 + a210 γ2 +

a210 γ2

a11β + a01q01

a11(1 − β) + a01(1 − q01)

a10β + a00q00

a10(1 − β) + a00(1 − q00)

(10)

Similar to the proof above, let B1 denote the Bernoulli random variable with parameter β, and let B2 denote the Bernoulli random variable with parameter β + γ. Then, given m deconfounded samples, we have KL(B1m B2m) ≤ mβ ln( β+β γ ) + m(1 − β) ln( 1−1−β−β γ ) ≤ m ln(1 + 1−βγ−γ ) ≤ m( 1−βγ−γ − 2(1−γβ2−γ)2 ). Thus, we have m ∼ Ω( ln(γδ2−1) ).
From Equation (10), we observe that = ATEa(q) − ATEa(q ) ∼ Ω(γ). Combining above, we have m ∼ Ω( ln(δ2−1) ).

B.5 Proof of Theorems 3 and 5

Theorem 3. (Upper Bound) When incorporating (inﬁnite) confounded data, P (|ATE − ATE| ≥ ) < δ is satisﬁed if the number of deconfounded samples m is at least

C y ayt

PT (t)

mnsp := max
t,z

z

2

=

max
t,z

PT,Z (t, z)2 C.

(2)

y aytqyt

Theorem 5. (Upper Bound) Under the uniform selection policy, with (inﬁnite) confounded data incorporated, P (|ATE − ATE| ≥ ) < δ is satisﬁed if µusp is at least

musp := max
t,z

C y 4a2yt

4

2 = max

y aytqyzt

t,z

y PY,T (y, t)2 PT,Z (t, z)2 C.

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

Similarly, for the outcome-weighted selection policy:

2

2C y ayt

2

mowsp := max
t,z

2

= max t,z P

(z|t)2 C.

y aytqyzt

Z |T

Proof of Theorems 3 and 5. In these theorems, we derive the concentration of the ATE assuming inﬁnite confounded data, and parametrize pzyt by pzyt = aytqyzt. Since under inﬁnite confounded data, ayt’s are known, and thus we only need to estimate the qyzt’s. The key diﬀerence between Theorem 5 and Theorem 1 is that now we deﬁne the random variables Xyzt to map the event (Z = z|Y = y, T = t) to {0, 1}. Thus, Xyzt is distributed according to Bernoulli(qyzt). Thus, to decompose |a1tq1zt + a0tq0zt − a1tqˆ1zt − a0tqˆ0zt|, we ﬁrst show the following lemma:

Lemma 4. Let X1, ..., Xx1m and Y1, ..., Yx2m be independent random variables in [0,1]. Then for any t > 0, we

have





x1 m i=1

Xi

−

E

[Xi]

x2 m j=1

Yj

−

E

[Yj ]

2m(αt + βk)2

Pα

+β

≥ αt + βk ≤ 2 exp −

.

x1m

x2m

+ α2

β2

x1

x2

Proof of Lemma 4. First observe that

P α xi=1m1 Xi − E [Xi] + β jx=2m1 Yj − E [Yj ] ≥ αt + βk

x1m

x2m

α x1m

β x2m

=P x1

(Xi − E [Xi]) + x2

(Yj − E [Yj]) ≥ mαt + mβk .

i=1

j=1

Now, let Zi = xα1 Xi if i ∈ [1, x1m], and Zi = xβ2 Yi if i ∈ [x1m + 1, (x1 + x2)m]. Then applying Theorem 7, we have





(x1 +x2 )m

P

(Zi − E[Zi]) ≥ mαt + mβk ≤ 2 exp −

i=1

2m2(αt + βk)2 i(=x11+x2)m(Mi − mi)2

2m(αt + βk)2

= 2 exp − α2 + β2

.

x1

x2

As deﬁned in Section 3, let xyt denote the percentage data we sample from the group yt.

Recall that from the proof of Theorem 1, we have

P |ATE − ATE| ≥

≤P
t,z

|pz1t − pˆz1t| ≥

y pzyt 5k

+ P |pz1t + pz0t − pˆz1t − pˆz0t| ≥

y pzyt 5k

=P
t,z

|a1tq1zt − a1tqˆ1zt| ≥

y aytqyzt 5k

+ P |a1tq1zt + a0tq0zt − a1tqˆ1zt − a0tqˆ0zt| ≥

y aytqyzt 5k

=P
t,z

|q1zt − qˆ1zt| ≥

y aytqyzt 5ka1t

+ P |a1tq1zt + a0tq0zt − a1tqˆ1zt − a0tqˆ0zt| ≥

y aytqyzt 5k



≤ 4k max 2 exp −2x1tm

t,z 



2



y aytqyzt 2

25k2a2

 , 2 exp −2m





1t

2
y aytqyzt

25k2

a2yt y xyt


2
 

≤ δ,

where the second to last line follows from applying Lemma 4 to the second half of the line above it.

Causal Inference with Selectively Deconfounded Data

Solving the equation above, we have



m ≥ 12.5k2 ln( 8δk ) max 

2

t,z 

a21t/x1t 2,
y aytqyzt



y a2yt/xyt

12.5k2 ln( 8δk )

2

 

=

max

2

t,z

y aytqyzt

The last equality is because a22/x2, a21/x1 > 0. Under NSP, xyt = ayt. Thus, we have

12.5k2 ln( 8k )

mnsp :=

δ max

2

t,z

y ayt 2 . y aytqyzt

Similarly, under USP, xyt = 14 , and we have

12.5k2 ln( 8k )

musp :=

δ max

2

t,z

y 4a2yt 2.
y aytqyzt

Lastly, under OWSP, xyt = 2 ayytayt , and we have

12.5k2 ln( 8k )

mowsp :=

δ max

2

t,z

2( y ayt)2 2.
y aytqyzt

y a2yt/xyt 2.
y aytqyzt

B.6 Proof of Theorem 4
Theorem 4. For any ﬁxed ∈ [0, 0.5 − 2β(1 − β)] and any ﬁxed δ < 1, there exist distributions where µowsp/µnsp is arbitrarily close to zero. In addition, for any estimator and every distribution, µowsp/µnsp ≤ 2.

Proof of Theorem 4. We proceed by construction. For simplicity, we illustrate the correctness of Theorem 4 for binary confounders. The extension to the multi-valued confounder is straightforward and will be demonstrated in the proof of Theorem 6.

Consider the following example: a01 = a10 = a11 = η, a00 = 1 − 3η, and consider the following pair of q’s: q = (β, β, β, cβ) and q = (β, β, β, β), where c ≤ 1−ββ is some constant. Here, one of the q and q represents the true ATE, and the other represents the estimated ATE using the best estimator. Without loss of generality, we

assume that we have already identiﬁed three components of the true conditional distribution. (In general, we can

always construct an instance by modifying the values of a01 and a10 so that the majority error is induced by

estimation

error

on

q11.)

Then,

we

have

ATEa(q)

=

cβ 1+c

+

(1−cβ)(1−β) 2−cβ−β

−

1−η2η ,

and

ATEa(q

)

=

1 2

−

1−η2η .

Thus,

∆ATE := |ATEa(q) − ATEa(q )|:

1 cβ (1 − cβ)(1 − β)

∆ATE = −

−

.

2 c + 1 2 − cβ − β

Note that when c = 1−ββ , ∆ATE = 0.5 − 2β(1 − β) ≈ 0.5. Thus, for any ∈ [0, 0.5 − 2β(1 − β)], there exists some c such that = ∆ATE. Then, for any δ, let µ denote the minimum expected number of samples that we need to

distinguish q from q under the best estimator. Then under NSP, the minimum number of samples that we need

under the best estimator equals to µnsp := µ/η, and under OWSP, the minimum number of samples that we need under the best estimator equals to µoswp = 4µ. (Note that xyt = ( 2(11−−32ηη) , 14 , 2(1−η 2η) , 14 ) under OWSP in this

example.) Thus, µowsp/µnsp = 4η. Since in this example, η is at most 1/4, µowsp/µnsp ≤ 1 and can be arbitrarily

close to 0 as η → 0. (Intuitively, the ﬁrst statement is true because when t a0t

t a1t and a00 ≈ a01, it is

equally important to estimate q0zt’s and q1zt’s according to the ATE expression. However, under this setup, the

number of samples allocated to groups (0, t)’s decreases as a0,t’s approach to 0 under NSP, while under OWSP, half of the deconfounded samples are always dedicated to estimate the q0zt’s.)

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur
Next, we show the last sentence in Theorem 4 is true. For any ﬁxed , δ < 1, let µnsp be the minimum expected number of samples needed to achieve P (|ATE − ATE| ≥ ) < δ under natural selection policy for the best estimator, then when wowsp := 2µnsp maxt y ayt also achieves P (|ATE − ATE| ≥ ) < δ under the outcomeweighted selection policy. The reason is that when using wowsp number of deconfounded samples, the number of deconfounded data allocated to each yt group is at least as much as those under the natural selection policy. Thus, we have µowsp ≤ wowsp ≤ 2µnsp, where the last inequality is because maxt y ayt < 1.

B.7 Proof of Theorem 6

Theorem 6. (Lower Bound) For every a, there exists a q such that µnsp is at least

C1

a1t( y ayt¯)2 a0t( y ayt¯)2

wnsp := β2 mtax ( y ayt)2 , ( y ayt)2 ;

similarly for uniform selection policy:

C1

a21t( y ayt¯)2 a20t( y ayt¯)2

wusp := β2 mtax 4 ( y ayt)2 , 4 ( y ayt)2 ;

similarly for outcome-weighted sample selection policy:

C1

a1t( y ayt¯)2 a0t( y ayt¯)2

wowsp := β2 mtax 2 y ayt , 2 y ayt ,

where t¯ = 1 − t and C1 ∝ (kβ − 1)2 ln(δ−1) −2.

Proof. Consider q = (q0z0, q0z1, q1z0, q1z1) where q011 = β, q111 = β + γ, and q1z1 = q0z1 − γ/(k − 1) for z = 2, ..., k,

with z q0z1 = z q1z1 = 1. We assume that q1z1, q0z1 ∈ [β, 1 − β] for some suitable β and γ for all values of Z.

Similarly, we consider the q

where the entries of q0z1 and q1z1 are ﬂipped, i.e., q

=

(

q

z 00

,

q1z1

,

q1z0

,

q0z1

)

,

for

some

small γ, where the qyzt’s are deﬁned above. Then,

ATEa(q) =
z

a11q1z1

ay1qz −

y

y1

a10q1z0 y ay0qyz0

aytqyzt
y,t

a11(β + γ)

1

1

a10q110

1

1

= a11(β + γ) + a01β (a00q00 + a01β + a10q10 + a11(β + γ)) − a10q1 + a00q1 (a00q00 + a01β + a10q10+

10

00

k
a11(β + γ)) +

a11

q0z1

−

γ k−1

a00q0z0 + a01q0z1 + a10q1z0 + a11 q0z1 − γ −

z=2 a11

q0z1

−

γ k−1

+ a01q0z1

k−1

k

a10q1z0

z

z

z

z

γ

a10qz + a00qz a00q00 + a01q01 + a10q10 + a11 q01 − k − 1 ,

z=2

10

00

and similarly, we have

a11β

1

1

a10q110

1

ATEa(q ) = a11β + a01(β + γ) (a00q00 + a01(β + γ) + a10q10 + a11β) − a10q1 + a00q1 (a00q00 + a01(β + γ)+

10

00

1

k

a11q0z1

a10q10 + a11β) +

z=2 a11q0z1 + a01

q0z1

−

γ k−1

a00q0z0 + a01 q0z1 − k −γ 1 + a10q1z0 + a11q0z1 −

k

a10q1z0

z=2 a10q1z0 + a00q0z0

a00q0z0 + a01

q0z1 − k −γ 1

+ a10q1z0 + a11q0z1

Causal Inference with Selectively Deconfounded Data

Ignoring the γ in the denominator, we have that

a11

1

1

a10q110(a01 − a11)

ATEa(q) − ATEa(q ) ≈ a11β + a01β (a00q00 + a01β + a10q10 + a11β)γ + a10q1 + a00q1 γ

10

00

k

a11/k − 1

z

z

z

−

a11qz + a01qz (a00q00 + a10q10 + (a01 + a11)q10)

z=2

01

01

k a10q1z0(a01 − a11) 1

γ−

a10qz + a00qz

γ k−1

z=2

10

00

a211

k 2

a211

γ2

+

γ+

a11β + a01β

a11qz + a01qz (k − 1)2

z=2

01

01

a11

1

1

a10q110(a01 − a11)

a11 k a00q0z0 + a10q1z0

= a11β + a01β (a00q00 + a10q10)γ + a10q1 + a00q1

γ− k−1

a11qz + a01qz γ

10

00

z=2

01

01

1 k a10q1z0(a01 − a11)

a211

k 2

a211

γ2

− k−1

a10qz + a00qz

γ+

γ+

a11β + a01β

a11qz + a01qz (k − 1)2

z=2

10

00

z=2

01

01

(11)

Since the second order terms in γ is dominated by the ﬁrst order terms in γ, thus to ﬁnd the highest lower bound for sample complexity in this instance is to ﬁnd the largest coeﬃcient in front of γ.

Assuming that β k and kβ < 1, then the maximum of Equation (11) is achieved when q0z0 = q1z0 = β , q010 = q110 = 1 − kβ, and q0z1 = (1 − β)/(k − 1), and the coeﬃcient in front of γ is

a11 (a00 + a10)( 1 − k − β ) ≈ a11 (a00 + a10) 1 − k .

a11 + a01

β 1 − β a11 + a01

β

Similar to the proof of Theorem 2, we have m ∼ Ω( ln(γδ2−1) ). From Equation (10), we observe that = ATEa(q) −

ATEa(q ) ∼ Ω(γ). Combining above, we have m ∼ Ω( ln(δ2−1) ). In the case above, thus, the number of deconfounded samples needed is approximately

≈ a11a+11a01 (a00 + a10) β1 γ,

m ∝ ln(δ−1)a211(a00 + a10)2 2(a11 + a01)2

1

2

−k .

β

Let C1 ∝ (kβ − 1)2 ln(δ−1) −2. Then m ∼ Ω Cβ21 a(21a11(1a+00a+01a)2120) .

If we ﬂip the values of qz and qz with the values of qz and qz in both q and q , then we have m ∼ C1 a210(a01+a11)2 .

01

11

00

10

β2 (a10+a00)2

Note that this is because that the estimation error on ATE and 1 − ATE is symmetric. In addition, under natural

selection policy, we need at least am11 samples; uniform selection policy, we need at least 4m deconfounded samples; under outcome-weighted selection policy, we need at least 2 a11a+11a01 m deconfounded samples. Combining all of the above, we obtained Theorem 6.

C Finite Confounded Data

In this case, deconfounding reveals the value of Z for one (initially confounded) sample, and thus we gain no
additional information about PY,T . Thus, these n confounded data provide us with an estimate of the confounded distribution, PˆY,T (y, t), which we denote aˆyt, and thus provide us an estimated OWSP. Similarly, we estimate aˆyt using the MLE from the confounded data. To check the robustness of OWSP, we extend our analysis to handle
ﬁnite confounded data. With xyt deﬁned as in Section 3.2, we can derive a theorem analogous to Theorems 1-5:

Theorem 8. (Upper Bound) Given n confounded and m deconfounded samples, with n ≥ m, P (|ATE − ATE| ≥ ) ≤ δ is satisﬁed when

min
y,t,z

2
y aytqyzt

+ 1
xyt m

(qyzt )2 n





PT,Z (t, z)2

= min 

y,t,z

1

(qz )2  ≥ 4C. yt

+n

xyt m

(12)

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

The proof of Theorem 8 (Appendix C.1) requires a bound we derive (Appendix, Lemma 5) for the product of two independent random variables. A few results follow from Theorem 8. First, a quick calculation shows that when m is held constant, P (|ATE − ATE| ≥ ) remains positive as n → ∞. This means that for a certain combinations
of , δ, n, there does not necessarily exist a suﬃciently large m s.t. P (|ATE − ATE| ≥ ) ≤ δ can be satisﬁed. However, when there exists such an m, then

PT,Z (t, z)2

(qz )2 −1
yt

m ≥ max x−yt1

−

.

y,t,z

4C

n

Although Theorem 8 does not recover Theorems 3 and 5 exactly when n → ∞,1 it provides us with insights into relative performance of our sampling policies. Theorem 8 implies that when n (qyzt)2xytm ∀y, t, the majority of the estimation error comes from not deconfounding enough data. This is because when the number of confounded
data that we have is more than Ω(m), the error on the ATE in Equation (12) is dominated by fact that we have
not deconfounded enough data. To put it another way, for a given m, having n = Ω(m) confounded samples is
suﬃcient.

C.1 Proof of Theorem 8
Theorem 8. (Upper Bound) Given n confounded and m deconfounded samples, with n ≥ m, P (|ATE − ATE| ≥ ) ≤ δ is satisﬁed when

min
y,t,z

2
y aytqyzt

+ 1
xyt m

(qyzt )2 n





PT,Z (t, z)2

= min 

y,t,z

1

(qz )2  ≥ 4C. yt

+n

xyt m

(12)

Proof of Theorem 8. In this theorem, we derive the concentration for the ATE under ﬁnite confounded data. The diﬀerence between Theorem 5 and Theorem 8 is that now we need to estimate ayt in addition to qyzt. Thus, to decompose |aytqyzt − aˆytqˆyzt|, we ﬁrst derive Lemma 5.

C.1.1 Lemma 5

Lemma 5 (Sample complexity for two independent r.v.s with two independent sampling processes). Let X1, ..., Xn

and Y1, ..., Ym be two sequences of Bernoulli random variables independently drawn from distribution p1 and p2,

n

m

respectively. Let SX = Xi, SY = Yi. Then,

i=1

i=1

−2t2

P SX SY − E [SX ] E [SY ] ≥ nmt ≤ 2 exp 1 p2 .

m+

2
n

1We could apply Lemma 2 (Appendix B) to obtain a bound that recovers Theorems 3 and 5 exactly as n → ∞. However, this method does not give us suﬃcient insights into the comparative performance of our sampling policies.

Causal Inference with Selectively Deconfounded Data

Proof of Lemma 5. The proof follows the proof of Hoeﬀding’s inequality:

P SX SY − E[SX ]E[SY ] ≥ nmt = P exp(aSX SY − aE[SX ]E[SY ])) ≥ exp(anmt)

(13)

≤ exp(−anmt)E [exp(aSX SY − aE[SX ]E[SY ]))] ,

(because of Markov’s inequality)

(14)

= exp(−anmt)E [exp(aSX (SY − E[SY ]) + aE[SY ](SX − E[SX ])]

≤ exp(−anmt)E [exp(a max(SX )(SY − E[SY ]) + aE[SY ](SX − E[SX ]))] (because SX ≥ 0)

(15)

= exp(−anmt)E [exp(an(SY − E[SY ]) + aE[SY ](SX − E[SX ]))]

|=

= exp (−anmt) E [exp (an(SY − E[SY ]))] E [exp(aE[SY ](SX − E[SX ]))] (becauseX Y )

(16)

mn

= exp(−anmt)

E [exp(an(Yi − E[Yi]))] E [exp(aE[SY ](Xj − E[Xj]))]

i=1 j=1

m

a2

n

a2

≤ exp(−anmt) exp n2

exp E[SY ]2

(17)

8

8

i=1

j=1

= exp −anmt + a2 mn2 + a2 nm2p2 (because the minimum is achieved at a =

4t )

(18)

8

8

2

n + mp22

2mnt2

2t2

≤ exp − n + mp2
2

= exp − 1 + p22

.

mn

Line (17) is because Yi − E[Yi] ∈ {−E[Yi], 1 − E[Yi]), and thus n(Yi − E(Yi)) ∈ [−nE[Yi], n(1 − E[Yi])]. Furthermore, E[SY ](Xi − E[Xi]) ∈ (−E[X]E[SY ], (1 − E[X])E[SY ]). Finally, applying Hoeﬀding’s Lemma (Lemma 1), we obtain line (17).

Now we are ready to prove Theorem 8.

C.1.2 Proof of Theorem 8

In this theorem, we assume that the number of confounded data is ﬁnite. Thus, instead of ayt, we have estimates
of them, namely aˆyt. Let nyt denote the number of samples in the confounded data such that (Y = y, T = t). Let mzyt be the number of samples in the deconfounded data such that (Y = y, T = t, Z = z). Furthermore, let n = y,t nyt, m = y,t,z mzyt. Then, under our setup, we estimate ayt and qyzt as follows:

aˆyt = nyt , and qˆz =

n

yt

mzyt mz .
z yt

Thus, following the proof of Theorem 1, we have

P |ATE − ATE| <

≥P

|pz1t − pˆz1t| <

t,z

y pzyt 5k

|pz1t + pz0t − pˆz1t − pˆz0t| <
t,z

y pzyt 5k

z

z

y aytqyzt

z

z

z

z

y aytqyzt

=P

|a1tq1t − aˆ1tqˆ1t| < 5k

|a1tq1t + a0tq0t − aˆ1tqˆ1t − aˆ0tqˆ0t| < 5k

.

t,z

t,z

Notice that |a1tq1zt + a0tq0zt − aˆ1tqˆ1zt − aˆ0tqˆ0zt| < y 5akytqyzt is satisﬁed when both

|a1tq1zt − aˆ1tqˆ1zt| <

y aytqyzt

z

z

10k , and |a0tq0t − aˆ0tqˆ0t| <

y aytqyzt .
10k

We have:

P |ATE − ATE| <

≥P

|a1tq1zt − aˆ1tqˆ1zt| <

t,z

y aytqyzt 10k

|a0tq0zt − aˆ0tqˆ0zt| <
t,z

z

z

y aytqyzt

=P

aytqyt − aˆytqˆyt < 10k

.

y,t,z

y aytqyzt 10k

Kyra Gan, Andrew A. Li, Zachary C. Lipton, Sridhar Tayur

Lemma 5 suggests that





z

z

2t2

P (|aytqyt − aˆytqˆyt| ≥ t) ≤ 2 exp − 1 + (qyzt)2  .

xyt m

n

Thus, applying a union bound and Lemma 5, we have

P |ATE − ATE| ≥

≤P
y,t,z

aytqyzt − aˆytqˆyzt <

y aytqyzt 10k



2



y aytqyzt 2

≤ 8k max exp −2

y,t,z

 (1

 + (qyzt)2 )100k2 

xyt m

n

≤ δ.

Simplifying the equations above, we have

2

y aytqyzt

50k2 ln 8k

min

(qz )2 ≥

2 δ.

y,t,z ( xy1tm + ynt )

D Corresponding Stories

In this section, we will provide an example for each selection method such that this particular sampling performs
the worst when compared with the other two methods. For the purpose of illustration, we consider binary confounder throughout this section. To ease notation, let qyt denote qy1t.

A Scenario in Which NSP Performs the Worst A drug repositioning start-up discovered that drug T

can potentially cure a disease γ. which has no known drug cure and goes away without treatments once a while.

Since drug T is commonly used to treat another disease η, the majority patients who has disease γ do not

receive any treatment. Among the ones who received drug T , the start-up discovered that the health outcomes

of the majority of patients have improved. The start-up proposes to bring drug T to an observational study to

verify whether drug T could treat disease γ while not controlling for patient’s treatment adherence levels. As in

most cases, patient’s treatment adherence levels could inﬂuence doctors’ decision of whether to prescribe drug

T and whether the treatment for disease γ will be successful. Translating this scenario into our notations, we

have a01 = 1, a10 = 2, a11 = 3, and a00 = 1 −

3 i=1

i, say a = (0.9, 0.02, 0.01, 0.07).

Now, imagine in the

clinical trial, the patients are given a drug case containing drug T such that the drug case automatically records

the frequency that the patient takes the drug. Somehow we know a priori that the patients who do not have

health improvement have on average poor treatment adherence, e.g., q00 = 0.9, q01 = 0.7; furthermore, those who

have health improvement on average have good treatment adherence, e.g., q10 = 0.01, q11 = 0.3. Deconfounding

according to NSP, i.e., x = (a00, a01, a10, a11), in this case, will select most samples from the group (Y = 0, T = 0).

Since the ATE depends on the estimation that relies on both T = 0, and T = 1, one would expect that NSP and

OWSP will outperform NSP. The left column in Figure 3 conﬁrms this hypothesis.

A Scenario in Which USP Performs the Worst A group biostatisticians discovered that mutations on gene T is likely to cause cancer Y in patients with a particular type of heart disease. In particular, they discovered that among the those heart disease patients, 79% of patients have neither mutation on T nor cancer Y ; 18% patients have both mutation on T and cancer Y . In other words, a00 = 0.79, a11 = 0.18. Furthermore, we have a01 = 0.01, a10 = 0.02. This group of biostatisticians want to run a small experiment to conﬁrm whether gene T causes cancer Y . In particular, they are interested in knowing whether those patients also have mutations on gene Z, which is also suspected by the same group of biostatisticians to cause cancer Y . Somehow, we know a priori that q00 = 0.5, q01 = 0.01, q10 = 0.05, q11 = 0.5. From the calculation of the ATE, it is not diﬃcult to observe that the error on the ATE is dominated by the estimation errors on q00, q11. Thus, we should sample more from the groups (Y = 0, T = 0) and (Y = 1, T = 1).

Causal Inference with Selectively Deconfounded Data
A Scenario in Which OWSP Performs the Worst A team wants to reposition drug T to cure diabetes. Drug T has been used to treat a common comorbid condition of diabetes that appears in 31% of the diabetic patient population. Among those patients who receive drug T , about 97% has improved health, that is a01 = 0.01 and a11 = 0.3. Among the patients who have never received drug T , about 70% have no health improvement, that is a00 = 0.5, and a10 = 0.19. Let q00 = 0.05, q01 = 0.5, q10 = 0.055, and q11 = 0.4. In the ATE, it is easy to observe that a11qa1111+qa1101q01 and a11(1−aq1111()1+−aq0111()1−q01) are both dominated by 1 regardless of the estimates of q11 and q01. In this case, USP outperforms OWSP and NSP when the sample size is larger than 200. On the other hand, the bottom ﬁgure in the third column of Figure 3 shows that, when averaged over all possible values of q, OWSP performs the best.
E Approximate Sampling Policies Under Finite Confounded Data
To deconfound according to NSP with ﬁnite confounded data is to deconfound the ﬁrst m confounded data. For USP, we split the samples to the 4 groups as evenly as possible. That is, we max out the bottleneck group/groups and distribute the excess data as evenly as possible among the remaining groups. For OWSP, we have xyt = aˆyyaˆtyt , and when implementing OWSP, we will ﬁrst ensure that the deconfounded samples are split as evenly as possible across treatment groups, and then within the each group, we split the samples close as possible to the outcome ratio.

