Prospection: Interpretable Plans From Language By Predicting the Future
Chris Paxton1, Yonatan Bisk2, Jesse Thomason2, Arunkumar Byravan2, Dieter Fox1,2

arXiv:1903.08309v1 [cs.AI] 20 Mar 2019

Abstract— High-level human instructions often correspond to behaviors with multiple implicit steps. In order for robots to be useful in the real world, they must be able to to reason over both motions and intermediate goals implied by human instructions. In this work, we propose a framework for learning representations that convert from a natural-language command to a sequence of intermediate goals for execution on a robot. A key feature of this framework is prospection, training an agent not just to correctly execute the prescribed command, but to predict a horizon of consequences of an action before taking it. We demonstrate the ﬁdelity of plans generated by our framework when interpreting real, crowd-sourced natural language commands for a robot in simulated scenes.
I. INTRODUCTION
A robot agent executing natural language commands must solve a series of problems. First, human language must be translated to an understanding of intent. For example, the command pick up the yellow block and place it on top of the red block corresponds to an intended change in world state that results in a yellow block on top of a red one. Given that understanding, an agent must plan a sequence of actions it can take to reach the target world state. In the above example, this could be (move(yellow); grasp(yellow); move(yellow, red); release(yellow)). Finally, these high level controls have to be executed in the world by servoing an arm to appropriate positions and controlling the gripper.
Each of these problems is challenging and has been investigated by existing research. Commonly, a pipeline approach is used, where each problem is addressed sequentially, and the outputs of one are fed to the next. In the example above, the semantic understanding that the goal is on(yellow, red) from natural language is passed to a high level controller. To simplify high level control prediction, robot perception is often augmented with visual semantics information, such as oracle object detections, bounding boxes, or 6d poses [1], [2]. In this work, we instead train a single model end-to-end that takes natural language and raw pixels, depths, and joint states to produce low-level controls to accomplish a goal.
Additionally, rather than treat the pipeline problems above independently, we introduce a prospection component to an agent’s training and inference, which facilitates “dreaming” about the consequences of chosen high level actions in the current scene. Prospection is the ability to reason about the consequences of future actions without executing them [3]. Our approach allows the agent to predict whether its high level actions will lead to undesirable world states.
1NVIDIA, USA 2Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA

Goal( ):

World( ):

take the yellow object from the table and place it on top of the red object

Next Prediction

Interpretable Predicted Futures

subgoal grasp(yellow) move(yellow,red) release (yellow)
action
control
Fig. 1: DREAMCELLS convert instructions to interpretable subgoals which can be visualized (Wˆ ) and executed (θ).
We consider a simple pick-and-place task, where the goal is to stack one block on top of another (Figure 1). This is limited in that there are only a few high-level actions the robot can take in a given world. Still, it proves challenging when speciﬁed with real, crowd-sourced natural language.
In short, our contributions are: • A language embedding rich enough to specify a se-
quence of actions to achieve a task-level goal. • A DREAMCELL architecture to predict future world
states from language and raw sensor observations. • An approach to convert a task plan output from these
DREAMCELLS into low-level executions.
II. RELATED WORK Our work draws inspiration from recent efforts on learning abstract representations for planning [4]. We build primarily on work in planning and natural language processing, with important future work in manipulation. Communicating control and goals has traditionally been accomplished by specifying high level operations [5], [1], [6], via formal languages like the Problem Domain Description Language (PDDL) [7] or as a Hierarchical Task Network [8]. Such systems provide a straightforward way to compose black-box operations to solve problems. While we maintain an interpretable intermediate layer, our interface is natural language, most akin to [9], [10] though we work in a fully end-to-end differentiable paradigm where embedded

RNN

Predictor
+

Dream Cell Fig. 2: At each timestep, the model receives an LSTM encoded language vector L, the initial world state W0, and the current world state Wt. Using these, it predicts the next world state Wˆ t+1 and sub-goal Gt.

language representations are learned alongside visual encodings of the world.
Our work is also motivated by the Universal Planning Networks which learn an embedding for images and a world state vector used to generate motion plans to goals speciﬁed via a target image [11]. That work learns a distance metric from the current state image to the target image which is used to perform rollouts for training and inference. While learned generic representations have notions of agency and planning, the produced plans lack human interpretability, which may be important to modularity and generalization [12]. Neural approaches and scaling robotic learning within simulation have become common as they allow for end-to-end training and can easily acquire more data (often from multiple domains) than otherwise possible on a physical device [2]. This has proven particularly important for RL-based approaches [13] and interpretability [14]. More generally there is a growing literature on learning deep representations that can be used to accomplish local control tasks or simple object manipulations [15], [16], [11], [17].
Core to our contribution is simulating the future actions and dynamics of our system. High level process simulation has been used in the NLP literature [18] without sensor data. Simultaneously, prospection has been used before in RL, often as a means of model-based control [19], [17], [20], and for ﬁne control tasks like cutting [19]. In addition, our approach is compatible with work in Visual Robot Task Planning [4], which shows that prospective subgoal predictions can be used to generate task plans.
Complementary to our work is the growing literature in NLP focusing on complex grounding instructions with esoteric references and other long tail linguistic phenomena [21], [22]. Natural language communication with robotics also allows for learning joint multimodal representations [23], [24] which harness the unique perceptual and manipulation capabilities of robotics.
While accurate grasping and placement is not a focus of our work, it has been explored in the literature [25], [26], [27]. In particular, high-precision grasping with deep neural networks generally takes the form of predicting a grasp success classiﬁer [25], [26].

III. PROBLEM DESCRIPTION
Given a natural language command and raw sensor readings for a scene, the task is to issue a sequence of low level controls to a robotic arm that accomplish the intended task.
Speciﬁcation. A DREAMCELL takes in the initial W0 and current Wt simulation-based state observations and a naturallanguage sentence s to produce a sequence of intermediate, latent-space goals zi, . . . , zi+h for this pick-and-place task up to horizon h. Each goal is a semantically meaningful break point in the execution, e.g., a completed grasp on the target.
DREAMCELLS make three predictions at every time step:
1) A sequence of subgoals predictions, representing the next high level actions out to some planning horizon;
2) A sequence of hidden state representations zi, . . . , zi+h representing the results of these subgoals; and
3) The end effector command θ that parameterizes the low-level controller for task execution, consisting of a 6DOF pose and gripper command.
These predictions allow us to learn an interpretable, executable representation for hallucinating future world states.
Metrics. We measure both extrinsic performance on the task and intrinsic performance of DREAMCELL components. Extrinsically, we evaluate how well the robot agent completes the pick-and-place task. We record binary task success/failure as whether the target block to be moved is dropped within a threshold of its intended position based on the language command. We also record the average mean-squared error of the predicted end effector goal at each step of the task. This metric penalizes moving the wrong block while giving partial credit for moving the right block to towards the right place, even if it never arrives there or is placed unstably (e.g., if it falls off the target block after release). Intrinsically, we evaluate the language-to-action component by how closely the predicted sequence of subgoals matches ground truth execution.
IV. APPROACH
We train the system end-to-end using simulated data. This allows us to automatically generate training sequences for both images and high-level subgoals. Subgoals take the form of semantic predicates like grasp and move with block arguments. To simplify notation, throughout the paper, we refer to the union of RGB, pose, and depth images with the single world state variable W . At every timestep the

tile 1x1x64 8x6x64
8x6x64

Predictor

cconv(5)

cconv(5)

cconv(5) +pad

deconv(5)

cconv(1)

cat

8x6x128

8x6x64

4x3x128

8x6x64

8x6x64

+
Fig. 3: Diagram of a single prediction cell. The prediction cell predicts a change in hidden state ∆z, and is an important component of the DREAMCELL, used both for visualizing possible futures and for predicting the goal of a particular motion for execution on our robot.

model is provided the current world observation (Wt), a description of the goal conﬁguration in natural language (encoded as L). In practice Wt also includes the initial state W0 to capture changes over time. All aspects of the model (including the encoders for both language and the world) are trained together. The model is trained on supervised demonstration data collected from an expert policy as per previous work [15], [4].
The basic unit in our model is the DREAMCELL (Fig. 2) which produces a sub-goal and a corresponding predicted image of the arm’s position at the next time step. This formulation allows for recurrent chaining of cells to rollout future goals and states (Fig. 1). Speciﬁcally, because the output of the DREAMCELL includes a deconvolved hallucination of the next world state (Wˆ t+1) we can simply continue to run the network forward, where true observations are replaced with the network’s predictions. Our cell has two outputs at every timestep t: 1. Subgoals (Gt) and 2. Predicted Worlds (Wˆ t+1). We provide intrinsic evaluations on future prediction performance in Section VI.
At inference time, we generate a task plan by rolling out multiple DREAMCELL timesteps into a possible future given state observations, z. The core of our approach is that these subgoals are converted into estimated world states Wˆ and associated end effector goals θ, which are fed into a lower level controller π that will convert them into trajectories.
A. DREAMCELL Subgoal Module
The subgoal module predicts the next subgoal from the current world state and the language instruction. It is formulated similarly to image captioning and sequence to sequence prediction. First, we use an LSTM [28] to encode the goal as expressed in language. Words are embedded as 64 dimensional vectors initialized randomly. We concatenate the ﬁnal hidden state (L) with the output of our world encoder (zt) as the initial hidden state of a new LSTM cell for decoding.
We generate an output of Gt = (verb, to obj, with obj)

tuples at each timestep, to a horizon of length ﬁve. We divide the subgoal Gt into: verb the action to be taken, to obj the object to servo towards, and with obj the optional object in hand (e.g., move(red, yellow) moves the yellow block in hand to the target red block).
During training, we use cross entropy loss on all 5x3 generations. The ﬁrst timestep in the RNN is passed the current hidden state and the At each timestep the RNN cell is passed the prediction from the previous timestep. As is common practice in the language modeling literature [29], we tie the emission and embedding matrix parameters. Traditionally, this is achieved by simply transposing a single matrix. Our model produces tuples by passing the hidden vector through three different feed-forward layers, so, to re-embed predictions we multiply by the three transposed embedding matrices and average the outputs to reconstruct an embedding.
B. DREAMCELL World Predictor Module
The prediction cell, shown in Fig. 3, takes in the current hidden state zt and the predicted subgoal Gt. Each prediction model outputs a predicted latent-state subgoal zˆt+1, such that P (zt, Gt) = zˆt+1. In effect, we learn a many-to-one mapping across multiple timesteps, all of which need to produce the same goal. We should also be able to roll this simulation forward in time in order to visualize future actions. Training this prediction space is a difﬁcult problem and requires a complex loss function involving multiple components.
The prediction cell is a simple autoencoder mapping inputs W to and from a learned latent space, as show in Fig. 2. World observations Wt and W0 are combined into a single estimated latent state zt. The vector containing the predicted subgoal Gt is tiled onto this state. We use a bottleneck within each prediction cell to force information to propagate across the entire predicted image, and then estimate a change in latent state ∆z such that zˆt+1 = zt + ∆z.
When visualizing the predicted image Wˆ t+1, we use a decoder consisting of a series of 5x5 convolutions and

*1-hot
selector

predicted current

Fig. 4: The actor module takes in a hidden state and associated subgoal API and converts this to a motion goal, which is represented as a Cartesian (x, y, z) position, a unit quaternion q = (a, b, c, d), and a gripper command g ∈ (0, 1). This motion goal can then be sent to the control module for execution.

bilinear interpolation for upsampling.
C. Actor Module
The actor module, shown in Fig. 4, predicts the parameters of an action that can be executed on the robot. Speciﬁcally, it takes in zt and the current high-level action and predicts a destination end effector pose that corresponds to the robot’s position at zt.
The architecture is a simple set of convolutions: the highlevel action is concatenated with the current zt as in the prediction module, then a set of three 3x3 convolutions with 64, 128, and 256 ﬁlters, each followed by a 2x2 max pool. This is followed by a dropout and a single 512 dimensional fully connected layer, and then to Nverbs × 8 outputs, predicting gripper command g ∈ (0, 1), Cartesian end effector position, and a unit quaternion for each highlevel action verb. The gripper command uses a sigmoid activation where 0 represents closed and 1 represents open, and Cartesian end effector position uses a tanh activation function. All pose values are normalized to be in (−1, 1).
A one-hot attention over action verbs chooses which pose and gripper command should be executed. In effect, the actor learns to compute a set of pose features for predicting the next manipulation goal and learns a simple perceptron model for each action verb in order to choose where the arm should go and whether the gripper should be opened or closed after the motion is complete.
D. Training
We train the encoder and decoder jointly when training the Prediction and Actor modules and optimize with Adam [30], using an initial learning rate of 1e − 3. We ﬁx the latent state encoder and decoder functions after this step, then use the learned hidden space to train the Subgoal module.
Image Reconstruction Loss This determines how well our model can reconstruct an image from a given hidden state zt, and is trained on the output of our visualization module. We used an L2 loss on pixels both for RGB and depth. Depth values were capped at 2 meters and were normalized to be between 0 and 1.
Subgoal Recovery Loss Image reconstruction losses are often insufﬁcient for capturing ﬁne details. This issue has

motivated recent work on GANs [31]. These are often unstable, so we propose an alternative solution specialized to our problem. Since each successful high-level action has a predictable result, we jointly train a classiﬁer that will recover the subgoal associated with each successive high-level action Gˆt. We use CG(zt) as the classiﬁer loss, minimizing cross entropy between the recovered estimate Gˆt and ground truth Gt.
Actor Loss Instead of estimating the full joint state of the robot as the result of a high-level action, our Actor module estimates the end-effector pose θt associated with sugboal Gt.
These poses are represented as θ = (pˆ, qˆ, gˆ), where pˆ is the Cartesian position, qˆ is a quaternion describing the orientation, and gˆ ∈ (0, 1) is the gripper command. When regressing to poses, we use a mixture of the L2 loss between Cartesian position and a loss derived from the quaternion angular distance (to capture both spatial and rotational error). The angle between two quaternions qˆ1 and qˆ2 is given as:
ω = cos−1(2 qˆ1, qˆ2 2).

To avoid computing the inverse cosine as a part of the loss, we use a squared distance metric. In addition, normalize gripper commands to be between 0 and 1, where 0 is closed and 1 is open, and trained with an additional L2 loss on predicted gripper commands. Given estimated pose θˆ and ﬁnal pose θ, we calculate pose estimation loss:

Cactor(θˆ, θ) = λactor

pˆ − p

2 2

+

(1

−

qˆ, q ) +

gˆ − g

2 2

.

Object Pose Estimation Loss It is important to ensure that our learned latent states zt capture all the necessary information to perform the task. As such, we use an augmented loss Cobj(zt) that predicts the position of each of the four blocks in the scene at the observed frame. This information is not used at test time, but is structurally identical to the pose estimation loss Cpose
Combined Prediction Loss The ﬁnal loss function for predicting the effects of performing a sequence of high-level

put the blue cube onto the yellow cube stack the top most cube onto the second highest cube
Fig. 5: Human participants on Mechanical Turk gave two commands for how to create the target image (right) from the initial image (left).

actions is then:

C(Zˆ) =λW

Wˆ t − Wt

2 2

+

Cobj (zt)+

λW

Wˆ t+i − Wt+i

2 2

+ C (θˆ , θ ) + C (Gˆ , G ) .

i∈h

actor t+i t+i

G t+i t+i

E. Execution

When executing in a new environment, the robot agent takes in the current world state W0 and a natural language instruction L. The agent computes a future prediction using a DREAMCELL, by rolling out predicted goals Gt, . . . , Gt+H which generate latent space subgoals. These subgoals can then be visualized to provide insight into how the robot expects the task to progress. This also illuminates misunderstandings and limitations of the system (see Analysis VI).
The system generates new prospective plans out to a given planning horizon. After predicting the next subgoal zt+1, it will then use the actor to estimate the next motion goal θt+1. This goal is sent to the low-level execution system, which in our case is a traditional motion planner that does not have knowledge of object positions. In our case, the planner used was RRT-connect [32], via MoveIt
In the future, these subgoals shown to the user, who can give the ﬁnal conﬁrmation on whether or not to execute this hallucinated task plan if it accomplishes what they requested. Alternatively, the user could input a new L, or the agent could sample a new sequence of goals.

V. EXPERIMENT SETUP
We performed a number of variations on a simple block stacking task. All experiments were performed in simulation. We collected 5015 trials using a sub-optimal expert policy, of which 2370 were successes. Our model was trained only on successful examples.
We generated trials using a simple simulation of an ABB YuMi robot picking up 3.5 cm cubes. There were four cubes, one of each color: red, green, yellow, blue. When collecting data, we ﬁrst randomly compute a table position within a 50 cm box centered in front of the robot. Blocks were randomly placed in non-intersecting positions on this table. The arm’s initial position was also randomized to an area off the right side of the table.
We selected manipulation goals at random, and provided a simple expert policy which moved to pick up each object using an RRT motion planner. The plan has ﬁve steps:

Oracle GT Action
Template Real Lang

Align
0.04 0.32
0.32 0.51

L2 distance in cm ↓ Grasp Lift Move
0.03 0.04 0.04 0.31 0.48 0.63
0.39 0.47 0.65 1.23 1.50 2.39

Release
0.04 0.63
0.65 2.40

Success ↑ Rate
98.4% 90.4%
87.8% 77.1%

TABLE I: L2 distances and accuracy when executing plans generated from either ambiguous natural language instructions or unambiguous template language.

align with the top of a random block, grasp that block and close the gripper, lift the block off the table, move the block to atop another block, and then release the block.
We collected natural language commands from human annotators through the Mechanical Turk crowd-sourcing platform.1 Annotators were shown two scene images: one before and one after a block had been stacked on another block. They were instructed to give two distinct commands that would let someone create the second scene from the ﬁrst (Figure 5), and were paid $0.25 per such annotation. For each of our 2370 successful trials, we obtained two language commands describing the high level pick-and-place goal. On average, commands are 11 words long.2 We compare Turk data to unambiguous templated language that was procedurally generated from the manipulated blocks.
VI. RESULTS
We ran a set of experiments on our simulation, and computed task execution success rate. We analyze the performance of the Subgoal and Predictor modules given different classes of language input.
A. Execution Results
Finally, we test our model on a set of held-out scenarios, and compare to ground-truth execution. We compared accuracy of the estimated motion under each of three conditions: with oracle subgoals Gt from the test data, with unambiguous templated language, and with natural language. Position accuracy results are shown in Table I.
In all cases, we compute an execution plan Gˆ1, . . . , Gˆ5 at the beginning and use our Predictor and Actor networks to follow this execution plan until all steps have been executed. Results are shown in Table I. We count successes when the block was moved to within 1.5 cm of the target in the x and y direction, and 0.5 cm z of the ﬁnal position from which it was dropped.
We see only a handful of failures when the robot was sent to ground truth “oracle” poses, due to stochastic interactions between the objects and gripper and randomness at the control level. 88.1% of execution with ground truth actions, 88.0% of unambiguous templated language, and 84.0% of natural language were successfully able to pick up a block and put it in the right area—indicating a high level of precision independent of the task speciﬁcation. Grasp success rates tended to be very high. The most dramatic failures we

1https://www.mturk.com/ 2Participants used 389 unique words after lowercasing and tokenization.

Predicted

Oracle

Fig. 6: Comparison of generated subgoal predictions. Top two rows: RGB and depth images generated from predicted subgoal G. Bottom two rows: RGB and depth images generated from ground truth G from training data.

Horizon
Template Verb To Object With Object
Overall
Real Lang Verb To Object With Object
Overall

h=1
84.0% 91.5% 93.7% 82.9%
84.2% 87.8% 91.4% 79.6%

h=2
87.9% 90.4% 91.8% 86.8%
87.8% 87.4% 90.1% 83.8%

h=3
93.0% 93.0% 94.4% 92.5%
92.9% 89.8% 92.5% 89.1%

h=4
97.4% 97.4% 96.8% 97.3%
97.2% 93.9% 96.4% 93.7%

h=5
100.0% 100.0% 100.0% 100.0%
100.0% 98.2% 100.0% 98.2%

TABLE II: Subgoal prediction accuracy (↑) at different horizons with templated versus natural language. We see higher accuracy as we move closer to the end of the task, when the space of possible remaining plans is less ambiguous.

observed were situations where one or more necessary blocks was out of the camera’s viewpoint, in which case our visionbased system fails by default.
The similar performance between templated language and ground truth actions suggests that unambiguous, templated language is insufﬁcient to demonstrate the language learning capabilities of our system. We ﬁnd our method is robust to real natural language from Mechanical Turk workers, achieving 95% of the success rate seen on unambiguous templates.
B. Subgoal Module
We analyze the language learning component of our model. A full breakdown of subgoal prediction accuracy is given in Table II. Performance was comparable between templated language and natural language data collected from Amazon Mechanical Turk. We see that it is more difﬁcult to make accurate predictions on real language data. Additionally, accuracy is remarkably consistent over time, meaning

that the model properly learned the correct sequence of actions that should be executed. Accuracy farther out into the future is stable because the network knows when and how a sequence should end.
There are two major sources of error we observe in these examples. First, the difference in accuracy between Mechanical Turk language and templated language is largely explained by the ambiguity and underspeciﬁcity in Turk commands (e.g., not specifying a destination after a grasp). Second, the overall error is largely due to sequence error at transition points, where multiple possible actions are reasonable depending on whether or not the low level control has arrived at its destination. This further supports our hypothesis that we need to reason about all three sub-problems jointly.
C. Prediction Model
The role of the prediction model is to generate subgoal predictions zˆt+1, . . . , zˆt+h representing the h next actions that the robot can take in order to perform the task. Fig. 6 shows an example of one course of error we see during these prediction rollouts. The top two rows show a sequence of predictions coming from the sequence to sequence model, while the bottom row shows predictions using ground truth actions from our data. In this case, we see that the sequence to sequence model started the grasp verb earlier than the ground truth execution, but both models generate good image predictions.
As we can see in Table I, there is persistently some error in the low-level predictions from our actor module, even when given oracle arguments Average placement error increases as we move away from the ground-truth arguments. Often failures occur because the object is not clearly visible in the ﬁrst frame.

Fig. 7: Results of different simulated executions. Successful grasps (top row) can be undermined by small errors in the low-level actor network that compound to create accuracy issues at execution time (bottom row).
Our reconstruction results have another advantage, however, as seen in Fig. 6: they are clearly interpretable, which means that the robot can readily justify its decisions even when it does make a mistake, facilitating a human user providing a new instruction that considers this mistake. Overall, these results show that we can learn representations for a task that are sufﬁcient for planning and execution purely from language and raw sensory data.
We performed an ablation analysis on the best prediction models to determine how much they use information from different layers. In particular, we see similar performance when training without the image loss (89.5% successful on held out test data) and without the image and object losses (88.6% successful). This suggests that our image reconstruction loss may help, and certainly does not have a negative impact.
VII. CONCLUSIONS
We present an approach for inferring interpretable plans from natural language and raw sensor input using prospection. Our DREAMCELL architecture predicts future world states from language and raw sensor observations, facilitating high level plan inference that can be converted into low-level execution. Prospection enables end-to-end plan inference that is agnostic to the nature of sensory input and low-level controller modules. In the future, using this architecture to bootstrap language understanding for execution on a real robot using sim-to-real transfer techniques could facilitate end-to-end control on a physical platform.
VIII. ACKNOWLEDGEMENTS
This work was funded in part by the National Science Foundation under contract no. NSF-NRI-1637479, and the DARPA CwC program through ARO (W911NF-15-1-0543).

We would like to thank Jonathan Tremblay for valuable
discussions.
REFERENCES
[1] C. Paxton, A. Hundt, F. Jonathan, K. Guerin, and G. D. Hager, “CoSTAR: Instructing collaborative robots with behavior trees and vision,” Robotics and Automation (ICRA), 2017 IEEE International Conference on, 2017.
[2] D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese, “Neural task programming: Learning to generalize across hierarchical tasks,” International Conference on Robotics and Automation (ICRA), 2018.
[3] D. T. Gilbert and T. D. Wilson, “Prospection: Experiencing the future,” Science, vol. 317, no. 5843, pp. 1351–1354, 2007.
[4] C. Paxton, Y. Barnoy, K. D. Katyal, R. Arora, and G. D. Hager, “Visual robot task planning,” in International Conference on Robotics and Automation (ICRA), 2019.
[5] E. Guizzo. (2017) Rethink’s robots get massive software upgrade, rodney brooks “so excited”. [Online]. Available: https://spectrum.ieee.org/automaton/robotics/industrial-robots/ rethink-robots-get-massive-software-upgrade
[6] C. Paxton, F. Jonathan, A. Hundt, B. Mutlu, and G. D. Hager, “Evaluating methods for end-user creation of robot task plans,” Intelligent Robots and Systems (IROS), 2018 IEEE International Conference on, 2018.
[7] M. Ghallab, C. Knoblock, D. Wilkins, A. Barrett, D. Christianson, M. Friedman, C. Kwok, K. Golden, S. Penberthy, D. E. Smith et al., “PDDL-the planning domain deﬁnition language,” http://www.citeulike.org/group/13785/article/4097279, 1998.
[8] K. Erol, J. Hendler, and D. S. Nau, “Htn planning: Complexity and expressivity,” in AAAI, vol. 94, 1994, pp. 1123–1128.
[9] R. Paul, J. Arkin, N. Roy, and T. Howard, “Efﬁcient grounding of abstract spatial concepts for natural language interaction with robot manipulators,” in Proceedings of the 2016 Robotics: Science and Systems Conference, June 2016.
[10] D. Arumugam, S. Karamcheti, N. Gopalan, L. L. Wong, and S. Tellex, “Accurately and efﬁciently interpreting human-robot instructions of varying granularities,” in Proceedings of the 2017 Robotics: Science and Systems Conference, 2017.
[11] A. Srinivas, A. Jabri, P. Abbeel, and S. Levine, “Universal planning networks,” in Proceedings of the International Conference in Machine Learning (ICML), 2018.
[12] M. Garnelo, K. Arulkumaran, and M. Shanahan, “Towards deep symbolic reinforcement learning,” in Deep Reinforcement Learning Workshop at NIPS, 2016.
[13] O. Nachum, S. Gu, H. Lee, and S. Levine, “Data-efﬁcient hierarchical reinforcement learning,” 2018.
[14] J. Tremblay, T. To, A. Molchanov, S. Tyree, J. Kautz, and S. Birchﬁeld, “Synthetically trained networks for learning human-readable plans from real-world demonstrations,” International Conference on Robotics and Automation (ICRA), 2018.
[15] A. Byravan and D. Fox, “Se3-nets: Learning rigid body motion using deep neural networks,” in Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 2017, pp. 173–180.
[16] C. Finn and S. Levine, “Deep visual foresight for planning robot motion,” in Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 2017, pp. 2786–2793.
[17] T. Weber, S. Racanie`re, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li et al., “Imaginationaugmented agents for deep reinforcement learning,” arXiv preprint arXiv:1707.06203, 2017.
[18] A. Bosselut, L. Omer, A. Holtzmann, C. Ennis, D. Fox, and Y. Choi, “Simulating action dynamics with neural process networks,” International Conference on Learning Representations, 2018.
[19] I. Lenz, R. A. Knepper, and A. Saxena, “DeepMPC: Learning deep latent features for model predictive control.” in Robotics: Science and Systems, 2015.
[20] R. Pascanu, Y. Li, O. Vinyals, N. Heess, L. Buesing, S. Racanie`re, D. Reichert, T. Weber, D. Wierstra, and P. Battaglia, “Learning modelbased planning from scratch,” arXiv preprint arXiv:1707.06170, 2017.
[21] Y. Bisk, D. Yuret, and D. Marcu, “Natural language communication with robots,” in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 751–761.

[22] Y. Bisk, K. J. Shih, Y. Choi, and D. Marcu, “Learning interpretable spatial operations in a rich 3d blocks world,” in Proceedings of the Thirty-Second Conference on Artiﬁcial Intelligence (AAAI-18), 2017.
[23] J. Thomason, J. Sinapov, M. Svetlik, P. Stone, and R. Mooney, “Learning multi-modal grounded linguistic semantics by playing “I spy”,” in Proceedings of the 25th International Joint Conference on Artiﬁcial Intelligence (IJCAI-16), July 2016, pp. 3477–3483.
[24] J. Thomason, J. Sinapov, R. J. Mooney, and P. Stone, “Guiding exploratory behaviors for multi-modal grounding of linguistic descriptions,” Intelligence (AAAI-18), 2018.
[25] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning handeye coordination for robotic grasping with deep learning and largescale data collection,” in Proceedings of the International Symposium on Experimental Robotics, 2016.
[26] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics,” 2017.
[27] D. Morrison, P. Corke, and J. Leitner, “Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach,” Proceedings of the 2018 Robotics: Science and Systems Conference, 2018.
[28] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[29] O. Press and L. Wolf, “Using the output embedding to improve language models,” in Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Association for Computational Linguistics, April 2017, pp. 157–163.
[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in 3rd International Conference for Learning Representations, 2015.
[31] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Proceedings of the 2014 Conference on Neural Information Processing Systems, 2014, pp. 2672–2680.
[32] J. J. Kuffner Jr and S. M. LaValle, “Rrt-connect: An efﬁcient approach to single-query path planning,” in ICRA, vol. 2, 2000.

A-I. TEMPLATED VS REAL LANGUAGE
Table A1 contains examples of the different types of language that occur when asking humans to describe the action versus using templated language.

Before

After

Template
place yellow block on the red block

Human
stack warm colors

stack the red block on the green one

Unknown concepts
move red right to same x and y axis as green

place green on the yellow one

Coordinate System
move the green box forward three spaces

stack the blue one on yellow
put the yellow one on the green block

Spatial language
take the blue block in your hand and raise it above the table. move the block back and to the right until it is directly above the yellow block. lower the blue block down onto the yellow block and release it
Latent details about hand movement
move the yellow cube to the right until it is on top of the green cube with the front half of the yellow cube touching the far half of the top of the green cube

Denotes speciﬁc nuance
TABLE A1: Above are the initial and ﬁnal visual frames for each task, next to the template language and human descriptions for examples from our training set. These examples illustrate why it can be so difﬁcult for a model to predict speciﬁc motions that correspond to a particular natural language command, and further justify our approach for visualizing robot actions before execution. Speciﬁc reasons why each description is difﬁcult to ground are indicated in bold
A-II. PREDICTION RESULTS One advantage of proposed DREAMCELL system is that it allows us to generate multiple hallucinations of possible futures. Here, we show example plans generated from four unseen test environments, given a natural-language prompt. We show predictions for the ﬁrst four high level actions: align, grasp, lift, and move to. Environments and trials were chosen at random, and should be indicative of performance on the prospection problem.

Prompt: “put red on blue”
1.
2.
3.
4. Fig. A1: Example showing predicted plans given straightforward language.
Prompt: “put blue on the other one”
1.
2.
3.
4. Fig. A2: Example showing predicted plans given underspeciﬁed language. The system always picks up the blue block, and correctly places it on the “other” one; however, it always chooses red. Handling ambiguity is left to future work.

Prompt: “take the red block in your hand and lift it off the table and move it to the blue block and lower it and open your hand”
1.
2.
3.
4. Fig. A3: Example showing predicted plans given overspeciﬁed language.
Prompt: “stack warm colors”
1.
2.
3.
4. Fig. A4: Example showing predicted plans given language speciﬁed using rare (N = 1 in train) terminology.

