Contextual Games: Multi-Agent Learning with Side Information

arXiv:2107.06327v1 [cs.GT] 13 Jul 2021

Pier Giuseppe Sessa ETH Zürich
sessap@ethz.ch

Ilija Bogunovic ETH Zürich
ilijab@ethz.ch

Andreas Krause ETH Zürich
krausea@ethz.ch

Maryam Kamgarpour ETH Zürich
maryamk@ethz.ch

Abstract
We formulate the novel class of contextual games, a type of repeated games driven by contextual information at each round. By means of kernel-based regularity assumptions, we model the correlation between different contexts and game outcomes and propose a novel online (meta) algorithm that exploits such correlations to minimize the contextual regret of individual players. We deﬁne game-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and optimal contextual welfare for this new class of games and show that c-CCEs and optimal welfare can be approached whenever players’ contextual regrets vanish. Finally, we empirically validate our results in a trafﬁc routing experiment, where our algorithm leads to better performance and higher welfare compared to baselines that do not exploit the available contextual information or the correlations present in the game.
1 Introduction
Several important real-world problems, ranging from economics, engineering, and computer science involve multiple interactions of self-interested agents with coupled objectives. They can be modeled as repeated games and have received recent attention due to their connection with learning (e.g., [11]).
An important line of research has focused, on the one hand, on characterizing game-theoretic equilibria and their efﬁciency and, on the other hand, on deriving fast learning algorithms that converge to equilibria and efﬁcient outcomes. Most of these results, however, are based on the assumption that the players always face the exact same game, repeated over time. While this leads to strong theoretical guarantees, it is often unrealistic in practical scenarios: In routing games [30], for instance, the agents’ travel times and hence the ‘rules’ of the game are governed by many time-changing factors such as network’s capacities, weather conditions, etc. Often, players can observe such factors, and hence could take better decisions depending on the circumstances.
Motivated by these considerations, we introduce the new class of contextual games. Contextual games deﬁne a more general class of repeated games described by different contextual, or side, information at each round, denoted also as contexts in analogy with the bandit optimization literature (e.g., [24]). Importantly, in contextual games players can observe the current context before playing an action, which allows them to achieve better performance, and converge to stronger notions of equilibria and efﬁciency than in standard repeated games.
Related work. Learning in repeated static games has been extensively studied in the literature. The seminal works [17, 18] show that simple no-regret strategies for the players converge to the set of Coarse Correlated Equilibria (CCEs), while the efﬁciency of such equilibria and learning dynamics has been studied in [6, 31]. Exploiting the static game structure, moreover, [38, 15] propose faster learning algorithms, and a long array of works (e.g., [35, 7, 4]) study convergence to Nash equilibria. Learning in time-varying games, instead, has been recently considered [14], where the authors show that dynamic regret minimization allows players to track the sequence of Nash equilibria, provided that the stage games are monotone and slowly-varying. Adversarially changing zero-sum games have
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

also been studied [10], with convergence guarantees to the Nash equilibrium of the time-averaged game. Our contextual games model is fundamentally different than [14, 10] in that we assume players observe the current context (and hence have prior information on the game) before playing. This leads to new equilibria and a different performance benchmark, denoted as contextual regret, described by the best policies mapping contexts to actions. Perhaps closer to ours is the setup of stochastic (or Markov) games [34], at the core of multi-agent reinforcement learning (see [9] for an overview). There, players observe the state of the game before playing but, differently from our setup, the evolution of the state depends on the actions chosen at each round. This leads to a nested game structure, which requires signiﬁcant computational power and players’ coordination to compute equilibrium strategies via backward induction [16, 13]. Instead, we consider arbitrary contexts’ sequences (potentially chosen by an adversarial Nature) and show that efﬁcient algorithms converge to our equilibria in a decentralized fashion.
From a single player’s perspective, a contextual game can be reduced to a special adversarial contextual bandit problem [8, Chapter 4], for which several no-regret algorithms exist. All such algorithms, however, rely on high-variance estimates for the rewards of non-played actions and thus their performance degrades with the number of actions available. A fact not exploited by these algorithms is that in a contextual game similar contexts and game outcomes likely produce similar rewards (e.g., in a routing game, similar network capacities and occupancy proﬁles lead to similar travel times). We encode this fact using kernel-based regularity assumptions and (similarly to [32] in non-contextual games) show that exploiting these assumptions, and additionally observing the past opponents’ actions, players can achieve substantially improved performance compared to using standard bandit algorithms. For instance, for K actions and adversarially chosen contexts from a ﬁnite set Z, the bandit S-EXP3[8] i√ncurs O( T K|Z| log K) contextual regret, while our approach leads to a O( T |Z| log K + γT T ) guarantee, where γT is a sample-complexity parameter describing the degrees of freedom in the player’s reward function. For commonly used kernels, this results in a sublinear regret bound that grows only l√ogarithmically i√n K. Moreover, when contexts are stochastic and private to a player, w√e obtain a O( T log K + γT T ) pseudo-regret bound. This bound should be compared to the O( T K log K) pseudo-reg√ret of [5] which –unlike us– assumes observing the rewards for non-revealed contexts, and the O( cT K log K) pseudo-regret of [28], which assumes known contexts distribution and a linear dependence of rewards on contexts in Rc.
Contributions. We formulate the novel class of contextual games, a type of repeated games characterized by (potentially) different contextual information available at each round. - We identify the contextual regret as a natural benchmark for players’ individual performance,
and propose novel online algorithms to play contextual games with no-regret. Unlike existing contextual bandit algorithms, our algorithms exploit the correlation between different game outcomes, modeled via kernel-based regularity assumptions, yielding improved performance.
- We characterize equilibria and efﬁciency of contextual games, deﬁning the new notions of contextual Coarse Correlated Equilibria (c-CCE) and optimal contextual welfare. We show that c-CCEs and contextual welfare can be approached in a decentralized fashion whenever players minimize their contextual regrets, thus recovering important game-theoretic results for our larger class of games.
- We demonstrate our results in a repeated trafﬁc routing application. Our algorithms effectively use the available contextual information (network capacities) to minimize agents’ travel times and converge to more efﬁcient outcomes compared to other baselines that do not exploit the observed contexts and/or the correlations present in the game.
2 Problem Setup
We consider repeated interactions among N agents, or players. At every round, each player selects an action and receives a payoff that depends on the actions chosen by all the players as well as the context of the game at that round. More formally, we let Z represent the (potentially inﬁnite) set of possible contexts, and Ai be the set of actions available to player i. Then, we deﬁne ri : A × Z → [0, 1] to be the reward function of each player i, where A := A1 × · · · × AN is the joint action space. Importantly, we assume ri is unknown to player i. With the introduced notation, a repeated contextual game proceeds as follows. At every round t:
• Nature reveals context zt • Players observe zt and, based on it, each player i selects action ait ∈ Ai, for i = 1, . . . N . • Players obtain rewards ri ait, a−t i, zt , i = 1, . . . , N .
2

Moreover, as speciﬁed later, player i receives feedback information at the end of each round that it can use to improve its strategy. Let Πi be the set of all policies π : Z → Ai, mapping contexts to
actions. After T game rounds, the performance of player i is measured by the contextual regret:

T

T

Rci (T ) = max ri π(zt), a−t i, zt − ri ait, a−t i, zt .

(1)

π∈Πi

t=1

t=1

The contextual regret compares the cumulative reward obtained throughout the game with the one achievable by the best ﬁxed policy in hindsight, i.e., had player i known the sequence {zt, a−t i}Tt=1 of contexts and opponents’ actions ahead of time, as well as the reward function ri(·). Crucially, Rc(T ) sets a stronger benchmark than competing only with the best ﬁxed action a ∈ Ai and captures the fact that players should use the revealed context information to improve their performance. A strategy is no-regret for player i if Rci (T )/T → 0 as T → ∞.
Contextual games generalize the class of standard (non-contextual) repeated games, allowing the game to change from round to round due to a potentially different context zt (we recover the standard repeated games setup and regret deﬁnition by assuming zt = z0 for all t). In Section 4 we deﬁne new notions of equilibria and efﬁciency for such games and show that the contextual regret deﬁned in (1), besides measuring individual players’ performance, has a close connection with game equilibria and efﬁciency. First, however, motivated by these considerations, we focus on the individual perspective of a generic player i and seek to derive suitable no-regret strategies. In this regard, the algorithms and guarantees presented in the next section do not rely on the other players complying with any pre-speciﬁed rule, but consider the worst case over the opponents’ actions a−t i (also, potentially chosen as a function of the observed game data). To simplify notation, we denote player i’s reward function with r(·), unless otherwise speciﬁed.
Regularity assumptions. Even with a single known context, achieving no-regret is impossible unless we make further assumptions on the game [11]. We assume the action set Ai is ﬁnite with |Ai| = K. We consider a generic set Z ⊆ Rc and make no assumptions on how contexts are generated (they could be adversarially chosen by Nature, possibly as a function of past game rounds). In Section 3.3, however, we consider a special case where contexts are sampled i.i.d. from a static distribution ζ. Our next regularity assumptions concern the reward function r(·). Let D := A×Z. We assume the unknown function r(·) has a bounded norm in a Reproducing Kernel Hilbert Space (RKHS) associated with a positive-semideﬁnite kernel function k : D × D → [−1, 1]. Kernel k(·, ·) measures the similarity between two different context-action pairs, and the norm
r k = r, r k measures the smoothness of r with respect to k. This is a standard non-parametric assumption used, e.g., in Bayesian optimization [37] and recently exploited to model correlations in repeated games [32]. It encodes the fact that similar action proﬁles (e.g., similar network’s occupancy proﬁles in a routing game) lead to similar rewards (travel times), and allows player i to generalize experience to non-played actions and, in our case, unseen contexts. Popularly used kernels include polynomial, Squared Exponential (SE), and Matérn kernels [29], while composite kernels [22] can also be used to encode different dependences of r on ai, a−i, and context z.
Feedback model. We assume player i receives a noisy bandit observation r˜t = r(ait, a−t i, zt) + t of the reward at each round, where t is σ-sub-Gaussian (i.e., E[exp(α t)] ≤ exp(α2σ2/2), ∀α ∈ R) and independent over time. Moreover, we assume, similar to [32], that at the end of each round, player i also observes the actions a−t i chosen by the other players. The latter assumption will allow player i to achieve improved performance compared to the standard bandit feedback. In some applications (e.g., aggregative games such as trafﬁc routing), it is only sufﬁcient to observe an aggregate function of a−t i.

3 Algorithms and Guarantees
From the perspective of player i, playing a contextual game corresponds to an adversarial contextual bandit problem (see, e.g., [8, Chapter 4]) where, at each round, context zt is revealed, an adversary picks a reward function rt(·, zt) : Ai → [0, 1] and player i obtains reward rt(ait, zt). Therefore, player i could in principle use existing adversarial contextual bandits algorithms to achieve no-regret. Such algorithms come with different regret guarantees depending on the assumptions made but, importantly, incur a regret which scales poorly with the size of the action space Ai. This is because they use highvariance estimators to estimate the rewards of non-played actions, i.e., the so-called full information feedback. Also, some of these algorithms assume parametric (e.g., linear [28]) dependence of the rewards rt(·, zt) on the context zt and hence cannot deal with our general game structure.

3

Algorithm 1 The C.GP-MW (meta) algorithm

Require: Finite set Ai of K actions, kernel k, learning rates {ηt}t≥0, conﬁdence levels {βt}t≥0. 1: for t = 1, . . . , T do

/* Nature chooses context zt

/*

2: Observe context zt

3: Compute distribution pt(zt) ∈ ∆K using: zt, ηt, and {ucbτ (·), a−τ i, zτ }tτ−=11.

4: Sample action ait ∼ pt(zt)

/* Simultaneously, opponents choose their actions a−t i

/*

5: Observe noisy reward r˜t and opponents’ actions a−t i

// r˜t = r(ait, a−t i, zt) + t

6: Use the observed data to update ucbt(·) according to (2) and (3).

Instead, we exploit the fact that in a contextual game the rewards obtained at different times are cor-
related through the reward function r(·) (i.e., contextual games correspond to the speciﬁc contextual bandit problem where rt(·, zt) = r(·, a−t i, zt) for all t). This fact, together with our feedback model, allows player i to use past game data to obtain (with increasing conﬁdence) an estimate of the reward
function r and use it to emulate the full-information feedback.

RKHS regression. Using past game data {aiτ , a−τ i, zτ , r˜τ }tτ=1, standard kernel ridge regression [29] can be used to compute posterior mean and corresponding variance estimates of the reward function r(·). For any x = (a, a−i, z) ∈ D, and regularization parameter λ > 0, they can be obtained as:

µt(x) = kt(x)T Kt + λIt −1yt , σt2(x) = k(x, x) − kt(x)T Kt + λIt −1kt(x) , (2)

where kt(x) = k xj, x tj=1, yt = r˜j tj=1, and Kt = k xj, xj j,j is the kernel matrix. Moreover, such estimates can be used to build the upper conﬁdence bound function:

ucbt(·) := min{µt(·) + βtσt(·) , 1} ,

(3)

where βt is a conﬁdence parameter, and the function is truncated at 1 since r(x) ∈ [0, 1] for all x ∈ D. A standard result from [37] shows that βt can be chosen such that r(x) ∈ [µt(x) + βtσt(x), µt(x) − βtσt(x)] with high probability for any x ∈ D (see Lemma 7 in the Appendix). The function ucbt(·) hence represents an optimistic estimate of r(·) and can be used by player i to emulate the full-information feedback. We outline our (meta) algorithm C.GP-MW in Algorithm 1.

C.GP-MW extends and generalizes the recently proposed GP-MW [32] algorithm to play repeated games, to the case where contextual information is available to the players and the goal is to compete with the optimal policy in hindsight. At each time step, after observing context zt the algorithm computes a distribution pt(zt) ∈ ∆K , where ∆K is the K-dimensional simplex, and samples an action from it. At the same time, the algorithm uses the observed game data to construct upper conﬁdence bound functions of the player’s rewards using (3). Such functions, together with the observed context zt are used to compute the distribution pt(zt) at each round. Note that C.GP-MW is a well-deﬁned algorithm after we specify the rule used to compute pt(zt) (line 3 of Algorithm 1). We left such rule unspeciﬁed, as we will specialize it to different settings throughout this section.

The regret bounds obtained in this section depend on the so-called maximum information gain [37] about the unknown function r(·) from T noisy observations, deﬁned as:
γT := max 0.5 log det(IT + KT /λ).
{xt }Tt=1
This quantity represents a sample-complexity parameter which, importantly, for popularly used kernels does not grow with the number of actions K but only with the dimension d of the domain D. It can be bounded analytically as, e.g., γT ≤ O(d log T ) and γT ≤ O(log(T )d+1) for squared exponential and linear kernels, respectively [37]. Moreover, we remark that although in the worst case d grows linearly with the number of players N , in many applications (such as the trafﬁc routing game considered in Section 5) the reward function r(·) depends only on some aggregate function of the opponents’ actions a−i and therefore d is independent from the number of players in the game.

3.1 Finite (small) number of contexts
When the context set Z is ﬁnit√e, a basic version of C.GP-MW achieves a high-probability regret bound of O( T |Z| log K +γT T ): We simply maintain a distribution pt(z) ∈ ∆K for each context z and update it only when z is observed, using the Multiplicative Weights (MW) method [26]. We formally introduce and study such strategy in Appendix A.1. In the same setting, and with standard

4

Strategy 2 Exploit similarity across contexts

1: Set Radius > 0, C = {z1}, and let p1(z1) be the uniform distribution.

2: for t = 2, . . . , T do

3: Observe context zt and let zt = arg minz∈C zt − z 1

4: if zt − zt 1 > then

5:

Add zt to the set C, set zt = zt, and let pt(zt) be the uniform distribution

6: else

t−1

pt(zt)[a] ∝ exp ηt · ucbτ (a, a−τ i, zτ ) · 1{zτ = zt}

a = 1, . . . , K . (4)

τ =1

bandit feedback, the S-EXP3[8] algorithm achieves regret O( T |Z|K log K), which has a worse dependence on the number of actions K. These regret bounds, however, are appealing only when the set Z has low cardinality, and become worthless otherwise. Intuitively, this is because no information is shared across different contexts and each context is treated independently from each other.

3.2 Exploit similarity across contexts

For large or even inﬁnite Z, we want to exploit the fact that similar contexts should lead to similar

performance and take this into account when computing the action distribution pt(zt). We capture

this fact by assuming the optimal policy in hindsight π = arg maxπ∈Πi

T t=1

r(π(zt),

a−t i,

zt)

is

Lp-Lipschitz:

|π (z1) − π (z2)| ≤ Lp z1 − z2 1, ∀z1, z2 ∈ Z .

Moreover, we assume Z ⊆ [0, 1]c to obtain a scale-free regret bound, and that the reward function r(·) is Lr-Lipschitz with respect to the decision set Ai, i.e., |r(a1, a−i, z)−r(a2, a−i, z)| ≤ Lr a1 − a2 1, ∀a1, a2 ∈ Ai, ∀(a−i, z), which is readily satisﬁed for most kernels [12, Lemma 1].

These assumptions allow player i to share information across different, but similar, contexts to improve the performance. This can be done by using the online Strategy 2 to compute pt(zt) at each round (Line 3 in Algorithm 1). Such strategy consists of building, in a greedy fashion as new contexts are revealed, an -net [23] of the context space Z, similarly to the algorithm by [20] for online convex optimization: At each time t, either a new L1-ball centered at zt is created or zt is assigned to the closest ball. In the latter case, pt(zt) is computed via a MW rule using the sequence of ucbτ (·) functions for those time steps τ < t that zτ belongs to such ball. Note that Strategy 2 can also be implemented recursively, by maintaining a probability distribution for each new ball and updating only the one corresponding to the ball zt belongs to. The radius is a tunable parameter, which can be set as follows.

Theorem 1. Fix δ ∈ (0, 1) and assume ri k ≤ B, π is Lp-Lipschitz, and ri is Lr-Lipschitz in Ai. If player i plays according to C.GP-MW using Strategy 2 with λ ≥ 1, βt = B +

σλ−1/2 2(γt−1 + log(2/δ)), ηt = 2 log K/

t τ

=1

1{zτ

=

zt},

and

=

(Lr

Lp

)−

2 c+2

T

−

1 c+2

,

then with probability at least 1 − δ,

Rci (T )

≤

c
2(LrLp) c+2

T

c+1 c+2

log K +

0.5T log(2/δ) + 4βT

γT λT .

Compared to Section 3.1, the obtained regret bound is now independent of the size of Z√, although its sublinear dependence on T degrades with the contexts’ dimension c. The additive O(βT γT T ) term represents the cost of learning the reward function r(·) online. Note that even if r(·) was known, and
c+1
hence full-information feedback was available, the O( T |Z|) and O(T c+2 ) rates obtained so far are shown optimal in their respective settings, i.e., when Z is ﬁnite [8] or when the discussed Lipschitz assumptions are satisﬁed [20]. An interesting future direction is to understand whether more reﬁned bounds can be derived as a function of the contexts’ sequence using adaptive partitions as proposed by [36]. In the next section we show that signiﬁcantly improved guarantees are achievable when contexts are i.i.d. samples from a static distribution.
Finally, we remark that all the discussed computations are efﬁcient, as they do not iterate over the set of policies Πi (which has exponential size). Improved regret bounds can be obtained if this requirement is relaxed, e.g., assuming a ﬁnite pool of policies [3], or a value optimization oracle [39]. We believe such results are complementary to our work and can be coupled with our RKHS game assumptions.

5

3.3 Stochastic contexts and non-reactive opponents
In this section, we consider a special case where contexts are i.i.d. samples from a static distribution ζ, i.e., zt ∼ ζ for t = 1, . . . , T . Importantly, we consider the realistic case in which player i does neither know, nor can sample from, such distribution. Moreover, we focus on the setting where the opponents’ decisions a−t i are not based on the current realization of zt, but can only depend on the history of the game. Examples of such a setting are games where the context zt represents ‘private’ information for player i (e.g., in Bayesian games, zt can represent player i’s type [19]), or where zt is only relevant to player i and hence the opponents have no reason to decide based on it.
In this case, we analyze the following strategy to compute pt(zt) at each round (Line 3 in Algorithm 1):

t−1

pt(zt)[a] ∝ exp ηt · ucbτ (a, a−τ i, zt)

a = 1. . . . , K .

(5)

τ =1

Crucially, the distribution pt(zt) is now computed using the whole sequence of past ucbτ functions, evaluated at context zt, regardless of whether zt was observed in the past. Hence, while according to rule (4) – and most of the MW algorithms – pt(zt) can be updated in a recursive manner, using rule (5) such distribution is re-computed at each round after observing zt (this requires storing the previous ucbτ functions, or re-computing them using (2) at each round).1 Such strategy exploits the stochastic assumption on the contexts and reduces the contextual game to a set of auxiliary games, one
for each context. This idea was recently used also by [5, 28] in the ﬁnite and linear contextual bandit
setting, respectively, while we specialize it to repeated games coupled with our RKHS assumptions.

The next theorem provides a pseudo-regret bound for C.GP-MW when using strategy (5), i.e.,

we

bound

the

quantity

ER

i c

(T

,

π

)

(expectation

with

respect

to

the

contexts’

sequence

and

the

randomization of C.GP-MW), where Rci (T, π) is the regret with respect to a generic policy π ∈ Π.

Note that the pseudo-regret is smaller than the expected contextual regret ERci (T ) which, however,

is proven to grow linearly with T when Z is sufﬁciently large [5]. Nevertheless, [5, Theorem 22]

shows that |ERci (T, π) − ERci (T )| can be bounded assuming each context occurs sufﬁciently often.

Theorem 2. Fix δ ∈ (0, 1) and assume ri k ≤ B and zt ∼ ζ for all t. Moreover, assume the

opponents cannot observe the current context zt. If player i plays according to C.GP-MW using

strategy (5) with λ ≥ 1, βt = B + σλ−1/2 2(γt−1 + log(1/δ)) and ηt = (8 log K)/T , then

with probability at least 1 − δ,

T

T

sup E r π(zt), a−t i, zt − r at, a−t i, zt ≤ 0.5T log K + 4βT γT λT ,

π∈Πi t=1

t=1

where expectation is with respect to both the contexts’ sequence and the randomization of C.GP-MW.

The above guarantee signiﬁcantly improves upon the ones obtained in the previous sections, as it does not depend on the context spa√ce Z, and matches the regret of GP-MW in non-contextual games. It should be compared with the O( T K log K) guara√ntee of [5] which assumes rewards for the nonrevealed contexts are also observed, and the bandit O( cT K log K) pseudo-regret of [28] assuming a linear dependence between contexts and rewards and a known contexts distribution. Exploiting our game assumptions, C.GP-MW’s performance decreases only logarithmically with K, relies on a more realistic feedback model than [5] and can deal with more complex rewards structures than [28].

4 Game Equilibria and Efﬁciency
In this section, we introduce new notions of equilibria and efﬁciency for contextual games. We recover game-theoretic learning results [18, 31] showing that equilibria end efﬁciency (as deﬁned below) can be approached when players minimize their contextual regret.
4.1 Contextual Coarse Correlated Equilibria
A typical solution concept of multi-player static games is the notion of Coarse Correlated Equilibria (CCEs) (see, e.g., [31, Section 3.1]). CCEs include Nash equilibria and have received increased attention because of their amenability to learning: a fundamental result from [18] shows that CCEs can be approached by decentralized no-regret dynamics, i.e., when each player uses a no-regret algorithm. These results, however, are not applicable to contextual games, where suitable notions of
1We note that strategy (5) can be implemented recursively in case the contexts’ set Z is known and ﬁnite.

6

equilibria should capture the fact that players can observe the current context before playing. To cope with this, we deﬁne a notion of CCEs for contextual games, denoted as contextual CCE (c-CCE).
Deﬁnition 3 (Contextual CCE). Consider a contextual game described by contexts z1, . . . , zT . Let Πi be the set of all policies π : Z → Ai for player i, and A be the joint space of actions a = (ai, a−i). A contextual coarse-correlated equilibrium (c-CCE) is a policy ρ : Z → ∆|A| such that:

1T

E ri a, zt

T

a∼ρ(zt )

t=1

1T

≥ T

E ri π(zt), a−i, zt
a∼ρ(zt )

t=1

∀π ∈ Πi, ∀i = 1, . . . , N . (6)

As opposed to CCEs (which are elements of ∆|A|), a c-CCE is a policy ρ : Z → ∆|A| from which
no player has incentive to deviate looking at the time-averaged expected reward. In other words,
suppose there is a trusted device that, for any context zt, samples a joint action from ρ(zt), where ρ is a c-CCE. Then, in expectation, each player is better off complying with such device, instead of using any other π : Z → Ai. We say that ρ is a -c-CCE if inequality (6) is satisﬁed up to a ∈ R+ accuracy. Finally, we remark that c-CCEs reduce to CCEs in case zt = z0 for all t.

Example (c-CCEs in trafﬁc routing) In trafﬁc routing applications, the trusted device can be a routing system (e.g., a maps server) which, given current weather, trafﬁc conditions, or other contextual information, decides on a route for each user. If such routes are sampled according to a c-CCE, then each user is better-off complying with such device to ensure a minimum expected travel time.

The next proposition shows that, similarly to CCEs in static games, c-CCEs can be approached
whenever players minimize their contextual regrets. Hence, it provides a fully decentralized and efﬁcient scheme for computing -c-CCEs. To do so, we deﬁne the notion of empirical policy at round T as follows. After T game rounds, let ZT be the set of all the distinct observed contexts. Then, the empirical policy at round T is the policy ρT : Z → ∆|A| such that, for each z ∈ ZT , ρT (z) is the empirical distribution of played actions when context z was revealed, while for the unseen contexts z ∈ Z \ ZT , ρT (z) is an arbitrary (e.g., uniform) distribution.
Proposition 4 (Finite-time approximation of c-CCEs). After T game rounds, let Rci (T )’s denote the players’ contextual regrets and ρT be the empirical policy at round T . Then, ρT is a -c-CCE of the played contextual game with ≤ maxi∈{1,...,N} Rci (T )/T .

Proposition 4 implies that, as T → ∞, if players use vanishing contextual regret algorithms (such as the ones discussed in Section 3), then the empirical policy ρT converges to a c-CCE of the contextual game. When contexts are stochastic, i.e., zt ∼ ζ for all t, an alternative notion of c-CCE can be deﬁned by considering the expected context realization. We treat this case in Appendix B.2 and
prove similar ﬁnite-time and asymptotic convergence results using standard concentration arguments.

4.2 Approximate Efﬁciency

The efﬁciency of an outcome a ∈ A in a non-contextual game, i.e., for a ﬁxed context z0, can

be quantiﬁed as the distance between the social welfare Γ(a, z0) :=

N i=1

ri(a,

z0)

(where

:=

is

sometimes replaced by ≥ if the reward of the game authority is also considered), and the optimal

welfare maxa Γ(a, z0). Optimal welfare is typically not achieved as players are self-interested agents

aiming at maximizing their individual rewards, instead of Γ. Nevertheless, main results of [6, 31]

show that such efﬁciency loss can be bounded whenever players minimize their regrets and the game

is

(λ, µ)-smooth,

i.e.,

if

for

any

pair

of

outcomes

a1

=

(

a11

,

.

.

.

,

a

N 1

)

and

a2

=

(

a12

,

.

.

.

,

a

N 2

)

,

it

satisﬁes:

N

r

i

(a

i 2

,

a−1

i

,

z0

)

≥

λ · Γ(a2, z0)

− µ · Γ(a1, z0) .

(7)

i=1
Examples of smooth games are routing games with polynomial delay functions, several classes of

auctions, submodular welfare games, and many more (see, e.g., [40, 31, 33] and references therein).

In contextual games, on the other hand, a different context zt describes the game at each round, and hence the social welfare Γ(a, zt) of an outcome a ∈ A depends on the speciﬁc context realization. The efﬁciency of a contextual game can therefore be quantiﬁed by the optimal contextual welfare:

Deﬁnition 5 (Optimal contextual welfare).

1T

OPT =

max

Γ π1(zt), . . . , πN (zt), zt .

(8)

π1∈Π1,...,πN ∈ΠN T

t=1

7

Figure 1: Time-averaged losses (Left) and network congestion (Right), when agents use different routing strategies (average over 5 runs). C.GP-MW leads to reduced losses and congestion compared to the other baselines.

Equation (8) generalizes the optimal welfare for non-contextual games, and sets the stronger benchmark of ﬁnding the policies (instead of static actions) mapping contexts to actions which maximize the time-averaged social welfare. In routing games, for instance, it corresponds to ﬁnding the best routes for the agents as a function of current trafﬁc conditions. As shown in our experiments (Section 5), such policies can signiﬁcantly reduce the network’s congestion compared to ﬁnding the best static routes. The next proposition generalizes the well-known results of [31], showing that in a smooth contextual game such optimal welfare can be approached when players minimize their contextual regret. First, we note that a contextual game can satisfy the smoothness condition (7) for different constants λ, µ, depending on the context zt, and hence we use the notation λ(zt), µ(zt) to highlight their dependence.
Proposition 6 (Convergence to approximate efﬁciency). Let Rci (T )’s be the players’ contextual regrets, and assume the game is (λ(zt), µ(zt))-smooth at each time t. Then,

1 T 1 N λ¯

1 N Rci (T )

T

Γ at , . . . , at , zt

≥

OPT −

1 + µ¯

1 + µ¯

, T

t=1

i=1

where λ¯ = maxt∈{1,...,T } λ(zt) and µ¯ = mint∈{1,...,T } µ(zt).

The approximation factor λ¯/(1 + µ¯) (also known as Price of Total Anarchy [6]) depends on the constants λ¯ and µ¯, which in our case represent the ‘worst-case’ smoothness of the game. We remark however that game smoothness is not necessarily context-dependent, e.g., routing games (such as the one considered in the next section) are smooth regardless of the network’s size and capacities [31].

5 Experiments - Contextual Trafﬁc Routing Game

We consider a contextual routing game on the trafﬁc network of Sioux-Falls, a directed graph with

24 nodes and 76 edges, with the same game setup of [32] (network data and congestion model are

taken from [25], while Appendix C gives a complete description of our experimental setup). There

are N = 528 agents in the network. Each agent wants to send di units from a given origin to a

given destination node in minimum time, and can choose among K = 5 routes at each round. The

traveltime of an agent depends on the routes chosen by the other agents (if all choose the same route

the network becomes highly congested) as well as the network’s capacity at round t. We let xit ∈ R76 represent the route chosen by agent i at round t, where xit[e] = di if edge e belongs to such route, and xit[e] = 0 otherwise. Moreover, we let context zt ∈ R7+6 represent the capacity of the network’s edges at round t (capacities are i.i.d. samples from a ﬁxed distribution, see Appendix C). Then, agents’

rewards can be written as ri(xit, x−t i, zt) = −

76 e=1

xit[e]·te(xit +x−t i,

zt[e]),

where

x−t i

=

j=i xjt

and te(·)’s are the edges’ traveltime functions, which are unknown to the agents. Note that, strictly

speaking, agent i’s reward depends only on the entries xit[e], x−t i[e], zt[e] for e ∈ Ei, where Ei is the subset of edges that agent i could potentially traverse. According to our model, we assume agent i

observes context {zt[e], e ∈ Ei} and, at end of each round, the edges’ occupancies {x−t i[e], e ∈ Ei}.

We let each agent select routes according to C.GP-MW (using rule (4) or (5)) and compare its performance with the following baselines: 1) No-Learning, i.e., agents select the shortest free-ﬂow routes at each round, 2) GP-MW [32] which neglects the observed contexts, and 3) ROBUSTLINEXP3 [28] for contextual linear bandits, which is robust to model misspeciﬁcation but does not exploit the correlation in the game (also, it requires knowing the contexts’ distribution). To run C.GP-MW we use the composite kernel k(xit, x−t i, zt) = k1(xit) ∗ k2((xit + x−t i)/zt), while for GP-MW the kernel k(xit, x−t i, zt) = k1(xit) ∗ k2(xit + x−t i), where k1, k2 are linear and polynomial kernels respectively.

8

We set ηt according to Theorems 1 and 2, and βt = 2.0 (theoretical values for βt are found to be overly conservative [37, 32]). For rule (4) we set = 30|Ei|. Figure 1 shows the time-averaged losses (i.e., traveltimes scaled in [0, 1] and averaged over all the agents), which are inversely proportional to the game welfare, and the resulting network’s congestion (computed as in Appendix C). We observe, in line to what discussed in Section 4, that minimizing individual regrets the agents increase the game welfare (this is expected as routing games are smooth [30]). Moreover, when using C.GP-MW agents exploit the observed contexts and correlations, and achieve signiﬁcantly more efﬁcient outcomes and lower congestion levels compared to the other baselines. We also observe strategy (5) outperforms strategy (4) in our experiments. This can be explained by the contexts being stochastic and, also, since each agent i is only inﬂuenced by the coordinates zt[e] of the relevant edges e ∈ Ei.
6 Conclusions
We have introduced the class of contextual games, a type of repeated games described by contextual information at each round. Using kernel-based regularity assumptions, we modeled the correlation between different contexts and game outcomes, and proposed novel online algorithms that exploit such correlations to minimize the players’ contextual regret. We deﬁned the new notions of contextual Coarse Correlated Equilibria and optimal contextual welfare and showed that these can be approached when players have vanishing contextual regret. The obtained results were validated in a trafﬁc routing experiment, where our algorithms led to reduced travel times and more efﬁcient outcomes compared to other baselines that do not exploit the observed contexts or the correlation present in the game.
Broader Impact
As systems using machine learning get deployed more and more widely, these systems increasingly interact with each other. Examples range from road trafﬁc over auctions and ﬁnancial markets, to robotic systems. Understanding these interactions and their effects for individual participants and the reliability of the overall system becomes ever more important. We believe our work contributes positively to this challenge by studying principled algorithms that are efﬁcient, while converging to suitable, and often efﬁcient, equilibria.
Acknowledgments
This work was gratefully supported by the Swiss National Science Foundation, under the grant SNSF 200021_172781, by the European Union’s ERC grant 815943, and ETH Zürich Postdoctoral Fellowship 19-2 FEL-47.
References
[1] Transportation Network Test Problems. http://www.bgu.ac.il/ bargera/tntp/.
[2] Yasin Abbasi-Yadkori. Online learning for linearly parametrized control problems. 2013.
[3] Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 2003.
[4] David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning (ICML), 2018.
[5] Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, and Jon Schneider. Contextual bandits with cross-learning. In Advances in Neural Information Processing Systems (NeurIPS). 2019.
[6] Avrim Blum, Mohammad Taghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and the price of total anarchy. In Annual ACM Symposium on Theory of Computing (STOC), 2008.
[7] Michael Bowling. Convergence and no-regret in multiagent learning. In Advances in Neural Information Processing Systems (NeurIPS). 2005.
[8] S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. 2012.
9

[9] Lucian Bus¸oniu, Robert Babuška, and Bart De Schutter. Multi-agent Reinforcement Learning: An Overview. Springer Berlin Heidelberg, 2010.
[10] Adrian Rivera Cardoso, Jacob Abernethy, He Wang, and Huan Xu. Competing against Nash equilibria in adversarially changing zero-sum games. In International Conference on Machine Learning (ICML), 2019.
[11] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
[12] Nando de Freitas, Alex Smola, and Masrour Zoghi. Regret bounds for deterministic Gaussian process bandits. arXiv preprint arXiv:1203.2177, 2012.
[13] Liam M. Dermed and Charles L. Isbell. Solving stochastic games. In Advances in Neural Information Processing Systems (NeurIPS). 2009.
[14] Benoit Duvocelle, Panayotis Mertikopoulos, Mathias Staudigl, and Dries Vermeulen. Learning in time-varying games. arXiv preprint arXiv:1809.03066, 2018.
[15] Dylan J. Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Éva Tardos. Learning in games: Robustness of fast convergence. In Advances in Neural Information Processing Systems (NeurIPS), 2016.
[16] Amy Greenwald and Keith Hall. Correlated-Q Learning. In International Conference on Machine Learning (ICML), 2003.
[17] James Hannan. Approximation to Bayes risk in repeated play. Princeton University Press, 1957.
[18] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 2000.
[19] Jason Hartline, Vasilis Syrgkanis, and Éva Tardos. No-Regret Learning in Bayesian Games. In Advances in Neural Information Processing Systems (NeurIPS), 2015.
[20] Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In Annual Conference on Learning Theory (COLT), 2007.
[21] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 1963.
[22] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2011.
[23] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity search. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2004.
[24] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in Neural Information Processing Systems (NeurIPS). 2008.
[25] Larry J. LeBlanc, Edward K. Morlok, and William P. Pierskalla. An efﬁcient approach to solving the road network equilibrium trafﬁc assignment problem. In Transportation Research Vol. 9, 1975.
[26] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and Computation, 1994.
[27] Jaouad Mourtada and Stéphane Gaïffas. On the optimality of the hedge algorithm in the stochastic regime. Journal of Machine Learning Research, 2019.
[28] Gergely Neu and Julia Olkhovskaya. Efﬁcient and robust algorithms for adversarial linear contextual bandits. arXiv preprint arXiv:2002.00287, 2020.
[29] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.
[30] Tim Roughgarden. Routing Games. Cambridge University Press, 2007.
[31] Tim Roughgarden. Intrinsic robustness of the price of anarchy. Journal of the ACM, 2015.
[32] Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, and Andreas Krause. No-regret learning in unknown games with correlated payoffs. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
10

[33] Pier Giuseppe Sessa, Maryam Kamgarpour, and Andreas Krause. Bounding inefﬁciency of equilibria in continuous actions games using submodularity and curvature. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2019.
[34] L. S. Shapley. Stochastic games. Proceedings of the National Academy of Sciences, 1953. [35] Satinder P. Singh, Michael J. Kearns, and Yishay Mansour. Nash convergence of gradient
dynamics in general-sum games. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2000. [36] Aleksandrs Slivkins. Contextual bandits with similarity information. volume 19 of Proceedings of Machine Learning Research, pages 679–702, 2011. [37] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine Learning (ICML), 2010. [38] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in games. In Advances in Neural Information Processing Systems (NeurIPS), 2015. [39] Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Processing Systems (NeurIPS). 2016. [40] Vasilis Syrgkanis and Eva Tardos. Composable and efﬁcient mechanisms. In Annual ACM Symposium on Theory of Computing (STOC), 2013.
11

Supplementary Material
Contextual Games: Multi-Agent Learning with Side Information
Pier Giuseppe Sessa, Ilija Bogunovic, Andreas Krause, Maryam Kamgarpour (NeurIPS 2020)

A Supplementary Material for Section 3

The theoretical guarantees obtained in Section 3 rely on the following two main lemmas. The ﬁrst lemma is from [2] and shows that given the previously observed rewards, contexts, and players’ actions, the reward function of player i belongs (with high probability) to the interval [µt(·, ·) ± βtσt(·, ·)], for a carefully chosen conﬁdence parameter βt ≥ 0.
Lemma 7. Let r ∈ Hk such that r k ≤ B and consider the kernel-ridge regression mean and standard deviation estimates µt(·) and σt(·), with regularization constant λ > 0. Then for any δ ∈ (0, 1), with probability at least 1 − δ, the following holds simultaneously over all x ∈ D and t ≥ 1:
|µt(x) − r(x)| ≤ βtσt(x),

where βt = Bλ−1/2 + σλ−1

2

log

(

1 δ

)

+

log(det(It

+

Kt/λ)).

Therefore, according to Lemma 7, the function ucbt deﬁned in (3) represents a valid upper conﬁdence bound for the rewards obtained by player i.
The second main lemma concerns the properties of the Multiplicative Weights (MW) update method [26], which is used as a subroutine in our algorithms to compute the action distribution pt(zt) (Line 3 of Algorithm 1) at each round. Its proof follows from standard online learning arguments equivalently to, e.g., [27, Proposition 1].
Lemma 8. Consider a sequence of functions g1(·), . . . , gT (·) ∈ [0, 1]K and let pt’s be the distributions computed using the MW rule:

t−1

pt[a] ∝ exp ηt · gτ (a)

a = 1, . . . , K ,

(9)

τ =1

where p1 is initialized as the uniform distribution. Then, provided that {ηt}Tt=1 is a decreasing sequence, for any action a ∈ {1, . . . , K}:

T

TK

log K

gt(a ) −

pt[a] · gt(a) ≤ ηT +

t=1

t=1 a=1

Tt=1 ηt . 8

A.1 The case of a ﬁnite (and small) number of contexts

In this section we consider the simple case of a ﬁnite (and small cardin√ality) set of contexts Z. In such a case, a high-probability regret bound of O( T |Z| log K + γT T ) can be achieved when C.GP-MW is run with the following strategy:

pt(zt)[a] ∝ exp

t−1
ηt · ucbτ (a, a−τ i, zτ ) · 1{zτ = zt}
τ =1

a = 1, . . . , K . (10)

That is, p(zt) is computed using the sequence of previously computed upper conﬁdence bound functions for the game rounds in which the speciﬁc context zt was revealed.

Theorem 9. Fix δ ∈ (0, 1) and assume ri ki ≤ B. If player i plays according to C.GPMW using strategy (10) with λ ≥ 1, βt = B + σλ−1/2 2(γt−1 + log(2/δ)), and ηt =

2 log K/

t τ

=1

1{zτ

=

zt},

then

with

probability

at

least

1

−

δ,

Rci (T ) ≤ T |Z| log K + 0.5T log(2/δ) + 4βT γT λT .

12

Proof. Let π = arg maxπ∈Πi Tt=1 r π(zt), a−t i, zt . Our goal is to bound Rci (T ) = Tt=1 r π (zt), a−t i, zt − r ait, a−t i, zt with high probability.
By conditioning on the event of the conﬁdence lemma (Lemma 7) holding true, we can state that, with probability at least 1 − δ/2,

T
Rci (T ) = r π (zt), a−t i, zt − r ait, a−t i, zt

t=1

T
≤

ucbt π (zt), a−t i, zt − ucbt ait, a−t i, zt

T
+ 2βtσt ait, a−t i, zt

t=1

t=1

T

≤

ucbt π (zt), a−t i, zt − ucbt ait, a−t i, zt +4βT γT λT .

(11)

t=1

Rˆi (T )
c

The ﬁrst inequality follows by the deﬁnition of ucbt(·) (see (3)), the speciﬁc choice of the conﬁdence level βt, and Lemma 7. The last inequality follows by [37, Lemma 5.4]. The rest of the proof proceeds to show that, with probability at least 1 − δ/2,

Rˆci (T ) ≤ T |Z| log K + 0.5T log(2/δ) .

(12)

The theorem statement then follows by a standard union bound argument.
First, by straightforward application of the Hoeffding–Azuma inequality (e.g., [11, Lemma A.7]), it follows that with probability at least 1 − δ/2,

T

ucbt ait, a−t i, zt −

pt(zt)[a] · ucbt a, a−t i, zt ≤ 0.5T log(2/δ) ,

(13)

t=1

a∈Ai

Xt

since the variables Xt’s form a martingale difference sequence, being a∈Ai pt(zt)[a] ·

ucbt a, a−t i, zt

the

expected

value

of

ucbt

a

i t

,

a−t

i

,

zt

conditioned on the history

{aiτ , a−τ i, zτ , τ }tτ−=11 and on context zt. Then, using (13), Rˆci (T ) can be bounded, with

probability 1 − δ/2, as

T
Rˆci (T ) ≤ ucbt π (zt), a−t i, zt −

pt(zt)[a] · ucbt a, a−t i, zt +

t=1

a∈Ai

0.5T log(2/δ)

=

ucbt π (zc), a−t i, zc −

pt(zc)[a] · ucbt a, a−t i, zc +

z∈Z t:zt=z

a∈Ai

0.5T log(2/δ) . (14)

At this point, we can use the properties of the MW rule used to compute the distribution pt(z) ∈ ∆K.

Note that, for each context z ∈ Z, the distribution pt(z) computed by C.GP-MW precisely follows

the MW rule (9) with the sequence of functions {ucbτ (·, a−τ i, z)}τ:zτ =z and the sequence of learning

rates

{η

τ

}

Tz τ =1

=

{2

log

K

/τ

}

Tz τ =1

,

where

Tz

=

t τ

=1

1{zτ

=

zt}

is

the

number

of

times

context

z was revealed. Hence, we can apply Lemma 8 for each context z and obtain:

ucbt π (zc), a−t i, zc −

pt(zc)[a] · ucbt a, a−t i, zc ≤

t:zt =z

a∈Ai

2√log K Tz 1

≤ 0.5 Tz log K + 8 √τ

√

τ =1

2 log K

≤ 0.5 Tz log K + 8 2 Tz = Tz log K . (15)

13

Equation (15), together with the bound (14) leads to

Rˆci (T ) ≤
z∈Z

Tz log K +

0.5T log(2/δ)

≤ T |Z| log K + 0.5T log(2/δ) ,

where in the last inequality we have used Cauchy–Schwarz inequality and z∈Z Tz = T . Hence, we ﬁnally proved (12). Therefore, with probability at least 1 − δ/2 − δ/2 = 1 − δ we obtain the ﬁnal
regret bound combining (11) and (12).

A.2 Proof of Theorem 1

Proof. Similarly to Appendix A.1, we let π = arg maxπ∈Πi Tt=1 r π(zt), a−t i, zt and seek to bound Rci (T ) = Tt=1 r π (zt), a−t i, zt − r ait, a−t i, zt with high probability. Recall that Strategy 2 builds an -net of the contexts space by creating new L1-balls in a greedy fashion. After T
game rounds, the set C contains the centers z ∈ Z of the balls created so far. Moreover, at each round
t, the variable zt indicates the ball that context zt has been associated to. According to this notation, player i’s regret can be rewritten as

Rci (T ) =

r π (zt), a−t i, zt − r ait, a−t i, zt

z∈C t:zt=z

=

r π (zt), a−t i, zt − r π (z), a−t i, zt +

r π (z), a−t i, zt − r ait, a−t i, zt ,

z∈C t:zt=z

z∈C t:zt=z

RL(T )

RC (T )

where in the last equality we have added and subtracted the term z∈C t:z =z r π (z), a−t i, zt . t
The regret term RL(T ) can be bounded using Lr-Lipschizness of r(·) in its ﬁrst argument and Lp-Lipschizness of the optimal policy:

RL(T ) ≤

Lr π (zt) − π (z) 1 ≤

LrLp zt − z 1

z∈C t:zt=z

z∈C t:zt=z

≤ LrLpT ,

where in the last step we have used that, when zt = z, zt belongs to the ball centered at z.
We can now proceed in bounding RC(T ). Note that we can apply the same proof steps of Appendix A.1 (namely, Equations (11) and (13)) to show that, with probability at least 1 − δ, we have:

RC (T ) ≤

ucbt π (zt), a−t i, zt −

pt(zt)[a] · ucbt a, a−t i, zt

z∈C t:zt=z

a∈Ai

+ 4βT γT λT + 0.5T log(2/δ) .

where we have conditioned on the event of Lemma 7 and applied the Hoeffding-Azuma inequality.

At this point, we can use the properties of the MW rule used in Strategy 2 to compute pt(zt) at each

round. Note that Strategy 2 corresponds to maintaining a distribution for each z ∈ C and update it

when context zt belongs to such ball. After T rounds, for each z ∈ Z let Tz =

T t=1

1{zt

=

z}

be

the number of times the revealed context belonged to the ball centered at z. Hence, for each z ∈ C, a

straightforward application of Lemma 8 leads to:

ucbt π (zt), a−t i, zt −

pt(zt)[a] · ucbt a, a−t i, zt ≤

t:zt =z

a∈Ai

≤ 0.5

2√log K Tz 1 log KTz + 8 √τ
τ =1

14

≤ 0.5

√

2 log K

log KTz +

2 8

Tz =

Tz log K .

where we have used the same steps to obtain Equation (15) in Appendix A.1. Hence, summing over all the contexts in C we obtain

RC (T ) ≤

Tc log K + 4βT γT λT + 0.5T log(1/δ2)

z∈C

≤ T |C| log K + 4βT γT λT + 0.5T log(1/δ2) = −c/2 T log K + 4βT γT λT + 0.5T log(1/δ2) .

In the last inequality we have used |C| ≤ (1/ )c, because the contexts space Z ⊆ [0, 1]c can be covered by at most (1/ )c balls of radius such that the distance between their centers is at least .

Therefore, combining the bounds for RL(T ) and RC(T ), the contextual regret of player i is bounded, with probability at least 1 − δ, by:

Rci (T ) ≤ LrLpT + −c/2 T log K + 4βT γT λT + 0.5T log(2/δ)

c

c+1

c

c+1

= (LrLp) c+2 T c+2 + (LrLp) c+2 T c+2 log K + 4βT γT λT +

0.5T log(2/δ) .

where we have substituted the choice of

=

(Lr

Lp

)−

c

2 +2

T

− c+1 2

.

A.3 Proof of Theorem 2
Proof. We let Rci (π, T ) = Tt=1 r π(zt), a−t i, zt − r ait, a−t i, zt be the regret of player i with respect to a generic policy π : Z → Ai. Our goal is to bound Rci (π, T ) in expectation, with respect to the random sequence of contexts and played actions. For ease of exposition, we use the notation v···T to indicate the sequence of variables v1, . . . , vT . Moreover, we will explicitly consider an adaptive adversary that selects a−t i as a function a−t i = f (Ht−1) of the history Ht−1 := {ai···t−1, z···t−1} but not of zt.
First, note that the expected value of Rci (π, T ) is still a random variable which depends on the realization of the observation noise t’s. As it was done in proof of Theorems 1 and Appendix A.1, we can condition on the event of the conﬁdence Lemma 7, and state that, with probability at least 1 − δ/2, it can be bounded by

Ez···T
ai···T

Rci (π, T )

= Ez···T
ai···T

≤ Ez···T
ai···T

≤ Ez···T
ai···T

T
r π(zt), a−t i, zt − r ait, a−t i, zt
t=1
T
ucbt π(zt), a−t i, zt − ucbt ait, a−t i, zt
t=1
T
ucbt π(zt), a−t i, zt − ucbt ait, a−t i, zt
t=1

T
+ 2βtσt ait, a−t i, zt
t=1
+4βT γT λT , (16)

Rˆi (π,T )
c

where we have used the deﬁnition of ucbt(·), βt set according to Lemma 7, and [37, Lemma 5.4].
Now, we consider a generic sequence { t}Tt=1 of noise realizations and proceed bounding the expected value of Rˆci (π, T ) for any of such sequences. Moreover, as in [28] we will make use of a ghost sample z0 ∼ ζ which is sampled from the contexts’ distribution independently from the whole history HT of the game. Also, we now explicitly consider the adaptiveness of the adversary. Using the law of total expectation, the expected value of Rˆci (π, T ) can be rewritten as

15

Ez···T
ai···T

Rˆci (π, T )

= Ez···T
ai···T

T
ucbt π(zt), f (Ht−1), zt − ucbt ait, f (Ht−1), zt
t=1

= Ez···T
ai···T

T
Ezt ,ait
t=1

ucbt π(zt), f (Ht−1), zt

− ucbt

a

i t

,

f

(H

t

−1

),

zt

| Ht−1

T

= Ez···T
ai···T

Ezt ucbt π(zt), f (Ht−1), zt −

pt(zt)[a] · ucbt a, f (Ht−1), zt | Ht−1

t=1

a∈Ai

T

= Ez···T
ai···T

Ez0 ucbt π(z0), f (Ht−1), z0 −

pt(z0)[a] · ucbt a, f (Ht−1), z0 | Ht−1

t=1

a∈Ai

T

= Ez···T

ucbt π(z0), f (Ht−1), z0 −

pt(z0)[a] · ucbt a, f (Ht−1), z0

(17)

ai···T z0

t=1

a∈Ai

The second equality follows by the law of total expectation. The third equality holds since, conditioned on the history Ht−1, ait is distributed according to pt(zt). The fourth equality follows since zt and z0 have the same distribution and the functions ucbt(·) and pt(·) do not depend on the realization of zt. The last inequality is obtained by applying again the law of total expectation.
At this point, we can apply Lemma 8 considering the sequence of functions g1, . . . , gT with gτ (·) = ucbτ (·, f (Hτ−1), z0) for τ = 1, . . . , T and noting that, for each z0, pt(z0) computed using the MW rule (5) corresponds to the distribution computed according to rule (9) for each t = 1, . . . , T . Therefore, (17) implies that

Ez···T

Rˆi (π, T )

log K

≤

+

η ai···T c T

Tt=1 ηt . 8

Finally, the theorem statement is obtained substituting the bound above in (16) and considering the constant learning rate ηt = 8 log(K)/T .

B Supplementary Material for Section 4

B.1 Proof of Proposition 4 (Finite-time approximation of c-CCEs)

Proof. After T rounds of the contextual game, consider a generic player i. By deﬁnition of contextual regret, see (1), we have

1T i

−i

1T i

−i

Rci (T )

i

T r (at, at , zt) ≥ T r (π(zt), at , zt) − T

∀π ∈ Π .

(18)

t=1

t=1

Let now ρT be the empirical policy up to time T , deﬁned as in Section 4.1. Then, it is not hard to verify that the above cumulative rewards for player i can be written as

1T i

−i

1T

T r (at, at , zt) = T

E r(a, zt) ,
a∼ρ (z )

t=1

t=1

Tt

1T i

−i

1T

−i

T r (π(z), at , zt) = T

E r(π(z), a , zt) .
a∼ρ (z )

t=1

t=1

Tt

Therefore, (18) becomes:

1T

1T

−i

Rci (T )

i

E
(z

r(a, zt)
)

≥

T

E r(π(z), a , zt) −
a∼ρ (z )

T

∀π ∈ Π . (19)

T t=1 a∼ρT t

t=1

Tt

16

Note that this is precisely the condition of -c-CCE (see Deﬁnition 3) for player i. The ﬁnal result is then simply obtained by considering the player with the highest regret.

B.2 An alternative notion of c-CCE for stochastic contexts

In Section 4 we deﬁned the notion of c-CCE (Deﬁnition 3) for a contextual game described by an arbitrary sequence of contexts z1, . . . , zT . In this section, we consider the case in which contexts are stochastic samples from the same distribution ζ, i.e., zt ∼ ζ for all t. In such a case, the following alternative notion of c-CCE can be deﬁned by considering the expected context realization (rather than considering the time-averaged game as in Deﬁnition 3).
Deﬁnition 10. Consider a contextual game and assume contexts are sampled i.i.d. from distribution ζ. A contextual coarse-correlated equilibrium for stochastic contexts (c-ζ-CCE) is a policy ρ : Z → ∆|A| mapping contexts to distributions over A such that:

E E ri a, z ≥ E E ri π(z), a−i, z ∀π ∈ Πi, ∀i = 1, . . . , N . (20)

z∼ζ a∼ρ(z)

z∼ζ a∼ρ(z)

Moreover, ρ is an -c-ζ-CCE if the above inequality is satisﬁed up to an ∈ R+ accuracy.

Similarly to Proposition 4, the following proposition shows that, in this speciﬁc setting, c-ζ-CCEs can also be approached whenever players minimize their contextual regrets.
Proposition 11 (Asymptotic and ﬁnite-time convergence to c-ζ-CCEs). Consider a contextual game and assume contexts are sampled i.i.d. from distribution ζ. Let ρT be the empirical policy at round T . Then, as T → ∞, if players have vanishing contextual regrets, ρT converges to a c-ζ-CCE almost surely. Moreover, after T game rounds, let Rci (T )’s denote the players’ contextual regrets, δ ∈ (0, 1), and assume Z is ﬁnite. Then, with probability at least 1 − δ, ρT is a -c-ζ-CCE with

≤ 2 log(|Z| · |A|) + log(2/δ) + max Rci (T ) .

2

2T

i∈{1,...,N } T

Compared to CCEs (and c-CCEs), c-ζ-CCEs can be approximated in ﬁnite time only with highprobability and with an extra approximation factor of O(log(|Z| |A|) + log(1/δ)/T ). Intuitively, this is because the empirical distribution of observed contexts needs to concentrate around the true contexts distribution ζ. We recover asymptotic convergence to c-ζ-CCEs since such distribution converges to ζ with probability 1.

Proof. By deﬁnition of contextual regret, see (1), for each player i

1T i

−i

1T i

−i

Rci (T )

i

T r (at, at , zt) ≥ T r (π(zt), at , zt) − T

∀π ∈ Π .

(21)

t=1

t=1

Let ζT be the empirical distribution of observed contexts. Moreover, let ρT be the empirical policy up to time T , deﬁned in Section 4.1. Then, following the same steps of Proof of Proposition 4:

1T i

−i

T t=1 r (at, at , zt) = z∼EζT a∼ρET (z) r(a, z) ,

1T i

−i

−i

T t=1 r (π(z), at , zt) = z∼EζT a∼ρET (z) r(π(z), a , z) .

Therefore, (21) rewrites as

−i

Rci (T )

i

E E r(a, z) ≥ E E r(π(z), a , z) −

ζ a∼ρ (z)

z∼ζ a∼ρ (z)

T

∀π ∈ Π . (22)

z∼ T

T

T

T

As T → ∞, ζT → ζ as contexts are i.i.d. samples from ζ. Moreover, if players use no-regret strategies, Rci (T )/T → 0 for i = 1, . . . N and hence the above inequality implies that ρT converges to a c-ζ-CCE (see Deﬁnition 10).

17

For ﬁnite T , the above inequality resembles the desired c-ζ-CCE condition, with the difference that the outer expectations are taken with respect to the empirical contexts’ distribution ζT instead of the true one. To cope with this, we show that such expectations indeed concentrate, up to some accuracy, around the expectations with respect to the true distribution ζ. More precisely, we show that with probability at least 1 − δ

log(|Z| · |A|) log(2/δ)

E f (z, ρT ) − E f (z, ρT ) ≤

z∼ζ

z∼ζ

+

,

2

2T

(23)

T

where f (z, ρT ) = Ea∼ρT (z) r(a, z). Moreover, the same condition holds for f (z, ρT ) = Ea∼ρT (z) r(π(z), a−i, z) for each π ∈ Πi. Combined with (22), this implies that for each player i and each π ∈ Πi, with probability 1 − δ,

E E r(a, z) ≥ E E r(π(z), a−i, z) − 2

z∼ζ a∼ρT (z)

z∼ζ a∼ρT (z)

log(|Z| · |A|) + log(2/δ) − Rci (T ) ,

2

2T

T

which would prove Proposition 11.
It remains to show (23). For a given policy ρ : Z → ∆|A|, a straightforward application of Hoeffding’s inequality [21] shows that for any > 0

P E f (z, ρ) − E f (z, ρ) > ≤ 2 exp − 2T 2 ,

(24)

z∼ζT

z∼ζ

where we have used the fact that f (z, ρ) = Ea∼ρ(z)ri(a, z) ∈ [0, 1] and that z1, . . . , zT are i.i.d. sampled from ζ. Unfortunately, we cannot apply the condition above directly to the empirical policy
ρT , since it is not ﬁxed a-priori, but is computed as a function of the realized samples z1, . . . , zT . However, we can consider the set PT of all the possible empirical policies ρ : Z → ∆|A| resulting from T rounds of the repeated game. Note that each of such policies is uniquely deﬁned by the sequence {ait, a−t izt}Tt=1 of revealed contexts and actions played up to round T . Therefore, PT is a ﬁnite set of cardinality |PT | = (|Z| · |A|)T . Hence, it holds

P E f (z, ρT ) − E f (z, ρT ) >

z∼ζT

z∼ζ

≤ P sup E f (z, ρ) − E f (z, ρ) >

ρ∈PT z∼ζT

z∼ζ





= P

E f (z, ρ) − E f (z, ρ) > 

ρ∈PT z∼ζT

z∼ζ

≤ |PT | P E f (z, ρ) − E f (z, ρ) >

z∼ζT

z∼ζ

≤ 2|PT | exp − 2T 2 .

The ﬁrst equality holds since, given a set of random variables x1, . . . , xn, asking that supi xi > is equivalent to asking that at least one of the xi’s is greater than . The second inequality is a standard
probability union bound, while the last inequality follows by (24). This proves (23) after setting the right hand side equal to δ and substituting PT = (|Z| · |A|)T .

B.3 Proof of Proposition 6 (Convergence to approximate efﬁciency)

Proof. For ease of notation, let π1, . . . , πN be the optimal policies that solve (8), so that OPT =

1 T

Tt=1 Γ π1(zt), . . . , πN (zt), zt . Using the deﬁnitions of contextual regret and (λ, µ)-smoothness,

the sum of cumulative rewards can be lower bounded as:

1 T N i i −i

T

r (at, at , zt)

t=1 i=1

1T N i i

−i

≥ T

r π (zt), at , zt

t=1 i=1

− N Rci (T ) T
i=1

18

1T

≥

λ(zt) · Γ π1(zt), . . . , πN (zt), zt

T

t=1

≥ λ¯ · OPT − µ¯ · T1 T Γ(a1t , . . . , aNt , zt)
t=1

− µ(zt) · Γ(a1t , . . . , aNt , zt) − N Rci (T ) .
T
i=1

− N Rci (T ) T
i=1

In the ﬁrst inequality we have used the deﬁnition of contextual regret (see (1)) with respect to policy πi for each player (note that πi is not necessarily the optimal policy in hindsight for player i). In the
second inequality we have used the fact that the game is λ(zt), µ(zt) -smooth at each time t and applied condition (7) with outcomes a1 = (a1t , . . . , aNt ) and a2 = π1(zt), . . . , πN (zt) . The last inequality follows from the deﬁnition of λ¯, µ¯, and OPT.

At this point, note that T1

T t=1

Γ(a1t

,

.

.

.

,

aNt

,

zt

)

≥

1 T

T t=1

N i=1

ri(ait,

a−t i,

zt)

since

by

deﬁni-

tion of social welfare Γ(a, z) ≥ Ni=1 ri(a, z) for all (a, z). Then, the above inequalities imply

that

T1 T Γ(a1t , . . . , aNt , zt) ≥ λ¯ · OPT − µ¯ · T1 T Γ(a1t , . . . , aNt , zt)

t=1

t=1

− N Rci (T ) , T
i=1

which after rearranging yields the desired result.

C Contextual Trafﬁc Routing - Experimental Setup
In this section we describe the experimental setup of the contextual trafﬁc routing game of Section 5. We consider the trafﬁc network of Sioux-Falls, a directed graph with 24 nodes and 76 edges and use the game model of [32]. Data from [25, 1] include node coordinates and capacities Ce ∈ R+ of each network’s edge e = 1, . . . , 76. Moreover, data also include the units (e.g., cars) that need to be sent from any node to any other node in the network, for a total of 528 distinct origin-destination pairs. Hence, we let N = 528 be the number of agents in the network and assume, at every round, each agent i needs to send di units from origin node Oi to destination node Di. In order to send these units, each agent can choose one of the K = 5 shortest routes between Oi and Di. We let xit ∈ R76 represent the route chosen by agent i at round t, where xit[e] = di if edge e belongs to such route, and xit[e] = 0 otherwise. Moreover, we let x−t i = j=i xjt represent the routes chosen by the rest of the agents.
At each round, the network displays different capacities (network’s capacities represent the contextual information of the game) which are observed by the agents and should be used to choose better routes, depending on the circumstances. This is different from the game model of [32] where network capacities are assumed constant. We let the context vector zt ∈ R7+6 represent the network’s capacities at round t, and assume each zt is i.i.d. sampled from a static distribution ζ. The contexts distribution ζ is generated as follows. We let Z be a set of 10 randomly generated capacity proﬁles z where z[e] is uniformly distributed in [0, 1.2 · Ce] for e = 1, . . . , 76. Then, we let ζ be the uniform distribution over Z.
Given context zt and routes xit, x−t i, the reward of each agent i is:
76
ri(xit, x−t i, zt) = − xit[e] · te(xit + x−t i, zt[e]) ,
e=1
where te(·) is the traveltime function of edge e (i.e., the relation between number of units traversing edge e and the time needed to traverse it). Such functions are unknown to the agents, and according to [25, 1] are deﬁned by the Bureau of Public Roads (BPR) congestion model:
x4 te(x, z) = fe · 1 + 0.15 z ,
where fe ∈ R+ is the free-ﬂow traveltime of edge e (also provided by the network’s data). At the end of each round, hence, we quantify the congestion of each edge e with the quantity 0.15((xit[e] + x−t i[e])/zt[e])4.

19

To run our experiments, we estimate upper and lower bounds on the agents’ rewards by sampling

10 000 random contexts and game outcomes, and feed such bounds to the agents so that rewards

can be scaled in the [0, 1] interval. Moreover, at each round agents receive a noisy measurement

of their rewards, with noise standard deviation σ set to 0.1%. To run ROBUSTLINEXP3 [28,

Theorem 1] we set learning rate η = 0.3 and exploration parameter γ = 0.2 (we observe worse

performance when setting them to their theoretical values). For GP-MW we use the composite

kernel k(xit, x−t i, zt) = k1(xit) ∗ k2(xit + x−t i) used also in [32], while for C.GP-MW the kernel

k

(

x

i t

,

x−t

i

,

zt

)

=

k1(xit)

∗

k2((xit

+

x−t i)/zt),

where

k1

is

a

linear

kernel

and

k2

is

a

polynomial

kernel of degree 4. However, we observe similar performance when polynomials of different degrees

are used or when k2 is the widely used SE kernel. Kernel hyperparameters are optimized ofﬂine over

100 random datapoints and kept ﬁxed. We set ηt according to Theorems 1 and 2, and conﬁdence level

βt = 2.0 (theoretical values for βt are found to be overly conservative, as also observed in [37, 32]).

20

