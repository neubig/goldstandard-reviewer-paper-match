On Primal–Dual Approach for Distributed Stochastic Convex Optimization over Networks
Darina Dvinskikh, Eduard Gorbunov, Alexander Gasnikov, Pavel Dvurechensky, Ce´sar A. Uribe

arXiv:1903.09844v4 [math.OC] 26 Nov 2019

Abstract— We introduce a primal-dual stochastic gradient oracle method for distributed convex optimization problems over networks. We show that the proposed method is optimal in terms of communication steps. Additionally, we propose a new analysis method for the rate of convergence in terms of duality gap and probability of large deviations. This analysis is based on a new technique that allows to bound the distance between the iteration sequence and the optimal point. By the proper choice of batch size, we can guarantee that this distance equals (up to a constant) to the distance between the starting point and the solution.

I. INTRODUCTION

Distributed algorithms have been prevalent in the control theory and machine learning communities since early 70s and 80s [1]–[3]. The structural ﬂexibilities introduced by a networked structure has been particularly relevant for recent applications, such as robotics and resource allocation [4]–[8], where large quantities of data are involved, and generation and processing of information is not centralized [9]–[13].
A distributed system is usually modeled as a network of computing agents connected in a deﬁnite way. These agents can act as local processors or sensors, and have communication capabilities to exchange information with each other. Precisely, the communication between agents is subject to the constraints imposed by the network structure. The object of study of distributed optimization is then to design algorithms that can be locally executed by the agents, and that exploit the network communications to solve a network-wide global problem cooperatively [14], [15].
Formally, we consider the optimization problem of minimizing the ﬁnite sum of m convex functions

m

min f (x) := fi(x),

(1)

x∈Rn

i=1

where each agent i = {1, 2, . . . , m} in the network has access to the function fi only, and yet, we seek that every agent cooperatively achieves a solution of (1).

The work of D. Dvinskikh and P. Dvurechensky was funded by Russian Science Foundation (project 18-71-10108). The work of E. Gorbunov was supported by RFBR 18-31-20005 mol-a-ved. The work of A. Gasnikov was supported by RFBR 18-29-03071 mk.
D.D and P.D. are with the Weierstrass Institute for Applied Analysis and Stochastics, Germany, and the Institute for Information Transmission Problems, Russia ({darina.dvinskikh,pavel.dvurechensky}@wias-berlin.de). E.G. is with the Moscow Institute of Physics and Technology, Russia (eduard.gorbunov@phystech.edu). A.G. is with Moscow Institute of Physics and Technology, Institute for Information Transmission Problems, Russia and National Research University Higher School of Economics, Russia (gasnikov@yandex.ru). C.A.U. is with the the Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology, USA (cauribe@mit.edu).

In this paper, we consider the stochastic version of problem (1), when fi(x) = Ef˜i(x, ξ), and ξ is a random variable. We provide an accelerated dual gradient method for this stochastic problem and estimate the number of communication steps in the network and the number of stochastic oracle calls in order to obtain a solution with high probability.

Optimal methods for distributed optimization over networks were recently proposed and analyzed [16], [17]. However, there were only studied for deterministic settings. In [18], the authors studied a primal-dual method for stochastic problems. The setting of the latter paper is close to what we consider as the primal approach, but our algorithm and analysis are different, and, unlike [18], we consider smooth primal problem. Other approaches for distributed stochastic optimization has been studied in the literature [19], [20]. In contrast, we provide optimal communication complexities, as well as explicit dependency on the network topology. We want to mention that primal approaches were recently studied in [21], [22].

Notation: We deﬁne the maximum eigenvalue and minimal

non-zero eigenvalue of a symmetric matrix W as λmax(W ) and λ+min(W ) respectively, and deﬁne the condition number of matrix W as χ(W ). We denote by 1m the vector of ones in Rm. Denoting by · 2 the standard Euclidean norm, we

say that a function f is M -Lipschitz if ∇f (x) 2 ≤ M , a

function f is L-smooth if ∇f (x)−∇f (y) 2 ≤ L x−y 2, a

function f is µ-strongly convex (µ-s.c.) if, for all x, y ∈ Rn,

f (y) ≥ f (x)+

∇f (x), y−x

+

µ 2

x−y

22. Given β ∈ (0, 1),

we denote ρβ = 1 + ln(1/β) + ln(1/β).

II. DUAL DISTRIBUTED APPROACHES

In this section, we follow [16], [17], [23], [24] and use

primal-dual accelerated gradient methods [25]–[29], and use

a dual formulation of the distributed optimization problem

to design a class of optimal algorithms that can be executed

over a network. Consider a network of m agents whose

interactions are represented by a connected and undirected

graph G = (V, E) with the set V of m vertices and the

set of edges E = {(i, j) : i, j ∈ V }. Thus, agent i can

communicate with agent j if and only if (i, j) ∈ E. Assume

that each agent i has its own vector vector yi0 ∈ Rn, and

its goal is to ﬁnd an approximation to the vector y∗ =

1 m

m i=1

yi0

by

performing

communications

with

neighboring

agents. To do this, consider the Laplacian of the graph G, to

be deﬁned as a matrix W¯ with entries,



−1, if (i, j) ∈ E,

[W¯ ]ij = deg(i), if i = j,

0,

otherwise,

where deg(i) is the degree of vertex i (i.e., the number of neighboring nodes). Let us denote W = W¯ ⊗ In, where ⊗ denotes Kronecker product and In is the unit matrix.
First, we present the dual formulation of the distributed
optimization problem for the deterministic case, and then
we develop our novel analysis for the case of stochastic dual
oracles. We assume that for all i = 1, . . . , m function fi can be
represented as the Fenchel-Legendre transform

fi(x) = max{ y, x − ϕi(y)}.
y∈Rn

Thus, we rewrite the problem (1) as follows

m

max −F (x) : = − fi(xi)

x1 ,...,xm ∈Rn ,

x1=···=xm

i=1

m

= max − fi(xi), (2)
x1,√...,xm∈Rn, i=1 W x=0

where x = [x1, . . . , xm]T ∈ Rnm is the stacked column

vector.

Then, we introduce the Lagrangian dual problem to prob-

lem

(2)

with

dual

variables

y

=

[

y

T 1

,

·

·

·

,

y

T m

]

T

∈

Rmn

as

m
min max
y∈Rmn x∈Rnm i=1

√ yi, [ W x]i − fi(xi)

√

m

√

= min ψ(y) := ϕ( W y) := ϕi([ W y]i), (3)

y∈Rmn

i=1

√

√

where we used the notations [ W x]i and [ W y√]i for

descr√ibing the i-th n-dimensional block of vectors W x

and W y√respectively, and√also we used the equality

m i=1

yi, [

W x]i

=

m i=1

[

W y]i, xi .

Note that dealing with the dual problem does not oblige

us to use dual oracle of ∇ϕi. Indeed,

√

√√

∇ϕ([ W y]i) = [ W x( W y)]i,

(4)

√ where xi([W y]i) = argmax [ W x]i, yi − fi(xi) . So
xi ∈Rn
we can use the primal oracle ∇fi to solve this auxiliary

subproblem and ﬁnd an approximation t√o ∇ϕi. Making the change of variables y¯ := W y and structure

of Laplacian matrix W allows us to present accelerated

gradient method in a distributed manner for the dual problem.

Theorem 1: Let ε > 0 be a desired accuracy and assume

that ∇F (x∗) 2 = MF and that the primal objective in (2) is µ-strongly convex. Then the sequences xN and yN generated

by Algorithm 1 after N = O

(M

2 F

/

µ

ε)χ

(W

)

iterations

and oracle calls of dual function ∇ϕi per node i = 1, . . . m

satisfy the following condition F (xN ) + ψ(y¯N ) ≤ ε

Next, we focus on the case where we only have access to

the stochastic dual oracle.

Algorithm 1 Distributed Dual Algorithm

Input: Starting point λ¯0 = y¯0 = ζ¯0 = x0 = 0, number of

iterations N , C0 = α0 = 0.

1: Each agent i do

2: for k = 0, . . . , N − 1 do

3:

αk+1 = k4+L2 , Ak+1 =

k+1 i=1

αi

4: λ¯ki +1 = (αk+1ζ¯ik + Aky¯ik)/Ak+1.

5:

ζ¯ik+1 = ζ¯ik − αk+1

m j=1

Wij

xj

(λ¯tj

).

6: y¯ik+1 = (αk+1ζ¯ik+1 + Aky¯ik)/Ak+1.

7: xNi = A1N

N k=0

αk

xi(λ¯ki

).

Output: xN , y¯N .

A. Dual Approach with Stochastic Dual Oracle

In this section we will assume that the dual function ϕ(y) d=ef maxx∈Rmn { y, x − F (x)} could be represented as an expectation of differentiable in y functio√ns ϕ(y, ξ), i.e. ϕ(y) = Eξ [ϕ(y, ξ)]. It implies t√hat ϕ( W y) d=ef ψ(y) = Eξ[ψ(y, ξ)], where ψ(y, ξ) d=ef ϕ( W y, ξ). Next we introduce F (x, ξ) in such a way that the following relation

holds:

ψ(y, ξ) = max
x∈Rnm

√ y, W x − F (x, ξ) .

√

Note

that √ for

x( W y, ξ)

argmaxx∈Rnm y, W x − F (x, ξ)

d=ef Demyanov–

D√anskin√’s theorem [30] states that ∇ψ(y, ξ) = W x( W y, ξ) where the gradient is taken with respect the

ﬁrs√t variable. Finall√y, our deﬁnitions give us new relations: x( W y) = Eξ[x( W y, ξ)] and ∇ψ(y) = Eξ[∇ψ(y, ξ)], where x(y) d=ef argmaxx∈Rnm { y, x − F (x)} = ∇ϕ(y) and the last equality is again due to Demyanov-Danskin

theorem.

We suppose that ψ(y) is known only through the stochas-

tic ﬁrst-order oracle ∇ψ(y, ξ), satisfying the following assumption for all y ∈ Rnm1:

Eξ exp x(y, ξ) − x(y) 22/σx2 ≤ exp(1).

Note that this implies

Eξ exp ∇ψ(y, ξ) − ∇ψ(y) 22/σψ2 ≤ exp(1).

for all y ∈ Rnm, where σψ2 = λmax(W )σx2 . We assume that the function ψ is Lψ-smooth. If, the primal
objective is µ-strongly convex, then Lψ ≤ λmax(W )/µ.
Moreover, we assume that we can construct an approximation
for ∇ψ(y) using batches of size r in the following form:

r

r

1r

∇ ψ(y, {ξi}i=1) = r ∇ψ(y, ξi)

(5)

i=1

and, similarly,

√

r

1r √

x( W y, {ξi}i=1) = r x( W y, ξi).

i=1

Algorithm 2 Dual Stochastic Algorithm

Input: Starting point λ0 = y0 = ζ0 = x0 = 0, number of

iterations N , C0 = α0 = 0,

1: for k = 0, . . . , N − 1 do

2:

Ak+1 = Ak + αk+1 = 2Lψα2k+1

(6)

3:

λk+1 = (αk+1ζk + Akyk)/Ak+1.

(7)

4:

Calculate

∇rk+1

ψ

(λk+1

,

{ξs

}rk+1
s=1

)

according

to

(5)

with batch size

rk+1 = O max 1, σψ2 αk+1 ln(N/δ)/ε

5:

ζ k+1

=

ζk

−

αk+1

∇rk+1

ψ

(λk+1

,

{ξs

}rk+1
s=1

).

(8)

6:

yk+1 = (αk+1ζk+1 + Akyk)/Ak+1.

(9)

7: Set xN = A1N Output: xN , yN .

N

αk

√ x(

W

λ

k

,

{

ξi

}

r

k

).

k=0

i=1

Theorem 2: Assume that F is µ-strongly convex and

∇F (x∗) 2 = MF . Let ε > 0 be a desired accuracy.

Assume that at each iteration of Algorithm 2 the ap-

proximation for ∇ψ(y) is chosen according to (5) with

batch size rk = Ω max 1, σψ2 αk ln(N/δ)/ε . Assume additionally that F is LF -Lipschitz continuous on the set BRF (0) = {x ∈ Rnm | x√ 2 ≤ RF } where RF = Ω max ARNy λm6Cax2(HW ) , λmax( µW )JRy , Rx , Ry is such that y∗ 2 ≤ Ry, y∗ being√an optimal solution of the dual problem and Rx = x( W y∗) 2. Then, after N =

O

(M

2 F

/

µ

ε)χ

(W

)

iterations, the outputs xN and yN of

Algorithm 2 satisfy

√

F (xN ) − F (x∗) ≤ ε,

W xN 2 ≤ ε/Ry

(10)

with probability at least 1 − 4δ, where δ ∈ (0, 1/4),

ln(N/δ) ≥ 3. Moreover, the number of stochastic oracle calls for the
dual function ∇ϕi per node i = 1, . . . m is





 σψ2 MF2

1

O max ε2λ+ (W ) ln  δ

 min

 MF2 χ(W ) , µε

 MF2 χ(W ) µε 

To prove the theorem we ﬁrst state a number of technical

lemmas.

Lemma 3: For the sequence αk+1 deﬁned in (6) we have

for all k ≥ 0

αk+1 ≤ αk+1 d=ef k + 2 .

(11)

2Lψ

Lemma 4: Let A, B, and {ri}Ni=0 be non-negative num-

bers such that for all l = 1, . . . , N

1 r2 ≤ Ar2 + B r0

l−1
(k + 2)r2.

(12)

2l

0

N

k

k=0

1We believe that the light-tail assumption can be relaxed to a more general setting [31].

Then rl ≤ Cr0, where C is such positive number that C2 ≥ max{1, 2A + 2BC}.
The proof of the Lemma is followed from induction.

Lemma 5: Let the sequences of non-negative numbers {αk}k≥0, random non-negative variables {Rk}k≥0 and random vectors {ηk}k≥0 and {ak}k≥0 for all l = 1, . . . , N satisfy

12

l−1 k+1 k

2 Rl ≤ A + u αk+1 η , a

k=0

l−1
+ c α2k+1
k=0

ηk+1

2 2

(13)

where A is deterministic non-negative number, ak 2 ≤
dRk, d ≥ 1 is some positive deterministic constant and Rk = max{Rk−1, Rk} for all k ≥ 1, R0 = R0, Rk depends only on η0, . . . , ηk. Moreover, assume, vector ak is a function of η0, . . . , ηk−1 ∀k ≥ 1, a0 is a deterministic vector, and ∀k ≥ 0,

E ηk | {ηj }jk=−01 = 0, E exp ηk 22σk−2 | {ηj}jk=−01 ≤ exp(1), (14)

αk+1 ≤ αk+1 = D(k + 2), σk2 ≤ (Cε)/(αk+1 ln(N/δ)) for some D, C > 0, ε > 0. If additionally ε ≤ HR02/N 2, then with probability at least 1 − 2δ the inequalities

Rl ≤ J R0 and

(15)

u

l−1 k=0

αk+1

ηk+1, ak

+c

l−1 k=0

α2k+1

ηk+1

2 2

≤ 24cCDH + udC1 CDHJg(N ) R02 (16)

hold ∀l = 1, . . . , N simultaneously. Here C1 is some positive constant, g(N ) = ln (N/δ) + ln ln (B/b) /ln (N/δ),

B = 2d2CDHR02 2A + udR02 +12CDε (2c + ud) N (N + 3) (2ud)N ,

b = σ02α21d2R02 and

J = max 1, udC1 CDHg(N )

+

u2d2C12CDHg(N )

+

2A R2

+

48cCDH

.

0

B. Example: Computation of Wasserstein Barycenters

It may seem that the problem with dual stochastic oracle is artiﬁcial. Next, we present the regularized Wasserstein barycenter problem [32]–[35], which is a recent example of a function with stochastic dual oracle,

m

min

Wµ,qi (p),

p∈Sn(1) i=1

(17)

where Wµ,qi (p) = min { C, π + µ π ln π } .
π1=p,πT 1=q π≥0
Here C is a transportation cost matrix, p, q are elements of standard probability simplex, logarithm of a matrix is taken
componentwise. Problem (17) is not easily tractable in the
distributed setting since cost of approximating of the gradient

of Wµ,qi (p) requires to solve a large-scale minimization problem. On the other hand, as it is shown in [32],

Wµ,qi (p) = max u, p − Wq∗,µ(u)
u∈Rn

n
W∗ (u) = µ q ln

1 n exp −Cij + ui

.

q,µ j=1 j qj i=1 µ

So, the conjugate function has an explicit expression and

its gradient can be calculated explicitly. Moreover, as the

conjugate function has the form of ﬁnite-sum, we can use

randomization and take a component i with probability qi.

As a corollary of our general Theorem 2, we obtain

Corollary 6: Taking the batch size rk

=

O (σψ2 αk ln(N/β)/εµ) , where σψ2 = mλmax(W ) after

N =O

(M

2 F

/

µ

ε)χ

(W

)

iterations the following holds

for the output pN of Algorithm 2 with probability

at least 1 − 4δ, where δ ∈ (0, 1/4) is such that

(1 + ln(1/δ))/ ln(N/δ) ≤ 2.

m

m

Wµ,qi (pNi ) − Wµ,qi (p∗) ≤ ε,

i=1

i=1

√ W pN 2 ≤ ε/Ry.

Moreover, the total complexity per node is







O n max  mMF2 χ ln  1  ε2 δ

 MF2 χ , µε

 MF2 χ , µε 

where MF 2 = 2nm

C

2 ∞

[33] and χ = χ(W ) .

III. CONCLUSION
We consider primal-dual distributed accelerated gradient method for stochastic ﬁnite-sum minimization. One of the key features of our analysis are large deviations bounds for the error of the algorithms. Moreover, we show that the proposed method has optimal communication complexity, up to logarithmic factors. For the proposed method we provide an explicit oracle and communication complexity analysis. We illustrate the dual approach by the Wasserstein barycenter problem. As a future work we consider extending these results for different classes of problems, i.e., non-smooth and/or also strongly convex problems.
Acknowledgements: We are grateful to A. Nemirovski for fruitful discussions.

REFERENCES
[1] V. Borkar and P. P. Varaiya, “Asymptotic agreement in distributed estimation,” IEEE Transactions on Automatic Control, vol. 27, no. 3, pp. 650–655, 1982.
[2] J. N. Tsitsiklis and M. Athans, “Convergence and asymptotic agreement in distributed decision problems,” IEEE Transactions on Automatic Control, vol. 29, no. 1, pp. 42–50, 1984.
[3] M. H. DeGroot, “Reaching a consensus,” Journal of the American Statistical Association, vol. 69, no. 345, pp. 118–121, 1974.
[4] L. Xiao and S. Boyd, “Optimal scaling of a gradient method for distributed resource allocation,” Journal of Optimization Theory and Applications, vol. 129, no. 3, pp. 469–488, 2006.
[5] M. Rabbat and R. Nowak, “Decentralized source localization and tracking wireless sensor networks,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 3, 2004, pp. 921–924.

[6] T. Kraska, A. Talwalkar, J. C. Duchi, R. Grifﬁth, M. J. Franklin, and M. I. Jordan, “Mlbase: A distributed machine-learning system.” in CIDR, vol. 1, 2013, pp. 2–1.
[7] A. Nedic´, A. Olshevsky, and C. A. Uribe, “Distributed learning for cooperative inference,” arXiv preprint arXiv:1704.02718, 2017.
[8] A. Ivanova, P. Dvurechensky, and A. Gasnikov, “Composite optimization for the resource allocation problem,” arXiv:1810.00595, 2018.
[9] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in Proceedings of COMPSTAT’2010. Springer, 2010, pp. 177–186.
[10] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of multipliers,” Foundations and Trends R in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.
[11] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow: Largescale machine learning on heterogeneous distributed systems.” in Conf. on Language Resources and Evaluation (LREC’08), 2016, pp. 3243– 3249.
[12] A. Nedic´, A. Olshevsky, and W. Shi, “Achieving geometric convergence for distributed optimization over time-varying graphs,” SIAM Journal on Optimization, vol. 27, no. 4, pp. 2597–2633, 2017.
[13] A. Nedic´, A. Olshevsky, and C. A. Uribe, “Fast convergence rates for distributed non-Bayesian learning,” IEEE Transactions on Automatic Control, vol. 62, no. 11, pp. 5538–5553, Nov 2017.
[14] A. Nedic´, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis, “On distributed averaging algorithms and quantization effects,” IEEE Transactions on Automatic Control, vol. 54, no. 11, pp. 2506–2517, 2009.
[15] S. S. Ram, A. Nedic´, and V. V. Veeravalli, “Distributed stochastic subgradient projection algorithms for convex optimization,” Journal of Optimization Theory and Applications, vol. 147, no. 3, pp. 516– 545, 2010.
[16] K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulie´, “Optimal algorithms for smooth and strongly convex distributed optimization in networks,” in Proc. of the 34th International Conference on Machine Learning, 2017, pp. 3027–3036.
[17] C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedic´, “A dual approach for optimal algorithms in distributed optimization over networks,” arXiv:1809.00710, 2018.
[18] G. Lan, S. Lee, and Y. Zhou, “Communication-efﬁcient algorithms for decentralized and stochastic optimization,” Mathematical Programming, pp. 1–48, 2017.
[19] D. Jakovetic, D. Bajovic, A. K. Sahu, and S. Kar, “Convergence rates for distributed stochastic optimization over random networks,” in 2018 IEEE Conference on Decision and Control (CDC), 2018, pp. 4238– 4245.
[20] W. Li, M. Assaad, and P. Duhamel, “Distributed stochastic optimization in networks with low informational exchange,” in 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2017, pp. 1160–1167.
[21] D. Dvinskikh and A. Gasnikov, “Decentralized and parallelized primal and dual accelerated methods for stochastic convex programming problems,” arXiv preprint arXiv:1904.09015, 2019.
[22] E. Gorbunov, D. Dvinskikh, and A. Gasnikov, “Optimal decentralized distributed algorithms for stochastic convex optimization,” arXiv preprint arXiv:1911.07363, 2019.
[23] K. Scaman, F. Bach, S. Bubeck, L. Massoulie´, and Y. T. Lee, “Optimal algorithms for non-smooth distributed optimization in networks,” in Advances in Neural Information Processing Systems, 2018, pp. 2745– 2754.
[24] M. Maros and J. Jalde´n, “PANDA: A Dual Linearly Converging Method for Distributed Optimization Over Time-Varying Undirected Graphs,” in 2018 IEEE Conference on Decision and Control (CDC), 2018, pp. 6520–6525.
[25] P. Dvurechensky, A. Gasnikov, E. Gasnikova, S. Matsievsky, A. Rodomanov, and I. Usik, “Primal-dual method for searching equilibrium in hierarchical congestion population games,” in Supplementary Proceedings of the 9th International Conference on Discrete Optimization and Operations Research and Scientiﬁc School (DOOR 2016) Vladivostok, Russia, September 19 - 23, 2016, 2016, pp. 584– 595, arXiv:1606.08988.
[26] A. Chernov, P. Dvurechensky, and A. Gasnikov, “Fast primal-dual gradient method for strongly convex minimization problems with linear constraints,” in Discrete Optimization and Operations Research: 9th International Conference, DOOR 2016, Vladivostok, Russia, Septem-

ber 19-23, 2016, Proceedings, Y. Kochetov, M. Khachay, V. Beresnev, E. Nurminski, and P. Pardalos, Eds. Springer International Publishing, 2016, pp. 391–403. [27] A. S. Anikin, A. V. Gasnikov, P. E. Dvurechensky, A. I. Tyurin, and A. V. Chernov, “Dual approaches to the minimization of strongly convex functionals with a simple structure under afﬁne constraints,” Computational Mathematics and Mathematical Physics, vol. 57, no. 8, pp. 1262–1276, 2017. [28] P. Dvurechensky, A. Gasnikov, and A. Kroshnin, “Computational optimal transport: Complexity by accelerated gradient descent is better than by Sinkhorn’s algorithm,” in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, Eds., vol. 80, 2018, pp. 1367–1376, arXiv:1802.04367. [29] S. V. Guminov, Y. E. Nesterov, P. E. Dvurechensky, and A. V. Gasnikov, “Accelerated primal-dual gradient descent with linesearch for convex, nonconvex, and nonsmooth optimization problems,” Doklady Mathematics, vol. 99, no. 2, pp. 125–128, 2019. [30] R. T. Rockafellar, Convex analysis. Princeton university press, 2015. [31] P. E. Dvurechensky, A. V. Gasnikov, and A. A. Lagunovskaya, “Parallel algorithms and probability of large deviation for stochastic convex optimization problems,” Numerical Analysis and Applications, vol. 11, no. 1, pp. 33–37, 2018, arXiv:1701.01830. [32] M. Cuturi and G. Peyre´, “A smoothed dual approach for variational wasserstein problems,” SIAM J. on Imaging Sciences, vol. 9, no. 1, pp. 320–343, 2016. [33] A. Kroshnin, N. Tupitsa, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and C. Uribe, “On the complexity of approximating Wasserstein barycenters,” in Proceedings of the 36th International Conference on Machine Learning, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97, 2019, pp. 3530–3540, arXiv:1901.08686. [34] C. A. Uribe, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and A. Nedic´, “Distributed Computation of Wasserstein Barycenters Over Networks,” in 2018 IEEE Conference on Decision and Control (CDC), Dec 2018, pp. 6544–6549. [35] P. Dvurechensky, D. Dvinskikh, A. Gasnikov, C. A. Uribe, and A. Nedic´, “Decentralize and randomize: Faster algorithm for Wasserstein barycenters,” in Advances in Neural Information Processing Systems 31, 2018, pp. 10 783–10 793, arXiv:1806.03915. [36] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan, “A short note on concentration inequalities for random vectors with subgaussian norm,” arXiv preprint arXiv:1902.03736, 2019. [37] A. Juditsky and A. S. Nemirovski, “Large deviations of vectorvalued martingales in 2-smooth normed spaces,” arXiv preprint arXiv:0809.0813, 2008. [38] P. Dvurechenskii, D. Dvinskikh, A. Gasnikov, C. Uribe, and A. Nedich, “Decentralize and randomize: Faster algorithm for wasserstein barycenters,” in Advances in Neural Information Processing Systems, 2018, pp. 10 760–10 770. [39] Y. Nesterov, Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004.

IV. APPENDIX

A. Auxiliary results

In this subsection, we present the results from other papers that we rely on in our proofs.

Lemma 7 (Lemma 2 from [36]): For random vector ξ ∈ Rn following statements are equivalent up to absolute constant

difference in σ.

1) Tails: P { ξ 2 ≥ γ} ≤ 2 exp − 2γσ22 ∀γ ≥ 0.

2)

Moments:

(E

[

ξ

p

])

1 p

≤ σ√p

for

any

positive

integer

p.

3) Super-exponential moment: E exp σξ222 ≤ exp(1).

Lemma 8 (Corollary 8 from [36]): Let {ξk}Nk=1 be a sequence of random vectors with values in Rn such that for k = 1, . . . , N and for all γ ≥ 0

E [ξk | ξ1, . . . , ξk−1] = 0,

γ2 E [ ξk 2 ≥ γ | ξ1, . . . , ξk−1] ≤ exp − 2σ2
k

almost surely,

N
where σk2 belongs to the ﬁltration σ(ξ1, . . . , ξk−1) for all k = 1, . . . , N . Let SN = ξk. Then there exists an absolute
k=1
constant C1 such that for any ﬁxed δ > 0 and B > b > 0 with probability at least 1 − δ:

N
either σk2 ≥ B or
k=1

SN 2 ≤ C1

max

N
σk2, b
k=1

ln 2n + ln ln B .

δ

b

Lemma 9 (corollary of Theorem 2.1, item (ii) from [37]): Let {ξk}Nk=1 be a sequence of random vectors with values in Rn such that

E [ξk | ξ1, . . . , ξk−1] = 0 almost surely, k = 1, . . . , N

N
and let SN = ξk. Assume that the sequence {ξk}Nk=1 satisfy “light-tail” assumption:
k=1

E exp

ξk

2 2

| ξ1, . . . , ξk−1 ≤ exp(1) almost surely, k = 1, . . . , N,

σk2

where σ1, . . . , σN are some positive numbers. Then for all γ ≥ 0







√√

N

γ2

P  SN ≥ 2 + 2γ

σk2 ≤ exp − 3 .

(18)

k=1

B. Proof of Theorem 1

For Algorithm 1 the following holds

F (xN ) + ϕ(y¯N ) ≤ LψRy2¯ , N2
where Ry¯ is such that y¯∗ ≤ Ry¯ is the radius of the solution. As it follows from [18], Ry¯ can be taken as Ry2¯ = ∇F (x∗) / . 22 λ+ min(W ) Since the Lipschitz constant for the dual function ψ is Lψ = λmax(W )/µ, we get the statement of the theorem.

C. Proof of Theorem 2

The proof includes several steps. We start with the proofs of the technical lemmas. For convenience we repeat statements of lemmas again.
Lemma 10: For the sequence αk+1 deﬁned in (6) we have for all k ≥ 0

αk+1 ≤ αk+1 d=ef k + 2 .

(19)

2Lψ

Proof: We prove (19) by induction. For k = 0 equation (6) gives us α1 = 2Lψα21 ⇐⇒ α1 = 2L1ψ . Next we assume

that (19) holds for all k ≥ l − 1 and prove it for k = l:

2

l+1 (6)

(11)

1l

l(l + 3)

2Lψαl+1 =

αi ≤ αl+1 + 2Lψ (i + 1) = αl+1 + 4Lψ .

i=1

i=1

√

√

1+ 4k2+12k+1 ≤ 1+ (2k+3)2 ≤ 2k+4 = k+2 .

4L

4L

2L

This quadratic inequality implies that αk+1 ≤

4Lψ

ψ

ψ

ψ

Lemma 11: Let A, B, and {ri}Ni=0 be non-negative numbers such that for all l = 1, . . . , N

1 r2 ≤ Ar2 + B r0

l−1
(k + 2)r2.

(20)

2l

0

N

k

k=0

Then

rl ≤ Cr0,

(21)

where C is such positive number that C2 ≥ max{1, 2A + 2BC}, i.e. one can choose C = max{1, B + √B2 + 2A}.

Proof: We prove (21) by induction. For l = 0 the inequality rl ≤ Cr0 trivially follows since C ≥ 1. Next we assume that (21) holds for some l < N and prove it for l + 1:

rl+1

(20)
≤

√ 2

Ar2 + B r0

0

N

l

(21) √

(k + 2)rk2 ≤ r0 2

k=0

A + BC N

l
(k + 2)
k=0

√

BC (l + 1)(l + 2) √

BC N (N + 1)

= r0 2 A + N

2

≤ r0 2 A + N

2

√

≤ r0 2A + 2BC ≤ Cr0.

≤C

Lemma 12: Let the sequences of non-negative numbers {αk}k≥0, random non-negative variables {Rk}k≥0 and random vectors {ηk}k≥0 and {ak}k≥0 for all l = 1, . . . , N satisfy

12

l−1

l−1

k+1 k

2

k+1 2

2 Rl ≤ A + u αk+1 η , a + c αk+1 η 2

k=0

k=0

(22)

where A is deterministic non-negative number, ak 2 ≤ dRk, d ≥ 1 is some positive deterministic constant and Rk = max{Rk−1, Rk} for all k ≥ 1, R0 = R0, Rk depends only on η0, . . . , ηk. Moreover, assume, vector ak is a function of η0, . . . , ηk−1 ∀k ≥ 1, a0 is a deterministic vector, and ∀k ≥ 0,

E ηk | {ηj}jk=−01 = 0, E exp ηk 22σk−2 | {ηj }jk=−01 ≤ exp(1),

(23)

αk+1 ≤ αk+1 = D(k + 2), σk2 ≤ αk+1Clnε(N/δ) for some D, C > 0, ε > 0. If additionally ε ≤ HR20/N2, then with probability at least 1 − 2δ the inequalities

Rl ≤ J R0 and

(24)

u

l−1 k=0

αk+1

ηk+1, ak

+c

l−1 k=0

α2k+1

ηk+1

2 2

≤

24cCDH + udC1

CDHJ g(N ) R02

(25)

hold

∀l

=

1, . . . , N

simultaneously.

Here

C1

is

some

positive

constant,

g(N ) =

, ln(N/δ)+ln ln(B/b)
ln(N/δ)

B = 2d2CDHR02 2A + udR02 + 12CDε (2c + ud) N (N + 3) (2ud)N ,

b = σ02α21d2R02 and

J = max 1, udC1 CDHg(N ) + u2d2C12CDHg(N ) + 2RA02 + 48cCDH . Proof: We start with applying Cauchy-Schwartz inequality to the second term in the right-hand side of (13):

1 R2

≤

l−1
A + ud α

l−1
ηk R + c α2

ηk 2,

2l

k+1

2k

k+1

2

k=0

k=0

ud l−1 2

ud

l−1 2

k2

≤ A + 2 Rk + c + 2

αk+1 η 2.

k=0

k=0

(26)

The idea of the proof is as following: estimate RN2 roughly, then apply Lemma 8 in order to estimate second term in the last row of (22) and after that use the obtained recurrence to estimate right-hand side of (22).

Using

Lemma

9

we

get

that

with

probability

at

least

1

−

δ N

ηk 2

√ ≤ 2 1+

3 ln N δ

√ σk ≤ 2 1 +

3 ln N δ





√ Cε

αk+1 ln

N δ

=

1

+ 3  √2Cε ≤ 2 3 √2Cε,

(27)

αk+1 ln

N δ

αk+1

αk+1

where

in

the

last

inequality

we

use

ln

N δ

≥

3.

Using

union

bound

we

get

that

with

probability

≥

1−δ

the

inequality

1 R2

≤

A + ud l−1 R2 + 24Cε c + ud

l−1
α

2l

2

k

2

k+1

k=0

k=0

≤

A + ud l−1 R2 + 24CDε c + ud

l−1
(k + 2)

2

k

2

k=0

k=0

≤ A + ud l−1 R2 + 12CDε c + ud l(l + 3)

2

k

2

k=0

holds for all l = 1, . . . , N simultaneously. Note that the last row in the previous inequality is non-decreasing function of l. If we deﬁne ˆl as the largest integer such that ˆl ≤ l and Rˆl = Rˆl, we will get that Rˆl = Rˆl = Rˆl+1 = . . . = Rl and, as a consequence, with probability ≥ 1 − δ

1 R2 ≤ A + ud ˆl−1 R2 + 12CDε c + ud ˆl(ˆl + 3)

2l

2

k

2

k=0

≤ A + ud l−1 R2 + 12CDε c + ud l(l + 3), ∀l = 1, . . . , N.

2

k

2

k=0

Therefore, we have that with probability ≥ 1 − δ

l−1
Rl2 ≤ 2A + ud Rk2 + 12CDε (2c + ud) l(l + 3)
k=0

l−2

≤ 2A (1 + ud) + (ud + u2d2) Rk2 + 12CDε(2c + ud) (l(l + 3) + ud(l − 1)(l + 2))

≤2ud

≤2u2d2 k=0

≤2udl(l+3)

l−2
≤ 2ud 2A + ud Rk2 + 12CDε (2c + ud) l(l + 3) , ∀l = 1, . . . , N.
k=0

Unrolling the recurrence we get that with probability ≥ 1 − δ

Rl2 ≤ 2A + udR02 + 12CDε (2c + ud) l(l + 3) (2ud)l, ∀l = 1, . . . , N.

We emphasize that it is very rough estimate, but we show next that such a bound does not spoil the ﬁnal result too much. It implies that with probability ≥ 1 − δ

l−1

Rk2 ≤ l 2A + udR02 + 12CDε (2c + ud) l(l + 3) (2ud)l, ∀l = 1, . . . , N.

(28)

k=0

Next we apply delicate result from [36] which is presented in Section IV-A as Lemma 8. We consider random variables ξk = αk+1 ηk, ak . Note that E ξk | ξ0, . . . , ξk−1 = αk+1 E ηk | η0, . . . , ηk−1 , ak = 0 and

E exp

(ξ k )2 σk2 α2k+1 d2 Rk2

| ξ0, . . . , ξk−1

≤ E exp

α2k+1 ηk 22d2Rk2 σk2 α2k+1 d2 Rk2

| η0, . . . , ηk−1

= E exp

ηk

2 2

σk2

| η0, . . . , ηk−1 ≤ exp(1)

due to Cauchy-Schwartz inequality and assumptions of the lemma. If we denote σˆk2 = σk2α2k+1d2Rk2 and apply Lemma 8

with B = 2d2CDHR02 2A + udR02 + 12CDε (2c + ud) N (N + 3) (2ud)N and b = σˆ02, we get that for all l = 1, . . . , N

with

probability

≥

1

−

δ N

l−1

l−1

either σˆ2 ≥ B or

ξk ≤ C

l−1 σˆ2 ln N + ln ln B

k

1

k

δ

b

k=0

k=0

k=0

with some constant C1 > 0 which does not depend on B or b. Using union bound we obtain that with probability ≥ 1 − δ

l−1

l−1

either σˆ2 ≥ B or

ξk ≤ C

l−1 σˆ2 ln N + ln ln B

k

1

k

δ

b

k=0

k=0

k=0

and it holds for all l = 1, . . . , N simultaneously. Note that with probability at least 1 − δ

l−1
σˆ2

=

l−1
d2 σ2α2

l−1
R2 ≤ d2

Cε α

R2

k k=0

k k+1 k k=0

k=0 ln Nδ k+1 k

≤

d2 C DH R02

l−1
(k

+ 2)R2

≤

d2 C DH R02

·

N

+1

l−1

R2

N2

ln

N δ

k=0

k

3N

N

k

k=0

(28)
≤

d2CDHR02 l 2A + udR2 + 12CDε (2c + ud) l(l + 3) (2ud)l

N

0

≤B

2

for all l = 1, . . . , N simultaneously. Using union bound again we get that with probability ≥ 1 − 2δ the inequality

l−1
ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

(29)

1

k

δ

b

k=0

k=0

holds for all l = 1, . . . , N simultaneously. Note that we also proved that (27) is in the same event together with (29) and holds with probability ≥ 1 − 2δ. Putting
all together in (22), we get that with probability at least 1 − 2δ the inequality

1 2 (13)

l−1

l−1

kk

2

k2

2 Rl ≤ A + u αk+1 η , a + c αk+1 η 2

k=0

k=0

(29)

l−1

N

B

≤ A + uC1

σˆk2 ln δ + ln ln b

k=0

l−1
+ 24cCε αk+1
k=0

holds

for

all

l

=

1, . . . , N

simultaneously.

For

brevity,

we

introduce

new

notation:

g(N )

=

ln(

N δ

)+ln

ln(

B b

)

≈

1

(neglecting

ln(

N δ

)

constant factor). Using our assumption σk2 ≤ αk+1Clnε( Nδ ) and deﬁnition σˆk2 = σk2α2k+1d2Rk2 we obtain that with probability at least 1 − 2δ the inequality

1 R2

≤

l−1
A+u α

l−1
ηk, ak + c α2

ηk 2

2l

k+1

k+1

2

k=0

k=0

l−1

l−1

≤ A + 24cCε αk+1 + udC1 Cεg(N )

αk+1 Rk2

k=0

k=0

l−1

l−1

≤ A + 24cCDε (k + 2) + udC1 CDεg(N )

(k + 2)Rk2

k=0

k=0

≤ A + 24cCD HR02 l(l + 1) + udC

CD HR02 g(N )

l−1
(k + 2)R2

N2 2

1

N2

k

k=0

≤

A + 24cCDH R2 + udC1R0

CDHg(N )

l−1
(k + 2)R2

R02

0

N

k k=0

holds

for

all

l

=

1, . . . , N

simultaneously.

Next

we

apply

Lemma

4

with

A

=

A R2

+ 24cCDH,

B

=

udC1

0

rk = Rk and get that with probability at least 1 − 2δ inequality

Rl ≤ J R0

holds for all l = 1, . . . , N simultaneously with

(30) CDHg(N ),

J = max 1, udC1

CDHg(N ) +

u2d2C2CDHg(N ) + 2A + 48cCDH

1

2

.

R0

It implies that with probability at least 1 − 2δ the inequality

l−1

l−1

A+u

αk+1 ηk, ak + c

α2k+1

ηk

2 2

k=0

k=0

≤

A R2

+

24cCDH

R02

+

udC1 R20 N

0

CDHg(N )

l−1
(k + 2)J
k=0

≤ A + 24cCDH + udC1 CDHJ g(N ) N1

l(l+1) 2

R02

≤ A + 24cCDH + udC1 CDHJ g(N ) R02

holds for all l = 1, . . . , N simultaneously. Lemma 13 (see also Theorem 1 from [38]): For each iteration of Algorithm 2 we have

A ψ(yN )

≤

1

λ − ζ0

2− 1

λ − ζN

N −1
2+ α

ψ(λk+1) + ∇Ψ(λk+1, ξk+1), λ − λk+1

N

2

22

2

k+1

k=0

N −1
+ Ak ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1

k=0

N −1 Ak+1

k+1

k+1 k+1 2

+

2Lψ ∇ψ(λ ) − ∇Ψ(λ , ξ ) 2,

(31)

k=0

where we use the following notation for the stochastic approximation of ∇ψ(λ) according to (5)

∇Ψ(λk, ξk) := ∇rk ψ(λk, {ξik}ri=k 1),

(32)

where ξk = (ξ1k, . . . , ξrkk ). Proof: The proof of this lemma follows a similar way as in the proof of Theorem 1 from [38]. We can rewrite the
update rule for ζk in the equivalent way:

ζk = aλrg∈mRnin αk+1 ∇Ψ(λk+1, ξk+1), λ − λk+1 + 12 λ − ζk 22 . From the optimality condition we have that for all z ∈ Rn

ζk+1 − ζk + αk+1∇Ψ(λk+1, ξk+1), λ − ζk+1 ≥ 0.

(33)

Using this we get

αk+1 ∇Ψ(λk+1, ξk+1), ζk − λ = αk+1 ∇Ψ(λk+1, ξk+1), ζk − ζk+1 + αk+1 ∇Ψ(λk+1, ξk+1), ζk+1 − λ
(33)
≤ αk+1 ∇Ψ(λk+1, ξk+1), ζk − ζk+1 + ζk+1 − ζk, λ − ζk+1 .

One can check via direct calculations that

a, b ≤ 1 a + b 2 − 1 a 2 − 1 b 2,

2

22 22 2

∀ a, b ∈ Rn.

Combining previous two inequalities we obtain

αk+1 ∇Ψ(λk+1, ξk+1), ζk − λ By deﬁnition of yk+1 and λk+1

≤ αk+1 ∇Ψ(λk+1, ξk+1), ζk − ζk+1 − 21 ζk − ζk+1 22

+ 1 ζk − λ 2 − 1 ζk+1 − λ 2.

2

22

2

yk+1 = Akyk + αk+1ζk+1 = Akyk + αk+1ζk + αk+1 ζk+1 − ζk = λk+1 + αk+1 ζk+1 − ζk .

Ak+1

Ak+1

Ak+1

Ak+1

Together with previous inequality, it implies

αk+1 ∇Ψ(λk+1, ξk+1), ζk − λ

≤ Ak+1 ∇Ψ(λk+1, ξk+1), λk+1 − yk+1 − A2k+1 λk+1 − yk+1 2

2α2k+1

2

+ 1 ζk − λ 2 − 1 ζk+1 − λ 2

2

22

2

≤ Ak+1

∇Ψ(λk+1, ξk+1), λk+1 − yk+1 − 2Lψ λk+1 − yk+1 2

2

2

+ 1 ζk − λ 2 − 1 ζk+1 − λ 2

2

22

2

= Ak+1

∇ψ(λk+1), λk+1 − yk+1 − 2Lψ λk+1 − yk+1 2

2

2

+Ak+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λk+1 − yk+1

+ 1 ζk − λ 2 − 1 ζk+1 − λ 2.

2

22

2

From Fenchel-Young inequality

a, b

≤

1 2η

a

2 2

+

η 2

b 22, a, b ∈ Rn, η > 0, we have

∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λk+1 − yk+1 ≤ 2L1ψ ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1) 22 + L2ψ λk+1 − yk+1 22.

Using this, we get

αk+1 ∇Ψ(λk+1, ξk+1), ζk − λ

≤ Ak+1

∇ψ(λk+1), λk+1 − yk+1 − Lψ λk+1 − yk+1 2

2

2

+ Ak+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1) 2

2Lψ

2

+ 1 ζk − λ 2 − 1 ζk+1 − λ 2

2

22

2

≤ Ak+1 ψ(λk+1) − ψ(yk+1) + 12 ζk − λ 22 − 21 ζk+1 − λ 22

+ Ak+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1) 2 ,

(34)

2Lψ

2

where the last inequality follows from the Lψ-smoothness of ψ(y). From the convexity of ψ(y), we have

∇Ψ(λk+1, ξk+1), yk − λk+1

= ∇ψ(λk+1), yk − λk+1 + ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1

≤ ψ(yk) − ψ(λk+1) + ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1 .

(35)

By deﬁnition of λk+1 we have

αk+1 λk+1 − ζk = Ak yk − λk+1 .

(36)

Putting all together, we get

αk+1

∇Ψ(λk+1, ξk+1), λk+1 − λ = αk+1 ∇Ψ(λk+1, ξk+1), λk+1 − ζk + αk+1 ∇Ψ(λk+1, ξk+1), ζk − λ (=36) Ak ∇Ψ(λk+1, ξk+1), yk − λk+1 + αk+1 ∇Ψ(λk+1, ξk+1), ζk − λ
(34),(35)
≤ Ak ψ(yk) − ψ(λk+1) + Ak ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1

+Ak+1

ψ(λk+1) − ψ(yk+1)

+

1 2

ζk − λ

2 2

−

1 2

ζk+1 − λ

2 2

+ A2kL+ψ1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1) 22 .

Rearranging the terms and using Ak+1 = Ak + αk+1, we obtain

Ak+1ψ(yk+1) − Akψ(yk) ≤ αk+1 ψ(λk+1) + ∇Ψ(λk+1, ξk+1), λ − λk+1 + 12 ζk − λ 22

− 1 ζk+1 − λ 2 + Ak+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1) 2

2

2 2Lψ

2

+Ak ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1 ,

and after summing these inequalities for k = 0, . . . , N − 1 we get

A ψ(yN )

≤

1

λ − ζ0

2− 1

λ − ζN

N −1
2+ α

ψ(λk+1) + ∇Ψ(λk+1, ξk+1), λ − λk+1

N

2

22

2

k+1

k=0

N −1
+ Ak ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1

k=0

+ N −1 Ak+1 k=0 2Lψ

∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 22,

where we use that A0 = 0.

Now, we are ready to prove our main result in Theorem 2 on the communication and oracle complexity of Algorithm 2.

For convenience we provide the statement of the theorem once again.

Theorem 14: Assume that F is µ-strongly convex and ∇F (x∗) 2 = MF . Let ε > 0 be a desired accuracy. Assume

that at each iteration of Algorithm 2 the approximation for ∇ψ(y) is chosen according to (5) with batch size rk = Ω max 1, σψ2 αk ln(N/δ)/ε . Assume additionally that F is √LF -Lipschitz continuous on the set BRF (0) = {x ∈ Rnm |
x 2 ≤ RF } where RF = Ω max ARNy λm6Cax2(HW ) , √λmax( µW )JRy , Rx , Ry is such that y∗ 2 ≤ Ry, y∗ being an optimal solution of the dual problem and Rx = x( W y∗) 2. Then, after N = O (MF2/µε)χ(W ) iterations, the

outputs xN and yN of Algorithm 2 satisfy

F (xN ) − F (x∗) ≤ ε, √W xN 2 ≤ ε

(37)

Ry

with probability at least 1 − 4δ, where δ ∈ (0, 1/4), ln(N/δ) ≥ 3. Moreover, the number of stochastic oracle calls for the

dual function ∇ϕi per node i = 1, . . . m is





 O max

σψ2 MF2

ln  1

 ε2λ+min(W )

δ

 MF2 χ(W ) , µε



MF2

χ(W

 )

µε



Proof: From Lemma 13 we have

N

1

02 1

N 2 N −1

k+1

k+1 k+1

k+1

AN ψ(y ) ≤ 2 λ − ζ 2 − 2 λ − ζ 2 + αk+1 ψ(λ ) + ∇Ψ(λ , ξ ), λ − λ

k=0

N −1
+ Ak ∇Ψ(λk+1 − ∇ψ(λk+1, ξk+1), yk − λk+1

k=0

+ N−1 Ak+1 ∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 2.

(38)

k=0 2Lψ 2

From deﬁnition of λk+1 (see (7)) we have

αk+1 λk+1 − ζk = Ak yk − λk+1 .

(39)

Using this, we add and subtract

N −1 k=0

αk+1

∇ψ(λk+1), λ∗ − λk+1

in (38), and obtain by choosing λ = λ∗

N

1 ∗ 0 2 1 ∗ N 2 N −1

k+1

k+1 ∗

k+1

AN ψ(y ) ≤ 2 λ − ζ 2 − 2 λ − ζ 2 + αk+1 ψ(λ ) + ∇ψ(λ ), λ − λ

k=0

N −1
+ αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), ak

k=0

N −1

+ α2k+1 ∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 22,

(40)

k=0

where ak = λ∗ − ζk. From convexity of ψ we have

N −1
αk+1
k=0

ψ(λk+1) +

∇ψ(λk+1), λ∗ − λk+1

N −1

≤

αk+1 ψ(λk+1) + ψ(λ∗) − ψ(λk+1)

k=0

N −1
= ψ(λ∗) αk+1 = AN ψ(λ∗) ≤ AN ψ(yN )

k=0

From this and (40) we get

1 λ∗ − ζN 2

(40)
≤

1

λ∗ − ζ0

N −1
2+ α

∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), ak

2

2

2

2

k+1

k=0

N −1

+ α2k+1 ∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 22.

(41)

k=0

Next step we introduce sequences {Rk}k≥0 and {Rk}k≥0 as follows

Rk = ζk − λ∗ 2 and Rk = max Rk−1, Rk , R0 = R0.

Since ζ0 = 0 in Algorithm 2, then R0 = Ry, where Ry is such that λ∗ 2 ≤ Ry. One can obtain by induction that ∀k ≥ 0 λk+1, yk, ζk ∈ BRk (λ∗), where BRk (λ∗) is Euclidean ball with radius Rk and center λ∗. Indeed, since from (9) yk+1 is a convex combination of ζk+1 ∈ BRk+1 (λ∗) ⊆ BRk+1 (λ∗) and yk ∈ BRk (λ∗) ⊆ BRk+1 (λ∗), where we use the fact that a ball is a convex set, we get yk+1 ∈ BRk+1 (λ∗). Analogously, since from (7) λk+1 is a convex combination of yk and ζk we have λk+1 ∈ BRk (λ∗). Using new notation we can rewrite (41) as

1 R2

≤

1 R2

+

N −1
α

∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), ak

2N

2y

k+1

k=0

N −1

+ α2k+1 ∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 22,

(42)

k=0

where ak 2 = λ∗ − ζk 2 ≤ Rk. Let us denote ηk+1 = ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1). Theorem 2.1 from [37] (see Lemma 9 in the Section IV-A) says

that






k

√√

σψ2


j k−1

γ2

P  η 2 ≥ 2 + 2γ rk+1 | {η }j=0  ≤ exp − 3 .

Using this and Lemma 2 from [36] (see Lemma 7 in the Section IV-A) we get that E exp

ηk

2 2

σ2

|{ηj }jk=−01

≤ exp(1),

k

where σk2 ≤ rCkσ+ψ21 ≤ αk+1Clεn( Nδ ) , where αk+1 is deﬁned in (19), C and C are some positive constants. Moreover, ak depends

only on η0, . . . , ηk−1. Putting all together in (42) and changing the indices we get, for all l = 1, ..., N ,

1 R2 ≤ 1 R2 + l−1 α

l−1
ηk+1, ak + α

ηk+1 2.

2l 2y

k+1

k+1

2

k=0

k=0

Next we

apply the

Lemma

5

with

the

constants A =

21 R02, u = 1, c = 1, D =

1 2L

,

d

=

1

and

using

ε≤

H LR20 N2

which holds

for some positive constant H due to our choice of N , and get that with probability at least 1 − 2δ the inequalities

Rl ≤ J Ry and

(43)

l−1

l−1

kk

2

k2

CHJ g(N ) 2

αk+1 η , a + αk+1 η 2 ≤ 12CH + C1

2

Ry ,

(44)

k=0

k=0

hold for all l

=

1, . . . , N

simultaneously, where C1 is some positive constant, g(N )

=

ln(

N δ

)+ln

ln(

B b

)

,

B

=

ln(

N δ

)

C H R02

2R02

+

18C L

εN (N

+

3)

2N , b = σ02α21R02 and

J = max 1, C1

CHg(N ) + 2

C12CHg(N ) + 1 + 24CH . 2

To estimate the duality gap we need again refer to (38). Since λ is chosen arbitrary we can take the minimum in λ by the set B2Ry (0) = {λ : λ 2 ≤ 2Ry}

N

1

N −1 02

k+1

k+1 k+1

k+1

AN ψ(y ) ≤ λ∈Bm2Riny (0) 2 λ − ζ 2 + k=0 αk+1 ψ(λ ) + ∇Ψ(λ , ξ ), λ − λ

N −1
+ Ak ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), yk − λk+1

k=0

+ N−1 Ak k=0 2Lψ

∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1)

2 2

(39)

N −1

≤ 2Ry2 + min

αk+1 ψ(λk+1) + ∇Ψ(λk+1, ξk+1), λ − λk+1

λ∈B2Ry (0) k=0

N −1
+ αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λk+1 − ζk

k=0

+ N −1 Ak+1 k=0 2Lψ

∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 22,

where we also used

1 2

λ − ζN

2 2

≥

0

and

ζ0

=

0.

By

adding

and

subtracting

minimum in (45) we obtain

N −1 k=0

αk+1

∇ψ(λk+1), λ∗ − λk+1

(45) under

N −1

min

αk+1

λ∈B2Ry (0) k=0

ψ(λk+1) +

∇Ψ(λk+1, ξk+1), λ − λk+1

≤ min
λ∈B2Ry (0)

N −1 k=0

αk+1

ψ(λk+1) +

∇ψ(λk+1), λ − λk+1

N −1

+ max

αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λ

λ∈B2Ry (0) k=0

N −1
+ αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), −λk+1 .
k=0

Since −λ∗ ∈ B2R (0) we have that y

N −1
αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), −λk+1
k=0

N −1

=

αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λ∗ − λk+1

k=0

N −1
+ αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), −λ∗

k=0

N −1

≤

αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λ∗ − λk+1

k=0

N −1

+ max

αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λ .

λ∈B2Ry (0) k=0

Putting all together in (45) and using (6) we get

N −1

AN ψ(yN ) ≤ 2Ry2 + min

αk+1 ψ(λk+1) + ∇ψ(λk+1), λ − λk+1

λ∈B2Ry (0) k=0

N −1

+2 max

αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λ

λ∈B2Ry (0) k=0

N −1
+ αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), ak

k=0

N −1
+ α2k+1 ∇ψ(λk+1) − ∇Ψ(λk+1, ξk+1) 22,

k=0

(46)

where ak = λ∗ − ζk. From (44) we have that with probability at least 1 − 2δ the following inequality holds:

N −1

AN ψ(yN ) ≤ min

αk+1 ψ(λk+1) + ∇ψ(λk+1), λ − λk+1

λ∈B2Ry (0) k=0

N −1

+2 max

αk+1 ∇Ψ(λk+1, ξk+1)−∇ψ(λk+1), λ

λ∈B2Ry (0) k=0

+2Ry2 + 12CH + C1 CHJ2g(N ) Ry2 . (47)

By the deﬁnition of the norm we get

N −1

max

αk+1 ∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1), λ

λ∈B2Ry (0) k=0

≤ 2Ry

N −1
αk+1(∇Ψ(λk+1, ξk+1)−∇ψ(λk+1) .

k=0

2

(48)

Next we apply Lemma 9 to the r.h.s of previous inequality and get



 N−1

P  αk+1(∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1) ≥

k=0

2

√√ 2 + 2γ



N −1
α2

σψ2  ≤ exp − γ2 .

k=0 k+1 rk+1  3

Since N 2 ≤ HLεψR20 and rk = Ω max 1, σψ2 αk lεn(N/δ)

one can choose such C2 > 0 that σψ2 ≤ C2ε ≤ HLψC2R20 .

rk

αk

ln(

N δ

)

αk

N

2

ln(

N δ

)

Let us choose γ such that exp − γ32 = δ : γ = 3 ln(1/δ). From this we get that with probability at least 1 − δ

N −1
αk+1(∇Ψ(λk+1, ξk+1) − ∇ψ(λk+1))

k=0

2

√

1

H LψC2 N−1 αk+1

≤ 2 1 + ln δ Ry ln N

N2

δ

k=0

(11) √

N−1 k + 2

≤ 2 2Ry HLψC2

2LψN 2 = 2Ry HC2

k=0

≤ 4Ry C2.

N (N + 3) N2
(49)

Putting all together and using union bound we get that with probability at least 1 − 3δ

AN ψ(yN )

(47)+(48)+(49)
≤

N −1

min

αk+1

λ∈B2Ry (0) k=0

ψ(λk+1) +

∇ψ(λk+1), λ − λk+1

+ 8 HC2 + 2 + 12CH + C1 CHJ2g(N ) Ry2. (50)

This brings us to the ﬁnal part of the proof. Firstly, by deﬁnition of ψ(λk) and Demyanov–Danskin’s theorem we have

ψ(λk) − ∇ψ(λk), λk

= λk, √W x(√W λk) − F (x(√W λk)) − ∇ψ(λk), λk = −F (x(√W λk)).

Summing up this equality for k = 1, . . . , N with weights αk and using convexity of F we get

N −1

k+1

k+1 k+1

N −1 αk+1

√ k+1

αk+1(ψ(λ ) − ∇ψ(λ ), λ ) = −AN

AN F (x( W λ ))

k=0

k=0

≤

−A F

N −1

αk+1

√ x( W

λk+1 )

= −A F (xˆ ),

(51)

N

AN

N

N

k=0

where xˆN d=ef A1N Nk=−01 αk+1x(√W λk+1). Secondly, by deﬁnition of the norm

N −1

min

αk+1

λ∈B2Ry (0) k=0

∇ψ(λk+1), λ

N −1

= min

αk+1∇ψ(λk+1), λ

λ∈B2Ry (0) k=0

= −2R A

1 N−1 α

∇ψ(λk+1)

y N AN

k+1

k=0

2

= −2R A

1 N−1 α

√W x(√W λk+1)

y N AN

k+1

√ k=0

2

= −2RyAN W xˆN 2.

(52)

Combining inequalities (50), (51) and (52) we obtain that with probability at least 1 − 3δ

(50)

N −1

N −1

AN ψ(yN ) ≤

αk+1(ψ(λk+1) − ∇ψ(λk+1), λk+1 ) + min

αk+1 ∇ψ(λk+1), λ

k=0

λ∈B2Ry (0) k=0

+ 8 HC2 + 2 + 12CH + C1 CHJ2g(N ) Ry2

(51)+(52)
≤

√ −AN F (xˆN ) − 2RyAN W xˆN 2 +

8 HC2 + 2 + 12CH + C1

CHJg(N ) 2

Ry2. (53)

Lemma 9 states that for all γ > 0





 N−1

P

αk+1

x(√W λk+1, ξk+1) − x(√W λk+1)

≥ (√2 + √2γ) N−1 α2k+1σx2  ≤ exp − γ2 .

rk+1 

3

k=0

2

k=0

Taking γ = 3 ln 1δ and using rk ≥ σψ2 αCk2lεn Nδ we get that with probability at least 1 − δ

xN − xˆN

1 N−1

=

α

x(√W λk+1, ξk+1) − x(√W λk+1)

2

AN

k+1

k=0

2

√

≤ 2 1 + 3 ln 1

AN

δ

N −1 α2k+1σx2 k=0 rk2+1

≤2 AN

6 ln 1 δ

1

ln

N δ

N−1 C2αk+1ε k=0 λmax(W )

2

6C2

N−1 (k + 2)HLψRy2 2Ry 6C2H

≤ AN λmax(W )

2LψN 2 ≤ AN λmax(W ) .

(54)

k=0

It implies that with probability at least 1 − δ

√

√

W xN − W xˆN 2 ≤

√ W 2 · xN − xˆN 2

(54)
≤

λmax(W ) 2Ry

6C2H = 2Ry 6C2H

(55)

AN λmax(W ) AN

and due to triangle inequality with probability ≥ 1 − δ

√

√

√

√

2RyAN W xˆN 2 ≥ 2RyAN W xN 2 − 2RyAN W xˆN − W xN 2

(55)

√

≥ 2RyAN W xN 2 − 4Ry2 6C2H.

(56)

Now we want to apply Lipschitz-continuity of F on the ball BRF (0) and specify our choice of RF . Recall that x(λ) d=ef

argmaxx∈Rnm { λ, x − F (x)} and due to Demyanov-Danskin theorem x(λ) = ∇ϕ(λ). Together with Lϕ-smoothness of

ϕ it implies that

√

√

√

√

√

x( W λk+1) 2 =

∇ϕ(

W λk+1) 2 ≤

∇ϕ(

W λk+1) − ∇ϕ(

W

y∗) √

2

+

∇ϕ(

W y∗) 2

≤ Lϕ √W λk+1 − √W y∗ 2 + x(√W y∗) 2 ≤ λmaxµ( W ) λk+1 − y∗ 2 + Rx

From this and (43) we get that with probability at least 1 − 2δ the inequality

√

√ (43) λmax( W )J Rx

x( W λk+1) 2 ≤

+

Ry

(57)

µ

Ry

holds for all k = 0, 1, 2, . . . , N − 1 simultaneously since λk+1 ∈ BRk (y∗) ⊆ BRk+1(y∗). Using the convexity of the norm we get that with probability at least 1 − 2δ

1 N−1

√

√ (57) λmax( W )J R

xˆN 2 ≤

αk+1 x( W λk+1) 2 ≤

+ x Ry.

(58)

AN k=0

µ

Ry

We notice that the last inequality lies in the same probability event when (43) holds.

Consider the probability event E = {inequalities (53) − (58) hold simultaneously}. Using union bound we get that

P{E} ≥ 1 − 4δ. Combining (54) and (58) we get that inequality

√

xN 2 ≤ xN − xˆN 2 + xˆN 2 ≤ 2

6C2H + λmax( W )J + Rx Ry

(59)

AN λmax(W )

µ

Ry

lies in the event E. Here we can specify our choice of RF : RF should be at least A2N

+ + 6C2H

√ λmax( W )J Rx

λmax(W )

µ

Ry

Ry .

Then we get that the fact that points xN and xˆN lie in BRF (0) is a consequence of E. Therefore, we can apply Lipschitzcontinuity of F for the points xN and xˆN and get that inequalities

|F (xˆN ) − F (xN )| ≤ LF

xˆN − xN

2

(54)
≤

2LF Ry

6C2H

(60)

AN

λmax(W )

and

AN F (xˆN ) = AN F (xN ) + AN

F (xˆN ) − F (xN )

(60)
≥

AN

F

(xN

)

−

2LF

Ry

6C2H

(61)

λmax(W )

also lie in the event E. It remains to use inequalities (56) and (61) to bound ﬁrst and second terms in the right hand side of inequality (53) and obtain that with probability at least 1 − 4δ

√ AN ψ(yN ) + AN F (xN ) + 2RyAN W xN 2 ≤

4 6C2H + 2LF Ry

6C2H + 8 HC2 λmax(W )

+2 + 12CH + C1 CHJ2g(N ) Ry2. (62)

Using

that

AN

grows

as

Ω(N 2/Lψ)

[39],

Lψ

≤

λmax(W ) µ

and,

as

in

the

Section

IV-B,

Ry

≤

∇λ+F (x(W∗))22 , we obtain that

the

choice

of

N

in

the

theorem

statement

guarantees

that

the

r.h.s.

of

the

last

inequality

is

no

min
greater than

εAN .

By weak

duality −F (x∗) ≤ ψ(y∗), we have with probability at least 1 − 4δ

F (xN ) − F (x∗) ≤ F (xN ) + ψ(y∗) ≤ F (xN ) + ψ(yN ) ≤ ε.

(63)

√

Since y∗ is an optimal solution of the dual pro√blem, we have, for any√x, F (x∗) ≤ F (x√)− y∗, W x . Then using assumption

y∗ 2 ≤ Ry, Cauchy-Schawrz inequality y, W x ≥ − y∗ 2 · W x 2 = −Ry W x 2 and choosing x = xN , we get

√

F (xN ) ≥ F (x∗) − Ry W xN 2

(64)

Using this and weak duality −F (x∗) ≤ ψ(y∗), we obtain √
ψ(yN ) + F (xN ) ≥ ψ(y∗) + F (xN ) ≥ −F (x∗) + F (xN ) ≥ −Ry W xN 2,

which implies that inequality

√W xN

(62)+(63)
2≤

ε

(65)

Ry

holds together with (63) with probability at least 1 − 4δ. Number of communication rounds is equal to the number of

iterations similarly as for Algorithm 1. The total number of stochastic gradient oracle calls is

N k=1

rk ,

which

gives

the

bound in the problem statement since

N k=1

αk+1

=

AN .

