arXiv:1911.07363v6 [math.OC] 11 Nov 2020

Optimal Decentralized Distributed Algorithms for Stochastic Convex Optimization
Eduard Gorbunov1,2,3, Darina Dvinskikh2,3,4, and Alexander Gasnikov1,3,5
1Moscow Institute of Physics and Technology, Russia 2Sirius University of Science and Technology, Russia 3Institute for Information Transmission Problems RAS, Russia 4Weierstrass Institute for Applied Analysis and Stochastics, Germany 5National Research University Higher School of Economics, Russia
February 16, 2020∗

Abstract
We consider stochastic convex optimization problems with aﬃne constraints and develop several methods using either primal or dual approach to solve it. In the primal case, we use a special penalization technique to make the initial problem more convenient for using optimization methods. We propose algorithms to solve it based on Similar Triangles Method [30, 67] with Inexact Proximal Step for the convex smooth and strongly convex smooth objective functions and methods based on Gradient Sliding algorithm [55] to solve the same problems in the nonsmooth case. We prove the convergence guarantees in the smooth convex case with deterministic ﬁrst-order oracle.
We propose and analyze three novel methods to handle stochastic convex optimization problems with aﬃne constraints: SPDSTM, SSTM sc and R-RRMA-AC-SA2. We develop convergence analysis for these methods for the unbiased (for R-RRMA-AC-SA2) and biased (for SPDSTM and SSTM sc) stochastic oracles.
Finally, we apply all aforementioned results and approaches to solve the decentralized distributed optimization problem and discuss the optimality of the obtained results in terms of communication rounds and the number of oracle calls per node.

Contents

1 Introduction

3

1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.2 Outline of the Paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

2 Notation and Deﬁnitions

5

3 Optimal Bounds for Stochastic Convex Optimization

6

∗The content of this version is the same as in the version from February 16, 2020. The changes are only in the restructuring of the paper.

1

4 Stochastic Convex Optimization with Aﬃne Constraints: Primal Approach

8

5 Stochastic Convex Optimization with Aﬃne Constraints: Dual Approach

11

5.1 Convex Dual Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

5.2 Strongly Convex Dual Functions and Restarts Technique . . . . . . . . . . . . . . . . 13

5.3 Direct Acceleration for Strongly Convex Dual Function . . . . . . . . . . . . . . . . . 19

6 Applications to Decentralized Distributed Optimization

25

7 Discussion

31

7.1 Possible Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

8 Application for Population Wasserstein Barycenter Calculation

32

8.1 Deﬁnitions and Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

8.2 SA Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

8.3 SAA Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

8.4 SA vs SAA comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

A Basic Facts

45

B Useful Facts about Duality

45

C Auxiliary Results

46

D Technical Results

47

E Similar Triangles Method with Inexact Proximal Step

51

F Missing Proofs from Section 4

57

F.1 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

F.2 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

G Missing Lemmas and Proofs from Section 5.1

59

G.1 Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

G.2 Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

H Missing Proofs from Section 5.2

79

H.1 Proof of Theorem 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

H.2 Proof of Corollary 5.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

I Missing Proofs from Section 5.3

83

I.1 Proof of Lemma 5.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

I.2 Proof of Lemma 5.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

I.3 Proof of Theorem 5.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

I.4 Proof of Corollary 5.14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93

2

1 Introduction

In this paper, we are interested in the convex optimization problem

min f (x),

(1)

x∈Q⊆Rn

where f is a convex function and Q is closed and convex subset of Rn. More precisely, we study

particular case of (1) when the objective function f could be represented as a mathematical expec-

tation

f (x) = Eξ [f (x, ξ)] ,

(2)

where ξ is a random variable. Problems of this type play a central role in a bunch of applications of machine learning [80, 83] and mathematical statistics [84]. Typically x represents the feature vector deﬁning the model, only samples of ξ are available and the distribution of ξ is unknown. One possible way to minimize generalization error (2) is to solve empirical risk minimization or ﬁnite-sum minimization problem instead, i.e. solve (1) with the objective

fˆ(x) = m1 m f (x, ξi), (3)
i=1

where m should be suﬃciently large to approximate the initial problem. Indeed, if f (x, ξ) is convex and M -Lipschitz continuous for all ξ, Q has ﬁnite diameter D and xˆ = argminx∈Q fˆ(x), then (see [10, 81]) with probability at least 1 − β

f (xˆ) − min f (x) = O M 2D2n ln(m) ln (n/β) , (4)

x∈Q

m

and if additionally f (x, ξ) is µ-strongly convex for all ξ, then (see [24]) with probability at least

1−β

f (xˆ) − min f (x) = O M 2D2 ln(m) ln (m/β) + M 2D2 ln (1/β) . (5)

x∈Q

µm

m

In other words, to solve (1)+(2) with ε functional accuracy via minimization of empirical risk (3) it is needed to have m = Ω (M2D2n/ε2) in the convex case and m = Ω (max {M2D2/µε, M2D2/ε2}) in the µ-strongly convex case where Ω(·) hides a constant factor, a logarithmic factor of 1/β and a polylogarithmic factor of 1/ε.
Stochastic ﬁrst-order methods such as Stochastic Gradient Descent (SGD) [35, 62, 68, 73, 90] or its accelerated variants like AC-SA [55] or Similar Triangles Method (STM) [22, 30, 67] are very popular choice to solve either (1)+(2) or (1)+(3). In contrast with their cheap iterations in terms of computational cost, these methods converge only to the neighbourhood of the solution, i.e. to the ball centered at the optimality and radius proportional to the standard deviation of the stochastic estimator. For the particular case of ﬁnite-sum minimization problem one can solve this issue via variance-reduction trick [13, 34, 41, 79] and its accelerated variants [2, 94, 95]. Unfortunately, this technique is not applicable in general for the problems of type (1)+(2). Another possible way to reduce the variance is mini-batching. When the objective function is L-smooth one can accelerate the computations of batches using parallelization [14, 21, 30, 32], and it is one of the examples where centralized distributed optimization appears naturally [9].

3

In other words, in some situations, e.g. when the number of samples m is too big, it is preferable in practice to split the data into q blocks, assign each block to the separate worker, e.g. processor, and organize computation of the gradient or stochastic gradient in the parallel or distributed manner. Moreover, in view of (4)-(5) sometimes to solve an expectation minimization problem it is needed to have such a big number of samples that corresponding information (e.g. some objects like images, videos and etc.) cannot be stored on 1 machine because of the memory limitations (see Section 8 for the detailed example of such a situation). Then, we can rewrite the objective function in the following form

1q

1 si

f (x) = q fi(x), fi(x) = Eξi [f (x, ξi)] or fi(x) = si f (x, ξij).

(6)

i=1

j=1

Here fi corresponds to the loss on the i-th data block and could be also represented as an expectation or a ﬁnite sum. So, the general idea for parallel optimization is to compute gradients or stochastic gradients by each worker, then aggregate the results by the master node and broadcast new iterate or needed information to obtain the new iterate back to the workers.
The visual simplicity of the parallel scheme hides synchronization drawback and high requirement to master node [76]. The big line of works is aimed to solve this issue via periodical synchronization [47, 48, 85, 93], error-compensation [46, 86], quantization [1, 38, 39, 61, 91] or combination of these techniques [7, 60].
However, in this paper we mainly focus on another approach to deal with aforementioned drawbacks — decentralized distributed optimization [9, 49]. It is based on two basic principles: every node communicates only with its neighbours and communications are performed simultaneously. Moreover, this architecture is more robust, e.g. it can be applied to time-varying (wireless) communication networks [75].

1.1 Contributions
One can consider this paper as a continuation of work [18] where authors mentioned the key ideas that form a basis of this work. However, in this paper we provide formal proofs of some results announced in [18] together with couple of new results that were not mentioned. Our contributions include:
• Accelerated primal-dual method with biased stochastic dual oracle for convex and smooth dual problem. We extent the result from the recent work [19] to the case when we have an access to the biased stochastic gradients. We emphasize that our analysis works for the minimization on whole space and we do not assume that the sequence generated by the method is bounded. It creates extra diﬃculties in the analysis, but we handle it via advanced technique for estimating recurrences (see also [19, 33]).
• Two accelerated methods with stochastic dual oracle for strongly convex and smooth dual problem. For the case when the dual function is strongly convex with Lipschitz continuous gradient we analyze two methods: one is R-RRMA-AC-SA2 and another is SSTM sc. The ﬁrst one was described in [19], but in this paper we formally state the method and prove high probability bounds for its convergence rate. The second method is also wellknown, but to the best of our knowledge there were no convergence results for it in such

4

generality that we handle. That is, we consider SSTM sc with biased stochastic oracle applied to the unconstrained smooth and strongly convex minimization problem and prove high probability bounds for its convergence rate together with the bound for the noise level. As for the convex case, we also do not assume that the sequence generated by the method is bounded. Then we show how it can be applied to solve stochastic optimization problem with aﬃne constraints using dual oracle.
• Analysis of STM applied to convex smooth minimization problem with smooth convex composite term and inexact proximal step for unconstrained minimization. Surprisingly, but before this paper there were no analysis for STM in this case. The closest work to ours in this topic is [87], but in [87] authors considered optimization problems on bounded sets.
1.2 Outline of the Paper
In Section 2, we introduce the notation and main deﬁnitions used in the paper. Then, we discuss optimal bounds for stochastic convex optimization in Section 3. In Section 4, we present the stochastic optimization problems with aﬃne constraints and the state-of-the-art methods that solve the speciﬁc penalized unconstrained problem instead of the original one together with the novel approach which we call STP IPS that aims to solve convex smooth unconstrained minimization problems with the smooth convex composite term and inexact proximal step. Next, we consider the same type of problems but using a dual approach and develop three diﬀerent accelerated methods for this case together with the convergence analysis for each of them (Section 5). The ﬁrst one is Stochastic Primal-Dual STM (SPDSTM), and it uses a biased stochastic dual oracle to solve primal and dual problems simultaneously for the case when the primal problem is µ-strongly convex and Lipschitz continuous on some ball centered at zero. The next two methods are R-RRMA-AC-SA2 and SSTM sc, and they solve the same problem when the primal functional is additionally L-smooth using a stochastic dual oracle. The diﬀerence between them is that R-RRMA-AC-SA2 uses special restarts technique and works with unbiased stochastic oracle, while SSTM sc is directly accelerated and able to work with biased stochastic gradients. Then we show how to apply the results of the previous sections to the decentralized distributed optimization problems and derive the bounds for the proposed methods in Section 6. Finally, in Section 7, we compare bounds for the convergence rate in parallel and decentralized optimization, discuss the optimality of the obtained results, and present possible directions for future work. To illustrate how our theory works, we consider the problem of calculation of population Wasserstein barycenter in Section 8. We leave long proofs, auxiliary and technical results, and the whole section about STP IPS in the appendix.

2 Notation and Deﬁnitions

To denote standard inner product between two vectors x, y ∈ Rn we use x, y d=ef

n i=1

xiyi,

where

xi is i-th coordinate of vector x, i = 1, . . . , n. Standard Euclidean norm of vector x ∈ Rn is deﬁned

as x 2 d=ef x, x . By λmax(A) and λ+min(A) we mean maximal and minimal positive eigenvalues of matrix A ∈ Rn×n respectively and we use χ(A) d=ef λmax(A)/λ+min(A) to denote condition number of A.

To deﬁne vector of ones we use 1n d=ef (1, . . . , 1)⊤ ∈ Rn and omit the subscript n when one can recover

the dimension from the context. Moreover, we use O(·), Ω(·) and Θ(·) that deﬁne exactly the same

5

as O(·), Ω(·) and Θ(·) but besides constants factors they can hide polylogarithmical factors of the

parameters of the method or the problem. Conditional mathematical expectation with respect to

all randomness coming from random variable ξ is denoted in our paper by Eξ[·]. We use Br(y) ⊆ Rn

to denote Euclidean ball centered at y ∈ Rn with radius r: Br(y) d=ef {x ∈ Rn | x − y 2 ≤ r}. The Kronecker product of two matrices A ∈ Rm×m with elements Aij, i, j = 1, . . . , m and B ∈ Rn×n is

such mn × mn matrix C d=ef A ⊗ B that





A11B A12B A13B . . . A1mB

 A21B A22B A23B . . . A2mB 

C = 

...

...

...

...

...

. 

(7)

Am1B Am2B Am3B . . . AmmB

By In we denote n × n identity matrix and omit the subscript when the size of the matrix is obvious from the context.
Below we list some classical deﬁnitions for optimization (see, for example, [63] for the details).

Deﬁnition 2.1 (L-smoothness). Function f is called L-smooth in Q ⊆ Rn with L > 0 when it is diﬀerentiable and its gradient is L-Lipschitz continuous in Q, i.e.

∇f (x) − ∇f (y) 2 ≤ L x − y 2, ∀x, y ∈ Q.

(8)

Deﬁnition 2.2 (µ-strong convexity). Diﬀerentiable function f is called µ-strongly convex in Q ⊆ Rn with µ ≥ 0 if

f (x) ≥ f (y) + ∇f (y), x − y + µ x − y 2, ∀x, y ∈ Q.

(9)

2

2

If µ > 0 then there exists unique minimizer of f on Q which we denote by x∗, except the situations when we explicitly specify x∗ in a diﬀerent way. In the case when µ = 0, i.e. f is convex, we assume that there exists at least one minimizer x∗ of f on Q and in the case when the set of minimizers of f on the set Q is not a singleton we choose x∗ to be either arbitrary or closest to the starting point of a method. When we consider some optimization method with a starting point x0 we use R or R0 to denote the Euclidean distance between x0 and x∗.

3 Optimal Bounds for Stochastic Convex Optimization

In this section our goal is to present the overview of the optimal methods and their convergence rates for the stochastic convex optimization problem (1)+(2) in the case when the gradient of the objective function is available only through (possibly biased) stochastic estimators with “light tails” or, equivalently, with σ2-sub-Gaussian variance. That is, we are interested in the situation when for an arbitrary x ∈ Q one can get such stochastic gradient ∇f (x, ξ) that

Eξ [∇f (x, ξ)] − ∇f (x) 2 ≤ δ,

(10)

Eξ exp ∇f (x, ξ) − σE2ξ [∇f (x, ξ)] 22 ≤ exp(1), (11)

6

Assumptions on f µ-strongly convex,
L-smooth
L-smooth
µ-strongly convex, ∇f (x) 2 ≤ M ∇f (x) 2 ≤ M

Method R-STM
STM MD MD

Citation [30, 67] [30, 67] [8, 42] [8, 42]

# of oracle calls O Lµ ln µRε 2

O

LR2 ε

O

M2 µε

O

M 2R2 ε2

Table 1: Optimal number N of deterministic ﬁrst-order oracle calls in order to get such a point xN that f (xN ) − f (x∗) ≤ ε. First column contains assumptions on f in addition to the convexity. MD
states for Mirror Descent.

where δ ≥ 0 and σ ≥ 0. If σ = 0, let us suppose that ∇f (x, ξ) = Eξ [∇f (x, ξ)] almost surely in ξ. When σ = δ = 0 we get that ∇f (x, ξ) = ∇f (x) almost surely in ξ which is equivalent to the deterministic ﬁrst-order oracle. For clarity, we start with this simplest case of stochastic oracle and provide an overview of the state-of-the-art results for this particular case in Table 1. Note that for the methods mentioned in the table number of oracle calls and number of iterations are identical. In the case when the gradient of f is bounded it is often enough to assume this only in some ball centered at the optimality point x∗ with radius proportional to R [28, 65, 67].
In this paper we are mainly focus on smooth optimization problems and use diﬀerent modiﬁcations of Similar Triangles Method (STM) since it gives optimal rates in this case and it is easy enough to analyze at least in the deterministic case. For convenience, we state the method in this section as Algorithm 1. Interestingly, if we run STM with µ > 0 to solve (1) with µ-strongly convex and
Algorithm 1 Similar Triangles Methods (STM), the case when Q = Rn
Input: x˜0 = z0 = x0, number of iterations N , α0 = A0 = 0 1: for k = 0, . . . , N do 2: Set αk+1 = (1+Akµ)/2L + (1+Akµ)/4L2 + Ak(1+Akµ)/L, Ak+1 = Ak + αk+1 3: x˜k+1 = / (Akxk+αk+1zk) Ak+1 4: zk+1 = zk − ∇f (x˜k+1) − µx˜k+1 αk+1/(1+µ) 5: xk+1 = / (Akxk+αk+1zk+1) Ak+1 6: end for
Output: xN

L-smooth objective, it will return xN such that f (xN ) − f (x∗) ≤ ε after N = O L/µ ln (LR2/ε)
iterations which is not optimal, see1 Table 1. To match the optimal bound in this case one should use classical restart of STM which is run with µ = 0 [30].
We notice that another highly widespread in machine learning applications type of problems is regularized or composite optimization problem

min f (x) + h(x),

(12)

x∈Q

1In some places we put references not to the ﬁrst work where this bound was shown but to the works where this complexity bound was shown for either more convenient or more relevant to our work method.

7

where h is a convex proximable function. For this case STM can be generalized via modifying the update rule in the following way [30, 67]:

zk+1 = argmin

1

z − z0

k+1
2+ α

∇f (x˜l), z − x˜l + h(z) + µ z − x˜l 2

.

(13)

z∈Q 2

2

l

l=0

2

2

We address such problems with Lh-smooth composite term in the Appendix, see Section E for the details.
Next, we go back to the problem (1)+(2) and consider more general case when δ = 0 and σ2 > 0. In this case one can construct unbiased estimator

r

1r

∇f (x, {ξi}i=1) = r ∇f (x, ξi),

i=1

where ξ1, . . . , ξr are i.i.d. samples and ∇f (x, {ξi}ri=1) has r times smaller variance than ∇f (x, ξi):

Eξ1,...,ξr exp

∇f (x, {ξi}ri=1) − ∇f (x)

2 2

σ2/r

≤ exp(1).

Then in order to get such a point xN that f (xN ) − f (x∗) ≤ ε with probability at least 1 − β where β ∈ (0, 1) and f is µ-strongly convex (µ ≥ 0) and L-smooth one can run STM for

N = O min LR2 , L ln LR2 (14)

εµ

ε

iterations with small modiﬁcation: instead of using ∇f (x˜k+1) the method uses mini-batched stochastic approximation ∇f (x˜k+1, {ξi}ri=k+11) where the batch size is

σ2αk+1

ln

N β

rk+1 = Θ max 1, (1 + Ak+1µ)ε .

(15)

The total number of oracle calls is

N

σ2R2

LR2/ε σ2

LR2

L/µ

rk = O N + min ε2 ln β , µε ln ε ln β

(16)

k=1

which is optimal up to logarithmic factors. We call this modiﬁcation Stochastic STM (SSTM). As for the deterministic case we summarize the state-of-the-art results for this case in Table 2.

4 Stochastic Convex Optimization with Aﬃne Constraints: Primal Approach

Now, we are going to make a step towards decentralized distributed optimization and consider convex optimization problem with aﬃne constraints:

min f (x),

(17)

Ax=0,x∈Q

8

Assumptions on f µ-strongly convex,
L-smooth

L-smooth

µ-strongly convex,

Eξ

∇f (x, ξ)

2 2

≤ M2

Eξ

∇f (x, ξ)

2 2

≤ M2

Method R-SSTM
SSTM MD MD

Citation [30, 54, 67] [30, 54, 67]
[8, 42] [8, 42]

# of oracle calls O max Lµ ln µRε 2 , σµ2ε

O max

LR2 ε

,

σ2 R2 ε2

O

M2 µε

O

M 2R2 ε2

Table 2: Optimal (up to logarithmic factors) number of stochastic unbiased ﬁrst-order oracle calls in order to get such a point xN that f (xN ) − f (x∗) ≤ ε with probability at least 1 − β, β ∈ (0, 1)
and f is deﬁned in (2). First column contains assumptions on f in addition to the convexity. Blue
terms in the last column correspond to the number of iterations of the method.

where A 0 and KerA = {0}. Up to a sign we can deﬁne the dual problem in the following way

min ψ(y),

where

(18)

y

ϕ(y) = max { y, x − f (x)} ,

(19)

x∈Q

ψ(y) = ϕ(A⊤y) = max { y, Ax − f (x)} = A⊤y, x(A⊤y) − f (x(A⊤y)), (20)
x∈Q

where x(y) d=ef argmaxx∈Q { y, x − f (x)}. Since KerA = {0} the solution of the dual problem (18)

is not unique. We use y∗ to denote the solution of (18) with the smallest ℓ2-norm Ry d=ef y∗ 2.

However, in this section we are interested only in primal approaches to solve (17) and, in

particular, the main goal of this section is to present ﬁrst-order methods that are optimal both in

terms of ∇f (x) and A⊤Ax calculations. Before we start our analysis let us notice that typically

in decentralized optimization matrix A from (17) is chosen as a square root of Laplacian matrix

W of√communication network [76] (see Section 6 for the details). In asynchronous case the square root W is replaced by incidence matrix M [37] (W = M ⊤M ). Then in asynchronous case

instead of accelerated methods for (18) one should use accelerated block-coordinate descent methods

[22, 27, 37, 82].

To solve problem (17) we use the following trick [18, 28]: instead of (17) we consider penalized

problem

min F (x) = f (x) + Ry2 Ax 2 ,

(21)

x∈Q

ε

2

where ε > 0 is the desired accuracy of the solution in terms of f (x) that we want to achieve. The motivation behind this trick is revealed in the following theorem.

Theorem 4.1 (See also Remark 4.3 from [28]). Assume that xN ∈ Q is such that

F (xN ) − min F (x) ≤ ε.

(22)

x∈Q

9

Then

f (xN ) − min f (x) ≤ ε, AxN ||2 ≤ 2ε .

(23)

Ax=0,x∈Q

Ry

We start with the analysis of the case when f is L-smooth and convex.

Theorem 4.2. Let f be convex and L-smooth, Q = Rn and h(x) = R2y Ax 22/ε. Assume that full gradients of f and h are available. Then STM IPS (see Algorithm 8, Section E) applied to solve problem (21) requires
O LR2 calculations of ∇f (x), (24) ε

O LR2 χ(A⊤A) calculations of A⊤Ax

(25)

ε

to produce point xN such that (22) holds.

That is, number of A⊤Ax calculations matches the optimal bound for deterministic convex and

L-smooth problems of type (1) multiplied by χ(A⊤A) up to logarithmic factors (see Table 1).

We believe that using the same recurrence technique that we use in Sections E and 5 one can

generalize this result for the case when instead of ∇f (x) only stochastic gradient ∇f (x, ξ) (see

inequalities (10)-(11)) is available. To the best of our knowledge it is not done in the literature for

the case when Q = Rn. Moreover, it is also possible to extend our approach to handle strongly

convex case via variants of STM.

We conjecture that the same technique in the case when f is µ-strongly convex and L-smooth

gives the method that requires such number of A⊤Ax calculations that matches the second rows of

Tables 1 and 2 in the corresponding cases with additional factor χ(A⊤A) and logarithmic factors.

Recently such bounds were shown in [23] for the distributed version of Multistage Accelerated

Stochastic Gradient method from [6]. However, this bounds were shown for the case when the

stochastic gradient is unbiased.

Next, we assume that Q is closed and convex and f is µ-strongly convex, but possibly non-

smooth function with bounded gradients: ∇f (x) 2 ≤ M for all x ∈ Q. Let us start with the case

µ = 0. Then, to achieve (22) one can run Sliding method from [54, 56] considering f (x) as a

composite term. In this case Sliding requires





O  λmax(A⊤A)Ry2R2  calculations of A⊤Ax,

(26)

ε2

M 2R2

O ε2 calculations of ∇f (x).

(27)

In the case when Q is a compact set and ∇f (x) is not available and unbiased stochastic gradient
∇f (x, ξ) is used instead (see inequalities (10)-(11) with δ = 0) one can show [54, 56] that Stochastic
Sliding (S-Sliding) method can achieve (22) with probability at least 1 − β, β ∈ (0, 1), and it requires the same number of calculations of A⊤Ax as in (26) up to logarithmic factors and

(M 2 + σ2)R2

O

ε2

calculations of ∇f (x, ξ).

(28)

10

When µ > 0 one can apply restarts technique on top of S-Sliding (RS-Sliding) [18, 89] and

get that to guarantee (22) with probability at least 1 − β, β ∈ (0, 1) RS-Sliding requires





O  λmax(A⊤A)Ry2  calculations of A⊤Ax,

(29)

µε

O M 2 + σ2 calculations of ∇f (x, ξ). (30) µε
We notice that bounds presented above for the non-smooth case are proved only for the case when Q is bounded. For the case of unbounded Q the convergence results with such rates were proved only in expectation. Moreover, it would be interesting to study S-Sliding and RS-Sliding in the case when δ > 0, i.e. stochastic gradient is biased, but we leave these questions for future works.

5 Stochastic Convex Optimization with Aﬃne Constraints: Dual Approach

In this section we assume that one can construct a dual problem for (17). If f is µ-strongly convex
in ℓ2-norm, then ψ and ϕ have Lψ–Lipschitz continuous and Lϕ–Lipschitz continuous in ℓ2-norm gradients respectively [45, 74], where Lψ = λmax(A⊤A)/µ and Lϕ = 1/µ. In our proofs we often use Demyanov–Danskin theorem [74] which states that

∇ψ(y) = Ax(A⊤y), ∇ϕ(y) = x(y).

(31)

We notice that in this section we do not assume that A is symmetric or positive semideﬁnite.
Below we propose a primal-dual method for the case when f is additionally Lipschitz continuous
on some ball and two methods for the problems when the primal function is also L-smooth and Lipschitz continuous on some ball. In the subsections below we assume that Q = Rn.

5.1 Convex Dual Function

In this section we assume that the dual function ϕ(y) could be rewritten as an expectation, i.e.
ϕ(y) = Eξ [ϕ(y, ξ)], where stochastic realisations ϕ(y, ξ) are diﬀerentiable in y functions almost surely in ξ. Then, we can also represent ψ(y) as an expectation: ψ(y) = Eξ [ψ(y, ξ)]. Consider the stochastic function f (x, ξ) which is deﬁned implicitly as follows:

ϕ(y, ξ) = max { y, x − f (x, ξ)} .

(32)

x∈Rn

Similarly to the deterministic case we introduce x(y, ξ) d=ef argmaxx∈Rn { y, x − f (x, ξ)} which satisﬁes ∇ϕ(y, ξ) = x(y, ξ) due to Demyanov-Danskin theorem, where the gradient is taken w.r.t. y. As a simple corollary, we get ∇ψ(y, ξ) = Ax(A⊤y). Finally, introduced notations and obtained
relations imply that x(y) = Eξ[x(y, ξ)] and ∇ψ(y) = Eξ[∇ψ(y, ξ)]. Consider the situation when x(y, ξ) is known only through the noisy observations x˜(y, ξ) =
x(y, ξ) + δ(y, ξ) and assume that the noise is bounded in expectation, i.e. there exists non-negative
deterministic constant δy ≥ 0, such that

Eξ[δ(y, ξ)] 2 ≤ δy, ∀y ∈ Rn.

(33)

11

Assume additionally that x(y, ξ) satisﬁes so-called “light-tails” inequality:

E exp

x˜(y, ξ) − Eξ [x˜(y, ξ)]

2 2

≤ exp(1), ∀y ∈ Rn,

(34)

ξ

σx2

where σx is some positive constant. It implies that we have an access to the biased gradient ∇˜ ψ(y, ξ) d=ef Ax˜(y, ξ) which satisﬁes following relations:

Eξ ∇˜ ψ(y, ξ) − ∇ψ(y) ≤ δ, ∀y ∈ Rn,

(35)



2 2

  ∇˜ ψ(y, ξ) − Eξ ∇˜ ψ(y, ξ) 2 

d

Eξ exp 

σ2

 ≤ exp(1), ∀y ∈ R ,

(36)

ψ

where δ d=ef λmax(A⊤A)δy and σψ d=ef stochastic gradient:

λmax(A⊤A)σx. We will use ∇˜ Ψ(y, ξk) to denote batched

∇˜ Ψ(y, ξk) = 1

rk
∇˜ ψ(y, ξl),

x˜(y, ξk) = 1 rk x˜(y, ξl)

(37)

rk l=1

rk l=1

The size of the batch rk could always be restored from the context, so, we do not specify it here. Note that the batch version satisﬁes

E ∇˜ Ψ(x, ξk) − ∇ψ(x) ≤ δ, ∀x ∈ Rn,

(38)



2 2

  ∇˜ Ψ(x, ξk) − E ∇˜ Ψ(x, ξk) 2 

n

E exp 

O(σψ2 /r2 )

 ≤ exp(1), ∀x ∈ R ,

(39)

k

where in the last inequality we used combination of Lemmas C.1 and C.3 (see two inequalities after (176) for the details). We call this approach SPDSTM (Stochastic Primal-Dual Similar Triangles Method, see Algorithm 2). Note that Algorithm 4 from [19] is a special case of SPDSTM when δ = 0, i.e. stochastic gradient is unbiased, up to a factor 2 in the choice of L˜.

Algorithm 2 SPDSTM

Input: y˜0 = z0 = y0 = 0, number of iterations N , α0 = A0 = 0

1: for k = 0, . . . , N do 2: Set L˜ = 2Lψ 3: Set Ak+1 = Ak + αk+1, where 2L˜α2k+1 = Ak + αk+1 4: y˜k+1 = / (Akyk+αk+1zk) Ak+1 5: zk+1 = zk − αk+1∇˜ Ψ(y˜k+1, ξk) 6: yk+1 = / (Akyk+αk+1zk+1) Ak+1

7: end for Output: yN , x˜N = A1N

N k=0

αk

x˜(A⊤

y˜k

,

ξ

k

).

Below we present the main convergence result of this section.

12

Theorem 5.1 (see also Theorem 2 from [19]). Assume that f is µ-strongly convex and ∇f (x∗) 2 =

Mf . Let ε > 0 be a desired accuracy. Next, assu√me that f is Lf -Lipschitz continuous on the

ball BRf (0) with Rf = Ω˜ max

√ Ry

,

AN λmax(A⊤A)

λmax

(A⊤ µ

A)Ry

,

Rx

, where Ry is such that

y∗ 2 ≤ Ry, y∗ is the solution of the dual problem (18), and Rx = x(A⊤y∗) 2. Assume that at

iteration k of Algorithm 2 batch size is chosen according to the formula rk ≥ max 1, σψ2 αk ˆln(N/β) ,
Cε

where

αk

=

k2+L˜1 ,

0<ε≤

HNL˜R2 20 ,

0≤δ

≤

GL˜ R0 (N +1)2

and

N

≥1

for

some

numeric

constant

H

> 0,

G > 0 and Cˆ > 0. Then with probability ≥ 1 − 4β, where β ∈ (0, 1/4) is such that 1+ ln β1 ≤ 2,

ln

N β

after N = O

Mf µε

χ(A⊤

A)

iterations where χ(A⊤A) = λλm+ax((AA⊤⊤AA)) , the outputs x˜N and yN of

min

Algorithm 2 satisfy the following condition

f (x˜N ) − f (x∗) ≤ f (x˜N ) + ψ(yN ) ≤ ε, Ax˜N 2 ≤ ε

(40)

Ry

with probability at least 1 − 4β. What is more, to guarantee (40) with probability at least 1 − 4β Algorithm 2 requires

O

max

σ

2 x

Mf2

χ

(A⊤

A)

ln

1

Mf χ(A⊤A) ,

Mf χ(A⊤A)

(41)

ε2

β µε

µε

calls of the biased stochastic oracle ∇˜ ψ(y, ξ), i.e. x˜(y, ξ).

5.2 Strongly Convex Dual Functions and Restarts Technique

In this section we assume that primal functional f is additionally L-smooth. It implies that the

dual function ψ in (18) is additionally µψ-strongly convex in y0 +(KerA⊤)⊥ where µψ = λ+min(A⊤A)/L [45, 74] and λ+min(A⊤A) is the minimal positive eigenvalue of A⊤A.
From weak duality −f (x∗) ≤ ψ(y∗) and (20) we get the key relation of this section (see also

[3, 4, 66])

f (x(A⊤y)) − f (x∗) ≤ ∇ψ(y), y = Ax(A⊤y), y

(42)

This inequality implies the following theorem.

Theorem 5.2. Consider function f and its dual function ψ deﬁned in (20) such that problems (17) and (18) have solutions. Assume that yN is such that ∇ψ(yN ) 2 ≤ ε/Ry and yN ≤ 2Ry, where ε > 0 is some positive number and Ry = y∗ 2 where y∗ is any minimizer of ψ. Then for xN = x(A⊤yN ) following relations hold:

f (xN ) − f (x∗) ≤ 2ε, AxN 2 ≤ ε ,

(43)

Ry

where x∗ is any minimizer of f .

13

Proof. Applying Cauchy-Schwarz inequality to (42) we get

(42)
f (xN ) − f (x∗) ≤

∇ψ(yN ) 2 · yN 2 ≤ ε · 2Ry = 2ε. Ry

The second part (43) immediately follows from ∇ψ(yN ) 2 ≤ ε/Ry and Demyanov-Danskin theorem which implies ∇ψ(yN ) = AxN .

That is why, in this section we mainly focus on the methods that provides optimal convergence

rates for the gradient norm. In particular, we consider Recursive Regularization Meta-Algorithm

from (see Algorithm 3) [25] with AC-SA2 (see Algorithm 5) as a subroutine (i.e. RRMA-AC-SA2) which

is based on AC-SA algorithm (see Algorithm 4) from [31]. We notice that RRMA-AC-SA2 is applied

for a regularized dual function

ψ˜(y) = ψ(y) + λ y − y0 2,

(44)

2

2

where λ > 0 is some positive number which will be deﬁned further. Function ψ˜ is λ-strongly convex and L˜ψ-smooth in Rn where L˜ψ = Lψ + λ. For now, we just assume w.l.o.g. that ψ˜ is (µψ + λ)-strongly convex in Rn, but we will go back to this question further.

In this section we consider the same oracle as in Section 5, but we additionally assume that

δ = 0, i.e. stochastic ﬁrst-order oracle is unbiased. To deﬁne batched version of the stochastic

gradient we will use the following notation:

∇Ψ(y, ξt, r ) = 1

rt
∇ψ(y, ξl),

x(y, ξt, r ) = 1

rt
x(y, ξl).

(45)

t rt

t rt

l=1

l=1

As before in the cases when the batch-size rt can be restored from the context, we will use simpliﬁed notation ∇Ψ(y, ξt) and x(y, ξt). In the AC-SA algorithm we use batched stochastic gradients of

Algorithm 3 RRMA-AC-SA2 [25]

Input: y0 — starting point, m — total number of iterations

1: ψ0 ← ψ˜, yˆ0 ← y0, T ←

log2

L˜ψ λ

2: for k = 1, . . . , T do

3: Run AC-SA2 for m/T iterations to optimize ψk−1 with yˆk−1 as a starting point and get the

output yˆk

4:

ψk(y) ← ψ˜(y) + λ

k l=1

2l−1

y − yˆl

2 2

5: end for

Output: yˆT .

functions ψk which are deﬁned as follows:

t

1 rt

l

∇Ψk(y, ξ ) = rt ∇ψk(y, ξ ),

(46)

l=1

k
∇ψk(y, ξ) = ∇ψ(y, ξ) + λ(y − y0) + λ 2l(y − yˆl).

l=1

The following theorem states the main result for RRMA-AC-SA2 that we need in the section.

14

Algorithm 4 AC-SA [31]

Input: z0 — starting point, m — number of iterations, ψk — objective function 1: ya0g ← z0, ym0 d ← z0

2: for t = 1, . . . , m do

3:

αt

←

t+21 ,

γt

←

4L˜ ψ t(t+1)

4:

ymt d

←

(1−αt )(λ+ γ +(1−α2

γt ) )λ

y

t− ag

1

+

αtγ((+1−(1α−t)αλ2+)λγt) zt−1

t

t

t

t

5: zt ← λα+tγλt ymt d + (1−λα+t)γλt+γt zt−1 − λ+αtγt ∇Ψk(ymt d, ξt)

6: yat g ← αtzt + (1 − αt)xta−g 1

7: end for

Output: yamg.

Algorithm 5 AC-SA2 [25] Input: z0 — starting point, m — number of iterations, ψk — objective function
1: Run AC-SA for m/2 iterations to optimize ψk with z0 as a starting point and get the output y1 2: Run AC-SA for m/2 iterations to optimize ψk with y1 as a starting point and get the output y2 Output: y2.

Theorem 5.3 (Corollary 1 from [25]). Let ψ be Lψ-smooth and µψ-strongly convex function and λ = Θ (Lψ ln2 N)/N2 for some N > 1. If the Algorithm 3 performs N iterations in totala with
batch size r for all iterations, then it will provide such a point yˆ that

E ∇ψ(yˆ) 2 | y0, r ≤ C L2ψ y0 − y∗ 22 ln4 N + σψ2 ln6 N ,

(47)

2

N4

rN

where C > 0 is some positive constant and y∗ is a solution of the dual problem (18).
aThe overall number of performed iterations during the calls of AC-SA2 equals N .

Let us show that w.l.o.g. we can assume in this section that function ψ deﬁned in (20) is
µψ-strongly convex everywhere with µψ = λ+min(A⊤A)/L. In fact, from L-smoothness of f we have only that ψ is µψ-strongly convex in y0 + Ker(A⊤) ⊥ (see [45, 74] for the details). However, the structure of the considered here methods is such that all points generated by the RRMA-AC-SA2 and, in particular, AC-SA lie in y0 + Ker(A⊤) ⊥.

Theorem 5.4. Assume that Algorithm 4 is run for the objective ψk(y) = ψ˜(y) + λ

k l=1

2l−1

y−

yˆl

2 2

with

z0

as

a

starting

point,

where

z0, yˆ1, . . . , yˆk

are

some

points

from

y0

+

Ker(A⊤) ⊥ and

y0

∈

Rn.

Then

for

all

t

≥

0

we

have

y

t md

,

z

t

,

y

t ag

∈

y0

+

Ker(A⊤) ⊥.

Proof. We prove the statement of the theorem by induction. For t = 0 the statement is trivial,

since ym0 d = ya0g = z0 ∈ y0 +

Ker(A⊤) ⊥.

Assume

that

y

t md

,

z

t

,

y

t ag

∈

y0

+

Ker(A⊤) ⊥ for some

t ≥ 0 and prove it for t + 1. Since y0 + Ker(A⊤) ⊥ is a convex set and ymt+d1 is a convex combination

of yat g and zt we have ymt+d1 ∈ y0 + Ker(A⊤) ⊥. Next, the point λα+tλγt ymt+d1 + (1−λα+t)γλt+γt zt also lies

15

in y0 + Ker(A⊤) ⊥ since it is convex combination of the points lying in this set. Due to (44),

(45)

and

(46)

we

have

that

∇

Ψ

k

(y

t+1 md

,

ξ

t

)

=

Ax

(A⊤

y

t+1 md

,

ξ

t

)

+

λ(ymt+d1

−

y0)

+

λ

k l=1

2l (ymt+d1

−

yˆl).

The ﬁrst term lies in Ker(A⊤) ⊥ since Im(A) = Ker(A⊤) ⊥ and the second and the third terms

also lie in Ker(A⊤) ⊥ since ymt+d1, y0, yˆ1, . . . , yˆk ∈ y0 + Ker(A⊤) ⊥. Putting all together we get zt+1 ∈ y0 + Ker(A⊤) ⊥. Finally, yat+g 1 lies in y0 + Ker(A⊤) ⊥ as a convex combination of points

from this set.

Corollary 5.5. Assume that Algorithm 3 is run for the objective ψk(y) = ψ˜(y)+λ

k l=1

2l−1

y−

yˆl

2 2

with

y0

as

a

starting

point.

Then

for

all

k

≥

0

we

have

yˆk

∈

y0

+

Ker(A⊤) ⊥.

Proof. We prove this result by induction. For t = 0 the statement is trivial since yˆ0 = y0. Next, assume that yˆ0, yˆ1, . . . , yˆk ∈ y0 + Ker(A⊤) ⊥ and prove that yˆk+1 ∈ y0 + Ker(A⊤) ⊥. Our
assumption implies that the assumptions from Theorem 5.4 and applying the result of the theorem we get that y1 and y2 from the method AC-SA2 applied to the ψk also lie in y0 + Ker(A⊤) ⊥. That is, the output of AC-SA2 applied for ψk lies in y0 + Ker(A⊤) ⊥.

Now we are ready to present our approach which was sketched in [18] of constructing an accel-
erated method for the strongly convex dual problem using restarts of RRMA-AC-SA2. To explain the
main idea we start with the simplest case: σψ2 = 0, r = 0. It means that there is no stochasticity in the method and the bound (47) can be rewritten in the following form:

√ C Lψ

y0 − y∗

2 ln2 N

√ C Lψ

∇ψ(y0)

2 ln2 N

∇ψ(yˆ) 2 ≤

N2

≤

µψN 2

,

(48)

where we used inequality ∇ψ(y0) ≥ µψ y0 − y∗ which follows from the µψ-strong convexity of ψ. It implies that after N¯ = O˜( Lψ/µψ) iterations of RRMA-AC-SA2 the method returns such

y¯1 = yˆ that

∇ψ(y¯1)

2

≤

1 2

∇ψ(y0)

2.

Next, applying RRMA-AC-SA2 with y¯1

as a starting point

for the same number of iterations we will get new point y¯2 such that

∇ψ(y¯2)

2

≤

1 2

∇ψ(y¯1)

2≤

1 4

∇ψ(y0)

2.

Then, after l = O(ln(Ry

∇ψ(y0) 2/ε)) of such restarts we can get the point y¯l such that

∇ψ(y¯l) 2 ≤ ε/Ry with total number of gradients computations N¯ l = O˜ Lψ/µψ ln(Ry ∇ψ(y0) 2/ε) .

In the case when σψ2 = 0 we need to modify this approach. The ﬁrst ingredient to handle the stochasticity is large enough batch size for the l-th restart: rl should be Ω (σψ2/(N¯ ∇ψ(y¯l−1) 22)). However, in the stochastic case we do not have an access to the ∇ψ(y¯l−1), so, such batch size is

impractical. One possible way to ﬁx this issue is to independently sample large enough number

rˆl ∼ R2y/ε2 of stochastic gradients additionally, which is the second ingredient of our approach, in order to get good enough approximation ∇Ψ(y¯l−1, ξl−1, rˆl) of ∇ψ(y¯l−1) and use the norm of such

an approximation which is close to the norm of the true gradient with big enough probability in order to estimate needed batch size rl for the optimization procedure. Using this, we can get the

bound of the following form:

E

∇ψ(y¯l)

2 2

| y¯l−1, rl, rˆl

≤ Al

d=ef

∇ψ(y¯l−1)

2 2

8

+

∇Ψ(y¯l−1, ξl−1, rˆl) − ∇ψ(y¯l−1)

2
2.

32

16

The third ingredient is the ampliﬁcation trick: we run pl = Ω(ln(1/β)) independent trajectories of

RRMA-AC-SA2, get points y¯l,1, . . . , y¯l,pl and choose such y¯l,p(l) among of them that ∇ψ(y¯l,p(l)) 2 is

close enough to min

∇ψ(y¯l,p) 2 with high probability, i.e.

∇ψ(y¯l,p(l))

2 2

≤2

min

∇ψ(y¯l,p) 22+

p=1,...,pl

p=1,...,pl

ε2/8R2y with probability at least 1 − β for ﬁxed ∇Ψ(y¯l−1, ξl−1, rˆl). We achieve it due to additional sampling of r¯l ∼ R2y/ε2 stochastic gradients at y¯l,p for each trajectory and choosing such p(l) corre-

sponding to the smallest norm of the obtained batched stochastic gradient. By Markov’s inequality

for all p = 1, . . . , pl

P ∇ψ(y¯l,p) 22 ≥ 2Al | y¯l−1, rl, r¯l ≤ 21 ,

hence

P

min

∇ψ(y¯l,p)

2 2

≥ 2Al

| y¯l−1, rl, r¯l

p=1,...,pl

1 ≤ 2pl .

That is, for pl = log2(1/β) we have that with probability at least 1 − 2β

∇ψ(y¯l,p(l))

2 2

≤

∇ψ(y¯l−1)

2 2

+

2

∇Ψ(y¯l−1, ξl−1, rˆl) − ∇ψ(y¯l−1)

2 2

+

ε2

8

8Ry2

for ﬁxed ∇Ψ(y¯l−1, ξl−1, rˆl) which means that

∇ψ(y¯l,p(l))

2 2

≤

∇ψ(y¯l−1)

2 2

+

ε2

2

4Ry2

with probability at least 1 − 3β. Therefore, after l = log2(2R2y ∇ψ(y0) 22/ε2) of such restarts our method provide the point y¯l,p(l) such that with probability at least 1 − 3lβ

∇ψ(y¯l,p(l))

2 2

≤

∇ψ(y0)

2 2

+

ε2

l−1
2−k ≤

ε2

+

ε2

· 2 = ε2 .

2l

4Ry2 k=0

2Ry2 4Ry2

Ry2

The approach informally described above is stated as Algorithm 6.

Theorem 5.6. Assume that ψ is µψ-strongly convex and Lψ-smooth. If Algorithm 6 is run with

2Ry2

∇ψ(y0)

2 2

l = max 1, log2

ε2



2

 4σψ2 1 +

3

ln

l β

R

2 y



rˆk = max 1,

ε2

 ,

64Cσψ2 ln6 N¯ rk = max 1, N¯ ∇Ψ(y¯k−1,p(k−1), ξk−1,p(k−1), rˆk) 2 ,
2

l

pk = max 1, log2 β



2

 128σψ2 1 +

3

ln

lpk β

R

2 y



r¯k = max 1,

ε2



(49)

17

Algorithm 6 Restarted-RRMA-AC-SA2

Input: y0 — starting point, l — number of restarts, {rˆk}lk=1, {r¯k}lk=1 — batch-sizes, {pk}lk=1 —

ampliﬁcation parameters

1:

Choose the smallest

integer

N¯

> 1 such

that

CL2ψ ln4 N¯ µ2 N¯ 4

≤

1 32

ψ

2: y¯0,p(0) ← y0

3: for k = 1, . . . , l do 4: Compute ∇Ψ(y¯k−1,p(k−1), ξk−1,p(k−1), rˆk) 5: rk ← max 1, N¯ ∇Ψ(y¯k−1,6p4(Ck−σ1ψ2),lξnk6−N¯1,p(k−1),rˆk) 22 6: Run pk independent trajectories of RRMA-AC-SA2 for N¯ iterations with batch-size rk with
y¯k−1,p(k−1) as a starting point and get outputs y¯k,1, . . . , y¯k,pk 7: Compute ∇Ψ(y¯k,1, ξk,1, r¯k), . . . , ∇Ψ(y¯k,pk, ξk,pk, r¯k) 8: p(k) ← argminp=1,...,pk ∇Ψ(y¯k,p, ξk,p, r¯k) 2 9: end for
Output: y¯l,p(l).

for

all

k

=

1, . . . , l

where

N¯

>1

is

such

that

CL2ψ ln4 N¯ µ2 N¯ 4

≤

312 ,

β

∈ (0, 1/3)

and

ε

> 0,

then

with

ψ

probability at least 1 − 3β

∇ψ(y¯l,p(l)) 2 ≤ ε

(50)

Ry

and the total number of the oracle calls equals

l
(rˆ + N¯ p r + p r¯ ) = O max

Lψ , σψ2 Ry2

.

(51)

k k=1

kk kk

µψ ε2

Corollary 5.7. Under assumptions of Theorem 5.6 we get that with probability at least 1 − 3β

y¯l,p(l) − y∗ 2 ≤ ε ,

(52)

µψ Ry

where β ∈ (0, 1/3) the total number of the oracle calls is deﬁned in (51).

Proof. Inequalities (50) and µψ y − y∗ 2 ≤ ∇ψ(y) 2 which follows from µψ-strong convexity of ψ

imply that

y¯l,p(l) − y∗ 2 ≤

∇ψ(y¯l,p(l))

2

(50)
≤

ε.

µψ

µψ Ry

Now we are ready to present convergence guarantees for the primal function and variables. Corollary 5.8. Let the assumptions of Theorem 5.6 hold. Assume that f is Lf -Lipschitz con-

18

tinuous on BRf (0) where

Rf =

µψ

+

8 λmax(A⊤A)

λmax(A⊤A) + Rx Ry

µ

Ry

and Rx = x(A⊤y∗) 2. Then, with probability at least 1 − 4β

f (xl) − f (x∗) ≤ 2 +

Lf

ε, Axl ≤ 9ε ,

(53)

8Ry λmax(A⊤A)

8Ry

where

β

∈

(0, 1/4),

ε

∈

(0,

µ

ψ

R

2 y

)

xl

d=ef

x(A⊤y¯l,p(l), ξl,p(l), r¯l)

and

to

achieve

it

we

need

the

total

number of oracle calls equals

l
(rˆ + N¯ p r + p r¯ ) = O max

L χ(A⊤A), σx2M 2 χ(A⊤A)

(54)

k

kk kk

µ

ε2

k=1

where M = ∇f (x∗) 2.

5.3 Direct Acceleration for Strongly Convex Dual Function

We consider ﬁrst the following minimization problem:

min ψ(y),

(55)

y∈Rn

where ψ(y) is µψ-strongly convex and Lψ-smooth. We use the same notation to deﬁne the objective in (55) as for the dual function from (18) because later in the section we apply the algorithm introduced below to the (18), but for now it is not important that ψ is a dual function for (17) and we prefer to consider more general situation. As in Section 5.1, we do not assume that we have an access to the exact gradient of ψ(y) and consider instead of it biased stochastic gradient ∇˜ ψ(y, ξ) satisfying inequalities (35) and (36) with δ ≥ 0 and σψ ≥ 0. In the main method of this section batched version of the stochastic gradient is used:

∇˜ Ψ(y, ξk) = 1

rk
∇˜ ψ(y, ξl),

(56)

rk l=1

where rk is the batch-size that we leave unspeciﬁed for now. Note that ∇˜ Ψ(y, ξk) satisﬁes inequalities (38) and (39).
We use Stochastic Similar Triangles Method which is stated in this section as Algorithm 7 to solve problem (55). To deﬁne the iterate zk+1 we use the following sequence of functions:

g˜0(z) d=ef 21 z − z0 22 + α0 ψ(y0) + ∇˜ Ψ(y0, ξ0), z − y0 + µ2ψ z − y0 22 , g˜k+1(z) d=ef g˜k(z) + αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), z − y˜k+1 + µ2ψ z − y˜k+1 22

=

1

z − z0

k+1
2+ α

ψ(y˜l) + ∇˜ Ψ(y˜l, ξl), z − y˜l + µψ z − y˜l 2

(57)

2

2

l

2

2

l=0

19

Algorithm 7 Stochastic Similar Triangles Methods for strongly convex problems (SSTM sc)
Input: y˜0 = z0 = y0 — starting point, N — number of iterations
1: Set α0 = A0 = 1/Lψ 2: Get ∇˜ Ψ(y0, ξ0) to deﬁne g˜0(z) 3: for k = 0, 1, . . . , N − 1 do 4: Choose αk+1 such that Ak+1 = Ak + αk+1, Ak+1(1 + Akµψ) = α2k+1Lψ 5: y˜k+1 = / (Akyk+αk+1zk) Ak+1 6: zk+1 = argminz∈Rn g˜k+1(z), where g˜k+1(z) is deﬁned in (57) 7: yk+1 = / (Akyk+αk+1zk+1) Ak+1
8: end for Output: xN

We notice that g˜k(z) is (1 + Akµψ)-strongly convex.

Lemma 5.9. Assume that Algorithm 7 is run to solve problem (55) with ψ(y) being µψ-strongly convex and Lψ-smooth. Then, for all k ≥ 0 we have

k

k k−1 Alµψ l l+1 2

Akψ(y ) ≤ g˜k(z ) −

2 y − y˜ 2

l=0

k
+

αl

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l)

2
.

(58)

l=0 2µψ 2

Lemma 5.10. Let the sequences of non-negative numbers {αk}k≥0, random non-negative variables {Rk}k≥−1, {Rk}k≥−1 and random vectors {ηk}k≥0, {ak}k≥0, {a˜k}k≥0 satisfy inequality

l−1

l

AlRl2 + AkRk2 ≤ A + hδ αk(Rk−1 + Rk)

k=0

k=0

l−1

l−1

+u αk+1 ηk, ak + a˜k + c αk+1 ηk 22,

k=0

k=0

(59)

for all l = 1, . . . , N , where h, δ, u and c are some non-negative constants and Ak+1 = Ak + αk+1, αk+1 ≤ DAk for some D ≥ 1, A0 = α0 > 0. Assume that for each k ≥ 1 vector ak is a function of η0, . . . , ηk−1, a0 is a deterministic vector, u ≥ 1, sequence of random vectors {ηk}k≥0 satisfy

E ηk | η0, . . . , ηk−1 = 0, E exp

ηk

2 2

| η0, . . . , ηk−1 ≤ exp(1),

(60)

σk2

∀k ≥ 0, σk2 ≤

Cε 2 for some C > 0, ε > 0, β ∈ (0, 1), sequences {ak}k≥0 and {a˜k}k≥0

N 2 1+

3

ln

N β

are such that ak 2 ≤ Rk and a˜k 2 ≤ Rk, Rk and Rk depend only on η0, . . . , ηk and R0 = 0. If

20

additionally δ ≤ NG√RA0N and ε ≤ HARN20 Then with probability at least 1 − 2β the inequalities

Rl ≤ √J R0 , Rl−1 ≤ J R0

(61)

Al

Al−1

and

l−1

l−1

l−1

hδ

αk+1(Rk + Rk) + u

αk+1 ηk, ak + a˜k + c

αk+1

ηk

2 2

k=0

k=0

k=0

≤ 2cHC + 2JD hG + uC1 2HCg(N ) R02

(62)

hold for all l = 1, . . . , N simultaneously, where C1 is some positive constant, g(N ) = ln Nβ +ln ln( Bb )2 ,

1+

3 ln

N β

B = 8HCDR2 N 3 N + 1

0

2

A + 2Dh2G2R02 + 2C c + 2Du2 HR02 ,

b = 2σ02α21R02 and

  J = max 

3B1D + A0,



9B12D2

+

4A R2

+

8cH C



2 0 ,

B1 = hG + uC1 2HCg(N ).

Theorem 5.11. Assume that the function ψ is µψ-strongly convex and Lψ-smooth,

µψ

3/2

N 2σψ2

ln

N β

rk = Θ max 1, Lψ

ε,

1

µψ 3/2 N 2σψ2 1+ 3 ln Nβ 2

i.e. rk ≥ C max 1, Lψ

ε

with positive constants C > 0, ε > 0 and N ≥ 1.

If additionally δ ≤ NG√RA0N and ε ≤ HARN20 where R0 = y∗ − y0 2 and Algorithm 7 is run for N iterations, then with probability at least 1 − 3β

yN − y∗

2≤

Jˆ2

R

2 0

,

(63)

2 AN

21

where β ∈ (0, 1/3),

ln

N β

gˆ(N ) =

1+

+ ln ln

Bˆ b

2σ12α21R02

(129)

µψ

2 , b = r1 , D = 1 + Lψ +

3 ln

N β

1 + µψ , Lψ

Bˆ

= 8HC

Lψ

3/2
DR4

N

3N +1

µψ

0

2

Aˆ + 2Dh2G2

+2C

Lψ

3/2
c + 2Du2 H ,

µψ

2

2

h = u = µψ , c = µ2 ,

ψ

Aˆ = 1 +

2G√

2G2

Lψ 3/4

√ 2 2CH

+

+

√+

µψ LψµψN AN µ2ψN 2

µψ

LψµψN AN

  Jˆ = max 

1 3Bˆ1D + Lψ ,

9Bˆ12D2 + 4Aˆ + 8cHC 2

Lψ 3/2 4CH

µψ

Lψµ2 N 2AN ,

ψ

 Lµψψ 3/2 
, 

Bˆ = hG + uC 2HC Lψ 3/2 gˆ(N )

1

1

µψ

and C1 is some positive constant. In other words, to achieve

yN

− y∗

2 2

≤

ε

with

probability at

least 1 − 3β Algorithm 7 needs N = O Lµψψ iterations and O max Lµψψ , σεψ2 oracle calls

where O(·) hides polylogarithmic factors depending on Lψ, µψ, R0, ε and β.

Next, we apply the SSTM sc for the problem (18) when the objective of the primal problem (17) is L-smooth, µ-strongly convex and Lf -Lipschitz continuous on some ball which will be speciﬁed next, i.e. we consider the same setup as in Section 5 but we additionally assume that the primal functional f has L-Lipschitz continuous gradient. As in Section 5 we also consider the case when the gradient of the dual functional is known only through biased stochastic estimators, see (32)–(39) and the paragraphs containing these formulas.
In Section 5 and 5.2 we mentioned that in the considered case dual function ψ is Lψ-smooth on Rn and µψ-strongly convex on y0 + (KerA⊤)⊥ where Lψ = λmax(A⊤A)/µ and µψ = λ+min(A⊤A)/L. Using the same technique as in the proof of Theorem 5.4 we show next that w.l.o.g. one can assume that ψ is µψ-strongly convex on Rn since ∇˜ Ψ(y, ξk) lies in ImA = (KerA⊤)⊥ by deﬁnition of ∇˜ Ψ(y, ξk). For this purposes we need the explicit formula for zk+1 which follows from the equation ∇g˜k+1(zk+1) = 0:

zk+1 =

z0

k+1
+

αlµψ

y˜l −

1

k+1
α ∇˜ Ψ(y˜l, ξl).

1 + Ak+1µψ

1 + Ak+1µψ

1 + Ak+1µψ

l

l=0

l=0

(64)

22

Theorem 5.12. For all k ≥ 0 we have that the iterates of Algorithm 7 y˜k, zk, yk lie in y0 + Ker(A⊤) ⊥.

Proof. We prove the statement of the theorem by induction. For k = 0 the statement is trivial, since y˜0 = z0 = y0. Assume that for some k ≥ 0 we have y˜t, zt, yt ∈ y0 + Ker(A⊤) ⊥ for all 0 ≤ t ≤ k

and prove it for k + 1. Since y0 + Ker(A⊤) ⊥ is a convex set and y˜k+1 is a convex combination of

yk and zk we have y˜k+1 ∈ y0 + Ker(A⊤) ⊥. Next, the point 1+Azk0+1µψ + k+1 1+Aαlkµ+ψ1µψ y˜l also lies in
l=0
y0 + Ker(A⊤) ⊥ since it is convex combination of the points lying in this set which follows from

Ak+1 =

k+1 l=0

αl

.

By

deﬁnition

∇˜ Ψ(y˜l, ξl)

of

we

have

that

∇˜ Ψ(y˜l, ξl)

lies

in

ImA

=

(KerA⊤)⊥

for

all y˜l. Putting all together and using (64) we get zk+1 ∈ y0 + Ker(A⊤) ⊥. Finally, yk+1 lies in

y0 + Ker(A⊤) ⊥ as a convex combination of points from this set.

This theorem makes it possible to apply the result from Theorem 5.11 for SSTM sc which is run on the problem (18).

Corollary 5.13. Under assumptions of Theorem 5.11 we get that after N = O Lµψψ ln 1ε iterations of Algorithm 7 which is run on the problem (18) with probability at least 1 − 3β

∇ψ(yN ) 2 ≤ ε ,

(65)

Ry

where β ∈ (0, 1/3) and the total number of oracles calls equals

Lψ σψ2 Ry2

O max µψ , ε2

.

(66)

If additionally ε ≤ µψRy2, then with probability at least 1 − 3β

yN − y∗ 2 ≤ ε ,

(67)

µψ Ry

yN 2 ≤ 2Ry

(68)

Proof. Theorem 5.11 implies that with probability at least 1 − 3β we have

yN − y∗

2≤

Jˆ2

R

2 0

.

2 AN

Using this and Lψ-smoothness of ψ we get that with probability ≥ 1 − 3β

N2

N

∗2

2 N ∗ 2 L2ψJˆ2R02

∇ψ(y ) 2 = ∇ψ(y ) − ∇ψ(y ) 2 ≤ Lψ y − y 2 ≤ AN .

23

(128)

Since A

≥

1 L

1

+

1 2

ψ

Lµψψ 2k, it implies that after N = O

Lµψψ ln 1ε iterations of SSTM sc we

will get (65) with probability at least 1 − 3β and the number of oracle calls will be

N

Lψ σψ2 Ry2

rk = O max µψ , ε2

.

k=0

Next, from µψ-strong convexity of ψ(y) we have that with probability at least 1 − 3β

yN − y∗ 2 ≤ ∇ψ(yN ) 2 ≤ ε

µψ

µψ Ry

and from this we obtain that with probability at least 1 − 3β yN 2 ≤ yN − y∗ 2 + y∗ 2 ≤ ε + Ry ≤ 2Ry. µψ Ry

Corollary 5.14. Let the assumptions of Theorem 5.11 hold. Assume that f is Lf -Lipschitz continuous on BRf (0) where

Rf =

2C λmax(A⊤A) + G1 +

λmax(A⊤A) µ

ε Ry + Rx,

Rx =

x(A⊤y∗)

2,

ε ≤ µψRy2

and

δy

≤

G1ε N Ry

for

some

positive

constant

G1.

Assume

additionally

that the last batch-size rN is slightly bigger than other batch-sizes, i.e.

2

1

µψ 3/2 N 2σψ2 1 +

3

ln

N β

Ry2

rN ≥ C max 1, Lψ

ε2

,

2

σψ2 1 +

3

ln

N β

Ry2

ε2

.

(69)

Then, with probability at least 1 − 4β

f (x˜N ) − f (x∗) ≤ 2 + λmax2(CA⊤A) + G1 RLfy ε, (70)

Ax˜N 2 ≤

√ 1 + 2C + G1 λmax(A⊤A)

ε,

(71)

Ry

where β ∈ (0, 1/4), x˜N d=ef x˜(A⊤yN , ξN , rN ) and to achieve it we need the total number of oracle calls including the cost of computing x˜N equals

O max L χ(A⊤A), σx2M 2 χ(A⊤A)

(72)

µ

ε2

24

where M = ∇f (x∗) 2.

6 Applications to Decentralized Distributed Optimization

In this section we apply our results to the decentralized optimization problems. But let us consider ﬁrst the centralized or parallel architecture. As we mentioned in the introduction, when the objective function is L-smooth one can compute batches in parallel [14, 21, 30, 32] in order to accelerate the work of the method and (14)-(16) imply that

O σ2R2/ε2 LR2/ε

or O

σ2/µε L/µ ln (µR2/ε)

(73)

number of workers in such a parallel scheme gives the method with working time proportional to the number of iterations deﬁned in (14). However, number of workers deﬁned in (73) could be too big in order to use such an approach in practice. But still computing the batches in parallel even with much smaller number of workers could reduce the working time of the method if the communication is fast enough and it follows from (16).
Besides the computation of batches in parallel for the general type of problem (1)+(2), parallel optimization is often applied to the ﬁnite-sum minimization problems (1)+(3) or (1)+(6) that we rewrite here in the following form:

1m

min f (x) =

x∈Q⊆Rn

m

fk(x).

(74)

k=1

We notice that in this section m is a number of workers and fk(x) is known only for the k-th worker.

Consider the situation when workers are connected in a network and one can construct a spanning

tree for this network. Assume that the diameter of the obtained graph equals d, i.e. the height of

the tree — maximal distance (in terms of connections) between the root and a leaf [76]. If we run

STM on such a spanning tree then we will get that the number of communication rounds will be d

times larger than number of iterations deﬁned in (14).

Now let us consider decentralized case when workers can communicate only with their neigh-

bours. Next, we describe the method of how to reﬂect this restriction in the problem (74). Consider

the Laplacian matrix W ∈ Rm×m of the network with vertices V and edges E which is deﬁned as

follows:

 −1,

if (i, j) ∈ E,

W ij = deg(i), if i = j,

(75)

0

otherwise,

where deg(i) is degree of i-th node, i.e. number of neighbours of the i-th worker. Since we consider

only connected networks the matrix W has unique eigenvector 1m d=ef (1, . . . , 1)⊤ ∈ Rm corresponding to the eigenvalue 0. It implies that for all vectors a = (a1, . . . , am)⊤ ∈ Rm the following

equivalence holds:

a1 = . . . = am ⇐⇒ W a = 0.

(76)

Now let us think about ai as a number that i-th node stores. Then, using (76) we can use Laplacian matrix to express in the short matrix form the fact that all nodes of the network store the same

25

number. In order to generalize it for the case when ai are vectors from Rn we should consider the

matrix W d=ef W ⊗ In where ⊗ represents the Kronecker product (see (7)). Indeed, if we consider

vectors x1, . . . , xm ∈ Rn and x =

x

⊤ 1

,

.

.

.

,

x

⊤ m

∈ Rnm, then (76) implies

x1 = . . . = xm ⇐⇒ W x = 0.

(77)

For simplicity, we also call W as a Laplacian matrix and it does not lead to misunderstanding

since everywhere below we use W instead of W . The key observation here that computation of

W x requires one round of communications when the k-th worker sends xk to all its neighbours and receives xj for all j such that (k, j) ∈ E, i.e. k-th worker gets vectors from al√l its neighbours. Note, that W is symmetric √and positive semideﬁnite [76] and, as a consequence, W exists. Moreover, we can replace W by W in (77) and get the equivalent statement:

√

x1 = . . . = xm ⇐⇒ W x = 0.

(78)

Using this we can rewrite the problem (74) in the following way:

1m

√ min
W =0,

f (x) = m

fk (xk ).

x x1 ,...,xm ∈Q⊆Rn

k=1

(79)

We are interested in the general case when fk(xk) = Eξk [fk(xk, ξk)] where {ξk}mk=1 are independent. This type of objective can be considered as a special case of (6). Then, as it was mentioned in the
introduction it is natural to use stochastic gradients ∇fk(xk, ξk) that satisfy

Eξk [∇fk(xk, ξk)] − ∇fk(xk) 2 ≤ δ,

(80)

Eξk exp ∇fk(xk, ξk) − Eσξ2k [∇fk(xk, ξk)] 22 ≤ exp(1). (81)

Then, the stochastic gradient

∇f (x, ξ) d=ef ∇f (x, {ξ }m

) d=ef 1

m
∇f (x , ξ )

k k=1

m

k kk

k=1

satisﬁes (see also (39))

Eξ exp

∇f (x, ξ) − Eξ [∇f (x, ξ)]

2 2

σf2

≤ exp(1)

with σf2 = O (σ2/m). As always, we start with the smooth case with Q = Rn and assume that each fk is L-smooth,
µ-strongly convex and satisﬁes ∇kfk(xk) 2 ≤ M on some ball BRM (x∗) where we use ∇kf (xk) to emphasize that fk depends only on the k-th n-dimensional block of x. Since the functional f (x)
in (79) has separable structure, it implies that f is L/m-smooth, µ/m-strongly convex and satisﬁes

26

∇f (x) 2 ≤ M/√m on B√mRM (x∗). Indeed, for all x, y ∈ Rn

m

x−y

2 2

=

xk − yk 22,

k=1

∇f (x) − ∇f (y) 2 =

1 m ∇ f (x ) − ∇ f (y ) 2

m2

kk k

kk k 2

k=1

≤ L2 m x − y 2 = L x − y ,

m2

k k2 m

2

k=1

1m

1m

µ k k2

f (x) = m fk(xk) ≥ m

f (yk) + ∇kfk(yk), xk − yk + 2 x − y 2

k=1

k=1

= f (y) + ∇f (y), x − y + 2µm x − y 22,

2

1m

2

∇f (x) 2 = m2

∇kfk(xk) 2.

k=1

Therefore, one can consider the problem (79) as (17) with A = √W and Q = Rnm. Next, if the starting point x0 is such that x0 = (x0, . . . , x0)⊤ then

R2 d=ef

x0 − x∗

2 2

=

m

x0 − x∗

2 2

=

mR2,

R2 d=ef
y

y∗

2 2

≤

∇f (x∗)

2 2

≤

M2 .

λ+min(W ) mλ+min(W )

Now it should become clear why in Section 4 we p√aid ⊤m√ost of our attention on number of A⊤Ax calculations. In this particular scenario A⊤Ax = W W x = W x which can be computed via one round of communications of each node with its neighbours as it was mentioned earlier in this section. That is, for the primal approach we can simply use the results discussed in Section 4. For convenience, we summarize them in Tables 3 and 4 which are obtained via plugging the parameters that we obtained above in the bounds from Section 4. Note that the results presented in this match the lower bounds obtained in [5] in terms of the number of communication rounds up to logarithmic factors and and there is a conjecture [18] that these bounds are also optimal in terms of number of oracle calls per node for the class of methods that require optimal number of communication rounds. Recently, the very similar result about the optimal balance between number of oracle calls per node and number of communication round was proved for the case when the primal functional is convex and L-smooth and deterministic ﬁrst-order oracle is available [92].
Finally, consider the situation when Q = Rn and each fk from (79) is dual-friendly, i.e. one can construct dual problem for (79)

min Ψ(y),

where

y

=

(y

⊤ 1

,

.

.

.

,

y

⊤ m

)⊤

∈

Rnm,

y1, . . . , ym

∈

Rn,

(82)

y∈Rnm

ϕk(yk) = max { yk, xk − fk(xk)} ,

(83)

xk ∈Rn

1m

√

1m

√

Φ(y) = m ϕk(myk), Ψ(y) = Φ( W y) = m ϕk(m[ W x]k),

(84)

k=1

k=1

27

Assumptions on fk µ-strongly convex,
L-smooth
L-smooth
µ-strongly convex, ∇fk(x) 2 ≤ M ∇fk(x) 2 ≤ M

Method
D-MASG, Q = Rn,
[23] STP IPS with STP as a subroutine,
Q = Rn, [This paper]
R-Sliding, [18, 54, 56, 57]
Sliding, [54, 56, 57]

# of communication rounds
O Lµ χ

O

LR2 ε

χ

O

M2 µε

χ

O

M 2R2 ε2

χ

# of ∇fk(x) oracle calls per node

O

L µ

O

LR2 ε

O

M2 µε

O

M 2R2 ε2

Table 3: Summary of the covered results in this paper for solving (79) using primal deterministic approach from Section 4. First column contains assumptions on fk, k = 1, . . . , m in addition to the convexity, χ = χ(W ). All methods except D-MASG should be applied to solve (21).

Assumptions on fk µ-strongly convex,
L-smooth
L-smooth
µ-strongly convex, ∇fk(x) 2 ≤ M ∇fk(x) 2 ≤ M

Method
D-MASG, in expectation,
Q = Rn, [23]
SSTP IPS with STP as a subroutine,
Q = Rn, conjecture, [This paper], [18]
RS-Sliding Q is bounded, [18, 54, 56, 57]
S-Sliding Q is bounded,
[54, 56, 57]

# of communication rounds
O Lµ χ

O

LR2 ε

χ

O

M2 µε

χ

O

M 2R2 ε2

χ

# of ∇fk(x, ξ) oracle calls per node

O max

Lµ ,

σ2 µε

O max

LR2 ε

,

σ2 R2 ε2

O

M 2+σ2 µε

O (M 2+εσ2 2)R2

Table 4: Summary of the covered results in this paper for solving (79) using primal stochastic approach from Section 4 with the stochastic oracle satisfying (80)-(81) with δ = 0. First column contains assumptions on fk, k = 1, . . . , m in addition to the convexity, χ = χ(W ). All methods except D-MASG should be applied to solve (21). The bounds from the last two rows hold even in the case when Q is unbounded, but in the expectation (see [59]).

√

√

where [ W x]k is the k-th n-dimensional block of W x. Note that

m

1m

max { y, x − f (x)} = max

x∈Rnm

x∈Rnm

yk, xk − m fk(xk)

k=1

k=1

1m

1m

= m k=1 xmk∈aRxn { myk, xk − fk(xk)} = m k=1 ϕk(myk) = Φ(y),

28

so, Φ(y) is a dual function for f (x). As for the primal approach, we are interested in the general case
when ϕk(yk) = Eξk [ϕk(yk, ξk)] where {ξk}mk=1 are independent and stochastic gradients ∇ϕk(xk, ξk) satisfy

Eξk [∇ϕk(yk, ξk)] − ∇ϕk(yk) 2 ≤ δϕ,

(85)

Eξk exp ∇ϕk(yk, ξk) − Eσξ2k [∇ϕk(yk, ξk)] 22 ≤ exp(1). (86)

Consider the stochastic function fk(xk, ξk) which is deﬁned implicitly as follows:

ϕk(yk, ξk) = max { yk, xk − f (xk, ξk)} .

(87)

xk ∈Rn

Since

m

m

∇Φ(y) = ∇ϕk(myk) (3=1) xk(myk) d=ef x(y),

k=1

k=1

xk(yk) d=ef argmax { yk, xk − fk(xk)}
xk ∈Rn

it is natural to deﬁne the stochastic gradient ∇Φ(y, ξ) as follows:

m

m

∇Φ(y, ξ) d=ef ∇Φ(y, {ξk}mk=1) d=ef ∇ϕk(myk, ξk) (=31) xk(myk, ξk) d=ef x(y, ξ),

k=1

k=1

xk(yk, ξk) d=ef argmax { yk, xk − fk(xk, ξk)} .
xk ∈Rn

It satisﬁes (see also (39))

Eξ exp

Eξ [∇Φ(y, ξ)] − ∇Φ(y) 2 ≤ δΦ,

∇Φ(y, ξ) − Eξ [∇Φ(y, ξ)]

2 2

σΦ2

≤ exp(1)

w∇iΨth(yδ,Φξ)=d=efm√δWϕ ∇anΦd(√σWΦ2 y=, ξO) =m√σW2 .x(√UsWinyg, tξh)isa,ndw,easdeaﬁcnoenstheqeusetnocceh,awsteicgegtradient of Ψ(y) as

Eξ exp

Eξ [∇Ψ(y, ξ)] − ∇Ψ(y) 2 ≤ δΨ,

∇Ψ(y, ξ) − Eξ [∇Ψ(y, ξ)]

2 2

σΨ2

≤ exp(1)

with δΨ = λmax(W )δΦ and σΨ = λmax(W )σΦ.

Ta√king all of this into account we conclude that problem (82) is a special case of (18) with A = W . To make the algorithms from√ Section 5 distributed we should change the variables in those methods via multiplying them by W from the left [18, 19, 89], e.g. for the iterates of SPDSTM

we will get

√

√

√

y˜k+1 := W y˜k+1, zk+1 := W zk+1, yk+1 := W yk+1,

√

which means that it is needed to multiply lines 4-6 of Algorithm 2 by W from the left. After such

a change of variables all methods from Section 5 become suitable to run them in the distributed

29

fashion. Besides that, it does not spoil the ability of recovering the prim√al variables √since before the change of variables all of the methods mentioned in Section 5 used x( W y) or x( W y, ξ) where

points y were some dual iterates of those methods, so, after the change √of variables we should use

x(y) or x(y, ξ) respectively. Moreover, it is also possible to compute

Wx

2 2

=

x, W x

in the

distributed fashion using consensus type algorithms: one communication step is needed to compute

W x, then each worker computes xk, [W x]k locally and after that it is needed to run consensus

algorithm. We summarize the results for this case in Tables 5 and 6. Note that the proposed

bounds are optimal in terms of the number of communication rounds up to polylogarithmic factors

[5, 76, 77, 78]. Note that the lower bounds from [76, 77, 78] are presented for the convolution of two

criteria: number of oracle calls per node and communication rounds. One can obtain lower bounds

for the number of communication rounds itself using additional assumption that time needed for

one communication is big enough and the term which corresponds to the number of oracle calls can

be neglected. Regarding the number of oracle calls there is a conjecture [18] that the bounds that

we present in this paper are also optimal up to polylogarithmic factors for the class of methods

that require optimal number of communication rounds.

Assumptions on fk
µ-strongly convex, L-smooth,
∇fk(x) 2 ≤ M
µ-strongly convex, ∇fk(x) 2 ≤ M

Method
R-RRMA-AC-SA2 (Algorithm 6), Corollary 5.8,
SSTM sc (Algorithm 7), Corollary 5.14
SPDSTM (Algorithm 2), Theorem 5.1

# of communication rounds
O Lµ χ

O

M2 µε

χ

# of ∇ϕk(y, ξ) oracle calls per node
O max Lµ χ, σΦ2εM2 2 χ

O max

M2 µε

χ,

σΦ2 M 2 ε2

χ

Table 5: Summary of the covered results in this paper for solving (82) using dual stochastic approach from Section 5 with the stochastic oracle satisfying (80)-(81) with δ = 0. First column contains assumptions on fk, k = 1, . . . , m in addition to the convexity, χ = χ(W ).

Assumptions on fk
µ-strongly convex, L-smooth,
∇fk(x) 2 ≤ M
µ-strongly convex, ∇fk(x) 2 ≤ M

Method
SSTM sc (Algorithm 7), Corollary 5.14
SPDSTM (Algorithm 2), Theorem 5.1

# of communication rounds
O Lµ χ

O

M2 µε

χ

# of ∇ϕk(y, ξ) oracle calls per node
O max Lµ χ, σΦ2εM2 2 χ
O max Mµε2 χ, σΦ2εM2 2 χ

Table 6: Summary of the covered results in this paper for solving (82) using biased dual stochastic approach from Section 5 with the stochastic oracle satisfying (80)-(81) with δϕ > 0. First column contains assumptions on fk, k = 1, . . . , m in addition to the convexity, χ = χ(W ). For both cases the noise level should satisfy
δϕ = O (ε/M√mχ).

30

7 Discussion
In this section we want to discuss some aspects of the proposed results that were not covered in the main part of this paper. First of all, we should say that in the smooth case for the primal approach our bounds for the number of communication steps coincides with the optimal bounds for the number of communication steps for parallel optimization if we substitute the diameter d of the spanning tree in the bounds for parallel optimization by O( χ(W )).
However, we want to discuss another interesting diﬀerence between parallel and decentralized optimization in terms of the complexity results which was noticed in [18]. From the line of works [51, 52, 53, 58] it is known that for the problem (1)+(6) (here we use m instead of q and iterator k instead of i for consistency) with L-smooth and µ-strongly convex fk for all k = 1, . . . , m the optimal number of oracle calls, i.e. calculations of of the stochastic gradients of fk with σ2-subgaussian variance is
O m + m L + σ2 . (88) µ µε
The bad news is that (88) does not work with full parallelization trick and the best possible way to parallelize it is described in [58]. However, standard accelerated scheme using mini-batched versions of the stochastic gradients without variance-reduction technique and incremental oracles which gives the bound
O m L + σ2 (89) µ µε
for the number of oracle calls and it admits full parallelization. It means that in the parallel optimization setup when we have computational network with m nodes and the spanning tree for it with diameter d the number of oracle calls per node is

O L + σ2 = O max L , σ2 (90)

µ mµε

µ mµε

and the number of communication steps is

O d L . (91) µ

However, for the decentralized setup the second row of Table 4 states that the number of communication rounds is the same as in (91) up to substitution of d by χ(W ) and the number of oracle calls per node is
O max L , σ2 (92) µ µε
which has m times bigger statistical term under the maximum than in (90). What is more, recently it was shown that there exists such a decentralized distributed method that requires
O σ2 mµε

31

stochastic gradient oracle calls per node [69, 70], but it is not optimal in terms of the number of communications. Moreover, there is a hypothesis [18] that in the smooth case the bounds from Tables 3 and 4 (rows 2 and 3) are optimal in terms of the number of oracle calls per node for the class of methods that require optimal number of communication rounds up to polylogarithmic factors.
The same claim but for Table 5 was also presented in [18] as a hypothesis and in this paper we propose the same hypothesis for the result stated Table 6 up to polylogarithmic and additionally we hypothesise that the noise level that we obtained is also unimprovable up to polylogarithmic factors.

7.1 Possible Extensions

• As it was mentioned in Section 4, the recurrence technique that we use in Sections E and 5 can be very useful in the generalization of the results for STM from Section 4 for the case when instead of ∇f (x) only stochastic gradient ∇f (x, ξ) (see inequalities (10)-(11)) is available, f is L-smooth and proximal step is computed in an inexact manner. It would be nice also to compare proposed methods for the case when δ with the results from [23]. For the convex but non-strongly convex case one can also try to combine Nesterov’s smoothing technique [15, 64, 89] with D-MASG from [23].

• We believe that the technique presented in the proofs of Lemmas G.3 and 5.10 can also be
extended or modiﬁed in order to be applied for diﬀerent optimization methods to obtain high probability bounds in the case when Q = Rn.

• We emphasize that in our results we assume that each fi from (79) is L-smooth and µ-strongly

convex. When each fi is Li-smooth and µi-strongly convex, it means that in order to satisfy

the assumption we use in our paper we need to choose L = max1≤i≤m Li and µ = min1≤i≤m µi.

This choice can lead to a very slow rate in some situations, e.g. the worst-case L can be m

times larger than L for f as for the case when m = d and f (x) = x 22/2m = 1/m

m i=1

fi

(x),

fi(x) = x2i/2 where Li = 1 for all i but f is 1/d-smooth [88]. It was shown [76, 89] that instead

of worst-case µ and L one can use µ¯ = 1/m

m i=1

µi

and

Lˆ

to

be

some

weighted

average

of

Li, but such techniques can spoil number of communication rounds needed to achieve desired

accuracy.

• It would be also interesting to generalize the proposed results for the case of more general stochastic gradients [6, 35, 68, 90].

8 Application for Population Wasserstein Barycenter Calculation
In this section we consider the problem of calculation of population Wasserstein barycenter since this example hides diﬀerent interesting details connected with the theory discussed in this paper. In our presentation of this example we rely mostly on the recent work [17].

8.1 Deﬁnitions and Properties

We deﬁne the probability simplex in Rn as Sn(1) = x ∈ Rn+ |

n i=1

xi

=

1

.

One can interpret

the elements of Sn(1) as discrete probability measures with n shared atoms. For an arbitrary

32

pair of measures p, q ∈ Sn(1) we introduce the set Π(p, q) = π ∈ R+n×n | π1 = p, π⊤1 = q called transportation polytope. Optimal transportation (OT) problem between measures p, q ∈ Sn(1) is

deﬁned as follows n

W(p, q) = min C, π = min

Cij πij

π∈Π(p,q)

π∈Π(p,q) i,j=1

(93)

where C is a transportation cost matrix. That is, (i, j)-th component Cij of C is a cost of transportation of the unit mass from point xi to the point xj where points x1, . . . , xn ∈ R are atoms of measures from Sn(1).
Next, we consider the entropic OT problem (see [71, 72])

n

Wµ(p, q) = min

(Cij πij + µπij ln πij ) .

π∈Π(p,q) i,j=1

(94)

Consider some probability measure P on Sn(1). Then one can deﬁne population barycenter of measures from Sn(1) as

p∗µ = argmin

Wµ(p, q)dP(q) = argmin Eq [Wµ(p, q)] .

p∈Sn(1) q∈Sn(1)

p∈Sn(1)

Wµ (p)

(95)

For a given set of samples q1, . . . , qm we introduce empirical barycenter as

∗

1m

i

pˆµ = apr∈gSmn(i1n) m i=1 Wµ(p, q ) . (96)

Wˆ (p)

We consider the problem (95) of ﬁnding population barycenter with some accuracy and discuss possible approaches to solve this problem in the following subsections.
However, before that, we need to mention some useful properties of Wµ(p, q). First of all, one can write explicitly the dual function of Wµ(p, q) for a ﬁxed q ∈ Sn(1) (see [12, 17]):

Wµ(p, q) = max λ, p − Wq∗,µ(λ)

(97)

λ∈Rn

∗

n

1n

−Cij + λi

Wq,µ(λ) = µ qj ln qj exp

µ

.

(98)

j=1

i=1

Using this representation one can deduce the following theorem.

Theorem 8.1 ([17]). For an arbitrary q ∈ Sn(1) the entropic Wasserstein distance Wµ(·, q) : Sn(1) → R is√µ-strongly convex w.r.t. ℓ2-norm and M -Lipschitz continuous w.r.t. ℓ2-norm. Moreover, M ≤ nM∞ where M∞ is Lipschitz constant of Wµ(·, q) w.r.t. ℓ∞-norm and M∞ = O( C ∞).
We also want to notice that function Wq∗,µ(λ) is only strictly convex and the minimal eigenvalue of its hessian γ d=ef λmin(∇2Wq,µ(λ∗)) evaluated in the solution λ∗ d=ef argmaxλ∈Rn λ, p − Wq∗,µ(λ) is very small and there exist only such bounds that are exponentially small in n.

33

We will also use another useful relation (see [17]):

∇Wµ(p, q) = λ∗, λ∗, 1 = 0

(99)

where the gradient ∇Wµ(p, q) is taken w.r.t. the ﬁrst argument.

8.2 SA Approach

Assume that one can obtain and use fresh samples q1, q2, . . . in online regime. This approach
is called Stochastic Approximation (SA). It implies that at each iteration one can draw a fresh sample qk and compute the gradient w.r√.t. p of function Wµ(p, qk) which is µ-strongly convex and M -Lipschitz continuous with M = O( n C ∞). Optimal methods for this case are based on iterations of the following form

pk+1 = projSn(1) pk − ηk∇Wµ(pk, qk)

where projSn(1)(x) is a projection of x ∈ Rn on Sn(1) and the gradient ∇Wµ(pk, qk) is taken w.r.t. the ﬁrst argument. One can show that restarted-SGD (R-SGD) from [44] that using biased stochastic gradients (see also [42, 26, 17]) ∇˜ Wµ(p, q) such that

∇˜ Wµ(p, q) − ∇Wµ(p, q) 2 ≤ δ

(100)

for some δ ≥ 0 and for all p, q ∈ Sn(1) after N calls of this oracle produces such a point pN that with probability at least 1 − β the following inequalities hold:

Wµ(pN ) − Wµ(p∗µ) = O

n

C

2 ∞

ln(N/α)

+

δ

µN

(101)

and, as a consequence of µ-strong convexity of Wµ(p, q) for all q,

pN − p∗µ 2 = O

n

C

2 ∞

ln(N/α)

+

δ

.

µ2N

µ

(102)

That is, to guarantee

pN − p∗µ 2 ≤ ε

with probability at least 1 − β, R-SGD requires

(103)

O

n

C

2 ∞

µ2ε2

∇˜ Wµ(p, q) oracle calls

(104)

under additional assumption that δ = O(µε2). However, it is computationally hard problem to ﬁnd ∇Wµ(p, q) with high-accuracy, i.e. ﬁnd
∇˜ Wµ(p, q) satisfying (100) with δ = O(µε2). Taking into account the relation (99) we get that it is needed to solve the problem (97) with accuracy δ = O(µε2) in terms of the distance to the optimum. i.e. it is needed to ﬁnd such λ˜ that λ˜ − λ∗ 2 ≤ δ and set ∇˜ Wµ(p, q) = λ˜. Using variants of Sinkhorn algorithm [50, 87, 36] one can show [17] that R-SGD ﬁnds point pN such that (103) holds with probability at least 1 − β and it requires

O

n3

C

2
∞ min

exp

µ2ε2

C∞ µ

C ∞ + ln C ∞

µ

γµ2ε4

n , γµ3ε4

(105)

arithmetical operations.

34

8.3 SAA Approach
Now let us assume that large enough collection of samples q1, . . . , qm is available. Our goal is to ﬁnd such p ∈ Sn(1) that pˆ− p∗µ 2 ≤ ε with high probability, i.e. ε-approximation of the population barycenter, via solving empirical barycenter problem (96). This approach is called Stochastic Average√Approximation (SAA). Since Wµ(p, qi) is µ-strongly convex and M -Lipschitz in p with M = O( n C ∞) for all i = 1, . . . , m we can conclude that with probability ≥ 1 − β

Wµ(pˆ∗µ) − Wµ(p∗µ) (=5) O

n

C

2 ∞

ln(m)

ln

(m/β)

+

µm

n

C

2 ∞

ln

(1/β

)

m

(106)

where we use that the diameter of Sn(1) is O(1). Moreover, in [81] it was shown that one can guarantee that with probability ≥ 1 − β

Wµ(pˆ∗ ) − Wµ(p∗ ) (=5) O

n

C

2 ∞

.

µ

µ

βµm

(107)

Taking advantages of both inequalities we get that if

m=Ω

min

max

n

C

2 ∞

,

n

C

2 ∞

,n

C

2 ∞

µ2ε2 µ2ε4

βµ2ε2

= Ω n min

C

2
∞,

C

2 ∞

µ2ε4 βµ2ε2

(108)

then

with

probability

at

least

1

−

β 2

pˆ∗µ − p∗µ 2 ≤ µ2 Wµ(pˆ∗µ) − Wµ(p∗µ) (106),(1≤07),(108) 2ε .

(109)

Assuming

that

we

have

such

pˆ ∈ Sn(1)

that

with

probability

at

least

1−

β 2

the

inequality

pˆ − pˆ∗µ 2 ≤ 2ε

holds, we apply the union bound and get that with probability ≥ 1 − β

(110)

pˆ − p∗µ 2 ≤ pˆ − pˆ∗µ 2 + pˆ∗µ − p∗µ 2 ≤ ε.

(111)

It remains to describe the approach that ﬁnds such pˆ ∈ Sn(1) that satisﬁes (111) with probability at least 1 − β. Recall that in this subsection we consider the following problem

Wˆ (p) = 1

m
W (p, qi) →

min

.

µ

m

µ

p∈Sn(1)

i=1

(112)

For each summand Wµ(p, qi) in the sum above we have the explicit formula (98) for the dual function Wq∗i,µ(λ). Note that one can compute the gradient of Wq∗i,µ(λ) via O(n2) arithmetical operations. What is more, Wq∗i,µ(λ) has a ﬁnite-sum structure, so, one can sample j-th component of qi with probability qji and get stochastic gradient

∇W∗ (λ, j) = µ∇ ln 1 n exp −Cij + λi

qi,µ

qji i=1

µ

(113)

35

which requires O(n) arithmetical operations to be computed.

We start with the simple situation. Assume that each measures qi are stored on m separate

machines that form some network with Laplacian matrix W ∈ Rm×m. For this scenario we can

apply the dual approach described in Section 6 and apply bounds from Tables 5 and 6. If for all

i = 1, . . . , m the i-th node computes the full gradient of dual functions Wqi,µ at each iteration then

in

order

to

ﬁnd

such

a

point

pˆ

that

with

probability

at

least

1

−

β 2

Wˆ µ(pˆ) − Wˆ µ(pˆ∗µ) ≤ εˆ,

(114)

where W = W ⊗ In, this approach requires O n Cµεˆ2∞ χ(W ) communication rounds and

O n2.5

C

2 ∞

µεˆ

χ

(W

)

arithmetical operations per node to ﬁnd gradients ∇Wq∗i,µ(λ). If instead of

full

gradients

workers

use

stochastic

gradients

∇

W

∗ qi

,

µ

(λ,

j

)

deﬁned

in

(113)

and

these

stochastic

gradients have light-tailed distribution, i.e. satisfy the condition (86) with parameter σ > 0, then to

guarantee

(114)

with

probability

≥

1−

β 2

the

aforementioned

approach

needs

the

same

number

of

communications rounds and O n max

n

C

2 ∞

µεˆ

χ(W

),

mσ2n C εˆ2

2
∞ χ(W )

arithmetical operations

per

node

to

ﬁnd

gradients

∇

W

∗ qi

,

µ

(λ,

j

).

Using

µ-strong

convexity

of

Wµ(p, qi)

for

all

i

=

1, . . . , m

and

taking

εˆ =

µε2 8

we

get

that

our

approach

ﬁnds

such

a

point

pˆ that

satisﬁes

(110)

with

probability

at

least

1−

β 2

using

√

O n C ∞ χ(W ) communication rounds

(115)

µε

and

O n2.5 C ∞ χ(W )

(116)

µε

arithmetical operations per node to ﬁnd gradients in the deterministic case and

O n max

√n C ∞ µε

χ(W ), mσ2n

C

2
∞ χ(W )

µ2ε4

arithmetical operations per node to ﬁnd stochastic gradients in the stochastic case. However, the

state-of-the-art theory of learning states (see (108)) that m should so large that in the stochastic case

the second term in the bound for arithmetical operations typically dominates the ﬁrst term and the

dimensional dependence reduction from n2.5 in the deter√ministic case to n1.5 in the stochastic case

is typically negligible in comparison with how much mσ2 µ2nε4C 2∞ χ(W ) is larger than

C∞ µε

χ(W ).

That is, our theory says that it is better to use full gradients in the particular example considered

in this section (see also Section 7). Therefore, further in the section we will assume that σ2 = 0,

i.e. workers use full gradients of dual functions Wq∗i,µ(λ).

However, bounds (115)-(116) were obtained under very restrictive at the ﬁrst sight assumption

that we have m workers and each worker stores only one measure which is unrealistic. One can relax this assumption in the following way. Assume that we have ˆl < m machines connected in a network with Laplacian matrix Wˆ and j-th machine stores mˆ j ≥ 1 measures for j = 1, . . . , ˆl

and

ˆl j=1

mˆ j

=

m.

Next, for j-th machine we introduce mˆ j virtual workers also connected in

some network that j-th machine can emulate along with communication between virtual workers

36

and for every virtual worker we arrange one measure, e.g. it can be implemented as an array-

like data structure with some formal rules for exchanging the data between cells that emulates

communications. We also assume that inside the machine we can set the preferable network for

the virtual nodes in such a way that each machine emulates communication between virtual nodes

and computations inside them fast enough. Let us denote the Laplacian matrix of the obtained

network of m virtual nodes as W . Then, our approach ﬁnds such a point pˆ that satisﬁes (110) with

probability

at

least

1−

β 2

using





 O 


max Tcm,j
j=1,...,ˆl

√n C ∞ µε

 χ(W )


(117)

Tcm,max

time to perform communications and





 O 


max Tcp,j
j=1,...,ˆl

n2.5 C ∞ µε

 χ(W )


(118)

Tcp,max

time for arithmetical operations per machine to ﬁnd gradients where Tcm,j is time needed for j-th machine to emulate communication between corresponding virtual nodes at each iteration and Tcp,j is time required by j-th machine to perform 1 arithmetical operation for all corresponding virtual
nodes in the gradients computation process at each iteration. For example, if we have only one
machine and network of virtual nodes forms a complete graph than χ(W ) = 1, but Tcm,max and Tcp,max can be large and to reduce the running time one should use more powerful machine. In contrast, if we have m machines connected in a star-graph than Tcm,max and Tcp,max will be much smaller, but χ(W ) will be of order m which is large. Therefore, it is very important to choose
balanced architecture of the network at least for virtual nodes per machine if it is possible. This
question requires a separate thorough study and lies out of scope of this paper.

8.4 SA vs SAA comparison
Recall that in SA approach we assume that it is possible to sample new measures in online regime which means that the computational process is performed on one machine, whereas in SAA approach we assume that large enough collection of measures is distributed among the network of machines that form some computational network. In practice measures from Sn(1) correspond to some images. As one can see from the complexity bounds, both SA and SAA approaches require large number of samples to learn the population barycenter deﬁned in (95). If these samples are images, then they typically cannot be stored in RAM of one computer. Therefore, it is natural to use distributed systems to store the data.
Now let us compare complexity bounds for SA and SAA. We summarize them in Table 7. When the communication is fast enough and µ is small we typically have that SAA approach signiﬁcantly outperforms SA approach in terms of the complexity as well even for communication architectures with big χ(W ). Therefore, for balanced architecture one can expect that SAA approach will outperform SA even more.

37

Approach
SA
SA, the 2-d term
is smaller
SAA
SAA, χ(W ) = Ω(m), Tcm,max = O(1), Tcp,m√ax = O(1),
β≥ε

Complexity

O n3µ2Cε22∞ min exp

C∞ µ

C∞ µ

+

ln

C∞ γ µ2 ε4

arithmetical operations

,

n γ µ3 ε4

O n√3γ.5µ3C.5ε2∞4 arithmetical operations

√

O

Tcm,max

nC ∞ µε

χ(W ) time to perform communications,

O

Tcp,maxn2.5

C∞ µε

χ(W ) time for arithmetical operations per machine,

where m = Ω n min

, C

2 ∞

µ2 ε4

C

2 ∞

βµ2ε2

O √n βCµ22∞ ε2 communication rounds, O n√3βCµ2ε2∞2 arithmetical operations per machine

Table 7: Complexity bounds for SA and SAA approaches for computation of population barycenter deﬁned in (95) with accuracy ε. The third row states the complexity bound for SA approach when the second term under the minimum in (105) is dominated by the ﬁrst one, e.g. whe√n µ is small enough. The last row corresponds to the case when Tcm,max = O(1), Tcp,max = O(1), β ≥ ε, e.g. β = 0.01 and ε ≤ 0.1, and the communication network is star-like, which implies χ(W ) = Ω(m)

To conclude, we state that population barycenter computation is a natural example when it is typically much more preferable to use distributed algorithms with dual oracle instead of SA approach in terms of memory and complexity bounds.
Acknowledgments
We would like to thank F. Bach, P. Dvurechensky, M. Gu¨rbu¨zbalaban, D. Kovalev, A. Nemirovski, A. Olshevsky, N. Srebro, A. Taylor and C. Uribe for useful discussions. The work of E. Gorbunov was supported by RFBR, project number 19-31-51001. The work of D. Dvinskikh was supported by Russian Science Foundation (project 18-71-10108). The work of A. Gasnikov was supported by RFBR, project number 19-31-51001.
References
[1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-eﬃcient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1709–1720, 2017.
[2] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 1200–1205, New York, NY, USA, 2017. ACM. arXiv:1603.05953.
[3] Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information Processing Systems, pages 1157–1167, 2018.

38

[4] A. S. Anikin, A. V. Gasnikov, P. E. Dvurechensky, A. I. Tyurin, and A. V. Chernov. Dual approaches to the minimization of strongly convex functionals with a simple structure under aﬃne constraints. Computational Mathematics and Mathematical Physics, 57(8):1262–1276, Aug 2017.
[5] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. In Advances in neural information processing systems, pages 1756–1764, 2015.
[6] Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. A universally optimal multistage accelerated stochastic gradient method. arXiv preprint arXiv:1901.08022, 2019.
[7] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd with quantization, sparsiﬁcation, and local computations. arXiv preprint arXiv:1906.02367, 2019.
[8] Aaron Ben-Tal and Arkadi Nemirovski. Lectures on Modern Convex Optimization. Society for Industrial and Applied Mathematics, 2001.
[9] Dimitri P Bertsekas and John N Tsitsiklis. Parallel and distributed computation: numerical methods, volume 23. Prentice hall Englewood Cliﬀs, NJ, 1989.
[10] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. volume 50, pages 2050–2057. IEEE, 2004.
[11] Alexey Chernov, Pavel Dvurechensky, and Alexander Gasnikov. Fast primal-dual gradient method for strongly convex minimization problems with linear constraints. In Yury Kochetov, Michael Khachay, Vladimir Beresnev, Evgeni Nurminski, and Panos Pardalos, editors, Discrete Optimization and Operations Research: 9th International Conference, DOOR 2016, Vladivostok, Russia, September 19-23, 2016, Proceedings, pages 391–403. Springer International Publishing, 2016.
[12] Marco Cuturi and Gabriel Peyr´e. A smoothed dual approach for variational wasserstein problems. SIAM Journal on Imaging Sciences, 9(1):320–343, 2016.
[13] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS’14, pages 1646– 1654, Cambridge, MA, USA, 2014. MIT Press.
[14] Olivier Devolder. Exactness, inexactness and stochasticity in ﬁrst-order methods for large-scale convex optimization. PhD thesis, ICTEAM and CORE, Universit´e Catholique de Louvain, 2013.
[15] Olivier Devolder, Franc¸ois Glineur, and Yurii Nesterov. Double smoothing technique for largescale linearly constrained convex optimization. SIAM Journal on Optimization, 22(2):702–727, 2012.
39

[16] Olivier Devolder, Franc¸ois Glineur, Yurii Nesterov, et al. First-order methods with inexact oracle: the strongly convex case. 2013.
[17] Darina Dvinskikh. Sa vs saa for population wasserstein barycenter calculation. arXiv preprint arXiv:2001.07697, 2020.
[18] Darina Dvinskikh and Alexander Gasnikov. Decentralized and parallelized primal and dual accelerated methods for stochastic convex programming problems. arXiv preprint arXiv:1904.09015, 2019.
[19] Darina Dvinskikh, Eduard Gorbunov, Alexander Gasnikov, Pavel Dvurechensky, and Cesar A Uribe. On dual approach for distributed stochastic convex optimization over networks. arXiv preprint arXiv:1903.09844, 2019.
[20] Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, and Angelia Nedich. Decentralize and randomize: Faster algorithm for wasserstein barycenters. In Advances in Neural Information Processing Systems, pages 10760–10770, 2018.
[21] Pavel Dvurechensky and Alexander Gasnikov. Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. Journal of Optimization Theory and Applications, 171(1):121–145, 2016.
[22] Pavel Dvurechensky, Alexander Gasnikov, and Alexander Tiurin. Randomized similar triangles method: A unifying framework for accelerated randomized optimization methods (coordinate descent, directional search, derivative-free method). arXiv:1707.08486, 2017.
[23] Alireza Fallah, Mert Gurbuzbalaban, Asu Ozdaglar, Umut Simsekli, and Lingjiong Zhu. Robust distributed accelerated stochastic gradient methods for multi-agent networks. arXiv preprint arXiv:1910.08701, 2019.
[24] Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. arXiv preprint arXiv:1902.10710, 2019.
[25] Dylan Foster, Ayush Sekhari, Ohad Shamir, Nathan Srebro, Karthik Sridharan, and Blake Woodworth. The complexity of making the gradient small in stochastic convex optimization. arXiv preprint arXiv:1902.04686, 2019.
[26] A. V. Gasnikov, A. A. Lagunovskaya, I. N. Usmanova, and F. A. Fedorenko. Gradient-free proximal methods with inexact oracle for convex stochastic nonsmooth optimization problems on the simplex. Automation and Remote Control, 77(11):2018–2034, Nov 2016. arXiv:1412.3890.
[27] Alexander Gasnikov. Universal gradient descent. arXiv preprint arXiv:1711.00394, 2017.
[28] Alexander Gasnikov. Universal gradient descent. MIPT, 2018.
[29] Alexander Gasnikov and Yurii Nesterov. Universal fast gradient method for stochastic composit optimization problems. arXiv:1604.05275, 2016.
[30] Alexander Vladimirovich Gasnikov and Yu E Nesterov. Universal method for stochastic composite optimization problems. Computational Mathematics and Mathematical Physics, 58(1):48–64, 2018.
40

[31] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469–1492, 2012.
[32] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. arXiv:1309.5549.
[33] Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov. An accelerated method for derivative-free smooth stochastic convex optimization. arXiv preprint arXiv:1802.09022, 2018.
[34] Eduard Gorbunov, Filip Hanzely, and Peter Richt´arik. A uniﬁed theory of sgd: Variance reduction, sampling, quantization and coordinate descent. arXiv preprint arXiv:1905.11261, 2019.
[35] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtarik. Sgd: General analysis and improved rates. arXiv preprint arXiv:1901.09401, 2019.
[36] Sergey Guminov, Pavel Dvurechensky, and Alexander Gasnikov. Accelerated alternating minimization. arXiv preprint arXiv:1906.03622, 2019.
[37] Hadrien Hendrikx, Francis Bach, and Laurent Massouli´e. Accelerated decentralized optimization with local updates for smooth and strongly convex objectives. arXiv preprint arXiv:1810.02660, 2018.
[38] Samuel Horvath, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter Richtarik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019.
[39] Samuel Horva´th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richt´arik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
[40] Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736, 2019.
[41] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315–323, 2013.
[42] A. Juditsky and A. Nemirovski. First order methods for non-smooth convex large-scale optimization, i: General purpose methods. In Stephen Wright Suvrit Sra, Sebastian Nowozin, editor, Optimization for Machine Learning, pages 121–184. Cambridge, MA: MIT Press, 2012.
[43] Anatoli Juditsky and Arkadii S Nemirovski. Large deviations of vector-valued martingales in 2-smooth normed spaces. arXiv preprint arXiv:0809.0813, 2008.
[44] Anatoli Juditsky and Yuri Nesterov. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stochastic Systems, 4(1):44–80, 2014.
41

[45] Sham Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09.pdf, 2(1), 2009.
[46] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U Stich, and Martin Jaggi. Error feedback ﬁxes signsgd and other gradient compression schemes. arXiv preprint arXiv:1901.09847, 2019.
[47] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. Better communication complexity for local sgd. arXiv preprint arXiv:1909.04746, 2019.
[48] Ahmed Khaled, Konstantin Mishchenko, and Peter Richta´rik. First analysis of local gd on heterogeneous data. arXiv preprint arXiv:1909.04715, 2019.
[49] VM Kibardin. Decomposition into functions in the minimization problem. Avtomatika i Telemekhanika, (9):66–79, 1979.
[50] Alexey Kroshnin, Darina Dvinskikh, Pavel Dvurechensky, Alexander Gasnikov, Nazarii Tupitsa, and Cesar Uribe. On the complexity of approximating wasserstein barycenter. arXiv preprint arXiv:1901.08686, 2019.
[51] Andrei Kulunchakov and Julien Mairal. Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise. arXiv preprint arXiv:1901.08788, 2019.
[52] Andrei Kulunchakov and Julien Mairal. Estimate sequences for variance-reduced stochastic composite optimization. arXiv preprint arXiv:1905.02374, 2019.
[53] Andrei Kulunchakov and Julien Mairal. A generic acceleration framework for stochastic composite optimization. arXiv preprint arXiv:1906.01164, 2019.
[54] George Lan. Lectures on optimization methods for machine learning. e-print, 2019.
[55] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365–397, Jun 2012. Firs appeared in June 2008.
[56] Guanghui Lan. Gradient sliding for composite optimization. Mathematical Programming, 159(1):201–235, Sep 2016.
[57] Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-eﬃcient algorithms for decentralized and stochastic optimization. Mathematical Programming, pages 1–48, 2017.
[58] Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic optimization. SIAM Journal on Optimization, 28(4):2753–2782, 2018.
[59] Guanghui Lan and Zhiqiang Zhou. Algorithms for stochastic optimization with expectation constraints. arXiv:1604.03887, 2016.
[60] Xiaorui Liu, Yao Li, Jiliang Tang, and Ming Yan. A double residual compression algorithm for eﬃcient distributed learning. arXiv preprint arXiv:1910.07561, 2019.
42

[61] Konstantin Mishchenko, Eduard Gorbunov, Martin Tak´aˇc, and Peter Richt´arik. Distributed learning with compressed gradient diﬀerences. arXiv preprint arXiv:1901.09269, 2019.
[62] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[63] Yurii Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004.
[64] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127–152, 2005.
[65] Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221–259, Aug 2009. First appeared in 2005 as CORE discussion paper 2005/67.
[66] Yurii Nesterov. How to make the gradients small. Optima, 88:10–11, 2012.
[67] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[68] Lam M Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richt´arik, Katya Scheinberg, and Martin Tak´aˇc. Sgd and hogwild! convergence without the bounded gradients assumption. arXiv preprint arXiv:1802.03801, 2018.
[69] Alex Olshevsky, Ioannis Ch Paschalidis, and Shi Pu. Asymptotic network independence in distributed optimization for machine learning. arXiv preprint arXiv:1906.12345, 2019.
[70] Alex Olshevsky, Ioannis Ch Paschalidis, and Shi Pu. A non-asymptotic analysis of network independence for distributed stochastic gradient descent. arXiv preprint arXiv:1906.02702, 2019.
[71] Gabriel Peyr´e, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019.
[72] Philippe Rigollet and Jonathan Weed. Entropic optimal transport is maximum-likelihood deconvolution. Comptes Rendus Mathematique, 356(11-12):1228–1235, 2018.
[73] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400–407, 1951.
[74] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.
[75] Alexander Rogozin and Alexander Gasnikov. Projected gradient method for decentralized optimization over time-varying networks. arXiv preprint arXiv:1911.08527, 2019.
[76] Kevin Scaman, Francis Bach, S´ebastien Bubeck, Yin Tat Lee, and Laurent Massouli´e. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3027–3036. JMLR. org, 2017.
43

[77] Kevin Scaman, Francis Bach, S´ebastien Bubeck, Yin Tat Lee, and Laurent Massouli´e. Optimal convergence rates for convex distributed optimization in networks. Journal of Machine Learning Research, 20(159):1–31, 2019.
[78] Kevin Scaman, Francis Bach, S´ebastien Bubeck, Laurent Massouli´e, and Yin Tat Lee. Optimal algorithms for non-smooth distributed optimization in networks. In Advances in Neural Information Processing Systems, pages 2745–2754, 2018.
[79] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83–112, 2017.
[80] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
[81] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization. In COLT, 2009.
[82] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 64–72, Bejing, China, 22–24 Jun 2014. PMLR. First appeared in arXiv:1309.2375.
[83] A. Shapiro, D. Dentcheva, and A. Ruszczyn´ski. Lectures on Stochastic Programming. Society for Industrial and Applied Mathematics, 2009.
[84] Vladimir Spokoiny et al. Parametric estimation. ﬁnite sample theory. The Annals of Statistics, 40(6):2877–2909, 2012.
[85] Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018.
[86] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed sgd with memory. In Advances in Neural Information Processing Systems, pages 4447–4458, 2018.
[87] Fedor S Stonyakin, Darina Dvinskikh, Pavel Dvurechensky, Alexey Kroshnin, Olesya Kuznetsova, Artem Agafonov, Alexander Gasnikov, Alexander Tyurin, C´esar A Uribe, Dmitry Pasechnyuk, et al. Gradient methods for problems with inexact model of the objective. In International Conference on Mathematical Optimization Theory and Operations Research, pages 97–114. Springer, 2019.
[88] Junqi Tang, Karen Egiazarian, Mohammad Golbabaee, and Mike Davies. The practicality of stochastic optimization in imaging inverse problems. arXiv preprint arXiv:1910.10100, 2019.
[89] C´esar A Uribe, Soomin Lee, Alexander Gasnikov, and Angelia Nedi´c. Optimal algorithms for distributed optimization. arXiv preprint arXiv:1712.00232, 2017.
[90] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for overparameterized models and an accelerated perceptron. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1195–1204, 2019.
44

[91] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1509–1519, 2017.
[92] Jinming Xu, Ye Tian, Ying Sun, and Gesualdo Scutari. Accelerated primal-dual algorithms for distributed smooth convex optimization over networks. arXiv preprint arXiv:1910.10666, 2019.
[93] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication eﬃcient momentum sgd for distributed non-convex optimization. arXiv preprint arXiv:1905.03817, 2019.
[94] Kaiwen Zhou. Direct acceleration of saga using sampled negative momentum. arXiv preprint arXiv:1806.11048, 2018.
[95] Kaiwen Zhou, Fanhua Shang, and James Cheng. A simple stochastic variance reduced algorithm with fast convergence rates. arXiv preprint arXiv:1806.11027, 2018.

A Basic Facts
In this section we enumerate for convenience basic facts that we use many times in our proofs.

Fenchel-Young inequality. For all a, b ∈ Rn and λ > 0

| a, b | ≤

a

2 2

+

λ

b

2
2.

2λ

2

Squared norm of the sum. For all a, b ∈ Rn

a+b

2 2

≤2

a

2 2

+

2

b

22 .

(119) (120)

B Useful Facts about Duality

This section contains several useful results that we apply in our analysis.

Lemma B.1 ([57]). Let y∗ be the solution of (18) with the smallest ℓ2-norm Ry d=ef y∗ 2. Then

R2 ≤

∇f (x∗)

2
2.

y λ+min(A⊤A)

(121)

Lemma B.2. Consider the function f (x) deﬁned on a closed convex set Q ⊆ Rn and linear opera-

tor A such that KerA = {0} and its dual function ψ(y) deﬁned as ψ(y) = maxx∈Q { y, Ax − f (x)}.

Then

ψ(y∗) = −f (x∗) ≥ y∗, Axˆ − f (xˆ) ∀xˆ ∈ Q.

(122)

45

Proof. We have

ψ(y∗) = y∗, Ax(A⊤y∗) − f x(A⊤y∗) .

From Demyanov–Danskin theorem [74] we have that ∇ψ(y) = Ax(A⊤y) which implies

0 = ∇ψ(y∗) = Ax(A⊤y∗).

Using this we get

−f x(A⊤y∗)

= ψ(y∗) = max
Ax=0,x∈Q
= −f (x∗).

y∗, Ax −f (x)
=0

Finally,

ψ(y∗) = −f (x∗) = max { y∗, Ax − f (x)} ≥ y∗, Axˆ − f (xˆ).
Ax=0,x∈Q

C Auxiliary Results

In this section, we present the results from other papers that we rely on in our proofs.

Lemma C.1 (Lemma 2 from [40]). For random vector ξ ∈ Rn following statements are equivalent up to absolute constant diﬀerence in σ.

1. Tails: P { ξ 2 ≥ γ} ≤ 2 exp

−

γ2 2σ2

∀γ ≥ 0.

2.

Moments:

(

E

[ξ

p

]

)

1 p

≤ σ√p

for

any

positive

integer

p.

3. Super-exponential moment: E exp σξ222 ≤ exp(1).

Lemma C.2 (Corollary 8 from [40]). Let {ξk}Nk=1 be a sequence of random vectors with values in Rn such that for k = 1, . . . , N and for all γ ≥ 0

E [ξk | ξ1, . . . , ξk−1] = 0,

γ2 E [ ξk 2 ≥ γ | ξ1, . . . , ξk−1] ≤ exp − 2σ2
k

almost surely,

N
where σk2 belongs to the ﬁltration σ(ξ1, . . . , ξk−1) for all k = 1, . . . , N . Let SN = ξk. Then
k=1
there exists an absolute constant C1 such that for any ﬁxed β > 0 and B > b > 0 with probability

46

at least 1 − β:
N
either σk2 ≥ B or
k=1

SN 2 ≤ C1

max

N

σ

2 k

,

b

k=1

ln 2n + ln ln B .

β

b

Lemma C.3 (corollary of Theorem 2.1, item (ii) from [43]). Let {ξk}Nk=1 be a sequence of random vectors with values in Rn such that

E [ξk | ξ1, . . . , ξk−1] = 0 almost surely, k = 1, . . . , N

N
and let SN = ξk. Assume that the sequence {ξk}Nk=1 satisfy “light-tail” assumption:
k=1

E exp

ξk

2 2

| ξ1, . . . , ξk−1 ≤ exp(1) almost surely,

k = 1, . . . , N,

σk2

where σ1, . . . , σN are some positive numbers. Then for all γ ≥ 0

  P  SN 2 ≥

√√ 2 + 2γ



N

γ2

σk2 ≤ exp − 3 .

k=1

(123)

D Technical Results

Lemma D.1. For the sequence αk+1 ≥ 0 such that

Ak+1 = Ak + αk+1, Ak+1 = 2Lα2k+1

we have for all k ≥ 0

Moreover, Ak = Ω

N2 L

.

αk+1 ≤ αk+1 d=ef k + 2 . 2L

(124) (125)

Proof. We prove (125) by induction. For k = 0 equation (124) gives us α1 = 2Lα21 ⇐⇒ α1 = 21L . Next we assume that (125) holds for all k ≤ l − 1 and prove it for k = l:

2Lα2l+1

(1=24)

l+1 (125)

1l

l(l + 3)

αi ≤ αl+1 + 2L (i + 1) = αl+1 + 4L .

i=1

i=1

√

√

This quadratic inequality implies that αk+1 ≤ 1+ 4k42L+12k+1 ≤ 1+ (42Lk+3)2 ≤ 2k4+L4 = k2+L2 .

Finally, the relation Ak = Ω

N2 L

is proved in Lemma 1 from [30] (see also [63]).

47

Lemma D.2 (See Lemma 3 from [29] and Lemma 4 from [16]). that
Ak+1 = Ak + αk+1, Ak+1(1 + Akµ) = Lα2k+1,
we have for all k ≥ 0

For the sequence 1
α0 = A0 = L

αk+1

≥ 0 such (126)

αk+1 = 1 + Akµ + 2L

(1 + Akµ)2 + Ak(1 + Akµ) ,

4L2

L

1

1

Ak ≥ L 1 + 2

µ 2k L,

αk+1 ≤

1+ µ + L

µ 1 + L Ak.

(127) (128) (129)

Proof. If we solve quadratic equation Ak+1(1 + Akµ) = Lα2k+1, Ak+1 = Ak + αk+1 with respect to

αk+1, we will get (127). Inequality (12√8) was established in Lemma 3 from [29] and Lemma 4 from

[16]. It remains to prove (129). Since

a2

+ b2

≤

a+b

for

all

a, b

≥

0

and

Ak

≥

A0

=

1 L

we

have

αk+1

(1=27)

1 + Akµ + 2L

(1 + Akµ)2 + Ak(1 + Akµ)

4L2

L

≤ 1 + µ Ak + 1 + Akµ + Ak + µ A2

2L 2L

2L

L Lk

1µ

µ

µ

µ

≤ L + L Ak + Ak 1 + L = 1 + L + 1 + L Ak.

Lemma D.3. Let A, B, D, r0, r1, . . . , rN , where N ≥ 1, be non-negative numbers such that

12

2

Dr0 l−1

r0

2 rl ≤ Ar0 + (N + 1)2 (k + 2)rk + B N

k=0

l−1
(k + 2)rk2,
k=0

∀l = 1, . . . , N.

(130)

Then for all l = 0, . . . , N we have

rl ≤ Cr0,

(131)

where C is such positive number that C2 ≥ max{2A + 2(B + D)C, 1}, i.e. one can choose C = max{B + D + (B + D)2 + 2A, 1}.

Proof. We prove (131) by induction. For l = 0 the inequality rl ≤ Cr0 trivially follows since C ≥ 1.

48

Next we assume that (131) holds for some l < N and prove it for l + 1:

r

(130) √ ≤ 2 Ar2 +

Dr0

l (k + 2)r + B r0

l+1

0 (N + 1)2

k

N

k=0

l
(k + 2)rk2
k=0

(131) √

DC l

BC

≤ r0 2 A + (N + 1)2 (k + 2) + N

k=0

l
(k + 2)
k=0

√

DC (l + 1)(l + 2) BC (l + 1)(l + 2)

≤ r0 2 A + (N + 1)2

2

+N

2

√

BC N (N + 1)

≤ r0 2 A + DC + N

2 ≤ r0 2A + 2(B + D)C ≤ Cr0.

≤C

Lemma D.4. Let C, r0, r1, . . . , rN , where N ≥ 1, be non-negative numbers such that

2

2

2C l−1

1/2 2

rl ≤ r0 + (N + 1)3/2 (k + 2) rk+1,

k=0

∀l = 1, . . . , N,

and C ∈ (0, 1/4). Then for all l = 0, . . . , N we have

(132)

rl ≤ 2r0,

(133)

Proof. We prove (133) by induction. For l = 0 the inequality rl ≤ 2r0 trivially follows. Next we assume that (133) holds for some l ≤ N − 1 and prove it for l + 1. From (132), C < 1/4, N ≥ 1 and
l ≤ N − 1 we have

3 r2 4 l+1

≤
(132)
≤
(133)
≤

1 − 2C(l + 2)1/2 r2 (N + 1)3/2 l+1

r2 +

2C

l−1
(k + 2)1/2r2

0 (N + 1)3/2

k+1

k=0

r2 +

1

l · (l + 1)1/2 · 4r2 ≤ 3r2,

0 2(N + 1)3/2

0

0

which implies rl+1 ≤ 2r0.

Lemma D.5. Let A, B, D, r0, r1, . . . , rN , r˜0, r˜1, . . . , r˜N , α0, α1, . . . , αN , where N ≥ 1, be non-

49

negative numbers such that

l−1
A r2 + A r˜2 ≤ Ar2 +

B√r0

l−1
α

(r + r˜ ),

ll

kk

k=0

0 N AN k=0 k+1 k k

∀l = 1, . . . , N,

(134)

where r˜0 = 0, A0 = α0 > 0, Al = Al−1 + αl and αl ≤ DAl−1 for l = 1, . . . , N and D ≥ 1. Then

for all l = 1, . . . , N we have

rl ≤ √Cr0 , Al

r˜l−1 ≤

C r0 Al−1

(135)

and r0 ≤ √CAr00 where C is such positive number that

C ≥ max

BD A0, 2 +

B2D2 + A + 2BCD , 4

i.e. one can choose C = max √A0, 3BD+√92B2D2+4A .

Proof. We prove (135) by induction. For l = 1 the inequality r˜0 ≤ √CAr00 trivially follows since r˜0 = 0. What is more, (134) implies that

A1r12 ≤ Ar02 + B√α1r02 =⇒ r1 ≤ r0 N AN

A + BD√A0 ≤ r0 A1 A1N AN

√

A + BD A0 ≤ √Cr0 ,

A1

A1

√

√

since C ≥ A0 and C ≥ A + BCD ≥

√ A + BD A0.

Note

that

we

also

have

r0

≤

√Cr0 .

Next

A0

we assume that (135) holds for some l ≤ N − 1 and prove it for l + 1:

Alr˜l2

(134)
≤

Ar2 + B√r0

l
α (r + r˜ )

0 N AN k=0 k+1 k k

(135)
≤

Ar2 + B√Cr02

l

α√k+1 + B√Cr02

l−1 αk+1 Br0αl+1r˜l √+ √

0 N AN k=0 Ak N AN k=0 Ak N AN

≤ Ar2 + BC√Dr02 l A + BC√Dr02 l−1 A + BD√r0Alr˜l 0 N AN k=0 k N AN k=0 k AN

≤

Ar2 + BC√Dr02 (l + 1)
0

Al + BC√Dr02 l

Al−1 + BD√r0Alr˜l

N AN

N AN

AN

≤ (A + 2BCD)r2 + BD√r0Alr˜l

0

AN

0 ≥ r˜2 − B√Dr0r˜l − (A + 2BCD)r02 .

l

AN

Al

From this we have that r˜l is not greater than the biggest root of the quadratic equation correspond-

50

ing to the last inequality, i.e.

r˜l ≤ B√Dr0 + 2 AN

B2D2r02 + (A + 2BCD)r02

4AN

Al

≤ BD + B2D2 + A + 2BCD √r0 ≤ √Cr0 .

2

4

Al

Al

≤C

It implies that

Al+1rl2+1 rl+1

(134)
≤
(135)
≤
≤
≤

Ar2 + B√r0

l
α (r + r˜ )

0 N AN k=0 k+1 k k

Ar2 + 2B√Cr02 l α√k+1 0 N AN k=0 Ak

Ar2 + 2B√CDr02 (l + 1) 0 N AN

Al ≤ Ar02 + 2BCDr02,

r0 A + 2BCD ≤ Cr0 .

Al+1

Al+1

That is, we proved the statement of the lemma for

C ≥ max

BD A0, 2 +

B2D2 + A + 2BCD . 4

In particular, via solving the equation

C = BD + 2

B2D2 + A + 2BCD 4

w.r.t. C one can show that the choice C = max of the lemma on C.

√A0, 3BD+√92B2D2+4A

satisﬁes the assumption

E Similar Triangles Method with Inexact Proximal Step

In this section we focus on the composite optimization problem. i.e. problems of the type

min F (x) = f (x) + h(x),
x∈Rn

(136)

where f (x) is convex and L-smooth and h(x) is convex and Lh-smooth. Before we present our method, let us introduce new notation.

51

Deﬁnition E.1. Assume that function g(x) deﬁned on Rn is such that there exists (possibly nonunique) x∗ satisfying g(x∗) = minx∈Rn g(x). Then for arbitrary δ > 0 we say that xˆ is δ-solution of the problem g(x) → minx∈Rn and write xˆ = argminδx∈Rn g(x) if g(xˆ) − g(x∗) ≤ δ.

Note that δ-solution could be non-unique, but for our purposes in such cases it is enough to use any point from the set of δ-solutions. In the analysis we will need the following result.

Lemma E.2 (See also Theorem 9 from [87]). Let g(x) be convex, L-smooth, x∗ is such that g(x∗) = minx∈Rn g(x) and xˆ = argminδx∈Rn g(x) for some δ > 0. Then for all x ∈ Rn

√ ∇g(xˆ), xˆ − x ≤ 2Lδ xˆ − x 2.

(137)

Proof. Since x∗ is a minimizer of g(x) on Rn, we have ∇g(x∗) = 0 and [63] ∇g(xˆ) 2 ≤ 2L(g(xˆ) − g(x∗)).

Next, using this, Cauchy-Schwarz inequality and deﬁnition of xˆ we get √
∇g(xˆ), xˆ − x ≤ ∇g(xˆ) 2 · xˆ − x 2 ≤ 2L(g(xˆ) − g(x∗)) xˆ − x 2 ≤ 2Lδ xˆ − x 2,

that concludes the proof. The main method of this section is stated as Algorithm 8. In the STM IPS we use functions

Algorithm 8 Similar Triangles Methods with Inexact Proximal Step (STM IPS)

Input: x˜0 = z0 = x0 — starting point, N — number of iterations

1: Set α0 = A0 = 0

2: for k = 0, 1, . . . , N − 1 do

3: Choose αk+1 such that Ak + αk+1 = 2Lα2k+1, Ak+1 = Ak + αk+1 4: x˜k+1 = / (Akxk+αk+1zk) Ak+1

5:

zk+1 = argminδzk∈+R1n gk+1(z), where gk+1(z) is deﬁned in (138) and δk+1 = δ

zk − zˆk+1

2 2

6: xk+1 = / (Akxk+αk+1zk+1) Ak+1

7: end for Output: xN

gk+1(z) which are deﬁned for all k = 0, 1, . . . as follows:

gk+1(z) = 21 zk − z 22 + αk+1 f (x˜k+1) + ∇f (x˜k+1), z − x˜k+1 + h(z) .

(138)

Each gk+1(z) is 1-strongly convex function with, as a consequence, unique minimizer zˆk+1 d=ef argminz∈Rn gk+1(z).
Let us discuss a little bit the proposed method. First of all, if we slightly modify the method
and choose δk+1 = 0, then we will get STM which is well-studied in the literature. Secondly, it may seem that in order to run the method we need to know zk − zˆk+1 2, but in fact we do not need it. If gk+1(z) is Lk+1-smooth and µk+1-strongly convex, then one can run STP for
T = O / Lk+1 µk+1 ln Lk+1/δ iterations with zk as a starting point to solve the problem gk+1(z) →

52

minz∈Rn and get zk+1 = argminδzk∈+R1n gk+1(z). Note that in this case we do not need to know zˆk+1. Moreover, we do not assume that iterates of STM IPS are bounded and instead of assuming
it we prove such result which makes the analysis a little bit more technical then ones for STP.
Finally, we notice that one can prove the results we present below even with such αk+1 that Ak+1 = Ak + αk+1 = Lα2k+1. It improves numerical constants in the upper bounds a little bit, but for simplicity we use the same choice of αk+1 as for the stochastic case.
We start our analysis with the following lemma.

Lemma E.3 (see also Theorem 1 from [20]). Assume that f (x) is convex and L-smooth, h(x) is convex and Lh-smooth and δ < 12 . Then after N ≥ 1 iterations of Algorithm 8 we have

AN F (xN ) − F (x∗)

≤

1 R2 − 1 R2

+

δˆ

N

−1

√ k

+

2R2

,

20 2N

k+1

k=0

(139)

where x∗ is the solution of (136) closest to the starting point z0, Rk+1 d=ef R0 d=ef x∗ − z0 2, Rk+1 d=ef max{Rk, Rk+1} for k = 0, 1, . . . , N − 1 and δˆ d=ef

x∗ − zk+1 2, R0 d=ef ((1L−h√+22δL)2)δL .

Proof. First of all, we prove by induction that x˜k+1, xk, zk ∈ BRk (x∗) for k = 0, 1, . . .. For k = 0 this is true since x0 = z0, R0 = R0 = z0 − x∗ and x˜1 = (A0x0+αk+1z0)/A1 = z0, since A0 = α0 = 0 and A1 = α1. Next, assume that x˜k+1, xk, zk ∈ BRk (x∗) for some k ≥ 0. By deﬁnition of Rk+1 and Rk+1 we have zk+1 ∈ BRk+1 (x∗) ⊆ BRk+1(x∗). Due to the assumption that xk ∈ BRk (x∗) ⊆ BRk+1(x∗) ⊆ BRk+1 (x∗) and convexity of the BRk+1 (x∗) we get that xk+1 ∈ BRk+1(x∗) since it is a convex combination of xk and zk+1, i.e. xk+1 = / . (Akxk+αk+1zk+1) Ak+1 Similarly, x˜k+2 lies in the ball
BRk+1(x∗) since it is a convex combination of xk+1 and zk+1, i.e. xk+1 = (Akxk+1+αk+1zk+1)/Ak+1. That is, we proved that x˜k+1, xk, zk ∈ BRk (x∗) for all non-negative integers k.
Since zk+1 = argminδzk∈+R1n gk+1(z) and gk+1(z) is 1-strongly convex and (αk+1Lh + 1)-smooth we can apply Lemma E.2 and get

∇gk+1(zk+1), zk+1 − x∗ ≤

2(αk+1Lh + 1)δ

zk − zˆk+1

2 2

·

zk+1 − x∗ 2.

(140)

From 1-strong convexity of gk+1(z) we have

zk+1 − zˆk+1

2 2

≤

2(gk+1(zk+1)

−

gk+1(zˆk+1))

≤

2δ

zk − zˆk+1

22.

Together with triangle inequality it implies that √
zk − zˆk+1 2 ≤ zk − x∗ 2 + x∗ − zk+1 2 + zk+1 − zˆk+1 2 ≤ 2Rk+1 + 2δ zk − zˆk+1 2,

and, after rearranging the terms,
zk − zˆk+1 2 ≤ 2√ Rk+1. 1 − 2δ
Applying inequality above and (125) for the r.h.s. of (140) we obtain zk+1 − zk + αk+1∇f (x˜k+1) + αk+1∇h(zk+1), zk+1 − x∗ ≤ δˆ√k + 2R2 ,
k+1

(141) (142)

53

where we used 2 2(αk+1L√h + 1)δ (1 − 2δ)2

(125)
≤2

2 ((k + 2)Lh +√ 2(k + 2)L) δ ≤ 2 2(1 − 2δ)2L

(Lh

+√2L)

δ

√ k

+

2

(1 − 2δ)2L

and δˆ d=ef 2 ((1L−h√+22δL)2)δL . Using this we get

αk+1 ∇f (x˜k+1), zk − x∗

=
(142)
≤

αk+1 ∇f (x˜k+1), zk − zk+1 + αk+1 ∇f (x˜k+1), zk+1 − x∗

αk+1 ∇f (x˜k+1), zk − zk+1

+

zk+1 − zk, x∗ − zk+1 √

+αk+1 ∇h(zk+1), x∗ − zk+1 + δˆ k + 2Rk2+1.

One can check via direct calculations that a, b = 12 a + b 22 − 12 a 22 − 21 b 22,
From the convexity of h

∀ a, b ∈ Rn.

∇h(zk+1), x∗ − zk+1 ≤ h(x∗) − h(zk+1).

Combining previous three inequalities we obtain

αk+1 ∇f (x˜k+1), zk − x∗ ≤ By deﬁnition of xk+1 and x˜k+1

αk+1 ∇f (x˜k+1), zk − zk+1 − 12 zk+1 − x∗ 22 + αk+1

− 1 zk − zk+1 2 + 1 zk − x∗ 2

2

22

2

h(x∗) − h(zk+1) + δˆ√k + 2R2 .

k+1

xk+1 = Akxk + αk+1zk+1 = Akxk + αk+1zk + αk+1 zk+1 − zk

Ak+1

Ak+1

Ak+1

= x˜k+1 + αk+1 zk+1 − zk .

Ak+1

Together with the previous inequality and Ak+1 = 2Lα2k+1, it implies

αk+1 ∇f (x˜k+1), zk − x∗

≤ Ak+1 ∇f (x˜k+1), x˜k+1 − xk+1

− A2k+1 x˜k+1 − xk+1 2 + 1 zk − x∗ 2 − 1 zk+1 − x∗ 2

2α2k+1

22

22

2

+αk+1 h(x∗) − h(zk+1) + δˆ√k + 2R2

k+1

≤ Ak+1

∇f (x˜k+1), x˜k+1 − xk+1 − 2L x˜k+1 − xk+1 2

2

2

+1

zk − x∗

2 2

−

1

zk+1 − x∗

2 2

2

2

√

+αk+1 h(x∗) − h(zk+1) + δˆ k + 2Rk2+1

≤

Ak+1(f (x˜k+1) − f (xk+1)) + 1

zk − x∗

2−1
2

zk+1 − x∗

2 2

2√

2

+αk+1 h(x∗) − h(zk+1) + δˆ k + 2Rk2+1

(143)

54

From the convexity of f we get ∇f (x˜k+1), xk − x˜k+1

≤ f (xk) − f (x˜k+1).

By deﬁnition of x˜k+1 we have

αk+1 x˜k+1 − zk = Ak xk − x˜k+1 .

(144) (145)

Putting all together, we get αk+1 ∇f (x˜k+1), x˜k+1 − x∗

= (1=45)
(143),(144)
≤

αk+1 ∇f (x˜k+1), x˜k+1 − zk +αk+1 ∇f (x˜k+1), zk − x∗
Ak ∇f (x˜k+1), xk − x˜k+1 +αk+1 ∇f (x˜k+1), zk − x∗
Ak f (xk) − f (x˜k+1)

+Ak+1 f (x˜k+1) − f (xk+1)

+ 1 zk − x∗ 2 − 1 zk+1 − x∗ 2

2

22

2

+αk+1 h(x∗) − h(zk+1) + δˆ√k + 2R2 .

k+1

Rearranging the terms and using Ak+1 = Ak + αk+1, we obtain

Ak+1f (xk+1) − Akf (xk) ≤ αk+1 f (x˜k+1) + ∇f (x˜k+1), x∗ − x˜k+1

+ 1 zk − x∗ 2

2

2

−1

zk+1 − x∗

2 2

+

αk+1

h(x∗) − h(zk+1)

+ δˆ√k + 2R2 ,
k+1

2

and after summing these inequalities for k = 0, . . . , N −1 and applying convexity of f , i.e. inequality ∇f (x˜k+1), x∗ − x˜k+1 ≤ f (x∗) − f (x˜k+1), we get

A f (xN )

≤

1 R2 − 1 R2

+A

f (x∗) + A

N −1
h(x∗) − α

h(zk+1)

+

δˆ

N −1

√ k

+

2R2

,

N

20 2N

N

N

k+1

k+1

k=0

k=0

where we used that A0 = 0. Finally, convexity of h and deﬁnition of xk+1, i.e. xk+1 = / , (Akxk+αk+1zk+1) Ak+1
implies AN h(xN ) ≤ AN−1h(xN−1) + αN h(zN ).

Applying this inequality for AN−1h(xN−1), AN−2h(xN−2), . . . , A1h(x1) in a sequence we get

which implies

N −1

N −1

AN h(xN ) ≤ A0h(x0) + αk+1h(zk+1) = αk+1h(zk+1),

k=0

k=0

AN F (xN ) − F (x∗) that ﬁnishes the proof.

≤

1 R2 − 1 R2

+

δˆ

N

−1

√ k

+

2R2

,

20 2N

k+1

k=0

Below we state our main result of this section.

55

Theorem E.4. Let f (x) be convex and L-smooth, h(x) be convex and Lh-smooth and δ ≤ 14 .

Assume that for a given number of iterations N ≥ 1 the number δˆ d=ef 2 ((1L−h√+22δL)2)δL satisﬁes

δˆ ≤

C
3

with some positive constant C ∈ (0, 1/4). Then after N iteration of Algorithm 8 we

(N +1) /2

have

F (xN ) − F (x∗) ≤ 3R02 . 2AN

(146)

Proof. Lemma E.3 implies that

A

F (xl) − F (x∗)

≤

1 R2

−

1 R2

+

l−1
δˆ

√ k

+

2R2

l

20 2l

k+1

k=0

for l = 1, 2, . . . , N . Since F (xl) ≥ F (x∗) for each l and δˆ ≤

C
3

we get the recurrence

(N +1) /2

(147)

2

2

2C l−1

1/2 2

Rl ≤ R0 + (N + 1)3/2 (k + 2) Rk+1,

k=0

∀l = 1, . . . , N.

Note that the r.h.s. of the previous inequality is non-decreasing function of l. Let us deﬁne ˆl as the largest integer such that ˆl ≤ l and Rˆl = Rˆl. Then Rˆl = Rˆl = Rˆl+1 = . . . = Rl and, as a consequence,

R2 ≤ R2 +

2C

l−1
(k + 2)1/2R2 ,

l

0 (N + 1)3/2

k+1

k=0

∀l = 1, . . . , N.

(148)

Using Lemma D.4 we get that Rl ≤ 2R02 for all l = 1, . . . , N . We plug this inequality together with

δ≤

C (N +1)3/2

≤

1 4(N +1)3/2

and

RN2

≥0

in

(147)

and

get

N

∗

12

4R02 N−1

1/2

AN (F (x ) − F (x )) ≤ 2 R0 + 4(N + 1)3/2 (k + 2)

k=0

≤ 3R2, 20

which concludes the proof.

Corollary E.5. Under assumptions of Theorem E.4 we get that for an arbitrary ε > 0 after

N = O LR02 ε

(149)

iterations of Algorithm 8 we have F (xN ) − F (x∗) ≤ ε. Moreover, we get that δ should satisfy

L δ = O (Lh + L)N 3 .

(150)

56

Proof. The ﬁrst part of the corollary follows from (146) and Lemma D.1. Relation (150) follows

from the deﬁnition of δˆ and δˆ ≤

C
3

. Indeed, since δˆ d=ef 2

(N +1) /2

(Lh√+2L)δ (1− 2δ)2L

and

C

≤

1 4

we

get

that

C2(1 − √2δ)2L

L

1

L

δ ≤ 4(Lh + 2L)(N + 1)3 ≤ 64(Lh + 2L)N 3 ≤ 64 (Lh + L)N 3 .

That is, if the auxiliary problem gk+1(z) → minz∈Rn is solved with good enough accuracy, then STM IPS requires the same number of iterations as STM to achieve F (xN ) − minx∈Rn F (x) ≤ ε.
Finally, we notice that one can set δk+1 in Algorithm 8 in a diﬀerent way in order to get the same convergence guarantees, e.g. one can use δk+1 = δRk2+1 and the order of δ given by (150) will be the same. In this case inequalities (140) and (142) transform to

∇gk+1(zk+1), zk+1 − x∗ ≤ 2(αk+1Lh + 1)δRk2+1 · zk+1 − x∗ 2

and

√

zk+1 − zk + αk+1∇f (x˜k+1) + αk+1∇h(zk+1), zk+1 − x∗ ≤ δˆ k + 2Rk2+1,

respectively, where δˆ d=ef 2 (Lh+L2L)δ . Then the remaining part of the proof remains the same and gives the same result up to small changes in the numerical constants.

F Missing Proofs from Section 4

F.1 Proof of Theorem 4.1

By deﬁnition of F

F (xN ) − min F (x) = f (xN ) + Ry2 AxN 2 − min f (x) + Ry2 Ax 2

x∈Q

ε

2 x∈Q

ε

2

≥ f (xN ) + Ry2 AxN 2 − min f (x) + Ry2 Ax 2

ε

2 Ax=0,x∈Q

ε

2

= f (xN ) − min f (x) + Ry2 AxN 2,

Ax=0,x∈Q

ε

2

which implies

f (xN ) − f (x∗) + Ry2

AxN

(22)
2 ≤ ε,

ε

2

where x∗ is an arbitrary solution of (17). Taking inequality AxN

ﬁrst part of (23). From Cauchy-Schwarz inequality we obtain

(151)

2 2

≥

0

into

account

we

get

the

(122)
−Ry AxN 2 ≤ − y∗ 2 · AxN 2 ≤ y∗, AxN ≤ f (xN ) − f (x∗).

Together with (151) it gives us quadratic inequality on Ry AxN 2:

−Ry AxN 2 + Ry2 AxN 2 ≤ ε.

ε

2

57

Therefore, Ry AxN √2 should be less then the greatest root of the corresponding quadratic equation,

i.e. Ry

AxN

2

≤

1+ 2

5ε

<

2ε.

F.2 Proof of Theorem 4.2

Note that h(x) is convex and Lh-smooth in Rn with Lh = 2R2yλmax(A⊤A)/ε since ∇h(x) = 2R2yA⊤Ax/ε and

∇h(x) − ∇h(y) 2 = 2Ry2 A⊤A(x − y) 2 ≤ 2Ry2 A⊤A 2 · x − y 2

ε

ε

2Ry2 λmax (A⊤ A)

≤

ε

x−y 2

for all x, y ∈ Rn. We can apply STM with inexact proximal step (STP IPS) which is presented in

Section E as Algorithm 8 to solve problem (21). Corollary E.5 (see Section E in the Appendix; see also the text after the corollary) states that in order to get such xN that satisfy (22) we should run STP IPS for N = O LR2/ε iterations with δ = O ε3/2/((Lh+L)√LR3) , where R = x0 − x∗ 2, x∗

is the closest to x0 minimizer of F and δ is such that for all k = 0, . . . , N − 1 the auxiliary problem

gk+1(z) → minz∈Rn for ﬁnding zk+1 is solved with accuracy gk+1(zk+1)−gk+1(zˆk+1) ≤ δ

zk −zˆk+1

2 2

where gk+1(z) is deﬁned as (see also (138))

gk+1(z) = 12 zk − z 22 + αk+1 f (x˜k+1) + ∇f (x˜k+1), z − x˜k+1 + h(z)

for k = 0, 1, . . . and zˆk+1 = argminz∈Rn gk+1(z). That is, if the auxiliary problem is solved accurate enough at each iteration, then number of iterations, i.e. number of calculations ∇f (x), corresponds
to the optimal bound presented in Table 1.
However, in order to solve the auxiliary problem minz∈Rn gk+1(z) one should run another optimization method as a subroutine, e.g. STM. Note that ImA = ImA⊤ = (KerA)⊥ and if the starting point for this problem is chosen as zk − αk+1∇f (x˜k+1) then the iterates of STM applied to solve problem minz∈Rn gk+1(z) lie in zk − αk+1∇f (x˜k+1) + (KerA)⊥ since ∇gk+1(z) ∈ Im(A) for all z ∈ zk − αk+1∇f (x˜k+1) + (KerA)⊥ (one can prove it using simple induction, see Theorem 5.12 for the details of the proof of the similar result). Therefore, the auxiliary problem can be considered as a minimization of (1 + 2αk+1R2yλ+min(A⊤A)/ε)-strongly convex on zk − αk+1∇f (x˜k+1) + (KerA)⊥ and (1 + 2αk+1R2yλmax(A⊤A)/ε)-smooth on Rn function. Then, one can estimate the overall complexity of the auxiliary problem using the condition number of gk+1(z) on zk − αk+1∇f (x˜k+1) + (KerA)⊥:

1 + / 2αk+1R2yλmax(A⊤A) ε ≤ λmax(A⊤A) d=ef χ(A⊤A). 1 + / 2αk+1R2yλ+min(A⊤A) ε λ+min(A⊤A)

(152)

Assume that zk+1 is such that gk+1(zk+1) − gk+1(zˆk+1) ≤ δ˜ zk − αk+1∇f (x˜k+1) − zˆk+1 2. Then

zk − αk+1∇f (x˜k+1) − zˆk+1 2

≤ ≤ ≤
(125)
≤

zk − zˆk+1 2 + αk+1 ∇f (x˜k+1) 2 zk − zˆk+1 2 + αk+1 ∇f (x˜k+1) − ∇f (x∗) 2 + αk+1 ∇f (x∗) 2 zk − zˆk+1 2 + αk+1L x˜k+1 − x∗ 2 + αk+1 ∇f (x∗) 2

zk − zˆk+1 2 + k + 2 Rk+1 + k + 2 ∇f (x∗) 2

2

2L

58

and using the similar steps as in the proof of inequality (141) we get

√

zk − zˆk+1 2 ≤

2 + (k+22) 2δ˜

Rk+1 (k + 2) ∇f (x∗) 2

+

.

1 − 2δ˜

2L 1 − 2δ˜

Combining previous two inequalities we conclude that



√



zk − α ∇f (x˜k+1) − zˆk+1

≤

2

+

(k+2) 2

2δ˜ + k + 2  R

k+1

2

1 − 2δ˜

2

k+1

+k + 2 1 + 1

2L

1 − 2δ˜

∇f (x∗) 2.

It means that to achieve gk+1(zk+1) − gk+1(zˆk+1) ≤ δRk2+1 with δ = O ε3/2/((Lh+L)√LR3) one can run STM to solve the auxiliary problem gk+1(z) → minz∈Rn for T iterations with the starting point zk − αk+1∇f (x˜k+1) where

T =O

χ(A⊤A) ln LgN L3/2(R2yλmax(A⊤A)/ε + L)R3 R2 + ∇f (x∗) 22/L2

,

ε5/2

2αk+1Ry2λmax(A⊤A) (149)+(125)

Ry2Rλmax(A⊤A)

LgN = 1 +

ε

=O

√ Lε3/2

or, equivalently,

T = O χ(A⊤A) ln λmax(A⊤A)L(R2y λmax(A⊤A)/ε + L)Ry2R4 R2 + ∇f(x∗) 22/L2

.

ε4

G Missing Lemmas and Proofs from Section 5.1

G.1 Lemmas
The following lemma is rather technical and provides useful inequalities that show how biasedness of ∇˜ Ψ(y, ξk) interacts with convexity and Lψ-smoothness of ψ.

Lemma G.1. Assume that function ψ(y) is convex and Lψ-smooth on Rn. Then for all x, y ∈ Rn

ψ(y) ≥ ψ(x) + E ∇˜ Ψ(x, ξk) , y − x − δ y − x 2,

ψ(y)

≤

ψ(x) +

E ∇˜ Ψ(x, ξk) , y − x

+ Lψ

y−x

2 2

+

δ2 .

2Lψ

(153) (154)

Proof. From the convexity of ψ we have ψ(x) − ψ(y) ≤ ∇ψ(x), x − y = E ∇˜ Ψ(x, ξk) , x − y + ∇ψ(x) − E ∇˜ Ψ(x, ξk) , x − y

≤ E ∇˜ Ψ(x, ξk) , x − y + ∇ψ(x) − E ∇˜ Ψ(x, ξk) · x − y 2
2

(38)
≤

E ∇˜ Ψ(x, ξk) , x − y + δ x − y 2,

59

which proves the inequality (153). Applying L-smoothness of ψ(x) we get

ψ(y) ≤ ψ(x) + ∇ψ(x), y − x + L2 y − x 22 = ψ(x) + E ∇˜ Ψ(x, ξk) , y − x + ∇ψ(x) − E ∇˜ Ψ(x, ξk) , y − x

Due to Fenchel-Young inequality

a, b

≤

1 2λ

a

2 2

+

λ 2

b 22, a, b ∈ Rn, λ > 0,

+ L2 y − x 22.

∇ψ(x) − E ∇˜ Ψ(x, ξk) , y − x

≤ 1 ∇ψ(x) − E ∇˜ Ψ(x, ξk) 2 + L y − x 2

2L

22

2

(38)
≤

δ2 + L y − x 2.

2L 2

2

Combining these two inequalities we get (154).

Next, we will use the following notation: Ek[·] = Eξk+1 [·] which denotes conditional mathematical expectation with respect to all randomness that comes from ξk+1.

Lemma G.2 (see also Theorem 1 from [20]). For each iteration of Algorithm 2 we have

AN ψ(yN ) ≤ for arbitrary z ∈ Rn.

1 z − z0 2 − 1 z − zN 2

2

22

2

N −1
+ αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), z − y˜k+1

k=0

N −1
+ Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

k=0

N −1 Ak+1

+

2L˜

k=0

Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇˜ Ψ(y˜k+1, ξk+1) 2
2

+δ N−1 Ak yk − y˜k+1 2 + δ2 N−1 AkL˜+1 ,

k=0

k=0

(155)

Proof. The proof of this lemma follows a similar way as in the proof of Theorem 1 from [20]. We can rewrite the update rule for zk in the equivalent way:

zk+1 = argmin

αk+1 ∇˜ Ψ(y˜k+1, ξk+1), z − y˜k+1

+1

z − zk

2 2

.

z∈Rn

2

From the optimality condition we have that for all z ∈ Rn

zk+1 − zk + αk+1∇˜ Ψ(y˜k+1, ξk+1), z − zk+1 ≥ 0.

(156)

Using this we get

αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − z = αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − zk+1 + αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk+1 − z

(156)
≤

αk+1

∇˜ Ψ(y˜k+1, ξk+1), zk

− zk+1

+

zk+1 − zk, z − zk+1 .

60

One can check via direct calculations that

a, b ≤ 1 a + b 2 − 1 a 2 − 1 b 2,

2

22 22 2

Combining previous two inequalities we obtain

∀ a, b ∈ Rn.

αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − z By deﬁnition of yk+1 and y˜k+1

≤ αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − zk+1 − 12 zk − zk+1 22

+ 1 zk − z 2 − 1 zk+1 − z 2.

2

22

2

yk+1 = Akyk + αk+1zk+1 = Akyk + αk+1zk + αk+1 zk+1 − zk

Ak+1

Ak+1

Ak+1

= y˜k+1 + αk+1 zk+1 − zk .

Ak+1

Together with previous inequality, it implies

αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − z

≤ Ak+1 ∇˜ Ψ(y˜k+1, ξk+1), y˜k+1 − yk+1

− A2k+1 y˜k+1 − yk+1 2 + 1 zk − z 2 − 1 zk+1 − z 2

2α2k+1

22

22

2

≤ Ak+1 ∇˜ Ψ(y˜k+1, ξk+1), y˜k+1 − yk+1

− 2L˜ y˜k+1 − yk+1 2

2

2

+ 1 zk − z 2 − 1 zk+1 − z 2

2

22

2

= Ak+1 Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜k+1 − yk+1

− 2L˜ y˜k+1 − yk+1 2

2

2

+Ak+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek + 12 zk − z 22 − 21 zk+1 − z 22.

∇˜ Ψ(y˜k+1, ξk+1)

, y˜k+1 − yk+1

From Fenchel-Young inequality

a, b

≤

1 2λ

a

2 2

+

λ 2

b 22, a, b ∈ Rn, λ > 0, we have

∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜k+1 − yk+1

≤

1 2L˜

∇˜ Ψ(y˜k+1, ξk+1) − Ek

∇˜ Ψ(y˜k+1, ξk+1)

2 + L˜ y˜k+1 − yk+1 2.

22

2

61

Using this, we get

αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − z ≤ Ak+1 Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜k+1 − yk+1

(154)
≤ With Lemma G.1 in hand, we have

− L˜ y˜k+1 − yk+1 2

2

2

+ A2kL˜+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek

+ 1 zk − z 2 − 1 zk+1 − z 2

2

22

2

Ak+1

ψ(y˜k+1) − ψ(yk+1) + δ2 L˜

+ 21 zk − z 22 − 12 zk+1 − z 22

+ A2kL˜+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek

∇˜ Ψ(y˜k+1, ∇˜ Ψ(y˜k+1,

ξk+1) ξk+1)

2 2
(157)
2
.
2

∇˜ Ψ(y˜k+1, ξk+1), yk − y˜k+1

= Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

+ ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

(153)
≤ ψ(yk) − ψ(y˜k+1) + δ yk − y˜k+1 2

(158)

+ ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1 .

By deﬁnition of y˜k+1 we have

αk+1 y˜k+1 − zk = Ak yk − y˜k+1 .

(159)

Putting all together, we get

αk+1 ∇˜ Ψ(y˜k+1, ξk+1), y˜k+1 − z = αk+1 ∇˜ Ψ(y˜k+1, ξk+1), y˜k+1 − zk + αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − z (1=59) Ak ∇˜ Ψ(y˜k+1, ξk+1), yk − y˜k+1 + αk+1 ∇˜ Ψ(y˜k+1, ξk+1), zk − z
(157),(158)
≤ Ak ψ(yk) − ψ(y˜k+1) + δ yk − y˜k+1 2
+Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

+Ak+1

ψ(y˜k+1)

−

ψ(yk+1)

+

δ2 L˜

+

1 2

zk − z

2 2

−

1 2

zk+1 − z

2 2

+

A

k+
˜

1

∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1)

2
.

2L

2

62

Rearranging the terms and using Ak+1 = Ak + αk+1, we obtain
Ak+1ψ(yk+1) − Akψ(yk) ≤ αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), z − y˜k+1 + 12 zk − z 22 − 21 zk+1 − z 22 + Akδ yk − y˜k+1 2 + Ak+L˜1δ2 + A2kL˜+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) 22 +Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1 ,

and after summing these inequalities for k = 0, . . . , N − 1 we get

A ψ(yN )

≤

1

z − z0

2−1

z − zN

N −1
2+ α

ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), z − y˜k+1

N

2

22

2

k+1

k=0

N −1
+ Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

k=0

N −1 Ak+1

+

2L˜

k=0

Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇˜ Ψ(y˜k+1, ξk+1) 2
2

+δ N−1 Ak yk − y˜k+1 2 + δ2 N−1 AkL˜+1 ,

k=0

k=0

where we use that A0 = 0.

The following lemma plays the central role in our analysis and it serves as the key to prove that the iterates of SPDSTM lie in the ball of radius Ry up to some polylogarithmic factor of N .

Lemma G.3 (see also Lemma 7 from [19]). Let the sequences of non-negative numbers {αk}k≥0, random non-negative variables {Rk}k≥0 and random vectors {ηk}k≥0, {ak}k≥0 satisfy inequality

12

l−1

l−1

l−1

kk

2

k2

2 Rl ≤ A + hδ αk+1Rk + u αk+1 η , a + c αk+1 η 2,

k=0

k=0

k=0

(160)

for all l = 1, . . . , N , where h, δ, u and c are some non-negative constants. Assume that for each k ≥ 1 vector ak is a function of η0, . . . , ηk−1, a0 is a deterministic vector, u ≥ 1, sequence of random vectors {ηk}k≥0 satisfy ∀k ≥ 0

E ηk | η0, . . . , ηk−1 = 0, E exp

ηk

2 2

σk2

| η0, . . . , ηk−1 ≤ exp(1),

(161)

αk+1 ≤ αk+1 = D(k + 2), σk2 ≤ Cε N for some D, C > 0, ε > 0, β ∈ (0, 1) and sequence of
αk+1 ln β
random variables {Rk}k≥0 is such that ak 2 ≤ dRk with some positive deterministic constant d ≥ 1 and Rk = max{Rk−1, Rk} for all k ≥ 1, R0 = R0, Rk depends only on η0, . . . , ηk and also

63

assume that ln

N β

≥ 3.

If

additionally

ε≤

H R20
2

and

δ

≤

GR0 2 , then with probability at least

N

(N +1)

1 − 2β the inequalities

Rl ≤ J R0

(162)

and

l−1

l−1

u

αk+1 ηk, ak + c

α2k+1

ηk

2 2

≤

k=0

k=0

24cCDH + hGDJ +udC1 CDHJ g(N ) R02

(163)

ln
hold for all l = 1, . . . , N simultaneously, where C1 is some positive constant, g(N ) =

N β

+

l

n

l

n(

B b

)

,

ln

N β

B = 2d2CDHR02 2A + (1 + ud)R02 + 48CDHR02 (2c + ud) + h2G2R02D (2(1 + ud))N ,

b = σ02α21d2R02 and

J = max 1, udC1 CDHg(N ) + hGD

2 2A + udC1 CDHg(N ) + hGD + R2 + 48cCDH .
0

Proof. We start with applying Cauchy-Schwarz inequality to the second and the third terms in the right-hand side of (160):

12

l−1

l−1 k

l−1

2

k2

2 Rl ≤ A + hδ αk+1Rk + ud αk+1 η 2Rk + c αk+1 η 2,

k=0

k=0

k=0

≤ A + h2δ2 l−1 α2

+ ud + 1 l−1 R2 + c + ud

l−1
α2

ηk 2.

2

k+1

2

k

2

k+1

2

k=0

k=0

k=0

(164)

The idea of the proof is as following: estimate RN2 roughly, then apply Lemma C.2 in order to estimate second term in the last row of (160) and after that use the obtained recurrence to estimate

right-hand side of (160).

Using

Lemma

C.3

we

get

that

with

probability

at

least

1

−

β N

ηk 2

≤

√ 2

1+



3 ln N β

√ σk ≤ 2 1 +


3 ln N β

√ Cε

αk+1 ln

N β

=  1 +

αk+1 ln

N β

3



√ 2C ε

≤

2

αk+1 

3 √2Cε, αk+1

(165)

64

where

in

the

last

inequality

we

use

ln

N β

≥ 3.

Using

union

bound

and

αk+1

≤

αk+1

= D(k + 2)

we

get that with probability ≥ 1 − β the inequality

1 R2

≤

A+

h2 δ2 D2

l−1
(k

+ 2)2

+

ud + 1

l−1

R2

+ 24Cε

c + ud

l−1
α

2l

2

2

k

2

k+1

k=0

k=0

k=0

h2 δ2 D2

2 ud + 1 l−1 2

ud l−1

≤ A + 2 l(l + 1) + 2

Rk + 24CDε c + 2

(k + 2)

k=0

k=0

h2 δ2 D2

2 ud + 1 l−1 2

ud

≤ A + 2 l(l + 1) + 2

Rk + 12CDε c + 2 l(l + 3)

k=0

holds for all l = 1, . . . , N simultaneously. Note that the last row in the previous inequality is non-decreasing function of l. If we deﬁne ˆl as the largest integer such that ˆl ≤ l and Rˆl = Rˆl, we will get that Rˆl = Rˆl = Rˆl+1 = . . . = Rl and, as a consequence, with probability ≥ 1 − β

1 R2 ≤ A + h2δ2D2 ˆl(ˆl + 1)2 + ud + 1 ˆl−1 R2 + 12CDε c + ud

2l

2

2

k

2

k=0

≤ A + h2δ2D2 l(l + 1)2 + ud + 1 l−1 R2 + 12CDε c + ud

2

2

k

2

k=0

ˆl(ˆl + 3) l(l + 3),

∀l = 1, . . . , N.

Therefore, we have that with probability ≥ 1 − β

l−1
Rl2 ≤ 2A + (ud + 1) Rk2 + 12CDε (2c + ud) l(l + 3) + h2δ2D2l(l + 1)2
k=0

l−2

≤ 2A (2 + ud) + (1 + ud + (1 + ud)2) Rk2

≤2(1+ud)

≤2(1+ud)2

k=0

+12CDε(2c + ud) (l(l + 3) + (1 + ud)(l − 1)(l + 2))

≤2(1+ud)l(l+3)
+h2δ2D2 (l(l + 1)2 + (1 + ud)(l − 1)l2)

≤2(1+ud)l(l+1)2
l−2
≤ 2(1 + ud) 2A + (1 + ud) Rk2 + 12CDε (2c + ud) l(l + 3) + h2δ2D2l(l + 1)2 ,
k=0

for all l = 1, . . . , N . Unrolling the recurrence we get that with probability ≥ 1 − β

Rl2 ≤ 2A + (1 + ud)R02 + 12CDε (2c + ud) l(l + 3) + h2δ2D2l(l + 1)2 (2(1 + ud))l,

for all l = 1, . . . , N . We emphasize that it is very rough estimate, but we show next that such a bound does not spoil the ﬁnal result too much. It implies that with probability ≥ 1 − β

l−1
Rk2 ≤ l
k=0

2A + (1 + ud)R02 + 12CDε (2c + ud) l(l + 3) + h2δ2D2l(l + 1)2

(2(1 + ud))l,

(166)

65

for all l = 1, . . . , N . Next we apply delicate result from [40] which is presented in Section C as Lemma C.2. We consider random variables ξk = αk+1 ηk, ak . Note that E ξk | ξ0, . . . , ξk−1 = αk+1 E ηk | η0, . . . , ηk−1 , ak = 0 and

E exp

(ξ k )2 σk2α2k+1d2Rk2

| ξ0, . . . , ξk−1

≤ E exp α2k+1 ηk 22d2Rk2 σk2α2k+1d2Rk2

| η0, . . . , ηk−1

= E exp

ηk

2 2

σk2

| η0, . . . , ηk−1 ≤ exp(1)

due to Cauchy-Schwarz inequality and assumptions of the lemma. If we denote σˆk2 = σk2α2k+1d2Rk2 and apply Lemma C.2 with

B = 2d2CDHR02 2A + (1 + ud)R02 + 48CDHR02 (2c + ud) + h2G2R02D2 (2(1 + ud))N

and

b

=

σˆ02,

we

get

that

for

all

l

=

1, . . . , N

with

probability

≥

1−

β N

l−1

l−1

either σˆ2 ≥ B or

ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

k

1

k

β

b

k=0

k=0

k=0

with some constant C1 > 0 which does not depend on B or b. Using union bound we obtain that with probability ≥ 1 − β

l−1

l−1

either σˆ2 ≥ B or

ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

k

1

k

β

b

k=0

k=0

k=0

and it holds for all l = 1, . . . , N simultaneously. Note that with probability at least 1 − β

l−1
σˆk2
k=0

=
≤
(166)
≤

l−1

2

22

2

l−1 2

Cε

2

d k=0 σkαk+1Rk ≤ d k=0 ln Nβ αk+1Rk

d2 C DH R02

l−1
(k

+ 2)R2

≤

d2 C DH R02

·

N

+1

l−1

R2

N

2

ln

N β

k=0

k

3N

N

k

k=0

d2CDHR02 l(2(1 + ud))l 2A + (1 + ud)R2 + 12CDε (2c + ud) l(l + 3)

N

0

+h2δ2D2l(l + 1)2

≤ d2CDHR02 2A + (1 + ud)R02 + 48CDHR02 (2c + ud) + h2G2R02D2 (2(1 + ud))N =B
2

for all l = 1, . . . , N simultaneously. Using union bound again we get that with probability ≥ 1 − 2β the inequality

l−1
ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

1

k

β

b

k=0

k=0

(167)

66

holds for all l = 1, . . . , N simultaneously. Note that we also proved that (165) is in the same event together with (167) and holds with
probability ≥ 1 − 2β. Putting all together in (160), we get that with probability at least 1 − 2β the inequality

1 R2 2l

(160)
≤
(167)
≤

l−1

l−1

l−1

A + hδ

αk+1Rk + u

αk+1 ηk, ak + c

α2k+1

ηk

2 2

k=0

k=0

k=0

l−1
A + hδ α

R + uC

l−1
σˆ2 ln

N

+ ln ln

B

k+1 k

1

k

β

b

k=0

k=0

l−1
+ 24cCε αk+1
k=0

holds for all l = 1, . . . , N simultaneously. For brevity, we introduce new notation: g(N ) =

ln

N β

+

l

n

l

n(

B b

)

≈

1

(neglecting

constant

factor).

Using our assumption σ2 ≤

Cε

ln

N β

k

αk+1 ln

N β

and deﬁni-

tion σˆk2 = σk2α2k+1d2Rk2 we obtain that with probability at least 1 − 2β the inequality

12

l−1

l−1

l−1

kk

2

k2

2 Rl ≤ A + hδ αk+1Rk + u αk+1 η , a + c αk+1 η 2

k=0

k=0

k=0

≤ A + hGDR0 l−1 (k + 2)R + udC

l−1

Cεg(N )

α

l−1
R2 + 24cCε α

(N + 1)2

k

1

k+1 k

k+1

k=0

k=0

k=0

≤ A + hGDR0 l−1 (k + 2)R + udC

l−1

CDεg(N )

(k + 2)R2

(N + 1)2

k

1

k

k=0

k=0

l−1

+24cCDε (k + 2)

k=0

≤ A + 24cCD HNR202 l(l +2 1) + (hNG+DR1)02 l−1 (k + 2)Rk
k=0

+udC1

C

D

H

R

2 0

g

(N

)

N2

l−1
(k + 2)Rk2
k=0

≤

A + 24cCDH

R2 +

hGDR0

l−1
(k + 2)R

R02 0 (N + 1)2 k=0 k

+ udC1R0 N

CDHg(N )

l−1
(k + 2)Rk2
k=0

(168)

holds for all l = 1, . . . , N

simultaneously.

Next we apply Lemma D.3 with A =

A R2

+

24cC DH ,

0

B = udC1 CDHg(N ), D = hGD, rk = Rk and get that with probability at least 1− 2β inequality

Rl ≤ J R0

67

holds for all l = 1, . . . , N simultaneously with

J = max 1, udC1 CDHg(N ) + hGD

2 2A + udC1 CDHg(N ) + hGD + R2 + 48cCDH .
0

It implies that with probability at least 1 − 2β the inequality

l−1

l−1

l−1

A + hδ

αk+1Rk + u

αk+1 ηk, ak + c

α2k+1

ηk

2 2

k=0

k=0

k=0

≤

A + 24cCDH

R2

+

hGDJ R20

l−1
(k

+ 2) +

udC1 R20

R20

0 (N +1)2

N

k=0

CDHg(N )

l−1
(k + 2)J
k=0

≤ A + 24cCDH + hGDJ + udC1 CDHJg(N ) N1

l(l+1) 2

R02

≤ A + 24cCDH + hGDJ + udC1 CDHJg(N ) R02

holds for all l = 1, . . . , N simultaneously.

G.2 Proof of Theorem 5.1
For the convenience we put here the extended statement of the theorem.

Theorem G.4. Assume that f is µ-strongly convex and ∇f (x∗) 2 = Mf . Let ε > 0 be a desired accuracy. Next, assume that f is Lf -Lipschitz continuous on the ball BRf (0) with

Rf = Ω˜ max

Ry

, λmax(A⊤A)Ry , Rx ,

AN λmax(A⊤A)

µ

where Ry is such that y∗ 2 ≤ Ry, y∗ is the solution of the dual problem (18), and Rx = x(A⊤y∗) 2. Assume that at iteration k of Algorithm 2 batch size is chosen according to the

formula rk ≥ max

1, σψ2 αk ˆln(N/β)

,

where

αk

=

k+˜1 ,

0<ε≤

HNL˜R2 20 ,

0≤δ

≤

GL˜ R0 (N +1)2

and

N

≥1

Cε

2L

for some numeric constant H > 0, G > 0 and Cˆ > 0. Then with probability ≥ 1 − 4β

ψ(yN ) + f (x˜N ) + 2Ry Ax˜N 2 ≤ Ry2 8 HC2 + 2 + 12CH + G(6J + 4)

AN √

2

+ Lf 96C2H + G + G2

2Ry λmax(A⊤A) 2(N + 1)

CHJg(N )

+C1

2 + 96C2H + G ,

(169)

68

where β ∈ (0, 1/4) is such that

ln
g(N ) =

N β

+

l

n

l

n(

B b

)

,

ln

N β

1+

ln

1 β

ln

N β

≤ 2, C2, C, C1 are some positive numeric constants,

B = CHR2 2A + 2R2 + 72CHR2 + 9G2L˜R02 4N ,

0

0

0

2

b = σ02α21R02 and

CHg(N ) 3G

J = max 1, C1

2 +2+

2

CHg(N ) 3G

2A

C1

2 + 2 + R2 + 24CH .

0

This means that after N = O

Mf µε

χ(A⊤A)

iterations where χ(A⊤A) = λλm+ax((AA⊤⊤AA)) , the outputs

min

x˜N and yN of Algorithm 2 satisfy the following condition

f (x˜N ) − f (x∗) ≤ f (x˜N ) + ψ(yN ) ≤ ε,

Ax˜N 2 ≤ ε Ry

(170)

with probability at least 1 − 4β. What is more, to guarantee (170) with probability at least 1 − 4β Algorithm 2 requires

O

max

σ

2 x

Mf2

χ

(A⊤

A)

ln

1

ε2

β

Mf χ(A⊤A) , µε

Mf χ(A⊤A) µε

(171)

calls of the biased stochastic oracle ∇˜ ψ(y, ξ), i.e. x˜(y, ξ).

Proof. Lemma G.2 states that

AN ψ(yN ) ≤ 12 y˜ − z0 22 − 12 y˜ − zN 22

N −1
+ αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), y˜ − y˜k+1

k=0

N −1
+ Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

k=0

N −1 Ak+1

+

2L˜

k=0

Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇˜ Ψ(y˜k+1, ξk+1) 2
2

N −1

k k+1

2 N −1 Ak+1

+δ Ak y − y˜ 2 + δ

L˜ ,

k=0

k=0

for arbitrary y˜. By deﬁnition of y˜k+1 we have

αk+1 y˜k+1 − zk = Ak yk − y˜k+1 .

(172) (173)

69

Using this, we add and subtract

N −1 k=0

αk+1

Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜∗ − y˜k+1

in (172), and obtain

the following inequality by choosing y˜ = y˜∗ — the minimizer of ψ(y):

AN ψ(yN ) ≤ 12 y˜∗ − z0 22 − 12 y˜∗ − zN 22

N −1
+ αk+1 ψ(y˜k+1) + Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜∗ − y˜k+1

k=0

N −1
+ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , ak

k=0

N −1

2

+ α2k+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1)

k=0 2

+δ N−1 αk+1 y˜k+1 − zk 2 + δ2 N−1 AkL˜+1 ,

k=0

k=0

where ak = y˜∗ − zk. From (153) we have

(174)

N −1
αk+1 ψ(y˜k+1) + Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜∗ − y˜k+1

k=0

(153) N −1

≤

αk+1 ψ(y˜k+1) + ψ(y˜∗) − ψ(y˜k+1) + δ y˜k+1 − y˜∗ 2

k=0

N −1
= αk+1 ψ(y˜∗) + δ y˜k+1 − y˜∗ 2
k=0

N −1
= AN ψ(y˜∗) + δ αk+1 y˜k+1 − y˜∗ 2
k=0

N −1
≤ AN ψ(yN ) + δ αk+1 y˜k+1 − y˜∗ 2
k=0

From this and (174) we get

12 y˜∗ − zN 22

(174)
≤

1 ∗ 0 2 2 N−1 Ak+1

2 y˜ − z 2 + δ

L˜

k=0

N −1
+δ αk+1

y˜k+1 − zk 2 + y˜k+1 − y˜∗ 2

k=0

N −1
+ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , ak

k=0

N −1

2

+ α2k+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) .

k=0 2

(175)

70

Next, we introduce the sequences {Rk}k≥0 and {Rk}k≥0 as

Rk = zk − y˜∗ 2 and Rk = max Rk−1, Rk , R0 = R0

Since in Algorithm 2 we choose z0 = 0, then R0 = Ry. One can obtain by induction that ∀k ≥ 0 we

have y˜k+1, yk, zk ∈ BR (y˜∗), where BR (y˜∗) is Euclidean ball with radius Rk at centre y˜∗. Indeed,

k

k

5 of Algorithm 2 yk+1 is a convex combination of zk+1 ∈ BR

(y˜∗) ⊆

since from lines 2 and

k+1

BRk+1(y˜∗) and yk ∈ BRk (y˜∗) ⊆ BRk+1(y˜∗), where we use the fact that a ball is a convex set, we

get yk+1 ∈ BRk+1(y˜∗). Analogously, since from lines 2 and 3 of Algorithm 2 y˜k+1 is a convex

combination of yk and zk we have y˜k+1 ∈ BRk (y˜∗). It implies that

y˜k+1 − zk 2 + y˜k+1 − y˜∗ 2 ≤ 2Rk + Rk = 3Rk.

Using new notation we can rewrite (175) as

1 R2

≤

1 R2

+

δ2

N −1

Ak+1

+

3δ

N −1
α

R

2N

20

k=0 L˜

k+1 k k=0

N −1
+ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , ak

k=0

N −1

2

+ α2k+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) ,

k=0 2

(176)

where ak 2 = y˜∗ − zk 2 ≤ Rk. Note that (176) holds for all N ≥ 1. Let us denote ηk = ∇˜ Ψ(y˜k+1, ξk+1)−Ek ∇˜ Ψ(y˜k+1, ξk+1) . Theorem 2.1 from [43] (see Lemma C.3

in the Section C) says that

  P  ηk 2 ≥

√√ 2 + 2γ



σψ2 | η0, . . . , ηk−1 ≤ exp − γ2 .

rk+1



3

Using this and Lemma 2 from [40] (see Lemma C.1 in the Section C) we get that

E exp

ηk

2 2

σk2

| η0, . . . , ηk−1 ≤ exp(1),

where σ2 ≤ Cσψ2 ≤ Cε , C and C = C · Cˆ are some positive constants. From (125) we have

k

rk+1

αk+1

ln(

N δ

)

that αk+1 ≤ αk+1 = k2+L˜2 . Moreover, ak depends only on η0, . . . , ηk−1. Putting all together in (176)

and changing the indices we get that for all l = 1, . . . , N

1 2 1 2 2 N−1 Ak+1

l−1

l−1

l−1

kk

2

k2

2 Rl ≤ 2 R0 + δ

L˜ + 3δ αk+1Rk + αk+1 η , a + αk+1 η 2.

k=0

k=0

k=0

k=0

71

Next we apply Lemma G.3 with the constants A = 1 R2 + δ2 N−1 Ak+1 , h = 3, u = 1, c = 1, D =

20

L˜

k=0

1 , d = 1, ε ≤ HL˜R20 and δ ≤ GL˜R0 , and get that with probability at least 1 − 2β the inequalities

2L˜

N2

(N +1)3

Rl ≤ J R0

(177)

and

l−1

l−1

kk

2

k2

3GJ

CHJg(N ) 2

αk+1 η , a + αk+1 η 2 ≤ 12CH + 2 + C1

2

R0

(178)

k=0

k=0

ln
hold for all l = 1, . . . , N simultaneously, where C1 is some positive constant, g(N ) =

N β

+

l

n

l

n(

B b

)

,

ln

N β

B = CHR02

2A

+

2R02

+

72C H R02

+

9G2 L˜ R20 2

4N , b = σ02α21R02 and

  J = max 1, C1

CHg(N ) + 3G +

2

2



2

CHg(N ) 3G

2A



C1

2 + 2 + R02 + 24CH .

To estimate the duality gap we need again refer to (172). Since y˜ is chosen arbitrary we can take the minimum in y˜ over the set B2Ry (0) = {y˜ : y˜ 2 ≤ 2Ry}:

AN ψ(yN ) ≤ min
y˜∈B2Ry (0)

1 y˜ − z0 2

2

2

N −1
+ αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), y˜ − y˜k+1

k=0

N −1
+ Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

k=0

N −1 Ak+1

+

2L˜

k=0

Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇˜ Ψ(y˜k+1, ξk+1) 2
2

+δ N−1 Ak yk − y˜k+1 2 + δ2 N−1 AkL˜+1

k=0

k=0

N −1

≤ 2Ry2 + min

αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), y˜ − y˜k+1

y˜∈B2Ry (0) k=0

N −1
+ Ak ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , yk − y˜k+1

k=0

N −1 Ak+1

+

2L˜

k=0

Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇˜ Ψ(y˜k+1, ξk+1) 2
2

+δ N−1 Ak yk − y˜k+1 2 + δ2 N−1 AkL˜+1 ,

k=0

k=0

(179)

72

where we also used

1 2

y˜ − zN

2 2

≥

0

and

z0

=

0.

By adding and subtracting

N −1 k=0

αk+1

Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜ − y˜k+1

under the minimum in (179) we obtain

N −1

min

αk+1

y˜∈B2Ry (0) k=0

ψ(y˜k+1) +

∇˜ Ψ(y˜k+1, ξk+1), y˜ − y˜k+1

N −1

≤ min

αk+1 ψ(y˜k+1) + Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜ − y˜k+1

y˜∈B2Ry (0) k=0

N −1

+ max

αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜

y˜∈B2Ry (0) k=0

N −1
+ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , −y˜k+1 .
k=0

Since −y˜∗ ∈ B2Ry (0) we can bound the last term in the previous inequality as follows

N −1
αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , −y˜k+1

k=0

N −1
= αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜∗ − y˜k+1
k=0

N −1
+ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , −y˜∗
k=0

N −1
≤ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜∗ − y˜k+1
k=0

N −1

+ max

αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜ .

y˜∈B2Ry (0) k=0

Putting all together in (179) and using (173) and line 2 from Algorithm 2 we get

N −1

AN ψ(yN ) ≤ 2Ry2 + min

αk+1 ψ(y˜k+1) + Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜ − y˜k+1

y˜∈B2Ry (0) k=0

N −1

+2 max

αk+1

y˜∈B2Ry (0) k=0

∇˜ Ψ(y˜k+1, ξk+1) − Ek

∇˜ Ψ(y˜k+1, ξk+1) , y˜

N −1
+ αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1) , ak

k=0

N −1

2

+ α2k+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1)

k=0 2

N −1

k+1 k

2 N −1 Ak+1

+δ αk+1 y˜ − z 2 + δ

L˜ ,

k=0

k=0

(180)

where ak = y˜∗ − zk. From (177) and (178) we have that with probability at least 1 − 2β the

73

following inequality holds:

N −1

AN ψ(yN ) ≤ min

αk+1 ψ(y˜k+1) + Ek ∇˜ Ψ(y˜k+1, ξk+1) , y˜ − y˜k+1

y˜∈B2Ry (0) k=0

N −1

+2 max

αk+1

y˜∈B2Ry (0) k=0

∇˜ Ψ(y˜k+1, ξk+1) − Ek

∇˜ Ψ(y˜k+1, ξk+1) , y˜

+2Ry2 + 12CH + 5GJ + G2 + C1 CHJ g(N ) R02,

2 2(N + 1)

2

(181)

where

we

used

that

Ak+1

≤

(k+2)2 ˜

due

to

αk+1 ≤

k+2 ˜

and

2L

2L

N −1
δα

y˜k+1 − zk

N −1
≤ 2δJR α

≤ 2GL˜R02J

1

N −1
(k + 2) ≤ GJR2,

k=0 k+1 2 0 k=0 k+1 (N + 1)2 2L˜ k=0 0

δ2 N−1 Ak+1 ≤ G2L˜2R02 N−1 (k + 2)2 ≤ G2R02

k=0 L˜

(N + 1)4 k=0 2L˜2

2(N + 1)

By the deﬁnition of the norm we get

N −1

max

αk+1

y˜∈B2Ry (0) k=0

∇˜ Ψ(y˜k+1, ξk+1) − Ek

≤ 2Ry

N −1
αk+1
k=0

∇˜ Ψ(y˜k+1, ξk+1) , y˜ ∇˜ Ψ(y˜k+1, ξk+1) − Ek

∇˜ Ψ(y˜k+1, ξk+1)

Next we apply Lemma C.3 to the right-hand side of the previous inequality and get

. (182)
2

N −1

P

αk+1 ∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1)

k=0

2

√√ ≥ 2 + 2γ

N −1
α2

σψ2

k+1 rk+1

k=0

≤ exp

−

γ2 3

.

Since N 2 ≤ HL˜εR20 and rk = Ω max 1, σψ2 αk lεn(N/β) one can choose such C2 > 0 that σrψk2 ≤

C2 ε

αk ln

N β

≤ HL˜C2R20 . Moreover, let us choose γ such that exp − γ2

αkN 2 ln

N β

3

From this we get that with probability at least 1 − β

= β =⇒ γ =

3

ln

1 β

.

N −1
αk+1
k=0

∇˜ Ψ(y˜k+1, ξk+1) − Ek ∇˜ Ψ(y˜k+1, ξk+1)

√ ≤ 2 1+

ln

1 β

Ry

H L˜ C2

ln

N β

2 N −1 αk+1
N2 k=0

(125) √ ≤ 2 2Ry

H L˜ C2

N −1

k+2

√ = 2Ry HC2

2L˜N 2

k=0

N(NN2+3) ≤ 4Ry√HC2.

(183)

74

In the above inequality we used the fact that Ry = R0. Putting all together and using union bound we get that with probability at least 1 − 3β

AN ψ(yN )

(181)+(182)+(183)

≤

miny˜∈B2Ry (0)

N −1 k=0

αk+1

ψ(y˜k+1) +

Ek

∇˜ Ψ(y˜k+1, ξk+1)

+ 8√HC2 + 2 + 12CH + 5G2J + 2(NG+21)3

, y˜ − y˜k+1

+C1

CHJg(N ) 2

Ry2

≤ miny˜∈B2Ry (0) kN=−01 αk+1 ψ(y˜k+1) + ∇ψ(y˜k+1), y˜ − y˜k+1

N −1
˜ Ψ(y˜k+1, ξk+1)

+ maxy˜∈B2Ry (0)

αk+1 Ek ∇

k=0

+ 8√HC2 + 2 + 12CH + 5G2J + 2(NG+2 1) + C1

− ∇ψ(y˜k+1), y˜ − y˜k+1

CHJg(N ) 2

Ry2

(184)

(177)
First of all, we notice that in the same probabilistic event we have y˜k+1 − y˜∗ 2 ≤ Rk ≤ JR0. Therefore, in the same probabilistic event we get that y˜k+1 − y˜ 2 ≤ y˜k+1 − y˜∗ 2 + y˜∗ − y˜ 2 ≤ (J + 4)Ry for all y˜ ∈ B2Ry (0), where we used R0 = Ry. It implies that in the same probabilistic event we have

N −1

max

αk+1 Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇ψ(y˜k+1), y˜ − y˜k+1

y˜∈B2Ry (0) k=0

N −1

≤ max

αk+1 Ek ∇˜ Ψ(y˜k+1, ξk+1) − ∇ψ(y˜k+1) · y˜ − y˜k+1 2

y˜∈B2Ry (0) k=0

2

(35) N −1

≤

αk+1δ(J + 4)Ry

N −1

≤

k+2

GL˜ R0

(J + 4)Ry

≤

G(J+4)R2y .

k=0 k=0 2L˜ (N +1)2 2

Secondly, using the same trick as in the proof of Theorem 1 from [11] we get that for arbitrary point y

ψ(y) − ∇ψ(y), y (20)=+(31) y, Ax(A⊤y) − f x(A⊤y) − Ax(A⊤y), y = −f (x(A⊤y)).

Using these relations in (184) we obtain that with probability at least 1 − 3β

N −1

N −1

AN ψ(yN ) ≤ − αk+1f (x(A⊤y˜k+1)) + min

αk+1 ∇ψ(y˜k+1), y˜

k=0

y˜∈B2Ry (0) k=0

G(6J + 4)

G2

+ 8 HC2 + 2 + 12CH + 2 + 2(N + 1) +

+C1 CHJ2g(N ) Ry2.

(185)

75

To bound the ﬁrst term in (185) we apply convexity of f and introduce the virtual primal iterate
N −1
xˆN = A1N k=0 αk+1x(A⊤y˜k+1):

N −1

⊤ k+1

N −1 αk+1

⊤ k+1

N

− αk+1f (x(A y˜ )) = −AN

AN f (x(A y˜ )) ≤ −AN f (xˆ ).

k=0

k=0

In order to bound the second term in the right-hand side of the previous inequality we use the deﬁnition of the norm we have

N −1

min

αk+1 ∇ψ(y˜k+1), y˜

y˜∈B2Ry (0) k=0

N −1

= min
y˜∈B2Ry (0)

αk+1∇ψ(y˜k+1), y˜
k=0

N −1

= −2Ry

αk+1∇ψ(y˜k+1)

k=0

2

= −2RyAN AxˆN 2,

where we used equality (31). Putting all together we obtain that with probability at least 1 − 3β

ψ(yN ) + f (xˆN ) + 2Ry AxˆN 2 ≤ Ry2 8 HC2 + 2 + 12CH + G(6J + 4)

AN

2

G2

CHJg(N )

+ 2(N + 1) + C1

2

.

(186)

Lemma C.3 implies that for all γ > 0

N −1

P

αk+1 x˜(A⊤y˜k+1, ξk+1) − E x˜(A⊤y˜k+1, ξk+1) | y˜k+1

k=0

2

√√ ≥ ( 2 + 2γ)

N −1 α2k+1σx2 k=0 rk+1

≤ exp

−

γ2 3

.

Using this inequality with γ = 3 ln β1 and rk ≥ σψ2 αCk2lεn Nβ we get that with probability at least

76

1−β

x˜N − xˆN

1 N−1

=

α

x˜(A⊤y˜k+1, ξk+1) − x(A⊤y˜k+1)

2

AN

k+1

k=0

2

1 N−1

⊤ k+1 k+1

⊤ k+1 k+1 k+1

≤ AN

αk+1 x˜(A y˜ , ξ ) − E x˜(A y˜ , ξ ) | y˜

k=0

2

+1 AN

N −1
αk+1

E x˜(A⊤y˜k+1, ξk+1) | y˜k+1 − x(A⊤y˜k+1)

k=0

2

√

≤ 2 1 + 3 ln 1

AN

β

N−1 α2k+1σx2 k=0 rk2+1

1 N−1

⊤ k+1 k+1 k+1

⊤ k+1

+ AN

αk+1 E x˜(A y˜ , ξ ) | y˜

− x(A y˜ )
2

k=0

(3≤3) 2 AN

6 ln 1 β

1

ln

N β

N−1 C2αk+1ε

1 N−1

λmax(A⊤A) + AN

αk+1δy

k=0

k=0

2

6C2

N−1 (k + 2)HL˜Ry2

≤ AN λmax(A⊤A)

2L˜N 2

k=0

1 N−1 k + 2

GL˜ Ry

+ AN

2L˜

· (N + 1)2

λmax(A⊤A)

k=0

≤ 2Ry AN

6C2H +

G

.

λmax(A⊤A) 4 λmax(A⊤A)

(187)

It implies that with probability at least 1 − β

Ax˜N − AxˆN 2 ≤ A 2 · x˜N − xˆN 2

(187)
≤

λmax(A⊤A) 2Ry AN

6C2H + λmax(A⊤A) 4

= Ry 2AN

96C2H + G

and due to triangle inequality with probability ≥ 1 − β

G λmax(A⊤A)

(188)

2Ry AxˆN 2 ≥ 2Ry Ax˜N 2 − 2RyAN AxˆN − Ax˜N 2

(188)

Ry2

√ 96C2H + G

≥ 2Ry Ax˜N 2 −

.

AN

The next step is in applying Lipschitz continuity of f on BRf (0). Recall that

(189)

x(y) d=ef argmax { y, x − f (x)}
x∈Rn

77

and due to Demyanov-Danskin theorem x(y) = ∇ϕ(y). Together with Lϕ-smoothness of ϕ it implies that
x(A⊤y˜k+1) 2 = ∇ϕ(A⊤y˜k+1) 2 ≤ ∇ϕ(A⊤y˜k+1) − ∇ϕ(A⊤y∗) 2 + ∇ϕ(A⊤y∗) 2 ≤ Lϕ A⊤y˜k+1 − A⊤y∗ 2 + x(A⊤y∗) 2 ≤ λmax(A⊤A) y˜k+1 − y∗ 2 + Rx. µ
From this and (177) we get that with probability at least 1 − 2β the inequality

(177)
x(A⊤y˜k+1) 2 ≤

λmax(A⊤A)J + Rx Ry

µ

Ry

(190)

holds for all k = 0, 1, 2, . . . , N − 1 simultaneously since y˜k+1 ∈ BRk (y∗) ⊆ BRk+1(y∗). Using the convexity of the norm we get that with probability at least 1 − 2β

xˆN

1 N−1

≤

α

(190)
x(A⊤y˜k+1) ≤

2 AN

k+1

2

k=0

λmax(A⊤A)J + Rx Ry.

µ

Ry

(191)

We notice that the last inequality lies in the same probability event when (177) holds. Consider the probability event E = {inequalities (186) − (191) hold simultaneously}. Using
union bound we get that P{E} ≥ 1 − 4β. Combining (187) and (191) we get that inequality

x˜N 2 ≤ ≤

x˜N − xˆN 2 + xˆN 2 √ 96C2H + G +
2AN λmax(A⊤A)

λmax(A⊤A)J + Rx Ry

µ

Ry

(192)

lies in the event E. From this we can obtain a lower bound for Rf :

Rf ≥

√ 96C2H + G +
2AN λmax(A⊤A)

λmax(A⊤A)J + Rx Ry.

µ

Ry

Then we get that the fact that points x˜N and xˆN lie in BRf (0) is a consequence of E. Therefore, we can apply Lipschitz-continuity of f for the points x˜N and xˆN and get that inequalities

√

|f (xˆN ) − f (x˜N )| ≤ Lf

xˆN − x˜N

(187) Lf Ry 2≤

96C2H + G

2AN λmax(A⊤A)

(193)

and

√

f (xˆN ) = f (x˜N ) +

f (xˆN ) − f (x˜N )

(193)
≥

f (x˜N )

−

Lf Ry

96C2H + G

2AN λmax(A⊤A)

(194)

78

also lie in the event E. It remains to use inequalities (189) and (194) to bound ﬁrst and second terms in the right hand side of inequality (186) and obtain that with probability at least 1 − 4β

ψ(yN ) + f (x˜N ) + 2Ry Ax˜N 2 ≤ Ry2 8 HC2 + 2 + 12CH + G(6J + 4)

AN

√

2

+ Lf 96C2H + G + G2

2Ry λmax(A⊤A) 2(N + 1)

CHJg(N )

+C1

2 + 96C2H + G .

(195)

Using that AN grows as Ω

N2 L˜

[63],

L˜

≤

2λmax (A⊤ A) µ

and

Ry

≤

∇f (x∗)

2 2

λ+ (A⊤A)

(see

Section

V-D

from

min

[19] for the details), we obtain that the choice of N in the theorem statement guarantees that the

r.h.s. of the last inequality is no greater than ε. By weak duality −f (x∗) ≤ ψ(y∗) and we have

with probability at least 1 − 4β

f (x˜N ) − f (x∗) ≤ f (x˜N ) + ψ(y∗) ≤ f (x˜N ) + ψ(yN ) ≤ ε.

(196)

Since y∗ is the solution of the dual problem, we have, for any x, f (x∗) ≤ f (x) − y∗, Ax . Then

using assumption y∗ 2 ≤ Ry, Cauchy-Schawrz inequality y, Ax ≥ − y∗ 2 · Ax 2 ≥ −Ry Ax 2 and choosing x = x˜N , we get

f (x˜N ) ≥ f (x∗) − Ry Ax˜N 2

(197)

Using this and weak duality −f (x∗) ≤ ψ(y∗), we obtain

ψ(yN ) + f (x˜N ) ≥ ψ(y∗) + f (x˜N ) ≥ −f (x∗) + f (x˜N ) ≥ −Ry Ax˜N 2,

which implies that inequality

AxN

(195)+(196)
2≤

ε

Ry

(198)

holds together with (196) with probability at least 1 − 4β. The total number of stochastic gradient

N

N

oracle calls is rk, which gives the bound in the problem statement since αk+1 = AN .

k=1

k=1

H Missing Proofs from Section 5.2

H.1 Proof of Theorem 5.6

For simplicity we analyse only the ﬁrst restart since the analysis of the later restarts is the same. We apply Theorem 5.3 with N = N¯ such that

CL2ψ ln4 N¯ 1 µ2 N¯ 4 ≤ 32
ψ

and batch-size

r1 = max

64Cσψ2 ln6 N¯ 1, N¯ ∇Ψ(y0, ξ0, rˆ1) 2
2

79

together with simple inequality ∇ψ(y0) 2 ≥ µψ y0 − y∗ 2 and get for all p = 1, . . . , p1

E ∇ψ(y¯1,p) 2 | y0, r1, rˆ1

≤

∇ψ(y0)

2 2

+

∇Ψ(y0, ξ0, rˆ1)

2 2

2

32

64

(120)
≤

∇ψ(y0)

2 2

+

∇Ψ(y0, ξ0, rˆ1) − ∇ψ(y0)

2
2.

16

32

(199)

By Markov’s inequality we have for each p = 1, . . . , p1 that for ﬁxed ∇Ψ(y0, ξ0, rˆ1) with probability

at most 1/2

∇ψ(y¯1,p) 2 ≥

∇ψ(y0)

2 2

+

∇Ψ(y0, ξ0, rˆ1) − ∇ψ(y0)

2
2.

2

8

16

Then, with probability at least 1 − 1/2p1 ≥ 1 − β/l

∇ψ(y¯1,pˆ1 ) 2 ≤

∇ψ(y0)

2 2

+

∇Ψ(y0, ξ0, rˆ1) − ∇ψ(y0)

2
2,

2

8

16

(200)

where pˆ1 is such that

∇ψ(y¯1,pˆ1 )

2 2

=

minp=1,...,p1

∇ψ(y¯1,p) 22. From Lemma C.3 we have for all

p = 1, . . . , p1





√

P  ∇Ψ(y¯1,p, ξ1,p, r¯1) − ∇ψ(y¯1,p) 2 ≥ 2 + 2γ



σψ2 | y¯1,p ≤ exp − γ2 .

r¯1



3

  128σ2
ψ
Since r¯1 = max 1,

1+ 3 ln lp1
β
ε2

2 R2y 
 we can take γ =

3

ln

lp1 β

in

the

previous

inequality

and get that for all p = 1, . . . , p1 and ﬁxed points y¯1,p with probability at least 1 − β/(lp1)

∇Ψ(y¯1,p, ξ1,p, r¯1) − ∇ψ(y¯1,p) 2 ≤ ε2 . 2 64Ry2

Using union bound we get that with probability at least 1 − β/l inequality

∇Ψ(y¯1,p, ξ1,p, r¯1) − ∇ψ(y¯1,p) 2 ≤ ε2 . 2 64Ry2

(201)

holds for all p = 1, . . . , p1 simultaneously with ﬁxed points y¯1,p. Using union bound again we get that with probability at least 1 − 2β/l for ﬁxed ∇Ψ(y0, ξ0, rˆ1)

∇ψ (y¯1,p(1) )

2 2

(120)
≤
(201)
≤
(120)
≤
(200)+(201)
≤

2 ∇Ψ(y¯1,p(1), ξ1,p(1), r¯1) 2
2

+2 ∇Ψ(y¯1,p(1), ξ1,p(1), r¯1) − ∇ψ(y¯1,p(1)) 2
2

2

∇Ψ(y¯1,pˆ1, ξ1,pˆ1, r¯1)

2
+

ε2

2 32Ry2

4

∇ψ(y¯1,pˆ1 )

2 2

+

4

∇Ψ(y¯1,pˆ1, ξ1,pˆ1, r¯1) − ∇ψ(y¯1,pˆ1 )

∇ψ(y0)

2 2

+

∇Ψ(y0, ξ0, rˆ1) − ∇ψ(y0)

2 2

+

ε2 .

2

4

8Ry2

2 ε2 2 + 32R2
y
(202)

80

Using Lemma C.3 with γ =

3

ln

l β

and

rˆ1

=

max

4σψ2 1+ 3 ln βl 2R2y

1,

ε2

we get that with prob-

ability at least 1 − β/l

∇Ψ(y0, ξ0, rˆ1) − ∇ψ(y0) 22 ≤ 2εR2y2 .

(203)

Applying union bound again we get that with probability at least 1 − 3β/l the following inequality

holds:

(202)+(203)

∇ψ (y¯1,p(1) )

2 2

≤

∇ψ(y0)

2 2

+

ε2 .

2

4Ry2

Similarly, for all k = 1, . . . , l with probability at least 1 − 3β/l

∇ψ(y¯k,p(k))

2 2

≤

∇ψ(y¯k−1,p(k−1))

2 2

+

ε2 .

2

4Ry2

Using union bound we get that with probability at least 1 − 3β the inequality

∇ψ(y¯k,p(k))

2 2

≤

∇ψ(y¯k−1,p(k−1))

2 2

+

ε2

2

4Ry2

(204)

holds for all k = 1, . . . , l simultaneously. Finally, unrolling the recurrence an using our choice of l = max {1, log2 (2R2y ∇ψ(y0) 22/ε2)} we obtain that with probability at least 1 − 3β

∇ψ(y¯l,p(l))

2 2

(204)
≤ ≤ =

∇ψ(y0)

2 2

+

ε2

l−1
2−k

2l

4Ry2 k=0

ε2 + ε2

∞
2−k

2Ry2 4Ry2 k=0

ε2

ε2

ε2

2R2 + 4R2 · 2 = R2 ,

y

y

y

l
which concludes the proof. To get (51) we need to estimate (rˆk + N¯ pkrk + pkr¯k) using our choice
k=1
of parameters stated in (49).

H.2 Proof of Corollary 5.8
Theorem 5.6, Corollary 5.7 and inequality ε ≤ µψRy2 imply that with probability at least 1 − 3β

∇ψ(y¯l,p(l)) 2 ≤ ε , Ry

(52)
y¯l,p(l) 2 ≤ y¯l,p(l) − y∗ 2 + y∗ 2 ≤ 2Ry.

(205)

Applying Theorem 5.2 we get that with probability 1 − 3β we also have

f (xˆl) − f (x∗) ≤ 2ε,

Axˆl 2 ≤ ε , Ry

(206)

81

where xˆl d=ef x(A⊤y¯l,p(l)). Next, we show that points xˆl,p = x(A⊤y¯l,p) and xl,p d=ef x(A⊤y¯l,p, ξl,, r¯l)

are close to each other with high probability for all p = 1, . . . , pl and both lie in BRf (0) with high

probability. Lemma C.3 states that





 P xˆl,p − xl,p

√ ≥( 2+

2γ) σx2 | y¯l,p(l) ≤ exp − γ2 .



2

r¯l



3

Taking γ =



 128σ2

3 ln

pl β

and

using

r¯l

= max 1,

ψ

1+ 3 ln lβpl ε2

 R2y 
 we get that for all p = 1, . . . , pl

with probability at least 1 − β/pl

xˆl,p − xl,p 2 ≤ ε · 8Ry

σx2 = σψ2 8Ry

ε, λmax(A⊤A)

where we use σψ = the inequality

λmax(A⊤A)σx. Using union bound we get that with probability at least 1 − β

xˆl,p − xl,p 2 ≤ 8Ry

ε, λmax(A⊤A)

holds for all p = 1, . . . , p(l) simultaneously and, in particular, we get that with probability at least

1−β

xˆl − xl 2 ≤ 8Ry

ε. λmax(A⊤A)

(207)

It implies that with probability at least 1 − β

Axˆl − Axl 2

≤
(207)
≤

A 2 · xˆl − xl 2 λmax(A⊤A) 8Ry

ε = ε, λmax(A⊤A) 8Ry

(208)

and due to triangle inequality with probability ≥ 1 − β

(208)
Axˆl 2 ≥ Axl 2 − Axˆl − Axl 2 ≥ Axl 2 −

ε.

8Ry

(209)

Applying Demyanov-Danskin’s theorem, Lϕ-smoothness of ϕ with Lϕ = 1/µ and ε ≤ µψRy2 we obtain that with probability at least 1 − β

xˆl 2 = ∇ϕ(A⊤y¯l,p(l)) 2 ≤ ∇ϕ(A⊤y¯l,p(l)) − ∇ϕ(A⊤y∗) 2 + ∇ϕ(A⊤y∗) 2

≤ Lϕ A⊤y¯l,p(l) − A⊤y∗ 2 + x(A⊤y∗) 2 ≤

λmax(A⊤A) y¯l,p(l) − y∗ 2 + Rx µ

(52)
≤

λmax(A⊤A)ε + Rx ≤

λmax(A⊤A) + Rx Ry

µµψ Ry

µ

Ry

(210)

and also

xl 2

≤
(207)+(210)
≤

xl − xˆl 2 + xˆl 2

µψ

+

8 λmax(A⊤A)

λmax(A⊤A) + Rx Ry.

µ

Ry

(211)

82

That is, we proved that with probability at least 1 − β points xˆl and xl lie in the ball BRf (0). In this ball function f is Lf -Lipschitz continuous, therefore, with probability at least 1 − β

f (xˆl) = f (xl) + f (xˆl) − f (xl) ≥ f (xl) − |f (xˆl) − f (xl)|

(207)
≥ f (xl) − Lf xˆl − xl 2 ≥ f (xl) −

εLf

.

8Ry λmax(A⊤A)

(212)

Combining inequalities (206), (209) and (212) and using union bound we get that with probability at least 1 − 4β

f (xl) − f (x∗) ≤ 2 +

Lf

ε,

8Ry λmax(A⊤A)

Axl ≤ 9ε . 8Ry

Finally, in order to get the bound for the total number of oracle calls from (54) we use (51) together with σψ2 = σx2λmax(A⊤A) and (121).

I Missing Proofs from Section 5.3

I.1 Proof of Lemma 5.9
We prove (58) by induction. For k = 0 this inequality is trivial since Ak = L1 , y˜1 = y0 and z0 = y˜0. Next, assume that (58) holds for some k ≥ 0 and prove it for k + 1. By deﬁnition of gk+1(z) we have

g˜k+1(zk+1) = g˜k(zk+1)

(213)

+αk+1 ψ(y˜k+1) + ∇˜ Ψ(y˜k+1, ξk+1), zk+1 − y˜k+1 + µ2ψ zk+1 − y˜k+1 22 .

Since g˜k(z) is (1 + Akµψ)-strongly convex we can estimate the ﬁrst term in the r.h.s. of the previous inequality as follows:

g˜k(zk+1) ≥ g˜k(z) + 1 + A2 kµψ zk+1 − zk 22

(≥58) Akψ(yk) + 1 + A2 kµψ zk+1 − zk 22

+ k−1 Alµψ l=0 2

yl − y˜l+1

2 k αl 2 − 2µψ
l=0

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l) 2
2

Applying µψ-strong convexity of ψ and the relation

yk+1 = Akyk + αk+1zk+1 = Akyk + αk+1zk + αk+1 zk+1 − zk

Ak+1

Ak+1

Ak+1

= y˜k+1 + αk+1 zk+1 − zk

Ak+1

83

to the previous inequality we get

g˜k(zk+1) ≥ Akψ(y˜k+1) + ∇ψ(y˜k+1), Ak(yk − y˜k+1) + Ak2µψ yk − y˜k+1 22

A2k+1(1 + Akµψ) k+1 k+1 2 k−1 Alµψ l l+1 2

+

2α2

y − y˜ 2 +

2 y − y˜ 2

k+1

l=0

− k αl l=0 2µψ

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l)

2
.

2

(214)

Next, we use (214) in (213) together with relations Ak+1 = Ak + αk+1, Ak+1(1 + Akµψ) = α2k+1Lψ and Ak(yk − y˜k+1) + αk+1(zk+1 − y˜k+1) = Ak+1(yk+1 − y˜k+1):

g˜k+1(zk+1) ≥ Ak+1ψ(y˜k+1) + ∇ψ(y˜k+1), Ak(yk − y˜k+1) + αk+1(zk+1 − y˜k+1)

A2k+1(1 + Akµψ) k+1 k+1 2 k Alµψ l l+1 2

+

2α2

y − y˜ 2 +

2 y − y˜ 2

k+1

l=0

− k αl l=0 2µψ

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l) 2
2

+αk+1 ∇˜ Ψ(y˜l+1, ξl+1) − ∇ψ(y˜l+1), zk+1 − y˜k+1

+ αk+1µψ zk+1 − y˜k+1 2

2

2

= Ak+1 ψ(y˜k+1) + ∇ψ(y˜k+1), yk+1 − y˜k+1 + L2ψ yk+1 − y˜k+1 22

+ k Alµψ l=0 2

yl − y˜l+1

2 k αl 2 − 2µψ
l=0

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l) 2
2

+αk+1 ∇˜ Ψ(y˜l+1, ξl+1) − ∇ψ(y˜l+1), zk+1 − y˜k+1

+ αk+1µψ zk+1 − y˜k+1 2.

2

2

From Lψ-smoothness of ψ we have

ψ(y˜k+1) + ∇ψ(y˜k+1), yk+1 − y˜k+1 + Lψ yk+1 − y˜k+1 2 ≥ ψ(yk+1).

2

2

Next, Fenchel-Young inequality (see inequality (119)) implies that

∇˜ Ψ(y˜l+1, ξl+1) − ∇ψ(y˜l+1), zk+1 − y˜k+1

≥ − 1 ∇˜ Ψ(y˜l+1, ξl+1) − ∇ψ(y˜l+1) 2 − µψ zk+1 − y˜k+1 2.

2µψ

22

2

Putting all together and rearranging the terms we get

g˜ (zk+1) ≥ A ψ(yk+1) + k Alµψ

k+1

k+1

2

l=0

yl − y˜l+1

k+1
2−

αl

2 l=0 2µψ

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l)

2
.

2

84

I.2 Proof of Lemma 5.10
The idea behind the proof of this lemma is exactly the same as for Lemma G.3. We start with applying Cauchy-Schwarz inequality to the second and the third terms, i.e.

hδ(Rk + Rk) ≤ Dh2δ2 + Rk2 + Dh2δ2 + Rk2 = 2Dh2δ2 + Rk2 + Rk2 ,

4D

4D

4D

u ηk, ak + a˜k ≤ u ηk 2 · ak 2 + u ηk 2 · a˜k 2 ≤ u ηk 2Rk + u ηk 2Rk

≤ u2D ηk 2 + Rk2 + u2D ηk 2 + Rk2 ≤ 2u2D ηk 2 + Rk2 + Rk2 ,

2 4D

2 4D

2

4D

in the right-hand side of (59):

l−1

l−1

A R2 + A R2 ≤ A + 2Dh2δ2 α

1 l−1

+

α

(R2 + R2)

ll

kk

k+1 2D

k+1 k

k

k=0

k=0

k=0

Al

l−1

+ c + 2Du2

αk+1 ηk 22.

k=0

Using

Lemma

C.3

we

get

that

with

probability

at

least

1

−

β N

ηk 2

≤

√ 2

1+

√ = 2Cε.

3 ln N β

√ σk ≤ 2 1 +

3 ln N β

√ Cε

N 1+

3

ln

N β

(215) (216)

Using union bound and αk+1 ≤ DAk we get that with probability ≥ 1 − β inequalities

l−1
AlRl2 + AkRk2

k=0

A R2 +

1

l−1
A

R2

ll 2

kk

k=0

22

1 l−1

2

2

2

≤ A + 2Dh δ Al + 2 Ak(Rk + Rk) + 2C c + 2Du Alε,

k=0

≤

A + 2Dh2δ2A

+

1

l−1
A

R2 + 2C

c + 2Du2

Aε

l2

kk

l

k=0

(217)

hold for all l = 1, . . . , N simultaneously. Therefore, with probability ≥ 1 − β the inequality

2

22

2

1 l−1

2

AlRl ≤ A + 2Dh δ Al + 2C c + 2Du Alε + 2 AkRk

k=0

≤ 3 A + 2Dh2δ2 A + 1 A

+2C c + 2Du2 ε A + 1 A

+3

·

1

l−2
A

R2

2

l 2 l−1

l 2 l−1 2 2

kk

k=0

≤

3 2

A

l

≤

3 2

A

l

3

22

2

1 l−2

2

≤ 2 A + 2Dh δ Al + 2C c + 2Du Alε + 2 AkRk ,

k=0

85

holds for all l = 1, . . . , N simultaneously. Unrolling the recurrence we get that with probability ≥1−β

AlRl2 ≤ 32 l A + 2Dh2δ2Al + 2C c + 2Du2 Alε ,

for all l = 1, . . . , N . We emphasize that it is very rough estimate, but as for the convex case we show

next that such a bound does not spoil the ﬁnal result too much. It implies that with probability

≥1−β

l−1
AkRk2 ≤ l
k=0

32 l A + 2Dh2δ2Al + 2C c + 2Du2 Alε ,

(218)

for all l = 1, . . . , N simultaneously. Moreover, since (217) holds we have in the same probability event that inequalities

l−1 2

3l

AkRk ≤ l 2 + 2

k=0

A + 2Dh2δ2Al + 2C c + 2Du2 Alε

(219)

hold with probability ≥ 1− β for all l = 1, . . . , N simultaneously with (218). Next we apply delicate
result from [40] which is presented in Section C as Lemma C.2. We consider random variables ξk = αk+1 ηk, ak + a˜k . Note that E ξk | ξ0, . . . , ξk−1 = αk+1 E ηk | η0, . . . , ηk−1 , ak = 0 and

E exp

(ξk)2 2σk2α2k+1(Rk2 + Rk2)

| ξ0, . . . , ξk−1

≤ E exp

α2k+1

ηk

2 2

ak + a˜k

2 2

2σk2α2k+1(Rk2 + Rk2)

| η0, . . . , ηk−1

= E exp

ηk

2 2

σk2

| η0, . . . , ηk−1 ≤ exp(1)

due to Cauchy-Schwarz inequality and assumptions of the lemma. If we denote σˆk2 = 2σk2α2k+1(Rk2 + Rk2) and apply Lemma C.2 with

B = 8HCDR2

N

3N +1

0

2

A + 2Dh2G2R02 + 2C c + 2Du2 HR02

and

b

=

σˆ02,

we

get

that

for

all

l

=

1, . . . , N

with

probability

≥

1−

β N

l−1

l−1

either σˆ2 ≥ B or

ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

k

1

k

β

b

k=0

k=0

k=0

with some constant C1 > 0 which does not depend on B or b. Using union bound we obtain that with probability ≥ 1 − β

l−1

l−1

either σˆ2 ≥ B or

ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

k

1

k

β

b

k=0

k=0

k=0

86

and it holds for all l = 1, . . . , N simultaneously. Note that αk+1 ≤ Ak+1, ε ≤ HARN20 , δ ≤ NG√RA0N and with probability at least 1 − β

l−1
σˆk2
k=0

=
≤
(218)+(219)
≤

l−1

2 σk2α2k+1(Rk2 + Rk2) ≤

k=0

N2

2C ε

1+

3

ln

N β

l−1
2 Ak+1 · DAk(Rk2 + Rk2)
k=0

l−1
2εCDAN Ak(Rk2 + Rk2)

k=0

3l 4εCDAN l 2 + 1

A + 2Dh2δ2Al + 2C c + 2Du2 Alε

≤

4H C DR2

N

3N +1

A + 2Dh2G2R2 + 2C c + 2Du2 HR2

0

2

0

0

=B 2

for all l = 1, . . . , N simultaneously. Using union bound again we get that with probability ≥ 1 − 2β

the inequality

l−1
ξk ≤ C

l−1
σˆ2 ln

N

+ ln ln

B

1

k

β

b

k=0

k=0

(220)

holds for all l = 1, . . . , N simultaneously. Note that we also proved that (216) is in the same event together with (220) and holds with
probability ≥ 1 − 2β. Putting all together in (59), we get that with probability at least 1 − 2β the inequality

l−1
AlRl2 + AkRk2
k=0

(59)
≤
(216)+(220)
≤

l−1

A + hδ αk+1(Rk

k=0

l−1

+c

αk+1

ηk

2 2

k=0

l−1

A + hδ αk+1(Rk

k=0

l−1
+ Rk) + u αk+1
k=0
+ Rk)

ηk, ak

+ a˜k

+uC

l−1
σˆ2 ln

N

+ ln ln

B

1

k

β

b

k=0

+ 2cCεAl

holds for all l = 1, . . . , N simultaneously. For brevity, we introduce new notation: g(N ) =

ln

N β

+

l

n

l

n(

B b

)

≈ 1 (neglecting constant factor).

Using our assumptions σ2 ≤

Cε

,

2

k

2

1+

3 ln

N β

N 2 1+

3 ln

N β

ε ≤ HARN20 , δ ≤ NG√RA0N and deﬁnition σˆk2 = 2σk2α2k+1(Rk2 + Rk2) we obtain that with probability at

87

least 1 − 2β the inequality

l−1

l−1

l−1

AlRl2 + AkRk2 ≤ A + hδ αk+1(Rk + Rk) + u αk+1 ηk, ak + a˜k

k=0

k=0

k=0

l−1

+c

αk+1

ηk

2 2

k=0

hGR0 l−1

≤ A+ √

αk+1(Rk + Rk)

N AN k=0

+ uC1R0

2HCg(N ) √

N AN

l−1
α2k+1(Rk2 + Rk2) + 2cHCR02
k=0

≤ A + 2cHC R2

R02

0

hG + uC1 2HCg(N ) R0 l−1

+

√

N AN

αk+1(Rk + Rk)
k=0

(221)

holds for all l = 1, . . . , N simultaneously, where in the last row we applied well-known inequality:

m i=1

a2i

≤

m i=1

ai

for

ai

≥

0,

i

=

1, . . . , m.

Next we use Lemma D.5 with A =

A R2

+

2cH C ,

0

B = hG + uC1 2HCg(N ), rk = Rk, r˜k = Rk and get that with probability at least 1 − 2β

inequalities

Rl ≤ √J R0 , Al

Rl−1 ≤

J R0 Al−1

hold for all l = 1, . . . , N simultaneously with

  J = max 

3B1D + A0,



9B12D2

+

4A R2

+

8cH C



0
2

 , B1 = hG + uC1

2HCg(N ).

It implies that with probability at least 1 − 2β the inequality

l−1

l−1

l−1

A + hδ

αk+1(Rk + Rk) + u

αk+1 ηk, ak + a˜k + c

αk+1

ηk

2 2

k=0

k=0

k=0

√

≤

A + 2cHC

R2 + 2J

hG+uC1 2HCg(N ) √

R20

l−1 α√k+1

R20

0 N AN k=0 Ak

√

≤ A + 2cH C + 2JD hG+NuC√1AN2HCg(N) kl−=10 √Ak R02

√ ≤ A + 2cH C + 2JD hG+NuC√1AN2HCg(N) l Al−1 R02

≤ A + 2cHC + 2JD hG + uC1 2HCg(N ) R02 holds for all l = 1, . . . , N simultaneously.

88

I.3 Proof of Theorem 5.11

From Lemma 5.9 we have

A ψ(yk) ≤ g˜ (zk) − k−1 Alµψ yl − y˜l+1 2 + k αl

k

k

2

l=0

2 l=0 2µψ

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l) 2
2

(222)

for all k ≥ 0. By deﬁnition of zk we get that

g˜k(zk) = min
z∈Rn

1

k 02

2 z − z 2 + αl

l=0

ψ(y˜l) + ∇˜ Ψ(y˜l, ξl), z − y˜l

+ µψ 2

z − y˜l

2 2

≤ 1 y∗ − z0 2 + k α ψ(y˜l) + ∇˜ Ψ(y˜l, ξl), y∗ − y˜l + µψ y∗ − y˜l 2

2

2

l

2

2

l=0

1 ∗ 02 k

l

l ∗ l µψ ∗ l 2

= 2 y − z 2 + αl ψ(y˜ ) + ∇ψ(y˜ ), y − y˜ + 2 y − y˜ 2

l=0

k
+ αl ∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l), y∗ − y˜l

l=0

≤ 1 y∗ − y0 2 + A ψ(y∗) + k α ∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l), y∗ − y˜l ,

2

2

k

l

l=0

(223)

where the last inequality follows from µψ-strong convexity of ψ and Ak =

k l=0

αl

.

For

brevity,

we

introduce new notation: Rk d=ef yk − y∗ 2 and Rk d=ef yk − y˜k+1 2 for all k ≥ 0. Using this and

another

consequence

of

strong

convexity,

i.e.

ψ(y)

−

ψ(y∗)

≥

µψ 2

y − y∗

22, we obtain

Akµψ 2 k−1 Alµψ 2

2 Rk +

2 Rl

l=0

≤
(222)+(223)
≤

k

∗

k−1 Alµψ 2

Ak ψ(y ) − ψ(y ) +

2 Rl

l=0

1 R2 + k α ∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l), y∗ − y˜l

20

l

l=0

+ k αl l=0 2µψ

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l)

2
.

2

(224)

89

From Cauchy-Schwarz inequality and the well-known fact that

a+b

2 2

≤ 2a2 + 2b2

for all a, b ∈ Rn

we have

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l), y∗ − y˜l = E ∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l), y∗ − y˜l

+ ∇˜ Ψ(y˜l, ξl) − E ∇˜ Ψ(y˜l, ξl) , y∗ − y˜l

(38)
≤

δ y∗ − y˜l 2 +

∇˜ Ψ(y˜l, ξl) − E ∇˜ Ψ(y˜l, ξl) , y∗ − y˜l

,

∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l) 2 ≤ 2 E ∇˜ Ψ(y˜l, ξl) − ∇ψ(y˜l) 2

2

2

+2 ∇˜ Ψ(y˜l, ξl) − E ∇˜ Ψ(y˜l, ξl) 2
2

(38)
≤

2δ2 + 2 ∇˜ Ψ(y˜l, ξl) − E ∇˜ Ψ(y˜l, ξl)

2

2

for all l ≥ 0. Next, we introduce new notation

A˜ d=ef 1 R2 + δα0R0 + AN δ2 + α0 ∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0) , y∗ − y˜0

20

µψ

+ α0

∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0)

2
.

µψ

2

(225)

Putting all together in (224) we get

Akµψ 2 k−1 Alµψ 2

12

k

∗l

2 Rk +

2 Rl ≤ 2 R0 + δ αl y − y˜ 2

l=0

l=0

k
+ αl ∇˜ Ψ(y˜l, ξl) − E ∇˜ Ψ(y˜l, ξl) , y∗ − y˜l

l=0

+ δ2

k

1

α+

k
α

∇˜ Ψ(y˜l, ξl) − E ∇˜ Ψ(y˜l, ξl)

2

µψ

l µψ

l

2

l=0

l=0

k−1
≤ A˜ + δ αl+1 y∗ − y˜l+1 2

l=0

k−1
+ αl+1 ∇˜ Ψ(y˜l+1, ξl+1) − E ∇˜ Ψ(y˜l+1, ξl+1) , y∗ − y˜l+1

l=0

1 k−1

+

α

∇˜ Ψ(y˜l+1, ξl+1) − E ∇˜ Ψ(y˜l+1, ξl+1)

2
.

µψ

l+1

2

l=0

(226)

To simplify previous inequality we deﬁne new vectors al d=ef y∗ − yl, a˜l d=ef yl − y˜l+1, ηl d=ef ∇˜ Ψ(y˜l+1, ξl+1) − E ∇˜ Ψ(y˜l+1, ξl+1) for all l ≥ 0. Note that al 2 = Rl, a˜l 2 = Rl and

90

a˜0 = y0 − y˜1 = 0. Using this we can rewrite (226) in the following form:

k−1
A R2 + A R2

≤

2δ k−1

A+

α

2 k−1

(R + R ) +

α

ηl, al + a˜l

kk

ll l=0

µψ l=0 l+1 l

l µψ l=0 l+1

2 k−1

l2

+ µ2 αl+1 η 2,

ψ l=0

(227)

where we used A d=ef µ2Aψ˜ and triangle inequality, i.e. y∗ −y˜l+1 2 ≤ y∗ −yl 2 + yl −y˜l+1 2 = Rl +Rl.

Next,

we

apply

Lemma

5.10

with

h

=

u

=

µ2ψ ,

c

=

2 µ2

and get that with probability at least 1 − 2β

ψ

where

R2 ≤ J 2R02 N AN

ln

N β

g(N ) =

1+

+ ln ln

B b

2σ12α21R02

(129)

µψ

2 , b = r1 , D = 1 + Lψ +

3 ln

N β

1 + µψ , Lψ

(228)

B

= 8HC

Lψ

3/2
DR2

N

3N +1

µψ

0

2

A + 2Dh2G2R02

+2C

Lψ

3/2
c + 2Du2 HR2

,

µψ

0

  J = max 

3B1D + A0,

9B12D2

+

4A R2

+

8cH C

0

2

 Lµψψ 3/2 
, 

Lψ 3/2 B1 = hG + uC1 2HC µψ g(N ) and C1 is some positive constant. However, J depends on A which is stochastic. That is, to ﬁnish the proof we need ﬁrst to get an upper bound for A. Recall that A = µ2Aψ˜ and

A (2=25) R02 + 2δα0R0 + 2AN δ2 + 2α0 ∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0) , y∗ − y˜0

µψ

µψ

µ2ψ

µψ

+ 2α0

∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0)

2
.

µ2ψ

2

(229)

Lemma C.3 implies that





 P ∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0)

≥ √2(1 + √γ) σψ2  ≤ exp − γ2 .



2

r0 

3

91

Taking γ =

3 ln

1 β

and

using

r0

≥

at least 1 − β

µ

3/2 N 2σ2 1+ 3 ln N 2

ψ

β

H R2

ψ
Lψ

Cε

,ε≤

0
AN

we

get

that

with

probability

∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0) , y∗ − y˜0 ≤ ≤ ≤
∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0) 2 ≤
2

∇˜ Ψ(y˜0, ξ0) − E ∇˜ Ψ(y˜0, ξ0)

Lψ

3/4

√ 2C εR0

· y∗
2

µψ

N

Lψ 3/4 √2CHR2

√ 0,

µψ

N AN

Lψ 3/2 2Cε

µψ

N2 ≤

Lψ 3/2 2CHR02 .

µψ

N 2AN

− y0

2
(230) (231)

From this and δ ≤ NG√RA0N we obtain that with probability ≥ 1 − β

A

(229)+(230)+(231)
≤

AˆR2,

0

√

Aˆ

d=ef

1+

2G√ + 2G2 + Lψ 3/4 2 2C√H

µψ LψµψN AN µ2ψN 2

µψ

LψµψN AN

Lψ 3/2 4CH + µψ Lψµ2 N 2AN .
ψ

Using union bound we get that with probability at least 1 − 3β

R2 ≤ Jˆ2R02 , N AN

where

ln

N β

gˆ(N ) =

1+

+ ln ln

Bˆ b

2,

3 ln

N β

Bˆ = 8HC

Lψ

3/2
DR4

N

3 N +1

µψ

0

2

Aˆ + 2Dh2G2 + 2C

Lψ

3/2
c + 2Du2 H

,

µψ

  Jˆ = max 

3Bˆ1D + A0,

9Bˆ12D2 + 4Aˆ + 8cHC 2

 Lµψψ 3/2 
, 

Note that

Bˆ = hG + uC 2HC Lψ 3/2 gˆ(N ).

1

1

µψ

(128) 1 Ak ≥ Lψ

1+ 1 2

µψ 2k . Lψ

92

It means that in order to achieve RN2 ≤ ε with probability at least 1 − 3β the method requires N = O˜ Lµψψ ln 1ε iterations and

N
rk = O
k=0

max

Lψ , σψ2 ln 1 ln 1 µψ ε β ε

oracle calls where O(·) hides polylogarithmic factors depending on Lψ, µψ, R0, ε and β.

I.4 Proof of Corollary 5.14

Corollary 5.13 implies that with probability at least 1 − 3β

yN 2 ≤ 2Ry,

∇ψ(yN ) 2 ≤ ε Ry

and the total number of oracle calls to get this is of order (72). Together with Theorem 5.2 it gives us that with probability at least 1 − 3β

f (xˆN ) − f (x∗) ≤ 2εˆ,

AxˆN 2 ≤ εˆ , Ry

(232)

where xˆN d=ef x(A⊤yN ). It remains to show that x˜N and xˆN are close to each other with high

probability. Lemma C.3 states that





N N N √

σx2


N

γ2

P  x˜ − E x˜ | y 2 ≥ ( 2 + 2γ) rN | y  ≤ exp − 3 .

Taking γ =

1

1 σψ2 R2y 1+

3 ln 1 2
β

3 ln β and using rN ≥ C

ε2

we get that with probability at least 1 − β

x˜N − E x˜N | yN 2 ≤

2C σx2ε2 = σψ2 Ry2 Ry

√ 2Cε ,
λmax(A⊤A)

x˜N − xˆN 2 ≤ x˜N − E x˜N | yN 2 + E x˜N | yN − xˆN 2

√

(33)
≤

2C ε

+ G1ε

Ry λmax(A⊤A) N Ry

2C

ε

≤

λmax(A⊤A) + G1 Ry .

(233)

It implies that with probability at least 1 − β

Ax˜N − AxˆN 2

≤
(233)
≤

=

A 2 · x˜N − xˆN 2

λmax(A⊤A)

2C

ε

λmax(A⊤A) + G1 Ry

√ 2C + G1 λmax(A⊤A)

ε,

Ry

(234)

93

and due to triangle inequality with probability ≥ 1 − β

AxˆN 2

≥
(234)
≥

Ax˜N 2 − AxˆN − Ax˜N 2

√ Ax˜N 2 − 2C + G1 λmax(A⊤A)

ε.

Ry

(235)

Applying Demyanov-Danskin theorem and Lϕ-smoothness of ϕ with Lϕ = 1/µ we obtain that with probability at least 1 − β

xˆN 2 = ∇ϕ(A⊤yN ) 2 ≤ ∇ϕ(A⊤yN ) − ∇ϕ(A⊤y∗) 2 + ∇ϕ(A⊤y∗) 2

≤ Lϕ A⊤yN − A⊤y∗ 2 + x(A⊤y∗) 2 ≤ λmax(A⊤A) yN − y∗ 2 + Rx µ

(67)
≤

λmax(A⊤A)ε + Rx

µRy

(236)

and also

x˜N 2

≤
(233)+(236)
≤

x˜N − xˆN 2 + xˆN 2 2C
λmax(A⊤A) + G1 +

λmax(A⊤A) µ

ε Ry + Rx.

(237)

That is, we proved that with probability at least 1 − β points xˆl and x˜l lie in the ball BRf (0). In this ball function f is Lf -Lipschitz continuous, therefore, with probability at least 1 − β

f (xˆN ) = f (x˜N ) + f (xˆN ) − f (x˜N ) ≥ f (x˜N ) − |f (xˆN ) − f (x˜N )| ≥ f (x˜N ) − Lf xˆN − x˜N 2

(233)
≥ f (x˜N ) −

2C λmax(A⊤A) + G1

Lf ε . Ry

(238)

Combining inequalities (232), (235) and (238) and using union bound we get that with probability at least 1 − 4β

f (x˜N ) − f (x∗) ≤ Ax˜N 2 ≤

2 + λmax2(CA⊤A) + G1 RLfy ε,

√ 1 + 2C + G1 λmax(A⊤A)

ε.

Ry

Finally, in order to get the bound for the total number of oracle calls from (72) we use (66) together with σψ2 = σx2λmax(A⊤A) and (121).

94

