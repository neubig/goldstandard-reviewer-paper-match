Sequential Experimental Design for Transductive Linear Bandits

Tanner Fiez ∗

Lalit Jain† Kevin Jamieson‡ June 21, 2019

Lillian Ratliﬀ§

arXiv:1906.08399v1 [stat.ML] 20 Jun 2019

Abstract
In this paper we introduce the transductive linear bandit problem: given a set of measurement vectors X ⊂ Rd, a set of items Z ⊂ Rd, a ﬁxed conﬁdence δ, and an unknown vector θ∗ ∈ Rd, the goal is to infer argmaxz∈Z z θ∗ with probability 1 − δ by making as few sequentially chosen noisy measurements of the form x θ∗ as possible. When X = Z, this setting generalizes linear bandits, and when X is the standard basis vectors and Z ⊂ {0, 1}d, combinatorial bandits. Such a transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages X a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may diﬀer vastly from those compounds and dosages Z that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books X a user is queried about may be restricted to well known best-sellers even though the goal might be to recommend more esoteric titles Z. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we provide the ﬁrst non-asymptotic algorithm for linear bandits that nearly achieves the information theoretic lower bound.

1 Introduction

In content recommendation or property optimization in the physical sciences, often there

is a set of items (e.g., products to purchase, drugs) described by a set of feature vectors

Z ⊂ Rd, and the goal is to ﬁnd the z ∈ Z that maximizes some response or property

(e.g., aﬃnity of user to the product, drug combating disease). A natural model for these

settings is to assume that there is an unknown vector θ∗ ∈ Rd and the expected response to any item z ∈ Z, if evaluated, is equal to z θ∗. However, we often cannot measure z θ∗

directly, but we may infer it transductively through some potentially noisy probes. That

is, given a ﬁnite set of probes X ⊂ Rd we observe x θ∗ + η for any x ∈ X where η is

independent

mean-zero,

sub-Gaussian

noise.

Given

a

set

of

measurements

{

(x

i

,

ri

)}

N i=1

one

∗Department of Electrical and Computer Engineering, University of Washington, ﬁezt@uw.edu †Paul G. Allen School of Computer Science & Engineering, University of Washington, lal-
itj@cs.washington.edu, contribution shared equally among ﬁrst two authors ‡Paul G. Allen School of Computer Science & Engineering, University of Washington,
jamieson@cs.washington.edu §Department of Electrical and Computer Engineering, University of Washington, ratliﬄ@uw.edu

1

can construct the least squares estimator θ = arg minθ Ni=1(ri − xi θ)2 and then use θ as a plug-in estimate for θ∗ to estimate the optimal z∗ := argmaxz∈Z z θ∗. However, the accuracy of such a plug-in estimator depends critically on the number and choice of probes used to construct θ. Unfortunately, the optimal allocation of probes cannot be decided a priori: it must be chosen sequentially and adapt to the observations in real-time to optimize the accuracy of the prediction.
If the probing vectors X are equal to the item vectors Z, this problem is known as pure exploration for linear bandits which is considered in [19, 28, 29, 31]. This naturally arises in content recommendation, for example, if X = Z is a feature representation of songs, and θ∗ represents a user’s music preferences, a music recommendation system can elicit the preference for a particular song z ∈ Z directly by enqueuing it in the user’s playlist. However, often times there are constraints on which items in Z can be shown to the user. 1. X ⊂ Z. Consider a whiskey bar with hundreds of whiskies ranging in price from dollars a
shot to hundreds of dollars. The bar tender may have an implicit feature representation of each whiskey, the patron has an implicit preference vector θ∗, and the bar tender wants to select the aﬀordable whiskeys X ⊂ Z in a taste test to get an idea of the patron’s preferences before recommending the expensive whiskies that optimize the patron’s preferences in Z. 2. Z ⊂ X . In drug discovery, thousands of compounds are evaluated in order to determine which ones are eﬀective at combating a disease. However, it may be that while Z is the set of compounds and doses that are approved for medical use (e.g., safe), it may be advantageous to test even unsafe compounds or dosages X such that X ⊃ Z. Such unsafe X may aid in predicting the optimal z∗ ∈ Z because they provide more information about θ∗. 3. Z ∩X = ∅. Consider a user shopping for a home among a set Z where each is parameterized by a number of factors like distance to work, school quality, crime rate, etc. so that each z ∈ Z can be described as a linear combination of the relevant factors described by X : z = x∈X αz,xx, where we may take each x ∈ X to simply be one-hot-encoded. The response x θ∗ + η reﬂects the user’s preferences for the query x, a speciﬁc attribute of the house. Indeed, if all αz,x ∈ {0, 1} this is known as pure exploration for combinatorial bandits [9, 7]. That is, a house either has the attribute, or not. Given items Z, measurement probes X , a conﬁdence δ, and an unknown θ∗, this paper develops algorithms to sequentially decide which measurements in X to take in order to minimize the number of measurements necessary in order to determine z∗ with high probability.
1.1 Contributions
Our goals are broadly to ﬁrst deﬁne the transductive bandit problem and then characterize the instance-optimal sample complexity for this problem. Our contributions include the following. 1. In Section 2 we provide instance dependent lower bounds for the transductive bandit
problem that simultaneously generalize previous known lower bounds for linear bandits and combinatorial bandits using standard arguments. 2. In Section 3 the main contribution of this paper, we give an algorithm (Algorithm 1) for transductive linear bandits and prove an associated sample complexity result (Theorem 2). We show that the sample complexity we obtain matches the lower bound up to logarithmic factors. Along the way, we discuss how rounding procedures can be used to improve upon
2

the computational complexity of this algorithm. 3. Following Section 3, we review the related work, and then contrast our algorithm with
previous results from a theoretical and empirical perspective. Our experiments show that our theoretically superior algorithm is empirically competitive with previous algorithms on a range of problem scenarios.

1.2 Notation

For each z ∈ Z deﬁne the gap of z, ∆(z) = (z∗−z) θ∗ and furthermore, ∆min = minz=z∗ ∆(z).

If

A

∈

d×d
R≥0

is

a

positive

semideﬁnite

matrix,

and

y

∈

Rd

is

a

vector,

let

y

2 A

:=

y

Ay

denote the induced semi-norm. Let X := {λ ∈ R|X | : λ ≥ 0, x∈X λx = 1} denote the set of probability distributions on X . Taking S ⊂ Z to a subset of the arm set, we deﬁne two

operators we deﬁne Y(S) = {z − z : ∀ z, z ∈ S, z = z } as the directions obtained from the

diﬀerences between each pair of arms and Y∗(S) = {z∗ − z : ∀ z ∈ S \ z∗} as the directions

obtained from the diﬀerences between the optimal arm and each suboptimal arm. Finally,

for an arbitrary set of vectors V ⊂ Rd, deﬁne ρ(V) = minλ∈ X maxv∈V v 2( x∈X λxxx )−1 this quantity will be crucial in the discussion of our sample complexity and is motivated in

Section 2.2

2 Transductive Linear Bandits Problem
Consider known ﬁnite collections of d-dimensional vectors X ⊂ Rd and Z ⊂ Rd , known conﬁdence δ ∈ (0, 1), and unknown θ∗ ∈ Rd. The objective is to identify z∗ = argmaxz∈Z z θ∗ with probability at least 1 − δ while taking as few measurements in X as possible. Formally, a transductive linear bandits algorithm is described by a selection rule Xt ∈ X at each time t given the history (Xs, Rs)s<t, stopping time τ with respect to the ﬁltration Ft = (Xs, Rs)s≤t, and recommendation rule z ∈ Z invoked at time τ which is Fτ -measurable. We assume that Xt is Ft−1-measurable and may use additional sources of randomness; in addition at each time t that Rt = Xt θ∗ + ηt where ηt is independent, zero-mean, and 1-sub-Gaussian. Let Pθ∗ , Eθ∗ denote the probability law of Rt|Ft−1 for all t.
Deﬁnition 1. We say that an algorithm for a transductive bandit problem is δ-PAC for X , Z ⊂ Rd if for all θ∗ ∈ Rd we have Pθ∗ (z = z∗) ≥ 1 − δ.

2.1 Optimal allocations

In this section we discuss a number of ways we can allocate a measurement budget to the diﬀerent arms. The following establishes a lower bound on the expected number of samples any δ-PAC algorithm must take.

Theorem 1. Assume ηt i∼id N (0, 1) for all t. Then for any δ ∈ (0, 1), any δ-PAC algorithm must satisfy

Eθ∗ [τ ] ≥ log(1/2.4δ) min max
λ∈ X z∈Z\{z∗}

z∗ − z

2 (

x∈X λxxx

)−1
.

((z∗ − z) θ∗)2

This lower bound is proved in Appendix C using standard techniques and employs the transportation inequality of [20]. It generalizes a previous lower bound in the setting of linear bandits [27] and lower bounds in the combinatorial bandit literature [9].

3

Optimal static allocation. To demonstrate that this lower bound is tight, deﬁne

λ∗ := argmin max
λ∈ X z∈Z\{z∗}

z∗ − z 2( x∈X λxxx )−1 and ψ∗ = max

((z∗ − z) θ∗)2

Z \{z∗ }

z∗ − z

2 (

λ∗ xx )−1

x∈X x

,

((z∗ − z) θ∗)2

(1)

where ψ∗ is the value of the lower bound and λ∗ is the allocation that achieves it. Suppose we

sample arm x ∈ X exactly 2 λ∗xN times where we assume1 N ∈ N is suﬃciently large so that minx:λx>0 λxN > 0. If N = 2ψ∗ log(|Z|/δ) then as we will show shortly (Section 2.2),

the least squares estimator θ satisﬁes (z∗ − z) θ > 0 for all z ∈ Z \ z∗ with probability at

least 1 − δ. Thus, with probability at least 1 − δ, z∗ is equal to z = arg maxz∈Z z θ and the

total number of samples is bounded by 2N which is within 4 log(|Z|) of the lower bound. Unfortunately, of course, the allocation λ∗ relies on knowledge of θ∗ (which determines z∗)

which is unknown a priori, and thus this is not a realizable strategy. Other static allocations. Short of λ∗ it is natural to consider allocations that arise from

optimal linear experimental design [25]. For the special case of X = Z it has been argued ad nauseam that a G-optimal design, argminλ∈ X maxx∈X ,x=x∗ x 2( x∈X λxxx )−1 , is woefully loose since it does not utilize the diﬀerences x − x , x, x ∈ X [23, 28, 31]. Also for the X = Z

case, [32, 28] have proposed the static X Y-allocation given as argminλ∈ X maxx,x ∈X x − x 2( x∈X λxxx )−1 . In [28] it is shown that no more than O( ∆2mdin log(|X | log(1/∆min)/δ)) samples from each of these allocations suﬃce to identify the best arm. While the above
discussion demonstrates that for every θ∗ there exists an optimal static allocation (that
explicitly uses θ∗) that nearly achieves the lower bound, any ﬁxed allocation with no prior
knowledge of θ∗ can require a factor of d more samples.

Proposition 1. Let c, c be universal constants. For any γ > 0, d even, there exists sets

X

=Z

⊂ Rd

and

a

set

Θ ⊂ Rd,

such

that

infA maxθ∈Θ Eθ[τ ] ≥

cd log(1/δ) γ

where

A

is

the

set

of all algorithms that are δ-PAC for X , Z and take a static allocation of samples. On the

other hand ψ∗/c ≤ d + γ1 for every choice of θ∗ ∈ Θ.

The proof of this proposition can be found in Appendix D. Adaptive allocations. As suggested by the problem deﬁnition, our strategy is to adapt our allocation over time, informed by the observations up to the current time. Speciﬁcally, our algorithm will proceed in rounds where at round t, we perform an X Y-allocation that is suﬃcient to remove all arms z ∈ Z that have gaps of at least 2−(t+1). We show that the total number of measurements accumulates to ψ∗ log(|Z|/δ) times some additional logarithmic factors, nearly achieving the optimal allocation as well as the lower bound. In Section 4, we review other related procedures for the speciﬁc case of X = Z.

2.2 Review of Least Squares

Given a ﬁxed design xT = (xt)Tt=1 with each xt ∈ X and associated rewards (rt)Tt=1, a natural

approach is to construct the ordinary-least squares (OLS) estimate θ = (

T t=1

xtxt

)−1(

T t=1

rtxt).

One can show θ is unbiased with covariance

(

T t=1

xtxt

)−1.

Moreover,

for

any

y

∈

Rd,

we have2

P y (θ∗ − θ) ≥ y 2 T

2 log(1/δ) ≤ δ.

(2)

( t=1 xtxt )−1

1Such an assumption is avoided by a sophisticated rounding procedure that we will describe shortly. 2There is a technical issue of whether the set Z lies in the span of X which in general is necessary to obtain unbiased estimates of (z∗ − z) θ∗. Throughout the following we assume that span(X ) = Rd.

4

In particular, if we want this to hold for all y ∈ Y∗(Z), we need to union bound over Z
replacing δ with δ/|Z|. Let us now use this to analyze the procedure discussed above (in
the discussion on the optimal static allocation after Theorem 1) that gives an allocation matching the lower bound. With the choice of N = 2ψ∗ log(|Z|/δ) and the allocation 2 λ∗xN for each x ∈ X , we have for each z ∈ Z \ z∗ that with probability at least 1 − δ,

(z∗ − z) θ ≥ (z∗ − z) θ∗ −

z∗ − z

2 (

2 Nλ∗xxT )−1 2 log(|Z|/δ) ≥ 0

x

x

since for each y = z∗ − z ∈ Y∗(Z) we have

−1

−1

y

2 N λ∗x xx y ≤ y

λ∗xxx y/N ≤ ((z∗ − z) θ∗)2/(2 log(|Z|/δ)), (3)

x∈X

x∈X

where the last inequality plugs in the value of N and the deﬁnition of ψ∗. The fact that at

most one z ∈ Z can satisfy (z − z) θ > 0 for all z = z ∈ Z, and that z = z∗ does, certiﬁes

that z = arg maxz∈Z z θ is indeed the best arm with probability at least 1 − δ. Note that equation (3) provides the motivation for how the form of ψ∗ is obtained. Rearranging, it is

equivalent to,

z∗ − z

2 (

λ∗ xx )−1

N ≥ 2 log(|Z|/δ) max
Z \{z∗ }

x∈X x
((z∗ − z) θ∗)2

for all z ∈ Z \ {z∗}

Thinking of the right hand side of the inequality as a function of λ, λ∗ is precisely chosen to

minimize this quantity and hence the sample complexity.

2.3 Rounding Procedures
We brieﬂy digress to address a technical issue. Given an allocation λ and an arbitrary subset of vectors Y, in general, drawing N samples xN := {x1, . . . , xN } at random from X according to the distribution λx may result in a design where maxy∈Y y 2( N t=1 xtxt )−1 (which appears in the width of the conﬁdence interval (2)) diﬀers signiﬁcantly from maxy∈Y y 2( x∈X λxxx )−1 /N . Naive strategies for choosing xN will fail. We can not simply use an allocation of N λx samples for any speciﬁc x since this may not be an integer. Furthermore, greedily rounding N λx to an allocation N λx or N λx may result in too few than necessary, or far more than N total samples if the support of λ is large. However, given > 0, there are eﬃcient rounding procedures that produce (1 + )-approximations as long as N is greater than some minimum number of samples r( ). In short, given λ and a choice of N they return an allocation xN satisfying maxy∈Y y 2( N i=1 xixi )−1 ≤ (1 + ) maxy∈Y y 2( x∈X λxxx )−1 /N . Such a procedure with r( ) ≤ O(d/ 2) is described in Section B in the supplementary. In our experiments we use a rounding procedure from [25] that is easier to implement (also see [28, Appendix C]) with r( ) = (d(d + 1)/2 + 1)/ . In general should be thought of as a constant, i.e. = 1/5. The number of samples N we need to take in our algorithm will be signiﬁcantly larger than 5d2, so the impact of the rounding procedure is minimal.

3 Sequential Experimental Design for Transductive Linear Bandits
Our algorithm for the pure exploration transductive bandit is presented in Algorithm 1. The algorithm proceeds in rounds, keeping track of the active arms Zt ⊆ Z in each round t. At

5

the start of round t − 1, it samples in such a way to remove all arms with gaps greater than 2−t. Thus denoting St := {z ∈ Z : ∆(z) ≤ 2−t}, in round t we expect Zt ⊂ St.
As described above, if we knew θ∗, we would sample according to the optimal allocation argminλ∈ X maxz∈Zt z∗ − z 2( x∈X λxxx )−1 /((z∗ − z) θ∗)2. However, instead at the start of round t, if we simply have an upper bound on the gaps, ∆(z) ≤ 2−t and we do not know the best arm, we can lower-bound the above objective by (2t)2 minλ∈ X maxy∈Y(Zt) y 2( x∈X λxxxT )−1 3. This motivates our choice of λt and ρ(Y(Zt)). Thus by the same logic used in Section 2.2,
Nt = 8(2t+1)2(1 + )ρ(Y(Zt)) log(|Z|2/δt) samples should suﬃce to guarantee that we
can construct a conﬁdence interval on each (z − z ) θ∗ for (z − z ) ∈ Y(Zt) of size at most 2−(t+1) (with the |Z|2 in the logarithm accounting for a union bound over arms). The (1 + )
accounts for slack from the rounding principle. Finally, we remove an arm z if there exists an arm z so that the empirical gap (z − z) θt > 2−(t+2).

Algorithm 1: RAGE(X , Z, δ): Randomized Adaptive Gap Elimination
Input: Arm set X , rounding approximation factor with default value 1/5, minimum number of samples needed to obtain rounding approximation r( ), and conﬁdence level δ ∈ (0, 1). Let Z1 ← Z, t ← 1 while |Zt| > 1 do

δt ← tδ2 λ∗t ← arg minλ∈ X maxy∈Y(Zt) y 2( x∈X λxxx )−1

ρ(Y (Zt)) ← minλ∈ X maxy∈Y(Zt) y 2( x∈X λxxx )−1

Nt ← max 8(2t+1)2ρ(Y(Zt))(1 + ε) log(|Z|2/δt) , r( ) xNt ← Round(λ∗, Nt)

Pull arms x1, . . . , xNt and obtain rewards r1, . . . , rNt

Compute θt = A−t 1bt using At :=

Nt j=1

xj

xj

and bt :=

Zt+1 ← Zt \ z ∈ Z|∃ z ∈ Z : 2−(t+2) ≤ (z − z) θt

t←t+1

Nt j=1

xj

rj

Output: Zt+1

Theorem 2. With probability greater than 1 − δ, using an -eﬃcient rounding procedure, Algorithm 1 correctly identiﬁes the optimal arm z∗ and requires a worst-case sample complexity

N≤

log2 (1/∆min ) t=1

max

8 (2t+1)2ρ(Y(St))(1 +

) log(t2|Z|2/δ) , r( )

where St = {z ∈ Z : ∆(z) ≤ 2−t}. In particular, Round can be chosen so that r( ) = O(d/ 2). Furthermore, N ≤ cψ∗ log(1/∆min) log(|Z|2 log(1/∆min)2/δ) + r( ) log2(1/∆min) for some
absolute constant c, in other words Algorithm 1 is instance optimal up to logarithmic factors.

We provide a proof of the sample complexity bound in Section A.
3 Where we recall for any subset S ⊂ Z, Y(S) := {z − z : z, z ∈ S} and for an arbitrary subset V ⊂ Rd we have ρ(V ) = minλ∈ X maxv∈V v 2( x∈X λxxx )−1 .

6

3.1 Interpreting the sample complexity.
Up to logarithmic factors, Algorithm 1 matches the lower bound obtained in Theorem 1. However, the term ρ(Y(St)) may seem a bit mysterious. In this section we try to interpret this quantity in terms of the geometry of X and Z.
Let conv(X ∪ −X ) denote the convex hull of X ∪ −X , and for any set Y ⊂ Rd deﬁne the gauge of Y,
γY = max{c > 0 : cY ⊂ conv(X ∪ −X )}.
In the case where Y is a singleton Y = {y}, γ(y) := γY is the gauge norm of y with respect to conv(X ∪ −X ), a familiar quantity from convex analysis [26]. We can provide a natural upper bound for ρ(Y) in terms of the gauge.
Lemma 1. Let Y ⊂ Rd. Then

max y 22/(max x 2) ≤ ρ(Y) ≤ d/γY2 .

(4)

y∈Y

x∈X

In the case of a singleton Y = {y}, we can improve the upper bound to ρ(Y) ≤ 1/γ(y)2.

The proof of this Lemma is in Appendix E. To see the potential for adaptive gains
we focus on the case of linear bandits where X = Z. Consider an example with X = {ei}di=1 ∪ {z } for z = (cos(α), sin(α), 0, · · · , 0) where α ∈ [0, .1), and θ∗ = e1. Note that ∆min ≈ sin(α) ≈ α. Then S1 = X , and an easy comptation shows γY(X ) ≤ 2. After the ﬁrst round, all arms except e1 and z will be removed, so Y(St) = {e1 − z } for t ≥ 2, and γY(St) ≈ 1/ sin(α) ≈ 1/α. Summing over all rounds, we see that this implies a sample complexity of O(d log(log(1/α)d2/δ)) which up to log factors is independent of the gap and a signiﬁcant improvement over the static X Y-allocation sample complexity of d/α2.

4 Related Work

When X = Z = {e1, · · · , ed} ⊂ Rd is the set of standard basis vectors, the problem reduces

to that of the best-arm identiﬁcation problem for multi-armed bandits which has been

extensively studied [13, 17, 18, 20, 10]. In addition, pure exploration for combinatorial

bandits where X = {e1, · · · , ed} ⊂ Rd and Z ⊂ {0, 1}d has also received a great deal of

attention [9, 7, 11, 8].

In the setting of linear bandits when X = Z, despite a great deal of work in the regret

and contextual settings [1, 24, 23, 12], there has been far less work on linear bandits for pure

exploration. This problem was ﬁrst introduced in [28] and since then, there have been a few

other works on this topic, [29, 19, 31] that we now discuss.

• Soare et al. [28] made the initial connections to G-optimal experimental design. That work

provides the ﬁrst passive algorithm with a sample complexity of O(

d
2

log(|X |/δ) + d2).

∆min

Note that the d2 comes from the minimum number of samples needed for an eﬃcient

rounding procedure and thus could be reduced to d using improved rounding procedures

(see section [2]). In addition to the passive algorithm, they provide an adaptive algorithm,

X Y-adaptive algorithm for linear bandits. Their algorithm is very similar to ours, with

two notable diﬀerences. Firstly, instead of using an eﬃcient rounding procedure, they use

a greedy iterative scheme to compute an optimal allocation. Secondly, their algorithm

does not discard items that are provably sub-optimal. As a result, their sample complexity

(up to logarithmic factors) scales as max{M ∗, ψ∗} log(|X |/(∆minδ)) + d2 where M ∗ is

7

deﬁned (informally) as the amount of samples needed using a static allocation to remove all sub-optimal directions in Y(X ) \ Y∗(X ).

• In Tao et al. [29], the focus is on developing diﬀerent estimators with the goal of removing

the constant term d2 in Soare et al.’s passive sample complexity. Instead of using a

rounding procedure, they use a diﬀerent estimator than the OLS estimator θ∗. Note that

the rounding procedure in [2] and described in the supplementary could have been applied

directly to Soare’s static allocation algorithm giving the same sample complexity as the

one obtained in [29]. They also provide an adaptive algorithm ALBA, that achieves a

sample complexity of O(

d i=1

1/∆2i )

where

i

is

the

i-th

smallest

gap

of

the

vectors

in

X . It is easy to see that this sample complexity is not optimal: imagine a situation in

which the vectors of X with the (d − 1)-smallest gaps are identical to the vector x = x .

Then we only need to pay once for the samples needed to remove x , not (d − 1)-times.

Finally, their algorithms do not compute the optimal allocation over diﬀerences of vectors

in X , but instead on X directly à la G-optimal design. We will see the ineﬃciency of this

strategy in the experiments.

• Karnin [19] provides an algorithm that uses repeated rounds (for probability ampliﬁcation) of exploration phases combined with veriﬁcation phases to provide an asymptotically optimal algorithm, meaning when δ → 0 the sample complexity divided by log(1/δ) approaches ψ∗. Though this is a nice theoretical result, the algorithm is not practical; the exploration phase is simply a naïve passive G-optimal design.

• In Xu et al. [31], a fully adaptive algorithm, inspired by the UGapE algorithm [14], LinGapE is proposed. Since LinGapE is fully adaptive, a conﬁdence bound allowing for dependence in the samples is necessary and the authors employ the self-normalized bound of [1]. The algorithm requires each arm to be pulled once - an undesirable characteristic of a linear bandit algorithm since the structure of the problem allows for information to be obtained about arms that are not pulled. A recent work [21], extends this algorithm to the setting of generalized linear models where the expected reward of pulling arm z reward is given by a noisy ovservation of a non-linear link function of z θ∗.
Finally, we mention [32], which considers transductive experimental design from a computational and optimization perspective, and explores X Y-allocation for arbitrary kernels.

5 Experiments
In this section, we present simulations for the linear bandit pure exploration problem and the general transductive bandit problem. We compare our proposed algorithm with both adaptive and non-adaptive strategies. The adaptive strategies are X Y-Adaptive allocation from [28], LinGapE from [31], and ALBA from [29], and the non-adaptive strategies are static X Y-allocation, as described in Section 2, and an oracle strategy that knows θ∗ and samples according to λ∗. We do not compare to the algorithm given in [19] since it is primarily a theoretical contribution and in moderate-conﬁdence regimes obtains only the non-adaptive sample complexity. We run each algorithm at a conﬁdence level of δ = 0.05. The empirical failure probability of each of the algorithms in the presented simulations is zero. To compute the samples for RAGE, we ﬁrst used the Frank-Wolfe algorithm (with a precise stopping condition in the supplementary) to ﬁnd λt, and then a rounding procedure from [25] with various values of < 1/3. Further implementation details of RAGE and discussion pertaining

8

to the implementation of the other algorithms can be found in the supplementary material in Section F. We remark here that in our implementation of the X Y-Adaptive allocation, we follow the experiments in [28] and allow for provably suboptimal arms to be discarded (though this is not how the algorithm is written in their paper). The resulting algorithm is then similar to our algorithm.
Linear bandits: benchmark example. The ﬁrst experiment we present has become a benchmark in the linear bandit pure exploration literature since it was introduced in [28]. In this problem, X = {e1, . . . , ed, x } ⊂ Rd where ei is the i-th standard basis vector, x = cos(.01)e1 + sin(.01)e2, and θ∗ = 2e1 so that x∗ = x1. An eﬃcient sampling strategy for this problem needs to focus on reducing uncertainty in the direction (x1 − xd+1), which can be achieved by focusing pulls on arm x2 = e2 since it is most aligned with this direction.
The results for this experiment are shown in Fig. 1a. The proposed RAGE algorithm performs competitively with existing algorithms and the oracle allocation. The X Y-Adaptive algorithm is similar to RAGE, but with weaker theoretical guarantees, so naturally it performs nearly equivalently. The LinGapE algorithm performs well when the number of dimensions and arms is small. However, as the number of arms grows, LinGapE will fall behind the competing algorithms. ALBA performs the worst of the recently proposed algorithms and this is to be expected since it computes an allocation on the X set instead of on the Y(X ) set. This example clearly highlights the gains of adaptive sampling over non-adaptive allocations like static X Y-allocation. However, since X is relatively small in this case, it fails to tease out important diﬀerences between the algorithms that can greatly increase the sample complexity. We construct examples to demonstrate these claims now.
Many arms with moderate gaps. In this example, for a given value of n ≥ 3, we construct a set of arms X ⊂ R2, where X = {e1, cos(3π/4)e1 + sin(3π/4)e2} ∪ {cos(π/4 + φi)e1 + sin(π/4 + φi)e2}ni=3 with φi ∼ N (0, .09) for each i ∈ {3, . . . , n}. The parameter vector is ﬁxed to be θ∗ = e1 so that x1 is the optimal arm, x2 gives the most information to identify the optimal arm, and the remaining arms roughly point in the same direction with an expected gap of ∆ ≈ 0.3.
In Fig. 1b, we show the results of the experiment as we increase the number of arms n. We focus on the LinGapE algorithm in this example to demonstrate a linear scaling in the number of arms that occurs since LinGapE samples each arm once. An eﬃcient sampling strategy should focus energy on x2, and as it does so, it will gain information about the arms that are nearly duplicates of each other, which is how RAGE performs.
Uniform Distribution on a Sphere. In this example, X is sampled from a unit sphere of dimension d = 5 centered at the origin. Following [29], we select the two closest arms x, x ∈ X and let θ∗ = x + α(x − x) where α = 0.01. In Fig. 1c, we show the sample complexity of the RAGE and ALBA algorithms as the number of arms is increased. The RAGE algorithm signiﬁcantly outperforms ALBA and this is primarily due to the fact that ALBA computes a G-optimal design on the active vectors in each round instead of on the diﬀerences between these vectors. Thus the ALBA sampling distribution can be focused on a very diﬀerent set of arms from the optimal one.
Transductive example. To conclude our experiments, we present a general transductive bandit example. Since the existing algorithms in the linear bandit literature do not generalize to this problem, we compare with a static X Y-allocation on X , Y(Z) and an oracle X Yallocation on X , Y∗(Z) that knows the optimal arm and the gaps. We construct an example in Rd with d even where X = {e1, . . . , ed}. The set Z is also chosen so |Z| = d, the ﬁrst d/2 vectors are given by z1, . . . , zd/2 = (e1, . . . , ed/2) and then zd/2+j = cos(.1)ej + sin(.1)ej+d/2 for each j ∈ {1, . . . , d/2}. Take θ∗ = e1 so z1 is the optimal arm. The results of this
9

Sample Complexity

108

107

106

105

Rage XY-Adaptive

ALBA Oracle

LinGapE

XY-Static

104 5 10 15 20 25 30 35

Dimension

(a) Benchmark

Sample Complexity

30000

Rage

LinGapE

25000

20000

15000

15000

20000 25000 Number of Arms

(b) Duplicate arms

30000

Sample Complexity

Sample Complexity

108

107

107

106

Rage

ALBA

106

Rage Oracle

XY-Static

105

105

10

20

30

40

Number of Arms

(c) Uniform sphere

20

40

60

80

Dimension

(d) Transductive

Figure 1: Simulation Results

simulation are depicted in Fig. 1d. The RAGE algorithm signiﬁcantly outperforms the static allocation.
6 Conclusion
In this paper we have proposed the problem of best-arm identiﬁcation for transductive linear bandits, provided an algorithm, and matching upper and lower bounds. As a remark it is straightforward to exit our algorithm early with an ε-good arm. It still remains to develop anytime algorithms for this problem, as has been done in pure exploration for multi-armed bandits [17] that do not throw out samples. In addition, we suspect our algorithm actually matches the lower-bound and the log(1/∆min) factor is unnecessary. Finally, it is possible that some of the ideas developed here extend to the setting of regret and could be used to give instance based regret bounds for linear bandits [23]. We hope to explore connections to both the regret and ﬁxed budget settings in further works.

10

References
[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312–2320, 2011.
[2] Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174, 2017.
[3] Jean-Yves Audibert and Sébastien Bubeck. Best arm identiﬁcation in multi-armed bandits. In Conference on Learning Theory, pages 41–53, 2010.
[4] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-oﬀs. Journal of Machine Learning Research, 3:397–422, 2002.
[5] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175, 2003.
[6] Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems. In International Conference on Algorithmic Learning Theory, pages 23–37, 2009.
[7] Tongyi Cao and Akshay Krishnamurthy. Disagreement-based combinatorial pure exploration: Eﬃcient algorithms and an analysis with localization. arXiv preprint arXiv:1711.08018, 2017.
[8] Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid constraints. In Conference on Learning Theory, pages 647–669, 2016.
[9] Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling algorithms for combinatorial pure exploration. arXiv preprint arXiv:1706.01081, 2017.
[10] Lijie Chen and Jian Li. On the optimal sample complexity for best arm identiﬁcation. arXiv preprint arXiv:1511.03774, 2015.
[11] Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of multi-armed bandits. In Advances in Neural Information Processing Systems, pages 379–387, 2014.
[12] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. 2008.
[13] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun):1079–1105, 2006.
[14] Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identiﬁcation: A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence. In Advances in Neural Information Processing Systems, pages 3212–3220, 2012.
11

[15] Matthew Hoﬀman, Bobak Shahriari, and Nando Freitas. On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning. In International Conference on Artiﬁcial Intelligence and Statistics, pages 365–374, 2014.
[16] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1), pages 427–435, 2013.
[17] Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sébastien Bubeck. lil’ucb: An optimal exploration algorithm for multi-armed bandits. In Conference on Learning Theory, pages 423–439, 2014.
[18] Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multiarmed bandits. In International Conference on Machine Learning, pages 1238–1246, 2013.
[19] Zohar S Karnin. Veriﬁcation based solution for structured mab problems. In Advances in Neural Information Processing Systems, pages 145–153, 2016.
[20] Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the complexity of best-arm identiﬁcation in multi-armed bandit models. Journal of Machine Learning Research, 17:1–42, 2016.
[21] Abbas Kazerouni and Lawrence M Wein. Best arm identiﬁcation in generalized linear bandits. arXiv preprint arXiv:1905.08224, 2019.
[22] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics, 12:363–366, 1960.
[23] Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of ﬁnite-armed linear bandits. arXiv preprint arXiv:1610.04491, 2016.
[24] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670. ACM, 2010.
[25] Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.
[26] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.
[27] Marta Soare. Sequential resource allocation in linear stochastic bandits. PhD thesis, Université Lille 1-Sciences et Technologies, 2015.
[28] Marta Soare, Alessandro Lazaric, and Rémi Munos. Best-arm identiﬁcation in linear bandits. In Advances in Neural Information Processing Systems, pages 828–836, 2014.
[29] Chao Tao, Saúl Blanco, and Yuan Zhou. Best arm identiﬁcation in linear bandits with linear dimension dependency. In International Conference on Machine Learning, pages 4884–4893, 2018.
[30] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics, pages 135–166, 2004.
12

[31] Liyuan Xu, Junya Honda, and Masashi Sugiyama. A fully adaptive algorithm for pure exploration in linear bandits. In International Conference on Artiﬁcial Intelligence and Statistics, pages 843–851, 2018.
[32] Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In Proceedings of the 23rd international conference on Machine learning, pages 1081–1088. ACM, 2006.
13

A Proof of Theorem 2

Proof. Let the good event for the tth round of Algorithm 1 be

Et := Nt ≤ max{ 8(2t+1)2ρ(Y(St))(1+ ) log( |Zδt|2 ) , r( ) ∩ {z∗ ∈ Zt+1} ∩ {Zt+1 ⊆ St+1}

where we recall that St = {z ∈ Z : ∆(z) ≤ 2−t}. The good event characterizes the worst-case sample complexity of the t-th phase of Algorithm 1 and guarantees that the set of active arms at the end of the phase contains the optimal arm and is contained in the set of arms with gaps below the threshold that is to be eliminated in the phase. Note that for t > log2(1/∆min) we have St = {z∗}.
The proof proceeds as follows. We begin by showing that the good event holds with probability at least 1 − δt in phase t given that the good event held in phase t − 1. We then show that the probability of the good event holding in every phase is at least 1 − δ. As a result, we simply sum over the bound on the sample complexity in each phase given in the good event to obtain the stated bound on the sample complexity.
The following lemma shows that good event holds in phase t with probability at least 1 − δt conditioned on the good event holding in phase t − 1.

Lemma 2. P(Et|Et−1, · · · , E1) ≥ 1 − δt.

Proof. Conditioned on a choice of Y(Zt), since θ is a least squares estimator of θ∗ and the

noise is i.i.d., we know that y (θ∗ − θt) is y 2 −1 -subGaussian for all y ∈ Y(Zt). Further-
At

more, due to the guarantees of the rounding procedure,

y

2
−1

≤

(1 +

)ρ(Y(Zt))/Nt ≤

At

8(2t+1)2 log(|Z|2/δt) −1 for all y ∈ Y(Zt) by our choice of Nt. Since the right-hand side is

deterministic, independent of Y(Zt), for any ν > 0, we have that

P |y (θ∗ − θ)| >

2 log(2/ν) 8(2t+1)2 log(|Z|2/δt) Et−1, · · · , E1 ≤ ν

for any y ∈ Y(Zt). Taking ν = 2δt/|Z|2 and union bounding over all the possible y ∈ Y(Zt) where |Y(Zt)| ≤ |Y(Z)| ≤ |Z|2/2, gives us that

P(∃y ∈ Y(Zt) |y (θ∗ − θ)| > 2−(t+2)|Et−1, · · · , E1) ≤ δt.

(5)

Claim 1: Every arm z ∈ Zt such that ∆(z) ≥ 2−(t+1) is discarded in phase t so that
Zt+1 ⊆ St+1 with probability at least 1 − δt. Proof. Since we conditioned on Et−1 , z∗ ∈ Zt. If z ∈ Stc+1 ∩ Zt then by deﬁnition
∆(z) = (z∗ − z) θ∗ > 2−(t+1). Taking y = z∗ − z by the conﬁdence bound (5)

y (θ∗ − θt) ≤ 2−(t+2) ⇒ y θt ≥ y θ∗ − 2−(t+2) > 2−(t+1) − 2−(t+2) = 2−(t+2).

However, this is precisely the discard condition of the algorithm guaranteeing z will be eliminated.
We now show that the optimal arm cannot be discarded in a phase with high probability.

Claim 2: z∗ ∈ Zt+1 with probability at least 1 − δt.

14

Proof. We prove this claim by contradiction. To begin, observe that z∗ is in Zt since Et−1 holds. Now, suppose that z∗ is discarded in phase t. This implies that there exists a z = z∗ for z ∈ Zt such that 2−(t+2) ≤ (z − z∗) θt. However from the conﬁdence interval (5), (z − z∗) (θt − θ∗) ≤ 2−(t+2). Combining these we see that (z − z∗) (θt − θ∗) < (z − z∗) θt which implies (z − z∗) θ∗ > 0 which is a contradiction.
We complete the proof by showing that the sample complexity of phase t given in the
good event holds with probability 1 − δt. Since Et−1 is given, Zt ⊆ St, which implies with probability at least 1 − δt,

Nt = max ≤ max

8(2t+1)2ρ(Y(Zt))(1 + ε) log(|Z|2/δt) , r( ) 8(2t+1)2ρ(Y(St))(1 + ε) log(|Z|2/δt) , r( )

where we note that the quantity on the right hand side is deterministic.

Lemma 3. P(E1 ∩ · · · ∩ E log2(1/∆min) ) ≥ 1 − δ.
Proof. Let us ﬁrst expand the intersection of the events into a product of conditional probabilities as follows:

P(E1 ∩ · · · ∩ E log2(1/∆min) ) = Πt=lo1g2(1/∆min) P(Et|Et−1 ∩ · · · ∩ E1)
We now obtain a lower bound on the success probability using Lemma 2 and facts about inﬁnite products:

Πt=lo1g2(1/∆min) P(Et|Et−1 ∩ · · · ∩ E1) ≥ Πt=lo1g2(1/∆min) (1 − δt) ≥ Π∞ t=1 1 − tδ2

sin(πδ)

=

.

πδ

Finally,

using

the

fact

that

sin(πδ) πδ

≥

1−δ

for

δ

∈

(0, 1),

we

obtain

the

result

P(E1

∩···∩

E log2(1/∆min) ) ≥ 1 − δ.

The ﬁnal result then follows immediately from Lemmas 2 and 3 since we can now sum the number of samples taken in each phase to get the sample complexity. With probability at least 1 − δ,

log2 (1/∆min )

N≤

max

8(2t+1)2ρ(Y(St))(1 + ) log(t2|Z|2/δ) , r( )

t=1

≤ 128ψ∗(1 + ) log(1/∆min) log(log2(1/∆min)2|Z|2/δ) + (1 + r( )) log2(1/∆min).

15

Recall that Y∗(S) = {z∗ − z : ∀ z ∈ S \ z∗}. To see the second inequality, note that

ψ∗ = min max

y2
( x∈X λxxx )−1

λ∈ X y∈Y∗(Z)

∆(y)2

= min

max

max

y2
( x∈X λxxx )−1

λ∈ X t≤ log2(1/∆min) y∈Y∗(St)

∆(y)2

≥ min max

max

y2
( x∈X λxxx )−1

λ∈ X t≤ log(1/∆min) y∈Y∗(St)

(2−t)2

(i)

1

≥

min log2(1/∆min) max

y2
( x∈X λxxx )−1

log2(1/∆min) λ∈ X

t=1

y∈Y ∗ (St )

(2−t)2

(ii)

1

≥

log2 (1/∆min )

(2t)2 min max

y

2 (

λ xx )−1

log2(1/∆min)

t=1

λ∈ X y∈Y∗(St)

x∈X x

(iii)

1

≥

log2 (1/∆min )

(2t)2 min max

y

2 (

λ xx )−1

4 log2(1/∆min)

t=1

λ∈ X y∈Y(St)

x∈X x

log(1/∆min )

1 =

(2t )2 ρ(Y (St ))

4 log2(1/∆min) i=1

where (i) follows from the fact that the maximum of positive numbers is always less than
the average, and (ii) by the fact that the minimum of a sum is greater than the sum of
minimums. To see (iii), note that for y ∈ Y(St), if y = zi − zj, then y = (zi − z∗) − (z∗ − zj). Hence maxy∈Y(St) y 2( x∈X λxxx )−1 ≤ 4 maxy∈Y∗(St) y 2( x∈X λxxx )−1 .

B Eﬃcient Rounding Procedures

Throughout the following we assume that Y ⊂ Rd is arbitrary and that X = {x1, · · · , xn} ⊂ Rd is a subset with dim span(X ) = d.

Deﬁnition 2. A rounding procedure is an algorithm that takes as input λ ∈ n, a set of

vectors X , and a number of samples N and returns a ﬁnite allocation s = (s1, · · · , sn) ∈ Nn

satisfying the following properties: 1.

n i=1

si

=

N;

2.

there

exists

a

function

r(

)

such

that

if N > r( ), then maxy∈Y y 2( ni=1 sixixi )−1 ≤ (1 + ) maxy∈Y y 2( ni=1 λixixi )−1 /N .

Fortunately, there has been extensive work on eﬃcient rounding procedures, motivated
by the strong connection to G-optimal design in optimal linear experimental design [25].
Here we discuss two important rounding procedures. The ﬁrst is due to [25] and has an r( ) = d2/ where the d2 arises from the support size of λ.
Rounding Procedure of [25]. An eﬃcient rounding procedure is given in Chapter 12 of [25] to transform a design λ ∈ n into a discrete allocation s ∈ Nn for any ﬁxed number of samples N . The rounding procedure determines the number of pulls Ni to allocate to each arm xi in the support of λ such that i≤p Ni = N where p is the cardinality of the support of λ. The discrete allocation from the rounding procedure is obtained in two phases:

1. Given the number of samples N to obtain and the cardinality of the support of λ,

samples to allocate to arms in the support of λ are computed using Ni =

(N

−

1 2

p)λi

,

where N1, N2, . . . , Np are positive integers constrained such that i≤p Ni ≥ N .

16

2. Following the previous phase of the rounding procedure, loop until the discrepancy

( i≤p Ni) − N = 0, from either decreasing a sample count Nj which obtains (Nj − 1)/λj = mini≤p(Ni − 1)/λi to Nj + 1, or increasing a sample count Nj which obtains

Nj /λj = maxi≤p Ni/λi to Nj − 1. The eﬃcient design apportionment theorem in Section 12.5 of [25] provides the foundation

the procedure; details on the characterization of eﬃciency are provided in Chapter 12 of the

same reference.

Rounding Procedure of [2]. We refer the reader to Algorithm 1 in [2] for details

about their rounding procedure. Here we describe their result and how to modify it to our

setting. Let Sb,k = {s ∈ [b]n :

n i=1

si

≤

k}.

n i=1

si

≤

k}

and

a

continuous

relaxation

Cb,k

=

{s

∈

[0,

b]n

:

Theorem 3 (Theorem 2.1 of [2]). Suppose ∈ (0, 1/3], n ≥ k ≥ 180d/ 2, b ∈ [k]. Let
π ∈ Cb,k, then in polynomial-time (in n and d) we can round π to an integral solution s ∈ Sb,k satisfying maxy∈Y y 2( sixixi )−1 ≤ (1 + ) maxy∈Y y 2( πixixi )−1 .

To apply this theorem to obtain an eﬃcient rounding procedure, consider the following.
Given a λ ∈ X , and a number of samples N , let π = N λ and consider the case where b = k = N . Then kλ ∈ Ck,k. In general the theorem does not allow N = k > n, but we can circumvent this by just duplicating each vector in X exactly N times. Then the allocation s obtained will satisfy the conditions of the above with r( ) = 180d/ 2. The authors remark that it is most likely true that r( ) = d/ 2 suﬃces, but we are not aware of any such result
in the literature.

C Proof of Theorem 1

Proof. In this section we assume X = {x1, · · · , xn} and Z = {z1, · · · , zm}. Without loss of generality, we assume that z1 = argmaxzi∈Z zi θ∗. Let C := {θ ∈ Rd : ∃i s.t. θ (z1 −zi) ≤ 0}, i.e. θ ∈ C if and only if z1 is not the best arm in the linear bandit instance (X , Z, θ).
We now recall the transportation lemma of [20]. Under a δ-PAC strategy for ﬁnding the best arm for the bandit instance (X , Z, θ∗), let Ti denote the random variable which is the
number of times arm i is pulled. In addition let νθ,i denote the reward distribution of the
i-th arm of Z, i.e. νθ,i = N (zi θ, 1). Then for any θ ∈ C we have that

n
E[Ti]KL(νθ∗,i, νθ,i) ≥ log(1/2.4δ).
i=1

In particular,

n i=1

E[Ti]

≥

the optimization problem,

n i=1

ti

for

any

t

:=

(t1, · · ·

, tn)

which

is

a

feasible

solution

of

n
min ti
i=1

n
subject to min tiKL(νθ∗,i||νθ,i) ≥ log(1/2.4δ).
θ∈C i=1

Taking t∗ to be an optimal solution to the previous problem, note that

n
min
θ∈C i=1

t∗i

log(1/2.4δ) log(1/2.4δ)

nj=1 t∗j KL(νθ∗,i||νθ,i) ≥

nj=1 t∗j ≥

n j=1

E[Tj

]

17

In particular, since

n

t∗i
n

= 1, we see that

i=1 j=1 t∗j

n

log(1/2.4δ)

max min λiKL(νθ∗,i||νθ,i) ≥ n

.

λ∈ n θ∈C i=1

i=1 E[Ti]

Rearranging, we see that

n

1

E[Ti] ≥ log(1/2.4δ) min max n

.

(6)

i=1

λ∈ n θ∈C i=1 λiKL(νθ∗,i||νθ,i)

Now for j = 1, λ ∈ n and > 0, deﬁne

∗ (yj θ∗ + )A(λ)−1yj θj( , λ) = θ − y A(λ)−1y .

j

j

where A(λ) :=

n i=1

λixixi

and yj = z1 − zj. Note that yj θj( , λ) = −

that θj ∈ C. Also, the KL-divergence is given by

< 0 which implies

KL(νθ∗,i||νθj( ,λ),i) = (xi (θ∗ − θj ( , λ)))2

−1 (yj θ∗ + )2xixi

−1

= yj A(λ) (yj A(λ)−1yj)2 A(λ) yj.

Hence, returning to (6), we have that

n
E[Ti] ≥ log(1/2.4δ) min max
λ∈ n θ∈C i=1

1

n i=1

λi

K

L(νθ∗

||νθ

)

≥ log(1/2.4δ) min max
λ∈ n j=2,··· ,m

1

n i=1

λi K L(νθ∗ ,i ||νθj (

,λ),i)

(yj A(λ)−1yj )2 ≥ log(1/2.4δ) λm∈inn j=m2,a···x,m (yj θ∗ + )2yj A(λ)−1( ni=1 λixixi )A(λ)−1yj

yj A(λ)−1yj = log(1/2.4δ) min max
λ∈ n y∈Y∗(Z) (yj θ∗ + )2

where in the second to last line we used the fact that

n i=1

λixixi

= A(λ). Letting

→0

establishes the result.

Remark:

Note that θj = argminθ∈Rd

θ − θ∗

2 A(λ)

subject

to

yj

θ

=

−

.

D Proof of Proposition 1

Proof. Assume d is even and each t ∼ N (0, 1). Fix some α ∈ (0, 1) which will depend on γ
in a clear way momentarily, and consider an instance where X = Z = {ei}di=/21 ∪ {cos(α)ei + sin(α)ed/2+i}di=/21} where ei is the i-th standard basis vector.
If an algorithm is δ-PAC, and takes Ni samples from arm i, then for any j ≤ d/2 it will

be able to distinguish between θ = zj and θ = zj+d/2. By standard Le Cam arguments

[30]

this

hypothesis

test

requires

Nj + Nj+d/2

≥

c log(1/δ) (1−cos(α))2

for

some

universal

constant

c > 0. Because (1 − cos(α))2 ≈ α4/4 and these inequalities must hold for all j = 1, . . . , d/2

simultaneously for the single static allocation, we obtain the result.

18

E Proof of Lemma 1

Proof.

ρ(Y) =

min

max

y

2 (

λ xx )−1

λ∈ |X | y∈Y

x∈X x

1 =

min max yγY 2

γY2 λ∈ |X | y∈Y

( x∈X λxxx )−1

1 ≤ γY2 λ∈mi|nX| x∈conmv(aXx∪−X )

x

2 (

x∈X λxxx )−1

1 =

min max x 2

γY2 λ∈ |X | x∈X

( x∈X λxxx )−1

The third equality follows from the fact that the maximum value of a convex function on a
convex set must occur at a vertex. By the celebrated Kiefer-Wolfowitz theorem for G-optimal design [25], minλ∈ |X| maxx∈X x 2( λxxx )−1 = d so we see that ρ(Y) ≤ d/γY2 . For a lower bound, note that

min max

y

2 (

λ xx )−1 ≥ min max σmin((

λxxx

)−1) y

2 2

λ∈ X y∈Y

x∈X x

λ∈ X y∈Y

x∈X

= min max y 22/σmax(( λxxx )−1)
λ∈ X y∈Y x∈X

where σmax and σmin are respectively the largest and smallest eigenvalue operators of a matrix.

Since σmax(

n i=1

λixixi

)

≤

maxx∈X

x 2, we have that ρ(Y(St)) ≥ maxy∈Y(St)

y 22/(maxx∈X

The ﬁnal statement in the case of a singleton is also known as Elfving’s Theorem, see Section

2.14 in [25]

x 2).

F Experiment Details
In this section, we provide further details on the implementation of each algorithm. Each experiment was repeated 20 times with the mean sample complexity is reported and error bars representing the standard error are plotted. Noise in the observations was generated from a standard normal distribution as described in the text. Simulations were implemented in Python 3 and parallelized on an Intel(R) Xeon(R) CPU E5-2690.
For each algorithm that requires computing a design λ from an optimization of the form minλ∈ X maxs∈S s 2( x λxxx )−1 for S ⊂ Rd, i.e. RAGE, XY-static, XY-oracle, and ALBA, we used a Franke-Wolfe algorithm [16] with constant step-size 2/(k + 2) (k being the iteration counter). The algorithm was run until the relative change in λ with respect to the 2 norm was less than .01 or 1000 iterations were reached. Any values of λ < 10−5 were then thresholded to 0 and λ was scaled to sum to 1. • X Y-Adaptive [28]: This algorithm requires a parameter α that governs the length of each
adaptive phase. We follow the simulations in [28] and let α = 0.1. We remark that the algorithm given in the paper implements a greedy update to select arms in contrast to rounding the optimal allocation as is considered in the analysis. We implement the greedy arm selection procedure to match the simulations in the paper. It is worth noting that in several of the recent linear bandit papers that have implemented this algorithm, the active arm set has been reset at the conclusion of a phase before discarding arms. We do not

19

reset the arm set at the conclusion of a phase to match what was done in [28]. Finally, in the conﬁdence interval, we include the phase index and not the number of samples since we only need to union bound over when it is evaluated. • X Y-Static and X Y-Oracle: To implement each allocation, we compute the optimal design on the set Y(Z) for the static strategy and the set Y∗(Z) normalized by the gaps for the oracle. Each algorithm proceeds in phases drawing vt samples from the allocation and pulling the selected arms, where v was optimized in the range (1, 2) for performance, and the stopping condition as discussed in Section 2.2 is evaluated at the end of each phase t. • LinGapE [31]: We run this algorithm with a regularizer on the least squares estimator of λ = 1 following the implementation given in the paper. LinGapE is designed to ﬁnd an ε good arm. We let ε = 0 to ensure the optimal arm is identiﬁed. We note that in the code the authors of [31] provided with their paper, ε was set to the minimum gap. Since this value is unknown a priori, we do not follow this approach and as a result our simulations may not match with the simulations in [31] for identical problem instances. Moreover, the simulations in the paper apply a greedy arm selection strategy that deviates from the algorithm that is analyzed. We instead implement the LinGapE algorithm in the form that it is analyzed. • ALBA [29]: This algorithm is parameter free and we implement the Y-ElimTil subprocedure following the paper since it gives improved empirical results compared to the X -ElimTil sub-procedure that provides identical theoretical results. • RAGE: To compute the discrete allocation given a design, we use the rounding procedure discussed in Section B from [25]. For the Benchmark example, and uniform points on a sphere we used = 1/5, for the many arms example, we used = 1/50 and for the case of uniform points on a sphere, we used = 1/3.5. The algorithm we propose is computationally eﬃcient since there is at most log2(1/∆min) phases and each phase only requires solving a convex optimization to obtain the design, an eﬃcient rounding procedure, and solving a least squares problem. The time required between each pull is negligible.
20

