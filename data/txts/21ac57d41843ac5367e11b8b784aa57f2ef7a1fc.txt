arXiv:2106.05958v1 [math.OC] 10 Jun 2021

Near-Optimal High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise
Eduard Gorbunov1,2 Marina Danilova3 Innokentiy Shibaev1,2 Pavel Dvurechensky4,2,5 Alexander Gasnikov1,2,4
1 Moscow Institute of Physics and Technology, Russian Federation 2 National Research University Higher School of Economics, Russian Federation
3 Institute of Control Sciences RAS, Russian Federation 4 Weierstrass Institute for Applied Analysis and Stochastics, Germany 5 Institute for Information Transmission Problems RAS Russian Federation
Abstract
Thanks to their practical eﬃciency and random nature of the data, stochastic ﬁrst-order methods are standard for training large-scale machine learning models. Random behavior may cause a particular run of an algorithm to result in a highly suboptimal objective value, whereas theoretical guarantees are usually proved for the expectation of the objective value. Thus, it is essential to theoretically guarantee that algorithms provide small objective residual with high probability. Existing methods for non-smooth stochastic convex optimization have complexity bounds with the dependence on the conﬁdence level that is either negative-power or logarithmic but under an additional assumption of sub-Gaussian (light-tailed) noise distribution that may not hold in practice, e.g., in several NLP tasks. In our paper, we resolve this issue and derive the ﬁrst high-probability convergence results with logarithmic dependence on the conﬁdence level for non-smooth convex stochastic optimization problems with non-sub-Gaussian (heavy-tailed) noise. To derive our results, we propose novel stepsize rules for two stochastic methods with gradient clipping. Moreover, our analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, we provide an extension for strongly convex problems. Finally, our results imply that the ﬁrst (accelerated) method we consider also has optimal iteration and oracle complexity in all the regimes, and the second one is optimal in the non-smooth setting.
1 Introduction
Stochastic ﬁrst-order optimization methods like SGD [33], Adam [21], and their various modiﬁcations are extremely popular in solving a number of diﬀerent optimization problems, especially those appearing in statistics [37], machine learning, and deep learning [14]. The success of these methods in real-world applications motivates the researchers to investigate theoretical properties for the methods and to develop new ones with better convergence guarantees. Typically, stochastic methods are analyzed in terms of the convergence in expectation (see [13, 25, 16] and references therein), whereas high-probability complexity results are established much rarely. However, as illustrated in [15], guarantees in terms of the convergence in expectation have much worse correlation with the real behavior of the methods than high-probability convergence guarantees when the noise in the stochastic gradients has heavy-tailed distribution.
1

Recent studies [36, 35, 42] show that in several popular problems such as training BERT [38] on Wikipedia dataset the noise in the stochastic gradients is heavy-tailed. Moreover, in [42], the authors justify empirically that in such cases SGD works signiﬁcantly worse than clipped-SGD [31] and Adam. Therefore, it is important to theoretically study the methods’ convergence when the noise is heavy-tailed.
For convex and strongly convex problems with Lipschitz continuous gradient, i.e., smooth convex and strongly convex problems, this question was properly addressed in [26, 3, 15] where the ﬁrst high-probability complexity bounds with logarithmic dependence on the conﬁdence level were derived for the stochastic problems with heavy-tailed noise. However, a number of practically important problems are non-smooth on the whole space [41, 23]. For example, in deep neural network training, the loss function often grows polynomially fast when the norm of the network’s weights goes to inﬁnity. Moreover, non-smoothness of the activation functions such as ReLU or loss functions such as hinge loss implies the non-smoothness of the whole problem. While being well-motivated by practical applications, the existing high-probability convergence guarantees for stochastic ﬁrst-order methods applied to solve non-smooth convex optimization problems with heavy-tailed noise depend on the negative power of the conﬁdence level that dramatically increases the number of iterations required to obtain high accuracy of the solution with probability close to one. Such a discrepancy in the theory between algorithms for stochastic smooth and non-smooth problems leads us to the natural question: is it possible to obtain high-probability complexity bounds with logarithmic dependence on the conﬁdence level for non-smooth convex stochastic problems with heavy-tailed noise? In this paper, we give a positive answer to this question. To achieve this we focus on gradient clipping methods [31, 11, 24, 23, 41, 42].

1.1 Preliminaries
Before we describe our contributions in detail, we formally state the considered setup.

Stochastic optimization. We focus on the following problem

min f (x), f (x) = Eξ [f (x, ξ)] ,

(1)

x∈Rn

where f (x) is a convex but possibly non-smooth function. Next, we assume that at each point

x ∈ Rn we have an access to the unbiased estimator ∇f (x, ξ) of ∇f (x) with uniformly bounded

variance

Eξ[∇f (x, ξ)] = ∇f (x),

Eξ

∇f (x, ξ) − ∇f (x)

2 2

≤ σ2,

σ > 0.

(2)

This assumption on the stochastic oracle is widely used in stochastic optimization literature [12, 13, 20, 22, 27]. We emphasize that we do not assume that the stochastic gradients have so-called “light tails” [22], i.e., sub-Gaussian noise distribution meaning that P{ ∇f (x, ξ) − ∇f (x) 2 > b} ≤ 2 exp(−b2/(2σ2)) for all b > 0.

Level of smoothness. Finally, we assume that function f has (ν, Mν)-Hölder continuous gradients on a compact set Q ⊆ Rn for some ν ∈ [0, 1], Mν > 0 meaning that

∇f (x) − ∇f (y)

2 ≤ Mν

x−y

ν 2

∀x, y ∈ Q.

(3)

2

When ν = 1 inequality (3) implies M1-smoothness of f , and when ν = 0 we have that ∇f (x) has bounded variation which is equivalent to being uniformly bounded. Moreover, when ν = 0
diﬀerentiability of f is not needed, and one can assume uniform boundedness of the subgradients
of f . Linear regression in the case when the noise has generalized Gaussian distribution (Example
4.4 from [2]) serves as a natural example of the situation with ν ∈ (0, 1). Moreover, when (3) holds for ν = 0 and ν = 1 simultaneously then it holds for all ν ∈ [0, 1] with Mν ≤ M01−νM1ν [29]. As we show in our results, the set Q should contain the ball centered at the solution x∗ of (1) with radius ∼ R0 ≥ x0 − x∗ 2, where x0 is a starting point of the method, i.e., our analysis does not require (3) to hold on Rn.

High-probability convergence. For a given accuracy ε > 0 and conﬁdence level β ∈ (0, 1) we are interested in ﬁnding ε-solutions of problem (1) with probability at least 1 − β, i.e., such x that P{f (x) − f (x∗) ≤ ε} ≥ 1 − β. For brevity, we will call such (in general, random) points x as (ε, β)-solution of (1). Moreover, by high-probability complexity of a stochastic method M we mean the suﬃcient number of oracle calls, i.e., number of ∇f (x, ξ) computations, needed to guarantee that the output of M is an (ε, β)-solution of (1).

1.2 Contributions

• We propose novel stepsize rules for clipped-SSTM [15] to handle the problems with Hölder continuous gradients and derive high-probability complexity guarantees for convex stochastic optimization problems without using “light tails” assumption, i.e., we prove that our version of clipped-SSTM

2(1+ν) D σ2R02 D

O

max

D ln 1+3ν

, β

ε2

ln β

2

2(1+ν)

,

D

=

M 1+3ν
ν

R 1+3ν
0

2

ε 1+3ν

high-probability complexity. Unlike all previous high-probability complexity results in this setup with ν < 1 (see Tbl. 1), our result depends only logarithmically on the conﬁdence level β that is highly important when β is small. Moreover, up to the diﬀerence in logarithmic factors the derived complexity guarantees meet the known lower bounds [22, 18] obtained for the problems with light-tailed noise. In particular, when ν = 1 we recover accelerated convergence rate [30, 22]. That is, neglecting the logarithmic factors our results are unimprovable and, surprisingly coincide with the best-known results in the “light-tailed case”.

• We derive the ﬁrst high-probability complexity bounds for clipped-SGD when the objective functions is convex with (ν, Mν)-Hölder continuous gradient and the noise is heavy tailed., i.e., we derive

2

1+ν σ2R02

D2 + D1+ν

O max D , max D , ε2 ln β

1

Mν1+ν R0

, D=

1

ε 1+ν

high-probability complexity bound. Interestingly, when ν = 0 the derived bound for clipped-SGD has better dependence on the logarithms than the corresponding one for clipped-SSTM. Moreover, neglecting the dependence on ε under the logarithm, our bound for clipped-SGD has the same dependence on the conﬁdence level as the tightest known result in this case under the “light tails” assumption [17].

3

Table 1: Summary of known and new high-probability complexity bounds for solving (1) with

f being convex and having (ν, Mν)-Hölder continuous gradients. Columns: “Ref.” = reference,

“Complexity” = high-probability complexity (ε – accuracy, β – conﬁdence level, numerical constants

and logarithmic factors are omitted), “HT” = heavy-tailed noise, “UD” = unbounded domain, “HCC”

= Hölder continuity of the gradient is required only on the compact set. The results labeled by ♣ are

obtained from the convergence guarantees in expectation via Markov’s inequality. Negative-power

dependencies on the conﬁdence level β are colored in red.

Method

Ref.

Complexity

ν HT? UD? HCC?

SGD

[27]

max

, M02 R02
ε2

σ 2 R02 ε2

0







AC-SA

[12, 22]

max

, M1 R02
ε

σ 2 R02 ε2

1







SIGMA

2

2(1+ν )

[6]

max Mν1+3ν R01+3ν , σ2R02

[0, 1] 





2
ε 1+3ν

ε2

SGD

[27]♣

max

, M02 R02
β 2 ε2

σ 2 R02 β 2 ε2

0







AC-SA

[12, 22]♣

max

, M1 R02
βε

σ 2 R02 β 2 ε2

1







SIGMA

2

2(1+ν )

[6]♣

max Mν1+3ν R01+3ν , σ2R02

[0, 1] 





2

2

β 1+3ν ε 1+3ν

β 2 ε2

clipped-SSTM [15]

max

, M1 R02
ε

σ 2 R02 ε2

1







clipped-SGD

[15]

max

, M1 R02
ε

σ 2 R02 ε2

1







2

2(1+ν )

clipped-SSTM Thm. 2.2 max Mν1+3ν R01+3ν , σ2R02

[0, 1] 





2
ε 1+3ν

ε2

clipped-SGD Thm. 3.1

max

2

, Mν1+ν R02 2 ε 1+ν

σ 2 R02 ε2

[0, 1]  



• Using restarts technique we extend the obtained results for clipped-SSTM and clipped-SGD to the strongly convex case (see Tbl. 2). As in the convex case, the obtained results are superior to all previous known results in the general setup we consider.
• As one of the key contributions of this work, we emphasize that in our theoretical results it is suﬃcient to assume Hölder continuity of the gradients of f only on the ball with radius 2R0 = 2 x0 − x∗ 2 and centered at a solution of the problem. This makes our results applicable to much larger class of problems than functions with Hölder continuous gradients on Rn, e.g., our analysis works even for polynomially growing objectives.
• To test the performance of the considered methods we conduct several numerical experiments on image classiﬁcation and NLP tasks, and observe that 1) clipped-SSTM and clipped-SGD show a comparable performance with SGD on the image classiﬁcation task, when the noise distribution is almost sub-Gaussian, 2) converge much faster than SGD on the NLP task, when the noise distribution is heavy-tailed, and 3) clipped-SSTM achieves a comparable performance with Adam on the NLP task enjoying both the best known theoretical guarantees and good practical performance.

4

Table 2: Summary of known and new high-probability complexity bounds for solving (1) with f

being µ-strongly convex and having (ν, Mν)-Hölder continuous gradients. Columns: “Ref.” = reference, “Complexity” = high-probability complexity (ε – accuracy, β – conﬁdence level, numerical

constants and logarithmic factors are omitted), “HT” = heavy-tailed noise, “UD” = unbounded

domain, “HCC” = Hölder continuity of the gradient is required only on the compact set. The results labeled by ♣ are obtained from the convergence guarantees in expectation via Markov’s inequality.

Negative-power dependencies on the conﬁdence level β are colored in red.

Method

Ref.

Complexity

ν HT? UD? HCC?

SGD

[27]

max M02 , σ2
µε µε

0







AC-SA

[12, 22]

max

Mµ1 , σµ2ε

1







SIGMA SGD

max Nˆ, σµ2ε ,

[6]

2

2 1 [0, 1] 





Nˆ =

+ Mν
µR1−ν

1+3ν

Mν µ1+ν ε1−ν

1+3ν

0

[27]♣

max M02 , σ2
µβε µβε

0







AC-SA

[12, 22]♣

max

Mµ1 , µσβ2ε

1







SIGMA

max Nˆ, σ2 , εˆ = βε,

[6]♣

µεˆ

2

1 [0, 1] 





Nˆ =

Mν µR1−ν

+ 1+3ν

Mν2 µ1+ν εˆ1−ν

1+3ν

0

R-clipped-SSTM [15]

max

, M1 σ2
µ µε2

1







R-clipped-SGD

[15]

max Mµ1 , µσε22

1







max Nˆ, σµ2ε ,

R-clipped-SSTM Thm. 2.1

2

2 1 [0, 1] 





Nˆ =

+ Mν
µR1−ν

1+3ν

Mν µ1+ν ε1−ν

1+3ν

0

2

2

R-clipped-SGD Thm. 3.2 max 2Mν1+2(ν1−ν) , Mν11− +νν , σµ2ε [0, 1]   
µ 1+ν R0 1+ν µε 1+ν

1.3 Related work
Light-tailed noise. The theory of high-probability complexity bounds for convex stochastic optimization with light-tailed noise is well-developed. Lower bounds and optimal methods for the problems with (ν, Mν)-Hölder continuous gradients are obtained in [27] for ν = 0, and in [12] for ν = 1. Up to the logarithmic dependencies these high-probability convergence bounds coincide with the corresponding results for the convergence in expectation (see ﬁrst two rows of Tbl. 1) While not being directly derived in the literature, the lower bound for the case when ν ∈ (0, 1) can be obtained as a combination of lower bounds in the deterministic [28, 18] and smooth stochastic settings [12]. The corresponding optimal methods are analyzed in [4, 6] through the lens of inexact oracle.
Heavy-tailed noise. Unlike in the “light-tailed” case, the ﬁrst theoretical guarantees with reasonable dependence on both the accuracy ε and the conﬁdence level β appeared just recently. In [26], the ﬁrst such results without acceleration [30] were derived for Mirror Descent with special truncation technique for smooth (ν = 1) convex problems on a bounded domain, and then were accelerated and extended in [15]. For the strongly convex problems the ﬁrst accelerated high-probability convergence

5

guarantees were obtained in [3] for the special method called proxBoost requiring solving auxiliary

nontrivial problems at each iteration. These bounds were tightened in [15].

In contrast, for the case when ν < 1 and, in particular, when ν = 0 the best-known high-

probability complexity bounds suﬀer from the negative-power dependence on the conﬁdence level β,

i.e., have a factor 1/βα for some α > 0, that aﬀects the convergence rate dramatically for small enough

β. Without additional assumptions on the tails these results are obtained via Markov’s inequality

P{f (x) − f (x∗) > ε} < E[f(x)−f(x∗)]/ε from the guarantees for the convergence in expectation to the

accuracy εβ, see the results labeled by ♣ in Tbl. 1. Under an additional assumption on noise tails

that

P{

∇f (x, ξ) − ∇f (x)

2 2

> sσ2} = O(s−α)

for

α>2

these

results

can

be

tightened

[10]

when

ν = 0 as ∼ max ln(β−1)/ε2, (1/βεα)2/(3α−2) without removing the negative-power dependence on the

conﬁdence

level

β.

Diﬀerent

stepsize

policies

allow

to

change

the

last

term

in

max

to

β

−

1 2α−1

ε−

2α 2α−1

without removing the negative-power dependence on β.

Gradient clipping. The methods based on gradient clipping [31] and normalization [19] are popular in diﬀerent machine learning and deep learning tasks due to their robustness in practice to the noise in the stochastic gradients and rapid changes of the objective function [14]. In [41, 23], clipped-GD and clipped-SGD are theoretically studied in applications to non-smooth problems that can grow polynomially fast when x − x∗ 2 → ∞ showing the superiority of gradient clipping methods to the methods without clipping. The results from [41] are obtained for non-convex problems with almost surely bounded noise, and in [23], the authors derive the stability and expectation convergence guarantees for strongly convex under assumption that the central p-th moment of the stochastic gradient is bounded for p ≥ 2. Since the authors of [23] do not provide convergence guarantees with explicit dependencies on all important parameters of the problem it complicates direct comparison with our results. Nevertheless, convergence guarantees from [23] are sub-linear and are given for the convergence in expectation, and, as a consequence, the corresponding high-probability convergence results obtained via Markov’s inequality also suﬀer from negative-power dependence on the conﬁdence level. Next, in [42], the authors establish several expectation convergence guarantees for clipped-SGD and prove their optimality in the non-convex case under assumption that the central α-moment of the stochastic gradient is uniformly bounded, where α ∈ (1, 2]. It turns out that clipped-SGD is able to converge even when α < 2, whereas vanilla SGD can diverge in this setting.

2 Clipped Stochastic Similar Triangles Method

In this section, we propose a novel variation of Clipped Stochastic Similar Triangles Method [15] adjusted to the class of objectives with Hölder continuous gradients (clipped-SSTM, see Alg. 1).
The method is based on the clipping of the stochastic gradients:

clip(∇f (x, ξ), λ) = min 1, λ ∇f (x, ξ) (4) ∇f (x, ξ) 2

where

∇f (x, ξ)

=

1 m

m i=1

∇f

(x,

ξi

)

is

a

mini-batched

stochastic

gradient.

Gradient

clipping

ensures

that the resulting vector has a norm bounded by the clipping level λ. Since the clipped stochastic

gradient cannot have arbitrary large norm, the clipping helps to avoid unstable behavior of the

method when the noise is heavy-tailed and the clipping level λ is properly adjusted.

6

Algorithm 1 Clipped Stochastic Similar Triangles Method (clipped-SSTM): case ν ∈ [0, 1]

Input: starting point x0, number of iterations N , batchsizes {mk}Nk=1, stepsize parameter α, clipping parameter B, Hölder exponent ν ∈ [0, 1].
1: Set A0 = α0 = 0, y0 = z0 = x0

2: for k = 0, . . . , N − 1 do

3: Set αk+1 = α(k + 1) 12+νν , Ak+1 = Ak + αk+1, λk+1 = αkB+1

4:

xk+1 = / (Akyk+αk+1zk) Ak+1

5: Draw mini-batch mk of fresh i.i.d. samples ξ1k, . . . , ξmk k and compute ∇f (xk+1, ξk) = m1k mi=k1 ∇f (xk+1, ξik)

6: Compute ∇f (xk+1, ξk) = clip(∇f (xk+1, ξk), λk+1) using (4)

7: zk+1 = zk − αk+1∇f (xk+1, ξk)

8:

yk+1 = / (Akyk+αk+1zk+1) Ak+1

9: end for Output: yN

However, unlike the stochastic gradient, clipped stochastic gradient is a biased estimate of ∇f (x): the smaller the clipping level the larger the bias. The biasedness of the clipped stochastic gradient complicates the analysis of the method. On the other hand, to circumvent the negative eﬀect of the heavy-tailed noise on the high-probability convergence one should choose λ to be not too large. Therefore, the question on the appropriate choice of the clipping level is highly non-trivial.
Fortunately, there exists a simple but insightful observation that helps us to obtain the right formula for the clipping level λk in clipped-SSTM: if λk is chosen in such a way that ∇f (xk) 2 ≤ λk/2 with high probability, then for the realizations ∇f (xk+1, ξk) of the mini-batched stochastic gradient such that ∇f (xk+1, ξk) − ∇f (xk+1) 2 ≤ λk/2 the clipping is an identity operator. Next, if the probability mass of such realizations is big enough then the bias of the clipped stochastic gradient is properly bounded that helps to derive needed convergence guarantees. It turns out that the choice λk ∼ 1/αk ensures the method convergence with needed rate and high enough probability.
Guided by this observation we derive the precise expressions for all the parameters of clippedSSTM and derive high-probability complexity bounds for the method. Below we provide a simpliﬁed version of the main result for clipped-SSTM in the convex case. The complete formulation and the full proof of the theorem are deferred to Appendix B.1 (see Thm. B.1).

Theorem 2.1. Assume that function f is convex and its gradient satisfy (3) with ν ∈ [0, 1], Mν > 0

on Q = B3R0 = {x ∈ Rn | x − x∗ 2 ≤ 2R0}, where R0 ≥ x0 − x∗ 2. Then there exist such a choice

of parameters that clipped-SSTM achieves f (yN ) − f (x∗) ≤ ε with probability at least 1 − β after

O

2(1+ν)
D ln 1+3ν

D

β

2

2(1+ν)

iterations with D = Mν1+3ν R2 01+3ν and requires

ε 1+3ν

O max D ln 21(+1+3νν) D , σ2R02 ln D oracle calls.

(5)

β ε2 β

The obtained result has only logarithmic dependence on the conﬁdence level β and optimal dependence on the accuracy ε up to logarithmic factors [22, 18] for all ν ∈ [0, 1]. Moreover, we emphasize that our result does not require f to have (ν, Mν)-Hölder continuous gradient on the whole space. This is because we prove that for the proposed choice of parameters the iterates of

7

clipped-SSTM stay inside the ball B3R0 = {x ∈ Rn | x − x∗ 2 ≤ 3R0} with probability at least 1 − β, and, as a consequence, Hölder continuity of the gradient is required only inside this ball. In particular, this means that the better starting point leads not only to the reduction of R0, but also it can reduce Mν. Moreover, our result is applicable to much wider class of functions than the standard class of functions with Hölder continuous gradients in Rn, e.g., to the problems with polynomial growth.
For the strongly convex problems, we consider restarted version of Alg. 1 (R-clipped-SSTM, see Alg. 2) and derive high-probability complexity result for this version. Below we provide a simpliﬁed
Algorithm 2 Restarted clipped-SSTM (R-clipped-SSTM): case ν ∈ [0, 1]
Input: starting point x0, number of restarts τ , number of steps of clipped-SSTM in restarts {Nt}τt=1, batchsizes {m1k}Nk=1−1 1, {m2k}Nk=2−1 1, . . . , {mτk}Nk=τ 1−1, stepsize parameters {αt}τt=1, clipping parameters {Bt}τt=1, Hölder exponent ν ∈ [0, 1].
1: xˆ0 = x0
2: for t = 1, . . . , τ do 3: Run clipped-SSTM (Alg. 1) for Nt iterations with batchsizes {mtk}kN=t−1 1, stepsize parameter
αt, clipping parameter Bt, and starting point xˆt−1. Deﬁne the output of clipped-SSTM by xˆt. 4: end for Output: xˆτ

version of the result. The complete formulation and the full proof of the theorem are deferred to Appendix B.2 (see Thm. B.2).

Theorem 2.2. Assume that function f is µ-strongly convex and its gradient satisfy (3) with ν ∈ [0, 1], Mν > 0 on Q = B3R0 = {x ∈ Rn | x − x∗ 2 ≤ 3R0}, where R0 ≥ x0 − x∗ 2. Then there exist such a choice of parameters that R-clipped-SSTM achieves f (xˆτ ) − f (x∗) ≤ ε with probability at least
1 − β after

Nˆ = O

2(1+ν) D D ln 1+3ν

,

D = max

Mν 1+23ν ln µR02 ,

Mν2

1 1+3ν

(6)

β

µR01−ν

ε

µ1+ν ε1−ν

iterations of Alg. 1 in total and requires

O max D ln 21(+1+3νν) D , σ2 ln D oracle calls. (7) β µε β

Again, the obtained result has only logarithmic dependence on the conﬁdence level β and, as our result in the convex case, it has optimal dependence on the accuracy ε up to logarithmic factors depending on β [22, 18] for all ν ∈ [0, 1].

3 SGD with clipping
In this section, we present a new variant of clipped-SGD [31] properly adjusted to the class of objectives with (ν, Mν)-Hölder continuous gradients (see Alg. 3).
We emphasize that as for clipped-SSTM we use clipping level λ inversely proportional to the stepsize γ. Below we provide a simpliﬁed version of the main result for clipped-SGD in the convex case. The complete formulation and the full proof of the theorem are deferred to Appendix C.1 (see Thm. C.1).

8

Algorithm 3 Clipped Stochastic Gradient Descent (clipped-SGD): case ν ∈ [0, 1]

Input: starting point x0, number of iterations N , batchsize m, stepsize γ, clipping parameter B > 0.

1: for k = 0, . . . , N − 1 do

2:

Draw

mini-batch

of

m

fresh

i.i.d.

samples

ξ

k 1

,

.

.

.

,

ξ

k m

and

compute

∇f (xk+1, ξk)

=

1 m

m i=1

∇f

(xk+1

,

ξik

)

3: Compute ∇f (xk, ξk) = clip(∇f (xk, ξk), λ) using (4) with λ = B/γ

4: xk+1 = xk − γ∇f (xk, ξk)

5: end for Output: x¯N = N1

N −1 k=0

xk

Theorem 3.1. Assume that function f is convex and its gradient satisfy (3) with ν ∈ [0, 1], Mν > 0
on Q = B7R0 = {x ∈ Rn | x − x∗ 2 ≤ 7R0}, where R0 ≥ x0 − x∗ 2. Then there exist such a choice of parameters that clipped-SGD achieves f (x¯N ) − f (x∗) ≤ ε with probability at least 1 − β after

1

O max D2, D1+ν ln D2 + D1+ν , D = Mν1+ν R0

(8)

β

ε1 1+ν

iterations and requires

O max D2, max D1+ν , σ2R02 ln D2 + D1+ν oracle calls.

(9)

ε2

β

As all our results in the paper, this result for clipped-SGD has two important features: 1) the dependence on the conﬁdence level β is logarithmic and 2) Hölder continuity is required only on the ball B2R0 centered at the solution. Moreover, up to the diﬀerence in the expressions under the logarithm the dependence on ε in the result for clipped-SGD is the same as in the tightest known results for non-accelerated SGD-type methods [4, 17]. Finally, we emphasize that for ν < 1 the logarithmic factors appearing in the complexity bound for clipped-SSTM are worse than the corresponding factor in the complexity bound for clipped-SGD. Therefore, clipped-SGD has the best known high-probability complexity results in the case when ν = 0 and f is convex.
For the strongly convex problems, we consider restarted version of Alg. 3 (R-clipped-SGD, see Alg. 4) and derive high-probability complexity result for this version. Below we provide a simpliﬁed

Algorithm 4 Restarted clipped-SGD (R-clipped-SGD): case ν ∈ [0, 1]
Input: starting point x0, number of restarts τ , number of steps of clipped-SGD in restarts {Nt}τt=1, batchsizes {mt}τk=1, stepsizes {γt}τt=1, clipping parameters {Bt}τt=1
1: xˆ0 = x0
2: for t = 1, . . . , τ do 3: Run clipped-SGD (Alg. 3) for Nt iterations with batchsize mt, stepsize γt, clipping parameter
Bt, and starting point xˆt−1. Deﬁne the output of clipped-SGD by xˆt. 4: end for Output: xˆτ

version of the result. The complete formulation and the full proof of the theorem are deferred to Appendix C.2 (see Thm. C.2).

9

density density

0.4 ResNet-18 + ImageNet-100, batch count=~60k

Bert + CoLA, batch count=~96k

0.3

0.6

0.2

0.4

0.1

0.2

0.0 12 14 16 18 20 22

0.0

2

4

6

8

noise norm

noise norm

Figure 1: Noise distribution of the stochastic gradients for ResNet-18 on ImageNet-100 and BERT ﬁne-tuning on the CoLA dataset before the training. Red lines: probability density functions with means and variances empirically estimated by the samples. Batch count is the total number of samples used to build a histogram.

Theorem 3.2. Assume that function f is µ-strongly convex and its gradient satisfy (3) with ν ∈ [0, 1],
Mν > 0 on Q = B7R0 = {x ∈ Rn | x − x∗ 2 ≤ 7R0}, where R0 ≥ x0 − x∗ 2. Then there exist such a choice of parameters that R-clipped-SGD achieves f (x¯N ) − f (x∗) ≤ ε with probability at least 1 − β

after

2 1+ν

µR02

2 1+ν

µR02

D

O max D1 ln ε , D2 , max D1 ln ε , D2 ln β

iterations of Alg. 3 in total and requires

2 1+ν

µR02

2 1+ν

µR02

σ2 D

O max D1 ln ε , D2 , max D1 ln ε , D2, µε ln β

oracle calls, where

Mν D1 = µR1−ν ,
0

D2 = 1+Mν ν1−ν , µ2ε2

2 1+ν

µR02

2 1+ν

D = (D1 + D1) ln ε + D2 + D2 .

As in the convex case, for ν < 1 the log factors appearing in the complexity bound for R-clippedSSTM are worse than the corresponding factor in the bound for R-clipped-SGD. Thus, R-clipped-SGD has the best known high-probability complexity results for strongly convex f and ν = 0.

4 Numerical experiments
We tested the performance of the methods on the following problems:
• BERT ﬁne-tuning on CoLA dataset [39]. We use pretrained BERT from Transformers library [40] (bert-base-uncased) and freeze all layers except the last two linear ones.
• ResNet-18 training on ImageNet-100 (ﬁrst 100 classes of ImageNet [34]).
First, we study the noise distribution for both problem as follows: at the starting point we sample large enough number of batched stochastic gradients ∇f (x0, ξ1), . . . , ∇f (x0, ξK) with batchsize 32 and plot the histograms for ∇f (x0, ξ1) − ∇f (x0) 2, . . . , ∇f (x0, ξK ) − ∇f (x0) 2, see Fig. 1. As one can see, the noise distribution for BERT + CoLA is substantially non-sub-Gaussian, whereas the distribution for ResNet-18 + Imagenet-100 is almost Gaussian.
Next, we compared 4 diﬀerent optimizers on these problems: Adam, SGD (with Momentum), clipped-SGD (with Momentum and coordinate-wise clipping) and clipped-SSTM (with norm-clipping
10

and ν = 1). The results are presented in Fig. 2. We observed that the noise distributions do not change signiﬁcantly along the trajectories of the considered methods, see Appendix D. During the hyper-parameters search we compared diﬀerent batchsizes, emulated via gradient accumulation (thus we compare methods with diﬀerent batchsizes by the number of base batches used). The base batchsize was 32 for both problems, stepsizes and clipping levels were tuned. One can ﬁnd additional details regarding our experiments in Appendix D.

loss

2.0 1.8 1.6 1.4 1.2 1.0 0.8 0.2
0.70 0.65 0.60 0.55

Train loss, ResNet-18 Adam SGD clipped-SSTM clipped-SGD
0.4 b0a.6tch co0u.8nt 1.0 11.e25 Train loss, BERT Adam clipped-SSTM clipped-SGD SGD

0.0 0.5 ba1t.c0h co1un.5t 2.0 1e23.5

loss

loss

Validation loss, ResNet-18 1.8

1.6

1.4

1.2 0.4 0b.a6tch c0o.u8nt 1.0 11e.25

0.60 0.58 0.56 0.54 0.52
0.0

Validation loss, BERT 0.5 ba1t.0ch co1u.n5t 2.0 1e23.5

accuracy

accuracy

70 Validation accuracy, ResNet-18

65

60

55

75 74 73 72 71
0.0

0.4 0b.a6tch c0o.u8nt 1.0 11e.25 Validation accuracy, BERT
0.5 b1a.t0ch cou1n.5t 2.0 1e23.5

loss

Figure 2: Train and validation loss + accuracy for diﬀerent optimizers on both problems. Here, “batch count” denotes the total number of used stochastic gradients.

Image classiﬁcation. On ResNet-18 + ImageNet-100 task, SGD performs relatively well, and even ties with Adam (with batchsize of 4 × 32) in validation loss. clipped-SSTM (with batchsize of 2 × 32) also ties with Adam and clipped-SGD is not far from them. The results were averaged from 5 diﬀerent launches (with diﬀerent starting points/weight initializations). Since the noise distribution is almost Gaussian even vanilla SGD performs well, i.e., gradient clipping is not required. At the same time, the clipping does not slow down the convergence signiﬁcantly.
Text classiﬁcation. On BERT + CoLA task, when the noise distribution is heavy-tailed, the methods with clipping outperform SGD by a large margin. This result is in good correspondence with the derived high-probability complexity bounds for clipped-SGD, clipped-SSTM and the best-known ones for SGD. Moreover, clipped-SSTM (with batchsize of 8 × 32) achieves the same loss on validation as Adam, and has better accuracy. These results were averaged from 5 diﬀerent train-val splits and 20 launches (with diﬀerent starting points/weight initializations) for each of the splits, 100 launches in total.
Acknowledgments
The work of Marina Danilova in Sections 3 and C was funded by RFBR, project number 20-31-90073.

11

References
[1] George Bennett. Probability inequalities for the sum of independent random variables. Journal of the American Statistical Association, 57(297):33–45, 1962.
[2] Caroline Chaux, Patrick L Combettes, Jean-Christophe Pesquet, and Valérie R Wajs. A variational formulation for frame-based inverse problems. Inverse Problems, 23(4):1495–1518, jun 2007.
[3] Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, and Junyu Zhang. From low probability to high conﬁdence in stochastic convex optimization. Journal of Machine Learning Research, 22(49):1–38, 2021.
[4] Olivier Devolder. Exactness, inexactness and stochasticity in ﬁrst-order methods for large-scale convex optimization. PhD thesis, PhD thesis, 2013.
[5] Olivier Devolder, François Glineur, and Yurii Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146(1):37–75, 2014.
[6] Pavel Dvurechensky and Alexander Gasnikov. Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. Journal of Optimization Theory and Applications, 171(1):121–145, 2016.
[7] Kacha Dzhaparidze and JH Van Zanten. On bernstein-type inequalities for martingales. Stochastic processes and their applications, 93(1):109–117, 2001.
[8] David A Freedman et al. On tail probabilities for martingales. the Annals of Probability, 3(1):100–118, 1975.
[9] Alexander Gasnikov and Yurii Nesterov. Universal fast gradient method for stochastic composit optimization problems. arXiv:1604.05275, 2016.
[10] Alexander Vladimirovich Gasnikov, Yu E Nesterov, and Vladimir Grigor’evich Spokoiny. On the eﬃciency of a randomized mirror descent algorithm in online optimization problems. Computational Mathematics and Mathematical Physics, 55(4):580–596, 2015.
[11] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1243–1252. JMLR. org, 2017.
[12] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469–1492, 2012.
[13] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.
[14] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.
12

[15] Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15042–15053. Curran Associates, Inc., 2020.
[16] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. Sgd: General analysis and improved rates. In International Conference on Machine Learning, pages 5200–5209, 2019.
[17] Vincent Guigues, Anatoli Juditsky, and Arkadi Nemirovski. Non-asymptotic conﬁdence bounds for the optimal value of a stochastic program. Optimization Methods and Software, 32(5):1033– 1058, 2017.
[18] Cristóbal Guzmán and Arkadi Nemirovski. On lower complexity bounds for large-scale smooth convex optimization. Journal of Complexity, 31(1):1–14, 2015.
[19] Elad Hazan, Kﬁr Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems, pages 1594–1602, 2015.
[20] Anatoli Juditsky, Arkadi Nemirovski, et al. First order methods for nonsmooth convex large-scale optimization, i: general purpose methods. Optimization for Machine Learning, pages 121–148, 2011.
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[22] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365–397, 2012.
[23] Vien V Mai and Mikael Johansson. Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness. arXiv preprint arXiv:2102.06489, 2021.
[24] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient clipping mitigate label noise? In International Conference on Learning Representations, 2020.
[25] Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451–459, 2011.
[26] Aleksandr Viktorovich Nazin, AS Nemirovsky, Aleksandr Borisovich Tsybakov, and AB Juditsky. Algorithms of robust stochastic optimization based on mirror descent method. Automation and Remote Control, 80(9):1607–1627, 2019.
[27] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574– 1609, 2009.
[28] Arkadi Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method eﬃciency in optimization. 1983.
13

[29] Yu Nesterov. Universal gradient methods for convex optimization problems. Mathematical Programming, 152(1-2):381–404, 2015.
[30] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983.
[31] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the diﬃculty of training recurrent neural networks. In International conference on machine learning, pages 1310–1318, 2013.
[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. 2019.
[33] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951.
[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
[35] Umut Şimşekli, Mert Gürbüzbalaban, Thanh Huy Nguyen, Gaël Richard, and Levent Sagun. On the heavy-tailed theory of stochastic gradient descent for deep neural networks. arXiv preprint arXiv:1912.00018, 2019.
[36] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. arXiv preprint arXiv:1901.06053, 2019.
[37] Vladimir Spokoiny et al. Parametric estimation. ﬁnite sample theory. The Annals of Statistics, 40(6):2877–2909, 2012.
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
[39] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.
[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics.
[41] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justiﬁcation for adaptivity. In International Conference on Learning Representations, 2020.
14

[42] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15383–15393. Curran Associates, Inc., 2020.
15

Contents

1 Introduction

1

1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

2 Clipped Stochastic Similar Triangles Method

6

3 SGD with clipping

8

4 Numerical experiments

10

A Basic facts, technical lemmas, and auxiliary results

17

A.1 Notation, missing deﬁnitions, and useful inequalities . . . . . . . . . . . . . . . . . . 17

A.2 Auxiliary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

A.3 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

B Clipped Similar Triangles Method: missing details and proofs

21

B.1 Convergence in the convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

B.1.1 Two lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

B.1.2 Proof of Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

B.1.3 On the batchsizes and numerical constants . . . . . . . . . . . . . . . . . . . . 35

B.2 Convergence in the strongly convex case . . . . . . . . . . . . . . . . . . . . . . . . . 37

C SGD with clipping: missing details and proofs

40

C.1 Convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

C.2 Strongly convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

D Additional experimental details

51

D.1 Main experiment hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

D.1.1 Image classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

D.1.2 Text classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

D.2 On the relation between stepsize parameter α and batchsize . . . . . . . . . . . . . . 52

D.3 Evolution of the noise distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

16

A Basic facts, technical lemmas, and auxiliary results

A.1 Notation, missing deﬁnitions, and useful inequalities

Notation and missing deﬁnitions. We use standard notation for stochastic optimization. For all x ∈ Rn we use x 2 = x, x to denote standard Euclidean norm, where x, y = x1y1 + x2y2 + . . . + xnyn, x = (x1, . . . , xn) , x = (x1, . . . , xn) ∈ Rn. Next, we use E[ξ] and E[ξ | η] to denote expectation of ξ and expectation of ξ conditioned on η respectively. In some places of the paper, we
also use Eξ[·] to denote conditional expectation taken w.r.t. the randomness coming from ξ. The probability of event E is deﬁned as P{E}.
Finally, we use a standard deﬁnition of diﬀerentiable strongly convex function.

Deﬁnition A.1. Diﬀerentiable function f : Q ⊆ Rn → R is called µ-strongly convex for some µ ≥ 0

if for all x, y ∈ Q

f (y) ≥ f (x) + ∇f (x), y − x + µ2 y − x 22.

When µ = 0 function f is called convex.

Useful inequalities. For all a, b ∈ Rn and λ > 0

| a, b | ≤ a 22 + λ b 22 ,

(10)

2λ

2

a+b

2 2

≤2

a

2 2

+

2

b

22,

(11)

a, b = 12 a + b 22 − a 22 − b 22 . (12)

A.2 Auxiliary lemmas
Lemma A.1 ([5, 29]). Let f be (ν, Mν)-Hölder continuous on Q ⊆ Rn. Then for all x, y ∈ Q and for all δ > 0
f (y) ≤ f (x) + ∇f (x), y − x + 1M+νν x − y 12+ν, (13)
1−ν
f (y) ≤ f (x) + ∇f (x), y − x + L(δ2, ν) x − y 22 + 2δ , L(δ, ν) = 1δ 1+ν Mν1+2ν . (14)

Lemma A.2 (Bernstein inequality for martingale diﬀerences [1, 7, 8]). Let the sequence of random

variables {Xi}i≥1 form a martingale diﬀerence sequence, i.e. E [Xi | Xi−1, . . . , X1] = 0 for all i ≥ 1.

Assume

that

conditional

variances

σi2

def
=

E

Xi2 | Xi−1, . . . , X1

exist and are bounded and assume

also that there exists deterministic constant c > 0 such that Xi 2 ≤ c almost surely for all i ≥ 1.

Then for all b > 0, F > 0 and n ≥ 1

n

n

2

b2

P

Xi > b and

σi ≤ F

≤ 2 exp −

.

2F + 2cb/3

(15)

i=1

i=1

17

A.3 Technical lemmas

Lemma A.3. Let sequences {αk}k≥0 and {Ak}k≥0 satisfy

α0 = A0 = 0,

2ν

1−ν

(k + 1) 1+ν (ε/2) 1+ν

αk+1 = 2ν 2 ,

2 1+ν aMν1+ν

for all k ≥ 0. Then for all k ≥ 0 we have

Ak+1 = Ak + αk+1,

a, ε, Mν > 0, ν ∈ [0, 1]

(16)

Ak ≥ aLkαk2,

1+3ν

1−ν

k 1+ν (ε/2) 1+ν

Ak ≥ 1+3ν

2,

2 1+ν aMν1+ν

(17)

where L0 = 0 and for k > 0

Lk =

2Ak αk ε

M . 1−ν
1+ν

2 1+ν
ν

(18)

Moreover, for all k ≥ 0

1+3ν

1−ν

k 1+ν (ε/2) 1+ν

Ak ≤ 2ν

2.

2 1+ν aMν1+ν

(19)

Proof. We start with deriving the second inequality from (17). The proof goes by induction. For k = 0 the inequality holds. Next, we assume that it holds for all k ≤ K. Then,

AK+1 = AK + αK+1 ≥

K ( / ) 1+3ν

1−ν

1+ν ε 2 1+ν

2 aM 1+3ν 1+ν

2 1+ν
ν

2ν

1−ν

(K + 1) 1+ν (ε/2) 1+ν

+ 2ν 2 .

2 1+ν aMν1+ν

Let us estimate the right-hand side of the previous inequality. We want to show that

K ( / ) 1+3ν

1−ν

1+ν ε 2 1+ν

2 aM 1+3ν 1+ν

2 1+ν
ν

2ν

1−ν

(K + 1) 1+ν (ε/2) 1+ν

+ 2ν

2

2 1+ν aMν1+ν

that is equivalent to the inequality:

1+3ν

1−ν

(K + 1) 1+ν (ε/2) 1+ν

≥ 1+3ν

2

2 1+ν aMν1+ν

1+3ν

1+3ν

1+3ν

2ν

K 1+ν

2ν (K + 1) 1+ν

K 1+ν (K + 1) 1+ν (K − 1)

+ (K + 1) 1+ν ≥

⇐⇒

≥

.

2

2

2

2

If K = 1, it trivially holds. If K > 1, it is equivalent to

K ≥
K −1

K +1 K

2− 1+2 ν
.

Since

2−

2 1+ν

is

monotonically

increasing

function

for

ν

∈ [0, 1]

we

have

that

K + 1 2− 1+2ν K + 1

K

≤

≤

.

K

K K−1

18

That is, the second inequality in (17) holds for k = K + 1, and, as a consequence, it holds for all k ≥ 0. Next, we derive the ﬁrst part of (17). For k = 0 it trivially holds. For k > 0 we consider cases ν = 0 and ν > 0 separately. When ν = 0 the inequality is equivalent to

1≥

2aαkM02 ,

where

2aαk M02

(16)
= 1,

ε

ε

i.e., we have Ak = aLkαk2 for all k ≥ 0. When ν > 0 the ﬁrst inequality in (17) is equivalent to

1+3ν

1−ν

Ak ≥ a 12+νν αk1+2ν3ν (ε/2)− 12−νν Mνν1 ⇐(1⇒6) Ak ≥ k 11++3νν (ε/2) 12+ν ,

2 1+ν aMν1+ν

where the last inequality coincides with the second inequality from (17) that we derived earlier in the proof.
To ﬁnish the proof it remains to derive (19). Again, the proof goes by induction. For k = 0 inequality (19) is trivial. Next, we assume that it holds for all k ≤ K. Then,

AK+1 = AK + αK+1 ≤

K ( / ) 1+3ν

1−ν

1+ν ε 2 1+ν

2 aM 2ν 1+ν

2 1+ν
ν

2ν

1−ν

(K + 1) 1+ν (ε/2) 1+ν

+ 2ν 2 .

2 1+ν aMν1+ν

Let us estimate the right-hand side of the previous inequality. We want to show that

K ( / ) 1+3ν

1−ν

1+ν ε 2 1+ν

2 aM 2ν 1+ν

2 1+ν
ν

2ν

1−ν

(K + 1) 1+ν (ε/2) 1+ν

+ 2ν

2

2 1+ν aMν1+ν

that is equivalent to the inequality:

1+3ν

1−ν

(K + 1) 1+ν (ε/2) 1+ν

≤ 2ν

2

2 1+ν aMν1+ν

1+3ν

2ν

1+3ν

K 1+ν + (K + 1) 1+ν ≤ (K + 1) 1+ν .

This inequality holds due to

1+3ν

2ν

K 1+ν ≤ (K + 1) 1+ν K.

That is, (19) holds for k = K + 1, and, as a consequence, it holds for all k ≥ 0.

Lemma A.4. Let f have Hölder continuous gradients on Q ⊆ Rn for some ν ∈ [0, 1] with constant Mν > 0, be convex and x∗ ∈ Q be some minimum of f (x) on Rn. Then, for all x ∈ Rn

∇f (x) 2 ≤

1+ν ν

ν

1+ν

1
Mν1+ν

(f (x)

−

f (x∗)) 1+ν ν

,

(20)

where for ν = 0 we use

ν

ν

1+ν 1+ν ν

:= limν→0

1+ν ν

1+ν = 1.

ν=0

Proof. For ν = 0 inequality (20) follows from (3) and1 ∇f (x∗) = 0. When ν > 0 for arbitrary point

x ∈ Q we consider the point y = x − α∇f (x), where α =

1
∇f(Mx)ν 12−ν ν . Since x∗ ∈ Q and f is

1When f is not diﬀerentiable, we use subgradients. In this case, 0 belongs to the subdiﬀerential of f at the point x∗ and we take it as ∇f (x∗).

19

convex one can easily show that y ∈ Q. For the pair of points x, y we apply (13) and get

f (y) ≤ f (x) + ∇f (x), y − x + 1M+νν x − y 12+ν

2 αν+1Mν

1+ν

= f (x) − α ∇f (x)

+ 1+ν

∇f (x) 2

1+ν

1+ν

1+ν

=

f (x) −

∇f (x)

ν
2

+

∇f (x)

ν
2

ν = f (x) −

∇f (x)

ν
2

1

1

1

Mνν

(1 + ν)Mνν

(1 + ν)Mνν

implying ∇f (x) 2 ≤

1+ν ν

ν

1+ν

1

ν

Mν1+ν (f (x) − f (y)) 1+ν ≤

1+ν ν

ν

1+ν

1
Mν1+ν

(f (x)

−

f

(x

∗

))

ν 1+ν

.

Lemma A.5. Let f have Hölder continuous gradients on Q ⊆ Rn for some ν ∈ [0, 1] with constant Mν > 0, be convex and x∗ ∈ Q be some minimum of f (x) on Rn. Then, for all x ∈ Rn and all δ > 0,

1−ν
∇f (x) 22 ≤ 2 1δ 1+ν Mν1+2ν (f (x) − f (x∗)) + δ 12+νν Mν1+2ν .

(21)

Proof. For a given δ > 0 we consider an arbitrary point x ∈ Q and y = x − L(δ1,ν) ∇f (x), where

1−ν

2

L(δ, ν) =

1 δ

1+ν Mν1+ν . Since x∗ ∈ Q and f is convex one can easily show that y ∈ Q. For the pair

of points x, y we apply (14) and get

f (y) ≤ f (x) + ∇f (x), y − x + L(δ2, ν) x − y 22 + 2δ = f (x) − 2L(1δ, ν) x − y 22 + 2δ

implying

∇f (x)

2 2

≤

2L(δ, ν) (f (x) − f (y)) + δL(δ, ν)

1−ν

≤

2

1

1+ν

2
Mν1+ν

(f (x)

−

f (x∗))

+

δ M . 2ν 1+ν

2 1+ν
ν

δ

20

B Clipped Similar Triangles Method: missing details and proofs
B.1 Convergence in the convex case
In this section, we provide the full proof of Thm. 2.1 together with complete statement of the result.

B.1.1 Two lemmas
The analysis of clipped-SSTM consists of 3 main steps. The ﬁrst one is an “optimization lemma” – a modiﬁcation of a standard lemma for Similar Triangles Method (see [9] and Lemma F.4 from [15]). This result helps to estimate the progress of the method after N iterations.
Lemma B.1. Let f be a convex function with a minimum at some2 point x∗, its gradient be (ν, Mν)-Hölder continuous on a ball B3R0(x∗), where R0 ≥ x0 − x∗ 2, and let stepsize parameter a satisfy a ≥ 1. If xk, yk, zk ∈ B3R0(x∗) for all k = 0, 1, . . . , N , N ≥ 0, then after N iterations of clipped-SSTM for all z ∈ Rn we have

AN f (yN ) − f (z)

10

2 1N

N −1 2

k

≤ 2 z − z 2 − 2 z − z 2 + αk+1 θk+1, z − z

k=0

N −1
+ αk2+1
k=0

N −1

θk+1

2 2

+

αk2+1

k=0

θk+1, ∇f (xk+1)

+ AN ε , 4

(22)

θk+1

def
=

∇f (xk+1, ξk) − ∇f (xk+1).

(23)

Proof. Consider an arbitrary k ∈ {0, 1, . . . , N − 1}. Using zk+1 = zk − αk+1∇f (xk+1, ξk) we get that for all z ∈ Rn

αk+1 ∇f (xk+1, ξk), zk − z Next, we notice that

= αk+1 ∇f (xk+1, ξk), zk − zk+1

+αk+1 ∇f (xk+1, ξk), zk+1 − z

= αk+1 ∇f (xk+1, ξk), zk − zk+1 + zk+1 − zk, z − zk+1

(12)
=

αk+1

∇f (xk+1, ξk), zk − zk+1

− 1 zk − zk+1 22

2

+ 12 zk − z 22 − 12 zk+1 − z 22.

(24)

yk+1 = Akyk + αk+1zk+1 = Akyk + αk+1zk + αk+1

Ak+1

Ak+1

Ak+1

zk+1 − zk

= xk+1 + αk+1 Ak+1

zk+1 − zk

(25)

2Our proofs are valid for any solution x∗ and, for example, one can take as x∗ the closest solution to the starting point x0.

21

implying

αk+1 ∇f (xk+1, ξk), zk − z

(23),(24)
≤
(25)
=
(14)
≤
(25)
=

αk+1 ∇f (xk+1), zk − zk+1 − 1 zk − zk+1 22 2

+αk+1 θk+1, zk − zk+1 + 1 zk − z 22 − 1 zk+1 − z 22

2

2

Ak+1 ∇f (xk+1), xk+1 − yk+1 − 1 zk − zk+1 22 2

+αk+1 θk+1, zk − zk+1 + 1 zk − z 22 − 1 zk+1 − z 22

2

2

Ak+1

f (xk+1) − f (yk+1)

+ Ak+1Lk+1

xk+1 − yk+1

2 2

2

+ αk+1ε − 1

zk − zk+1

2 2

+

αk+1

θk+1, zk − zk+1

42

+ 12 zk − z 22 − 12 zk+1 − z 22

Ak+1

f (xk+1) − f (yk+1)

1 +

2

αk2+1Lk+1 − 1 Ak+1

zk − zk+1

2 2

+αk+1 θk+1, zk − zk+1 + 1 zk − z 22 − 1 zk+1 − z 22 + αk+1ε ,

2

2

4

where in the third inequality we used xk+1, yk+1 ∈ B3R0(x∗) and (14) with δ = 2αAkk++11 ε and

L(δ, ν) = Lk+1 =

1−ν

2

2εAαkk++11 1+ν Mν1+ν . Since Ak+1 ≥ aLk+1αk2+1 (Lemma A.3) and a ≥ 1 we can

continue our derivations:

αk+1 ∇f (xk+1, ξk), zk − z ≤ Ak+1 f (xk+1) − f (yk+1) + αk+1 θk+1, zk − zk+1 + 12 zk − z 22 − 12 zk+1 − z 22 + αk4+1ε . (26)
Next, due to convexity of f we have

∇f (xk+1, ξk), yk − xk+1

(23)
=

∇f (xk+1), yk − xk+1 + θk+1, yk − xk+1

≤ f (yk) − f (xk+1) + θk+1, yk − xk+1 .

(27)

By deﬁnition of xk+1 we have xk+1 = AkykA+kα+k1+1zk implying

αk+1 xk+1 − zk = Ak yk − xk+1

(28)

22

since Ak+1 = Ak + αk+1. Putting all together we derive that

αk+1 ∇f (xk+1, ξk), xk+1 − z

= αk+1 ∇f (xk+1, ξk), xk+1 − zk

+αk+1 ∇f (xk+1, ξk), zk − z

(28)
=

Ak ∇f (xk+1, ξk), yk − xk+1

+αk+1 ∇f (xk+1, ξk), zk − z

(27),(26)
≤ Ak f (yk) − f (xk+1) + Ak θk+1, yk − xk+1

+Ak+1 f (xk+1) − f (yk+1) + αk+1 θk+1, zk − zk+1

+ 21 zk − z 22 − 12 zk+1 − z 22 + αk4+1ε

(28)
=

Akf (yk) − Ak+1f (yk+1) + αk+1 θk+1, xk+1 − zk

+αk+1f (xk+1) + αk+1 θk+1, zk − zk+1
+ 21 zk − z 22 − 12 zk+1 − z 22 + αk4+1ε = Akf (yk) − Ak+1f (yk+1) + αk+1f (xk+1)
+αk+1 θk+1, xk+1 − zk+1
+ 21 zk − z 22 − 12 zk+1 − z 22 + αk4+1ε .

Rearranging the terms we get

Ak+1f (yk+1) − Akf (yk) ≤ αk+1 f (xk+1) + ∇f (xk+1, ξk), z − xk+1 + 21 zk − z 22

1 −

zk+1 − z

2 2

+

αk+1

θk+1, xk+1 − zk+1

+ αk+1ε

2

4

(23)
=

αk+1

f (xk+1) +

∇f (xk+1), z − xk+1

+αk+1 +αk+1

θk+1, z − xk+1 + 1 zk − z 22 − 1 zk+1 − z 22

2

2

θk+1, xk+1 − zk+1 + αk+1ε

4

≤

1 αk+1f (z) +

zk − z

22 − 1

zk+1 − z

2 2

+

αk+1

θk+1, z − zk+1

+ αk+1ε

2

2

4

where in the last inequality we use the convexity of f . Taking into account A0 = α0 = 0 and

23

AN =

N −1 k=0

αk+1

we

sum

up

these

inequalities

for

k

=

0, . . . , N

−

1

and

get

N

10

2 1N

N −1 2

k+1 AN ε

AN f (y ) ≤ AN f (z) + 2 z − z 2 − 2 z − z 2 + αk+1 θk+1, z − z

+ 4

k=0

10

2 1N

N −1 2

k

= AN f (z) + 2 z − z 2 − 2 z − z 2 + αk+1 θk+1, z − z

k=0

N −1
+ αk2+1
k=0

θk+1, ∇f (xk+1, ξk)

+ AN ε 4

(23)

10

2 1N

N −1 2

k

= AN f (z) + 2 z − z 2 − 2 z − z 2 + αk+1 θk+1, z − z

k=0

N −1
+ αk2+1
k=0

N −1

θk+1

2 2

+

αk2+1

k=0

θk+1, ∇f (xk+1)

+ AN ε 4

that concludes the proof.

From Lemma A.3 we know that

1+3ν 1−ν

N 1+ν ε 1+ν

AN ∼

2.

Mν1+ν

Therefore, in view of Lemma B.1 (inequality (22) with z = x∗), to derive the desired complexity

bound from Thm. 2.1 it is suﬃcient to show that

N −1

N −1

k

2

N −1

2

2

k+1

AN ε

2

αk+1 θk+1, z − z +

αk+1 θk+1 2 +

αk+1 θk+1, ∇f (x

)+ 4

R0.

k=0

k=0

k=0

with probability at least 1 − β. One possible way to achieve this goal is to apply some concentration inequality to these three sums. Since we use clipped stochastic gradients, under a proper choice of the clipping parameter, random vector θk+1 = ∇f (xk+1, ξk) − ∇f (xk+1) is bounded in 2-norm by 2λk+1 with high probability as well. Taking into account the assumption on the stochastic gradients (see (2)), it is natural to apply Bernstein’s inequality (see Lemma A.2). Despite the seeming simplicity, this part of the proof is the trickiest one.
First of all, it is useful to derive tight enough upper bounds for bias, variance and distortion of ∇f (xk+1, ξk) – this is the second step of the whole proof. Fortunately, Lemma F.5 from [15] does exactly what we need in our proof and holds without any changes.

Lemma B.2 (Lemma F.5 from [15].). For all k ≥ 0 the following inequality holds:

∇f (xk+1, ξk) − Eξk ∇f (xk+1, ξk) ≤ 2λk+1.

(29)

2

24

Moreover, if

∇f (xk+1)

2≤

λk+1 2

for

some

k ≥ 0,

then

for

this

k

we

have:

E ∇f (xk+1, ξk) − ∇f (xk+1) ≤ 4σ2 ,

(30)

ξk

2

mk λk+1

E ∇f (xk+1, ξk) − ∇f (xk+1) 2 ≤ 18σ2 ,

(31)

ξk

2

mk

E ∇f (xk+1, ξk) − E ∇f (xk+1, ξk) 2 ≤ 18σ2 .

(32)

ξk

ξk

2

mk

B.1.2 Proof of Theorem 2.1
The ﬁnal, third, step of the proof is consists of providing explicit formulas and bounds for the parameters of the method and derivation of the desired result using induction and Bernstein’s inequality. Below we provide the complete statement of Thm. 2.1.
Theorem B.1. Assume that function f is convex, achieves minimum value at some3 x∗ , and the gradients of f satisfy (3) with ν ∈ [0, 1], Mν > 0 on B3R0(x∗), where R0 ≥ x0 − x∗ 2. Then for all β ∈ (0, 1) and N ≥ 1 such that
ln 4N ≥ 2 (33) β
we have that after N iterations of clipped-SSTM with

1−ν
(ε/2) 1+ν

20736N

σ2αk2+1

ln

4N β

α= 2ν

2 , mk = max 1,

C 2 R2

,

2 1+ν aMν1+ν

0

C R0 B = 16 ln 4N ,
β

a ≥ 16384 ln2 4N , β

1−ν
1−ν aCMν1+ν R01−ν ε 1+ν ≤ 16 ln 4N ,
β

2

1+ν 2

a

1+ν 2

C

1+ν

R01+ν

Mν

ε≤

1+3ν

,

100 2

ε 1−ν 1+3ν



2+3ν−ν2

 2(1+3ν) ≤ min  a 2+4ν+ 3+8ν−5ν2−6ν3
2 (1+ν)(1+3ν)

(1+ν)2 1+3ν
, a 4N 4+7ν+ 2+(17+ν+ν)2(ν12+−3ν3ν) 3 ln 2 β

ln1+ν



 1−ν2 1−ν2

1−ν

C R M 1+3ν

1+3ν
0

1+3ν
ν

 4N

β

with probability at least 1 − β

(34) (35) (36) (37)

where

2

N

∗ 4aC2R02Mν1+ν

f (y ) − f (x ) ≤

1+3ν 1−ν ,

N 1+ν ε 1+ν

 1+ν 1+ν 2(1+ν) 2(1+ν)

2

N =  2 1+3ν a 1+3ν C 1+32ν R01+3ν Mν1+3ν  + 1,

 

ε 1+3ν

 

√ C = 7.

(38) (39)

3Our proofs are valid for any solution x∗ and, for example, one can take as x∗ the closest solution to the starting point x0.

25

In other words, if we choose a = 16384 ln2 4βN , then the method achieves f (yN ) − f (x∗) ≤ ε with

probability at least 1 − β after O

2

2(1+ν)

Mν1+3ν R01+3ν

2

ε 1+3ν

2(1+ν)
ln 1+3ν

2

2(1+ν)

Mν1+3ν R01+3ν

2

ε 1+3ν β

iterations and requires





2

2(1+ν)

O max  Mν1+3ν R2 01+3ν



ε 1+3ν

2(1+ν)
ln 1+3ν

2

2(1+ν)

2

2(1+ν) 

M 1+3ν
ν

R 1+3ν
0

,

σ2R02

ln

M 1+3ν
ν

R 1+3ν
0



2
ε 1+3ν β

ε2

2
ε 1+3ν β

 

oracle calls.

(40)

Proof. First of all, we notice that for each k ≥ 0 iterates xk+1, zk, yk lie in the ball BRk (x∗),

where Rk = zk − x∗ 2, R0 = R0, Rk+1 = max{Rk, Rk+1}. We prove it using induction. Since

y0 = z0 = x0, R0 = R0 ≥ z0 − x∗ 2 and x1 = A0y0A+1α1z0 = z0 we have that x1, z0, y0 ∈ BR0 (x∗).

Next, assume that xl, zl−1, yl−1 ∈ BRl−1(x∗) for some l ≥ 1. By deﬁnitions of Rl and Rl we have

that zl ∈ BRl(x∗) ⊆ BRl(x∗). Since yl is a convex combination of yl−1 ∈ BRl−1(x∗) ⊆ BRl(x∗),

zl ∈ BRl(x∗) and BRl(x∗) is a convex set we conclude that yl ∈ BRl(x∗). Finally, since xl+1 is a convex combination of yl and zl we have that xl+1 lies in BRl(x∗) as well.
Next, our goal is to prove via induction that for all k = 0, 1, . . . , N with probability at least

1−

kβ N

the

following

statement

holds:

inequalities

t−1

t−1

Rt2 ≤ R02 + 2 αl+1 θl+1, x∗ − zl + 2 αl2+1 θl+1, ∇f (xl+1)

l=0

l=0

t−1 2

2 AN ε

+2 αk+1 θl+1 2 + 2

l=0

≤ C2R02

(41)

hold for t = 0, 1, . . . , k simultaneously where C is deﬁned in (39). Let Ek denote the probabilistic event that this statement holds. Then, our goal is to show that P{Ek} ≥ 1− kNβ for all k = 0, 1, . . . , N .

For t = 0 inequality (41) holds with probability 1 since C ≥ 1, hence P{E0} = 1. Next, assume

that for some k = T − 1 ≤ N − 1 we have P{Ek} = P{ET −1} ≥ 1 − (T −N1)β . Let us prove that

P{ET }

≥

1−

Tβ N

.

First

of

all,

since

RT −1

implies

Rt

≤

C R0

for

all

t

=

0, 1, . . . , T

−1

we

have

that

RT −1 ≤ CR0, and, as a consequence, zT −1 ∈ BCR0(x∗). Therefore, probability event ET −1 implies

zT − x∗ 2 = zT −1 − x∗ − αT ∇f (xT , ξT −1) 2 ≤ zT −1 − x∗ 2 + αT ∇f (xT , ξT −1) 2

1 ≤ CR0 + αT λT = 1 + 16 ln 4N
β

(33),(39)
CR0 ≤

1 1+
32

√ 7R0 ≤ 3R0,

hence RT ≤ 3R0. Then, one can apply Lemma B.1 and get that probability event ET −1 implies

At f (yt) − f (x∗)

10

∗2 1 t

t−1 ∗2

∗k

≤

z −x 2

2− 2 z −x

2+

αk+1 θk+1, x − z

k=0

t−1
+ αk2+1
k=0

t−1

θk+1

2 2

+

αk2+1

k=0

θk+1, ∇f (xk+1)

+ Atε , 4

θk+1 d=ef ∇f (xk+1, ξk) − ∇f (xk+1)

(42) (43)

26

for all t = 0, 1, . . . , T − 1, T . Taking into account that f (yt) − f (x∗) ≥ 0 for all yt we derive that probability event ET −1 implies

t−1
Rt2 ≤ R02 + 2 αl+1
l=0

θl+1, x∗ − zl

t−1
+ 2 αl2+1
l=0

θl+1, ∇f (xl+1)

t−1 2

2 Atε

+ 2 αl+1 θl+1 2 + 2 . (44)

l=0

for all t = 0, 1, . . . , T . The rest of the proof is based on the reﬁned analysis of inequality (44). First of all, when ν = 0
from (14) for all t ≥ 0 we have

t+1

16M0B

ln

4N β

aM02B

B

λt+1

∇f (x ) 2 ≤ M0 =

C R0

≤

=

=

ε

2αt+1

2

where we use B =

C R0
4N

and

ε

≤

aC

M0 R0
4N

.

Next, we prove that

16 ln β

16 ln β

For t = 0 we have

∇f (xt+1)

2

≤

λt+1 2

when

ν

> 0.

∇f (x1) 2 =

∇f (z0)

(3)
2 ≤ Mν

z0 − x∗

1−ν
16ε 1+ν

ln

4N

ν 2

≤

Mν R0ν

=

β
1−ν

aCMν1+ν R01−ν

B ≤
2α1

= λ1 2

1−ν
since ε 11−+νν ≤ aCM16ν1l+nν4NR01−ν . For 0 < t ≤ T − 1 probability event ET −1 implies β

∇f (xt+1) 2 ≤

∇f (xt+1) − ∇f (yt) 2 + ∇f (yt) 2

ν
(≤3) Mν xt+1 − yt ν2 + 1 +ν ν 1+ν Mν1+1ν f (yt) − f (x∗) 1+ν ν

(28),(41)
≤ Mν

αt+1 ν t+1 t ν

At

x −z 2+

1+ν ν

M ν
1+ν

1 1+ν
ν

ν
C2R02 1+ν 2At

λt+1 2Mν αt+1 ν

1+ν

2M ν
1+ν

1 1+ν
ν

ν
C2R2 1+ν

= 2 λt+1 At xt+1 − zt ν2 + ν

0

.

λt+1

2At

D1

D2

Next, we show that D1 + D2 ≤ 1. Using the deﬁnition of λt+1, triangle inequality xt+1 − zt 2 ≤

27

xt+1 − x∗ 2 + zt − x∗ 2 ≤ 2CR0, and lower bound (17) for At (see Lemma A.3) we derive

2ν+4Mν αt1++1ν

ln

4N β

2ν+4Mν (t

+

1)2ν (ε/2)1−ν

ln

4N β

D1 =

C1−ν R1−ν Aν = 22ν a1+ν C1−ν R1−ν M 2Aν

0

t

0

νt

(17) 23(t + 1)2ν ε1−ν ln 4βN

2 a M (1+3ν)ν
1+ν ν

2ν 1+ν
ν

≤ a1+ν C 1−ν R01−ν Mν · t (1+1+3νν)ν (ε/2) ν(11+−νν)

=
(39)
≤
=

(t + 1)2ν 23+2ν ε 11−+νν ln 4βN

23+4ν

t

ν

(1−ν 1+ν

)

ε

1−ν 1+ν

ln

4N

β

· ν(1+3ν)

1−ν

≤

1−ν

t 1+ν

aMν1+ν C1−ν R01−ν

aMν1+ν C1−ν R01−ν

3+4ν

1−ν 1+ν

2 ε ln · 2 a C R M 1−ν
1+ν
aM C R ε ν

1−ν

4N β
1−ν 0

2ν(1−ν)(1+2ν) (1+ν)(1+3ν)

ν(1−ν) 1+3ν

2ν(1−ν) 1+3ν

2ν(1−ν)
1+3ν
0

2ν(1−ν)

(1+ν)(1+3ν)

2ν(1−ν)
(1+ν)(1+3ν)
ν

2 ε ln 3+4ν+ 2ν(1(+1−νν)()1(+1+3ν2ν) ) 11+−3νν

4N

β

a M C R (1+ν)2 1+3ν

1−ν 1+3ν
ν

(1−ν)(1+ν) 1+3ν

(1−ν)(1+ν)
1+3ν
0

≤ 2 1 a . (37)

3+6ν−7ν2−2ν3 ν (1+ν)(1+3ν) 2

ν

Applying the same inequalities and

1+ν ν

1+ν ≤ 2 we estimate D2:

D2 =

1+ν ν

1+ν ν 24− 1+ν ν Mν1+1ν αt+1 ln 4βN ≤ 2 · 24− 1+ν ν Mν1+1ν ln 4βN · (t + 1) 12+νν (ε/2) 11−+νν

1−ν 1−ν

ν

1−ν 1−ν

ν

2ν

2

C 1+ν R01+ν At1+ν

C 1+ν R01+ν At1+ν

2 1+ν aMν1+ν

≤ 24− 1+ν ν · 2 12+νν t 12+νν ε 11−+νν ln 4βN

1−ν 1−ν

1

ν

aC 1+ν R01+ν Mν1+ν At1+ν

(17)
≤

24+ 1+ν ν t 12+νν ε 11−+νν ln 4βN

aC R M 1−ν 1+ν

1−ν 1+ν
0

1 1+ν
ν

ν(1+3ν) (1+ν)2

ν 1+ν

· 2 a M ν(1+3ν)

2ν (1+ν)2
ν
ν(1−ν)

t (1+ν)2 (ε/2) (1+ν)2

4+ 13+νν
= 2 t ε ln 1 a C R M 1+ν

ν(1−ν) 1−ν

(1+ν)2 (1+ν)2

4N

β

1−ν 1+ν

1−ν 1+ν
0

1−ν
(1+ν)2
ν

(39)
≤

2 ε ln 4+ 13+νν (11+−νν)2

4N

β

a C R M 1 1+ν

1−ν 1+ν

1−ν 1+ν
0

1−ν
(1+ν)2
ν

2ν(1+2ν)(1−ν)
· 2 a C R M (1+ν)2(1+3ν)

ν(1−ν) (1+ν)(1+3ν)

2ν(1−ν) (1+ν)(1+3ν)
2ν(1−ν)

2ν(1−ν) (1+ν)(1+3ν)
0

2ν(1−ν)
(1+ν )2 (1+3ν )
ν

ε (1+ν)2(1+3ν)

4+

3ν 1+ν

+

2ν(1+2ν)(1−ν) (1+ν )2 (1+3ν )

1−ν (1+ν)(1+3ν)

4N

2 ε ln 1 β

= ≤ . 1+ν a C R M 2 1+3ν

1−ν 1+3ν

1−ν 1+3ν
0

1−ν
(1+ν)(1+3ν)
ν

(37)

2+5ν+ν3 (1+ν )2 (1+3ν )

Combining the upper bounds for D1 and D2 we get

D + D ≤ 2 1 a + 2 1 . 1

2

3+6ν−7ν2−2ν3 ν (1+ν)(1+3ν) 2

2+5ν+ν3 (1+ν )2 (1+3ν )

28

Since (1+2+ν)52ν(+1+ν33ν) is a decreasing function of ν for ν ∈ [0, 1] we continue as

1

1

D1 + D2 ≤

2 3 +√ .

2 a 3+6ν−7ν −2ν ν (1+ν)(1+3ν) 2

2

Next,

we

use

a

≥

16384 ln2

4N β

≥

210

and

obtain

1

1

D1 + D2 ≤

2 3 +√ .

3+11ν+13ν +13ν
2 (1+ν)(1+3ν)

2

One can numerically verify that 3+11ν+113ν2+13ν3 + √12 is smaller than 1 for ν ∈ [0, 1]. Putting all
2 (1+ν)(1+3ν)
together we conclude that probability event ET −1 implies

∇f (xt+1) 2 ≤ λt+1

(45)

2

for all t = 0, 1, . . . , T − 1. Having inequality (45) in hand we show in the rest of the proof that (41) holds for t = T with large enough probability. First of all, we introduce new random variables:

ηl = 0x,∗ − zl, oifthxer∗w−isze,l 2 ≤ CR0, and ζl = 0∇,f (xl+1), iofth∇erfw(ixsel+, 1) 2 ≤ 2αBl+1 , (46)

for l = 0, 1, . . . T − 1. Note that these random variables are bounded with probability 1, i.e. with probability 1 we have
ηl 2 ≤ CR0 and ζl 2 ≤ B . (47) 2αl+1
Secondly, we use the introduced notation and get that ET −1 implies

(44),(41),(45),(46)

2

2

T −1

T −1 2

T −1

2

2

AN ε

RT

≤

R0 + 2 αl+1 θl+1, ηl + 2 αl+1 θl+1 2 + 2 αl+1 θl+1, ζl + 2

l=0

l=0

l=0

T −1 2

T −1 2

2 AN ε

=

R0 +

αl+1 θl+1, 2ηl + 2αl+1ζl + 2

αl+1 θl+1 2 +

. 2

l=0

l=0

Finally, we do some preliminaries in order to apply Bernstein’s inequality (see Lemma A.2) and obtain that ET −1 implies

(11)

T −1

T −1

RT2 ≤ R02 + αl+1 θlu+1, 2ηl + 2αl+1ζl + αl+1 θlb+1, 2ηl + 2αl+1ζl

l=0

l=0

T −1
+ 4αl2+1
l=0

x

θlu+1

2 2

−

Eξl

θlu+1

2 2

y
T −1
+ 4αl2+1Eξl
l=0

θlu+1

2 2

z

{

T −1 2

b 2 AN ε

+ 4αl+1 θl+1 2 + 2

l=0

|

(48)

29

where we introduce new notations:

θlu+1 d=ef ∇f (xl+1, ξl) − Eξl ∇f (xl+1, ξl) , θlb+1 d=ef Eξl ∇f (xl+1, ξl) − ∇f (xl+1),

(49)

θl+1

(23)
=

θlu+1

+

θlb+1.

It remains to provide tight upper bounds for x, y, z, { and |, i.e. in the remaining part of the proof we show that x + y + z + { + | ≤ δC2R02 for some δ < 1.
Upper bound for x. First of all, since Eξl[θlu+1] = 0 summands in x are conditionally unbiased:

Eξl αl+1 θlu+1, 2ηl + 2αl+1ζl = 0.

Secondly, these summands are bounded with probability 1:

αl+1 θlu+1, 2ηl + 2αl+1ζl

≤
(29),(47)
≤
=

αl+1 θlu+1 2 2ηl + 2αl+1ζl 2

2αl+1λl+1 (2CR0 + B) = 2B(2CR0 + B)

1 1 + 32 ln 4N
β

C2R02 (33) 4 ln 4N ≤
β

1 1+
64

C 2 R02 4 ln 4N .
β

Finally, one can bound conditional variances σl2 d=ef Eξl αl2+1 θlu+1, 2ηl + 2αl+1ζl 2 in the following way:

σl2

≤

Eξl

αl2+1

θlu+1

2 2

2ηl + 2αl+1ζl

2 2

(47)
≤ αl2+1Eξl

θlu+1

2 2

(2CR0 + B)2 = 4αl2+1Eξl

(33)
≤ 4αl2+1Eξl

θlu+1

2 2

1 2 22

1+ 64

C R0.

θlu+1

2 2

1 1 + 32 ln 4N
β

2
C 2 R02

(50)

In other words, sequence αl+1 θlu+1, 2ηl + 2αl+1ζl l≥0 is a bounded martingale diﬀerence sequence with bounded conditional variances {σl2}l≥0. Therefore, we can apply Bernstein’s inequality, i.e. we apply Lemma A.2 with Xl = αl+1 θlu+1, 2ηl + 2αl+1ζl , c = 1 + 614 4Cln2R4N02 and F = c2 l1n84βN and get
β
that for all b > 0

T −1

T −1 2

b2

P

Xl > b and

σl ≤ F

≤ 2 exp − 2F + 2cb/3

l=0

l=0

or, equivalently, with probability at least 1 − 2 exp − 2F +b22cb/3

T −1
either σl2 > F or
l=0

T −1
Xl
l=0

≤ b.

|x|

30

The choice of F will be clariﬁed below. Let us now choose b in such a way that 2 exp − 2F +b22cb/3 = 2βN . This implies that b is the positive root of the quadratic equation

b2 − 2c ln 4βN b − 2F ln 4N = 0,

3

β

hence

c ln 4βN

c2 ln2 4βN

4N c ln 4βN

2c2 ln2 4βN

b=

+

+ 2F ln ≤

+

3

9

β

3

9

√

1 + 2 4N

4N

1 C2R02

11

22

=

c ln ≤ c ln = 1 +

3

β

β

64

4

=

+ 4 256

C R0.

That

is,

with

probability

at

least

1

−

β 2N

T −1 2

11

either

σl > F

or

|x| ≤

+ 4 256

l=0

probability event Ex

C2R02 .

Next, we notice that probability event ET −1 implies that

T −1
σl2
l=0

(50)
≤
(32),(45)
≤
(34)
≤
T ≤N
≤

1

2

T −1

22

2

4 1+ 64

C R0

αl+1Eξl

l=0

θlu+1

2 2

1 2 2 2 2 T −1 αl2+1

72 1 + 64

σ C R0

ml

l=0

1 + 614 2 C4R04 T −1 1

288 ln 4βN

N
l=0

1 + 614 2 C4R04 c2 ln 4βN

288 ln 4N

=

= F.

18

β

Upper bound for y. The probability event ET −1 implies

αl+1 θlb+1, 2ηl + 2αl+1ζl

≤
(30),(47)
≤
=
(34)
≤

αl+1 θlb+1 2ηl + 2αl+1ζl 2
2
4σ2 αl+1 · mlλl+1 (2CR0 + B)

4σ2αl2+1 ml

1 + 2CR0 B

4σ2αl2+1 =

1 + 32 ln 4βN ml

4 ln 14N + 32 C2R02 (33) 11C2R2

β

≤

0.

20736N

1728N

31

This implies that

T −1 b

T ≤N 11C2R02

y=

αl+1 θl+1, 2ηl + 2αl+1ζl

≤

. 1728

l=0

Upper bound for z. We derive the upper bound for z using the same technique as for x. First of all, we notice that the summands in z are conditionally unbiased:

Eξl 4αl2+1

θlu+1

2 2

−

Eξl

θlu+1

2 2

= 0.

Secondly, the summands are bounded with probability 1:

4αl2+1

θlu+1

2 2

−

Eξl

θlu+1

2 2

≤

4αl2+1

θlu+1

2 2

+

Eξl

θlu+1

2 2

(29)
≤ 4αl2+1 4λ2l+1 + 4λ2l+1

2

C2R02 (33) C2R02 def

= 32B = 8 ln2 4N ≤ 16 ln 4N = c1.

β

β

(51)

Finally, one can bound conditional variances σˆl2 d=ef Eξl following way:

4αl2+1

θlu+1

2 2

−

Eξl

θlu+1

2 2

2
in the

(51)
σˆl2 ≤ c1Eξl 4αl2+1

θlu+1

2 2

−

Eξl

θlu+1

2 2

≤

4c1αl2+1Eξl

θlu+1

2 2

+

Eξl

θlu+1

2 2

= 8c1αl2+1Eξl

θlu+1

2 2

.

(52)

In other words, sequence 4αl2+1

θlu+1

2 2

−

Eξl

θlu+1

2 2

is bounded martingale diﬀerence
l≥0

sequence with bounded conditional variances {σˆl2}l≥0. Therefore, we can apply Bernstein’s inequality,

i.e. we apply Lemma A.2 with Xl = Xˆl = 4αl2+1

θlu+1

2 2

−

Eξl

θlu+1

2 2

, c = c1 = 16Cl2nR402N and β

F = F1 = c21 l1n84βN and get that for all b > 0

T −1

T −1

P

Xˆ > b and σˆ2 ≤ F

≤ 2 exp −

b2

l

l

1

2F1 + 2c1b/3

l=0

l=0

or, equivalently, with probability at least 1 − 2 exp − 2F1+b22c1b/3

T −1
either σˆl2 > F1 or
l=0

T −1
Xˆl ≤ b.
l=0
|z|

As in our derivations of the upper bound for x we choose such b that 2 exp − 2F1+b22c1b/3

c1

ln

4N β

b=

+

3

c21

ln2

4N β

√ 4N 1 + 2

4N C2R2

+ 2F1 ln ≤

c1 ln ≤

0.

9

β

3

β

16

=

β 2N

,

i.e.,

32

That

is,

with

probability

at

least

1

−

β 2N

T −1 2

C 2 R02

either σˆl > F1 or |z| ≤ 16 .

l=0

probability event Ez

Next, we notice that probability event ET −1 implies that

T −1
σˆl2
l=0

(52)
≤
(32),(45)
≤
T ≤N
≤

T −1
8c1 αl2+1Eξl
l=0

θlu+1

2 2

9σ2C2R02 T −1 αl2+1 (34) C4R04 T −1 1

ln 4N

ml ≤ 2304 ln2 4N N

β l=0

β l=0

C4R04 (33) C4R04

c21

ln

4N β

2304 ln2 4N ≤ 4608 ln 4N = 18 = F1.

β

β

Upper bound for {. The probability event ET −1 implies

T −1 2

u 2 (32),(45) T −1 72αl2+1σ2 (34) T −1 C2R02

{=

4αl+1Eξl θl+1 2 ≤

≤ ml

288N ln 4N

l=0

l=0

l=0

β

T ≤N
≤

C2R02 (33) C2R02

288 ln 4N ≤

. 576

β

Upper bound for |. Again, we use corollaries of probability event ET −1:

T −1 2

b 2 (30),(45) T −1 64αl2+1σ4 64σ4 T −1 αl4+1

|=

4αl+1 θl+1 2 ≤

m2λ2 = B2

m2

l=0

l=0 l l+1

l=0 l

(34),(35)
≤

256 · 64σ4 ln2 4βN T −1

C 4 R04

T ≤N C2R02

C 2 R2

207362N 2σ4 ln2 4N

≤

.

26244

0

l=0

β

Now we summarize all bounds that we have: probability event ET −1 implies

(44)

2

2

T −1

∗l

k−1 2

T −1

l+1

2

2 AN ε

RT ≤ R0 + 2 αl+1 θl+1, x − z + 2 αl+1 θl+1, ∇f (x ) + 2 αl+1 θl+1 2 + 2

l=0

l=0

l=0

(≤48) R02 + x + y + z + { + | + A2N ε ,

y ≤ 11C2R02 , { ≤ CR02 , | ≤ C2R02 ,

1728

576

26244

T −1

T −1

σl2 ≤ F,

σˆl2 ≤ F1

l=0

l=0

and

(T − 1)β

β

β

P{ET −1} ≥ 1 − N , P{Ex} ≥ 1 − 2N , P{Ez} ≥ 1 − 2N ,

33

where

Ex = Ez =

T −1 2

11

22

either

σl > F

or

|x| ≤

+ 4 256

C R0

,

l=0

T −1 2

C 2 R02

either σˆl > F1 or |z| ≤ 16 .

l=0

(39) 2 11++3νν a 11++3νν C 21(+1+3νν) R 21(+1+3νν) M 1+23ν

(36)

2

1+ν 2

a

1+ν 2

C 1+ν

R1+ν

M

Moreover, since N ≤

0 2

ν + 1 and ε ≤

0 1+3ν

ν we have

ε 1+3ν

100 2

AN ε 2

(19)
≤

N ε 1+3ν 2 1+ν 1+ν
2
4aMν1+ν

≤  2 a C ε R M (39)

1+ν 1+ν 1+3ν 1+3ν

2(1+ν) 1+3ν
2 1+3ν

2(1+ν)
1+3ν
0

2 1+3ν
ν

1+3ν
 1+ν
+ 1

ε2 1+ν
2
4aMν1+ν

(36)

101

1+3ν 1+ν

C 2 R2

10201C 2 R2

≤

0≤

0.

100

2

20000

Taking into account these inequalities we get that probability event ET −1 ∩ Ex ∩ Ez implies

(44)

2

2

T −1

∗l

k−1 2

T −1

l+1

2

2 AN ε

RT ≤ R0 + 2 αl+1 θl+1, x − z + 2 αl+1 θl+1, ∇f (x ) + 2 αl+1 θl+1 2 + 2

l=0

l=0

l=0

≤ 1 + 14 + 2156 + 117128 + 116 + 5176 + 261244 + 2100020001 C2 R02

(39)
≤ C2R02.

(53)

Moreover, using union bound we derive

Tβ

{ET −1 ∩ Ex ∩ Ez} = 1 − P ET −1 ∪ Ex ∪ Ez ≥ 1 − .

(54)

P

N

That is, by deﬁnition of ET and ET −1 we have proved that

(53)

(54)

Tβ

P{ET } ≥ P {ET −1 ∩ Ex ∩ Ez} ≥ 1 − N ,

which

implies

that

for

all

k

=

0, 1, . . . , N

we

have

P{Ek}

≥

1−

kβ N

.

Then,

for

k

=

N

we

have

that

with probability at least 1 − β

AN f (yN ) − f (x∗)

(42) 1 0

2 1N

N −1 2

k

≤ 2 z − z 2 − 2 z − z 2 + αk+1 θk+1, z − z

k=0

N −1
+ αk2+1
k=0

N −1

θk+1

2 2

+

αk2+1

k=0

θk+1, ∇f (xk+1)

+ AN ε 4

(≤41) C2R02 . 2

34

(17) 1+3ν

1−ν

N 1+ν (ε/2) 1+ν we get that with probability at least 1 − β

Since AN ≥ 1+3ν 2

2 1+ν aMν1+ν

2

N

∗ 4aC2R02Mν1+ν

f (y ) − f (x ) ≤

1+3ν 1−ν .

N 1+ν ε 1+ν

In other words, clipped-SSTM with a = 16384 ln2 4βN achieves f (yN ) − f (x∗) ≤ ε with probability

at least 1 − β after O

2

2(1+ν)

Mν1+3ν R01+3ν

2

ε 1+3ν

2(1+ν)
ln 1+3ν

2

2(1+ν)

Mν1+3ν R01+3ν

2

ε 1+3ν β

iterations and requires

N −1

N −1 (34)

σ2αk2+1N

ln

N β

mk =

O max 1,

R2

k=0

k=0

0



 

N −1

σ2(k

+

4ν
1) 1+ν

2(1−ν)
ε 1+ν

N

ln

N

 

= O max N,

β

4



 k=0

Mν1+ν R02a2





2(1−ν) 2(1+3ν) 

(35)

 σ2ε 1+ν N 1+ν 

= O max N,

4





Mν1+ν

R02

ln3

N β







2

2(1+ν)

2

2(1+ν)

2

2(1+ν) 

=

O

max



M 1+3ν
ν

R 1+3ν
0

2(1+ν)
ln 1+3ν

M 1+3ν
ν

R 1+3ν
0

,

σ2R02

ln

M 1+3ν
ν

R 1+3ν
0



.



2



ε 1+3ν

2
ε 1+3ν β

ε2

2
ε 1+3ν β

 

oracle calls.

B.1.3 On the batchsizes and numerical constants

The obtained complexity result is discussed in details in the main part of the paper. Here we discuss
the choice of the parameters. For convenience, we provide all assumptions from Thm. B.1 on the
parameters below: ln 4N ≥ 2 (55) β

1−ν
(ε/2) 1+ν

20736N

σ2αk2+1

ln

4N β

α= 2ν

2 , mk = max 1,

C 2 R2

,

2 1+ν aMν1+ν

0

(56)

B = CR0 , a ≥ 16384 ln2 4N ,

(57)

16 ln 4βN β

1−ν
1−ν aCMν1+ν R01−ν ε 1+ν ≤ 16 ln 4N ,
β

2

1+ν 2

a

1+ν 2

C

1+ν

R01+ν

Mν

ε≤

1+3ν

,

100 2

(58)

ε 1−ν 1+3ν



2+3ν−ν2

 2(1+3ν) ≤ min  a 2+4ν+ 3+8ν−5ν2−6ν3
2 (1+ν)(1+3ν)

(1+ν)2 1+3ν
, a 4N 4+7ν+ 2+(17+ν+ν)2(ν12+−3ν3ν) 3 ln 2 β

ln1+ν



 1−ν2 1−ν2

1−ν

C R M 1+3ν

1+3ν
0

1+3ν
ν

 4N

β

(59)

35

 1+ν 1+ν 2(1+ν) 2(1+ν)

2

N =  2 1+3ν a 1+3ν C 1+32ν R01+3ν Mν1+3ν  + 1,

 

ε 1+3ν

 

√ C = 7.

(60)

We emphasize that (55), (58), and (59) are not restrictive at all since the target accuracy ε and conﬁdence level β are often chosen to be small enough, whereas a can be made large enough.
Next, one can notice that the assumptions on parameter a and batchsize mk contain huge numerical constants (see (56)-(57)) that results in large numerical constants in the expression for the number of iterations N and the total number of oracle calls required to guarantee accuracy ε of the solution. However, for the sake of simplicity of the proofs, we do not try to provide an analysis with optimal or near-optimal dependence on the numerical constants. Moreover, the main goal in this paper is to derive improved high-probability complexity guarantees in terms of O(·)-notation – such guarantees are insensitive to numerical constants by deﬁnition.
Finally, (56) implies that the batchsize at iteration k is



 

N σ2(k

+

4ν
1) 1+ν

2(1−ν)
ε 1+ν

ln

N

 

mk = Θ max 1,

β

4





a2Mν1+ν R02



meaning that for k ∼ N and a = O ln2 Nβ we have that the second term in the maximum is

proportional

to

N ε . 1+5ν 2(1−ν)

1+ν

1+ν

When

ν

is

close

to

1

and

σ2

0 it implies that mk is huge for big

enough k making the method completely impractical. Fortunately, this issue can be easily solved

without sacriﬁcing the oracle complexity of the method: it is suﬃcient to choose large enough a.

Corollary B.1. Let the assumptions of Thm. B.1 hold and


1+3ν
a = max 16384 ln2 4N , 5184 1+ν β 

· 2 σ C R 2(1+5ν)(1+2ν) (1+ν)2

2(1+3ν) 1+ν

4ν 1+ν

4ν 1+ν
0

M ε 2 1+ν ν

6ν 1+ν

ln 1+3ν 1+ν



4N 

β

 .

 

(61)

Then for all k = 0, 1, . . . , N − 1 we have mk = 1 and to achieve f (yN ) − f (x∗) ≤ ε with probability at least 1 − β clipped-SSTM requires





2

2(1+ν)

2

2(1+ν)



O max  Mν1+3ν R2 01+3ν



ε 1+3ν

2(1+ν)
ln 1+3ν

M 1+3ν
ν

R 1+3ν
0

, σ2R02 ln σ2R02 

2
ε 1+3ν β

ε2

ε2β 



(62)

iterations/oracle calls.

Proof. We start with showing that for the new choice of a we have mk = 1 for all k = 0, 1, . . . , N − 1. Indeed, using the assumptions on the parameters from Thm. B.1 we derive





20736N

σ2αk2+1

ln

4N β



5184N σ2(k

+

4ν
1) 1+ν

2(1−ν)
ε 1+ν



mk = max 1,

C 2 R2

= max 1,

4

0



a2Mν1+ν C2R02



k<N
≤







5184σ2N

1+5ν 1+ν

2(1−ν)
ε 1+ν



max 1,

4



a2Mν1+ν C2R02 



2(1+5ν)(1+2ν)

4ν

4ν



(39)



5184 · 2

(1+ν)(1+3ν)

σ2C

1+3ν

R 1+3ν
0

ln 4βN  (61)

≤ max 1,

1+ν

2

6ν

≤ 1.



a

1+3ν

M 1+3ν
ν

ε

1+3ν



36

That is, with the choice of the stepsize parameter a as in (61) the method uses unit batchsizes at each iteration. Therefore, iteration and oracle complexities coincide in this case. Next, we consider two possible situations.

1.

If

a = 16384 ln2

4N β

,

then

 1+ν 1+ν 2(1+ν) 2(1+ν)

2



2

2(1+ν)



(39)
N=

2 1+3ν a 1+3ν C

1+3ν

R 1+3ν
0

M 1+3ν
ν

+1=O

M 1+3ν
ν

R 1+3ν
0

2(1+ν)
ln N 1+3ν



2



ε 1+3ν





2



ε 1+3ν

 β







2

2(1+ν)

2

2(1+ν) 

= O  Mν1+3ν R2 01+3ν ln 21(+1+3νν) Mν1+3ν2R01+3ν  .

ε 1+3ν

ε 1+3ν β

2. If a = , then 1+3ν 2(1+5ν)(1+2ν) 2(1+3ν) 4ν

4ν

1+3ν

5184 1+ν ·2

(1+ν)2

σ

1+ν

C 1+ν R01+ν ln 1+ν

4N β

2 6ν
Mν1+ν ε 1+ν

 1+ν 1+ν 2(1+ν) 2(1+ν)

2

N (=39)  2 1+3ν a 1+3ν C 1+32ν R01+3ν Mν1+3ν  + 1

 

ε 1+3ν

 



2

2(1+ν)

4ν



M 1+3ν
ν

R 1+3ν
0

σ2R01+3ν

ln

4N β

σ2R02 σ2R02

= O

ε2 1+3ν

·

2

=O

6ν

M 1+3ν ε 1+3ν

ε2 ln ε2β .

ν

Putting all together we derive (62).

B.2 Convergence in the strongly convex case
In this section, we provide the full proof of Thm. 2.2 together with complete statement of the result. Note that due to strong convexity the solution x∗ is unique.

Theorem B.2. Assume that function f is µ-strongly convex and its gradients satisfy (3) with ν ∈ [0, 1], Mν > 0 on Q = B3R0 = {x ∈ Rn | x − x∗ 2 ≤ 3R0}, where R0 ≥ x0 − x∗ 2. Let ε > 0, β ∈ (0, 1) and for t = 1, . . . , τ

 1+ν 1+ν 2(1+ν) 2(1+ν)

2

Nt =  2 1+3ν at1+3ν(1C+ν)1(+t−3ν1) R012+3ν Mν1+3ν  + 1,

 

2

1+3ν

ε 1+3ν
t

 

µR02 εt = 2t+1 ,

µR0 τ = log2 2ε ,

ln 4Ntτ ≥ 2, β

√ C = 7,

1−ν

t

(εt/2) 1+ν

t

20736

·

2t−1Ntσ2(αkt +1)2

ln

4Ntτ β

α= 2ν

2 , mk = max 1,

C 2 R2

,

2 1+ν atMν1+ν

0

αkt +1 = αt(k + 1) 12+νν ,

C R0 B = 16 ln 4Ntτ ,
β

at = 16384 ln2 4Ntτ , β

(63) (64) (65) (66)

37

1−ν

1−ν 1+ν

atCMν1+ν R01−ν

εt ≤

(1−ν)(t−1)

,

16 · 2 2 ln 4Nβtτ

2

1+ν 2

a

1+ν 2

C

1+ν

R1+ν

Mν

εt ≤

t

0

,

1+3ν

(1+ν)(t−1)

100 2 · 2 2

(67)

1−ν
ε 1+3ν
t



2+3ν−ν2

 

a 2(1+3ν)
t

≤ min

23

 22+4ν+ 3+(18+ν−ν)5(ν1+−3ν6ν)

(1+ν)2

, a 4Ntτ ln 2 β

1+3ν
t
4+7ν+ 2+7ν+2ν2−3ν3
(1+ν)(1+3ν)

ln1+ν

 C R M . 4Ntτ  2 β

1−ν2 1+3ν

1−ν2 1+3ν
0

1−ν 1+3ν
ν

(1−ν 2 )(t−1)

2(1+3ν)

(68)

Then, after τ restarts R-clipped-SSTM produces xˆτ such that with probability at least 1 − β

f (xˆτ ) − f (x∗) ≤ ε.

(69)

That is, to achieve (69) with probability at least 1 − β the method requires

 Nˆ = O max

Mν µR01−ν

2 1+3ν

µR2

ln 0 ,

ε

Mν2 µ1+ν ε1−ν

1 1+3ν

2(1+ν)
ln 1+3ν

Mν1+23ν ln µRε 02  1+ν 1−ν 
µ 1+3ν ε 1+3ν β

(70)

iterations of Alg. 1 and



 

σ2

2
M 1+3ν
ν

ln

µR02

 

O max Nˆ, ln

ε  oracle calls.

 µε

µ ε β  1+ν 1−ν 1+3ν 1+3ν

(71)

Proof.

Applying

Thm.

B.1,

we

obtain

that

with

probability

at

least

1−

β τ

f (xˆ1) − f (x∗) ≤ µR02 . 4

Since f is µ-strongly convex we have

µ xˆ1 − x∗ 22 ≤ f (xˆ1) − f (x∗). 2

Therefore,

with

probability

at

least

1

−

β τ

f (xˆ1) − f (x∗) ≤ µR02 , 4

1 ∗ 2 R02

xˆ − x

2≤

. 2

From mathematical induction and the union bound for probability events it follows that inequalities

t

∗ µR02

f (xˆ ) − f (x ) ≤ 2t+1 ,

t ∗ 2 R02 xˆ − x 2 ≤ 2t

hold simultaneously for t = 1, . . . , τ with probability at least 1 − β. In particular, it means that after

τ=

log2

µR02 ε

− 1 restarts R-clipped-SSTM ﬁnds an ε-solution with probability at least 1 − β. The

38

total number of iterations Nˆ is

  τ N = O  M R ln M R τ  t
2 ε 2 ε β t=1

τ t=1

2 1+3ν
ν
(1+ν)t 1+3ν

2(1+ν)
1+3ν
0
2 1+3ν
t

2(1+ν) 1+3ν

2 1+3ν
ν
(1+ν)t 1+3ν

2(1+ν)
1+3ν
0
2 1+3ν
t

  τ = O  M2 Rµ R2 ln M2 Rµ R2 βτ  t=1

2 1+3ν
ν
(1+ν)t 1+3ν

2(1+ν) 1+3ν
0
2 1+3ν

2t 1+3ν
4 1+3ν
0

2(1+ν) 1+3ν

2 1+3ν
ν
(1+ν)t 1+3ν

2(1+ν) 1+3ν
0
2 1+3ν

2t 1+3ν
4 1+3ν
0

  τ = O  Mµ R2 ln Mµ R2 βτ  t=1

2 1+3ν
ν
2 1+3ν

(1−ν)t 1+3ν
2(1−ν) 1+3ν
0

2(1+ν) 1+3ν

2 1+3ν
ν
2 1+3ν

(1−ν)t 1+3ν
2(1−ν) 1+3ν
0

2

M 1+3ν
ν

max

(1−ν)τ
τ, 2 1+3ν

 = O


µ R 2 1+3ν

2(1−ν)
1+3ν
0

2(1+ν)
ln 1+3ν



2 1+3ν
M 2 τ  ν

(1−ν)τ 1+3ν

 2 µ R β 1+3ν

2(1−ν)
1+3ν
0

 = O max

Mν µR01−ν

2 1+3ν

µR2

ln 0 ,

ε

Mν2 µ1+ν ε1−ν

1 1+3ν

2(1+ν)
ln 1+3ν

2
M 1+3ν
ν

ln

µR02



ε ,

1+ν 1−ν

µ 1+3ν ε 1+3ν β

and the total number of oracle calls equals

τ Nt−1

 τ

τ σ2R2

2 (1−ν)t 

M 1+3ν
ν

2

1+3ν

τ

mtk = O max

Nt,

0 ln



2tε2

2

2(1−ν)

t=1 k=0

 t=1

t=1

t

µ

1+3ν

R 1+3ν
0

β



 

τ σ2 · 2t

2 (1−ν)τ 

M 1+3ν
ν

2

1+3ν

τ

= O max Nˆ,

ln



 t=1 µ2R02

2
µ R β  1+3ν

2(1−ν) 1+3ν

0



 

σ2

2
M 1+3ν
ν

ln

µR02

 

= O max Nˆ, ln

ε .

 µε

µ ε β  1+ν 1−ν 1+3ν 1+3ν

One can also derive a similar result for R-clipped-SSTM when stepsize parameter a is chosen as in Cor. B.1 for all restarts.

39

C SGD with clipping: missing details and proofs

C.1 Convex case
In this section, we provide a full statement of Thm. 3.1 together with its proof. The proof is based on a similar idea as the proof of the complexity bounds for clipped-SSTM.
Theorem C.1. Assume that function f is convex, achieves its minimum at a point x∗, and its gradients satisfy (3) with ν ∈ [0, 1], Mν on Q = B7R0 = {x ∈ Rn | x − x∗ 2 ≤ 7R0}, where R0 ≥ x0 − x∗ 2. Then, for all β ∈ (0, 1) and N such that
ln 4N ≥ 2, (72) β

we have that after N iterations of clipped-SGD with

R0

81N σ2

λ = γ ln 4N , m ≥ max 1, λ2 ln 4N

(73)

β

β

and stepsize

1−ν

ε 1+ν

R0

R01−ν

γ ≤ min 8Mν1+2ν , √2N ε 1+ν ν Mν1+1ν , 2Cν Mν ln 4βN ,

(74)

with probability at least 1 − β it holds that

f (x¯N ) − f (x∗) ≤ C2R02 ,

(75)

γN

where x¯N = N1

N −1 k=0

xk

and

C = 7.

(76)

In other words, clipped-SGD with γ = min

, , 1−ν
ε 1+ν
2
8Mν1+ν

R0

√

ν

1

2N ε 1+ν Mν1+ν

R01−ν 2Cν Mν ln 4βN

achieves f (x¯N ) −

f (x∗) ≤ ε with probability at least 1 − β after O

max

, ln 2
Mν1+ν R02
2
ε 1+ν

Mν R01+ν ε

Mν R01+ν εβ

iterations

and requires

 2

 Mν1+ν R02

O max

2 , max

 ε 1+ν

Mν R01+ν σ2R02 ε , ε2

 ln Mν R01+ν 
εβ 

(77)

oracle calls.

Proof. Since f (x) is convex and its gradients satisfy (3), we get the following inequality under

40

assumption that xk ∈ B7R0(x∗):

xk+1 − x∗

2 2

=

=

xk − γ∇f (xk, ξk) − x∗

2 2

xk − x∗

2 2

+

γ2

∇f (xk, ξk)

2 2

−

2γ

xk − x∗, ∇f (xk, ξk)

=

xk − x∗

2 2

+

γ2

∇f (xk) + θk

2 2

−

2γ

xk − x∗, ∇f (xk) + θk

(11)

≤

xk − x∗

2 2

+

2γ2

∇f (xk)

2 2

+

2γ2

θk

2 2

−

2γ

xk − x∗, ∇f (xk) + θk

1−ν
(≤21) xk − x∗ 22 − 2γ 1 − 2γ 1ε 1+ν Mν1+2ν

f (xk) − f (x∗)

+ 2γ2

θk

2 2

−2γ

xk − x∗, θk

+

2γ2ε

2ν 1+ν

2
Mν1+ν

,

where θk = ∇f (xk, ξk) − ∇f (xk) and the last inequality follows from the convexity of f . Using notation Rk d=ef xk − x∗ 2, k > 0 we derive that for all k ≥ 0

1−ν
Rk2+1 ≤ Rk2−2γ 1 − 2γ 1ε 1+ν Mν1+2ν

f (xk) − f (x∗) +2γ2 θk 22−2γ

xk − x∗, θk

+2γ

2ε

2ν 1+ν

2
Mν1+ν

1−ν

2 (74)

under assumption that xk ∈ B7R0(x∗). Let us deﬁne A = 2γ

1 − 2γ

1 ε

1+ν Mν1+ν

≥ γ > 0, then

A

f (xk) − f (x∗)

≤ Rk2 − Rk2+1 + 2γ2

θk

2 2

−

2γ

xk − x∗, θk

+

2γ2ε

2ν 1+ν

2
Mν1+ν

under assumption that xk ∈ B7R0(x∗). Summing up these inequalities for k = 0, . . . , N − 1, we obtain

A N−1 f (xk) − f (x∗) N
k=0

1 N−1 2

2

2 2ν

2 1+ν

2γ2 N−1

2

≤ N

Rk − Rk+1 + 2γ ε 1+ν Mν

+ N

θk 2

k=0

k=0

2γ N−1 −
N
k=0

xk − x∗, θk

12 2

2 2ν

2 1+ν

2γ2 N−1

2

= N R0 − RN + 2γ ε 1+ν Mν

+ N

θk 2

k=0

2γ N−1 −
N
k=0

xk − x∗, θk

N −1

under

assumption

that

xk

∈

B7R0 (x∗).

Noticing

that

for

x¯N

=

1 N

xk Jensen’s inequality gives

k=0

f (x¯N ) = f

N −1
N1 xk
k=0

N −1
≤ N1 f (xk), we have
k=0

2

N −1

N −1

AN

f (x¯N ) − f (x∗)

≤

R02

− RN2

+

2γ2N

ε

2ν 1+ν

Mν1+ν

+ 2γ2

θk

2 2

−

2γ

xk − x∗, θk

k=0

k=0

(78)

41

under assumption that xk ∈ B7R0(x∗) for k = 0, 1, . . . , N − 1. Taking into account that f (x¯N ) − f (x∗) ≥ 0 and changing the indices we get that for all k = 0, 1, . . . , N

2

k−1

k−1

Rk2

≤

R02

+

2γ2kε

2ν 1+ν

Mν1+ν

+ 2γ2

θl

2 2

−

2γ

xl − x∗, θk .

l=0

l=0

(79)

under assumption that xl ∈ B7R0(x∗) for l = 0, 1, . . . , k − 1. The remaining part of the proof is based

on the analysis of inequality (79). In particular, via induction we prove that for all k = 0, 1, . . . , N

with

probability

at

least

1−

kβ N

the

following

statement

holds:

inequalities

(79)

2ν 2 t−1

Rt2 ≤ R02 + 2γ2tε 1+ν Mν1+ν + 2γ2

l=0

t−1

θk

2 2

−

2γ

l=0

xk − x∗, θk

≤ C2R02

(80)

hold for t = 0, 1, . . . , k simultaneously where C is deﬁned in (76). Let us deﬁne the probability

event when this statement holds as Ek.

Then,

our

goal

is

to

show

that

P{Ek}

≥

1

−

kβ N

for all

k = 0, 1, . . . , N . For t = 0 inequality (80) holds with probability 1 since C ≥ 1. Next, assume that for

some

k

=

T −1

≤

N −1

we

have

P{Ek}

=

P{ET −1}

≥

1 − (T −N1)β .

Let

us

prove

that

P{ET }

≥

1

−

Tβ N

.

First of all, probability event ET −1 implies that xt ∈ B7R0(x∗) for t = 0, 1, . . . , T − 1, and, as a

consequence, (79) holds for k = T . Since ∇f (x) is (ν, Mν)-Hölder continuous on B7R0(x∗), we have

that probability event ET −1 implies

∇f (xt) 2 (≤3) Mν xt − x0 ν ≤ Mν Cν R0ν (≤74) λ2

(81)

for t = 0, . . . , T − 1. Next, we introduce new random variables:

x∗ − xl, if x∗ − xl 2 ≤ CR0,

ηl =

(82)

0,

otherwise,

for l = 0, 1, . . . T − 1. Note that these random variables are bounded with probability 1, i.e. with

probability 1 we have

ηl 2 ≤ CR0.

(83)

Using the introduced notation, we obtain that ET −1 implies

(73),(74),(79),(80),(82)

T −1

T −1

RT2

≤

2R02 + 2γ

θl, ηl + 2γ2

θl 22.

l=0

l=0

Finally, we do some preliminaries in order to apply Bernstein’s inequality (see Lemma A.2) and obtain that ET −1 implies

(11)

T −1

T −1

T −1

RT2 ≤ 2R02 + 2γ

θ

u l

,

ηl

+ 2γ

θ

b l

,

ηl

+ 4γ2

l=0

l=0

l=0

x
T −1
+ 4γ2 Eξl
l=0

y

T −1

θlu

2 2

+ 4γ2

θlb 22,

l=0

{

|

θlu

2 2

−

Eξl

z

θlu

2 2

(84)

42

where we introduce new notations:

θlu d=ef ∇f (xl, ξl) − Eξl ∇f (xl, ξl) , θlb d=ef Eξl ∇f (xl, ξl) − ∇f (xl),

(85)

θl = θlu + θlb.
It remains to provide tight upper bounds for x, y, z, { and |, i.e. in the remaining part of the proof we show that x + y + z + { + | ≤ δC2R02 for some δ < 1.
Upper bound for x. First of all, since Eξl[θlu] = 0 summands in x are conditionally unbiased:

Eξl [2γ

θ

u l

,

ηl

] = 0.

Secondly, these summands are bounded with probability 1:

(29),(83)

|2γ

θ

u l

,

ηl

|

≤

2γ θlu 2 ηl 2

≤

4γ λC R0 .

Finally, one can bound conditional variances σl2 d=ef Eξl

4γ2

θ

u l

,

ηl

2

in the following way:

(83)

σl2

≤

Eξl

4γ2

θlu

2 2

ηl

2 2

≤ 4γ2(CR0)2Eξl

θlu

2 2

.

In other words, sequence {2γ

θ

u l

,

ηl

}l≥0 is a bounded martingale diﬀerence sequence with bounded

conditional variances {σl2}l≥0. Therefore, we can apply Bernstein’s inequality, i.e., we apply Lemma A.2 with Xl = 2γ θlu, ηl , c = 4γλCR0 and F = c2 ln6 4βN and get that for all b > 0

T −1

T −1 2

b2

P

Xl > b and

σl ≤ F

≤ 2 exp − 2F + 2cb/3

l=0

l=0

or, equivalently, with probability at least 1 − 2 exp − 2F +b22cb/3

T −1
either σl2 > F or
l=0

T −1
Xl
l=0

≤ b.

|x|

The choice of F will be clariﬁed further, let us now choose b in such a way that 2 exp − 2F +b22cb/3 This implies that b is the positive root of the quadratic equation

b2 − 2c ln 4βN b − 2F ln 4N = 0,

3

β

hence

=

β 2N

.

b = c ln 4βN + 3

c2 ln2 4βN

4N c ln 4βN

+ 2F ln =

+

9

β

3

4N

4N

= c ln β = 4γλCR0 ln β .

4c2 ln2 4βN 9

43

That

is,

with

probability

at

least

1

−

β 2N

T −1 2

4N

either σl > F or |x| ≤ 4γλCR0 ln β .

l=0

probability event Ex

Next, we notice that probability event ET −1 implies that

T −1
σl2
l=0

≤
T ≤N
≤

T −1
4γ2(CR0)2 Eξl
l=0
72γ2(CR0)2σ2 N ≤ m

θlu 22 (≤32) 72γ2(CR0)2σ2 mT c2 ln 4βN = F,
6

where the last inequality follows from c = 4γλCR0 and simple arithmetic. Upper bound for y. First of all, we notice that probability event ET −1 implies

2γ

θ

b l

,

ηl

This implies that

b

(30),(83) 4σ2

8γ σ2 C R0

≤ 2γ θl ηl 2
2

≤

2γ mλ CR0 =

. mλ

T −1 b

T ≤N 8γσ2CR0N (73) 8

4N

y = 2γ

θl , ηl ≤

mλ ≤ 81 λγCR0 ln β .

l=0

Upper bound for z. We derive the upper bound for z using the same technique as for x. First of all, we notice that the summands in z are conditionally unbiased:

Eξl 4γ2

θlu

2 2

−

Eξl

θlu

2 2

= 0.

Secondly, the summands are bounded with probability 1:

4γ2

θlu

2 2

−

Eξl

θlu

2 2

≤ 4γ2

θlu

2 2

+

Eξl

= 32γ2λ2 d=ef c1.

θlu

2 2

(29)
≤ 4γ2 4λ2 + 4λ2

(86)

Finally, one can bound conditional variances σˆl2 d=ef Eξl 4γ2 ing way:

θlu

2 2

−

Eξl

θlu

2 2

2
in the follow-

(86)
σˆl2 ≤ c1Eξl 4γ2

θlu

2 2

−

Eξl

θlu

2 2

≤

4γ2c1Eξl

θlu

2 2

+

Eξl

θlu

2 2

= 8γ2c1Eξl

θlu

2 2

.

(87)

In other words, sequence 4γ2

θlu

2 2

−

Eξl

θlu

2 2

is a bounded martingale diﬀerence sequence
l≥0

with bounded conditional variances {σˆl2}l≥0. Therefore, we can apply Bernstein’s inequality, i.e.

44

we apply Lemma A.2 with Xl = Xˆl = 4γ2

c21

ln

4N β

and get that for all b > 0

18

θlu

2 2

−

Eξl

θlu

2 2

, c = c1 = 32γ2λ2 and F = F1 =

T −1

T −1

P

Xˆ > b and σˆ2 ≤ F

≤ 2 exp −

b2

l

l

1

2F1 + 2c1b/3

l=0

l=0

or, equivalently, with probability at least 1 − 2 exp − 2F1+b22c1b/3

T −1
either σˆl2 > F1 or
l=0

T −1
Xˆl ≤ b.
l=0
|z|

As in our derivations of the upper bound for x we choose such b that 2 exp − 2F1+b22c1b/3 = 2βN , i.e.,

c1

ln

4N β

b=

+

3

c21 ln2 4βN + 2F1 ln 4N ≤ c1 ln 4N = 32γ2λ2 ln 4N .

9

β

β

β

That

is,

with

probability

at

least

1

−

β 2N

T −1 2

2 2 4N

either σˆl > F1 or |z| ≤ 32γ λ ln β .

l=0

probability event Ez

Next, we notice that probability event ET −1 implies that

T −1
σˆl2
l=0

(87)
≤
T ≤N
≤

T −1
8γ2c1 Eξl
l=0

θlu 22 (≤32) 144γ2c1σ2 mT

144γ2c1σ2 N = c21 ln 4βN ≤ F1.

m

18

Upper bound for {. The probability event ET −1 implies

T −1
{ = 4γ2 Eξl
l=0

θlu

2 2

(≤32) 72γ2σ2 T −1 1 T ≤≤N 72γ2N σ2 (≤73) 8 λ2γ2 ln 4N .

m

m

9

β

l=0

Upper bound for |. Again, we use corollaries of probability event ET −1:

T −1 2

(30) b2

2 4 T T ≤N

2 4 N (73) 64 λ2γ2 ln2 4βN

| = 4γ

θl 2 ≤ 64γ σ m2λ2 ≤ 64γ σ m2λ2 ≤ 6561 N .

l=0

45

Now we summarize all bound that we have: probability event ET −1 implies

(79)

T −1

T −1

RT2 ≤ 2R02 + 2γ2

θl

2 2

−

2γ

xl − x∗, θl

l=0

l=0

(84)
≤ 2R02 + x + y + z + { + |,

y≤

8

4N

λγCR0 ln ,

{ ≤ 8 λ2γ2 ln 4N ,

81

β

9

β

64 λ2γ2 ln2 4βN | ≤ 6561 N ,

T −1

T −1

σl2 ≤ F,

σˆl2 ≤ F1

l=0

l=0

and where

(T − 1)β

β

β

P{ET −1} ≥ 1 − N , P{Ex} ≥ 1 − 2N , P{Ez} ≥ 1 − 2N ,

Ex = Ez =

T −1 2

4N

either σl > F or |x| ≤ 4γλCR0 ln β ,

l=0

T −1 2

2 2 4N

either σˆl > F1 or |z| ≤ 32γ λ ln β .

l=0

Taking into account these inequalities and our assumptions on λ and γ (see (73) and (74)) we get that probability event ET −1 ∩ Ex ∩ Ez implies

(79)

T −1

T −1

RT2 ≤ 2R02 + 2γ2

θl

2 2

−

2γ

xl − x∗, θl

l=0

l=0

≤ 2R02 + 74 + 5867 + 4196 + 4441 + 32614489 C2R02 (≤76) C2R02. (88)

Moreover, using union bound we derive

Tβ

{ET −1 ∩ Ex ∩ Ez} = 1 − P ET −1 ∪ Ex ∪ Ez ≥ 1 − .

(89)

P

N

That is, by deﬁnition of ET and ET −1 we have proved that

(88)

(89)

Tβ

P{ET } ≥ P {ET −1 ∩ Ex ∩ Ez} ≥ 1 − N ,

which

implies

that

for

all

k

=

0, 1, . . . , N

we

have

P{Ek}

≥

1−

kβ N

.

Then,

for

k

=

N

we

have

that

with probability at least 1 − β

(78)

N −1

N −1

AN f (x¯N ) − f (x∗) ≤ 2R02 + 2γ2

θk

2 2

−

2γ

xk − x∗, θk

k=0

k=0

(80)
≤ C2R02.

46

1−ν

2 (74)

Since A = 2γ

1 − 2γ

1 ε

1+ν Mν1+ν

≥ γ we get that with probability at least 1 − β

f (x¯N ) − f (x∗) ≤ C2R02 = C2R02 .

AN

γN

When

γ = min

1−ν

ε 1+ν

R0

R01−ν

8Mν1+2ν , √2N ε 1+ν ν Mν1+1ν , 2Cν Mν ln 4βN

we have that with probability at least 1 − β



2

 8C2Mν1+ν R2

√

1

ν

2C2Mν1+ν R0ε 1+ν



2C2+ν Mν R01+ν

ln

4N β



f (x¯N ) − f (x∗) ≤ max

0,

√

,

.

1−ν
 ε 1+ν N

N

N

Next, we estimate the iteration and oracle complexities of the method and consider 3 possible

situations.

1−ν

1. If γ =

ε 1+ν
2

, then with probability at least 1 − β

8Mν1+ν

2

N

∗ 8C2Mν1+ν R02

f (x¯ ) − f (x ) ≤

1−ν

.

ε 1+ν N

In other words, clipped-SGD achieves f (x¯N ) − f (x∗) ≤ ε with probability at least 1 − β after

2  Mν1+ν R02
O 2  ε 1+ν

iterations and requires

(73)

N 2σ2γ2 ln Nβ

N m = O max N, R2

0



 

N ε 2(1−ν)
2 1+ν

σ2

ln

N

 

= O max N,

β

4





Mν1+ν R02



 2

2 

 Mν1+ν R02 σ2R02 Mν1+ν R02  = O max  ε 1+2ν , ε2 ln ε 1+2ν β 

oracle calls.

2. If γ = √ Rν0 1 , then with probability at least 1 − β
2N ε 1+ν Mν1+ν

√

1

ν

f (x¯N ) − f (x∗) ≤

2C2Mν1+ν R0ε 1+ν

√

.

N

In other words, clipped-SGD achieves f (x¯N ) − f (x∗) ≤ ε with probability at least 1 − β after

2  Mν1+ν R02
O 2  ε 1+ν

47

iterations and requires

(73)

N 2σ2γ2 ln Nβ

N m = O max N, R2

0

=O

max

N, N σ2 ln Nβ

2ν

2

ε 1+ν Mν1+ν

 2

2 

 Mν1+ν R02 σ2R02 Mν1+ν R02  = O max  ε 1+2ν , ε2 ln ε 1+2ν β 

oracle calls. 3. If γ = 2CνRM01ν−lνn 4N , then with probability at least 1 − β
β

f (x¯N ) − f (x∗) ≤ 2C2+ν Mν R01+ν ln 4βN . N

In other words, clipped-SGD achieves f (x¯N ) − f (x∗) ≤ ε with probability at least 1 − β after

 Mν R1+ν

ln

Mν R01+ν



O

0

εβ


ε

iterations and requires
(73)
Nm = O
=O

max max

N, N 2σ2γ2 ln Nβ R02
Mν R01+ν σ2R02 ε , ε2

= O max ln Mν R01+ν
εβ

N 2σ2

N, M 2R2ν ln N

ν0

β

oracle calls.
2
Putting all together and noticing that ln Mν1+2ν R02 = O
ε 1+ν β

ln Mν R01+ν
εβ

we get the desired result.

As for clipped-SSTM it is possible to get rid of using large batchsizes without sacriﬁcing the oracle complexity via a proper choice of γ, i.e., it is suﬃcient to choose

1−ν

ε 1+ν

R0

R01−ν

R0

γ = min 8Mν1+2ν , √2N ε 1+ν ν Mν1+1ν , 2Cν Mν ln 4βN , 9σN ln 4βN .

C.2 Strongly convex case

In this section, we provide a full statement of Thm. 3.2 together with its proof. Note that due to strong convexity the solution x∗ is unique.

Theorem C.2. Assume that function f is µ-strongly convex and its gradients satisfy (3) with ν ∈ [0, 1], Mν > 0 on Q = B2R0 = {x ∈ Rn | x − x∗ 2 ≤ 2R0}, where R0 ≥ x0 − x∗ 2. Let ε > 0, β ∈ (0, 1), and for all t = 1, . . . , τ





2

 

2C

4

Mν1+ν

R02

4C2+ν Mν R01+ν ln 16C2(1++ννM)tν R01+ν 
2 2 εtβ

Nt = max

2

,

(1+ν)t

,

  

2tεt1+ν

2 2 εt

 



µR02 εt = 2t+1 ,

48

R0

81Ntσ2

4Ntτ

λt = 2 2t γ ln 4Ntτ ,

mt ≥ max 1, λ2 ln 4Ntτ

,

ln β

≥ 2,

t

β

t

β

 1−ν

γt = min  εt1+ν2 ,



8M

1+ν
ν

R0

t√

ν

1,

2 2 2Ntεt1+ν Mν1+ν

 R01−ν  . 21+ (1−2ν)t Cν Mν ln 4Nβtτ 

Then R-clipped-SGD achieves f (x¯τ ) − f (x∗) ≤ ε with probability at least 1 − β after

2 1+ν

µR02

2 1+ν

µR02

D

O max D1 ln ε , D2 , max D1 ln ε , D2 ln β

iterations of Alg. 3 in total and requires

O max D11+2ν ln µRε 02 , D21+2ν , max D1 ln µRε 02 , D2, µσε2 ln Dβ (90)

oracle calls, where

Mν D1 = µR1−ν ,
0

D2 = 1+Mν ν1−ν , µ2ε2

D = D2 ln µR02 . ε

Proof.

Applying

Thm.

C.1,

we

obtain

that

with

probability

at

least

1−

β τ

f (xˆ1) − f (x∗) ≤ µR02 . 4

Since f is µ-strongly convex we have

µ xˆ1 − x∗ 22 ≤ f (xˆ1) − f (x∗). 2

Therefore,

with

probability

at

least

1

−

β τ

f (xˆ1) − f (x∗) ≤ µR02 , 4

1 ∗ 2 R02

xˆ − x

2≤

. 2

From mathematical induction and the union bound for probability events it follows that inequalities

t

∗ µR02

f (xˆ ) − f (x ) ≤ 2t+1 ,

t ∗ 2 R02 xˆ − x 2 ≤ 2t

hold simultaneously for t = 1, . . . , τ with probability at least 1 − β. In particular, it means that after

τ=

log2

µR02 ε

− 1 restarts R-clipped-SGD ﬁnds an ε-solution with probability at least 1 − β. The

total number of iterations Nˆ is

τ


τ

2  Mν1+ν R2 Mν R1+ν

 Mν R1+ν τ 

Nt = O  max

0
, ln  2

0
(1+ν)t

0
(1+ν)t

t=1

t=1

 2tεt1+ν 2 2 εt 2 2 εtβ 


τ

2

(1−ν)t

 Mν1+ν · 2 1+ν

(1−ν)t
Mν · 2 2


(1−ν)τ
Mν · 2 2 τ 

= O  max

2 , 2(1−ν) µR1−ν ln µR1−ν β



t=1

 µ 1+ν R0 1+ν

0

0





2

 Mν1+ν

µR2 

Mν

Mν ln 0 

µR2

1−ν  µR2 2

= O max

,

ln

ε · max ln 0 ,

0



2
 µ R 1+ν

2(1−ν) 1+ν

µR01−ν

1+ν 1−ν
µ 2 ε 2 β

ε

ε

0

2 1+ν

µR02

2 1+ν

µR02

D

= O max D1 ln ε , D2 , max D1 ln ε , D2 ln β ,

49

where

Mν D1 = µR1−ν ,
0

D2 = 1+Mν ν1−ν , µ2ε2

Finally, the total number of oracle calls equals

D = D2 ln µR02 . ε

τ Nt−1 t

τ

τ σ2R02 Mν R01+ν τ

mk = O max

Nt,

2tε2 ln (1+ν)t

t=1 k=0

t=1 t=1 t 2 2 εtβ

=

O

max

τ
Nˆ,

σ2 · 2t D ln

t=1 µ2R02 β

= O max Nˆ, σ2 ln D . µε β

50

D Additional experimental details
D.1 Main experiment hyper-parameters
In our experiments, we use standard implementations of Adam and SGD from PyTorch [32], we write only the parameters we changed from the default.
To conduct these experiments we used Nvidia RTX 2070s. The longest experiment (evolution of the noise distribution for image classiﬁcation task) took 53 hours (we iterated several times over train dataset to build better histogram, see Appendix D.3).
D.1.1 Image classiﬁcation
For ResNet-18 + ImageNet-100 the parameters of the methods were chosen as follows:
• Adam: lr = 1e − 3 and a batchsize of 4 × 32
• SGD: lr = 1e − 2, momentum = 0.9 and a batchsize of 32
• clipped-SSTM: ν = 1, stepsize parameter α = 1e − 3 (in code we use separately lr = 1e − 2 and L = 10 and α = lLr ), norm clipping with clipping parameter B = 1 and a batchsize of 2 × 32. We also upper bounded the ratio / Ak Ak+1 by 0.99 (see a_k_ratio_upper_bound parameter in code).
• clipped-SGD: lr = 5e − 2, momentum = 0.9, coordinate-wise clipping with clipping parameter B = 0.1 and a batchsize of 32
The main two parameters that we grid-searched were lr and batchsize. For both of them we used logarithmic grid (i.e. for lr we used 1e − 5, 2e − 5, 5e − 5, 1e − 4, . . . , 1e − 2, 2e − 2, 5e − 2 for Adam). Batchsize was chosen from 32, 2 · 32, 4 · 32 and 8 · 32. For SGD we also tried various momentum parameters.
For clipped-SSTM and clipped-SGD we used clipping level of 1 and 0.1 respectively. Too small choice of the clipping level, e.g. 0.01, slow downs the convergence signiﬁcantly.
Another important parameter for clipped-SSTM here, was a_k_ratio_upper_bound – we used it to upper bound the maximum ratio of Ak/Ak+1. Without this modiﬁcation the method is to conservative. e.g., after 104 steps / Ak Ak+1 ∼ 0.9999. Eﬀectively, it can be seen as momentum parameter of SGD.
D.1.2 Text classiﬁcation
For BERT + CoLA the parameters of the methods were chosen as follows:
• Adam: lr = 5e − 5, weight_decay = 5e − 4 and a batchsize of 32
• SGD: lr = 1e − 3, momentum = 0.9 and a batchsize of 32
• clipped-SSTM: ν = 1, stepsize parameter α = 8e − 3, norm clipping with clipping parameter B = 1 and a batchsize of 8 × 32
• clipped-SGD: lr = 2e − 3, momentum = 0.9, coordinate-wise clipping with clipping parameter B = 0.1 and a batchsize of 32
51

There we used the same grid as in the previous task. The main diﬀerence here is that we didn’t bound clipped-SSTM Ak/Ak+1 ratio – there are only ∼ 300 steps of the method (because the batch size is 8 · 32), thus the the method is still not too conservative.

D.2 On the relation between stepsize parameter α and batchsize

In our experiments, we noticed that clipped-SSTM show similar results when the ration bs2/α is kept unchanged, where bs is batchsize (see Fig. 3). We compare the performance of clipped-SSTM with 4 diﬀerent choices of α and the batchsize.

loss loss accuracy

0.70 0.65 0.60 0.55
0.0

Train loss, BERT = 40 0, bs = 1 32 = 41 0, bs = 2 32 = 42 0, bs = 4 32 = 43 0, bs = 8 32
0.5 ba1t.c0h co1un.5t 2.0 1e23.5

0.60 0.58 0.56 0.54 0.52 0.0

Validation loss, BERT 0.5 ba1t.0ch co1u.n5t 2.0 1e23.5

75 74 73 72 71
0.0

Validation accuracy, BERT 0.5 b1a.t0ch cou1n.5t 2.0 1e23.5

Figure 3: Train and validation loss + accuracy for clipped-SSTM with diﬀerent parameters. Here α0 = 0.000125, bs means batchsize. As we can see from the plots, increasing α 4 times and batchsize 2 times almost does not aﬀect the method’s behavior.

Thm. B.1 explains this phenomenon in the convex case. For the case of ν = 1 we have (from (34) and (39)):

1 α∼ ,
aM1

αk ∼ kα,

N aσ2αk2+1 mk ∼ C2R2 ln 4N ,
0β

1

1

N

∼

a

2

C

R0

M

2
1

∼

CR0 ,

1

11

ε2

α2ε2

whence

CR0aσ2α2(k + 1)2

σ2α2(k + 1)2

1

mk ∼ 1 1

∼1

1

∼ α2,

α

2

ε2

C 2 R02

ln

4N β

α

2

αM1ε

2

C R0

ln

4N β

where the dependencies on numerical constants and logarithmic factors are omitted. Therefore, the observed empirical relation between batchsize (mk) and α correlates well with the established theoretical results for clipped-SSTM.

D.3 Evolution of the noise distribution
In this section, we provide our empirical study of the noise distribution evolution along the trajectories of diﬀerent optimizers. As one can see from the plots, the noise distribution for ResNet-18 + ImageNet-100 task is always close to Gaussian distribution, whereas for BERT + CoLA task it is signiﬁcantly heavy-tailed.

52

density

density

Adam, iteration 0 0.6 0.4 0.2 0.0 2.5 5.0 7.5 10.0
noise norm SGD, iteration 0 0.6 0.4 0.2 0.0 2.5 5.0 7.5
noise norm clipped-SSTM, iteration 0 0.6 0.4 0.2 0.0 2.5 5.0 7.5
noise norm clipped-SGD, iteration 0
0.6 0.4 0.2 0.0 2.5 5.0 7.5
noise norm

density

density

density

density

Adam, iteration 500 0.6 0.4 0.2 0.00.0 2.5 5.0 7.5
noise norm 0.6 SGD, iteration 500
0.4
0.2
0.0 2.5 5.0 7.5 noise norm
clipped-SSTM, iteration 500 0.6 0.4 0.2 0.0 0 2 4 6
noise norm 0.6 clipped-SGD, iteration 500
0.4
0.2
0.0 2.5 5.0 7.5 noise norm

density

density

density

density

Adam, iteration 1000
0.6 0.4 0.2 0.0 0 2 4 6
noise norm SGD, iteration 1000 0.6
0.4
0.2
0.0 2 4 6 noise norm
clipped-SSTM, iteration 1000 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm
clipped-SGD, iteration 1000 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm

density

density

density

density

0.8 Adam, iteration 1500 0.6 0.4 0.2 0.0 0 2 4 6
noise norm SGD, iteration 1500 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm
clipped-SSTM, iteration 1500 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm
clipped-SGD, iteration 1500 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm

density

density

density

density

0.8 Adam, iteration 2000 0.6 0.4 0.2 0.0 0 2 4 6
noise norm SGD, iteration 2000 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm
clipped-SSTM, iteration 2000 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm
clipped-SGD, iteration 2000 0.6
0.4
0.2
0.0 2.5 5.0 7.5 noise norm

Figure 4: Evolution of the noise distribution for BERT + CoLA task.

density

density

density

density

Adam, iteration 0

0.3

0.2

0.1

0.0

15

20

noise norm

SGD, iteration 0

0.3 0.2 0.1 0.0 15 20
noise norm clipped-SSTM, iteration 0

0.3

0.2

0.1

0.0

15

20

noise norm

0.4 clipped-SGD, iteration 0

0.3

0.2

0.1

0.0

15

20

noise norm

density

density

density

density

Adam, iteration 25000 0.3

0.2

0.1

0.0 10 15 20 noise norm

SGD, iteration 25000 0.4

0.3

0.2

0.1

0.0 5

10

noise norm

clipped-SSTM, iteration 25000

0.6

0.4

0.2

0.0 2 4 6 noise norm
1.00clipped-SGD, iteration 25000 0.75 0.50 0.25 0.00 0 1noise n2orm 3

density

density

density

density

Adam, iteration 50000

0.3

0.2

0.1

0.0 5

10

15

noise norm

0.4 SGD, iteration 50000

0.3

0.2

0.1

0.0 5

10

noise norm

clipped-SSTM, iteration 50000

0.6

0.4

0.2

0.0

24

noise norm

1.00clipped-SGD, iteration 50000

0.75

0.50

0.25

0.00 0 noise n2orm

density

density

density

density

Adam, iteration 75000

0.4

0.3

0.2

0.1

0.0 5 10 noise norm

0.4 SGD, iteration 75000

0.3

0.2

0.1

0.0 5

10

noise norm

clipped-SSTM, iteration 75000

0.6

0.4

0.2

0.0 0 2 4 noise norm

clipped-SGD, iteration 75000

0.8

0.6

0.4

0.2

0.0 0

2

noise norm

density

density

density

density

Adam, iteration 100000 0.4

0.3

0.2

0.1

0.0

5

10

noise norm

SGD, iteration 100000 0.4

0.3

0.2

0.1

0.0 2.5 n5o.0ise no7r.m5 10.0

clipped-SSTM, iteration 100000

0.6

0.4

0.2

0.0 0 2 4 noise norm

clipped-SGD, iteration 100000

0.8

0.6

0.4

0.2

0.0 0

2

noise norm

density

density

Figure 5: Evolution of the noise distribution for ResNet-18 + ImageNet-100 task.

53

