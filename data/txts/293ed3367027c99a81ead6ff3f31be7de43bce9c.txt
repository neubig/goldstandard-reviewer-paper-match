arXiv:1604.03632v4 [cs.GT] 30 Apr 2019

Strategyproof Peer Selection using Randomization, Partitioning, and Apportionment
Haris Aziz
UNSW Sydney and Data61 CSIRO, Sydney 2052, Australia
Omer Lev
Ben-Gurion University of the Negev, Beersheba 8410501, Israel
Nicholas Mattei
Tulane University, New Orleans, LA 70115, USA
Jeﬀrey S. Rosenschein
Hebrew University of Jerusalem, Jerusalem 91904, Israel
Toby Walsh
UNSW Sydney and Data61 CSIRO, Sydney 2052, Australia

Abstract
Peer reviews, evaluations, and selections are a fundamental aspect of modern science. Funding bodies the world over employ experts to review and select the best proposals from those submitted for funding. The problem of peer selection, however, is much more general: a professional society may want to give a subset of its members awards based on the opinions of all members; an instructor for a Massive Open Online Course (MOOC) or an online course may want to crowdsource grading; or a marketing company may select ideas from group brainstorming sessions based on peer evaluation.
We make three fundamental contributions to the study of peer selection, a speciﬁc type of group decision-making problem, studied in computer science, economics, and political science. First, we propose a novel mechanism that is strategyproof, i.e., agents cannot beneﬁt by reporting insincere valuations. Second, we demonstrate the eﬀectiveness of our mechanism by a comprehensive
Email addresses: haris.aziz@data61.csiro.au (Haris Aziz), omerlev@bgu.ac.il (Omer Lev), nsmattei@tulane.edu (Nicholas Mattei), jeff@cs.huji.ac.il (Jeﬀrey S. Rosenschein), toby.walsh@data61.csiro.au (Toby Walsh)
This is a signiﬁcantly revised and expanded version of our conference paper from AAAI 2016 [3]. This version introduces the exact version of Dollar Partition along with new proofs and a new experiment.

Preprint submitted to Elsevier

May 1, 2019

simulation-based comparison with a suite of mechanisms found in the literature. Finally, our mechanism employs a randomized rounding technique that is of independent interest, as it solves the apportionment problem that arises in various settings where discrete resources such as parliamentary representation slots need to be divided proportionally.
Keywords: peer review; crowdsourcing; algorithms; allocation
1. Introduction
Since the beginning of civilization, societies have been selecting small groups from within. Athenian society, for example, selected a random subset of citizens to participate in the Boule, the council of citizens that ran daily aﬀairs in Athens. Peer review, evaluation, and selection has been the main process by which scientiﬁc conferences have selected a subset of papers for publication. Increasingly, peer evaluation is becoming popular and necessary to scale grading in MOOCs (Massive Open Online Courses, e.g., Coursera and EdX) [47, 38, 14]. In all of these peer selection settings, however, we do not wish to select an arbitrary subset of size k, but the “best k”, and we need, therefore, a procedure in which the candidates are rated according to the opinions of the group. In peer selection problems we are not seeking an external, “independent” agent to make choices, but desire a crowdsourced approach, in which participants are those making the selection. Mechanisms for peer selection and the properties of these mechanisms receive considerable attention within economics, political science, and computer science [2, 34, 24, 19, 26, 25, 33, 52].
Our initial motivation comes from the recent U.S. National Science Foundation (NSF) “mechanism design pilot,” which was an attempt to spread the review load amongst all submitters of proposals [25, 45]. The program uses “reviewers assigned from among the set of PIs whose proposals are being reviewed.” Reviewers’ own proposals get “supplemented with ‘bonus points’ depending upon the degree to which his or her ranking agrees with the consensus ranking ([56], Page 46).” This mechanism employed by the NSF is not strategyproof; reviewers are incentivized to guess what others are thinking, not to provide their honest feedback. Hence the mechanism induces a type of Keynesian “Beauty Contest” [30] where the incentives are misaligned and humans have been shown to not behave truthfully [17]. Removing the bonus may be worse, as reviewers would then be able to increase the chance of their own proposal being accepted by rating other proposals lower [46]. In either case, reviewers can beneﬁt from reporting something other than their true values. When agents have the incentive to misrepresent their truthful reports, the eﬀect on the results of the aggregation or selection mechanism can be problematic. Indeed, in a comprehensive evaluation of the peer review process, Wenneras and Wold [59] wrote, “. . . the development of peer-review systems with some built-in resistance to the weakness of human nature is therefore of high priority.”
2

We propose a novel strategyproof, (which we shall also call impartial) mechanism2 where agents can never gain by being insincere. There are many reasons to prefer a strategyproof mechanism: ﬁrst, the mechanism does not favor “sophisticated” agents who have the expertise to behave strategically. Second, agents with partial or no knowledge of other agents’ rankings are not at a disadvantage when using a strategyproof mechanism. Third, normative properties of a mechanism typically assume sincere behavior on the part of the agents. If agents act strategically, we may lose some desirable normative properties. Fourth, it is, in general, easier to persuade people to use a strategyproof mechanism than one that can be (easily) manipulated. Note that while strategyproofness does not handle all potential biases of agents, it eliminates an obvious “weakness in human nature.”
To achieve strategyproofness we could use a lottery (as in the Athenian democracy). However, this method does not select based on merit. A diﬀerent option is to use a mechanism based on a voting rule. However, following Gibbard and Satterthwaite [21, 53], any “reasonable” mechanism based on voting will not be strategyproof unless it is a dictatorship. Another option is to employ a mechanism like the Page Rank algorithm that uses Markov chains to compute a ranking of agents [58]. However, such mechanisms are also not strategyproof.
Contributions. First, we propose a novel peer selection mechanism, ExactDollarPartition, that satisﬁes several desirable axiomatic properties including strategyproofness and two natural monotonicity properties. Second, we conduct a detailed experimental comparison with other strategyproof mechanisms with regard to their ability to recover the “ground truth”. Our experiments demonstrate that ExactDollarPartition selects more high-quality agents more often, selects more high-quality agents in the worst case, and has more consistent quality than any other strategyproof mechanism in the literature. Third, our mechanism uses a novel randomized apportionment subroutine to fairly round selected fractional group sizes to integers. This subroutine is interesting in its own right as it provides a compelling solution to the fundamental problem of apportionment: allocating representatives or resources in proportion to group size or strength of demand. Young [61] motivates the problem as follows: “This surprisingly diﬃcult problem has concerned statesmen, political analysts and mathematicians for over two hundred years.”
2. Discussion and Related Work
Peer review is the cornerstone of modern science and hence, the quality, veracity, and accuracy of peer review and peer evaluation is a topic of interest across a broad set of disciplines. Most empirical studies of peer review and peer selection focus on the eﬀectiveness and limits of the system, typically by
2Strategyproof in the peer selection setting diﬀers from the voting setting. In peer selection impartial means one cannot make themselves be selected if they wish to do so.
3

assembling large corpora of peer-reviewed proposals and cross-examining them with new panels or review processes [16, 35]. Questions of bias, nepotism, sexism, cronyism, among other issues, have received extensive coverage, and have been substantiated to varying degrees, in the literature [16, 44, 59]. However, a consistent conclusion in the meta-research on peer review is that, in order to decrease the role of chance and/or any systematic bias, the community needs to broaden the base of reviewers. Indeed, one way for the results of the review process to reﬂect the views of the entire scientiﬁc constituency and provide more value to the community is to increase the number of reviewers [28, 49]. The key scientiﬁc question lies in ﬁnding a mechanism that allows for crowdsourcing the work of reviewing, without compromising the incentives and quality of the peer review and selection process.
The criticism that prominent peer selection mechanisms such as those under consideration by American and European funding bodies [45, 25] are not strategyproof [46] has underscored the need to devise mechanisms with better incentive properties. The literature most directly relevant to this article is a series of papers on strategyproof (impartial) selection [26, 2] and more recently impartial ranking [29]. The explosive growth in computer science and machine learning conference submissions in the past years has led to more work in the computer science and machine learning ﬁelds are using data from large conferences [54] and even performing human experiments [31] to analyze the assignment [36, 55] and outcomes of various novel mechanisms for peer review. We survey and provide details of these mechanisms in the next section. Most of the work on strategyproof peer selection focuses on the setting in which agents simply approve (nominate) a subset of agents [2, 12, 19, 26], with the latter three of these restricting attention to the setting in which exactly one agent is selected (k = 1).3 A popular class of strategyproof peer selection mechanisms are Partition based mechanisms, as presented in Alon et al. [2], where agents are divided into non-intersecting groups. Kurokawa et al. [33] present an interesting strategyproof mechanism (Credible Subset) that performs well when each agent reviews a very small number of agents relative to the total number of agents. Other recent work focuses on tradeoﬀs between diﬀerent axioms concerning peer selection [7, 39].
Both Alon et al. [2] and Holzman and Moulin [26] examine the selection problem in which agents simply approve (nominate) a subset of agents. Holzman and Moulin [26], Fischer and Klimm [19], and Bousquet et al. [12] restrict their attention to a setting in which exactly one agent is selected (k = 1). Fischer and Klimm [19] also present the Permutation mechanism that achieves the same bound as the Partition mechanisms when only one agent is selected (k = 1). Alon et al. [2] and Holzman and Moulin [26] showed that for the peer selection
3In Alon et al. [2] and Kurokawa et al. [33] the letter k is used to denote the number of partitions, in our paper and many others, k designates the number of agents selected. Therefore we use to denote the number of partitions in this paper and k to denote the number of agents selected.
4

problem, deterministic impartial mechanisms are extremely limited, and must sometimes select an agent with zero nominations even though other agents receive nominations, or an agent with one nomination when another agent receives n−1 nominations [19]. Bjelde et al. [9] built on this work to show that allowing a mechanism where agents simply approve of some subset of agents to select fewer than k agents allows the mechanism to guarantee some bounds on the selected items—they are within about 1 − 1e from the optimal selection. Kurokawa et al. [33] present a more general mechanism called Credible Subset that is strategyproof but may select no winners with non-zero probability. Credible Subset performs well when each agent reviews a few other agents, and this number is considerably smaller than k.
There are a number of practical application areas that are related to and/or use peer selection. The peer selection problem is closely related to peer-based grading/marking [1, 28, 32, 47, 51, 58, 60] especially when students are graded based on percentile scores. For peer grading, mechanisms have been proposed that make a student’s grade slightly dependent on the student’s grading accuracy (see e.g., Walsh [58] and Merriﬁeld and Saari [45]). However such mechanisms are not strategyproof as one may alter one’s reviews to obtain a better personal grade. Finally, as an additional and recent application area, economists have studied mechanisms and the strategic issues that arise in using peer evaluation for micro-ﬁnancing and other reputation based resource allocation problems [6, 10, 27].
3. Setup and Survey of Existing Mechanisms
Given a set N of agents {1, . . . , n} where each agent, depending on the setting, evaluates some m of the other agents where 0 ≤ m ≤ n − 1. Each agent reports a valuation (review) over the other agents (proposals). These reports could be cardinal valuations vi(j) for agent i’s valuations of agent j, or they could be a weak order reported by agent i of agents in N \ {i}, which may be transformed to cardinal valuations using a scoring rule. Based on these reported evaluations, around k agents are selected. Some mechanisms, such as Credible Subset, may not always return a size of exactly k even if the target size is k.
A particular family of mechanisms with which we will deal are based on partitioning. The general idea of partitioning-based mechanisms is to divide the agents into a set of clusters C = {C1, . . . , C }. This partition can be done using either a random process or some predetermined process that does not include randomization. We will assume that cluster sizes are such that selection from them is not a problem: for all 1 ≤ i ≤ , k ≤ |Ci|. If N is not an integer then we assume that k ≤ N , the smallest cluster size.
3.1. Mechanisms
There are three prominent mechanisms for peer selection that appear in the literature.
5

Vanilla: Select the k agents with the highest total value based on their reviews by other agents (as done today, for example, in many scientiﬁc conferences). Vanilla is not strategyproof; unselected agents have an incentive to lower their reported valuations of selected agents.
Partition: Divide the agents into clusters and select a preset number of agents from each cluster, typically k/ (rounded in some way if k/ is not an integer), according to the valuations of the agents not in that cluster. This class of mechanisms is a straightforward generalization of the Partition mechanism [2, 19] (and in an early version of Kurokawa et al. [33]) which is strategyproof.
Credible Subset [33]: Let T be the set of agents who have the top k scores, as in Vanilla. Let P be the set of agents who do not have the top k scores but will make it to the top k if they do not contribute any score to other agents (hence |P | ≤ m). With probability (k + |P |)/(k + m), Credible Subset selects a set of k agents uniformly at random from T ∪ P , and with probability 1 − (k + |P |)/(k + m), it selects no one. The mechanism is strategyproof.
There are a number of other mechanisms that are tailor-made for k = 1 and when agents only mark approval of a subset of agents: Partition [26]; Permutation [19]; and Slicing [12]. When designing our mechanism, we were inspired by mechanisms for dividing a continuous resource from the economics literature [18, 57]. In particular, we use ideas from the following mechanism.
Dividing a Dollar: Each agent i reports a value vi(j) that is his estimation of how much of the resource agent j should receive. These values are normalized so that j∈N\{i} vi(j) = 1/n. Hence, the Dollar share of each agent i is xi = j∈N\{i} vj (i).
3.2. Properties of Mechanisms We consider some basic axioms of peer selection mechanisms. When algo-
rithms involve randomization, these properties are with regard to the probability of selection.
Anonymity: For some permutation of n agents π, if W is the outcome of the mechanism for agents N , with each agent i giving a valuation on agents i1, . . . im, then π(W ) is the outcome of the mechanism for agents N where each agent π(i) gives valuations on agents π(i1), . . . , π(im).
Non-imposition: For any target set W , there is a valuation proﬁle and a randomization seed that achieves W .
Strategyproofness (Impartiality): Agents cannot aﬀect their own selection.
Monotonicity: If agent i is selected, and some other agent j reinforce it, increasing i’s relative position in her ranking without changing the relative
6

position of other agents, then agent i will still be selected. 4
Committee Monotonicity: If W is the outcome when the target set size is k, then all the agents in W are still selected if the target set size is k + 1.
4. ExactDollarPartition
The algorithm ExactDollarPartition is formally described in Algorithm 1. Broadly, it works as follows: agents are partitioned into clusters such that the sizes of clusters are equal or as near as possible, with diﬀerence at most 1. Each agent i ∈ N assigns a value vi(j) to each agent i that is among the m agents that i reviews, none of which are in i’s cluster. Agent i may directly give a cardinal value to the agents they review or the cardinal value may be obtained by a scoring function that converts an ordinal ranking given by i to cardinal values. In either case, the values that i gives are normalized so that agent i assigns a total value of 1 to the m agents they are to review outside their own cluster. Based on the values from agents outside the cluster we assign the normalized weight (Dollar Share) xj to each cluster Cj. Based on each Dollar Share xj, each cluster has a quota sj = xj · k (possibly real but not rational). If all sj’s are integers, then each sj is the quota of cluster Cj, i.e., the top graded sj agents are selected from cluster Cj. If not all sj are integers, then we use the function AllocationFromShares in line 7 (detailed in the next section) to enumerate discrete cluster allocations in which each cluster gets an allocation of either sj or sj . AllocationFromShares then computes a probability distribution over these discrete allocations, requiring at most such allocations, so that the expected quota for each cluster will be exactly sj. We draw a discrete allocation (t1, . . . , t ) using the distribution computed in AllocationFromShares and select exactly the tj agents from each cluster Cj with the highest score. The agents who have a higher score will be referred to as having a higher ranking. We note that the algorithm gracefully handles the case where an agent is absent, i.e., does not submit their reviews, or if she gives zero score to every other agent that she is responsible for reviewing. The algorithm handles this case by forcing the agent to give equal score to all the other agents reviewed, i.e., m1 .
We illustrate the working of our algorithm with the following example.
Example 1. Suppose we want to select k = 5 winners from our agents, which are divided into four clusters, each with 2 agents, giving us n = 8, each agent being responsible for reviewing m = 2 other agents. Table 1 shows the initial grades, on a scale of 0 − 100 given by the row agent to their peers listed in the columns. Table 2 shows these grades following normalization so that each agent distributes 1.0 point to the m = 2 agents they review.
4When scores are used instead of ordinal rankings, we are, in a sense, converting them to ordinal rankings by looking at normalized scores, in which the sum of all scores is 1. The property states that if only agent i’s normalized score is raised, it will still be selected.
7

Algorithm 1 ExactDollarPartition
Input: Set of agents N , valuations (v1, . . . , vn) of the agents, m the number of reviews per agent, and the number of clusters.
Output: Set of winning agents W .
1 Initialize W ← ∅ 2 Generate a partition {C1, . . . , C } of N where the diﬀerence between the
sizes of any two clusters is at most 1. 3 Each i ∈ N reviews m agents outside C(i), where C(i) is the cluster of agent
i, so that any reviewed agent j is assigned a valuation vi(j). 4 Ensure j∈/C(i) vi(j) = 1 by normalizing. If vi(j) = 0 for all j, then we
vi(j) = 1/m for each j reviewed by i. 5 xi, the value of a cluster Ci, is deﬁned as:

1

xi ← n ×

vj (j).

j∈Ci,j ∈/Ci

{Using the xi values, we now compute the number of agents ti to be chosen from each cluster Ci.} 6 Let each share si ← xi · k for each i ∈ {1, . . . , }. 7 (t1, . . . , t ) ← AllocationFromShares (s1, . . . , s ) where (t1, . . . , t ) are the number of agents to be allocated from each cluster.
8 For each i ∈ C(i), the score of agent i is i ∈/C(i) vi (i). 9 Select tj agents with the highest scores from each cluster Cj and place them
in set W .
10 return W

This means the overall scores are:

Cluster 1: x1 =

j∈C1,j ∈/C1 vj (j) = 1.9526 = 0.244075 ⇒

n

8

Therefore, s1 = x1 · k = 1.220375

1.9484 Cluster 2: x2 = 8 = 0.24355 ⇒ s2 = 1.21775
1.6266 Cluster 3: x3 = 8 = 0.203325 ⇒ s3 = 1.016625
2.4724 Cluster 4: x4 = 8 = 0.30905 ⇒ s4 = 1.54525

This process leaves us with a share vector of

s = (1.220375, 1.21775, 1.016625, 1.54525).

While s = k observe that not all the numbers are integers, leaving us the need to apportion the remainders. The function AllocationFromShares is explored further in Example 2, but for now, it suﬃces to know that since the

8

Table 1: Example grades of row agent for column agent.

ABCDEFG H

A [cluster 1]

0

100

B [cluster 1]

80

30

C [cluster 2] 83

42

D [cluster 2]

77

50

E [cluster 3]

65

65

F [cluster 3]

56

98

G [cluster 4] 29

62

H [cluster 4]

75

29

Table 2: Example grades following normalization.

A

B

C

D

E

F

G

H

A [cluster 1]

0

1.00

B [cluster 1]

0.7272

0.2728

C [cluster 2] 0.664

0.336

D [cluster 2]

0.6063

0.3937

E [cluster 3]

0.50

0.50

F [cluster 3]

0.3636

0.6364

G [cluster 4] 0.3187

0.6813

H [cluster 4]

0.7212

0.2788

number of agents for each cluster is rounded up or down, from one of the clusters we need to choose 2 agents, and 1 agent from the others. The highest probability is given to the event in which cluster 4 is the only one that will select 2 agents, giving us our allocation t = (1, 1, 1, 2). This allocation vector leads to the selection of the agents A, C, F, G, H, which are the top-ranked agent in clusters 1, 2, and 3, and both agents of cluster 4.
We defer our proofs and analysis of the properties of the apportionment method— function AllocationFromShares—to the next section. For the analysis of the overall mechanism, it is enough to assume it chooses an allocation of size k from a probability space constructed so that the expected share of each cluster j is sj. As no agent is treated diﬀerently in the mechanism, ExactDollarPartition is anonymous and satisﬁes non-imposition.
Theorem 1. ExactDollarPartition is strategyproof.
Proof. Suppose agent i is in cluster Cj of the generated partition. Agent i will be selected in W if and only if its score is among the top tj scores from agents in Cj. Therefore agent i can manipulate either by increasing tj or by increasing
9

its score relative to other agents in Cj given by agents outside Cj. Since agent i cannot aﬀect the latter, the only way it can manipulate is by increasing tj. We argue that agent i cannot change its expected tj by changing its valuation vi for agents outside the cluster. Note that i contributes a probability weight of 1/n to agents outside Cj and zero probability weight to agents in Cj. Hence it cannot aﬀect the value xj of cluster Cj. As sj is derived from xj, agent i cannot aﬀect sj.
As we will show in our analysis of AllocationFromShares, speciﬁcally Theorem 6, the expected value of tj will be sj (and its value is either sj or sj , in the unique probabilities that make the expected value sj). Since any agent in cluster Cj that changes its report does not aﬀect sj, it does not aﬀect the expected tj, nor the respective probabilities of getting sj and sj . Hence, agent i cannot manipulate by either increasing the tj of his cluster or by increasing his score relative to the agents in Cj. Therefore, ExactDollarPartition is strategyproof.
Remark 1. ExactDollarPartition is not just strategyproof but even groupstrategyproof if manipulating coalitions involve agents from the same cluster (so potentially colluding agents, e.g., with conﬂict of interest, can be put in the same cluster).
Theorem 2. ExactDollarPartition is monotonic.
Proof. Let us compare the valuation proﬁle v when i is not reinforced and v when i is reinforced. The relative ranking of i is at least as good when i is reinforced. Since any decrease in valuation that an agent j in C(i) receives translates into the same increase in the valuation received by agent i, the total valuation that C(i) receives does not decrease and hence the number of agents selected from C(i) is at least as high as before.
Theorem 3. ExactDollarPartition is committee monotonic.
Proof. The only diﬀerence between running the algorithm for diﬀerent target k values is when calculating the quota vector s. However, if agent i in cluster Cj was selected, that means its ranking in the cluster Cj was above tj. When k increases, sj will only increase (as xj remains the same), and hence so will tj, ensuring that i will be selected again.
5. A Randomized Apportionment Rule
The randomized allocation technique we call AllocationFromShares used for ExactDollarPartition is of independent interest since it addresses the classic apportionment problem in a randomized way. Consider the problem in which n agents divided into disjoint groups are to be allocated a given number of slots k < n in proportion to the group sizes. The problem is ubiquitous in apportionment settings such as proportional representation of seats in the
10

U.S. congress, European Parliament, and the German Bundestag as well as

various other committee selection settings [4, 5, 8, 43, 50]. This problem has

been studied in political science, economics, operations research, and computer

science for over 200 years [61].

In these settings, each group i has a quota si with

n i=1

si

=

k,

which

we call its target quota. Since si may not be an integer, we have to resort to

apportionment, which means that in order to allocate exactly k slots, some group

may be assigned an integer quota slightly more or less that its target quota.

Numerous apportionment procedures have been introduced in the literature

including the methods of Hamilton, Jeﬀerson, Webster, Adams, and Hill [4];

each with its own drawbacks. In fact, Balinski and Young [4] proved that no

deterministic apportionment procedure can satisfy a group of three minimal

axioms: (1) Quota Rule, each group should get quota that is the result of the

target quota being rounded up or down; (2) Committee Monotonicity, if k in-

creases then the quotas do not decrease; and (3) Monotonicity, if si < sj and

the quotas are perturbed such that the percentage increase of si is more than

the percentage increase of sj, then i should not lose a slot to j. We call discrete

quota allocations that satisfy the quota rule and allocate exactly k slots as nice

allocations.

5.1. Curse of Determinism: The Need for Randomization
We ﬁrst demonstrate that randomization is a necessary feature of an apportionment mechanism in order to select exactly k agents and strategyproofness. Therefore, any deterministic and strategyproof method of using fractional quotas to derive integer quotas can result in outcomes that are not of the target size (e.g., [3]).
Theorem 4. No Partition-based method, which assigns non-integer quotas to each cluster can select exactly k agents by rounding the quotas in a deterministically strategyproof way.
We ﬁrst prove the following lemma.
Lemma 1. In a deterministic strategyproof allocation mechanism that selects k agents from clusters, the number of agents chosen from cluster i with a share si will not change, regardless of the rest of the shares.
Proof. Let s = (s1, . . . , s ) be the shares for each cluster, under which a mechanism allocates y agents from cluster i. Now, let s = (s1, . . . , si−1, si, si+1, . . . , s ) be a diﬀerent share allocation. We wish to show that the number of agents selected from cluster i remains y.
We know j=i sj = j=i sj. If j=i |(sj − sj)| ≤ n2 , this means a single agent can cause the change from s to s . As the share values do not contain data on actual agents votes, that single agent could be in cluster i (since si did not change at all). Thanks to strategyproofness, this means there is no change in the number of agents selected from cluster i—it is still y agents.

11

If j=i |(sj − sj)| > n2 , we make the move from s to s using intermediary steps, s0, . . . , sh such that aj = (sj1, . . . , sj) a share allocation where sji = si, s0 = s, sh = s , and for 1 ≤ t ≤ h, j=i |(stj − stj−1)| ≤ n2 . Thanks to the argument in the previous paragraph, the number of agents selected from cluster i stays y in s1. Now we can look at s1 and s2 by themselves, and due to the same argument, the number of agents from cluster i needs to be the same in s1 and s2, hence it is still y in s2. We apply this argument again and again, until we reach the point where the number of agents selected from cluster i in sh = s is y as well.
Proof of Theorem 4. Suppose there is a rounding of quotas that guarantees the selection of k agents. Let us assume k clusters and k > 3 is odd. Using Lemma 1, we know that each cluster’s slot allocation is ﬁxed according to its share, regardless of other clusters’ share. Hence, for each cluster with a share of 1.5, it receives an allocation of either 1 slot or 2.
Case I: There are 2 clusters that are allocated 2 slots when their share is 1.5. Suppose these 2 clusters have a share of 1.5, some other cluster has share 0, and all remaining clusters have share 1. Hence we had k shares, but received k + 1 slot allocations.
Case II: There are 2 clusters that are allocated 1 slot when their share is 1.5. Suppose these 2 clusters have a share of 1.5, some other cluster has share 0, and all remaining clusters have share 1. Hence we had k shares, but received k − 1 slot allocations.
Determinism does not just prevent strategyproof mechanisms, but also anonymous ones as shown by the following Theorem.
Theorem 5. No Partition-based method, which assigns non-integer quotas to each cluster can select exactly k agents by rounding the quotas in a deterministically anonymous way.
Proof. Let = 3, with clusters being of equal size. All agents in cluster 1 rank agents in cluster 2 before any in cluster 3; agents in cluster 2 rank those in cluster 3 ahead of cluster 1; and those in cluster 3 rank agents in cluster 1 ahead of those in 2. So share of each cluster is equal, and for k < there is no deterministic anonymous way to allocate quotas.
5.2. A Novel Randomized Apportionment Rule We resort to randomization to achieve ex ante fairness and the target size.
Using randomization, our goal is to ensure that the target number of total agents chosen is exactly k ex post. Therefore we will require that the integer quota allocation returned by the lottery is a nice allocation. Randomization has been used in various settings such as voting and fair allocation of indivisible goods to achieve ex ante as well as procedural fairness [13, 11, 22].
12

One possible way to achieve the appropriate randomization is to enumerate all the feasible discrete quota allocations and then solve equations to ﬁnd the probability distribution over these quota allocations. If such a probability distribution exists, the method outlined involves enumerating an exponential number of such quota allocations that is computationally infeasible if the number of groups is large (for example, in U.S. elections, is 50). Hence, some suggested approaches to this problem require multiple rounds of randomization and also do not enumerate the possible ex post outcomes (there may be an exponential number of them) [20]. Previously, a stochastic apportionment rule was presented that achieves the quota requirements [23] by two randomizations, one of them using a stochastic continuous variable. This means that it does not involve a probability distribution over discrete nice allocations, hence it cannot be used to achieve fairness via repeated representation. Moreover, due to computers not being able to reproduce truly continuous values, this may compromise strategyproofness.
In view of these challenges, we present a simple method AllocationFromShares that achieves the target quotas, relies on a probability distribution over a linear number of nice allocations, the probability distribution can be computed in linear time, and requires minimal randomization (only one round). Our randomized procedure can be easily de-randomized in repetitive settings. In frequently repeated allocation settings, one could use the nice allocations computed by AllocationFromShares (at most , compared to the potentially exponential number) in a way such that the allocations have the same frequency as the probability distribution computed by the algorithm. In this sense, our randomized apportionment routine has an advantage over other proposed methods.
Informally, our method proceeds gradually from quotas which need to be rounded up with low probability, while keeping an eye on our two main constraints: not rounding up a quota too much, on the one hand, while not being left with not enough probability for allocations with quotas that need to be rounded up with high probability.
Example 2. We give an example of the working of AllocationFromShares, shown in Algorithm 2, when a set of quotas are not integers. Suppose we have the following Dollar shares for = 5:
s = (1.1, 2.1, 1.3, 1.7, 1.8)
We wish to select k = 8 agents. The α, number of clusters that need to be rounded up, computed on line 5, is 2, and we start with low = 1; high = 5. We begin by considering the allocation:
t1 = ( 1.1 , 2.1 , 1.3 , 1.7 , 1.8 ) = (2, 3, 1, 1, 1)
Since 0.1 = s1 − s1 < s5 − s5 = 0.2, this allocation will get a probability of 0.1. Now low = 2 (since cluster 1 should not be rounded up any more), p¯ = 0.1, and we “slide” our allocation by one to the right, and look at the next allocation:
t2 = ( 1.1 , 2.1 , 1.3 , 1.7 , 1.8 ) = (1, 3, 2, 1, 1)
13

Algorithm 2 AllocationFromShares (s1, . . . , s )

Input: A real-value allocation (s1, . . . , s ) over objects. Output: A discrete allocation (t1, . . . , t ) over objects.

1 Sort and renumber (s1, . . . , s ) according to size of si − si , with s1 − s1 being minimal.

2 Let (p1, . . . , p ) ←− (0, . . . , 0) where pi is the probability of rounding up cluster i.

3 Let p¯ ←− 0, the total probability allocated so far.

4 Let D ←− ∅, where D maps: allocation → probability.

5 α ←− i=1(si − si ) 6 low ←− 1; high ←−

7 while low ≤ high do

8 Let allocation ←− ( s1 , . . . , slow−1 , slow , . . . ,

9

slow+α−1 , slow+α , . . . , shigh , shigh+1 , . . . , s )

10

Where if low=1, we start with s1 ; if high = , we

11

end with shigh ; and if α = 0, we have only slow .

12 prob ←− 0

13 prevLow ←− low; prevHigh ←− high

14 if α = 0 then

15

prob ←− 1 − p¯; high ←− high − 1

16 else

17

if slow − slow − plow < shigh − shigh − p¯ + phigh then

18

prob ←− slow − slow − plow; low ←− low + 1

19

else

20

prob ←− shigh − shigh − p¯ + phigh

21

high ←− high − 1; α ←− α − 1

22

end if

23 end if

24 for all i such that prevLow ≤ i < prevLow + α or prevHigh < i ≤ do

25

pi ←− pi + prob

26 end for

27 p¯ ←− p¯ + prob

28 D ←− D ∪ (allocation → prob)

29 end while

30 Select an allocation (t1, . . . , t ) according to D. 31 return (t1, . . . , t )

Since the second cluster has been rounded up with a probability of 0.1 in the previous allocation, s2 − s2 − p(v2) = 0. Therefore this allocation is given probability 0, p¯ does not change and now low = 3. We now move to allocation:
t3 = ( 1.1 , 2.1 , 1.3 , 1.7 , 1.8 ) = (1, 2, 2, 2, 1)
We see 0.3 = s3 − s3 − pv3 > s5 − s5 − p¯ + pv5 = 0.1, so this allocation is given probability 0.1, p¯ = 0.2, and from now on cluster 5 will always be rounded
14

up in every allocation we consider. Hence, α and high now change: α = 1 and high = 4. We now turn to look at:

t4 = ( 1.1 , 2.1 , 1.3 , 1.7 , 1.8 ) = (1, 2, 2, 1, 2)

Since 0.2 = s3 − s3 − pv3 = s4 − s4 − p¯ + pv4 = 0.2, we give this allocation the probability 0.2, p¯ = 0.4, and low = 4. Finally, we look at:

t5 = ( 1.1 , 2.1 , 1.3 , 1.7 , 1.8 ) = (1, 2, 1, 2, 2)

We give this allocation the probability s4 − s4 − pv4 = 0.6. Overall, the algorithm yields a probability distribution over
tors.
t1 = (2, 3, 1, 1, 1) : 0.1 t2 = (1, 3, 2, 1, 1) : 0.1 t3 = (1, 2, 2, 2, 1) : 0.1 t4 = (1, 2, 2, 1, 2) : 0.2 t5 = (1, 2, 1, 2, 2) : 0.6

allocation vec-

This deﬁnes our probability space, and the expected number of agents selected from each cluster is exactly its Dollar share: (1.1, 2.1, 1.3, 1.7, 1.8).
Theorem 6. AllocationFromShares deﬁnes a distribution and the expected allocation of each cluster i is its share si.
To prove this theorem, we ﬁrst need several lemmas. Note that any cluster i needs to be rounded up with probability of si − si , and rounded down with probability si − si.
Lemma 2. Let z = (z1, . . . , z ) ∈ N and let set A = {z ∈ N | in α ∈ N coordinates zi = zi + 1. In the rest zi = zi}. For any zˆ = (zˆ1, . . . , zˆ ) that is a simplex of A (i.e., zˆ = a∈A paa such that a∈A pa = 1), i=1(zˆi − zi) = α.
Proof.

(zˆi − zi) =

( (paai)) − ( pa)zi = pa (ai − zi)

i=1

i=1 a∈A

a∈A

a∈A

i=1

For any a ∈ A, from A’s deﬁnition we know i=1(ai − zi) = α. Hence:

pa (ai − zi) = paα = α pa = α

a∈A

i=1

a∈A

a∈A

15

Note that Lemma 2 is applicable to our algorithm, as we can consider z = ( s1 , . . . , s ) as a basis, and in each allocation the algorithm rounds up α coordinates, and this rounding up is equivalent to taking α coordinates, and instead of using the value from z (the rounded down value, si ), we round up and add 1 to the coordinate.
Lemma 3. At no point in the algorithm is a cluster rounded up or down in allocations that, together, have more probability than it should, i.e., for any cluster i, it is always true that pi ≤ si − si and p¯ − pi ≤ si − si. Moreover, for any i < low, the probability that cluster i is rounded up is si − si . For any i > high, the probability that cluster i is rounded down is si − si.
Proof. Recall that the probability cluster i is rounded up needs to be si − si . When a set of clusters is being rounded up, the probability of the allocation is bounded by slow − slow − plow (Line 17), i.e., the probability slow should be rounded up which still remains to be allocated. Any other cluster i rounded up in the same allocation has been rounded with slow in every previous allocation when it has been rounded up (since i > low), so pi ≤ plow. Since the clusters are ordered according to si − si in line 1, we know that si − si > slow − slow . Hence, slow − slow − plow ≤ si − si − pi, so no cluster is rounded up more than it should be.
At any point in the algorithm, if i < low, then there was a stage where low = i, and line 18 changed low to i + 1. However, at that point, cluster i is rounded up exactly the additional probability it needed to be rounded up (si − si − pi) , and no further allocation in the algorithm will round it up.
Similarly, if i > high (and α = 0), there was a stage where high = i, and line 21 changed high to i − 1. However, at that point, cluster i is rounded down exactly the additional probability it needs to be rounded down. It could not have been rounded down too much previously, as every allocation’s probability is bounded so that cluster high will not be rounded down more than it is supposed to ( shigh − shigh). Once again, once an index is high + 1, no further allocation will round it down.
For cluster i, low ≤ i ≤ high, it has only been rounded down when shigh was rounded down as well (though not vice versa) and rounded up when slow was rounded up (again, not vice versa), and as the si − si ≥ shigh − shigh and si − si ≥ slow − slow , these clusters have not been rounded up more than si − si , or rounded down more than si − si. Therefore, the sum of allocation probabilities is never larger than 1 (since si − si + si − si = 1). Hence, also, for clusters i < low, as they have received the exact needed probability of being rounded up, and since the sum of allocations does not exceed 1, they have not been rounded down more than needed.
Proof of Theorem 6. We ﬁrst show all the allocations we consider only round up i=1(si − si ) clusters, as otherwise, allocations are not allocating exactly k agents. As long as low + α ≤ high in the algorithm this is trivially true. We now wish to show the situation low + α > high cannot happen (as that results in too few clusters rounded up).
16

If low + α > high then there was a stage in which low + α = high, and then we executed line 18. This means that cluster high needs more unallocated probability to be rounded down than cluster low needs unallocated probability to be rounded up. Moreover, thanks to the monotonicity of elements of the si vector, we know cluster high still has need for more allocations with positive probability in which it is rounded up. But this property means that if we advanced low and assigned all remaining unassigned probability to the allocation rounding up clusters low + 1, . . . , , we would be rounding them up too much, and for clusters low + 1, . . . , high, strictly so. But we know from Lemma 3 that for all i ≤ low, we have rounded the share si up exactly correctly, so looking at the vector zˆ ∈ N in which each coordinate is the expected allocation for that cluster, we have:

low

(zˆi − si ) = (si − si ) +

zˆi − si >

i=1

i=1

i=low+1

> (si − si ) = α
i=1
This contradicts Lemma 2, as all allocations had exactly α clusters rounded up. So it cannot be that cluster high still needed more probability to be rounded down, and therefore, if low + α = high, line 18 would not have been executed at this point.
Since low + α ≤ high at all times, the algorithm will end when low = high. Hence, the previous step ended with α becoming 0 in line 21. Observe that α = 0 only in this case: otherwise, it means the sum of expected value—that is, probability to be rounded up—over all clusters is above α: we have too many clusters that need to be rounded up.
We now wish to prove that the last step, where the clusters high + 1, . . . , are rounded up, results in what we desired. Since clusters 1, . . . , low − 1 have been allocated the right probability to be rounded up, as well as clusters high + 1, . . . , (Lemma 3), we only need to verify this for cluster low. But according to Lemma 2, the probability of low being rounded up is exactly α− 1≤i≤ ,i=low(si− si ), which is exactly slow − slow , which means cluster low has the correct allocation.

6. Analytical Comparisons with Other Mechanisms
Though ExactDollarPartition draws inspiration from Dividing a Dollar and Partition, there are key diﬀerences between these mechanisms and clear reasons to use ExactDollarPartition over other potential variants.
6.1. Comparison with other Dollar Based Mechanisms Although ExactDollarPartition is partly based on the Dollar mecha-
nism for dividing a bonus (division of a divisible item between agents), it is

17

more desirable than some other natural mechanisms one can construct based on the Dollar framework. Consider the following possible adaptations of the Dollar framework and their shortcomings.

Dollar Raﬄe: Take the dollar mechanism (without any partitions), compute the relative fraction of the dollar each agent should receive. Use these fractions as a probability distribution over the agents and then repeatedly select an agent according to its dollar share until k diﬀerent agents are selected.

Dollar Partition Raﬄe: Take the Dollar shares of the clusters in Dollar Rafﬂe and use these shares to deﬁne a probability distribution over the clusters. A cluster is drawn with respect to the cluster Dollar probabilities and the next best agent, based on reviews of agents outside the cluster, is selected, until k diﬀerent agents are selected.

Top Dollar: Select the agents with maximum Dollar shares.5

Both Dollar Raﬄe and Dollar Partition Raﬄe have a non-zero probability of selecting the k worst agents. While Top Dollar is not strategyproof for any k < n, Dollar Raﬄe and Dollar Partition Raﬄe are strategyproof for k = 1. None, however, are strategyproof for n > k > 1.

Theorem 7. Dollar Raﬄe, Dollar Partition Raﬄe, and Top Dollar are not strategyproof for n > k > 1.

Proof. For Dollar Raﬄe and Dollar Partition Raﬄe, the proof follows a similar

path: The mechanism iterates until it chooses k diﬀerent agents, which is equiv-

alent to eliminating each selected agent and re-normalizing the dollar partitions,

i.e., the probabilities of being selected, since once some agent is selected we ig-

nore its repeated selection. This re-normalization prevents the mechanism from

being strategyproof, as now the probabilities of others matter for each agent.

For example, an agent will prefer to contribute to a very strong agent. This

strong agent, once eliminated, will make our agent’s probability increase signiﬁ-

cantly. Suppose k = 2 using Dollar Raﬄe (Dollar Partition Raﬄe), and suppose

all agents (clusters) except b1, b2, b3 allocate their points equally between those

3. b1 divides its point equally between b2 and b3, as does b2 between b1 and b3.

Suppose b3 believes it should also divide its point equally between b1 and b3. In

that case, it has a probability 13 of being selected ﬁrst, and a probability of 31 of being selected second, ultimately, 23 . But if agent (in) b3 decides to give its point

fully to (cluster) b1, the probability of b3 being selected ﬁrst does not change.

But

the

probability

of

b1

being

selected

and

then

b3

is

1 3

(in

Dollar

Partition

Raﬄe: ( 1 +

1 )2

1
3 1

), and the probability of b2 being selected and then b1 is

3 2n 3 − 2n

115 (in Dollar Partition Raﬄe: ( 13 − 21n ) 2 +13 1 ). The sum of these is more than

3 2n

13 , hence doing so would improve agent (in cluster) b3 chances of being selected.

This proof can easily be extended to any additional k.

5Vanilla is equivalent to Top Dollar when agents’ valuations are normalized.
18

For Top Dollar, agents are a1, . . . , ak+1. Agents a1, . . . ak−1 allocate each of

their

points

by

giving

1 k

−

1 k2

to

agent

ak+1,

1 k2

to

agent

ak

and

1 k

to

all

other

agents.

Agent

ak

gives

1 k

to

all

other

agents.

Agent

ak+1

would

like

to

allocate

its

point to agent ak, but that would mean it would not be selected itself. Giving its

point to other agents will mean it will be, contradicting strategyproofness.

Interestingly, the proof of this theorem for Dollar Raﬄe and Dollar Partition Raﬄe carries on, quite straightforwardly, to the various mechanisms presented for k = 1 (e.g., [19]). Simply running the algorithm several times destroys its strategyproofness. This is true even for mechanisms that are strategyproof for k = 1, as long as any agent has the power to inﬂuence the outcome, i.e., not purely random, a dictatorship, or a combination of both.

6.2. Comparison with Partition Mechanisms
ExactDollarPartition seems similar to the Partition mechanism but while Partition must preset the number of agents to be selected from each cluster, ExactDollarPartition relies on the peer reviews to decide the number of agents to be selected from each cluster. This diﬀerence allows ExactDollarPartition to have more consistent performance, no matter the clustering. Hence, in contrast to ExactDollarPartition, if a particularly bad partition is chosen at random, the rigidity of Partition means that it may not choose a large proportion of the best agents even if agents have unanimous valuations.
Example 3. Consider the setting in which N = {1, . . . , 18}, k = 6, and = 3. Let the clusters be C1 = {1, . . . , 6}, C2 = {7, . . . , 12}, C3 = {13, . . . , 18}. C1 puts all its weight on C2, equally dividing its points between 7, 8, . . . , 12, with a slight edge to 7 and 8, C2 and C3 put all the weight on C1, dividing their points between 1, 2, 3 and 4. Now Partition will choose 1, 2, 7, 8, 13, 14 where everyone thinks that 1, 2, 3, 4, 7, 8 are the best. ExactDollarPartition will select exactly that set. Moreover, if we increase the number of clusters, the disparity between ExactDollarPartition and Partition only grows.
Partition, in contrast to ExactDollarPartition, performs poorly ex post6 if the clusters are lopsided, with some cluster containing all good agents and other clusters containing low value agents. One natural ﬁx is to deliberately choose a balanced partition where the weight of a cluster is based on the ratings of agents outside the cluster and we choose a clustering that minimizes the diﬀerence between the cluster weights. However, for this and various notions of balanced partitions, computing the most balanced partition is NP-hard. What is even more problematic is that if we choose a balanced partition, the resulting mechanism is not strategyproof.
We point out that there are instances where Partition may perform better than ExactDollarPartition even if the rankings of the agents are unanimous. Consider a case where a highly preferred agent is in the same group as

6For high stakes outcomes, we want a mechanism that performs well on average and rarely returns an especially bad outcome.
19

the lowest preferred agents, while other groups only contain medium preferred agents. In that case the weight of the cluster with the highest preferred agent might be so high that the lowest ranked agents might also be selected.The normalization of scores entailed in ExactDollarPartition causes a certain loss of information and granularity compared to the other mechanisms. However, even in the example above, ExactDollarPartition will ensure that when agents have highly correlated or unanimous preferences, the agent(s) that are unanimously on the top will be selected, even if some low-ranked agents are also selected.
7. Experimental Comparison with Other Mechanisms
Using Python and extending code from PrefLib [42] we have implemented the ExactDollarPartition, Credible Subset, Partition, Dollar Raﬄe, Dollar Partition Raﬄe, and Vanilla peer selection mechanisms. All the code developed for this project is available open-sourced in the PeerSelection repository on GitHub.7 As in all simulations there are many parameters to consider that can drastically aﬀect the outcome (see e.g., [48]). By focusing on a target domain, the NSF Mechanism Design Pilot [45, 56], we can draw focused conclusions from our simulations. In 2014, this program had n = 131 proposals, with each submitter reviewing m = 7 other proposals, broken into = 4 clusters. The acceptance numbers are not broken out from the global ≈20% acceptance rate, so we use this as the acceptance rate.
7.1. Experimental Setup To create NSF-like data we generate a sparse N = {1, . . . n} scoring matrix
(proﬁle) using a Mallows Model to generate the ordinal evaluation [40, 41]. Mallows models are parameterized by a reference order (σ) and a dispersion parameter (φ). The reference order σ can be thought of as the underlying ground truth; the NSF mechanism assumes implicitly that there is some true ordering of proposals of which the reviewers provide a noisy observation. Intuitively, the dispersion parameter φ is the probability of committing ranking errors by swapping neighboring elements of σ, where φ = 0 means that no agent ever commits an error and φ = 1.0 means that orderings are generated uniformly at random [37]. Mallows models are used when each agent is assumed to have the same reference ranking subject to some independent noise from a common noise model. Each agent i ∈ N ends up with a ranking rank(i, j) → {0, . . . , m − 1} over each agent j ∈ N where rank(i, j) = 0 means j is the highest reviewed proposal by i. In our evaluation we assume that all agents share the same ground truth ranking, σ, and that all agents share the same noise parameter φ that we sweep across a range of values. An interesting direction for future work would be comparing the algorithms when agents may have diﬀerent notions of the ground
7https://github.com/nmattei/peerselection
20

truth ordering, i.e., diﬀerent values for σ, and/or have diﬀerent levels of ability, i.e., diﬀerent values for φ (as [15, 14] implicitly do).
Each agent reviews m of the n proposals and is also reviewed by m other agents. Agents are clustered under the constraint that each agent reviews m agents outside his cluster. We call a reviewer assignment satisfying these constraints a balanced m-regular assignment. To maximize inter-cluster comparison, we want the m reviews provided by agent i to be balanced among the clusters (less Ci) so agent i in cluster Ci reviews in total m−1 agents from each other cluster. We generate this assignment randomly and as close to balanced as possible. Up to this point, our setup is similar to that of Caragiannis et al. [15], used for studying grade aggregation in MOOCs.
We generate a sparse n × n score matrix by: drawing a balanced m-regular assignment; generating a complete ordinal ranking using a Mallows model for agent i; removing all candidates from i’s ordinal ranking not assigned to i; and assigning score m − rank(i, j) to each agent j that i ranks (known as the Borda score). This process mimics the underlying assumption of the NSF mechanism in that, if a reviewer were to see all proposals, they could strictly order the complete set. The Borda score is well-motivated in this setting as it is the optimal scoring rule, i.e., returns a result closest to the ground truth ranking, for aggregation when agents submit correct orderings [15], and is the one used by the NSF in their pilot. This process leaves us with a sparse n × n score matrix which obeys an m-regular assignment of agents partitioned into clusters.
We use two diﬀerent orderings to evaluate the performance of all the mechanisms presented: the ground truth (GT) ordering and the ordering selected by vanilla (V). Since a Vanilla-like mechanism is used in many settings, it is ﬁtting to see how well the strategyproof mechanisms approximate it (though we assume it is not manipulated by the participants despite not being strategyproof). This allows us to understand the “price” we are paying for strategyproofness. Formally, let W, W be the winning sets returned by two mechanisms, we measure the similarity of W to W by |W ∩ W |/k. For n = 130 and s = 1000, we looked at an “NSF Like” space with k ∈ {15, 20, 25, 30, 35}, m ∈ {5, 7, 9, 11, 13, 15}, φ ∈ {0.0, 0.10, 0.20, 0.35, 0.50}, and ∈ {3, 4, 5, 6}.
7.2. General Results
It is hard to directly compare results for Credible Subset due to the high probability of returning an empty set under the given parameters. In fact, counter to intuition, Credible Subset performs worse as we increase the number of reviews because this increases the chance of returning an empty set (see, e.g., Figure 1.) This problem is not easy to overcome; removing the ability to return an empty set means Credible Subset is no longer strategyproof. When Credible Subset does return a set, it performs very well, on par with Vanilla. However, the minimum (0 in some cases), average, and standard deviation for Credible Subset are all unacceptable for practical implementation.
Throughout the testing ExactDollarPartition strictly outperforms, for all parameter settings, the other Dollar-based mechanisms described in Section
21

1.0 0.8 0.6 0.4 0.2 0.0
1.0 0.8 0.6 0.4 0.2 0.0
Vanilla

5

15

7 20
9 25
11 30
13

Versus Vanilla
k
m
ExactDollarPartition

Partition

35

15

Versus Ground Truth
k

35

7 20
9 25
11 30
13

15

15

5

DollarPartitionRaffle

m
DollarRaffle

CredibleSubset

Figure 1: Performance of the six mechanisms surveyed in this paper as we vary settings to k (top row) and m (bottom row) as measured in selected percentage of the Vanilla set (V, left) and ground truth ordering (GT, right). Note that the colors of the bars are in the same order as the table key. For both ﬁgures we have set n = 130, = 4, and φ = 0.5; for the top row, we set m = 9 as we varied k and for the bottom row we set k = 30 as we vary m. Across the tested range ExactDollarPartition outperforms all other mechanisms when measured against the V or GT ordering with a lower standard deviation. As we increase the value of m or k, ExactDollarPartition improves its performance at a faster rate and more consistently than any other mechanism.

6. Hence, we conclude that the extra steps developed for ExactDollarPartition are necessary and result in a dramatically increased performance. In this discussion, we will focus on comparing the performance of ExactDollarPartition and Partition only, as these two mechanisms are the top two performing mechanisms that are also strategyproof. Broadly, we ﬁnd that ExactDollarPartition outperforms Partition. On average, ExactDollarPartition selects between 0.5% and 5% more top agents, measured against V or GT. It does this with between 3% and 25% lower standard deviation, and selects as many or, in the most extreme cases, up to ﬁve more top performing agents. This improvement in the worst case means that ExactDollarPartition is an improvement of up to 25% when the clustering of the agents is lopsided. Hence, ExactDollarPartition is the best in our experiments: it selects more top agents, more often, with better worst case performance and lower variance than any other strategyproof mechanism.
7.3. Varying Noise(φ) and Clusters ( )
Varying the noise parameter φ has little to no eﬀect on the approximation to V for all mechanisms. This is not surprising as all mechanisms receive the same (noisy) information. When approximating GT, the value of φ has a negligible eﬀect on the performance of the mechanisms unless φ ≥ 0.95; for the remainder, of the discussion we ﬁx φ = 0.5. Varying the setting of we see that all mechanisms perform best with respect to GT when we set = 5 and with respect

22

to V when = 3; with a decrease in performance as we continue to increase . No matter the setting, increasing the number of clusters hurt the performance of Vanilla and ExactDollarPartition the least, i.e., their performance decreases less quickly than the other mechanisms. For the remainder we set = 4 as was done for the NSF pilot.
7.4. Varying the Number of Selections (k) and Reviews (m)
Figure 1 captures our metrics as we vary the number of selections k, in the top row, and the number of reviews per item m, in the bottom row. Varying the setting to k we observe fairly consistent performance by the mechanisms with ExactDollarPartition maintaining a 1.5% to 3% advantage. The biggest percentage-wise advantages are found when k = 15, where ExactDollarPartition selects up to two more top agents according to V, in the worst case, resulting in a ≈ 25% improvement. In the worst case, up to two more top agents according to GT are selected by ExactDollarPartition than Partition. Because both mechanisms perform worse (in absolute terms) than they do as measured by V, this translates to a 10–20% increase in performance for ExactDollarPartition when k is small and a 5–10% increase when k is large. Measured against both V and GT we can draw the general conclusion that, as we increase k, ExactDollarPartition increases its advantage over Partition.
For the most NSF-like setting where we have n = 130, k = 30, l = 4, φ = 0.5, we sweep m ∈ {5, 7, 9, 11, 13, 15}, depicted as the bottom row of Figure 1. Looking closely at the numbers as measured against V we see that ExactDollarPartition performs 2.6 to 3.0% better on average, i.e., one better agent. ExactDollarPartition does this with a nearly 20% smaller standard deviation, always selecting at least one more and up to three more top agents (15%) in the worst case. This pattern is similar across settings to the other parameters as vary m; ExactDollarPartition performs consistently better as we increase the number of reviews. Compared to the performance of Vanilla, ExactDollarPartition selects about one more non-top agent on average, up to two more non-top agents in the worst case (≈ 7%). Hence, the loss in performance we see for moving to a strategyproof mechanism is similar to the loss in performance we see when moving to Partition from ExactDollarPartition.
8. Conclusion
The problem we have considered here is one that has many common applications: from NSF funding allocations and conference paper selection to voting for a committee in an organization’s board and decision making within groups. All these problems are, fundamentally, a set of peers selecting the “best” subset of themselves according to their own quality criteria. We detail a new strategyproof mechanism, which incorporates ideas from the Partition mechanism [2] and literature on dividing a continuous resource [18, 57], combined with a new allocation mechanism, which addresses a long-standing problem of turning a fraction allocation into an integer one, while adding some desirable properties
23

over existing solutions. Moreover, we are able to show, via a set of simulations, that our proposed mechanism performs better than other existing mechanisms.
The next stage in this line of research, we believe, will not have to do with ﬁnding additional strategyproof mechanisms, but rather with ﬁnding ways to eliminate problematic agents’ preferences. This might be achieved either by relaxing the notion of strategyproofness in return for a degree of agent incentive, or by identifying “problematic” agents or very good ones, whose opinions may be weighed diﬀerently.
Acknowledgments
Authors wish to thank Allan Borodin, Markus Brill, Manuel Cebrian, Serge Gaspers, Ian Kash, Julian Mestre, and Herv´e Moulin for useful comments. Data61/CSIRO (formerly known as NICTA) is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. This research has also been partly funded by Microsoft Research through its PhD Scholarship Program, Israel Science Foundation grant grants #1227/12 and #1340/18, and NSERC grant 482671. This work has also been partly supported by COST Action IC1205 on Computational Social Choice. Haris Aziz was supported by a Julius Career Award and a UNSW Scientia Fellowship.
References
[1] Alfaro, L. D., Shavlovsky, M., 2014. CrowdGrader: Crowdsourcing the Evaluation of Homework Assignments. In: Proceedings of the ACM Technical Symposium on Computer Science Education (ACM-SIGCSE). pp. 415–420.
[2] Alon, N., Fischer, F., Procaccia, A. D., Tennenholtz, M., 2011. Sum of us: Strategyproof selection from the selectors. In: Proceedings of the 13th Conference on Theoretical Aspects of Rationality and Knowledge (TARK). pp. 101–110.
[3] Aziz, H., Lev, O., Mattei, N., Rosenschein, J. S., Walsh, T., 2016. Strategyproof peer selection: Mechanisms, analyses, and experiments. In: Proceedings of the 30th AAAI Conference on Artiﬁcial Intelligence (AAAI). pp. 397–403.
[4] Balinski, M., Young, H. P., 1982. Fair Representation. Yale University Press.
[5] Balinski, M. L., Young, H. P., 1980. The webster method of apportionment. Proceedings of the National Academy of Sciences (PNAS) 77 (1), 1–4.
[6] Baumann, L., 2018. Self-ratings and peer review. SSRN Research Archive.
[7] Berga, D., Gjorgjiev, R., 2014. Impartial social rankings, working paper.
24

[8] Birkhoﬀ, G., 1976. House monotone apportionment schemes. Proceedings of the National Academy of Sciences (PNAS) 73 (3), 684–686.
[9] Bjelde, A., Fischer, F., Klimm, M., December 2015. Impartial selection and the power of up to two choices. In: Proceedings of the 11th International Conference on Web and Internet Economics (WINE). Amsterdam, The Netherlands, pp. 146–158.
[10] Bloch, F., Olckers, M., 2018. Friend-based ranking. SSRN Research Archive.
[11] Bogomolnaia, A., Moulin, H., 2001. A new solution to the random assignment problem. Journal of Economic Theory 100 (2), 295–328.
[12] Bousquet, N., Norin, S., Vetta, A., 2014. A near-optimal mechanism for impartial selection. In: Proceedings of the 10th International Workshop on Internet and Network Economics (WINE). Lecture Notes in Computer Science (LNCS). pp. 133–146.
[13] Budish, E., Che, Y.-K., Kojima, F., Milgrom, P., 2013. Designing random allocation mechanisms: Theory and applications. American Economic Review 103 (2), 585–623.
[14] Caragiannis, I., Krimpas, G. A., Voudouris, A. A., 2015. Aggregating partial rankings with applications to peer grading in massive online open courses. In: Proceedings of the 14th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). IFAAMAS, pp. 675—683.
[15] Caragiannis, I., Krimpas, G. A., Voudouris, A. A., 2016. How eﬀective can simple ordinal peer grading be? In: Proceedings of the 17th ACM Conference on Electronic Commerce (ACM-EC). ACM Press, pp. 323–340.
[16] Cole, S., Cole, J., Simon, G., 1981. Chance and consensus in peer review. Science 214 (4523), 881–886.
[17] Court, D., Gillen, B., McKenzie, J., Plott, C. R., 2015. Two information aggregation mechanisms for predicting the opening weekend box oﬃce revenues of ﬁlms: Boxoﬃce Prophecy and guess of guesses. Tech. Rep. SSWP1412, California Institute of Technology.
[18] de Clippel, G., Moulin, H., Tideman, N., 2008. Impartial division of a dollar. Journal of Economic Theory 139, 176–191.
[19] Fischer, F., Klimm, M., 2014. Optimal impartial selection. In: Proceedings of the 15th ACM Conference on Economics and Computation (ACM-EC). ACM Press, pp. 803–820.
[20] Gandhi, R., Khuller, S., Parthasarathy, S., Srinivasan, A., 2006. Dependent rounding and its applications to approximation algorithms. Journal of the ACM 53 (3), 324–360.
25

[21] Gibbard, A., July 1973. Manipulation of voting schemes. Econometrica 41 (4), 587–602.
[22] Gibbard, A., 1977. Manipulation of schemes that mix voting with chance. Econometrica 45 (3), 665–681.
[23] Grimmet, G., 2004. Stochastic apportionment. The American Mathematical Monthly 111 (4), 299–307.
[24] Hall, R. L., Grofman, B., December 1990. The committee assignment process and the conditional nature of committee bias. The American Political Science Review 84 (4), 1149–1166.
[25] Hazelrigg, G. A., 2013. Dear Colleague Letter: Information to Principal Investigators (PIs) Planning to Submit Proposals to the Sensors and Sensing Systems (SSS) Program October 1, 2013, Deadline. NSF Website, http://www.nsf.gov/pubs/2013/nsf13096/nsf13096.jsp.
[26] Holzman, R., Moulin, H., 2013. Impartial nominations for a prize. Econometrica 81 (1), 173–196.
[27] Hussam, R., Rigol, N., Roth, B., 2018. Targeting high ability entrepreneurs using community information: Mechanism design in the ﬁeld. SSRN Research Archive.
[28] Joachims, T., Raman, K., 2015. Bayesian ordinal aggregation of peer assessments: A case study on KDD 2015. Tech. rep., Cornell University.
[29] Kahng, A., Kotturi, Y., Kulkarni, C., Kurokawa, D., Procaccia, A. D., 2018. Ranking wily people who rank each other. In: Proceedings of the32ndAAAI Conference on Artiﬁcial Intelligence (AAAI).
[30] Keynes, J. M., 1936. The General Theory of Employment, Interest and Money. Palgrave Macmillan.
[31] Kotturi, Y., Kahng, A., Procaccia, A. D., Kulkarni, C., 2018. Rising above conﬂicts of interest: Algorithms and interfaces to assess peers impartially. Working Paper.
[32] Kulkarni, C., Wei, K., Le, H., Chia, K. D., Dec. 2013. Peer and self assessment in massive online classes. ACM Transactions on Computer Human Interaction (TOCHI) 20 (6), 1–31.
[33] Kurokawa, D., Lev, O., Morgenstern, J., Procaccia, A. D., 2015. Impartial peer review. In: Proceedings of the 23rd International Joint Conference on Artiﬁcial Intelligence (IJCAI). AAAI Press, pp. 582–588.
[34] Lakhani, K. R., Garvin, D. A., Lonstein, E., 2010. Topcoder (a): Developing software through crowdsourcing. Harvard Business Review.
26

[35] Li, D., Agha, L., 2015. Research funding. Big names or big ideas: do peerreview panels select the best science proposals? Science (New York, N.Y.) 348 (6233), 434–438.
[36] Lian, J. W., Mattei, N., Noble, R., Walsh, T., 2018. The conference paper assignment problem: Using order weighted averages to assign indivisible goods. In: Proceedings of the32ndAAAI Conference on Artiﬁcial Intelligence (AAAI).
[37] Lu, T., Boutilier, C., 2011. Learning Mallows models with pairwise preferences. In: Proceedings of the 28th International Conference on Machine Learning (ICML). pp. 145–152.
[38] Luo, H., Robinson, A. C., Park, J.-Y., 2014. Peer grading in a MOOC: Reliability, validity, and perceived eﬀects. Journal of Asynchronous Learning Networks 18 (2).
[39] Mackenzie, A., 2015. Symmetry and impartial lotteries. Games and Economic Behavior 94 (1), 15–28.
[40] Mallows, C., 1957. Non-null ranking models. Biometrika 44 (1), 114–130.
[41] Marden, J. I., 1996. Analyzing and Modeling Rank Data. No. 64 in Monographs on Statistics and Applied Probability. CRC Press.
[42] Mattei, N., Walsh, T., 2013. Preﬂib: A library for preferences. http://www.preflib.org. In: Proceedings of the 3rd International Conference on Algorithmic Decision Theory (ADT). pp. 259–270.
[43] Mayberry, J. P., 1978. Quota methods for congressional apportionment are still non-unique. Proceedings of the National Academy of Sciences (PNAS) 75 (8), 3537–3539.
[44] McNutt, R. A., Evans, A. T., Fletcher, R. H., Fletcher, S. W., 1990. The eﬀects of blinding on the quality of peer review: A randomized trial. The Journal of the American Medical Association 263 (10), 1371–1376.
[45] Merriﬁeld, M. R., Saari, D. G., 2009. Telescope time without tears: a distributed approach to peer review. Astronomy & Geophysics 50 (4), 4–16.
[46] Naghizadeh, P., Liu, M., 2013. Incentives, quality, and risks: A look into the NSF proposal review pilot. arXiv preprint arXiv:1307.6528, 1–10. URL http://arxiv.org/abs/1307.6528
[47] Piech, C., Huang, J., Chen, Z., Do, C., Ng, A., Koller, D., July 2013. Tuned models of peer assessment in MOOCs. In: Proceedings of The 6th International Conference on Educational Data Mining (EDM). Memphis, Tennessee, pp. 153–160.
27

[48] Popova, A., Regenwetter, M., Mattei, N., 2013. A behavioral perspective on social choice. Annals of Mathematics and Artiﬁcial Intelligence 68 (1–3), 135–160.
[49] Price, E., December 2014. The NIPS experiment. http://blog.mrtz.org/2014/12/15/the-nips-experiment.html.
[50] Pukelsheim, F., 2014. Proportional Representation: Apportionment Methods and Their Applications. Springer.
[51] Robinson, R., 2001. Calibrated peer review an application to increase student reading and writing skills. The American Biology Teacher 63 (7), 474–476.
[52] Roos, M., Rothe, J., Scheuermann, B., 2011. How to calibrate the scores of biased reviewers by quadratic programming. In: Proceedings of the 25th AAAI Conference on Artiﬁcial Intelligence (AAAI). pp. 255–260.
[53] Satterthwaite, M. A., April 1975. Strategy-proofness and Arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare functions. Journal of Economic Theory 10 (2), 187–217.
[54] Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., Von Luxburg, U., 2018. Design and analysis of the NIPS 2016 review process. Journal of Machine Learning Research 19 (1), 1913–1946.
[55] Stelmakh, I., Shah, N. B., Singh, A., 2018. Peerreview4all: Fair and accurate reviewer assignment in peer review. In: Proceedings of the30th International Conference on Algorithmic Learning Theory (ALT).
[56] The National Science Foundation, 2014. Report to the National Science Board on the National Science Foundation Merit Review Process Fiscal Year 2014. Tech. rep., The National Science Foundation (USA).
[57] Tideman, T. N., Plassmann, F., 2008. Paying the partners. Public Choice 136 (1/2), 19–37.
[58] Walsh, T., 2014. The PeerRank method for peer assessment. In: Proceedings of the 21st European Conference on Artiﬁcial Intelligence (ECAI). pp. 909–914.
[59] Wenneras, C., Wold, A., 1997. Nepotism and sexism in peer-review. Nature 387, 341–343.
[60] Wright, J., Thornton, C., Leyton-Brown, K., 2015. Mechanical TA: Partially automated high-stakes peer grading. In: Proceedings of the ACM Technical Symposium on Computer Science Education (ACM-SIGCSE). pp. 96–101.
[61] Young, H. P., 1994. Equity: in Theory and Practice. Princeton University Press.
28

