1
Apparel-invariant Feature Learning for Person Re-identiﬁcation
Zhengxu Yu, Yilun Zhao, Bin Hong, Zhongming Jin, Jianqiang Huang, Deng Cai, Member, IEEE, Xiaofei He, Senior Member, IEEE and Xian-Sheng Hua, Fellow, IEEE

arXiv:2008.06181v2 [cs.CV] 17 Aug 2020

Abstract—With the rise of deep learning methods, person ReIdentiﬁcation (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons’ appearance rarely changes. In real-world applications such as in a shopping mall, the same person’s clothing may change, and different persons may wearing similar clothes. All these cases can result in an inconsistent ReID performance, revealing a critical problem that current ReID models heavily rely on person’s apparels. Therefore, it is critical to learn an apparel-invariant person representation under cases like cloth changing or several persons wearing similar clothes. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparelsimulation GAN (AS-GAN) to synthesize cloth changing images according to the target cloth embedding. It’s worth noting that the images used in ReID tasks were cropped from real-world lowquality CCTV videos, making it more challenging to synthesize cloth changing images. We conduct extensive experiments on several datasets comparing with several baselines. Experimental results demonstrate that our proposal can improve the ReID performance of the baseline models.
Index Terms—Person Re-Identiﬁcation, Image Synthesis, GAN, Transfer Learning
I. INTRODUCTION
Given an image of the person-of-interest under one camera, person re-identiﬁcation (ReID) aims to annotate the target person by matching it with historical images captured by other cameras in the same multi-camera network. ReID has attracted a lot of attention from the community in recent years due to its broad application prospects in new retail business and public security, such as crime prevention [1], ﬁnding elder and children [2] and person activity analysis [3].
Person ReID is challenging because the images ReID used are cropped from low quality CCTV videos, which contain intensive variations like illumination, human pose, camera
Z. Yu, D. Cai and X. He are with the State Key Laboratory of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, Zhejiang 310058, China (emails: yuzxfred@gmail.com; dengcai@gmail.com; xiaofeihe@zju.edu.cn).
Y. Zhao is with Zhejiang University, Hangzhou, Zhejiang 310058, China (emails: zhaoyilun@zju.edu.cn).
Z. Jin, B. Hong, J. Huang and X.-S. Hua are with Alibaba Group, Hangzhou, Zhejiang, China (emails: zhongming.jinzm@alibaba-inc.com; bin hong@zju.edu.cn; jianqiang@alibaba-inc.com; xiansheng.hxs@alibabainc.com).

Fig. 1. Bad case analysis of previous work. Each column is a top-10 ReID list obtained using the ReID method proposed by Hermans et al. [4]. The ﬁrst left person image of each column is the target person sampled from PAVIS dataset [5]. The red box indicates an incorrect match, and the green one indicates a correct match. It is clear that the ReID model heavily relied on the cloth information.
viewpoint, occlusion, and clothing change. Hence, learning a discriminative and robust person representation under such variations is one of the most fundamental problems in person ReID tasks. Early person ReID methods usually use handcrafted discriminative features constructed with low-level features such as color and texture information of the images [6]– [8]. However, in cases of illumination change or cases like several persons wearing similar clothes, these low-level features become unreliable [9]. More recently, many deep-learning based methods were proposed [10]–[18], which applied deep convolutional networks to learn identity-sensitive and viewinsensitive embeddings based on image classiﬁcation and metric learning. Meanwhile, there are also many unsupervised methods were proposed to make use of all possible training data [13], [19]–[22].
Although these recently proposed ReID methods greatly improved the ReID performance in several public datasets, there is a critical problem that most of these ReID models are easily misled by the cloth feature which they rely on. This problem leeds to an unstable performance in real-world longterm ReID applications. We provided a case study in Fig.1 to help understand the most discriminative features the previous ReID methods relied on is the target person’s apparel features, especially cloth features. In most of the existing datasets, images are taking in a short time window like in several days, which people rarely change their clothing. Thus, the same person’s appearance is roughly consistent in different images. Unlike that, the real-world ReID systems (e.g., in a shopping

2

mall) are usually continuously online after deployment. In this case, there may be a long interval between the two shots of the target person, during which there will be many environmental variations, as well as cloth changing and different persons wearing similar clothes. Hence, in real-world applications, the previous ReID models which heavily relied on the apparel features would have substantial performance degradation.
In this paper, we consider cases where the most consistent feature (apparel) is no longer reliable due to apparel changing or different people wearing similar clothes cases. These apparel-changed person ReID problem widely existed in long term person ReID applications like criminal investigation and person activity analysis. Hence, it is crucial to ﬁnd an apparel-invariant feature learning method to learn a more discriminative feature in this apparel-changed scenario.
To solve this apparel-changed ReID problem, we propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework (Sec. III-D) to learn an apparel irrelevant feature representation. To the best of our knowledge, there is no existing dataset contains cloth IDs for each person. Therefore, we ﬁrst propose a GAN based apparel-simulation image generator AS-GAN(Sec. III-C) to generated synthetic apparel changed images for persons in the training dataset. The simplest way to use the AS-GAN is to create a new training dataset by mixing the images generated by AS-GAN and the original images. It could be considered as a data augmentation method. However, the intruding of synthetic images changed the data distribution, which might deviate from the original distribution. As shown in the experiment results in Table. III, directly use the synthetic images is not an effective way.
We categorize the contribution of this work into threefold:
1) We propose a novel semi-supervised Apparel-invariant Feature Learning (AIFL) framework in this work to learn a discriminative feature embedding in cases like apparel changing and the different persons wearing similar cloth cases, and thus improve the ReID performance.
2) To solve the problem of insufﬁcient training data. We construct an apparel-simulation GAN (AS-GAN) to generate cloth changing images.
3) We conduct extensive experiments on datasets with signiﬁcant cloth changing and the different person wearing similar cloth cases, i.e. PAVIS and MSMT17. Experimental results have shown that our proposal could effectively improve the ReID performance of baseline models.
II. RELATED WORKS
A. Person ReID under intensive appearance variations
The negative effects of appearance variation on deeplearning based ReID models have been recognized recently. This appearance variation not only includes the external changes like illuminations and occlusion but also included the person’s appearance changes like apparel and pose. A number of models have proposed to address these problems [22]– [25]. As for the external variations, Deng et al. [22] proposed the similarity preserving generative adversarial network to mitigate the domain difference by conducting image style

transferring. Wei et al. [25] proposed a person transfer model which transfer peoples from source domain into the target domain to mitigate the environment difference. As for the person’s appearance changes, many pose-guided deep-learning ReID models [23], [24] were proposed recently. For instance, Qian et al. [24] proposed a pose-normalized image generator to generate different pose images for the same person. Peng et al. [20] propose an asymmetric multi-task dictionary learning method to transfer the view-invariant representation learned from source data to target data.
However, the person’s apparel variations like cloth changing or different persons wearing similar clothes are rarely mentioned. The reason is easy to understand that the cloth is the most salient characteristic in most public ReID datasets. Thus most of RGB based ReID models heavily rely on the apparel similarity [9]. To solve this problem, Wu et al. [9] proposed an RGB-D based person ReID model, which combined the estimated depth features with RGB appearance features to reduce the visual ambiguities of appearance caused by similar clothes. However, the RGB-D cameras they used are much more expensive than the regular RGB camera, which hindered its application in the real world. Distinct from them, our proposal is still an RGB-based model that reduces the visual ambiguities by learning a self-restraint apparel-invariant feature. Nevertheless, Wu et al. [26] proposed a deep zeropadding model to minimize the modality gap between the RGB image and the IR image. The IR image they used does not contain color information. Our work is distinct from theirs in that we only use the RGB image without any extra information in the whole process.
B. Image Generation
One of the bottlenecks of deep-learning based person ReID models is the lack of training data. Hence, generating realistic images using GAN has received much interest recently.
Zhu et al. [27] proposed a approach to generated realistic person images with different clothes. In their work, they used high-resolution images that are captured in a front view, and the size of the person in this image is nearly the same. They applied the openpose [28], [29] to generate human keypoint map to guild the cloth generation. Their work is related to our image generator. However, our image generator still distinct from their work in that we used the real-world low-resolution image, which contains intensive pose and viewpoint variations.
Ma et al. [30] proposed a two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates new person images at the same time. The generated image of their work could have different apparel and pose, but the generated image does not guarantee the person ID consistency. Different from their work, the ASGAN only changes the cloth part of the image to make sure the generated person image could be identiﬁed as the same person in the input image.
Qian et al. [24] proposed a pose-normalized image generator to generate the same person’s images with a different pose. Inspired by their work, we propose our apparel-simulation GAN based upon conditional GANs (cGANs) [31]. However, our

3

proposal is distinct from Qian’s work because the condition information used in our image generator is different, and the network architecture is also different. We used cloth features extracted by a CNN model as the conditional code. This cloth feature extractor model is also trained in an unsupervised way. Meanwhile, the network structure is different. They used the ’U-net’ structure to preserve the low-level information. However, in apparel-changed image generation, the original cloth’s low-level information must be removed from the feature maps. Pathak et al. [32] proposes an unsupervised visual feature learning algorithm driven by context-based pixel prediction. They use the context information to generate the missing part of the input image. Inspired by their work, we erase the cloth part of the input image to eliminate the cloth’s impact on the generated image. Our proposal is distinct from their work because the goal of their work is to repair the input image, but what we want is to change the content of the missing part.

C. Unsupervised Learning
Many unsupervised [19], [33]–[36] person ReID methods have been proposed recently. RGB-based Hand-crafted features [6], [37], [38] can be directly employed for unsupervised ReID. All these methods focused on feature design but ignored the information from the distribution of samples in the dataset. Moreover, the most commonly used feature in the hand-crafted feature based ReID methods is colour [39], especially the color of clothes. Consequently, these features will become unreliable in the presence of intensive apparel variations. Classical unsupervised learning methods such as auto-encoders [40], [41] aim to learn useful feature representations by simply reconstructing it after a bottleneck. More related to our work, Donahue et al. [36] proposed a GAN based feature learning framework BiGAN, which learning feature representation by reconstructing the input image. However, our proposed AIFL framework is an auto-encoder based network which input and target image are two different images.

III. PROPOSED METHOD
A. Problem Formulation and Overview
Before presenting our method, we ﬁrst introduce some basic notions and terminologies. Suppose a gallery set G contains N cropped person images {xk}Nk=1. They belong to N different identities 1, 2, ..., N . We ﬁrst denote a feature extraction model as φ(·; θ), which is parameterized by θ. Given a query person q ∈ Q, the identity of q, denote as id (q), is determined as:

k∗ = arg min d(φ(q; θ), φ(xk; θ))

(1)

k∈N

Where d(·, ·) is some kind of distance function.

In the apparel-changed person ReID scenario, we step

further to add a constraint to restraint the queried person

should acquire the same ID when he/she is wearing different

apparels:

id (qj) = id (qj )

(2)

Where j, j is the two different apparels which person q has worn.

Start

Input image 𝐼

input
AS-GAN

generate

Synthetic image 𝐼"

Image pair

CNN model 𝐹

input
AIFL

After initialized model 𝐹$
Fine-tuning

Final model

Fig. 2. Overview of our proposal. Our proposal is consists of two parts: 1) the apparel-simulation GAN(AS-GAN), 2) the apparel-invariant feature learning(AIFL) framework. The AS-GAN is design to generate synthetic cloth images. The AIFL framework is an auto-encoder based newtwork which is demonstrated in Sec. III-D.

By observing that:

|d(φ(qj; θ), φ(xk; θ)) − d(φ(qj ; θ), φ(xk; θ))| (3) < d(φ(qj; θ), φ(qj ; θ))

holds for ∀k ∈ N . To ensure Eq. 2, we need to make d(φ(qj; θ), φ(qj ; θ)) as
small as possible for ∀q ∈ Q. Hence, the person ReID problem
could be formulated as optimize the fellowing equation:

1

min |Q|

d(φ(qj; θ), φ(qj ; θ))

(4)

q∈Q

B. Framework Overview
As we mentioned above, there is no existing ReID dataset contains cloth label. Hence, we proposed a novel apparelsimulation image generator (AS-GAN) to generate the apparelchanged images using unlabelled irrelevant person images.
To make use of these unlabelled images, we introduce the core proposal of this paper – the apparel-invariant feature learning (AIFL) framework.
The overview of our proposal have shown in Fig.2, our proposal has two key components, i.e., the apparel-invariant feature learning framework(AIFL) (Sec. III-D), and the GAN based person image generation model(AS-GAN) (Sec. III-C).
During training, the original image and the generated image will be paired up and send into the AIFL framework. Give a CNN based feature extractor model F , we use the image pairs generated by AS-GAN to learn an apparel-invariant feature in AIFL. For each image pair (or an iteration), F will be trained twice. We ﬁrst set the original image and the synthetic image as target and input to train the F for the ﬁrst time. After that, we reverse the input and target and train the networks once more. By doing so, the apparel feature will be ignored by F to yield a lower loss. We then add a CNN based classiﬁer on the output of F to conduct ReID application. To build up the

4

Cloth image 𝐼#

Apparel Encoder 𝐸%
Apparel Code 𝑐#

mCalsokth𝐼" * 𝐿+,

dot

Input image 𝐼

Pixel2Pixel

product

Background Mask 𝐼"

*
dot product
The rest part 𝐼-

Concat.

Feature maps

Feature

𝐺%

maps

AS-GAN

Image Generator 𝐺

𝐿',
𝐷'

Fig. 3. Schematic of the apparel-simulation GAN. We ﬁrst applied a pixel2pixel model to parse out the cloth mask, and then, extract the cloth code using an auto-encoder network. Then this cloth code will be used in the AS-GAN. During training, the cloth code is extracted from the input image. During generation, the cloth code is extracted from another randomly selected image in the same dataset. This ﬁgure only demonstrates the training process here.

connection between feature embedding and person IDs, we also conduct model ﬁne-tuning on the ReID model F and the classiﬁer.

C. Apparel-simulation Image Generator
Inspired by [24] and [32], we constructed the apparelsimulation GAN (AS-GAN) to synthesize the same person’s images conditional on apparel. This generator is formed by a GAN based image generator G and an auto-encoder based apparel encoder EA, as shown in Fig.3. Since the only thing we want to change is the cloth, it is natural to parse out the cloth and keep anything else unchanged. To know where the clothes are in the input image, we ﬁrst trained a pixel2pixel [42] model on HumanParsing-Dataset [43] to generate cloth mask (Im) for the input person image I. We then multiply the input image I with the cloth mask Im to separate the input image into the cloth part (Ic) and the rest part (Ib). All the images used in this work are resized into 256x128 (height x width) in advance.
Apparel encoder EA. We ﬁrst constructed the auto-encoder based cloth encoder EA to generate a unique code for each cloth. The EA is constructed by a ﬁve-layer convolution neural network as encoder, following by a ﬁve-layer deconvolution neural network as the decoder. Each layer of the encoder is consists of a convolutional block (Conv2d block) contains a 2dimensional convolutional (Conv2d) layer, a LeakyReLU layer and a BatchNorm layer. The negative slope is set to 0.2 in the LeakyReLU layer. Each layer of the decoder contains a 2-dimensional transpose convolutional (TransConv2d) block included a TransConv2d layer and the following ReLU layer and a BatchNorm layer. The encoder down-sampling the cloth image Ic into a 512x8x4 (channel x height x width) feature maps (cloth code cc). During training, we set the input cloth image Ic as input and target at the same time. And we applied L1 loss in this auto-encoder network. The loss function LEA has shown below:

LEA = Ic − EA(Ic) 1

(5)

Where · is the L1 loss, Ic is a cloth image. Image generator G. To generate a more realistic image, we constructed the image generator G based on conditional GANs. The G has two components, a Generator GA and a Discriminator DG , as shown in Fig.3. Given an input image Iq of person q and a random select cloth Ic. We can use Ic as input to get the cloth code cc using the EA. our image generator aims to synthesise a new image Iq under condition cloth code cc, and ensure the identity consistency of Iq and Iq. The generator GA consists of 5 down-sampling block, followed by ﬁve up-sampling blocks. Except for the second down-sampling block, all the other down-sampling blocks contain a 4x4 kernel size Conv2d layer with stride 2 and padding 1 followed by a LeakyReLU layer and a BatchNorm layer. The negative slope of these LeakyReLU layers is set to 0.2. And each up-sampling block includes a TransConv2d layer followed by a ReLU layer and a BatchNorm layer. The apparel code cc generated by EA is concatenated with the output of the last down-sampling block of GA. We then send the mixture into a 1x1 Conv2d layer and the following upsampling blocks. To eliminate the information of the original apparel in Iq, inspired by [32], we use the complement I¯m of the cloth mask Im to set the cloth part of the input image I as 0. We denote this image without cloth part as Ib. The second down-sampling block consists of three different kernel size Conv2d layers (names the reﬁned layer). Each one followed by a LeakyReLU layer and a BatchNorm layer, as shown in Fig.5. The output of the ﬁrst layer is divided along the channel into three equal parts. Each part feed into the different kernel size Conv2d block. The generated images compare with only use one 4x4 Conv2d block as the second down-sampling block has shown in Fig.4. The discriminator DG consists of 6 Conv2d blocks and a linear layer. Each Conv2d block contains a Conv2d layer and a BatchNorm layer and a LeakyReLU layer. This discriminator aims to discriminate between real data samples from the generated samples and thus helps to improve the quality of produced images. Work Flow. As shown in Fig.3. During training, we ﬁrst use the cloth mask Im generated by the pixel2pixel model(as discussed above) to separate the input image I into cloth image Ic and the rest part image Ib. We then extract the cloth code cc using cloth image Ic. By setting the input image as the target image at the same time, this image generator could learn how to use the cloth code to reconstruct the original image. To help GA reconstruct the cloth in the right place, Ic was randomly ﬂipped and cropped before feeding into apparel encoder EA. Because of the cloth code only contains the cloth information and the only thing missing in the rest part image is the cloth, the image generator will learn to use the cloth code to reconstruct the cloth part while using the rest part image to reconstruct other things like human body and background. The objective of the AS-GAN could be expressed as:
LDG = min max EI,cc {log DG (I) + log(1 − DG (GA(Ib, cc))}
GA DG
LGA = I − GA(Ib, cc) 1 +λDG LDG (6)

5

Generated images with refined layer1

Original image

Masked input image

Generated Cloth Generated image3 Code4 image

Cloth code

Generated image

Cloth code

Generated image

Cloth code

Generated image

Cloth code

Generated images use normal layer2
Generated Cloth Generated Cloth Generated Cloth image code image code image code

Fig. 4. Samples generated by using AS-GAN. We sampled 100 thousand images from the MARS dataset [44], and then generated ﬁve different clothes for each image using different cloth code sampled from the same dataset. As we can see, the most conspicuous part of the original cloth has changed according to the cloth code. Worth to mention, the generated cloth number is not optimal, more clothes could be generated using different cloth code. 1. Image generated by image generator with the second Conv2d block replaced by the reﬁned layer, as shown in Fig.5. 2. Image generated by image generator constructed by the normal 4x4 Conv2d block. 3. The AS-GAN generates the generated image with the randomly selected cloth code on the right. 4. The cloth code is generated by the apparel encoder EA with a randomly selected cloth as input.

Input image

4x4 Conv block

Split into 3 part along the channel dimension

1x1 Conv block 4x4 Conv block 7x7 Conv block
Concat.

Input Image pair 𝐼"#$%

step2

From AS-GAN

𝐹 Feature 𝑅 𝐷0
maps
step1

step2

step1

AIFL

Labelled Image 𝐼3#453356

After-trained
Add classifier
Softmax 𝐿𝑜𝑠𝑠78

𝐹1

Fine-tuning

𝐿𝑜𝑠𝑠+,-.

Following blocks
Fig. 5. The reﬁned layer. The output of the ﬁrst layer is split into 3 part, and each part feed into a sub-block. The sub-blocks is consist of three Conv2d blocks with kernel size 1x1, 4x4 and 7x7 respectively. The output of these sub-block will be concatenated and feeded into the following layers.

Fig. 6. Framework of our AIFL method. Our AIFL framework has an autoencoder based structure. During training, each iteration is consists of two steps. In the ﬁrst step, we set the original image as input and generated image as the target to run the whole network for once. In the second step, we set the generated image as input and the original image as the target. After the training process, we take out the feature extractor F and ﬁne-tuning it using softmax loss on the target dataset like MSMT17 and PAVIS.

Where the LC is the generate loss of image generator, LD is the adversarial loss.
During generation, the input image I also separated into cloth image Ic and the rest part image Ib just like in the training process. However, we only use the Ib in the generation process. As for the cloth code, we randomly sampling a cloth part image Ic of another person’s image, and use the EA to extract the cloth code cc of Ic. Different from the training process, the Ic will not be ﬂipped and cropped during the generation process. With the randomly sampled cloth

code cc and the rest part image Ib, the image generator GA would generate person image with different apparel but keep the person’s identity in I unchanged. The generated image samples has shown in Fig.4.
D. Apparel-invariant Feature Learning framework
As shown in Fig.6, two key components construct the AIFL framework: an auto-encoder based transfer net T and the followed discriminator DT . This framework is trained in an unsupervised manner since we do not have any person or cloth label.

6

Apparel changed image generation. Given a person image dataset, we ﬁrst obtain the cloth code of each image in this dataset using our proposed apparel encoder EA. Theoretically, we can obtain n cloth code from a dataset contains n images. Hence, we can generate n−1 apparel changed images for each person using the image generator G.
Transfer net T . To the best of our knowledge, no existing deep-learning based model has been proposed to solve the apparel changed person ReID problem. The main idea of our Apparel-invariant Feature Learning (AIFL) framework is to train a feature extractor to learn the apparel-irrelevant information. To achieve this goal, given an off-the-shelf CNN model F like ResNet-50 or DenseNet-161, we ﬁrst implant F as the encoder in the transfer net T and followed by a decoder R. To improve the performance, we also applied a discriminator DT to distinguish whether an input image is real. After the training process, the F will be taken out and be ﬁne-tuned into the ﬁnial ReID model using softmax loss. The structure of T has shown in Fig.6.
During training, we ﬁrst pair up the synthetic image I with the original image I. As shown in Fig.6, the transfer net will be trained in the step in each iteration. In step 1, we set the I as input and I as the reconstruct target to train the whole transfer net T . We took out the feature maps of the last Conv2d layer of the feature extractor F and fed into the decoder R. In the decoder, the feature maps will be used to reconstruct an image with the same size as the original image. Next, we calculate the loss given in Eq.9 and conduct backpropagation. After that, we reverse the input and target input to train the whole transfer net T in step 2.
Since the synthetic image generated image and original image contain the same person worn different clothes, the model has to ignore the difference (apparel) between these two images to yield a lower loss in the training process. Moreover, the illumination variations of images containing the same person could also be mitigated by imperfect image generation. As for objective, we use a L1 loss as the reconstruction loss, as shown in Eq.7:

Lrecon = Itarget − T (x) 1

(7)

Where x is the input image (I in step 1 and I in step 2), and Itarget is the target image (I in step 1 and I in step 2).
Discriminator DT . To improve the performance of our model, we construct a 7 layers discriminator DT including 6 Conv2d blocks and a linear classiﬁcation layer. Each convoluational blocks contains a Conv2d layer and a BatchNorm layer and a LeakyReLU layer with negtive slope 0.2. The discriminator is aims to differentiate the input images is real or fake by generate a binary classiﬁcation mask. The adversarial loss has shown below:

LDT = min max Ex log[DT (x)] + ET(x) log[1 − DT (T (x)))]
DT T (x)
(8)
Where x is input image, T (x) is reconstruction output of T with input x.
This objective encourages the transfer net T to reconstructed a more realistic image, and eventually makes the

features extracted by F of I and I closer. We also conducted an ablation experiment to demonstrate the effects of the discriminator, which will be demonstrated later.
Overall, the joint loss of the AIFL framework can be expressed as Eq.9:

LAIF L = Lrecon + λDT LDT

(9)

Where λDT is the hyper-parameter to control the impact of the discriminator, we select 0.001 by practice in all the following experiments.

E. Fine-tuning algorithm
After the training process in the Transfer net, we took out the trained CNN model F from T and added an average pooling layer and three linear layers as a classiﬁer at the end of the F . After that, we conduct the ﬁne-tuning process on F as in standard classiﬁcation task using softmax loss. In the ﬁne-tuning process, a small number of labeled images will be used to ﬁne-tune this model. The label information here only contains person ID, and the cloth ID is not requested.
IV. EXPERIEMENT AND RESULTS
A. Datasets
We conduct experiments on two ReID datasets to evaluate the performance of the proposed method, including PAVIS [5] and MSMT17 [25]. The irrelevant person images used in AIFL and AS-GAN are sampled from a large-scale unlabeled MARS [44] dataset. The summary of these datasets could be found in Table I.
PAVIS. We used two groups of images in the PAVIS dataset for evaluation here, denoted by ’Walking1’ and ’Walking2’. Images of ’Walking1’ and ’Walking2’ were obtained by recording the same 79 people with a frontal view, walking slowly in an indoor scenario using an RGB-D camera. Among these 79 persons, from one group to another, 60 persons have changed their apparel. Each person has 4 or 5 and no more than 5 images in each group. By following the train-test policy, we randomly sampled 36 persons for training, 8 persons for validation, and 35 persons for testing. We repeated the dataset split 10 times and trained 3 models in each dataset. We then take the arithmetic mean of these model’s evaluation results as the ﬁnal result.
MSMT17. The cloth id for each person is not available in this dataset as well. However, the raw video in this dataset is recorded in 4 days with different weather conditions in a month using 12 outdoor cameras and 3 indoor cameras. Therefore, the illumination variations and pedestrian apparel changes in this dataset are richer. Meanwhile, there are abundant different persons wearing similar cloth cases in this dataset. The MSMT17 dataset contains 126,441 bounding boxes of 4,101 identities. The dataset partition has shown in Table I, we followed the same dataset split in [25], and used the evaluation code provided by them as well.
C-MARS. We randomly sampled 100,000 images without any label information from the large-scale dataset MARS [44] as the training data in the AIFL framework. The images in

7

TABLE I SUMMARY OF DATASETS

Dataset PAVIS2

Cams 2

Max. cloth change1 1

Training (Images/IDs) 36/-

Gallery (Images/IDs) 35/-

Query (Images/IDs) 35/-

Validation (Images/IDs) 8/-

Total (Images/IDs) 788/79

MSMT17 15

-

C-MARS3

6

6

30,248/1,041 600,000

82,161/3,060 -

11,659/3060 -

2,373/1,041 -

126,441/4,101 600000

1 The maximum cloth change times. The cloth id in the MSMT17 is not available. 2 We split the PAVIS dataset 10 times. The image number of each set is not ﬁxed. The gallery set IDs is the same as in the query set. 3 The C-MARS contains 100,000 images sampled from the MARS dataset, and 5 sets of synthetic images generated by AS-GAN using the sampled images. These images only use to initialise the CNN model.

MARS is collected by ﬁve HD cameras and one SD camera. There are more than 1.1 million bounding-boxes in this dataset. Moreover, it is consists of 1,261 different pedestrians who are captured by at least two cameras. We generated ﬁve sets of cloth changed images for the sampled images using ASGAN to form the C-MARS dataset. As shown in Table I, there are 60,000 image in the C-MARS datasets. Each synthetic image of the ﬁve sets was paired up with the original images to form 50,000 image pairs. One set corresponds to 100,000 pairs of images. This dataset is only used to initialize the feature extractor model via the AIFL framework. Hence, we do not keep label information in this dataset.
B. Evaluation Settings
Feature extractor F . Most of the existing CNN models could be implanted in the proposed AIFL framework as the feature extractor. In this work, we selected two off-the-shelf model, ResNet-50 [45] and DenseNet-161 [46] to illustrate the effectiveness of our method. Due to the outstanding performance, these models are widely used in many computer vision tasks, including person ReID. All the ResNet-50 and DenseNet-161 models used in the experiments were pretrained on the ImageNet [47]. Moreover, all of the codings are completed using Pytorch with version 0.4.0 and torchvision with version 0.2.1.
Parameter setting. All images used in this work are resized to 256 x 128 (height x width) in advance. All experiments used the same parameter settings. The batch size in the initializing process and the ﬁne-tuning process are set to 128 and 64. We use stochastic gradient descent (SGD) with 0.9 momentum and 5e-4 weight decay as an optimizer in all experiments.
In the initializing process in the AIFL framework, we used an initial learning rate of 0.1, and decline 10 times per epoch. We set the 0.001 hyper-parameter λDT by practice.
In the ﬁne-tuning process with softmax loss, the newly added layers use 0.1 as the initial learning rate, and the rest layers use 0.01 as the initial learning rate.
We conducted randomly cropping, resizing and horizontal ﬂipping on training images for data augmentation during the ﬁne-tuning process in all experiments. Unless otherwise speciﬁed, the model after the initializing process will be ﬁnetuned for 20 epochs. During the evaluation, we take out the output feature of the average pooling layer as the input image’s embedding. Every experiment is repeated for 3 times, and we only reported the mean values.

TABLE II RESULTS IN PAVIS

Network Structure AlignedReID [48] ResNet-50 (baseline) DenseNet-161 (baseline) ResNet-50 (Our) DenseNet-161 (Our)

mAP 59.66 59.20 59.90 59.90 64.50

CMC@1 53.64 47.10 50.10 53.20 59.40

CMC@5 56.10 57.70 59.70 57.40 61.00

C. Evaluation Protocals
We applied two evaluation methods to quantitatively measure the ReID performance:
1) mean Average Precision(mAP); 2) Cumulative Matching Characteristics (CMC).
D. Evaluation on PAVIS
We ﬁrst use the image pairs in the C-MARS dataset to initialize the ResNet-50 and DenseNet-161 in the AIFL framework. After the initializing process, we took out the CNN model and ﬁne-tuned it use the labeled images in the PAVIS dataset.
As for baselines, we compared with the DenseNet-161 and ResNet-50 model pre-train on ImageNet and also ﬁnetuned in the PAVIS dataset. We also compared with the other state-of-the-art person ReID method AlignedReID [48] on the PAVIS dataset. To the best of our knowledge, the author has not released its codes. Therefore, the implementation of AlignedReID we used is the reproduce version1 on the Github.
As we mentioned above, the PAVIS dataset is consists of two groups of images. People had changed their cloth when the other camera captured him. We have not compared with the previous work [5] on the PAVIS because it is an RGBD based method. Meanwhile, they used a speciﬁc experiment setting by using ’Walking 1’ as the query set and ’Walking 2’ as the gallery set. To evaluate our method in a more persuasive method, we used all the images of the test part IDs as query and gallery set in the same time. During the evaluation, we conduct the cross-group person ReID on the query set images, which means images in the same group will be ignored. Therefore, the model needs to re-identify the
1https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch

8

images in ’Walking 1’ and re-identify the images in ’Walking 2’ by using the ’Walking 1’ as gallery set.
Results has shown in Table II. We can notice that initializing the feature extractor model using the AIFL framework could signiﬁcantly improve the performance. Both the ResNet50 and DenseNet-161 model initialized via the AIFL framework outperforms the baseline models at higher matching accuracies in each rank. The performance of DenseNet-161 is raised from 59.9% to 64.5% on mAP.

E. Evaluation on MSMT17
As we mentioned above, the MSMT17 dataset does not have cloth IDs as well. However, the relatively long time interval has brought variations like illumination and apparel changes in this dataset. To verify the effect of our proposal on the feature extractor model in the apparel-invariant feature learning scenario. We followed the same evaluation setting as in [25] to compare with the previous works. The dataset split has shown in Table I. We conduct the same initialize process as in the PAVIS dataset before ﬁne-tuning it on the MSMT17 dataset.
As for baseline, we use the ResNet-50 and DenseNet-161 pre-train on ImageNet as baseline and ﬁne-tuned it using the train set of MSMT17. Meanwhile, we also compare with three state-of-the-art methods including GoogLeNet [49], PDC [23] and GLAD [50].

TABLE III RESULTS IN MSMT17

Network Structure GoogLeNet PDC GLAD
ResNet-50 (baseline) DenseNet-161 (baseline) DenseNet-161 (baseline)
ResNet-50 (our) DenseNet-161 (our) DenseNet-161 (our)

Training data MSMT17 MSMT17 MSMT17 MSMT17
MSMT17(extend)1 MSMT17 MSMT17
MSMT17(extend) MSMT17

mAP 23.00 29.70 34.00 26.32 21.42 29.43 29.66 22.34 35.24

CMC@1 47.60 58.00 61.40 55.21 49.44 61.67 58.28 49.68 65.96

CMC@5 65.00 73.60 76.80 70.61 64.83 74.77 73.33 65.74 79.01

CMC@10 71.80 79.40 81.60 76.48 71.24 79.49 79.15 72.13 83.80

CMC@20 78.20 84.50 85.90 81.61 77.25 83.36 83.79 78.27 87.76

1. MSMT17(extend) is consists of the original training set of MSMT17 and the 3 sets of synthetic images generated by AS-GAN using the training set image of MSMT17. The generated images is labelled with the same person ID and camera ID as in the original image.

We summarize the experimental Results in Table III. As shown in the table, the DenseNet-161 model initialized by our proposal substantially outperforms all the baseline’s performance. We also notice that the models initialized via the AIFL framework could achieve a better person ReID performance in the MSMT17 dataset. The above experiments clearly show the effectiveness of our proposal in learning an apparel-invariant feature. As we mentioned above, the most direct way to use the generated images is to extend training data. Therefore, we generated 3 sets of synthetic images for each image in the training set of MSMT17. We use these synthetic images and the original images in the MSMT17 training set to form an extended training set denote as MSMT17(extend).
The results has shown in Table III. We can notice that directly use these synthetic images is not helpful in the training process. We can observe a tremendous performance fallout in both the baseline and the model initialized by the

TABLE IV EVALUATE THE IMPACT OF THE REFINED LAYER

Network Structure DenseNet-161 (normal)1 DenseNet-161 (reﬁned)2

mAP 33.10 35.24

CMC@1 65.63 65.96

CMC@5 78.36 79.01

CMC@10 82.58 83.80

CMC@20 86.67 87.76

1 This model is initialized using the image pairs generated by the AS-GAN with normal Conv2d blocks. 2 This model is initialized using the image pairs generated by the AS-GAN with the reﬁned layer.

TABLE V DATA VOLUME INCREASE EXPERIMENT

Network Structure Data Volume mAP CMC@1 CMC@5 CMC@10 CMC@20

DenseNet-161 (AIFL) DenseNet-161 (AIFL) DenseNet-161 (AIFL)

1 sets 3 sets1 5 sets

30.80 32.04 35.24

61.64 64.07 65.96

75.82 76.88 79.01

80.97 81.59 83.80

85.25 85.73 87.76

1. The 3 sets in the data volume means we randomly selected 60% of the 500,000 generated images and paired up with the original images to form 300,000 image pairs. One set corresponds to 100,000 pairs of images.

AIFL. However, the model initialized using the AIFL still outperforms the baseline model.
F. Further Evaluations
Evaluate the impact of the reﬁned layer. As we mentioned above, we used the reﬁned layer to replace the second normal Conv2d block in the AS-GAN. This modify could impact the generated image, and further impact the initialising process of the AIFL framework. To evaluate the inﬂuence of this modify, we conduct several experiments in the MSMT17 dataset. In these experiments, we use the normal Conv2d block as the secend layer in the AS-GAN to trained a image generator as baseline. We then generated 5 sets of image pairs using this image generator. With these image pairs, we initialised a DenseNet-161 model by following the same setting in the MSMT17 experiments. The generate image samples has shown in Fig.4, the left part is the image samples generated using AS-GAN with reﬁned layer and the right part is image samples generated using normal Conv2d block. We can observe that the left part images are more realistic than the right part. As for numeric metric, the results has shown in Table. IV, we can notice that by using the reﬁned layer in the AS-GAN, we could generate more realistic image and further improve the performance of the AIFL framework.
Data volume increase experiment. In our assumption, the AIFL framework could achieve better performance by using more image pairs. To verify this assumption, we conducted the data volume increase experiment using the C-MARS dataset. We conduct two groups of experiments. In each group, we randomly select 1 and 3 sets of image pairs from the C-MARS dataset to initialize the feature extractor model using the AIFL framework. To evaluate the performance, we also used MSMT17 to ﬁne-tune and test the feature extractor model’s performance. The results have shown in Table V. As in the table, using more image pairs will increase the effectiveness of the AIFL framework. As an unsupervised training framework, the obtaining of the training data is relatively easier. Meanwhile, the training data we used is randomly sampled from irrelevant datasets, making the framework more practical

9

TABLE VI DISCRIMINATOR ABLATION EXPERIMENT.

Network Structure DenseNet-161 (AIFL noDT )1
DenseNet-161 (AIFL)

mAP 31.61 35.24

CMC@1 62.50 65.96

CMC@5 76.31 79.01

CMC@10 81.13 83.80

CMC@20 85.44 87.76

1. noDT stands for the model initialized using the AIFL framework but without discriminator DT .

and easy to implement in different datasets. These results also veriﬁed our contribution in this work that our proposal could effectively improve the apparel changed ReID performance by generated realistic images.
Ablation. To verify the necessity of the discriminator DT we used in the AIFL framework, we conduct the ablation experiment in the MSMT17 dataset. We summarise the experimental Results Table. VI. We can observe a considerable decline in both mAP and CMC score when initializing the model without the discriminator DT . We argue that the discriminator could help the model to learn a more accurate feature representation in the AIFL framework.
V. CONCLUSION
In this paper, we study the apparel changing and different persons wearing similar cloth cases in person ReID tasks. To learn an apparel-invariant feature embeeding, we proposed a semi-supervised apparel-invariant feature Learning (AIFL) framework. We also constructed an apparel-simulation GAN (AS-GAN) to generate realistic cloth-changed images or similar person’s images for AIFL framework. We show that an apparel-invariant feature could be learned via our approach. Nevertheless, only limited labelled images are requested in our approach, which could effectively reduce the cost of human labelling. Extensive experiments are conducted in two person ReID datasets shown the progressiveness of our proposal.
Further Works. As in the newly deﬁned apparel-changed person ReID scenario, many possible ways could be explored in the future. In this work, the ﬁne-tuning process is still a supervised method. We are looking forward to exploring a fully unsupervised method in this new scenario.
REFERENCES
[1] X. Wang, “Intelligent multi-camera video surveillance: A review,” Pattern recognition letters, vol. 34, no. 1, pp. 3–19, 2013.
[2] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang, and X. Tang, “Spindle net: Person re-identiﬁcation with human body region guided feature decomposition and fusion,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1077–1085.
[3] C. C. Loy, T. Xiang, and S. Gong, “Multi-camera activity correlation analysis,” in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 1988–1995.
[4] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss for person re-identiﬁcation,” arXiv preprint arXiv:1703.07737, 2017.
[5] B. I. Barbosa, M. Cristani, A. Del Bue, L. Bazzani, and V. Murino, “Re-identiﬁcation with rgb-d sensors,” in First International Workshop on Re-Identiﬁcation, October 2012.
[6] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-identiﬁcation by local maximal occurrence representation and metric learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2197–2206.

[7] D. Chen, Z. Yuan, B. Chen, and N. Zheng, “Similarity learning with spatial constraints for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1268–1277.
[8] R. Zhao, W. Ouyang, and X. Wang, “Person re-identiﬁcation by salience matching,” in Computer Vision (ICCV), 2013 IEEE International Conference on. IEEE, 2013, pp. 2528–2535.
[9] A. Wu, W.-S. Zheng, and J.-H. Lai, “Robust depth-based person reidentiﬁcation,” IEEE Transactions on Image Processing, vol. 26, no. 6, pp. 2588–2603, 2017.
[10] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep ﬁlter pairing neural network for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 152– 159.
[11] E. Ahmed, M. Jones, and T. K. Marks, “An improved deep learning architecture for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3908–3916.
[12] L. Zheng, H. Zhang, S. Sun, M. Chandraker, Y. Yang, and Q. Tian, “Person re-identiﬁcation in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1367–1376.
[13] H.-X. Yu, A. Wu, and W.-S. Zheng, “Cross-view asymmetric metric learning for unsupervised person re-identiﬁcation,” in IEEE International Conference on Computer Vision, 2017.
[14] J. Almazan, B. Gajic, N. Murray, and D. Larlus, “Re-id done right: towards good practices for person re-identiﬁcation,” arXiv preprint arXiv:1801.05339, 2018.
[15] W. Li, X. Zhu, and S. Gong, “Person re-identiﬁcation by deep joint learning of multi-loss classiﬁcation,” in Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence. AAAI Press, 2017, pp. 2194–2200.
[16] L. Zhao, X. Li, J. Wang, and Y. Zhuang, “Deeply-learned partaligned representations for person re-identiﬁcation,” arXiv preprint arXiv:1707.07256, 2017.
[17] L. Zheng, Y. Huang, H. Lu, and Y. Yang, “Pose invariant embedding for deep person re-identiﬁcation,” arXiv preprint arXiv:1701.07732, 2017.
[18] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, “Joint detection and identiﬁcation feature learning for person search,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017, pp. 3376–3385.
[19] H. Fan, L. Zheng, and Y. Yang, “Unsupervised person re-identiﬁcation: Clustering and ﬁne-tuning,” arXiv preprint arXiv:1705.10444, 2017.
[20] J. Lv, W. Chen, Q. Li, and C. Yang, “Unsupervised cross-dataset person re-identiﬁcation by transfer learning of spatial-temporal patterns,” arXiv preprint arXiv:1803.07293, 2018.
[21] J. Wang, X. Zhu, S. Gong, and W. Li, “Transferable Joint AttributeIdentity Deep Learning for Unsupervised Person Re-Identiﬁcation,” ArXiv e-prints, Mar. 2018.
[22] W. Deng, L. Zheng, G. Kang, Y. Yang, Q. Ye, and J. Jiao, “Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identiﬁcation,” CoRR, vol. abs/1711.07027, 2017. [Online]. Available: http://arxiv.org/abs/1711.07027
[23] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Pose-driven deep convolutional model for person re-identiﬁcation,” in Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017, pp. 3980–3989.
[24] X. Qian, Y. Fu, W. Wang, T. Xiang, Y. Wu, Y.-G. Jiang, and X. Xue, “Pose-Normalized Image Generation for Person Re-identiﬁcation,” ArXiv e-prints, Dec. 2017.
[25] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer GAN to bridge domain gap for person re-identiﬁcation,” CoRR, vol. abs/1711.08565, 2017. [Online]. Available: http://arxiv.org/abs/1711.08565
[26] A. Wu, W. S. Zheng, H. X. Yu, S. Gong, and J. Lai, “Rgb-infrared cross-modality person re-identiﬁcation,” in 2017 IEEE International Conference on Computer Vision (ICCV), Oct 2017, pp. 5390–5399.
[27] S. Zhu, S. Fidler, R. Urtasun, D. Lin, and C. C. Loy, “Be your own prada: Fashion synthesis with structural coherence,” CoRR, vol. abs/1710.07346, 2017. [Online]. Available: http://arxiv.org/abs/1710. 07346
[28] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d pose estimation using part afﬁnity ﬁelds,” in CVPR, 2017.
[29] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convolutional pose machines,” in CVPR, 2016.
[30] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz, “Disentangled person image generation,” in IEEE Conference on Computer Vision and Pattern Recognition, 2018.

10
[31] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in neural information processing systems, 2014, pp. 2672– 2680.
[32] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context encoders: Feature learning by inpainting,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2536–2544.
[33] R. Zhao, W. Oyang, and X. Wang, “Person re-identiﬁcation by saliency learning,” IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 2, pp. 356–370, 2017.
[34] M. Ye, A. J. Ma, L. Zheng, J. Li, and P. C. Yuen, “Dynamic label graph matching for unsupervised video re-identiﬁcation,” in International conference on commuter vision, 2017.
[35] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and Y. Tian, “Unsupervised cross-dataset transfer learning for person reidentiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1306–1315.
[36] J. Donahue, P. Kra¨henbu¨hl, and T. Darrell, “Adversarial feature learning,” arXiv preprint arXiv:1605.09782, 2016.
[37] D. Gray and H. Tao, “Viewpoint invariant pedestrian recognition with an ensemble of localized features,” in European conference on computer vision. Springer, 2008, pp. 262–275.
[38] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato, “Hierarchical gaussian descriptor for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1363–1372.
[39] L. Zheng, Y. Yang, and A. G. Hauptmann, “Person re-identiﬁcation: Past, present and future,” CoRR, vol. abs/1610.02984, 2016. [Online]. Available: http://arxiv.org/abs/1610.02984
[40] Y. Bengio et al., “Learning deep architectures for ai,” Foundations and trends R in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.
[41] H. Bourlard and Y. Kamp, “Auto-association by multilayer perceptrons and singular value decomposition,” Biological cybernetics, vol. 59, no. 4-5, pp. 291–294, 1988.
[42] P. Isola, J. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” CoRR, vol. abs/1611.07004, 2016. [Online]. Available: http://arxiv.org/abs/1611.07004
[43] X. Liang, S. Liu, X. Shen, J. Yang, L. Liu, J. Dong, L. Lin, and S. Yan, “Deep human parsing with active template regression,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 37, no. 12, pp. 2402–2414, Dec 2015.
[44] MARS: A Video Benchmark for Large-Scale Person Re-identiﬁcation. Springer, 2016.
[45] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[46] G. Huang, Z. Liu, and K. Q. Weinberger, “Densely connected convolutional networks,” CoRR, vol. abs/1608.06993, 2016. [Online]. Available: http://arxiv.org/abs/1608.06993
[47] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A Large-Scale Hierarchical Image Database,” in CVPR09, 2009.
[48] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang, and J. Sun, “Alignedreid: Surpassing human-level performance in person re-identiﬁcation,” arXiv preprint arXiv:1711.08184, 2017.
[49] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” CoRR, vol. abs/1409.4842, 2014. [Online]. Available: http://arxiv.org/abs/1409.4842
[50] L. Wei, S. Zhang, H. Yao, W. Gao, and Q. Tian, “GLAD: global-local-alignment descriptor for pedestrian retrieval,” CoRR, vol. abs/1709.04329, 2017. [Online]. Available: http://arxiv.org/abs/1709. 04329

