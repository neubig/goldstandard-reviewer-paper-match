End-to-End Speaker Diarization for an Unknown Number of Speakers with Encoder-Decoder Based Attractors
Shota Horiguchi1, Yusuke Fujita1, Shinji Watanabe2, Yawen Xue1, Kenji Nagamatsu1
1Hitachi, Ltd. 2Johns Hopkins University
{shota.horiguchi.wk, yusuke.fujita.su, yawen.xue.wn}@hitachi.com, shinjiw@ieee.org

arXiv:2005.09921v3 [eess.AS] 5 Oct 2020

Abstract
End-to-end speaker diarization for an unknown number of speakers is addressed in this paper. Recently proposed end-toend speaker diarization outperformed conventional clusteringbased speaker diarization, but it has one drawback: it is less ﬂexible in terms of the number of speakers. This paper proposes a method for encoder-decoder based attractor calculation (EDA), which ﬁrst generates a ﬂexible number of attractors from a speech embedding sequence. Then, the generated multiple attractors are multiplied by the speech embedding sequence to produce the same number of speaker activities. The speech embedding sequence is extracted using the conventional self-attentive end-to-end neural speaker diarization (SA-EEND) network. In a two-speaker condition, our method achieved a 2.69 % diarization error rate (DER) on simulated mixtures and a 8.07 % DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained 4.56 % and 9.54 %, respectively. In unknown numbers of speakers conditions, our method attained a 15.29 % DER on CALLHOME, while the x-vectorbased clustering method achieved a 19.43 % DER. Index Terms: speaker diarization, encoder-decoder, attractor calculation
1. Introduction
Speaker diarization is the task to estimate “who spoke when” from an audio recording. It is a key technology for various applications using automatic speech recognition (ASR) in multitalker scenarios such as telephone conversations [1], meetings [2], conferences and lectures [3], TV shows [4], and movies [5]. Accurate diarization has been proven to improve ASR performance by constraining a speech mask when constructing a beamformer for speech separation [6, 7].
One major approach for speaker diarization is the clustering-based method [8, 9], which applies the following processes to an input audio one by one: speech activity detection, speech segmentation, feature extraction, and clustering. Progress on better speaker embeddings, such as x-vectors [10, 11] and d-vectors [12, 13], have enabled accurate clusteringbased diarization. However, most clustering-based approaches (except for a few studies, e.g., [14]) cannot deal with speaker overlap because each time slot is assigned to one speaker.
End-to-end speaker diarization called EEND [15, 16] has been proposed to overcome this situation. The EEND is optimized to calculate diarization results for every speaker in a mixture from input audio features using permutation invariant training (PIT) [17]. The EEND, especially self-attentive EEND (SAEEND), showed the effectiveness of end-to-end training of the diarization model by outperforming conventional clusteringbased methods. One drawback it has is that the maximum number of speakers is pre-determined by the network architecture, and it cannot deal with a case where the number of speakers is

higher. On this point, EEND is less ﬂexible than clusteringbased methods, where the number of speakers can be easily changed by setting the number of clusters during inferences.
This paper proposes an encoder-decoder based attractor calculation method called EDA. It determines a ﬂexible number of—and theoretically an inﬁnite number of attractors—from a speech embedding sequence. We applied it to SA-EEND to enable diarization with a ﬂexible number of speakers. Then, the diarization results are calculated using dot products between all pairs of attractors and embeddings. Evaluation results on both simulated mixtures and real recordings showed that our method achieved better results with both ﬁxed and unknown numbers of speakers than the x-vector-based clustering method and conventional SA-EEND.
2. Related work
Several methods in the context of speech separation can process speech mixtures of a ﬂexible number of speakers. One series of methods involve applying the one-vs-rest approach iteratively [18, 19, 20, 21]. However, it has a major drawback in that the calculation is conducted until all the speakers are extracted, so the computational time increases linearly as the number of speakers increases. Another series involve attractorbased approaches including Deep Attractor Network (DANet) [22]. It does not limit the number of speakers in the inference phase; however, the number of speakers has to be known a priori. Anchored DANet [23] successfully solved the aforementioned problems, but it always requires calculating dot products between all the possible selections of anchors and extracted embeddings even in the inference phase. Thus, it is not scalable in terms of the number of speakers.
Several efforts have been made to calculate representatives from an embedding sequence in an end-to-end manner. Lee et al. proposed Set Transformer to implement set-to-set transformation [24], but the number of outputs has to be deﬁned beforehand. Meier et al. implemented end-to-end clustering by estimating the distribution for every possible number of clusters K ∈ {1, . . . , Kmax} [25] so that the maximum number is limited by the network architecture. Li et al. proposed encoderdecoder based clustering for speaker diarization [26], which is the most related to EDA. However, the output is a sequence of cluster numbers of each input, so each time slot is assigned to one cluster; therefore, it cannot deal with speaker overlap. Our proposed EDA, in contrast, determines a ﬂexible number of attractors from an embedding sequence without prior knowledge about the number of clusters.
3. End-to-end neural diarization: Review
Here we brieﬂy introduce our end-to-end diarization framework named EEND [15, 16]. The EEND takes a T -length se-

quence of log-scaled Mel-ﬁlterbank based features as an input,
and processes it using bi-directional long short-term memory
(BLSTM) [15] or Transformer encoders [16] to obtain an embedding et ∈ RD at each time slot. After that, a linear transformation f : RD → RS with an element-wise sigmoid function is applied to calculate posteriors yˆt = [yˆt,1, . . . , yˆt,S]T ∈ (0, 1)S
of S speakers at time slot t. In the training phase, the EEND is
optimized using the PIT scheme [17], i.e., the loss is calculated between yˆt and the groundtruth labels yt = [yt,1, . . . , yt,S]T ∈ {0, 1}S as follows:

1

T φ

Ld = T S φ∈paerrgmm(1,i.n..,S) t=1 H yt , yˆt , (1)

where perm(1, . . . , S) is the set of all the possible permutation of speakers, ytφ ∈ {0, 1}S is the permuted labels at t, and H(yt, yˆt) is the binary cross entropy determined as follows:

H (yt, yˆt) := −yt,s log yˆt,s − (1 − yt,s) log (1 − yˆt,s).
s
(2)

4. Proposed method
The EEND has a critical problem, in that the output size is limited by the network architecture; the linear transformation f restricts the number of speakers S during inference. Therefore, it cannot deal with a case where the input mixture contains a higher number of speakers than the capacity. Therefore, we utilized an attractor-based method. To make our method endto-end trainable, we designed Encoder-Decoder based Attractor calculation (EDA) to determine attractors from an embedding sequence. The overview of our proposed method is shown in Figure 1. We used the same self-attentive network in [16] as a backbone to obtain an embedding et at each time slot. In this section, we explain how we calculate a ﬂexible number of attractors from the embeddings and obtain diarization results using the attractors.

4.1. Encoder-decoder based attractor calculation

To calculate a ﬂexible number of attractor points from vari-
able lengths of embedding sequences, we utilize LSTM-based
encoder-decoder [27]. A sequence of D-dimensional embeddings (et)Tt=1 is fed into the unidirectional LSTM encoder, obtaining the ﬁnal hidden state embedding h0 ∈ RD and the cell state c0 ∈ RD:

h0, c0 = Encoder (e1, · · · , eT ) .

(3)

Next, time-invariant D-dimensional attractors (as)s are calculated using an unidirectional LSTM decoder with the initial states h0 and c0 as follows.

hs, cs, as = Decoder (hs−1, cs−1, 0)

(4)

We use a D-dimensional zero vector 0 as the input for the decoder at each decoding step. Theoretically inﬁnite numbers of attractors can be calculated using the LSTM decoder. The probability of whether or not the attractor as exists to determine when to stop the attractor calculation is computed using a fullyconnected layer with a sigmoid function as

1

ps

=

1+

exp (− (wTas

, + b))

(5)

where w and b are the trainable weights and bias of the fullyconnected layer, respectively.
We note that the output attractors (as)s depend on the order

Labels
Diarization results

...

1 1 ... 1 0 Labels

...

...

Attractor existence

probabilies

Sigmoid

××

×

LSTM encoder

...

Linear + Sigmoid
... ...

Attractors LSTM decoder

Embeddings

...

...

Zero vectors

Audio feaures

SA-EEND
...

Encoder-Decoder Based Attractor Calculation

Figure 1: SA-EEND with encoder-decoder based attractor calculation.

of the input embeddings (et)Tt=1 because we use LSTMs for the EDA. To investigate the effect of the input order, we used two
types of embedding order. One was a chronological order, i.e.,
the embeddings were sorted by time slot indexes. The other
was a shufﬂed order. In this case, we used a shufﬂed order of embeddings, namely (eψ(t))Tt=1, where (ψ(1), . . . , ψ(T )) is one of the permutations of (1, . . . , T ), for the input to the EDA.
In the training phase, we deﬁned the groundtruth labels l = [l1, . . . , lS+1]T using the actual number of speakers S as
follows:

1 (s ∈ {1, . . . , S})

ls =

(6)

0 (s = S + 1) .

Also the attractor existence loss La between the labels and the estimated probabilities p = [p1, . . . , pS+1]T were calculated
using the binary cross entropy in Equation 2 as

La = 1 H (l, p) . (7) 1+S
In the inference phase, if the number of speakers S was given, we use the ﬁrst S attractors, which were the output from the EDA. If the number of speakers was unknown, we ﬁrst estimated it using

Sˆ = max {s | s ∈ Z+ ∧ ps ≥ τ }

(8)

with a given threshold τ and then used the ﬁrst Sˆ attractors.

4.2. Speaker diarization using EDA

We respectively deﬁne the matrix formulations of the embeddings extracted from the SA-EEND and the attractors from the EDA as follows.

E

:=

[e1,

.

.

.

,

eT

]

∈

D×T
R

(9)

A

:=

[a1,

.

.

.

,

aS ]

∈

D×S
R

(10)

The posterior probabilities can be calculated using the inner product of every embedding-attractor pair as follows:

Yˆ = σ(ATE) ∈ (0, 1)S×T ,

(11)

where σ(·) is the element-wise sigmoid function. Note that the output size was determined using the number of attractors so that our method could output the diarization results of a ﬂexible number of speakers. Finally, diarization loss was calculated in the same way as SA-EEND using the PIT found in Equation 1.
The total loss is deﬁned by the diarization loss in Equation 1 and the attractor existence loss in Equation 7 as follows:

L = Ld + αLa,

(12)

where α is the weighting parameter. In this study, α was set to

Table 1: Dataset to train and test our diarization models. (a) Simulated datasets.

Dataset
Train Sim1spk Sim2spk Sim3spk Sim4spk
Test Sim1spk Sim2spk Sim3spk Sim4spk

#Spk
1 2 3 4
1 2 3 4

#Mixtures
100,000 100,000 100,000 100,000
500 500/500/500 500/500/500
500

Overlap ratio ρ (%)
0.0 34.1 34.2 31.5
0.0 34.4/27.3/19.6 34.7/27.4/19.1
32.0

(b) Real datasets.

Dataset
Train CALLHOME [30] CALLHOME [30] CALLHOME [30] DIHARD dev [32]
Test CALLHOME [30] CALLHOME [30] CALLHOME [30] CSJ [31] DIHARD eval [32]

#Spk
2 3 2-7 1-10
2 3 2-6 2 1-9

#Mixtures
155 61 249 192
148 74 250 54 194

Overlap ratio ρ (%)
14.0 19.6 17.0 9.8
13.1 17.0 16.7 20.1 8.9

1.0 when the simulated data were used for training and 0.01 for adaptation on real datasets.
5. Experiments
5.1. Data
For the training and evaluation, we used simulated mixtures created from Switchboard-2 (Phase I & II & III), Switchboard Cellular (Part 1 & 2), and the NIST Speaker Recognition Evaluation (2004 & 2005 & 2006 & 2008) for speech and the MUSAN corpus [28] for noise with simulated room impulse responses used in [29] following the procedure in [16]. We note that the speaker sets for the training and test datasets were not overlapped. In [16], only the 2-speaker dataset was constructed. In this study, we created 1-, 3-, and 4-speaker datasets with similar overlap ratios ρ to the 2-speaker mixtures. We also used the telephone conversation dataset CALLHOME (CH) [30], the dialogue recordings from the Corpus of Spontaneous Japanese (CSJ) [31], and the dataset used for the second DIHARD challenge [32] to evaluate the performance on real recordings. The statistics of the datasets used are summarized in Table 1.
5.2. Experimental settings
We basically followed the training protocol of the best model described in [33]1. We used SA-EEND with four-stacked Transformer encoders as a baseline and a backbone of our method. The inputs for the SA-EEND were 345-dimensional log-scaled Mel-ﬁlterbank based features, which were also the same as those used in the original paper. For our method, we extracted a sequence of 256-dimensional embeddings after the last layer normalization [34] of the SA-EEND, and fed them into the EDA
1SA-EEND is available at https://github.com/ hitachi-speech/EEND. We will release the source code of SA-EEND with EDA at the same repository.

Table 2: DERs (%) on 2-speaker datasets.

Method
i-vector clustering x-vector clustering BLSTM-EEND [15] SA-EEND [16] SA-EEND + EDA (Chronol.) SA-EEND + EDA (Shufﬂed)

Sim2spk

ρ = 34.4 % 27.3 %

33.74 28.77 12.28 4.56 3.07 2.69

30.93 24.46 14.36 4.50 2.74 2.44

19.6 %
25.96 19.78 19.69 3.85 3.04 2.60

Real

CH CSJ

12.10 11.53 26.03 9.54 8.24 8.07

27.99 22.96 39.33 20.48 18.89 16.27

Table 3: DERs (%) on 3-speaker datasets.

Method
x-vector clustering SA-EEND SA-EEND + EDA (Chronol.) SA-EEND + EDA (Shufﬂed)

Sim3spk

ρ = 34.7 % 27.4 %

31.78 8.69 13.02 8.38

26.06 7.64 11.65 7.06

19.1 %
19.55 6.92 10.41 6.21

Real
CH
19.01 14.00 15.86 13.92

to calculate attractors. The threshold τ in Equation 8 to determine whether or not the attractor existed was set to 0.5. As we explained in subsection 4.1, we used two types of input order for the EDA: chronological order and shufﬂed order. Unless otherwise noted, we used the same type of order in the training and inference phases.
In this paper, we evaluated our method under the following two conditions: a ﬁxed number of speakers and a ﬂexible number of speakers. For the ﬁxed number of speakers, we ﬁrst trained our model using Sim2spk with ρ = 34.1 % or Sim3spk with ρ = 34.2 % for 100 epochs. We used the Adam optimizer [35] with the learning rate schedule proposed in [36] with warm-up steps of 100,000. We also ﬁnetuned those models using subsets of corresponding numbers of speakers from CALLHOME data to evaluate the performance on the real recordings. For comparison, the performance on i-vectors or x-vectors using agglomerative hierarchical clustering with probabilistic linear discriminate analysis (PLDA) scoring according to Kaldi’s pretrained model [37] was also evaluated. In these cases, TDNNbased speech activity detection [38] and the oracle number of speakers were used for the evaluation. For experiments on the ﬂexible speaker condition, we ﬁnetuned the 2-speaker model trained on Sim2spk on the concatenation of Sim1spk, Sim2spk, Sim3spk, and Sim4spk for 25 epochs. We ﬁnetuned the model using CALLHOME or DIHARD dev to evaluate the performance on real datasets. The x-vector-based methods based on the oracle number of speakers and the clustering threshold determined using the training set were also evaluated.
For the evaluation metric, we used the diarization error rate (DER). The 0.25 s of collar tolerance was deﬁned at the start and end of each segment for the evaluation on the simulated datasets and the CALLHOME dataset. For the DIHARD dataset, we also used the Jaccard error rate (JER), and we did not use collar tolerance, following the regulation of the second DIHARD challenge [32].
5.3. Results on a ﬁxed number of speakers
First, we evaluated our method on the 2-speaker condition like the one in [15, 16]. The results are shown in Table 2. The best DERs were attained using EDA trained on shufﬂed embeddings. When the model was trained using embeddings in chronological order, the DERs slightly degraded. We also show the results on the 3-speaker condition in Table 3. Our method with shufﬂed embeddings achieved better DERs compared with the conventional x-vector clustering and vanilla SA-EEND.

Table 4: DERs on Sim2spk (ρ = 34.4 %) using various types of sequences.

Method
SA-EEND + EDA (Chronol.) SA-EEND + EDA (Shufﬂed)

Use whole sequence

Chronol. Shufﬂed

3.07

30.04

2.69

2.69

N =2
3.54 2.70

Subsample 1/N

N = 4 N = 8 N = 16

7.32 14.48 21.13

2.68 2.79

3.09

N = 32
27.18 5.08

N =2
3.67 3.36

Use the last 1/N

N = 4 N = 8 N = 16

4.97 5.40

6.11

5.92 7.46

8.59

N = 32
7.68 10.65

6LOHQFH 6SN 6SN 2YHUODS $WWUDFWRU

6LOHQFH 6SN 6SN 2YHUODS $WWUDFWRU

Figure 2: Visualization of embeddings and attractors on 2speaker mixtures in Sim2spk (ρ = 34.4 %).

Table 5: DERs (%) on simulated mixtures of a ﬂexible number of speakers.

Method
x-vector clustering Threshold Oracle #Spk
SA-EEND + EDA Estimated #Spk Oracle #Spk

Sim1spk ρ = 0.0 %
37.42 1.67
0.39 0.16

Sim2spk 34.4 %
7.74 28.77
4.33 4.26

Sim3spk 34.7 %
11.46 31.78
8.94 8.63

Sim4spk 32.0 %
22.45 35.76
13.76 13.31

Effect of the input order: To better understand the EDA, we evaluated the diarization performance on both chronologicallyordered sequences and shufﬂed sequences. We also tried to reduce the length of sequences by subsampling embeddings or using the last 1/N of the sequences. The results on Sim2spk (ρ = 34.4 %) are shown in Table 3. When the EDA was trained on chronologically-ordered embeddings, it worked better on chronologically-ordered embeddings but degraded shufﬂed embeddings. If the embeddings were subsampled, the performance degradation was also severe even if the samples were ordered chronologically, while using the last 1/N could suppress the performance degradation. These results were that the model captured speech length tendency to output attractors. However, when the EDA was trained on shufﬂed embeddings, the model was not affected very much by the order and subsampling. These results show that the EDA could capture the overall sequence successfully.
Visualization: In Figure 2, we visualized embeddings and attractors of 2-speaker mixtures by applying PCA to reduce their dimensionality. The embeddings of two speakers were well separated from the silent region, and those of overlapping regions were distributed between two clusters. Attractors were successfully calculated for each of the two speakers.
5.4. Results on a ﬂexible number of speakers
We also evaluated our method on a condition involving a ﬂexible number of speakers. In this case, the order of the embeddings was always shufﬂed. The model was ﬁrst ﬁnetuned from the weights trained on Sim2spk and evaluated on simulated mixtures of a ﬂexible number of speakers. The results are shown in Table 5. Our method achieved better DERs than the xvector clustering-based method. It achieved 4.33 % and 8.94 % DERs on two- and three-speaker mixtures, which showed only 1.64 and 0.56 point degradation from two- or three-speaker speciﬁc models, respectively. Furthermore, our method further improved performance when the actual number of speakers was given, while x-vector clustering worsened performance in most cases using the oracle number of speakers.
We also evaluated our method with real conversations using the CALLHOME. In this case, the model was ﬁnetuned again using the CALLHOME training set and evaluated on the test set. The results are shown in Table 6. Our method achieved a 15.29 % DER, which outperformed the clusteringbased method. However, it did not perform well when the num-

Table 6: DERs (%) on CALLHOME of a ﬂexible number of speakers.

Method
x-vector clustering Threshold Oracle #Spk
SA-EEND + EDA Estimated #Spk Oracle #Spk

2
15.45 8.93
8.50 8.35

3
18.01 19.01
13.24 13.20

#Spk

4

5

22.68 31.40 24.48 32.14
21.46 33.16 21.71 33.00

6
34.27 34.95
40.29 41.07

All
19.43 18.98
15.29 15.43

ber of speakers was higher than four. This is because the CALLHOME contains only ten recordings that include more than four speakers.
Finally, we evaluated our method on the DIHARD dataset. The evaluation follows the DIHARD 2019 track 2, where speech activity detection has to be conducted from single channel audio. Because utilizing a high number of speakers with PIT is difﬁcult, our system was only trained to output the most dominant seven speakers even if the input contained more than seven speakers. The results are shown in Table 7. Our SA-EEND with EDA achieved a DER of 32.59 %, which outperformed the baseline [39] and the best pre-is2019-deadline system by the DI-IT team [40], but it could not beat the best post-is2019-deadline system by the BUT team [41]. We note that our system is based on 8 kHz audio, while others use 16 kHz audio with additional training data from VoxCeleb datasets [42]. Evaluations on highresolution audio with additional data are left for future work.

6. Conclusions
In this paper, we proposed EDA to calculate attractors from a sequence of embeddings, and we applied it to SA-EEND to implement end-to-end speaker diarization for speech mixtures of a ﬂexible number of speakers. Our method achieved state-ofthe-art DERs on conditions including both a ﬁxed and a ﬂexible number of speakers.

Table 7: DERs and JERs (%) on DIHARD eval.

Method
DIHARD-2 baseline [39] Best pre-is2019-deadline [40] Best post-is2019-deadline [41] SA-EEND + EDA (Estimated #Speakers)

DER
40.86 35.10 27.11 32.59

JER
66.60 57.11 49.07 55.99

7. References
[1] P. Kenny, D. Reynolds, and F. Castaldo, “Diarization of telephone conversations using factor analysis,” IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 6, pp. 1059–1070, 2010.
[2] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE TASLP, vol. 15, no. 7, pp. 2011–2022, 2007.
[3] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker diarization for conference and lecture meetings,” in Multimodal technologies for perception of humans. Springer, 2007, pp. 533–542.
[4] F. Vallet, S. Essid, and J. Carrive, “A multimodal approach to speaker diarization on TV talk-shows,” IEEE TMM, vol. 15, no. 3, pp. 509–520, 2012.
[5] I. Kapsouras, A. Tefas, N. Nikolaidis, G. Peeters, L. Benaroya, and I. Pitas, “Multimodal speaker clustering in full length movies,” Multimedia Tools and Applications, vol. 76, no. 2, pp. 2223–2242, 2017.
[6] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi, K. Nagamatsu, and R. Haeb-Umbach, “Guided source separation meets a strong ASR backend: Hitachi/Paderborn University joint investigation for dinner party scenario,” in INTERSPEECH, 2019, pp. 1248–1252.
[7] C. Zorila˘, C. Boeddeker, R. Doddipatla, and R. Haeb-Umbach, “An investigation into the effectiveness of enhancement in ASR training and test for CHiME-5 dinner party transcription,” in ASRU, 2019, pp. 47–53.
[8] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,” IEEE TASLP, vol. 21, no. 10, pp. 2015–2028, 2013.
[9] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in SLT, 2014, pp. 413–417.
[10] D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, “Speaker recognition for multi-speaker conversations using x-vectors,” in ICASSP, 2019, pp. 5796–5800.
[11] M. Diez, L. Burget, S. Wang, J. Rohdin, and J. Cˇ ernocky`, “Bayesian HMM based x-vector clustering for speaker diarization,” in INTERSPEECH, 2019, pp. 346–350.
[12] Q. Wang, C. Downey, L. Wan, P. Andrew Mansﬁeld, and I. Lopez Moreno, “Speaker diarization with LSTM,” in ICASSP, 2018, pp. 5239–5243.
[13] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully supervised speaker diarization,” in ICASSP, 2019, pp. 6301–6305.
[14] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, and S. Khudanpur, “Speaker diarization with region proposal network,” in ICASSP, 2020, pp. 6514–6518.
[15] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in INTERSPEECH, 2019, pp. 4300–4304.
[16] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with selfattention,” in ASRU, 2019, pp. 296–303.
[17] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in ICASSP, 2017, pp. 241–245.
[18] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Listening to each speaker one by one with recurrent selective hearing networks,” in ICASSP, 2018, pp. 5064–5068.
[19] J. Shi, J. Xu, G. Liu, and B. Xu, “Listen, think, and listen again: Captureing top-down auditory attention for speaker-independent speech separation,” in IJCAI, 2018, pp. 4353–4360.
[20] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, and R. Haeb-Umback, “All-neural online source separation, counting, and diarization for meeting analysis,” in ICASSP, 2019, pp. 91–95.

[21] N. Takahashi, S. Parthasaarathy, N. Goswami, and Y. Mitsufuji, “Recursive speech separation for unknown number of speakers,” in INTERSPEECH, 2019, pp. 1348–1352.
[22] Z. Chen, Y. Luo, and N. Mesgarani, “Deep attractor network for single-microphone speaker separation,” in ICASSP, 2017, pp. 246–250.
[23] Y. Luo, Z. Chen, and N. Mesgarani, “Speaker-independent speech separation with deep attractor network,” IEEE/ACM TASLP, vol. 26, no. 4, pp. 787–796, 2018.
[24] J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and Y. W. Teh, “Set Transformer: A framework for attention-based permutationinvariant neural networks,” in ICML, 2019, pp. 3744–3753.
[25] B. B. Meier, I. Elezi, M. Amirian, O. Du¨rr, and T. Stadelmann, “Learning neural models for end-to-end clustering,” in ANNPR, 2018, pp. 126–138.
[26] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, “Discriminative neural clustering for speaker diarisation,” arXiv:1910.09703, 2019.
[27] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in NeurIPS, 2014, pp. 3104–3112.
[28] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech, and noise corpus,” arXiv:1510.08484, 2015.
[29] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in ICASSP, 2017, pp. 5220–5224.
[30] “2000 NIST Speaker Recognition Evaluation,” https://catalog.ldc. upenn.edu/LDC2001S97.
[31] K. Maekawa, “Corpus of spontaneous Japanese: Its design and evaluation,” in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[32] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, and M. Liberman, “The Second DIHARD Diarization Challenge: Dataset, task, and baselines,” in INTERSPEECH, 2019, pp. 978– 982.
[33] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, and K. Nagamatsu, “End-to-end neural diarization: Reformulating speaker diarization as simple multi-label classiﬁcation,” arXiv:2003.02966, 2020.
[34] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” in NIPS 2016 Deep Learning Symposium, 2016.
[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in ICLR, 2015.
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, 2017, pp. 5998–6008.
[37] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in ICASSP, 2018, pp. 5329–5333.
[38] V. Peddinti, G. Chen, V. Manohar, T. Ko, D. Povey, and S. Khudanpur, “JHU ASpIRE system: Robust LVCSR with TDNNs, iVector adaptation and RNN-LMS,” in ASRU, 2015, pp. 539–546.
[39] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, and S. Khudanpur, “Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge,” in INTERSPEECH, 2018, pp. 2808–2812.
[40] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa, A. Avdeeva, A. Gorlanov, and A. Kozlov, “Speaker diarization with deep speaker embeddings for DIHARD Challenge II,” in INTERSPEECH, 2019, pp. 1003–1007.
[41] F. Landini, S. Wang, M. Diez, L. Burget, P. Mateˇjka, K. Zˇ mol´ıkova´, L. Mosˇner, A. Silnova, O. Plchot, O. Novotny`, H. Zeinali, and S. Rohdin, “BUT system for the Second DIHARD Speech Diarization Challenge,” in ICASSP, 2020, pp. 6529–6533.
[42] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “VoxCeleb: Large-scale speaker veriﬁcation in the wild,” Computer Speech & Language, vol. 60, p. 101027, 2020.

