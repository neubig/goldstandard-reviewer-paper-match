Adapting Text Embeddings for Causal Inference

arXiv:1905.12741v2 [cs.LG] 25 Jul 2020

Victor Veitch∗

Dhanya Sridhar∗

David M. Blei

Department of Statistics and Department of Computer Science Columbia University

Abstract
Does adding a theorem to a paper affect its chance of acceptance? Does labeling a post with the author’s gender affect the post popularity? This paper develops a method to estimate such causal effects from observational text data, adjusting for confounding features of the text such as the subject or writing quality. We assume that the text sufﬁces for causal adjustment but that, in practice, it is prohibitively high-dimensional. To address this challenge, we develop causally sufﬁcient embeddings, lowdimensional document representations that preserve sufﬁcient information for causal identiﬁcation and allow for efﬁcient estimation of causal effects. Causally sufﬁcient embeddings combine two ideas. The ﬁrst is supervised dimensionality reduction: causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. The second is efﬁcient language modeling: representations of text are designed to dispose of linguistically irrelevant information, and this information is also causally irrelevant. Our method adapts language models (speciﬁcally, word embeddings and topic models) to learn document embeddings that are able to predict both treatment and outcome. We study causally sufﬁcient embeddings with semi-synthetic datasets and ﬁnd that they improve causal estimation over related embedding methods. We illustrate the methods by answering the two motivating questions—the effect of a theorem on paper acceptance and the effect of a gender label on post popularity. Code and data available at github.com/vveitch/causaltext-embeddings-tf2.
∗ Equal contribution.
Proceedings of the 36th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), PMLR volume 124, 2020.

1 INTRODUCTION
This paper is about causal inference on text.
Example 1.1. Consider a corpus of scientiﬁc papers submitted to a conference. Some have theorems; others do not. We want to infer the causal effect of including a theorem on paper acceptance. The effect is confounded by the subject of the paper—more technical topics demand theorems, but may have different rates of acceptance. The data does not explicitly list the subject, but it does include each paper’s abstract. We want to use the text to adjust for the subject and estimate the causal effect.
Example 1.2. Consider comments from Reddit.com, an online forum. Each post has a popularity score and the author of the post may (optionally) report their gender. We want to know the direct effect of a ‘male’ label on the score of the post. However, the author’s gender may affect the text of the post, e.g., through tone, style, or topic choices, which also affects its score. Again, we want to use the text to accurately estimate the causal effect.
In these two examples, the text encodes features such as the subject of a scientiﬁc paper or the writing quality of a Reddit comment. These features bias the estimation of causal effects from observed text documents. By assumption, the text carries sufﬁcient information to identify the causal effect; we can use adjustment methods from causal inference to estimate the effects. But in practice we have ﬁnite data and the text is high dimensional, prohibiting efﬁcient causal inference. The challenge is to reduce the text to a low-dimensional representation that sufﬁces for causal identiﬁcation and enables efﬁcient estimation from ﬁnite data. We refer to these text representations as causally sufﬁcient embeddings.
The method for learning causally sufﬁcient embeddings is based on two ideas. The ﬁrst comes from examining causal adjustment. Causal adjustment only requires the parts of text that are predictive of the treatment and outcome. Causally sufﬁcient embeddings preserve such

predictive information while discarding the parts of text that are irrelevant for causal adjustment. We use supervised dimensionality reduction to learn embeddings that predict the treatment and outcome.
The second idea for learning the embeddings comes from research on language modeling [e.g., Mik+13b; Mik+13a; Dev+18; Pet+18]. Outcomes such as paper acceptance or comment popularity are judgments made by humans, and human judgments stem from processing natural language. Thus, when performing causal adjustment for text, the confounding aspects must be linguistically meaningful. Consequently, causally sufﬁcient embeddings model the language structure in text.
We combine these two ideas to adapt modern methods for language modeling — BERT [Dev+18] and topic models [Ble+03] — in service of causal inference. Informally, we learn embeddings of text documents that retain only information that is predictive of the treatment and outcome, and relevant for language understanding.
We empirically study these methods. Any empirical evaluation must use semi-synthetic data since ground truth causal effects are not available in real data. A key challenge is that text is hard to simulate, and realistic models that explicitly relate text to confounding aspects are not available. We show how to circumvent these issues by using real text documents and extra observed features of the documents as confounders. The empirical evaluation demonstrates the advantages of supervised dimensionality reduction and language modeling for producing causally sufﬁcient embeddings. Code and data to reproduce the studies will be publicly available.
Contributions. The contributions of this paper are the following: 1) Adapting modern text representation learning methods to estimate causal effects from text documents; 2) Establishing the validity of this estimation procedure; 3) A new approach for empirically validating causal estimation problems with text, based on semisynthetic data.
Related work. This paper connects to several areas of related work.
The ﬁrst area is causal inference for text. Roberts et al. [Rob+18] also discuss how to estimate effects of treatments applied to text documents. They rely (in part) on topic modeling to reduce the dimension of the text. In this paper, we adapt topic modeling to produce representations that predict the treatment and outcome well, and demonstrate empirically that this strategy improves upon unsupervised topic modeling for causal effect estimation.
In other work, Egami et al. [Ega+18] reduce raw text to interpretable outcomes; Wood-Doughty et al. [WD+18]

Wi

Ti

Yi

Wi

Ti

Yi

Confounding

Mediating

Figure 1: Models for the ATT (left) and NDE (right).

estimate treatment effects when confounders are observed, but missing or noisy treatments are inferred from text. In contrast, we are concerned with text as the confounder.
A second area of related work addresses causal inference with unobserved confounding when there is an observed proxy for the confounder [KM99; Pea12; KP14; Mia+18; Kal+18]. They usually assume that the observed proxy variables are noisy realizations of the unobserved confounder, and then derive conditions under which causal identiﬁcation is possible. One view of our problem is that each unit has a latent attribute (e.g., topic) such that observing it would sufﬁce for causal identiﬁcation, and the text is a proxy for this attribute. Unlike the proxy variable approach, however, we assume the text fully captures confounding. Our interest is in methods for ﬁnite-sample estimation rather than inﬁnite-data identiﬁcation.
Finally, there is work on adapting representation learning for effect estimation by directly optimizing causal criteria such as balance and overlap [Joh+16; Joh+18; Joh+19; Joh+20; DF]. As in this paper, Shi et al. [Shi+19] and Veitch et al. [Vei+19] learn representations by predicting the treatment and outcome. We extend these ideas to learn text representations.
2 BACKGROUND
We begin by ﬁxing notation and recalling some ideas from the estimation of causal effects. Each statistical unit is a document represented as a tuple Oi = (Yi, Ti, Wi), where Yi is the outcome, Ti is the treatment, and Wi is the sequence of words. The observed dataset contains n observations drawn independently and identically at random from a distribution, Oi ∼ P .
We review estimation of the average treatment effect on the treated (ATT) and the natural direct effect (NDE). For both, we assume that the words are sufﬁcient for adjustment.
ATT. The ATT is
ψ = E[Y | do(T = 1), T = 1]−E[Y | do(T = 0), T = 1].
Pearl’s do notation indicates that the effect of interest is causal: what happens if we intervene by adding a theorem

to a paper, given that we observe that it has a theorem? We assume that the words Wi carry sufﬁcient information to adjust for confounding (common causes) between Ti and Yi. Figure 1 on the left depicts this assumption. We deﬁne Zi = f (Wi) to be the part of Wi which blocks all ‘backdoor paths’ between Yi and Ti. The causal effect is then identiﬁable from observational data as:

ψ = E[E[Y | Z, T = 1] − E[Y | Z, T = 0] | T = 1]. (2.1)
Our task is to estimate the ATT ψ from a ﬁnite data sample. Deﬁne Q(t, z) = E[Y | t, z] to be the conditional expected outcome and Qˆ to be an estimate for Q. Deﬁne g(z) = P(T = 1 | z) to be the propensity score and gˆ to be an estimate for g. The “plugin” estimator of 2.1 is

ψˆplugin = 1 ni

Qˆ(1, zi) − Qˆ(0, zi) gˆ(zi)/ 1 n

i ti .

(2.2)

Here ψ is estimated by a two-stage procedure: First estimate Qˆ and gˆ with predictive models; then plug Qˆ and gˆ

into a pre-determined statistic to compute the estimate of

the ATT. What is important is that the estimator depends on zi only through Qˆ(t, zi) and gˆ(zi).

The Q-only estimator, which only uses the conditional expected outcomes, is

ψˆQ =

1

ti Qˆ(1, zi) − Qˆ(0, zi) .

i ti i

(2.3)

NDE. The direct effect is the expected change in outcome if we apply the treatment while holding ﬁxed any mediating variables that are affected by the treatment and that affect the outcome. Figure 1 on the right depicts the text as mediator of the treatment and outcome. For the estimation of the direct effect, we take Z = f (W) to be the parts of Wi that mediate T and Y . The natural direct effect of treatment β is average difference in outcome induced by giving each unit the treatment, if the distribution of Z had been as though each unit received treatment. That is,
β = EZ|T =1[E[Y | Z, do(T = 1)]−E[Y | Z, do(T = 0)]].
In the Reddit example, this is the expected difference in score between a post labeled as written by a man versus labeled as written by a woman, where the expectation is taken over the distribution of posts written by men.
Mathematically, β is equivalent to ψ in (2.1). The causal parameters have different interpretations depending on the graph. Under minimal conditions, the NDE may be estimated from observational data [Pea14]. The estimators for β are the same as those for the ATT, given in (2.2) and (2.3) [LR11, Ch. 8].

3 CAUSAL TEXT EMBEDDINGS
Following the previous section, we want to produce estimates of the propensity score g(zi) and the conditional expected outcome Q(ti, zi). We assume that some properties of the text zi = f (wi) sufﬁce for identiﬁcation. These properties are generally lower-dimensional than the text wi itself and are linguistically meaningful (i.e., they have to do with what the language means) But, we do not directly observe the confounding features zi. Instead, we observe the raw text.
A simple approach is to abandon zi and learn models for the propensities and conditional outcomes directly from the words wi. Since wi contains all information about zi, the direct adjustment will also render the causal effect identiﬁable. Indeed, in an inﬁnite-data setting this approach would be sound. However, the text is highdimensional and with ﬁnite data, this approach produces a high-variance estimator.
To this end, our goal is to reduce the words wi to a feature zi that contains sufﬁcient information to render the causal effect identiﬁable and that allows us to efﬁciently learn the propensity scores and conditional outcomes with a ﬁnite sample of data.
Our strategy is to use the words of each document to produce an embedding vector λ(w) that captures the confounding aspects of the text. Using the embedding, the propensity score is is g(λ(w)) = P(T = 1 | λ(w)). The conditional outcomes are Q(t, λ(w)) = E[Y | t, λ(w)]. The embeddings λ(w) are causally sufﬁcient if we can use them to estimate the propensities and conditional outcomes required by the downstream effect estimator. This result builds on [RR83]; Theorem 3.1 makes this formal.
In ﬁnite sample, additional domain knowledge is useful for ﬁnding these embeddings more efﬁciently. With text in particular, we assume that features that are useful for language understanding are also useful for eliminating confounding. The reason is that humans interpret the language and then produce the outcome based on aspects such as topic, writing quality or sentiment.
To produce causally sufﬁcient embeddings, we will adapt models for language understanding and reﬁne the representations they produce to predict the treatment and outcome. Informally, these models take in words wi and produce a tuple (λi, Q˜(ti, λi), g˜(λi)), which contains an embedding λi and estimates of g and Q that use that embedding. Such models provide an effective black-box tool for both distilling the words into the information relevant to prediction problems, and for solving those prediction problems.
Finally, to estimate the average treatment effect, we follow

the strategy of Section 2. Fit the embedding-based prediction model to produce estimated embeddings λˆi, propensity scores g˜(λˆi) and conditional outcomes Q˜(ti, λˆi). Then, plug these values into a downstream estimator.
We develop two methods that produce causally sufﬁcient document embeddings: causal BERT and the causal topic model.
Causal BERT. We modify BERT, a state-of-the-art language model [Dev+18]. Each input to BERT is the document text, a sequence of word-piece tokens wi = (wi1, . . . , wil). The model is tasked with producing three kinds of outputs: 1) document-level embeddings, 2) a map from the embeddings to treatment probability, 3) a map from the embeddings to expected outcomes for the treated and untreated.
The model assigns an embedding ξw to each word-piece w. It then produces a document-level embedding for document text wi as λi = f ((ξwi1 , . . . , ξwil ), γU) for a particular function f . The embeddings and global parameter γU are trained by minimizing an unsupervised objective, denoted as LU(wi; ξ, γU). Informally, random word-piece tokens are censored from each document and the model is tasked with predicting their identities.1
Following Devlin et al. [Dev+18], we use a ﬁne-tuning approach to solve the prediction problem. We add a logitlinear layer mapping λi → g˜(λi; γg) and a 2-hidden layer neural net for each of λi → Q˜(0, λi; γQ0 ) and λi → Q˜(1, λi; γQ1 ). We learn the parameters for the embedding model and the prediction model jointly. This supervised dimensionality reduction adapts the embeddings to be useful for the downstream prediction task, i.e., for causal inference.
We write γ for the full collection of global parameters. The ﬁnal model is trained as:
λˆi = f ((ξˆn,wi1 , . . . , ξˆn,wil ), γˆU) ξˆ, γˆ = argmin 1 L(wi; ξ, γ),
ξ,γ n i
where the objective is designed to predict both the treatment and outcome. It is
L(wi; ξ, γ) = yi − Q˜(ti, λi; γ) 2
+ CrossEnt ti, g˜(λi; γ) + LU(wi; ξ, γ).
Effect estimation. Computing causal effect estimates simply requires plugging in the propensity scores and expected outcomes that the trained model predicts on the
1BERT also considers a ‘next sentence’ prediction task, which we do not use.

held-out units. For example, using the plug-in estimator (2.2),

ψˆQ := 1 n

Q˜(1, λˆn,i; γˆnQ) − Q˜(0, λˆn,i; γˆnQ). (3.1)

i

The estimation procedure is the same for the NDE.

Causal Amortized Topic Model. We adapt the standard topic model [Ble+03], a generative model of text documents. Formally, a document wi (a sequence of word tokens) is generated from k latent topics by:

ri ∼ N (0, I) θi = softmax(ri) wij ∼ Cat(θi β)

The vector θi represents the document’s topic proportions, drawn from log normal prior distribution. It is the embedding λ(wi) of the document. The parameters β represent topics.
Typically, this model is ﬁt with variational inference, which seeks to maximize a lower bound of the marginal log likelihood of the documents. It is a sum of perdocument bounds. Each is

Li(β, η) = Eq(θ;η)[p(wi | θi; β)] − DKL(q(θi; η), p(θi)).

The ﬁrst term pushes the variational distribution over document representations q(θ; η) to reconstruct the observed documents well. This term encourages language modeling.
We use amortized inference to deﬁne a variational family q(θ | w; η) that depends on the observed documents. It uses a feedforward neural network called an encoder with parameters η to produce a representation θ. For each document, the encoder produces two vectors (µi, Σi) such that q(θi | w; η) = N (µi, Σi). This is the amortized topic model (ATM).
We adapt the training objective of this model to produce representations that predict the treatment and outcome well. A logit linear mapping θi → g˜(θi; γg) produces propensity scores from the document representation. A linear mapping θi → Q˜(ti, θi; γQ) produces expected outcomes from the document representations. The ﬁnal loss is
L(wi; η, β, γ) = −Li(β, η)
+ Eq(θ|w;η)[CrossEnt ti, g˜(θi; γ)] + Eq(θ|w;η)[yi − Q˜(ti, θi; γ)2].

It encourages good reconstruction of both the observed documents, and the treatment and outcome under the learned representation. The objective is minimized with

stochastic optimization, forming noisy gradients using the reparameterization trick. We refer to this model as the causal amortized topic model (Causal ATM).

Validity. The central idea of the method is that instead of adjusting for all of the information in w it sufﬁces to adjust for the limited information in λ(w). We now formalize this observation and establish the validity of our estimation procedure. To avoid notational overload, we state the result for only the ATT. The same arguments carry through for the NDE as well.

A key observation is that for z to be confounding, it must causally inﬂuence both treatment assignment and the outcome. Accordingly, z must be predictive of both the treatment and outcome. Put differently: any information in w that is not predictive for both T and Y is not relevant for z, is not confounding, and may be safely excluded from λ(w). Or, if λ(w) carries the relevant information for the prediction task then it is also causally sufﬁcient. The next result formalizes the observation that predictive sufﬁciency is also causal sufﬁciency.

Theorem 3.1. Suppose λ(w) is some function of the words such that at least one of the following is λ(W)measurable:

1. (Q(1, W), Q(0, W)),

2. g(W),

3. g((Q(1, W), Q(0, W)))

or

(Q(1, g(W)), Q(0, g(W))).

If adjusting for W sufﬁces to render the ATT identiﬁable then adjusting for only λ(W) also sufﬁces. That is, ψ = E[E[Y | λ(W), T = 1] − E[Y | λ(W), T = 0]].

In words: the random variable λ(W) carries the information about W relevant to the prediction of both the propensity score and the conditional expected outcome. While λ(W) will typically throw away much of the information in the words, Theorem 3.1 says that adjusting for it sufﬁces to estimate causal effects. Item 1 is immediate from (2.1), and says it sufﬁces to preserve information about Y . Item 2 is a classic result of [RR83, Thm. 3], and says it sufﬁces to preserve information about T . Item 3 weakens our requirements further: we can even throw away information relevant to Y , so long as this information is not also relevant to T (and vice versa).

The next result extends this identiﬁcation result to estimation, establishing conditions for the validity of our estimation procedure.

Theorem 3.2. Let η(z) = (E[Y | T = 0, z], E[Y | T =

1, z], P[T = 1 | z)) be the conditional outcomes and

propensities given z. Suppose that ψˆ({(ti, yi, zi)}; η) =

1 n

i φ(ti, yi, η(zi))+op(1) is some consistent estimator

for the average treatment effect ψ. Further suppose that

there is some function λ of the words such that
1. (identiﬁcation) λ satisﬁes the condition of Theorem 3.1.
2. (consistency) η(λ(Wi)) − η˜(λˆi) P,2 → 0 as n → ∞, where η˜ is the estimated conditional outcome and propensity model.
3. (well-behaved estimator) ∇ηφ(t, y, η) 2 ≤ C for some constant C ∈ R+,
then, ψ˜({(ti, yi, λˆi)}; η˜) −→p ψ. Remark 3.3. The requirement that the estimator ψˆ behaves asymptotically as a sample mean is not an important restriction; most commonly used estimators have this property [Ken16]. The third condition is a technical requirement on the estimator. In the cases we consider, it sufﬁces that the range of Y and Q are bounded and that g is bounded away from 0 and 1. This later requirement is the common ‘overlap’ condition, and is anyway required for the estimation of the causal effects.
The proof is given in the appendix.
As with all causal inference, the validity of the procedure relies on uncheckable assumptions that the practitioner must assess on a case-by-case basis. Particularly, we require that:
1. (properties z of) the document text renders the effect identiﬁable,
2. the embedding method extracts semantically meaningful text information relevant to the prediction of both t and y,
3. the conditional outcome and propensity score models are consistent.
Only the second assumption is non-standard. In practice, we use the best possible embedding method and take the strong performance on (predictive) natural language tasks in many contexts as evidence that the method effectively extracts information relevant to prediction tasks. Additionally, we use the domain knowledge that features which are useful for language understanding are also relevant for adjusting for confounding. Informally, assumption 2 is satisﬁed if we use a good language model, so we satisfy it by using the best available model.
4 EXPERIMENTS
We now empirically study the causally sufﬁcient embeddings produced by causal BERT and causal ATM.2 The
2Code and data available at github.com/vveitch/causal-textembeddings-tf2

question of interest is whether supervised dimensionality reduction and language modeling produce embeddings that admit efﬁcient causal estimation.
Empirically validating effect estimation is difﬁcult since known causal effects in text are unavailable. We address this gap with semi-synthetic data We use real text documents and simulate an outcome that depends on both the treatment of interest and a confounder.
We ﬁnd: 1) modeling language improves effect estimation; 2) supervised representations perform better than their unsupervised counterparts; 3) the causally sufﬁcient representations proposed in this paper effectively adjust for confounding.
Finally, we apply causal BERT to the two motivating examples in the introduction. We estimate causal effects on paper acceptance and post popularity on Reddit.com. Our application suggest that much of the apparent treatment effects is attributable to confounding in the text. Code and data to reproduce all studies will be publicly available.
PeerRead. PeerRead is a corpus of computer-science papers [Kan+18]. We consider a subset of the corpus consisting of papers posted to the arXiv under cs.cl, cs.lg, or cs.ai between 2007 and 2017 inclusive. The data only includes papers which are not cross listed with any non-cs categories and are within a month of the submission deadline for a target conferences. The conferences are: ACL, EMNLP, NAACL, EACL, TACL, NeurIPS, ICML, ICLR and AAAI. A paper is marked as accepted if it appeared in one of the target venues. Otherwise, the paper is marked as rejected. The dataset includes 11,778 papers, of which 2,891 are accepted.
For each paper, we consider the text of abstract, the accept/reject decision, and two attributes that can be predicted from the abstract text:
1. buzzy: the title contains any of ‘deep’, ‘neural’, ‘embed’, or ‘adversarial net’.
2. theorem: the word ‘Theorem’ appears in the paper.
Reddit. Reddit is an online forum divided into topicspeciﬁc subforums called ‘subreddits’. We consider three subreddits: keto, okcupid, and childfree. In these subreddits, we identify users whose username ﬂair includes a gender label (usually ‘M’ or ‘F’). We collect all top-level comments from these users in 2018. We use each comment’s text and score, the number of likes minus dislikes from other users. The dataset includes 90k comments in the selected subreddits. We consider the direct effect of the labeled gender on posts’ scores.

4.1 Empirical Setup
Empirical evaluation of causal estimation procedures requires semi-synthetic data because ground truth causal effects are usually not available. We want semi-synthetic data to be reﬂective of the real world. This is challenging in the text setting: it is difﬁcult to generate text on the basis of confounding aspects such as topic or writing quality.
We use real text and metadata —subreddit and title buzziness—as the confounders z˜ for the simulation. We checked that these confounders are related to the text. We simulate only the outcomes, using the treatment and the confounder. We compute the true propensity score π(z˜) as the proportion of units with ti = 1 in each strata of z˜. Then, Yi is simulated from the model:
Yi = ti + b1(π(z˜i) − 0.5) + εi εi ∼ N (0, γ).
Or, for binary outcomes,
Yi ∼ Bernoulli(σ(0.25ti + b1(π(z˜i) − 0.2)))
The parameter b1 controls the level of confounding; e.g., the bias of the unadjusted difference E[Y |T = 1] − E[Y |T = 0] increases with b1. For PeerRead, we report estimates of the ATE for binary simulated outcomes. For Reddit, we compute the NDE for simulated real-valued outcomes.
Methods. The goal is to study the utility of language modeling and supervision in representation learning. We use Causal BERT (C-BERT) and Causal ATM (C-ATM), explained in Section 3, to test these ideas.
First, we compare the methods to two that omit language modeling. We ﬁt C-BERT and C-ATM without the loss terms that encourage language modeling: in BERT, it is the censored token prediction and in ATM, it is the expected reconstruction loss. They amount to ﬁtting attention-based or multi-layer feedforward supervised models. We refer to these baselines as BERT (sup. only) and NN.
Second, we consider methods that omit supervised representation learning. These include ﬁtting regression models for the expected outcomes and propensity scores with ﬁxed representations of documents. We compare four: 1) bag-of-words (BOW), 2) out-of-the-box BERT embeddings, 3) the document-topic proportions produced by latent Dirichlet allocation (LDA), 4) and the document representation produced by ATM. To produce documentlevel embeddings from BERT, the embeddings for each token in the document are summed together.
Estimator. For each experiment, we consider two downstream estimators: the plugin estimator (2.2) and the Qonly estimator (2.3). For all estimators, we exclude units

that have a predicted propensity score greater than 0.97 or less than 0.03.
BERT pre-processing. For BERT models, we truncate PeerRead abstracts to 250 word-piece tokens, and Reddit posts to 128 word-piece tokens. We begin with a BERT model pre-trained on a general English language corpus. We further pre-train a BERT model on each dataset, running training on the unsupervised objective until convergence.

NDE Estimate

2.5

2.0

1.5

Plug-in TMLE

1.0

Unadjusted

0.0 0.2 0.4 0.6 0.8 1.0

Exogeneity

Figure 2: The method improves the unadjusted estimator even with exogeneous mediatiors. Plot shows estimates of NDE from simulated data based on Reddit. Ground truth is 1.

4.2 Results
Semi-synthetic data is used to investigate three questions about causal adjustment: 1) does language modeling help? 2) does supervised dimensionality reduction help? 3) do C-BERT and C-ATM produce embeddings that sufﬁciently adjust for confounding? Results are summarized in Tables 1 to 3.
Language modeling helps. The left table in Table 1 illustrates the point that representations that model language produce better causal estimates. C-ATM and CBERT recover the simulated treatment effect in Reddit and PeerRead more accurately than NN and BERT (sup. only) . Among them, C-BERT performs best. This experiment uses the confounding setting β1 = 10.0, γ = 1.0 for Reddit and β1 = 5.0 for PeerRead.
Supervision helps. The right table in Table 1 summarizes effect estimation for ﬁxed representations that are used to ﬁt expected outcomes and propensity scores. Among these, LDA, ATM and BERT model language structure. No method estimates the treatment effects as accurately as C-BERT or C-ATM. The ﬁnding suggests supervising representations to discard information not relevant to predicting the treatment or outcome is useful for effect estimation. The improvement of C-ATM over

ATM and C-BERT over BERT suggests that combining language modeling with supervision works best.
Methods adjust for confounding. Tables 2 and 3 show the quality of effect estimation as the amount of confounding increases. For Reddit, the confounding setting β1 varies across 1.0, 10.0, 100.0 and the noise γ varies across 1.0 and 4.0. For PeerRead, β1 varies across 1.0, 5.0, 25.0. Results from the NN method are shown as a baseline.
Compared to the unadjusted estimate, all methods perform adjustments that reduce confounding. However, C-BERT and C-ATM recover the most accurate causal estimate in all settings. In the Reddit setting, C-ATM performs best when the outcome variance is higher (γ = 4.0). However, C-BERT is best when the outcome variance is lower (γ = 1.0). In the PeerRead setting, C-BERT performs best.
The effect of exogeneity. We assume that the text carries all information about the confounding (or mediation) necessary to identify the causal effect. In many situations, this assumption may not be fully realistic. For example, in the simulations just discussed, it may not be possible to exactly recover the confounding from the text. We study the effect of violating this assumption by simulating both treatment and outcome from a confounder that consists of a part that can be fully inferred from the text and part that is wholly exogenous.
The challenge is ﬁnding a realistic confounder that can be exactly inferred from the text. Our approach is to (i) train BERT to predict the actual treatment of interest, producing propensity scores gˆi for each i, and (ii) use gˆi as the inferrable part of the confounding. Precisely, we simulate propensity scores as logit gsim = (1 − p) logit gˆi + pξi, with ξi i∼id N(0, 1). The outcome is simulated as above.
When p = 0, the simulation is fully-inferrable and closely matches real data. Increasing p allows us to study the effect of exogeneity; see Section 4.1. As expected, the adjustment quality decays. Remarkably, the adjustment improves the naive estimate at all levels of exogeneity— the method is robust to violations of the theoretical assumptions.
Application We apply causal BERT to estimate the treatment effect of buzzy and theorem, and the effect of gender on log-score in each subreddit. Although unadjusted estimates suggest strong effects, our results show this is in large part explainable by confounding or mediating. See Tables 4 and 5. On PeerRead, both estimates suggest a positive effect from including a theorem on paper acceptance. On Reddit, both estimates suggest a positive effect from labeling a post as female on its score in okcupid and childfree.

Table 1: Comparisons on semi-simulated data show that effect estimation is improved by: (a) language modeling (left); (b)supervised dimensionality reduction (right). (a) C-BERT and C-ATM improve effect estimation over NN and BERT (sup. only), which do not model language structure. (b) C-BERT and C-ATM improve effect estimation over BOW, BERT, LDA and ATM, which produce representations that are not supervised to predict the treatment and outcome. The tables report estimated NDE for Reddit and estimated ATT for PeerRead. Shaded numbers are closest to the ground truth. The simulation setting used is β1 = 10.0, γ = 1.0 for Reddit and β1 = 5.0 for PeerRead.

(a) Language Modeling Helps

Dataset: Reddit PeerRead (NDE) (ATT)

Ground truth Unadjusted

1.00

0.06

1.24

0.14

NN ψˆQ

1.17

0.10

NN ψˆplugin

1.17

0.10

BERT (sup. only) ψˆQ

0.93

0.19

BERT (sup. only) ψˆplugin 1.17

0.18

C-ATM ψˆQ

1.16

0.10

C-ATM ψˆplugin

1.13

0.10

C-BERT ψˆQ

1.07

0.07

C-BERT ψˆplugin

1.15

0.09

(b) Supervision Helps

Dataset: Reddit PeerRead (NDE) (ATT)

Ground truth
Unadjusted
BOW ψˆQ BOW ψˆplugin BERT ψˆQ BERT ψˆplugin LDA ψˆQ LDA ψˆplugin ATM ψˆQ ATM ψˆplugin

1.00 1.24
1.17 1.18 -15.0 -14.1 1.20 1.20 1.17 1.17

0.06 0.14
0.13 0.14 -0.25 -0.28 0.07 0.09 0.08 0.08

Table 2: Embedding adjustment recovers the NDE on Reddit. This persists even with high confounding and high noise. Table entries are estimated NDE. Columns are labeled by confounding level. Low, Med., and High correspond to β1 = 1.0, 10.0 and 100.0.

Noise: Confounding:
Ground truth Unadjusted
NN ψˆQ NN ψˆplugin C-ATM ψˆQ C-ATM ψˆplugin C-BERT ψˆQ C-BERT ψˆplugin

Low
1.00 1.03
1.03 1.03 1.01 1.01 1.07 1.08

γ = 1.0 Med. High
1.00 1.00 1.24 3.48
1.18 2.04 1.18 1.40 1.16 2.45 1.13 2.09 1.07 1.14 1.15 0.94

Low
1.00 0.99
0.89 0.85 1.04 0.95 1.50 2.07

γ = 4.0 Med. High
1.00 1.00 1.22 3.51
1.08 2.24 1.05 2.07 1.04 1.72 0.94 1.11 0.95 1.12 1.07 1.27

5 DISCUSSION
We have examined the use of black box embedding methods for causal inference with text. The challenge is to produce a low-dimensional representation of text that is sufﬁcient for causal adjustment. We adapt two modern tools for language modeling — BERT and topic modeling — to produce representations that predict the treatment and outcome well. This marries two ideas to produce causally sufﬁcient text representations: modeling language and supervision relevant to causal adjustment. We propose a methodology for empirical evaluation that uses real text documents to simulate outcomes with confounding. The

studies in this simulated setting validate the representation learning insights of this paper. The application to real data completes the demonstration of our methods.
There are several directions for future work. First, the black box nature of the embedding methods makes it difﬁcult for practitioners to assess whether the causal assumptions hold: is it possible to develop visualizations and sensitivity analyses to aid these judgments? Second, we require both the treatment and outcome to be external to the text. How can the approach here be extended to estimate the causal effect of (or on) aspects of the writing itself? Third, the deep learning methods we have used are mainly geared towards predictive performance. Are there

Table 3: Embedding adjustment recovers the ATT on PeerRead. This persists even with high confounding. Table entries are estimated ATT. Columns are labeled by confounding level. Low, Med., and High correspond to β1 = 1.0, 5.0 and 25.0.

Confounding:
Ground truth Unadjusted
NN ψˆQ NN ψˆplugin C-ATM ψˆQ C-ATM ψˆplugin C-BERT ψˆQ C-BERT ψˆplugin

Low
0.06 0.08
0.05 0.05 0.07 0.07 0.09 0.10

Med.
0.05 0.15
0.10 0.10 0.10 0.10 0.07 0.09

High
0.03 0.16
0.30 0.30 0.32 0.32 0.04 0.05

Table 4: Embedding adjustment reduces estimated treatment effects in PeerRead. Entries are estimated treatment effect and 10-fold bootstrap standard deviation.

Unadjusted C-BERT ψˆQ C-BERT ψˆplugin

buzzy
0.08 ± 0.01 −0.03 ± 0.01 −0.02 ± 0.01

theorem
0.21 ± 0.01 0.16 ± 0.01 0.18 ± 0.02

Table 5: Embedding adjustment reduces estimated direct effects in Reddit. Entries are estimated treatment effect and 10-fold bootstrap standard deviation.

Unadjusted C-BERT ψˆQ C-BERT ψˆplugin

okcupid
−0.18 ± 0.01 −0.10 ± 0.04 −0.15 ± 0.05

childfree
−0.19 ± 0.01 −0.10 ± 0.04 −0.16 ± 0.05

keto
−0.00 ± 0.00 −0.03 ± 0.02 −0.01 ± 0.00

improvements that will help with estimation accuracy? For example, should we adapt methods that speciﬁcally target well-calibrated predictions? Finally, how can methods in this domain be reliably empirically evaluated? We have developed a new strategy for realistic semi-synthetic simulations in this paper. Can our approach be extended to a complete approach for benchmarking?
6 ACKNOWLEDGEMENTS
This work was supported by ONR N00014-15-1-2209, ONR N00014-17-1-2131 , NIH 1U01MH115727-01, NSF CCF-1740833, DARPA SD2 FA8750-18-C-0130, the Alfred P. Sloan Foundation, 2Sigma, and the government of Canada through NSERC. The GPUs used for this research were donated by the NVIDIA Corporation.

References
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet allocation,” Journal of machine Learning research, 2003.
[2] A. D’Amour and A. Franks, “Covariate reduction for weighted causal effect estimation with deconfounding scores,”
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language understanding,” ArXiv eprints, arXiv:1810.04805, 2018.
[4] N. Egami, C. J. Fong, J. Grimmer, M. E. Roberts, and B. M. Stewart, “How to make causal inferences using texts,” ArXiv preprint arXiv:1802.02163, 2018.
[5] F. Johansson, U. Shalit, and D. Sontag, “Learning representations for counterfactual inference,”

in International conference on machine learning, 2016.
[6] F. D. Johansson, N. Kallus, U. Shalit, and D. Sontag, “Learning weighted representations for generalization across designs,” ArXiv preprint arXiv:1802.08598, 2018.
[7] F. D. Johansson, R. Ranganath, and D. Sontag, “Support and invertibility in domain-invariant representations,” ArXiv preprint arXiv:1903.03448, 2019.
[8] F. D. Johansson, U. Shalit, N. Kallus, and D. Sontag, “Generalization bounds and representation learning for estimation of potential outcomes and causal effects,” ArXiv preprint arXiv:2001.07426, 2020.
[9] N. Kallus, X. Mao, and M. Udell, “Causal inference with noisy and missing covariates via matrix factorization,” in Advances in Neural Information Processing Systems, 2018.
[10] D. Kang, W. Ammar, B. Dalvi, M. van Zuylen, S. Kohlmeier, E. Hovy, and R. Schwartz, “A dataset of peer reviews (PeerRead): Collection, insights and nlp applications,” ArXiv e-prints, arXiv:1804.09635, 2018.
[11] E. H. Kennedy, “Semiparametric theory and empirical processes in causal inference,” in Statistical Causal Inferences and their Applications in Public Health Research, 2016.
[12] M. Kuroki and M. Miyakawa, “Identiﬁability criteria for causal effects of joint interventions,” Journal of the Japan Statistical Society, no. 2, 1999.
[13] M. Kuroki and J. Pearl, “Measurement bias and effect restoration in causal inference,” Biometrika, no. 2, 2014.
[14] M. van der Laan and S. Rose, Targeted learning: Causal inference for observational and experimental data. 2011.
[15] W. Miao, Z. Geng, and E. J. Tchetgen Tchetgen, “Identifying causal effects with proxy variables of an unmeasured confounder,” Biometrika, no. 4, 2018.
[16] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in Neural Information Processing Systems, 2013.
[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of word representations in vector space,” ArXiv preprint arXiv:1301.3781, 2013.
[18] J. Pearl, “On measurement bias in causal inference,” ArXiv e-prints, arXiv:1203.3504, 2012.
[19] J. Pearl, “Interpretation and identiﬁcation of causal mediation,” Psychological methods, 2014.

[20] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” ArXiv e-prints, arXiv:1802.05365, 2018.
[21] M. E. Roberts, B. M. Stewart, and R. A. Nielsen, Adjusting for confounding with text matching, 2018.
[22] P. R. Rosenbaum and D. B. Rubin, “The central role of the propensity score in observational studies for causal effects,” Biometrika, no. 1, 1983.
[23] C. Shi, D. Blei, and V. Veitch, “Adapting neural networks for the estimation of treatment effects,” in Advances in Neural Information Processing Systems, 2019.
[24] V. Veitch, Y. Wang, and D. Blei, “Using embeddings to correct for unobserved confounding in networks,” in Advances in Neural Information Processing Systems, 2019.
[25] Z. Wood-Doughty, I. Shpitser, and M. Dredze, “Challenges of using text classiﬁers for causal inference,” in Empirical Methods in Natural Language Processing, 2018.

