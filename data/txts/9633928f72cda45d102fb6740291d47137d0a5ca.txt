Mitigating Backdoor Attacks in Federated Learning

Chen Wu Computer Science and Engineering
Pennsylvania State University cvw5218@psu.edu
Sencun Zhu Computer Science and Engineering
Pennsylvania State University sxz16@psu.edu

Xian Yang Computer Science North Carolina State University xyang45@ncsu.edu
Prasenjit Mitra Information Sciences and Technology
Pennsylvania State University pum10@psu.edu

arXiv:2011.01767v2 [cs.CR] 14 Jan 2021

Abstract—Malicious clients can attack federated learning systems using malicious data, including backdoor samples, during the training phase. The compromised global model will perform well on the validation dataset designed for the task, but a small subset of data with backdoor patterns may trigger the model to make a wrong prediction. There has been an arms race between attackers who tried to conceal attacks and defenders who tried to detect attacks during the aggregation stage of training on the server-side.In this work, we propose a new and effective method to mitigate backdoor attacks after the training phase. Speciﬁcally, we design a federated pruning method to remove redundant neurons in the network and then adjust the model’s extreme weight values. Our experiments conducted on distributed Fashion-MNIST show that our method can reduce the average attack success rate from 99.7% to 1.9% with a 5.5% loss of test accuracy on the validation dataset. To minimize the pruning inﬂuence on test accuracy, we can ﬁne-tune after pruning, and the attack success rate drops to 6.4%, with only a 1.7% loss of test accuracy. Further experiments under Distributed Backdoor Attacks on CIFAR-10 also show promising results that the average attack success rate drops more than 70% with less than 2% loss of test accuracy on the validation dataset.
Index Terms—federated learning, backdoor attack, federated model pruning, machine-learning security
I. INTRODUCTION
The success of deep learning models relies on large-scale datasets. Traditional training methods require collecting the training data and centralizing the data in one machine or a data center. Nowadays, people are getting more sensitive and careful about sharing data with others. It becomes much harder and more expensive to gather data from different sources compared with the old days. To solve this problem, researchers proposed a new training method called federated learning [14] to enable collaborative model training without sharing datasets among different clients or with the server.
However, federated learning systems [14] are vulnerable to attacks from malicious clients [1]–[3], [7], [15]. The server does not have access to the data of clients, so it cannot verify model updates from clients, especially when the system is augmented with secure aggregation protocols to further protect the client privacy [5]. Theoretically, malicious clients can send any updates to the server and the server’s global model could

be easily compromised if the server has no effective protection to identify/process those malicious updates.
Existing defense methods focus on the federated aggregation process where the server receives model updates from all the clients. These methods try to distinguish malicious updates from benign ones. Byzantine-robust aggregation rules, such as Krum [4], Bulyan [16], trimmed mean [22] and median [22], all use statistical characteristics of model weights. However, they have failed to detect backdoor attacks in federated learning [1], [3], [8], [20], [21] because the non-IID distribution of data among different clients creates enough space for the attacker to hide malicious updates from being detected.
Backdoor attacks often trigger “backdoor neurons” which are the neurons that are activated only in the presence of backdoor-ed images [9]. Studies [13], [18] have shown that pruning those “backdoor neurons” could greatly mitigate backdoor attacks without hurting too much model performance. However, these pruning methods cannot be directly used in our case because they rely on reliable sources of “clean” data, which is not guaranteed in federated learning scenarios (which is designed to protect the privacy of clients’ data). To address this problem, we propose a novel federated pruning process that does not require access to the clients’ original data and helps in deciding the pruning sequence of neurons.
We introduce two federated pruning methods by eliminating neurons that are not being activated by clients’ inputs. Our federated pruning method requires clients to rank the dormant level of neurons in the network. Basically, neurons that are not frequently activated by the network are called dormant neurons. After aggregation, the server will determine the dormant neurons based on the information provided by all the clients and steadily prune the neural networks while checking the model performance on a small validation dataset.
Our empirical studies have shown that a single federated pruning process is insufﬁcient to thoroughly eliminate backdoor attacks. The success of the federated pruning method highly depends on the attacker’s targets. For example, the pruning process can fully eliminate backdoor behavior when the attacker tries to backdoor digit number 9 to number 1, while the same pruning process fails when the attacker tries

to backdoor 9 to 5. The reason is that in some cases, benign and backdoor behaviors utilize the same set of neurons. Thus, pruning those neurons will degrade the performance of the model on both clean data and backdoored instances.
Despite the challenges, we observe that under the majority assumption when the number of neurons/features supporting correct labels is more than the number of neurons/features supporting backdoor labels, a malicious client has to introduce extreme values for the inputs or extreme values for the weights of the neurons to reverse the correct prediction results to backdoor labels . Thus, by limiting the inputs and the weights of the neurons, we can mitigate the backdoor attacks.
The contributions of this paper are as follows:
• We propose two federated pruning methods to remove redundant neurons in the network without degrading accuracy signiﬁcantly in a scenario where we do not have access to the clients’ private dataset.
• Our experiments show that the defensive effect of pruning neurons highly depends on the target label attacked by the malicious clients. Thus, we propose to adjust extreme weights in the network to degrade the backdoor attacks. Experiments on MNIST and Fashion-MNIST have shown that our method can effectively reduce the average attack success rate from over 99% to be less than 2%. Experiments on CIFAR-10 under state-of-the-art distributed backdoor attacks have also shown that our method can reduce the average attack success rate more than 70% with a loss of test accuracy less than 2%.
• We propose a federated ﬁne-tuning process after neuron pruning to improve the model performance on the validation dataset. Experiments show that adjusting extreme weights after ﬁne-tuning can degrade the attack success rate to less than 10% while the global model’s accuracy rate on the validation dataset can improve 5%.
II. RELATED WORKS
A. Backdoor Federated Learning
Bagdasaryan et al. [1] introduced the “semantic backdoor” concept that uses rare features in the real world as a trigger and does not require the attacker to modify the input of the model at inference time. For example, an adversary could compromise the global model by predicting car images with racing stripes as birds, whereas other car images would still be predicted as cars. Sun et al. [17] further showed that allowing the non-malicious clients to have correctly labeled samples from the targeted tasks could not prevent such backdoor attacks. Bhagoji et al. [3] showed that with 10% of the clients being compromised, a backdoor can be introduced by poisoning the model sent back to the server, even with the presence of anomaly detectors or Byzantine-resilient aggregation mechanisms used by the server.
B. Attacks and Defenses in Federated Learning
Previously proposed byzantine-robust aggregation rules, like Krum [4], Bulyan [16], trimmed mean [22] and median [22], all using statistical characteristics of model weights, were

reported to have failed to detect backdoor attacks in federated learning [1], [3]. Non-IID distribution of data gives attackers enough space to forge their backdoor-model updates. Fang et al. [6] have further shown an untargeted attack that by directly manipulating the local model parameters on the compromised client devices during the learning process, the global models with previous byzantine-robust aggregation rules would suffer a worse testing error rate. To better conceal attackers’ updates from being detected, Xie et al. [20] proposed a distributed backdoor attack method that decomposes a global trigger pattern into separate local patterns and embeds them into the training set of different attackers.
Li et al. [12] proposed a spectral anomaly detection based framework that detects the abnormal model updates based on their low-dimensional embeddings, in which the noisy and irrelevant features are removed while the essential features are retained. They showed that in low-dimensional latent feature space, the abnormal (malicious) model updates can be easily differentiated from the normal updates. Most existing defense methods are trying to distinguish attackers’ updates from benign clients’ updates, while the attackers are trying to modify their updates as close to the other updates as possible. It is hard to tell which method is more effective under different datasets and various data distributions. Our work differs from the existing ones by focusing on mitigating backdoor attacks further after the training phase.

C. Pruning Against Backdoor Attacks
Gu et al. [9] showed that poisoned data designed to introduce a backdoor often triggers “backdoor neurons”. Based on this assumption, pruning defenses [13], [18] attempt to remove activation units that are inactive on clean data. This method requires “clean” data that is representative of the global dataset being used to identify those “backdoor neurons”, and such “clean” data is typically not approachable by the server in federated learning scenarios. Different from the above work, our work proposes a federated neuron pruning method.

III. PROBLEM DEFINITION
In this section, we introduce our federated learning environment settings, including the aggregation rule, data distribution among clients, and backdoor attack methods employed in this paper.

A. Global Model Learning

According to the federated learning deﬁnition proposed by

McMahan et al. [14], training data is from a number of clients

and cannot be shared with others other than the client himself.

We assume the total number of clients is N and each client

has ni number of samples. At each time t, the server randomly

selects a portion of clients k · N (0 < k ≤ 1) from all clients

and asks them to train the global model on their local dataset.

Then, the FedAvg algorithm [14] will update the global model

by aggregating the weight updates from these clients as shown

below.

ωt+1 = ωt + η ·

kN i=1

ni∆ωti+1

kN i=1

ni

0 Original

0 1 pixel

0 3 pixels

0 5 pixels

0 7 pixels

0 9 pixels

10

10

10

10

10

10

20

20

20

20

20

20

0 10 20

0 10 20

0 10 20

0 10 20

0 10 20

0 10 20

Fig. 1. An original image from the MNIST dataset, and the backdoored version of this image using different pixels backdoor pattern.

where ωt is the previous model parameters at time t, ωt+1 is the updated model parameters at time t + 1, ∆ωti+1 is the
updated changes of model parameters provided by client i, η is

the global model learning rate. In this paper, since our defense

method is not focused on the aggregation process, we simplify

the above rules to speed up the backdoor attack and make sure

the attack is successful. To be more speciﬁc, we make some

assumptions to simplify the above learning process. Firstly, we

set the number of samples ni same for every client, because

otherwise the attacker can simply strengthen their updates

by claiming that they have a larger number of samples to

overwhelm the updates from others. Secondly, to improve the

global training speed and better monitor the attacking process

behavior, we set the same learning rate ηi for each client

(including the attacker). Thirdly, we assume all clients will

participate in every round of aggregation so that the random

selection of clients process will not affect the performance of

the global model and attacks. The global model aggregation

rule is simpliﬁed as follows.

1N

ωt+1 = ωt +

∆ωti+1

N

i=1

B. Threat Model

The goal of the attacker is to ensure the ﬁnal global model only fails on a speciﬁc task while performing reasonably well on the other tasks. For example, on the MNIST dataset [11], the attacker would like to make the global model mispredict images with backdoor patterns as another label selected by the attacker, while the other normal images would still be predicted correctly. Consequently, the server can hardly recognize that the global model has been compromised even with a substantial evaluation dataset. In general, the attacker would like the global model to predict all testing samples {xi} with correct labels yi = T correctly, while corrupted samples with backdoor patterns {xi} will be predicted to a wrong label F , where label T and label F can both be selected by the attacker.
We make the following assumptions about the attacker in our experiments: (i) According to our federated learning scenario, all the clients participate in every round of model aggregation. So, there will be at least one attacker in each training iteration. (ii) The malicious client only has access to his own dataset and has no access to the testing dataset. The backdoor training samples are created by applying backdoor patterns to the local dataset owned by the attacker.

C. Backdoor Attack
In this paper, we start with similar backdoor patterns as in BadNets [9], which changes some pixels in a picture to form a pattern. During the local training phase, the attacker would train the model with both original images and the backdoored

version of those images at the same time. In this way, he can enforce the model to learn the backdoored pattern instead of misclassifying both the original images and the backdoored images. For example, among the images in Fig 1, the original image would still be predicted as ‘9’, while the backdoored images are predicted as ‘1’ or any other label chosen by the attacker.

D. Model Replacement Attack

After local training, we use the model replacement attack

[1] to make sure the updates from the attacker will survive the

FedAvg aggregation on the server’s side. The basic idea behind

model replacement attack is that the attacker wants the global

model to be as close to his local trained model as possible.

In the ideal situation, the global model will be completely

replaced by the attacker’s model as shown below:

1N

xatk = ωt+1 = ωt +

(xit+1 − ωt)

(1)

N

i=1

where xatk is the attacker’s model, ωt+1 is the global model

at time t + 1, xit+1 is the local model trained by client i at

time t + 1, N is the number of all clients that participate in

the federated learning process. So this function means that the

global model ω at time t + 1 is an averaged mean of model

updates from N clients at time t+1 and the goal of the attacker

is to replace this global model ω at time t + 1 with attacker’s model xatk. Let xm t+1 be the update from the malicious client m at time t + 1. Then, by solving Equation 1, we can have

the following:
N −1
xm t+1 = N · xatk − N · ωt − (xit+1 − ωt) + ωt

i=1

As assumed in the work of Bagdasaryan et al. [1], iN=−11(xit+1−ωt) ≈ 0. With the convergence of global model,
these deviations cancel out, so the attacker’s update can be

simpliﬁed as following:

xm t+1 = N · (xatk − ωt) + ωt

However, since the deviations do not cancel out in the

training process according to our experiments, we replace

N in the above equation with an attack update ampliﬁcation

coefﬁcient α (1 ≤ α ≤ N ). We ﬁnd that the value α is related

to the data distribution among clients. If the distribution of

dataset is more similar among clients, a higher value of α is

needed to achieve better results on the backdoor tasks.

IV. DEFENSE METHOD
Empirical studies by Gu et al. [9] showed that the backdoor inputs will trigger the neurons in a DNN that are typically not used by normal clean inputs. These so-called “backdoor neurons” are leveraged by the attacker to recognize backdoor

Local Data
malicious client
benigh client

Input

Input

Calculating Activation Values
0.43 0.79 0.21 0.88 0.92
0.52 0.83 0.17 0.79 0.04

Voting

Ranking Majority

2

0

Pred

3

1

1

0

4

1

5

1

3

1

5

1

2

0

4

1

1

0

Pred

Voting Aggregation Ranking Majority

3.5

0.6

5.3

0.8

1.6

0.1

4.2

0.7

2.1

0.2

Server Pruning
Normal Neurons Backdoor Neuron
Pruned Neuron

Pred Input

Fig. 2. Illustration of the federated pruning process.

patterns and trigger misbehavior while keeping silent when the input data is clean. Based on this ﬁnding, Liu et al. [13] proposed a Fine-Pruning method to remove the “backdoor neurons” in the DNN, thus mitigating backdoor attacks. The key idea behind this pruning method is to ﬁnd the dormant neurons that are not frequently activated in the network. By using clean training inputs to activate the DNN, they record the averaged activation values of each neuron in the last convolutional layer of the network. Then, they can iteratively prune neurons based on the increasing order of averaged activation values and stop before the accuracy on the validation dataset drops below a certain threshold.
However, in federated learning, the server does not have access to the training data of clients. Also, there is no way to guarantee a clean source of training inputs to activate the network, since attackers hide among the clients. So, in this paper, we propose a distributed way of pruning neurons, as shown in Fig 2. The ﬁrst step is to ask all the clients to record the averaged activation value of each neuron based on their local training dataset. Then, each client will determine a local pruning sequence based on the collected information and send this local pruning sequence to the server. The server will gather such information from all clients to further determine a global pruning sequence indicating which neurons are less used and thus can be pruned ﬁrst. It can test on a small validation dataset to decide the pruning rate (how many neurons need to be pruned) following this pruning sequence and select the maximum pruning rate with an acceptable test accuracy on the validation dataset. On the other hand, if the server does not have such a validation dataset or its dataset is too small and not able to represent the data distribution in the real world, it can pass the global pruning sequence back to each client and ask them to report the test accuracy on each client’s dataset under different pruning rates. The server will collect this feedback and determine the ﬁnal pruning strategy.
There are two advantages to this method. Firstly, this method does not require access to the clients’ datasets and thus protects the privacy of each client. Secondly, it is computationally efﬁcient in federated learning since it only needs

one round of communication between server and clients if the server has a validation dataset. Otherwise, an extra round of communication is required to collect the clients’ feedback on the pruning sequence. It is even possible to include more clients to participate in this step than in the training process. Along with the advantages, it also comes with shortcomings. For example, we can no longer have guaranteed “clean data”. Since the attackers hide among the clients, they can return manipulated updates to maintain the attack success rate. Thus, we further propose two algorithms to minimize the inﬂuence from a minority group of attackers.
A. First Approach: Ranking Vote
The ideal situation in federated pruning is that the server can directly collect all the average activation records from each client. However, directly passing the real values may result in both privacy and security problems. For privacy concerns, real values may reveal some information about the clients’ original dataset. For security concerns, the attackers may easily manipulate the ﬁnal aggregation results by changing the updated values just as they did in the model learning process.
To solve this problem, we propose a ranking vote algorithm, where each client provides a ranking of all the neurons in the last convolutional layer based on their averaged activation values. We consider each neuron stands for a sliding window in the convolutional layer. The sum of the output from that sliding window will be treated as the neuron’s activation value. If a convolutional layer has an output size of (B, C, H, W ), where B is the batch size, C number of channels (sliding windows), H height of input planes in pixels, and W width in pixels, then we say this convolutional layer has C neurons and the activation value of each neuron is the sum of (H, W ) outputs for each channel in C. The averaged activation value of each neuron will be the sum of all the activation values divided by the number of images used in the training. Each client will rank these neurons based on their averaged activation values in ascending order from 1 to C. In the next step, the server can create a global dormant ranking of all the neurons by averaging the ranking positions of individual neurons from all the clients. If we denote the ranking position of neuron

k provided by client i as Rki , the aggregation of ranking for

each

neuron

k

can

be

represented

as

Rk

=

1 N

N i=1

Rki

.

In

the end, the server iteratively prunes neurons in the decreasing

order of the global dormant ranking until the accuracy on the

validation dataset drops below a certain threshold.

On the other hand, if the server does not have sufﬁcient

representative validation dataset to determine when to stop

pruning, it can send the global neuron ranking back to each

client and ask them to test on clients’ local datasets following

the same pruning sequence. After collecting the pruning results

of each client, the server can decide how many neurons are

going to be pruned following this sequence.

B. Second Approach: Majority Vote
In the above Ranking Vote algorithm, with more neurons in the model, malicious updates of ranking information will have larger inﬂuence on the server’s global ranking. To further limit the impact of potential attackers, we can simplify the updates from the clients to be only zero or one for each neuron. The value one indicates that the neuron needs to be kept while zero means the neuron needs to be pruned from the view of this client. Then, it would be much harder for the attackers to manipulate the ﬁnal results if benign users are the majority group. Also, this method can better protect the clients’ privacy by revealing less information about the activation records on the local dataset.
Speciﬁcally, the server will provide a pruning rate , say p%, to all the clients. Each client needs to decide which neurons should be pruned based on their averaged activation records and the pruning rate. If the average activation value of a neuron belongs to the smallest value group (i.e., the value is below p% of all the values), it will be pruned. If the neuron should be pruned, it will be assigned value ‘0’; otherwise, it will be assigned ‘1’. At this point, we can imagine that each client will create a mask for all neurons and the mask only has values ‘0’ and ‘1’. Then, the server will aggregate all the masks from all clients to have a majority vote information about each neuron. Neurons with higher values stand for their importance among clients; neurons with lower values stand for their dormancy among clients. In the last step, the server can prune neurons in increasing order of their aggregated values until the accuracy on the validation dataset drops below a certain threshold.
With the improvement of security and privacy, this method may require more rounds of communication between clients and the server compared with the Ranking Vote algorithm. Because the pruning rate p% can hardly be selected without any prior knowledge. Although our experiments show that the pruning rate between 30% and 70% performs well in various situations, the server may still need to try different pruning rates in a real situation. Thus, extra rounds of calculation and communication between clients and the server may be needed.

C. Adjusting Extreme Weights
In our empirical study, we ﬁnd that the pruned network alone cannot guarantee the mitigation of backdoor attacks. The effect of pruning is highly dependent on the data distribution

among clients and the target labels chosen by the attackers. Despite the challenges, we observe that under the majority assumption when the number of neurons/features supporting correct labels is more than the number of neurons/features supporting backdoor labels, a malicious client has to introduce extreme values for the inputs or extreme values for the weights of the neurons to reverse the correct prediction results to backdoor labels. Thus, by limiting the inputs and the weights of the neurons, we can mitigate the backdoor attacks.
To limit the input ranges, we make an input normalization step for all the inputs to the model. To adjust extreme weight values in the network, we scan all the weights in the last convolutional layer and zero the weights that are larger or smaller than thresholds based on the mean and standard deviation of the weights in that layer. For example, if the mean value of all the weights in layer i is µi and the standard deviation is σi. Then, we set the threshold s = µi ± ∆ · σi that controls the valid range of all the weights in this layer. ∆ is a server-deﬁned hyperparameter in this function. We will discuss more about the selection of ∆ values in the Experiments section. Finally, all the weights that are beyond the threshold s = µi ± ∆ · σi will be set to zero. Our empirical studies have shown that this process is effective in reducing the backdoor task success rate to a low level after model pruning.
D. Fine-tuning
Pruning neurons and adjusting extreme weights will cause a drop in the model’s test accuracy on the validation dataset. We neither want our model to be vulnerable under backdoor attacks nor expect the model to perform poorly on designed tasks. So, we propose to employ the following ﬁne-tuning process after pruning neurons. Speciﬁcally, the server will send the pruned neural network back to clients and ask for training updates. Each client will train the pruned network using its local data again. Just as we did in the training process, the server will update the global model by aggregating clients’ weight updates. This process is called ﬁne-tuning. Fine-tuning will continue for a few rounds until the pruned model recover the lost test accuracy (with the validation dataset) as much as possible, that is, When the test accuracy does not improve any further or even starts to decrease, the server will stop the ﬁne-tuning process. In our experiments, it usually takes about ten rounds of updates. During this process, the attacker may also participate and try to push the backdoor attack success rate back. To mitigate the attack, we perform an adjusting extreme weights process after ﬁne-tuning. Empirical studies have shown that adjusting extreme weights can reduce the attack success rate back to a low level.
V. EXPERIMENTS
Our experiments are performed on MNIST [11], FashionMNIST [19] and CIFAR-10 [10] dataset with non-i.i.d. data distributions. We veriﬁed our defense method on backdoor tasks in different client data distribution, different backdoor tasks, different model architectures, and under different backdoor patterns. For experiments on MNIST, we use a model that

Accuracy Rate(%) Accuracy Rate(%)

1.0

0.8

0.6

0.4

3-label acc 3-label atk

5-label acc

0.2

5-label atk

7-label acc

0.0

7-label atk

0

5

10 15 20 25 30

Step(s)

Fig. 3. The training process of 3-label, 5-label and 7-label MNIST data distribution among 10 clients, tested on server’s evaluation dataset. Solid line stands for the test accuracy on evaluation dataset, while dashed line stands for attack success rate on target labels.

consists of 2 convolutional layers and 2 fully connected layers. For experiments on Fashion-MNIST, we use a model that consists of 3 convolutional layers and 2 fully connected layers. For experiments on CIFAR-10, we use a model that consists of 4 convolutional layers and 3 fully connected layers. Further more, we applied the state-of-the-art Distributed Backdoor Attack [20] on the CIFAR-10 experiments.

A. Client Data Distribution
Our experiments show that the data distribution among clients can signiﬁcantly inﬂuence the federated learning process. We deﬁne a K-label distribution among all the clients to simulate various real-world situations. K-label distribution means each client will be randomly assigned data belonging to K different labels. For example, if K = 10, each client will have randomly assigned data from all the 10 labels (MNIST dataset only has 10 different labels, from digit “0” to digit “9”). The data in each client follows the same uniform distribution, which means that every client will have roughly the same number of samples for each label. In another extreme case, if K = 1, each client will have data that belongs to a single label. Given the condition of 10 clients, in this case each client will hold all the data belonging to a unique label. In this extreme situation, the training process would take much longer than the 10-label distribution, and the backdoor attack would also be easier. An example of the models’ training performance under 3-label, 5-label, and 7-label distribution is in Figure 3. It shows the change of test accuracy of the model on the evaluation dataset and the shift in backdoor attack success rate along with the number of rounds of training (X-axis). Each point in the ﬁgure represents the performance of the global model at that round. There’s no defense method used in this process. By the end of the training, both test accuracy on the evaluation dataset and backdoor attack success rate is higher than 98%. We can notice that the training of 3-label distribution is much more challenging (needs more rounds) than the 7-label distribution. In contrast, the attack to 3-label distribution is much easier (needs fewer rounds) than the 7-label distribution. A balanced distribution of data among clients is better for training and more robust under attacks.

1.0

0.8

0.6

target 0 rank acc target 0 rank atk

0.4

target 0 vote acc target 0 vote atk

target 2 rank acc

0.2

target 2 rank atk

target 2 vote acc

0.0

target 2 vote atk

0

10

20

30

40

50

Pruned Number

Fig. 4. The neuron pruning process of models on MNIST dataset with 3-label distribution among 10 clients, tested on server’s evaluation dataset. “rank” stands for using ranking vote algorithm and “vote” for using majority vote algorithm. Solid line stands for test accuracy on server’s evaluation dataset, and dashed line stands for attack success rate on target labels.

B. Neuron Pruning Methods Comparison
In this paper, we proposed two algorithms to determine the pruning sequence of neurons: ranking vote and majority vote. Figure 4 shows their performance under different numbers of pruned neurons. There are 50 neurons in total. “target 0” means the attacker wants to backdoor digit 9 as digit 0, and “target 2” means they want to backdoor digit 9 as digit 2. As we can see that the pruning effect is almost identical between these two methods. More detailed experiment results will be reported in Table II in the Defense Method Evaluation section.
Another question that comes with this pruning process is when should we stop. As we can see in Figure 4, when attacker’s target label is 0, the attack success rate decreases after the test accuracy drops. Also, the server does not have backdoor images during testing. Hence, we can only decide the stop point based on the test performance. In this experiment, we determine the pruning stop point when the test accuracy drops over 1% between two testing points.

C. Extreme Value Threshold
After pruning neurons in the network, the last step is to limit extreme weight values in the same layer that we prune neurons. We calculate the mean µi and the standard deviation σi of all the weights in layer i. Then, we change all the weights that are beyond the threshold s = µi ± ∆ · σi to be zero. Figure 5 shows two examples that this pruning process can effectively mitigate backdoor attack without hurting the accuracy of the model on designed tasks. In one example, the attacker tries to backdoor digit 9 as digit 0. In the other example, the attacker tries to backdoor digit 9 as digit 2. When the attack success rate is high at the beginning of this process, pruning extreme values can greatly decrease the attack success rate with even large ∆ values. On the other hand, when the attack success rate is low (pruning neurons process already mitigate the backdoor attacks) at the beginning of this process, pruning extreme values will not increase the attack success rate. Typically, we can use the same stopping criteria as in the above pruning process. We can start the process with large ∆ value and gradually decrease ∆ value until the test accuracy

TABLE I

FEDERATED PRUNING PERFORMANCE UNDER DIFFERENT ATTACK PATTERNS

Attack Pattern Training Phase

Pruning Neurons

Adjusting Extreme Weights

pixels

test acc atk acc num test acc atk acc num test acc atk acc

1

98.5

99.9 22

97.3

98.7 131 97.2

0.4

3

98.5

100

30

96.6

100 139

97

34.8

5

98.4

100

34

97

100 138 95.2

1.4

7

98.5

100

30

96.2

96.9 138 96.8

32.9

9

98.4

99.8 30

97.6

2.3 133 96.2

0.5

1.0

Rate(%)

0.8

target 0 test acc

0.6

target 0 atk succ target 0 loss change

target 2 test acc

0.4

target 2 atk succ

target 2 loss change

0.2

0.0 0

5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 value

Fig. 5. The extreme value pruning process of model with different pruning threshold deﬁned by ∆. The ﬁrst data point in the graph, ∆ = 0 stands for the original model without pruning extreme values. The model is trained on MNIST dataset in 3-label distribution among 10 clients.

drops below a certain threshold or the loss rises above a certain threshold.
D. Backdoor Patterns
We conduct experiments to study the impact of backdoor patterns on the attack and our defense method. As shown in Figure 1, we implemented ﬁve different attack patterns with the backdoor task of changing the prediction results of digit 9 to digit 1 in the MNIST dataset. The experiment results of federated pruning performance under different attack patterns are shown in Table I. An example of number of pixels attack can be seen in Figure 1. test acc stands for the performance of model on the test dataset. atk acc stands for the performance of model on backdoor samples. num in pruning neurons stands for the number of neurons that are pruned during this process. num in adjusting extreme weights stands for the number of weights that are changed to zero in this process. The experiment data in this table is trained on a two layer convolutional neural network, with the ﬁrst layer containing 20 neurons and the second layer containing 50 neurons. The backdoor task is trying to backdoor digit 9 in MNIST to digit 1.
In the extreme weights adjusting process, we ﬁxed the threshold index ∆ = 3. Although we could achieve a much lower attack success rate in some patterns (3-pixels and 7pixels attack patterns) when using methods, as shown in Figure 5, by selecting the ∆ value that test accuracy begins to decrease. To maintain a fair comparison between different attack patterns, we use the ﬁxed ∆ value to conduct the comparison experiment. We can observe that the structure of the attack pattern has some impact on the ﬁnal attack

performance. In other words, some patterns are more resistant to the neuron pruning process, and some are more resistant to the adjusting extreme weights process.
E. Defense Method Evaluation
Next we show the experiment results of our pruning method under different circumstances. We conduct experiments on the MNIST dataset to show that both the neuron pruning process and the process of adjusting the extreme weights are all essential parts in our defense method. Further experiments on the ﬁne-tuning process are also reported here. Then, we test the whole defense procedure on the Fashion-MNIST dataset and present the promising results. In the end, we use the same defense procedure on the CIFAR-10 dataset under the state-of-the-art Distributed Backdoor Attack and prove the effectiveness of our method.
1) Pruning Neurons: According to the deﬁnition of our threat model, the goal of the attacker is to manipulate the prediction results of some inputs xi with backdoor patterns to another label. To be more speciﬁc, those inputs xi should be predicted as label yi = T without backdoor patterns. However, after adding the backdoor patterns to the original images, xi will be predicted as label yi = F by the compromised model. Under this attack, the backdoor patterns and the victim label T and target label F should all be determined by the attacker.
The work of Liu et al. [13] has shown that pruning dormant neurons can effectively mitigate the backdoor attack success rate. However, when we pruned neurons in federated learning scenarios, our experiments showed that the success rate of mitigating backdoor attacks depends on the victim label T and target label F selected by the attacker. Detailed experiment results of neurons pruning process on distributed MNIST dataset when attackers have different attack targets are represented in Table II. vic stands for the victim label that the attackers want to attack. atk stands for the target label that the attackers want the backdoor data being predicted. test acc stands for the performance of model on test dataset. atk acc stands for the performance of model on backdoor dataset. Backdoor dataset is composed of images that originally belong to vic label in the test dataset been added backdoor patterns. num stands for the number of neurons that are pruned during this process. There are 50 neurons in this convolutional layer.
With the Ranking Vote method, the backdoor attack success rates drop below 10% only in 5 out of 18 cases shown in II. With the Majority Vote method, the chance of successful defense is 38.9% (7 out of 18). These results show that pruning neurons only is not able to mitigate backdoor attacks effectively. Figure 4 shows that the attack success rate will not

Target vic atk 90 91 92 93 94 95 96 97 98 09 19 29 39 49 59 69 79 89

TABLE II

NEURONS PRUNING PROCESS WITH DIFFERENT ATTACK TARGETS

Training Phase

Pruning Neurons - Ranking Vote Pruning Neurons - Majority Vote

test acc atk acc num test acc

atk acc

num test acc

atk acc

98.5

100

35

96.2

100

40

95.7

99.9

98.2

100

37

96.2

0.4

39

93.8

0.6

98.6

100

36

96.5

0.1

37

95.8

0.2

98.6

99.6 32

96.2

6.7

34

96.1

2.3

98.7

100

34

94.7

100

35

95.2

0.5

98.4

99.9 39

95.7

99.9

39

95.7

99.9

98.1

99.8 33

95.6

100

32

96.6

99.9

98.8

99.9 37

96.7

98

37

96.1

99.2

98.2

99.7 36

96.2

99.5

36

95.5

99.7

98.4

100

40

95.3

4.1

39

96

3.4

98.7

99.7 38

95.9

99.9

38

96.4

0.3

98.4

99.8 32

97.5

100

30

97.2

100

97.2

99.8 30

90.6

100

32

95.8

99.3

97.9

100

37

94.1

100

38

93.6

100

98.6

100

34

96.6

0.8

34

96.7

0.7

98.6

99.9 36

97

99

33

96.8

100

98.8

99.7 36

96.9

97.6

35

97.2

98.4

98.3

100

34

96.3

97.8

37

95

99.5

TABLE III

EXPERIMENTS WITH ONLY ADJUSTING EXTREME WEIGHTS

Target

Adjusting Extreme Weights

vic atk num test acc (%) atk acc (%)

9 0 30

98.2

0.4

9 1 34

98.2

2.8

9 2 37

97.5

0.1

9 3 24

98.2

0.3

9 4 28

97.1

2.8

9 5 36

98.6

8

9 6 29

98.4

0.1

9 7 38

98.5

1.9

9 8 29

98.3

0

0 9 30

98.6

0.3

1 9 28

97.3

19.3

2 9 31

98.4

0

3 9 31

98.6

0

4 9 31

98.3

10.1

5 9 18

97.8

1.5

6 9 38

98.5

0.2

7 9 33

98.2

1.2

8 9 25

98.5

7.9

decrease until the test accuracy drops to an unacceptable level. This is because some neurons that support backdoor patterns may also perform an essential function in supporting benign inputs.
2) Adjusting Extreme Weights: Above we showed that pruning dormant neurons alone in federated learning is not enough to mitigate backdoor attacks. Next we show that adjusting extreme weights alone can mitigate backdoor attacks when the model structure is concise enough. Speciﬁcally, we are using a two-layer convolutional neural network with the ﬁrst layer containing 8 neurons and the second layer containing 16 neurons to train on MNIST. Table III represents experiment results of adjusting extreme weights process on distributed MNIST dataset when attackers have different attack targets. The success rate of backdoor attacks decrease from over 99% to an average of 3.2% while the accuracy of the model on test datasets remains almost the same. However, if we perform this process on a larger network (e.g.: twolayer convolutional network with the ﬁrst layer containing 20 neurons, and the second layer containing 50 neurons), the

attack success rate will not decrease. The reason could be that the backdoor training samples are leveraging lots of redundant neurons to reverse the correct prediction results while those backdoor neurons do not necessarily have extreme weights since they can dominate through numbers. So, pruning neurons is necessary when there are redundant neurons in the network that can be leveraged by the backdoor attacks.
3) Fine-tuning: Since the neuron pruning process could not eliminate all the backdoor neurons, we can further improve the model performance on a normal test dataset by ﬁne-tuning the pruned models through federated learning. We apply the same procedure in this step as we did in the previous training process. The only difference is that the model structure has been changed due to the neuron pruning process. During the federated ﬁne-tuning, the attack success rate will also increase along with the improvement of test accuracy on the testing dataset, since attackers also participate in this process. However, after we perform the process of adjusting extreme weights in the end, we can ﬁnd that the attack success rate will drop again to the same level as without the ﬁne-tuning process, while the test performance will be much higher. Experiments on the MNIST dataset have shown that without the ﬁne-tuning process, the average test accuracy on the validation dataset drops from 98.3% to 94/4%, and the average attack success rate drops from 99.7% to 8.4%. With the ﬁne-tuning process, the average test accuracy on the validation dataset only drops to 96.9%, and the average attack success rate drops to 4.7%. Detailed experiment results of the comparison between with and without ﬁne-tuning process are reported in Table IV. vic label stands for the victim label that the attackers want to attack. atk label stands for the target label that the attackers want the backdoor data being predicted. test acc stands for the performance of model on test dataset. atk acc stands for the performance of model on backdoor dataset. Backdoor dataset is composed of images that originally belong to vic label in the test dataset been added backdoor patterns.
4) Experiments on Fashion-MNIST: Experiment results of the training process, neuron pruning process, extreme weights

Attacker Target

vic label atk label

9

0

9

1

9

2

9

3

9

4

9

5

9

6

9

7

9

8

0

9

1

9

2

9

3

9

4

9

5

9

6

9

7

9

8

9

TABLE IV

EXPERIMENTS OF FINE-TUNING PROCESS ON MNIST DATASET

Training Phase

Pruning without Fine-tuning Pruning with Fine-tuning

test acc (%) atk acc (%) test acc (%) atk acc (%) test acc (%) atk acc (%)

98.2

99.7

94.7

0.3

96.6

0.8

98.6

100

95.1

0.4

96.9

0.1

98.2

99.9

95.7

4.1

97.2

2.5

98.3

99.6

95.6

36.9

97.2

8.8

98.5

100

97.2

5

98.1

3.5

98.5

99.9

86.6

14

94.6

2.8

98.6

99.7

96.4

0.1

97.5

0.1

98.9

99.8

94.1

0.8

98.2

0.5

98.5

99.9

93.9

4.5

96.6

3.1

97.8

99.8

95.4

0

96.9

0.9

98.2

99.5

94.5

0.1

97.2

0.5

98.3

100

94.9

1.2

97.5

0.5

98.5

99.5

95

34.9

96

26.3

98.5

99.9

93.4

27.1

96.4

14.7

98.5

99.8

95.2

1.3

97.5

1.6

98.3

99.3

94.8

1.5

97.3

0.3

97.9

99.1

93.8

18.5

96.9

16.4

97.5

99.4

93.3

1.2

96.3

0.5

Target vic atk 90 91 92 93 94 95 96 97 98

Training test acc
88.8 88.7 87.8 87.5 87.8 88.6 86.3 88.5 88.8

Phase atk acc 99.8 99.4 99.8 99.6 99.7 99.7 99.8 99.7 99.9

TABLE V

EXPERIMENTS ON FASHION-MNIST DATASET

Pruning Neurons Adjusting Extreme Weights

test acc atk acc test acc

atk acc

83.2

2.8

82.9

2.1

82.6

6.5

82.1

0

82.2

2.7

82.1

3.1

84.4

0.3

84.4

0

81.0

2.9

80.6

0

81.2

11.9

80.6

3.6

82.8

93.6

82.0

0.2

82.9

4.3

83.1

4.6

84.7

87.7

85.0

3.2

Defense with Fine-tuning

test acc

atk acc

86.3

9.0

87.0

0

85.6

12.6

86.2

0.4

85.8

2.3

86.1

10.4

86.0

3.1

87.0

17.1

87.2

2.9

0 Original

0 1st Attacker

0 2nd Attacker

0 3rd Attacker

0 4th Attacker

0 Attack Evaluation

10

10

10

10

10

10

20

20

20

20

20

20

30

0

20

30

0

20

30

0

20

30

0

20

30

0

20

30

0

20

Fig. 6. An original image from the CIFAR-10 dataset, followed by four different attackers each provides a portion of the attack pattern and the ﬁnal attack evaluation will use the full attack pattern in the last image.

adjusting process and pruning with ﬁne-tuning process on distributed Fashion-MNIST dataset are shown in Table V. The backdoor pattern used in this experiment is single-pixel backdoor pattern. We have a total of 10 clients and use a 3-label data distribution among all the clients. There is one attacker among the clients. The global model has three convolutional layers and another two fully connected layers. We tested our defense method under different attacking targets selected by the attacker. According to the result, we can also notice that the neuron pruning process can only mitigate backdoor attacks targeting certain labels, but the following adjusting extreme weights can mitigate all backdoor attacks targeting any labels. On the other hand, the ﬁne-tuning process can signiﬁcantly improve the model performance on the main task, although it may also beneﬁt the attacker since the attacker is also among the group of clients participating in the ﬁne-tuning. Without a ﬁne-tuning process, the average attack success rate drops from 99.7% to 1.9%, and the test accuracy on the validation dataset drops from 88.1% to 82.5%. With the ﬁne-tuning process,

the test accuracy on the validation dataset rises up to 86.4%, although the average attack success rate also increases from 1.9% to 6.4%, as a tradeoff between performance and security.
5) Experiments on CIFAR-10: On CIFAR-10 dataset, we use the same attacking strategies as provided in the Distributed Backdoor Attack [20]. The attack decomposes a global backdoor pattern into separate local patterns and embed them into the training set of different attackers respectively, as shown in Figure 6. Experiment results are shown in Table VI. We have a total of 10 clients and use a 3-label data distribution among all the clients. There are four attackers among the clients. Despite of different attacking strategies, the experiment results are very similar to what we have observed in the experiments on Fashion-MNIST. The neuron pruning process can mitigate some but not all backdoor attacks targeting different labels. The following adjusting extreme weights can further mitigate all backdoor attacks targeting different labels. In average, the attack success rate drops from 87.6% to 13.0% and the test accuracy on the validation dataset drops from 72.4% to 71.0%.

target vic atk 90 91 92 93 94 95 96 97 98

TABLE VI

EXPERIMENTS ON CIFAR-10 DATASET UNDER DISTRIBUTED BACKDOOR ATTACK

training phase

pruning neurons adjusting extreme weights defense with ﬁne-tuning

test acc atk acc test acc atk acc test acc

atk acc

test acc

atk acc

73.0

88.5

72.7

59.9

71.6

21.2

71.8

39.6

71.6

87.0

71.0

87.6

70.9

18.1

71.1

19.2

72.9

84.0

71.7

73.3

70.4

26.6

71.6

29.6

72.7

87.7

71.4

12.0

71.3

13.1

71.0

45.8

72.9

85.3

71.9

3.3

70.5

4.1

71.5

49.1

72.8

85.5

72.4

4.4

71.8

4.8

71.5

46.9

71.0

91.3

71.7

82.6

70.5

7.9

72.2

5.8

72.1

91.2

72.2

85.7

71.3

8.5

71.5

8.8

72.6

88.2

72.4

10.8

71.2

12.8

71.5

49.9

Rate(%) Accuracy Rate(%)

1.0

0.8

5-clients acc 10-clients acc

15-clients acc

0.6

20-clients acc 25-clients acc

5-clients atk

0.4

10-clients atk

15-clients atk

0.2

20-clients atk 25-clients atk

0.0 0

5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 value

Fig. 7. The test accuracy of model on validation dataset and backdoor attack success rate of model during the adjusting extreme values phase with different number of clients selected in each round in the training process. The model is trained on MNIST dataset in 3-label distribution among 50 clients.

With ﬁne-tuning process, the average attack success rate drops 54.9% while the average test accuracy only drops 0.9%.
F. Randomly Selected Clients Evaluation
Previous experiments are all tested with 10 clients and each client participate in every round of training process. This is a simpliﬁed process compared with original federated learning deﬁnition [14] as we discussed before. In this section, we perform experiments on a more realistic federated learning scenario with 50 clients and 10% of them (5 clients) are attackers. We randomly select different number of clients (5, 10, 15, 20 and 25 clients, respectively) in each experiment and observe the performance of our defense method. We use the MNIST dataset under 3-label distribution for this comparison experiments. From Fig 7, we can see that the performance of the model behave very similar to each other, although they are trained with randomly selected different number of clients. With reasonable number of pruned neurons and ∆ values (same settings as previous experiments), the attack success rate can still be greatly mitigated using our defense method.
VI. DISCUSSION
A. Regularization Method
The extreme weights adjusting method tries to eliminate those extreme values in the network weights. This method is very similar to the idea of regularization. However, we ﬁnd that directly applying the L2 regularization penalty of all layers on the loss function will potentially degrade the model’s performance on the test dataset. Instead, using the L2

1.0

0.8

0.6

0.4

= 0.2 acc = 0.2 atk

= 0.3 acc

0.2

= 0.3 atk

= 0.4 acc

0.0

= 0.4 atk

0

5

10 15 20 25 30

Step(s)

Fig. 8. The training process of model with different regularization coefﬁcient λ in the last convolutional layer. The model is trained on MNIST dataset in 3-label distribution among 10 clients. The training processes are identical with the same data distribution, 30 epochs of training time and the same attacker’s ampliﬁcation update coefﬁcient α = 3. Solid line stands for the accuracy rate and dotted line stands for the attack success rate.

regularization penalty only on the last convolutional layer can increase the network’s robustness against a backdoor attack. Related experiment results can be seen on Figure 8. After adding the regularization method in the last convolutional layer, it becomes much more challenging for the attackers to compromise the global model with a sufﬁcient large regularization factor. However, such robustness of the model comes with a loss of performance on the designed tasks (test accuracy on the testing dataset). So, there is a trade-off between the robustness against backdoor attacks and the performance of the model. A larger regularization coefﬁcient provides more robustness but will suffer from a loss in the test performance.

B. Model Architectures
The neuron pruning method tries to simplify the model architectures and ensure that every remaining neuron is essential to the designed task (measured by the test accuracy on the testing dataset). Experiments (in Table III) have shown that when the model is concise/simple enough, we can even skip the neuron pruning step, while simply adjusting extreme weights in the last convolutional layer can mitigate backdoor attacks. However, it is almost impossible to design a network architecture that perfectly matches the least requirement of the complexity with an unseen dataset. Simple architectures may cause the model to suffer from large bias errors, while complex architectures will make the model more vulnerable to backdoor attacks. That is why we introduced the neuron pruning process to leverage the information from all clients

to prune as many unnecessary neurons as possible without hurting the model’s performance on its designed tasks.
C. Possible Attacks
There is and will always be an arms race between attackers and defenders. In this section, we will brieﬂy discuss some possible attacks on our defense mechanism. Firstly, we consider two different attacks on the federated pruning process. Recall that the assumption of the backdoor neurons is that the backdoor attacks will trigger those neurons that are typically not being activated by the normal input. Naturally, we can form two kinds of attacks based on this assumption. The ﬁrst one will try to attack the neurons ranking algorithm, such that the backdoor neurons ranked higher than those essential neurons used by normal input. So, the backdoor neurons will not be removed until the test accuracy on the validation dataset drops below a certain threshold. The second attack method will try not to generate such backdoor neurons and use the essential neurons being used by the other normal input to trigger the backdoor pattern, which is called “pruning-aware attack” [13]. To achieve this, we assume the attacker could obtain the ﬁnal pruning mask created by all the clients (which is nearly impossible in real cases). The attacker would then use this pruning mask in the local training process to avoid using the pruned neurons. Moreover, the attacker can enforce the backdoor neurons are the same as the essential neurons used by other normal inputs. We empirically studied the inﬂuence of these two attacks on our federated pruning algorithm. Such attacks nearly do not inﬂuence the defense results (attack success rate will still signiﬁcantly decrease). The reason could be that the attackers still take the minority group (10%) in our federated system settings. Besides, by using ranking numbers or voting masks in our pruning algorithm, the inﬂuence of some updates of manipulated rankings is trivial. What is more, the pruning extreme values process will further mitigate the inﬂuence of “pruning-aware attack” as we have shown in the previous experiments.
Secondly, we consider attacks to pruning extreme values. If we assume the attackers are aware of the pruning extreme values process, they could self-prune extreme values during the training process. In this way, when the server prunes extreme values, it will not affect the backdoor attack success rate since the backdoor attacks will not rely on extreme values. However, in our experiments, we ﬁnd that the federated pruning process will signiﬁcantly mitigate such attacks. The reason could be that by limiting extreme values during the training process, the backdoor tasks will have to leverage the dormant neurons to launch backdoor attacks. At the same time, our federated pruning process will restrain the use of dormant neurons. Thus, our defense method is robust under these attacks.
VII. CONCLUSION
We proposed a new method to mitigate backdoor attacks in federated learning. We can simplify the model architectures through the federated neuron pruning process while maintaining good performance of the model on designed tasks.

Then, adjusting extreme weights in the simpliﬁed model can
effectively degrade the success rate of backdoor tasks. We
evaluated our method with different attack settings (attack tar-
gets/labels, backdoor patterns, data distribution among clients,
and datasets). Our experiments also showed that the feder-
ated ﬁne-tuning process after pruning neurons could further
improve the model performance on designed tasks without
hurting the defending method’s performance after adjusting
the extreme values process.
REFERENCES
[1] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov. How to backdoor federated learning. CoRR, abs/1807.00459, 2018.
[2] G. Baruch, M. Baruch, and Y. Goldberg. A little is enough: Circumventing defenses for distributed learning. In NeurIPS, 2019.
[3] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634–643. PMLR, 2019.
[4] P. Blanchard, R. Guerraoui, J. Stainer, et al. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119–129, 2017.
[5] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In CCS, 2017.
[6] M. Fang, X. Cao, J. Jia, and N. Z. Gong. Local model poisoning attacks to byzantine-robust federated learning. CoRR, abs/1911.11815, 2019.
[7] M. Fang, X. Cao, J. Jia, and N. Z. Gong. Local model poisoning attacks to byzantine-robust federated learning. In USENIX. USENIX Association, 2020.
[8] C. Fung, C. J. M. Yoon, and I. Beschastnikh. Mitigating sybils in federated learning poisoning. CoRR, abs/1808.04866, 2018.
[9] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. CoRR, abs/1708.06733, 2017.
[10] Krizhevsky, Alex, Hinton, Geoffrey, et al. Learning multiple layers of features from tiny images. 2009.
[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278– 2324, 1998.
[12] S. Li, Y. Cheng, W. Wang, Y. Liu, and T. Chen. Learning to detect malicious clients for robust federated learning. CoRR, abs/2002.00211, 2020.
[13] K. Liu, B. Dolan-Gavitt, and S. Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In RAID, pages 273–294. Springer, 2018.
[14] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics. PMLR, 2017.
[15] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault. The hidden vulnerability of distributed learning in byzantium. arXiv preprint arXiv:1802.07927, 2018.
[16] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault. The hidden vulnerability of distributed learning in byzantium. In ICML. PMLR, 2018.
[17] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan. Can you really backdoor federated learning? CoRR, abs/1911.07963, 2019.
[18] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In SSP. IEEE, 2019.
[19] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017.
[20] C. Xie, K. Huang, P. Chen, and B. Li. DBA: distributed backdoor attacks against federated learning. In ICLR, 2020.
[21] C. Xie, O. Koyejo, and I. Gupta. Fall of empires: Breaking byzantinetolerant SGD by inner product manipulation. In Uncertainty in Artiﬁcial Intelligence, PMLR, 2020.
[22] D. Yin, Y. Chen, K. Ramchandran, and P. L. Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In ICML. PMLR, 2018.

