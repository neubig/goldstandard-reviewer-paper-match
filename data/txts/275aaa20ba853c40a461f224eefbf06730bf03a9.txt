Escape saddle points by a simple gradient-descent based algorithm

arXiv:2111.14069v1 [math.OC] 28 Nov 2021

Chenyi Zhang1 Tongyang Li2,3,4∗ 1 Institute for Interdisciplinary Information Sciences, Tsinghua University, China
2 Center on Frontiers of Computing Studies, Peking University, China 3 School of Computer Science, Peking University, China
4 Center for Theoretical Physics, Massachusetts Institute of Technology, USA

Abstract
Escaping saddle points is a central research topic in nonconvex optimization. In this paper, we propose a simple gradient-based algorithm such that for a smooth function f : Rn → R, it outputs an -approximate second-order stationary point in O˜(log n/ 1.75) iterations. Compared to the previous state-of-the-art algorithms by Jin et al. with O˜(log4 n/ 2) or O˜(log6 n/ 1.75) iterations, our algorithm is polynomially better in terms of log n and matches their complexities in terms of 1/ . For the stochastic setting, our algorithm outputs an -approximate second-order stationary point in O˜(log2 n/ 4) iterations. Technically, our main contribution is an idea of implementing a robust Hessian power method using only gradients, which can ﬁnd negative curvature near saddle points and achieve the polynomial speedup in log n compared to the perturbed gradient descent methods. Finally, we also perform numerical experiments that support our results.

1 Introduction

Nonconvex optimization is a central research area in optimization theory, since lots of modern machine learning problems can be formulated in models with nonconvex loss functions, including deep neural networks, principal component analysis, tensor decomposition, etc. In general, ﬁnding a global minimum of a nonconvex function is NP-hard in the worst case. Instead, many theoretical works focus on ﬁnding a local minimum instead of a global one, because recent works (both empirical and theoretical) suggested that local minima are nearly as good as global minima for a signiﬁcant amount of well-studied machine learning problems; see e.g. [4, 11, 13, 14, 16, 17]. On the other hand, saddle points are major obstacles for solving these problems, not only because they are ubiquitous in high-dimensional settings where the directions for escaping may be few (see e.g. [5, 7, 10]), but also saddle points can correspond to highly suboptimal solutions (see e.g. [18, 27]).

Hence, one of the most important topics in nonconvex optimization is to escape saddle points. Speciﬁcally, we consider a twice-differentiable function f : Rn → R such that

• f is -smooth: ∇f (x1) − ∇f (x2) ≤ x1 − x2 ∀x1, x2 ∈ Rn, • f is ρ-Hessian Lipschitz: H(x1) − H(x2) ≤ ρ x1 − x2 ∀x1, x2 ∈ Rn;

here H is the Hessian of f . The goal is to ﬁnd an -approximate second-order stationary point x :1

√

∇f (x ) ≤ , λmin(H(x )) ≥ − ρ .

(1)

∗Corresponding author. Email: tongyangli@pku.edu.cn 1We can ask for an ( 1, 2)-approx. second-order stationary point s.t. ∇f (x) ≤ 1 and λmin(∇2f (x)) ≥ − 2 in general. The scaling in (1) was adopted as a standard in literature [1, 6, 9, 19, 20, 21, 25, 28, 29, 30].

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

In other words, at any -approx. second-order stationary point x , the gradient is small with n√orm being at most and the Hessian is close to be positive semi-deﬁnite with all its eigenvalues ≥ − ρ .
Algorithms for escaping saddle points are mainly evaluated from two aspects. On the one hand, considering the enormous dimensions of machine learning models in practice, dimension-free or almost dimension-free (i.e., having poly(log n) dependence) algorithms are highly preferred. On the other hand, recent empirical discoveries in machine learning suggests that it is often feasible to tackle difﬁcult real-world problems using simple algorithms, which can be implemented and maintained more easily in practice. On the contrary, algorithms with nested loops often suffer from signiﬁcant overheads in large scales, or introduce concerns with the setting of hyperparameters and numerical stability (see e.g. [1, 6]), making them relatively hard to ﬁnd practical implementations.
It is then natural to explore simple gradient-based algorithms for escaping from saddle points. The reason we do not assume access to Hessians is because its construction takes Ω(n2) cost in general, which is computationally infeasible when the dimension is large. A seminal work along this line was by Ge et al. [11], which found an -approximate second-order stationary point satisfying (1) using only gradients in O(poly(n, 1/ )) iterations. This is later improved to be almost dimension-free O˜(log4 n/ 2) in the follow-up work [19],2 and the perturbed accelerated gradient descent algorithm [21] based on Nesterov’s accelerated gradient descent [26] takes O˜(log6 n/ 1.75) iterations. However, these results still suffer from a signiﬁcant overhead in terms of log n. On the other direction, Refs. [3, 24, 29] demonstrate that an -approximate second-order stationary point can be ﬁnd using gradients in O˜(log n/ 1.75) iterations. Their results are based on previous works [1, 6] using Hessian-vector products and the observation that the Hessian-vector product can be approximated via the difference of two gradient queries. Hence, their implementations contain nested-loop structures with relatively large numbers of hyperparameters. It has been an open question whether it is possible to keep both the merits of using only ﬁrst-order information as well as being close to dimension-free using a simple, gradient-based algorithm without a nested-loop structure [22]. This paper answers this question in the afﬁrmative.
Contributions. Our main contribution is a simple, single-loop, and robust gradient-based algorithm that can ﬁnd an -approximate second-order stationary point of a smooth, Hessian Lipschitz function f : Rn → R. Compared to previous works [3, 24, 29] exploiting the idea of gradient-based Hessian power method, our algorithm has a single-looped, simpler structure and better numerical stability. Compared to the previous state-of-the-art results with single-looped structures by [21] and [19, 20] using O˜(log6 n/ 1.75) or O˜(log4 n/ 2) iterations, our algorithm achieves a polynomial speedup in log n:
Theorem 1 (informal). Our single-looped algorithm ﬁnds an -approximate second-order stationary point in O˜(log n/ 1.75) iterations.
Technically, our work is inspired by the perturbed gradient descent (PGD) algorithm in [19, 20] and the perturbed accelerated gradient descent (PAGD) algorithm in [21]. Speciﬁcally, PGD applies gradient descents iteratively until it reaches a point with small gradient, which can be a potential saddle point. Then PGD generates a uniform perturbation in a small ball centered at that point and then continues the GD procedure. It is demonstrated that, with an appropriate choice of the perturbation radius, PGD can shake the point off from the neighborhood of the saddle point and converge to a second-order stationary point with high probability. The PAGD in [21] adopts a similar perturbation idea, but the GD is replaced by Nesterov’s AGD [26].
Our algorithm is built upon PGD and PAGD but with one main modiﬁcation regarding the perturbation idea: it is more efﬁcient to add a perturbation in the negative curvature direction nearby the saddle point, rather than the uniform perturbation in PGD and PAGD, which is a compromise since we generally cannot access the Hessian at the saddle due to its high computational cost. Our key observation lies in the fact that we do not have to compute the entire Hessian to detect the negative curvature. Instead, in a small neighborhood of a saddle point, gradients can be viewed as Hessianvector products plus some bounded deviation. In particular, GD near the saddle with learning rate 1/ is approximately the same as the power method of the matrix (I − H/ ). As a result, the most negative eigenvalues stand out in GD because they have leading exponents in the power method, and thus it approximately moves along the direction of the most negative curvature nearby the sad-
2The O˜ notation omits poly-logarithmic terms, i.e., O˜(g) = O(g poly(log g)).
2

dle point. Following this approach, we can escape the saddle points more rapidly than previous algorithms: for a constant , PGD and PAGD take O(log n) iterations to decrease the function value by Ω(1/ log3 n) and Ω(1/ log5 n) with high probability, respectively; on the contrary, we can ﬁrst take O(log n) iterations to specify a negative curvature direction, and then add a larger perturbation in this direction to decrease the function value by Ω(1). See Proposition 3 and Proposition 5. After escaping the saddle point, similar to PGD and PAGD, we switch back to GD and AGD iterations, which are efﬁcient to decrease the function value when the gradient is large [19, 20, 21].
Our algorithm is also applicable to the stochastic setting where we can only access stochastic gradients, and the stochasticity is not under the control of our algorithm. We further assume that the stochastic gradients are Lipschitz (or equivalently, the underlying functions are gradient-Lipschitz, see Assumption 2), which is also adopted in most of the existing works; see e.g. [8, 19, 20, 34]. We demonstrate that a simple extended version of our algorithm takes O(log2 n) iterations to detect a negative curvature direction using only stochastic gradients, and then obtain an Ω(1) function value decrease with high probability. On the contrary, the perturbed stochastic gradient descent (PSGD) algorithm in [19, 20], the stochastic version of PGD, takes O(log10 n) iterations to decrease the function value by Ω(1/ log5 n) with high probability.
Theorem 2 (informal). In the stochastic setting, our algorithm ﬁnds an -approximate second-order stationary point using O˜(log2 n/ 4) iterations via stochastic gradients.
Our results are summarized in Table 1. Although the underlying dynamics in [3, 24, 29] and our algorithm have similarity, the main focus of our work is different. Speciﬁcally, Refs. [3, 24, 29] mainly aim at using novel techniques to reduce the iteration complexity for ﬁnding a second-order stationary point, whereas our work mainly focuses on reducing the number of loops and hyper-parameters of negative curvature ﬁnding methods while preserving their advantage in iteration complexity, since a much simpler structure accords with empirical observations and enables wider applications. Moreover, the choice of perturbation in [3] is based on the Chebyshev approximation theory, which may require additional nested-looped structures to boost the success probability. In the stochastic setting, there are also other results studying nonconvex optimization [15, 23, 31, 36, 12, 32, 35] from different perspectives than escaping saddle points, which are incomparable to our results.

Setting Non-stochastic Non-stochastic Non-stochastic Non-stochastic Non-stochastic
Stochastic Stochastic Stochastic Stochastic Stochastic

Reference [1, 6]
[19, 20] [21]
[3, 24, 29] this work [19, 20]
[9] [3] [8] this work

Oracle Hessian-vector product
Gradient Gradient Gradient Gradient Gradient Gradient Gradient Gradient Gradient

Iterations O˜(log n/ 1.75) O˜(log4 n/ 2) O˜(log6 n/ 1.75) O˜(log n/ 1.75) O˜(log n/ 1.75) O˜(log15 n/ 4) O˜(log5 n/ 3.5) O˜(log2 n/ 3.5) O˜(log2 n/ 3) O˜(log2 n/ 4)

Simplicity Nested-loop Single-loop Single-loop Nested-loop Single-loop Single-loop Single-loop Nested-loop Nested-loop Single-loop

Table 1: A summary of the state-of-the-art results on ﬁnding approximate second-order stationary points by the ﬁrst-order (gradient) oracle. Iteration numbers are highlighted in terms of the dimension n and the precision .

It is worth highlighting that our gradient-descent based algorithm enjoys the following nice features:
• Simplicity: Some of the previous algorithms have nested-loop structures with the concern of practical impact when setting the hyperparameters. In contrast, our algorithm based on negative curvature ﬁnding only contains a single loop with two components: gradient descent (including AGD or SGD) and perturbation. As mentioned above, such simple structure is preferred in machine learning, which increases the possibility of our algorithm to ﬁnd real-world applications.
• Numerical stability: Our algorithm contains an additional renormalization step at each iteration when escaping from saddle points. Although in theoretical aspect a renormalization step does not affect the output and the complexity of our algorithm, when ﬁnding negative curvature near

3

saddle points it enables us to sample gradients in a larger region, which makes our algorithm more numerically stable against ﬂoating point error and other errors. The introduction of renormalization step is enabled by the simple structure of our algorithm, which may not be feasible for more complicated algorithms [3, 24, 29].
• Robustness: Our algorithm is robust against adversarial attacks when evaluating the value of the gradient. Speciﬁcally, when analyzing the performance of our algorithm near saddle points, we essentially view the deﬂation from pure quadratic geometry as an external noise. Hence, the effectiveness of our algorithm is unaffected under external attacks as long as the adversary is bounded by deﬂations from quadratic landscape.
Finally, we perform numerical experiments that support our polynomial speedup in log n. We perform our negative curvature ﬁnding algorithms using GD or SGD in various landscapes and general classes of nonconvex functions, and use comparative studies to show that our Algorithm 1 and Algorithm 3 achieve a higher probability of escaping saddle points using much fewer iterations than PGD and PSGD (typically less than 1/3 times of the iteration number of PGD and 1/2 times of the iteration number of PSGD, respectively). Moreover, we perform numerical experiments benchmarking the solution quality and iteration complexity of our algorithm against accelerated methods. Compared to PAGD [21] and even advanced optimization algorithms such as NEON+ [29], Algorithm 2 possesses better solution quality and iteration complexity in various landscapes given by more general nonconvex functions. With fewer iterations compared to PAGD and NEON+ (typically less than 1/3 times of the iteration number of PAGD and 1/2 times of the iteration number of NEON+, respectively), our Algorithm 2 achieves a higher probability of escaping from saddle points.
Open questions. This work leaves a couple of natural open questions for future investigation:
• Can we achieve the polynomial speedup in log n for more advanced stochastic optimization algorithms with complexity O˜(poly(log n)/ 3.5) [2, 3, 9, 28, 30] or O˜(poly(log n)/ 3) [8, 33]?
• How is the performance of our algorithms for escaping saddle points in real-world applications, such as tensor decomposition [11, 16], matrix completion [13], etc.?
Broader impact. This work focuses on the theory of nonconvex optimization, and as far as we see, we do not anticipate its potential negative societal impact. Nevertheless, it might have a positive impact for researchers who are interested in understanding the theoretical underpinnings of (stochastic) gradient descent methods for machine learning applications.
Organization. In Section 2, we introduce our gradient-based Hessian power method algorithm for negative curvature ﬁnding, and present how our algorithms provide polynomial speedup in log n for both PGD and PAGD. In Section 3, we present the stochastic version of our negative curvature ﬁnding algorithm using stochastic gradients and demonstrate its polynomial speedup in log n for PSGD. Numerical experiments are presented in Section 4. We provide detailed proofs and additional numerical experiments in the supplementary material.
2 A simple algorithm for negative curvature ﬁnding
We show how to ﬁnd negative curvature near a saddle point using a gradient-based Hessian power method algorithm, and extend it to a version with faster convergence rate by replacing gradient descents by accelerated gradient descents. The intuition works as follows: in a small enough region nearby a saddle point, the gradient can be approximately expressed as a Hessian-vector product formula, and the approximation error can be efﬁciently upper bounded, see Eq. (6). Hence, using only gradients information, we can implement an accurate enough Hessian power method to ﬁnd negative eigenvectors of the Hessian matrix, and further ﬁnd the negative curvature nearby the saddle.
2.1 Negative curvature ﬁnding based on gradient descents
We ﬁrst present an algorithm for negati√ve curvature ﬁnding based on gradient descents. Spec√iﬁcally, for any x˜ ∈ Rn with λmin(H(x˜)) ≤ − ρ , it ﬁnds a unit vector eˆ such that eˆT H(x˜)eˆ ≤ − ρ /4.
4

Algorithm 1: Negative Curvature Finding(x˜, r, T ).

1 y0 ←Uniform(Bx˜(r)) where Bx˜(r) is the 2-norm ball centered at x˜ with radius r;

2 for t = 1, ..., T do

3

yt ← yt−1 −

yt−1 r

∇f (x˜ + ryt−1/ yt−1 ) − ∇f (x˜) ;

4 Output yT /r.

Proposition 3. Suppose the function f : Rn → R is -smooth and ρ-Hessian Lipschitz. For any 0 < δ0 ≤ 1, we specify our choice of parameters and constants we use as follows:

8

n

π

T = √ρ · log δ0 πρ , r = 8 n δ0. (2)

Suppose that x˜ satisﬁes λmin(∇2f (x˜)) ≤ −√ρ . Then with probability at least 1 − δ0, Algorithm 1

outputs a unit vector eˆ satisfying

eˆT H(x)eˆ

≤

√ −ρ

/4,

(3)

using O(T ) = O˜ l√ogρn iterations, where H stands for the Hessian matrix of function f .

Proof. Without loss of generality we assume x˜ = 0 by shifting Rn such that x˜ is mapped to 0. Deﬁne a new n-dimensional function

hf (x) := f (x) − ∇f (0), x ,

(4)

for the ease of our analysis. Since ∇f (0), x is a linear function with Hessian being 0, the Hessian of hf equals to the Hessian of f , and hf (x) is also -smooth and ρ-Hessian Lipschitz. In addition, note that ∇hf (0) = ∇f (0) − ∇f (0) = 0. Then for all x ∈ Rn,

1

1

∇hf (x) = H(ξx) · x dξ = H(0)x + (H(ξx) − H(0)) · x dξ.

(5)

ξ=0

ξ=0

Furthermore, due to the ρ-Hessian Lipschitz condition of both f and hf , for any ξ ∈ [0, 1] we have H(ξx) − H(0) ≤ ρ x , which leads to

∇hf (x) − H(0)x ≤ ρ x 2.

(6)

Observe that the Hessian matrix H(0) admits the following eigen-decomposition:

n

H(0) = λiuiuTi ,

(7)

i=1

where the set {ui}ni=1 forms an orthonormal basis of Rn. Without loss of generality, we assume the eigenvalues λ1, λ2, . . . , λn corresponding to u1, u2, . . . , un satisfy

λ1 ≤ λ2 ≤ · · · ≤ λn,

(8)

√

√

in which λ1 ≤ − ρ . I√f λn ≤ − ρ /2, Proposition 3 holds directly. Hence, we only need to prove

the case where λn > − ρ /2, in which there exists some p > 1, p > 1 with

√

√

λp ≤ − ρ ≤ λp+1, λp ≤ − ρ /2 < λp +1.

(9)

We use S , S⊥ to separately denote the subspace of Rn spanned by {u1, u2, . . . , up}, {up+1, up+2, . . . , un}, and use S , S⊥ to denote the subspace of Rn spanned by
{u1, u2, . . . , up }, {up +1, up+2, . . . , un}. Furthermore, we deﬁne

p

n

yt, :=

ui, yt ui,

yt,⊥ :=

ui, yt ui;

(10)

i=1

i=p

p

n

yt, :=

ui, yt ui, yt,⊥ :=

ui, yt ui

(11)

i=1

i=p

5

respectively to denote the component of yt in Line 3 in the subspaces S , S⊥, S , S⊥, and let αt := yt, / yt . Observe that

Pr α0 ≥ δ0 π/n ≥ Pr |y0,1|/r ≥ δ0 π/n ,

(12)

where y0,1 := u1, y0 denotes the component of y0 along u1. Consider the case where α0 ≥ δ0 π/n, which can be achieved with probability

π

π Vol(Bn0 −1(1))

π

n

Pr α0 ≥ n δ0 ≥ 1 − n δ0 · Vol(Bn(1)) ≥ 1 − n δ0 · π = 1 − δ0. (13)

0

We prove that there exists some t0 with 1 ≤ t0 ≤ T such that √
yt0,⊥ / yt0 ≤ ρ /(8 ).

Assume the contrary, for any 1 ≤ t ≤ T , we all have yt,⊥ / yt

satisﬁes the following recurrence formula:

√

√

yt+1,⊥ ≤ (1 + ρ /(2 )) yt,⊥ + ∆⊥ ≤ (1 + ρ /(2 ) +

> √ρ /(8 ). ∆ / yt,⊥ )

Then yt,⊥

(14) yt,⊥
, (15)

where

∆ := yt (∇hf (ryt/ yt ) − H(0) · (ryt/ yt )) r

stands for the deviation from pure quadratic approximation and ∆ / yt

Hence,

yt+1,⊥

√ρ ∆

≤ 1+ +

2

yt,⊥

√ρ 8ρr yt,⊥ ≤ 1 + 2 + · √ρ

(16) ≤ ρr/ due to (6).
yt+1,⊥ , (17)

which leads to

yt,⊥

≤

y0,⊥

(1

+

√ ρ

/(2

)

+

√ 8ρr/ ρ

)t

≤

y0,⊥

(1

+

√ 5ρ

/(8

))t,

∀t ∈ [T ].

(18)

Similarly, we can have the recurrence formula for yt, :

√

√

yt+1, ≥ (1 + ρ /(2 )) yt, − ∆ ≥ (1 + ρ /(2 ) − ∆ /(αt yt )) yt, . (19)

Considering that ∆ / yt ≤ ρr/ due to (6), we can further have

√

yt+1, ≥ (1 + ρ /(2 ) − ρr/(αt )) yt, .

(20)

Intuitively, yt, should have a faster increasing rate than yt,⊥ in this gradient-based Hessian power method, ignoring the deviation from quadratic approximation. As a result, the value the value
αt = yt, / yt should be non-decreasing. It is demonstrated in Lemma 18 in Appendix B that, even if we count this deviation in, αt can still be lower bounded by some constant αmin:

αt ≥ αmin = δ0 π , ∀1 ≤ t ≤ T . (21) 4n

by which we can further deduce that

yt,

≥

y0,

√ (1 + ρ /

− ρr/(αmin

))t ≥

y0,

(1

+

√ 7ρ

/(8

))t,

∀1 ≤ t ≤ T . (22)

Observe that

yT ,⊥

√

√

√

y0,⊥

1 + 5 ρ /(8 ) T 1 n 1 + 5 ρ /(8 ) T

ρ

≤

·

√

≤

√

≤ . (23)

yT ,

y0,

1 + 7 ρ /(8 )

δ0 π 1 + 7 ρ /(8 )

8

√ Since yT , ≤ yT , we have yT ,⊥ / yT ≤ ρ /(8 √), contradiction. Hence, there here exists some t0 with 1 ≤ t0 ≤ T such that yt0,⊥ / yt0 ≤ ρ /(8 ). Consider the normalized

vector eˆ = yt0 /r, we use eˆ⊥ and eˆ to separately denote the component of eˆ in S⊥ and S . Then, eˆ⊥ ≤ √ρ /(8 ) whereas eˆ ≥ 1 − ρ /(8 )2. Then,

eˆT H(0)eˆ = (eˆ⊥ + eˆ )T H(0)(eˆ⊥ + eˆ ) = eˆT⊥ H(0)eˆ⊥ + eˆT H(0)eˆ

(24)

6

since H(0)eˆ⊥ ∈ S⊥ and H(0)eˆ ∈ S . Due to the -smoothness of the function, all eigenvalue of the Hessian matrix has its absolute value upper bounded by . Hence,

eˆT⊥ H(0)eˆ⊥ ≤

eˆT⊥

2 2

=

ρ

/(64

2).

(25)

Further according to the deﬁnition of S , we have

eˆT H(0)eˆ

√ ≤− ρ

eˆ

2/2.

(26)

Combining these two inequalities together, we can obtain eˆT H(0)eˆ = eˆT⊥H(0)eˆ⊥ + eˆT H(0)eˆ ≤ −√ρ eˆ 2/2 + ρ /(64 2) ≤ −√ρ /4. (27)

Remark 4. In practice, the value of yt can become large during the execution of Algorithm 1. To ﬁx this, we can renormalize yt to have 2-norm r at the ends of such iterations, and this does not inﬂuence the performance of the algorithm.
2.2 Faster negative curvature ﬁnding based on accelerated gradient descents
In this subsection, we replace the GD part in Algorithm 1 by AGD to obtain an accelerated negative curvature ﬁnding subroutine with similar effect and faster convergence rate, based on which we further implement our Accelerated Gradient Descent with√Negative Curvature Finding (Algorithm 2). Near any saddle point x˜ ∈ Rn with λmin(H(x˜)) ≤ − ρ , Algorithm 2 ﬁnds a unit vector eˆ such that eˆT H(x˜)eˆ ≤ −√ρ /4.

Algorithm 2: Perturbed Accelerated Gradient Descent with Accelerated Negative Curvature Finding(x0, η, θ, γ, s, T , r )

1 tperturb ← 0, z0 ← x0, x˜ ← x0, ζ ← 0;

2 for t = 0, 1, 2, ..., T do

3 if ∇f (xt) ≤ and t − tperturb > T then

4

x˜ = xt;

5

xt ← Uniform(Bx˜(r )) where Uniform(Bx˜(r )) is the

radius r , zt ← xt, ζ ← ∇f (x˜), tperturb ← t;

2-norm ball centered at x˜ with

6 if t − tperturb = T then

7 eˆ := xxtt−−xx˜˜ ;

8

xt

←

x˜

−

feˆ (x˜ ) 4|f (x˜)|

ρ · eˆ, zt ← xt, ζ = 0;

eˆ

9 xt+1 ← zt − η(∇f (zt) − ζ);
10 vt+1 ← xt+1 − xt;
11 zt+1 ← xt+1 + (1 − θ)vt+1; 12 if tperturb = 0 and t − tperturb < T then 13 zt+1 ← x˜ + r · zztt+ +11−−xx˜˜ , xt+1 ← x˜ + r · xztt++11−−xx˜˜ ;

14 else

15

if f (xt+1) ≤ f (zt+1) + ∇f (zt+1), xt+1 − zt+1 − γ2 zt+1 − xt+1 2 then

16

(xt+1, vt+1) ←NegativeCurvatureExploitation(xt+1, vt+1, s)3;

17

zt+1 ← xt+1 + (1 − θ)vt+1;

The following proposition exhibits the effectiveness of Algorithm 2 for ﬁnding negative curvatures near saddle points:
3 This NegativeCurvatureExploitation (NCE) subroutine was originally introduced in [21, Algorithm 3] and is called when we detect that the current momentum vt coincides with a negative curvature direction of zt. In this case, we reset the momentum vt and decide whether to exploit this direction based on the value of vt .
7

Proposition 5. Suppose the function f : Rn → R is -smooth and ρ-Hessian Lipschitz. For any

0 < δ0 ≤ 1, we specify our choice of parameters and constants we use as follows:

√

1

(ρ )1/4

32

n

η := 4

θ := √ 4

T := (ρ )1/4 log δ0 ρ

γ := θ2 s := γ r := δ0 π . (28)

η

4ρ

32 ρn

Then for a point x˜ satisfying λmin(∇2f (x˜)) ≤ −√ρ , if running Algorithm 2 with the uniform perturbation in Line 5 being added at t = 0, the unit vector eˆ in Line 7 obtained after T iterations

satisﬁes:

P

eˆT

H(x)eˆ

≤

√ −ρ

/4

≥ 1 − δ0.

(29)

The proof of Proposition 5 is similar to the proof of Proposition 3, and is deferred to Appendix B.2.

2.3 Escaping saddle points using negative curvature ﬁnding

In this subsection, we demonstrate that our Algorithm 1 and Algorithm 2 with the ability to ﬁnd

negative curvature near saddle points can further escape saddle points of nonconvex functions. The

intuition works as follows: we start with gradient descents or accelerated gradient descents until the

gradient becomes small. At this position, we compute the negative curvature direction, described by

a unit vector eˆ, via Algorithm 1 or the negative curvature ﬁnding subroutine of Algorithm 2. Then,

we add a perturbation along this direction of negative curvature and go back to gradient descents

or accelerated gradient descents with an additional NegativeCurvatureExploitation subroutine (see

Footnote 3). It has the following guarantee:

Lemma 6. Suppose the function f : Rn → R is -smooth and ρ-Hessian Lipschitz. Then for any

√
n, if there exists a unit vector eˆ satisfying eˆT H(x0)eˆ ≤ − ρ where H stands for the

point x0 ∈ R

4

Hessian matrix of function f , the following inequality holds:

f x − feˆ(x0)

1

3

· eˆ ≤ f (x ) −

,

(30)

0 4|feˆ(x0)| ρ

0 384 ρ

where feˆ stands for the gradient component of f along the direction of eˆ.

Proof.

Without loss of generality, we assume x0 = 0. We can also assume

∇f (0), eˆ
√

≤ 0; if this

is not the case we can pick −eˆ instead, which still satisﬁes (−eˆ)T H(x0)(−eˆ) ≤ − 4ρ . In practice,

to ﬁgure out whether we should use eˆ or −eˆ, we apply both of them in (30) and choose the one with

smaller function value. Then, for any x = xeˆeˆ with some xeˆ > 0, we have ∂∂2xf2 (x) ≤ − √4ρ + ρxeˆ

eˆ

due to the ρ-Hessian Lipschitz condition of f . Hence,

√

∂f (x) ≤ f (0) −

ρ xeˆ + ρx2,

(31)

∂xeˆ

eˆ

4

eˆ

by which we can further derive that

√

√

f (xeˆeˆ) − f (0) ≤ feˆ(0)xeˆ − 8ρ x2eˆ + ρ3 x3eˆ ≤ − 8ρ x2eˆ + ρ3 x3eˆ. (32)

Settings

xeˆ

=

1 4

ρ gives (30).

We give the full algorithm details based on Algorithm 1 in Appendix C.1. Along this approach, we achieve the following:

Theorem 7 (informal, full version deferred to Appendix C.3). For any > 0 and a constant 0 < δ ≤ 1, Algorithm 2 satisﬁes that at least one of the iterations xt will be an -approximate secondorder stationary point in

O˜ (f (x0) − f ∗) · log n

(33)

1.75

iterations, with probability at least 1 − δ, where f ∗ is the global minimum of f .

8

Intuitively, the proof of Theorem 7 has two parts. The ﬁrst part is similar to the proof of [21,

Theorem 3], which shows that PAGD uses O˜(log6 n/ 1.75) iterations to escape saddle points. We

show that there can be at most O˜(∆f / 1.75) iterations with the norm of gradient larger than using

almost the same techniques, but with slightly different parameter choices. The second part is based

on the negative curvature part of Algorithm 2, our accelerated negative curvature ﬁnding algorithm.

Speciﬁcally, at each saddle point we encounter, we can take O˜(log n/ 1/4) iterations to ﬁnd its

negative curvature (Proposition 5), and add a perturbation in this direction to decrease the function

value by O( 1.5) (Lemma 6). Hence, the iterations introduced by Algorithm 4 can be at most

O˜

log n
1.5

·

1
0.25

= O˜(log n/ 1.75), which is simply an upper bound on the overall iteration number.

The detailed proof is deferred to Appendix C.3.

Remark 8. Although Theorem 7 only demonstrates that our algorithms will visit some approximate second-order stationary point during their execution with high probability, it is straightforward to identify one of them if we add a termination condition: once Negative Curvature Finding (Algorithm 1 or Algorithm 2) is applied, we record the position xt0 and the function value
decrease due to the following perturbation. If the function value decrease is larger than 3184 ρ3 as per Lemma 6, then the algorithms make progress. Otherwise, xt0 is an -approximate second-order stationary point with high probability.

3 Stochastic setting

In this section, we present a stochastic version of Algorithm 1 using stochastic gradients, and demonstrate that it can also be used to escape saddle points and obtain a polynomial speedup in log n compared to the perturbed stochastic gradient (PSGD) algorithm in [20].

3.1 Stochastic negative curvature ﬁnding

In the stochastic gradient descent setting, the exact gradients oracle ∇f of function f cannot be accessed. Instead, we only have unbiased stochastic gradients g(x; θ) such that

∇f (x) = Eθ∼D[g(x; θ)] ∀x ∈ Rn,

(34)

where D stands for the probability distribution followed by the random variable θ. Deﬁne

ζ(x; θ) := g(x; θ) − ∇f (x)

(35)

to be the error term of the stochastic gradient. Then, the expected value of vector ζ(x; θ) at any x ∈ Rn equals to 0. Further, we assume the stochastic gradient g(x, θ) also satisﬁes the following assumptions, which were also adopted in previous literatures; see e.g. [8, 19, 20, 34].
Assumption 1. For any x ∈ Rn, the stochastic gradient g(x; θ) with θ ∼ D satisﬁes:

Pr[( g(x; θ) − ∇f (x) ≥ t)] ≤ 2 exp −t2/(2σ2) , ∀t ∈ R.

(36)

Assumption 2. For any θ ∈ supp(D), g(x; θ) is ˜-Lipschitz for some constant ˜:

g(x1; θ) − g(x2; θ) ≤ ˜ x1 − x2 , ∀x1, x2 ∈ Rn.

(37)

Assumption 2 emerges from the fact that the stochastic gradient g is often obtained as an exact gradient of some smooth function,

g(x; θ) = ∇f (x; θ).

(38)

In this case, Assumption 2 guarantees that for any θ ∼ D, the spectral norm of the Hessian of f (x; θ) is upper bounded by ˜. Under these two assumptions, we can construct the stochastic version of Algorithm 1, as shown in Algorithm 3.
Similar to the non-stochastic setting, Algorithm 3 can be used to escape from saddle points and obtain a polynomial speedup in log n compared to PSGD algorithm in [20]. This is quantitatively shown in the following theorem:

9

Algorithm 3: Stochastic Negative Curvature Finding(x0, rs, Ts, m).

1 y0 ← 0, L0 ← rs;

2 for t = 1, ..., Ts do

3 Sample θ(1), θ(2), · · · , θ(m) ∼ D;

4

g(yt−1)

←

1 m

m j=1

g(x0 + yt−1; θ(j)) − g(x0; θ(j)) ;

5 yt ← yt−1 − 1 (g(yt−1) + ξt/Lt−1), ξt ∼ N 0, rds2 I ;

6 Lt ← yrst Lt−1 and yt ← yt · yrst ;

7 Output yT /rs.

Theorem 9 (informal, full version deferred to Appendix D.2). For any > 0 and a constant 0 < δ ≤ 1, our algorithm4 based on Algorithm 3 using only stochastic gradient descent satisﬁes that at
least one of the iterations xt will be an -approximate second-order stationary point in

O˜ (f (x0) − f ∗) · log2 n

(39)

4

iterations, with probability at least 1 − δ, where f ∗ is the global minimum of f .

4 Numerical experiments
In this section, we provide numerical results that exhibit the power of our negative curvature ﬁnding algorithm for escaping saddle points. More experimental results can be found in Appendix E. All the experiments are performed on MATLAB R2019b on a computer with Six-Core Intel Core i7 processor and 16GB memory, and their codes are given in the supplementary material.
Comparison between Algorithm 1 and PGD. We compare the performance of our Algorithm 1 with the perturbed gradient descent (PGD) algorithm in [20] on a test function f (x1, x2) = 116 x41 − 12 x21 + 98 x22 with a saddle point at (0, 0). The advantage of Algorithm 1 is illustrated in Figure 1.

Figure 1:

Run Algorithm 1 and PGD on landscape f (x1, x2)

=

1 16

x41

−

1 2

x21

+

98 x22.

Parameters:

η

=

0.05

(step length), r = 0.1 (ball radius in PGD and parameter r in Algorithm 1), M = 300 (number of samplings).

Left: The contour of the landscape is placed on the background with labels being function values. Blue

points represent samplings of Algorithm 1 at time step tNCGD = 15 and tNCGD = 30, and red points represent

samplings of PGD at time step tPGD = 45 and tPGD = 90. Algorithm 1 transforms an initial uniform-circle

distribution into a distribution concentrating on two points indicating negative curvature, and these two ﬁgures

represent intermediate states of this process. It converges faster than PGD even when tNCGD tPGD.

Right: A histogram of descent values obtained by Algorithm 1 and PGD, respectively. Set tNCGD = 30 and

tPGD = 90. Although we run three times of iterations in PGD, there are still over 40% of gradient descent paths

with function value decrease no greater than 0.9, while this ratio for Algorithm 1 is less than 5%.

4Our algorithm based on Algorithm 3 has similarities to the Neon2online algorithm in [3]. Both algorithms ﬁnd a second-order stationary point for stochastic optimization in O˜(log2 n/ 4) iterations, and we both apply
directed perturbations based on the results of negative curvature ﬁnding. Nevertheless, our algorithm enjoys simplicity by only having a single loop, whereas Neon2online has a nested loop for boosting their conﬁdence.

10

Comparison between Algorithm 3 and PSGD. We compare the performance of our Algorithm 3 with the perturbed stochastic gradient descent (PSGD) algorithm in [20] on a test function f (x1, x2) = (x31 − x32)/2 − 3x1x2 + (x21 + x22)2/2. Compared to the landscape of the previous experiment, this function is more deﬂated from a quadratic ﬁeld due to the cubic terms. Nevertheless, Algorithm 3 still possesses a notable advantage compared to PSGD as demonstrated in Figure 2.
Figure 2: Run Algorithm 3 and PSGD on landscape f (x1, x2) = x31−2 x32 − 3x1x2 + 12 (x21 + x22)2. Parameters: η = 0.02 (step length), r = 0.01 (variance in PSGD and rs in Algorithm 3), M = 300 (number of samplings). Left: The contour of the landscape is placed on the background with labels being function values. Blue points represent samplings of Algorithm 3 at time step tSNCGD = 15 and tSNCGD = 30, and red points represent samplings of PSGD at time step tPSGD = 30 and tPSGD = 60. Algorithm 3 transforms an initial uniform-circle distribution into a distribution concentrating on two points indicating negative curvature, and these two ﬁgures represent intermediate states of this process. It converges faster than PSGD even when tSNCGD tPSGD. Right: A histogram of descent values obtained by Algorithm 3 and PSGD, respectively. Set tSNCGD = 30 and tPSGD = 60. Although we run two times of iterations in PSGD, there are still over 50% of SGD paths with function value decrease no greater than 0.6, while this ratio for Algorithm 3 is less than 10%.
Acknowledgements
We thank Jiaqi Leng for valuable suggestions and generous help on the design of numerical experiments. TL was supported by the NSF grant PHY-1818914 and a Samsung Advanced Institute of Technology Global Research Partnership.
References
[1] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma, Finding approximate local minima faster than gradient descent, Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1195–1199, 2017, arXiv:1611.01146.
[2] Zeyuan Allen-Zhu, Natasha 2: Faster non-convex optimization than SGD, Advances in Neural Information Processing Systems, pp. 2675–2686, 2018, arXiv:1708.08694.
[3] Zeyuan Allen-Zhu and Yuanzhi Li, Neon2: Finding local minima via ﬁrst-order oracles, Advances in Neural Information Processing Systems, pp. 3716–3726, 2018, arXiv:1711.06673.
[4] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro, Global optimality of local search for low rank matrix recovery, Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 3880–3888, 2016, arXiv:1605.07221.
[5] Alan J. Bray and David S. Dean, Statistics of critical points of Gaussian ﬁelds on large-dimensional spaces, Physical Review Letters 98 (2007), no. 15, 150201, arXiv:cond-mat/0611023.
[6] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford, Accelerated methods for nonconvex optimization, SIAM Journal on Optimization 28 (2018), no. 2, 1751–1772, arXiv:1611.00756.
[7] Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio, Identifying and attacking the saddle point problem in high-dimensional non-convex optimization, Advances in neural information processing systems, pp. 2933–2941, 2014, arXiv:1406.2572.
[8] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang, Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator, Advances in Neural Information Processing Systems, pp. 689–699, 2018, arXiv:1807.01695.
11

[9] Cong Fang, Zhouchen Lin, and Tong Zhang, Sharp analysis for nonconvex SGD escaping from saddle points, Conference on Learning Theory, pp. 1192–1234, 2019, arXiv:1902.00247.
[10] Yan V. Fyodorov and Ian Williams, Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity, Journal of Statistical Physics 129 (2007), no. 5-6, 1081–1116, arXiv:cond-mat/0702601.
[11] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan, Escaping from saddle points – online stochastic gradient for tensor decomposition, Proceedings of the 28th Conference on Learning Theory, Proceedings of Machine Learning Research, vol. 40, pp. 797–842, 2015, arXiv:1503.02101.
[12] Rong Ge, Chi Jin, and Yi Zheng, No spurious local minima in nonconvex low rank problems: A uniﬁed geometric analysis, International Conference on Machine Learning, pp. 1233–1242, PMLR, 2017, arXiv:1704.00708.
[13] Rong Ge, Jason D. Lee, and Tengyu Ma, Matrix completion has no spurious local minimum, Advances in Neural Information Processing Systems, pp. 2981–2989, 2016, arXiv:1605.07272.
[14] Rong Ge, Jason D. Lee, and Tengyu Ma, Learning one-hidden-layer neural networks with landscape design, International Conference on Learning Representations, 2018, arXiv:1711.00501.
[15] Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang, Stabilized SVRG: Simple variance reduction for nonconvex optimization, Conference on Learning Theory, pp. 1394–1448, PMLR, 2019, arXiv:1905.00529.
[16] Rong Ge and Tengyu Ma, On the optimization landscape of tensor decompositions, Advances in Neural Information Processing Systems, pp. 3656–3666, Curran Associates Inc., 2017, arXiv:1706.05598.
[17] Moritz Hardt, Tengyu Ma, and Benjamin Recht, Gradient descent learns linear dynamical systems, Journal of Machine Learning Research 19 (2018), no. 29, 1–44, arXiv:1609.05191.
[18] Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli, Global convergence of non-convex gradient descent for computing matrix squareroot, Artiﬁcial Intelligence and Statistics, pp. 479–488, 2017, arXiv:1507.05854.
[19] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan, How to escape saddle points efﬁciently, Proceedings of the 34th International Conference on Machine Learning, vol. 70, pp. 1724–1732, 2017, arXiv:1703.00887.
[20] Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I. Jordan, On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points, Journal of the ACM (JACM) 68 (2021), no. 2, 1–29, arXiv:1902.04811.
[21] Chi Jin, Praneeth Netrapalli, and Michael I. Jordan, Accelerated gradient descent escapes saddle points faster than gradient descent, Conference on Learning Theory, pp. 1042–1085, 2018, arXiv:1711.10456.
[22] Michael I. Jordan, On gradient-based optimization: Accelerated, distributed, asynchronous and stochastic optimization, https://www.youtube.com/watch?v=VE2ITg%5FhGnI, 2017.
[23] Zhize Li, SSRGD: Simple stochastic recursive gradient descent for escaping saddle points, Advances in Neural Information Processing Systems 32 (2019), 1523–1533, arXiv:1904.09265.
[24] Mingrui Liu, Zhe Li, Xiaoyu Wang, Jinfeng Yi, and Tianbao Yang, Adaptive negative curvature descent with applications in non-convex optimization, Advances in Neural Information Processing Systems 31 (2018), 4853–4862.
[25] Yurii Nesterov and Boris T. Polyak, Cubic regularization of Newton method and its global performance, Mathematical Programming 108 (2006), no. 1, 177–205.
[26] Yurii E. Nesterov, A method for solving the convex programming problem with convergence rate O(1/k2), Soviet Mathematics Doklady, vol. 27, pp. 372–376, 1983.
[27] Ju Sun, Qing Qu, and John Wright, A geometric analysis of phase retrieval, Foundations of Computational Mathematics 18 (2018), no. 5, 1131–1198, arXiv:1602.06664.
[28] Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I. Jordan, Stochastic cubic regularization for fast nonconvex optimization, Advances in neural Information Processing Systems, pp. 2899– 2908, 2018, arXiv:1711.02838.
[29] Yi Xu, Rong Jin, and Tianbao Yang, NEON+: Accelerated gradient methods for extracting negative curvature for non-convex optimization, 2017, arXiv:1712.01033.
[30] Yi Xu, Rong Jin, and Tianbao Yang, First-order stochastic algorithms for escaping from saddle points in almost linear time, Advances in Neural Information Processing Systems, pp. 5530–5540, 2018, arXiv:1711.01944.
[31] Yaodong Yu, Pan Xu, and Quanquan Gu, Third-order smoothness helps: Faster stochastic optimization algorithms for ﬁnding local minima, Advances in Neural Information Processing Systems (2018), 4530– 4540.
12

[32] Xiao Zhang, Lingxiao Wang, Yaodong Yu, and Quanquan Gu, A primal-dual analysis of global optimality in nonconvex low-rank matrix recovery, International Conference on Machine Learning, pp. 5862–5871, PMLR, 2018.
[33] Dongruo Zhou and Quanquan Gu, Stochastic recursive variance-reduced cubic regularization methods, International Conference on Artiﬁcial Intelligence and Statistics, pp. 3980–3990, 2020, arXiv:1901.11518.
[34] Dongruo Zhou, Pan Xu, and Quanquan Gu, Finding local minima via stochastic nested variance reduction, 2018, arXiv:1806.08782.
[35] Dongruo Zhou, Pan Xu, and Quanquan Gu, Stochastic variance-reduced cubic regularized Newton methods, International Conference on Machine Learning, pp. 5990–5999, PMLR, 2018, arXiv:1802.04796.
[36] Dongruo Zhou, Pan Xu, and Quanquan Gu, Stochastic nested variance reduction for nonconvex optimization, Journal of Machine Learning Research 21 (2020), no. 103, 1–63, arXiv:1806.07811.
13

A Auxiliary lemmas

In this section, we introduce auxiliary lemmas that are necessary for our proofs.

Lemma 10 ([20, Lemma 19]). If f (·) is -smooth and ρ-Hessian Lipschitz, η = 1/ , then the gradient descent sequence {xt} obtained by xt+1 := xt − η∇f (xt) satisﬁes:

f (xt+1) − f (xt) ≤ η ∇f (x) 2/2,

(40)

for any step t in which Negative Curvature Finding is not called.

Lemma 11 ([20, Lemma 23]). For a -smooth and ρ-Hessian Lipschitz function f with its stochastic gradient satisfying Assumption 1, there exists an absolute constant c such that, for any ﬁxed t, t0, ι > 0, with probability at least 1 − 4eι, the stochastic gradient descent sequence in Algorithm 8 satisﬁes:

η t−1

σ2

f (xt +t) − f (xt ) ≤ −

∇f (xt +i) 2 + c · (t + ι),

(41)

0

0

8

0

i=0

if during t0 ∼ t0 + t, Stochastic Negative Curvature Finding has not been called.

Lemma 12 ([20, Lemma 29]). Denote α(t) := ηγ)t/√2ηγ. If ηγ ∈ [0, 1], then we have

t−1 (1 + ηγ)2(t−1−τ) 1/2 and β(t) := (1 +
τ =0

1. α(t) ≤ β(t) for any t ∈ N; √
2. α(t) ≥ β(t)/ 3, ∀t ≥ ln 2/(ηγ).
Lemma 13 ([20, Lemma 30]). Under the notation of Lemma 12 and Appendix D.1, letting −γ := λmin(H˜), for any 0 ≤ t ≤ Ts and ι > 0 we have
Pr qp(t) ≤ β(t)ηrs · √ι ≥ 1 − 2e−ι, (42)

and

Pr

qp(t) ≥ β(t√)ηrs · δ

δ ≥1− ,

(43)

10 n Ts

Ts

Pr qp(t) ≥ β(t√)ηrs · δ ≥ 1 − δ.

(44)

10 n

Deﬁnition 14 ([20, Deﬁnition 32]). A random vector X ∈ Rn is norm-subGaussian (or nSG(σ)), if there exists σ so that:

Pr( X − E[X]

≥

t)

≤

2e

−

t2 2σ2

,

∀t ∈ R.

(45)

Lemma 15 ([20, Lemma 37]). Given i.i.d. X1, . . . , Xτ ∈ Rn all being zero-mean nSG(σi) deﬁned
in Deﬁnition 14, then for any ι > 0, and B > b > 0, there exists an absolute constant c such that, with probability at least 1 − 2n log(B/b) · e−ι:

n

τ

τ

σi2 ≥ B or

Xi ≤ c · max

σi2, b · ι.

(46)

i=1

i=1

i=1

The next two lemmas are frequently used in the large gradient scenario of the accelerated gradient descent method:
Lemma 16 ([21, Lemma 7]). Consider the setting of Theorem 23, deﬁne a new parameter √
T˜ := (ρ )1/4 · cA, (47)

for some large enough constant cA. If ∇f (xτ ) ≥ for all τ ∈ [0, T˜], then there exists a large

enough positive constant cA0, such that if we choose cA ≥ cA0, by running Algorithm 2 we have

ET˜ − E0 ≤ −E , in which E =

3
ρ

·

c−A7,

and

Eτ

is

deﬁned

as:

1 Eτ := f (xτ ) +

vτ 2.

(48)

2η

14

Lemma 17 ([21, Lemma 4 and Lemma 5]). Assume that the function f is -smooth. Consider the setting of Theorem 23, for every iteration that is not within T steps after uniform perturbation, we have:

Eτ+1 ≤ Eτ ,

(49)

where Eτ is deﬁned in Eq. (48) in Lemma 16.

B Proof details of negative curvature ﬁnding by gradient descents

B.1 Proof of Lemma 18

Lemma 18. Under the setting of Proposition 3, we use αt to denote

αt = yt, / yt ,

(50)

where yt, deﬁned in Eq. (10) is the component of yt in the subspace S spanned by {u1, u2, . . . , up}. Then, during all the T iterations of Algorithm 1, we have αt ≥ αmin for

αmin = δ0 π , (51) 4n
given that α0 ≥ nπ δ0.

To prove Lemma 18, we ﬁrst introduce the following lemma:

Lemma 19. Under the setting of Proposition 3 and Lemma 18, for any t > 0 and initial value y0,

αt achieves its minimum possible value in the speciﬁc landscapes satisfying

√

λ1 = λ2 = · · · = λp = λp+1 = λp+2 = · · · = λn−1 = − ρ .

(52)

Proof. We prove this lemma by contradiction. Suppose for some τ > 0 and initial value y0, ατ achieves its minimum value in some lands√cape g that does not satisfy Eq. (52). That is to say, there exists some k ∈ [n − 1] such that λk = − ρ . We ﬁrst consider the case k > p. Since λn−1, . . . , λp+1 ≥ −√ρ , we have λk > −√ρ . We use {yg,t} to denote the iteration sequence of yt in this landscape. Consider another landscape h with λk = −√ρ and all other eigenvalues being the same as g, and we use {yh,t} to denote the iteration sequence of yt in this landscape. Furthermore, we assume that at each gradient query, the deviation from pure quadratic approximation deﬁned in Eq. (16), denoted ∆h,t, is the same as the corresponding deviation ∆g,t in the landscape g along all the directions other than uk. As for its component ∆h,t,k along uk, we set |∆h,t,k| = |∆g,t,k| with its sign being the same as yt,k.
Under these settings, we have yh,τ,j = yg,τ,j for any j ∈ [n] and j = k. As for the component along uk, we have |yh,τ,j| > |yg,τ,j|. Hence, by the deﬁnition of yt, in Eq. (10), we have

yg,τ,

p

p

=

yg2,τ,j 1/2 =

yh2,τ,j 1/2 = yh,τ, ,

j=1

j=1

(53)

whereas

yg,τ =

n
yg2,τ,j 1/2 <

n
yh2,τ,j 1/2 = yh,τ ,

j=1

j=1

(54)

indicating

αg,τ =

yg,τ, yg,τ

> yh,τ, yh,τ

= αh,τ ,

(55)

contradicting to the supposition that ατ achieves its minimum possible value in g. Similarly, we can also obtain a contradiction for the case k ≤ p.

15

Equipped with Lemma 19, we are now ready to prove Lemma 18.

Proof. In this proof, we consider the worst case, where the initial value α0 =

π n

δ0.

Also,

accord-

ing to Lemma 19, we assume that the eigenvalues satisfy

√

λ1 = λ2 = · · · = λp = λp+1 = λp+2 = · · · = λn−1 = − ρ .

(56)

Moreover, we assume that each time we make a gradient call at some point x, the derivation term ∆ from pure quadratic approximation

∆ = ∇hf (x) − H(0)x

(57)

lies in the direction that can make αt as small as possible. Then, the component ∆⊥ in S⊥ should

be in the direction of x⊥, and the component ∆ in S should be in the opposite direction to x ,

as long as ∆ ≤ x . Hence in this case, we have yt,⊥ / yt being non-decreasing. Also, it

admits the following recurrence formula:

√

yt+1,⊥ = (1 + √ρ / ) yt,⊥ + ∆⊥ / (58)

≤ (1 + ρ / ) yt,⊥ + ∆ /

(59)

√

∆

≤ 1 + ρ / + yt,⊥ yt,⊥ , (60)

where the second inequality is due to the fact that ν can be an arbitrarily small positive number. Note that since yt,⊥ / yt is non-decreasing in this worst-case scenario, we have

∆ ≤ ∆ · y0 ≤ 2 ∆ ≤ 2ρr, (61)

yt,⊥

yt

y0,⊥

yt

which leads to

√

yt+1,⊥ ≤ (1 + ρ / + 2ρr/ ) yt,⊥ .

(62)

On the other hand, suppose for some value t, we have αk ≥ αmin with any 1 ≤ k ≤ t. Then,

√

yt+2, = (1 + ρ / ) yt+1,⊥ − ∆ /

(63)

√

∆

≥ 1 + ρ / − yt, yt, . (64)

Note that since yt, / yt ≥ αmin, we have

∆

∆

yt, ≥ αmin yt = ρr/αmin, (65)

which leads to

√

yt+1, ≥ (1 + ρ / − ρr/(αmin )) yt, .

(66)

Then we can observe that

√

yt,

y0,

1 + ρ / − ρr/(αmin ) t

≥

·

√

,

(67)

yt,⊥

y0,⊥

1 + ρ / + 2ρr/

where

√

1 + ρ / − ρr/(αmin )

√

√

1 + √ρ / + 2ρr/ ≥ (1 + ρ / − ρr/(αmin ))(1 − ρ / − 2ρr/ ) (68)

≥1−ρ / 2−

2ρr

1 ≥1− ,

(69)

αmin

T

by which we can derive that

yt, ≥ y0, (1 − 1/T )t

(70)

yt,⊥

y0,⊥

≥ y0, exp − t ≥ y0, , (71)

y0,⊥

T − 1 2 y0,⊥

indicating

αt = yt, ≥ y0, ≥ αmin. (72) yt, 2 + yt,⊥ 2 4 y0,⊥

Hence, as long as αk ≥ αmin for any 1 ≤ k ≤ t − 1, we can also have αt ≥ αmin if t ≤ T . Since we have α0 ≥ αmin, we can thus claim that αt ≥ αmin for any t ≤ T using recurrence.

16

B.2 Proof of Proposition 5
To make it easier to analyze the properties and running time of Algorithm 2, we introduce a new Algorithm 4, which has a more straightforward structure and has the same effect as Algorithm 2

Algorithm 4: Accelerated Negative Curvature Finding without Renormalization(x˜, r , T ).
1 x0 ←Uniform(B0(r )) where B0(r ) is the 2-norm ball centered at x˜ with radius r ; 2 z0 ← x0; 3 for t = 1, ..., T do 4 xt+1 ← zt − η ztr−x˜ ∇f r · zztt−−xx˜˜ + x˜ − ∇f (x˜) ; 5 vt+1 ← xt+1 − xt; 6 zt+1 ← xt+1 + (1 − θ)vt+1;
7 Output xxT T −−xx˜˜ .

near any saddle point x˜. Quantitatively, this is demonstrated in the following lemma:

Lemma 20. Under the setting of Proposition 5, suppose the perturbation in Line 5 of Algorithm 2 is added at t = 0. Then with the same value of r , T , x˜ and x0, the output of Algorithm 4 is the same as the unit vector eˆ in Algorithm 2 obtained T steps later. In other words, if we separately denote the iteration sequence of {xt} in Algorithm 2 and Algorithm 4 as

{x1,0, x1,1, . . . , x1,T } , {x2,0, x2,1, . . . , x2,T } ,

(73)

we have

x1,T − x˜ = x2,T − x˜ . (74)

x1,T − x˜

x2,T − x˜

Proof. Without loss of generality, we assume x˜ = 0 and ∇f (x˜) = 0. Use recurrence to prove the

desired relationship. Suppose the following identities holds for all k ≤ t with t being some natural

number:

x2,k = x1,k , z2,k = z1,k . (75)

x2,k

r

x2,k

r

Then,

x2,t+1 = z2,t − η · z2,t ∇f (z2,t · r / z2,t ) = z2,t (z1,t − η∇f (z1,t)). (76)

r

r

Adopting the notation in Algorithm 2, we use x1,t+1 and z1,t+1 to separately denote the value of x1,t+1 and z1,t+1 before renormalization:

x1,t+1 = z1,t − η∇f (z1,t), z1,t+1 = x1,t+1 + (1 − θ)(x1,t+1 − x1,t).

(77)

Then,

z2,t

z2,t

x2,t+1 = r (z1,t − η∇f (z1,t)) = r · x1,t+1,

(78)

which further leads to

z2,t

z2,t+1 = x2,t+1 + (1 − θ)(x2,t+1 − x2,t) = r · z1,t+1.

(79)

Note that z1,t+1 =

r z

· z1,t+1, we thus have

1,t+1

z2,t+1 z2,t+1

= z1,t+1 . r

(80)

Hence,

z2,t

z2,t

z1,t+1

z2,t+1

x2,t+1 = r

· x1,t+1 = r

· z1,t

· x1,t+1 =

r

· x1,t+1.

(81)

Since (75) holds for k = 0, we can now claim that it also holds for k = T .

17

Lemma 20 shows that, Algorithm 4 also works in principle for ﬁnding the negative curvature near any saddle point x˜. But considering that Algorithm 4 may result in an exponentially large xt during execution, and it is hard to be merged with the AGD algorithm for large gradient scenarios. Hence, only Algorithm 2 is applicable in practical situations.

Use H(x˜) to denote the Hessian matrix of f at x˜. Observe that H(x˜) admits the following eigendecomposition:

n

H(x˜) = λiuiuTi ,

(82)

i=1

where the set {ui}ni=1 forms an orthonormal basis of Rn. Without loss of generality, we assume the eigenvalues λ1, λ2, . . . , λn corresponding to u1, u2, . . . , un satisfy

λ1 ≤ λ2 ≤ · · · ≤ λn,

(83)

in which λ1 ≤ −√ρ . If λn ≤ −√ρ /2, Proposition 5 holds directly, since no matter the value

of eˆ, we√can have f (xT ) − f (x˜) ≤ − 3/ρ/384. Hence, we only need to prove the case where λn > − ρ /2, where there exists some p > 1 with

√

λp ≤ − ρ ≤ λp+1.

(84)

We use S to denote the subspace of Rn spanned by {u1, u2, . . . , up}, and use S⊥ to denote the subspace spanned by {up+1, up+2, . . . , un}. Then we can have the following lemma:
Lemma 21. Under the setting of Proposition 5, we use αt to denote

xt, − x˜

αt =

, xt − x˜

(85)

in which xt, is the component of xt in the subspace S . Then, during all the T iterations of Algorithm 4, we have αt ≥ αmin for

δ0 π

αmin = 8

, n

(86)

given that α0 ≥ nπ δ0.

Proof. Without loss of generality, we assume x˜ = 0 and ∇f (x˜) = 0. In this proof, we consider

the worst case, where the initial value α0 = nπ δ0 and the component x0,n along un equals 0. In addition, according to the same proof of Lemma 19, we assume that the eigenvalues satisfy

√

λ1 = λ2 = λ3 = · · · = λp = λp+1 = λp+2 = · · · = λn−1 = − ρ .

(87)

Out of the same reason, we assume that each time we make a gradient call at point zt, the derivation term ∆ from pure quadratic approximation

∆ = zt · ∇f (zt · r / zt ) − H(0) · r · zt (88)

r

zt

lies in the direction that can make αt as small as possible. Then, the component ∆ in S should be in the opposite direction to v , and the component ∆⊥ in S⊥ should be in the direction of v⊥.
Hence in this case, we have both xt,⊥ / xt and zt,⊥ / zt being non-decreasing. Also, it admits the following recurrence formula:
√ xt+2,⊥ ≤ (1 + η ρ ) xt+1,⊥ + (1 − θ)( xt+1,⊥ − xt,⊥ ) + η ∆⊥ . (89)

Since xt,⊥ / xt is non-decreasing in this worst-case scenario, we have

∆⊥ ≤ ∆ · x0 ≤ 2 ∆ ≤ 2ρr , (90)

xt+1,⊥

xt+1

x0,⊥

xt+1

which leads to

√

xt+2,⊥ ≤ (1 + η ρ + 2ηρr ) (2 − θ) xt+1,⊥ − (1 − θ) xt,⊥ .

(91)

18

On the other hand, suppose for some value t, we have αk ≥ αmin with any 1 ≤ k ≤ t + 1. Then,

√

xt+2, ≥ (1 + η( ρ − ν)) xt+1, + (1 − θ)( xt+1, − xt, ) + η ∆

(92)

√

≥ (1 + η ρ ) xt+1, + (1 − θ)( xt+1, − xt, ) − η ∆ .

(93)

Note that since xt+1, / xt ≥ αmin for all t > 0, we also have

xt+1,

xt ≥ αmin, ∀t ≥ 0,

(94)

which further leads to

∆

∆

zt+1, ≥ αmin zt+1 = ρr /αmin, (95)

which leads to

√

xt+2, ≥ (1 + η ρ − ηρr /αmin) (2 − θ) xt+1, − (1 − θ) xt, .

(96)

Consider the sequences with recurrence that can be written as

ξt+2 = (1 + κ) (2 − θ)ξt+1 − (1 − θ)ξt

(97)

for some κ > 0. Its characteristic equation can be written as

x2 − (1 + κ)(2 − θ)x + (1 + κ)(1 − θ) = 0,

(98)

whose roots satisfy

indicating

x = 1 + κ (2 − θ) ± (2 − θ)2 − 4(1 − θ) ,

2

1+κ

ξt =

1+κ

t
C1(2 − θ + µ)t + C2(2 − θ − µ)t ,

2

(99) (100)

where µ :=

(2 − θ)2 − 4(11+−κθ) , for constants C1 and C2 being



2−θ−µ

1

C1 = − 2µ ξ0 + (1 + κ)µ ξ1,

2−θ+µ

1

C2 = 2µ ξ0 − (1 + κ)µ ξ1.

(101)

Then by the inequalities (91) and (96), as long as αk ≥ αmin for any 1 ≤ k ≤ t − 1, the values xt,⊥ and xt, satisfy

xt,⊥ ≤

− 2 − θ − µ⊥ ξ0,⊥ +

1 ξ1,⊥ ·

2µ⊥

(1 + κ⊥)µ⊥

1 + κ⊥

t
· (2 − θ + µ⊥)t

2

+ 2 − θ + µ⊥ ξ0,⊥ −

1

ξ1,⊥

·

1 + κ⊥

t
· (2 − θ − µ⊥)t,

2µ⊥

(1 + κ⊥)µ⊥

2

(102) (103)

and

xt, ≥ − 2 −2θµ− µ ξ0, + (1 + 1κ )µ ξ1, · 1 +2κ t · (2 − θ + µ )t

2−θ+µ

1

+ 2µ ξ0, − (1 + κ )µ ξ1,

1+κ ·
2

t
· (2 − θ − µ )t,

(104) (105)

where √
κ⊥ = η ρ + 2ηρr , √
κ = η ρ − ηρr /αmin,

ξ0,⊥ = x0,⊥ , ξ0, = x0, ,

ξ1,⊥ = (1 + κ⊥)ξ0,⊥, ξ1, = (1 + κ )ξ0, .

(106) (107)

19

Furthermore, we can derive that

xt,⊥ ≤

− 2 − θ − µ⊥ ξ0,⊥ +

1 ξ1,⊥ ·

2µ⊥

(1 + κ⊥)µ⊥

1 + κ⊥

t
· (2 − θ + µ⊥)t

2

+ 2 − θ + µ⊥ ξ0,⊥ −

1

ξ1,⊥

·

1 + κ⊥

t
· (2 − θ + µ⊥)t

2µ⊥

(1 + κ⊥)µ⊥

2

≤ ξ0,⊥ ·

1 + κ⊥

t
· (2 − θ + µ⊥)t =

x0,⊥

·

1 + κ⊥

t
· (2 − θ + µ⊥)t,

2

2

and

xt, ≥ − 2 −2θµ− µ ξ0, + (1 + 1κ )µ ξ1, · 1 +2κ t · (2 − θ + µ )t

2−θ+µ

1

+ 2µ ξ0, − (1 + κ )µ ξ1,

1+κ ·
2

t
· (2 − θ − µ )t

≥ − 2 −2θµ− µ ξ0, + (1 + 1κ )µ ξ1, · 1 +2κ t · (2 − θ + µ )t

µ +θ

1+κ

= 2µ ξ0, · 2

t
· (2 − θ + µ )t

≥ x0, 2

1+κ ·
2

t
· (2 − θ + µ )t.

Then we can observe that

xt, ≥ x0,

1+κ t 2−θ+µ t

·

·

,

xt,⊥

2 x0,⊥

1 + κ⊥

2 − θ + µ⊥

where

1+κ 1 + κ⊥ ≥ (1 + κ )(1 − κ⊥)
≥ 1 − (2 + 1/αmin)ηρr − κ κ⊥ ≥ 1 − 2ηρr /αmin,

and

2 − θ + µ 1 + µ /(2 − θ) =
2 − θ + µ⊥ 1 + µ⊥/(2 − θ)

1+ =
1+

1 − (1+4κ(1)−(2θ−) θ)2 1 − (1+κ4(⊥1)−(2θ−) θ)2

1 ≥ 1+
2−θ

θ2 + κ (2 − θ)2 1+κ

≥ 1 − 2(κ⊥ − κ ) ≥ 1 − 3ηρr ,

θ

αminθ

1 1−
2−θ

θ2 + κ⊥(2 − θ)2 1 + κ⊥

by which we can derive that

xt, xt,⊥

≥ x0, 2 x0,⊥
≥ x0, 2 x0,⊥
≥ x0, 2 x0,⊥

4ρr t · 1−
αminθ (1 − 1/T )t
t exp −
T −1

≥ x0, , 4 x0,⊥

(108) (109) (110)
(111) (112) (113) (114) (115)
(116)
(117) (118) (119)
(120) (121)
(122) (123)
(124) (125) (126)

20

indicating

αt =

xt,

x0,

xt, 2 + xt,⊥ 2 ≥ 8 x0,⊥ ≥ αmin.

(127)

Hence, as long as αk ≥ αmin for any 1 ≤ k ≤ t − 1, we can also have αt ≥ αmin if t ≤ T . Since we have α0 ≥ αmin and α1 ≥ αmin, we can claim that αt ≥ αmin for any t ≤ T using recurrence.

Equipped with Lemma 21, we are now ready to prove Proposition 5.

Proof. By Lemma 20, the unit vector eˆ in Line 7 of Algorithm 2 obtained after T iterations equals

to the output of Algorithm 4 starting from x˜. Hence in this proof we consider the output of Algo-

rithm 4 instead of the original Algorithm 2.

√ If λn ≤√− ρ /2, Proposition 5 holds directly. Hence, we only need to prove the case where λn > − ρ /2, in which there exists some p with

√ λp ≤ − ρ /2 < λp+1.

(128)

We use S , S⊥ to denote the subspace of Rn spanned by {u1, u2, . . . , up },

{up +1, up+2, . . . , un}. Furthermore, we deﬁne xt, :=

p i=1

ui, xt

ui,

xt,⊥

:=

n i=p

ui, xt ui, vt,

:=

p i=1

ui, vt

ui, vt,⊥

:=

n i=p

ui, vt ui respectively to denote

the component of xt and vt in Algorithm 4 in the subspaces S , S⊥, and let αt := xt, / xt .

Consider the case where α0 ≥

π n

δ0,

which

can

be

achieved

with

probability

Pr α0 ≥

π n δ0 ≥ 1 −

π Vol(Bn0 −1(1)) n δ0 · Vol(Bn(1)) ≥ 1 −
0

π n δ0 ·

n π = 1 − δ0,

(129)

we prove that there exists some t0 with 1 ≤ t0 ≤ T such that

xt0,⊥ xt0

√ρ ≤.
8

(130)

Assume the contrary, for any t with 1 ≤ t ≤ T , we all have xtx,⊥ t

√ρ

z

>8

and

t,⊥
z

t

√
ρ. >8

Focus on the case where xt,⊥ , the component of xt in subspace S⊥, achieves the largest value possible. Then in this case, we have the following recurrence formula:

√ xt+2,⊥ ≤ (1 + η ρ /2) xt+1,⊥ + (1 − θ)( xt+1,⊥ − xt,⊥ ) + η ∆⊥ .

(131)

z

√ρ

Since

k,⊥
z

≥8

for any 1 ≤ k ≤ t + 1, we can derive that

k

∆⊥ ≤ ∆ xt+1,⊥ + (1 − θ)( xt+1,⊥ − xt,⊥ ) zt,⊥

2ρr ≤ √ρ ,

(132)

which leads to

xt+2,⊥

√

≤ (1 + η ρ /2) xt+1,⊥ + (1 − θ)( xt+1,⊥ − xt,⊥ ) + η ∆⊥

√

√

≤ (1 + η ρ /2 + 2ρr / ρ ) (2 − θ) xt+1,⊥ − (1 − θ) xt,⊥ .

(133) (134)

Using similar characteristic function techniques shown in the proof of Lemma 21, it can be further derived that

xt,⊥

≤ x0,⊥

·

1 + κ⊥

t
· (2 − θ + µ⊥ )t,

2

(135)

for κ⊥ = η√ρ /2 + 2ρr /√ρ and µ⊥ = (2 − θ)2 − 41(+1κ−⊥θ) , given xkx,k⊥

z

√ρ

k,⊥ ≥ 8 for any 1 ≤ k ≤ t − 1. Due to Lemma 21,

zk

√
ρ and ≥8

δ0 π

αt ≥ αmin = 8

, n

∀1 ≤ t ≤ T .

(136)

21

and it is demonstrated in the proof of Lemma 21 that,
xt, ≥ x20, · 1 +2κ t · (2 − θ + µ )t, ∀1 ≤ t ≤ T , for κ = η√ρ − ηρr /αmin and µ = (2 − θ)2 − 41(+1−κθ) . Observe that

(137)

xT ,⊥ xT ,

≤ 2 x0,⊥ x0,
2n ≤
δ0 π

· 1 + κ⊥ 1+κ
1 + κ⊥ T 1+κ

T · 2 − θ + µ⊥ T 2−θ+µ
· 2 − θ + µ⊥ T , 2−θ+µ

(138) (139)

where

1 + κ⊥

1

1 η√ρ

≤

=1− √

√ ≤1−

,

1 + κ 1 + (κ − κ⊥ )

η ρ /2 + ρr (η/αmin + 2/ ρ )

4

(140)

and Hence,

2 − θ + µ⊥ 2−θ+µ

1+ =
1+

1 − (1+κ4⊥(1−)(θ2)−θ)2 1 − (1+4κ(1)−(2θ−) θ)2

≤ 1+

1 1 − (1+κ4⊥(1−)(θ2)−θ)2 −

≤ 1 − κ − κ⊥

θ

η√ρ

(ρ )1/4

≤1−

=1− √ .

4θ

16

1 − (1+4κ(1)−(2θ−) θ)2

xT ,⊥ xT ,

2 ≤
δ0

n

(ρ )1/4 T √ρ

1− √

≤.

π

16

8

(141) (142) (143) (144)
(145)

Since xT ,

≤ xT , we have xTxT,⊥

√
ρ , contradiction. Hence, there here exists some ≤8
√

t0 with 1 ≤ t0 ≤ T such that xt0,⊥ ≤ ρ . Consider the normalized vector eˆ = xt /r, we

xt0

8

√0

use eˆ⊥ and eˆ to separately denote the component of eˆ in S⊥ and S . Then, eˆ⊥ ≤ ρ /(8 )

whereas eˆ ≥ 1 − ρ /(8 )2. Then,

eˆT H(0)eˆ = (eˆ⊥ + eˆ )T H(0)(eˆ⊥ + eˆ ),

(146)

since H(0)eˆ⊥ ∈ S⊥ and H(0)eˆ ∈ S , it can be further simpliﬁed to

eˆT H(0)eˆ = eˆT⊥ H(0)eˆ⊥ + eˆT H(0)eˆ ,

(147)

Due to the -smoothness of the function, all eigenvalue of the Hessian matrix has its absolute value upper bounded by . Hence,

eˆT⊥ H(0)eˆ⊥ ≤ eˆT⊥ 22 = 6ρ4 2 .

(148)

Further according to the deﬁnition of S , we have √
eˆT H(0)eˆ ≤ − ρ eˆ 2. 2

(149)

Combining these two inequalities together, we can obtain √ρ eˆ
eˆT H(0)eˆ = eˆT⊥H(0)eˆ⊥ + eˆT H(0)eˆ ≤ − 2

√

2+ ρ

ρ ≤− .

64 2

4

(150)

22

C Proof details of escaping from saddle points by negative curvature ﬁnding
C.1 Algorithms for escaping from saddle points using negative curvature ﬁnding In this subsection, we ﬁrst present algorithm for escaping from saddle points using Algorithm 1 as Algorithm 5.

Algorithm 5: Perturbed Gradient Descent with Negative Curvature Finding

1 Input: x0 ∈ Rn;

2 for t = 0, 1, ..., T do

3 if ∇f (xt) ≤ then

4

eˆ ←NegativeCurvatureFinding(xt, r, T ) ;

5 xt ← xt − 4|ffeˆeˆ((xx00))| ρ · eˆ;

6 xt+1 ← xt − 1 ∇f (xt);

Observe that Algorithm 5 and Algorithm 2 are similar to perturbed gradient descent and perturbed accelerated gradient descent but the uniform perturbation step is replaced by our negative curvature ﬁnding algorithms. One may wonder that Algorithm 5 seems to involve nested loops since negative curvature ﬁnding algorithm are contained in the primary loop, contradicting our previous claim that Algorithm 5 only contains a single loop. But actually, Algorithm 5 contains only two operations: gradient descents and one perturbation step, the same as operations outside the negative curvature ﬁnding algorithms. Hence, Algorithm 5 is essentially single-loop algorithm, and we count their iteration number as the total number of gradient calls.

C.2 Proof details of escaping saddle points using Algorithm 1
In this subsection, we prove: Theorem 22. For any > 0 and 0 < δ ≤ 1, Algorithm 5 with parameters chosen in Proposition 3 satisﬁes that at least 1/4 of its iterations will be -approximate second-order stationary point, using
O˜ (f (x0) − f ∗) · log n
2
iterations, with probability at least 1 − δ, where f ∗ is the global minimum of f .

Proof. Let the parameters be chosen according to (2), and set the total step number T to be:

8 (f (x0) − f ∗)

∗

T = max

2

, 768(f (x0) − f ) ·

ρ 3,

(151)

similar to the perturbed gradient descent algorithm [20, Algorithm 4]. We ﬁrst assume that for each xt we apply negative curvature ﬁnding (Algorithm 1) with δ0 contained in the parameters be chosen as

1

3

δ0 = 384(f (x0 − f ∗)

δ, ρ

(152)

we can successfully obtain a unit vector eˆ with eˆT Heˆ ≤ −√ρ /4, as long as λmin(H(xt)) ≤ √
− ρ . The error probability of this assumption is provided later.

Under this assumption, Algorithm 1 can be called for at most 384(f (x0) − f ∗) ρ3 ≤ T2 times, for otherwise the function value decrease will be greater than f (x0) − f ∗, which is not possible. Then, the error probability that some calls to Algorithm 1 fails is upper bounded by

384(f (x0) − f ∗)

ρ · δ0 = δ.

3

(153)

23

For the rest of iterations in which Algorithm 1 is not called, they are either large gradient steps, ∇f (xt) ≥ , or -approximate second-order stationary points. Within them, we know that the number of large gradient steps cannot be more than T /4 because otherwise, by Lemma 10 in Appendix A: f (xT ) ≤ f (x0) − T η 2/8 < f ∗,
a contradiction. Therefore, we conclude that at least T /4 of the iterations must be -approximate second-order stationary points, with probability at least 1 − δ.

The number of iterations can be viewed as the sum of two parts, the number of iterations needed for

gradient descent, denoted by T1, and the number of iterations needed for negative curvature ﬁnding,

denoted by T2. with probability at least 1 − δ,

T1 = T = O˜ (f (x0)2− f ∗) .

(154)

As for T2, with probability at least 1 − δ, Algorithm 1 is called for at most 384(f (x0) − f ∗)

ρ
3

times, and by Proposition 3 it takes O˜ l√ogρn iterations each time. Hence,

T2 = 384(f (x0) − f ∗)

ρ · O˜

log n √

= O˜ (f (x0) − f ∗) · log n .

3

ρ

2

As a result, the total iteration number T1 + T2 is O˜ (f (x0) − f ∗) · log n .
2

(155) (156)

C.3 Proof details of escaping saddle points using Algorithm 2
We ﬁrst present here the Negative Curvature Exploitation algorithm proposed in proposed in [21, Algorithm 3] appearing in Line 16 of Algorithm 2:
Algorithm 6: Negative Curvature Exploitation(xt, vt, s) 1 if vt ≥ s then 2 xt+1 ← xt; 3 else 4 ξ = s · vt/ v t; 5 xt ← argminx∈{xt+ξ,xt−ξ}f (x); 6 Output (zt+1, 0).

Now, we give the full version of Theorem 7 as follows:

Theorem 23. Suppose that the function f is -smooth and ρ-Hessian Lipschitz. For any

a constant 0 < δ ≤ 1, we choose the parameters appearing in Algorithm 2 as follows:

δ

3

δ0 = 384∆f ρ ,

√ 32 T = (ρ )1/4 log δ0

n ,
ρ

ζ = √ρ ,

r = δ0 π , 32 ρn

1 η= ,
4

1 θ= √ ,
4ζ

> 0 and (157) (158)

E=

3
ρ · c−A7,

θ2 γ= ,
η

γ s= ,
4ρ

(159)

where ∆f := f (x0) − f ∗ and f ∗ is the global minimum of f , and the constant cA is chosen large

enough to satisfy both the condition in Lemma 16 and cA ≥ (384)1/7. Then, Algorithm 2 satisﬁes

that at least one of the iterations zt will be an -approximate second-order stationary point in

O˜ (f (x0) − f ∗) · log n
1.75

(160)

iterations, with probability at least 1 − δ.

24

Proof. Set the total step number T to be:

4∆f (T˜ + T )

T = max

E , 768∆f T

ρ = O˜ (f (x0) − f ∗) · log n ,

3

1.75

(161)

where

T˜

=

√ ζ

·

cA

as

deﬁned

in

Lemma

16,

similar

to

the

perturbed

accelerated

gradient

descent

algorithm [21, Algorithm 2]. We ﬁrst assert that for each iteration xt that a uniform pe√rturbation is added, after T iteration√s we can successfully obtain a unit vector eˆ with eˆT Heˆ ≤ − ρ /4, as long as λmin(H(xt)) ≤ − ρ . The error probability of this assumption is provided later.

Under this assumption, the uniform perturbation can be called for at most 384(f (x0) − f ∗)

ρ
3

times, for otherwise the function value decrease will be greater than f (x0) − f ∗, which is not

possible. Then, the probability that at least one negative curvature ﬁnding subroutine after uniform

perturbation fails is upper bounded by

384(f (x0) − f ∗)

ρ · δ0 = δ.

3

(162)

For the rest of steps which is not within T steps after uniform perturbation, they are either large gradient steps, ∇f (xt) ≥ , or -approximate second-order stationary points. Next, we demonstrate that at least one of these steps is an -approximate stationary point.
Assume the contrary. We use NT˜ to denote the number of disjoint time periods with length larger than T˜ containing only large gradient steps and do not contain any step within T steps after uniform perturbation. Then, it satisﬁes

T

NT˜ ≥

2(T˜ + T

− 384∆f )

ρ3 ≥ (2c7A − 384)∆f

ρ ≥ ∆f . 3E

(163)

From Lemma 16, during these time intervals the Hamiltonian will decrease in total at least NT˜ ·E = ∆f , which is impossible due to Lemma 17, the Hamiltonian decreases monotonically for every step except for the T steps after uniform perturbation, and the overall decrease cannot be greater than ∆f , a contradiction. Therefore, we conclude that at least one of the iterations must be an -approximate second-order stationary point, with probability at least 1 − δ.

D Proofs of the stochastic setting

D.1 Proof details of negative curvature ﬁnding using stochastic gradients

In this subsection, we demonstrate that Algorithm 3 can ﬁnd a negative curvature efﬁciently. Specifically, we prove the following proposition:

Proposition 24. Suppose the function f : Rn → R is -smooth and ρ-Hessian Lipschitz. For any

0 < δ < 1, we specify our choice of parameters and constants we use as follows:

8

n

Ts = √ρ · log δ√ρ ,

nT 2

√ n

ι = 10 log s log

,

δ

ηrs

(164)

δ

ρ

rs = 480ρnTs ι ,

160( + ˜) m = δ√ρ Tsι,

(165)

Then for any point x˜ ∈ Rn satisfying λmin(H(x˜)) ≤ −√ρ , with probability at least 1 − 3δ,

Algorithm 3 outputs a unit vector eˆ satisfying

√

eˆT H(x˜)eˆ ≤ −

ρ ,

4

(166)

where H stands for the Hessian matrix of function f , using O(m · Ts) = O˜ lδog12/2n iteartions.

Similarly to Algorithm 1 and Algorithm 2, the renormalization step Line 6 in Algorithm 3 only guarantees that the value yt would not scales exponentially during the algorithm, and does not affect

25

Algorithm 7: Stochastic Negative Curvature Finding without Renormalization(x˜, rs, Ts, m).

1 z0 ← 0; 2 for t = 1, ..., Ts do 3 Sample θ(1), θ(2), · · · , θ(m)

∼ D;

4 g(zt−1) ← ztr−s 1 · m1 m j=1 g x˜ + ztr−s 1 zt−1; θ(j) 5 zt ← zt−1 − 1 (g(zt−1) + ξt), ξt ∼ N 0, rds2 I ;

− g(x˜; θ(j)) ;

6 Output zT / zT .

the output. We thus introduce the following Algorithm 7, which is the no-renormalization version of Algorithm 3 that possess the same output and a simpler structure. Hence in this subsection, we analyze Algorithm 7 instead of Algorithm 3.
Without loss of generality, we assume x˜ = 0 by shifting Rn such that x˜ is mapped to 0. As argued in the proof of Proposition 3, H(0) admits the following following eigen-decomposition:

n
H(0) = λiuiuTi ,
i=1

(167)

where the set {ui}ni=1 forms an orthonormal basis of Rn. Without loss of generality, we assume the eigenvalues λ1, λ2, . . . , λn corresponding to u1, u2, . . . , un satisfy

λ1 ≤ λ2 ≤ · · · ≤ λn,

(168)

√

√

where λ1 ≤ − ρ . If λ√n ≤ − ρ /2, Proposition 24 holds directly. Hence, we only need to prove

the case where λn > − ρ /2, where there exists some p > 1 and p > 1 with

√

√

λp ≤ − ρ ≤ λp+1, λp ≤ − ρ /2 < λp +1.

(169)

Notation: Throughout this subsection, let H˜ := H(x˜). Use S , S⊥ to separately denote the subspace of Rn spanned by {u1, u2, . . . , up}, {up+1, up+2, . . . , un}, and use S , S⊥ to denote the subspace of Rn spanned by {u1, u2, . . . , up }, {up +1, up+2, . . . , un}. Furthermore, deﬁne zt, :=

p i=1

ui, zt

ui, zt,⊥ :=

n i=p

ui, zt

ui, zt,

:=

p i=1

ui, zt

ui, zt,⊥

:=

n i=p

ui, zt ui

respectively to denote the component of zt in Line 5 of Algorithm 7 in the subspaces S , S⊥, S ,

S⊥, and let γ = −λ1.

To prove Proposition 24, we ﬁrst introduce the following lemma:

Le√mma 25. Under the setting of Proposition 24, for any point x˜ ∈ Rn satisfying λmin(∇2f (x˜) ≤ − ρ , with probability at least 1 − 3δ, Algorithm 3 outputs a unit vector eˆ satisfying

n √ρ

eˆ⊥ :=

ui, eˆ ui ≤ 8

i=p

(170)

using O(m · Ts) = O˜ lδog12/2n iteartions.

D.1.1 Proof of Lemma 25
√ In the proof of Lemma√25, we consider the worst case, where λ1 √= −γ = − ρ is the only eigenvalue less than − ρ /2, and all other eigenvalues equal to − ρ /2 + ν for an arbitrarily small constant ν. Under this scenario, the component zt,⊥ is as small as possible at each time step.

The following lemma characterizes the dynamics of Algorithm 7: Lemma 26. Consider the sequence {zi} and let η = 1/ . Further, for any 0 ≤ t ≤ Ts we deﬁne

ζt := g(zt−1) − zt rs

∇f x˜ + rs zt − ∇f (x˜) , zt

(171)

26

to be the errors caused by the stochastic gradients. Then zt = −qh(t) − qsg(t) − qp(t), where:

t−1
qh(t) := η (I − ηH˜)t−1−τ ∆τ zˆτ ,
τ =0

(172)

for ∆τ = 01 Hf ψ zrτs zτ dψ − H˜, and

t−1
qsg(t) := η (I − ηH˜)t−1−τ ζτ ,
τ =0

t−1
qp(t) := η (I − ηH˜)t−1−τ ξτ .
τ =0

(173)

Proof. Without loss of generality we assume x˜ = 0. The update formula for zt can be written as

zt+1 = zt − η zt ∇f rs zt − ∇f (0) + ζt + ξt ,

rs

zt

(174)

where

zt ∇f rs z − ∇f (0) = zt

1

rs

Hψ z

rs z dψ = (H˜ + ∆ )z , (175)

rs

zt t

rs 0 f

zt t zt t

tt

indicating

zt+1 = (I − ηH˜)xt − η(∆tzt + ζt + ξt)
t
= −η (I − ηH˜)t−τ (∆tzt + ζt + ξt),
τ =0

(176) (177)

which ﬁnishes the proof.

At a high level, under our parameter choice in Proposition 24, qp(t) is the dominating term controlling the dynamics, and qh(t) + qsg(t) will be small compared to qp(t). Quantitatively, this is shown in the following lemma:

Lemma 27. Under the setting of Proposition 24 while using the notation in Lemma 12 and

Lemma 26, we have

√ β(t)ηrsδ ρ Pr qh(t) + qsg(t) ≤ 20√n · 16 , ∀t ≤ Ts ≥ 1 − δ,

(178)

where −γ := λmin(H˜) = −√ρ .

Proof. Divide qp(t) into two parts:

qp,1(t) := qp(t), u1 u1,

and

qp,⊥ (t) := qp(t) − qp,1(t).

Then by Lemma 13, we have

Pr qp,1(t) ≤ β(√t)nηrs · √ι ≥ 1 − 2e−ι,

and Pr qp,1(t) ≥ β(t√)ηrs · δ ≥ 1 − δ/4. 20 n

Similarly,

Pr qp,⊥ (t) ≤ β⊥ (t)ηrs · √ι ≥ 1 − 2e−ι,

(179) (180) (181) (182) (183)

27

and Pr qp,⊥ (t) ≥ β⊥ (t)ηrs · δ ≥ 1 − δ/4, 20
where β⊥ (t) := (1+√ηηγγ/2)t . Set t⊥ := loηgγn . Then for all τ ≤ t⊥ , we have
β(τ ) √ ≤ n,
β⊥ (τ )
which further leads to Pr qp,⊥ (τ ) ≤ 2β⊥ (t)ηrs · √ι ≥ 1 − 2e−ι.

(184) (185) (186)

Next, we use induction to prove that the following inequality holds for all t ≤ t⊥ :

√

Pr

qh(τ ) + qsg(τ )

δ ≤ β⊥ (τ )ηrs · , ∀τ ≤ t

≥ 1 − 10nt2 log

n e−ι.

20

ηrs

(187)

For the base case t = 0, the claim holds trivially. Suppose it holds for all τ ≤ t for some t. Then

due to Lemma 13, with probability at least 1 − 2t⊥ e−ι, we have √
zt ≤ η qp(t) + η qh(t) + qsg(t) ≤ 3β⊥ (τ )ηrs · ι.

(188)

By the Hessian Lipschitz property, ∆τ satisﬁes:

∆τ ≤ ρrs.

(189)

Hence,

qh(t + 1)

t
≤ η (I − ηH˜)t−τ ∆τ zτ

τ =0

t
≤ ηρrs (I − ηH˜)t−τ zτ

τ =0

√

≤ (ηρrsnTs) · (3β⊥ (t)ηrs) · ι

√ β⊥ (t + 1)ηrs δ ρ

≤

√

·

.

10 n

16

(190)
(191) (192) (193)

As for qsg(t), note that ζˆτ |Fτ−1 satisﬁes the norm-subGaussian property deﬁned in Deﬁnition 14.

Speciﬁcally, ζˆτ |Fτ−1 ∼ nSG((

+ ˜)

zˆτ

√ / m).

By

applying

Lemma

15

with

b

=

α2(t)

·

η2(

+

˜)2/m and b = α2(t)η2( + ˜)2η2rs2/(mn), with probability at least

√ 1 − 4n · log n · e−ι,
ηrs

(194)

we have

η( + ˜)√t

√ β (t + 1)ηr δ√ρ

qsg(t + 1) ≤

· (β⊥(t)ηrs) · ι ≤ ⊥

s·

.

m

20

8

Then by union bound, with probability at least √
1 − 10n(t + 1)2 log n e−ι, ηrs

(195) (196)

we have

δ √ρ qh(t + 1) + qsg(t + 1) ≤ β⊥ (t + 1)ηrs · 20 · 8 ,

indicating that (187) holds. Then with probability at least

√

1 − 10nt2 log n e−ι − δ/4,

⊥

ηrs

(197) (198)

28

we have

√ρ qh(t⊥ ) + qsg(t⊥ ) ≤ qp,1(t⊥ ) · 16 .

(199)

Based on this, we prove that the following inequality holds for any t⊥ ≤ t ≤ Ts:

√

√

Pr qh(τ ) + qsg(τ ) ≤ β(τ√)ηrs · δ ρ , ∀t⊥ ≤ τ ≤ t ≥ 1 − 10nt2 log n e−ι. (200)

20 n 16

ηrs

We still use recurrence to prove it. Note that its base case τ = t⊥ is guaranteed by (187). Suppose it holds for all τ ≤ t for some t. Then with probability at least 1 − 2te−ι, we have

zt ≤ η qp(t) + η qh(t) + qsg(t) ≤ 2 qp,1(t) + η qh(t) + qsg(t) 3β(τ )ηrs √ ≤ √ · ι. n

(201) (202)
(203)

Then following a similar procedure as before, we can claim that √
β(t + 1)ηrs δ ρ qh(t + 1) + qsg(t + 1) ≤ √n · 20 · 8 ,

(204)

holds with probability

√

1 − 10n(t + 1)2 log

n

e−ι

−

δ ,

ηrs

4

(205)

indicating that (200) holds. Then under our choice of parameters, the desired inequality √
β(t)ηrsδ ρ qh(t) + qsg(t) ≤ 20√n · 16

(206)

holds with probability at least 1 − δ.

Equipped with Lemma 27, we are now ready to prove Lemma 25.

Proof. First note that under our choice of Ts, we have

√

qp,⊥ (Ts)

ρ

Pr

≤

qp,1(Ts)

16

≥ 1 − δ.

(207)

Further by Lemma 27 and union bound, with probability at least 1 − 2δ,

qh(Ts) + qsg(Ts)

20√n √ρ

≤ qh(Ts) + qsg(Ts) ·

≤.

qp(Ts)

δβ(t)ηrs 16

(208)

For the output eˆ, observe that its component eˆ⊥ = eˆ − eˆ1, since u1 is the only component in

subspace S . Then with probability at least 1 − 3δ,

√ eˆ⊥ ≤ ρ /(8 ).

(209)

D.1.2 Proof of Proposition 24 Based on Lemma 25, we present the proof of Proposition 24 as follows:

Proof. By Lemma 25, the component eˆ⊥ Since eˆ = eˆ + eˆ⊥ , we can derive that

of output e satisﬁes √ρ
eˆ⊥ ≤ 8 .

ρ

ρ

eˆ ≥ 1 − (8 )2 ≥ 1 − (8 )2 .

29

(210) (211)

Note that

eˆT H˜eˆ = (eˆ⊥ + eˆ )T H˜(eˆ⊥ + eˆ ),

(212)

which can be further simpliﬁed to

eˆT H˜eˆ = eˆT⊥ H˜eˆ⊥ + eˆT H˜eˆ .

(213)

Due to the -smoothness of the function, all eigenvalue of the Hessian matrix has its absolute value upper bounded by . Hence,

eˆT⊥H˜eˆ⊥ ≤ eˆT⊥ 22 = 6ρ4 2 ,

(214)

whereas

√ eˆT H˜eˆ ≤ − ρ eˆ 2.
2

(215)

Combining these two inequalities together, we can obtain √ρ eˆ
eˆT H˜eˆ = eˆT⊥ H˜eˆ⊥ + eˆT H˜eˆ ≤ − 2

√

2+ ρ

ρ ≤− .

64 2

4

(216)

D.2 Proof details of escaping saddle points using Algorithm 3
In this subsection, we demonstrate that Algorithm 3 can be used to escape from saddle points in the stochastic setting. We ﬁrst present the explicit Algorithm 8, and then introduce the full version Theorem 9 with proof.

Algorithm 8: Stochastic Gradient Descent with Negative Curvature Finding.

1 Input: x0 ∈ Rn; 2 for t = 0, 1, ..., T do 3 Sample θ(1), θ(2), · · · , θ(M)

∼ D;

4

g(xt)

=

1 M

M j=1

g(xt

;

θ(j)

);

5 if g(xt) ≤ 3 /4 then

6

eˆ ←StochasticNegativeCurvatureFinding(xt, rs, Ts, m);

7 xt ← xt − 4|ffeˆeˆ((xx00))| ρ · eˆ;

8

Sample θ(1), θ(2), · · · , θ(M) ∼ D;

9

g(xt)

=

1 M

M j=1

g(xt

;

θ(j)

);

10 xt+1 ← xt − 1 g(xt; θt);

Theorem 28 (Full version of Theorem 9). Suppose that the function f is -smooth and ρ-Hessian Lipschitz. For any > 0 and a constant 0 < δs ≤ 1, we choose the parameters appearing in Algorithm 8 as

δ = δs 3 2304∆f ρ

8

n

Ts = √ρ · log δ√ρ ,

nT 2

√ n

ι = 10 log s log

, (217)

δ

ηrs

δ

ρ

rs = 480ρnTs ι ,

160( + ˜) m = δ√ρ Tsι,

M = 16 ∆f
2

(218)

where ∆f := f (x0) − f ∗ and f ∗ is the global minimum of f . Then, Algorithm 8 satisﬁes that at least 1/4 of the iterations xt will be -approximate second-order stationary points, using

O˜ (f (x0) − f ∗) · log2 n
4

(219)

iterations, with probability at least 1 − δs.

30

Proof. Let the parameters be chosen according to (2), and set the total step number T to be:

8 (f (x0) − f ∗)

∗

T = max

2

, 768(f (x0) − f ) ·

ρ 3.

We will show that the following two claims hold simultaneously with probability 1 − δs:

(220)

1. At most T /4 steps have gradients larger than ;

2. Algorithm 3 can be called for at most 384∆f

ρ
3

times.

Therefore, at least T /4 steps are -approximate secondary stationary points. We prove the two claims separately.

Claim 1. Suppose that within T steps, we have more than T /4 steps with gradients larger than . Then with probability 1 − δs/2,

η T −1

σ2

f (xT ) − f (x0) ≤ −

∇f (xi) 2 + c · (T + log(1/δs)) ≤ f ∗ − f (x0),

8

M

i=0

(221)

contradiction.

Claim 2. We ﬁrst assume that for each xt we apply negative √curvature ﬁnding (Algorithm 3), we ca√n successfully obtain a unit vector eˆ with eˆT H(xt)eˆ ≤ − ρ /4, as long as λmin(H(xt)) ≤ − ρ . The error probability of this assumption is provided later.

Under this assumption, Algorithm 3 can be called for at most 384(f (x0) − f ∗) ρ3 ≤ T2 times, for otherwise the function value decrease will be greater than f (x0) − f ∗, which is not possible. Then, the error probability that some calls to Algorithm 3 fails is upper bounded by

384(f (x0) − f ∗)

ρ · (3δ) = δs/2.

3

(222)

The number of iterations can be viewed as the sum of two parts, the number of iterations needed in

large gradient scenario, denoted by T1, and the number of iterations needed for negative curvature

ﬁnding, denoted by T2. With probability at least 1 − δs,

T1 = O(M · T ) = O˜ (f (x0)4− f ∗) .

(223)

As for T2, with probability at least 1 − δs, Algorithm 3 is called for at most 384(f (x0) − f ∗)

ρ
3

times, and by Proposition 24 it takes O˜ lδo√g2ρn iterations each time. Hence,

T2 = 384(f (x0) − f ∗)

ρ · O˜

log2 n √

= O˜ (f (x0) − f ∗) · log2 n .

3

δρ

4

(224)

As a result, the total iteration number T1 + T2 is O˜ (f (x0) − f ∗) · log2 n .
4

(225)

E More numerical experiments
In this section, we present more numerical experiment results that support our theoretical claims from a few different perspectives compared to Section 4. Speciﬁcally, considering that previous experiments all lies in a two-dimensional space, and theoretically our algorithms have a better dependence on the dimension of the problem n, it is reasonable to check the actual performance of our algorithm on high-dimensional test functions, which is presented in Appendix E.1. Then in Appendix E.2, we introduce experiments on various landscapes that demonstrate the advantage of Algorithm 2 over PAGD [21]. Moreover, we compare the performance of our Algorithm 2 with the NEON+ algorithm [29] on a few test functions in Appendix E.3. To be more precise, we compare the negative curvature extracting part of NEON+ with Algorithm 2 at saddle points in different types of nonconvex landscapes.
31

E.1 Dimension dependence Recall that n is the dimension of the problem. We choose a test function h(x) = 21 xT Hx + 116 x41 where H is an n-by-n diagonal matrix: H = diag(− , 1, 1, ..., 1). The function h(x) has a saddle point at the origin, and only one negative curvature direction. Throughout the experiment, we set
= 1. For the sake of comparison, the iteration numbers are chosen in a manner such that the statistics of Algorithm 1 and PGD in each category of the histogram are of similar magnitude.
Figure 3: Dimension dependence of Algorithm 1 and PGD. We set = 0.01, r = 0.1, n = 10p for p = 1, 2, 3. The iteration number of Algorithm 1 and PGD are separately set to be 30p and 20p2 + 10, and the sample size M = 100. As we can see, to maintain the same performance, the number of iterations in PGD grows faster than the number of iterations in Algorithm 1.
E.2 Comparison between Algorithm 2 and PAGD on various nonconvex landscapes Quartic-type test function Consider the test function f (x1, x2) = 116 x41 − 12 x21 + 98 x22 with a saddle point at (0, 0). The advantage of Algorithm 2 is illustrated in Figure 4.
Figure 4: Run Algorithm 2 and PAGD on landscape f (x1, x2) = 116 x41 − 12 x21 + 98 x22. Parameters: η = 0.05 (step length), r = 0.08 (ball radius in PAGD and parameter r in Algorithm 2), M = 300 (number of samplings). Left: The contour of the landscape is placed on the background with labels being function values. Blue points represent samplings of Algorithm 2 at time step tANCGD = 10 and tANCGD = 20, and red points represent samplings of PAGD at time step tPAGD = 20 and tPAGD = 40. Similarly to Algorithm 1, Algorithm 2 transforms an initial uniform-circle distribution into a distribution concentrating on two points indicating negative curvature, and these two ﬁgures represent intermediate states of this process. It converges faster than PAGD even when tANCGD tPAGD. Right: A histogram of descent values obtained by Algorithm 2 and PAGD, respectively. Set tANCGD = 20 and tPAGD = 40. Although we run two times of iterations in PAGD, there are still over 20% of PAGD paths with function value decrease no greater than 0.9, while this ratio for Algorithm 2 is less than 5%.
32

Triangle-type test function.

Consider the test function f (x1, x2)

=

1 2

cos(πx1)

+

1 2

x2 +

2
cos(2π2x1)−1 − 12 with a saddle point at (0, 0). The advantage of Algorithm 2 is illustrated in

Figure 5.

2
Figure 5: Run Algorithm 2 and PAGD on landscape f (x1, x2) = 12 cos(πx1) + 21 x2 + cos(2π2x1)−1 − 21 .
Parameters: η = 0.01 (step length), r = 0.1 (ball radius in PAGD and parameter r in Algorithm 2), M = 300 (number of samplings). Left: The contour of the landscape is placed on the background with labels being function values. Blue points represent samplings of Algorithm 2 at time step tANCGD = 10 and tANCGD = 20, and red points represent samplings of PAGD at time step tPAGD = 40 and tPAGD = 80. Algorithm 2 converges faster than PAGD even when tANCGD tPAGD. Right: A histogram of descent values obtained by Algorithm 2 and PAGD, respectively. Set tANCGD = 20 and tPAGD = 80. Although we run four times of iterations in PAGD, there are still over 20% of gradient descent paths with function value decrease no greater than 0.9, while this ratio for Algorithm 2 is less than 5%.

Exponential-type test function. Consider the test function f (x1, x2) =

1
x2

+

1 2

x2 −

1+e 1

x21e−x21 2 − 1 with a saddle point at (0, 0). The advantage of Algorithm 2 is illustrated in Figure 6.

Figure 6: Run Algorithm 2 and PAGD on landscape f (x1, x2) = f (x1, x2) = 1 2 + 1 x2 − x21e−x21 2 − 1. 1+ex1 2
Parameters: η = 0.03 (step length), r = 0.1 (ball radius in PAGD and parameter r in Algorithm 2), M = 300 (number of samplings). Left: The contour of the landscape is placed on the background with labels being function values. Blue points represent samplings of Algorithm 2 at time step tANCGD = 10 and tANCGD = 20, and red points represent samplings of PAGD at time step tPAGD = 30 and tPAGD = 60. Algorithm 2converges faster than PAGD even when tANCGD tPAGD. Right: A histogram of descent values obtained by Algorithm 2 and PAGD, respectively. Set tANCGD = 20 and tPAGD = 60. Although we run three times of iterations in PAGD, its performance is still dominated by our Algorithm 2.
Compared to the previous experiment on Algorithm 1 and PGD shown as Figure 1 in Section 4, these experiments also demonstrate the faster convergence rates enjoyed by the general family of "momentum methods". Speciﬁcally, using fewer iterations, Algorithm 2 and PAGD achieve larger function value decreases separately compared to Algorithm 1 and PGD.
33

E.3 Comparison between Algorithm 2 and NEON+ on various nonconvex landscapes

Triangle-type test function.

Consider the test function f (x1, x2)

=

1 2

cos(πx1)

+

1 2

x2 +

2
cos(2π2x1)−1 − 12 with a saddle point at (0, 0). The advantage of Algorithm 2 is illustrated in

Figure 7.

Figure 7: Run Algorithm 2 and NEON+ on landscape f (x1, x2) = 12 cos(πx1) + 12 x2 + cos(2π2x1)−1 2 − 21 . Parameters: η = 0.04 (step length), r = 0.1 (ball radius in NEON+ and parameter r in Algorithm 2), M = 300
(number of samplings).
Left: The contour of the landscape is placed on the background with labels being function values. Red points represent samplings of NEON+ at time step tNEON = 20, and blue points represent samplings of Algorithm 2 at time step tANCGD = 10. Algorithm 2 and the negative curvature extracting part of NEON+ both transform an
initial uniform-circle distribution into a distribution concentrating on two points indicating negative curvature. Note that Algorithm 2 converges faster than NEON+ even when tANCGD tNEON. Right: A histogram of descent values obtained by Algorithm 2 and NEON+, respectively. Set tANCGD = 10 and tNEON = 20. Although we run two times of iterations in NEON+, none of NEON+ paths has function value decrease greater than 0.95, while this ratio for Algorithm 2 is larger than 90%.

Exponential-type test function. Consider the test function f (x1, x2) =

1
x2

+

1 2

x2 −

1+e 1

x21e−x21 2 − 1 with a saddle point at (0, 0). The advantage of Algorithm 2 is illustrated in Figure 8

Figure 8: Run Algorithm 2 and NEON+ on landscape f (x1, x2) = 1 2 + 1 x2 −x21e−x21 2 −1. Parameters: 1+ex1 2
η = 0.03 (step length), r = 0.1 (ball radius in NEON+ and parameter r in Algorithm 2), M = 300 (number of samplings). Left: The contour of the landscape is placed on the background with labels being function values. Red points represent samplings of NEON+ at time step tNEON = 40, and blue points represent samplings of Algorithm 2 at time step tANCGD = 20. Algorithm 2 converges faster than NEON+ even when tANCGD tNEON. Right: A histogram of descent values obtained by Algorithm 2 and NEON+, respectively. Set tANCGD = 20 and tNEON = 40. Although we run two times of iterations in NEON+, there are still over 20% of NEON+ paths with function value decrease no greater than 0.9, while this ratio for Algorithm 2 is less than 10%.
Compared to the previous experiments on Algorithm 2 and PAGD in Appendix E.2, these two experiments also reveal the faster convergence rate of both NEON+ and Algorithm 2 against PAGD [21] at small gradient regions.
34

