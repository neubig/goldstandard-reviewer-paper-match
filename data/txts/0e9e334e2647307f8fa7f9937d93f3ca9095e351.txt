Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity

arXiv:2110.04261v2 [math.OC] 22 Feb 2022

Eduard Gorbunov MIPT, Russia
Mila & UdeM, Canada

Nicolas Loizou Johns Hopkins University
Baltimore, USA

Gauthier Gidel Mila & UdeM, Canada Canada CIFAR AI Chair

Abstract
Extragradient method (EG) (Korpelevich, 1976) is one of the most popular methods for solving saddle point and variational inequalities problems (VIP). Despite its long history and signiﬁcant attention in the optimization community, there remain important open questions about convergence of EG. In this paper, we resolve one of such questions and derive the ﬁrst last-iterate O(1/K) convergence rate for EG for monotone and Lipschitz VIP without any additional assumptions on the operator unlike the only known result of this type (Golowich et al., 2020b) that relies on the Lipschitzness of the Jacobian of the operator. The rate is given in terms of reducing the squared norm of the operator. Moreover, we establish several results on the (non-)cocoercivity of the update operators of EG, Optimistic Gradient Method, and Hamiltonian Gradient Method, when the original operator is monotone and Lipschitz.
1 INTRODUCTION
Saddle point problems receive a lot of attention during recent years, especially in the machine learning community. These problems appear in various applications such as robust optimization (Ben-Tal et al., 2009) and control (Hast et al., 2013), adversarial training (Goodfellow et al., 2015; Madry et al., 2018) and generative adversarial networks (GANs) (Goodfellow et al., 2014). Saddle point problems are often studied from the perspective of variational inequality problems
Proceedings of the 25th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the author(s).

(VIP) (Harker and Pang, 1990; Ryu and Yin, 2020; Gidel et al., 2019). In the unconstrained case, VIP is deﬁned as follows:
ﬁnd x∗ ∈ Rd such that F (x∗) = 0, (VIP)
where F : Rd → Rd is some operator.
Such problems are usually solved via ﬁrst-order methods due to their practical eﬃciency. The simplest example of such a method is Gradient Descent (GD): xk+1 = xk − γF (xk). However, there exist examples of simple (monotone and L-Lipschitz) problems such that GD does not converge to the solution. To circumvent this issue Extragradient Method (EG) xk+1 = xk − γF (xk − γF (xk)) (Korpelevich, 1976) and Optimistic Gradient Method (OG) xk+1 = xk−2γF (xk)+γF (xk−1) (Popov, 1980) were proposed. After their discovery, these methods were revisited and extended in various ways, e.g., stochastic (Gidel et al., 2019; Mishchenko et al., 2020; Hsieh et al., 2020; Li et al., 2021), distributed (Liu et al., 2020; Beznosikov et al., 2020, 2021), and non-Euclidean versions (Juditsky et al., 2011; Azizian et al., 2021) were proposed and analyzed.
Surprisingly, despite the long history of and huge interest in EG and OG, there exist signiﬁcant gaps in the theory of these methods. In particular, it is well known that both methods converge in terms of mink=0,1,...,K F (xk) 2 with rate O(1/K) for monotone L-Lipschitz operator F (Solodov and Svaiter, 1999; Ryu et al., 2019). Although such best-iterate guarantees provide valuable information about the rate of convergence, they do not state anything about lastiterate convergence rate. Recently, this limitation was partially resolved in Golowich et al. (2020b,a) where the authors proved last-iterate O(1/K) convergence rate for EG and OG under the additional assumption that the Jacobian ∇F (x) of operator F (x) is Λ-Lipschitz. However, the obtained rates depend on the Λ that can be much larger than L or even undeﬁned for some operators (see Appendix B). That is,

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

the following important question remains open:
Q1: Is it possible to prove last-iterate O(1/K)
convergence rate for EG/OG when F is monotone
and L-Lipschitz without additional assumptions?
Next, there is a noticeable activity in the analysis of various methods for solving (VIP) under the cocoercivity assumption on F during the last few years (Chavdarova et al., 2019; Malinovskiy et al., 2020; Loizou et al., 2021). Unfortunately, this assumption is stronger than monotonicity and Lipschitzness of F : it does not hold even for bilinear games. However, under the cocoercivity of F the analysis of some methods becomes extremely simple. For example, if operator F is cocoercive, then one can easily prove lastiterate O(1/K) convergence rate for GD (Br´ezis and Lions, 1978; Diakonikolas and Wang, 2021).
Furthermore, it is known that Proximal Point operator FPP,γ(x) implicitly deﬁned as FPP,γ(x) = F (y), where y = x − γF (y), is cocoercive for any monotone F (Corollary 23.10 from (Bauschke et al., 2011)). Therefore, Proximal Point method (PP) xk+1 = xk − γF (xk+1) (Martinet, 1970; Rockafellar, 1976) can be seen as GD for operator FPP,γ and last-iterate O(1/K) convergence rate follows from the analysis of GD under the cocoercivity. Since EG and OG are often considered as approximations of PP when F is LLipschitz (Mokhtari et al., 2019), there is a hope that EG and OG can be rewritten as GD for some cocoercive operator. In particular, for EG one can consider FEG,γ(x) = F (x − γF (x)) and get that EG for F is GD for FEG,γ. Using matrix notation and rewriting OG using zk = ((xk) , (xk−1) ) , one can also construct FOG,γ(x) and consider OG as GD for this operator. Keeping in mind the simplicity of getting last-iterate O(1/K) convergence rate for GD under the cocoercivity, the following question arises:
Q2: Are operators FEG,γ and FOG,γ cocoercive when F is monotone and L-Lipschitz?
In this paper, we give a positive answer to the ﬁrst question (Q1) and negative answer to the second question (Q2). Before we summarize our main contributions, we introduce necessary deﬁnitions.
1.1 Preliminaries
If the opposite is not speciﬁed, throughout the paper we assume that operator F from (VIP) is monotone
F (x) − F (x ), x − x ≥ 0 ∀x, x ∈ Rd, (1)
and L-Lipschitz
F (x) − F (x ) ≤ L x − x ∀x, x ∈ Rd. (2)

Next, we also rely on the deﬁnition of cocoercivity. Deﬁnition 1.1 (Cocoercivity). Operator F : Rd → Rd is called -cocoercive if for all x, x ∈ Rd
F (x) − F (x ) 2 ≤ F (x) − F (x ), x − x . (3)
Using Cauchy-Schwarz inequality, one can easily show that L-cocoercivity of F implies its monotonicity and L-Lipschitzness. The opposite is not true: it is sufﬁcient to take F corresponding to the bilinear game (see (Carmon et al., 2019) and references therein).
Measures of convergence. In the literature on VIP, the convergence of diﬀerent methods is often measured via so-called merit or gap functions, e.g., restricted gap function GapF (x) = maxy∈Rd: y−x∗ ≤R F (y), x − y , where R ∼ x0 − x∗ (Nesterov, 2007). When F is monotone, GapF (x) can be seen as a natural extension of optimization error for VIP. However, it is unclear how to tightly estimate GapF (x) in practice and how to generalize it to nonmonotone case. From this perspective, the squared norm of the operator is preferable as a measure of convergence (see (Yoon and Ryu, 2021) and references therein). Therefore, we focus on F (xK ) 2. We notice here that in the constrained case (squared) norm of the operator is not a valid measure of convergence.
1.2 Contributions
Below we summarize our main contributions.
• We prove that F (xK ) 2 = O (1/K) where xK is generated after K iterations of Extragradient Method (EG) applied to solve (VIP) with monotone L-Lipschitz operator F (Theorem 3.3). That is, we derive the ﬁrst last-iterate O (1/K) convergence rate for EG under monotonicity and LLipschitzness assumptions and without additional ones. The key part of our proof is obtained via solving1 special Performance Estimation Problem (Taylor et al., 2017; Ryu et al., 2020).
• When F (x) is additionally aﬃne, we show that FEG,γ is 2/γ-cocoercive (Lemma D.1) that gives an alternative proof of last-iterate O (1/K) convergence for EG based on the result for GD (Theorem 2.2).
• Guided by the solution of a certain Performance Estimation Problem we prove that for all γ ∈
1Our code is available at https://github.com/ eduardgorbunov/extragradient_last_iterate_ AISTATS_2022. In the MATLAB code, we use PESTO (Taylor et al., 2017), SEDUMI (Sturm, 1999), YALMIP (Lofberg, 2004) libraries, and in the part written in Python, we use CVXPY (Diamond and Boyd, 2016).

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

(0, 1/L] there exists L-Lipschitz and monotone operator F such that FEG,γ is not -cocoercive for any > 0 (Theorem 3.1). This fact emphasizes the signiﬁcant diﬀerence between EG and Proximal Point method.
• We show that FEG,γ is 2/γ-star-cocoercive, i.e., cocoercive towards the solution, when F is starmonotone and L-Lipschitz (Lemma 3.1).
• For Optimistic Gradient method (OG) we consider two popular representations – standard one and extrapolation from the past (EFTP) – and corresponding operators FOG,γ and FEFTP,γ . We prove that these operators are even non-star-cocoercive for any γ > 0 (Theorems 4.1). This fact emphasize the diﬀerence between OG and EG.
• Finally, in the case when we additionally have Lipschitzness of the Jacobian ∇F , we show that operator FH(x) = ∇F (x) F (x) of Hamiltonian Gradient Method xk+1 = xk − γ∇F (xk) F (xk) (HGM) (Balduzzi et al., 2018) can be noncocoercive when F is non-aﬃne (Theorem F.3). Moreover, we derive best-iterate O (1/K) convergence rate in terms of the squared norm of the gradient of the Hamiltonian function H(x) = 21 F (x) 2 when F and ∇F are Lipschitzcontinuous but F is not necessary monotone (Theorem F.4). The details are given in Appendix F.
1.3 Related Work
As we mention earlier, when F is monotone and LLipschitz both EG and OG are usually analyzed in terms of the convergence for the best-iterate or the averaged-iterate. In particular, guarantees of the form GapF (xK ) = O(1/K) with xK being the average of the iterates are shown in Nemirovski (2004); Mokhtari et al. (2019); Hsieh et al. (2019); Monteiro and Svaiter (2010); Auslender and Teboulle (2005) and results like mink=0,1,...,K F (xk) 2 = O(1/K) are given in Solodov and Svaiter (1999); Ryu et al. (2019). Unfortunately, these results do not provide convergence rates for the last-iterate, i.e., for GapF (xK ) and F (xK ) 2. It turns out that both EG and OG satisfy the following lower bound: GapF (xK ) = Ω(1/√K) (Golowich et al., 2020b,a), i.e., in terms of the gap function EG and OG have slower convergence for the last iterate than for the averaged iterate.
However, as it is explained above, we focus on the convergence rates for F (xK ) 2. The mentioned negative results do not imply anything about the convergence in terms of F (xK ) 2. Moreover, for EG and OG Golowich et al. (2020b,a) prove F (xK ) 2 = O(1/K) rate under the additional assumption that the Jacobian ∇F (x) is Λ-Lipschitz. In particular, the derived

rate depends on the Λ, which can be much larger than L for some operators, e.g., for the gradient of logistic loss function (see Appendix B), or simply be undeﬁned when ∇F (x) does not exist on the whole space. In contrast, we prove F (xK ) 2 = O(1/K) rate for EG without any additional assumptions.
Next, the state-of-the-art last-iterate convergence rates are O(1/K2) (Kim, 2021; Yoon and Ryu, 2021). Moreover, Yoon and Ryu (2021) derive the optimality of O(1/K2) rate in the class of monotone and Lipschitz VIP. Although this rate is better than what we derive for EG, this is obtained for diﬀerent methods (Accelerated PP and Anchored EG). Since EG is one of the most popular methods for solving (VIP), it is important to resolve open questions about it like last-iterate convergence rates. Moreover, in view of the lower bound from Golowich et al. (2020b), our result for the last-iterate convergence of EG is optimal for EG up to numerical constants.
Finally, we emphasize that it is possible to obtain a linear last-iterate convergence rate when F is additionally strongly monotone. The corresponding results are well-known both for EG (Tseng, 1995) and OG (Gidel et al., 2019; Mokhtari et al., 2020). Moreover, one can achieve a linear rate under slightly weaker assumptions like quasi-strong monotonicity (Loizou et al., 2021), its local variant (with local guarantees) (Azizian et al., 2021), positive-deﬁniteness of the Jacobian around the solution (also with local guarantees) (Hsieh et al., 2019), and error bound (see (Hsieh et al., 2020) and references therein).

2 COCOERCIVITY AND STAR-COCOERCIVITY

In this section, we introduce the main tools connected with cocoercivity. First of all, it is known that cocoercivity is closely related to non-expansiveness in the following sense.
Lemma 2.1 (Proposition 4.2 from Bauschke et al. (2011)). For any operator F : Rd → Rd the following are equivalent: (i) Id − 2 F is non-expansive; (ii) F is -cocoercive.

We use this lemma to prove non-cocoercivity of FEG,γ.
Next, we also study a relaxation of cocoercivity called star-cocoercivity, which turns out to be suﬃcient to derive best- or random-iterate O(1/K) rate for GD.
Deﬁnition 2.1 (Star-cocoercivity). Operator F : Rd → Rd is called -star-cocoercive around x∗ if F (x∗) = 0 and for all x ∈ Rd

F (x) 2 ≤ F (x), x − x∗ .

(4)

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Further discussion of cocoercivity and starcocoercivity is deferred to Appendix C.

2.1 Analysis of Gradient Descent Under Cocoercivity

The simplest method for solving (VIP) is Gradient De-

scent (GD): xk+1 = xk − γF (xk).

(GD)

If operator F is star-cocoercive, then one can easily show random-iterate convergence of GD.
Theorem 2.1 (Random-iterate convergence of GD). Let F : Rd → Rd be -star-cocoercive around x∗. Then for all K ≥ 0 we have

E F (xK ) 2 ≤ x0 − x∗ 2 , (5) γ(K + 1)

where xK is chosen uniformly at random from the set of iterates {x0, x1, . . . , xK } produced by GD with 0 <
γ ≤ 1/ .

The proof of this result requires a few lines of simple derivations. Next, to establish last-iterate convergence we need to assume cocoercivity of F . In particular, when F is cocoercive it is possible to show that { F (xk) 2}k≥0 monotonically decreases. Using this and previous results one can derive last-iterate convergence (see also (Diakonikolas and Wang, 2021)).
Theorem 2.2 (Last-iterate convergence of GD). Let F : Rd → Rd be -cocoercive. Then for all K ≥ 0 we have
F (xK ) 2 ≤ x0 − x∗ 2 , (6) γ(K + 1)
where xK is produced by GD with 0 < γ ≤ 1/ .

Overall, the analysis of GD under star-cocoercivity and cocoercivity is straightforward and almost identical to the analysis of GD for convex smooth minimization.

2.2 Proximal Point Method

Consider the following iterative process called Proximal Point method (PP):

xk+1 = xk − γF (xk+1).

(PP)

That is, the next point xk+1 is deﬁned implicitly for given xk and γ > 0. Moreover, for given γ > 0 and any point x we can deﬁne operator FPP,γ : Rd → Rd such that ∀x ∈ Rd

FPP,γ(x) = F (y), where y = x − γF (y). (7)

Therefore, (PP) can be rewritten as GD for FPP,γ: xk+1 = xk − γFPP,γ (xk).

It turns out that FPP,γ is 2/γ-cocoercive (Corollary 23.10 from Bauschke et al. (2011)). For completeness, we provide the proof of this fact in the appendix.

Then, applying Theorem 2.2 to the method

xk+1 = xk − γFPP,2/ (xk),

(PP-γ- )

we get the following result (see also (Gu and Yang, 2019)).
Theorem 2.3 (Last-iterate convergence of (PP-γ- )). Let F : Rd → Rd be monotone, > 0 and 0 < γ ≤ 1/ . Then for all K ≥ 0 we have

E F (xK ) 2 ≤ x0 − x∗ 2 , (8) γ(K + 1)

where xK = xK − 2/ F (xK ) = xK − 2/ FPP,2/ (xK ) and xK is produced by (PP-γ- ).

3 EXTRAGRADIENT METHOD

Inspired by the result on cocoercivity of FPP,γ, we study Extragradient method (EG) through the lens of cocoercivity. Indeed, EG can be seen as a practical approximation of PP (Mokhtari et al., 2019, 2020). Therefore, we consider operator FEG,γ = F (Id − γF ) deﬁning the update of EG:

xk+1 = xk − γ F xk − γF (xk) .

(EG)

FEG,γ (xk)

Aﬃne case. We start with the case when F (x) is
aﬃne, i.e., it can be written as F (x) = Ax+b for some A ∈ Rd×d, b ∈ Rd. In Appendix D.1, we show that FEG,γ = F (Id − γF ) is 2/γ-cocoercive for 0 < γ ≤ 1/L. Therefore, applying Theorem 2.2 to the method

xk+1 = xk − γ2FEG,γ1 (xk),

(EG-γ1-γ2)

one can derive O(1/K) last-iterate convergence rate in this case (see Appendix D.2).

Random-iterate guarantees for EG. Motivated by the positive results on the cocoercivity of FEG,γ in the aﬃne case, below we make an attempt to generalize this approach to the general case. The ﬁrst result establishes star-cocoercivity of extragradient operator FEG,γ for any star-monotone and Lipschitz operator F .
Lemma 3.1 (Star-cocoercivity of extragradient operator). Let F : Rd → Rd be star-monotone around x∗, i.e., F (x∗) = 0 and

∀x ∈ Rd F (x), x − x∗ ≥ 0,

(9)

and L-Lipschitz. Then, operator FEG,γ = F (Id − γF ) with γ ≤ 1/L is 2/γ-star-cocoercive around x∗.

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Therefore, applying Theorem 2.1 to (EG-γ1-γ2) we get the following result.
Theorem 3.1 (Random-iterate convergence of (EG-γ1-γ2): non-linear case). Let F : Rd → Rd be star-monotone around x∗ and L-Lipschitz, 0 < γ2 ≤ γ1/2, 0 < γ1 ≤ 1/L. Then for all K ≥ 0 we have

E F (xK ) 2 ≤ 2 x0 − x∗ 2 , (10) γ1γ2(K + 1)

where xK = xK −γ1F (xK ) and xK is chosen uniformly at random from the set of iterates {x0, x1, . . . , xK } produced by (EG-γ1-γ2).
We notice that the result is derived under starmonotonicity of F that can hold even for nonmonotone F (Loizou et al., 2021).

Non-cocoercivity of EG operator. Taking into account all positive results observed in the previous sections, it is natural to expect that FEG,γ is cocoercive when F is monotone and L-Lipschitz and γ is suﬃciently small. Surprisingly, this is not true in general: FEG,γ can be non-cocoercive for monotone and Lipschitz (and even cocoercive2) F !
In view of Lemma 2.1, it is suﬃcient to show that for any > 0 and any γ1, γ2 > 0 there exists -cocoercive operator F such that operator Id−γ2FEG,γ1 is not nonexpansive. In other words, our goal is to show that for all , γ1, γ2 > 0 the quantity

x−y 2

ρEG( , γ1, γ2) = max x − y 2

(11)

s.t. F is -cocoercive,

x, y ∈ Rd, x = y,

x = x − γ2F (x − γ1F (x)),

y = y − γ2F (y − γ1F (y))

is bigger than 1, i.e., ρEG( , γ1, γ2) > 1. In the above problem, maximization is performed on the set of all -cocoercive operators and pairs of vectors x, y ∈ Rd, i.e., one needs to solve inﬁnitely dimensional problem. In such form, it is computationally infeasible.
Fortunately, there exists an equivalent SDP that can be solved eﬃciently. To construct such a problem, we follow Performance Estimation Problem (PEP) tech-

2Cocoercivity of F implies its monotonicity and Lipschitzness.

Figure 1: Numerical estimation of ρEG( , γ1, γ2) deﬁned in (11) for = 1 and diﬀerent γ1, γ2.

nique from Ryu et al. (2020) and rewrite (11) as

max s.t.

x − γ2xF2 − y + γ2yF2 2 x−y 2

(12)

x, y, xF1 , yF1 , xF2 , yF2 ∈ Rd, x = y,

∃F is -cocoercive :

xF2 = F (x − γ1xF1 ), xF1 = F (x),

yF2 = F (y − γ1yF1 ), yF1 = F (y).

Problem (12) is ﬁnite-dimensional and equivalent to
(11). Next, for all α > 0 the following equivalence holds: F is -cocoercive ⇐⇒ α−1Id ◦ F ◦
(αId) is -cocoercive. Therefore, in problem (12) one can apply the change of variables x := α−1x, y := α−1y, xF1 := α−1xF1 , yF1 := α−1yF1 , xF2 := α−1xF2 , yF2 := α−1yF2 , F := α−1Id ◦ F ◦ (αId) , where α = x−y and get another equivalent problem:

max s.t.

x − γ2xF2 − y + γ2yF2 2

(13)

x, y, xF1 , yF1 , xF2 , yF2 ∈ Rd, x = y,

x − y 2 = 1 and ∃F is -cocoercive :

xF2 = F (x − γ1xF1 ), xF1 = F (x),

yF2 = F (y − γ1yF1 ), yF1 = F (y).

However, the constraints are deﬁned implicitly via the existence of -cocoercive operator F that interpolates the introduced points. Therefore, the problem is still hard to solve. It turns out (Proposition 2 from Ryu et al. (2020)) that the constraints about the existence of -cocoercive operator are equivalent to the ﬁnite set of inequalities. These inequalities are called interpolation conditions and, essentially, it is inequality (3) written for all pairs of points that F has to interpolate.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

That is, (13) is equivalent to the following problem:

max s.t.

x − γ2xF2 − y + γ2yF2 2

(14)

x, y, xF1 , yF1 , xF2 , yF2 ∈ Rd, x − y 2 = 1,

xF1 − xF2 , γ1xF1 ≥ xF1 − xF2 2,

xF1 − yF1 , x − y ≥ xF1 − yF1 2,

xF1 − yF2 , x − y + γ1yF1 ≥ xF1 − yF2 2,

xF2 − yF1 , x − γ1xF1 − y ≥ xF2 − yF1 2,

xF2 − yF2 , x − γ1xF1 − y + γ1yF1 ≥ xF2 − yF2 2,
yF1 − yF2 , γ1yF1 ≥ yF1 − yF2 2.

Although the above representation is much better for numerical solving than (11), we do not stop here and consider a Grammian representation of U = (x, y, xF1 , yF1 , xF2 , yF2 ) : G = U U. One can easily show that for all d ≥ 6 we have G ∈ S6+ iﬀ there exist x, y, xF1 , yF1 , xF2 , yF2 ∈ Rd such that G is Gram matrix for these vectors. Since the objective and constrainsts of (14) are linear in the entries of matrix G, problem (14) is equivalent to the following SDP problem:

max Tr(M0G)

(15)

s.t. G ∈ S6+,

Tr(MiG) ≥ 0, i = 1, 2, . . . , 6,

Tr(M7G) = 1,

where M0, . . . , M7 are some symmetric matrices (see the details in Appendix D.5). For any given , γ1, γ2 > 0 this problem can be easily solved numerically using PESTO (Taylor et al., 2017). Therefore, to compute the expansiveness parameter ρEG( , γ1, γ2) of EG we solved (15) for = 1 and diﬀerent values of γ1, γ2. The results are reported in Figure 1.

Although these numerical results show that FEG,γ1 can be non-2/γ2-cocoercive for diﬀerent values of γ1, γ2, and
= 1, it is not a rigorous proof that for any and
γ1, γ2 ∈ (0, 1/ ] there exists -cocoercive operator F such that FEG,γ1 is not 2/γ2-cocoercive. Nevertheless, one can utilize numerical results to construct a rigor-
ous proof but it requires to change the problem (15).

The main diﬃculty is that the solution of (15) is at least of rank 5 in our experiments. It means, that the dimension of the space where the counter-example F is deﬁned is also at least 5 complicates the visualization of the solution3. To overcome this issue, we consider another problem with so-called Log-det heuristic
3We also tried to solve this problem symbolically, but the problem turned out to be computationally infeasible for standard symbolic solvers. Therefore, we focused on the visualization of the solutions in the hope of ﬁnding useful dependencies between the solution and parameters , γ1, γ2.

(Fazel et al., 2003):

min log det (G + δI)

(16)

s.t. G ∈ S6+,

Tr(M0G) ≥ 1.0005,

Tr(MiG) ≥ 0, i = 1, 2, . . . , 6,

Tr(M7G) = 1,

where δ > 0 is some small positive regularization parameter. For γ1, γ2, in some intervals, the solution of the new problem also provides an example of x, y and operator F that proves non-2/γ2-cocoercivity of FEG,γ1 : we ensure this via the constraint Tr(M0G) ≥ a = 1.0005 > 1. In theory, any a > 1 can be used but due to the inevitability of the numerical errors in practice we used a = 1.0005. However, due to the change of the objective the solution may have lower rank since log det (G + δI) can be seen as an diﬀerentiable approximation of the rank of G.
Solving problem (16) for = 1, and γ1 = γ2, we obtained the solutions of rank 2, i.e., we obtained x, y, xF1 , yF1 , xF2 , yF2 in R2. We observed that x = −y for all tested values of γ1. However, numerical solutions were not consistent enough to guess the right dependencies. To overcome this issue, we rotated x, y, xF1 , yF1 , xF2 , yF2 in such a way that x = (−1/2, 0) , y = (1/2, 0) , and plotted the components of xF1 , yF1 , xF2 , yF2 for diﬀerent γ1. Although the resulting dependencies were not perfect, the obtained plots helped us to sequentially construct the needed example:

x = −y = − 21 , x = − 2γ11 , y =

0

F1

1

F1

2γ1

xF2 =

− 1−2γγ11
1 2γ1

, yF2 =

− 1−γ1
2γ1 1−γ12 2
2γ1

− 1−2γγ11 ,
1+γ1 2γ1
. (17)

That is, via plotting the components of x, y, xF1 , yF1 , xF2 , yF2 we observed 4 interesting dependencies, see Figure 2. Mimicking these dependencies, we assumed that

xF2 [1] = yF2 [1], yF1 [1] = xF2 [1] and xF1 [1] < yF1 [1] < 0,
0 < xF1 [2] < yF1 [2], xF1 [2] = xF2 [2],

plugged these relations in the interpolation conditions from (14), and obtained the following inequalities:

yF1 [1] ≤ (1 − γ1)xF1 [1], yF1 [2] ≤ (1 + γ1)xF2 [2],

yF [2] ≤ yF2 [2] ,

1

1 − γ1

xF [2] ≤ yF2 [2]

2

1 − γ12

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Figure 2: Observations that we made after plotting the components of x, y, xF1 , yF1 , xF2 , yF2 for = 1 and diﬀerent values of γ1 (we used γ2 = γ1).

To fulﬁll these constraints, we simply assumed that they hold as equalities and got:

xF [2] = xF [2] = yF2 [2] ,

2

1

1 − γ12

yF [2] = yF2 [2] ,

1

1 − γ1

yF1 [1] = xF2 [1] = yF2 [1] = (1 − γ1)xF1 [1].

Using these dependencies in the remaining interpolation conditions, we derived xF1 [1] + γ1(xF1 [1])2 + γ1((1y−Fγ212[2)]2)2 ≤ 0. After that, we assumed that yF2 [2] = −xF1 [1](1 − γ12). Together with previous inequality it gives xF1 [1] + 2γ1(xF1 [1])2 ≤ 0. Next, we chose xF1 = −1/2γ1 and put it in all previously derived dependencies. Finally, we generalized the example to the case of non-unit using “physical-dimension” arguments and got (17).
These derivations lead to the following result that we rigorously prove in Appendix D.5.
Theorem 3.2. For all > 0 and γ1 ∈ (0, 1/ ] there exists -cocoercive operator F such that F (x) = xF1 , F (y) = yF1 , F (x − γ1xF1 ) = xF2 , F (y − γ1yF1 ) = yF2 for x, y, xF1 , yF1 , xF2 , yF2 deﬁned in (17) and

x−γ2F (x−γ1F (x))−y+γ2F (y−γ1F (y)) > 1 (18)

for all γ2 > 0, i.e., FEG,γ1 = F (Id − γ1F ) is noncocoercive.

First of all, this result emphasizes the diﬀerence between PP and EG, Moreover, it also means that one cannot apply the technique from Section 2.1 to prove last-iterate O(1/K) convergence for EG applied to (VIP) with monotone and L-Lipschitz operator F . However, it does not imply that one cannot prove this fact in general.

Last-iterate guarantees for EG. Inspired by the proof of non-cocoercivity of the operator FEG,γ obtained via PEP, we apply PEP technique to ﬁnd the rate of convergence in terms of F (xK ) 2 for F being monotone and Lipschitz-continuous. That is, we consider the problem

max F (xK ) 2

(19)

s.t. F is mon. and L-Lip., x0 ∈ Rd,

x0 − x∗ 2 ≤ 1,

xk+1 = xk − γ2F xk − γ1F (xk) ,

k = 0, 1, . . . , K − 1

and following similar steps to what we do for showing non-cocoercivity of EG operator, we construct a special SDP using the deﬁnitions of monotonicity (1) and (2) as interpolation conditions. However, the resulting SDP gives just an upper bound for the value of (19) since the resulting SDP might produce such solutions that cannot be interpolated by any mono-

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Figure 3: Comparison of the worst-case rate of EG obtained via solving PEP and the guessed upper-bound 16L2 x0−x∗ 2/k. The vertical axis is shown in logarithmic scale and after iteration k = 20 the curves are almost parallel, i.e., PEP answer and 16L2 x0−x∗ 2/k diﬀer almost by a constant factor. In view of Proposition 3 from Ryu et al. (2020), PEP may give the answer that is not tight for the class of monotone and Lipschitz operators. However, in this particular case, it turns out to be quite tight.
tone and L-Lipschitz operator F (see Proposition 3 from Ryu et al. (2020)). Nevertheless, we solved the resulting SDP using PESTO (Taylor et al., 2017) for L = 1, γ1 = γ2 = 1/2L, and various values of K. We observed that the PEP answer behaves as O (1/K) (see Figure 3). Moreover, using standard duality theory for SDP (De Klerk, 2006) one can show that the solution of the dual problem to the SDP obtained from (19) gives the proof of convergence: it is needed just to sum up the constraints with weights corresponding to the solution of the dual problem (De Klerk et al., 2017). The only thing that remains to do is to guess analytical form of the dual solution.
However, it is not always an easy task: the dependencies on the parameters of the problem like L, γ1, γ2 might be quite tricky. In particular, this might happen due to inaccuracy of the obtained numerical solution and large number of constraints. Therefore, we consider a simpler problem:
∆EG(L, γ1, γ2) = max F (x1) 2 − F (x0) 2 (20)
s.t. F is mon. and L-Lip., x0 ∈ Rd, x0 − x∗ 2 ≤ 1, x1 = x0 −γ2F x0 − γ1F (x0)
with γ1 = γ2 = γ. As for (19), we construct a corresponding SDP and solve it for diﬀerent values of L and γ. In these numerical tests, we observed that ∆EG(L, γ1, γ2) ≈ 0 for all tested pairs of L and γ and the dual variables λ1, λ2, λ3 that correspond to 3 particular constraints – monotonicity (1) for (xk, xk+1) and (xk − γF (xk), xk+1) and Lipschitzness for (xk − γF (xk), xk+1) – are always very close to 2/γ,

1/2γ, and 3/2, while other dual variables are negligible. Although λ2 and λ3 were sometimes slightly smaller, e.g., sometimes we had λ2 ≈ 3/5γ and λ3 ≈ 13/20, we simpliﬁed these dependencies and simply summed up the corresponding inequalities with weights λ1 = 2/γ, λ2 = 1/2γ, and λ3 = 3/2 respectively. After that, it was just needed to rearrange the terms and apply Young’s inequality to some inner products. This is how we obtained the following result (see the details in Appendix D.6).
Lemma 3.2. Let F : Rd → Rd be monotone and LLipschitz, 0 < γ ≤ 1/√2L. Then for all k ≥ 0 the iterates produced by (EG) satisfy F (xk+1) ≤ F (xk) .

This result on its own is novel and plays the central part in deriving last-iterate O(1/K) rate for EG in our analysis. We emphasize that Golowich et al. (2020b) do not derive F (xk+1) ≤ F (xk) to show lastiterate O(1/K) convergence of EG and use completely diﬀerent arguments based on the Lipschitzness of the Jacobian of F . Moreover, the assumption on γ can be relaxed to γ ≤ 1/L, but the proof would be slightly diﬀerent in this case (though it can be obtained from the same PEP).
Next, one might ask a question: is it true that FEG,γ1 (xk+1) ≤ FEG,γ1 (xk) for a reasonable choice of γ1 and γ2? Indeed, this is a good question, since the last-iterate O(1/K) convergence would directly follow from the random-iterate guarantee (Theorem 3.1), if the inequality FEG,γ1 (xk+1) ≤ FEG,γ1 (xk) held. Perhaps, surprisingly, but this is not true even for Lcocoercive F : we observed this phenomenon via solving the SDP constructed for
max FEG,γ1 (x1) 2 − FEG,γ1 (x0) 2 (21) s.t. F is L-cocoercive, x0 ∈ Rd,
x0 − x∗ 2 ≤ 1, x1 = x0 − γ2F x0 − γ1F (x0)

with L = 1 and γ1 ∈ [1/4L, 1/L], γ2 ∈ [γ1/4, γ1]. In our numerical tests, we observed that the optimal value
in the above problem is signiﬁcantly larger than 0 for
given values of L, γ1, γ2. Since it cannot be caused by the inaccuracy of the numerical solution, we conclude that inequality FEG,γ1 (xk+1) ≤ FEG,γ1 (xk) is violated in some cases.

Moreover, when γ2 < γ1 we noticed a similar phenomenon for the norms of F . In particular, we solved the SDP constructed for

max F (x1) 2 − F (x0) 2

(22)

s.t. F is L-cocoercive, x0 ∈ Rd,

x0 − x∗ 2 ≤ 1,

x1 = x0 − γ2F x0 − γ1F (x0)

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

with L = 1, γ1 ∈ [1/4L, 1/L], γ2 ∈ [γ1/4, γ1/2] and observed that the optimal value in the above problem
is signiﬁcantly larger than 0 in this case. Therefore,
we conclude that using the same stepsizes γ1 = γ2 for extrapolation and for the update is crucial for EG to have F (xk+1) ≤ F (xk) .

To derive the desired last-iterate O(1/K) convergence of EG it remains to combine Lemma 3.2 with standard arguments for EG.
Theorem 3.3 (Last-iterate convergence of (EG): non-linear case). Let F : Rd → Rd be monotone and L-Lipschitz. Then for all K ≥ 0 and 0 < γ ≤ 1/√2L

F (xK ) 2 ≤

x0 − x∗ 2 , (23)

γ2(1 − L2γ2)(K + 1)

where xK is produced by (EG) with stepsize γ, and

K

2 x0 − x∗ 2

GapF (x ) ≤

√

. (24)

γ 1 − L2γ2 K + 1

This is the ﬁrst result establishing last-iterate rates F (xK ) 2 = O(1/K) and GapF (xK ) = O(1/√K) for EG that relies on monotonicity and Lipschitzness of F
only. Moreover, it matches the lower bounds for EG
from Golowich et al. (2020b).

4 OPTIMISTIC GRADIENT METHOD

As EG, Optimistic Gradient method (OG) is also often treated as an approximation of PP. Therefore, similar questions to those that we study for EG arise for OG. OG can be written in the following way:

xk+1 = xk − 2γF (xk) + γF (xk−1).

(OG)

For the iterates zk = ((xk) , (xk−1) ) (OG) is

zk+1 = zk−γFOG,γ (zk), FOG,γ =

2F
1

−F 1 . (25)

− γ Id γ Id

There exists another popular form of (OG) called Extrapolation from the past (EFTP): x0 = x0 and
xk+1 = xk−γF (xk), xk+1 = xk−γF (xk+1). (EFTP)

One can show that (EFTP) and (OG) are equivalent:

xk+1 = xk − γF (xk) = xk−1 − 2γF (xk)

= xk − 2γF (xk) + γF (xk−1).

(26)

However, update rule (EFTP) hints the following matrix representation of the method: for zk = ((xk) , (xk) ) (EFTP) is equivalent to

zk+1 = zk − γFEFTP,γ (zk),

FEFTP,γ =

F0 0 Id

Id −γF

− 1 Id 1 Id + F . (27)

γ

γ

It turns out that for any γ > 0 operators FOG,γ, FEFTP,γ can be non-star-cocoercive even for F being linear, monotone, and Lipschitz.
Theorem 4.1. Let the linear operator F (x) = Ax be monotone and L-Lipschitz. Assume that Sp(∇F (x)) = Sp(A) contains at least one eigenvalue λˆ such that Re(λˆ) = 0 and Im(λˆ) = 0. Then, for any > 0 and γ > 0 operators FOG,γ , FEFTP,γ are not -star-cocoercive.
Therefore, for the particular representations (25) and (27) of (OG) one cannot apply the results from Section 2.1 to derive even random-iterate convergence guarantees.
However, this negative result does not imply that it is impossible to show random-iterate or best-iterate O(1/K) convergence rate for OG or EFTP. In fact, such convergence guarantees can be derived using similar steps as in the proof of the corresponding result for EG, see Lemma 11 from Golowich et al. (2020a). Although this result is derived for monotone and Lipschitz operator F , the proof uses only star-monotonicity of F . For completeness, we provide the complete statement of this result and the full proof in Appendix E.2.
Moreover, Golowich et al. (2020a) derive O(1/K) lastiterate convergence rate for OG/EFTP when F is linear or has Lipschitz Jacobian. Establishing O(1/K) lastiterate convergence rate for OG or EFTP for monotone and L-Lipschitz operator F without additional assumptions is still an open problem.
5 CONCLUSION
In this paper, we close an important gap in the convergence theory for EG by showing F (xK ) 2 = O(1/K). Our proof is computer-assisted and is based on the PEP technique (Taylor et al., 2017; Ryu et al., 2020). Moreover, the ideas of reducing the proof to solving SDP problems helped Kim (2021); Yoon and Ryu (2021) to derive last-iterate O(1/K2) rates for Accelerated PP and Anchored EG. We believe that this approach of deriving new proofs is very prominent.
Next, the established connections between EG, OG, HGM and cocoercivity emphasize the diﬀerences between these methods and PP. This is especially important for EG and OG that are often treated as similar methods for solving (VIP) and as approximations PP. Moreover, establishing the result like F (xK ) 2 = O(1/K) for OG without additional assumptions on F (e.g., without assuming Lipschitzness of Jacobian) remains an open problem.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Acknowledgements
This work was partially supported by a grant for research centers in the ﬁeld of artiﬁcial intelligence, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identiﬁer 000000D730321P5Q0002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138. Part of this work was done while Nicolas Loizou was a postdoctoral research fellow at Mila, Universit´e de Montr´eal, supported by the IVADO Postdoctoral Funding Program. Gauthier Gidel is supported by an IVADO grant. Part of this work was done while Eduard Gorbunov was an intern at Mila, Universit´e de Montr´eal under the supervision of Gauthier Gidel. We thank Adrien Taylor for fruitful discussions, suggestions to consider log-detheuristic and plot the output of PEP. We also thank Laurent Condat, Konstantin Mishchenko, Adil Salim, and Vladimir Semenov for pointing out important references and useful suggestions regarding the improvement of the text. Finally, we thank anonymous reviewers for their feedback and valuable suggestions regarding improvements to the paper structure.
References
Abernethy, J., Lai, K. A., and Wibisono, A. (2019). Last-iterate convergence rates for min-max optimization. arXiv preprint arXiv:1906.02027.
Auslender, A. and Teboulle, M. (2005). Interior projection-like methods for monotone variational inequalities. Mathematical programming, 104(1):39– 68.
Azizian, W., Iutzeler, F., Malick, J., and Mertikopoulos, P. (2021). The last-iterate convergence rate of optimistic mirror descent in stochastic variational inequalities. Proceedings of Machine Learning Research vol, 134:1–31.
Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., and Graepel, T. (2018). The mechanics of n-player diﬀerentiable games. In International Conference on Machine Learning, pages 354–363. PMLR.
Bauschke, H. H., Combettes, P. L., et al. (2011). Convex analysis and monotone operator theory in Hilbert spaces, volume 408. Springer.
Ben-Tal, A., El Ghaoui, L., and Nemirovski, A. (2009). Robust optimization. Princeton university press.
Beznosikov, A., Dvurechensky, P., Koloskova, A., Samokhin, V., Stich, S. U., and Gasnikov, A. (2021). Decentralized local stochastic extragradient for variational inequalities. arXiv preprint arXiv:2106.08315.

Beznosikov, A., Samokhin, V., and Gasnikov, A. (2020). Distributed saddle-point problems: Lower bounds, optimal algorithms and federated gans. arXiv preprint arXiv:2010.13112.
Borwein, J., Reich, S., and Shafrir, I. (1992). Krasnoselski-mann iterations in normed spaces. Canadian Mathematical Bulletin, 35(1):21–28.
Br´ezis, H. and Lions, P. L. (1978). Produits inﬁnis de r´esolvantes. Israel Journal of Mathematics, 29(4):329–345.
Carmon, Y., Jin, Y., Sidford, A., and Tian, K. (2019). Variance reduction for matrix games. Advances in Neural Information Processing Systems, 32:11381– 11392.
Chavdarova, T., Gidel, G., Fleuret, F., and LacosteJulien, S. (2019). Reducing noise in gan training with variance reduced extragradient. Advances in Neural Information Processing Systems, 32:393–403.
De Klerk, E. (2006). Aspects of semideﬁnite programming: interior point algorithms and selected applications, volume 65. Springer Science & Business Media.
De Klerk, E., Glineur, F., and Taylor, A. B. (2017). On the worst-case complexity of the gradient method with exact line search for smooth strongly convex functions. Optimization Letters, 11(7):1185–1199.
Diakonikolas, J. and Wang, P. (2021). Potential function-based framework for making the gradients small in convex and min-max optimization. arXiv preprint arXiv:2101.12101.
Diamond, S. and Boyd, S. (2016). Cvxpy: A pythonembedded modeling language for convex optimization. The Journal of Machine Learning Research, 17(1):2909–2913.
Fazel, M., Hindi, H., and Boyd, S. P. (2003). Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices. In Proceedings of the 2003 American Control Conference, 2003., volume 3, pages 2156–2162. IEEE.
Gidel, G., Berard, H., Vincent, P., and Lacoste-Julien, S. (2019). A variational inequality perspective on generative adversarial nets. In ICLR.
Golowich, N., Pattathil, S., and Daskalakis, C. (2020a). Tight last-iterate convergence rates for noregret learning in multi-player games. arXiv preprint arXiv:2010.13724.
Golowich, N., Pattathil, S., Daskalakis, C., and Ozdaglar, A. (2020b). Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In Conference on Learning Theory, pages 1758–1784. PMLR.

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.
Goodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples. ICLR 2015.
Groetsch, C. (1972). A note on segmenting mann iterates. Journal of Mathematical Analysis and Applications, 40(2):369–372.
Gu, G. and Yang, J. (2019). Optimal nonergodic sublinear convergence rate of proximal point algorithm for maximal monotone inclusion problems. arXiv preprint arXiv:1904.05495.
Harker, P. T. and Pang, J.-S. (1990). Finitedimensional variational inequality and nonlinear complementarity problems: a survey of theory, algorithms and applications. Mathematical programming, 48(1):161–220.
Hast, M., ˚Astro¨m, K. J., Bernhardsson, B., and Boyd, S. (2013). Pid design by convex-concave optimization. In 2013 European Control Conference (ECC), pages 4460–4465. IEEE.
Hicks, T. L. and Kubicek, J. D. (1977). On the mann iteration process in a hilbert space. Journal of Mathematical Analysis and Applications, 59(3):498–504.
Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. (2019). On the convergence of single-call stochastic extra-gradient methods. Advances in Neural Information Processing Systems, 32:6938– 6948.
Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. (2020). Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 16223–16234. Curran Associates, Inc.
Juditsky, A., Nemirovski, A., and Tauvel, C. (2011). Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17– 58.
Kim, D. (2021). Accelerated proximal point method for maximally monotone operators. Mathematical Programming, pages 1–31.
Korpelevich, G. M. (1976). The extragradient method for ﬁnding saddle points and other problems. Matecon, 12:747–756.
Krasnosel’skiı, M. (1955). Two remarks on the method of successive approximations, uspehi mat. Nauk, 10:123–127.

Li, C. J., Yu, Y., Loizou, N., Gidel, G., Ma, Y., Roux, N. L., and Jordan, M. I. (2021). On the convergence of stochastic extragradient for bilinear games with restarted iteration averaging. arXiv preprint arXiv:2107.00464.
Liu, M., Zhang, W., Mroueh, Y., Cui, X., Ross, J., Yang, T., and Das, P. (2020). A decentralized parallel algorithm for training generative adversarial nets. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 11056–11070. Curran Associates, Inc.
Lofberg, J. (2004). Yalmip: A toolbox for modeling and optimization in matlab. In 2004 IEEE international conference on robotics and automation (IEEE Cat. No. 04CH37508), pages 284–289. IEEE.
Loizou, N., Berard, H., Gidel, G., Mitliagkas, I., and Lacoste-Julien, S. (2021). Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. arXiv preprint arXiv:2107.00052.
Loizou, N., Berard, H., Jolicoeur-Martineau, A., Vincent, P., Lacoste-Julien, S., and Mitliagkas, I. (2020). Stochastic hamiltonian gradient methods for smooth games. In International Conference on Machine Learning, pages 6370–6381. PMLR.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2018). Towards deep learning models resistant to adversarial attacks. ICLR 2018.
Malinovskiy, G., Kovalev, D., Gasanov, E., Condat, L., and Richtarik, P. (2020). From local sgd to local ﬁxed-point methods for federated learning. In International Conference on Machine Learning, pages 6692–6701. PMLR.
Mann, W. R. (1953). Mean value methods in iteration. Proceedings of the American Mathematical Society, 4(3):506–510.
Martinet, B. (1970). Regularisation d’inequations variationelles par approximations successives. Revue Francaise d’Informatique et de Recherche Operationelle, 4:154–159.
Mishchenko, K., Kovalev, D., Shulgin, E., Richt´arik, P., and Malitsky, Y. (2020). Revisiting stochastic extragradient. In International Conference on Artiﬁcial Intelligence and Statistics, pages 4573–4582. PMLR.
Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2019). Proximal point approximations achieving a convergence rate of O(1/k) for smooth convex-concave saddle point problems: Optimistic gradient and extragradient methods. arXiv preprint arXiv:1906.01115.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2020). A uniﬁed analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1497– 1507. PMLR.
Monteiro, R. D. and Svaiter, B. F. (2010). On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean. SIAM Journal on Optimization, 20(6):2755–2787.
Necoara, I., Nesterov, Y., and Glineur, F. (2019). Linear convergence of ﬁrst order methods for nonstrongly convex optimization. Mathematical Programming, 175(1):69–107.
Nemirovski, A. (2004). Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229–251.
Nesterov, Y. (2007). Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 109(2):319–344.
Nesterov, Y. et al. (2018). Lectures on convex optimization, volume 137. Springer.
Popov, L. D. (1980). A modiﬁcation of the arrowhurwicz method for search of saddle points. Mathematical notes of the Academy of Sciences of the USSR, 28(5):845–848.
Rockafellar, R. T. (1976). Monotone operators and the proximal point algorithm. SIAM journal on control and optimization, 14(5):877–898.
Ryu, E. and Yin, W. (2020). Large-scale convex optimization via monotone operators.
Ryu, E. K., Hannah, R., and Yin, W. (2021). Scaled relative graphs: nonexpansive operators via 2d euclidean geometry. Mathematical Programming, pages 1–51.
Ryu, E. K., Taylor, A. B., Bergeling, C., and Giselsson, P. (2020). Operator splitting performance estimation: Tight contraction factors and optimal parameter selection. SIAM Journal on Optimization, 30(3):2251–2271.
Ryu, E. K., Yuan, K., and Yin, W. (2019). Ode analysis of stochastic gradient methods with optimism and anchoring for minimax problems. arXiv preprint arXiv:1905.10899.
Solodov, M. V. and Svaiter, B. F. (1999). A hybrid approximate extragradient–proximal point algorithm using the enlargement of a maximal monotone operator. Set-Valued Analysis, 7(4):323–345.

Sturm, J. F. (1999). Using sedumi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software, 11(1-4):625–653.
Taylor, A. B., Hendrickx, J. M., and Glineur, F. (2017). Performance estimation toolbox (pesto): automated worst-case analysis of ﬁrst-order optimization methods. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pages 1278– 1283. IEEE.
Tseng, P. (1995). On linear convergence of iterative methods for the variational inequality problem. Journal of Computational and Applied Mathematics, 60(1-2):237–252.
Yoon, T. and Ryu, E. K. (2021). Accelerated algorithms for smooth convex-concave minimax problems with O(1/k2) rate on squared gradient norm. In International Conference on Machine Learning, pages 12098–12109. PMLR.

Supplementary Material: Extragradient Method: O (1/K) Last-Iterate Convergence for
Monotone Variational Inequalities and Connections With Cocoercivity

Contents

1 INTRODUCTION

1

1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

2 COCOERCIVITY AND STAR-COCOERCIVITY

3

2.1 Analysis of Gradient Descent Under Cocoercivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

2.2 Proximal Point Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

3 EXTRAGRADIENT METHOD

4

4 OPTIMISTIC GRADIENT METHOD

9

5 CONCLUSION

9

A BASIC FACTS AND AN AUXILIARY LEMMA

14

B ON THE CONVERGENCE RATES UNDER LIPSCHITZNESS OF JACOBIAN

15

C MISSING PROOFS AND DETAILS FROM SECTION 2

16

C.1 Proof of Lemma 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

C.2 Proof of Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

C.3 Proof of Theorem 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

C.4 Proof of Theorem 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

C.5 Further Details on Cocoercivity and Star-Cocoercivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

C.6 Spectral Viewpoint on Cocoercivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

D MISSING PROOFS AND DETAILS FROM SECTION 3

22

D.1 Cocoercivity of EG Operator in the Aﬃne Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

D.2 Last-Iterate Convergence of EG in the Aﬃne Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

D.3 Linear Case: Non-Spectral Analysis of Extragradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

D.4 Proof of Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

D.5 Details on Performance Estimation Problem for Showing Non-Cocoercivity of Extragradient Operator . . . . . . . . . . . . . 25

D.6 Proof of Lemma 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

D.7 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

E MISSING PROOFS FROM SECTION 4

30

E.1 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

E.1.1 Non-Cocoercivity of FOG,γ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

E.1.2 Non-Cocoercivity of FEFTP,γ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

E.2 Random-Iterate Convergence of (EFTP) for Star-Monotone Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

F HAMILTONIAN GRADIENT METHOD

34

F.1 Aﬃne Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

F.2 General Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

A BASIC FACTS AND AN AUXILIARY LEMMA

In our proofs, we often use the following simple inequalities: for all a, b ∈ Rd and α > 0

a, b

α ≤

a

2+

1

b 2,

(28)

2

2α

a + b 2 ≤ (1 + α) a 2 + (1 + α−1) b 2.

(29)

Moreover, the following lemma plays a key role in the proof of random-iterate convergence of (EFTP) for starmonotone and Lipschitz continuous (VIP).
Lemma A.1 (Lemma 5 from Gidel et al. (2019)). Let operator F : Rd → Rd be L-Lipschitz. Then, for any x ∈ Rd the iterates of (EFTP) satisfy

2γ F (xk+1), xk+1 − x ≤ xk − x 2 − xk+1 − x 2 − xk+1 − xk 2 + γ2L2 xk − xk+1 2.

(30)

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

B ON THE CONVERGENCE RATES UNDER LIPSCHITZNESS OF JACOBIAN

As we mention in the main part of the paper, Golowich et al. (2020b,a) obtain F (xK ) 2 = O(1/K) for EG and OG when F is monotone and L-Lipschitz under additional assumption that ∇F is Λ-Lipschitz. Therefore, the result is not applicable to the general case of monotone and L-Lipschitz F , which can have discontinuous ∇F . Moreover, even in the case of Λ-Lipschitz Jacobian, the rates from Golowich et al. (2020b,a) depend on Λ that can be much larger than L. Indeed, neglecting numerical factors only, Golowich et al. (2020b,a) obtain

F (xK ) 2 = O L2 x0 − x∗ 2 + Λ2 x0 − x∗ 4 . (31)

K

K

Consider the logistic loss with a tiny 2-regularization:

f (x) = ln (1 + eax) + δ x 2, a, x ∈ R, |a| δ. 2

This function is smooth and strongly convex, therefore, its gradient F (x) = ∇f (x) is (strongly) monotone and Lipschitz-continuous. Moreover,

aeax F (x) = 1 + eax + δx,

a2eax

a2e2ax

a2eax

a2

∇F (x) = 1 + eax − (1 + eax)2 + δ = (1 + eax)2 + δ = (e−ax/2 + eax/2)2 + δ,

∇2F (x) = a3eax − 2a3e2ax = a3eax(1 − eax) = a3 · 1 − eax ,

(1 + eax)2 (1 + eax)3

(1 + eax)3

(e−ax/2 + eax/2)2 1 + eax

and since α + α−1 ≥ 2 for all α > 0 we also have

a2

a2

|∇F (x)| = (e−ax/2 + eax/2)2 + δ ≤ 4 + δ,

|∇2F (x)| =

a3

1 − eax |a|3

·

≤.

(e−ax/2 + eax/2)2 1 + eax

4

Since these upper bounds are not too loose, we have that L ∼ a2 and Λ ∼ |a|3. If additionally x0 − x∗ ∼ a,

then the second term in the rate from (31) is ∼ a6 larger than the ﬁrst one. For example, if a = 10, then

Λ2 x0 − x∗ 4 is larger than L2 x0 − x∗ 2 by ∼ 6 orders of magnitude. In contrast, our result for last-iterate

convergence of EG (Theorem 3.3)

F (xK ) 2 = O

L2 x0 − x∗ 2 K

is obtained without assuming Lipschitzness of the Jacobian, and, thus, does not suﬀer from the issues mentioned above.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

C MISSING PROOFS AND DETAILS FROM SECTION 2
C.1 Proof of Lemma 2.1 Lemma C.1 (Lemma 2.1; Proposition 4.2 from Bauschke et al. (2011)). For any operator F : Rd → Rd the following are equivalent
(i) Id − 2 F is non-expansive. (ii) F is -cocoercive.

Proof. In fact, Proposition 4.2 from Bauschke et al. (2011) establishes equivalence of the following statements:

(i) Id − 2F is non-expansive. (ii) F is 1-cocoercive.

Therefore, it remains to check how scaling of the operator aﬀect the result. Consider the operator F1 = 1 F . This operator has the same solution of (VIP) as F and it is 1-cocoercive:

F1(x) − F1(x ) 2 =

1 F (x) − F (x ) 2
2

(3) 1 ≤ x − x , F (x) − F (x )

= x − x , F1(x) − F1(x ) .

Moreover, via similar derivation one can show stronger result:

1 F is -cocoercive ⇐⇒ F is 1-cocoercive.

Applying Proposition 4.2 from Bauschke et al. (2011), we obtain 1
F is -cocoercive ⇐⇒ F is 1-cocoercive ⇐⇒

2 Id − F is non-expansive.

C.2 Proof of Theorem 2.1

Lemma C.2 (Descent lemma for GD). Let F : Rd → Rd be -star-cocoercive around x∗. Then for all k ≥ 0 iterates produced by GD with γ > 0 satisfy

2 γ −γ

F (xk) 2 ≤ xk − x∗ 2 − xk+1 − x∗ 2.

(32)

Proof. Using the update rule of (GD) we derive

xk+1 − x∗ 2 =
=
(4)
≤

xk − γF (xk) − x∗ 2 xk − x∗ 2 − 2γ xk − x∗, F (xk) + γ2 F (xk) 2

xk − x∗ 2 − γ

2 −γ

F (xk) 2.

Rearranging the terms we get (32).

Averaging this inequality, one can easily show random-iterate convergence of GD.

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Theorem C.1 (Theorem 2.1; Random-iterate convergence of GD). Let F : Rd → Rd be x∗. Then for all K ≥ 0 we have
E F (xK ) 2 ≤ x0 − x∗ 2 , γ(K + 1)

-star-cocoercive around (33)

where xK is chosen uniformly at random from the set of iterates {x0, x1, . . . , xK } produced by GD with 0 < γ ≤ 1/ .

Proof. Summing up inequalities (32) for k = 0, 1, . . . , K and dividing both sides of the result by K + 1 we get

γ 2 − γ K F (xk) 2 ≤ 1 K xk − x∗ 2 − xk+1 − x∗ 2

K +1

K +1

k=0

k=0

x0 − x∗ 2 − xK+1 − x∗ 2 =
K +1

x0 − x∗ 2

≤

.

K +1

Next, we use γ ≤ 1/ to lower bound 2/ − γ by 1/ and obtain

1 K F (xk) 2 ≤ x0 − x∗ 2 . (34)

K +1

γ(K + 1)

k=0

Finally, since xK is chosen uniformly at random from the set {x0, x1, . . . , xK } we derive

F (xK ) 2 = 1

K
F (xk) 2 ≤

E

K +1

k=0

x0 − x∗ 2 .
γ(K + 1)

C.3 Proof of Theorem 2.2

As we mention in the main part of the paper, Theorem 2.2 is a well-known result (Br´ezis and Lions, 1978;

Diakonikolas and Wang, 2021). Moreover, it can derived from the analysis of Krasnoselski-Mann method (Kras-

nosel’skiı, 1955; Mann, 1953):

xk+1 = αxk + (1 − α)T (xk), α ∈ (0, 1).

Classical results on the convergence of the above method imply that xk+1 − T (xk+1) ≤ xk − T (xk) for
any non-expansive operator T (Groetsch, 1972; Hicks and Kubicek, 1977; Borwein et al., 1992). In view of Lemma 2.1, operator T = Id − 2 F is non-expansive for any -cocoercive F . Moreover, Krasnoselski-Mann method with such operator T is equivalent to (GD) with γ = 2α/ and xk − T (xk) = 2 F (xk). Therefore, xk+1 − T (xk+1) ≤ xk − T (xk) implies that F (xk+1) ≤ F (xk) .

We give an alternative proof of this fact below.
Lemma C.3. Let F : Rd → Rd be -cocoercive. Then for all k ≥ 0 iterates produced by GD with 0 < γ ≤ 2/ satisfy F (xk+1) ≤ F (xk) .

Proof. From cocoercivity we have

(3)
F (xk+1) − F (xk) 2 ≤

F (xk+1) − F (xk), xk+1 − xk

= −γ F (xk+1), F (xk) + γ F (xk) 2.

Expanding the square in the left-hand side of the inequality and rearranging the terms we get

F (xk+1) 2 ≤ (2 − γ ) F (xk+1), F (xk) − (1 − γ ) F (xk) 2

= F (xk) 2 − (2 − γ ) F (xk) − F (xk+1), F (xk)

(GD)
=

F (xk) 2 − 2 − γ F (xk) − F (xk+1), xk − xk+1 .

(35)

γ

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Since 0 < γ ≤ 2/ and F is cocoercive we have

2−γ

F (xk) − F (xk+1), xk − xk+1

2−γ ≥

F (xk) − F (xk+1) 2 ≥ 0.

γ

γ

Plugging this into (35) gives F (xk+1) ≤ F (xk) .

Theorem C.2 (Theorem 2.2; Last-iterate convergence of GD). Let F : Rd → Rd be K ≥ 0 we have
F (xK ) 2 ≤ x0 − x∗ 2 , γ(K + 1)

-cocoercive. Then for all (36)

where xK is produced by GD with 0 < γ ≤ 1/ .

Proof. Since cocoercivity implies star-cocoercivity we have

1 K F (xk) 2 (3≤4) K +1
k=0

x0 − x∗ 2 .
γ(K + 1)

From Lemma C.3 we have that F (xk+1) ≤ F (xk) . Putting all together we get (6).

C.4 Proof of Theorem 2.3
Lemma C.4 (Corollary 23.10 from Bauschke et al. (2011); Cocoercivity of Proximal Point operator). Let F : Rd → Rd be monotone and γ > 0. Then operator FPP,γ deﬁned in (7) is 2/γ-cocoercive.

Proof. In view of Lemma 2.1, it is enough to prove that Id − γFPP,γ is non-expansive. To show this we consider arbitrary x, y ∈ Rd and deﬁne x and y as follows:

x = x − γF (x) = x − γFPP,γ(x), y = y − γF (y) = y − γFPP,γ(y).

Using this notation, we derive

x−y 2 = = =
(1)
≤ ≤

x − y 2 − 2γ x − y, F (x) − F (y) + γ2 F (x) − F (y) 2 x − y 2 − 2γ x + γF (x) − y − γF (y), F (x) − F (y) + γ2 F (x) − F (y) 2 x − y 2 − 2γ x − y, F (x) − F (y) − γ2 F (x) − F (y) 2
x − y 2 − γ2 F (x) − F (y) 2 x − y 2.

That is, Id − γFPP,γ is non-expansive, and, as a result, FPP,γ is 2/γ-cocoercive.
Theorem C.3 (Theorem 2.3; Last-iterate convergence of (PP-γ- )). Let F : Rd → Rd be monotone, > 0 and 0 < γ ≤ 1/ . Then for all K ≥ 0 we have
E F (xK ) 2 ≤ x0 − x∗ 2 , (37) γ(K + 1)

where xK = xK − 2/ F (xK ) = xK − 2/ FPP,2/ (xK ) and xK is produced by (PP-γ- ).

Proof. Theorem 2.2 implies

FPP,2/ (xK ) 2 ≤

x0 − x∗ 2 .
γ(K + 1)

Since by deﬁnition of FPP,2/ we have FPP,2/ (xK ) = F (xK ), (8) holds.

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

C.5 Further Details on Cocoercivity and Star-Cocoercivity

In Section 2, we give the main deﬁnitions and results about cocoercivity and star-cocoercivity that we use in the paper. Here we continue this discussion and provide extra details on these properties of the operator.
As for cocoercivity, there is a certain relation between star-cocoercivity and non-expansiveness around x∗.
Deﬁnition C.1 (Non-expansiveness around x∗). Let x∗ ∈ Rd be some point. Then operator U : Rd → Rd is called non-expansive around x∗ if for all x ∈ Rd

U (x) − U (x∗) ≤ x − x∗ .

(38)

Lemma C.5. For any operator F : Rd → Rd and x∗ such that F (x∗) = 0 the following are equivalent:

(i) Id − 2F is non-expansive around x∗. (ii) F is 1-star-cocoercive around x∗.

Proof. Non-expansiveness of Id − 2F around x∗ is equivalent to x − x∗ − 2(F (x) − F (x∗)) 2 ≤ x − x∗ 2

that is equivalent to

x − x∗ 2 − 4 x − x∗, F (x) + 4 F (x) 2 ≤ x − x∗ 2.

Rearranging the terms, we get that the last inequality coincides with (4) for = 1.

Lemma C.6. For any operator F : Rd → Rd and x∗ such that F (x∗) = 0 the following are equivalent:

(i) Id − 2 F is non-expansive around x∗. (ii) F is -star-cocoercive around x∗.

Proof. The proof is identical to the proof of Lemma 2.1 up to the replacement of x by x∗.

Finally, we provide a connection between cocoercivity and star-cocoercivity. It is clear that the former implies the latter. Here the natural question arises: when the opposite implication is true? To answer this question we consider the class of linear operators. Deﬁnition C.2 (Linear operator). We say that operator F : Rd → Rd is linear if for any α, β ∈ R and x, y ∈ Rd the operator satisﬁes F (αx + βy) = αF (x) + βF (y).
It turns out that for linear operators cocoercivity and star-cocoercivity are equivalent. Lemma C.7. For any linear operator F : Rd → Rd the following are equivalent:

(i) F is -cocoercive. (ii) F is -star-cocoercive around x∗.

Proof. Implication (i) =⇒ (ii) holds always. Therefore, we need to prove that (ii) implies (i). Let F be -starcocoercive around x∗, i.e., F (x∗) = 0 and the following inequality holds for all x ∈ Rd:
F (x) 2 ≤ F (x), x − x∗ . Next, due to linearity of F we have F (x) = F (x) − F (x∗) = F (x − x∗) for all x ∈ Rd. Therefore, for all x ∈ Rd
F (x − x∗) 2 ≤ F (x − x∗), x − x∗ . For any y ∈ Rd one can take x = y + x∗ in the above inequality and get
F (y) 2 ≤ F (y), y .

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Finally, consider arbitrary x, x ∈ Rd. Replacing y with x − x and using linearity of F , we derive

F (x) − F (x ) 2 = ≤ =

F (x − x ) 2 F (x − x ), x − x F (x) − F (x ), x − x ,

i.e., F is -cocoercive.

We rely on this fact when deriving non-star-cocoercivity of two naturally arising operators corresponding to OG.

C.6 Spectral Viewpoint on Cocoercivity

The following result establishes the connection between cocoercivity and the spectrum of the Jacobian. This result is a corollary of Proposition 1 from Ryu et al. (2021). For completeness, we provide our proof in the appendix.
Lemma C.8 (Spectrum in a disk). Let F : Rd → Rd be a continuously diﬀerentiable operator. Then the following statements are equivalent:

F (x) is -cocoercive ⇐⇒ Re(1/λ) ≥ 1/ , ∀λ ∈ Sp(∇F (x)) , ∀x ∈ Rd .

(39)

As Figure 4 shows, such a constraint corresponds to a disk centered in /2 and of radius /2.

Proof. We start with proving (⇒) part of (39). Let us consider x, u ∈ Rd, by -cocoercivity we have, F (x) − F (x + tu) 2 ≤ t F (x) − F (x + tu), u , ∀t > 0 .

Divinding both side by t2 and letting t goes to 0 gives ∇F (x)u 2 ≤

∇F (x)u, u .

Now let us consider u = a + ib where a, b ∈ Rd an eigenvector of ∇F (x), we get |λ|2 u 2 = ∇F (x)u 2 = ∇F (x)a 2 + ∇F (x)b 2 ≤ ( ∇F (x)a, a + ∇F (x)b, b ) ,

where the last inequality comes from the co-coercivity applied twice. eigenvector, we have
∇F (x)a = Re(λ)a − Im(λ)b,
∇F (x)b = Im(λ)a + Re(λ)b.

Now let us notice that since u is an

Thus we get

∇F (x)a, a + ∇F (x)b, b = Re(λ)( a 2 + b 2)

which leads to,

|λ|2 ≤
Re(λ)

⇐⇒ Re(1/λ) ≥ 1/ .

We notice that Re(1/λ) ≥ 1/ is equivalent to λ ∈ D /2( /2) = {λ ∈ C | |λ − /2| ≤ /2}.

Next, we establish (⇐) part of (39). Let Sp(∇F (x)) ⊆ D /2( /2) for all x ∈ Rd. In view of Lemma 2.1, it is suﬃcient to show that Id − 2/ F is non-expansive that is equivalent to Sp(I − 2/ ∇F (x)) ⊆ D1(0) for all x ∈ Rd. Moreover, we have

2

2

2

Sp I − ∇F (x) = 1 − λ | λ ∈ Sp(∇F (x)) ⊆ 1 − λ | λ ∈ D /2( /2) .

Finally, for any λ ∈ D /2( /2) we have 2
1− λ i.e., 1 − 2 λ ∈ D1(0). This ﬁnishes the proof.

2

2

=

− λ ≤ · = 1,

2

2

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel
We use this lemma to show cocoercivity of FEG,γ when F is aﬃne.
Monotone & -Lipschitz 5i

0 0i

-cocoercive

−5i 0

5

10

Figure 4: Illustration of the constraint on the spectrum of the Jacobian of a -cocoercive operator. In yellow, the constraint for the eigenvalues of the Jacobian Sp(∇F (x)) of a monotone and -Lipschitz operator are shown. Red region Re(1/λ) ≥ 1/ corresponds to the constraints for the eigenvalues λ of the Jacobian of a -cocoercive operator (Lemma C.8).

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

D MISSING PROOFS AND DETAILS FROM SECTION 3

D.1 Cocoercivity of EG Operator in the Aﬃne Case
Lemma D.1. Let F : Rd → Rd be aﬃne, monotone and L-Lipschitz operator. Then, for all λ ∈ Sp(∇FEG,γ) we have Re(1/λ) ≥ γ/2 for all 0 < γ ≤ 1/L. In view of Lemma C.8, this implies that FEG,γ = F (Id − γF ) is 2/γ-cocoercive for all 0 < γ ≤ 1/L.

Proof. Since F = Ax + b is monotone and L-Lipschitz, we have

Sp(∇F ) = Sp(A) ⊆ {λ ∈ C | Re(λ) ≥ 0 & |λ| ≤ L}.

Next, FEG,γ(x) = A (x − γAx − γb) + b = A (I − γA) x − γAb + b and Sp(∇FEG,γ) = Sp (A (I − γA)) = {λ(1 − γλ) | λ ∈ Sp(A)}.
Therefore, it is suﬃcient to prove

{λ(1 − γλ) | Re(λ) ≥ 0 & |λ| ≤ L} ⊆ D1/γ (1/γ) := {λ ∈ C | |λ − 1/γ| ≤ 1/γ} ,

since Re(1/λ) ≥ γ/2 is equivalent to λ ∈ D1/γ (1/γ). In the remaining part of the proof, we will show even stronger

result:

{λ(1 − γλ) | Re(λ), Im(λ) ∈ [0, L]} ⊆ D1/γ (1/γ) .

(40)

Consider arbitrary λ = λ0 + iλ1 such that λ0, λ1 ∈ [0, L]. Then,

λ(1 − γλ) = (λ0 + iλ1) (1 − γλ0 − iγλ1) = λ0(1 − γλ0) + γλ21 + i (λ1(1 − γλ0) − γλ0λ1) = λ0(1 − γλ0) + γλ21 + iλ1 (1 − 2γλ0) ,

implying that

|λ(1 − γλ) − 1/γ|2

=

x:=λ0, y:=λ21
=

γλ0(1 − γλ0) + γ2λ21 − 1 2 2

2

γ

+ λ1(1 − 2γλ0)

γx(1 − γx) + γ2y − 1 2 + y(1 − 2γx)2. γ

One can notice that the expression above is a convex function of y. Since 0 ≤ y ≤ L2, we have

|λ(1 − γλ) − 1/γ|2 ≤ max

γx(1 − γx) − 1 2 , γx(1 − γx) + γ2L2 − 1 2 + L2(1 − 2γx)2 .

γ

γ

Since x ∈ [0, L] and γ ≤ 1/L we have 0 ≤ x(1 − γx) ≤ x ≤ L ≤ γ1 implying

γx(1 − γx) − 1 2

1

γ ≤ γ2 .

Next, we consider the second term in the maximum as a function of x:

f (x) =

γx(1 − γx) + γ2L2 − 1 2 + L2(1 − 2γx)2 γ

= −γx2 + x + γL2 − 1 2 + L2(1 − 4γx + 4γ2x2) γ

= γ2x4 + x2 + γ2 L2 − 1 2 − 2γx3 − 2γ2x2 L2 − 1 + 2γx L2 − 1

γ2

γ2

γ2

+L2 − 4γL2x + 4γ2L2x2

= γ2x4 − 2γx3 + x2 1 + 2γ2 L2 + 1 γ2

− 2γx L2 + 1 γ2

+ L2 + γ2

L2 − 1 2 . γ2

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Since for all x ∈ [0, L]

f (x) = 12γ2x2 − 12γx + 2 + 4γ2 L2 + 1 γ2

= 12γ2 x2 − x + 1 + L2 + 1 γ 6γ2 3 3γ2

= 12γ2

1 2 L2 1 x − 2γ + 3 + 4γ2 > 0,

function f (x) is convex. Therefore,

f (x) ≤ max {f (0), f (L)}

= max

γL2 − 1 γ

2
+ L2,

γL(1 − γL) + γ2L2 − 1 γ

2
+ L2(1 − 2γL)2

= max

γL2 − 1 2 + L2, L − 1 2 + L2(1 − 2γL)2 .

γ

γ

Since γ ≤ 1/L, we have

γL2 − 1 2 + L2 = γ2L4 − 2L2 + 1 + L2

γ

γ2

=

1

+ L2

γ2L2 − 1

≤

1 ,

γ2

γ2

and

L − 1 2 + L2(1 − 2γL)2 = L2 − 2L + 1 + L2 − 4γL3 + 4γ2L4

γ

γ γ2

=

1

+ 2L (γL − 1) + 4γL3(γL − 1) ≤

1 .

γ2 γ

γ2

Putting all together, we get f (x) ≤ 1/γ2 and, as a result, |λ(1 − γλ) − 1/γ|2 ≤ 1/γ2. Therefore, (40) holds. This ﬁnishes the proof.

D.2 Last-Iterate Convergence of EG in the Aﬃne Case
Theorem D.1 (Last-iterate convergence of (EG-γ1-γ2): aﬃne case). Let F : Rd → Rd be aﬃne, monotone and L-Lipschitz, 0 < γ2 ≤ γ1/2, 0 < γ1 ≤ 1/L. Then for all K ≥ 0 we have
E F (xK ) 2 ≤ 2 x0 − x∗ 2 , (41) γ1γ2(K + 1)
where xK = xK − γ1F (xK ) and xK is produced by (EG-γ1-γ2).

Proof. Lemma D.1 and Theorem 2.2 imply

K 2 2 x0 − x∗ 2

FEG,γ1 (x )

≤

.

γ1γ2(K + 1)

Since by deﬁnition of FEG,γ1 we have FEG,γ1 (xK ) = F (xk − γ1F (xK )) = F (xK ), (41) holds.

D.3 Linear Case: Non-Spectral Analysis of Extragradient Method
In this subsection, we give an alternative proof of cocoercivity of extragradient operator when F is linear, monotone and L-Lipschitz.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Lemma D.2. Let F : Rd → Rd be linear, monotone and L-Lipschitz operator. Then, for all γ ≤ 1/L extragradient operator FEG,γ = F (Id − γF ) is 2/γ-cocoercive.

Proof. Lemma 2.1 implies that it is suﬃcient to prove non-expansiveness of Id − γFEG,γ. Consider arbitrary x, y ∈ Rd and deﬁne

x = x − γF (x), y = y − γF (y), x = x − γF (x), y = y − γF (y).

Our goal is to show that x − y ≤ x − y . Using the monotonicity of F and

2 a, b = a 2 + b 2 − b − a 2

(42)

that holds for all a, b ∈ Rd, we derive

x−y 2 = = =

x − y 2 − 2γ x − y, F (x) − F (y) + γ2 F (x) − F (y) 2 x − y 2 − 2γ x + γF (x) − y − γF (y), F (x) − F (y) + γ2 F (x) − F (y) 2 x − y 2 − 2γ x − y, F (x) − F (y) −γ2 2 F (x) − F (y), F (x) − F (y) − F (x) − F (y) 2

(1),(42)

≤

x − y 2 + γ2 F (x) − F (y) − F (x) + F (y) 2 − F (x) − F (y) 2 .

(43)

Next, since F is linear and L-Lipschitz, we have

F (x) − F (y) − F (x) + F (y) 2 − F (x) − F (y) 2 = =

F (x − x) − F (y − y) 2 − F (x) − F (y) 2 F (γF (x)) − F (γF (y)) 2 − F (x) − F (y) 2

(2)
≤ L2γ2 − 1 F (x) − F (y) 2

≤ 0,

where in the ﬁnal step we apply γ ≤ 1/L. Putting this inequality in (43) we obtain x − y 2 ≤ x − y 2 that ﬁnishes the proof.

D.4 Proof of Lemma 3.1

Lemma D.3 (Lemma 3.1; Star-cocoercivity of extragradient operator). Let F : Rd → Rd be star-monotone

around x∗, i.e., F (x∗) = 0 and

∀x ∈ Rd F (x), x − x∗ ≥ 0,

(44)

and L-Lipschitz. Then, extragradient operator FEG,γ = F (Id − γF ) with γ ≤ 1/L is 2/γ-star-cocoercive around x∗.

Proof. Lemma C.6 implies that

FEG,γ

is

2 -star-cocoercive

around

x∗

⇐⇒

Id − γFEG,γ is non-expansive around x∗.

γ

Consider arbitrary x ∈ Rd and deﬁne

x = x − γF (x), x = x − γF (x).

Since FEG,γ(x∗) = 0, our goal is to show that x − x∗ ≤ x − x∗ . Using star-monotonicity of F and (42), we derive

x − x∗ 2 = = =

x − x∗ x − x∗ x − x∗

2 − 2γ x − x∗, F (x) + γ2 F (x) 2 2 − 2γ x + γF (x) − x∗, F (x) + γ2 F (x) 2 2 − 2γ x − x∗, F (x)) − γ2 2 F (x), F (x) −

F (x) 2

(9),(42)

≤

x − x∗ 2 + γ2 F (x) − F (x) 2 − F (x) 2 .

(45)

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Next, since F is L-Lipschitz, we have
(2)
F (x) − F (x) 2 − F (x) 2 ≤ L2 x − x 2 − F (x) 2 = L2γ2 − 1 F (x) 2 ≤ 0,
where in the ﬁnal step we apply γ ≤ 1/L. Putting this inequality in (45) we obtain x − x∗ 2 ≤ ﬁnishes the proof.

x − x∗ 2 that

D.5 Details on Performance Estimation Problem for Showing Non-Cocoercivity of Extragradient Operator

First of all, we provide the formulas for the matrices M0, . . . , M7 deﬁning problem (15):

 1 −1 0 0 −γ2 γ2 

0 0 0 0 0 0

 −1  0 M0 =  0

1 0 0 γ2 −γ2

0 0 0 0 0 0

0 00 0

 0

 0 0

, M=

γ1 − 1

0

1 − γ21

 0
,

 0 0 0 0 0

1

 0 0

0

0

0

 0

 −γ2

γ2

00

γ22

−γ22

γ2 −γ2 0 0 −γ22 γ22

0 0 1 − γ21 0 −1 0 00 0 0 0 0

0

0 

 M2 = 

2

− 2



0

0

0 0

0 0

 

0

0

M4 = − 2 2




2

−2

00

0 0 0 M6 = 0  0
0

0 2 −2 0 0 −2 2 0 − 2 −1 1 0 2 1 −1 0 0 0 00
0 0 00

0 −2 2

0
0
γ1 2
− γ21

2 −2 γ21 − γ21 −1 1
1 −1

000

00 0 0 00 0 0 00 0 0
0 0 γ1 − 1 0 00 0 0 0 0 1 − γ21 0

0

0 0

0

0 0





0 , M =  2

−2

 0

3

 0

0





0

0 0

0

−2 2

0

0 0

0

0 0

0

 

0

0

, 0

M5

=

 

0

0





0


2

−2

0

−2 2

0

0

0

 

1 − γ21  ,



0

1
−1  0 M7 =  0  0

−1

0

2
−2 −1
γ1 2
0
1
0 0
0
0 − γ21
γ1 2
−1 1 0 0 0 0

0 0
γ1 2
0 0 − γ21
0 0
0
0
γ1 2
− γ21
00 00 00 00 00 00

0 −2 

0 2

0

1

 

0

−

γ1  , 

2

0 0

0 −1

2
−2 − γ21
γ1 2
−1
1

−2 

2 γ1 

2 γ

 ,

−

1
2



1

−1

0 0
0 0 
0 0 .
0 0 
0 0
00

Next, we provide a rigorous proof that the example (17) is valid, i.e., we prove Theorem 3.2.
Theorem D.2 (Theorem 3.2). For all > 0 and γ1 ∈ (0, 1/ ] there exists -cocoercive operator F such that F (x) = xF1 , F (y) = yF1 , F (x − γ1xF1 ) = xF2 , F (y − γ1yF1 ) = yF2 for x, y, xF1 , yF1 , xF2 , yF2 deﬁned in (17) and

x − γ2F (x − γ1F (x)) − y + γ2F (y − γ1F (y)) > 1 = x − y

(46)

for all γ2 > 0, i.e., FEG,γ1 = F (Id − γ1F ) is non-cocoercive.

Proof. Proposition 2 from Ryu et al. (2020) implies that it is suﬃcient to show that
xF1 − xF2 , γ1xF1 ≥ xF1 − xF2 2, xF1 − yF1 , x − y ≥ xF1 − yF1 2, xF1 − yF2 , x − y + γ1yF1 ≥ xF1 − yF2 2, xF2 − yF1 , x − γ1xF1 − y ≥ xF2 − yF1 2, xF2 − yF2 , x − γ1xF1 − y + γ1yF1 ≥ xF2 − yF2 2, yF1 − yF2 , γ1yF1 ≥ yF1 − yF2 2

(47) (48) (49) (50) (51) (52)

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

in order to prove that there exists -cocoercive F such that F (x) = xF1 , F (y) = yF1 , F (x − γ1xF1 ) = xF2 , F (y − γ1yF1 ) = yF2 . Below we derive these inequalities for x, y, xF1 , yF1 , xF2 , yF2 deﬁned in (17).

Proof of (47). We have

xF1 − xF2 , γ1xF1 − xF1 − xF2 2 = =

− 1 + 1 − γ1

2γ1

2γ1

2

2

− = 0. 44

−γ1

1 1 − γ1 2

· −− +

2γ1

2γ1

2γ1

Proof of (48). We have

xF1 − yF1 , x − y − xF1 − yF1 2 = −

− 1 + 1 − γ1

2γ1

2γ1

1 1 − γ1 2

−− +

2γ1

2γ1

1 1 + γ1 2

−

−

2γ1

2γ1

2

2

2

= − − = 0. 244

Proof of (49). We have

xF1 − yF2 , x − y + γ1yF1 − xF1 − yF2 2 =

− 1 + 1 − γ1

2γ1

2γ1

−1 + 1 − γ1 2

+ 1 − 1 − γ12 2 · 1 + γ1

2γ1

2γ1

2

− − 1 + 1 − γ1

2γ1

2γ1

2 − 1 − 1 − γ12 2 2

2γ1

2γ1

= 2(1 + γ1 ) + γ1 3(1 + γ1 ) − 2 − γ12 4

4

4

44

= γ1 > 0. 2

Proof of (50). We have xF2 − yF1 , x − γ1xF1 − y − xF2 − yF1 2 = =

1 − 1 + γ1

2γ1

2γ1

2

2

− = 0. 44

−γ1

1 1 + γ1 2

·−

−

2γ1

2γ1

2γ1

Proof of (51). We have

xF2 − yF2 , x − γ1xF1 − y + γ1yF1 − xF2 − yF2 2 =

1 − 1 − γ12 2

2γ1

2γ1

− 1 + 1 + γ1

2

2

− 1 − 1 − γ12 2 2

2γ1

2γ1

= γ12 4 − γ12 4 = 0.

4

4

Proof of (52). We have yF1 − yF2 , γ1yF1 − yF1 − yF2 2 = =

1 + γ1 − 1 − γ12 2

2γ1

2γ1

1 + γ1 − 2

1 + γ1 − 1 − γ12 2 2

2γ1

2γ1

2(1 + γ1 )2 − 2(1 + γ1 )2 = 0.

4

4

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

That is, inequalities (47)-(52) hold and, as a result, there exists -cocoercive operator F such that F (x) = xF1 , F (y) = yF1 , F (x − γ1xF1 ) = xF2 , F (y − γ1yF1 ) = yF2 . Finally, for any γ2 > 0 we have

2

2 1 1 − γ12 2 2

x − γ2F (x − γ1F (x)) − y + γ2F (y − γ1F (y))

=

1 + γ2

− 2γ1

2γ1

= 1 + γ12γ22 4 > 1 = x − y 2. 4

In view of Lemma 2.1, it means that operator FEG,γ1 = F (Id − γ1F ) is non-cocoercive.

We emphasize that in the example (17) one can multiply all points by arbitrary α > 0 and get x − y = α: the proof will remain almost unchanged. That is, the points x, y can be arbitrary close/far to each other in the example showing non-cocoercivity of EG operator.
D.6 Proof of Lemma 3.2
As we explain in Section 3, we obtain the proof of Lemma 3.2 via solving the following problem:
∆EG(L, γ1, γ2) = max F (x1) 2 − F (x0) 2 s.t. F is monotone and L-Lipschitz, x0 ∈ Rd, x0 − x∗ 2 ≤ 1, x1 = x0 − γ2F x0 − γ1F (x0)
with γ1 = γ2 = γ. As for (19), we construct a corresponding SDP and solve it for diﬀerent values of L and γ. In these numerical tests, we observed that ∆EG(L, γ1, γ2) ≈ 0 for all tested pairs of L and γ and the dual variables λ1, λ2, λ3 that correspond to the constraints
0 ≤ 1 F (xk) − F (xk+1), xk − xk+1 , γ
0 ≤ 1 F (xk − γF (xk)) − F (xk+1), xk − γF (xk) − xk+1 , γ
F (xk − γF (xk)) − F (xk+1) 2 ≤ L2 xk − γF (xk) − xk+1 2
are always close to the constants 2, 1/2, and 3/2, while other dual variables are negligible. Although λ2 and λ3 were sometimes slightly smaller, e.g., sometimes we had λ2 ≈ 3/5 and λ3 ≈ 13/20, we simpliﬁed these dependencies and simply summed up the corresponding inequalities with weights λ1 = 2, λ2 = 1/2 and λ3 = 3/2 respectively. After that it was just needed to rearrange the terms and apply Young’s inequality to some inner products.
The rigorous proof is provided below. Lemma D.4 (Lemma 3.2). Let F : Rd → Rd be monotone and L-Lipschitz, 0 < γ ≤ 1/√2L. Then for all k ≥ 0 the iterates produced by (EG) satisfy F (xk+1) ≤ F (xk) .

Proof. Since F is monotone and L-Lipschitz we have
0 ≤ F (xk) − F (xk+1), xk − xk+1 , 0 ≤ F (xk − γF (xk)) − F (xk+1), xk − γF (xk) − xk+1 , F (xk − γF (xk)) − F (xk+1) 2 ≤ L2 xk − γF (xk) − xk+1 2.
Using the update rule of (EG) and introducing new notation xk = xk − γF (xk), we get
0 ≤ F (xk) − F (xk+1), F (xk) , 0 ≤ F (xk) − F (xk+1), F (xk) − F (xk) , F (xk) − F (xk+1) 2 ≤ L2γ2 F (xk) − F (xk) 2.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Summing up these inequalities with weights λ1 = 2, λ2 = 1/2 and λ3 = 3/2 respectively, we derive

3 F (xk) − F (xk+1) 2

≤

2 F (xk) − F (xk+1), F (xk)

1 +

F (xk) − F (xk+1), F (xk) − F (xk)

2

2

+ 3L2γ2 F (xk) − F (xk) 2. 2

Next, we expand the squared norms and rearrange the terms:

3 F (xk+1) 2 ≤ 2
=

2 − 1 − 3L2γ2 2

F (xk), F (xk) +

1 −2 − + 3

2

F (xk+1), F (xk)

+ 1 F (xk+1), F (xk) + 1 − 3 + 3L2γ2

2

22

2

F (xk) 2 + 3L2γ2 F (xk) 2 2

3 − 3L2γ2 2

F (xk), F (xk)

1 +

F (xk+1), F (xk)

1 +

F (xk+1), F (xk)

2

2

3L2γ2

+

−1

2

F (xk) 2 + 3L2γ2 F (xk) 2. 2

We notice that 23 − 3L2γ2 ≥ 0 since γ ≤ √12L . Therefore, applying Young’s inequality (28) to upper bound the inner products, we derive

3 F (xk+1) 2 ≤ 2
=

3 3L2γ2

−

4

2

F (xk) 2 +

F (xk) 2

1 +

4

F (xk+1) 2 + F (xk) 2

+ 1 F (xk+1) 2 + F (xk) 2 + 3L2γ2 − 1 F (xk) 2 + 3L2γ2 F (xk) 2

4

2

2

F (xk) 2 + 1 F (xk+1) 2. 2

Rearranging the terms, we get the result.

D.7 Proof of Theorem 3.3

Theorem D.3 (Theorem 3.3; Last-iterate convergence of (EG): non-linear case). Let F : Rd → Rd be monotone and L-Lipschitz. Then for all K ≥ 0

F (xK ) 2 ≤ x0 − x∗ 2 , (53) γ2(1 − L2γ2)(K + 1)

where xK is produced by (EG) with stepsize 0 < γ ≤ 1/√2L. Moreover,

GapF (xK ) =

max

F (y), xK − y ≤

y∈Rd: y−x∗ ≤ x0−x∗

γ

2 x0 − x∗ 2

√

.

1 − L2γ2 K + 1

(54)

Proof. We notice that in the proof of Lemma D.2 we get

(43)
x − y 2 ≤ x − y 2 + γ2 F (x) − F (y) − F (x) + F (y) 2 − F (x) − F (y) 2

without using linearity of F . Here, x and y are arbitrary points in Rd and

x = x − γF (x), y = y − γF (y), x = x − γF (x), y = y − γF (y).

Taking y = x∗ and x = xk we get y = y = x∗, x = xk+1, and

xk+1 − x∗ 2 ≤
(2)
≤

xk − x∗ 2 + γ2 F (xk − γF (xk)) − F (xk) − γ2 F (xk) 2 xk − x∗ 2 + L2γ4 F (xk) 2 − γ2 F (xk) 2

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

implying

γ2(1 − L2γ2) F (xk) 2 ≤ xk − x∗ 2 − xk+1 − x∗ 2.

(55)

Summing up these inequalities for k = 0, 1, . . . , K and dividing the result by γ2(1 − L2γ2)(K + 1), we obtain

1 K F (xk) 2 ≤

1 K xk − x∗ 2 − xk+1 − x∗ 2

K +1

γ2(1 − L2γ2)(K + 1)

k=0

k=0

x0 − x∗ 2 − xK+1 − x∗ 2 = γ2(1 − L2γ2)(K + 1)

x0 − x∗ 2 ≤ γ2(1 − L2γ2)(K + 1) .

Next, applying Lemma 3.2, we conclude

F (xK ) 2 ≤ 1

K
F (xk) 2 ≤

x0 − x∗ 2 ,

K +1

γ2(1 − L2γ2)(K + 1)

k=0

which gives (53). Finally, we notice that xk+1 − x∗ ≤ xk − x∗ , which can be seen from (55). Therefore, using monotonicity of F and Cauchy-Schwarz inequality, we derive

GapF (xK ) =

max

F (y), xK − y

y∈Rd: y−x∗ ≤ x0−x∗

(1)

≤

max

F (xK ), xK − y

y∈Rd: y−x∗ ≤ x0−x∗

≤ F (xK ) ·

max

xK − y

y∈Rd: y−x∗ ≤ x0−x∗

(55)
≤ 2 F (xK ) · x0 − x∗

(53)

2 x0 − x∗ 2

≤

√

,

γ 1 − L2γ2 K + 1

which ﬁnishes the proof.

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

E MISSING PROOFS FROM SECTION 4

E.1 Proof of Theorem 4.1 For convenience, we derive non-cocoercivity of FOG,γ and FEFTP,γ separately.

E.1.1 Non-Cocoercivity of FOG,γ
Before we provide the proof, we state the following technical lemma. Lemma E.1. Let operator F : Rd → Rd be linear, x∗ be such that F (x∗) = 0, and γ > 0. Then, operator FOG,γ is linear and FOG,γ(z∗) = 0 for z∗ = ((x∗) , (x∗) ) .

Proof. We start with proving linearity. Consider arbitrary

α, β ∈ R, x, y, x , y ∈ Rd, z = x , z = x ∈ R2d.

y

y

Then

FOG,γ (αz + βz ) =

2F −F − γ1 Id γ1 Id

αx + βx αy + βy

2F (αx + βx ) − F (αy + βy )

= − 1 (αx + βx ) + 1 (αy + βy )

γ

γ

2F (x) − F (y)

2F (x ) − F (y )

= α −1x+ 1y +β −1x + 1y

γ

γ

γ

γ

2F −F x

2F −F x

= α − γ1 Id γ1 Id y + β − γ1 Id γ1 Id y

= αFOG,γ (z) + βFOG,γ (z ),

i.e., FOG,γ is linear. Next, let F (x∗) = 0 for some x∗. For

z∗ = xx∗∗

we derive that FOG,γ(z∗) = 0:

FOG,γ (z∗) =

2F
1

−F
1

− γ Id γ Id

x∗

2F (x∗) − F (x∗)

0

x∗ = − γ1 x∗ + γ1 x∗ = 0 = 0.

Using this lemma, we establish the following result.
Theorem E.1 (Non-cocoercivity of FOG,γ). Let the linear operator F (x) = Ax be monotone and L-Lipschitz. Assume that Sp(∇F (x)) = Sp(A) contains at least one eigenvalue λˆ such that Re(λˆ) = 0 and Im(λˆ) = 0. Then, for any > 0 and γ > 0 operator FOG,γ is not -star-cocoercive around x∗.

Proof. In view of Lemma C.7, it is suﬃcient to show that FOG,γ is not -cocoercive for any positive , γ > 0. Since Sp(A) contains λˆ with Re(λˆ) = 0 and Im(λˆ) = 0, Sp(A) is not contained in any disk centered in /2 and
of radius /2. Therefore, due to Lemma C.8 operator F is not -cocoercive for any . Let us ﬁx arbitrary > 0
and γ > 0. There exist points x, x such that

F (x) − F (x ) 2 > x − x , F (x) − F (x )

(56)

2

Let us show that FOG,γ is not -cocoercive. In view of Lemma 2.1, it is suﬃcient to show that Id − 2 FOG,γ is not non-expansive. Consider the following points:

x

2

x

2

z = x∗ , zˆ = z − FOG,γ (z), z = x∗ , zˆ = z − FOG,γ (z ).

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Then

zˆ − zˆ 2 =

2

2

2

z − FOG,γ (z) − z + FOG,γ (z )

x 2 2F −F x

x

2 2F −F x 2

= x∗ − − γ1 Id γ1 Id x∗ − x∗ + − γ1 Id γ1 Id x∗

x − 4 F (x) − x + 4 F (x )

2

=

x∗ + 2 x − 2 x∗ − x∗ − 2 x + 2 x∗

γ

γ

γ

γ

= x − x − 4 (F (x) − F (x )) 2 + 4 x − x 2 2γ2

=

4 1+

x − x 2 + 16 F (x) − F (x ) 2 − x − x , F (x) − F (x )

2γ2

2

2

(56)

4

> 1+

x − x 2 > x − x 2 = z − z 2,

2γ2

i.e., Id − 2 FOG,γ is not non-expansive.

E.1.2 Non-Cocoercivity of FEFTP,γ

First of all, for any

z= x y

one can rewrite FEFTP,γ (z) as

FEFTP,γ (z) =

F (x − γF (y)) 1 (y − x) + F (y) .
γ

Using this, we derive the following technical result.

Lemma E.2. Let operator F : Rd → Rd be linear, x∗ be such that F (x∗) = 0, and γ > 0. FEFTP,γ is linear and FEFTP,γ (z∗) = 0 for z∗ = ((x∗) , (x∗) ) .

Then, operator

Proof. We start with proving linearity. Consider arbitrary

α, β ∈ R, x, y, x , y ∈ Rd, z = x , z = x ∈ R2d.

y

y

Then

FEFTP,γ (αz + βz ) =

F (αx + βx − γF (αy + βy )) γ1 (αy + βy − αx − βx ) + F (αy + βy )

F (αx − γαF (y) + βx − γβF (y ))

= 1 (αy − αx) + αF (y) + 1 (βy − βx ) + βF (y )

γ

γ

F (x − γF (y))

F (x − γF (y ))

= α 1 (y − x) + F (y) + β 1 (y − x ) + F (y )

γ

γ

= αFEFTP,γ (z) + βFEFTP,γ (z ),

i.e., FEFTP,γ is linear. Next, let F (x∗) = 0 for some x∗. For

z∗ = xx∗∗

we derive that FEFTP,γ (z∗) = 0:

∗

F (x∗ − γF (x∗))

F (x∗)

0

FOG,γ (z ) = 1 (x∗ − x∗) + F (y∗) = 0 = 0 = 0.

γ

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

Using this lemma, we establish the following result.
Theorem E.2 (Non-cocoercivity of FEFTP,γ). Let the linear operator F (x) = Ax be monotone and L-Lipschitz. Assume that Sp(∇F (x)) = Sp(A) contains at least one eigenvalue λˆ such that Re(λˆ) = 0 and Im(λˆ) = 0. Then, for any > 0 and γ > 0 operator FEFTP,γ is not -star-cocoercive around x∗.

Proof. In view of Lemma C.7, it is suﬃcient to show that FEFTP,γ is not -cocoercive for any positive , γ > 0. Since Sp(A) contains λˆ with Re(λˆ) = 0 and Im(λˆ) = 0, Sp(A) is not contained in any disk centered in /2 and
of radius /2. Therefore, due to Lemma C.8 operator F is not -cocoercive for any . Let us ﬁx arbitrary > 0
and γ > 0. There exist points x, x such that

F (x) − F (x ) 2 > x − x , F (x) − F (x )

(57)

Let us show that FEFTP,γ is not -cocoercive. In view of Lemma 2.1, it is suﬃcient to show that Id − 2 FEFTP,γ is not non-expansive. Consider the following points:

x

2

x

2

z = x∗ , zˆ = z − FEFTP,γ (z), z = x∗ , zˆ = z − FEFTP,γ (z ).

Then

zˆ − zˆ 2 =
=
=
= =
(57)
>

2

2

2

z − FEFTP,γ (z) − z + FEFTP,γ (z )

x2

F (x)

x

2

F (x )

2

x∗ − − γ1 (x∗ − x) − x∗ + − γ1 (x∗ − x )

x − 2 F (x) − x + 2 F (x )

2

x∗ − 2γ x + 2γ x∗ − x∗ + 2γ x − 2γ x∗

x − x − 2 (F (x) − F (x )) 2 + 4 x − x 2 2γ2

4 1 + 2γ2

x − x 2 + 4 F (x) − F (x ) 2 − x − x , F (x) − F (x )
2

4 1 + 2γ2

x − x 2 > x − x 2 = z − z 2,

i.e., Id − 2 FEFTP,γ is not non-expansive.

E.2 Random-Iterate Convergence of (EFTP) for Star-Monotone Operators

Theorem E.3 (Random-iterate convergence of (EFTP)). Let F : Rd → Rd be star-monotone around x∗, i.e., F (x∗) = 0 and
∀x ∈ Rd F (x), x − x∗ ≥ 0,
and L-Lipschitz. Then for all K ≥ 0 we have

K2

x0 − x∗ 2

E F (x ) ≤ γ2(1 − 10γ2L2)(K + 1) ,

(58)

where xK is chosen uniformly at random from the set of iterates {x0, x1, . . . , xK } produced by (EFTP) with 0 < γ < 1/√10L.

Proof. Lemma A.1 with x = x∗ implies 2γ F (xk+1), xk+1 − x∗ ≤ xk − x∗ 2 − xk+1 − x∗ 2 − xk+1 − xk 2 + γ2L2 xk − xk+1 2.

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Since F is star monotone and xk+1 − xk = −γF (xk), we have

γ2 F (xk) 2 ≤
(26)
=
(29)
≤
=

xk − x∗ 2 − xk+1 − x∗ 2 + γ2L2 xk − xk+1 2

xk − x∗ 2 − xk+1 − x∗ 2 + γ4L2 2F (xk) − F (xk−1) 2

xk − x∗ 2 − xk+1 − x∗ 2 + γ4L2

1 1+

4

2F (xk) 2

+γ4L2(1 + 4) F (xk−1) 2 xk − x∗ 2 − xk+1 − x∗ 2 + 5γ4L2 F (xk) 2 + 5γ4L2 F (xk−1) 2

for k ≥ 1, and

γ2 F (x0) 2

≤
x0 =x0
= ≤

x0 − x∗ 2 − x1 − x∗ 2 + γ2L2 x0 − x1 2
x0 − x∗ 2 − x1 − x∗ 2 + γ4L2 F (x0) 2 x0 − x∗ 2 − x1 − x∗ 2 + 5γ4L2 F (x0) 2.

Rearranging the terms, we derive for all k ≥ 1 that

γ2(1 − 5γ2L2) F (xk) 2 ≤ xk − x∗ 2 − xk+1 − x∗ 2 + 5γ4L2 F (xk−1) 2,

(59)

γ2(1 − 5γ2L2) F (x0) 2 ≤ x0 − x∗ 2 − x1 − x∗ 2.

(60)

Next, we sum up inequalities (59) for k = 1, . . . , K and (60):

K

K

γ2(1 − 5γ2L2) F (xk) 2 ≤

K −1

xk − x∗ 2 − xk+1 − x∗ 2 + 5γ4L2

F (xk) 2

k=0

k=1

k=0

K

≤ x0 − x∗ 2 + 5γ4L2

F (xk) 2.

k=0

Rearranging the terms and dividing the result by γ2(1 − 10γ2L2)(K + 1), we get

1 K F (xk) 2 ≤

x0 − x∗ 2 .

K +1

γ2(1 − 10γ2L2)(K + 1)

k=0

Finally, since xK is chosen uniformly at random from the set {x0, x1, . . . , xK } we derive

F (xK ) 2 = 1

K
F (xk) 2 ≤

x0 − x∗ 2 .

E

K +1

γ2(1 − 10γ2L2)(K + 1)

k=0

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities

F HAMILTONIAN GRADIENT METHOD

Although Hamiltonian Gradient Method (HGM) is not considered as an approximation of PP, it attracted a

lot of attention during the recent few years. Therefore, it is worth to study whether the operator of HGM is

cocoercive. First of all, HGM

xk+1 = xk − γ∇F (xk) F (xk),

(HGM)

can be seen as GD applied to minimize function H(x) = F (x) 2/2. The corresponding operator is FH(x) = ∇F (x) F (x).

F.1 Aﬃne Case
Let us start with the aﬃne case. Lemma F.1. Let F (x) = Ax + b be L-Lipschitz. Then, Hamiltonian operator FH(x) = ∇F (x) F (x) is Lcocoercive.

Proof. We have

H(x) = 1 F (x) 2 2

= 1 Ax + b 2 2

=

1 x

A

Ax + b

1 Ax +

b

2.

2

2

Since ∇2H(x) = A A 0 function H(x) is convex. Next, L-Lipschitzness of A implies that A 2 = λmax(A A) ≤ L, i.e., ∇2H(x) = A A LI. Therefore, H(x) is L-smooth function. It is well known (Nes-
terov et al., 2018) that the gradient of convex L-smooth function is L-cocoercive, i.e., ∇H(x) = ∇F (x) F (x) =
FH(x) is L-cocoercive operator.

As a direct application of Theorem 2.2 we get the following result.
Theorem F.1 (Last-iterate convergence of (HGM): aﬃne case). Let F (x) = Ax + b be L-Lipschitz. Then for all K ≥ 0 we have
∇F (xK ) F (xk) 2 ≤ L x0 − x∗ 2 , (61) γ(K + 1)
where xK is produced by (HGM) with 0 < γ ≤ 1/L.

However, this theorem completely ignores the fact that ∇F (x) F (x) corresponds to the gradient of function H(x). Taking into account that H(x) = 21 Ax + b 2, one can prove that H(x) is quasi-strongly convex (Necoara et al., 2019) and get the following result for Gradient Descent applied to minimize function H(x).
Theorem F.2 (See Theorem 11 from Necoara et al. (2019)). Let F (x) = Ax + b. Then for all K ≥ 0 we have

xK − x∗ 2 ≤ 1 − κ(A) x0 − x∗ 2,

(62)

1 + κ(A)

where κ(A) = / , σm 2 in(A) σm 2 ax(A) σm2 in(A) and σm2 ax(A) are the smallest non-zero and the largest singular values of A respectively, and xK is produced by (HGM) with γ = 1/σm 2 ax(A).

Similar results are also derived in Abernethy et al. (2019); Loizou et al. (2020).

F.2 General Case
Next, we consider the setup when F is monotone, L-Lipschitz, but not necessarily aﬃne. In this case, it turns out that Hamiltonian operator FH can be non-cocoercive and function H can be non-convex. To prove this, we provide an example of convex smooth function f (x) such that ∇f (x) 2 is non-convex.

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel
Theorem F.3 (Non-cocoercivity of the Hamiltonian operator). Consider strongly convex smooth function f (x) = ln(1 + ex) + 2x020 of a scalar argument x ∈ R. Then, Hamiltonian operator H(x) = ∇F (x) F (x) is non-cocoercive for monotone Lipschitz F (x) = ∇f (x).

Proof. Function f (x) = ln(1 + ex) + 2x020 is logistic loss with 2-regularization. Therefore, it is strongly convex smooth function and its gradient

ex

x

F (x) = ∇f (x) = 1 + ex + 100

is (strongly) monotone and Lipschitz operator. Below we prove that Hamiltonian function H(x) = 21 F (x) 2 is non-convex. To show that we compute its second derivative:

2H(x) =

ex

x2

e2x

xex

x2

1 + ex + 100 = (1 + ex)2 + 50(1 + ex) + 10000 ,

2e2x

2e3x

(x + 1)ex

xe2x

x

2∇H(x) = (1 + ex)2 − (1 + ex)3 + 50(1 + ex) − 50(1 + ex)2 + 5000

2e2x

e2x + ex(x + 1) x

= (1 + ex)3 + 50(1 + ex)2 + 5000

2∇2H(x) =

4e2x

6e3x

2e2x + ex(x + 2) 2e3x + 2e2x(x + 1) 1

−

+

−

+

(1 + ex)3 (1 + ex)4

50(1 + ex)2

50(1 + ex)3

5000

2e2x(2 − ex) ex (x + 2 + ex(2 − x)) 1

= (1 + ex)4 +

50(1 + ex)3

+. 5000

Using simple computations one can check H (x) is negative for some x ∈ R, e.g., one can check that H (3) < 0. Therefore, function H(x) is non-convex. Since convexity and smoothness of function H(x) is equivalent to the cocoercivity of its gradient ∇H(x) (Nesterov et al., 2018), we conclude that Hamiltonian operator is noncocoercive.

Finally, one can use the optimization viewpoint of the Hamiltonian method and derive O(1/K) random-iterate convergence guarantees in terms of ∇F (xK ) F (xK ) 2 when the Jacobian of F is Lipschitz-continuous but F is not necessary monotone. To show this the following lemma.
Lemma F.2 (Point-dependent smoothness of Hamiltonian function). Let operator F : Rd → Rd be L-Lipschitz, its Jacobian ∇F (x) be Λ-Lipschitz, and H(x) = 21 F (x) 2. Then for any x, y ∈ Rd the following inequality holds:
H(y) ≤ H(x) + ∇H(x), y − x + L2 + Λ F (x) y − x 2. (63) 2

Proof. Since F is L-Lipschitz its Jacobian has bounded norm: ∇F (x) = ∇F (x) this and Λ-Lipschitzness of the Jacobian, we derive for any x, y ∈ Rd

≤ L for all x ∈ Rd. Using

∇H(y) − ∇H(x) = ∇F (y) F (y) − ∇F (x) F (x)

≤ ∇F (y) F (y) − ∇F (y) F (x) + ∇F (y) F (x) − ∇F (x) F (x)

≤ ∇F (y) · F (y) − F (x) + ∇F (y) − ∇F (x) · F (x)

≤ ∇F (y) · L x − y + ∇F (y) − ∇F (x) · F (x)

≤ L2 x − y + Λ F (x) · x − y .

(64)

Extragradient Method: O (1/K) Last-Iterate Convergence for Monotone Variational Inequalities
Next, following standard arguments (Nesterov et al., 2018), we get
1
H(y) = H(x) + ∇H(x + t(y − x)), y − x dt
0 1
= H(x) + ∇H(x), y − x + ∇H(x + t(y − x)) − ∇H(x), y − x dt
0 1
≤ H(x) + ∇H(x), y − x + ∇H(x + t(y − x)) − ∇H(x) · y − x dt
0 1
(64)
≤ H(x) + ∇H(x), y − x + (L2 + Λ F (x) )t y − x 2dt
0
= H(x) + ∇H(x), y − x + L2 + Λ F (x) y − x 2. 2

Theorem F.4 (Best-iterate convergence of (HGM)). Let operator F : Rd → Rd be L-Lipschitz, its Jacobian ∇F (x) be Λ-Lipschitz. Then for any K ≥ 0 we have

min ∇F (xk) F (xk) 2 ≤ F (x0) 2 , (65)

k=0,1,...,K

γ (2 − γ(L2 + Λ F (x0) )) (K + 1)

where the sequence x0, x1, . . . , xK is generated by (HGM) with stepsize

2 γ ≤ L2 + Λ F (x0) .

Moreover, for all k ≥ 0 we have

F (xk+1) ≤ F (xk) .

(66)

Proof. We start with applying Lemma F.2: taking y = xk+1 = xk − γ∇H(xk) and x = xk in (63), we get

H(xk+1) (6≤3) H(xk) + H(xk), xk+1 − xk + L2 + Λ F (xk) xk+1 − xk 2

2

= H(xk) − γ 2 − γ(L2 + Λ F (xk) ) ∇H(xk) 2.

(67)

2

Using this inequality we will derive (66) by induction. For k = 0 we use our assumption on γ and get that 2 − γ(L2 + Λ F (xk) ≥ 0. Therefore, the second term in the right-hand side of (67) for k = 0 is non-positive. This implies that H(x1) ≤ H(x0), which is equivalent to F (x1) ≤ F (x0) . Next, assume that for some K > 0
inequality (66) holds for k = 0, 1, . . . , K − 1. Let us derive that (66) holds for k = K as well. Using (67) and
our inductive assumption, we derive

H(xK+1) ≤ H(xK ) − γ 2 − γ(L2 + Λ F (xK ) ) ∇H(xK ) 2 2

(66)
≤

H(xK ) − γ 2 − γ(L2 + Λ F (x0) )

∇H(xK ) 2 ≤ H(xK ),

2

where in the last inequality we use our assumption on γ. Therefore, F (xK+1) ≤ F (xK ) , i.e., (66) holds for all k ≥ 0. Using this, we continue our derivation from (67):

H(xk+1) ≤ H(xk) − γ 2 − γ(L2 + Λ F (xk) ) ∇H(xk) 2 2

(66)
≤

H(xk) − γ 2 − γ(L2 + Λ F (x0) )

∇H(xk) 2.

2

Eduard Gorbunov, Nicolas Loizou, Gauthier Gidel

Summing up the above inequality for k = 0, 1, . . . , K and rearranging the terms, we obtain

1 K ∇H(xk) 2 ≤

2 K H(xk) − H(xk+1)

K +1

γ (2 − γ(L2 + Λ F (x0) )) (K + 1)

k=0

k=0

2H(x0) ≤ γ (2 − γ(L2 + Λ F (x0) )) (K + 1) .

Finally, using the deﬁnition of H and

min
k=0,1,...,K

∇F (xk)

F (xk)

1K ≤
K + 1 k=0

∇H(xk) 2,

we get (65).

