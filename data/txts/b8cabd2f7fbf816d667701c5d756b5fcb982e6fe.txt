Gradient Descent-Ascent Provably Converges to Strict Local Minmax Equilibria with a Finite Timescale Separation

Tanner Fiez University of Washington
ﬁezt@uw.edu

Lillian J. Ratliﬀ University of Washington
ratliﬄ@uw.edu

arXiv:2009.14820v1 [cs.LG] 30 Sep 2020

Abstract
This paper concerns the local stability and convergence rate of gradient descent-ascent in two-player non-convex, non-concave zero-sum games. We study the role that a ﬁnite timescale separation parameter τ has on the learning dynamics where the learning rate of player 1 is denoted by γ1 and the learning rate of player 2 is deﬁned to be γ2 = τ γ1. Existing work analyzing the role of timescale separation in gradient descent-ascent has primarily focused on the edge cases of players sharing a learning rate (τ = 1) and the maximizing player approximately converging between each update of the minimizing player (τ → ∞). For the parameter choice of τ = 1, it is known that the learning dynamics are not guaranteed to converge to a game-theoretically meaningful equilibria in general as shown by Mazumdar et al. (2020) and Daskalakis and Panageas (2018). In contrast, Jin et al. (2020) showed that the stable critical points of gradient descent-ascent coincide with the set of strict local minmax equilibria as τ → ∞. In this work, we bridge the gap between past work by showing there exists a ﬁnite timescale separation parameter τ ∗ such that x∗ is a stable critical point of gradient descent-ascent for all τ ∈ (τ ∗, ∞) if and only if it is a strict local minmax equilibrium. Moreover, we provide an explicit construction for computing τ ∗ along with corresponding convergence rates and results under deterministic and stochastic gradient feedback. The convergence results we present are complemented by a non-convergence result: given a critical point x∗ that is not a strict local minmax equilibrium, then there exists a ﬁnite timescale separation τ0 such that x∗ is unstable for all τ ∈ (τ0, ∞). Finally, we extend the stability and convergence results regarding gradient descent-ascent to gradient penalty regularization methods for generative adversarial networks (Mescheder et al., 2018) and empirically demonstrate on the CIFAR-10 and CelebA datasets the signiﬁcant impact timescale separation has on training performance.

1 Introduction
In this paper we study learning in zero-sum games of the form
min max f (x1, x2)
x1∈X1 x2∈X2
where the objective function of the game f is assumed to be suﬃciently smooth and potentially non-convex and non-concave in the strategy spaces X1 and X2 respectively with each Xi a precompact subset of Rni . This general problem formulation has long been fundamental in game theory (Ba¸sar and Olsder, 1998) and recently it has become central to machine learning with applications in generative adversarial networks (Goodfellow et al., 2014), robust supervised learning (Madry et al., 2018; Sinha et al., 2018), reinforcement and multiagent reinforcement learning (Rajeswaran et al., 2020; Zhang et al., 2019), imitation learning (Ho and Ermon, 2016), constrained optimization (Cherukuri et al., 2017), and hyperparameter optimization (Lorraine et al., 2020; MacKay et al., 2019) among several others. From a game-theoretic viewpoint, the work on learning in games can, in some sense, be viewed as explaining how equilibria arise through an iterative competition for optimality (Fudenberg et al., 1998). However, in machine learning, the primary purpose of learning dynamics is to compute equilibria eﬃciently for the sake of providing meaningful solutions to problems of interest.
As a result of this perspective, there has been signiﬁcant interest in the study of gradient descent-ascent owing to the fact that the learning rule is computationally eﬃcient and a natural analogue to gradient descent

1

from function optimization. Formally, the learning dynamics are given by each player myopically updating a strategy with an individual gradient as follows:
x+1 = x1 − γ1D1f (x1, x2) x+2 = x2 + γ2D2f (x1, x2).
The analysis of gradient descent-ascent is complicated by the intricate optimization landscape in non-convex, non-concave zero-sum games. To begin, there is the fundamental question of what type of solution concept is desired. Given the class of games under consideration, local solution concepts have been proposed and are often taken to be the goal of a learning algorithm. The primary notions of equilibrium that have been adopted are the local Nash and local minmax/Stackelberg concepts with a focus on the set of strict local equilibrium that can be characterized by gradient-based suﬃcient conditions. Following several past works, from here on we refer to strict local Nash equilibrium and strict local minmax/Stackelberg equilibrium as diﬀerential Nash equilibrium and diﬀerential Stackelberg equilibrium, respectively.
Regardless of the equilibrium notion under consideration, a number of past works highlight failures of standard gradient descent-ascent in non-convex, non-concave zero-sum games. Indeed, it has been shown gradient descent-ascent with a shared learning rate (γ1 = γ2) is prone to reaching critical points that are neither diﬀerential Nash equilibrium nor diﬀerential Stackelberg equilibrium (Daskalakis and Panageas, 2018; Jin et al., 2020; Mazumdar et al., 2020). While an important negative result, it does not rule out the prospect that gradient descent-ascent may be able to guarantee equilibrium convergence as it fails to account for a key structural parameter of the learning dynamics, namely the ratio of learning rates between the players.
Motivated by the observation that the order of play between players is fundamental to the deﬁnition of the game, the role of timescale separation in gradient descent-ascent has been explored theoretically in recent years (Chasnov et al., 2019; Heusel et al., 2017; Jin et al., 2020). On the empirical side of past work, it has been widely demonstrated and prescribed that timescale separation in gradient descent-ascent between the generator and discriminator, either by heterogeneous learning rates or unrolled updates, is crucial to improving the solution quality when training generative adversarial networks (Arjovsky et al., 2017; Goodfellow et al., 2014; Heusel et al., 2017). Denoting γ1 as the learning rate of the player 1, the learning rate of player 2 can be redeﬁned as γ2 = τ γ1 where τ = γ2/γ1 > 0 is the ratio of learning rates or timescale separation parameter. The work of Jin et al. (2020) took a meaningful step toward understanding the eﬀect of timescale separation in gradient descent-ascent by showing that as τ → ∞ the stable critical points of the learning dynamics coincide with the set of diﬀerential Stackelberg equilibrium. In simple terms, the aforementioned result implies that all ‘bad critical points’ (that is, critical points lacking game-theoretic meaning) become unstable as the timescale separation approaches inﬁnity and that all ‘good critical points’ (that is, game-theoretically meaningful equilibria) remain or become stable as the timescale separation approaches inﬁnity. While a promising theoretical development on the local stability of the underlying dynamics, it does not lead to a practical, implementable learning rule or necessarily provide an explanation for the satisfying performance in applications of gradient descent-ascent with a ﬁnite timescale separation. It remains an open question to fully understand gradient descent-ascent as a function of the timescale separation and to determine whether the desirable behavior with an inﬁnite timescale separation is achievable for a range of ﬁnite learning rate ratios.
This paper continues the theoretical study of gradient descent-ascent with timescale separation in nonconvex, non-concave zero-sum games. We focus our attention on answering the remaining open questions regarding the behavior of the learning dynamics with ﬁnite learning rate ratios and provide a number of conclusive results. Notably, we develop necessary and suﬃcient conditions for a critical point to be stable for a range of ﬁnite learning rate ratios. The results imply that diﬀerential Stackelberg equilibria are stable for a range of ﬁnite learning rate ratios and that non-equilibria critical points are unstable for a range of ﬁnite learning rate ratios. Together, this means gradient descent-ascent only converges to diﬀerential Stackelberg equilibrium for a range of ﬁnite learning rate ratios. To our knowledge, this is the ﬁrst provable guarantee of its kind for an implementable ﬁrst-order method. Moreover, the technical results in this work rely on tools that have not appeared in the machine learning and optimization communities analyzing games and expose a number interesting directions of future research. Explicitly, the notion of a guard map, which is arguably even an obscure tool in modern control and dynamical systems theory, is ‘rediscovered’ in this work as a technique for analyzing the stability of game dynamics.
2

1.1 Contributions
To motivate our primary theoretical results, we present a self-contained description of what is known about the local stability of gradient descent-ascent around critical points in Section 3.1. The existing results primarily concern gradient descent-ascent without timescale separation and with a ratio of learning rates approaching inﬁnity (see Figure 1 for a graphical depiction of known results in each regime). In contrast, this paper is focused on characterizing the stability and convergence of gradient descent-ascent across a range of ﬁnite learning rate ratios. To hint at what is achievable in this realm, we present simple examples for which gradient descent-ascent converges to non-equilibrium critical points and games with diﬀerential Stackelberg equilibrium that are unstable with respect to gradient descent-ascent without timescale separation (see Examples 1 and 2, Section 3). While the existence of such examples is known (Daskalakis and Panageas, 2018; Jin et al., 2020; Mazumdar et al., 2020), we demonstrate in them that a ﬁnite timescale separation is suﬃcient to remedy the undesirable stability properties of gradient descent-ascent without timescale separation.
Toward characterizing this phenomenon in its full generality, we provide intermediate results which are known, but we prove using technical tools not yet broadly seen and exploited by this community. To begin, it is known that the set of diﬀerential Nash equilibrium are stable with respect to gradient descentascent (Daskalakis and Panageas, 2018; Mazumdar and Ratliﬀ, 2019), and that they remain stable for any timescale separation parameter τ ∈ (0, ∞) (Jin et al., 2020). We provide a proof for this result (Proposition 3) using the concept of quadratic numerical range (Tretter, 2008). Furthermore, Jin et al. (2020) recently showed that as the timescale separation τ → ∞, the stable critical points of gradient descent-ascent coincide with the set of diﬀerential Stackelberg equilibrium. We reveal that this result has long existed in the literature on singularly perturbed systems (Kokotovic et al., 1986, Chapter 2 and the citations within) and provide a proof (see Proposition 4) using analysis methods from the aforementioned line of work that are novel to the literature on learning in games from the machine learning and optimization communities in recent years.
A relevant line of study on singularly perturbed systems is that of characterizing the range of perturbation parameters for which a system is stable (Kokotovic et al., 1986; Saydy, 1996; Saydy et al., 1990). Debatably introduced by Saydy et al. (1990), guardian or guard maps act as a certiﬁcate that the roots of a polynomial lie in a particular guarded domain for a range of parameter values. Historically, guard maps serve as a tool for studying the stability of parameterized families of dynamical systems. We bring this tool to learning in games and construct a map that guards a class of Hurwitz stable matrices parameterized by the timescale separation parameter τ in order to analyze the range of learning rate ratios for which a critical point is stable with respect to gradient descent-ascent. This technique leads to the following result.
Informal Statement of Theorem 3. Consider a suﬃciently regular critical point x∗ of gradient descentascent. There exists a τ ∗ ∈ (0, ∞) such that x∗ is stable for all τ ∈ (τ ∗, ∞) if and only if x∗ is a diﬀerential Stackelberg equilibrium.
Theorem 3 conﬁrms that there does indeed exist a range of ﬁnite learning ratios such that a diﬀerential Stackelberg equilibrium is stable with respect to gradient descent-ascent. Moreover, such a range of learning rate ratios only exists if a critical point is a diﬀerential Stackelberg equilibrium. As we show in Corollary 2, the former implication of Theorem 3 nearly immediately implies there exists a τ ∗ ∈ (0, ∞) such that gradient descent-ascent converges locally asymptotically for all τ ∈ (τ ∗, ∞) if and only if x∗ is a diﬀerential Stackelberg equilibrium given a suitably chosen learning rate and deterministic gradient feedback. We give an explicit asymptotic rate of convergence in Theorem 5 and characterize the iteration complexity in Corollary 3. Moreover, we extend the convergence guarantees to stochastic gradient feedback in Theorem 6.
The latter implication of Theorem 3 says that there exists a ﬁnite learning rate ratio such that a nonequilibrium critical point of gradient descent-ascent is unstable. Building oﬀ of this, we complement the stability result of Theorem 3 with the following analagous instability result.
Informal Statement of Theorem 4. Consider any stable critical point x∗ of gradient descent-ascent which is not a diﬀerential Stackelberg equilibrium. There exists a ﬁnite learning rate ratio τ0 ∈ (0, ∞) such that x∗ is unstable for all τ ∈ (τ0, ∞).
Theorem 4 establishes that there exists a range of ﬁnite learning ratios non-equilibrium critical points are unstable with respect to gradient descent-ascent. This implies that for a suitably chosen ﬁnite timescale separation, gradient descent-ascent avoids critical points lacking game-theoretic meaning. Together, Theorem 3
3

and Theorem 4 answer aﬃrmatively that gradient descent-ascent with timescale separation can guarantee equilibrium convergence, which answers a standing open question. Moreover, we provide explicit constructions for computing τ ∗ and τ0 given a critical point. In fact our construction of τ ∗ in Theorem 3 is tight, and this is conﬁrmed by our numerical experiments.
We ﬁnish the theoretical analysis of gradient descent-ascent in this paper by connecting to the literature on generative adversarial networks. We show under common assumptions on generative adversarial networks (Mescheder et al., 2018; Nagarajan and Kolter, 2017) that the introduction of gradient penalty based regularization to the discriminator does not change the set of critical points for the dynamics and, further, there exists a ﬁnite learning rate ratio τ ∗ such that for any learning rate ratio τ ∈ (τ ∗, ∞) and any non-negative, ﬁnite regularization parameter µ, the continuous time limiting regularized learning dynamics remain stable, and hence, there is a range of learning rates γ1 for which the discrete time update locally converges asymptotically.
Informal Statement of Theorem 7. Consider training a generative adversarial network with a gradient penalty (for any ﬁxed regularization parameter µ ∈ (0, ∞)) via a zero-sum game with generator network Gθ, discriminator network Dω, and loss f (θ, ω) such that relaxed realizable assumptions are satisﬁed for a critical point (θ∗, ω∗). Then, (θ∗, ω∗) is a diﬀerential Stackelberg equilibrium, and for any τ ∈ (0, ∞), gradient descent-ascent converges locally asymptotically. Moreover, an asymptotic rate of convergence is given in Corollary 5.
The theoretical results we provide are complemented by extensive experiments. In simulation, we explore a number of interesting behaviors of gradient descent-ascent with timescale separation analyzed theoretically including diﬀerential Stackelberg equilibria shifting from being unstable to stable and non-equilibrium critical points moving from being stable to unstable. Furthermore, we examine how the vector ﬁeld and the spectrum of the game Jacobian evolve as a function of the timescale separation and explore the relationship with the rate of convergence. We experiment with gradient descent-ascent on the Dirac-GAN proposed by Mescheder et al. (2018) and illustrate the interplay between timescale separation, regularization, and rate of convergence. Building on this, we train generative adversarial networks on the CIFAR-10 and CelebA datasets with regularization and demonstrate that timescale separation can beneﬁt performance and stability. In the experiments we observe that regularization and timescale separation are intimately connected and there is an inherent tradeoﬀ between them. This indicates that insights made on simple generative adversarial network formulations may carry over to the complex problems where players are parameterized by neural networks.
Collectively, the primary contribution of this paper is the near-complete characterization of the behavior of gradient descent-ascent with ﬁnite timescale separation. Moreover, by introducing a novel set of analysis tools to this literature, our work opens a number of future research questions. As an aside, we believe these technical tools open up novel avenues for not only proving results about learning dynamics in games, but also for synthesizing algorithms.
1.2 Organization
The organization of this paper is as follows. Preliminaries on game theoretic notions of equilibria, gradientbased learning algorithms, and dynamical systems theory are reviewed in Section 2.
Convergence analysis proceeds in two phases. In Section 3, we study the stability properties of the continuous time limiting dynamical system given a timescale separation between the minimizing and maximizing players. Speciﬁcally, we show the ﬁrst result on necessary and suﬃcient conditions for convergence of the continuous time limiting system corresponding to gradient descent-ascent with time scale separation to game theoretically meaningful equilibria (i.e., local minmax equilibria in zero-sum games). Following this, in Section 4, we provide convergence guarantees for the original discrete time dynamical system of interest (namely, gradient descent ascent). Using the results in the proceeding section, we show that gradient descent-ascent converges to a critical point if and only if it is a diﬀerential Stackelberg equilibrium (i.e., a suﬃciently regular local minmax). In addition, we characterize the iteration complexity of gradient descent-ascent dynamics and provide ﬁnite-time bounds on local convergence to approximate local Stackelberg equilibria.
We apply the main results in the preceding sections to generative adversarial networks in Section 5, and in Section 6 we present several illustrative examples including generative adversarial networks where
4

we show that tuning the learning rate ratio along with regularization and the exponential moving average hyperparameter signiﬁcantly improves the Fr´echet Inception Distance (FID) metric for generative adversarial networks.
Given its length, prior to concluding in Section 7, we review related work drawing connections to solution concepts, gradient descent-ascent learning dynamics, applications to adversarial learning where the success of heuristics provide strong motivation for the theoretical work in this paper, and historical connections to dynamical systems theory. Throughout the sections proceeding Section 7, we draw connections to related works and results in an eﬀort to place our results in the context of the literature. We conclude in Section 8 with a discussion on the signiﬁcance of the results and open questions. The appendix includes the majority of the detailed proofs as well as additional experiments and commentary.

2 Preliminaries
In this section, we review game theoretic and dynamical systems preliminaries. Additionally, we formulate the class of learning rules analyzed in this paper.

2.1 Game Theoretic Preliminaries
A two–player zero-sum continuous game is deﬁned by a collection of costs (f1, f2) where f1 ≡ f and f2 ≡ −f with f ∈ Cr(X, R) for some r ≥ 2 and where X = X1 × X2 with each Xi a precompact subset of Rni for i = 1, 2. Let n = n1 + n2 be the dimension of the joint strategy space X = X1 × X2. Player i ∈ I seeks to minimize their cost function fi(xi, x−i) with respect to their choice variable xi where x−i is the vector of all other actions xj with j = i.
There are two natural equilibrium concepts for such games depending on the order of play—i.e., the Nash equilibrium concept in the case of simultaneous play and the Stackelberg equilibrium concept in the case of hierarchical play. Each notion of equilibria can be characterized as the intersection points of the reaction curves of the players (Ba¸sar and Olsder, 1998).

Deﬁnition 1 (Local Nash Equilibrium). The joint strategy x ∈ X is a local Nash equilibrium on i∈I Ui ⊂ X, where Ui ⊆ Xi, if f (x1, x2) ≤ f (x1, x2), for all x1 ∈ U1 ⊂ X1 and f (x1, x2) ≥ f (x1, x2) for all x2 ∈ U2 ⊂ X2. Furthermore, if the inequalities are strict, we say x is a strict local Nash equilibrium.
Deﬁnition 2 (Local Stackelberg Equilibrium). Consider Ui ⊂ Xi for i = 1, 2 where, without loss of generality, player 1 is the leader (minimizing player) and player 2 is the follower (maximizing player). The strategy x∗1 ∈ U1 is a local Stackelberg solution for the leader if, ∀x1 ∈ U1,

supx2∈rU2 (x∗1) f (x∗1, x2) ≤ supx2∈rU2 (x1) f (x1, x2),

where rU2 (x1) = {y ∈ U2|f (x1, y) ≥ f (x1, x2), ∀x2 ∈ U2} is the reaction curve. Moreover, for any x∗2 ∈

rU2 (x∗1),

the

joint

strategy

proﬁle

(

x

∗ 1

,

x∗2

)

∈

U1

×

U2

is

a

local

Stackelberg

equilibrium

on

U1

×

U2.

Predicated on existence,1 equilibria can be characterized in terms of suﬃcient conditions on player costs.
Indeed, in continuous games, ﬁrst and second order conditions on player cost functions leads to a diﬀerential
characterization (i.e., necessary and suﬃcient conditions) of local Nash equilibria reminiscent of optimality conditions in nonlinear programming (Ratliﬀ et al., 2016).2
We denote Difi as the derivative of fi with respect to xi, Dijfi as the partial derivative of Difi with respect to xj, Di2fi as the partial derivative of Difi with respect to xi, and D(·) as the total derivative.3

1Characterizing existence of equilibria is outside the scope of this work. However, we remark that Nash equilibria exist for convex costs on compact and convex strategy spaces and Stackelberg equilibria exist on compact strategy spaces (Ba¸sar and Olsder, 1998, Thm. 4.3, Thm. 4.8, & Sec. 4.9).
2The diﬀerential characterization of local Nash equilibria in continuous games was ﬁrst reported in (Ratliﬀ et al., 2013). Genericity and structural stability we studied in general-sum settings in (Ratliﬀ et al., 2014) and in zero-sum settings in (Mazumdar and Ratliﬀ, 2019).
3Example: given f (x, h(x)), Df = D1f + D2f ◦ Dh.

5

Proposition 1 (Necessary and Suﬃcient Conditions for Local Nash (Ratliﬀ et al., 2016, Thm. 1 & 2)). If
x is a local Nash equilibrium of the zero-sum game ((f, −f ), then D1f (x) = 0, −D2f (x) = 0, D12f (x) ≥ 0 and D22f (x) ≤ 0. On the other hand, if D1f (x) = 0, D2f (x) = 0, and D12f (x) > 0 and D22f (x) < 0, then x ∈ X is a local Nash equilibrium.

The following deﬁnition, characterized by suﬃcient conditions for a local Nash equilibrium as deﬁned in Deﬁnition 1, was ﬁrst introduced in (Ratliﬀ et al., 2013).

Deﬁnition 3 (Diﬀerential Nash Equilibrium (Ratliﬀ et al., 2013)). The joint strategy x ∈ X is a diﬀerential Nash equilibrium if D1f (x) = 0, −D2f (x) = 0, D12f (x) > 0 and D22f (x) < 0.
Analogous suﬃcient conditions can be stated to characterize a local Stackelberg equilibrium as deﬁned in Deﬁnition 2.4 Suppose that f ∈ Cr+1(X, R) for some r ≥ 1. Given a point x∗ at which det(D22f (x∗)) = 0, the implicit function theorem (Abraham et al., 2012, Thm. 2.5.7) implies there is a neighborhood U1 of x∗1 and a unique Cr map h : U1 → Rn2 such that for all x1 ∈ U1, D2f (x1, h(x1)) = 0. The map h : x1 → x2 is referred to as the implicit map. Further, Dh ≡ −(D22f )−1 ◦ D21f . Note that det(D22f (x)) = 0 is a generic condition (cf. (Fiez et al., 2020, Lem. C.1)). Let Df (x1, h(x1)) be the total derivative of f and analogously, let D2f be the second-order total derivative.

Deﬁnition 4 (Diﬀerential Stackelberg Equilibrium (Fiez et al., 2020)). The joint strategy x = (x1, x2) ∈ X is a diﬀerential Stackelberg equilibrium if D1f (x1, x2) = 0, −D2f (x1, x2) = 0, D2f (x1, x2) > 0, and D22f (x1, x2) < 0.
Observe that in a general sum setting the ﬁrst order conditions for player 1 are equivalent the total derivative of f being zero at the candidate critical point where x2 is implicitly deﬁned as a function of x1 via the implicit mapping theorem applied to D2f (x1, x2) = 0. Since in this paper and in Deﬁnition 4, the class of games is zero sum, D1f (x1, x2) = 0 and D2f (x1, x2) = 0 (along with the condition that det(D22f (x1, x2)) = 0 which is implied by the second order conditions) are suﬃcient to imply that the total derivative Df (x1, x1) is zero.
The Jacobian of the ﬁrst order necessary and suﬃcient condition—i.e., conditions that deﬁne potential candidate diﬀerential Nash and/or Stackelberg equilibria—is a useful mathematical object for understanding convergence properties of gradient based learning rules as we will see in subsequent sections. Consider the vector of individual gradients g(x) = (D1f (x), −D2f (x)) which deﬁne ﬁrst order conditions for a diﬀerential Nash equilibrium. Let J(x) denote the Jacobian of g(x) which is deﬁned by

J (x) = D12f1(x) D12f1(x) .

(1)

D21f2(x) D22f2(x)

We recall from Fiez et al. (2020) an alternative (to Deﬁnition 4, but equivalent set of suﬃcient conditions for a diﬀerential Stackelberg in terms of J(x). Let S1(·) denote the Schur complement of (·) with respect to the n2 × n block-row matrix in (·).
Proposition 2 (Proposition 1 Fiez et al. (2020)). Consider a zero-sum game (f, −f ) deﬁned by f ∈ Cr(X, R) with r ≥ 2 and player 1 (without loss of generality) taken to be the leader (minimizing player). Let x∗ satisfy −D2f (x∗) = 0 and −D22f (x∗) > 0. Then Df (x∗) = 0 and S1(J(x∗)) > 0 if and only if x∗ is a diﬀerential Stackelberg equilibria.

Remark 1 (A comment on the genericity of diﬀerential Stackelberg/Nash equilibria.). Due to Fiez et al. (2020, Theorem 1), diﬀerential Stackelberg are generic amongst local Stackelberg equilibria and, similarly, due to Mazumdar and Ratliﬀ (2019, Theorem 2), diﬀerential Nash equilibria are generic amongst local Nash equilibria. This means that the property of being a diﬀerential Stackelberg (respectively, diﬀerential Nash) equilibrium in a zero-sum game is generic in the class of zero-sum games deﬁned by C2(X, R) functions— that is, for almost all (in some formall sense) zero-sum games, all the local Stackelberg/Nash are diﬀerential Stackelberg/Nash.
4The diﬀerential characterization of Stackelberg equilibria was ﬁrst introduced in (Fiez et al., 2019). The genericity and structural structural stability were shown in (Fiez et al., 2020).

6

2.2 Gradient-based learning algorithms

As noted above, in this paper we focus on settings in which agents or players in this game are seeking

equilibria of the game via a learning algorithm. We study arguably the most natural learning rule in zero-

sum continuous games: gradient descent-ascent (GDA). This gradient-based learning rule is a simultaneous

gradient play algorithm in that agents update their actions at each iteration simultaneously.

Gradient descent-ascent is deﬁned as follows. At iteration k, each agent i ∈ I updates their choice

variable xi,k ∈ Xi by the process

xi,k+1 = xi,k − γigi(xi,k, x−i,k)

(2)

where γi is agent i’s learning rate, and gi(x) is agent i’s gradient-based update mechanism. For simultaneous

gradient play,

g(x) = (D1f1(x), D2f2(x))

(3)

is the vector of individual gradients and in a zero-sum setting, GDA is deﬁned using g(x) = (D1f (x), −D2f (x)) where the ﬁrst player is the minimizing player and the second player is the maximizing player.
One of the key contributions of this paper is that we provide convergence rates for settings in which
there is a timescale separation between the learning processes of the minimizing and maximizing players—
i.e., settings in which the agents learning rates γi are not homogeneous. Deﬁne Γ = blkdiag(γ1In1 , γ2In2 ) where Ini denotes the ni × ni identity matrix. Let τ = γ2/γ1 be the learning rate ratio and deﬁne Λτ = blockdiag(In1 , τ In2 ). The τ -GDA dynamics are given by

xk+1 = xk − γ1Λτ g(xk).

(4)

Tools for Convergence Analysis. We analyze the iteration complexity or local asymptotic rate of con-
vergence of learning rules of the form (2) in the neighborhood of an equilibrium. Given two real valued
functions F (k) and G(k), we write F (k) = O(G(k)) if there exists a positive constant c > 0 such that |F (k)| ≤ c|G(k)|. For example, consider iterates generated by (2) with initial condition x0 and critical point x∗. Suppose that we show xk+1 − x∗ ≤ M k x0 − x∗ . Then, we write F (k) = O(M k) where c = x0 − x∗ .

2.3 Dynamical Systems Primer
In this paper, we study learning rules employed by agents seeking game-theoretically meaningful equilibria in continuous games. Dynamical systems tools for both continuous and discrete time play a crucial role in this analysis.

Stability. Before we proceed, we recall and remark on some facts from dynamical systems theory concerning stability of equilibria in the continuous-time dynamics

x˙ = −g(x)

(5)

relevant to convergence analysis for the discrete-time learning dynamics in (2). Observe that equilibria are shared between (2) and (5). Our focus is on the subset of equilibria that satisfy Deﬁnition 4, and the subset thereof deﬁned in Deﬁnition 3. Recall the following equivalent characterizations of stability for an equilibrium of (5) in terms of the Jacobian matrix J(x) = Dg(x).
Theorem 1 ((Khalil, 2002, Thm. 4.15)). Consider a critical point x∗ of g(x). The following are equivalent: (a) x∗ is a locally exponentially stable equilibrium of x˙ = −g(x); (b) spec(−J(x∗)) ⊂ C◦−; (c) there exists a symmetric positive-deﬁnite matrix P = P > 0 such that P J(x∗) + J(x∗) P > 0.

It was shown in (Ratliﬀ et al., 2016, Prop. 2) that if the spectrum of −J(x) at a diﬀerential Nash equilibrium x is in the open left-half complex plane—i.e., spec(−J(x)) ⊂ C◦−—then x is a locally exponentially stable equilibrium of (5). Indeed, if all agents learn at the same rate so Γ = γIn in (2), then a straightforward application of the Spectral Mapping Theorem (Callier and Desoer, 2012, Thm. 4.7) ensures that an exponentially stable equilibrium x∗ for (5) is locally exponentially stable in (2) so long as γ > 0 is chosen
suﬃciently small (Chasnov et al., 2019). However, this observation does not directly tell us how to select γ
or the resulting iteration complexity in an asymptotic or ﬁnite-time sense; furthermore, this line of reasoning
does not apply when agents learn at diﬀerent rates (Γ = γI in (2)).

7

Limiting dynamical systems. The continuous time dynamical system takes the form x˙ = −Λτ g(x) due to the timescale separation τ . Such a system is known as a singularly perturbed system or a multi-timescale system in the dynamical systems theory literature (Kokotovic et al., 1986), particularly where τ −1 is small.
Singularly perturbed systems are classically expressed as

x˙ = −D1f1(x, z)

(6)

z˙ = −D2f2(x, z)

where = τ −1 is most often a physically meaningful quantity inherent to some dynamical system that describes the evolution of some physical phenomena; e.g., in circuits it may be a constant related to device material properties, and in communication networks, it is often the speed at which data ﬂows through a physical medium such as cable.
In the classical asymptotic analysis of a system of the form (6)—which we write more generally as x˙ = F (t, x, ) for the purpose of the following observations—the goal is to obtain an approximate solution, say x˜(t, ), such that the approximation error x(t, ) − x˜(t, ) is small in some norm for small | | and, further, the approximate solution is expressed in terms of a reduced order system. Such results have signiﬁcance in terms of revealing underlying structural properties of the original system x˙ = F (t, x, ) and its corresponding state x(t, ) for small | |. One of the contributions of this work is that we take a similar analysis approach in order to reveal underlying structural properties of the optimization landscape of zero-sum games/minimax optimization problems. Indeed, asymptotic methods can reveal multiple timescale structures that are inherent in many machine learning problems, as we observe in this paper for zero-sum games. One key point of separation in applying dynamical systems theory to the study of algorithms versus physical system dynamics—in particular, learning in games—this parameter no longer necessarily is a physical quantity but is most often a hyper-parameter subject to design. In this paper, treating the inverse of the learning rate ratio as a timescale separation parameter, we combine the asymptotic analysis tools from singular perturbation theory with tools from algebra to obtain convergence guarantees.

Leveraging Linearization to Infer Qualitative Properties. The Hartman-Grobman theorem asserts that it is possible to continuously deform all trajectories of a nonlinear system onto trajectories of the linearization at a ﬁxed point of the nonlinear system. Informally, the theorem states that if the linearization of the nonlinear dynamical system x˙ = F (x) around a ﬁxed point x¯—i.e., F (x¯) = 0—has no zero or purely imaginary eigenvalues, then there exists a neighborhood U of x¯ and a homeomorphism h : U → Rn—i.e., h, h−1 ∈ C(U, Rn)—taking trajectories of x˙ = F (x) and mapping them onto those of z˙ = DF (x¯)z. In particular, h(x¯) = 0.
Given a dynamical system x˙ = F (x), the state or solution of the system at time t starting from x at time t0 is called the ﬂow and is denoted φt(x).
Theorem 2 (Hartman-Grobman (Sastry, 1999, Thm. 7.3); (Teschl, 2000, Thm. 9.9)). Consider the ndimensional dynamical system x˙ = F (x) with equilibrium point x¯. If DF (x¯) has no zero or purely imaginary eigenvalues, there is a homeomorphism h deﬁned on a neighborhood U of x¯ taking orbits of the ﬂow φt to those of the linear ﬂow etDF (x¯) of x˙ = F (x)—that is, the ﬂows are topologically conjugate. The homeomorphism preserves the sense of the orbits and is chosen to preserve parameterization by time.
The above theorem says that the qualitative properties of the nonlinear system x˙ = F (x) in the vicinity (which is determined by the neighborhood U ) of an isolated equilibrium x¯ are determined by its linearization if the linearization has no eigenvalues on the imaginary axes in the complex plane. We also remark that Hartman-Grobman can also be applied to discrete time maps (cf. Sastry (1999, Thm. 2.18)) with the same qualitative outcome.

Internally Chain Transitivity. In proving results for stochastic gradient descent-ascent, we leverage what is known as the ordinary diﬀerential equation method in which the ﬂow of the limiting continuous time system starting at sample points from the stochastic updates of the players actions is compared to asymptotic psuedo-trajectories—i.e., linear interpolations between sample points. To understand stability in the stochastic case, we need the notion of internally chain transitive sets. For more detail, the reader is referred to (Alongi and Nelson, 2007, Chap. 2–3).

8

A closed set U ⊂ Rm is an invariant set for a diﬀerential equation x˙ = F (x) if any trajectory x(t) with x(0) ∈ U satisﬁes x(t) ∈ U for all t ∈ R. Let φt be a ﬂow on a metric space (X, d). Given ε > 0, T > 0 and x, y ∈ X, an (ε, T )-chain from x to y with respect to φt and d is a pair of ﬁnite sequences x = x0, x1, . . . , xk−1, xk = y in X and t0, . . . , tk−1 in [T, ∞), denoted together by (x0, x1, . . . , xk−1, xk; t0, . . . , tk−1), such that d(φti (xi), xi+1) < ε for i = 0, 1, 2, . . . , k − 1. A set U ⊆ X is (internally) chain transitive with respect to φt if U is a non-empty closed invariant set with respect to φt such that for each x, y ∈ U , > 0
and T > 0 there exists an (ε, T )-chain from x to y. A compact invariant set U is invariantly connected if it
cannot be decomposed into two disjoint closed nonempty invariant sets. It is easy to see that every internally
chain transitive set is invariantly connected.

3 Stability of Continuous Time GDA with Timescale Separation

To characterize the convergence of τ -GDA, we begin by studying its continuous time limiting system

x˙ = −Λτ g(x),

(7)

where we recall that Λτ = blockdiag(In1 , τ In2 ). Throughout this section, the class of zero-sum games we consider are suﬃciently smooth, meaning that f ∈ Cr(X, R) for some r ≥ 2. The Jacobian of the system from (7) in zero-sum games of the form (f1, f2) = (f, −f ) is given as

J (x) = D12f (x)

D12f (x) .

(8)

τ

−τ D12f (x) −τ D22f (x)

By analyzing the stability of the continuous time system as a function of the timescale separation τ using the Jacobian from (8) in this section, we can then draw conclusions about the stability and convergence of the discrete time system τ -GDA in Section 4.
The organization of this section is as follows. To begin, we present a collection of preliminary observations in Section 3.1 regarding the stability of continuous time gradient descent-ascent with timescale separation to motivate the results in the subsequent subsections by establishing known results and introducing alternative analysis methods that the technical results in this paper build on. Then, in Sections 3.2 and 3.3 respectively, we present necessary and suﬃcient conditions for stability of the continuous time system around critical points in terms of the learning rate ratio along with suﬃcient conditions to guarantee the instability of the continuous time system around non-equilibrium critical points in terms of the timescale separation.

3.1 Preliminary Observations
In Figure 1 we present a graphical representation of known results on the stability of gradient descentascent with timescale separation in continuous time, where we remark that such results nearly directly imply equivalent conclusions regarding the discrete time system τ -GDA with a suitable choice of learning rate γ1. The primary focus of past work has been on the edge cases of τ = 1 and τ → ∞. For τ = 1, the set of diﬀerential Nash equilibrium are stable, but diﬀerential Stackelberg equilibrium may be stable or unstable, and non-equilibrium critical points can be stable. As τ → ∞, the set of diﬀerential Nash equilibrium remain stable, each diﬀerential Stackelberg equilibrium is guaranteed to become stable, and each non-equilibrium critical point must be unstable. We ﬁll the gap between the known results by providing results as a function of ﬁnite τ . With an eye toward this goal, we now provide examples and preliminary results that illustrate the type of guarantees that may be achievable for a range of ﬁnite learning rate ratios.
To start oﬀ, we consider the set of diﬀerential Nash equilibrium. It is nearly immediate from the structure of the Jacobian that each diﬀerential Nash equilibrium is stable for τ = 1 (Daskalakis and Panageas, 2018; Mazumdar et al., 2020). Moreover, Jin et al. (2020) showed that regardless of the value of τ ∈ (0, ∞), the set of diﬀerential Nash equilibrium remain stable. In other words, the desirable stability characteristics of diﬀerential Nash equilibrium are retained for any choice of timescale separation. We state this result as a proposition for later reference and since our proof technique relies on the concept of quadratic numerical range (Tretter, 2008), which has not appeared previously in this context. The proof of Proposition 3 is provided in Appendix B.

9

critical points LASE
DNE DSE

critical points LASE DSE
DNE

(a) τ = 1

(b) τ → ∞

Figure 1: Graphical representation of the known stability results on τ -GDA in relationship to local equilibrium concepts with τ = 1 and τ → ∞. The acronyms in the ﬁgure are diﬀerential Nash equilibria (DNE), diﬀerential Stackelberg equilibria (DSE), and locally asymptotically stable equilibria (LASE). Note that the terminology of locally asymptotically stable equilibria refers to the set of stable critical points with respect to the system x˙ = −Λτ g(x) for the given τ . Fiez et al. (2020) reported the subset relationship between diﬀerential Nash equilibria and diﬀerential Stackelberg equilibria and Jin et al. (2020) gave a similar characterization in terms of local Nash and local minmax. For the regime of τ = 1, Daskalakis and Panageas (2018); Mazumdar et al. (2020) presented the relationship between the set of diﬀerential Nash equilibrium and the set of locally asymptotically stable equilibrium, and Jin et al. (2020) provided the relationship between the set of diﬀerential Stackelberg equilibrium and the set of locally asymptotically stable equilibrium. Finally, Jin et al. (2020) reported the characterization of the locally asymptotically stable equilibrium as τ → ∞. The missing pieces in the literature are results as a function of ﬁnite τ , which we answer in this work deﬁnitively.

Proposition 3. Consider a zero-sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x∗ is a diﬀerential Nash equilibrium. Then, spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (0, ∞).
Fiez et al. (2020) show that the set of diﬀerential Nash equilibrium is a subset of the set of diﬀerential Stackelberg equilibrium. In other words, any diﬀerential Nash equilibrium is a diﬀerential Stackelberg equilibrium, but a diﬀerential Stackelberg equilibrium need not be a diﬀerential Nash equilibrium. Moreover, Jin et al. (2020) show that the result of Proposition 3 fails to extend from diﬀerential Nash equilibria to the broader class of diﬀerential Stackelberg equilibrium. Indeed, not all diﬀerential Stackelberg equilibrium are stable with respect to the continuous time limiting dynamics of gradient descent-ascent without timescale separation. However, as the following example demonstrates, diﬀerential Stackelberg equilibrium that are unstable without timescale separation can become stable for a range of ﬁnite timescale learning rate ratios.

Example 1. Within the class of zero-sum games, there exists diﬀerential Stackelberg equilibrium that are unstable with respect to x˙ = −g(x) and stable with respect to x˙ = −Λτ g(x) for all τ ∈ (τ ∗, ∞) where τ ∗ is ﬁnite. Indeed, consider the quadratic zero-sum game deﬁned by the cost

1 f (x1, x2) =

x1

2 x2

−v 0 −v 0 

 0 21 v 0 12 v  x1

−v

0

− 12 v

 0  x2

0 12 v 0 −v

where x1, x2 ∈ R2 and v > 0. The unique critical point of the game given by x∗ = (0, 0) is a diﬀerential Stackelberg equilibrium since g(x∗) = 0, S1(J(x∗)) = diag(v, v/4) > 0 and −D22f (x∗) = diag(v/2, v) > 0.

The spectrum of the Jacobian of Λτ g(x) is given by

spec(Jτ (x∗)) =

√

√

v(2τ + 1 ± 4τ 2 − 8τ + 1) , v(τ − 2 ± τ 2 − 12τ + 4) .

4

4

Observe that for τ

=

1,

spec(Jτ

(x∗))

=

{

1

√ (3±i 3)v,

1

√ (−1±i 7)v}

⊂

C◦

for any v > 0 so that the diﬀeren-

4

4

+

tial Stackelberg equilibrium x∗ is never stable for the choice of τ . However, for any v > 0, spec(Jτ (x∗)) ⊂ C◦+

for all τ ∈ (2, ∞), meaning that the diﬀerential Stackelberg equilibrium x∗ is indeed stable with respect to the

dynamics x˙ = −Λτ g(x) for a range of ﬁnite learning rate ratios.

10

We explore Example 1 further via simulations in Section 6.1. The key takeaway from Example 1 is that it is clearly not always necessary for the timescale separation τ to approach inﬁnity in order to guarantee the stability of a diﬀerential Stackelberg equilibrium and instead there exists a suﬃcient ﬁnite learning rate ratio. Put simply, the undesirable property of diﬀerential Stackelberg equilibria not being stable with respect to gradient descent-ascent without timescale separation can potentially be remedied with only a ﬁnite timescale separation.
It is well-documented that some stable critical points of the continuous time gradient descent-ascent limiting dynamics without timescale separation can lack game-theoretic meaning, as they may be neither a diﬀerential Nash equilibria nor diﬀerential Stackelberg equilibria (Daskalakis and Panageas, 2018; Jin et al., 2020; Mazumdar et al., 2020). The following example demonstrates that such undesirable critical points that are stable without timescale separation can become unstable for a range of ﬁnite learning ratios.

Example 2. Within the class of zero-sum games, there exists non-equilibrium critical points that are stable with respect to x˙ = −g(x) and unstable with respect to x˙ = −Λτ g(x) for all τ ∈ (τ0, ∞) where τ0 is ﬁnite. Indeed, consider a zero sum game deﬁned by the cost

 12 v 0 21 v 0 

1 f (x , x ) =

x1

0

− 41 v

0

12 v  x1

(9)

12

2 x2  12 v

0

41 v

 0  x2

0 21 v 0 − 12 v

where x1, x2 ∈ R2 and v > 0. The unique critical point of the game given by x∗ = (0, 0) is neither a diﬀerential Nash equilibrium nor a diﬀerential Stackelberg equilibrium since D12f (x∗) = diag(v/2, −v/4) ≯ 0 and −D22f (x∗) = diag(−v/4, v/2) ≯ 0. The spectrum of the Jacobian of Λτ g(x) is given by

spec(Jτ (x∗)) =

√

√

v(2τ − 1 ± 4τ 2 − 12τ + 1) , v(2 − τ ± τ 2 − 12τ + 4) .

8

8

Given τ

=

1,

spec(Jτ (x∗))

=

{ 1 (1

±

√ i 7)v,

1 (1

±

√ i 7)v}

⊂

C◦

for any v > 0 so that the non-equilibrium

8

8

+

critical point x∗ is in fact stable for the choice of timescale separation τ . However, for any v > 0,

spec(Jτ (x∗)) ⊂ C◦+ for all τ ∈ (2, ∞), meaning that the non-equilibrium critical point x∗ is unstable with

respect to the dynamics x˙ = −Λτ g(x) for a range of ﬁnite learning rate ratios.

The game construction from (9) is quadratic and as a result has a unique critical point. Games can be

constructed in which critical points lacking game-theoretic meaning that are stable without timescale separa-

tion become unstable for all τ > τ0 even in the presence of multiple equilibria. Indeed, consider a zero-sum

game deﬁned by the cost

f (x1, x2)

=

5 4

x211 + 2x11x21 + 12 x221 − 12 x212 + 2x12x22 − x222

(x11 − 1)2

+ x211 2i=1(x1i − 1)2 − (x2i − 1)2 .

(10)

This game has critical points at (0, 0, 0, 0), (1, 1, 1, 1), and (−4.73, 0.28, −92.47, 0.53). The critical points
(1, 1, 1, 1) and (−4.73, 0.28, −92.47, 0.53) are diﬀerential Nash equilibria and are consequently stable for any choice of τ > 0. The critical point x∗ = (0, 0, 0, 0) is neither a diﬀerential Nash equilibrium nor a diﬀerential
Stackelberg equilibrium. Moreover, the Jacobian of Λτ g(x∗) for the game deﬁned by (9) with v = 5 is identical to that for the game deﬁned by (10). As a result, we know that x∗ is stable without timescale separation, but spec(Jτ (x∗)) ⊂ C◦+ for all τ ∈ (2, ∞) so that the non-equilibrium critical point x∗ is again unstable with respect to the dynamics x˙ = −Λτ g(x) for a range of ﬁnite learning rate ratios.

We investigate the game deﬁned in (10) from Example 2 with simulations in Section 6.2. In an analogous manner to Example 1, Example 2 demonstrates that it is not always necessary for the timescale separation τ to approach inﬁnity in order to guarantee non-equilibrium critical points become unstable as there can exist a suﬃcient ﬁnite learning rate ratio. This is to say that the unwanted property of non-equilibrium critical points being stable without timescale separation can also potentially be remedied with only a ﬁnite timescale separation.
The examples of this section have provided evidence that there exists a range of ﬁnite learning rate ratios for which diﬀerential Stackelberg equilibrium are stable and a range of learning rate ratios for which

11

non-equilibrium critical points are unstable. Yet, no result has appeared in the literature on gradient descent-ascent with timescale separation conﬁrming this behavior in general. We focus on doing precisely that in the subsection that follows. Before doing so, we remark on the closest existing result. As mentioned previously Jin et al. (2020) show that as τ → ∞, the set of stable critical points with respect to the dynamics x˙ = −Λτ g(x) coincide with the set of diﬀerential Stackelberg equilibrium. However, an equivalent result in the context of general singularly perturbed systems has been known in the literature (cf. Kokotovic et al. 1986, Chap. 2). We give a proof based on this type of analysis because it reveals a new set of analysis tools to the study of game-theoretic formulations of machine learning and optimization problems; a proof sketch is given below while the full proof is given in Appendix F.
Proposition 4. Consider a zero-sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x∗ is such that g(x∗) = 0 and det(D22f2(x∗)) = 0. Then, as τ → ∞, spec(Jτ (x∗)) ⊂ C◦+ if and only if x∗ is a diﬀerential Stackelberg equilibrium.

Proof Sketch. The basic idea in showing this result is that there is a (local) transformation of coordinates from the linearized dynamics of x˙ = −Λτ g(x), which we write as

x˙ = A11 −τ A12

A12 x, τ A22

in a neighborhood of a critical point to an upper triangular system that depends parametrically on τ and

hence, the asymptotic behavior is readily obtainable from the block diagonal components of the system in

the new coordinates. Indeed, consider the change of variables z = x2 + L(τ −1)x1 for the second player so

that

x˙ 1

A11 − A12L(τ −1)

A12

x1

z˙ =

R(L, τ )

A22 + τ −1L(τ −1)A12 z

(11)

where

R(L, τ ) = −A12 − A22L(τ −1) + τ −1L(τ −1)A11 − τ −1L(τ −1)A12L(τ −1) = 0

A transformation of coordinates L(τ ) such that R(L, τ ) = 0 always exists (cf. Lemma 7, Appendix F). Hence, the characteristic equation of (11) can be expressed as

χ(s, τ ) = τ nχs(s, τ )χf (p, τ ) = 0

where χs(s, τ ) = det(sI −(A11 −A12L(τ −1))) and χf (p, τ ) = det(pI −(A22 +τ −1A12L(τ −1))) with p = sτ −1. As τ → ∞, L(τ −1) → L(0) = −A−221A12. Consequently, n of the eigenvalues of x˙ = −Λτ g(x), denoted by {λ1, . . . , λn1 }, are the roots of the slow characteristic equation χs(s, τ ) = 0 and the rest of the eigenvalues {λn1+1, . . . , λn1+n2 } are denoted by λi = νj/ε for i = n1 + j and j ∈ {1, . . . , n2} where {ν1, . . . , νn2 } are the roots of the fast characteristic equation χf (p, τ ) = 0. The roots of χs(s, τ ) are precisely those of the (ﬁrst) Schur complement of −Jτ (x∗) while the roots of χf (p, τ ) are precisely those of D22f (x∗).
This simple transformation of coordinates to an upper triangular dynamical system shown in (11) leads immediately to the asymptotic result in Proposition 4. It also shows that if the eigenavlues of S1(Jτ (x∗)) are distinct5 and similarly, so are those of D22f (x∗) (although, S1(Jτ (x∗)) and D22f (x∗) are allowed to have eigenvalues in common), then the asymptotic results from Proposition 4 imply the following approximations for the elements of spec(Jτ (x∗)):
λi = λi(S1(Jτ (x∗)) + O(τ −1), i = 1, . . . , n1,
λj+n1 = τ (λj(−D22f (x∗)) + O(τ −1)), j = 1, . . . , n2.

This follows simply by observing that when the eigenvalues are distinct, the derivatives ds/dτ and dp/dτ are well-deﬁned by the implicit mapping theorem and the total derivative of χs(s, τ ) and χf (p, τ ), respectively.
5Distinct eigenvalues is a generic property in the space of n × n real matrices.

12

3.2 Necessary and Suﬃcient Conditions for Stability
The proof of Proposition 4 provides some intuition for the next result, which is one of our main contributions. Indeed, as shown in Kokotovic et al. (1986, Chap. 2), as τ → ∞ the ﬁrst n1 eigenvalues of x˙ = −Λτ g(x) tend to ﬁxed positions in the complex plane deﬁned by the eigenvalues of −S1 = −(D12f (x∗) − D12f (x∗)(D22f (x∗))−1D12f (x∗)), while the remaining n2 eigenvalues tend to inﬁnity, with the linear rate τ , along as asymptotes deﬁned by the eigenvalues of D22f (x∗). The asymptotic splitting of the spectrum provides some intuition for the following result.
Theorem 3. Consider a zero-sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x∗ is such that g(x∗) = 0 and S1(J(x∗)) and D22f2(x∗) are non-singular. There exists a τ ∗ ∈ (0, ∞) such that spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞) if and only if x∗ is a diﬀerential Stackelberg equilibrium.
Before getting into the proof sketch, we provide some intuition for the construction of τ ∗ and along the way revive an old analysis tool from dynamical systems theory which turns out to be quite powerful in analyzing stability properties of parameterized systems.

Construction of τ ∗. There is still the question of how to construct such a τ ∗ and do so in a way that
is as tight as possible. Recall Theorem 1 which states that a matrix is exponentially stable if and only if there exists a symmetric positive deﬁnite P = P > 0 such that P Jτ (x∗) + Jτ (x∗)P > 0. The operator L(P ) = Jτ (x∗)P + P Jτ (x∗) is known as the Lyapunov operator. Given a positive deﬁnite Q = Q > 0, −Jτ (x∗) is stable if and only if there exists a unique solution P = P to

((Jτ (x∗) ⊗ I) + (I ⊗ Jτ (x∗)))vec(P ) = (Jτ (x∗) ⊕ Jτ (x∗))vec(P ) = vec(Q)

(12)

where ⊗ and ⊕ denote the Kronecker product and Kronecker sum, respectively.6 The existence of a unique solution P occurs if and only if Jτ and −Jτ have no eigenvalues in common. Hence, using the fact that eigenvalues vary continuously, if we imagine varying τ and examining the eigenvalues of the map (Jτ (x∗) ⊕ Jτ (x∗)), this will tell us the range of τ for which spec(−Jτ (x∗)) remains in C◦−.
This method of varying parameters and determining when the roots of a polynomial (or correspondingly,
the eigenvalues of a map) cross the boundary of a domain uses what is known as a guardian or guard map
(cf. Saydy et al. (1990)). In particular, the guard map provides a certiﬁcate that the roots of a polynomial
lie in a particular guarded domain for a range of parameter values. Formally, let X be the set of all n × n real matrices or the set of all polynomials of degree n with real coeﬃcients. Consider S an open subset of X with closure S¯ and boundary ∂S. The map ν : X → C is said to be a guardian map for S if for all x ∈ S¯,

ν(x) = 0 ⇐⇒ x ∈ ∂S.

Consider an open subset Ω of the complex plane that is symmetric with respect to the real axis (e.g., the

open left-half complex plane C◦−). Then, elements of S(Ω) = {A ∈ Rn×n : spec(A) ⊂ Ω} are said to be stable relative to Ω. Given a pathwise connected subset U of R, a domain S(Ω) and a guard map ν, it is

known that the family {A(τ ) : τ ∈ U } is stable relative to Ω if and only if (i) it is nominally stable—i.e.,

A(τ0) ∈ S(Ω) for some τ0 ∈ U —and (ii) ν(A(τ )) = 0 for all τ ∈ U (Saydy et al., 1990, Prop. 1). To build intuition for the guard map, consider the scalar game on R2 so that the Jacobian at a critical point has the

structure

ab

Jτ (x) = −τ b τ d .

(13)

It is known that a critical point x is stable if det(Jτ (x)) > 0 and tr(Jτ (x)) > 0. Thus, it is fairly easy to see that ν : A → det(A) tr(A) is a guard map for the 2 × 2 Hurwitz stable matrices S(C◦−). Now, the trace

operator can be generalized using a bialternate product, which is denoted A B for matrices A and B and

deﬁned by A

B

=

1 2

(A

⊗

B

+B

⊗ A)

so

that

2(A

I) = A ⊕ A (cf. Govaerts (2000, Sec. 4.4.4)). For

2 × 2 matrices such as Jτ (x) in (13), 2(A I)) = a + τ d = tr(A). Hence, ν : A → det(A) det(2(A I))

generalizes the map ν : A → det(A) tr(A) to an n × n matrix. Replacing A with the parameterized family

6See Lancaster and Tismenetsky (1985); Magnus (1988) for more detail on the deﬁnition and properties of these mathematical operators, and Appendix E for more detail directly related to their use in this paper.

13

() Re(spec( (J J )))

10 5 0 5

1

0

1

2

1

0

1

2

−Jτ (x∗) ν(τ ) ∂ S (C◦−)

S(C◦−) Cn×n ν(τ ∗) = 0

Figure 2: Guard map ν(τ ) and real parts of the eigenvalues of the vectorized Lyapunov operator −(Jτ (x∗) Jτ (x∗)) using the reduction via the duplication matrix for the quadratic example given in Example 1. The largest real positive root of ν(τ ) is τ ∗ = 2 and in the right plot, we see that all the real parts of the eigenvalues
of the Lyapunov operator are negative indicating stability. The right most graphic is a cartoon visualization of the guard map method: the outer grey region represents Cn×n, the blue region represents the Hurwitz stable n × n matrices S(C◦−), the green region represents the parameterized class of matrices {Jτ (x∗)}τ , and the curve cutting through the regions is the guard map ν(τ ). The goal is to ﬁne the subset of {Jτ (x∗)}τ that lie within S(C◦−), which can be done by reducing the problem to ﬁnding the roots of ν(τ ).

of matrices −Jτ (x∗), we have the guard map ν(τ ) = det(−Jτ (x∗)) det(2(−Jτ (x∗) I)). It is fairly easy to see that this polynomial in τ also guards the open left-half complex plane C◦−; details are given in the full proof in Appendix E. In fact, using the Schur complement formula,

det(−Jτ (x∗)) = τ n2 det(D22f (x∗)) det(−S1)
so that if x∗ is a non-degenerate (a condition implied by the hyperbolicity of x∗ for S1 and D22f (x∗)), det(−Jτ (x∗)) does not change the properties of the guard map. In particular, the values of τ ∈ (0, ∞) where ν(τ ) = 0 does not depend on det(−Jτ (x∗)). Hence, we can use the reduced guard map
ν(τ ) = det(2(−Jτ (x∗) I)) = det(−Jτ (x∗) ⊕ (−Jτ (x∗)).

Reﬂecting back to (12), we see that this guard map in τ is closely related to the vectorization of the Lyapunov operator and of course, this is not a coincidence. For any symmetric positive deﬁnite Q = Q > 0, there will be a symmetric positive deﬁnite solution P = P > 0 of the Lyapunov equation

[−(Jτ (x∗) ⊕ Jτ (x∗))]vec(P ) = vec(−Q)

if and only if the operator −(Jτ (x∗) ⊕ Jτ (x∗)) is non-singular. In turn, this is equivalent to det(−(Jτ (x∗) ⊕ Jτ (x∗))) = 0. Hence, to ﬁnd the range of τ for which, given any Q = Q > 0, the solution switches from a
positive deﬁnite P = P > 0 to a negative deﬁnite P = P < 0 we need to ﬁnd the value of τ such that ν(τ ) = det(−(Jτ (x∗) ⊕ Jτ (x∗))) = 0—i.e., where it hits the boundary ∂S(C◦−).

Proof Sketch for Theorem 3. The ‘necessary’ direction follows directly from the above observation, while the

‘suﬃciency’ direction follows by construction.

We leverage the guard map as described above to construct τ ∗. Deﬁne

an

1 2

n(n

+

1)

×

12 n(n + 1)

matrix

from

a

matrix

A

∈

Rn×n

such

that

as an operator that generates

A A = Hn+(A ⊕ A)Hn

where Hn+ = (Hn Hn)−1Hn is the (left) pseudo-inverse of Hn, a full column rank duplication matrix (cf. Appendix C) which maps a n2 (n + 1) vector to a n2 vector generated by applying vec(·) to a symmetric matrix and it is designed to respect the vectorization map vec(·).7 It is fairly straightforward to see that the Kronecker sum A ⊕ A = A ⊗ I + I ⊗ A has spectrum {λj + λi} where λi, λj ∈ spec(A). The operator A A is simply a more computationally eﬃcient expression of A ⊕ A, and as such the eigenvalues of A A are

7The intuition can be gained simply by examining the map ν(τ ) = det(−(Jτ (x∗)⊗Jτ (x∗))), however, this does not produce a tight estimate of τ ∗ and requires more computation due to the redundancies from symmetries in the subcomponents of −Jτ (x∗).

14

those of A ⊕ A removing redundancies. We use A A speciﬁcally because of its computational advantages in computing τ ∗.
We show in Lemma 6 (Appendix C) that ν : A → det(A A) guards the set of n×n Hurwitz stable matrices S(C◦−). We then extend this guard map to the parametric guard map ν(τ ) = det(−(Jτ (x∗) Jτ (x∗))). Indeed, if we consider the subset of the family of matrices parameterized by τ that lies in S(C◦−), then for any τ such that −Jτ (x∗) is in this subset, we have that ν(τ ) = 0 if and only if −(Jτ (x∗) Jτ (x∗)) is singular if and only if −Jτ (x∗) ∈ ∂S(C◦−). This shows that ν(τ ) guards the space of n × n Hurwitz stable matrices S(C◦−). The map ν(τ ) deﬁnes a polynomial in τ and to determine the range of τ such that spec(−Jτ (x∗)) ⊂ C◦−, we need to ﬁnd the value of τ such that ν(τ ) = 0. Towards this end, the guard map ν(τ ) can be further decomposed by applying the Schur determinant formula to −(Jτ (x∗) Jτ (x∗)). This gives rise to a polynomial of the form
ν(τ ) = τ n2(n2+1)/2 det(D22f (x∗) D22f (x∗)) det(S1 S1) det(τ In1n2 − B)
for some n1n2 × n1n2 matrix B. Hence, the problem of determining the value of τ such that ν(τ ) = 0 (i.e., where the polynomial meets the boundary of S(C◦−)) is reduced to an eigenvalue problem in τ for the matrix B. This value of τ is precisely the value τ ∗ and since its derived from an eigenvalue problem it is precisely τ ∗ = λ+max(B) where λ+max(·) is the largest positive real eigenvalue of its argument if one exists and otherwise its zero (meaning that the matrix −Jτ (x∗) is stable for all τ ∈ (0, ∞)). The expression for the matrix B is given in the ﬁll proof contained in Appendix E.
Corollary 1. Consider a zero-sum game G = (f, −f ) with f ∈ Cr(X, R) for some r ≥ 2. Suppose that the assumptions of Theorem 3 hold and that the set of diﬀerential Stackelberg equilibria, denoted DSE(G), is ﬁnite. Let τ ∗ = maxx∗∈DSE(G) τ (x∗) where τ (x∗) is the value of τ obtained via Theorem 3 for each individual critical point x∗ ∈ DSE(G). Then, for all τ ∈ (τ ∗, ∞) and x∗ ∈ DSE(G), spec(−Jτ (x∗)) ⊂ C◦−.
In short, selecting the maximum value of τ ∗ over the ﬁnite set of equilibria guarantees that the local linearization of x˙ = −Λτ g(x) around any diﬀerential Stackelberg equilibria is stable, and hence, the nonlinear system is locally stable around each of these critical points.
New algebraic tools for analysis at the intersection of game theory and machine learning. Before moving on, we remark on the utility of the algebraic tools we use in the proof for Theorem 3. Indeed, the guard map concept is extremely powerful for understanding stability of parameterized families of dynamical systems, and it is not limited to single parameter families. Hence, there is potential to extend the above results to games with more than two players or additional parameters. In fact, we do exactly this in Section 5 where we present results for GANs trained with gradient-penalty type regularizers for the discriminator. Moreover, it is fairly easy to construct analogous guard maps for non-zero sum games. Many of the tools and constructions readily extend. We leave these results to a diﬀerent paper so as to not create too much clutter in the present work.
3.3 Suﬃcient Conditions for Instability
Note that Theorem 3 also implies that for any stable spurious critical points, meaning non-Nash/nonStackelberg equilibria, there is no ﬁnite τ ∗ such that spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞). In particular, there exists at least one ﬁnite, positive value of τ such that spec(−Jτ (x∗)) ⊂ C◦− since the only critical point attractors are diﬀerential Stackelberg equilibria for large enough ﬁnite τ . We can extend this result to address the question of whether or not there exists a ﬁnite learning rate ratio such that for all larger learning rate ratios −Jτ (x∗) has at least one eigenvalue with strictly positive real part, thereby implying that x∗ is unstable.
Theorem 4 (Instability of spurious critical points). Consider a zero sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x∗ is any stable critical point of x˙ = −g(x) which is not a diﬀerential Stackelberg equilibrium. There exists a ﬁnite learning rate ratio τ0 ∈ (0, ∞) such that spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ0, ∞).
Proof Sketch. The full proof is provided in Appendix G. The proof leverages the fact that a nonlinear system is unstable if its linearization is unstable, meaning that the linearization has at least one eigenvalue with
15

strictly positive real part. In our setting, this can be shown by leveraging the Lyapunov equation and Lemma 5 which states that if S1(−J(x∗)) has no eigenvalues with zero real part, then there exists matrices P1 = P1 and Q1 = Q1 > 0 such that P1S1(−J (x∗))+S1(−J (x∗))P1 = Q1 where P1 and S1(−J (x∗)) have the same inertia—i.e., the number of eigenvalues with positive, negative and zero real parts, respectively, are the same. An analogous statement applies to −D22f (x∗). From here, we construct a matrix P that is congruent to blockdiag(P1, P2) and a matrix Qτ such that −P Jτ (x∗) − Jτ (x∗)P = Qτ . Since P and blockdiag(P1, P2) are congruent, Sylvester’s law of inertia implies that they have the same number of eigenvalues with positive, negative, and zero real parts, respectively, so that in turn P has at least one eigenvalue with strictly negative real part. We then construct τ0 via an eigenvalue problem such that for all τ > τ0, Qτ > 0. Applying Lemma 5 again, we get that Jτ (x∗) has at least one eigenvalue with strictly negative real part so that spec(−Jτ (x∗)) ⊂ C◦− for all τ > τ0.
Unlike τ ∗ Theorem 3, τ0 in Theorem 4 is not tight in the sense that −Jτ (x∗) may become unstable for τ < τ0. The reason for this is that there are potentially many matrices P1 and Q1 that satisfy S1(J(x∗))P1 + P1S1(J(x∗)) = Q1 such that S1(J(x∗)) and P1 have the same inertia; an analogous statement holds for P2, Q2 and −D22f (x∗). The choice of these matrices impact the value of τ0. Hence, the question of ﬁnding the exact value of τ beyond which a spurious stable critical point for 1-GDA is unstable remains open.
4 Provable Convergence of GDA with Timescale Separation
In this section, derive convergence guarantees for τ -GDA to diﬀerential Stackelberg equilibria in both the deterministic (i.e., where agents have oracle access to their individual gradients) and the stochastic (i.e., where agents have an unbiased estimator of their individual gradient) settings.
4.1 Convergence Rate of Deterministic GDA with Timescale Separation
As a corollary to Theorem 3, we ﬁrst show that the discrete time τ -GDA update is locally asymptotically stable for a range of learning rates γ1.
We need the following lemma to prove asymptotic convergence as well as the subsequent results on convergence rates.
Lemma 1. Consider a zero-sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x∗ is a diﬀerential Stackelberg equilibrium and that given τ > 0, spec(−Jτ (x∗)) ⊂ C◦−. Let γ = minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2. For any γ1 ∈ (0, γ), τ -GDA converges locally asymptotically.
Corollary 2 (Asymptotic convergence of τ -GDA). Suppose the assumptions of Theorem 3 hold so that x∗ is a critical points of g and S1(J(x∗)) and D22f2(x∗) are non-singular. There exists a τ ∗ ∈ (0, ∞) such that τ -GDA with γ1 ∈ (0, γ(τ )) where γ(τ ) = arg minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2 converges locally asymptotically for all τ ∈ (τ ∗, ∞) if and only if x∗ is a diﬀerential Stackelberg equilibrium.
In addition to showing asymptotic convergence, we also provide an asymptotic convergence rate. To prove the main theorems on convergence rates for both diﬀerential Stackelberg and diﬀerential Nash equilibria, we use a common argument which is summarized in the lemma below.
Lemma 2. Consider a zero-sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x∗ is a diﬀerential Stackelberg equilibrium and that given τ , spec(−Jτ (x∗)) ⊂ C◦−. Let γ = minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2, and λm = arg minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2. For any α ∈ (0, γ), τ GDA with learning rate γ1 = γ − α converges locally asymptotically at a rate of O((1 − 4αβ )k/2) where β = (2Re(λm) − α|λm|2)−1.
The full proof of the above lemma is provided in Appendix C, and a short proof sketch with the main ideas is summarized below.
Proof Sketch. Consider a zero-sum game (f, −f ) as articulated in the lemma statement where x∗ is either a diﬀerential Stackelberg or diﬀerential Nash equilibrium and ﬁx any τ such that spec(Jτ (x∗)).
16

Im(z) λm

Im(λm)

θ Re(λm)

λ2

1

Re(z)

λ1

λ3

Figure 3: The inner maximization problem in (14) is over a ﬁnite set spec(Jτ (x∗)) = {λ1, . . . , λn} where Jτ (x∗) ∈ Rn×n. As γ → ∞, |1 − γλi| → 0. The last λi such that 1 − γλi hits the boundary of the unit circle in the complex plane—i.e., |1 − γλi| = 1—gives us the optimal value of γ = 2Re(λm)/|λm|2 = 2 cos(θ)/|λm| and the element of spec(Jτ (x∗)) that achieves it (see blue arrows).

For the discrete time dynamical system xk+1 = xk − γ1Λτ g(xk), it is well known that if γ1 is chosen such that ρ(I − γ1Jτ (x∗)) < 1, then xk locally asymptotically converges to x∗ (cf. Proposition 6, Appendix A).
With this in mind, we formulate an optimization problem to ﬁnd the upper bound γ on the learning rate
γ1 such that for all γ1 ∈ (0, γ), the spectral radius of the local linearization of the discrete time map is a contraction which is precisely ρ(I − γ1Jτ (x∗)) < 1. The optimization problem is given by

γ = min γ : max |1 − γλ| = 1 .

γ>0

λ∈spec(Jτ (x∗))

(14)

The intuition is as follows. The inner maximization problem is over a ﬁnite set spec(Jτ (x∗)) = {λ1, . . . , λn} where Jτ (x∗) ∈ Rn×n. As γ increases away from zero, each |1 − γλi| shrinks in magnitude. The last λi such that 1 − γλi hits the boundary of the unit circle in the complex plane (i.e., |1 − γλi| = 1) gives us the optimal value of γ and the element of spec(Jτ (x∗)) that achieves it. Examining the constraint, we have that for each
λi, γ(γ|λi|2 − 2Re(λi)) ≤ 0
for any γ > 0. As noted this constraint will be tight for one of the λ, in which case γ = 2Re(λ)/|λ|2 since γ > 0. Hence, by selecting γ = minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2, we have that |1 − γ1λ| < 1 for all λ ∈ spec(Jτ (x∗)) and any γ1 ∈ (0, γ).
From here, we let λm = arg minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2 so that by ﬁxing any α ∈ (0, 1) and deﬁning β = (2Re(λm) − α|λm|2)−1 and γ1 = γ − α, we obtain the claimed convergence rate by standard arguments
in numerical analysis. In particular, we apply an argument along the lines of Proposition 6 in Appendix A. Indeed, given the τ -GDA update, ρ(I − γ1Jτ (x∗)) = 1 − η < 1 for some η ∈ (0, 1) implies there exists a neighborhood U of x∗ such that if τ -GDA is initialized in U , xk − x∗ ≤ (1 − η/4)k/2 x0 − x∗ thereby giving
the desired rate. The full details on this part of the argument can be found in Appendix C.

The above lemma provides a convergence rate given a diﬀerential Stackelberg equilibrium x∗ and a learning rate ratio τ such that x∗ is stable with respect to the dynamics x˙ = −Λτ g(x).
The following theorem—which uses Lemma 2 in its proof—characterizes the iteration complexity for τ -GDA. Speciﬁcally, the result leverages Theorem 3 to construct a ﬁnite τ ∗ ∈ (0, ∞) such that −Jτ (x∗) is (Hurwitz) stable, and then for any τ ∈ (τ ∗, ∞), Lemma 2 implies a local asymptotic convergence rate.
Theorem 5. Consider a zero-sum game (f1, f2) = (f, −f ) deﬁned by f ∈ Cr(X, R) for r ≥ 2 and let x∗ be a diﬀerential Stackelberg equilibrium of the game. There exists a τ ∗ ∈ (0, ∞) such that for any τ ∈ (τ ∗, ∞) and α ∈ (0, γ), τ -GDA with learning rate γ1 = γ − α converges locally asymptotically at a

17

rate of O((1 − 4αβ )k/2) where γ = minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2, λm = arg minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2, and β = (2Re(λm)−α|λm|2)−1. Moreover, if x∗ is a diﬀerential Nash equilibrium, τ ∗ = 0 so that for any τ ∈ (0, ∞) and α ∈ (0, γ), τ -GDA with γ1 = γ − α converges with a rate O((1 − 4αβ )k/2).
Proof. To prove this result, we apply Theorem 3 to construct τ ∗ via the guard map ν(τ ) = det(−Jτ (x∗) −Jτ (x∗)) such that for all τ ∈ (τ ∗, ∞), spec(Jτ (x∗)) ⊂ C◦+. This guarantees that spec(−Jτ (x∗)) ⊂ C◦− for any τ ∈ (τ ∗, ∞) and hence the nonlinear dynamical system
x˙ = −Λτ g(x)
is locally asymptotically (in fact, exponentially) stable by the Hartman-Grobman theorem (cf. Theorem 2). Therefore, for any τ ∈ (τ ∗, ∞), by Lemma 2, τ -GDA converges with a rate of O((1 − 4αβ )k/2).
Remark 2. We note that as τ → ∞, the eigenvalues of Jτ (x∗) become real. In fact there exists a ﬁnite value of τ ∈ (τ ∗, ∞) after which spec(Jτ (x∗)) ⊂ R. Hence, there is an opportunity to take advantage of momentumbased approaches when timescale separation via τ is introduced. This may provide some explanation for why the heuristics of timescale separation or unrolled updates in combination with momentum work in practice when training generative adversarial networks. We believe there may be potential to precisely characterize when the eigenvalues of the Jacobian become purely real as a function of τ by constructing a suitable guard map and this is a direction of future work.
Theorem 5 directly implies a ﬁnite time convergence guarantee for obtaining an ε-diﬀerential Stackelberg equilibrium—i.e., an point with an ε-ball around a diﬀerential Stackelberg x∗.
Corollary 3. Given ε > 0, under the assumptions of Theorem 5, τ -GDA obtains an ε–diﬀerential Stackleberg equilibrium in 4αβ log( x0 − x∗ /ε) iterations for any x0 ∈ Bδ(x∗) with δ = α/(4Lβ) where L is the local Lipschitz constant of I − γJτ (x∗).
Comments on computing the neighborhood Bδ(x∗). We note that we have essentially given a proof that there exists a neighborhood on which τ -GDA converges. Of course, due to the non-convexity of the problem in general, this neighborhood could be arbitrarily small. We provide an estimate of the neighborhood size using the local Lipschitz constant of the local linearization I − γ1Jτ (x∗). One way to better understand the size of this neighborhood is to use Lyapunov analysis, a tool which is well explored in the singular perturbation theory (Kokotovic et al., 1986). In particular, Lyapunov methods can be applied directly to the nonlinear system if one can construct Lyapunov functions for the fast and slow subsystems individually— also known as the boundary layer model and reduced order model. With these Lyapunov functions in hand, one can “stitch” the two together (via convex combination) and show under some reasonable assumptions that this combined function is a Lyapunov function for the overall singularly perturbed system. The beneﬁt of this analysis is that the Lyapunov function gives one an estimate of the region of attraction (via, e.g., the level sets); however, it is not easy to construct a Lyapunov function for a nonlinear system in general. We leave expanding to such methods to future work.
Comments on avoiding saddle points. Before turning to the stochastic setting, we comment on saddle point avoidance in the deterministic setting. It was shown by Mazumdar et al. (2020) that gradient-based learning in continuous games with heterogeneous learning rates avoids saddles on all but a set of measure zero initializations. Hence, τ -GDA avoids saddles for almost every initialization. We also know that all diﬀerential Nash equilibria are locally asymptotically stable for zero-sum settings. Hence, there are no diﬀerential Nash equilibria that are saddle points of the dynamics x˙ = −Λτ g(x). On the other hand, as Example 1 shows, there are diﬀerential Stackelberg equilibria which correspond to saddle points of the dynamics for some choices of τ —in particular, τ = 1 in that example. Theorem 3 and Corollary 1, however, implies that for a given zero-sum game (or minmax problem), there exists a ﬁnite τ ∗ such that all locally asymptotically stable equilibria are diﬀerential Stackelberg equilibria. Hence, an ‘almost sure’ saddle point avoidance result together with the local convergence guarantee provided by Theorem 5 provides a strong characterization of long-run learning behavior.
Avoidance of saddles nor the if and only if convergence guarantee of Theorem 3 are, however, enough to ensure avoidance of limit cycles. In fact, it is known that limit cycles can exist in zero sum games (Daskalakis
18

et al., 2018; Mazumdar et al., 2020). Understanding when such complex phenomena exist in games and determining how to ascribe meaning the behavior is an active area of study (see, e.g., the work of Papadimitriou and Piliouras (2019)).

4.2 Convergence of Stochastic GDA with Timescale Separation
In this section, we analyze convergence when players do not have oracle access to their gradients but instead have an unbiased estimator in the presence of zero mean, ﬁnite variance noise. Speciﬁcally, we show that the agents will converge locally asymptotically almost surely to a diﬀerential Stackelberg equilibrium.
The stochastic form of the update is given by

xk+1 = xk − γk(Λτ g(xk) + wk+1)

(15)

where wk+1 is a zero mean, ﬁnite variance random variable and {γk} is the learning rate sequence.
Assumption 1. The stochastic process {wk} is a martingale diﬀerence sequence with respect to the increasing family of σ-ﬁelds deﬁned by
Fk = σ(x , w , ≤ k), ∀k ≥ 0,
so that E[wk+1| Fk] = 0 almost surely (a.s.) for all k ≥ 0. Moreover, wk is square-integrable so that, for some constant C > 0,
E[ wk+1 2| Fk] ≤ C(1 + xk 2) a.s., ∀k ≥ 0.

We note that this assumption has been relaxed in the literature (cf. Thoppe and Borkar (2019)), however simplicity, we state the theorem with the most accessible criteria. We remark below in the paragraph on extensions to concentration bounds on the nature of the relaxed assumptions.
Theorem 6. Consider a zero-sum game (f, −f ) such that f ∈ Cr(X, R) for some r ≥ 2. Suppose that Assumption 1 holds and that {γk} is square summable but not summable—i.e., k γk2 < ∞, yet k γk = ∞. For any τ ∈ (0, ∞), the sequence {xk} generated by (15) converges to a, possibly sample path dependent, internally chain transitive invariant set of x˙ = −Λτ g(x). Moreover, if x∗ is a diﬀerential Stackelberg equilibrium, then there exists a ﬁnite τ ∗ ∈ (0, ∞) such that {xk} almost surely converges locally asymptotically to x∗ for every τ ∈ (0, ∞).
Proof. The convergence of {xk} to a, possibly sample path dependent, compact connected internally chain transitive invariant set of x˙ = −Λτ g(x) follows from classical results in stochastic approximation theory (cf. Borkar (2008, Chap. 2); Benaim (1996)).
Suppose that x∗ is a diﬀerential Stackelberg equilibrium. By Theorem 3, there exists a ﬁnite τ ∗ ∈ (0, ∞) such that for all τ ∈ (τ ∗, ∞), x∗ is a locally exponentially stable equilibrium of the continuous time dynamics x˙ = −Λτ g(x)—that is, spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞).
Fix arbitrary τ ∈ (τ ∗, ∞). Since spec(−Jτ (x∗)) ⊂ C◦−, det(−Jτ (x∗)) = 0 so that x∗ is an isolated critical point. Furthermore, exponentially stability of x∗ implies that there exists a (local) Lyapunov function deﬁned on a neighborhood of x∗ by the converse Lyapunov theorem (cf. Sastry (1999, Thm. 5.17) or Krasovskii (1963, Thm. 4.3)). Let U be the neighborhood of x∗ on which the local Lyapunov function is deﬁned, such that U contains no other critical points (which is possible since x∗ is isolated). That is, let Φ : U → [0, ∞) be the local Lyapunov function deﬁned on U where x∗ ∈ U , Φ is positive deﬁnite on U , and for all x ∈ U , ddt Φ(x) ≤ 0 where equality holds for z ∈ U if and only if Φ(z) = 0. By Corollary 3 (Borkar, 2008, Chap. 2), {xk} converges to an internally chain transitive invariant set contained in U almost surely. The only internally chain transitive invariant set in U is x∗.
Corollary 4. Consider a zero-sum game (f, −f ) such that f ∈ C2(X, R). Suppose that Assumption 1 holds and that {γk} is square summable but not summable—i.e., k γk2 < ∞, yet k γk = ∞. If there exists a ﬁnite τ ∗ ∈ (0, ∞) such that spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞), then x∗ is a diﬀerential Stackelberg equilibrium and {xk} almost surely converges locally asymptotically to x∗.

19

While (local) almost sure convergence in gradient descent-ascent (Chasnov et al., 2019) to a critical point8 in the stochastic setting, the result requires time varying learning rates with a suﬃcient separation in timescale. Speciﬁcally, the players need to be using learning rate sequences {γi,k} for each i ∈ {1, 2} such that (without loss of generality) not only is it assumed that γ1,k = o(γ2,k), but also k γ12,k + γ22,k < ∞ and
k γi,k = ∞ for each i ∈ {1, 2}. The challenge with these assumptions on the learning rate sequences is that empirically the sequences that satisfy them result in poor behavior along the learning path such as getting stuck at saddle points or making no progress. This is, in essence, due to the fact that the faster player—i.e., player 2 if γ1,k = o(γ2,k)—equilibriates too quickly causing progress to stall. This can result in undesirable behavior such as vanishing gradients (so that the discriminator does not provide enough information for the generator to make progress), mode collapse, or failure to converge in practical applications such as generative adversarial networks.
On the other hand, our convergence result gives a similar guarantee with less restrictive requirements on the stepsize sequence. In particular, only a single stepsize sequence is required (so that the algorithm can be viewed as a single timescale stochastic approximation update) as long as the fast player (who, without loss of generality, is player 2 in this paper) scales their estimated gradient by τ ∈ (τ ∗, ∞) where τ ∗ is as in Theorem 3.
Extensions to concentration bounds and relaxed assumptions on stepsizes. It is possible to obtain concentration bounds and even ﬁnite time, high probability guarantees on convergence leveraging recent advances in stochastic approximation (Borkar, 2008; Kamal, 2010; Thoppe and Borkar, 2019). To our knowledge, the concentration bounds in (Thoppe and Borkar, 2019) require the weakest assumptions on learning rates—e.g., the stepsize sequence {γk} needs only to satisfy k γk = ∞, limk→∞ γk = 0, and
k γk ≤ 1. Speciﬁcally, since it is assumed, for the zero sum game (f, −f ), that f ∈ C2(X, R) and x∗ is a diﬀerential Stackelberg equilibrium, Theorem 3 implies that x∗ is a locally asymptotically stable attractor of x˙ = −Λτ g(x) for arbitrary ﬁxed τ ∈ (τ ∗, ∞), and hence, the concentration bounds in Theorem 1.1 and 1.2 of (Thoppe and Borkar, 2019) directly apply.
Furthermore, we note that in applications such as generative adversarial networks, while it has been observed that timescale separation heuristics such as unrolling or annealing the stepsize of the discriminator work well, in the stochastic case, summmable/square-summable assumptions on stepsizes are generally too restrictive in practice since they lead to a rapid decay in the stepsize which, in turn, can stall progress. On the other hand, stepsize sequences such as γk = 1/(k + 1)β for β ∈ (0, 1]—a sequence which satisﬁes the assumptions posed in (Thoppe and Borkar, 2019)—tend not to have this issue of decaying too rapidly for appropriately chosen β, while also maintaining the guarantees of the theoretical results. We state a convergence guarantee under these relaxed assumptions in Proposition 8 which is contained in Appendix J.
5 Regularization with Applications to Adversarial Learning
In this section, we focus on generative adversarial networks with regularization. Speciﬁcally, using the theory developed so far, we extend the results in Mescheder et al. (2018) to provide a convergence guarantee for a range of regularization parameters and learning rate ratios.
As has been repeatedly observed in recent theoretical works on generative adversarial networks, the training dynamics of generative adversarial networks are not well understood even though we have seen impressive practical advances over the last few years. In an attempt to address this, recent works—e.g., (Berard et al., 2020; Fiez et al., 2020; Mescheder et al., 2017; Nagarajan and Kolter, 2017) amongst others— study the optimization landscape of generative adversarial networks through the lens of dynamical systems theory which provides analysis tools for convergence based on the eigen-structure of the local linearization of the learning dynamics. Nagarajan and Kolter (2017) show, under suitable assumptions, that gradient-based methods for training generative adversarial networks are locally convergent assuming the data distributions are absolutely continuous. However, as observed by Mescheder et al. (2018), such assumptions not only may not be satisﬁed by many practical generative adversarial network training scenarios such as natural images, but it can often be the case that the data distribution is concentrated on a lower dimensional manifold. The latter characteristic leads to nearly purely imaginary eigenvalues and highly ill-condition problems.
8To date it has not been shown that for a suﬃcient separation in timescale the only critical point attractors are local minmax.
20

Mescheder et al. (2018) provide an explanation for observed instabilities consequent of the true data distribution being concentrated on a lower dimensional manifold using discriminator gradients orthogonal to the tangent space of the data manifold. Further, the authors introduce regularization via gradient penalties that leads to convergence guarantees under less restrictive assumptions than were previously known. Here, we further extend these results to show that convergence to diﬀerential Stackelberg equilibria is guaranteed under a wide array of hyperparameter conﬁgurations.
Consider the training objective of the form
f (θ, ω) = Ep(z)[ (D(G(z; θ); ω))] + EpD(x)[ (−D(x; ω))]
where Dω(x) and Gθ(z) are discriminator and generator networks, respectively, and pD(x) is the data distribution while p(z) is the latent distribution. The goal of the generator is to minimize the above loss and the discriminator to maximize it. The mapping ∈ C2(R) is some real-value function; e.g., a common choice is (x) = − log(1 + exp(−x)) as in the original generative adversarial network paper (Goodfellow et al., 2014). To motivate both regularization and time-scale separation, consider the following prototypical example of a Dirac-GAN.

Example: Dirac-GAN. The Dirac-GAN is arguably as simple an example as one can construct that posses interesting and non-trivial degeneracies. The parameter space for each the generator and discriminator is just R—that is, the generator and discriminator have scalar “network”.
Deﬁnition 5. The Dirac-GAN consists of a univariate generator distribution pθ = δθ and a linear discriminator D(x; ω) = ω x. The true data distribution pD is given by a Dirac-distribution concentrated at zero.

The objective of the Dirac-GAN is

f (θ, ω) = (ωθ) + (0).

Several choices exist for the mapping including (t) = − log(1+exp(−t)) which leads to the Jensen-Shannon

divergence between pθ and pD. As described in (Mescheder et al., 2018), when training a Dirac-GAN with

vanilla GDA, the dynamics are oscillatory. This can immediately be veriﬁed by examining the eigenvalues of

the Jacobian at the (unique) equilibrium (θ∗, ω∗) = (0, 0) which are purely complex taking on values ±i (0)

since

0 J(0) = − (0)

(0) . 0

It is observed in Lemma 3.1 Mescheder et al. (2018) that GDA will not generally converge even with unrolling of the discriminator, a proxy for timescale separation. We observe, however, that this is because the equilibrium is not hyperbolic, and hence it is not structurally stable (Broer and Takens, 2010). In fact, introducing regularization remedies these issues. Under reasonable generative adversarial network assumptions, we show that the introduction of a gradient penalty based regularization to the discriminator does not change the set of critical points for the dynamics and, further, for any learning rate ratio τ ∈ (0, ∞) and any positive, ﬁnite regularization parameter µ, the continuous time limiting regularized learning dynamics remain stable, and hence, there is a range of learning rates γ1 for which the discrete time update locally converges asymptotically.

Introducing Regularization. Gradient penalties ensure that the discriminator cannot create a non-zero gradient which is orthogonal to the data manifold without suﬀering a loss. Introduced by Roth et al. (2017) and reﬁned in Mescheder et al. (2018), we consider training generative adversarial networks with one of two fairly natural gradient-penalties used to regularize the discriminator:
R1(θ, ω) = µ2 EpD(x)[ ∇xD(x; ω) 2], (16) R2(θ, ω) = µ2 Epθ(x)[ ∇xD(x; ω) 2], (17)
where, by a slight abuse of notation, ∇x(·) denotes the partial gradient with respect to x of the argument (·) when the argument is the discriminator D(·; ω) in order prevent any confusion between the notation D(·) which we use elsewhere for derivatives.

21

Also following Mescheder et al. (2018), we use relaxed assumptions—as compared to the work by Nagara-
jan and Kolter (2017)—which allow us to consider generative adversarial networks with data distributions
that do not (locally) have the same support and hence, are concentrated on lower dimensional manifolds, a
commonly observed phenomena in practice (Arjovsky et al., 2017). To state these assumptions, we need some additional notation. Let h1(θ) = Epθ(x)[∇ωD(x; ω)|ω=ω∗ ] and
h2(ω) = EpD(x)[|D(x; ω)|2 + ∇xD(x; ω) 2]. Deﬁne reparameterization manifolds MG = {θ : pθ = pD} and MD = {ω : h2(ω) = 0} and let Tθ∗ MG and Tω∗ MD denote their respective tangent spaces at θ∗ and ω∗.
Assumption 2. Consider training a generative adversarial network via a zero-sum game deﬁned by f ∈ C2(Rm1 × Rm2 , R) where G(·; θ) and D(·; ω) are the generator and discriminator networks, respectively, and x = (θ, ω) ∈ Rm1 × Rm2 . Suppose that x∗ = (θ∗, ω∗) is an equilibrium.
a. At (θ∗, ω∗), pθ∗ = pD and D(x; ω∗) = 0 in some neighborhood of supp(pD).
b. The function ∈ C2(R) satisﬁes (0) = 0 and (0) < 0. c. There are –balls B (θ∗) and B (ω∗) centered around θ∗ and ω∗, respectively, so that MG ∩ B (θ∗) and
MD ∩ B (ω∗) deﬁne C1-manifolds. Moreover, (i) if w ∈/ Tθ∗ MG, then w ∇wh1(θ∗)w = 0, and (ii) if v ∈/ Tω∗ MD, then v ∇2ωh2(ω∗)v = 0.

Recalling the observations in Mescheder et al. (2018) used for explaining the last of the three assump-
tions, Assumption 2.c(i) implies that the discriminator is capable of detecting deviations from the generator distribution in equilibrium, and Assumption 2.c(ii) implies that the manifold MD is suﬃciently regular and, in particular, its (local) geometry is captured by the second (directional) derivative of h2.
The Jacobian of the regularized dynamics, for either j = 1 or 2, is of the form

J (θ, ω) = D12f (θ, ω)

D12f (θ, ω)

(18)

(τ,µ)

−τ D12f (θ, ω) τ (−D22f (θ, ω) + µD2Rj(θ, ω))

where we use the subscript (τ, µ) to denote the parameter dependence on the learning rate ratio τ and
regularization parameter µ. For shorthand, we often replace (θ, ω) simply with the variable x.
It is straightforward to compute the block components of the Jacobian. Observe that Assumption 2.a implies that D12f (θ∗, ω∗) is identically zero, and hence x∗ = (θ∗, ω∗) is never a diﬀerential Nash equilibrium. However, we show that x∗ is not only a diﬀerential Stackelberg equilibrium, but also characterize the learning rate ratio and regularization parameter range for which x∗ is (locally) stable with respect to τ -GDA and give a convergence rate.

Theorem 7. Consider training a generative adversarial network via a zero-sum game with generator network
Gθ, discriminator network Dω, and loss f (θ, ω) with regularization Rj(θ, ω) (for either j = 1 or j = 2) and any regularization parameter µ ∈ (0, ∞) such that Assumption 2 is satisﬁed for an equilibrium x∗ = (θ∗, ω∗) of the regularized dynamics. Then, x∗ = (θ∗, ω∗) is a diﬀerential Stackelberg equilibrium. Furthermore, for any τ ∈ (0, ∞), spec(−J(τ,µ)(x∗)) ⊂ C◦−—i.e., −J(τ,µ)(x∗) is Hurwitz stable.
Proof Sketch. To proof that x∗ is a diﬀerential Stackelberg equilibrium follows analogous arguments to those
in the proof of Theorem 4.1 in Mescheder et al. (2018). Given any positive regularization parameter µ, to prove the stability of x∗ for any ﬁxed τ ∈ (0, ∞), we leverage the concept of the quadratic numerical range of a block operator which is a superset of the spectrum of the operator (cf. Appendix A). The key for both
arguments is that Assumption 2 implies that the Jacobian of the regularized game has a speciﬁc structure. Indeed, observe that the structural form of J(τ,µ)(x∗) is

J(τ,µ)(x∗) = −τ0B

B τ (C + µR)

where B = D12f (x∗), C = −D22f (x∗) and R = D22Rj(x∗). This follows from Assumption 2-a., which implies

that

D(x; ω∗)

=

0

in

some

neighborhood

of

supp(pD )

and

hence,

∇xD(x; ω∗)

=

0

and

∇

2 x

D(

x;

ω

∗

)

=

0

for

x ∈ supp(pD). In turn, we have that D12f (x∗) = 0.

From here, it is straightforward to argue that B is full rank and C + µR > 0 away from Tθ∗ MG (by

Assumption 2-c). Together these observations imply that x∗ is a diﬀerential Stackelberg equilibrium. Then,

arguments analogous to those in the proof of Proposition 3 (via the quadratic numerical range) imply

stability.

22

Corollary 5. Under the assumptions of Theorem 7, consider any ﬁxed µ ∈ (0, µ1] and τ ∈ (0, ∞), and let γ = minλ∈spec(J(τ,µ)(x∗)) 2Re(λ)/|λ|2, and λm = arg minλ∈spec(J(τ,µ)(x∗)) 2Re(λ)/|λ|2. For any α ∈ (0, γ), τ -GDA with learning rate γ1 = γ − α converges locally asymptotically at a rate of O((1 − 4αβ )k/2) where β = (2Re(λm) − α|λm|2)−1.
The proof of the above corollary follows a similar line of reasoning as the proof of Corollary 2. Theorem A.7 of Mescheder et al. (2018) shows that matrices of the form
− J = B0 −−BC (19)
are stable if B is full rank and C > 0. The following proposition provides necessary conditions on the sizes of the network architectures for the discriminator and generator network for stability.
Proposition 5. Consider training a generative adversarial network via a zero-sum game with generator network Gθ, discriminator network Dω, and loss f (θ, ω) with regularization Rj(θ, ω) (for some j ∈ {1, 2}) such that Assumption 2 is satisﬁed for an equilibrium x∗ = (θ∗, ω∗). Independent of the learning rate ratio and the regularization parameter µ, for x∗ to be stable it is necessary that the dimension of the discriminator network parameter vector is at least half as large as the corresponding generator network parameter vector— i.e., n2 ≥ n1/2 where θ ∈ Rn1 and ω ∈ Rn2 .
The intuition for the why this proposition should hold follows immediately from observing the structure of the Jacobian: for any matrix of the form (19), at least one eigenvalue will be purely imaginary if n2 < n1/2 where B ∈ Rn1×n2 and C ∈ Rn2×n2 . The full proof is contained in Appendix I.

6 Experiments
We now present extensive numerical experiments examining gradient descent-ascent with timescale separation. As we explored theoretically so far, the stability of gradient descent-ascent critical points has an intricate relationship with timescale separation. We begin to investigate this behavior empirically by simulating the gradient descent-ascent dynamics for the games from Examples 1 and 2 and examining how the spectrum of the game Jacobian evolves as a function of the timescale separation. Then, on a polynomial game, we demonstrate how timescale separation warps the vector ﬁeld of gradient descent-ascent and consequently shapes the region of attraction around critical points in the optimization landscape. There are a number of both qualitative and quantitative theoretical questions that remain open related to characterizing the region of attraction and how it depends parameterically on τ .
After exploring the optimization landscape, we focus in on gradient descent-ascent in the Dirac-GAN game and illustrate the interplay between timescale separation, regularization, and rate of convergence. Finally, we train generative adversarial networks on the CIFAR-10 and CelebA datasets with regularization and show timescale separation can signiﬁcantly improve stability and performance. Moreover, we ﬁnd that several of the insights we draw from the Dirac-GAN game carry over to this complex setting. Appendix L contains several more experimental results including a generative adversarial network formulation to learn a covariance matrix and a torus game. The code for our experiments is available at github.com/fiezt/Finite-Learning-Ratio.

6.1 Quadratic Game: Timescale Separation and Stackelberg Stability

We now revisit the game from Example 1 that demonstrated there exists diﬀerential Stackelberg equilibrium that are unstable for choices of the timescale separation τ . To be clear, we repeat the game construction and some characteristics of the game. Let us consider the quadratic zero-sum game deﬁned by the cost

−v 0 −v 0 

1 f (x , x ) =

x1

0

21 v

0

12 v  x1

(20)

12

2 x2

−v

0

− 12 v

 0  x2

0 12 v 0 −v

23

(a)

(b)

(c)

(e)

(f )

(g)

Figure 4: Experimental results for the quadratic game deﬁned in (20) of Section 6.1 and presented in Example 1. Figures 4a and 4b show trajectories of the players coordinate pairs (x11, x21) and (x21, x22) for a range of learning rate ratios, respectively. Figures 4c shows the distance from the equilibrium along the learning paths. Figures 4e, 4f, and 4g show the trajectories of the eigenvalues, the real parts of the eigenvalues, and the imaginary parts of the eigenvalues for the Jτ (x∗) as a function of the τ , respectively.

where x1, x2 ∈ R2 and v > 0.

The

unique

critical

point

of

the

game

given

by

x∗

=

(

x

∗ 1

,

x∗2

)

=

(0, 0)

is

a

diﬀerential Stackelberg equilibrium. The spectrum of the Jacobian evaluated at the equilibrium is given by

spec(Jτ (x∗)) =

√

√

v(2τ + 1 ± 4τ 2 − 8τ + 1) , v(τ − 2 ± τ 2 − 12τ + 4) .

4

4

As mentioned in Example 1, it turns out that spec(Jτ (x∗) ⊂ C◦+ only when τ ∈ (2, ∞). We remark that we computed τ ∗ using the theoretical construction from Theorem 3 and found that it recovered the precise value

of τ ∗ = 2 such that the equilibrium is stable for all τ ∈ (τ ∗, ∞) with respect to the dynamics x˙ = −Λτ g(x). In the experiments that follow, we consistently observe that the construction of τ ∗ from the theory is tight.

For

this

experiment,

we

select

v

=

4

and

simulate

τ -GDA

from

the

initial

condition

(

x

0 1

,

x02

)

=

(5, 4, 3, 2)

with γ1 = 0.0005 and τ ∈ {2, 2.5, 3, 5, 10}. In Figures 4a and 4b, we show the trajectories of the players

coordinate pairs (x11, x21) and (x21, x22), respectively. We observe that τ -GDA cycles around the equilibrium

with τ = 2 since it is marginally stable with respect to the dynamics. For τ ∈ (2, ∞), the equilibrium is

stable and τ -GDA ends up converging to it at a rate that depends on the choice of τ . We demonstrate how

the convergence rate depends on the choice of τ in Figure 4c by showing the distance from the equilibrium

along the learning path for each of the trajectories. The primary observation is that the cyclic behavior of

τ -GDA dissipates as τ grows and as a result the dynamics then rapidly converge to the equilibrium. The behavior of the learning dynamics as a function of the timescale separation τ can be further explained

by evaluating the eigenvalues of the game Jacobian at the equilibrium. We show the eigenvalues of the

Jacobian at the equilibrium in several forms in Figures 4e, 4f, and 4g. Analyzing the spectrum, we are able to verify that for all τ ∈ (2, ∞) the equilibrium is indeed stable. Moreover, we see that the imaginary parts of the conjugate pairs of eigenvalues decay after τ = 1 and τ = 6, and then the eigenvalues of the

24

(a)

(b)

(c)

(e)

(f )

Figure 5: Experimental results for the polynomial game deﬁned in (21) of Section 6.2 and presented in
Example 2. Figures 5a and 5b show trajectories of the players coordinate pairs (x11, x21) and (x21, x22) for a range of learning rate ratios, respectively. Figures 5d, 5e, and 5f show the trajectories of the eigenvalues, the real parts of the eigenvalues, and the imaginary parts of the eigenvalues for Jτ (x∗) as a function of the τ , respectively where x∗ is the non-equilibrium critical point.

conjugate pairs eventually become purely real at τ = 1.87 and τ = 11.66, respectively. After the eigenvalues of a conjugate pair become purely real, they split so that one of the eigenvalues asymptotically converges to an eigenvalue of S1(J(x∗)) by moving back along the real line, while the other eigenvalue tends toward an eigenvalue of −τ D22f (x∗). This occurrence is exactly what was described in Section 3 as an immediate implication of Proposition 4 when the eigenvalues of S1(J(x∗)) and τ D22f (x∗) are distinct. The convergence rate is in fact limited by the eigenvalues splitting since as τ grows, the spectrum of the Jacobian is limited by the eigenvalues of the Schur complement which remain constant. A related open question centers on ﬁnding the worst case convergence rate as a function of the spectral properties of S1(J(x∗)) and D22f (x∗). Finally, the evolution of the eigenvalues as a function of the timescale separation τ demonstrates that the rotational dynamics in τ -GDA vanish as the ratio between the magnitude of the real and imaginary parts of the eigenvalues grows.
6.2 Polynomial Game: Timescale Separation and Non-Equilibrium Stability
We now return to the game from Example 2 that showed a non-equilibrium critical point which is stable without timescale separation and becomes unstable for a range of ﬁnite learning ratios with multiple equilibria in the vicinity. Again, we repeat the game construction along with some of the key characteristics that were

25

previously presented in Example 2. Consider a zero-sum game deﬁned by the cost

f (x1, x2)

=

5 4

x211 + 2x11x21 + 12 x221 − 12 x212 + 2x12x22 − x222

(x11 − 1)2

+ x211 2i=1(x1i − 1)2 − (x2i − 1)2 .

(21)

This game has critical points at (0, 0, 0, 0), (1, 1, 1, 1), and (−4.73, 0.28, −92.47, 0.53). Among the critical

points, only (1, 1, 1, 1) and (−4.73, 0.28, −92.47, 0.53) are game-theoretically meaningful equilibrium. In fact,

they are each diﬀerential Nash equilibrium and are locally stable for any choice of τ ∈ (0, ∞) as a result of Proposition 3. On the other hand, the critical point x∗ = (0, 0, 0, 0) is neither a diﬀerential Nash equilibrium

nor a diﬀerential Stackelberg equilibrium. However, x∗ is stable for τ ∈ (0, 2) and it is marginally stable for τ = 2. In general, convergence to the non-equilibrium critical point x∗ in the presence of multiple

game-theoretically meaningful equilibrium would be viewed as undesirable. In fact, this is precisely the

type of critical point that sophisticated schemes for converging to only diﬀerential Nash equilibria or only

diﬀerential Stackelberg equilibria seek to avoid (Adolphs et al., 2019; Fiez et al., 2020; Mazumdar et al.,

2019; Wang et al., 2020). We show in this example that the simple inclusion of timescale separation in

gradient descent-ascent is suﬃcient to avoid x∗ and instead converge to a diﬀerential Nash equilibrium.

Indeed, for all τ ∈ (2, ∞) the non-equilibrium critical point x∗ is unstable with respect to x˙ = −Λτ g(x).

We

simulate

τ -GDA

from

the

initial

condition

(x

0 1

,

x02

)

=

(−1.5, 2.5,

2.5, 3)

with

γ1

=

0.0005

and

τ

∈

{0.75, 2, 5, 12}, where we use the superscript to denote the time index so as not to be confused with the

multiple indexes for player choice variables. In Figures 5a and 5b, we show the trajectories of the players co-

ordinate pairs (x11, x21) and (x21, x22), respectively. We observe that τ -GDA converges to the non-equilibrium critical point x∗ with τ = 0.75 as expected and the dynamics move near it and then cycle around it with

τ = 2 since the critical point becomes marginally stable. However, for τ = 5 and τ = 12, τ -GDA avoids the

non-equilibrium critical point since it becomes unstable and instead the dynamics converge to the nearby

diﬀerential Nash equilibrium. We show the eigenvalues of the Jacobian at the non-equilibrium critical point

x∗ = (0, 0, 0, 0) in several forms in Figures 5d–5f. Again, we observe that the eigenvalues quickly become

purely real as τ grows and then they split, and asymptotically converge toward the eigenvalues of S1(J(x∗)) and −τ D22f (x∗). Together, this example demonstrates that often there is a reasonable ﬁnite learning rate

ratio such that non-meaningful critical points become unstable for τ -GDA.

6.3 Polynomial Game: Vector Field Warping and Region of Attraction

Consider a zero-sum game deﬁned by the cost

f (x1, x2) = −e−(0.01x21+0.01x22) (0.3x1 + x22)2 + (0.3x2 + x21)2 .

(22)

The cost structure of this game is visualized in Figure 6a, where we present a three dimensional view of

−f (x1, x2) along with the cost contours and the locations of critical points. This game has eleven critical

points including one diﬀerential Nash equilibrium and two diﬀerential Stackelberg equilibria that are not

a diﬀerential Nash equilibrium. The critical points that are neither a diﬀerential Nash equilibrium nor a

diﬀerential Stackelberg equilibrium are unstable for any choice of timescale separation τ . The diﬀerential

Nash equilibrium is at (x1, x2) = (10.57, −8.95) and it is stable for all τ ∈ (0, ∞) by Proposition 3. The

diﬀerential

Stackelberg

equilibria

are

at

(x1, x2)

=

(−1.625, −1.625)

and

(x

∗ 1

,

x∗2

)

=

(−11.03, −11.03);

each

is stable for all τ ∈ (1, ∞). We computed τ ∗ for the pair of diﬀerential Stackelberg equilibrium using the

theoretical construction from Theorem 3 and observed that it properly recovered τ ∗ = 1 for each equilibrium

as the timescale separation such that the continuous time system is stable for all τ ∈ (τ ∗, ∞). Finally, we

note that while the set of equilibrium follow a linear translation, this game is generic and the equilibria are

in fact isolated.

In Figure 6b, we show the trajectories of τ -GDA with γ1 = 0.0001 and τ ∈ {1, 2, 5, 20} given the ini-

tialization

(

x

0 1

,

x02

)

=

(−9, −9)

near

the

diﬀerential

Stackelberg

equilibrium

at

(

x

∗ 1

,

x∗2

)

=

(−11.03, −11.03).

Moreover, in Figure 7a, we overlay the trajectories on the vector ﬁeld generated by the respective timescale

separation parameters. As expected, the choice of τ = 1 results in a trajectory that cycles around the

equilibrium in a closed curve since it is marginally stable and Jτ (x∗) has purely imaginary eigenvalues. No-

tably, as τ grows, the cyclic behavior dissipates as the timescale separation reshapes the vector ﬁeld until

the trajectory moves near directly to the zero derivative line of the maximizing player and then follows a

26

(a) (b)

(c)

(d)

Figure 6: Experimental results for the polynomial game deﬁned in (22) of Section 6.3. Figures 6a provides a

3d view of the cost function −f (x1, x2) along with the cost contours and critical point locations. Figure 6b

shows trajectories of τ -GDA for a range of learning rate ratios given an initialization around the diﬀerential

Stackelberg

equilibrium

(x

∗ 1

,

x∗2

)

=

(−11.03, −11.03).

Figures

6c

and

6d

show

the

evolution

of

the

eigenvalues

from Jτ (x∗) as a function of τ where x∗ is the diﬀerential Stackelberg equilibrium (x∗1, x∗2) = (−11.03, −11.03).

path along that line toward the equilibrium and converges rapidly. The eigenvalues of Jτ (x∗) as a function of τ are presented in Figures 6c and 6d. As was the case for the previous experiments, we observe that after the eigenvalues become purely real as τ grows, they then split and asymptotically converge toward the eigenvalues of S1(J(x∗)) and −τ D22f (x∗). It is worth noting that much of the rotational behavior in the dynamics and vector ﬁeld disappears as a result of timescale separation well before the eigenvalues become purely real; this seems to occur after the timescale separation is such that the magnitude of the real part of the eigenvalues is greater than that of the imaginary part.
Finally, in Figure 7b, we demonstrate how the choice of timescale separation τ not only warps the vector ﬁeld but also shapes the regions of attraction around critical points. The vector ﬁeld is again shown for each τ ∈ {1, 2, 5, 20}, but now zoomed out to include each of the equilibria. The colors overlayed on the vector ﬁeld indicate the equilibria that the dynamics converge to given an initialization at that position. Positions in the strategy space without color did not converge to an equilibrium in the ﬁxed horizon of 75000 iterations with γ1 = 0.001. This is explained by the fact that the dynamics are not guaranteed to

27

(a)
(b)
Figure 7: Experimental results for the polynomial game deﬁned in (22) of Section 6.3. In Figure 7a, we overlay the trajectories from Figure 6b produced by τ -GDA onto the vector ﬁeld generated by the choice of timescale separation selection τ . The shading of the vector ﬁeld is dictated by its magnitude so that lighter shading corresponds to a higher magnitude and darker shading corresponds to a lower magnitude. Figure 7b demonstrates the eﬀect of timescale separation on the region of attractions around critical points by coloring points in the strategy space according to the equilibrium τ -GDA converges. We remark that areas without coloring indicate where τ -GDA did not converge in the time horizon.
be globally convergent and may get stuck in limit cycles or may simply move slowly for a long time in ﬂat regions of the optimization landscape. We produced this experiment by running τ -GDA for a dense set of initial conditions chosen uniformly over the space of interest. It is clear from the experiment that the choice of timescale separation determines not only the stability of equilibria, but also has a fundamental impact on the equilibria the dynamics converge to from a given initial condition as a result of the warping of the vector ﬁeld. As a concrete example, given an initialization of (x1, x2) = (−10, −2), the dynamics with τ = 1 converge to the diﬀerential Nash equilibria at (x1, x2) = (10.57, −8.95). However, for any τ > 1, the dynamics instead converge to the diﬀerential Stackelberg equilibrium at (x1, x2) = (−11.03, −11.03) that is signiﬁcantly closer to the initial condition. This example motivates future work on methods for obtaining accurate estimates of the regions of attraction around critical points and techniques to design τ in order to explicitly shape the region of attraction around an equilibrium of interest. We refer to the end of Section 4.1 for further discussion on potentially relevant analysis methods in this direction.
6.4 Dirac-GAN: Regularization, Timescale Separation, and Convergence Rate
In Section 5, we studied gradient descent-ascent with regularization in generative adversarial networks and showed that the general theory we provide can be extended to such a formulation. Recall that the training
28

(a)

(b)

(d)

(e) µ = 0.3

(f) µ = 1

Figure 8: Experimental results for the Dirac-GAN game deﬁned in (24) of Section 6.4. Figure 8a shows trajectories of τ -GDA for τ ∈ {1, 4, 8, 16} with regularization µ = 0.3 and τ = 1 with regularization µ = 1. Figure 8b shows the distance from the equilibrium along the learning paths. Figure 8f shows the trajectories
of τ -GDA overlayed on the vector ﬁeld generated by the respective timescale separation and regularization parameters. The shading of the vector ﬁeld is dictated by its magnitude so that lighter shading corresponds
to a higher magnitude and darker shading corresponds to a lower magnitude. Figures 8e and 8f show the trajectories of the eigenvalues for Jτ (θ∗, ω∗) as a function of τ with regularization set to µ = 0.3 and µ = 1, respectively where (θ∗, ω∗) is the unique critical point of the game.

objective for generative adversarial networks is of the form

f (θ, ω) = Ep(z)[ (D(G(z; θ); ω))] + EpD(x)[ (−D(x; ω))]

(23)

29

where Dω(x) and Gθ(z) are the discriminator and generator networks respectively, pD(x) is the data distribution while p(z) is the latent distribution, and ∈ C2(R) is some real-value function such that (0) = 0 and
(0) < 0. The goal of the generator is to minimize (23) while the discriminator seeks to maximize (23). As a motivating example, we mentioned the Dirac-GAN proposed by Mescheder et al. (2018), which constitutes an extremely simple, yet compelling generative adversarial network. Formally described in Deﬁnition 5, the Dirac-GAN consists of a univariate generator distribution pθ = δθ and a linear discriminator D(x; ω) = ωx, where the real data distribution pD is given by a Dirac-distribution concentrated at zero. The resulting zero-sum game is deﬁned by the cost
f (θ, ω) = (θω) + (0).
The unique critical point of gradient descent-ascent is a local Nash equilibrium given by (θ∗, ω∗) = (0, 0). However, the structure of the game is such that

Jτ (θ∗, ω∗) = −τ 0 (0)

(0) . 0

Consequently, spec(Jτ (θ∗, ω∗)) = {±i√τ (0)} so that spec(Jτ (θ∗, ω∗)) ⊂ C◦+ and regardless of the choice of timescale separation, τ -GDA oscillates and fails to converge to the equilibrium. This behavior is expected since
the equilibrium is not hyperbolic and corresponds to neither a diﬀerential Nash equilibrium nor a diﬀerential Stackelberg equilibrium since D12f (θ∗, ω∗) = 0 and −D22f (θ∗, ω∗) = 0, but it is undesirable nonetheless since (θ∗, ω∗) is the unique critical point and a local Nash equilibrium.

Mescheder et al. (2018) proposed to remedy the degeneracy issues of generative adversarial networks by

using the following gradient penalties to regularize the discriminator with µ > 0:

R1(θ, ω) = µ2 EpD(x)[ ∇xD(x; ω) 2] and R2(θ, ω) = µ2 Epθ(x)[ ∇xD(x; ω) 2].

For the Dirac-GAN,

R1(θ, ω) = R2(θ, ω) = µ ω2. 2

The zero-sum game corresponding to the Dirac-GAN with regularization can be deﬁned by the cost

f (θ, ω) = (θω) + (0) − µ ω2.

(24)

2

The unique critical point of the game remains at (θ∗, ω∗) = (0, 0), but we now see that

Jτ (θ∗, ω∗) = −τ 0 (0) τ(µ0) (25)

and spec(Jτ (θ∗, ω∗)) = {(τ µ ± τ 2µ2 − 4τ ( (0))2)/2}. Observe that for all τ ∈ (0, ∞) and µ ∈ (0, ∞), we get that spec(Jτ (θ∗, ω∗)) ⊂ C◦+. This implies that for all timescale separation parameters τ > 0 and all regularization parameters µ > 0, the local Nash equilibrium of the unregularized game is stable with respect to the dynamics x˙ = −Λτ g(x). As a result, for a suitably chosen learning rate γ1, the discrete time update τ -GDA converges to the equilibrium. It is worth pointing out that the critical point (θ∗, ω∗) = (0, 0) corresponds to a diﬀerential Stackelberg equilibrium of the regularized game since −D22f (θ∗, ω∗) = µ > 0 and S1(J (θ∗, ω∗)) = ( (0))2/µ > 0.
We now present experiments with τ -GDA for the regularized Dirac-GAN game deﬁned in (24) focused
on exploring the interplay between timescale separation, regularization, and convergence rate since the equilibrium is always stable for a positive regularization parameter. We let (t) = − log(1 + exp(−t)),
which corresponds to the choice made in the original generative adversarial network formulation proposed
by Goodfellow et al. (2014). Figure 8a shows trajectories of τ -GDA from the initial condition (θ0, ω0) = (1, 1) with learning rate γ1 = 0.01 for τ ∈ {1, 4, 8, 16} with regularization µ = 0.3 and τ = 1 with µ = 1. Moreover, Figure 8f shows the trajectories of τ -GDA overlayed on the vector ﬁeld generated by the respective timescale
separation and regularization parameters and Figure 8b shows the distance from the equilibrium along the learning paths. The choice of µ = 0.3 is arbitrary to a degree, but µ∗ = 1 is chosen since it corresponds to the critical regularization parameter such that spec(J(θ∗, ω∗)) ⊂ R+ for all µ > µ∗, meaning that the Jacobian

30

without timescale separation has purely real eigenvalues. Finally, Figures 8e and 8f show the trajectories of

the eigenvalues for Jτ (θ∗, ω∗) as a function of τ with regularization set to µ = 0.3 and µ = 1, respectively where (θ∗, ω∗) is the unique critical point of the game.

From Figures 8a and 8d, we observe that the impact of timescale separation with regularization µ = 0.3

is that the trajectory is not as oscillatory since it moves faster to the zero line of −D2f (θ, ω) and then

follows along that line until reaching the equilibrium. We further see from Figure 8b that with regularization

µ = 0.3, τ -GDA with τ = 8 converges faster to the equilibrium than τ -GDA with τ = 16, despite the fact

that the former exhibits some cyclic behavior in the dynamics while the latter does not. The eigenvalues of

the Jacobian with regularization µ = 0.3 presented in Figure 8e explains this behavior since the imaginary

parts are non-zero with τ = 8 and zero with τ = 16, but the eigenvalue with the minimum real part is

greater at τ = 8 than at τ = 16. This example highlights that a degree of oscillatory behavior in the

dynamics is not always harmful for convergence and it can even speed up the rate of convergence. Building

oﬀ of this, for regularization µ = 1 and timescale separation τ = 1, Figures 8a and 8b show that even

though τ -GDA follows a direct path toward the equilibrium and does not cycle since the eigenvalues of the

Jacobian are purely real, the trajectory converges slowly to the equilibrium. While not presented, we ran

experiments with τ ∈ {2, 4, 8, 16} with µ = 1 as well and timescale separation only made the convergence

rate worse. The eigenvalues of the Jacobian with each regularization parameter presented in Figures 8e

and 8f are able to explain this phenomenon. Indeed, for each regularization parameter, the eigenvalues split

after

becoming

purely

real

and

then

converge

toward

the

eigenvalues

of

S1(J (θ∗, ω∗))

and

−τ

D

2 2

f

(θ

∗

,

ω

∗

).

Since S1(J(θ∗, ω∗)) ∝ 1/µ and −τ D22f (θ∗, ω∗) ∝ τ µ, there is a trade-oﬀ between the choice of regularization

µ and the timescale separation τ on the conditioning of the Jacobian matrix. As shown in Figures 8e and 8f,

the minimum real part of the eigenvalues with µ = 0.3 is signiﬁcantly larger than with µ = 1 after suﬃcient

timescale separation, which makes the convergence rate faster. Together, this example demonstrates that

there may often be a delicate relationship between timescale separation, regularization, and convergence

rate, where after a certain threshold each parameter choice may inhibit the rate of convergence.

In Appendix L.2, we provide simulation results on the Dirac-GAN game using the non-saturating gener-

ative adversarial network objective proposed by Goodfellow et al. (2014). In this formulation, the game is

deﬁned by the costs (f1, f2) = (− (−ωθ) + (0), −( (ωθ) + (0))). While the non-saturating objective was

motivated by global considerations (avoiding vanishing gradients) rather than local considerations, it turns

out that it is locally equivalent in terms of the game Jacobian as the standard formulation for the Dirac-GAN.

As a result, the stability characteristics are identical and we draw equivalent conclusions from the experiment

regarding the behavior of gradient descent-ascent in the game. Finally, we note that in Appendix L.3 we

explore another simple generative adversarial network formulation using the Wasserstein cost function with

a linear generator and quadratic discriminator (each of arbitrary dimension) for the problem of learning a

covariance matrix. In that example, we draw analogous conclusions about the interplay between timescale

separation, regularization, and the rate of convergence.

6.5 Generative Adversarial Networks: Image Datasets
We now investigate the role timescale separation has on training generative adversarial networks parameterized by deep neural networks. The empirical beneﬁts of training with a timescale separation have been documented previously. For example, Heusel et al. (2017) showed on a number of image datasets that a timescale separation between the generator and discriminator improves generation performance as measured by the Frechet Inception Distance (FID). Since then a signiﬁcant number of papers have presented results training generative adversarial networks with timescale separation. Moreover, it is common in the literature for the discriminator to be updated multiple times between each update of the generator (Arjovsky et al., 2017). Indeed, it has been widely demonstrated that this heuristic improves the stability and convergence of the training process and locally it has a similar eﬀect as including a timescale separation between the generator and discriminator. The disadvantage of this approach is that the number of gradient calls per generator update increases and consequently the convergence is then slower in terms of wall clock time when a similar eﬀect could potentially be achieved by a learning rate separation between the generator and discriminator. We remark that it appears to be reasonably common for practitioners to ﬁx a shared learning rate for the generator and discriminator along with a pre-selected number of discriminator updates per generator update and not thoroughly investigate the impact timescale separation has on the training process.

31

(a) µ = 10
(b) µ = 1
Figure 9: CIFAR-10 FID scores with regularization parameter µ = 10 in Figure 9a and µ = 1 in Figure 9b.
The goal of our generative adversarial network experiments is to reinforce the importance of the timescale separation between the generator and the discriminator as a hyperparameter in the training process, demonstrate how it changes the behavior along the learning path, and show that it is compatible with a number of common training heuristics. This is to say that our goal is not necessarily to show state-of-the art performance, but rather to perform experiments that allow us to make insights relevant to the theory in this paper. We remark that our empirical work on training generative adversarial networks is distinct from and complimentary to that of Heusel et al. (2017) in several ways. The theory given by Heusel et al. (2017) only applies to stochastic stepsizes, however in the experiments they implemented constant step sizes. We train with mini-batches and decaying stepsizes, which does satisfy the theory we provide. Moreover, by and large, the experiments by Heusel et al. (2017) compare a ﬁxed learning rate ratio between the generator and discriminator to multiple ﬁxed shared learning rates for the generator and discriminator. In contrast, we ﬁx a learning rate for the generator and explore the behavior of the training process as the timescale parameter τ is swept over a given range.
We build our experiments based on the methods and implementations of Mescheder et al. (2018) and explore both the CIFAR-10 and CelebA image datasets. We train the generative adversarial networks with the non-saturating objective function and the R1 gradient penalty proposed by Mescheder et al. (2018) with regularization parameters µ ∈ {1, 10}. We note that the non-saturating objective results in a game that is not zero-sum, however it is commonly used in practice and under the realizable assumptions is it locally equivalent to the zero-sum objective as discussed at the end of Section 6.4. The network architectures for the generator and discriminator are both based on the ResNet architecture. The initial learning rate for the generator in all of our experiments is ﬁxed to be γ = 0.0001 and we decay the stepsizes so that at update k the learning rate of the generator is given by γ1,k = γ1/(1 + ν)k where ν = 0.005 and the learning rate of the discriminator is γ2,k = τ γ1,k. For each experiment the batch size is 64, the latent data is drawn from a standard normal distribution of dimension 256, and the resolution of the images is 32 × 32 × 3. Finally, as an optimizer, we run RMSprop with parameter α = 0.99. Again, the theory we provide does not strictly
32

(a) µ = 10
(b) µ = 1
Figure 10: CelebA FID scores with regularization parameter µ = 10 in Figure 9a and µ = 1 in Figure 9b.
apply to using RMSprop, but it is ubiquitous in practice for training generative adversarial networks and if the timescale separation is suﬃciently large so that the eigenvalues are purely real in the Jacobian then the theory we provide is applicable as remarked previously. We provide further details on the network and hyperparameters in Appendix L.4. A ﬁnal heuristic and hyperparameter that we explore in conjunction with the timescale separation τ is that of using an exponential moving average to produce the model that is evaluated. This means that at each update k, given that the parameters of the generator are given by x1,k, the moving average x¯k = x1,kβ +x¯1,k−1(1−β) is kept where β ∈ (0, 1). Experimental studies have shown that this heuristic can yield a signiﬁcant improvement in terms of both the inception score and the FID (Gidel et al., 2019a; Yazici et al., 2019). The success of this method is thought to be a result of dampening both rotational dynamics and the noise from the randomness in the mini-batches of data.
We run the training algorithm with the learning rate ratio τ belonging to the set {1, 2, 4, 8} and the regularization parameter µ belonging to the set {1, 10}. For each choice of τ and µ, we retain exponential moving averages of the generator parameters for β ∈ {0.99, 0.999, 0.9999}. The training process is repeated 3 times for each hyperparameter conﬁguration to rule out noise from random seeds and the performance is evaluated along the learning path at every 10,000 updates in terms of the FID score. We report the mean scores and the standard error of the mean over the repeated experiments. We run the experiments with µ = 1 for 150k mini-batch updates and the experiments with µ = 10 for 300k mini-batch updates.
The results for each dataset across the hyperparameter conﬁgurations are presented in numeric form in Figure 12. Figure 11 shows some generated samples selected at random for each dataset with the hyperparameter conﬁguration that performed best in terms of the FID score at the end of the training process. Figure 17 in Appendix L.4 has several more generated samples for each dataset selected at random. We now describe the key observations from the experiments for each dataset.
CIFAR-10. The FID scores along the learning path for CIFAR-10 with µ = 10 and µ = 1 are presented in Figures 9a and 9b, respectively. The corresponding scores in numeric form are given in Figures 12a, 12c, and
33

(a) CIFAR-10

(b) CelebA

Figure 11: CIFAR-10 and CelebA samples from generator at 300k iterations with β = 0.9999 and τ = 4.

τ \β

0.99

0.999

0.9999

1

29.39 ± 0.37 27.56 ± 0.34

26.29 ± 0.2

2

24.94 ± 0.25

23.4 ± 0.22

22.36 ± 0.23

4 24.14 ± 0.42 22.53 ± 0.23 21.62 ± 0.05

8

24.63 ± 0.28

23.35 ± 0.3

22.52 ± 0.27

(a) CIFAR-10 FID at 150k updates with µ = 10

τ \β

0.99

0.999

0.9999

1

7.42 ± 0.1 7.01 ± 0.09 7.07 ± 0.08

2 6.26 ± 0.06 6.04 ± 0.02 6.05 ± 0.01

4 6.34 ± 0.03 5.97 ± 0.03 5.93 ± 0.002

8 7.01 ± 0.09 6.69 ± 0.09 6.43 ± 0.04

(b) CelebA FID at 150k updates with µ = 10

τ \β

0.99

0.999

0.9999

1

28.1 ± 0.52 26.18 ± 0.54

24.6 ± 0.3

2

23.49 ± 0.49

22.0 ± 0.38

21.05 ± 0.38

4 22.35 ± 0.15 20.71 ± 0.06 19.49 ± 0.06

8

22.46 ± 0.37 21.36 ± 0.44

20.27 ± 0.4

(c) CIFAR-10 FID at 150k updates with µ = 1

τ \β

0.99

0.999

0.9999

1 7.22 ± 0.15 6.87 ± 0.17 7.01 ± 0.22 2 5.93 ± 0.12 5.69 ± 0.03 5.86 ± 0.04 4 5.8 ± 0.04 5.51 ± 0.04 5.59 ± 0.06 8 5.88 ± 0.05 5.68 ± 0.03 5.7 ± 0.05

(d) CelebA FID at 150k updates with µ = 1

τ \β

0.99

0.999

0.9999

1

26.1 ± 0.44 23.98 ± 0.39 22.4 ± 0.35

2

21.44 ± 0.5 20.15 ± 0.32 18.5 ± 0.31

4 20.67 ± 0.04 19.23 ± 0.11 17.72 ± 0.05

8

21.09 ± 0.33 19.81 ± 0.22 18.45 ± 0.25

(e) CIFAR-10 FID at 300k updates with µ = 1

τ \β

0.99

0.999

0.9999

1 5.93 ± 0.06 5.63 ± 0.01 5.72 ± 0.02 2 5.13 ± 0.06 4.88 ± 0.02 5.0 ± 0.01 4 5.03 ± 0.06 4.9 ± 0.03 4.95 ± 0.05 8 5.51 ± 0.11 5.27 ± 0.04 5.21 ± 0.05

(f) CelebA FID at 300k updates with µ = 1

Figure 12: FID Scores on CIFAR-10 and CelebA.

12e for µ = 10 at 150k iterations and µ = 1 at 150k and 300k iterations, respectively. To begin, we observe that the exponential moving average signiﬁcantly improves performance, and of the parameters considered, β = 0.9999 performed best. Relevant to this work, we observe that the performance gain from using an exponential moving average appears to be maximized when the ratio of learning rates is smallest. This may indicate that some of the dynamics in τ -GDA are dampened by timescale separation in this generative adversarial network experiment, similarly to as observed for the simpler experiments presented previously. Moreover, we that timescale separation also has a signiﬁcant impact on the FID score of the training process. Indeed, even selecting τ = 2 versus τ = 1 can yield an impressive performance gain. In this experiment for each regularization parameter, τ = 4 converges fastest, followed by τ = 8, then τ = 2, and ﬁnally τ = 1. Finally, observe that the performance with regularization µ = 1 is far superior to that with regularization µ = 10. Interestingly, the last pair of conclusions are in line with the insights drawn from the simple DiracGAN experiment in Section 6.4. In particular, timescale separation only speeds up to convergence until hitting a limiting value and there is a fundamental interplay between timescale separation, regularization, and convergence rate. This indicates that it may be possible to transfer some of the insights made on simple generative adversarial network formulations to the much more complex problem where players are parameterized by neural networks.

34

CelebA. The FID scores along the learning path for CIFAR-10 with µ = 10 and µ = 1 are presented in Figures 10a and 10b, respectively. The corresponding scores in numeric form are given in Figures 12b, 12d, and 12f for µ = 10 at 150k iterations and µ = 1 at 150k and 300k iterations, respectively. In this experiment we that while the exponential moving average helps performance, the gain is not as drastic as it was for CIFAR-10. However, timescale separation in combination with the regularization does have a major eﬀect on the the FID score of the training process in this experiment. For regularization µ = 10, the timescale parameters of τ = 2 and τ = 4 outperform τ = 1 and τ = 8 by a wide margin, again highlighting that timescale separation can speed up convergence until a certain point where it can potentially slow it down owing to the eﬀect on the conditioning of the problem locally. A similar trend can be observed with regularization µ = 1, but with τ = 8 performing closer to τ = 2 and τ = 4. We again observe in this experiment that for all timescale separation parameters, the performance is signiﬁcantly improved with regularization µ = 1 as compared with µ = 10. This once again highlights the importance of considering how this the hyperparameters of regularization and timescale interact and dictate the local convergence rates.
In summary, we took a well-performing method and implementation for training generative adversarial networks and demonstrated that timescale separation is an extremely important, and easy to implement, hyperparameter that is worth careful consideration since it can have a major impact on the convergence speed and ﬁnal performance of the training process.
7 Related Work
In this section, we provide a review of related work at the intersection machine learning and game theory, as well as connections to dynamical systems theory and control.
7.1 Machine Learning and Learning in Games
Given the extensive work on the topic of learning in games in machine learning that has gone on over the last several years, we cannot cover all of it and instead focus our attention on only the most relevant to this paper. We begin by reviewing solution concepts developed for the class of games under consideration and then discuss some learning dynamics studied in the literature beyond gradient descent-ascent. Following this, we delineate the related work studying gradient descent-ascent in non-convex, non-concave zero-sum games and ﬁnish by making note of the literature on non-convex, concave optimization.
Solution Concepts. Owing to the numerous applications in machine learning, a signiﬁcant portion of the modern work on learning in games has focused on the zero-sum formulation with non-convex, nonconcave cost functions. Most recently, Daskalakis et al. (2020) tout the importance and signiﬁcance of this class of games in a paper on the complexity of ﬁnding equilibria (in particular, in the constrained setting) in such games. Consequently, local solution concepts have been broadly adopted. Compared to the standard game-theoretic notions of equilibrium that characterize player’s incentive to deviate given the game and information structure, local equilibrium concepts restrict the deviation search space to a suitable local neighborhood. Following the standard game-theoretic viewpoint, a vast number of works in machine learning study the local Nash equilibrium concept and critical points satisfying gradient-based suﬃcient conditions for the equilibrium, which are often referred to as diﬀerential Nash equilibria (Ratliﬀ et al., 2013, 2014; Ratliﬀ et al., 2016). Based on the observation that in non-convex, non-concave zero-sum games the order of play is fundamental in the deﬁnition of the game, there has been a push toward considering local notions of the Stackelberg equilibrium concept, which is the usual game-theoretic equilibrium when there is an explicit order of play between players. In the zero-sum formulation, Stackelberg equilibrium are often referred to as minmax equilibria. Similar to as for the Nash equilibrium, gradient-based suﬃcient conditions for local minmax/Stackelberg equilibrium have been given (Daskalakis and Panageas, 2018; Fiez et al., 2020; Jin et al., 2020) and such critical points have been referred to as diﬀerential Stackelberg equilibria (Fiez et al., 2020). We remark that it has been shown that local/diﬀerential Nash equilibria are a subset of local/diﬀerential Stackelberg equilibria (Fiez et al., 2020; Jin et al., 2020). Following past works, we adopt the terminology of diﬀerential Nash equilibrium and diﬀerential Stackelberg equilibrium in this paper as the meaning of strict local Nash equilibrium and strict local minmax/Stackelberg equilibrium, respectively. Finally we mention
35

the proximal equilibria proposed by Farnia and Ozdaglar (2020), which we do not consider in this work, that depending on a regularization parameter can interpolate between the local Nash and local Stackelberg equilibrium notions.
Learning Dynamics. Given that the focus of this work is on gradient descent-ascent, we center our coverage of related work on papers analyzing its behavior. Nonetheless, we mention that a signiﬁcant number of learning dynamics for zero-sum games have been developed in the past few years, in some cases motivated by the shortcomings of gradient descent-ascent without timescale separation. The methods include optimistic and extra-gradient algorithms (Daskalakis et al., 2018; Gidel et al., 2019a; Mertikopoulos et al., 2019), negative momentum (Gidel et al., 2019b), gradient adjustments (Balduzzi et al., 2018; Letcher et al., 2019a; Mescheder et al., 2017), and opponent modeling methods (Foerster et al., 2018; Letcher et al., 2019b; Metz et al., 2017; Sch¨afer and Anandkumar, 2019; Zhang and Lesser, 2010), among others. While the aforementioned learning dynamics possess some desirable characteristics, they cannot guarantee that the set of stable critical points coincide with a set of local equilibria for the class of games under consideration. However, there have been a select few learning dynamics proposed that can guarantee the stable critical points coincide with either the set of diﬀerential Nash equilibria (Adolphs et al., 2019; Mazumdar et al., 2019) or the set of diﬀerential Stackelberg equilibria (Fiez et al., 2020; Wang et al., 2020)—eﬀectively solving the problem of guaranteeing local convergence to only a class of local equilibria. However, since each of the algorithms achieving the equilibrium stability guarantee require solving a linear equation in each update step, they are not eﬃcient and can potentially suﬀer from degeneracies along the learning path in applications such as generative adversarial networks. These practical shortcomings motivate either proving that existing learning dynamics using only ﬁrst-order gradient feedback achieve analogous theoretical guarantees or developing novel computationally eﬃcient learning dynamics that can match the theoretical guarantee of interest.
Gradient Descent-Ascent. Gradient descent-ascent has been studied extensively in non-convex, nonconcave zero-sum games since it is a natural analogue to gradient descent from optimization, is computationally eﬃcient, and has been shown to be eﬀective in practice for applications of interest when combined with common heuristics. A prevailing approach toward gaining understanding of the convergence characteristics of gradient descent-ascent has been to analyze the local stability around critical points of the continuous time limiting dynamical system. The majority of this work has not considered the impact of timescale separation. Numerous papers have pointed out that the stable critical points of gradient descent-ascent without timescale separation may not be game-theoretically meaningful. In particular, it has been shown that there can exist stable critical points that are not diﬀerential Nash equilibrium (Daskalakis and Panageas, 2018; Mazumdar et al., 2020). Furthermore, it is known that there can exist stable critical points that are not diﬀerential Stackelberg equilibria (Jin et al., 2020). The aforementioned results rule out the possibility that gradient descent-ascent without timescale separation can guarantee equilibrium convergence. In terms of the stability of equilibria, it is known that diﬀerential Nash equilibrium are stable for gradient descent-ascent without timescale separation (Daskalakis and Panageas, 2018; Mazumdar et al., 2020), but that there can exist diﬀerential Stackelberg equilibria which are not stable with respect to gradient descent-ascent without timescale separation.
The work of Jin et al. (2020) is the most relevant exploring how the aforementioned stability properties of gradient descent-ascent change with timescale separation. In particular, Jin et al. (2020) investigate whether the desirable stability characteristics (stability of diﬀerential Nash equilibria) and undesirable stability characteristics (stability of non-equilibrium critical points and instability of diﬀerential Stackelberg equilibria) of gradient descent without timescale separation are maintained and remedied, respectively with timescale separation. In terms of the former query, extending the examples shown in Mazumdar et al. (2020) and Daskalakis and Panageas (2018), Jin et al. (2020) show that diﬀerential Nash equilibrium are stable for gradient descent-ascent with any amount of timescale separation.
On the other hand, for the latter query, Jin et al. (2020) shows (in Proposition 27) two interesting examples: (a) for an a priori ﬁxed τ , there exists a game with a diﬀerential Stackelberg equilibrium that is not stable and (b) for an a priori ﬁxed τ , there exists a game with a stable critical point that is not a diﬀerential Stackelberg equilibrium. However, (a) does not imply that for the constructed game, there does not exist another (ﬁnite) τ —independent of the game parameters—such the diﬀerential Stackelebrg equilibrium is stable for all larger τ . In simple language, the result summarized in (a) says the following:
36

if a bad timescale separation is chosen, then convergence may not be guaranteed. Similarly, (b) does not imply that there is no τ such that for all larger τ for the constructed game instance, the critical point becomes unstable. Again, in simple language, the result summarized in (b) says the following: if a bad timescale separation is chosen, then non-game theoretically meaningful equilibria may persist. While at ﬁrst glance this set of results may appear to indicate that the undesirable stability characteristics of gradient descent without timescale separation cannot be averted by any ﬁnite timescale separation, it is important to emphasize that these results do not answer the questions of whether there (a) exists a game with a critical point that is not a diﬀerential Stackelberg equilibrium which is stable with respect to gradient descent-ascent without timescale separation and remains stable for all ﬁnite timescale separation ratios or (b) exists a game with a diﬀerential Stackelberg equilibrium that is not stable for all ﬁnite timescale separation ratios. The preceding questions are left open from previous work and are exactly the focus of this paper. In Appendix K, we go into greater detail on the comparison between Proposition 27 of Jin et al. (2020) as we believe this to be an important point of distinction between Theorem 3 and 4 in this paper.
Finally, Jin et al. (2020) study the an inﬁnite timescale separation ratio and show that the stable critical points of gradient descent-ascent coincide with the set of diﬀerential Stackelberg equilibria in this regime. This result eﬀectively shows that gradient descent-ascent can guarantee only equilibrium convergence with timescale separation, albeit inﬁnite. We remark that an equivalent result in the context of general singularly perturbed systems has been known in the literature (Kokotovic et al., 1986, Chap. 2) as we discuss further in Section 3.1. Finally, we point out that since an inﬁnite timescale separation does not result in an implementable algorithm, fully understanding the behavior with a ﬁnite timescale separation is of fundamental importance and the motivation for our work.
Beyond the work of Jin et al. (2020) considering timescale separation in gradient descent-ascent, it is worth mentioning the work of Chasnov et al. (2019) and Heusel et al. (2017). Chasnov et al. (2019) study the impact of timescale separation on gradient descent-ascent, but focus on the convergence rate as a function of it given an initialize around a diﬀerential Nash equilibrium and do not consider the stability questions examined in this paper. Heusel et al. (2017) study stochastic gradient descent-ascent with timescale separation and invoke the results of Borkar (2008) for analysis. The stochastic approximation results the claims rely on guarantee the convergence of the system locally to a stable critical point. Consequently, the claim of convergence to diﬀerential Nash equilibria of stochastic gradient descent-ascent given by Heusel et al. (2017) only holds given an initialization in a local neighborhood around a diﬀerential Nash equilibrium. In this regard, the issue of the local stability of the types of critical point is eﬀectively assumed away and not considered. In contrast, we are able to combine our stability results for gradient descent-ascent with timescale separation together with the stochastic approximation theory of Borkar (2008) to guarantee local convergence to a diﬀerential Stackelberg equilibrium in Section 4.2. We remark that Heusel et al. (2017) empirically demonstrate that timescale separation can signiﬁcantly improve the performance of gradient descent-ascent when training generative adversarial networks.
The ﬁnal relevant line of work studying gradient descent-ascent is speciﬁc to generative adversarial networks. The results from this literature develop assumptions relevant to generative adversarial networks and then analyze the stability and convergence properties of gradient descent-ascent under them (see, e.g., works by Daskalakis et al. (2018); Goodfellow et al. (2014); Mescheder et al. (2018); Metz et al. (2017); Nagarajan and Kolter (2017)). Within this body of work, there has been a signiﬁcant amount of eﬀort focusing on how the stability (and, hence, convergence properties) of gradient descent-ascent in generative adversarial networks can be enhanced with regularization methods. Nagarajan and Kolter (2017) show, under suitable assumptions, that gradient-based methods for training generative adversarial networks are locally convergent assuming the data distributions are absolutely continuous. However, as observed by Mescheder et al. (2018), such assumptions not only may not be satisﬁed by many practical generative adversarial network training scenarios such as natural images, but it can often be the case that the data distribution is concentrated on a lower dimensional manifold. The latter characteristic leads to nearly purely imaginary eigenvalues and highly ill-condition problems. Mescheder et al. (2018) provide an explanation for observed instabilities consequent of the true data distribution being concentrated on a lower dimensional manifold using discriminator gradients orthogonal to the tangent space of the data manifold. Further, the authors introduce regularization via gradient penalties that leads to convergence guarantees under less restrictive assumptions than were previously known. In this paper, we further extend these results to show that convergence to diﬀerential Stackelberg equilibria is guaranteed under a wide array of hyperparameter
37

conﬁgurations (i.e., learning rate ratio and regularization).

Nonconvex-Concave Optimization. A ﬁnal related line of work is on nonconvex-concave optimization (Lin et al., 2020a,b; Lu et al., 2020; Nouiehed et al., 2019; Ostrovskii et al., 2020; Raﬁque et al., 2018). The focus in this set of works (among many others on the topic) is on characterizing the iteration complexity to stationary points, rather than stability and asymptotic convergence as in the non-convex, non-concave zero-sum game setting. The focus on stationary points in this body of work is reasonable since, to our knowledge, the results obtained are for -weakly convex-concave games, a subclass of non-convex-concave games in which the minimizing player faces an -weakly convex optimization problem for each ﬁxed choice of the maximizing player. The primary relevance of work on this problem is that a number of the algorithms rely on timescale separation and variations of gradient descent-ascent. Moreover, the methods for obtaining fast convergence rates may be relevant to future work attempting to characterize fast rates in the non-convex, non-concave setting after there is a more fundamental understanding of the stability and asymptotic convergence.

7.2 Historical Perspective: Dynamical Systems and Control
The study of gradient descent-ascent dynamics with timescale separation between the minimizing and maximizing players is closely related to that of singularly perturbed dynamical systems (Kokotovic et al., 1986). Such systems arise in classical control and dynamical systems in the context of physical systems that either have multiple states which evolve on diﬀerent timescales due to some underlying immutable physical process or property, or a single dynamical system which evolves on a sub-manifold of the larger state-space. For example, robot manipulators or end eﬀectors often have have slower mechanical dynamics than electrical dynamics. On the other hand, in electrical circuits or mechanical systems, certain resistor-capacitor circuits or spring-mass systems have a state which evolves subject to a constraint equation (Lagerstrom and Casten, 1972; Sastry and Desoer, 1981). Due to their prevalence, singularly perturbed systems have been studied extensively with one of the outcomes being a number of works on determining the range of perturbation parameters for which the overall system is stable (Kokotovic et al., 1986; Saydy, 1996; Saydy et al., 1990). We exploit these results and analysis techniques to develop novel results for learning in games. One of contributions of this work is the introduction of the algebraic analysis techniques to the machine learning and game theory communities. These tools open up new avenues for algorithm synthesis; we comment on potential directions in the concluding discussion section.
This being said, there are a couple key diﬀerence between the present setting and that of the classical literature including the following:
1. The perturbation parameter is no longer an immutable characteristic of the physical system, but rather a hyperparameter subject to design. Indeed, in singular perturbation theory, the typical dynamical system studied takes the form

x˙ = g1(x, y) y˙ = g2(x, y)

(26)

where is a small parameter that abstracts some physical characteristics of the state variables. On the
other hand, in learning in games, the continuous time limiting dynamical system of gradient descentascent for a zero-sum game deﬁned by f ∈ C2(X × Y, R) takes the form

x˙ = −D1f (x, y) y˙ = τ D2f (x, y)

(27)

where the x–player seeks to minimize f with respect to x and the y–player seeks to maximize f with respect to y, and τ is the ratio of learning rates (without loss of generality) of the maximizing to the minimizing player. These learning rates—and hence the value of τ —are hyperparameters subject to design in most machine learning and optimization applications. Another feature of (27) as compared to (26), is that the dynamics Dif are partial derivatives of a function f , which leads to the second key diﬀerence.
2. There is structure in the dynamical system that arises from gradient-play which reﬂects the underlying game theoretic interactions between players. This structure can be exploited in obtaining convergence guarantees in machine learning and optimization applications of game theory.

38

For instance, minmax optimization is analogous to a zero sum game for which the local linearization of gradient descent-ascent dynamics has the structure

A J = −τ B

B −τ C

where A = A and C = C and τ is the learning rate ratio or timescale separation parameter. Such block matrices have very interesting properties. In particular, second order optimality conditions for a minmax equilibrium correspond to positive deﬁniteness of the ﬁrst Schur complement S1(J) = A − BC−1B > 0, and of −C > 0 (Fiez et al., 2020). This turns out to be keenly important for understanding convergence of gradient descent-ascent. Furthermore, due to the structure of J, tools from the theory of block operators (see, e.g., works by Lancaster and Tismenetsky (1985); Magnus (1988); Tretter (2008)) such as the quadratic numerical range can be exploited (and combined with singular perturbation theory) to understand the eﬀects of hyperparameters such as τ (the learning rate ratio) and regularization (which is common in applications such as generative adversarial networks) on convergence.

8 Discussion
In this paper, we prove a necessary and suﬃcient condition for the convergence of gradient descent-ascent with timescale separation to diﬀerential Stackelberg equilibria in zero sum games. This answers a long standing open question about provable convergence of ﬁrst order methods for zero-sum games to local minimax equilibria. Speciﬁcally, we provide necessary and suﬃcient conditions for the convergence of τ GDA to diﬀerential Stackelberg equilibria. A key component of the proof is the construction of a (tight) ﬁnite lower bound on the learning rate ratio τ for which stability of the game Jacobian is guaranteed, and hence local asymptotic convergence of τ -GDA. In addition, we provide results on iteration complexity and convergence rate and apply the results to generative adversarial networks under mild assumptions on the data distribution. For both diﬀerential Nash equilibira and the superset of diﬀerential Stackelberg equilibria, we provide estimates on the neighborhood on which convergence is guaranteed.
This being said, the question of the size of the region of attraction remains open. As commented on earlier in the paper, an alternative but related technique tackles the nonlinear system directly. The downside of this technique is that one needs to have in hand (or be able to construct) Lyapunov functions for both the boundary layer model (i.e., the system that arises from treating the choice variable of the slow player as being ‘static’) and the reduced order model (i.e., the system that arises from plugging in the implicit mapping from the fast player’s action to the slow player’s action into the slow player’s dynamics). A convex combination of these functions provides a Lyapunov function for the original system x˙ = −Λτ g(x). The level sets of this combined Lyapunov function then give a better sense of the region of attraction and, in fact, one can optimize over the weighting in the convex combination in order to obtain better estimates of the region of attraction. This is an interesting avenue to explore in the context of learning in games with lots of intrinsic structure that can potentially be exploited to improve both the rate of convergence and the region on which convergence is guaranteed.
Another signiﬁcant contribution of this work is the fact that we introduce tools that are arguably new to the machine learning and optimization communities and expose interesting new directions of research. In particular, the notion of a guard map, which is arguably even an obscure tool in modern control and dynamical systems theory, is ‘rediscovered’ in this paper. The is potential to leverage this concept in not only providing certiﬁcates for performance (e.g., beyond stability to robustness) but also in synthesizing algorithms with performance guarantees. For instance, one observation from our empirical analysis is that convergence rate is not only limited by the eigenvalues of the Schur complement of the Jacobian, but the fastest convergence appears to occur when there are complex components of the eigenvalues. In short, some cycling is beneﬁcial. Better understanding this fact from a theoretical perspective is an open question, as is optimizing the rate of convergence by exploiting these observations.
Finally, another set of related open questions center on practical considerations for the eﬃcient use of ﬁrst order methods. For instance, with respect to generative adversarial networks, the exponential moving average is known to empirically reduce the negative eﬀects of cycling. Additionally, increasing the learning

39

rate ratio does lead to predominantly real eigenvalues which in turn reduces cycling. Understanding the trade oﬀs between not only these two hyperparameters but also regularization is very important for practical implementations. Empirically, we study the tradeoﬀs between the learning rate ratio, regularization parameter, and the parameter controlling the degree of “smoothness” in the exponential moving average, another common heuristic that performs well in practice. There is an open line of research related to analytically characterizing the tradeoﬀs between these three hyperparameters. However, in the absence of theoretical tools for exploring these issues, what are reasonable and principled heuristics?
To conclude, while we arguably deﬁnitively address a standing open question for ﬁrst order methods for learning in zero-sum games/minmax optimization problems, there a many open directions exposed by the tools introduced and empirical observations discovered in this work.
Acknowledgements
This work is funded by the Oﬃce of Naval Research (YIP Award) and National Science Foundation CAREER Award (CNS-1844729). Tanner Fiez is also funded by a National Defense Science and Engineering Graduate Fellowship. We thank Daniel Calderone for the helpful discussions, in particular on linear algebra results as they pertain to the results in this paper. Finally, we thank Mescheder et al. (2018) for providing a high quality open source implementation of the generative adversarial network experiments they performed, which facilitated and expedited the experiments we performed.
References
EH Abed, L Saydy, and AL Tits. Generalized stability of linear singularly perturbed systems including calculation of maximal parameter range. In Robust Control of Linear Systems and Nonlinear Control, pages 197–203. Springer, 1990.
Ralph Abraham, Jerrold E Marsden, and Tudor Ratiu. Manifolds, tensor analysis, and applications, volume 75. Springer Science & Business Media, 2012.
Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point optimization: A curvature exploitation approach. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 486–495, 2019.
John M Alongi and Gail Susan Nelson. Recurrence and topology, volume 85. American Mathematical Soc., 2007.
I. K. Argyros. A generalization of ostrowski’s theorem on ﬁxed points. Applied Mathematics Letters, 12: 77–79, 1999.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning (ICML), pages 214–223, 2017.
D Balduzzi, S Racaniere, J Martens, J Foerster, K Tuyls, and T Graepel. The mechanics of n-player diﬀerentiable games. In International Conference on Machine Learning (ICML), volume 80, pages 363– 372, 2018.
Tamer Ba¸sar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
Michel Benaim. A Dynamical System Approach to Stochastic Approximations. SIAM Journal on Control and Optimization, 34(2):437–472, 1996.
Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A Closer Look at the Optimization Landscapes of Generative Adversarial Networks. In International Conference on Learning Representations (ICLR), 2020.
Vivek Borkar. Stochastic Approximation: A Dynamical Systems Approach. Springer, 2008.
40

HW Broer and F Takens. Preliminaries of dynamical systems theory. Handbook of Dynamical Systems, 3: 1–42, 2010.
Frank M Callier and Charles A Desoer. Linear system theory. Springer Science & Business Media, 2012.
Benjamin Chasnov, Lillian J. Ratliﬀ, Eric Mazumdar, and Samuel A. Burden. Convergence analysis of gradient-based learning in continuous games. Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2019.
Ashish Cherukuri, Bahman Gharesifard, and Jorge Cortes. Saddle-point dynamics: conditions for asymptotic stability of saddle points. SIAM Journal on Control and Optimization, 55(1):486–511, 2017.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In Advances in Neural Information Processing Systems, pages 9236–9246, 2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. International Conference on Learning Representations (ICLR), 2018.
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained minmax optimization. arXiv preprint arXiv:2009.09623, 2020.
Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria? International Conference on Machine Learning (ICML), 2020.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliﬀ. Convergence of Learning Dynamics in Stackelberg Games. arXiv:1906.01217 (a version of this work appeared in ICML 2020), 2019.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliﬀ. Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study. International Conference on Machine Learning (ICML), 2020.
Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In International Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 122–130, 2018.
Drew Fudenberg, Fudenberg Drew, David K Levine, and David K Levine. The theory of learning in games, volume 2. MIT press, 1998.
Gauthier Gidel, Hugo Berard, Ga¨etan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In International Conference on Learning Representations (ICLR), 2019a.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R´emi Le Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1802–1811, 2019b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NeurIPS), pages 2672–2680, 2014.
Willy J. F. Govaerts. Numerical Methods for Bifurcations of Dynamical Equilibria. Society for Industrial and Applied Mathematics, 2000.
Joao P Hespanha. Linear Systems Theory. Princeton University Press, 2nd edition, 2018.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems (NeurIPS), pages 6626–6637, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural information processing systems, pages 4565–4573, 2016.
41

Roger A Horn and Charles R Johnson. Matrix Analysis. Cambridge University Press, 1985.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? International Conference on Machine Learning (ICML, 2020.
Sameer Kamal. On the convergence, lock-in probability, and sample complexity of stochastic approximation. SIAM Journal on Control and Optimization, 48(8):5178–5192, 2010.
Hassan Khalil. Nonlinear Systems. Prentice Hall, 3rd edition, 2002.
Peter V Kokotovic, John O’Reilly, and Hassan K Khalil. Singular Perturbation Methods in Control: Analysis and Design. Academic Press, Inc., 1986.
NN Krasovskii. Stability of Motion: Application of Lyapunov’s Second Method to Diﬀerential Systems and Equations with Time-Delay, 1963.
PA Lagerstrom and RG Casten. Basic concepts underlying singular perturbation techniques. Siam Review, 14(1):63–120, 1972.
Peter Lancaster and Miron Tismenetsky. The theory of matrices: with applications. Elsevier, 1985.
Alistair Letcher, David Balduzzi, S´ebastien Racaniere, James Martens, Jakob N Foerster, Karl Tuyls, and Thore Graepel. Diﬀerentiable game mechanics. Journal of Machine Learning Research (JMLR), 20:84–1, 2019a.
Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rockt¨aschel, and Shimon Whiteson. Stable opponent shaping in diﬀerentiable games. International Conference on Learning Representations (ICLR), 2019b.
Tianyi Lin, Chi Jin, Michael Jordan, et al. Near-optimal algorithms for minimax optimization. Conference on Learning Theory (COLT), 2020a.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave minimax problems. International Conference on Machine Learning (ICML), 2020b.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit diﬀerentiation. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1540–1552, 2020.
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications. IEEE Transactions on Signal Processing, 2020.
Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse. Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions. International Conference on Learning Representations (ICLR), 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
Jan Magnus. Linear Structures. Griﬃn, 1988.
E. Mazumdar and L. J. Ratliﬀ. Local Nash Equilibria are Isolated, Strict Local Nash Equilibria in ‘Almost All’ Zero-Sum Continuous Games. In Proceedings of the 58th IEEE Conference on Decision and Control, pages 6899–6904, 2019.
Eric Mazumdar, Lillian J Ratliﬀ, and S Shankar Sastry. On Gradient-Based Learning in Continuous Games. SIAM Journal on Mathematics of Data Science, 2(1):103–131, 2020.
Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On ﬁnding local nash equilibria (and only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.
42

Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In International Conference on Learning Representations (ICLR), 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems (NeurIPS), pages 1825–1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International Conference on Machine learning (ICML), pages 3481–3490, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. International Conference on Learning Representations (ICLR), 2017.
D Mustafa and TN Davidson. Generalized integral controllability. In Proceedings of 1994 33rd IEEE Conference on Decision and Control, volume 1, pages 898–903. IEEE, 1994.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems (NeurIPS), pages 5585–5595, 2017.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative ﬁrst order methods. In Advances in Neural Information Processing Systems, pages 14934–14942, 2019.
J. M. Ortega and W. C. Rheinboldt. Iterative Solutions to Nonlinear Equations in Several Variables. Academic Press, 1970.
Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Eﬃcient search of ﬁrst-order nash equilibria in nonconvex-concave smooth min-max problems. arXiv preprint arXiv:2002.07919, 2020.
Christos Papadimitriou and Georgios Piliouras. Game dynamics as the meaning of a game. SIGecom Exch., 16(2):53–63, May 2019. doi: 10.1145/3331041.3331048.
Hassan Raﬁque, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization: Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model based reinforcement learning. International Conference on Machine Learning (ICML), 2020.
L. J. Ratliﬀ, S. A. Burden, and S. S. Sastry. Characterization and computation of local Nash equilibria in continuous games. In Allerton Conference on Communication, Control, and Computing, pages 917–924, 2013.
L. J. Ratliﬀ, S. A. Burden, and S. S. Sastry. Genericity and structural stability of non-degenerate diﬀerential Nash equilibria. In American Control Conference (ACC), pages 3990–3995, 2014.
Lillian J Ratliﬀ, Samuel A Burden, and S Shankar Sastry. On the characterization of local Nash equilibria in continuous games. IEEE Transactions on Automatic Control, 61(8):2301–2307, 2016.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems (NeurIPS), pages 2018–2028, 2017.
S. Shankar Sastry. Nonlinear Systems Theory. Springer-Verlag, 1999.
Shankar Sastry and C Desoer. Jump behavior of circuits and systems. IEEE Transactions on Circuits and Systems, 28(12):1109–1124, 1981.
Lahcen Saydy. New stability/performance results for singularly perturbed systems. Automatica, 32(6):807 – 818, 1996.
43

Lahcen Saydy, Andr´e L Tits, and Eyad H Abed. Guardian maps and the generalized stability of parametrized families of matrices and polynomials. Mathematics of Control, Signals and Systems, 3(4):345–371, 1990.
Florian Sch¨afer and Anima Anandkumar. Competitive gradient descent. In Advances in Neural Information Processing Systems (NeurIPS), pages 7625–7635, 2019.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with principled adversarial training. International Conference on Learning Representations (ICLR), 2018.
Gerald Teschl. Ordinary diﬀerential equations and dynamical systems. Graduate Studies in Mathematics, 140:08854–8019, 2000.
Gugan Thoppe and Vivek Borkar. A concentration bound for stochastic approximation via alekseev’s formula. Stochastic Systems, 9(1):1–26, 2019.
Christiane Tretter. Spectral theory of block operator matrices and applications. World Scientiﬁc, 2008.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A follow-theridge approach. International Conference on Learning Representations (ICLR, 2020.
Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay Chandrasekhar. The unusual eﬀectiveness of averaging in gan training. International Conference on Learning Representations (ICLR), 2019.
Chongjie Zhang and Victor R Lesser. Multi-agent learning with policy prediction. In AAAI, volume 3, page 8, 2010.
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.

Appendices

Table of Contents

A Helper Lemmas and Additional Mathematical Preliminaries

45

B Proof of Proposition 3

46

C Proof of Lemma 1 and Lemma 2

47

D Proof of Corollary 3

49

E Proof of Theorem 3 and Corollary 2

49

F Proof of Proposition 4

54

G Proof of Theorem 4

58

H Proof of Theorem 7

59

I Proof of Proposition 5

60

J Extensions in the Stochastic Setting

61

K Further Details on Related Work

62

44

L Experiments Supplement

63

A Helper Lemmas and Additional Mathematical Preliminaries

In this appendix, we present a handful of technical lemmas and review some additional mathematical preliminaries excluded from the main body but which are important in proving the results in the paper.
The following technical lemma is used in proving an upper bound on the spectral radius of the linearization of the discrete time update τ -GDA a requirement for obtaining the convergence rate results.

Lemma

3.

The

function

c(z) = (1 − z)1/2 +

z 4

− (1 −

z2 )1/2

satisﬁes

c(x) ≤ 0

for

all

z

∈ [0, 1].

Proof.

Since

c(0)

=

0

and

c(1)

=

1 4

−

√1 2

≤

0,

we

simply

need

to

show

that

c (z)

≤

0

on

(0, 1)

to

get

that

c(z)

is

a

decreasing

function

on

[0, 1],

and

hence

negative

on

[0, 1].

Indeed,

c

(z)

=

1 4

+

√1 2 4−2z

−

√1 2 1−z

≤

0

since (1 − z)−1/2 − (4 − 2z)−1/2 ≥ 1/2 for all z ∈ (0, 1).

The following proposition is a well-known result in numerical analysis and can be found in a number of books and papers on the subject. Essentially, it provides an asymptotic convergence guarantee for a discrete time update process or dynamical system.

Proposition 6 (Ostrowski’s Theorem (Argyros, 1999); Theorem 10.1.2 (Ortega and Rheinboldt, 1970)).
Let x∗ be a ﬁxed point for the discrete dynamical system xk+1 = F (xk). If the spectral radius of the Jacobian satisﬁes ρ(DF (x∗)) < 1, then F is a contraction at x∗ and hence, x∗ is asymptotically stable.

The following technical lemma, due to Mustafa and Davidson (1994), is used in constructing the ﬁnite learning rate ratio.
Lemma 4 ((Mustafa and Davidson, 1994, Lem. 15)). Let V, Z ∈ Rp×p, W ∈ Rp×q and Y ∈ Rq×q. If V and Y − XV −1W are non-singular, then

det V + Z W XY

= det(V ) det(Y − XV −1W ) det(I + V −1(I + W (Y − XV −1W )−1XV −1)Z)

For completeness (and because there is a typo in the original manuscript), we provide the proof here.

Proof. Suppose that V and Y − XV −1W are non-singular so that the partial Schur decomposition

VW

V

0

X Y = X Y − XV −1W

I V −1W 0I

holds, and

det V W = det(V ) det(Y − XV −1W ).

(28)

XY

Further,

V W −1 I −V −1W X Y =0 I

V −1 −(Y − XV −1W )−1XV −1

0 (Y − XV −1W )−1 .

Applying the determinant operator, we have that

V +Z W

VW

I0

V W −1 Z 0

det X Y = det X Y det 0 I + X Y 0 0 (29)

so that

I0

V W −1 Z 0

V −1(I + W (Y − XV −1W )−1XV −1)Z + I 0

det 0 I + X Y

0 0 = det

−(Y − XV −1W )V −1Z

I (30)

= det(V −1(I + W (Y − XV −1W )−1XV −1)Z + I).

(31)

Combining (28) with (31) in (29) gives exactly the claimed result.

45

The following lemma is Theorem 2 Lancaster and Tismenetsky (1985, Chap. 13.1). We use this lemma several times in the proofs of Theorem 3 and 4 so we include it here for ease of reference. For a given matrix A, υ+(A), υ−(A), and ζ(A) are the number of eigenvalues of the argument that have positive, negative and zero real parts, respectively.
Lemma 5. Consider a matrix A ∈ Rn×n.
(a) If P is a symmetric matrix such that AP + P A = Q where Q = Q > 0, then P is nonsingular and P and A have the same inertia—i.e.,

υ+(A) = υ+(P ), υ−(A) = υ−(P ), ζ(A) = ζ(P ).

(32)

(b) On the other hand, if ζ(A) = 0, then there exists a matrix P = P and a matrix Q = Q > 0 such that AP + P A = Q and P and A have the same inertia (i.e., (32) holds).

Numerical and Quadratic Numerical Range. The numerical range and quadratic numerical range of

a block operator matrix are particularly useful for proving results about the spectrum of a block operator matrix as they are supersets of the spectrum (Tretter, 2008). Given a matrix A ∈ Rn×n, the numerical range is deﬁned by
W(A) = {z ∈ Cn : Az, z , z = 1},

and is a convex subset of C. Deﬁne spaces Wi = {z ∈ Cni : z = 1} for each i ∈ {1, 2}. Consider a block

operator

A = A11 A21

A12 , A22

where Aii ∈ Rni×ni and Aij ∈ Rni×nj for each i, j ∈ {1, 2}. Given v ∈ W1 and w ∈ W2, let Av,w ∈ C2×2 be

deﬁned by

Av,w =

A11v, v A21v, w

A12w, v . A22w, w

The quadratic numerical range of A is deﬁned by

W2(A) =

spec(Av,w )

v ∈W1 ,w∈W2

where spec(·) denotes the spectrum of its argument. The quadratic numerical range can be described as the set of solutions of the characteristic polynomial

λ2 − λ( A11v, v + A22w, w ) + A11v, v A22w, w − A12v, w A21w, v = 0

(33)

for v ∈ W1 and w ∈ W2. We use the notation Av, w = v¯ Aw to denote the inner product. Note that W2(A) is a (potentially non-convex) subset of W(A) and contains spec(A).

B Proof of Proposition 3
Before proving this result, we note that the result has already been shown in the literature by Jin et al. (2020). We included the result primarily because the proof approach is diﬀerent and the tools we use (in particular, the quadratic numerical range) have not been utilized before in this type of analysis. Hence, we view the proof technique itself as a contribution.

46

Proof of Proposition 3 We leverage the quadratic numerical range to show that spec(Jτ (x∗)) ⊂ C◦+ for any τ ∈ (0, ∞). Indeed, the quadratic numerical range of a block operator matrix contains its spec-

trum (Tretter, 2008).

Recall that

W2(Jτ (x∗)) =

spec(Jτv,w (x∗ ))

v ∈W1 ,w∈W2

where and Wi = {z ∈ Cni :

Jτv,w(x∗) =

D

2 1

f

(x

∗

)v

,

v

−τ D12f (x∗)v, w

D12f (x∗)w, v

−τ

D

2 2

f

(x

∗

)w

,

w

z = 1} for each i = 1, 2. Fix v ∈ W1 and w ∈ W2 and consider

Jτv,w(x∗) =

a −τ ¯b

b τd

Then, the elements of W2(Jτ (x∗)) are of the form

λτ

=

1 2

(a

+

τ

d)

±

1 2

(a − τ d)2 − 4τ |b|2

where a = D12f (x∗)v, v , b = D12f (x∗)w, v and d = −D22f (x∗)w, w for vectors v ∈ W1 and w ∈ W2. We claim that for any τ ∈ (0, ∞), Re(λτ ) > 0 for all a,b, and d where a > 0 and d > 0 since x∗ is a
diﬀerential Nash equilibrium. Indeed, we argue this by considering the two possible cases: (1) (a−τ d)2 ≤ 4|b|2τ or (2) (a−τ d)2 > 4τ |b|2.

• Case 1: Suppose τ ∈ (0, ∞) is such that (a − τ d)2 ≤ 4|b|2τ . Then, Re(λτ ) = 12 (a + τ d) > 0 trivially since a + d > 0.
• Case 2: Suppose τ ∈ (0, ∞) is such that (a − τ d)2 > 4τ |b|2. In this case, we want to ensure that

Re(λτ )

>

21 (a

+

τ d)

−

1 2

(a − τ d)2 − 4τ |b|2 > 0.

The last inequality is equivalent to −ad < |b|2. Indeed,

(a + τ d)2 > (a − τ d)2 − 4τ |b|2 ⇐⇒ 4τ ad > −4τ |b|2 ⇐⇒ −ad < |b|2.

Moreover, −ad < |b|2 holds for any pair of vectors (v, w) such that v ∈ W1 and w ∈ W2 since a > 0 and d > 0.

Hence, for any τ ∈ (0, ∞), spec(Jτ (x∗)) ⊂ C◦+ since the spectrum of an operator is contained in its quadratic numerical range and the above argument shows that W2(Jτ (x∗)) ⊂ C◦+.

C Proof of Lemma 1 and Lemma 2
In this appendix section, we prove Lemma 1 and Lemma 2 from Section 4. We note that the proof of Lemma 2 starts where the proof of Lemma 1 leaves oﬀ.

C.1 Proof of Lemma 1

Suppose that x∗ is a diﬀerential Stackelberg or Nash equilibrium and that 0 < τ < ∞ is such that spec(−Jτ (x∗)) ⊂ C◦−. For the discrete time dynamical system xk+1 = xk − γ1Λτ g(xk), it is well known that if γ1 is chosen such that ρ(I − γ1Jτ (x∗)) < 1, then xk locally (exponentially) converges to x∗ (Ortega
and Rheinboldt, 1970). With this in mind, we formulate an optimization problem to ﬁnd the upper bound
γ on the learning rate γ1 such that for all γ1 ∈ (0, γ), the spectral radius of the local linearization of the discrete time map is a contraction which is precisely ρ(I − γ1Jτ (x∗)) < 1. The optimization problem is given
by

γ = min γ : max |1 − γλ| ≤ 1 .

γ>0

λ∈spec(Jτ (x∗))

(34)

47

The intuition is as follows. The inner maximization problem is over a ﬁnite set spec(Jτ (x∗)) = {λ1, . . . , λn} where Jτ (x∗) ∈ Rn×n. As γ increases away from zero, each |1 − γλi| shrinks in magnitude. The last λi such that 1 − γλi hits the boundary of the unit circle in the complex plane (i.e., |1 − γλi| = 1) gives us the optimal value of γ and the element of spec(Jτ (x∗)) that achieves it. Examining the constraint, we have that for each λi, γ(γ|λi|2 − 2Re(λi)) ≤ 0 for any γ > 0. As noted this constraint will be tight for one of the λ, in which case γ = 2Re(λ)/|λ|2 since γ > 0. Hence, by selecting γ = minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2, we have that |1 − γ1λ| < 1 for all λ ∈ spec(Jτ (x∗)) and any γ1 ∈ (0, γ).
To see this is the case, let γ = minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2 and λm = arg minλ∈spec(Jτ ) 2Re(λ)/|λ|2.
Using the expression for γ, we have that

1 − 2γRe(λ) + γ2(Re(λ)2 + Im(λ)2) = 1 − 2 2Re(λm) Re(λ) + |λm|2

2Re(λm)

2
|λ|2.

|λm|2

Now, using the fact that Re(λ)/|λ|2 > Re(λm)/|λm|2, we have

1 − 4 Re(λm) Re(λ) + |λm|2

2Re(λm) |λm|2

2 |λ|2 ≤ 1 − 2 2Re(λm) Re(λ) + |λm|2

2Re(λm) 2 |λm|2Re(λ)

|λm|2

Re(λm)

= 1 − 4 Re(λm) Re(λ) + 4 Re(λm) Re(λ)

|λm|2

|λm|2

=1

as claimed. From this argument, it is clear that for any γ1 ∈ (0, γ), |1 − γ1λ| < 1 for all λ ∈ spec(Jτ (x∗)). Now, consider any α ∈ (0, γ) and let β = (2Re(λm) − α|λm|2)−1. Observe that γ1 = γ − α so that
γ1 ∈ (0, γ). Hence,

|1 − (γ − α)λ |2 =

1−

2Re(λm) − α

2
Re(λ ) +

2Re(λm) − α

2
Im(λ )2

m

|λm|2

m

|λm|2

m

= 1 − 4 Re(λm)2 + 2αRe(λ ) + 4 Re(λm)2 − 4αRe(λ ) + α2|λ |2

|λm|2

m

|λm|2

m

m

= 1 − 2αRe(λm) + α2|λm|2

=1− α β

so that

ρ(I − γ1Jτ (x∗)) < 1 − αβ 1/2 .

Hence, the ρ(I − γ1Jτ (x∗)) < 1 so that an application of Proposition 6 gives us the desired result.

C.2 Proof of Lemma 2
To prove this lemma, we build directly on the conclusion of the proof of Lemma 1. Indeed, since ρ(I − γ1Jτ (x∗)) < 1 − αβ 1/2 ,
given ε = 4αβ > 0 there exists a norm · (cf. Lemma 5.6.10 in Horn and Johnson (1985))9 such that I − γ1Jτ (x∗) ≤ 1 − αβ 1/2 + 4αβ ≤ 1 − 2αβ 1/2
where the last inequality holds by Lemma 3. Taking the Taylor expansion of I − γ1gτ (x) around x∗, we have I − γ1gτ (x) = (I − γ1gτ (x∗)) + (I − γ1Jτ (x∗))(x − x∗) + R2(x − x∗)
9The norm that exists can easily be constructed as essentially a weighted induced 1-norm. Note that the norm construction is not unique. The proof in Horn and Johnson (1985) is by construction and the construction of this norm can be found there.

48

where R2(x − x∗) is the remainder term satisfying R2(x − x∗) = o( x − x∗ ) as x → x∗.10 This implies that

there is a δ > 0 such that

R2(x − x∗)

≤

α 8β

x − x∗

whenever

x − x∗

< δ. Hence,

I − γ1gτ (x) − (I − γ1gτ (x∗)) ≤ ≤

I − γ1Jτ (x∗) + α 4β

α 1/2 α

1 − 2β

+ 8β

x − x∗ x − x∗

≤

1− α

1/2
x − x∗

4β

where the last inequality holds again by Lemma 3. Hence,

xk − x∗ ≤ 1 − 4αβ k/2 x0 − x∗ (35)

whenever x0 − x∗ < δ which veriﬁes the claimed convergence rate.

D Proof of Corollary 3

Let · be the norm that exists (via construction a la Horn and Johnson (1985, Lem. 5.6.10)) in the proof
of Lemma 2 which is given in Appendix C. Following standard arguments, (35) in the proof of Lemma 2 implies a ﬁnite time convergence guarantee. Indeed, let ε > 0 be given. Since 0 < 4αβ < 1 we have that (1 − α/(4β))k < exp(−kα/(4β)). Hence,

xk − x∗ ≤ exp(−kα/(4β)) x0 − x∗ .

In turn, this implies that xk ∈ Bε(x∗), meaning that xk is a ε-diﬀerential Stackelberg equilibrium for all k ≥ 4αβ log( x0 − x∗ /ε) whenever x0 − x∗ < δ.
Now, given that fi ∈ Cr(X, R) for r ≥ 2, I − γ1Jτ (x) is locally Lipschitz with constant L so that we can ﬁnd an explicit expression for δ in terms of L. Indeed, recall that R2(x − x∗) = o( x − x∗ ) as x → x∗ which means limx→x∗ R2(x − x∗) / x − x∗ = 0 so that

R2(x − x∗) ≤

1
I − γ1Jτ (x∗ + η(x − x∗)) − (I − γ1Jτ (x∗))
0

x − x∗

dη ≤ L x − x∗ 2 2

Observing that R2(x − x∗) ≤ L 2
we have that the δ > 0 such that R2(x − x∗)

x − x∗ 2 = L x − x∗ x − x∗ , 2
≤ α/(8β) x − x∗ is δ = α/(4Lβ).

E Proof of Theorem 3 and Corollary 2
To prove Theorem 3 and Corollary 2, we introduce some techniques that are arguably new to the machine learning and artiﬁcial intelligence communities. The ﬁrst is the notion of a guard map. A guard map can be used to provide a certiﬁcate of a particular behavior for a dynamical system as a parameter(s) varies. A critical point of a dynamical systems is known to be stable if the spectrum of the Jacobian at the critical point lies in the open left-half complex plane, denoted C◦−. Hence, we construct a guard map as a function of τ and show that it guards C◦−. Speciﬁcally we show that the existence of a τ ∗ ∈ (0, ∞) such that ν(τ ∗) = 0 and ν(τ ) = 0 for all τ ∈ (τ ∗, ∞) is equivalent to S1(J(x∗)) > 0 and −D22f (x∗) > 0 where
S1(J (x∗)) = S1(Jτ (x∗)) = D12f (x∗) − D12f (x∗)(D22f (x∗))−1D21f (x∗).
Towards this end, we need to introduced some notation as well as formal deﬁnitions for important concepts such as the guard map.
10The notation R2(x − x∗) = o( x − x∗ ) as x → x∗ means limx→x∗ R2(x − x∗) / x − x∗ = 0.

49

E.1 Notation and Preliminaries
Given a matrix A ∈ Rn1×n2 , let vec(A) ∈ Rn1n2 be the vectorization of A. We use the convention that rows are transposed and stacked in order. That is,

 — a1 —   a1  vec :  ...  →  ... 

— an1 —

an1

Let ⊗ and ⊕ denote the Kronecker product and Kronecker sum respectively. Recall that A ⊕ B = A ⊗ B +

B ⊗ A. A less common operator, we deﬁne from a matrix A ∈ Rn×n such that
A

as

an

operator

that

generates

an

1 2

n

(

n

+

1)

×

12 n(n + 1)

matrix

A = Hn+(A ⊕ A)Hn

where Hn+ = (Hn Hn)−1Hn is the (left) pseudo-inverse of Hn, a full column rank dupplication matrix. A duplication matrix Hn ∈ Rn2×n(n+1)/2 is a clever linear algebra tool for mapping a n2 (n + 1) vector to a n2 vector generated by applying vec(·) to a symmetric matrix and it is designed to respect the vectorization map vec(·). In particular, if vech(X) is the half-vectorization map of any symmetric matrix X ∈ Rn×n, then vec(X) = Hnvech(X) and vech(X) = Hn+vec(X).
Given a square matrix A, let λ+max(A) be the largest positive real eigenvalue of A and if A does not have
a positive real eigenvalue then it is zero.

Guardian maps. The use of guardian maps for studying stability of parameterized families of dynamical systems was arguably introduced by Saydy et al. (1990). Guardian or guard maps act as a certiﬁcate for a performance criteria such as stability.
Formally, let X be the set of all n × n real matrices or the set of all polynomials of degree n with real coeﬃcients. Consider S an open subset of X with closure S¯ and boundary ∂S.
Deﬁnition 6. The map ν : X → C is said to be a guardian map for S if for all x ∈ S¯, ν(x) = 0 ⇐⇒ x ∈ ∂S.
Consider an open subset Ω of the complex plane that is symmetric with respect to the real axis. Then, elements of S(Ω) = {A ∈ Rn×n : spec(A) ⊂ Ω} are said to be stable relative to Ω.
The following result gives a necessary and suﬃcient condition for stability of parameterized families of matrices relative to some open subset of the complex plane.

Proposition 7 (Proposition 1 (Saydy et al., 1990); Theorem 2 (Abed et al., 1990)). Consider U to be a pathwise connected subset of R and A(τ ) ∈ Rn×n a matrix which depends continuously on τ . Let S(Ω) be guarded by the map ν. The family {A(τ ) : τ ∈ U } is stable relative to Ω if and only if (i) it is nominally
stable—i.e., A(τ0) ∈ S(Ω) for some τ0 ∈ U —and (ii) ν(A(τ )) = 0 for all τ ∈ U .

In proving Theorem 3, we deﬁne a guard map for the space of n × n Hurwitz stable matrices which is denoted by S(C◦−). Lemma 6. The map ν : A → det(A A) guards the set of n × n Hurwitz stable matrices S(C◦−). Proof. This follows from the following observation: for A ∈ Rn×n,
vech(AX + XA ) = Hn+vec(AX + XA ) = Hn+(A ⊕ A)vec(X) = Hn+(A ⊕ A)Hnvech(X)

from which it can be shown that the eigenvalues of A A are λi + λj for 1 ≤ j ≤ i ≤ n1 + n2 where λi for

i = 1, . . . , n are the eigenvalues of A.

Indeed, let S be a non-singular matrix such that S−1AS = M where M is upper triangular with λ1, . . . , λn

on

its

diagonal.

Observe

that

for

any

n×n

matrix

P,

HnHn+(P ⊗P )Hn

=

(P ⊗P )Hn

and

H

+ n

(

P

⊗

P

)

Hn

Hn+

=

Hn+(P ⊗ P ). Hence, using properties of the Kronecker product (namely, that (A1 ⊗ A2)(B1 ⊗ B2) = (A1B1 ⊗

A2B2)), we have that

Hn+(S−1 ⊗ S−1)HnHn+(I ⊗ A + A ⊗ I)HnHn+(S ⊗ S)Hn = Hn+(I ⊗ M + M ⊗ I)Hn

50

so that the spectrum of Hn+(I ⊗ A + A ⊗ I)Hn and Hn+(I ⊗ M + M ⊗ I)Hn coincide. Now, since M is upper triangular, Hn+(I ⊗ M + M ⊗ I)Hn is upper triangular with diagonal elements λi + λj (1 ≤ j ≤ i ≤ n) which can be veriﬁed by direct computation and using the deﬁnition of Hn. This implies that λi + λj (1 ≤ j ≤ i ≤ n) are exactly the eigenvalues of Hn+(I ⊗ A + A ⊗ I)Hn.
We note that there are several other guard maps for the space of Hurwtiz stable matrices including ν : A → det(A ⊕ A). To give some intuition for this map, it is fairly straightforward to see that the Kronecker sum A ⊕ A = A ⊗ I + I ⊗ A has spectrum {λj + λi} where λi, λj ∈ spec(A). The operator A A is simply a more computationally eﬃcient expression of A ⊕ A, and as such the eigenvalues of A A are those of A ⊕ A removing redundancies. We use A A speciﬁcally because of its computational advantages in computing τ ∗.

E.2 Proof of Theorem 3
We ﬁrst prove that if x∗ is a diﬀerential Stackelberg equilibrium (i.e., S1(Jτ (x∗)) > 0 and −D22f (x∗) > 0), then there exists a ﬁnite τ ∗ ∈ (0, ∞) such that for all τ ∈ (τ ∗, ∞), x∗ is exponentially stable for x˙ = −Λτ g(x) (i.e., spec(−Jτ (x∗)) ⊂ C◦−). Towards this end, we construct a guard map for the space of n × n Hurwtiz stable matrices and explicitly construct the τ ∗ using it.
Then we prove the other direction. That is, if there exists a ﬁnite τ ∗ ∈ (0, ∞) such that for all τ ∈ (τ ∗, ∞), x∗ is exponentially stable for x˙ = −Λτ g(x), then x∗ is a diﬀerential Stackelberg equilibrium. We prove this by contradiction.

E.2.1 Proof that if x∗ is a diﬀerential Stackelberg then ﬁnite τ ∗ exists Towards this end, for a critical point x∗, let

−Jτ (x∗) = −D12f (x∗) τ D12f (x∗)

−D12f (x∗)

A11

τ D2f (x∗) = −τ A

2

12

A12 τ A22

and deﬁne

S1 = S1(−Jτ (x∗)) = A11 − A12A−221A12.

Note that this is equivalent to the ﬁrst Schur complement of −J(x∗) (i.e., when τ = 1) since the τ and τ −1 cancel, and by assumption the ﬁrst Schur complement of −J(x∗) is positive deﬁnite. Suppose that x∗ is a diﬀerential Stackelberg equilibrium so that −S1 > 0 and −A22 > 0.

Polynomial guard map with family of matrices parameterized by τ . By Lemma 6, ν : A →
det(A A) is a guard map for S(C◦−). Indeed, using the fact that the determinant is the product of the eigenvalues of a matrix and the fact that spec(A A) = {λi + λj, 1 ≤ i ≤ j ≤ n, λi, λj ∈ spec(A)}, we have

that

det(A A) =

(λi + λj) =

2Re(λi)(4Re2(λi) + 4Im2(λi))

(λi + λj).

1≤j≤i≤n

1≤i≤n

1<i<j<n:
λi =λ¯ j

Hence, consider S¯(C◦−), det(A A) = 0 if and only if A A is singular if and only if A has a purely imaginary eigenvalue—that is, if and only if A ∈ ∂S(C◦−).11 Now, consider the parameterized family of matrices −Jτ (x∗), parameterized by τ . By an abuse of notation, let ν(τ ) = det(−Jτ (x∗) −Jτ (x∗)). If we consider the subset of this family of matrices that lies in S(C◦−) (this subset could a priori be empty thought we show it is not), then for any τ such that −Jτ (x∗) is in this subset, we have that ν(τ ) = 0 if and only if −Jτ (x∗) (−Jτ (x∗)) is singular if and only if −Jτ (x∗) ∈ ∂S(C◦−). Hence, ν(τ ) = det(−Jτ (x∗) −Jτ (x∗)) guards S(C◦−).
In particular, if we envision −Jτ (x∗) as the input to ν : A → det(A A) and simply vary τ (holding all the entries of −Jτ (x∗) otherwise ﬁxed), then ν : τ → det(−Jτ (x∗) (−Jτ (x∗))) can be thought of simply as

11Indeed, this holds since the only scenarios in which det(A A) = 0 are such that the eigenvalues of A do not lie in S¯(C◦−).

51

a function of τ which guards the set of Hurwitz stable matrices via the reasoning describe above. Indeed, by slightly overloading the notation for ν,
ν(τ ) := ν0 + ν1τ + · · · + νp−1τ p−1 + νpτ p = ν(−Jτ (x∗))
Hence, for intuition, observe that as τ decreases (towards zero) stability is ﬁrst lost when at least one eigenvalue of −Jτ (x∗) reaches the imaginary axis, at which point ν(τ ) = 0.
There are two cases to consider: Case 1: ν(τ ) is an identically zero polynomial. In this case, −Jτ (x∗) is in the interior of the complement
of the set of Hurwitz stable matrices for all values of τ > 0—that is, −Jτ (x∗) ∈ int(Sc(C◦−)) for all τ ∈ R+ = (0, ∞).
Case 2: ν(τ ) is not an identically zero polynomial. In this case, ν(τ ) has ﬁnitely many zeros. If ν(τ ) has no positive real roots, then as τ varies in R+, −Jτ (x∗) does not cross ∂S(C◦−—i.e., the boundary of the space of n × n Hurwitz stable matrices. Hence, {−Jτ (x∗) : τ ∈ R+} ⊂ Sc(C◦−) or {−Jτ (x∗) : τ ∈ R+} ⊂ int(Sc(C◦−)). It suﬃces to check −Jτ (x∗) ∈ Sc(C◦−) or −Jτ (x∗) ∈ int(Sc(C◦−)) for an arbitrary τ ∈ R+. On the other hand, if ν(τ ) has ≥ 1 real positive zeros, say 0 < τ1 < · · · < τ = τ ∗, then by Proposition 7, −Jτ (x∗) ∈ S(C◦−) for all τ > τ ∗ if and only if −Jτ (x∗) ∈ S(C◦−) for arbitrarily chosen τ > τ ∗. We choose the largest positive root τ because we are guaranteed that ν(τ ) stops changing sign for τ > τ ∗. Further, the largest neighborhood in R+ for which −Jτ (x∗) ∈ S(C◦−) is (τ , ∞).
Recall that we have assumed that x∗ is a diﬀerential Stackelberg equilibrium (i.e., S1 > 0 and −A22 > 0). We will show next (by way of explicit construction of τ ∗) that we are always in case 2.

Construction of τ ∗. We note that there are more elegant, simpler constructions, but to our knowledge

this construction gives the tightest bound on the range of τ for which −Jτ (x∗) is guarnateed to be Hurwitz

stable. Recall that

−Jτ (x∗) = −D12f (x∗) τ D12f (x∗)

−D12f (x∗)

A11

τ D2f (x∗) = −τ A

2

12

A12 τ A22

and S1 = A11 − A12A−221A12.

Let Im denote the m × m identity matrix.

Claim 1. The ﬁnite learning rate ratio is τ ∗ = λ+max(Q) where

Q = −(A11 ⊗ A−221) + 2 (A12 ⊗ A−221)Hn2 (In1 ⊗ A−221A12)Hn1

A¯−221 0

0 −S¯1−1

Hn+2 (A12 ⊗ In2 ) Hn+1 (S1 ⊗ A12A−221)

(36)

with A¯22 = A22 A22 and S¯1 = S1 S1.
Proof. Recall that ν(τ ) = det(−Jτ (x∗) (−Jτ (x∗))) is a guard map for S(C◦−). We apply basic properties of the Kronecker product and sum as well as Schur’s determinant formula to
obtain a reduced form of the guard map. To this end, we have that

−Jτ (x∗)



A11 A11

(−Jτ (x∗)) = τ (In1 ⊗ (−A12))Hn1

0

2Hn+1 (In1 ⊗ A12) A11 ⊕ τ A22
2τ Hn+2 (−A12 ⊗ In2 )

0



(A12 ⊗ In2 )Hn2 

τ (A22 A22)

Now, we apply Schur’s determinant formula to get that

ν(τ ) = τ n2(n2+1)/2 det(A22

A22) det

A11 A11 τ (In1 ⊗ (−A12))Hn1

2Hn+1 (In1 ⊗ A12) A11 ⊕ τ A22 + M1

(37)

where

M1 = −2Hn+2 (−A12 ⊗ In2 )(A22 A22)−1(A12 ⊗ In2 )Hn2

52

From here, we apply Lemma 4 to further reduce the guard map. First, note that

A11 ⊕ τ A22 = A11 ⊗ In2 + In1 ⊗ τ A22.

Let V = In1 ⊗ τ A22, Z = A11 ⊗ In2 + M1, Y = A11 A11, W = −τ (In1 ⊗ A12)Hn1 , and X = 2Hn+1 (In1 ⊗ A12). Using the two properties of the Kronecker product (B1 ⊗ B2)(B3 ⊗ B4) = (B1B3 ⊗ B2B4) and (B1 ⊗ B2)−1 = (B1−1 ⊗ B2−1), we have that

Y − XV −1W = A11 = A11 = A11 = S1

A11 + 2Hn+1 (In1 ⊗ A12)(In1 ⊗ A22)−1(In1 ⊗ A12)Hn1 A11 + 2Hn+1 (In1 ⊗ A12A−221A12)Hn1 A11 + Hn+1 ((In1 ⊗ A12A−221A12) + (A12A−221A12 ⊗ In1 ))Hn1 S1

(38) (39) (40) (41)

where (40) holds since Hn+1 (In1 ⊗ A12A2−21A12)Hn1 = Hn+1 (A12A−221A12 ⊗ In1 )Hn1 . Now, deﬁne V −1 + V −1W (Y − XV −1W )−1XV −1 = τ −1M2 where

M2 = In1 ⊗ A−221 − 2(In1 ⊗ A−221A12)Hn1 (S1 S1)−1Hn+1 (In1 ⊗ A12A−221)

so that applying Lemma 4 we have

ν(τ ) = τ n2(n2+1)/2 det(A22 A22) det(S1 S1) det(In1 ⊗ A22) det(τ In1n2 + M2(A11 ⊗ In2 + M1)) (42)

The assumptions that S1 > 0 and −A22 > 0 together imply that det(S1 S1) = 0 and det(In1 ⊗ A22) = 0. Hence, ν(τ ) = 0 if and only if det(τ In1n2 + M2(A11 ⊗ In2 + M1)) = 0 since 0 < τ < ∞. The determinant expression is exactly an eigenvalue problem.
Since by assumption the Schur complement of J(x∗) and the individual Hessian −D22f (x∗) are positive deﬁnite (i.e., x∗ is a diﬀerential Stackelberg equilibrium), Thus, the largest positive real root of ν(τ ) = 0 is

τ ∗ = λ+max(−M2(A11 ⊗ In2 + M1))

where λ+max(·) is the largest positive real eigenvalue of its argument if one exists and otherwise its zero. Using properties of the Kronecker product and duplication matrices, it can easily be seen that Q = −M2(A11 ⊗ In2 + M1).
The result of this claim concludes the proof that if x∗ is a diﬀerential Stackelberg, then there exists a ﬁnite τ ∗ ∈ [0, ∞) such that for all τ ∈ (τ ∗, ∞), spec(−Jτ (x∗)) ⊂ C◦−.

E.2.2 Proof that existence of ﬁnite τ ∗ implies that x∗ is a diﬀerential Stackelberg
The proof of this direction is argued by contradiction. Consider a critical point x∗ (i.e., where g(x∗) = 0 such that −C ≡ −D22f (x∗) and S1 ≡ S1(J (x∗)) = D12f (x∗) − D12f (x∗)(D22f (x∗))−1D12f (x∗) have no zero eigenvalues—that is, det(S1) = 0 and det(C) = 0.
Suppose that there exists a τ ∗ ∈ (0, ∞) such that for all τ ∈ (τ ∗, ∞), spec(−Jτ (x∗)) ⊂ C◦−, yet x∗ is not a diﬀerential Stackelberg equilibrium. That is, either −S1 or C have at least one positive eigenvalue. Without loss of generality, let −S1 have at least one positive eigenvalue.
Since det(S1) = 0 and det(C) = 0, by Lemma 5.b, there exists non-singular Hermitian matrices P1, P2 and positive deﬁnite Hermitian matrices Q1, Q2 such that −S1P1 − P1S1 = Q1 and CP2 + P2C = Q2. Further, −S1 and P1 have the same inertia, meaning
υ+(−S1) = υ+(P1), υ−(−S1) = υ−(P1), ζ(−S1) = ζ(P1)

where for a given matrix A, υ+(A), υ−(A), and ζ(A) are the number of eigenvalues of the argument that have positive, negative and zero real parts, respectively. Similarly, C and P2 have the same inertia:

υ+(C) = υ+(P2), υ−(C) = υ−(P2), ζ(C) = ζ(P2). Since −S1 has at least one strictly positive eigenvalue, υ+(P1) = υ+(−S1) ≥ 1.

53

Deﬁne

P = I L0 P1 0 I 0

(43)

0 I 0 P2 L0 I

where L0 = (D22f (x∗))−1D12f (x∗) = CD12f (x∗). Since P is congruent to blockdiag(P1, P2), by Sylvester’s law of inertia (Horn and Johnson, 1985, Thm. 4.5.8), P and blockdiag(P1, P2) have the same inertia, meaning
that υ+(P ) = υ+(blockdiag(P1, P2)), υ−(P ) = υ−(blockdiag(P1, P2)), and ζ(P ) = ζ(blockdiag(P1, P2)). Consider the matrix equation −P Jτ (x∗) − Jτ (x∗)P = Qτ for −Jτ (x∗) where

Qτ = I L0 0I

Q1 (P1D12f (x∗) − S1L0 P2)

P1D12f (x∗) − S1L0 P2 P2L0D12f (x∗) + (P2L0D12f (x∗))
Bτ

+ τ Q2

I0 L0 I

which can be veriﬁed by straightforward calculations. Observe that Qτ > 0 is equivalent to Bτ > 0 and both matrices are symmetric so that Bτ > 0 if and
only if Q1 > 0 and S2(Bτ ) > 0 where

S2(Bτ ) = P2L0D12f (x∗) + (P2L0D12f (x∗)) + τ Q2 − (P1D12f (x∗) − S1L0 P2) Q−1 1(P1D12f (x∗) − S1L0 P2).

Now, S2(Bτ ) is also a real symmetric matrix, and hence, it is positive deﬁnite if and only if all its eigenvalues are positive. To determine the range of τ such that S2(Bτ ) is positive deﬁnite, we can formulate an eigenvalue problem to determine the value of τ such that the matrix S2(Bτ ) becomes singular. This is analogous to the guard map approach used in the proof in the previous subsection for the other direction of the proof, and
in this case, we are varying τ from zero to inﬁnity and ﬁnding the point such that for all larger τ , S2(Bτ ) is positive deﬁnite. Intuitively, such an argument works since τ scales the positive deﬁnite matrix Q2. Towards this end, consider the eigenvalue problem in τ given by

0 = det τ I − Q−2 1 (P1D12f (x∗) − S1L0 P2) Q−1 1(P1D12f (x∗) − S1L0 P2) − P2L0D12f (x∗) − (P2L1D12f (x∗)) .

Let τ0 be the maximum positive eigenvalue, and zero otherwise. Then, since eigenvalues vary continuously, for all τ ∈ (τ0, ∞), Qτ > 0 so that by Lemma 5.a we conclude that P and −Jτ (x∗) have the same inertia, but this contradicts the stability of −Jτ (x∗) for all τ ∈ (τ ∗, ∞) since υ+(P ) ≥ 1.

E.3 Proof of Corollary 2
Suppose that x∗ is a diﬀerential Stackelberg equilibrium so that by Theorem 3, there exists a τ ∗ ∈ (0, ∞) such that spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞). Now that we have a guarantee that −Jτ (x∗) is Hurwitz stable for any τ ∈ (τ ∗, ∞), we apply Hartman-Grobman to get that the nonlinear system x˙ = −Λτ g(x) is stable in a neighborhood of x∗. Fix any τ ∈ (τ ∗, ∞) and let γ = arg minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2. Then, applying Lemma 1, for any γ1 ∈ (0, γ), τ -GDA converges locally asymptotically to x∗.
On the other hand, suppose that there exists a τ ∗ ∈ (0, ∞) such that spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞). Then by Theorem 3, if x∗ is a diﬀerential Stackelberg equilibrium. Furthermore, since spec(−Jτ (x∗)) ⊂ C◦− for all τ ∈ (τ ∗, ∞), if we let γ = arg minλ∈spec(Jτ (x∗)) 2Re(λ)/|λ|2, then by Lemma 1 τ -GDA converges locally asymptotically to x∗ for any choice of γ1 ∈ (0, γ).

F Proof of Proposition 4
The structure of this proof is as follows. We begin by introducing general background for analyzing general singularly perturbed systems. Following this, we consider the linearization of the singularly perturbed system that approximates the simultaneous gradient dynamics and describe how insights made about this system translate to the corresponding nonlinear system. Finally, we analyze the stability of the linear system around a critical point to arrive at the stated result. The analysis is primarily from Kokotovic et al. (1986).

54

Analysis of General Singularly Perturbed Systems. Let us begin by considering a general singularly perturbed system for x ∈ Rn, z ∈ Rm, and a suﬃciently small parameter ε > 0 given by

x˙ = f (x, z, ε, t), x(t0, ε) = x0, x ∈ Rn

εz˙ = g(x, z, ε, t), z(t0, ε) = z0, z ∈ Rm

(44)

where f and g are assumed to be suﬃciently many times continuously diﬀerential functions of the arguments x, z, ε, and t. Observe that when ε = 0, the dimension of the system given in (44) drops from n + m to n since z˙ degenerates into the equation

0 = g(x¯, z¯, 0, t)

(45)

where the notation of x¯, z¯ indicates that the variables belong to the system with ε = 0. We further require the assumption that (45) has k ≥ 1 isolated roots, which for each i ∈ {1, . . . , k} are given by
z¯ = φ¯i(x¯, t).

We now deﬁne an n-dimensional manifold Mε for any ε > 0 characterized by the expression

z(t, ε) = φ(x(t, ε), ε),

(46)

where φ is suﬃciently many times continuously diﬀerentiable function of x and ε. For Mε to be an invariant manifold of the system given in (44), the expression in (46) must hold for all t > t∗ if it holds for t = t∗.

Formally, if

z(t∗, ε) = φ(x(t∗, ε), ε) → z(t, ε) = φ(x(t, ε), ε) ∀t ≥ t∗,

(47)

then Mε is an invariant manifold for (44). Diﬀerentiating the expression in (47) with respect to t, we obtain

d

dφ

z˙ = φ(x(t, ε), ε) = x˙ .

(48)

dt

∂x

Now, multiplying the expression in (48) by ε and substituting in the forms of x˙ , z˙, and z from (44) and (46), the manifold condition becomes

∂φ

g(x, φ(x, ε), ε, t) = ε f (x, φ(x, ε), ε, t),

(49)

∂x

which φ(x, ε) must satisfy for all x of interest and all ε ∈ [0, ε∗], where ε∗ is a positive constant. We now deﬁne η = z − φ(x, ε).

Then, in terms of x and η, the system becomes

x˙ = f (x, φ(x, ε) + η, ε, t) εη˙ = g(x, φ(x, ε) + η, ε, t) − ε ∂φ f (x, φ(x, ε) + η, ε, t).
∂x
Remark 3. One interesting observation is that the above system is exactly the continuous time limiting system for the τ -Stackelberg learning update in Fiez et al. (2020) under a simple transformation of coordinates.
Observe that the invariant manifold Mε is characterized by the fact that η = 0 implies η˙ = 0 for all x for which the manifold condition from (49) holds. This implies that if η(t0, ε) = 0, it is suﬃcient to solve the system
x˙ = f (x, φ(x, ε), ε, t), x(t0, ε) = x0.
This system is often referred to as the exact slow model and is valid for all x, z ∈ Mε and Mε known as the slow manifold of (52).

55

Linearization of Simultaneous Gradient Descent Singularly Perturbed System. We now consider the singularly perturbed system for simulataneous gradient descent given by

x˙ = −D12f1(x, z) (50)
εz˙ = −D2f2(x, z).

Let us linearize the system around a point (x∗, z∗). Then,12

D1f1(x, z) ≈ D1f1(x∗, z∗) + D12f1(x∗, z∗)(x − x∗) + D12f1(x∗, z∗)(z − z∗) (51)
D2f2(x, z) ≈ D2f2(x∗, z∗) + D21f2(x∗, z∗)(x − x∗) + D22f2(x∗, z∗)(z − z∗).
Deﬁning u = (x − x∗) and v = (z − z∗) and considering a point (x∗, z∗) such that D1f1(x∗, z∗) = 0 and D2f2(x∗, z∗) = 0, then linearized singularly perturbed system is given by

u˙ = −D12f1(x∗, z∗)u − D12f1(x∗, z∗)v (52)
εv˙ = −D21f2(x∗, z∗)u − D22f2(x∗, z∗)v.

To simplify notation, let us deﬁne Jτ as follows

J=

D

2 1

f1

(x

∗

,

z

∗

)

τ ε−1D21f2(x∗, z∗)

D12f1(x∗, z∗)

A11

ε−1D22f2(x∗, z∗) = ε−1A21

A12 ε−1A22

along with

w˙ = u˙ v˙

Then, an equivalent form of (52) is given by

and w = u . v

w˙ = −Jτ w.

(53)

In what follows, we make insights about the behavior of the nonlinear system given in (50) around a critical point (x∗, z∗) by analyzing the linear system given in (53). Recall that if (x∗, z∗) is asymptotically stable with
respect to the linear system in (53), then it is also asymptotically stable with respect to the nonlinear system from (50). Moreover, to determine asymptotic stability, it is suﬃcient to prove that spec(Jτ ((x∗, z∗)) ⊂ C◦+. In what follows, we specialize the general analysis of singularly perturbed systems to the singularly perturbed
linear system given in (53).

Stability of Critical Points of Simutaneous Gradient Descent. The manifold condition from (49) for the system in (53) is given by

∂φ A21u + A22φ(u, ε) = ε ∂u (A11u + A12φ(u, ε)). (54) We claim that (54) can be satisﬁed by a function φ that is linear in u. Indeed, deﬁning

v = φ(u, ε) = −L(ε)u

and then substituting back into (49), we get the simpliﬁed manifold condition of

A21 − A22L(ε) = −εL(ε)A11 + εL(ε)A12L(ε).

(55)

Before we prove that an L(ε) always exists to satisfy (55), consider the change of variables

η = v + L(ε)u.
12Here, the ≈ means, e.g., D1f1(x, z) = D1f1(x∗, z∗)+D12f1(x∗, z∗)(x−x∗)+D12f1(x∗, z∗)(z −z∗)+O( x−x∗ 2 + z −z∗ 2), and similarly for D2f2(x, z).

56

The change of variables transforms the system from (53) into the equivalent representation

u˙ = A11 − A12L(ε)

A12

u

(56)

η˙

R(L, ε)

A22 + εL(ε)A12 η

where

R(L, ε) = A21 − A22L(ε) + εL(ε)A11 − εL(ε)A12L(ε).

(57)

Consider that R(L, ε) = 0. Then, the system from (56) has the upper block-triangular form

x˙ = A11 − A12L(ε)

A12

x,

(58)

η˙

0

A22 + εL(ε)A12 η

which has the eﬀect of generating a replacement fast subsystem given by

εη˙ = (A22 + εLA12)η.

We now proceed to show that an L(ε) such that R(L, ε) = 0 always exists.
Lemma 7. If A22 is such that det(A22) = 0, there is an ε∗ such that for all ε ∈ [0, ε∗], there exists a solution L(ε) to the matrix quadratic equation

R(L, ε) = A21 − A22L(ε) + εL(ε)A11 − εL(ε)A12L = 0

(59)

which is approximated according to

L(ε) = A−221A21 + εA−222A21A0 + O(ε2),

(60)

where

A0 = A11 − A12A−221A21.

Proof. To begin, observe that for ε = 0, the unique solution to (59) is given by L(0) = A−221A21. diﬀerentiating R(L, ε) from (59) with respect to ε, we ﬁnd

(61) Now,

A22

+

εL(ε)A12

dL

−

dL ε (A11

−

A12L(ε))

=

L(ε)A11

−

L(ε)A12L(ε).

dε dε

The unique solution of this equation at ε is

ddLε ε=0 = A−221L(0)(A11 − A12L(0)) = A−222A21A0.

Accordingly, (60) represents the ﬁrst two terms of the MacLaurin series for L(ε).

We remark that L(ε) as deﬁned in (60) is unique in the sense that even though R(L, ) as given in (59) may have several real solutions, only one is approximated by (60).
The characteristic equation of (58) is equivalent to that for the system from (53) owing to the similarity transform between the systems. The block-triangular form of (53) admits a characteristic equation given by

1

ψ(s, ε) = εm ψs(s, ε)ψf (p, ε) = 0,

(62)

where

ψs(s, ε) = det(sI − (A11 − A12L(ε)))

(63)

is the characteristic polynomial of the slow subsystem, and

ψf (p, ε) = det(pI − (A22 + εA12L(ε)))

(64)

is the characteristic polynomial of the fast subsystem in the timescale p = sε. Consequently, n of the eigenvalues of (53) denoted by {λ1, . . . , λn} are the roots of the slow characteristic equation ψs(s, ε) = 0

57

and the rest of the eigenvalues {λn+1, . . . , λn+m} are denoted by λi = νj/ε for i = n + j and j ∈ {1, . . . , m} where {ν1, . . . , νm} are the roots of the fast characteristic equation ψf (p, ε) = 0.
The roots of ψs(s, ε) at ε = 0, given by the solution to

ψs(s, 0) = det(sI − (A11 − A12L(0))) = 0,

(65)

are the eigenvalues of the matrix A0 deﬁned in (61) since L(0) = A−221A21 as shown in Lemma 7. The roots of the fast characteristic equation at ε = 0, given by the solution to

ψf (p, 0) = det(pI − A22) = 0

(66)

are the eigenvalues of the matrix A22. We now proceed by characterizing how closely the eigenvalues of the system at ε = 0 approximate the eigenvalues of the system from (53) as ε → 0.
If det(A22) = 0, then as ε → 0, n eigenvalues of the system given in (53) tend toward the eigenvalues

of the matrix A0 while the remaining m eigenvalues of the system from (53) tend to inﬁnity with the rate

1/ε along asymptotes deﬁned by the eigenvalues of A22 given as spec(A22)/ε as a result of the continuity of

coeﬃcients of the polynomials from (63) and (64) with respect to ε.

Now, consider the special (but generic) case in which the eigenvalues of A0 are distinct and the eigenvalues

of A22 are distinct, but A0 and A22 may have common eigenvalues. Then, taking the total derivative of (62)

with respect to ε we have that

∂ψs ds + ∂ψs = 0 ∂s dε ∂ε

Now, observe that ∂ψs/∂s = 0 since the eigenvalues of A0 = A11 − A12A−221A21 are distinct.13 For each i = 1, . . . , n, this gives us a well-deﬁned derivative ds/dε (by the implicit mapping theorem) and hence, with

s(0) = λi(A0), the O(ε) approximation of s(ε) follows directly. That is,

λi = λi(A0) + O(ε), i = 1, . . . , n1

Similarly, taking the total derivative of ψf (p, ε) = 0 and again applying the implicit function theorem, we have
λi+n1 = ε−1(λj (A22 + O(ε)), i = 1, . . . , n2
where we have used the fact that p = sε.

G Proof of Theorem 4

Let x∗ be a stable critical point of 1-GDA which is not a diﬀerential Stackelberg equilibrium. Without loss of
generality, suppose that S1(−J(x∗)) has at least one eigenvalue with strictly positive real part. Since both S1(−J(x∗)) and D22f (x∗) have no zero valued eigenvalues, by Lemma 5.b, there exists non-
singular Hermitian matrices P1, P2 and positive deﬁnite Hermitian matrices Q1, Q2 such that S1(−J(x∗))P1+ P1S1(−J (x∗)) = Q1 and D22f (x∗)P2 + P2D22f (x∗) = Q2. Further, S1(−J (x∗)) and P1 have the same inertia,
meaning υ+(S1(−J (x∗))) = υ+(P1), υ−(S1(−J (x∗))) = υ−(P1), ζ(S1(−J (x∗))) = ζ(P1)

where for a given matrix A, υ+(A), υ−(A), and ζ(A) are the number of eigenvalues of the argument that have positive, negative and zero real parts, respectively. Similarly, D22f (x∗) and P2 have the same inertia:

υ+(D22f (x∗)) = υ+(P2), υ−(D22f (x∗)) = υ−(P2), ζ(D22f (x∗)) = ζ(P2).

Recall that we assumed S1(−J(x∗)) has at least one eigenvalue with strictly positive real part.

υ+(P1) = υ+(S1(−J (x∗))) ≥ 1.

Deﬁne

P = I L0 0I

P1 0 0 P2

I0 L0 I

Hence,

13Recall that having distinct eigenvalues is a generic condition for a matrix an n1 × n1 matrix, though not explicitly required
for the asymptotic results; its only a condition for the big-O approximation λi = λi(A0) + O(ε) for i = 1, . . . , n1 and λi = ε−1(λj (A22) + O(ε)) where i = n1 + j for j = 1, . . . , n2.

58

where L0 = (D22f (x∗))−1D12f (x∗). Since P is congruent to blockdiag(P1, P2), by Sylvester’s law of inertia (Horn and Johnson, 1985, Thm. 4.5.8), P and blockdiag(P1, P2) have the same inertia, meaning that
υ+(P ) = υ+(blockdiag(P1, P2)), υ−(P ) = υ−(blockdiag(P1, P2)), and ζ(P ) = ζ(blockdiag(P1, P2)). Consider now the Lyapunov equation −P Jτ (x∗) − Jτ (x∗)P = Qτ for −Jτ (x∗) where

Qτ = I L0 0I

Q1 (P1D12f (x∗) + S1(−J (x∗))L0 P2)

P1D12f (x∗) + S1(−J (x∗))L0 P2 P2L0D12f (x∗) + (P2L0D12f (x∗)) + τ Q2
Bτ

I0 L0 I

which can be veriﬁed by straightforward calculations. Since υ+(P1) ≥ 1, we have that υ+(P ) ≥ 1. Now, we ﬁnd the value of τ0 such that for all τ > τ0, Qτ > 0
so that, in turn, we can apply Lemma 5.a, to conclude that spec(−Jτ (x∗)) ⊂ C◦−. Indeed, observe that Qτ > 0 is equivalent to Bτ > 0 and both matrices are symmetric so that Bτ > 0 if and only if Q1 > 0 and S2(Bτ ) > 0 where
S2(Bτ ) = P2L1D12f (x∗) + (P2L1D12f (x∗)) + τ Q2 − (P1D12f (x∗) + S1(−J (x∗))L0 P2) Q−1 1(P1D12f (x∗) + S1(−J (x∗))L0 P2).

Now, S2(Bτ ) is also a real symmetric matrix, and hence, it is positive deﬁnite if and only if all its eigenvalues are positive. To determine the range of τ for which Qτ > 0, we simply need to solve the eigenvalue problem
0 = det(τ I − Q−2 1((P1D12f (x∗) + S1(−J (x∗))L0 P2) Q−1 1(P1D12f (x∗) + S1(−J (x∗))L0 P2) − P2L1D12f (x∗) − (P2L1D12f (x∗)) )).

and extract the maximum eigenvalue, namely,

τ0 =λmax(Q−2 1((P1D12f (x∗) + S1(−J (x∗))L0 P2) Q−1 1(P1D12f (x∗) + S1(−J (x∗))L0 P2) − P2L1D12f (x∗) − (P2L1D12f (x∗)) )).
Hence, as noted previously, by Lemma 5.a, we conclude that for all τ ∈ (τ0, ∞), spec(−Jτ (x∗)) ⊂ C◦−. To provide some context for the proof approach, it follows the same idea as the proof of Theorem 3 in
Appendix E.2.2. Indeed, to determine the range of τ such that S2(Bτ ) is positive deﬁnite, we can formulate an eigenvalue problem to determine the value of τ such that the matrix S2(Bτ ) becomes singular. We vary τ from zero to inﬁnity in order to ﬁnd the point such that for all larger τ , S2(Bτ ) is positive deﬁnite. Intuitively, such an argument works since τ scales the positive deﬁnite matrix Q2.

H Proof of Theorem 7

As in Mescheder et al. (2018), we only apply the regularization to the discriminator. In the following proof, we use ∇x(·) to denote the partial gradient with respect to x of the argument (·) when the argument is the discriminator D(·; ω) in order prevent any confusion between the notation D(·) which we use elsewhere for derivatives.
To prove the ﬁrst part of this result, we following similar arguments to Theorem 4.1 of (Mescheder
et al., 2018). To prove the second part, we leverage the concept of the quadratic numerical range. For both
components of the proof, we will use the following form of the Jacobian of the regularized game. Indeed, ﬁrst observe that the structural form of J(τ,µ)(x∗) is

J(τ,µ)(x∗) = −τ0B τ (C +B µR) (67)

where B = D12f (x∗), C = −D22f (x∗) and R = D22Ri(x∗). This follows from Assumption 2-a., which implies

that

D(x; ω∗)

=

0

in

some

neighborhood

of

supp(pD )

and

hence,

∇xD(x; ω∗)

=

0

and

∇

2 x

D(x

;

ω

∗

)

=

0

for

x ∈ supp(pD). In turn, we have that D12f (x∗) = 0.

59

Proof that x∗ = (θ∗, ω∗) is a diﬀerential Stackelberg equilibrium. For any ﬁxed µ ∈ [0, ∞), then

we ﬁrst observe that x∗ is also a critical point of the unregularized dynamics. Indeed, by Assumption 2-

a.,

D(x; ω∗)

=

0

in

some

neighborhood

of

supp(pD )

and

hence,

∇xD(x; ω∗)

=

0

and

∇

2 x

D(

x

;

ω

∗

)

=

0

for

x ∈ supp(pD). Further, D2Ri(θ, ω) = µEpi(x)[D2(∇xD(x; ω))∇xD(x, ω)] for i = 1, 2 where p1(x) = pD(x)

and p2(x) = pθ(x). Thus, using the above observation that ∇xD(x; ω∗) = 0, we have that D2Ri(θ∗, ω∗) = 0

for i = 1, 2 meaning that the derivative of the regularizer with respect to ω is zero at x∗ = (θ∗, ω∗) which

in turn implies that D1f (x∗) = 0 and −D2f (x∗) = 0. Hence, x∗ is a critical point of the unregularized

dynamics as claimed. Further, C + µR > 0 which follows from Lemma D.5 in (Mescheder et al., 2018). From

Lemma D.6 in (Mescheder et al., 2018), due to Assumption 2-c., if v = 0 and v ∈ Tθ∗ MG, then Bv = 0

which implies that B can only be rank deﬁcient on Tθ∗ MG. Using this fact along with the structure of the

Jacobian as in (67), we have that the Schur complement of J(τ,µ)(x∗) is equal to B (C + µR)−1B > 0 since

C + µR > 0. Hence, x∗ = (θ∗, ω∗) is a diﬀerential Stackelberg equilibrium.

Proof of stability. Examining (67), it is straightforward to see that the quadratic numerical range W2(J(τ,µ)) has eigenvalues of the form

λτ,µ

=

21 (τ (c

+

µr))

±

1 2

(−τ (c + µr))2 − 4τ |b|2

where b =

D12f (x∗)v, w , c =

−D22f (x∗)w, w

and r =

D

2 2

Ri

(x

∗

)

w

,

w

for vectors v ∈ W1 ∩ (Tθ∗ MG)⊥

and w ∈ W2 ∩ (Tω∗ MD)⊥ where U ⊥ denotes the orthogonal complement of U . We claim that for any value

of µ ∈ [0, µ1] and any τ ∈ (0, ∞), Re(λτ,µ) > 0. Indeed, we argue this by considering the two possible cases:

(1) (τ (c + µr))2 ≤ 4|b|2τ or (2) (τ (c + µr))2 > 4τ |b|2.

• Case 1: Suppose that (τ (c + µr))2 ≤ 4|b|2τ . Then, Re(λτ,µ) = 21 (τ (c + µr)) > 0 trivially since c + µr > 0.

• Case 2: Suppose that (τ (c + µr))2 > 4τ |b|2. In this case, we want to ensure that

which is true since

Re(λτ )

>

12 (τ (c

+

µr))

−

1 2

(−τ (c + µr))2 − 4τ |b|2 > 0.

(τ (c + µr))2 > (−τ (c + µr))2 − 4τ |b|2 ⇐⇒ 0 > −4τ |b|2

This concludes the proof.

I Proof of Proposition 5

This proposition follows immediately from observing the structure of the Jacobian: for any matrix of the

form

−J = B0 −−BC

at least one eigenvalue will be purely imaginary if n2 < n1/2 where B ∈ Rn1×n2 and C ∈ Rn2×n2 . Indeed, by Lyapunov’s stability theorem for linear systems (Hespanha, 2018, Theorem 8.2), a matrix A is Hurwitz
stable if and only if for every symmetric positive deﬁnite Q = Q > 0, there exists a unique symmetric positive deﬁnite P = P > 0, such that A P + P A = −Q. Hence, −J is Hurwitz stable if and only if there exists a P = P > 0 such that

0<Q= =

0 −B P1 P2 + P1 P2 0 B

B C P2 P3

P2 P3 −B C

−BP2 − P2B B P1 + CP2 − P3B

−BP3 + P1B + P2C B P2 + CP3 + P2 B + P3C

Since this is a symmetric positive deﬁnite matrix, the block diagonal components must also be symmetric positive deﬁnite so that −BP2 − P2B > 0.14 Recall that B ∈ Rn1×n2 and P2 ∈ Rn2×n1 . Hence, a necessary
14If a block matrix Q with block entries Qij for i, j ∈ {1, 2} is positive deﬁnite symmetric, then Qii > 0 for i = 1, 2.

60

condition for this matrix to be positive deﬁnite is that n2 ≥ n1/2 for −BP2 − P2B to have full rank; of course this is not suﬃcient, but it is necessary. It is easy to see this argument is independent of whether a learning rate ratio τ = 0 or regularization is incorporated.
J Extensions in the Stochastic Setting
Let x˜(t) be the asymptotic pseudo-trajectories of the stochastic approximation process {xk}. That is, x˜(t) are linear interpolates between the sample points xk generated by the stochastic τ -GDA process, and are deﬁned by
x˜(t) = x˜(tk) + (t − tk) (x˜(tk+1) − x˜(tk)) γk
where tk = tk + γk and t0 = 0.
Assumption 3. The stochastic process {wk} is a martingale diﬀerence sequence with respect to the increasing family of σ-ﬁelds deﬁned by
Fk = σ(x , w , ≤ k), ∀k ≥ 0, so that E[wk+1| Fk] = 0 almost surely for all k ≥ 0. Furthermore, there exists c1, c2 ∈ C(Rd, R>0) such that
Pr{ wk+1 > v| Fk} ≤ c1(xk) exp(−c2(xk)v), n ≥ 0
for all v ≥ v˜ where v˜ is some suﬃciently large, ﬁxed number.
Proposition 8. Suppose that Assumption 3 holds and that x∗ is a diﬀerential Stackelberg equilibrium. Let γk = 1/(k + 1)β where β ∈ (0, 1]. There exists a τ ∗ ∈ (0, ∞) and an 0 ∈ (0, ∞) such that for any ﬁxed
∈ (0, 0], there exists functions h1( ) = O(log(1/ )) and h2( ) = O(1/ ) so that when T ≥ h1( ) and k0 ≥ Kτ where Kτ is such that 1/γk ≥ h2( ) for all k ≥ Kτ , the stochastic iterates of τ -GDA with stepsize sequence γk and timescale separation τ ∈ (τ ∗, ∞) satisfy
Pr{ x˜(t) − x∗ ≤ ∀t ≥ tk0 + T + 1| x˜(tk0 ) ∈ B (x∗)} = 1 − O(k01−β/2 exp(−Cτ k0β/2))
for some constant Cτ > 0.
The proof largely follows from the proofs of Theorem 1.1 and 1.2 in (Thoppe and Borkar, 2019), combined with the existence of a ﬁnite timescale separation parameter obtained via Theorem 3. Indeed, since x∗ is a diﬀerential Stackelberg equilibrium, by Theorem 3 there exists a range of τ —namely, (τ ∗, ∞)—such that for any τ ∈ (τ ∗, ∞), x∗ is a locally asymptotically stable equillibrium for x˙ = −Λτ g(x). Hence, ﬁxing any τ ∈ (τ ∗, ∞), a converse Lyapunov theorem can be applied to construct a local Lyapunov function. Let V : Rn → R be this Lyapunov function so that there exists r, r0, 0 > 0 such that r > r0, and
B (x∗) ⊆ V r0 ⊂ N 0 (V r0 ) ⊆ V r
for any ∈ (0, 0] where, for a given q > 0, V q = {x ∈ dom(V ) : V (x) ≤ q} and N 0 (V r0 ) is an 0– neighborhood of V r0 —i.e., N 0 (V r0 ) = {x ∈ Rn| ∃y ∈ V r0 , x − y ≤ 0}. From here, the result follows from an application of the results in the work by Thoppe and Borkar (2019).
The utility of this result is that it provides a guarantee in the stochastic setting for a more reasonable and practically useful stepsize sequence. However, constructing the constants such as Kτ , Cτ and 0 is highly non-trivial as can be seen in the work of Thoppe and Borkar (2019) and similar works in the area of stochastic approximation (Borkar, 2008). One direction of future work is examining the Lyapunov approach for directly analyzing the nonlinear singularly perturbed system; it is known, however, that the stochastic singularly perturbed systems have much weaker guarantees in terms of stability (Kokotovic et al., 1986, Chap. 4).
61

K Further Details on Related Work

In this section, we provide further details on the discussion from Section 7 regarding the results presented by Jin et al. (2020) on the local stability of gradient descent-ascent with a ﬁnite timescale separation. The purpose of this discussion is to make clear that Proposition 27 from the work of Jin et al. (2020) does not disagree with the results we provide in Theorem 3 and Theorem 4 and is instead complementary. In what follows, we recall Proposition 27 of Jin et al. (2020) in separate pieces in the terminology of this paper and delineate its meaning from our results on the stability of gradient descent-ascent with a ﬁnite timescale separation.
To begin, we consider the component of Proposition 27 from Jin et al. (2020) which says that given any ﬁxed and ﬁnite timescale separation τ > 0, a zero-sum game can be constructed with a diﬀerential Stackelberg equilibrium that is not stable with respect to the continuous time limiting system of τ -GDA given by the dynamics x˙ = −Λτ g(x).

Proposition 9 (Rephrasing of Jin et al. 2020, Proposition 27(a)). For any ﬁxed τ > 0, there exists a zero-sum game G = (f, −f ) such that spec(Jτ (x∗)) ⊂ C◦+ for a diﬀerential Stackelberg equilibrium x∗.

We now explain the proof. Let us consider any > 0 and the game

f (x, y)

=

−x2

+

√ 2

xy

−

(

/2)y2.

(68)

At the unique critical point (x∗, y∗) = (0, 0), the Jacobian of the dynamics is given by

Jτ (x∗, y∗) = −2−τ2√

√ 2. τ

Moreover, observe that (x∗, y∗) is a diﬀerential Stackelberg equilibrium and not a diﬀerential Nash equilib-

rium since D12f (x∗, y∗) = −2 ≯ 0, −D22f (x∗, y∗) = > 0 and S1(J (x∗, y∗)) = 2 > 0. Finally, the spectrum

of the Jacobian is

spec(Jτ (x∗, y∗)) =

√ −2 + τ ± τ 2 2 − 12τ + 4 .
2

Let us now ﬁx τ as any arbitrary positive value. Then, consider the game construction from (68) with

= 1/τ . For the ﬁxed choice of τ and subsequent game construction, we get that √
spec(Jτ (x∗, y∗)) = { − 1 ± i 7 /2} ⊂ C◦+.

This in turn means the diﬀerential Stackelberg equilibrium is not stable with respect to the dynamics x˙ = −Λτ g(x) for the given choice of τ . Since the choice of τ was arbitrary, this is a valid procedure to generate a game with a diﬀerential Stackelberg equilibrium that is not stable with respect to x˙ = −Λτ g(x) given a choice of τ beforehand.
This result contrasts with that of Theorem 3 in the following fundamental way. In the proof of Proposition 9, τ is ﬁxed and then the game is constructed, whereas in Theorem 3 the game is ﬁxed and then the conditions on τ given. To illustrate this point, consider the game construction from (68) with ﬁxed to be an arbitrary positive value. It can be veriﬁed that spec(Jτ (x∗, y∗)) ⊂ C◦+ for all τ > 2/ . This means that given the diﬀerential Stackelberg equilibria in this game construction, there is indeed a ﬁnite τ ∗ such that the equilibrium is stable with respect to x˙ = −Λτ g(x) for all τ ∈ (τ ∗, ∞). Put concisely, Proposition 9 is showing that there is exists a continuum of games for which a diﬀerential Stackelberg equilibrium is unstable with an improper choice of ﬁnite learning rate ratio τ . On the other hand, Theorem 3 is proving that given a game with a diﬀerential Stackelberg equilibrium, there exists a range of suitable ﬁnite learning rate ratios such that the diﬀerential Stackelberg equilibrium is guaranteed to be stable.
We now move on to examining the portion of Proposition 27 from Jin et al. (2020) which says that given any ﬁxed and ﬁnite timescale separation τ > 0, a zero-sum game can be constructed with a critical point that is not a diﬀerential Stackelberg equilibrium which is stable with respect to the continuous time limiting system of τ -GDA given by x˙ = −Λτ g(x).

Proposition 10 (Rephrasing of Jin et al. 2020, Proposition 27(b)). For any ﬁxed τ , there exists a zero-sum game G = (f, −f ) such that spec(Jτ (x∗)) ⊂ C◦+ for a critical point x∗ satisfying g(x∗) = 0 that is not a diﬀerential Stackelberg equilibrium.

62

In a similar manner as following Proposition 9, we now explain the proof of Proposition 10 and then contrast the result with Theorem 4. Again, consider any > 0, along with the game construction
f (x, y) = x21 + 2√ x1y1 + ( /2)y12 − x22/2 + 2√ x2y2 − y22. (69)

At the unique critical point (x∗, y∗) = (0, 0), the Jacobian of the dynamics is given by



√



2

02

√0

Jτ

(x∗,

y∗)

=

 

0√

−1

0 2 

−2τ

0√ −τ 0 

0 −2τ

0 2τ

Observe that (x∗, y∗) is neither a diﬀerential Nash equilibrium nor a diﬀerential Stackelberg equilibrium

since D12f (x∗, y∗) = diag(2, −1) and −D22f (x∗, y∗) = diag( , 2 ) are both indeﬁnite. The spectrum of the

Jacobian is

spec(Jτ (x∗, y∗)) =

√

√

2 − τ ± τ 2 2 − 12τ + 4 , −1 + 2τ ± 4τ 2 2 − 12τ + 1 .

2

2

Now, ﬁx τ as any arbitrary positive value, then consider the game construction from (69) with

the ﬁxed choice of τ and resulting game construction given the choice of , we have that

√

√

spec(Jτ (x∗, y∗)) = {1 ± i 7, 1 ± i 7} ⊂ C◦+.

= 1/τ . For

This indicates that the non-equilibrium critical point is stable with respect to the dynamics z˙ = −Λτ g(z) where z = (x, y) for the given choice of τ . Similar to the proof of Proposition 9, since the choice of τ was arbitrary, the procedure to generate a game with a non-equilibrium critical point that is stable with respect to z˙ = −Λτ g(z) is valid given a choice of τ beforehand.
The key distinction between Proposition 10 and Theorem 4 is analogous to that between Proposition 9 and Theorem 3. Indeed, the proof and result of Proposition 10 rely on τ being ﬁxed followed by the game being constructed. On the other hand, in Theorem 4 the game is ﬁxed and then the conditions on τ given. To make this clear, consider the game construction from (69) with ﬁxed to be an arbitrary positive value. It turns out that spec(Jτ (x∗, y∗)) ⊂ C◦+ for all τ > 2/ since
√ Re 2 − τ ± τ 2 2 − 12τ + 4 < 0.
2
As a result, given the unique critical point of the game there is a ﬁnite τ0 such that the non-equilibrium critical point is not stable with respect to x˙ = −Λτ g(x) for all τ ∈ (τ0, ∞). In summary, Proposition 10 is showing that there is exists a continuum of games for which a non-equilibrium critical point is stable given an unsuitable choice of ﬁnite learning rate ratio τ . In contrast, Theorem 4 is showing that given a game with a non-equilibrium critical point, there exists a range of ﬁnite learning rate ratios such that it is not stable.
To recap, the discussion in this section is meant to explicitly contrast Proposition 27 from the work of Jin et al. (2020) with Theorem 3 and Theorem 4 since they may potentially appear contradictory to each other without close inspection. The result of Jin et al. (2020) shows that (i) given a ﬁxed ﬁnite learning ratio, there exists a game for with a diﬀerential Stackelberg equilibria that is not stable and (ii) given a ﬁxed ﬁnite learning ratio, there exists a game with a non-equilibrium critical point that is stable. From a diﬀerent perspective, we show that (i) given a ﬁxed game and diﬀerential Stackelberg equilibrium, there exists a range of ﬁnite learning rate ratios for which the equilibrium is stable (Theorem 3) and (ii) given a ﬁxed game and a non-equilibrium critical point, there exists a range of ﬁnite learning rate ratios for which the critical point is not stable (Theorem 4).

L Experiments Supplement
In this section we present several experiments not included in the body of the paper along with supplemental simulation results and details for the experiments presented in Section 6. We study a torus game in

63

Section L.1 and examine the connection between timescale separation and the region of attraction. Then, in Section L.2, we return to the Dirac-GAN game and consider the non-saturating objective function. In Section L.3, we explore a generative adversarial network formulation using the Wasserstein cost function with a linear generator and quadratic discriminator for the problem of learning a covariance matrix. We ﬁnish in Section L.4 by presenting further results and details on our experiments training generative adversarial networks on image datasets.

L.1 Location Game on the Torus
We use the example in this section to further study the role of timescale separation on the regions of attraction around critical points. Consider the zero-sum game deﬁned by the cost

f (x1, x2) = −0.15 cos(x1) + cos(x1 − x2) + 0.15 cos(x2).

(70)

This game can be interpreted as a location game on the torus. Speciﬁcally, the ﬁrst player seeks to be far
from the second player but near zero, while the second player seeks to be near the ﬁrst player. This is a non-convex game on a non-convex strategy space. The critical points are given by the set15:

{x : g(x) = 0} = {(0, 0), (π, π), (π, 0), (0, π), (−1.646, −1.496), (1.646, 1.496)}.

The critical points (0, 0) and (π, π) are the only diﬀerential Stackelberg equilibrium and neither is a diﬀerential

Nash equilibrium. The diﬀerential Stackelberg equilibrium at (0, 0) is stable for all τ ∈ (τ ∗, ∞) where τ ∗ = 0.74 and the diﬀerential Stackelberg equilibrium (π, π) is stable for all τ ∈ (τ ∗, ∞) where τ = 1.35. The rest of the critical points are unstable for any choice of τ . We remark that we computed τ ∗ for each

diﬀerential Stackelberg equilibrium using the construction from Theorem 3 in Section 3 and it again gave

the exact value of τ ∗ such that the system is stable for all τ > τ ∗.

In Figure 13a, we show the trajectories of τ -GDA with γ1 = 0.001 and τ ∈ {1, 2, 5, 10} given the initializations (x01, x02) = (2, −1) and (x01, x02) = (1.9, −2.1) overlayed on the vector ﬁeld generated by the respective

timescale separation parameters. We observe that as the timescale separation τ grows, the rotational dy-

namics in the vector ﬁeld dissipate and the directions of movement become sharp. As we mentioned in

previous examples, τ -GDA moves directly to the zero line of −D2f (x1, x2) and then along that line to an

equilibrium given suﬃcient timescale separation. The warping of the vector ﬁeld that occurs as a result of

timescale separation impacts the equilibrium that the dynamics converge to from a ﬁxed initial condition

and the neighborhood on which τ -GDA converges to an equilibrium. In other words, the region of attraction

around critical points depends heavily on the timescale separation τ .

To illustrate this fact, in Figure 13b we show the regions of attraction for each choice of timescale sepa-

ration. The vector ﬁelds are again shown for each τ ∈ {1, 2, 5, 10}, but now with colors overlayed indicating

the equilibria that the dynamics converge to given an initialization at that position. This experiment was

generated by running τ -GDA with a dense set of initial conditions chosen uniformly over the strategy space.

Positions in the strategy space without color did not converge to an equilibrium in the ﬁxed horizon of

20000 iterations with γ1 = 0.04. This happens when τ -GDA is not initialized in the local neighborhood of

attraction around a stable equilibrium. For the choice of τ = 1, (0, 0) is the only stable equilibrium. How-

ever, as demonstrated in Figure 13a, τ -GDA fails to converge to the equilibrium from the initial conditions

(x

0 1

,

x02

)

=

(2, −1)

and

(x

0 1

,

x02

)

=

(1.9, −2.1).

This

behavior

is

further

demonstrated

over

the

strategy

space

in Figure 13b and highlights the local nature of the guarantees since convergence is only assured given an

initialization in a suitable local neighborhood around a stable critical point. On the other hand, τ -GDA con-

verges to an equilibrium from any initial condition for τ ∈ {2, 5, 10} as can be seen by Figure 13b. Notably,

the equilibrium to which the learning dynamics converge depends on the timescale separation and initial con-

dition.

To

give

a

concrete

example,

consider

the

initial

conditions

shown

in

Figure

13a

of

(x

0 1

,

x02

)

=

(2, −1)

and

(x

0 1

,

x02

)

=

(1.9, −2.1).

For

the

initial

condition

(x

0 1

,

x02

)

=

(2, −1),

τ -GDA

converges

to

the

equilibrium

at (0, 0) for each τ ∈ {2, 5, 10}. Yet, for the initial condition (x01, x02) = (1.9, −2.1), τ -GDA converges to the

equilibrium at {(0, 0), (π, π), (π, π)} for the respective choices of τ ∈ {2, 5, 10}. In other words, the region

of attraction around the critical points changes so that from a ﬁxed initial condition τ -GDA may converge to

distinct equilibrium depending on the initial condition. From Figure 13b, we see that the region of attraction

15Note that because the joint strategy space is a torus, (±π, ±π) = (∓π, ±π), (π, 0) = (−π, 0), and (0, −π) = (0, π).

64

(a)
(b)
Figure 13: Experimental results for the torus game deﬁned in (70) of Appendix L.1. In Figure 13a, we overlay multiple trajectories produced by τ -GDA onto the vector ﬁeld generated by the choice of timescale separation selection τ . The shading of the vector ﬁeld is dictated by its magnitude so that lighter shading corresponds to a higher magnitude and darker shading corresponds to a lower magnitude. Figure 13b demonstrates the eﬀect of timescale separation on the regions of attraction around critical points by coloring points in the strategy space according to the equilibrium τ -GDA converges. We remark that areas without coloring indicate where τ -GDA did not converge in the time horizon.
around (x01, x02) = (1.9, −2.1) grows from τ = 1 to τ = 2 and τ = 4, but then shrinks at τ = 10. This example highlights that timescale separation has a fundamental impact on the region of attraction around critical points and as τ grows it is possible for the region of attraction around an equilibrium to shrink. Collectively, this motivates explicit methods for trying to shape the region of attraction around desirable equilibria.
L.2 Dirac-GAN and Regularization: Non-Saturating Formulation
In Section 6.4, we presented experiments for the Dirac-GAN game studied by Mescheder et al. (2017) using the original generative adversarial network formulation of Goodfellow et al. (2014). In this section, we revisit the Dirac-GAN game using the non-saturating generative adversarial network formulation also proposed by Goodfellow et al. (2014). While we refer the reader back to Section 6.4 for complete details on the Dirac-GAN, we do recall some key components of the formulation. Recall that the zero-sum game which arises from the original objective with regularization µ > 0 is deﬁned by the cost
f (θ, ω) = (θω) + (0) − µ ω2. 2
As discussed in Section 6.4, the unique critical point of the game is (θ∗, ω∗) = (0, 0) and it corresponds to the local Nash equilibrium of the unregularized game and a diﬀerential Stackelberg equilibrium of the
65

(a)

(b)

(d)
Figure 14: Experimental results for the Dirac-GAN game deﬁned in (71) of Appendix L.2. Figure 14a shows trajectories of τ -GDA for τ ∈ {1, 4, 8, 16} with regularization µ = 0.3 and τ = 1 with regularization µ = 1. Figure 14b shows the distance from the equilibrium along the learning paths. Figure 14d shows the trajectories of τ -GDA overlayed on the vector ﬁeld generated by the respective timescale separation and regularization parameters. The shading of the vector ﬁeld is dictated by its magnitude so that lighter shading corresponds to a higher magnitude and darker shading corresponds to a lower magnitude.

regularized game. Moreover, the equilibrium is stable with respect to the continous time dynamics for all τ > 0 and µ > 0 so that the discrete time update τ -GDA converges with a suitable learning rate γ1.
The non-saturating generative adversarial network formulation proposed by Goodfellow et al. (2014) in the context of the Dirac-GAN game corresponds to player 1 maximizing (−θω) instead of minimizing (θω). This results in the general-sum game deﬁned by the costs

(f1(θ, ω), f2(θ, ω)) = (− (−θω) + (0) − µ ω2, − (θω) − (0) + µ ω2).

(71)

2

2

As shown by Mescheder et al. (2018), the unique critical point of the game remains at (θ∗, ω∗) = (0, 0). Moreover, it can be observed that Jτ (θ∗, ω∗) in this formulation is identical to that from (25) so this game is locally equivalent to the zero-sum game that arises from the original objective proposed by Goodfellow et al. (2014). This is despite the fact that the non-saturating objective was motivated by global concerns (vanishing gradients early in the training process) rather than local considerations. In Figure 14 we present experiments with τ -GDA for the regularized Dirac-GAN game with the non-saturating objective and (t) = − (1+exp(−t)). We observe similar behavior as the experiments with the standard objective and refer back to Section 6.4 for the insights we draw from the simulation. This experiment is primarily included for completeness and to motivate our use of the non-saturating objective in the generative adversarial networks experiments we perform on image datasets in Section 6.5.

66

L.3 Generative Adversarial Network: Learning a Covariance Matrix
We now consider a generative adversarial network formulation presented by Daskalakis et al. (2018) for learning a covariance matrix. This is a simple example with degeneracies much like the Dirac-GAN game, but it can be generalized to arbitrary dimensional strategy spaces and has served as a benchmark for comparing convergence rates in a number of recent papers on learning in games. Often, the example is used to show that gradient descent-ascent cycles and converges slowly. However, by and large, timescale separation is not considered. We show that gradient descent-ascent converges fast in this game with suitable timescale separation and further explore the interplay between timescale separation, regularization, and rate of convergence. We primarily follow the notation of Daskalakis et al. (2018) when describing the problem.
The objective of this problem is to learn a covariance matrix using the Wasserstein GAN formulation. The real data x is drawn from a mean-zero multivariate normal distribution with an unknown covariance matrix Σ. The generator is restricted to be a linear function of the random input noise z ∼ N (0, I) and is of the form GV (z) = V z. The discriminator is restricted to the set of all quadratic functions, which we represent by DW (x) = x W x. The parameters of the generator and the discriminator are given by W ∈ Rd×d and V ∈ Rd×d, respectively. For the given generator and discriminator classes the Wasserstein GAN game is deﬁned by the cost

f (V, W ) = Ex∼N (0,Σ)[x W x] − Ez∼N (0,I)[z V W V z].

As shown by Daskalakis et al. (2018), the cost function can be simpliﬁed to be expressed as

dd

d

f (V, W ) =

Wij Σij − VikVjk .

i=1 j=1

k=1

With this cost, the individual gradients for gradient descent-ascent are given by

g(V, W ) = (−(W + W )V, −(Σ − V V )).

From the individual gradients, it is clear that the critical points of the game are given by (V, W ) such that
V V = Σ and W + W = 0. Moreover, given the form of g(V, W ), the game Jacobian at any critical point (V ∗, W ∗) is of the form

J (V ∗, W ∗) =

0

D12f (V ∗, W ∗) .

τ

−τ D12f (V ∗, W ∗)

0

Consequently, the eigenvalues of the game Jacobian are purely imaginary and the critical points are not

stable. To ﬁx this problem, Daskalakis et al. (2018) regularized both the generator and discriminator. We

only regularize the discriminator in this example. The cost function of the zero-sum game with regularization

µ > 0 is given by

dd

f (V, W ) =

Wij

i=1 j=1

d
Σij − VikVjk
k=1

−

µ Tr(W

2

W ).

(72)

The individual gradients for gradient descent-ascent in this regularized game are then

g(V, W ) = (−(W + W

)V, −(Σ − V V

µ ) + W ).

2

We begin by considering the simplest form of this problem, which is that d = 1. The critical points with this restriction are (V ∗, W ∗) = (σ, 0) and (V ∗, W ∗) = (−σ, 0) and the game Jacobian evaluated at them is
Jτ (V ∗, W ∗) = 0 −2σ . 2τ σ τ µ
Each critical point is a local Nash equilibrium of the unregularized game and a diﬀerential Stackelberg equilibrium of the regularized game since −D22f (V ∗, W ∗) = µ > 0 and S1(J(V ∗, W ∗)) = 4σ2/µ > 0.

67

(a) d = 1, µ = 0.5

(b) d = 1, µ = 0.75

(c) d = 1, µ = 1

(d)

(e)

(f )

(g) d = 5, µ = 1

(h) d = 10, µ = 1

(i) d = 20, µ = 1

(j) Legend for Figures 15a, 15b, 15c, 15g, 15h, and 15i.
Figure 15: Experimental results for the generative adversarial network formulation for learning a covariance matrix deﬁned by the cost from (72) of Section L.3. Figures 15a, 15b, and 15c show the distance from the equilibrium along the learning paths of τ -GDA with d = 1. Figures 15d, 15e, and 15f show the trajectories of the eigenvalues of Jτ (x∗) as a function of the τ , respectively. Figures 15g, 15h, and 15i show the distance from the equilibrium along the learning paths of τ -GDA with d = 5, 10, 20.
Furthermore, spec(Jτ (V ∗, W ∗)) = {(τ µ ± τ 2µ2 − 16τ σ2)/2} so that each critical point is stable for all τ ∈ (0, ∞) and µ ∈ (0, ∞) since spec(Jτ (θ∗, ω∗)) ⊂ C◦+. Thus, given a suitably chosen learning rate γ1, the discrete time update τ -GDA locally converges to an equilibrium. For this reason, we focus on studying the rate of convergence for the problem as a function of timescale separation and regularization. Figures 15a, 15b, and 15c show the distance from an equilibrium along the learning path of τ -GDA with τ ∈ {1, 5, 10, 25} given a ﬁxed initial condition with learning rate γ1 = 0.001 and regularization µ ∈ {0.5, 0.75, 1}, respectively. Moreover, Figures 15d, 15e, and 15f show the trajectories of the eigenvalues for Jτ (V ∗, W ∗) as a function of τ for the regularization parameters µ ∈ {0.5, 0.75, 1}. Finally, Figures 16a, 16b, and 16c show the trajectories
68

(a) d = 1, µ = 0.5
(b) d = 1, µ = 0.75
(c) d = 1, µ = 1
Figure 16: Experimental results for learning a covariance matrix deﬁned by the cost from (72) of Section L.3. We overlay the trajectories produced by τ -GDA onto the vector ﬁeld generated by the choices of τ and µ. The shading of the vector ﬁeld is dictated by its magnitude so that lighter shading corresponds to a higher magnitude and darker shading corresponds to a lower magnitude.
of τ -GDA overlayed on the vector ﬁeld generated by the respective timescale separation and regularization parameters.
From the eigenvalue trajectories, we see that as µ grows, the eigenvalues become purely real at a smaller value of τ . Moreover, as µ increases, the magnitude of the real and imaginary parts of the eigenvalues decreases. We observe the eﬀect of this on the convergence, where the dynamics do not cycles as much for larger µ. Again, we see the trade-oﬀ between timescale separation, regularization, and convergence. For example, despite the eigenvalues being purely real with µ = 1 and τ = 25 so that there is no rotational dynamics, the convergence is slower than for µ = 0.75 where there is some non-zero imaginary piece of the eigenvalues.
Figures 15g, 15h, and 15i show the distance from a critical point along the learning path of τ -GDA with τ ∈ {1, 5, 10, 25} given a ﬁxed initial condition with learning rate γ1 = 0.001, regularization µ = 1, and the dimension of the problem d among the set {5, 10, 20}, respectively. The primary purpose of showing this set of results is simply to be clear that the behavior for d = 1, which is easier to explain and visualize, transfers over to higher dimensional formulations of this problem. This is to be expected since the problem dimension is not necessarily fundamental to the convergence rate, but rather it depends on the conditioning of Σ and each Σ was chosen so that the behavior was comparable for each choice of dimension.
69

L.4 Generative Adversarial Networks: Image Data
In this section we provide additional results and details from the experiments we ran training generative adversarial networks on the CIFAR-10 and CelebA datasets. In Figure 17 we show more generated samples on each of the datasets. We ran our simulations based on the work of Mescheder et al. (2018) and used the publicly available code from the link https://github.com/LMescheder/GAN_stability. We refer the readers to (Mescheder et al., 2018) for details on the implementation and architectures, as we primarily only changed the learning rates used to run the experiments. For the networks, we ran the experiments using the architecture provided in the gan training/models/resnet.py ﬁle of the repository. In Figure 18 we include the hyperparameters we used for the experiments. To be clear, we used the same exact setup for both training CIFAR-10 and CelebA datasets. We computed the Frechet Inception Distance using 10k samples from the real and generated data. For both experiments and across the set of hyperparameters we did the evaluation using a ﬁxed random noise vector to make for an equal comparison and a ﬁxed set of real images which were randomly selected. The evaluation was done using the training data. We used the FID score implementation in pytorch available at https://github.com/mseitzer/pytorch-fid.

(a) CIFAR-10 generated sample images

(b) CelebA generated sample images

Figure 17: Generated sample images with τ = 4 and β = 0.9999

Hyperparameter Objective Batch Size
Latent Distribution Generator Learning Rate Timescale Separation τ
Learning Rate Decay Optimizer
RMSprop Smoothing Constant α RMSprop
Regularization µ EMA Parameter β

Value(s)
NSGAN
64 z ∈ R256
0.0001
{1, 2, 4, 8} (1 + x)−0.005
RMSprop
0.99 10−8
{1, 10}
{0.99, 0.999, 0.9999}

Figure 18: Hyperparameters for GAN experiments on CIFAR-10 and CelebA

70

