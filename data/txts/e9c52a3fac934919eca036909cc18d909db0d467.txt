A data-driven peridynamic continuum model for upscaling molecular dynamics
Huaiqian Youa,c, Yue Yua,, Stewart Sillingb, Marta D’Eliac
aDepartment of Mathematics, Lehigh University, Bethlehem, PA bCenter for Computing Research, Sandia National Laboratories, Albuquerque, NM cComputational Science and Analysis, Sandia National Laboratories, Livermore, CA

arXiv:2108.04883v1 [cond-mat.mtrl-sci] 4 Aug 2021

Abstract
Nonlocal models, including peridynamics, often use integral operators that embed lengthscales in their deﬁnition. However, the integrands in these operators are diﬃcult to deﬁne from the data that are typically available for a given physical system, such as laboratory mechanical property tests. In contrast, molecular dynamics (MD) does not require these integrands, but it suﬀers from computational limitations in the length and time scales it can address. To combine the strengths of both methods and to obtain a coarse-grained, homogenized continuum model that eﬃciently and accurately captures materials’ behavior, we propose a learning framework to extract, from MD data, an optimal Linear Peridynamic Solid (LPS) model as a surrogate for MD displacements. To maximize the accuracy of the learnt model we allow the peridynamic inﬂuence function to be partially negative, while preserving the well-posedness of the resulting model. To achieve this, we provide suﬃcient well-posedness conditions for discretized LPS models with sign-changing inﬂuence functions and develop a constrained optimization algorithm that minimizes the equation residual while enforcing such solvability conditions. This framework guarantees that the resulting model is mathematically well-posed, physically consistent, and that it generalizes well to settings that are diﬀerent from the ones used during training. We illustrate the eﬃcacy of the proposed approach with several numerical tests for single layer graphene. Our two-dimensional tests show the robustness of the proposed algorithm on validation data sets that include thermal noise, diﬀerent domain shapes and external loadings, and discretizations substantially diﬀerent from the ones used for training. Keywords: nonlocal models, data-driven learning, machine learning, optimization, homogenization, peridynamics
Email addresses: huy316@lehigh.edu (Huaiqian You), yuy214@lehigh.edu (Yue Yu), sasilli@sandia.gov (Stewart Silling), mdelia@sandia.gov (Marta D’Elia)

Preprint submitted to Elsevier

August 12, 2021

Contents

1 Introduction

2

2 Coarse-graining of molecular dynamics displacements

5

3 Peridynamics

7

3.1 Peridynamics Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

3.2 The Linear Peridynamic Solid (LPS) Model . . . . . . . . . . . . . . . . . . 8

4 Operator regression for the LPS model

11

4.1 Operator regression algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 11

4.2 Solvability Constraints for the Discretized LPS Model . . . . . . . . . . . . . 12

4.3 Algorithm and Workﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

5 Consistency tests for manufactured solutions

23

6 Application to graphene using MD

24

6.1 Data sets and metrics of accuracy . . . . . . . . . . . . . . . . . . . . . . . . 26

6.2 Learning Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

6.3 Sensitivity to Thermal Oscillations . . . . . . . . . . . . . . . . . . . . . . . 29

6.4 Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

7 Conclusion

32

1. Introduction
Complex systems where small-scale dynamics and interactions aﬀect the global behavior are ubiquitous in scientiﬁc and engineering applications. In disciplines ranging from climate forecasts to material design, heterogeneities in materials and media at the micro or molecular scales need to be accurately captured to guarantee reliable and trustworthy predictions. However, higher degrees of complexity and heterogeneity require numerical simulations of classical mathematical models at small scales that cannot be aﬀorded, despite recent advances in computational power. This fact creates the need for new mathematical models that act at larger scales and that, combined with new advanced architectures, allow for fast predictions [1–11]. The process of upscaling models or data hides several pitfalls that may compromise the reliability of the resulting surrogates.
In the presence of heterogeneities, it is often the case that to adequately reproduce the large-scale behavior of a system, a homogenized model must follow diﬀerent governing

2

laws, as well as diﬀerent constitutive properties, than the ones that apply at the small scale. Homogenization theory addresses the approximate treatment of a partial diﬀerential equation (PDE) that contains small-scale oscillations in its coeﬃcients [12]. It seeks to replace these coeﬃcients with constant or slowly-varying coeﬃcients such that the resulting solutions closely approximate solutions to the original problem in an averaged sense. The resulting parameters are called eﬀective properties. In many theoretical treatments, the eﬀective properties are valid only in the limiting case of a very small length scale in the oscillatory behavior of the original parameters. The classical notion of eﬀective properties therefore “washes out” the length scale in the original problem, causing important information to be lost.
Nonlocality in the spatial dependence of a continuum model has long been recognized as a consequence of homogenization [13, 14]. For example, in continuum mechanics, nonlocality arises from taking the ensemble average of the displacement ﬁeld in a family of random linear elastic heterogeneous materials [15–20]. To some extent, this nonlocality can be incorporated in homogenized weakly nonlocal PDEs that embed length scales in their coeﬃcients. However, weakly nonlocal PDEs are generally insuﬃcient to fully reproduce coarse-grained data because of the limited spectrum of processes that they can describe [21]. In general, increasing the accuracy of weakly nonlocal PDEs to match small-scale data involves using higher and higher order partial derivatives, resulting in practical challenges in numerical implementations.
As pointed out in [21], nonlocal operators [22, 23] are among the best candidates as model descriptions that can circumvent these limitations. Theoretical and numerical techniques for nonlocal models are not as advanced as for classical PDEs. This is one of the main reasons why integral operators historically have not received a broader adoption in the context of numerical homogenization. However, current advances in nonlocal theory, computer power, and solution algorithms are making nonlocal equations viable as practical modeling tools. While integral operators have proved to be successful in several contexts such as mechanics [24, 25] and turbulence [26, 27], they have not been systematically explored for coarse graining, or upscaling, of molecular dynamics (MD) models, for which we propose a new rigorous modeling paradigm.
We stress the fact that with nonlocal operators, constitutive laws take the form of kernels (integrand functions), whose functional form cannot be established a priori. Although the integral constitutive laws must be consistent with the classical eﬀective properties, they contain information about the small-scale response of the system and must be chosen to reproduce this response with the greatest ﬁdelity. In a few cases, certain forms of nonlocal kernels have been adopted in the engineering community because experimental evidence
3

conﬁrms the eﬃcacy of the model, or because a closed form of integrand that matches desired physical properties can be analytically determined. An example of the former situation is fracture mechanics, where peridynamic models have been demonstrated to be accurate [24, 25, 28]. Examples of the latter case include those diﬀusion processes in which the mean square displacement does not exhibit the linear, classical behavior, but instead exhibits an anomalous fractional behavior [29]. However, at present, only a few preliminary works address the problem of ﬁnding an optimal form for the kernel function [30–33].
In view of the growing importance of MD as a tool for designing materials with reduced reliance on laboratory testing, we propose to use nonlocal operators as upscaled continuous models for MD displacements. We seek nonlocal models that capture important aspects of the small-scale behavior better than classical homogenization theory. Building on our previous works [32, 33] we address the question of how to obtain large-scale nonlocal descriptions that capture MD behavior that would remain hidden in classical approaches to homogenization. To accomplish this, we use machine learning to identify optimal nonlocal kernel functions. The machine learning method is required to perform well with small datasets that may include thermal noise.
We summarize below our main contributions.
• We identify the best upscaled nonlocal model, without prior knowledge of the material properties, that accurately describes the material’s global behavior based on a small set of possibly noisy data.
• The optimal nonlocal model is guaranteed to be well-posed and generalizes well to settings that are substantially diﬀerent from the ones used for training. The optimal model is equally accurate for diﬀerent sources and geometries, so that it enables generalization.
While our ultimate goal is to learn a general integrand for the nonlocal operator, in this work we consider a speciﬁc nonlocal model, the Linear Peridynamic Solid (LPS) model [34] as a ﬁrst step towards a more general learning tool. We focus our experiments on single layered graphene for which we identify optimal two-dimensional nonlocal models.
Outline of the paper. Section 2 shows how to obtain, via smoothing functions, a nonlocal model for MD displacements. In Section 3 summarizes the peridynamic theory, the LPS model, and the discretization technique used in this work. Section 4 presents our learning approach include the well-posedness of the learned model by construction. It also provides the algorithmic workﬂow and implementation details. Section 5 illustrates the consistency of the proposed method on manufactured solutions. Section 6 demonstrates the eﬀectiveness
4

of the learning technique for MD displacements. We illustrate several properties including generalization with respect to loadings, domain size and shape. The eﬀect of thermal noise and the sensitivity of the algorithm to noise intensity are considered. Section 7 summarizes our contributions and provides future research ideas.

2. Coarse-graining of molecular dynamics displacements
In this section it is shown how to deﬁne an integral, continuous model for a system of particles. More details can be found in [35]. Similar results obtained with statistical mechanics can be found in [36].
Consider an assembly of mutually interacting particles in a crystal with particle mass Mε, ε = 1, 2, . . . , N . Let the reference positions of these particles be Xε and their displacement vectors Uε(t).
Suppose that any particle γ exerts a force Fεγ(t) on particle ε, and set Fεε = 0. These forces are assumed to be antisymmetric: Fγε(t) = −Fεγ(t), for all ε, γ, and t. It is also assumed that there is a cutoﬀ distance d for the atomic interactions such that Fεγ = 0 if |Xγ − Xε| > d. Each particle ε is subjected to a prescribed external force Bε(t).
For any continuum material point x ∈ Rn, deﬁne a smoothing function ω(x, ·) such that the following normalization holds:

ω(x, Xε) dx = 1
Rn

(2.1)

for any ε. For convenience, assume that at any x, ω(x, ·) has compact support over the ball BR(x), for R > 0. Deﬁne the smoothed mass density and body force density ﬁelds by

N
ρ(x) = ω(x, Xε)Mε,
ε=1

N
b(x, t) = ω(x, Xε)Bε(t),
ε=1

(2.2)

and the smoothed displacement ﬁeld by

1N

u(x, t) = ρ(x)

ω(x, Xε)MεUε(t).

ε=1

(2.3)

The evolution equation for the smoothed displacements will now be derived. Newton’s second law for the particles has the following form: for any ε

N
MεU¨ ε(t) = Fεγ(t) + Bε(t).
γ=1

(2.4)

5

Diﬀerentiating (2.3) twice with respect to time yields

N
ρ(x)u¨(x, t) = ω(x, Xε)MεU¨ ε(t).
ε=1

(2.5)

From (2.2), (2.4), and (2.5),

ρ(x)u¨(x, t) = =

N

N

ω(x, Xε)

Fεγ(t) + Bε(t)

ε=1

γ=1

NN

ω(x, Xε)Fεγ(t) + b(x, t).

ε=1 γ=1

(2.6)

From (2.1) and (2.6), for any x,

NN

ρ(x)u¨(x, t) =

ω(x, Xε)Fεγ(t)

ε=1 γ=1

ω(y, Xγ) dy + b(x, t),

or, equivalently,

ρ(x)u¨(x, t) = f (y, x, t) dy + b(x, t)

(2.7)

where

NN

f (y, x, t) =

ω(x, Xε)ω(y, Xγ)Fεγ(t)

ε=1 γ=1

(2.8)

and b is given by (2.2). The properties of F guarantee that the integrand f is antisymmetric. Since, by assumption, the smoothing functions have support radius R and the interatomic forces have cutoﬀ distance d, it follows that

|y − x| > δ =⇒ f (y, x, t) = 0

for all t, where the horizon δ is given by

δ = 2R + d.

(2.9)

In summary, deﬁning the displacements and other ﬁelds in the continuum description using the smoothing function ω, leads directly the nonlocal (or integral) equation of motion (2.7). However, the derivation does not provide a material model, that is, the dependence of f on the deformation in terms of the continuum displacement ﬁeld u is not yet determined. The goal of the present work is to identify an optimal form of the integrand function f in (2.7)

6

such that the corresponding nonlocal model faithfully represents given MD displacements under a given set of loading conditions on the MD grid.
At ﬁnite temperature, thermal oscillations in displacement are present in any MD simulation. The details of these oscillations are of no interest for purposes of continuum modeling. However, their net eﬀect on the bulk material properties must be included. To smooth out the thermal motions while retaining their net eﬀect, a time-smoothing method is applied. In this method, the following expression is applied to obtain the time-smoothed displacement Uε(tn) of atom ε:

Uε(0) = 0, Uε(tn) = (1 − ι)Uε(tn−1) + ιUε(tn), n > 0

(2.10)

where Uε is the unsmoothed displacement of atom ε and ι is a positive constant, typically on the order of 0.01. Then, to obtain displacements that are smoothed in both space and time, the displacement given by (2.10) is used in (2.3):

1 u(x) =

N
ω(x, Xε)MεUε(tF ),

ρ(x)

ε=1

(2.11)

where tF is the ﬁnal time of the MD simulation. The displacements u(x) contain noise in the form of spatial ﬂuctuations due to the impossibility of completely smoothing out all of the thermal oscillations in an MD simulation within a ﬁnite simulation time tF , regardless of the value of ι. The machine learning algorithm described below attempts to extract continuum material properties from the training data even in the presence of this noise. In Section 6, we will present results on the eﬀectiveness of this ML algorithm in treating this type of noisy training data.

3. Peridynamics
3.1. Peridynamics Background In the previous section, a coarse-grained continuum momentum balance was derived,
given by (2.7). This momentum balance has a fundamentally nonlocal character, since the pairwise bond force densities given by (2.8) can be nonzero whenever the material points in the continuum x and y are separated by a ﬁnite distance up to the horizon δ (Figure 1). This form of the momentum balance is known as the peridynamic equation of motion [37]. In peridynamics, each x interacts through bond forces with other material points y within a neighborhood with radius δ known as the family of x, denoted by Bδ(x). The equation of

7

motion for material point x is then

ρ(x)u¨(x, t) =

f (y, x, t) dy + b(x, t).

Bδ (x)

(3.1)

A material model in peridynamics supplies values of f (y, x, t) in terms of the deformations of the families of x and y and any other relevant variables such as temperature. In general, material models in peridynamics are speciﬁed using operators called states that are nonlocal analogues of second order tensors [34]. Many material models have been developed for peridynamics, and any material model from the local theory can be translated into peridynamic form [38]. The most widely used capability that peridynamics oﬀers that is not available in the local theory is the direct modeling of fracture within the basic ﬁeld equations. Peridynamics can model fracture because the equation of motion (3.1) is an integro-diﬀerential equation that does not involve the partial derivatives of displacement with respect to position. However, the present paper concerns only small deformations in the linear regime of material response and does not address fracture. The extension of the methods described here to determine a linear peridynamic material model to the nonlinear regime, including fracture, is under investigation in separate work. The remainder of this paper deals with a speciﬁc material model described in the next section. Note that even though MD displacements are dynamic, we smooth them in time as described in (2.11) so that the time-space smoothed MD data can be described by the static counterpart of the nonlocal equation (2.7),

−

f (y, x, u) dy = b(x),

Bδ (x)

(3.2)

where f is to be determined (see the following section) and where we introduced the nonlocal interaction region Bδ(x). The horizon δ determines the extent of the nonlocal interactions. Although, according to Section 2, δ could be determined by the cutoﬀ distance d associated with F and the radius of the smoothing function R, in the following discussion it is treated as a learned parameter (Section 6.2). This approach allows for a ﬁnite value of δ to be obtained even if d = ∞, as would be the case with the Lennard-Jones potential.

3.2. The Linear Peridynamic Solid (LPS) Model
The main application considered in this work is the simulation of displacements in singlelayered graphene. The graphene sheet is treated using a two-dimensional nonlocal model under the assumption of plane stress, which is appropriate for a thin sheet. The pairwise bond force density f is determined using the state-based linear peridynamic solid (LPS) model. The LPS model is a prototypical state-based model appropriate for isotropic elastic

8

Figure 1: Left: the family of a point x in a peridynamic body. Right: typical bond and bond force vector.

materials. It may be regarded as a nonlocal generalization of the local model for an isotropic solid, which contains contributions from shear and dilatation. The LPS model has advantages over the previously developed bond-based peridynamic models in that it is not restricted to a Poisson’s ratio of 1/4. The LPS model has known well-posedness properties under certain assumptions [24]. This section summarizes the mathematical formulation for the LPS model and illustrates a meshfree discretization [39–44].
Consider a 2D body occupying the domain Ω ⊂ R2, and let θ be the nonlocal dilatation, generalizing the local divergence of the displacement. Let K(r) be the inﬂuence function [45] which modulates nonlocal eﬀects within a peridynamic model. In this work, we assume K to be a radial function compactly supported on the δ-ball Bδ(x) with α-th order singularity:

P (|x − y|) K(x, y) := K(|x − y|) = |x − y|α

(3.3)

where P (r) is a bounded function in [0, δ]. The momentum balance and nonlocal dilatation are given by

LKu(x) := − C1 m(δ)
− C2 m(δ)

(λ − µ) K(|y − x|) (y − x) (θ(x) + θ(y)) dy

Bδ (x)

(y − x) ⊗ (y − x)

µK(|y − x|)
Bδ (x)

|y − x|2

(u(y) − u(x)) dy = b(x),

(3.4)

2

θ(x) :=

K(|y − x|)(y − x) · (u(y) − u(x)) dy,

m(δ) Bδ(x)

where u ∈ R2 denotes the displacement, b ∈ R2 the prescribed body force density, and m(δ)

the weighted volume. In the present notation, the nonlocal operator LK[u](x) in (3.4) has

the subscript K to emphasize its dependence on the inﬂuence function K. This operator

corresponds to the integral term − f (y, x, u)dy in (2.7), or, equivalently, (3.2).

Given a forcing term b, in order to guarantee the existence of a unique solution u,

9

“nonlocal boundary conditions”, or volume constraints, must be prescribed on an appropriate interaction domain ΩI, so that the LPS problem becomes

LK[u](x) = b(x) x ∈ Ω, BIu(x) = q(x) x ∈ ΩI.

(3.5)

Here, BI is a nonlocal interaction operator specifying a volume constraint. In this work, without loss of generality, we consider the Dirichlet condition BI = I, where I is the identity operator. Other types of conditions, e.g., Neumann [40, 42, 46, 47], Robin [43, 48] or periodic [32], are also compatible with our learning algorithm.

A meshfree discretization of the LPS model. Given a collection of material points X = {xi}i=1,2,...,Np, we numerically evaluate LK(u) by employing the meshfree, particle discretization introduced in [40], which features ease of implementation and eﬃciency. At each material point xi, we adopt the following quadrature rule to approximate the integral in LK in (3.4), which we now denote by LhK.

Lh u(x ) := − C1

K

i

mi(δ)

(λ − µ) Kij (xj − xi) θh(xi) + θh(xj) Wj,i

xj ∈Bδ(xi)

C2

(xj − xi) ⊗ (xj − xi)

− mi(δ)

µKij

xj ∈Bδ(xi)

|xj − xi|2

(u(xj) − u(xi)) Wj,i = b(xi),

(3.6)

θh(x ) := 2 i mi(δ)

Kij(xj − xi) · (u(xj) − u(xi)) Wj,i,

xj ∈Bδ(xi)

(3.7)

where Kij := K(xj, xi) and mi(δ) :=

Kij|xj − xi|2Wj,i. The quadrature weights

xj ∈Bδ(xi)

Wj,i are obtained for material points on diﬀerent subdomains Bδ(xi), from the following

optimization problem

argmin

Wj2,i

{ωj,i} xj ∈Xh∩Bδ(xi)\{xi}

s. t.,

q(xi, xj)Wj,i =

q(xi, y)dy

xj ∈Bδ(xi)

Bδ (xi )

∀q ∈ V,

where V denotes the space of functions which should be integrated exactly. Following [40], in this work we take V := q(y) = |yp−(yx)|3 | p ∈ P5(R2) such that Bδ(x) q(y)dy < ∞ and P5(R2) denotes the space of quintic polynomials .
Although the developed learning approach as well as the meshfree quadrature rule can be
applied to the general collection of material points X , in this work we consider the uniform

10

Cartesian grid for simplicity:
Xh := {(p1h, p2h)|p = (p1, p2) ∈ Z2} ∩ (Ω ∩ ΩI ),
where h is the spatial grid size. As we discuss in the next section and in Section 6.4, diﬀerent grids Xh can be used for training and validation sample collection.
Remark 1. Using the same arguments as in [41], it can be seen that the chosen quadrature rule provides a consistent approximation of LK(u) when α < 3. Therefore, in the learning algorithm, we require the fractional order α to be bounded by 3, and we note that this requirement may be further relaxed by considering other discretization methods.
4. Operator regression for the LPS model
In this section we illustrate how to extend the data-driven approach developed in [32, 33], known as nonlocal operator regression, to the LPS model. Section 4.1 ﬁrst describes the regression algorithm under the assumption that coarse-grained MD displacements are available at any material point. Then, Section 4.2 introduces solvability constraints that guarantee that the optimal nonlocal model is well-posed by construction1. Lastly, Section 4.3 summarizes the complete workﬂow of the data-driven nonlocal operator regression algorithm, which is the process of going from high ﬁdelity MD simulations to data-driven optimal kernels via coarse graining and operator regression.
4.1. Operator regression algorithm The foundation of the nonlocal operator regression algorithm is the fact that a coarse-
grained displacement u follows a nonlocal evolution law of the form (3.5). For this nonlocal model, we seek to identify an optimal constitutive relation on the basis of MD data sets.
Let {us(xi,s), bs(xi,s)}, s = 1, · · · , S, be given pairs of displacement and body force ﬁelds available at xi,s ∈ X s, and let LK be the LPS operator deﬁned in (3.4) parametrized by the material properties λ and µ and by the inﬂuence function K. We aim to learn an optimal nonlocal operator LK. This optimal operator consists of the inﬂuence function K, which may be sign-changing, and parameters λ and µ, such that the action of LK most closely maps us(x) to bs(x) for all s. Formally, the optimal inﬂuence function and parameters,
1The solvability conditions derived in [32] for 1D nonlocal diﬀusion problems are not applicable to the more complex LPS model considered in the current work.
11

(λ∗, µ∗, K∗), are the solution of the following optimization problem:

(K∗, λ∗, µ∗) = argmin 1 S λ,µ,K S s=1

LhK [us](xi,s) − bs(xi,s) 22(X s).

To increase the ﬂexibility of the algorithm, each sample can be available on diﬀerent point sets X s.
The inﬂuence function K(x, y) will now be parameterized. Following [32], assume that K has the form of (3.3), and represent its numerator P as a linear combination of Bernstein polynomials evaluated at |x − y|:

M Dk

|x − y|

K(x, y) = |x − y|α Bk,M δ .

k=0

Here the Bernstein polynomials are deﬁned as

Bk,M (r) = M rk(1 − r)M−k, k

for 0 ≤ r ≤ 1.

To allow the learning of nonlocal models whose kernels may be partially negative, we allow Dk ∈ R, for all k. This generality, however, might compromise the well-posedness of the resulting optimal model, since known well-posedness results on LPS models only apply to positive kernels [49]. To guarantee that the LPS model associated with (λ∗, µ∗, K∗) is solvable by construction, we embed in our algorithm suﬃcient well-posedness conditions for the discretized operator; these are described in detail in the next section.
The formulation of the constrained optimization problem is as follows. Given a collection of training samples {us(xi,s), bs(xi,s)}, s = 1, · · · , S, we seek to learn the parameters λ and µ, the Bernstein polynomial coeﬃcients D = [D0, · · · , DM ] ∈ RM+1, the order α, the horizon δ, and the polynomial order M by minimizing the mean square loss (MSL) of the LPS equation:



1S

(λ∗, µ∗, D∗, α∗, δ∗, M ∗) = argmin λ,µ,D,α,δ,M S

||LhK us(xi,s) − bs(xi,s)||22(X s)

s=1

 s.t. solvability constraints.

(4.1)

4.2. Solvability Constraints for the Discretized LPS Model When the inﬂuence function K as described in (3.3) is nonnegative and α < 4, the
LPS model is well-posed, as shown, for example, in [49]. However, several works have indicated the practical need for sign-changing kernels [30, 31, 33, 50, 51]. While it is unclear

12

whether multiscale physics inherently leads to sign-changing kernels or if equally descriptive positive kernels could be derived, in [32] the authors found that allowing for sign-changing kernels provides a signiﬁcant increase in accuracy when modeling high-frequency material response. Therefore, in this work we seek a well-posed LPS model with possibly sign-changing inﬂuence functions K. As there is no available theory on suﬃcient conditions for the wellposedness of LPS models with sign-changing K, we impose well-posedness conditions on the discretized system directly; as a result, well-posedness of the learnt model is guaranteed for the discretization method used during training.
An inequality constraint for the well-posedness of the meshfree discretization approach (3.6)-(3.7) that allows for sign-changing inﬂuence functions will now be derived. For simplicity of analysis, and without loss of generality, assume homogeneous Dirichlet-type boundary conditions: u(x) = 0 in ΩI.
For the derivation of the solvability constraints that will be employed in our algorithm to ensure the well-posedness of the discretized LPS model, ﬁrst write the discretized LPS model (3.6)-(3.7) as the following linear system:

µΓ (Φ)t Φ − λ−1 µ I

U

−B

=

.

Θ

0

(4.2)

Here, U ∈ R2Np and λ−1µ Θ ∈ RNp are the vectors of the degrees of freedom (DOFs) of the displacement u and the nonlocal dilatation θ:

U = [(u(x1))t, · · · , (u(xNp))t]t, Θ = [(λ − µ)θ(x1), · · · , (λ − µ)θ(xNp)]t.

B is the vector of DOFs of the body load and has the same length and ordering of indices as U. I is an Np×Np identity matrix, and Γ and Φ are the matrices that correspond to the deviatoric and dilatation contributions of the deformation:

ΓU = [(fdev(x1))t, · · · , (fdev(xNp))t]t, ΦtU = [fdil(x1), · · · , fdil(xNp)]t,

where

C2

(xj − xi) ⊗ (xj − xi)

fdev(xi) = − mi(δ) Kij xj ∈Bδ(xi)

|xj − xi|2

(u(xj) − u(xi)) Wj,i

2

fdil(xi) = mi(δ)

Kij(xj − xi) · (u(xj) − u(xi)) Wj,i.

xj ∈Bδ(xi)

In what follows, for each vector V ∈ R2Np, write V = [v1t , · · · , vNt p]t with each vi ∈ R2.

13

C denotes a generic constant. The following theorem provides suﬃcient conditions that guarantee the solvability of (4.2). Let the energy “norm” be deﬁned as ||V||2E := VtΓV for all V ∈ E \ 0, where E denotes the quotient space of R2Np by the discrete space of
inﬁnitesimally rigid displacements:

ΠX = {[(Qx1 + d)t, · · · , (QxNp + d)t]t, Q ∈ R2×2, QT = −Q, d ∈ R2}.

Note that · E is indeed a norm only when certain conditions, reported in the following theorem, are satisﬁed.
Theorem 4.1 (Suﬃcient Conditions for Solvability). The discretized LPS formulation (4.2) is solvable for any values of λ + µ > 0 and µ > 0, provided that the following conditions hold:

i. Discrete Continuity and Coercivity: ∃ CΓ > 0, CΓ < ∞ s.t., ||V||2E ≥ CΓ||V||22 and ||V||2E ≤ CΓ||V||22 ∀ V ∈ E\{0}

VtΦtP

ii. Discrete Inf-Sup: ∃ CΦ > 0, s.t., inf sup

≥ CΦ,

P∈RNp \{0} V∈E\{0} ||V||E ||P|| 2

iii. Discrete, generalized Cauchy-Schwarz: ||V||2E ≥ 2||ΦV||22, ∀ V ∈ E.

(4.3) (4.4) (4.5)

Remark 2. In the continuous case with K ≥ 0, the property (4.5) is an immediate result from the Cauchy-Schwarz inequality, see, for example, [49]. This property yields the equivalence of the semi-norm from the deviatoric part of the deformation and the full strain energy.

Proof. Inequality (4.3) implies that the energy norm is a norm in E \ {0}. We consider two scenarios: λ − µ ≥ 0 and λ − µ < 0. Case 1: λ − µ ≥ 0. The symmetry property of the Cartesian grids implies that mi(δ) = mj(δ) := m and Wj,i = Wi,j for all i, j ∈ {1, · · · , Np}. By taking the inner product of (4.2) with (Vt, Ξt), component-wise, we reformulate the system as a general mixed formulation: ﬁnd U ∈ R2Np and Θ ∈ RNp such that

a(U, V) + b(V, Θ) = (−B, V), b(U, Ξ) − c(Θ, Ξ) = 0,

∀ V ∈ R2Np ∀ Ξ ∈ RNp.

Here (·, ·) denotes the inner product, a(U, V) := µVtΓU, b(V, Ξ) := ΞtΦV, and c(Θ, Ξ) := λ−1µ ΘtΞ. Firstly, note that when (4.3) is satisﬁed, the symmetric bilinear form a(·, ·) is

14

continuous and coercive. Similarly,





Np

b(V, Ξ) = − C1  m

(λ − µ) Kijvit (xj − xi) (ξi + ξj) Wj,i

i=1 xj ∈Bδ(xi)





Np

= C1  2m

(λ − µ) Kij(vj − vi)t (xj − xi) (ξi + ξj) Wj,i

i=1 xj ∈Bδ(xi)

≤C||V|| 2||Ξ|| 2 ≤ C||V||E||Ξ|| 2

so that b(·, ·) is also a continuous bilinear form. By combining (4.3) and (4.4) with the fact that c(Θ, Ξ) = λ−1µ ΘtΞ ≤ C||Θ|| 2||Ξ|| 2 and c(Ξ, Ξ) = λ−1µ ||Ξ||22 ≥ 0, the well-posedness of (4.2) follows using the same arguments of [52, Section 2.2]. Case 2: λ − µ < 0. In this case c(Ξ, Ξ) is not coercive with respect to the 2 norm and
therefore the theory in [52] does not apply. By reducing the discrete system (4.2) to ((λ − µ)ΦtΦ + µΓ)U = −B, we note that (λ − µ)ΦtΦ + µΓ is not solvable, or, equivalently, noninvertible, if and only if there exists V ∈ E\{0} such that ((λ − µ)ΦtΦ + µΓ)V = 0. We prove that this is not possible by showing that

Vt((λ − µ)ΦtΦ + µΓ)V = 0 ⇔ V = 0.

We split the proof in two parts: λ ≥ 0 and λ < 0, respectively. First, for λ ≥ 0, (4.3) and (4.5) yield

0 =Vt((λ − µ)ΦtΦ + µΓ)V = λVtΦtΦV + µ(||V||2E − ||ΦV||22) ≥λVtΦtΦV + µ2 ||V||2E ≥ µ2 ||V||22.

Hence V = 0, which contradicts our assumption. Similarly, when λ < 0, we assume that ||V||2E > 0. From the assumptions λ − µ < 0 and λ + µ > 0 we have

0 =Vt((λ − µ)ΦtΦ + µΓ)V ≥ λ − µ VtΓV + µ||V||2 = λ + µ ||V||2 ,

2

E

2

E

which, again, implies V = 0, contradicting our assumption. Therefore, (λ − µ)ΦtΦ + µΓ is invertible and (4.2) is solvable.

Remark 3. When the inﬂuence function K is nonnegative, the continuous LPS model satisﬁes the ellipticity condition and the inf-sup condition. Moreover, the energy density associated with the dilatation part is bounded by energy density associated with the deviatoric

15

part of the deformation, as shown in [49]. These facts, intuitively, support our well-posedness result, where the constraints (4.3), (4.4) and (4.5) are equivalent to requiring that the discretized LPS model associated with a sign-changing K satisﬁes the same three properties as its nonnegative-kernel counterpart.
Therefore, we augment our learning problem (4.1) with the inequality (solvability) constraints corresponding to (4.3), (4.4) and (4.5). The constants CΓ, CΓ, and CΦ are only related to the inﬂuence function K and are independent of the shear and Lam´e parameters λ and µ. We calculate the inf-sup constant CΦ in (4.4) by solving an eigenvalue problem, as indicated by the following theorem.
Theorem 4.2. The inf-sup constant in (4.4) in Theorem 4.1 can be expressed as
CΦ = Λmin(ΦΓ†Φt)
where, for a given a square matrix M , M † denotes its pseudoinverse and Λmin(M ) denotes its smallest nonzero eigenvalue.
Proof. The proof can be obtained following the same arguments as in [53, Proposition 3.1].

For any given K as in (3.3) and a ﬁxed discretization method, the largest eigenvalue of Γ is bounded by construction. Therefore CΓ is ﬁnite and, in practice, (4.3), (4.4) and (4.5) can be imposed as:

Λmin(Γ) ≥ ζ, Λmin(ΦΓ†Φt) ≥ ζ, Λmin(Γ − 2ΦtΦ) ≥ 0,

(4.6)

where ζ > 0 is a given, small number. We can now state the solvability-constrained optimization problem. Rename the matrices
Γ and Φ in (4.2) as Γ(α,D,δ,M) and Φ(α,D,δ,M) indicating that they are parameterized with the Bernstein polynomial coeﬃcients D, the fractional order α, the horizon δ, and the highest Bernstein polynomial order M . Given the tolerance parameter ζ > 0, the learning problem

16

is stated as



1S

(λ∗, µ∗, D∗, α∗, δ∗, M ∗) = argmin

 

λ,µ,D,α,δ,M S

||LhK us(xi,s) − bs(xi,s)||22(X s)

 

s=1





subject to: λ + µ > 0, µ > 0, α < 3, Λmin(Γ(α,D,δ,M)) ≥ ζ,

   

Λmin(Φ(α,D,δ,M)Γ†(α,D,δ,M)Φt(α,D,δ,M)) ≥ ζ ,







 

Λmin(Γ(α,D,δ,M) − 2Φt(α,D,δ,M)Φ(α,D,δ,M)) ≥ 0.

(4.7)

Remark 4. The constraints in (4.6) are suﬃcient to guarantee that the model associated with the optimal inﬂuence function K is well-posed only when discretized with the same technique and resolution utilized during training. Being only suﬃcient, these conditions may yield an optimal inﬂuence function whose associated model is still well-posed when discretized with diﬀerent schemes or resolutions. More discussion and numerical tests on this topic can be found in Section 6.4.

4.3. Algorithm and Workﬂow
In this section we describe the algorithmic details of our learning approach and describe the learning workﬂow that, starting with MD displacements, delivers the optimal inﬂuence function K and the material parameters.
Numerically, the constrained optimization problem (4.7) poses several challenges. Firstly, the quadrature weights Wj,i generated in the preprocessing generally depend on the horizon size δ, which hinders the application of a suite of continuous optimization techniques such as gradient descent or Adam. A similar issue applies to M . Moreover, due to the solvability constraints, (4.7) is expected to be nonconvex and likely to exhibit local minima. Lastly, the numerical evaluations of the eigenvalues are time-consuming. For all these reasons, for the sake of numerical eﬃciency, we treat δ and M as hyperparameters to be separately tuned to achieve the best learning accuracy without overﬁtting. As suggested by [32], the optimization problem (4.7) is split into a prediction step (without constraints) and a correction step (with constraints), and propose a “two-stage” strategy, whose key components are summarized in Algorithm 1.
The prediction step of the algorithm relies on the fact that, as shown in [49], when α < 3, λ + µ > 0, µ > 0 and K ≥ 0, the LPS problem is guaranteed to be well-posed, and therefore no additional solvability constraint of K are required. Therefore, in the prediction step, we ﬁnd a set of nonnegative Bernstein coeﬃcients that will be used as an initial guess for the second, correction step. The nonnegative coeﬃcients and the corresponding nonnegative inﬂuence function are denoted by Dpkre and Kpre, respectively. These are obtained by solving

17

Algorithm 1 Two-stage strategy to solve (4.7) for (λ∗, µ∗, α∗, D∗).

1: With ﬁxed δ and M , initialize λ, µ, α, Dk(0) ∼ U (0, 1),where U (a, b) denotes the uniform distribution on (a, b).
2: Obtain (λpre, µpre, αpre, Dpre) as a local minimum of Lpre(λ, µ, α, D), using the Adam

optimizer while updating λ ← ReLU(λ), µ ← µ, α ← 3 − ReLU(3 − α) and D ←

ReLU(D) after each step of gradient descent.

3: Initialize (λ(0), µ(0), α(0), D(0)) = (λpre, µpre, αpre, Dpre) and

(0) 1

=

(0) 2

=

(0) 3

=

1.

4: Set STEP MAX = 100, φ1 = φ2 = φ3 = 0, ψ = 1, r1 = 5, r2 = 1/4, = 10−8.

5: while j ≤ STEP MAX: do

Perform Augmented Lagrangian Algorithm

6: Solve the unconstrained optimization problem

(λ(j), µ(j), α(j), D(j), (j)) = argmin Lcorr(λ, µ, α, D, ).
λ,µ,α,D,

7: if Hp(α(j), D(j), (j)) ≤ for all p ∈ {1, 2, 3}, then

8:

Stop.

9: else

10:

if ∃ p ∈ {1, 2, 3} s.t. Hp(α(j), D(j), (j)) ≥ r2Hp(α(j−1), D(j−1), (j−1)) then

11:

Update penalty ψ ← r1ψ.

12:

if ψ ≥ 1020 then

13:

Stop.

14:

else

15:

Update Lagrange multiplier φp ← φp + ψHp(α(j), D(j), (j)) for p = 1, 2, 3.

16: Update the iteration number j ← j + 1.

17: (λ∗, µ∗, α∗, D∗) = (λ(j), µ(j), α(j), D(j)).

18

the following, unconstrained problem, whose full solution is denoted by (λpre, µpre, αpre, Dpre).

Lpre(λ, µ, α, D) = 1 S

S s=1 xi∈X s

s M Dk bi +

Bk,M |xj −δ xi| Wj,i

mi(δ)

|xj − xi|α

k=0

xj ∈Bδ(xi)

2

C (λ − µ) (x − x ) θs + θs + C µ (xj − xi) ⊗ (xj − xi) us − us ,

1

j

ii

j

2

|xj − xi|2

j

i

where

s M 2Dk θi :=

Bk,M

|xj −xi| δ

Wj,i (xj − xi) · usj − usi ,

mi(δ)

|xj − xi|α

k=0

xj ∈Bδ(xi)

M

Bk,M |xj −xi| Wj,i

δ

|xj − xi|2.

mi(δ) := Dk

|xj − xi|α

k=0 xj ∈Bδ(xi)

We solve this unconstrained optimization problem via the Adam optimizer [54] and we guarantee that D ≥ 0 by using the map D → ReLU(D) after each step of gradient descent. Here ReLU denotes the rectiﬁed linear unit function:

ReLU(x) :=

0, for x ≤ 0; x, for x > 0.

This operation does not modify the value of Lpre and ensures that the sequence of iterates

and the local minimum are nonnegative. Similar strategies are also applied to λ, µ and α as

follows2:

µ → ReLU(µ),

λ → ReLU(λ + µ) − µ,

(4.8)

α → 3 − ReLU(3 − α).

The second step of the algorithm corrects the prediction-step solution by solving for the fully constrained optimization problem. Speciﬁcally, by employing (λpre, µpre, αpre, Dpre) as the initial guess, we apply the augmented Lagrangian method [55, 56] and treat the inequality

2Note that although we require λ + µ > 0, µ > 0 and α < 3 for the well-posedness analysis in Theorem 4.1, for numerical simplicity we employ strategy (4.8) which only guarantees λ + µ ≥ 0, µ ≥ 0 and α ≤ 3. However, in all numerical tests we observed that the predicted λ, µ, α satisﬁes the well-posedness conditions, as will be shown in Section 6.

19

constraints via slack variables. To do this, introduce the functions

 H1(α, D, 1) =Λ(Γ(α,D,δ,M)) − ζ − 12,  

H2(α, D, 2) =Λ(Φ(α,D,δ,M)Γ+(α,D,δ,M)Φt(α,D,δ,M)) − ζ − 22,



 

H3(α, D,

3) =Λ(Γ(α,D,δ,M) − 2Φt(α,D,δ,M)Φ(α,D,δ,M)) −

32.

Here, = [ 1, 2, 3] is the vector of slack variables arising from the inequality constraints. Then, minimize the following penalized loss function:

Lcorr(λ, µ, α, D,

1 )=
S

S s=1 xi∈X s

s M Dk bi +

Bk,M |xj −δ xi| Wj,i

mi(δ)

|xj − xi|α

k=0

xj ∈Bδ(xi)

2

C (λ − µ) (x − x ) θs + θs + C µ (xj − xi) ⊗ (xj − xi) us − us

1

j

ii

j

2

|xj − xi|2

j

i

3

ψ3

+ φpHp(α, D, ) +

Hp2(α, D, ),

2

p=1

p=1

where φp, p = 1, 2, 3 are the Lagrange multipliers and ψ is a penalty parameter. At this stage, the Adam optimizer is used. An iterative procedure updates the Lagrange multipliers by 1) solving the unconstrained optimization problem, 2) updating φp via dual gradient ascent
φp → φp + ψHp(α, D, ),
and 3) increasing ψ when the decreasing ratio of Hp(α, D, ) (with respect to the last iteration) does not reach 4 for at least one p ∈ {1, 2, 3}. As done in the prediction step, at each iteration of the gradient descent, we map µ to ReLU(µ), λ to ReLU(λ + µ) − µ, and α to 3 − ReLU(3 − α). Further details on parameter updates are described in Algorithm 1. The optimal solution is denoted by (λ∗, µ∗, α∗, D∗).
In applying Algorithm 1, in all our tests, we run the Adam optimizer in PyTorch using a batch size of 70, the inequality constraint tolerance is set to =1E-5 and the learning rate to 1E-3. The algorithm runs until the loss stagnates, indicating that a stationary point has been reached. Stagnation typically happens between 1000 and 2000 epochs for the ﬁrst stage of the algorithm and at ∼500 epochs for the second stage at each iteration of the augmented Lagrangian method.
We now discuss how to tune δ and M , which are treated as hyperparameters. Divide the sample set into two sub-sets: the set of training samples Strain := {us(xi,s), bs(xi,s)}, s = 1, · · · , Strain, and the set of validation samples Sval := {us(xi,s), bs(xi,s)}, s = 1, · · · , Sval.

20

Algorithm 2 Workﬂow for learning the operator LK from MD displacements. 1: Generate MD displacements on ﬁne grids {Xεs} using diﬀerent external forcings and domains conﬁgurations and group the samples in three data sets:

MDtrain/val/test := {Mεs,s, Usε,s(t), Bsε,s(t)}, s = 1, · · · , Strain/val/test.

2: Smooth the data sets MDtrain/val/test in space and time and evaluate the smoothed data at coarser grids X s to obtain the sets

Strain/val/test := {us(xi,s), bs(xi,s)}, s = 1, · · · , Strain/val/test.

3: for M ∈ M: do

4: for δ ∈ D: do

5:

Perform the two-stage optimization strategy for ﬁxed (δ, M ) to obtain

(λ∗(δ,M), µ∗(δ,M), α(∗δ,M), D∗(δ,M)).

6: Find δM∗ that minimizes Res(δ, M ; Strain). 7: Calculate and store ERtreasin(δM∗ , M ), Eutrain(M ), ERvaels(M ) and Euval(M ). 8: Find M ∗ that minimizes the average of the normalized errors in step 7 and set
(λ∗, µ∗, α∗, D∗, δ∗, M ∗) = (λ∗(δM ∗ ∗ ,M∗), µ∗(δM ∗ ∗ ,M∗), α(∗δM ∗ ∗ ,M∗), D∗(δM ∗ ∗ ,M∗), δM∗ ∗ , M ∗) L∗K = LK(δ∗ ∗ ,M ∗)
M

21

While all samples correspond to the same material, the problem setting of training and
validation samples can be substantially diﬀerent. For example, they could be generated with diﬀerent grids X s, loading scenarios, boundary conditions, and geometric conﬁguration. For
M ∈ M, we perform Algorithm 1 using the training set Strain with diﬀerent values of δ ∈ D and denote the corresponding nonlocal operator by LK(δ,M). Then, for each M , deﬁne the optimal horizon δM∗ as the one that minimizes the average residual of the LPS equation, denoted by ERtreasin. Formally,

δM∗ = argmin ERtreasin(δ, M ) = argmin Res(δ, M ; Strain),

δ∈D

δ

where

1 Res(δ, M ; Strain) := Strain

||LhK(δ,M)us(xi,s) − bs(xi,s)||22(X s).

{us (xi,s ),bs (xi,s )}∈Strain

Denote the corresponding nonlocal operator by LK(δ∗ ,M). Next, to determine the optimal M
M that allows for accurate representation of the training samples without overﬁtting, we
test, for each M , the optimal operator LK(δ∗ ,M) on the validation set Sval by evaluating the M
average residual of the LPS equation; the latter, denoted by ERvaels, is deﬁned as

ERvaels(M ) = Res(δM∗ , M ; Sval).

In principle, small training and validation losses indicate that the model performs well on the training set and generalizes well to other data sets. As an additional metric of accuracy on both the training and validation sets, also consider the displacement mean square error (MSE), denoted by Eutrain and Euval, respectively. Formally,

Etrain(M ) := 1

u

Strain

||us(xi,s) − (LhK(δM ∗ ,M))−1bs(xi,s)||22(X s),

{us (xi,s ),bs (xi,s )}∈Strain

and Euval is deﬁned similarly by taking the average over the validation set Sval. Based on these metrics, we select M such that the average of the normalized ERtreasin(δM∗ , M ), Eutrain(M ), ERvaels(M ) and Euval(M ) is minimized. Here the normalization is taken with respect to the
same quantities evaluated at the baseline, which is the case with M = 0 (the constant
Bernstein polynomial). In particular, take M ∗ = argmin AvgE(M ) where
M

1 ERtreasin(δM∗ , M ) Eutrain(δM∗ , M ) ERvaels(δM∗ , M ) Euval(δM∗ , M )

AvgE(M ) := 4

Etrain(δ∗, 0) , Etrain(δ∗, 0) , Eval (δ∗, 0) , Eval(δ∗, 0)

.

Res 0

u

0

Res 0

u0

22

10 2 10 0
0

Learned kernel, h = 5 Learned kernel, h = 2.5 Learned kernel, h = 1.25

5

10

15

20

4 10 -4 3 2 1 0
0

Manufactured kernel Learned kernel, h = 5 Learned kernel, h = 2.5 Learned kernel, h = 1.25

5

10

15

Figure 2: Consistency tests for Algorithm 1 with ﬁxed α = 1 and positive inﬂuence function. Left: The training loss versus basis order M when using diﬀerent grid size; the dashed lines indicate the values of the loss functions when the manufactured kernel is used, colors reﬂect the values of h used for discretization. Right: The comparison of learned inﬂuence functions and the manufactured inﬂuence function Kman = 1r .

10 2 Learned kernel, h = 5

Learned kernel, h = 2.5

10 1

Learned kernel, h = 1.25

10 0

2 10 -4 1.5
1 0.5

Manufactured kernel Learned kernel, h = 5 Learned kernel, h = 2.5 Learned kernel, h = 1.25

10 -1

0

0

5

10

15

20

0

5

10

15

Figure 3: Consistency tests for Algorithm 1 and positive inﬂuence function, while learning α. Left: The training loss versus basis order M when using diﬀerent grid size; the dashed lines indicate the values of the loss functions when the manufactured kernel is used, colors reﬂect the values of h used for discretization. Right: The comparison of learned inﬂuence functions and the manufactured inﬂuence function Kman = 1r .
A summary of the above strategy can be found in Algorithm 2 where we report the overall workﬂow of our learning procedure.

5. Consistency tests for manufactured solutions
In this section, we test our operator regression algorithm by considering analytic training pairs {us(x), bs(x)} satisfying (3.4) for the manufactured inﬂuence function
1 Kman(x, y) = |x − y| .
In particular, data sets are considered with diﬀerent spatial resolutions Xh := {(p1h, p2h)|p = (p1, p2) ∈ Z2} ∩ (Ω ∩ ΩI) and diﬀerent Bernstein polynomial orders M with the purpose of validating Algorithm 1 before employing it on MD data sets.
For the computational domain Ω = [0, 100]2, the training data pairs {us(x), bs(x)}7s=0 1

23

are generated by setting
u(x1, x2) = (0.1 cos(2k1πx1) cos(2k2πx2), 0), or u(x1, x2) = (0, 0.1 cos(2k1πx1) cos(2k2πx2)),
with k1, k2 ∈ {0, 1, 2, 3, 4, 5}. Then, for each displacement ﬁeld us(x), the corresponding forcing ﬁeld bs is computed from (3.4) with λ = 0.1010, µ = 0.4545, and δ = 0.125. By evaluating us and bs on diﬀerent grid sets Xh with h = 5, h = 2.5 and h = 1.25, respectively, three training sets of size 70 are then obtained. These are denoted by Shtr=a5in, Shtr=a2in.5 and Sthr=a1in.25.
To verify the consistency of the prediction step in Algorithm 1 and its behavior with respect to increasing resolution and polynomial order, ﬁrst consider a positive inﬂuence function K with prescribed fractional order α = 1. Then choose the Bernstein basis order M in [0, 20]. The prediction step involves the solution of a convex optimization problem with a non-empty feasible set. Therefore, every local minimum is a global minimum.
For diﬀerent training sets, the training losses ERvaels(M ) are plotted with respect to increasing polynomial orders M in Figure 2, left. These results suggest that the training loss improves as the basis order M is increased and as the grid size h decreases. Furthermore, the manufactured ground-truth kernels have a higher training losses compared to the learned kernels due to the discretization error, which suggests that the learning algorithm is able to obtain better kernels on each grid set. Figure 2, right, shows a comparison of the learned inﬂuence functions for a ﬁxed polynomial order M = 10. The learned inﬂuence function gets closer to the manufactured inﬂuence function Kman as h → 0. This illustrates the consistency of the learning algorithm for a given α.
To investigate the eﬀects of the fractional order α, we now consider a positive inﬂuence function with unknown fractional order and use, again, the prediction step of Algorithm 1. In this case, the convergence of the optimal kernel to the manufactured one is not guaranteed, since the fractional order is a tunable parameter. The training losses and learnt inﬂuence functions are provided in Figure 3. Although the algorithm does not recover exactly the inﬂuence function Kman, low values of ERtreasin can be achieved as M increases.
6. Application to graphene using MD
To illustrate the eﬃcacy of our method in obtaining an optimal peridynamic model from coarse-grained MD displacements, we consider graphene sheets as the application. Graphene is a two-dimensional form of carbon with a hexagonal structure. Because of its high stiﬀness and strength, as well as other unusual physical properties, graphene is being studied for
24

Figure 4: Left: hexagonal graphene atomic structure. Center: full MD grid. Right: coarse grained node positions.

possible use in a number of applications, including as a structural material. For the present

study, an MD model was created using the Tersoﬀ interatomic potential [57]. This potential is

widely used in the MD community for graphene because it incorporates the relative rotation

angle between covalent bonds, which strongly aﬀects the mechanical response. A thermostat

is included in the MD model in the present study to control the temperature and periodic

boundary conditions are applied. The MD grid is shown in Figure 4, center. The grid has

3588 atoms. The corresponding coarse-grained node positions are shown in Figure 4, right.

To simplify the analysis, out-of-plane motions were not considered in this study, although

they would occur in a real material.

Unstressed graphene nominally has an interatomic spacing of 1.46˚A. In this study, values

of the coarse-grained quantities ui and bi are evaluated on a square lattice of nodes indexed by i with spacing h=5.0˚A. We also consider an additional, ﬁner data set generated for

validation purposes with spacing 2.5˚A.

For any coarse grained node position xi and any atomic position Xε, deﬁne the smoothing

function by

ω(xi, Xε) =

τ (xi, Xε) j τ (xj, Xε)

(6.1)

where the cone-shaped function τ (x, X) = max 0, R − |X − x| induces a coarse-graining radius of R=10.0˚A. Note that (6.1) satisﬁes the normalization requirement (2.1).
In all cases, external loading is applied to the atoms in the MD grid in addition to the random loads applied by the thermostat. The external loading for each atom Bε is constant over time. The magnitude of the loading is chosen so that the bond strains are no larger than 2%, which is less than the strains at which nonlinear eﬀects appear. As described in Section 6.3, the magnitude of the loading is varied relative to the forces on the atoms that sustain the thermal oscillations. This variation helps to test the robustness of the machine learning method in extracting the continuum material properties in the presence of thermal

25

Figure 5: Contours of U1 displacement in typical MD simulations at zero temperature for the three types of datasets. Left: training. Center: validation. Right: testing.
oscillations that create noise.
6.1. Data sets and metrics of accuracy Three sets of data are generated from the MD simulations for each of two values of
temperature, 0K and 300K. In all MD experiments, the atoms are initialized with positions on a hexagonal lattice in the x1-x2 plane with an interatomic spacing of 1.46˚A. The mass of each atom is 2.0E-26kg, or 12amu. For purposes of computing stresses, the thickness of the lattice is set to 3.35˚A, which is the approximate distance between layers in multilayer graphene. The MD time step size is 5.0E-16s, or 5.0fs.
Two types of samples are generated for each data set: the standard samples with spacing h=5.0˚A which will be denoted by S0trKa/in30/0vKal/test, and the samples with ﬁner grids h=2.5˚A , denoted by S0trKa/in30/0vKal,/ﬁtneest. Unless stated otherwise, the h=5.0˚A data sets are used in the learning tasks. The ﬁner grid data sets are employed to assess the generalization properties of the proposed learning approach to diﬀerent grids (further details and discussions are provided in Section 6.4). Images showing contours of U1, the component of atomic displacement in the x1 direction, for the training, validation, and testing samples are shown in Figure 5. 1) Training data set (70 samples): The MD domain is a 100˚A×100˚A square, and, for k1, k2 ∈ {0, 5π0 , 25π0 , . . . , 55π0 }, the prescribed external loadings are given by
b(x1, x2) = (Ck11,k2 cos(k1x1) cos(k2x2), 0), or b(x1, x2) = (0, Ck21,k2 cos(k1x1) cos(k2x2)). (6.2)
As mentioned above, the constant Ck11,k2 and Ck21,k2 are adjusted so that the bond strains are no larger than 2%, so the deformation remains in the linear range of material response.
2) Validation data set (10 samples). For the same MD grid and coarse-grained nodes as in
26

k Ck1

Ck2 pk Rk

1 0.001 0 0 25

2 0 0.001 0 25

3 0.001 0 0 15

4 0 0.001 0 15

5 0.001 0 0 10

6 0.001 0 1 25

7 0 0.001 1 25

8 0.001 0 1 15

9 0 0.001 1 15

10 0.001 0 1 10

Table 1: Parameters used in the MD loading in the 10 validation tests.

the training data set, the applied loads in the validation data set are as follows:

1
b(x1, x2) = (Ck1, Ck2) (−1)j cos
j=−1

π min
2

1, rj,k Rk

where

rj,k = (x1 − (1 − pk)Lj)2 + (x2 − pkLj)2

where L=50 and the values of the parameters Ck1, Ck2, pk and Rk are given in Table 1. In each case, loads are applied to the atoms within three disks of radius Rk with centers at the center of the grid and at the left and right boundaries (if pk = 0) or the upper and lower boundaries (if pk = 1). The direction of the load vectors is either in the x1 or x2 directions. The loads in all cases are self-equilibrated and periodic.

3) Test data set (4 samples). To demonstrate that the learned material model applies to geometries diﬀerent from the original square geometry, four additional test cases are considered. Here, the MD region is a disk of radius 100˚A. Within this disk loading is applied as listed in Table 2 to the exterior of a circle with radius 50˚A, with the interior unloaded. The equilibrium displacements, with smoothing as described previously, are computed at the coarse-grained nodes, which are spaced 5˚A or 2.5˚A, apart on a square lattice. All of these cases have loadings that are discontinuous functions of the radius and therefore are more challenging from a modeling perspective than the validation cases described above. In cases 2 and 4 the loading is also a discontinuous function of the angle because of the sign function (sgn).
As metrics of accuracy on the training, validation and test sets, in this section we calculate the averaged mean square loss (MSL) and displacement mean square error (MSE) for each

27

Case
1 2 3 4

b1 C cos 4θ cos θ C sgn(cos 4θ) cos θ 0 C sin 3θ sin θ

b2 C cos 4θ sin θ C sgn(cos 4θ) sin θ C sgn(sin θ) sin θ C sin 3θ cos θ

Table 2: Loading applied to the exterior of a disk of radius 50˚A in the four tests. In all cases, C=0.0005. b1 and b2 are components of b along the x1 and x2 directions, respectively, and θ is the polar angle in the plane.

learnt kernel on these three sets, which will be referred to as ERtreasi/nu/val/test, respectively. To provide a fair comparison between diﬀerent sets, all these accuracy metrics except ERteesst are normalized with respect to either the force loading or the displacement ﬁelds. Speciﬁcally, ERtreasin/val is normalized by ||bs||2l2(X s) and Eutrain/val/test is normalized by ||us||2l2(X s). For ERteesst we report the absolute value instead since on the test samples the loading is only applied outside the computational domain and we have bs = 0 on Ω. Therefore, we can not normalize ERteesst with respect to the force loading ﬁeld.
Changing the shape of the smoothing functions, while holding the radius R constant, has
only a small eﬀect on the results. For example, replacing the cone-shaped function τ used in
(6.1) with a paraboloid changes the coarse-grained displacements by about 0.3% in a typical
MD simulation used to generate the training data. Changing the radius R aﬀects the horizon
δ and therefore aﬀects the dispersion properties of waves with wavelengths comparable to or
less than the horizon given by (2.9) [37].

6.2. Learning Results

We ﬁrst tune the hyperparameters (δ, M ) following the procedure described in Algorithm
2. The optimal δM∗ for each ﬁxed polynomial order M and the corresponding ERtreasin, Eutrain, ERvaels, Euval, and AvgE are reported in Table 3 and Figure 6. Based on these results, we set (δM∗ , M ) = (20˚A, 10) for the 0K tasks, and (δM∗ , M ) = (20˚A, 15) for the 300K tasks. For both data sets the optimal horizon size is δ = 20˚A , which is twice the support radius

R. This value is very close to the horizon that would be predicted by (2.9), because the MD cutoﬀ distance d = 1.46˚A R, and there are very few interatomic interactions that

connect the smoothing functions with centers separated by 2R. Compared to the data set at

0K, the 300K data set requires a higher polynomial order M and therefore a more complex

inﬂuence function. This is possibly due to the occurrence of thermal oscillations in the MD

simulations

at

300K .

We

use

these

optimal

pairs

(δ

∗ M

,

M

)

as

the

default

choices

in

all

tests

below (unless stated otherwise).

The learning results are provided in the left plot of Figure 7 and in Table 4. For both the

0K and the 300K data sets, the Young’s modulus is estimated to be around 1TPa, which is

28

data set 0K
300K

M δM∗ 0 12.5˚A 5 12.5˚A 10 20˚A 15 22.5˚A 20 25˚A
0 12.5˚A 5 12.5˚A 10 20˚A 15 20˚A 20 25˚A

ERtreasin 13.91% 10.42% 9.81% 9.80% 9.75% 13.46% 10.50% 9.79% 9.82% 9.81%

Eutrain 17.54% 12.19% 11.72% 11.61% 11.89% 31.33% 13.80% 13.32% 13.16% 13.36%

ERvaels 16.31% 13.02% 13.28% 13.50% 13.53% 20.15% 17.83% 18.11% 18.08% 18.34%

Euval 14.49% 7.69% 7.16% 7.22% 7.00% 14.86% 9.66% 9.08% 8.88% 9.23%

AvgE 1
0.6933 0.6704 0.6731 0.6729
1 0.6784 0.6549 0.6505 0.6609

Table 3: Losses from the optimal δ∗(M ) for each values of M , where the optimal cases and the corresponding average of the normalized errors are highlighted with bold.

Relative MSL and MSE Relative MSL and MSE

0.2 0.15
0.1 0.05
0

0K, E tRraeisn 0K, E turain 0K, E vRaels 0K, E vual

5

10

15

20

Polynomial Order M

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0

300K, E tRraeisn 300K, E turain 300K, E vRaels 300K, E vual

5

10

15

20

Polynomial Order M

Figure 6: Optimal relative mean square loss (MSL) and relative mean square error (MSE) for each polynomial order M . Left: results at 0K. Right: results at 300K.

consistent with experimental evidence [58, 59] and computations via ﬁrst principles [60] or MD [61, 62]. The predicted Poisson ratio is negative, which results from graphene’s exceptionally high resistance to relative angle changes (shear) between the covalent bonds. The predicted value ν = −0.4 is consistent with other MD and molecular statistics simulations [63, 64]. As expected, the optimal inﬂuence functions shown in the left plot of Figure 7 are partially negative for both data sets. This fact highlights the importance of allowing for sign-changing inﬂuence functions.
6.3. Sensitivity to Thermal Oscillations Recall from (2.4) that each atom in the MD simulation experiences internal forces Fγε
due to interaction with other atoms and external forces Bε due to prescribed loads. At ﬁnite temperature, the internal forces consist largely of the random forces that produce thermal oscillations. A convenient way to characterize the relative magnitude of the random and

29

10 -5

10

5

0

0

5

0K, = 20, M = 10 300K, = 20, M = 15

10

15

20

20 10 -5 300K, low noise,

= 20, M = 15

300K, medium noise,

= 20, M = 15

15

300K, high noise,

= 20, M = 15

10

5

0

-5

0

5

10

15

20

Figure 7: Left: Optimal inﬂuence functions K at 0K and 300K. Right: Optimal inﬂuence functions K at 300K with diﬀerent level of noise.

data set 0K
data set 300K

M
10 ERtreasin 9.81%
M
15 ERtreasin 9.82%

α
2.8335 Eutrain 11.72%
α
2.5946 Eutrain 13.16%

λ (TPa)
-0.4796 ERvaels
13.28%
λ (TPa)
-0.4583 ERvaels
18.08%

µ (TPa)
0.7978 Euval 7.16%
µ (TPa)
0.7753 Euval 8.88%

E (TPa)
0.91 ERteesst 2.03E-1
E (TPa)
0.90 ERteesst 2.08E-1

ν
-0.4297 Eutest 6.75%
ν
-0.4196 Eutest 9.21%

Table 4: Optimal material parameters and MSL/MSE on training, validation and testing data sets at 0K and 300K. ERteesst shows the absolute l2 error as the inner circle in the test problem is unloaded.

prescribed forces is the signal-to-noise ratio (SN R) deﬁned by

SNR =

ε |Bε|2

.

2

ε Bε + γ Fγε

At ﬁnite temperature with zero loading, SN R = 0, while at zero temperature but ﬁnite loading in equilibrium, SN R = ∞. Most, but not all, of the random forces and oscillations are smoothed out by the coarse graining prior to application of the algorithm to learn the kernel and material parameters. The eﬀect of the random noise is also reduced by the smoothing of displacements over time using (2.10).
Next, we investigate the robustness of the learning approach by applying Algorithm 2 to training data sets with diﬀerent signal-to-noise ratios. With temperature 300K, three training data sets are created by changing the relative magnitude of the loading described in (6.2) to 1, 1/4 and 1/10, respectively. Due to the existence of thermal noise, the smaller the loading magnitude is, the smaller the signal-to-noise ratio will be. Therefore, we denote these three training data sets as the “Low noise” data set, “Med noise” data set and “High noise” data set, respectively.

30

300K Low Noise
300K Med Noise
300K High Noise

StN ratio 0.1556 ERtreasin 9.82%
StN ratio 0.0543 ERtreasin 14.52%
StN ratio 0.0224 ERtreasin 27.64%

α 2.5946 Eutrain 13.16%
α 2.5197 Eutrain 28.27%
α 1.9365 Eutrain 48.86%

λ (TPa) -0.4583
ERvaels 18.08%
λ (TPa) -0.4782
ERvaels 18.34%
λ (TPa) -0.3266
ERvaels 23.73%

µ (TPa) 0.7753 Euval 8.88%
µ (TPa) 0.7798 Euval 9.82%
µ (TPa) 0.6890 Euval 17.54%

E (TPa) 0.90 ERteesst
2.08E-1
E (TPa) 0.87 ERteesst
2.11E-1
E (TPa) 0.95 ERteesst
1.84E-1

ν -0.4196
Eutest 9.21%
ν -0.4422
Eutest 9.28%
ν -0.3106
Eutest 7.95%

Table 5: Test of algorithm robustness on 300K data set with diﬀerent noise levels. ERteesst shows the absolute l2 error as the inner circle in the test problem is unloaded.

To study the sensitivity of the learning algorithm with respect to decreasing SN Rs, we plot and compare the optimal inﬂuence function K in the right plot of Figure 7. It can be seen that while the learnt inﬂuence function from the “Med noise” data set is almost the same as the inﬂuence function from the “Low noise” data set, the learnt inﬂuence function from “High noise” data set slightly diﬀers. To provide a further quantitative comparison, Table 5 provides the estimated material parameters as well as the loss and errors on the validation and test data sets. The learnt inﬂuence functions from all training sets achieves a similar level of accuracy on the test data set. On the validation set, the learnt model from “Med noise” set achieves a similar accuracy as the model from “Low noise” set, while the displacement mean square error increases for the learnt model from “High noise” set. Not surprisingly, since the SN R decreases by 7 times in the “High noise” set, Euval doubles.
6.4. Generalization
This section discusses the generalization of the optimal inﬂuence function to diﬀerent loadings, domains and discretizations.
Diﬀerent Loadings: The loadings in the validation and test data sets are substantially diﬀerent from those in training data sets. Therefore, these can be used to assess the performance of the learnt models as reported in Table 4. The validation loss is consistently lower than 20% and the solution error smaller than 10%, illustrating that the optimal models can be generalized to problems with diﬀerent loadings.
Diﬀerent Domains: In the test data set the domain is a disk (as opposed to the square domain used for training). The results of applying the optimal learnt model to this test problem are provided in Table 4. Here the loss ERteesst is presented as the absolute error because

31

Strain h = 5˚A and h = 2.5˚A
h = 5˚A and h = 2.5˚A

Sval and Stest h = 2.5˚A
h = 5˚A

ERvaels 16.19%
13.24%

Euval 8.01%
9.29%

ERteesst 2.95E-0
1.97E-1

Eutest 8.44%
7.80%

Table 6: Learning results from hybrid resolution datasets with ﬁxed δ = 12.5˚A.

the interior circle is unloaded. For both 0K and 300K, the solution error is consistently below 10%, showing that the optimal models can perform well with diﬀerent domain conﬁgurations.
Hybrid Discretizations: Since the proposed approach learns a continuous nonlocal operator rather than a discrete surrogate for the solution, the learning approach can naturally handle data sets with diﬀerent resolutions or even diﬀerent discretization methods. To provide initial studies on the generalization properties with respect to diﬀerent resolutions, we consider a hybrid data set with samples of diﬀerent resolutions and investigate the performance of the learning algorithm. In particular, the training data set is deﬁned as the union of S0trKain and S0trKa,ifnine. Table 6 reports the accuracy of the learnt model on both standard and ﬁne validation and test data sets. From the results in the table, the solution error is consistently below 10%, which highlights the capability of the proposed algorithm to handle data sets with diﬀerent resolutions.

7. Conclusion
We introduced a new optimization-based, data-driven approach to extract an optimal linear peridynamic solid model from MD data. The peridynamic constitutive law is learned by optimizing the inﬂuence function and material parameters. The inﬂuence function is allowed to be sign-changing, thus improving the descriptive power of the optimal model. The nontrivial problem of learning well-posed models in the presence of sign-changing inﬂuence functions was addressed by deriving new suﬃcient conditions for the discretized peridynamic model, embedded in the learning procedure as inequality constraints. To assess the performance of the proposed learning algorithm, we tested the robustness with respect to noise and the ability of the optimal model to generalize to diﬀerent domain conﬁgurations, external loadings, and discretizations.
A fundamental aspect of the proposed procedure is the fact that we learn a continuous operator rather than a discrete operator or a surrogate for the solution; this fact guarantees generalization of the optimal model to settings that are diﬀerent from the ones used during training. Furthermore, the continuous setting opens new research direction, such as considering diﬀerent discretization methods when validating the optimal model.
Although the present work focuses on single layered graphene, the results suggest that this method may impact a broader range of materials and applications. As a follow-up
32

work we plan to extend our algorithm to more complex materials behaviors, such as large deformation and/or damage.
Acknowledgements
Sandia National Laboratories is a multi-mission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. This paper, SAND2021-9450, describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government.
The work of S. Silling, and M. D’Elia is supported by by the Sandia National Laboratories Laboratory Directed Research and Development (LDRD) program. M. D’Elia is also partially supported by the U.S. Department of Energy, Oﬃce of Advanced Scientiﬁc Computing Research under the Collaboratory on Mathematics and Physics-Informed Learning Machines for Multiscale and Multiphysics Problems (PhILMs) project. H. You and Y. Yu are supported by the National Science Foundation under award DMS 1753031. Portions of this research were conducted on Lehigh University’s Research Computing infrastructure partially supported by NSF Award 2019035.
References
[1] T. I. Zohdi, Homogenization methods and multiscale modeling, Encyclopedia of Computational Mechanics Second Edition (2017) 1–24.
[2] A. Bensoussan, J.-L. Lions, G. Papanicolaou, Asymptotic analysis for periodic structures, Vol. 374, American Mathematical Soc., 2011.
[3] E. Weinan, B. Engquist, Multiscale modeling and computation, Notices of the AMS 50 (9) (2003) 1062–1070.
[4] Y. Efendiev, J. Galvis, T. Y. Hou, Generalized multiscale ﬁnite element methods (gmsfem), Journal of computational physics 251 (2013) 116–135.
[5] C. Junghans, M. Praprotnik, K. Kremer, Transport properties controlled by a thermostat: An extended dissipative particle dynamics thermostat, Soft Matter 4 (1) (2008) 156–161.
33

[6] R. Kubo, The ﬂuctuation-dissipation theorem, Reports on progress in physics 29 (1) (1966) 255.
[7] F. Santosa, W. W. Symes, A dispersive eﬀective medium for wave propagation in periodic composites, SIAM Journal on Applied Mathematics 51 (4) (1991) 984–1005.
[8] M. Dobson, M. Luskin, C. Ortner, Sharp stability estimates for the force-based quasicontinuum approximation of homogeneous tensile deformation, Multiscale Modeling & Simulation 8 (3) (2010) 782–802.
[9] M. Ortiz, A method of homogenization of elastic media, International journal of engineering science 25 (7) (1987) 923–934.
[10] N. Mo¨es, J. T. Oden, K. Vemaganti, J.-F. Remacle, Simpliﬁed methods and a posteriori error estimation for the homogenization of representative volume elements (rve), Computer methods in applied mechanics and engineering 176 (1-4) (1999) 265–278.
[11] T. J. Hughes, G. N. Wells, A. A. Wray, Energy transfers and spectral eddy viscosity in large-eddy simulations of homogeneous isotropic turbulence: Comparison of dynamic smagorinsky and multiscale models over a range of discretizations, Physics of Fluids 16 (11) (2004) 4044–4052.
[12] G. W. Milton, The Theory of Composites, Cambridge University Press, 2002.
[13] A. C. Eringen, D. G. B. Edelen, On nonlocal elasticity, International Journal of Engineering Science 10 (3) (1972) 233–248.
[14] F. Bobaru, J. T. Foster, P. H. Geubelle, S. A. Silling, Handbook of peridynamic modeling, CRC press, 2016.
[15] M. Beran, J. McCoy, Mean ﬁeld variations in a statistical sample of heterogeneous linearly elastic solids, International Journal of Solids and Structures 6 (8) (1970) 1035– 1054.
[16] K. Cherednichenko, V. P. Smyshlyaev, V. Zhikov, Non-local homogenised limits for composite media with highly anisotropic periodic ﬁbres, Proceedings of the Royal Society of Edinburgh Section A Mathematics 136 (1) (2006) 87–114.
[17] F. C. Karal Jr, J. B. Keller, Elastic, electromagnetic, and other waves in a random medium, Journal of Mathematical Physics 5 (4) (1964) 537–547.
34

[18] Y. Rahali, I. Giorgio, J. Ganghoﬀer, F. dell’Isola, Homogenization a` la piola produces second gradient continuum models for linear pantographic lattices, International Journal of Engineering Science 97 (2015) 148–172.
[19] V. P. Smyshlyaev, K. D. Cherednichenko, On rigorous derivation of strain gradient effects in the overall behaviour of periodic heterogeneous media, Journal of the Mechanics and Physics of Solids 48 (6-7) (2000) 1325–1357.
[20] J. R. Willis, The nonlocal inﬂuence of density variations in a composite, International Journal of Solids and Structures 21 (7) (1985) 805–817.
[21] Q. Du, B. Engquist, X. Tian, Multiscale modeling, homogenization and nonlocal eﬀects: Mathematical and computational issues, Contemporary Mathematics 754.
[22] Q. Du, K. Zhou, Mathematical analysis for the peridynamic nonlocal continuum theory, ESAIM: Mathematical Modelling and Numerical Analysis 45 (02) (2011) 217–234.
[23] E. Madenci, A. Barut, M. Dorduncu, Peridynamic diﬀerential operator for numerical analysis, Springer, 2019.
[24] E. Emmrich, O. Weckner, et al., On the well-posedness of the linear peridynamic model and its convergence towards the navier equation of linear elasticity, Communications in Mathematical Sciences 5 (4) (2007) 851–864.
[25] S. A. Silling, M. Epton, O. Weckner, J. Xu, E. Askari, Peridynamic states and constitutive modeling, Journal of Elasticity 88 (2) (2007) 151–184.
[26] P. Clark Di Leoni, T. A. Zaki, G. Karniadakis, C. Meneveau, Two-point stress–strainrate correlation structure and non-local eddy viscosity in turbulent ﬂows, Journal of Fluid Mechanics 914 (2021) A6.
[27] G. Pang, M. D’Elia, M. Parks, G. E. Karniadakis, nPINNs: nonlocal Physics-Informed Neural Networks for a parametrized nonlocal universal Laplacian operator. Algorithms and Applications, to appear in Journal of Computational Physics (2020).
[28] P. Diehl, S. Prudhomme, M. L´evesque, A review of benchmark experiments for the validation of peridynamics models, Journal of Peridynamics and Nonlocal Modeling 1 (1) (2019) 14–35.
[29] D. Benson, S. Wheatcraft, M. Meerschaert, Application of a fractional advectiondispersion equation, Water Resources Research 36 (6) (2000) 1403–1412.
35

[30] X. Xu, J. T. Foster, Deriving peridynamic inﬂuence functions for one-dimensional elastic materials with periodic microstructure, arXiv:2003.05520 (2020).
[31] X. Xu, M. D’Elia, J. Foster, Bond-based peridynamic kernel learning with energy constraint, arXiv:2101.01095 (2021).
[32] H. You, Y. Yu, N. Trask, M. Gulian, M. D’Elia, Data-driven learning of robust nonlocal physics from high-ﬁdelity synthetic data, Computer Methods in Applied Mechnics and Engineering 374 (2021) 113553.
[33] H. You, Y. Yu, S. Silling, M. D’Elia, Data-driven learning of nonlocal models: from high-ﬁdelity simulations to constitutive laws, accepted in AAAI Spring Symposium: MLPS (2021).
[34] S. A. Silling, M. Epton, O. Weckner, J. Xu, E. Askari, Peridynamic states and constitutive modeling, Journal of Elasticity 88 (2) (2007) 151–184.
[35] S. Silling, Peridynamic modeling, numerical techniques, and applications, Elsevier, 2021.
[36] R. B. Lehoucq, M. P. Sears, Statistical mechanical foundation of the peridynamic nonlocal continuum theory: Energy and momentum conservation laws, Physical Review E 84 (3) (2011) 031112.
[37] S. A. Silling, Reformulation of elasticity theory for discontinuities and long-range forces, Journal of the Mechanics and Physics of Solids 48 (1) (2000) 175–209.
[38] S. A. Silling, R. B. Lehoucq, Peridynamic theory of solid mechanics, Advances in applied mechanics 44 (2010) 73–168.
[39] N. Trask, H. You, Y. Yu, M. L. Parks, An asymptotically compatible meshfree quadrature rule for nonlocal problems with applications to peridynamics, Computer Methods in Applied Mechanics and Engineering 343 (2019) 151–165.
[40] Y. Yu, H. You, N. Trask, An asymptotically compatible treatment of traction loading in linearly elastic peridynamic fracture, Computer Methods in Applied Mechanics and Engineering 377 (2021) 113691.
[41] Y. Fan, X. Tian, X. Yang, X. Li, C. Webster, Y. Yu, An asymptotically compatible probabilistic collocation method for randomly heterogeneous nonlocal problems, In preprint.
36

[42] H. You, X. Y. Lu, N. Trask, Y. Yu, An asymptotically compatible approach for Neumann-type boundary condition on nonlocal problems, arXiv:1908.03853 (2019).
[43] H. You, Y. Yu, D. Kamensky, An asymptotically compatible formulation for local-tononlocal coupling problems without overlapping regions, Computer Methods in Applied Mechanics and Engineering 366 (2020) 113038.
[44] M. Foss, P. Radu, Y. Yu, Convergence analysis and numerical studies for linearly elastic peridynamics with dirichlet-type boundary conditions, arXiv preprint arXiv:2106.13878.
[45] P. Seleson, M. Parks, On the role of the inﬂuence function in the peridynamic theory, International Journal for Multiscale Computational Engineering 9 (6).
[46] M. D’Elia, X. Tian, Y. Yu, A physically-consistent, ﬂexible and eﬃcient strategy to convert local boundary conditions into nonlocal volume constraints, SIAM Journal of Scientiﬁc Computing 42 (4) (2020) A1935–A1949.
[47] M. D’Elia, Y. Yu, On the prescription of boundary conditions for nonlocal poisson’s and peridynamics models, arXiv preprint arXiv:2107.04450.
[48] Y. Yu, F. F. Bargos, H. You, M. L. Parks, M. L. Bittencourt, G. E. Karniadakis, A partitioned coupling framework for peridynamics and classical theory: analysis and simulations, Computer Methods in Applied Mechanics and Engineering 340 (2018) 905– 931.
[49] T. Mengesha, Q. Du, Nonlocal constrained value problems for a linear peridynamic navier equation, Journal of Elasticity 116 (1) (2014) 27–51.
[50] O. Weckner, S. A. Silling, Determination of the constitutive model in peridynamics from experimental dispersion data, International Journal of Multiscale Computational Engineering, under review.
[51] T. Mengesha, Q. Du, Analysis of a scalar nonlocal peridynamic model with a sign changing kernel, Discrete & Continuous Dynamical Systems-B 18 (5) (2013) 1415–1437.
[52] K. J. Bathe, The inf −sup condition and its evaluation for mixed ﬁnite element methods, Computers & Structures 79 (2) (2001) 243 – 252.
[53] F. Brezzi, M. Fortin, Mixed and hybrid ﬁnite element methods, Vol. 15, Springer Science & Business Media, 2012.
37

[54] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv:1412.6980 (2014).
[55] Y. Yu, J. Chen, T. Gao, M. Yu, DAG-GNN: DAG structure learning with graph neural networks, arXiv:1904.10098 (2019).
[56] J. Nocedal, S. Wright, Numerical Optimization, Springer Science & Business Media, 2006.
[57] J. Tersoﬀ, Empirical interatomic potential for carbon, with applications to amorphous carbon, Physical Review Letters 61 (25) (1988) 2879.
[58] C. Lee, X. Wei, J. W. Kysar, J. Hone, Measurement of the elastic properties and intrinsic strength of monolayer graphene, science 321 (5887) (2008) 385–388.
[59] I. Frank, D. M. Tanenbaum, A. M. van der Zande, P. L. McEuen, Mechanical properties of suspended graphene sheets, Journal of Vacuum Science & Technology B: Microelectronics and Nanometer Structures Processing, Measurement, and Phenomena 25 (6) (2007) 2558–2561.
[60] F. Liu, P. Ming, J. Li, Ab initio calculation of ideal strength and phonon instability of graphene under tension, Physical Review B 76 (6) (2007) 064120.
[61] T.-w. HAN, P.-f. He, J. Wang, A.-h. Wu, Molecular dynamics simulation of single graphene sheet under tension, New Carbon Materials 25 (4) (2010) 261–266.
[62] T. Han, P. He, Y. Luo, X. Zhang, Research progress in the mechanical properties of graphene, Advances in Mechanics 41 (3) (2011) 279.
[63] H. Qin, Y. Sun, J. Z. Liu, M. Li, Y. Liu, Negative poisson’s ratio in rippled graphene, Nanoscale 9 (12) (2017) 4135–4142.
[64] J.-W. Jiang, T. Chang, X. Guo, H. S. Park, Intrinsic negative poisson’s ratio for singlelayer graphene, Nano letters 16 (8) (2016) 5286–5290.
38

