arXiv:2108.09265v2 [cs.LG] 30 Oct 2021

Efﬁcient Online Estimation of Causal Effects by Deciding What to Observe
Shantanu Gupta, Zachary C. Lipton, David Childers
Carnegie Mellon University {shantang,zlipton,dchilders}@cmu.edu
Abstract
Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we aim to estimate any functional of a probabilistic model (e.g., a causal effect) as efﬁciently as possible, by deciding, at each time, which data source to query. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by current estimates of the moments. We propose two selection strategies: (1) explore-then-commit (OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources may include subsets of mediators, confounders, and instrumental variables.
1 Introduction
Statistical and causal modeling typically proceed from the assumption that we already know which variables are (and are not) observed. However, this perspective fails to address the difﬁcult data collection decisions that precede such modeling efforts. Doctors must select a set of tests to run. Survey designers must select a slate of questions to ask. Companies must select which datasets to purchase. Whether or not we model these decisions, they pervade the practice of data science, inﬂuencing both what questions we can ask and how accurately we can answer them.
One might ask, why not collect everything? The answers are two-fold: First, data acquisition can be expensive. In a medical setting, blood tests can cost anywhere from tens to thousands of dollars. Running every test for every patient is infeasible. Likewise, space on surveys is limited, and asking every conceivable question of every respondent is infeasible. Second, in many settings, we lack complete control over the set of variables observed. Instead, we might have access to multiple data sources, each capturing a different subset of variables. Such data fusion problems pervade economic modeling and public health, and present interesting challenges: (i) efﬁciently estimating (or even identifying) a population parameter of interest often requires intelligently combining data from multiple sources; (ii) data collection is often iterative, with tentative conclusions at each stage informing choices about what data to collect next.
In this paper, we formalize the sequential problem of deciding, at each time, which data source to query (i.e., what to observe) in order to efﬁciently estimate a target parameter. We propose online moment selection (OMS), a framework that applies the generalized method of moments (GMM) [17] both to estimate the parameter and to decide which data sources to query. This framework can be
35th Conference on Neural Information Processing Systems (NeurIPS 2021).

applied to estimate any statistical parameter that can be identiﬁed by a set of moment conditions. For example, OMS can address (i) any (regular) maximum likelihood estimation problem [13, Page 109]; and (ii) estimating average treatment effects (ATEs) using instrumental variables (IVs), backdoor adjustment sets, mediators, and/or other identiﬁcation strategies.
Our strategy requires only that the agent has sufﬁcient structural knowledge to formulate the set of moment conditions and that each moment can be estimated using the variables returned by at least one of the data sources. Interestingly, the optimal decisions which lead to estimates with the lowest mean squared error (MSE) depend on the (unknown) model parameters. This motivates our adaptive strategy: as we collect more data, we better estimate the underlying parameters, improving our strategy for allocating our remaining budget among the available data sources.
We ﬁrst address the setting where the cost per instance is equal across data sources (Section 4). First, we show that any ﬁxed policy that differs from the oracle suffers constant asymptotic regret, as assessed by MSE. We then overcome this limitation by proposing two adaptive strategies—explorethen-commit (OMS-ETC) and explore-then-greedy (OMS-ETG)—both of which choose data sources based on the estimated asymptotic variance of the target parameter.
Under OMS-ETC, we use some fraction of the sample budget to explore randomly, using the collected data to estimate the model parameters. We then exploit the current estimated model, collecting the remaining samples according to the fraction expected to minimize our estimator’s asymptotic variance. In OMS-ETG, we continue to update our parameter estimates after every step as we collect new data. We prove that both policies achieve zero asymptotic regret. To overcome the non-i.i.d. nature of the sample moments, we draw upon martingale theory. To derive zero asymptotic regret, we show uniform concentration of sample moments and a ﬁnite-sample concentration inequality for the GMM estimator with dependent data. Next, we adapt OMS-ETC and OMS-ETG to handle heterogeneous costs over the data sources (Section 5) and prove that they still have zero asymptotic regret.
Finally, we validate our ﬁndings experimentally 1 (Section 6). Motivated by ATE estimation in causal models encoded as directed acyclic graphs, we generate synthetic data from a variety of causal graphs and show that the regret of our proposed methods converges to zero. Furthermore, we see that despite being asymptotically equivalent, OMS-ETG outperforms OMS-ETC in ﬁnite samples. Finally, we demonstrate the effectiveness of our methods on two semi-synthetic datasets: the Infant Health Development Program (IHDP) dataset [19] and a Vietnam era draft lottery dataset [2].
2 Related Work
Many works attempt to identify and estimate causal effects from multiple datasets. [4, 21] study the problem of combining multiple heterogeneous datasets and propose methods for dealing with various biases. Other works study causal identiﬁcation when observational and interventional distributions involving different sets of variables are available [28, 40]. [12] introduce estimators of the ATE that efﬁciently combine two datasets, one where confounders are observed (enabling the backdoor adjustment) and another where mediators are observed (enabling the frontdoor adjustment). [29] derive an estimator for the ATE in linear causal models with multiple confounders, where the confounders are observed in different datasets.
Another related line of work addresses ﬁnding optimal adjustment sets for covariate adjustment [18, 33, 45, 36]. While these works take for granted the collection of available datasets, we focus on the problem of deciding which data to collect. Our work shares motivation with active learning, where the learner strategically chooses which (unlabeled) samples to label in order to learn most efﬁciently [35, 26]. [8] design algorithms for actively collecting samples in a manner that minimizes the learner’s variance. In settings where there is a cost associated with collecting each feature, active feature acquisition methods incrementally query feature values to improve a predictive model [22, 34, 20]. [47] propose an active learning criterion to ﬁnd the most informative questions to ask each respondent in a survey. In the context of causal inference, [37] study active structure learning of causal DAGs by ﬁnding cost-optimal interventions, [11] demonstrate that for ATE estimation, actively deconfounding data can improve sample complexity. [44] propose strategies for acquiring missing confounders to efﬁciently estimate the ATE.
1The code and data are available at https://www.github.com/acmi-lab/ online-moment-selection.
2

Others have studied moment selection and IV selection from batch data. [1] introduce consistent moment selection procedures for the GMM setting with some incorrect moments. [6] propose an information-based lasso method for excluding invalid or redundant moment conditions. [41] propose a variable selection framework that uses lasso regression to decide which covariates to include. [14] propose four statistical criteria—including estimation efﬁciency and non-redundancy—for selecting among a set of candidate IVs. [9] develop an IV selection criteria based on asymptotic MSE and develop it for the GMM and generalized empirical likelihood estimators. [7] develop conservative conﬁdence intervals for structural parameters when the off-diagonal entries of the covariance matrix of the empirical moments are unknown (e.g., when the moments are obtained from different datasets). By contrast, we are interested in selecting the data sources for the moments in an online setting.
Previous works address learning from adaptively collected data. [25] propose methods for adaptive experimental design, where at each step, the experimenter must decide the treatment probability using past data in order to efﬁciently estimate the ATE. [23, 24] propose a doubly-robust estimator for off-policy evaluation with dependent samples. [46] provide regret bounds for learning an optimal policy using adaptively collected data, where the probability of selecting an action is a function of past data. [48, 49] study statistical inference for OLS and M-estimation with non-i.i.d. bandit data. While these settings are different from ours, some of the theoretical tools (e.g., martingale asymptotics and uniform martingale concentration bounds) are similar.

3 Preliminaries

In the GMM framework [17, 30], we estimate model parameters by leveraging moment conditions that are satisﬁed at the true parameters θ∗. A moment condition is a vector g(Xt, θ) such that
E[g(Xt, θ∗)] = 0. We estimate θ∗ by minimizing the objective QT :

θT = arg min QT (θ), where QT (θ) =
θ∈Θ

1T T gt(θ)
t=1

1T

W T

gt(θ) ,

t=1

Θ is the parameter space, gt(θ) := g(Xt, θ), and W is some (possibly data dependent) positive deﬁnite matrix. In this work, we use the two-step GMM estimator, where the one-step estimator θT(os)
(os) −1
is computed with W := I (identity) and the two-step estimator with W := ΩT (θT ) , where

ΩT (θ) =

1 T

T t=1

gt

(θ)gt

(θ)

.

Let V be the set of variables of interest and ψ a collection of subsets of V, each corresponding to

the speciﬁc variables observable via one of the available data sources. Our methods are applicable

whenever the target parameter can be identiﬁed by a set of moment conditions such that each moment relies on variables simultaneously observable in at least one data source. The selection vector, denoted by st ∈ {0, 1}|ψ|, is the binary vector indicating the data source selected at time t.

Assumption 1. The agent queries one data source at each step:

|ψ| i=1

st,i

=

1,

i.e.,

st

is

one-hot.

We can handle the querying of multiple sources by adding the union of their variables to ψ. In our setup, the moment conditions can be written as gt(θ) = m(st) ⊗ g˜t(θ) ∈ RM , where ⊗ is the Hadamard product, m : {0, 1}|ψ| → {0, 1}M is a ﬁxed known function such that m(st) determines which moments get selected, and g˜t(θ) are i.i.d. across t. For concreteness, we instantiate our setup with a simple example:
Example 1 (Instrumental Variable (IV) graph). Consider a linear IV causal model (Figure 2a) with instrument Z, treatment X, outcome Y , and the following data-generating process:
X := αZ + η, Y := βX + , ⊥⊥ η, ⊥⊥ Z, η ⊥⊥ Z.
The target parameter is the ATE β. For ψ = {{Z, X}, {Z, Y }}, the moment conditions are
gt(θ) = sstt,,12 ⊗ ZZtt((YXt t−−ααβZZtt)) = (1 −sts,1t,Z1)t(ZXt(tY−t −αZαtβ)Zt) ,

=m(st )

=g˜t (θ)

where θ = [β, α] and {Zt, Xt, Yt} are i.i.d.

3

For some known function ftar : Θ → R, let β∗ := ftar(θ∗) be the target parameter (e.g., the ATE). In

practice, we estimate the target parameter by plugging-in the GMM estimate: β = ftar(θ). Let Ht represent the history or the data collected until time t with H0 = {} and space Ht. A data collection policy π consists of a sequence of functions πt : Ht−1 → {0, 1}|ψ| with st = πt(Ht−1). Thus st can be dependent on data collected until time (t − 1) and so the sample moments gt(θ) are not i.i.d.

Deﬁnition 1 (Selection ratio). The selection ratio, denoted by κ(tπ), encodes the fraction of samples

collected

from

each

data

source

until

time

t:

κ(tπ)

=

1 t

t i=1

st

∈

∆|ψ|−1

(standard

simplex).

We use θt(π) and θt(os) to denote the two-step and one-step GMM estimators, respectively, that use the data Ht. To reduce clutter, we use ∆ψ := ∆|ψ|−1, ctr (∆ψ) = |ψ1| , |ψ1| , . . . , |ψ1| (center of the
simplex), and might drop the superscript π from κt, θt, and βt. . denotes the spectral and l2 norms for matrices and vectors, respectively, and N (θ) := {θ : θ − θ ≤ } ( -ball around θ).

4 Adaptive Data Collection

The central challenge in this work is to make strategic decisions about which data to observe so that we most efﬁciently estimate the target functional. In this section, we present three policies: (i) ﬁxed: query the data sources according to a pre-speciﬁed ratio; (ii) OMS-ETC: query uniformly for a speciﬁed exploration period, estimate the optimal ratio based on the inferred parameters and thereafter aim for that ratio; and (iii) OMS-ETG: same as OMS-ETC but continue to update parameter (and thus oracle ratio) estimates after the exploration period. For now, we analyze these policies for the case when the cost to query is identical across data sources. By T ∈ N, we denote the (known) horizon, which can be thought of as the agent’s data acquisition budget. We defer all proofs to Appendix A.

We now present sufﬁcient conditions for consistency and asymptotic normality of the GMM estimator

under adaptively collected data. We later use these results to derive the regret of our policies.

Assumption 2. (a) (Identiﬁcation) ∀θ = θ∗, P lim infT →∞ Q¯(θ) > 0 = 1, where Q¯(θ) =

gT∗ (θ)W gT∗ (θ)

and gT∗ (θ) =

1 T

T t=1

m(st

)

⊗

E

[g˜t

(θ)]

; (b) Θ ⊂ RD

is compact; (c) ∀θ, g˜i(θ)

is twice continuously differentiable (c.d.); (d) ∀θ, E[g˜i(θ)] is continuous; and (e) ftar is c.d. at θ∗.

By Assumption 2(a), the GMM objective is uniquely minimized at θ∗. Informally, this means that

each moment is (asymptotically) collected enough times to allow identiﬁcation. If M = D (just-

identiﬁed case), this holds when (i) an asymptotically non-negligible fraction of every moment is

collected: ∀j ∈ [M ], P

lim

infT →∞

1 T

T t=1

mj (st)

=

0

= 1; and (ii) ∀θ = θ∗, E [g˜t(θ)] = 0.

Property 1 (ULLN). Let ai(θ) := a(Xi; θ) ∈ R be a continuous function with Xi sampled i.i.d. We say that ai(θ) satisﬁes the ULLN property if (i) ∀θ, E ai(θ)2 < ∞; (ii) ai(θ) is dominated by a
function A(Xi): ∀θ, |ai(θ)| ≤ A(Xi); and (iii) E[A(Xi)] < ∞.

Proposition 1 (Consistency). Suppose that (i) Assumption 2 holds, (ii) ∀j ∈ [M ], g˜t,j(θ) satisﬁes Property 1, and (iii) ∀(i, j) ∈ [M ]2, g˜t(θ)g˜t(θ) i,j satisﬁes Property 1; Then, θT(π) −−−p−→ θ∗.
T →∞

Proposition 2 (Asymptotic normality). Suppose that (i) θT(π) −→p θ∗; (ii) ∀(i, j) ∈ [M ] ×

[D], ∂∂g˜θt (θ) satisﬁes Property 1; (iii) ∃δ > 0 such that E g˜i(θ∗) 2+δ < ∞, and (iv) (Se-
i,j

lection ratio convergence) κ(Tπ) →p k for some constant k ∈ ∆ψ. Then θT is asymptotically normal:

√ (π) ∗ d

∗

T (θT − θ ) → N (0, Σ(θ , k)) ,

where Σ(θ∗, k) is a constant matrix that depends only on θ∗ and k (see Appendix A.2 for the complete

expression). By Assumption 2(e) and the Delta method, βT is asymptotically normal:

√

∗d

∗

∗

∗

∗

∗

T (βT − β ) → N (0, V (θ , k)) , where V (θ , k) = ∇θftar(θ ) [Σ(θ , k)]∇θftar(θ ).

Proposition 2 shows that for a policy under which the selection ratio κT converges in probability to a
constant, the GMM estimator θT can be asymptotically normal. The speciﬁc order in which the data sources are queried does not affect asymptotic normality as long the selection ratio κ(Tπ) converges.

4

Input: T ∈ N, e ∈ (0, 1) 1 n = Te ; 2 Collect n samples s.t. κn = ctr (∆ψ);
3 θn = GMM(Hn);
4 k = arg minκ∈∆ψ V (θn, κ); 5 Collect remaining samples such that
κT = proj(k, ∆˜ );
6 θT = GMM(HT );
Output: ftar(θT )

Input: T ∈ N, s ∈ (0, 1)
1 k = ctr (∆ψ); 2 J = 1s ; 3 for j ∈ [1, 2, . . . , J] do 4 t = T sj ;
5 Collect T s samples s.t. κt = k;
6 θt = GMM(Ht, W = Wvalid);
7 kt = arg minκ∈∆ψ V (θt, κ); 8 k = proj(kt, ∆˜ j+1(κt)); 9 end
10 θT = GMM(HT , W = Wefﬁcient);
Output: ftar(θT )

(a) The Explore-then-Commit policy.

(b) The Explore-then-Greedy policy.

Figure 1: Algorithms for OMS-ETC and OMS-ETG.

A ﬁxed (or non-adaptive) policy, denoted by πk, has κT = k for some constant k ∈ ∆ψ. Here,
the collection decisions do not depend on the data and each data source is queried a ﬁxed fraction
of the time. By Proposition 2, θT(πk) is asymptotically normal. The oracle policy, denoted by π∗, is the ﬁxed policy with the lowest asymptotic variance. Thus for π∗, we have κ(Tπ∗) = κ∗, where κ∗ = arg minκ V (θ∗, κ). We call κ∗ the oracle selection ratio. For the oracle policy, we have √T (β(π∗) − β) →d N (0, V (θ∗, κ∗)). The following assumption ensures that κ∗ is unique and
T
consequently the data collection decisions are meaningful.

Assumption 3. κ∗ uniquely minimizes V (θ∗, κ): ∀κ ∈ ∆ψ s.t. κ = κ∗, V (θ∗, κ) > V (θ∗, κ∗).

Deﬁnition 2 (Asymptotic regret). The asymptotic regret captures how close the scaled asymptotic

error of a given policy is to the oracle policy. We deﬁne the asymptotic regret of a policy π as

√

(π)

∗

∗∗

R∞(π) = AMSE T βT − β − V (θ , κ ),

(1)

where AMSE is the asymptotic MSE (i.e., the MSE of the limiting distribution).

For any ﬁxed policy πk such that κ(Tπk) = k for some constant k = κ∗, we have R∞(πk) = [V (θ∗, k) − V (θ∗, κ∗)] > 0 (by Assumption 3). This shows that a ﬁxed policy suffers constant regret. This motivates the design of adaptive policies, where the regret asymptotically converges to zero.

4.1 Online Moment Selection via Explore-then-Commit (OMS-ETC)
OMS-ETC is inspired by the ETC strategy for multi-armed bandits (MABs) [27, Chapter 6]. Under OMS-ETC, we ﬁrst explore by collecting a ﬁxed number of samples for each choice. Then, we use these samples to estimate the oracle selection ratio κ∗. Finally, we commit to this ratio for the remaining time steps. We denote the OMS-ETC policy by πETC (Figure 1a).
The policy πETC is characterized by an exploration fraction e ∈ (0, 1). We ﬁrst collect T e samples by querying each data source equally so that κT e = ctr (∆ψ). We then estimate θT e and obtain the plugin estimate of κ∗ as k = arg minκ∈∆ψ V (θT e, κ). The feasible region for κT is deﬁned as the set of values that κT can take after we have devoted T e samples to exploration and is given by ∆˜ = {eκT e + (1 − e)κ : κ ∈ ∆ψ} (proof in Appendix C). We collect the remaining T (1−e) samples such that κT is as close to k as possible: κT = proj(k, ∆˜ ), where proj k, ∆˜ = arg mink ∈∆˜ k − k .
Remark. The feasible region shrinks as e increases because, as e increases, the T (1 − e) samples that remain after exploration decrease thereby shrinking the possible values that κT can take.
Theorem 1 (Regret of OMS-ETC). Suppose that (i) Conditions (i)-(iii) of Proposition 2 hold and (ii) Assumption 3 holds. Case (a): For a ﬁxed e ∈ (0, 1), if κ∗ ∈ ∆˜ , then the regret converges to zero:

5

R∞(πETC) = 0. If κ∗ ∈/ ∆˜ , then πETC suffers constant regret: R∞(πETC) = r for some constant r > 0. Case (b): If e depends on T such that e = o(1) and T e → ∞ as T → ∞ (e.g. e = √1 ), then
T
∀θ∗ ∈ Θ, we have R∞(πETC) = 0.
The theorem provides sufﬁcient conditions for when the regret converges to zero. Case (a) of the theorem shows that if we explore for a ﬁxed fraction of the horizon T , the regret will only converge to zero if κ∗ is inside the feasible region. Thus the regret will not converge to zero over the entire parameter space Θ as there would be certain parameter values for which κ∗ would be outside the feasible region. Case (b) shows that we can achieve zero asymptotic regret for every θ∗ ∈ Θ by setting e such that it becomes asymptotically negligible (e ∈ o(1)). The main idea in the proof (see Appendix A.3) is to show that κT →p κ∗ and apply Proposition 2. In Case (b), the feasible region ∆˜ asymptotically covers the entire simplex (∆˜ −T−→ −−∞→ ∆ψ) and this is sufﬁcient to show that κT →p κ∗.

4.2 Online Moment Selection via Explore-then-Greedy (OMS-ETG)

We extend OMS-ETC by periodically updating our estimate of κ∗ as we collect additional samples instead of committing to a value after exploration. The data is collected in batches. OMS-ETG (Figure 1b) is characterized by a batch fraction s ∈ (0, 1). The algorithm runs for J = 1s rounds and we collect b = T s samples in each round. In the ﬁrst round, we explore and thus κb = ctr (∆ψ). After every round j ∈ [J − 1], we estimate θbj and the oracle selection ratio: kbj =
arg minκ∈∆ψ V (θbj, κ). The feasible region for round j + 1 (the set of values of κb(j+1) can take) is ∆˜ j+1(κbj) = jκjb+j+1 κ : κ ∈ ∆ψ (proof in Appendix C). In round (j + 1), we (greedily) collect
samples such that κb(j+1) is as close to kj+1 as possible: κb(j+1) = proj kj, ∆˜ j+1(κbj) .

Theorem 2 states sufﬁcient conditions for when OMS-ETG has zero regret. We ﬁrst state a ﬁnitesample tail bound for the two-step GMM estimator under adaptively collected (non-i.i.d.) data in Lemma 1 (which might be of independent interest) and use it to prove the theorem.

Property 2 (Concentration). Let a˜i(θ) := a˜(Xi; θ) ∈ R with Xi sampled i.i.d., a˜∗(θ) = E [a˜(Xi; θ)], A(Xi, θ) = ∂a˜(∂Xθi;θ) , ui(η) = supθ,θ ∈Θ, θ−θ ≤η |a˜i(θ) − a˜i(θ )|, and u∗(η) = E [ui(η)]. Let L1, η0, and A0 be some positive constants. We say that a˜i(θ) satisﬁes the Concentration property if (i) a˜∗(θ) is L1-Lipschitz, (ii) ∀θ ∈ Θ, [a˜i(θ) − a˜∗(θ)] is sub-Exponential, (iii) E [ A(Xi, θ) ] < A0 < ∞, and (iv) one of the following two conditions hold: (a) ∀η ∈ (0, η0), [ui(η) − u∗(η)] is sub-Exponential, or (b) supθ∈Θ A(Xi, θ) is sub-Exponential.

Remark. Property 2 is used to derive a uniform law (see Lemma 4) that is used to prove Lemma 1.
Property 2(iv) might be hard to check but (iv)(a) is satisﬁed for bounded function classes, i.e., when
a˜i ∞ < A < ∞ (see Proposition 9) and (iv)(b) for linear function classes with sub-Exponential data (see Proposition 10). For the linear IV model in Example 1, Property 2(iv) would hold if Zi2 is sub-Exponential (e.g., when Zi is sub-Gaussian).

Lemma 1 (GMM concentration inequality). Let λ∗, C0, η1, η2, and δ0 be some positive constants. Suppose that (i) Assumption 2 holds; (ii) ∀j, g˜i,j(θ) satisﬁes Property 2; (iii) The

spectral norm of the GMM weight matrix W is upper bounded with high probability: ∀δ ∈

(0, C0) , P

W ≤ λ∗

≥

1

−

1 δD

exp

−O

T δ2

(see Remark 1); (iv) (Local strict con-

vexity) ∀θ ∈ Nη1 (θ∗), P ∂∂2θQ2¯ (θ)−1 ≤ h = 1 (Q¯(θ) is deﬁned in Assumption 2a); (v)

(Strict minimization) ∀θ ∈ Nη2 (θ∗), there is a unique minimizer κ(θ) = arg minκ V (θ, κ) s.t. V (θ, κ) − V (θ, κ(θ)) ≤ cδ2 =⇒ κ − κ(θ) ≤ δ; and (vi) supκ |V (θ, κ) − V (θ , κ)| ≤ L θ − θ . Then, for kt = arg minκ∈∆ψ V (θT(π), κ), any policy π, and ∀δ ∈ (0, δ0),

P θ(π) − θ∗ > δ < 1 exp −O T δ4

T

δ2D

and P kT − κ∗ > δ < 1 exp −O T δ8 . δ4D

Better rates for kT are applicable under additional restrictions on θ∗ (see Lemma 6).

To derive the tail bound for θT , we ﬁrst prove that the minimized empirical GMM objective is close to Q¯(θ∗) with high probability (w.h.p.) (using Conditions (i)-(iii)). Then we show that θT is close to

6

θ∗ w.h.p. (using Condition (iv)). Next, we use the inequality for θT to derive the tail bound for k. For this, we show that V (θ∗, k) is close to V (θ∗, κ∗) (using Condition (vi)) and then show that k is close to κ∗ w.h.p. (using Condition (v)).
Theorem 2 (Regret of OMS-ETG). Suppose that Conditions (i)-(iv) of Proposition 2 hold. Let ∆˜ (s) = {sκb + (1 − s)κ : κ ∈ ∆ψ}. Case (a): For a ﬁxed s ∈ (0, 1), if the oracle selection ratio κ∗ ∈ ∆˜ (s), then the regret converges to zero: R∞(πETG) = 0. If κ∗ ∈/ ∆˜ (s), then R∞(πETG) > 0 (non-zero regret). Case (b): Now also suppose that the conditions for Lemma 1 hold. If s = CT η−1 for some constant C and any η ∈ [0, 1), then ∀θ∗ ∈ Θ, R∞(πETG) = 0.
Similar to OMS-ETC, Case (a) of the theorem shows that if the batch size is a constant ﬁxed fraction, some values of κ∗ will be outside the feasible region and thus we do not get zero regret over the entire parameter space. Case (b) shows that to get zero asymptotic regret for every θ∗ ∈ Θ, s must depend on T and be asymptotically negligible. But unlike OMS-ETC, the estimate of κ∗ is updated after every round. The batch fraction s can be as small as CT −1 allowing the agent to collect a constant number of samples in each batch.
We prove the theorem by showing that κT →p κ∗ and applying Proposition 2. To do so for Case (b), we show that the estimated oracle ratio after every round is close to κ∗, i.e., ∀ >
0, P ∀j ∈ [J − 1], kbj ∈ N (κ∗) −−−−→ 1 (by using Lemma 1). Since we move as close as pos-
T →∞
sible to kbj after every round, this ensures that ∀ > 0, P (κT ∈ N (κ∗)) → 1 (and thus κT →p κ∗).
Both OMS-ETC and OMS-ETG are asymptotically equivalent as they can achieve zero regret for every θ∗ ∈ Θ. But our experiments show that OMS-ETG outperforms OMS-ETC (in terms of regret) for small sample sizes. This may be because with OMS-ETG, the estimate of κ∗ keeps improving as more samples are collected instead of being ﬁxed after exploration. This suggests that better estimates of κ∗ may lead to higher-order reductions in MSE.
Remark 1 (Weight matrix W ). For OMS-ETG, the GMM weight matrix W needs to satisfy Condi-
tion (iii) of Lemma 1 till round (J − 1). We denote this matrix by Wvalid in Figure 1b. For the ﬁnal
step of OMS-ETG, we use the standard efﬁcient two-step GMM weight matrix (denoted by Wefﬁcient in Figure 1b) and thus the ﬁnal estimate of θ∗ is still asymptotically efﬁcient. Condition (iii) of
(os) −1
Lemma 1 would hold for the efﬁcient two-step GMM weight matrix (i.e., for W := ΩT (θT ) ) if
∀(j, k), [g˜i,j(θ)g˜i,k(θ)] satisﬁes Property 2 (see Lemma 5 for proof). This would hold if the moments g˜i,j(θ) are uniformly bounded. Condition (iii) can also be satisﬁed with a regularized weight matrix: W := ΩT (θT(os)) + λW I −1 for some λW > 0 as this ensures that W ≤ λ−W1.
Additional exploration. MAB algorithms usually require additional exploration to perform well (e.g., -greedy and upper conﬁdence bound strategies [27, Chapter 7]). However, we empirically noticed that additional exploration hurts performance in our setup. This might be because unlike typical bandit setups where pulling one arm does not improve the estimates of another arm, querying any data source can improve the estimates of the model parameter θ∗ in our setup.

5 Incorporating a Cost Structure

In many real-world settings, the agent has a budget constraint and must pay a different cost to query
each data source. We adapt OMS-ETC and OMS-ETG to this setting where a cost structure is associated with the data sources in ψ. We prove that these policies still have zero asymptotic regret for every θ∗ ∈ Θ. Let (ψi)i∈[|ψ|] be an indexed family. We denote the (known) budget by B ∈ N and by c ∈ R|>ψ0|, a cost vector such that ci is the cost of querying data source ψi. Due to the cost structure,
the horizon T is a random variable dependent on π with T = B . The setting in Section 4 is a
κT c
special case of this formulation when ∀i, ci = 1 and B = T . We defer proofs to Appendix B.

For a ﬁxed policy πk, we have κ(Tπk) = k, for some constant k ∈ ∆ψ. By Proposition 2, as

√ B → ∞, we have B

β(πk) − β∗

−→d N

0, V (θ∗, k) k c

√ . Here we scale by B instead of

T

7

√ T to make comparisons across policies meaningful. The oracle selection ratio is now deﬁned as
κ∗ = arg minκ∈∆ψ V (θ∗, κ) κ c and the asymptotic regret of policy π now is

√

(π)

∗

R∞(π) = AMSE B βT − β

− V (θ∗, κ∗) (κ∗) c .

OMS-ETC-CS (OMS-ETC with cost structure) is an adaptation of OMS-ETC for this setting. We use

Be budget to explore and estimate κ∗ by k = arg minκ∈∆ V (θT , κ) κ c , where Te = eB

ψ

e

κTe c

and κTe = ctr (∆ψ). Exploration strategies that utilize the cost structure can also be used (e.g.,

evenly dividing the budget across data sources while exploring). With the remaining budget, we

collect samples such that κT = proj k, ∆˜ , where ∆˜ is the feasibility region (expression given in

Appendix C). The following proposition shows that OMS-ETC-CS can achieve zero regret.

Proposition 3 (Regret of OMS-ETC-CS). Suppose that the conditions of Theorem 1 hold. If e = o(1) such that Be → ∞ as B → ∞, then ∀θ∗ ∈ Θ, R∞(πETC-CS) = 0.

We propose two ways of adapting OMS-ETG to this setting: (i) OMS-ETG-FS (OMS-ETG with ﬁxed samples) where we collect a ﬁxed number of samples in every round and (ii) OMS-ETG-FB (OMS-ETG with ﬁxed budget) where we spend a ﬁxed fraction of the budget in every round.

Let cmax = maxi∈[|ψ|] ci and cmin = mini∈[|ψ|] ci. In OMS-ETC-FS (Figure 6a), we collect
b = cBmasx samples in every round except the last one (the last batch can be smaller as we may not have enough budget left for a full batch). The number of rounds J is random since, in every round, depending on what we collect, we use up a different amount of the budget. Like OMS-ETG, after each round, we estimate κ∗ and greedily collect samples to get as close to it as possible.

In OMS-ETG-FB (Figure 6b), we spend Bs budget in each round. Thus the number of rounds J is

ﬁxed with J = 1 but the number of samples collected per round is now random. Like OMS-ETG,

after

each

s
round,

we

estimate

κ∗

and

collect

samples

to

get

as

close

to

the

estimate

as

possible.

The

next two Propositions show that both OMS-ETG-FS and OMS-ETC-FB have zero asymptotic regret.

Proposition 4 (Regret of OMS-ETG-FS). Suppose that the conditions of Theorem 2b hold. If s = Bη−1 for any η ∈ [0, 1), then ∀θ∗ ∈ Θ, R∞ (πETG-FS) = 0.
Proposition 5 (Regret of OMS-ETG-FB). Suppose that the conditions of Theorem 2b hold. If s = Bη−1 for any η ∈ [0, 1), then ∀θ∗ ∈ Θ, R∞ (πETG-FB) = 0.

6 Experiments

6.1 Synthetic data

We validate our methods on synthetic data generated from known causal graphs (see Appendix D for
parameter values and moment conditions used). In our experiments (including the ones in Section 6.2), for OMS-ETG, we use the regularized GMM weight matrix with λW := 0.01 (see Remark 1). The regret is only minimally affected with an unregularized matrix (λW := 0) despite theoretical guarantees only holding for the regularized case (the maximum change in regret was 1.98%) and thus
our conclusions do not change. We ﬁrst simulate data from a linear IV graph (Figure 2a) and compare the performance of different polices (Figure 3a). We use ψ = {{Z, X}, {Z, Y }} and assume that both sources cost the same. We set parameter values such that κ∗ ≈ [0.36, 0.64] . We compare policies based on relative regret (RR) with respect to the oracle policy:

MSE(π) − MSE(oracle)

Relative regret = RR(π) =

MSE(oracle)

× 100%.

The MSE values are computed across 12, 000 runs. The label etc_{x} in the plot refers to OMSETC with exploration fraction e = x (e.g., etc_0.1 means e = 10%). Similarly, etg_{x} refers to OMS-ETG with s = x. We see that as the horizon increases, the RR of all policies converges to zero. This supports the claim that both OMS-ETC and OMS-ETG have zero asymptotic regret. However, when the horizon is small (T = 300), both etc_0.1 and etc_0.2 perform poorly due to insufﬁcient exploration. In contrast, OMS-ETG has close to zero RR even for small horizons which shows that repeatedly improving the parameter estimates can lead to faster convergence of regret.

8

Relative regret (%) Relative regret (%) Relative regret (%)

Z2

W

X

Y

Z

X

Y

Z1

X

M

Y

(a) Instrumental variable graph.

(b) Two IVs graph.

(c) Confounder-mediator graph.

Figure 2: Examples of causal models—with treatment X and outcome Y —where the ATE can be identiﬁed by different data sources returning different subsets of variables.

Relative regret vs horizon

60

etc_0.1

50

etc_0.2 etc_0.4

40

etg_0.1

30

etg_0.2

20

10

0

10
4T0o0tal sa6m00ples c8o0l0lecte1d00(H0 oriz1o2n0)0

(a) IV model.

60 Relative regret vs horizon etc_0.1

50

etc_0.2 etc_0.4

40

etg_0.1

30

etg_0.2

20

10

0

200 Tot4a0l0samp60le0s co8ll0e0cted10(H00orizo12n0)0

(b) Two IVs model.

Relative regret vs budget

25

20

etc_0.2

15

etc_0.4

10

etg-fb_0.2 etg-fs_0.2

5

fixed_equal

0

500 600 700 T8o00tal9b0u0dg10e0t0 1100 1200 1300

(c) Confounder-mediator model.

Figure 3: Relative regret (RR) across horizons/budgets for different policies (error bars denote 95% CIs). (a) RR for the IV model (Figure 2a) with ψ = {{Z, X}, {Z, Y }}. (b) RR for the two IVs model (Figure 2b) with ψ = {{X, Y, Z1}, {X, Y, Z2}}. (c) RR for the confounder-mediator model (Figure 2c) with ψ = {{X, Y, W }, {X, Y, M }} and a cost structure.

Next, we simulate data from a linear graph with two IVs (Figure 2b). Here ψ = {{X, Y, Z1}, {X, Y, Z2}} and both choices cost the same. We set the parameters such that κ∗ = [0, 1] (κ∗ is on the corner of the simplex). We compare the RR across various horizons (Figure 3b). We see the OMS-ETC performs worse than OMS-ETG for small horizons. One difference from the previous case (Figure 3a) is that etc_0.4 performs poorly even for large horizons. This is because after using 40% of the samples for exploration, the feasibility region is not large enough to get close to the corner of the simplex. This demonstrates another beneﬁt of OMS-ETG over OMS-ETC: OMS-ETG can achieve close to zero regret in ﬁnite samples when the oracle ratio κ∗ is either on the boundary or in the interior of the simplex.
Finally, we simulate data from a linear confounder-mediator graph (Figure 2c). Here, both the backdoor (using {X, Y, W }) and frontdoor (using {X, Y, M }) adjustments are applicable [31, Section 3.3]. We use ψ = {{X, Y, W }, {X, Y, M }} with cost structure c = [1.8, 1] (confounders W cost more than the mediators M ). We set the parameters such that κ∗ ≈ [0.15, 0.85] . We see similar conclusions as the previous cases. OMS-ETC with low exploration performs poorly but converges for large horizons. Both OMS-ETG variants—OMS-ETG-FS and OMS-ETG-FB—have close to zero RR for all horizons. We see no signiﬁcant difference between the regret of OMS-ETG-FS and OMS-ETG-FB. The policy ﬁxed_equal is a ﬁxed policy that collects an equal fraction of both subsets. Its RR does not converge and is substantially higher than the oracle (≈ 20%). This demonstrates that adaptive policies can lead to signiﬁcant gains in MSE over ﬁxed policies and that our methods remain applicable even with an associated cost structure on the data sources.
6.2 Semi-synthetic data
IHDP. Hill [19] constructed a dataset based on the Infant Health and Development Program (IHDP). The data [10] is from a randomized experiment studying the effect of home visits by a trained provider on future cognitive test scores of children. Following Hill [19], we create an observational dataset by removing a non-random subset of the data. The treatment X is binary. The dataset contains pre-treatment covariates which are measurements on the mother and the child. For simplicity, we only use two covariates: birth weight (continuous) (W1) and whether the mother smoked (binary) (W2). For each sample of the generated semi-synthetic data, (X, W1, W2) are sampled uniformly at random from the real data. The outcome Y (continuous) is simulated: Y := α1W1 + α2W2 + βX + y,
9

W1

W2

X

Y

(a) IHDP data causal graph

Relative regret (%) Relative regret (%)

Relative regret vs budget
50

40

etc_0.2

30

etc_0.4 etg-fb_0.4

20

etg-fs_0.4 collect_all

10

0
500 600 700 T8o0t0al 9b0u0dg10e0t0 1100 1200 1300

(b) RR for IHDP data

Relative regret vs horizon

30

etc_0.2 etc_0.4

etg_0.1

20

etg_0.2 fixed_equal

10

0

10
6000 Tota80l0s0amp1l0e0s00coll1e2c0t0e0d (H14o0r0iz0on)16000

(c) RR for Earnings data

Figure 4: Relative regret (RR) on semi-synthetic data (error bars denote 95% CIs). For both IHDP (b) and Earnings data (c), adaptive policies converge to zero RR but ﬁxed policies (collect_all, ﬁxed_equal) suffer constant regret. OMS-ETG outperforms OMS-ETC for small horizons.

where y ∼ N (0, σy2), α1, α2, β ∈ R, and σy2 ∈ R+ (see Figure 4a). Here α1, α2, β, and σy are model parameters with β being the ATE (target parameter).
For this experiment, we use ψ = {{X, Y, W1}, {X, Y, W2}, {X, Y, W1, W2}} with cost structure c = [1, 3, 3.5] . Thus, at each step, the agent can collect either one of the covariates or both of them, and each choice has a distinct cost. Setting model parameters such that κ∗ ≈ [0.59, 0, 0.41] , we compare performance across policies for various total budgets (Figure 4b). The policy collect_all is a ﬁxed policy that collects {X, Y, W1, W2} at every step. This policy has higher RR (≈ 50% higher MSE than the oracle) for all budgets demonstrating that collecting all covariates for every sample can be sub-optimal. The policy etc_0.2 does poorly with a small budget whereas etc_0.4 has close to zero RR. Both OMS-ETG-FB and OMS-ETG-FS have close to zero RR for all horizons.
The Vietnam draft and future earnings. Angrist [2] computed the effect of veteran status on future earnings from the Vietnam draft lottery data [3] using an IV (Figure 2a). The IV Z (binary) indicates whether an individual was eligible for the draft based on a random lottery. The treatment X (binary) indicates whether they actually served. The outcome Y (continuous) represents their future earnings. The IV removes bias caused by certain types of men being more likely to serve. In this dataset, {Z, X} and {Z, Y } were collected using different data sources (thus {Z, X, Y } are not observed simultaneously) , which suits our framework. We construct a semi-synthetic dataset that closely matches the real data so that we know the ground-truth causal effect (needed to compute the MSE). For each instance, we sample Z uniformly at random from the empirical distribution. X is sampled from the Bernoulli distribution P(X|Z) with conditional probabilities given by the empirical distribution (values taken from [2, Table 2]). We generate the outcome as Y := βX + γ + , where ∼ N (0, σ2) and ⊥⊥ X. The parameters β, γ, and σ2 are set such that the distribution of (Z, Y ) is close to the real data (see Appendix D.5 for details). We compare the RR of our policies on this dataset (Figure 4c). Most adaptive policies converge to near zero RR as the horizon gets large. OMS-ETC does poorly with low exploration while OMS-ETG policies have signiﬁcantly lower RR for smaller horizons. By contrast, ﬁxed_equal (the ﬁxed policy that queries both data sources equally) suffers constant regret and has ≈ 25% higher regret than the oracle even for large horizons. This demonstrates that adaptive policies can lead to substantial MSE gains in a real-world setting.
7 Conclusion
This paper takes some initial strides towards endogenizing decisions about which variables to solicit into the modeling process. Addressing the problem of deciding, sequentially, which data sources to query in order to efﬁciently estimate a parameter, we developed the online moment selection (OMS) framework and two instantiations: OMS-ETC and OMS-ETG. We prove that over the entire parameter space, adaptive data collection with either method can provide substantial MSE gains. While our work focuses on ATE estimation, our framework is more broadly applicable to any parameter identiﬁed by moment conditions. In future work, we hope to apply our framework to more general prediction problems, addressing practical considerations including high-dimensional data and complex model classes (e.g., neural networks). Moreover, in real-world settings, common assumptions like ignorability rarely hold. We hope to extend our framework to overcome issues such as model misspeciﬁcation, or to overcome biases present in some, but not all data sources.

10

References
[1] D. W. Andrews. Consistent moment selection procedures for generalized method of moments estimation. Econometrica, 1999.
[2] J. D. Angrist. Lifetime earnings and the vietnam era draft lottery: evidence from social security administrative records. The American Economic Review, 1990.
[3] J. D. Angrist. Replication data for: Lifetime Earnings and the Vietnam Era Draft Lottery: Evidence from Social Security Administrative Records, 2009. URL https://doi.org/10. 7910/DVN/PLF0YL.
[4] E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 2016.
[5] J. Bell. The lindeberg central limit theorem, 2015.
[6] X. Cheng and Z. Liao. Select the valid and relevant moments: An information-based lasso for gmm with many moments. Journal of Econometrics, 2015.
[7] M. D. Cocci and M. Plagborg-Møller. Standard errors for calibrated parameters. Princeton University, 2019.
[8] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal of artiﬁcial intelligence research, 1996.
[9] S. G. Donald, G. W. Imbens, and W. K. Newey. Choosing instrumental variables in conditional moment restriction models. Journal of Econometrics, 2009.
[10] V. Dorie. Non-parametrics for causal inference. https://github.com/vdorie/npci, 2016.
[11] K. Gan, A. A. Li, Z. C. Lipton, and S. Tayur. Causal inference with selectively-deconfounded data. arXiv preprint arXiv:2002.11096, 2020.
[12] S. Gupta, Z. C. Lipton, and D. Childers. Estimating treatment effects with observed confounders and mediators. arXiv preprint arXiv:2003.11991, 2020.
[13] A. R. Hall. Generalized method of moments. Oxford university press, 2005.
[14] A. R. Hall and F. P. Peixe. A consistent method for the selection of relevant instruments. Econometric reviews, 2003.
[15] P. Hall and C. C. Heyde. Martingale limit theory and its application. Academic press, 1980.
[16] J. D. Hamilton. Time series analysis. Princeton university press, 1994.
[17] L. P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica: Journal of the Econometric Society, 1982.
[18] L. Henckel, E. Perkovic´, and M. H. Maathuis. Graphical criteria for efﬁcient total effect estimation via adjustment in causal linear models. arXiv preprint arXiv:1907.02435, 2019.
[19] J. L. Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 2011.
[20] S.-J. Huang, M. Xu, M.-K. Xie, M. Sugiyama, G. Niu, and S. Chen. Active feature acquisition with supervised matrix completion. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2018.
[21] P. Hünermund and E. Bareinboim. Causal inference and data-fusion in econometrics. arXiv preprint arXiv:1912.09104, 2019.
[22] S. Ji and L. Carin. Cost-sensitive feature acquisition and classiﬁcation. Pattern Recognition, 2007.
11

[23] M. Kato. Adaptive doubly robust estimator and paradox concerning logging policy: Off-policy evaluation from dependent samples, 2021.
[24] M. Kato. Adaptive doubly robust estimator from non-stationary logging policy under a convergence of average probability, 2021.
[25] M. Kato, T. Ishihara, J. Honda, and Y. Narita. Adaptive experimental design for efﬁcient treatment effect estimation, 2020.
[26] P. Kumar and A. Gupta. Active learning query strategies for classiﬁcation, regression, and clustering: A survey. Journal of Computer Science and Technology, 2020.
[27] T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
[28] S. Lee and E. Bareinboim. Causal effect identiﬁability under partial-observability. In International Conference on Machine Learning. PMLR, 2020.
[29] H. Li, W. Miao, Z. Cai, X. Liu, T. Zhang, F. Xue, and Z. Geng. Causal data fusion methods using summary-level statistics for a continuous outcome. Statistics in Medicine, 2020.
[30] W. K. Newey and D. McFadden. Large sample estimation and hypothesis testing. Handbook of econometrics, 1994.
[31] J. Pearl. Causality. Cambridge university press, 2009.
[32] A. Rakhlin, K. Sridharan, and A. Tewari. Sequential complexities and uniform martingale laws of large numbers. Probability Theory and Related Fields, 2015.
[33] A. Rotnitzky and E. Smucler. Efﬁcient adjustment sets for population average treatment effect estimation in non-parametric causal graphical models. arXiv preprint arXiv:1912.00306, 2019.
[34] M. Saar-Tsechansky, P. Melville, and F. Provost. Active feature-value acquisition. Management Science, 2009.
[35] B. Settles. Active learning literature survey. Computer Science Technical Report 1648, 2009.
[36] E. Smucler, F. Sapienza, and A. Rotnitzky. Efﬁcient adjustment sets in causal graphical models with hidden variables. arXiv preprint arXiv:2004.10521, 2020.
[37] C. Squires, S. Magliacane, K. Greenewald, D. Katz, M. Kocaoglu, and K. Shanmugam. Active structure learning of causal dags via directed clique trees. In Advances in Neural Information Processing Systems, 2020.
[38] T. Tao. Topics in random matrix theory. American Mathematical Soc., 2012.
[39] G. Tauchen. Diagnostic testing and evaluation of maximum likelihood models. Journal of Econometrics, 1985.
[40] S. Tikka, A. Hyttinen, and J. Karvanen. Causal effect identiﬁcation from multiple incomplete data sources: A general search-based approach. arXiv preprint arXiv:1902.01073, 2019.
[41] O. Urminsky, C. Hansen, and V. Chernozhukov. Using double-lasso regression for principled variable selection. Available at SSRN 2733374, 2016.
[42] R. Vershynin. High-dimensional probability: An introduction with applications in data science. Cambridge university press, 2018.
[43] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press, 2019.
[44] S. Wang, S. E. Yi, S. Joshi, and M. Ghassemi. Confounding feature acquisition for causal effect estimation. In Machine Learning for Health. PMLR, 2020.
[45] J. Witte, L. Henckel, M. H. Maathuis, and V. Didelez. On efﬁcient adjustment in causal graphs. Journal of Machine Learning Research, 2020.
12

[46] R. Zhan, Z. Ren, S. Athey, and Z. Zhou. Policy learning with adaptively collected data. arXiv preprint arXiv:2105.02344, 2021.
[47] C. Zhang, S. J. Taylor, C. Cobb, J. Sekhon, et al. Active matrix factorization for surveys. Annals of Applied Statistics, 2020.
[48] K. W. Zhang, L. Janson, and S. A. Murphy. Inference for batched bandits, 2021. [49] K. W. Zhang, L. Janson, and S. A. Murphy. Statistical inference with m-estimators on bandit
data, 2021.
13

A Omitted Proofs for Section 4

A.1 Proof of Proposition 1 (Consistency)

Proposition 6 (MDS LLN [16, Example 7.11]). Let Y¯T be the sample mean from a martingale

difference

sequence

(MDS),

Y¯T

=

1 T

T t=1

Yi,

with

E [|Yt|r]

<

∞

for

some

r

>

1.

Then

Y¯T

→p

0.

Lemma 2 (Uniform convergence). Let ai(θ) := Sia˜(θ, Xi) be a real-valued function where Si ∈

{0, 1} is Hi−1-measurable and Xi are i.i.d. Suppose that (i) Θ is compact and (ii) a˜(θ, Xi) satisﬁes

Property 1. Then

1T

p

sup θ∈Θ T

[ai(θ) − Sia∗(θ)] → 0,

i=1

where a∗(θ) = E[a˜(θ; Xi)].

Proof. We follow a standard uniform law of large numbers proof (e.g. Tauchen [39, Lemma 1]) and modify it to work for dependent data. The key modiﬁcation is replacing the law of large numbers (LLN) in that proof with a MDS LLN.

Let (θ1, θ2, . . . , θK ) be a minimal δ-cover of Θ and Nδ(θk) denote the δ-ball around θk. By compactness of Θ, K is ﬁnite. For k ∈ [K] and θ ∈ Nδ(θk), we have

1T T [ai(θ) − Sia∗(θ)]
i=1

1T

= T

[ai(θ) − ai(θk) + ai(θk) − Sia∗(θk) + Sia∗(θk) − Sia∗(θ)]

i=1

1T

1T

1T

≤ T |ai(θ) − ai(θk)| + T [ai(θk) − Sia∗(θk)] + T |Sia∗(θk) − Sia∗(θ)|

i=1

i=1

i=1

1T

1T

1T

= T |Si (a˜(θ; Xi) − a˜(θk; Xi))| + T [ai(θk) − Sia∗(θk)] + T |Si (a∗(θk) − a∗(θ))|

i=1

i=1

i=1

1T

1T

≤ T |a˜(θ; Xi) − a˜(θk; Xi)| + T [ai(θk) − Sia∗(θk)] + |a∗(θk) − a∗(θ)| .

i=1

i=1

We now show that each of the three terms on the RHS above is small. In the third term, by continuity of a∗(θ), ∀ > 0, ∃δ > 0 s.t. |a∗(θk) − a∗(θ)| < .

In the second term, [ai(θk) − Sia∗(θk; Si)] is a MDS. By Property 1(i) and Proposition 6, we have

1 T

T i=1

[ai

(θk

)

−

Si

a∗

(θk

)]

→p 0.

Next, we examine ﬁrst term on the RHS. Let ui(δ) = supθ,θ ∈Θ, θ−θ ≤δ |a˜(θ, Xi) − a˜(θ , Xi)|.
By continuity of a˜(θ, Xi), compactness of Θ, and the Heine-Cantor theorem, a˜(θ, Xi) is uniformly continuous in θ. This ensures that ui(δ) is continuous in δ and thus ui(δ) ↓ 0 as δ ↓ 0. Since ui(δ) ≤ 2A(Xi) (by Property 1(iii)), using dominated convergence, we have E[ui(δ)] ↓ 0 as δ ↓ 0. Therefore, ∀ > 0, ∃δ > 0 s.t. E[ui(δ)] < . Thus we can write the ﬁrst term as

1T

1T

T |a˜(θ; Xi) − a˜i(θk; Xi)| ≤ T ui(δ)

i=1

i=1

1T = T ui(δ) − E[ui(δ)] + E[ui(δ)]
i=1

1T ≤ T ui(δ) − E[ui(δ)] +
i=1

(a)
= op(1) + ,

14

where (a) follows by the weak law of large numbers which applies because E[ui(δ)] ≤ E[A(Xi)] < ∞ (by Property 1(iii)).
Proposition (Consistency). Suppose that (i) Assumption 2 holds, (ii) ∀j ∈ [M ], g˜t,j(θ) satisﬁes Property 1, and (iii) ∀(i, j) ∈ [M ]2, g˜t(θ)g˜t(θ) i,j satisﬁes Property 1; Then, for any policy π, θ(π) −−−p−→ θ∗.
T T →∞

Proof. We begin by deﬁning the empirical and population analogues of the two-step GMM objective for a given policy π:

Empirical objective: Q(Tπ)(θ) =

1T T gt(θ)
t=1

1T

W T

gt(θ) ,

t=1

Population objective: Q¯(Tπ)(θ) =

1T T E [gt(θ)|Ht−1]
t=1

1T

W T

E [gt(θ)|Ht−1]

t=1

1T

1T

= T

m(st) ⊗ g∗(θ) W T

m(st) ⊗ g∗(θ)

t=1

t=1

= [mT ⊗ g∗(θ)] W [mT ⊗ g∗(θ)] ,

where g∗(θ)

=

E [g˜i(θ)] and mT

=

1 T

the one-step GMM estimate and

T

(os) −1

(os)

t=1 m(st). We have W = ΩT (θT ) , where θT is

1T

ΩT (θ) = T

gt(θ)gt(θ)

t=1

1T = T m(st)m(st) ⊗ g˜t(θ)g˜t(θ) .
t=1

Furthermore, we have W = [mΩ(κT ) ⊗ Ω(θ∗)]−1, where

T

mΩ(κT ) =

m(st)m(st) ,

t=1

Ω(θ) = E g˜t(θ)g˜t(θ) .

The two-step GMM estimator is obtained by minimizing the empirical objective: θT = arg minθ∈Θ QT(π)(θ). At the true parameter θ∗, Q¯(Tπ)(θ∗) = 0 and by Assumption 2(a), θ∗ uniquely minimizes Q¯(Tπ)(θ). By Newey and McFadden [30, Theorem 2.1], supθ∈Θ Q(Tπ)(θ) − Q¯(Tπ)(θ) →p
0 =⇒ θT →p θ∗.

Uniform convergence of Q(Tπ)(θ). We now prove that supθ∈Θ Q(Tπ)(θ) − Q¯(Tπ)(θ) →p 0. Following the proof of Newey and McFadden [30, Theorem 2.6], we have

Q(Tπ)(θ) − Q¯(Tπ)(θ)

1T

2 2

≤ T

[gt(θ) − m(st) ⊗ g∗(θ)] W + 2 g∗(θ)

t=1

g∗(θ) 2 W − W .

1T T [gt(θ) − m(st) ⊗ g∗(θ)]
t=1

W+ (2)

15

We ﬁrst prove that W − W →p 0. Due to Condition (iii) of the theorem, we can apply Lemma 2 to get

∀ (i, j) ∈ [M ]2, ∀ ∴ ∀ (i, j) ∈ [M ]2, ∀ ∴ ∀ (i, j) ∈ [M ]2, ∀ ∴ ∀ (i, j) ∈ [M ]2, ∀

> 0, P > 0, P > 0, P > 0, P

sup ΩT (θ)i,j − [mΩ(κT ) ⊗ Ω(θ)] >
θ∈Θ

→ 0,

ΩT (θT(os))i,j − mΩ(κT ) ⊗ Ω(θT(os)) > → 0,

ΩT (θT(os))i,j − [mΩ(κT ) ⊗ Ω(θ∗)] >

(a)
−−→ 0,

Wi,j − Wi,j >

(b)
−−→ 0,

∴ W − W →p 0,
where (a) follows because θT(os) →p θ∗ (by Proposition 1) and (b) by the continuous mapping theorem. Therefore, we have

W ≤ W + op(1)

≤ lim sup [mΩ(κT ) ⊗ Ω(θ∗)]−1 +op(1).
T →∞

:=λ0

Substituting these results in Eq. 2, we get

T

2

Q(π)(θ) − Q¯(π)(θ) ≤ 1

T

T

T

[gt(θ) − m(st) ⊗ g∗(θ)] λ20 + 2 g∗(θ) T1

t=1

Thus, to

supθ∈Θ

1 T

show uniform convergence of Q(Tπ)(θ), we need

T t=1

[gt

(θ)

−

m(st

)

⊗

g∗

(θ)]

→p 0. For any

> 0, we have

T
[gt(θ) − m(st) ⊗ g∗(θ)]
t=1
to show that

λ0 + op(1).

1T

P sup θ∈Θ T

[gt(θ) − m(st) ⊗ g∗(θ)] <

t=1





M 1T

≥ P sup
θ∈Θ

T [gt,j(θ) − mj(st)g∗(θ)j] < 

j=1 t=1

(a)

M

≥ 1− P

j=1

1T

sup θ∈Θ T

[gt,j(θ) − mj(st)g∗(θ)j] ≥ M

t=1

(b)
≥ 1 − op(1),

∴ sup
θ∈Θ

1T T [gt(θ) − m(st) ⊗ g∗(θ)]
t=1

→p 0,

where (a) follows by the union bound and (b) by applying Lemma 2 for every j ∈ [M ] (using Condition (ii)).

A.2 Proof of Proposition 2 (Asymptotic normality)

Proposition 7 (Martingale CLT [15, Corollary 3.1]). Let Mi with 1 ≤ i ≤ n be a martingale adapted

to the ﬁltration Fi with differences Xi = Mi − Mi−1 and M0 = 0. Suppose that the following two conditions hold: (i) (Conditional Lindeberg) ∀ > 0, ni=1 E Xi2I (|Xi| > ) |Fi−1 →p 0, and (ii) (Convergence of conditional variance) For some constant σ > 0, ni=1 E Xi2|Fi−1 →p σ2.

Then

n i=1

Xi

−→d

N (0,

σ2).

Proposition (Asymptotic normality). Suppose that (i) θT(π) −→p θ∗; (ii) ∀(i, j) ∈ [M ] ×

[D], ∂∂g˜θt (θ) satisﬁes Property 1; (iii) ∃δ > 0 such that E g˜i(θ∗) 2+δ < ∞, and (iv) (Se-
i,j

lection ratio convergence) κ(Tπ) →p k for some constant k ∈ ∆ψ. Then θT is asymptotically normal:

√ (π) ∗ d

∗

T (θT − θ ) → N (0, Σ(θ , k)) ,

16

where Σ(θ∗, k) is a constant matrix that depends only on θ∗ and k. By Assumption 2(e) and the Delta

method, βT is asymptotically normal:

√

∗d

∗

∗

∗

∗

∗

T (βT − β ) → N (0, V (θ , k)) , where V (θ , k) = ∇θftar(θ ) [Σ(θ , k)]∇θftar(θ ).

Proof. We follow a standard GMM asymptotic normality proof (e.g. Newey and McFadden [30, Theorem 3.4]) and modify it to work for dependent data. Applying the GMM ﬁrst-order condition to the two-step GMM estimator, we get

√T (θT − θ∗) = G (θT )Ω(θT(os))−1G(θ˜) −1 G (θT )Ω(θT(os))−1 √1T T gi(θ∗),
t=1

where θT(os) is the one-step GMM estimator, θ˜ is a point on the line-segment joining θT and θ∗,

1 T ∂gt(θ) G(θ) =
T t=1 ∂θ

1 T ∂m(st) ⊗ g˜t(θ) =
T t=1 ∂θ





1T

∂g˜t(θ)

= T [m(st), m(st), . . . , m(st)] ⊗ ∂θ  ,

t=1

D times

1T =
T t=1

mG(st) ⊗

∂g˜t(θ) ∂θ

, and

1T

Ω(θ) = T

gt(θ)gt(θ)

t=1

1T = T m(st)m(st) ⊗ g˜t(θ)g˜t(θ) ,
t=1

1T = T mΩ(st) ⊗ g˜t(θ)g˜t(θ) ,
t=1

where mG(st) = [m(st), m(st), . . . , m(st)] is a M × D matrix and mΩ(st) = m(st)m(st) .

D times

Convergence of G(θT ). Let G(θ) = E ∂g˜∂tθ(θ) . Applying Lemma 2 to every element of G (using Condition (ii)) and using the union bound, we get

sup G(θ) −
θ∈Θ

1T T mG(st)
t=1

⊗ G(θ) →p 0,

1T ∴ ∀ > 0, P G(θT ) − T mG(st) ⊗ G(θT ) > → 0. (3)
t=1

Since κT →p k for some constant k (by Condition (iv)), T1

T t=1

mG(st)

also converges in

probability to a constant matrix. That is, T1

T t=1

mG(st)

→p

m∗G(k)

for

some

constant

matrix

m∗G(k) that only depends on k. By the continuity of G and the fact that θT →p θ∗ (by Condition (i)),

we have G(θT ) →p G(θ∗). Using these results with Eq. 3, we get

G(θT ) →p m∗G(k) ⊗ G(θ)

= G∗(θ∗, k),

(4)

Similarly, G(θ˜) −−p→ G∗(θ∗, k),

(5)

(a)

17

where G∗(θ∗, k) = m∗G(k) ⊗ G(θ∗) and (a) follows because θ˜ →p θ∗.

Convergence of the weight matrix W . Let Ω(θ) = E g˜t(θ)g˜t(θ) . By applying Lemma 2 to every element of Ω (using Condition (iii)) and the union bound, we get

sup Ω(θ) −
θ∈Θ

1T T mΩ(st)
t=1

⊗ Ω(θ) →p 0,

(os)

1T

(os)

∴ ∀ > 0,P Ω(θT ) − T mΩ(st) ⊗ Ω(θT ) > → 0.

(6)

t=1

Since κT →p k for some constant k (by Condition (iv)), T1

T t=1

mΩ(st)

→p m∗Ω(k) for some

constant matrix m∗Ω(k) that only depends on k. By continuity of Ω and the fact that θT(os) →p θ∗ (which follows by Proposition 1), we have Ω(θT(os)) →p Ω(θ∗). Using these results with Eq. 6, we get

Ω(θT(os)) →p m∗Ω(k) ⊗ Ω(θ∗) = Ω∗(θ∗, k),

∴ W = Ω(θT(os))−1 →p Ω∗(θ∗, k)−1,

(7)

where Ω∗(θ∗, k) = m∗Ω(k) ⊗ Ω(θ∗).

Asymptotic normality of √1T Tt=1 gi(θ∗). For this part, we use the Cramer-Wold theorem and the martingale CLT in Proposition 7. For any v ∈ RM s.t. v = 1, v √gi(θ∗) is a MDS be-
T
cause E v gi(θ∗)|Hi−1 = v E [gi(θ∗)|Hi−1] = 0. We now show that the two conditions of Proposition 7 apply to this MDS.
(i) Conditional Lindeberg: The Lyapunov condition implies the Lindeberg condition [5, pg. 6]. In our case, the Lyapunov condition is easier to check and we show that it holds. For some δ > 0, we have

1T T 1+δ/2 v
i=1

2+δ (a) 1

T

gi(θ∗) ≤

T 1+δ/2

i=1

v 2+δ

gi(θ∗) 2+δ

(b) 1

T

= T 1+δ/2

i=1

gi(θ∗) 2+δ

1T ∴ T 1+δ/2 E
i=1

v gi(θ∗) 2+δ Hi−1 ≤ 1

T
E

T 1+δ/2

i=1

gi(θ∗) 2+δ Hi−1

1T = T 1+δ/2 E
i=1

m(si) ⊗ g˜i(θ∗) 2+δ

(c) 1

T

≤ T 1+δ/2 E

i=1

g˜i(θ∗) 2+δ

(d)
→ 0,

where (a) follows by Cauchy-Schwarz, (b) because v = 1, (c) because m(si) is a binary vector, and (d) because E g˜i(θ∗) 2+δ < ∞ (by Condition (iii)).

18

(ii) Convergence of conditional variance: The conditional variance can be written as

1T

1T

E v gt(θ∗)gt(θ∗) v Hi−1 =

v E gt(θ∗)gt(θ∗)

T

T

t=1

t=1

1T = v T m(st)m(st)
t=1

(a)
−−→ v

[m∗ (k) ⊗ Ω(θ∗)] v

p

Ω

= v [Ω∗(θ∗, k)] v,

Hi−1 v ⊗ Ω(θ∗) v

where (a) holds because κT →p k (by Condition (iv)). Thus, using Proposition 7, ∀v ∈ RM s.t. v = 1, we have

1T

∗d

∗

√

v gi(θ )v −→ N 0, v Ω∗(θ , k)v .

T i=1

Thus, by the Cramer-Wold theorem, we get

1T

∗d

∗

√

gi(θ ) −→ N (0, Ω∗(θ , k)) .

(8)

T i=1

Asymptotic normality of θT By Eqs. 4, 5, 7, and 8, and Slutsky’s theorem, we get

√

∗d

∗

T (θT − θ ) → N (0, Σ(θ , k)) ,

where Σ(θ∗, k) = G∗ (θ∗, k) Ω∗(θ∗, k)−1 G∗(θ∗, k) −1 .

A.3 Proof of Theorem 1 (Regret of OMS-ETC)
Lemma 3 (Consistency of kt). Suppose that Assumption 3 holds. If θt →p θ∗, then kt →p κ∗ where kt = arg minκ∈∆ψ V (θt, κ).

Proof. By continuity of V , compactness of ∆ψ, and Assumption 3, kt →p arg minκ∈∆ψ V (θ∗, κ) = κ∗.
Theorem (Regret of OMS-ETC). Suppose that (i) Conditions (i)-(iii) of Proposition 2 hold and (ii) Assumption 3 holds. Case (a): For a ﬁxed e ∈ (0, 1), if κ∗ ∈ ∆˜ , then the regret converges to zero: R∞(πETC) = 0. If κ∗ ∈/ ∆˜ , then πETC suffers constant regret: R∞(πETC) = r for some constant r > 0. Case (b): If e depends on T such that e = o(1) and T e → ∞ as T → ∞ (e.g. e = √1 ), then
T
∀θ∗ ∈ Θ, we have R∞(πETC) = 0.

Proof. We ﬁrst analyze Case (a) of the theorem where e is ﬁxed. By Condition (i), θT e →p θ∗. We have k = arg minκ∈∆ψ V (θT e, κ) and therefore k →p κ∗ (by Lemma 3). Thus, if κ∗ ∈ ∆˜ , then

κT →p k and therefore κT →p κ∗. Using Proposition 2, we get

√

∗d

∗∗

T βT − β → N (0, V (θ , κ ))

∴ R∞(πETC) = V (θ∗, κ∗) − V (θ∗, κ∗) = 0.

If κ∗ ∈/ ∆˜ , then κT →p κ¯ = κ∗, where κ¯ = arg minκ∈∆˜ V (θ∗, κ). Using Proposition 2, we have

√

∗d

∗

T βT − β → N (0, V (θ , κ¯))

(a)
∴ R∞(πETC) = V (θ∗, κ¯) − V (θ∗, κ∗) > 0,

19

where (a) follows by Condition (ii).
Now we analyze part (b) of the theorem. When e depends on T such that e = o(1), the feasible region converges to the entire simplex: ∆˜ → ∆ψ as T → ∞. Thus κT − k →p 0. Furthermore, since T e → ∞ as T → ∞, we have k →p κ∗ and therefore κT →p κ∗. Using Proposition 2, we get the desired result.

A.4 Proof of Lemma 1 (GMM concentration inequality)
Proposition 8 (MDS concentration inequality [43, Theorem 2.19]). Let {(Dk, Fk)}∞ k=1 be a MDS, and suppose that E [exp {λDk} |Fk−1] ≤ exp λ22ν2 almost surely for any λ < α1 . Then the sum satisﬁes the concentration inequality

1n

nη2

ν2

P n

Dk > η ≤ 2 exp − 2ν2

if 0 ≤ η < . α

k=1

Lemma 4 (Uniform law for dependent data). Let ai(θ) := Sia˜(θ; Xi), where ai is a real-valued function, Si ∈ {0, 1} is Hi−1-measurable, and Xi ∼iid Pθ∗ . Let a˜∗(θ) = E [a˜(θ; Xi)]. Suppose that a˜(θ) satisﬁes Property 2. Note that E [ai(θ)|Hi−1] = Sia˜∗(θ). Then, for some constant δ0 > 0 and ∀δ ∈ (0, δ0),

1T

1

P sup

[ai(θ) − Sia˜∗(θ)] > δ < exp −O T δ2 .

θ∈Θ T i=1 δD

Proof.

Let U

= {θ1, θ2, . . . , θN } be a minimal δ-cover of Θ. We have N

≤

C δD

for some constant C.

Let q : Θ → U be a function that returns the closest point from the cover: q(θ) = arg minθ ∈U θ −

θ . We have

1T

sup θ∈Θ T

[ai(θ) − Sia˜∗(θ)]

i=1

1T

= sup θ∈Θ T

[ai(θ) − ai(q(θ)) + ai(q(θ)) − Sia˜∗(q(θ)) + Sia˜∗(q(θ)) − Sia˜(θ)]

i=1

1T

1T

1T

≤ sup θ∈Θ T

|ai(θ) − ai(q(θ))| + nm∈a[Nx] T

[ai(θn) − Sia˜∗(θn)] + θs∈uΘp T

Si |a˜∗(q(θ)) − a˜∗(θ)|

i=1

i=1

i=1

1T

1T

= sup θ∈Θ T

|Si (a˜i(θ, Xi) − a˜i(q(θ), Xi))| + nm∈a[Nx] T

[ai(θn) − Sia˜∗(θn)] + sup |a˜∗(q(θ)) − a˜∗(θ)|
θ∈Θ

i=1

i=1

1T

1T

≤ sup θ∈Θ T

|a˜(θ, Xi) − a˜i(q(θ), Xi)| + nm∈a[Nx] T

[ai(θn) − Sia˜∗(θn)] + sup |a˜∗(q(θ)) − a˜∗(θ)| .
θ∈Θ

i=1

i=1

We now examine the three terms on the RHS one at a time.

Third term. By Lipschitzness of a˜∗ (Property 2(i)), we have:

sup |a˜∗(q(θ)) − a˜∗(θ)| ≤ L1 sup q(θ) − θ ≤ L1δ.

θ∈Θ

θ∈Θ

20

Second term. We note that it is a sum of a MDS. By Property 2(ii) and Proposition 8, there exists a constant C1 > 0 such that for δ ∈ (0, C1), we have

∀n ∈ [N ], P

1T T [ai(θn) − Sia˜∗(θn)] < δ
i=1

∴P

1T

max n∈[N ] T

[ai(θn) − Sia˜∗(θn)] < δ

i=1

> 1 − exp −O T δ2



1T

> 1−P

T [ai(θn) − Sia˜∗(θn)]

n∈[N ] i=1

> 1 − N exp −O T δ2

> 1 − 1 exp −O T δ2 . δD

 > δ

First term. We have

u∗(η) = E

sup

|a˜i(θ, Xi) − a˜i(θ , Xi)|

θ,θ ∈Θ; θ−θ ≤η

≤ E sup A(Xi, θ)

sup

θ−θ

θ∈Θ

θ,θ ∈Θ; θ−θ ≤η

≤ η sup A(Xi, θ)
θ∈Θ

(a)

≤ A0η.,

(9)

where (a) follows by Property 2(iii).

Suppose that Property 2(iv)(a) holds. Then

1 sup θ∈Θ T

T

1T

|a˜i(θ, Xi) − a˜i(q(θ), Xi)| ≤ T ui(δ)

i=1

i=1

1T ≤ T ui(δ) − u∗(δ) + u∗(δ)
i=1

(a) 1 T

≤ T

ui(δ) − u∗(δ) + A0δ,

i=1

where (a) follows by Eq. 9. By Property 2(iv)(a), (ui(δ) − u∗(δ)) is sub-Exponential. By the sub-exponential tail bound [43, Proposition 2.9], for some constant C2 > 0 and δ ∈ (0, C2), we have

1T

P

ui(δ) − u∗(δ) < δ > 1 − exp −O T δ2

T

i=1

∴P

1T

sup θ∈Θ T

|a˜i(θ, Xi) − a˜i(q(θ), Xi)| < (A0 + 1)δ

i=1

> 1 − exp −O T δ2

1T

∴ P sup

|a˜i(θ, Xi) − a˜i(q(θ), Xi)| < δ > 1 − exp −O T δ2 .

θ∈Θ T i=1

Now suppose that Property 2(iv)(b) holds instead. Then

1T

1T

sup θ∈Θ T

|a˜i(θ, Xi) − a˜i(q(θ), Xi)| ≤ T

sup A(Xi, θ) sup θ − q(θ)

θ∈Θ

θ∈Θ

i=1

i=1

δT ≤ T θs∈uΘp A(Xi, θ) .
i=1

21

Since supθ∈Θ A(Xi, θ) is sub-Exponential, so is

T i=1

supθ∈Θ

A(Xi, θ) . By a sub-Exponential

tail bound [42, Proposition 2.7.1(a)], we have for any C3 > 0,

1T

P T

sup A(Xi, θ) > C3
θ∈Θ

i=1

∴P

1T

sup θ∈Θ T

|a˜i(θ, Xi) − a˜i(q(θ), Xi)| > δC3

i=1

∴P

1T

sup θ∈Θ T

|a˜i(θ, Xi) − a˜i(q(θ), Xi)| > δ

i=1

≤ exp {−O (T C3)} ≤ exp {−O (T C3)} ≤ exp {−O (T )} .

Combining these results together using the union bound, we get

1T

P sup θ∈Θ T

[ai(θ) − Sia˜∗(θ; k)] < (L1 + L2 + 2)δ

i=1

>P

1T

max n∈[N ] T

[ai(θn) − Sia˜∗(θn)] < δ,

i=1

1T T ui(δ) − u∗(δ) < δ
i=1

N
>1− P
n=1

1T T [ai(θn) − Sia˜∗(θn)] > δ − P
i=1

1T T ui(δ) − u∗(δ) > δ
i=1

> 1 − 1 exp −O T δ2 δD

1T

1

∴ P sup

[ai(θ) − Sia˜∗(θ; k)] < δ > 1 − exp −O T δ2 .

θ∈Θ T i=1 δD

Proposition 9 (Boundedness and Property 2(iv)(a)). Property 2(iv)(a) is satisﬁed for bounded function classes, i.e., when a˜i ∞ < A < ∞.

Proof. We have:

ui(η) =

sup

θ,θ ∈Θ, θ−θ

≤ 2 sup |a˜i|
θ∈Θ

≤ 2A.

|a˜(θ, Xi) − a˜(θ , Xi)|
≤η

Thus ui(η) is bounded and therefore sub-Gaussian for every η.

Proposition 10 (Linearity and Property 2(iv)(b)). Suppose that (i) a˜(θ, Xi) is a linear function of θ, i.e., a˜(θ, Xi) = θT φ(Xi)+ρ(Xi), where φ and ρ are arbitrary functions; and (ii) ∀d ∈ [D], φ(Xi)d is sub-Exponential. Then a˜(θ, Xi) satisﬁes Property 2(iv)(b).
22

Proof.

We have that A(Xi, θ)

=

∂ a˜(Xi ;θ) ∂θ

=

φ(Xi) and thus supθ∈Θ

A(Xi, θ)

=

φ(Xi)

≤

D d=1

|φ(Xi)d|.

Therefore,

for

any

η

>

0,

we

have

P sup A(Xi, θ) < η = P ( φ(Xi) < η)
θ∈Θ

D

≥P

|φ(Xi)d| < η

d=1

η ≥ P ∀ d ∈ [D], |φ(Xi)d| < D

(a)

D

≥ 1− P

d=1

η |φ(Xi)d| > D

(b)

D

≥ 1 − exp {−O(η)}

d=1

≥ 1 − exp {−O(η)} ,

where (a) follows by the union bound and (b) because φ(Xi)d is sub-Exponential. This shows that supθ∈Θ A(Xi, θ) is also sub-Exponential (see Vershynin [42, Deﬁnition 2.7.5]).

Remark. Rakhlin et al. [32] derive a uniform martingale LLN and develop sequential analogues of classical complexity measures used in empirical process theory. These techniques are a potential alternative for deriving the tail bound in Lemma 4. However, the conditions required for these techniques are difﬁcult to check. In our case, the dependent and i.i.d. components can be separated more easily. Thus we opted for deriving a uniform concentration bound by modifying the classical uniform LLN proof. Zhan et al. [46] also derive a uniform LLN without requiring boundedness of the martingale difference terms, but with structural assumptions on the summands related to their speciﬁc application.

Lemma (GMM concentration inequality). Let λ∗, C0, η1, η2, and δ0 be some positive constants. Suppose that (i) Assumption 2 holds; (ii) ∀j, g˜i,j(θ) satisﬁes Property 2; (iii) The

spectral norm of the GMM weight matrix W is upper bounded with high probability: ∀δ ∈

(0, C0) , P

W ≤ λ∗

≥

1

−

1 δD

exp

−O

T δ2

(see Remark 1); (iv) (Local strict con-

vexity) ∀θ ∈ Nη1 (θ∗), P ∂∂2θQ2¯ (θ)−1 ≤ h = 1 (Q¯(θ) is deﬁned in Assumption 2(a)); (v)
(Strict minimization) ∀θ ∈ Nη2 (θ∗), there is a unique minimizer κ(θ) = arg minκ V (θ, κ) s.t. V (θ, κ) − V (θ, κ(θ)) ≤ cδ2 =⇒ κ − κ(θ) ≤ δ; and (vi) supκ |V (θ, κ) − V (θ , κ)| ≤ L θ − θ . Then, for kt = arg minκ∈∆ψ V (θT(π), κ), any policy π, and ∀δ ∈ (0, δ0),

P θ(π) − θ∗ > δ < 1 exp −O T δ4

T

δ2D

and P kT − κ∗ > δ < 1 exp −O T δ8 . δ4D

Proof. Below we give the empirical and population analogues of the GMM objective for a given policy π:

Empirical objective: Q(Tπ)(θ) =

1T T gt(θ)
t=1

1T

W T

gt(θ) ,

t=1

Population objective: Q¯(Tπ)(θ) = gT∗ (θ)W gT∗ (θ)

1T

= T

E [gt(θ)|Ht−1]

t=1

1T

W T

E [gt(θ)|Ht−1]

t=1

1T

1T

= T

m(st) ⊗ g˜∗(θ) W T

m(st) ⊗ g˜∗(θ) ,

t=1

t=1

where g˜∗(θ) = E [g˜t(θ)].

23

To simplify notation, let mt = m(st). By the triangle and Cauchy-Shwartz inequalities (see Newey and McFadden [30, Theorem 2.6]),

Q(Tπ)(θ) − Q¯(Tπ)(θ)

T

2

T

1 ≤

[gt(θ) − mt ⊗ g˜∗(θ)] W 2 + 2 g˜∗(θ) 1

[gt(θ) − mt ⊗ g˜∗(θ)] W

T t=1

T t=1

T

2

T

1 ≤

[gt(θ) − mt ⊗ g˜∗(θ)] W 2 + 2C 1

[gt(θ) − mt ⊗ g˜∗(θ)]

T t=1

T t=1

W,

where C = supθ∈Θ g˜∗(θ) . By applying Lemma 4 to each element of the vector gi(θ) and using the union bound, we get:





1T

M

1T

δ

P sup θ∈Θ T

[gt(θ) − mt ⊗ g˜∗(θ)] < δ ≥ P  θs∈uΘp T

[gt,j(θ) − mt,j ⊗ (g˜∗)j(θ)] < M 

i=1

j=1

t=1

This means that, for 0 < δ < 1,

M
≥1− P
j=1

1T

sup θ∈Θ T

[gt,j (θ) − mt,j ⊗ (g˜∗)j (θ)]

t=1

≥ 1 − 1 exp −O T δ2 .

(10)

δ2D

δ >
M

1T T [gt(θ) − mt ⊗ g˜∗(θ)]
t=1

≤ δ, W

≤ λ∗ =⇒

Q(Tπ)(θ) − Q¯(Tπ)(θ)

≤ λ2∗δ2 + 2λ∗Cδ = (2C + λ∗δ)λ∗δ

< (2C + λ∗)λ∗δ,

∴P

sup Q(Tπ)(θ) − Q¯(Tπ)(θ) < (2C + λ∗)λ∗δ
θ∈Θ

≥P

1T

sup θ∈Θ T

[gt(θ) − mt ⊗ g˜∗(θ)] ≤ δ, W

t=1

≤ λ∗

(a)

1T

≥ 1 − P sup θ∈Θ T

[gt(θ) − mt ⊗ g˜∗(θ)] > δ − P

t=1

(b)
≥ 1−

1

exp

−O T δ2

δ2D

∴ P sup Q(π)(θ) − Q¯(π)(θ) < δ ≥ 1 − 1 exp −O T δ2 ,

T

T

θ∈Θ

δ2D

where (a) follows by the union bound and (b) follows by Eq. 10 and Condition (iii). Using this uniform concentration bound, we get

P Q¯(Tπ)(θT ) < Q(Tπ)(θT ) + 2δ ≥ 1 − δ21D exp −O T δ2 ,

P Q(π)(θ∗) < Q¯(π)(θ∗) + δ ≥ 1 − 1 exp −O T δ2 .

T

T

2

δ2D

Since θT minimizes Q(Tπ) almost surely, we have P Q(Tπ)(θT ) ≤ Q(Tπ)(θ∗) = 1. Combining these inequalities using the union bound, we get
P Q¯(Tπ)(θT ) < Q¯(Tπ)(θ∗) + δ ≥ 1 − δ21D exp −O T δ2 ∴ P Q¯(Tπ)(θT ) < δ (≥a) 1 − δ21D exp −O T δ2 ,
where (a) follows because Q¯(Tπ)(θ∗) = 0.

W > λ∗

24

Intuitively, if Q¯(Tπ)(θT ) is small, then we would expect θT to be close to θ∗. To formally show this, we use the local curvature of Q¯(Tπ). By Condition (iv), Q¯(Tπ) is locally strictly convex in the η1-ball Nη1 (θ∗). Therefore, there exists a closed γ-ball Nγ(θ∗) ⊆ Nη(θ∗) such that
∀θ ∈/ Nγ (θ∗), Q¯(Tπ)(θ) > Q¯N , where Q¯N = sup Q¯T(π)(θ).
θ∈Nγ (θ∗)

This is analogous to an identiﬁcation condition and ensures that Q¯T(π)(θ) ≤ Q¯N =⇒ θ ∈ Nγ(θ∗).

Let H(θ) = ∂2∂Q¯θ(2π) (θ). Then, by twice continuous differentiability of g, for θ ∈ Nγ(θ∗), we have

Q¯(Tπ)(θ) (=a) Q¯(Tπ)(θ∗) + (θ − θ∗) [H(θ )] (θ − θ∗)

(b)
=

(θ

−

θ∗)

[H (θ

)]

(θ

−

θ∗)

,

∴ θ − θ∗ 2 ≤ Q¯(Tπ)(θ) H−1(θ )

(c)
≤

Q¯(π)(θ)

h,

T

where in (a), θ is a point on the line segment joining θ; (b) follows because Q¯(Tπ)(θ∗) = 0; and (c) follows by Condition (iv). Thus, for δ < Q¯N , we have
Q¯(Tπ)(θT ) < δ =⇒ θT − θ∗ < √δh ∴ P θT − θ∗ < δ ≥ P Q¯(Tπ)(θT ) < δh2
≥ 1 − 1 exp −O T δ4 . δ2D

Concentration inequality for kT

By Condition (vi), supκ∈∆ψ |V (θT , κ) − V (θ∗, κ)| ≤ L θT − θ∗ . Therefore,

θT − θ∗ < δ =⇒ sup |V (θT , κ) − V (θ∗, κ)| ≤ Lδ.
κ∈∆ψ

Furthermore, we have
sup |V (θT , κ) − V (θ∗, κ)| ≤ Lδ =⇒ V (θ∗, kT ) < V (θT , kT ) + Lδ, and
κ∈∆ψ
V (θT , κ∗) < V (θ∗, κ∗) + Lδ.

Since kT is the minimizer, we have V (θT , kT ) ≤ V (θT , κ∗). Combining these inequalities, we get θT − θ∗ < δ =⇒ V (θ∗, kT ) − V (θ∗, κ∗) < 2Lδ.
Due to Condition (v), we have

V (θ∗, kT ) − V (θ∗, κ∗) < 2Lδ =⇒ kT − κ∗ < ∴ θT − θ∗ < δ =⇒ kT − κ∗ <
∴ P( kT − κ∗ < δ) > 1 − P θT − θ∗

2Lδ ,
c 2Lδ
c < O δ2

> 1 − 1 exp −O T δ8 . δ4D

Lemma 5 (Sufﬁcient condition for W ). Suppose that ∀(j, k), [g˜i,j(θ)g˜i,k(θ)] satisﬁes Property 2.

Let

WT (θT(os))

=

Ω

T

(θ

(os T

)

)

−1

=

1 T

T

(os)

(os) −1

(os)

t=1 gt(θT )gt (θT ) , where θT is the one-step GMM

estimator

(that

uses

W

=

I ).

Then

W

T

(θ

(os T

)

)

satisﬁes

Condition

(iii)

of

Lemma

1.

25

Proof. We deﬁne WT (θ∗) as

T

−1

WT (θ∗) = ΩT (θ∗)−1 = 1 T

E gt(θ∗)gt (θ∗) Ht−1

t=1

T

−1

= 1 m(st)m (st) ⊗ E g˜t(θ∗)g˜ (θ∗) .

T

t

t=1

Let ∆ = ΩT (θT(os)) − ΩT (θT(os)) and λmin denote smallest eigenvalue. Using the eigenvalue stability inequality [38, Section 1.3.3], we get:

λmin

Ω

T

(

θ

(os) T

)

− λmin

Ω

T

(

θ

(os T

)

)

≤ ∆,

∴ WT (θ(os)) = ΩT (θ(os))−1 =

1

≤

1 . (11)

T

T

λmin ΩT (θ(os))

λmin ΩT (θ(os)) − ∆

T

T

By applying Lemma 4 to each term of the matrix and using the union bound, we have

(a)

P sup ΩT (θ) − ΩT (θ) ≤ δ ≥ P sup ΩT (θ) − ΩT (θ) ≤ δ

θ∈Θ

θ∈Θ

F





≥ P sup ΩT,i,j (θ) − ΩT,i,j (θ) ≤ δ
θ∈Θ i,j

∴ P ( ∆ ≤ δ) = P ΩT (θ˜T ) − Ω(θ˜T ) ≤ δ where in (a) . F denotes the Frobenius norm.

≥ 1 − P sup ΩT,i,j (θ) − ΩT,i,j (θ)

i,j

θ∈Θ

= 1 − 1 exp −O T δ2 δD
≥ 1 − 1 exp −O T δ2 , δD

δ > M2 (12)

For some δ0 > 0, let λ¯ = infθ∈Nδ0 (θ∗),κ∈∆ψ λmin (ΩT (θ)). For δ ≤ min δ0, λ¯2 , we have

∴P

(a)
∆ ≤ δ =⇒

WT (θ˜T )

WT (θ˜T ) ≤ λ2¯ ≥ P ( ∆ ≤ δ)

2 ≤ λ¯ ,

(b)
≥ 1−

1

exp

−O

T δ2

,

δD

where (a) follows by Eq. 11 and (b) by Eq. 12.

In the next lemma, we present a concentration inequality for kT with better rates under additional restrictions on θ∗. We do not require these better rates for proving zero regret for OMS-ETG. We present this lemma for the sake of completeness.

Lemma 6 (Another concentration inequality for kT ). Let κ(θ) = arg minκ V (θ, κ), Θboundary =

{θ ∈ Θ : κ(θ) ∈ boundary (∆ψ)}, where boundary (∆ψ) = {κ ∈ ∆ψ : ∃i, s.t. κi = 0},

Θminima = {θ ∈ Θ : ∂∂Vκ (θ, κ(θ)) = 0}, and Θrestricted = Θ \ (Θboundary the conditions of Lemma 1 hold, and (ii) θ ∈ Θrestricted. Then

Θminima) Suppose that (i)

P kT − κ∗ > δ < 1 exp −O T δ4 . δ2D
This means that if θ∗ is not such that the minimizer κ(θ) = arg minκ V (θ, κ) is on the boundary of the simplex and is also a local minimum of V (θ, κ) (informally, κ(θ) is not “just” on the boundary), we can get better rates.

26

𝜖

𝜅∗

𝜅𝑏𝑗

Figure 5: Illustration of the proof of OMS-ETG algorithm. When the event I( ) occurs, (a) if the selection ratio κbj is outside N (κ∗), then then selection ratio in the next round κb(j+1) will move closer to N (κ∗), and (b) if κbj is inside N (κ∗), it remains inside for all future rounds.

Proof. Now we use the tail bound for θT to derive a concentration inequality for kT when θ ∈ Θrestricted. kT is the solution to the following constrained optimization problem:

min V
κ∈R|ψ|

θT , κ

|ψ|
subject to κi = 1.
i=1

The Lagrangian function is





|ψ|

L (θ, κ, λ) = V (θ, κ) + λ  κi − 1 .

i=1

Let f (θ, κ, λ) = ∂L (θ, κ, λ) = ∂V (θ, κ) + λ[1, 1, . . . , 1] . Since λ[1, 1, . . . , 1] = 0, there exists

a

Lagrange

∂κ
multiplier

λ∗

∈

R

∂κ
such

that

f (θ∗,

κ∗,

λ∗)

=

0.

Condition (ii) is required to ensure that f (θ, κ, λ∗) is continuously differentiable in (θ, κ) which
allows us to use the implicit function theorem. To show this, we divide the space Θrestricted into two disjoint sets: (i) Θinterior = Θ \ Θboundary, and (ii) Θstrict-boundary = Θboundary Θcminima. When θ ∈ Θinterior, the constraint will not be active and thus λ∗ = 0. When θ ∈ Θstrict-boundary, the constraint will be active and thus λ∗ > 0. In both cases, f (θ, κ, λ∗) will be continuously differentiable in (θ, κ). Note that if θ ∈ Θ \ Θrestricted, then λ∗ = 0 but f is not differentiable because the constraint is “just” inactive.

Let Y (θ, κ) = ∂f (θ, κ) = ∂2V (θ, κ), and X(θ, κ) = ∂f (θ, κ) = ∂2V (θ, κ). By the implicit

function

∂κ
theorem,

since

Y

(θ∂∗κ, 2κ∗)

is

invertible

(by

∂θ
Condition

(v)),

∂θ∂κ
there

exist

neighbourhoods

N (θ∗) and N (κ∗) and a function φ : N (θ∗) → N (κ∗) such that kT = φ(θT ) and ∂∂φθ (θ) = − Y (θ, φ(θ))−1X(θ, φ(θ)) . By a Taylor expansion, we get

kT

=

φ(θT )

(a)
=

φ(θ∗) +

∂φ (θ˜)

θT − θ∗

∂θ

= κ∗ + ∂φ (θ˜) θT − θ∗ ∂θ

∴ kT − κ∗ ≤ ∂φ (θ˜) θT − θ∗ ∂θ

≤ C θT − θ∗ ,

where in (a) θ˜ is a point on the line segment joining θT and θ∗, and C = supθ∈N (θ∗) Therefore, we have

P ( κT − κ∗ ≤ δ) ≥ P

θT − θ∗

δ ≤

≥ 1 − 1 exp −O tδ4

.

C

δ2D

∂∂φθ (θ) .

A.5 Proof of Theorem 2 (Regret of OMS-ETG) Theorem (Regret of OMS-ETG). Suppose that Conditions (i)-(iv) of Proposition 2 hold. Let ∆˜ (s) = {sκb + (1 − s)κ : κ ∈ ∆ψ}. Case (a): For a ﬁxed s ∈ (0, 1), if the oracle selection ratio
27

κ∗ ∈ ∆˜ (s), then the regret converges to zero: R∞(πETG) = 0. If κ∗ ∈/ ∆˜ (s), then R∞(πETG) = r for some constant r > 0. Case (b): Now also suppose that the conditions for Lemma 1 hold. If s = CT η−1 for some constant C and any η ∈ [0, 1), then ∀θ∗ ∈ Θ, the regret converges to zero:
R∞(πETG) = 0.

Proof. We prove this theorem by ﬁrst showing that κT →p κ∗. Then we can apply Proposition 2 to get the desired result. Recall that b = T s is the batch size.
Case 1: when s ∈ (0, 1) is a ﬁxed constant and κ∗ ∈ ∆˜ (s).
Let I( ) be the event that kbj remains inside an -ball of κ∗ (denoted by N (κ∗)) for all rounds j ∈ [J]. That is, I( ) = ∀j ∈ [J], kbj ∈ N (κ∗) . If κ∗ ∈ ∆˜ (s), then to prove that κT →p κ∗, it
is sufﬁcient to show that ∀ > 0, I( ) occurs w.p.a. 1.
This is because in OMS-ETG, after every round, we move as close to kbj as possible. This is illustrated in Figure 5 for the case when ∆ψ is a 1-simplex. When I( ) occurs, if the selection ratio κbj after round j is outside N (κ∗), we move towards it in the subsequent round and thus κb(j+1) will be closer to N (κ∗). Once the selection ratio enters N (κ∗) (which it is guaranteed to if κ∗ ∈ ∆˜ (s)), it will remain inside N (κ∗) for every round after that. Thus I( ) =⇒ κT ∈ N (κ∗). Therefore, we have

∀ > 0, P(κT ∈ N (κ∗)) ≥ P(I( ))

= P ∀j ∈ [J ], kbj ∈ N (κ∗)


J
= 1−P
j=1

kbj − κ∗

 >

(a)

J

≥ 1− P

j=1

kbj − κ∗ >

(b)
−−→ 1, ∴ κT →p κ∗,

where (a) follows by the union bound and (b) follows because J is ﬁnite and ∀j, kbj →p κ∗ (by Lemma 3). Case 2: when s depends on the horizon T . Case 2(a): when s ∈ Ω(T η−1) for any η ∈ (0, 1). Similar to Case 1, it is sufﬁcient to show that the event I( ) = ∀j ∈ [J], kbj ∈ N (κ∗) occurs w.p.a. 1 for every > 0. However, since J → ∞, consistency of kbj is no longer sufﬁcient to prove
28

this. Instead, we use the concentration inequality in Lemma 1:

∀ > 0, P(κT ∈ N (κ∗)) ≥ P(I( )) = P ∀j ∈ [J ], kbj ∈ N (κ∗)

= P ∀j ∈ [J ], kbj − κ∗ ≤


J
= 1−P
j=1

kbj − κ∗

 >

(a)

J

≥ 1− P

j=1

kbj − κ∗ >

(b)

J

≥ 1−

1 exp −O −T sj 8
4D

j=1

(c)

J

≥ 1−

1 exp −O −T s 8
4D

j=1

= 1 − J exp −O −T s 8
4D

= 1 − 1 exp −O −T s 8 s 4D
→ 1 if s = CT η−1

for any η ∈ (0, 1) and some constant C. Here (a) follows by the union bound, (b) by Lemma 1, and (c) because j ≥ 1. Case 2(b): when s = CT for some constant C > 0. We prove this similarly to Case 2(a). However, in this case, the number of rounds J = 1s ∈ O(T ). Let f = T γ−1 for some γ ∈ (0, 1) and I(f, ) = ∀j ∈ [Jf + 1, . . . , J], kbj ∈ N (κ∗) be the event that kbj remains inside N (κ∗) after the ﬁrst Jf rounds.
29

Since f ∈ o(1), we have I(f, ) =⇒ κT ∈ N (κ∗) for every > 0. This is because the fraction f is asymptotically negligible and thus we can effectively ignore the ﬁrst Jf rounds. Therefore we have

∀ > 0, P(κT ∈ N (κ∗)) ≥ P(I(f, ))

= P ∀j ∈ [Jf + 1, Jf + 2, . . . , J], kbj − κ∗ ≤


J
= 1−P
j=Jf +1

kbj − κ∗

 >

J
≥1− P
j =J f

kbj − κ∗ >

J
≥1−

1 exp −O T sj 8
4D

j=Jf +1

(a)

J

≥ 1−

1 exp −O j 8
4D

j=Jf +1

(b)

J

≥ 1−

1 exp −O Jf 8
4D

j=Jf +1

(c)

J

≥ 1−

1 exp −O T γ 8
4D

j=Jf +1

≥ 1 − J exp −O T γ 8
4D

≥ 1 − T exp −O T γ 8
4D

→ 1,

where (a) follows because T s = C, (b) because j ≥ Jf , and (c) because Jf = O(T γ). We note that it is possible to unify the analysis of Case 2(a) and Case 2(b) by ignoring the ﬁrst Jf rounds in Case 2(a) as well. We prove the two cases separately for the sake of clarity.

B Incorporate Cost Structure
B.1 Proof of Proposition 3 (Regret of OMS-ETC-CS)
Proposition (Regret of OMS-ETC-CS). Suppose that the conditions of Theorem 1 hold. If e = o(1) such that Be → ∞ as B → ∞, then ∀θ∗ ∈ Θ, R∞(πETC-CS) = 0.

Proof. The proof is almost exactly like that of Theorem 1. We prove that κT →p κ∗ and then apply Proposition 2. Let the number of samples used for exploration be Te. Since κTe = |ψ1| , |ψ1| , . . . , |ψ1| , we have

Be

Te =

.

κTe c

Te is not a random variable because κTe is ﬁxed. By Lemma 3, we have kTe →p κ∗.
When e ∈ o(1), the feasible region converges to the entire simplex, i.e., ∆˜ → ∆ψ. Thus κT − kTe →p 0.

30

Input: B, s, c

1 k = ctr (∆ψ) ; 2 b = cBmasx ; 3 Bl = B; 4 j=0;
5 while Bl > 0 do 6 j ← j + 1;
7 last_step = cm Balx ≤ b; 8 if not last_step then

9

Collect b samples s.t. κbj = k;

10

Bl ← B − bj κbjc ;

11

t = b(j + 1);

12

θt = GMM(Ht, W = Wvalid);

13

kt = arg minκ∈∆ψ V (θt, κ) κ c ;

14

k = proj(kmin, ∆˜ j+1(κt));

15 else

16

Collect samples s.t. κT = k;

17

Bl ← 0;

18 end

19 end

20 θT = GMM(HT , W = Wefﬁcient); Output: θT

Input: B, s, c
1 k = ctr (∆ψ); 2 J = Bs ; 3 t = 0; 4 for j ∈ [1, 2, . . . , J] do 5 b = (kBsc) ; 6 t ← t + b;
7 Collect b samples s.t. κt = k;
8 θt = GMM(Ht, W = Wvalid);
9 kt = arg minκ∈∆ψ V (θt, κ) κ c ; 10 k = proj(kmin, ∆˜ j+1(κt)); 11 end
12 θT = GMM(HT , W = Wefﬁcient);
Output: θT

(a) OMS-ETG-FS (ﬁxed samples per batch).

(b) OMS-ETG-FB (ﬁxed budget per batch)

Figure 6: Algorithms for OMS-ETG-FS and OMS-ETG-FB.

B.2 Proof of Proposition 4 (Regret of OMS-ETG-FS)
Proposition (Regret of OMS-ETG-FS). Suppose that the conditions of Theorem 2 hold. If s = Bη−1 and any η ∈ [0, 1), then ∀θ∗ ∈ Θ, R∞ (πETG-FS) = 0.

Proof. We can prove this similarly to Theorem 2. The key difference is that the number of rounds J

is now a random variable. But we can use the fact the J is bounded:

1 ≤ J ≤ cmax ,

s

scmin

1 ∴ J∈O s .

Now we can proceed like Case 2 in the proof of Theorem 2.

B.3 Proof of Proposition 5 (Regret of OMS-ETG-FB)
Proposition (Regret of OMS-ETG-FB). Suppose that the conditions of Theorem 2 hold. If s = Bη−1 and any η ∈ [0, 1), then ∀θ∗ ∈ Θ, R∞ (πETG-FB) = 0.

Proof. We show this similarly to Theorem 2. In this case, the size of each batch is random but the numbers of rounds J = 1s is not random. Thus we can’t use the concentration inequality in Lemma 1 directly since that only holds for a ﬁxed time step t. We get around this by showing that the estimated selection ratio kt will remain in an -ball around κ∗ uniformly over all time steps after some asymptotically negligible fraction of the horizon T .
Let Tj be the number of samples collected after round j, i.e., Tj = Bsj . Let f = Bγ−1 for some
κTj c
γ ∈ (0, 1). Like the proof of Theorem 2, we can ignore the ﬁrst Jf rounds since they are f ∈ o(1) is

31

an asymptotically negligible fraction. And similarly to the proof of Theorem 2, in order to show that κT →p κ∗, it is sufﬁcient to show that P ∀j ∈ [J f + 1, J f + 2, . . . , J ], kTj − κ∗ ≤ −B−→ −−∞→
1. We can show this as follows:

P ∀j ∈ [Jf + 1, Jf + 2, . . . , J ], kTj − κ∗ ≤

≥ P ∀t ∈ [TJf+1, . . . , TJ ],

kt − κ∗ ≤ . (13)

The minimum and maximum batch sizes are bmin = cBmasx and bmax = cBmsin , respectively. Therefore,
Bs TJf+1 ≥ J f bmin = J f cmax ,
Bs TJ ≤ J bmax = J cmin .
Using these facts and continuing Eq. 13, we get:

P ∀j ∈ [Jf + 1, Jf + 2, . . . , J ], kTj − κ∗ ≤

≥ P ∀t ∈ [TJf+1, . . . , TJ ], kt − κ∗ ≤

≥ P ∀t ∈ [J f bmin, . . . , J bmax], kt − κ∗ ≤

(a)
≥ 1−

J bmax

1 exp −O t 8
4D

t=J f bmin

(b)
≥ 1−

J bmax

t=J f bmin

1 4D exp

−O

J f bmin 8

≥ 1 − Jbmax
t=J f bmin

1 4D exp

−O

Bf 8

≥ 1 − J bmax exp −O Bf 8
4D

≥ 1 − B exp −O Bf 8
4D

≥ 1 − B exp −O Bγ 8
4D

→ 1,

where (a) follows by the union bound and (b) because t ≥ Jf bmin.

C Feasible regions
In this section, we derive the feasibility regions for the various policies. OMS-ETC Recall that in OMS-ETC, we ﬁrst collect T e samples such that κT e = ctr (∆ψ). For the remaining T (1 − e) samples, the agent can query the data sources with any fraction κ ∈ ∆ψ. Therefore, the feasible values of κT are
∆˜ = T eκT e + T (1 − e)κ : κ ∈ ∆ψ T
= {eκT e + (1 − e)κ : κ ∈ ∆ψ} .
OMS-ETG After j rounds, the selection ratio is denoted by κbj. In every round, we collect b = T s samples. For the batch collected in round j + 1, the agent can query the data sources with any fraction κ ∈ ∆ψ.

32

Therefore, the feasible values of κb(j+1) are

∆˜ j+1(κbj ) =

bjκbj + bκ : κ ∈ ∆ψ b(j + 1)

= T sjκbj + T sκ : κ ∈ ∆ψ T s(j + 1)

= jκbj + κ : κ ∈ ∆ψ . (j + 1)

OMS-ETC-CS

The agent uses Be budget to uniformly query the available data sources. Let Te denote the number of samples collected after exploration. We have

Be

Te =

,

κTe c

where κTe = ctr (∆ψ) and c is the cost vector. With the remaining B(1 − e) budget, the agent can collect samples with any fraction κ ∈ ∆ψ. However, since the data sources can have different costs, the total number of samples T depends on the choice of κ:

B(1 − e) T = Te + κ c , for κ ∈ ∆ψ. Therefore the feasible values of κT are

∆˜ =

TeκTe + (T − Te)κ : κ ∈ ∆ψ T

 Be κT + B(1−e) κ



=  κTe c e

κc



: κ ∈ ∆ψ

 Be + B(1−e)



κTe c

κc

= e κe (κc κc)Te++(1(1−−e)e) κκTce c κ : κ ∈ ∆ψ .
Te

OMS-ETG-FS
Since we collect a ﬁxed number of samples in each round, the feasibility region for OMS-ETG-FS is that same as OMS-ETG:
∆˜ j+1(κbj ) = jκbj + κ : κ ∈ ∆ψ . (j + 1)

OMS-ETG-FB

Let the selection ratio after j rounds be κTj where Tj number of samples collected after round j: Tj = Bsj . For the batch collected in round j + 1, the agent can query the data sources with any
κbj c
fraction κ ∈ ∆ψ. However, the number of samples collected in round j + 1 would depend on the choice κ due to the cost structure. Therefore the number of samples collected after round j + 1 is

Bsj Tj+1 = Tj + κ c , for κ ∈ ∆ψ. Hence, the feasible values of κTj+1 are

∆˜ j+1(κTj ) =

Tj κTj + (Tj+1 − Tj )κ : κ ∈ ∆ψ Tj+1

 Bsj κT + Bsj κ



=  κbj c j κ c Bsj + Bsj

 : κ ∈ ∆ψ


 κbj c κ c

 j =


κ c κTj + κTj c j (κ c) + κTj c



κ



: κ ∈ ∆ψ .



33

D Experiments

D.1 Linear IV graph

Data from the linear IV graph (Figure 2a) is simulated as follows:

Z ∼ N 0, σz2 , U ∼ N 0, σu2 , X := αZ + γU + x,
Y := βX + φU + y,

x∼N y ∼N

0, σx2 , 0, σy2 ,

where x and y are exogenous noise terms independent of other variables and each other and U is an unobserved confounder. Here {β, α, γ, φ, σz2, σu2, σx2, σy2} are parameters that we set for simulating the data. For the experiment in Section 6.1, we used β = 1, α = 1, γ = 1, φ = 1, σz = 1, σu =
1, σx = 1, σy = 1.

The moment conditions used for estimation are

gt(θ) = sstt,,12 ⊗ ZZtt((YXt t−−ααβZZtt)) .

=m(st )

=g˜t (θ)

The parameter we estimate is θ = [β, α] and β = ftar(θ) = θ0.

D.2 Two IVs graph

Data from the two IVs graph (Figure 2b) is simulated as follows:
Z1 ∼ N 0, σz21 , Z2 ∼ N 0, σz22 , U ∼ N 0, σu2 , X := α1Z1 + α2Z2 + γU + x, x ∼ N 0, σx2 , Y := βX + φU + y, y ∼ N 0, σy2 ,
where x and y are exogenous noise terms independent of other variables and each other and U is an unobserved confounder. For the experiment in Section 6.1, we used β = 1, α = 1, γ = 1, φ = 1, σz = 1, σu = 1, σx = 1, σy = 1.
The moment conditions used for estimation are
gt(θ) = sstt,,12 ⊗ ((ZZ12))tt((YYtt −− ββXXtt)) .

=m(st )

=g˜t (θ)

The parameter we estimate is θ = [β] and β = ftar(θ) = θ0.

D.3 Confounder-mediator graph
Data from the confounder-mediator graph (Figure 2b) is simulated as follows:
W ∼ N 0, σw2 , X := dW + x, x ∼ N 0, σx2 , M := βa X + m, m ∼ N 0, σm2 , Y := aM + bW + y, y ∼ N 0, σy2 ,
where x, m, and y are exogenous noise terms independent of other variables and each other. For the experiment in Section 6.1, we used β = −0.32, a = 0.33, b = −0.34, d = 0.45, σw = 1, σx = 1, σm = 1, σy = 1.

34

The moment conditions used for estimation are

 st,1

Xt(Yt − bWt − βXt)



st,1  Wt(Yt − bWt − βXt) 



st,2

 

Xt(Mt

−

β a

Xt

)

 

 st,2

 Mt Yt − aMt −

bdσw2

 Xt 

gt(θ) = st,2 ⊗ 

 d2σw2 +σx2 . 2

st,1 Xt Yt − aMt − d2bσd2σ+wσ2 Xt 



wx



st,1 

Wt2 − σw2



st,1

 

Wt(Xt − dW )

 

1

Xt2 − (d2σw2 + σx2)

=m(st )

=g˜t (θ)

The parameter we estimate is θ = [β, a, b, d, σw2 , σx2] and β = ftar(θ) = θ0.

D.4 IHDP dataset

To generate semi-synthetic IHDP dataset, we use two covariates: birth weight (denoted by W1) and whether the mother smoked (denoted by W2). The binary treatment is denoted by X and the outcome is denoted by Y . The corresponding causal graph is shown in Figure 4a. For every sample of the semi-synthetic dataset, W1, W2, and X are sampled uniformly at random from the real data. The outcome Y is simulated as follows:
Y := βX + α1W1 + α2W2 + y, y ∼ N 0, σy2 ,

where y is an independent exogenous noise term. For the experiment in Section 6.2, we used β = 1, α1 = 1, α2 = 0.1, σy = 1.

The moment conditions used for estimation are

1 − st,2 (W1)t ((Yt − α1(W1)t − βXt) − α2d)

1 − st,2  Xt ((Yt − α1(W1)t − βXt) − α2τ2) 





1 − st,1

(W2)t ((Yt − α2(W2)t − βXt) − α1d)





1 − st,1

 Xt ((Yt − α2(W2)t − βXt) − α1τ1) 









 st,3  

(W1)t(W2)t − d



 gt(θ) = 1 − st,2 ⊗ 

X(W1)t − τ1

 .

 1 − st,1 

X(W2)t − τ2

 





 1 − st,2 

(W1)2t − σw2 1

 





1 − st,1

 

(W2)2t − σw2 2

 

1 − st,2 (Yt − α1(W1)t − βX)2 − α22σw2 2 − σy2

1 − st,1

(Yt − α2(W2)t − βX)2 − α12σw2 1 − σy2

=m(st )

=g˜t (θ)

The parameter we estimate is θ = [β, α1, α2, d, τ1, τ2, σw2 , σy2] and β = ftar(θ) = θ0.

D.5 The Vietnam draft and future earnings dataset
The causal graph for this dataset corresponds to Figure 2a with a binary IV Z, binary treatment X and continuous outcome Y . In this dataset, {Z, X} and {Z, Y } are collected from different data sources and thus {Z, X, Y } are not observed simultaneously. For our experiment, we only use data from the 1951 cohort.
In the semi-synthetic dataset, we sample Z uniformly at random from the real dataset. The treatment X is generated similarly to a probit model. We ﬁrst generate an intermediate variable X∗ and then use that to generate X as follows:
X∗ := αZ + c∗ + x, x ∼ N (0, 1), X := 1(X∗ > 0),
where 1 is the indicator function. To reduce clutter, let µz = P(Z = 1) = 0.3425, µ(x1) = P(X = 1|Z = 1) and µ(x0) = P(X = 1|Z = 0). We set the parameters α and c∗ such that µ(x1) = 0.2831 and

35

µ(x0) = 0.1468 (these values have been taken from [2, Table 2] to match the empirical distribution):
µ(x0) = P(1(X∗ > 0)|Z = 0) = P(c∗ + x > 0)) = P( x > −c∗)) = P( x < c∗)) = Φ(c∗),
∴ c∗ = Φ−1(µ(x0)) = Φ−1(0.1468)
= −1.050,
µ(x1) = P(1(X∗ > 0)|Z = 1) = P(α + c∗ + x > 0)) = Φ(α + c∗)
∴ α = Φ−1(µ(x1)) − c∗ = Φ−1(µ(x1)) − Φ−1(µ(x0)) = Φ−1(0.2831) − Φ−1(0.1468)
= 0.4766,

where Φ is the cumulative distribution function of the standard normal distribution.
In the real data, we standardize the outcome Y by subtracting its mean and dividing by its standard deviation and thus E[Y ] = 0 and Var(Y ) = 1. To generate the simulated outcome Y , we use Y := βX + γ + c0 x + y, where y ∼ N (0, σ2 ). When c0 = 0, the noise term (c0 x + y) ⊥⊥ X.
y
Thus c0 determines the extent of the confounding between X and Y .
We now describe how we set β and γ. Since E[Y ] = 0, we have

γ = −βE[X] = −β µ(x0)(1 − µz) + µ(x1)µz = −0.1934β.

Using the covariance of Y and Z, we have

Cov(Y, Z) = E[Y Z]
= βE[ZX] + γE[Z]
= β (E[ZX] − E[X]E[Z]) = β (E[Z1(αZ + c∗ + x > 0)] − E[X]E[Z]) = β (E[ZE[1(αZ + c∗ + x > 0)|Z]] − E[X]E[Z]) = β (E[ZE[1( x > −(αZ + c∗))|Z]] − E[X]E[Z]) = β (E[ZΦ(αZ + c∗)] − E[X]E[Z]) = β (Φ(αZ + c∗)µz − E[X]E[Z])
= βµz µ(x1) − E[X] .

Therefore, we set β and γ as

E[Y Z]

β=

= −0.4313,

µz µ(x1) − E[X]

γ = −0.1934β = 0.0834.

Now we describe how we set c0 and σ2 . For this, we use the variance of Y : y Var(Y ) = 1 = β2Var(X) + c20σ2y + 2βc0E[X x]. (14)

36

We have

Var(X) = Var[E(X|Z)] + E[Var(X|Z)]

= Var Zµ(x1) + (1 − Z)µ(x0) + µzµ(x1)(1 − µ(x1)) + (1 − µz)µ(x0)(1 − µ(x0))

E[X

= µz(1 − µz)(µ(x1) − µ(x0))2 + µzµ(x1)(1 − µ(x1)) + (1 − µz)µ(x0)(1 − µ(x0)) = 0.1560, x] = E[E[1(Zα + c∗ + x > 0) x|Z]] = E[E[1( x > −(Zα + c∗)) x|Z]]

(a)
= EZ

∞
xf (x)dx
−(Z α+c∗ )

1

−(Zα + c∗)2

= E √ exp

2π

2

1

−(c∗)2

−(α + c∗)2

= √2π exp 2 (1 − µz) + exp 2 µz

= 0.2670,

where in (a), f (x) is the probability density function of the standard normal distribution. We set c0 = 0.5 and using Eq. 14, we get σ2 = 0.6058.
y
To summarize, the data is generated as follows:

Z ∼ Bernoulli(µz), X∗ := αZ + c∗ + x, x ∼ N (0, 1), X := 1(X∗ > 0),
Y := βX + γ + c0 x + y, y ∼ N (0, σ2 ), y

where µz = 0.3424, α = 0.4766, c∗ = −1.0502, β = −0.4313, γ = 0.0834, and σ2 = 0.6058. y

The moment conditions used for estimation are

st,1  Zt(Yt − µ1) 

gt(θ) = sstt,,12 ⊗ (1 −ZtZ(Xt)t(Y−t τ−1)µ0) .

st,2

(1 − Zt)(Xt − τ0)

=m(st )

=g˜t (θ)

The parameter we estimate is θ = [µ1, µ0, τ1, τ0] and the target parameter is β = ftar(θ) = µτ11−−τµ00 .

37

