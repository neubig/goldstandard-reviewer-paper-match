The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics

Sebastian Gehrmann,9,* Tosin Adewumi,20,21 Karmanya Aggarwal,14 Pawan Sasanka Ammanamanchi,15 Aremu Anuoluwapo,21,38 Antoine Bosselut,28 Khyathi Raghavi Chandu,2 Miruna Clinciu,7,11,35 Dipanjan Das,9 Kaustubh D. Dhole,1 Wanyu Du,42 Esin Durmus,5 Ondrˇej Dušek,3 Chris Emezue,21,30 Varun Gangal,2 Cristina Garbacea,39 Tatsunori Hashimoto,28 Yufang Hou,13 Yacine Jernite,12 Harsh Jhamtani,2 Yangfeng Ji,42 Shailza Jolly,6,29 Mihir Kale,9 Dhruv Kumar,44 Faisal Ladhak,4 Aman Madaan,2 Mounica Maddela,8 Khyati Mahajan,34 Saad Mahamood,32 Bodhisattwa Prasad Majumder,37 Pedro Henrique Martins,16 Angelina McMillan-Major,43 Simon Mille,26 Emiel van Miltenburg,31 Moin Nadeem,22 Shashi Narayan,9 Vitaly Nikolaev,9 Rubungo Andre Niyongabo,21,36 Salomey Osei,19,21 Ankur Parikh,9 Laura Perez-Beltrachini,35 Niranjan Ramesh Rao,24 Vikas Raunak,23 Juan Diego Rodriguez,41 Sashank Santhanam,34 João Sedoc,25
Thibault Sellam,9 Samira Shaikh,34 Anastasia Shimorina,33 Marco Antonio Sobrevilla Cabezudo,40 Hendrik Strobelt,13 Nishant Subramani,17,21 Wei Xu,8
Diyi Yang,8 Akhila Yerukola,27 Jiawei Zhou10
1Amelia R&D, New York, 2Carnegie Mellon University, 3Charles University, Prague, 4Columbia University, 5Cornell
University, 6DFKI, Germany 7Edinburgh Centre for Robotics, 8Georgia Tech, 9Google Research, 10Harvard University,
11Heriot-Watt University, 12Hugging Face, 13IBM Research, 14IIIT Delhi, 15IIIT Hyderabad, 16Instituto de Telecomunicações,
17Intelligent Systems Lab, Intel, 18Johns-Hopkins University, 19Kwame Nkrumah University of Science and Technology
20Luleå University of Technology, 21Masakhane, Africa, 22Massachusetts Institute of Technology, 23Microsoft, 24National
Institute of Technology Karnataka India, 25New York University, 26Pompeu Fabra University, 27Samsung Research, 28Stanford
University, 29Technical University of Kaiserslautern, 30Technical University Munich, 31Tilburg University, 32trivago,
33Université de Lorraine, 34University of North Carolina Charlotte, 35University of Edinburgh, 36University of Electronic
Science and Technology of China, 37University of California San Diego, 38University of Lagos, 39University of Michigan Ann
Arbor, 40University of São Paulo, 41University of Texas at Austin, 42University of Virginia, 43University of Washington,
44University of Waterloo

Abstract

1 Introduction

arXiv:2102.01672v3 [cs.CL] 1 Apr 2021

We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with wellestablished, but ﬂawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.
* Correspondence to gehrmann@google.com

Natural language generation is the task to automatically generate understandable texts, typically using a non-linguistic or textual representation of information as input (Reiter and Dale, 2000). These texts aim to fulﬁll an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, ﬂuent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conﬂicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L met-

ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for ﬂuent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these ﬂawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different deﬁnitions of the measured quantities (Howcroft et al., 2020).
We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered ﬂaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and increase the robustness of model evaluations.
The initial set of eleven included datasets is presented in Table 1. They measure speciﬁc generation challenges, such as the content selection and planning (What to say?), and the surface realization (How to say it?) (Reiter and Dale, 2000; Gatt and Krahmer, 2018). Models need to be capable of paraphrasing, simpliﬁcation, and others. In addition to those challenges, GEM datasets also differ in their communicative goals, languages, the noisiness of data, and resource availability, to evaluate the consistency of evaluation schemes. About half of the datasets have multiple references and more

Evaluation on “solved” data

Improving Data

Varying experimental setups
Improving Models

Improving Metrics
Evaluation with gameable metrics

Consistent Human Eval

Non-repeatable human evaluation

Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Otherwise, results are challenging to interpret and compare to each other. Finally, as models improve and metrics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation.

than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for speciﬁc modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-speciﬁc version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019).
This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/.
2 Benchmarks in NLG
In this section, we summarize common criticisms of benchmarks in NLP, discuss how they apply to NLG, and how we plan to address them. Then, we describe opportunities that GEM can provide. NLP benchmarks such as GLUE (Wang et al., 2019b) are common for natural language understanding

Dataset
CommonGEN (Lin et al., 2020)

Communicative Goal

Language(s) Size Input Type

Produce a likely sentence which mentions

en

all of the source concepts.

67k Concept Set

Czech Restaurant

Produce a text expressing the given intent

cs

(Dušek and Jurcˇícˇek, 2019) and covering the speciﬁed attributes.

5k Meaning Representation

DART (Radev et al., 2020)

Describe cells in a table, covering all in-

en

formation provided in triples.

82k Triple Set

E2E clean (Novikova et al., 2017) (Dušek et al., 2019)

Describe a restaurant, given all and only

en

the attributes speciﬁed on the input.

42k Meaning Representation

MLSum (Scialom et al., 2020)

Summarize relevant points within a news article

*de/es

*520k

Articles

Schema-Guided Dialog (Rastogi et al., 2020)

Provide the surface realization for a vir-

en

*165k Dialog Act

tual assistant

ToTTo (Parikh et al., 2020)

Produce an English sentence that de-

scribes the highlighted cells in the context

en

of the given table.

136k Highlighted Table

XSum (Narayan et al., 2018)

Highlight relevant points in a news article

en

*25k Articles

WebNLG (Gardent et al., 2017)

Produce a text that verbalises the input triples in a grammatical and natural way.

en/ru

50k RDF triple

WikiAuto + Turk/ASSET Communicate the same information as

(Jiang et al., 2020)

the source sentence using simpler words

en

(Alva-Manchego et al., 2020) and grammar.

594k Sentence

WikiLingua (Ladhak et al., 2020)

Produce high quality summaries of an instructional article.

*ar/cs/de/en es/fr/hi/id/it ja/ko/nl/pt/ru th/tr/vi/zh

*550k

Article

Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM.

(NLU) tasks. They aggregate multiple tasks under a uniﬁed evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress.
However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result,

may mischaracterize a system’s performance.
Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with and conducting in-depth analyses of the data sets that models are trained on. The limited analysis delegates the responsibility to ensure that all included datasets have been collected fairly to the creators of the benchmark (Denton et al., 2020). The dataset and benchmark creators thus must provide in-depth statements that describe the data characteristics and surface potential issues and consider these issues when selecting datasets for a benchmark (Gebru et al., 2018; Bender and Friedman, 2018).
These dangers emphasize selecting datasets for a benchmark needs to be carefully done, that the setup has to remain ﬂexible to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead,

a living benchmark that can adjust its datasets and speciﬁc evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test data through a human-in-the-loop approach.
Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is the choice of the languages of the included datasets. It is often assumed that work on English transfers to other languages (Bender, 2011). However, this assumption does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a ﬂexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020).
As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been ﬁrst steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018).
There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level
1https://dynabench.org/

generation and translation featured German and English generation challenges (Heaﬁeld et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020).
A benchmark that focuses only on NLG can enable much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated.
Providing a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevic´ et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) ﬁnd that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or ﬂaws in the metric itself.2
Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integration of
2For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020).

additional metrics into our living benchmark system, which enables a much faster adoption.
Developing reproducible human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that deﬁnitions of measures like “accuracy” or “ﬂuency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classiﬁcation system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderboard-like score. To avoid further fragmentation of the ﬁeld, GEM is developing its own human evaluation approaches, but uses the infrastructure provided by GENIE to run its human evaluation.
In addition to GENIE, multiple other related efforts exist that work toward the goal of reproducible and robust in-depth human and automatic evaluation for NLG tasks, and which focus on speciﬁc modeling- or task-aspects that are different from those in GEM. Among those are KILT (Petroni et al., 2020) which focuses on knowledge-intensive tasks and retrieval-based models, Storium (Akoury et al., 2020) which focuses on open-ended story generation, and BIG bench3 which focuses on measuring few-shot and zero-shot capabilities of language models.
3 Dataset Selection
As highlighted in Figure 1, the selection of included datasets is an integral part of a benchmark. They should be challenging for models, but it should still be possible to evaluate models trained on them. Moreover, the datasets should cover a wide range of relevant generation challenges that allow for
3https://github.com/google/BIG-bench

ﬁndings to be as general as possible. Finally, the datasets should cover tasks that are interesting for contributors to work on to facilitate the wide adoption of the benchmark.
To collect datasets with those desired properties, the selection methodology for GEM is composed of three steps. First, we elicited a set of proposals from everyone involved in the effort. Second, we identiﬁed criteria for the selection. Third, all GEM members voted on individual dataset and criteria utilities. The ﬁnal selection maximizes the utility under constrained resources, similar to a knapsack solver.4 This can be seen as an extension of the selection process of SuperGLUE (Wang et al., 2019a) that had similar ﬁrst and second steps but made the ﬁnal decision based on which were harder for a baseline model to solve after identifying a ﬁnal set of candidate datasets. Since we are going to introduce challenge sets, the baseline performance of models on a dataset matters less.
Dataset Elicitation. In the ﬁrst step, all GEM participants were asked to suggest datasets following the schema provided in Appendix A. The categories included multiple brief categorizations, such as a description of the challenge that this dataset provides, its high-level task, and the communicative goal of an agent trained on the data. Following our goal to focus on non-English languages, we further asked for the languages included in the dataset, as well as the language locale. This step yielded 35 proposed datasets, listed in Appendix B.
Estimating Task+Criterion Utility. The second step focused on the selection of criteria to inform the selection. The initial set of criteria was selected through open discussion involving all members. We split criteria into “hard” and “soft” ones – hard criteria would lead to the deﬁnite inclusion/exclusion of a task if (not) satisﬁed. Soft criteria inform the utility of the remaining tasks. All GEM members ﬁlled out a survey asking them to rate, on a 5-point Likert scale, how much they wanted to see a task included in GEM. Additionally, we posed yes/no questions for all considered hard criteria and various questions about the soft criteria (e.g., “what percentage of the tasks should
4Consider the criterion “We need equal representation of large and small datasets” under the constraint that only two datasets can be selected. If we have two large datasets with utility 10, and one small one with utility 5, we may want to include the smaller dataset over the second large dataset to satisfy the criterion.

feature non-English language?”, or “do we prefer noisy or clean datasets?”). Finally, the survey included open text ﬁelds that asked for (1) comments on any of the tasks, (2) comments or suggestions on hard exclusion criteria, and (3) suggestions of additional criterion/criteria. The full list of questions is shown in Appendix C.
The survey received 28 responses, revealing that the initial version of GEM should include a median of 10 tasks or an average of 12. Of those tasks, about a third should feature non-English language.
Selected Criteria. For the hard criteria, there was an agreement to focus only on open-access datasets and that concurrent or past shared tasks for the same datasets are not an issue. Overall, the sentiment determined the following selection principles:
• We focus on diverse high-level tasks over a single high-level task evaluated in-depth. However, each high-level task should include multiple datasets.
• We focus on clean datasets to avoid conﬂating model mistakes and learned noise.
• We include a mix of high- and low-resource datasets.
• We focus on data with interesting test sets.
• We should not focus on the quality of current evaluation strategies for a given dataset.
• We prefer multi-reference datasets since those have been shown to lead to more robust automatic evaluation.
High-Level Tasks. Since these principles dictate that we should focus on a small set of high-level tasks, we used the free-text replies to evaluate the interest in different high-level tasks. Grouping the proposed tasks yielded the following candidates: Summarization, Dialog, Simpliﬁcation/Compression, Question Answering, Creative Writing, Data-to-Text, and Question Generation.5 There was a preference to exclude image inputs and question answering because those tasks add complexity to the evaluation beyond the generated text. Moreover, since creative generation tasks like story generation and poetry generation suffer even more from inadequate evaluation approaches, there was
5For a full overview of potential future expansions and challenges, we refer to the survey by Gatt and Krahmer (2018).

a consensus to not include them. There was, however, a strong preference for the high-level tasks Summarization, Data-to-text, and Dialog.6
Speciﬁc Datasets. The ﬁnal selection is shown in Table 1. To arrive at the selection, we ﬁrst ranked all datasets by their average rating. For this, we treated positive ratings as 1, negative ratings as -1, and neutral ratings as 0. The highestranked datasets were E2E with 0.577, XSum with 0.538, and ToTTo with 0.461. Unfortunately, nonEnglish datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets ﬁt, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simpliﬁcation. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality.
3.1 GEMifying the data
We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-speciﬁc template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is conducted responsibly.
These datasets are the base selection, and as part of GEM, we may change datasets and how they are used. For example, we may improve the training sets, make the test sets more challenging, or probe for speciﬁc skills a model must exhibit with testonly datasets (Perez-Beltrachini and Gardent, 2017; Linzen, 2020; Ribeiro et al., 2020; Schlegel et al., 2020). We may also ask to evaluate a single model
6One may question the absence of Translation from this list. While it is a generation task, we excluded it since Translation already has regular benchmarking efforts with WMT.
7Our template extends and restructures that from Hugging Face Datasets and along with a guide can be found at https: //gem-benchmark.com/data_cards.

on multiple test sets, following the design by Dua et al. (2019).
We are including modiﬁcations to several of the datasets: (1) MLSum: We excluded all languages besides Spanish and German since the sources for other languages disallow scraping content. Additionally, we removed all duplicate items (i.e., items with the same input text) and we used langdetect8 to ﬁlter out examples that were in the wrong language. In total, 147 examples were removed from the German portion (0.06%) and 7417 examples were removed from the Spanish portion (2.5%). (2) XSum: Summaries in this dataset often have divergence issues between the source and target texts since gold summaries are introductory sentences prefacing each article. Models agnostic to such noises are vulnerable to hallucinations (Wiseman et al., 2017; Dhingra et al., 2019). To combat this, we ﬁne-tuned a BERT-based (Devlin et al., 2019) classiﬁer on 500 document and gold summary pairs, manually annotated for faithfulness (Maynez et al., 2020) and excluded all document-summary pairs from the original XSum dataset where the classiﬁer was not conﬁdent (p(faithful) > 0.8) whether the summary is faithful to the document or not. (3) Schema-Guided Dialog: We are focusing on the response-generation part of the dataset and thus reformatted the dataset to treat the service agent utterances as the targets to be generated and the previous customer utterance and the agent’s dialog act as the input. We additionally reformat the dialog acts to directly conform to the format described in the paper (Kale and Rastogi, 2020). (4) WikiLingua: We focus on the same ﬁve languages that were benchmarked in its original release (en, es, ru, tr, vi) in a cross-lingual setup in which the inputs are in the respective language and the outputs are in English. However, we re-split the original data to avoid train-test overlaps between languages and provide training data in 13 additional languages (as shown in Table 1). For GEM, we allow submissions trained on any of the languages in isolation or as part of a multilingual model.
3.2 Challenge Sets
In addition to applying consistent metrics to existing test sets, understanding speciﬁc model behavior, such as model generalization capabilities or performance under targeted cases, is also key for improvement. This is difﬁcult to assess through evalu-
8https://pypi.org/project/langdetect/

ations on i.i.d. test splits. We thus release challenge sets to evaluate data-to-text and text-to-text models (overview in Table 2). In addition to enabling a more speciﬁc breakdown of how a model performs in the presence of challenging inputs, the set of system outputs on these test sets also constitutes a rich corpus that enables further error analysis and research. We apply multiple strategies to create the special test sets, in particular (I) alteration of the existing test sets (e.g., the introduction of distractors), (II) breaking down of the existing sets into subsets with certain properties (e.g., subsets with different complexity), and (III) the compilation of new test sets (e.g., out-of-vocabulary inputs). We restrict the size of each challenge set to about 500 examples to minimize computational overhead. On the WebNLG challenge sets, all subset items are selected proportionally from each category to ensure a similar distribution to the original set; on all other datasets the subset items are selected from the whole set. The results of the different systems on these subsets will be compared to the results obtained by the same systems on the same subsets of the original test data.
For case (I), altering existing test sets, the ﬁrst challenge set adds numerical variation in WebNLG. This variation attempts to respect the format of the current cardinal value (e.g. alpha, integer, or ﬂoating-point) and replaces the existing value with a new random value as a means to challenge existing trained models. The generated number is lower-bounded between zero and upper bounded to be within to the highest power of 10 unit for the given value (e.g. replacing a value of 54 would result in a random value between 0-100). Floating values are also bounded to have the same degree of precision as the input value. For structureto-text and dialog datasets, we produce a version of the test sets in which the order of the components of the input structures (triples, concepts, dialog acts, table rows, etc.) is randomly changed. For text-to-text datasets and Schema-guided Dialog, we introduce several types of perturbations: (a) typographical errors, using butter-ﬁngers 9 with two thresholds 0.02 and 0.05, which respectively correspond to lower and higher error frequencies; (b) removal of the ﬁnal punctuation sign (if any); (c) substitution of the input text by a backtranslated version, using the backtranslation implementation
9https://github.com/alexyorke/ butter-fingers

Challenge Set Type
Numerical Variation Attribute Order Typographical Errors No Punctuation Backtranslation
Train & Validation Samples Gender, Ethnicity, Nationality Input Shape Syntactic Complexity
Covid Summaries

Example
53 ->79 English Cheap ->Cheap English English Cheap ->Enlish Chesp ... the dog. ->... the dog fantastic ->toll ->great

Tasks
WebNLG All data-to-text tasks Schema-Guided, WikiAuto, XSum Schema-Guided, WikiAuto, XSum Schema-Guided, WikiAuto, XSum
All tasks ToTTo WebNLG WikiAuto
MLSUM (es+de), XSum

Table 2: An overview of the types of challenge sets for GEM. The ﬁrst category are modiﬁcations to inputs of a model, the second category identiﬁes contrast sets which are subsets of the original test set, and the third describes newly collected data.

by Xie et al. (2020). We rejected backtranslation outputs based on a character length to ensure that the difference in character length between original and backtranslation does not exceed 35% of the original source character length. For XSum 99.8% of the backtranslations were accepted, for WikiAuto 94.42% (ASSET) and 87.18% (TURK), and for Schema-Guided Dialog 78%.
In case (II), the breaking down existing sets, we ﬁrst provide for each dataset random samples of training and validation data, in order to assess to what extent the scores of the different systems drop when run on the test data. Then, speciﬁc splits are created for particular datasets, in order to assess possible biases of the models, and their robustness across inputs with different speciﬁcations. For ToTTo, test set splits are built according to several aspects that can be identiﬁed using WikiData: gender, ethnicity and nationality grouped by continent. For gender, we compare the performance between male and female people, but cannot compare other genders due to a lack of original data - only seven people in the original test set are marked as having a different gender. We compare across the continent of the underlying nationality to address the issue that data for each country can be very sparse – i.e., only 19 countries are represented by more than ten people and only one of these is located in Africa (Kenya). In case a person has citizenships across multiple continents, we may include the person in any of the included continents. Finally, we compare African Americans vs. all Americans. Ethnicity is very sparsely annotated in WikiData with fewer than 150 annotated test examples in total and 128 of these are African Americans. We thus are unable to compare the per-

formance on, e.g., Yoruba or Punjabi people, both of which have fewer than ﬁve instances. Another caveat here is that only 21 of the 128 people are female. Our contrast subset that can include any US citizens matches these counts. Across all three challenge subsets, we additionally match the fraction of the existing non-overlap and overlap properties. For WebNLG, we propose subsets based on the shape of the inputs (number of triples, number of common subjects and/or objects, depth, etc.) For Turk/ASSET, splits are created in terms of the syntactic complexity of the sentences to be simpliﬁed. To characterise sentence complexity we use the developmental level scale proposed by Covington et al. (2006).10 For all datasets, we propose splits based on the frequency of the parts that compose the input in the training data; the resulting test sets range from being made of very common components to being made only from components unseen in the training data. For case (III), we collect timeshifted test data for news summarization in the form of articles with Covid19-related keywords. Since MLSum and XSum were collected before the pandemic, we can measure how a model responds to context not seen in the training data (outside of potential pretraining). The new set of articles covers existing article topics (economy, sports, etc.) but all in relation to the Covid19 pandemic. In addition, some new topics appear in the collected data derived from outlet sections that were not part of the original data collection.11
10We use the implementation provided by Lu (2010). 11To collect this data we use the scripts provided for the re-creation of MLSum and XSum datasets.

4 Experimental Setup
Since the GEM test sets and ﬁnal metrics selection have not been released yet, we describe an experimental setup that will ensure that participating models are trained correctly and evaluated on publicly available data with available metrics that will give a sufﬁcient indication of a model’s performance. To do this, we are reporting the results of the baseline models on the validation sets.
4.1 Modeling Baselines
Much of the recent modeling progress in NLP can be attributed to the rise of the pretrain-then-ﬁnetune paradigm which has led to consistently better results. This ﬁnding is consistent with human judgments for summarization, as shown by Fabbri et al. (2020), among others. However, many of the tasks included in GEM may not beneﬁt from a language model encoder since their input is not natural language. We thus apply a variety of different architectures that vary in size, complexity, and training schema. Our main baselines are T5 with 60M parameters (Raffel et al., 2020) and BART with 139M parameters (Lewis et al., 2020a). For nonEnglish datasets, we use their multilingual counterparts mT5 in various sizes (Xue et al., 2020) and mBART (Liu et al., 2020b). We additionally train the following baselines on a subset of tasks: TGen (with added language model and lemma tags denoted as TGen+/++) (Dušek and Jurcˇícˇek, 2016b), an architecture for generation from dialog acts, an LSTM-based Sequence-to-sequence model with attention (Bahdanau et al., 2015), DialoGPT (Zhang et al., 2020c), a pretraining approach for conversational models, and PEGASUS (Zhang et al., 2020a), which uses a summarization-speciﬁc pretraining schema that masks and predicts entire sentences.For WikiLingua, we additionally report results on a setup proposed by Ladhak et al. (2020) which includes ﬁrst training a monolingual model followed by ﬁnetuning with the correct source language, coupled with synthetic data generated through translation (mBART+). Almost all baselines can be reproduced on a GPUbased colaboratory notebook within 2-3 hours.
4.2 Automated Evaluation
As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to

avoid overﬁtting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories:
Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018).
Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is ﬁne-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT.
Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks.
In addition to QA-based evaluation, there have also been related efforts to develop more ﬁne-

Dataset CommonGen Czech Restaurant
DART E2E clean MLSum (de)
MLSum (es)
Schema-Guided ToTTo XSum WebNLG (en)
WebNLG (ru)
Turk ASSET WikiLingua (es→en)
WikiLingua (ru→en)
WikiLingua (tr→en)
WikiLingua (vi→en)

Model
BART T5
mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++
BART T5
BART LSTM T5 TGen
mBART mT5-small mT5-base mT5-large mT5-XL mBART mT5-small mT5-base mT5-large mT5-XL
BART T5
T5
PEGASUS
mBART mT5-small mT5-base mT5-large mT5-XL mBART mT5-small mT5-base mT5-large mT5-XL
BART T5
BART T5
mBART mBART+ mT5-small mT5-base mT5-large mT5-XL mBART mBART+ mT5-small mT5-base mT5-large mT5-XL mBART mBART+ mT5-small mT5-base mT5-large mT5-XL mBART mBART+ mT5-small mT5-base mT5-large mT5-XL

Metrics (Lexical Similarity and Semantic Equivalence)
METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT

0.301

63.5

32.5

55.1 27.5

0.943 -0.400

0.291

64.0

29.4

54.5 26.4

0.942 -0.412

0.229

47.3

28.6

43.0 17.9

0.895

–

0.23

48.1

28.8

44.2 17.1

0.898

–

0.233

51.3

30.0

46.4 17.5

0.902

–

0.229

52.1

31.3

47.3 17.0

0.905

–

0.152 13.6

0.0

13.6 0.03

0.650

–

0.151 13.8

0.0

13.8 0.03

0.651

–

0.167

9.7

0.0

9.7 0.03 0.648

–

0.107

7.1

0.0

7.1 0.02 0.862 -0.261

0.115

8.4

0.0

8.4 0.02 0.901 -0.091

0.373

73.6

48.5

57.8 43.5

0.948 0.190

0.394

75.0

50.3

58.9 46.9

0.950 0.252

0.369

72.6

47.5

56.4 43.0

0.945 0.384

0.391

74.7

49.6

58.4 46.0

0.949 0.412

0.437

43.8

33.1

39.8 28.2

0.888

–

0.098 11.8

3.4

10.0 5.0

0.826

–

0.099 12.2

3.5

10.2 5.1

0.830

–

0.101 12.4

3.6

10.4 5.2

0.832

–

0.102 12.6

3.7

10.5 5.3

0.832

–

0.210

28.4

10.9

22.4 7.4

0.836

–

0.198

28.1

10.5

22.8 8.2

0.834

–

0.214

29.5

11.7

23.9 9.6

0.839

–

0.235

31.8

13.8

26.0 11.0

0.845

–

0.247

33.1

15.0

27.2 11.9

0.849

–

0.089 13.6

4.4

11.3 2.7

0.691 -1.355

0.331

58.2

36.8

52.6 33.4

0.874 0.009

0.363

70.1

48.3

60.1 42.2

0.914 0.179

0.216

46.5

23.2

38.1 17.0

0.918 -0.186

0.462

83.4

63.1

70.3 66.1

0.967 0.458

0.442

78.8

59.2

67.2 60.2

0.948 0.416

0.461

82.3

62.1

69.7 65.2

0.955 0.451

0.473

83.8

64.4

71.6 68.0

0.959 0.479

0.472

83.5

63.6

71.0 67.6

0.958 0.47

0.613

34.8

13.4

33.0 47.0

0.888

–

0.553

29.7

10.5

28.4 41.1

0.942

–

0.602

33.0

12.7

31.3 44.3

0.949

–

0.614

33.4

13.4

32.1 46.4

0.952

–

0.624

34.3

13.7

32.8 47.2

0.952

–

0.556

90.3

86.1

89.9 88.3

0.967 0.358

0.649

95.7

92.9

95.5 95.1

0.974 0.495

0.560

90.1

82.3

89.6 92.4

0.982 0.407

0.581

92.1

92.3

92.6 93.4

0.984 0.468

0.178

38.3

15.4

32.4 12.2

0.853 -0.290

0.196

40.7

16.9

34.1 14.3

0.858 -0.248

0.135 29.8

9.8

25.5 7.4

0.832 -0.437

0.162

36.3

13.7

30.6 10.1

0.85 -0.324

0.183

39.3

15.7

33.0 12.5

0.857 -0.27

0.203

41.8

17.4

34.7 15.2

0.862 -0.218

0.153

33.1

11.9

27.8 9.3

0.839 -0.369

0.174

37.3

14.9

31.9 12.0

0.851 -0.303

0.128 27.2

8.5

23.2 6.9

0.825 -0.471

0.149

32.5

11.1

26.9 8.8

0.839 -0.377

0.167

35.0

12.7

28.8 11.0

0.846 -0.337

0.185

38.6

15.4

32.3 13.6

0.855 -0.268

0.164

34.4

13.0

28.1 11.7

0.837 -0.414

0.204

43.7

20.8

37.9 17.5

0.866 -0.252

0.154

29.4

10.9

23.4 13.0

0.823 -0.595

0.168

32.5

13.6

26.0 15.5

0.834 -0.507

0.185

36.2

15.0

29.1 16.9

0.846 -0.405

0.208

41.5

19.6

34.7 19.9

0.86 -0.291

0.150

32.0

11.1

26.4 9.2

0.836 -0.394

0.183

38.1

15.4

32.5 13.3

0.853 -0.284

0.12 23.5

6.0

19.0 6.1

0.812 -0.56

0.129 26.0

7.5

20.5 7.4

0.82 -0.513

0.146 29.9

9.6

23.8 9.2

0.833 -0.421

0.173

35.5

13.0

29.2 12.4

0.847 -0.308

Table 3: The set of baseline results we release alongside GEM with a focus on reference-based evaluation.

Dataset CommonGen Czech Restaurant
DART E2E clean MLSum (de)
MLSum (es)
Schema-Guided ToTTo XSum WebNLG (en)
WebNLG (ru)
Turk ASSET WikiLingua (es→en)
WikiLingua (ru→en)
WikiLingua (tr→en)
WikiLingua (vi→en)

Model
BART T5
mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++
BART T5
BART LSTM T5 TGen
mBART mT5-small mT5-base mT5-large mT5-XL mBART mT5-small mT5-base mT5-large mT5-XL
BART T5
T5
PEGASUS
mBART mT5-small mT5-base mT5-large mT5-XL mBART mT5-small mT5-base mT5-large mT5-XL
BART T5
BART T5
mBART mBART+ mT5-small mT5-base mT5-large mT5-XL mBART mBART+ mT5-small mT5-base mT5-large mT5-XL mBART mBART+ mT5-small mT5-base mT5-large mT5-XL mBART mBART+ mT5-small mT5-base mT5-large mT5-XL

MSTTR
0.57 0.51
0.51 0.49 0.57 0.6 0.57 0.61 0.56
0.55 0.51
0.32 0.31 0.30 0.31
0.78 0.75 0.76 0.76 0.77 0.71 0.69 0.71 0.71 0.72
0.56 0.67
0.73
0.73
0.53 0.5 0.53
0.54 0.54 0.46 0.43 0.47 0.48 0.46
0.73 0.73
0.73 0.73
0.55 0.58 0.39 0.52 0.57 0.6 0.54 0.55 0.4 0.55 0.59 0.6 0.45 0.52 0.55 0.59 0.58 0.58 0.54 0.54 0.5 0.58 0.6 0.61

Metrics (Diversity and System Characterization)

Distinct1 Distinct2 H1 H2 Unique1 Unique2

|V |

0.12 0.41 7.1 10.7 583 2.7k 1.2k 0.11 0.36 6.5 10.1 465 2.0k 1.0k

0.04

0.1 6.2 7.8

86 278 287

0.03 0.09 6.1 7.6

80 249 273

0.05 0.13 6.6 8.4 103 387 361

0.06 0.19 6.8 9.0 146 614 438

0.03 0.11 6.4 8.0

58 239 245

0.04 0.12 6.5 8.1

84 290 305

0.04 0.11 6.5 8.1

85 280 297

0.19 0.45 8.4 11.3 1.3k 3.6k 2.4k 0.19 0.42 8.0 10.7 1.2k 3.1k 2.1k

0.005 0.004 0.004 0.004

0.02 5.7 7.2 0.02 5.6 7.1 0.01 5.6 6.9 0.02 5.6 7.2

16 104 149

19 106 139

7

60 125

19 116 140

0.11 0.52 10.6 16.3 27k 166k 46k 0.12 0.52 10.4 15.8 20.1k 113.8k 33.6k 0.12 0.53 10.4 15.8 20.2k 113.0k 33.3k 0.12 0.53 10.4 15.8 20.0k 114.0k 33.3k 0.12 0.53 10.4 15.8 20.0k 114.6k 33.3k 0.10 0.47 10.1 15.7 19k 120k 35k 0.12 0.48 10.0 15.1 14.0k 77.6k 25.5k 0.12 0.5 10.1 15.3 15.1k 85.2k 27.2k 0.12 0.5 10.1 15.3 14.9k 82.0k 26.6k 0.12 0.5 10.1 15.3 14.8k 80.5k 26.1k

0.02 0.06 7.0 9.2 1.8k 6.2k 3.9k 0.03 0.10 7.9 10.6 1.6k 5.8k 3.8k

0.18 0.54 10.1 14.4 15k 60k 21k

0.20 0.64 9.3 13.1 3.0k 13k 5k

0.09 0.27 8.6 11.8 969 4.0k 3.2k 0.09 0.25 8.6 11.8 864 3.9k 3.2k 0.09 0.27 8.7 11.9 983 4.4k 3.3k 0.09 0.29 8.7 12.0 1.1k 4.8k 3.4k 0.09 0.29 8.7 12.0 1.1k 4.8k 3.4k 0.08 0.20 8.1 10.3 334 1.1k 1.2k 0.08 0.20 7.9 10.2 349 1.2k 1.2k 0.09 0.23 8.2 10.7 482 1.6k 1.4k 0.09 0.24 8.2 10.7 474 1.6k 1.4k 0.09 0.22 8.2 10.5 418 1.4k 1.3k

0.23 0.74 9.8 14.1 5.5k 23k 8.6k 0.22 0.72 9.9 14.2 5.9k 25k 9.3k

0.23 0.73 9.8 14.1 5.9k 24k 9.1k 0.22 0.72 9.9 14.2 5.9k 26k 9.4k

0.03 0.19 8.8 14.0 4.7k 63k 15k 0.03 0.21 9.1 14.5 5.9k 83k 18k 0.03 0.15 8.3 12.8 2.3k 20.9k 8.2k 0.04 0.23 8.7 13.7 3.5k 34.4k 10.3k 0.04 0.26 8.9 14.0 4.2k 44.4k 11.7k 0.04 0.29 9.1 14.4 5.0k 57.7k 13.5k 0.04 0.20 8.5 13.3 2.8k 28k 8.7k 0.04 0.23 8.8 13.7 3.5k 35k 10k 0.04 0.19 8.2 12.6 1.5k 11.6k 5.5k 0.06 0.3 8.6 13.4 2.5k 21.0k 7.1k 0.06 0.32 8.7 13.6 3.0k 26.1k 7.9k 0.07 0.35 8.8 13.8 3.4k 29.0k 8.5k 0.08 0.28 7.7 11.2 743 4.1k 2.1k 0.12 0.38 8.0 11.9 1.2k 6.1k 2.8k 0.14 0.46 8.1 11.6 935 4.4k 2.1k 0.16 0.51 8.2 11.9 1.0k 4.8k 2.2k 0.16 0.5 8.1 11.8 1.0k 4.7k 2.2k 0.16 0.51 8.2 11.8 1.0k 4.7k 2.1k 0.07 0.28 8.2 12.3 1.5k 9.3k 4.0k 0.08 0.33 8.6 12.9 2.1k 13k 5.3k 0.09 0.33 8.2 12.1 1.2k 6.4k 3.1k 0.12 0.43 8.4 12.6 1.6k 8.9k 3.7k 0.12 0.45 8.5 12.7 1.7k 9.3k 3.8k 0.12 0.47 8.6 12.9 1.8k 10.2k 4.0k

Output Len.
10.5 9.6
10.2 10.5 10.1 9.5 9.1 9.2 9.5
12.0 10.8
22.0 23.1 23.0 23.2
35.7 24.7 24.2 24.4 24.5 32.3 21.7 23.0 22.1 21.4
22.0 11.8
15.3
22.9
20.7 22.7 21.7 21.7 21.6 18.9 19.2 19.9 19.4 19.5
18.4 20.1
20.1 21.3
29.4 32.5 31.8 28.7 30.8 34.7 27.3 28.4 31.8 28.7 31.1 31.4 34.2 30.7 40.2 38.7 38.0 36.8 26.9 29.8 32.9 31.1 30.7 31.5

Table 4: Results of the baseline results we release with GEM, focusing on diversity of the outputs and neutral system characterizations.

Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be ﬁltered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission.

grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score.
Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1, H2), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2), and the count of n-grams that only appear once across the entire test output (Unique1,2, Li et al., 2016).
System Characterization. The ﬁnal section of metrics will characterize the systems. While the

focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019).
5 Results
One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We thus will not distill the complex interplay of the data, metrics, and model outputs into a single number or statement, and we do not present results in a traditional leaderboard. Instead, we developed an interactive result exploration system that allows analyses of model results, and which we describe in this section. To further motivate this change, consider the following conclusion someone may draw from looking at a leaderboard:
System Foo performs the best.
Our interactive system aims to enable more nuanced statements such as:

System Foo leads to consistent performance increases in Bar-type metrics on challenges that measure Baz while maintaining equal performance on most metrics of type Qux.
A screenshot of our system is presented in Figure 2.12 In addition, our baseline results are presented in a tabular view in Tables 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the ﬁltered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar ﬁltering and sorting, but presents the results in a table.
However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it is unclear whether this difference can be attributed to the dataset or the system but this relationship will be uncovered once we receive submissions.
The ﬁnal system will additionally be able to display the model cards and other related metainformation associated with submissions. It will also be able to show (and compare) exemplary outputs for each test set. Those two features will improve the transparency of the results and systems to those who are not familiar with a task and provide necessary information to those who consider using a particular system. The combination of all components will enable analysis on quantitative, individual, and qualitative level which can support formulating new research hypotheses and gather in-depth insights about system performance. For example, the functionality to compare human anno-
12An initial version showcasing our baseline results is deployed on our website.

tation and automatic measures could lead to a better understanding how ﬂuency affect BERTScore.
In addition to the interactive self-directed result exploration, our shared task features an evaluation and analysis part. Instead of dictating the interpretation of the modeling shared task results, we will release all system outputs and metrics in this second part and participants of this part may run their own evaluation and conduct interesting analyses.
6 Submitting to the benchmark
While we ask submitters to try to cover as many tasks as possible, we acknowledge potential restrictions on computation resources. We thus do not require that a submissions to GEM has to include predictions on every included test and challenge sets. All predictions from a model should be formatted and added into a single ﬁle as outlined on our website.
In addition, we require every submitter to answer a series of questions that we will use to construct a model card (Mitchell et al., 2019) and externalize potential concerns regarding the social impact of a model and its use, or its training data. The card will additionally display information to replicate the experiments. While we require responses to these questions at submission time, we allow the information about a model to remain anonymous during required anonymization periods should a paper describing the model be under submission elsewhere. All submitted model outputs will be made publicly available for download.
After a submission, we will run the evaluation suite on the submitted outputs and additionally collect human annotations.
Human Evaluation GEM will be used to develop reproducible and consistent human evaluation strategies for generated text. This task involves selecting and deﬁning which quantities of the generated text should be measured, developing annotation schemes and rater guidelines to capture these quantities accurately, and infrastructure to annotate system outputs.
We aim to develop these setups for all task setups such as summarization, dialogue, simpliﬁcation, and data-to-text. To approach this task, we will follow the recently proposed taxonomy of human evaluation measures by Belz et al. (2020) and follow the reporting strategies proposed by Howcroft et al. (2020). The detailed setups will be described in a evaluation datasheet (Shimorina and Belz, 2021).

All shared task participants will be asked to provide gold annotations on system outputs, which we will then use to evaluate the consistency of crowdsourced annotations.13
7 Next Steps
This section lists the currently active developments and the long-term steps we will take to ensure that GEM will continue to evolve and improve.
7.1 Collecting more multilingual data
Many of the initial datasets in GEM are focused on (American or British) English; we see this release as a starting point for the collection of new datasets to improve the inclusiveness of other languages and cultures. From the task point of view, to ensure the longevity of the dataset, we want it to be practical and socially beneﬁcial. Through GEM, we have developed a set of desired criteria for NLG datasets and we aim to apply this knowledge to data collection and actively work toward reducing the disparity in data availability between languages (Joshi et al., 2020). To this end, we are focusing on a task that requires content selection, planning, and surface realization along in a grounded scenario. The idea is in the prototyping stage with prospects broadly towards dialog response generation and topic summarization in multiple languages. We plan to do so by collaborating with speakers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort.
7.2 Personalizing and Controlling NLG
GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g.,
13This approach has been successfully used by WMT for many years. See, e.g., http://www.statmt.org/ wmt20/translation-task.html.

dataset, evaluation) to incorporate this line of userfocused NLG.
7.3 Regular updates to the living benchmark
To activate the beneﬁts of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selection process similar to the one described in Section 3. The ﬁrst update process will be run after the GEM workshop at ACL 2021. To be able to have a robust comparison between different versions of GEM, we will only replace a small subset of datasets at a time.
8 Conclusion
In this paper, we have introduced GEM, a living natural language generation benchmark with a focus on evaluation. While GEM does not claim to instantly solve all issues of benchmarks in NLG, we aim to provide an environment in which systems can be tested in a principled manner and which can elevate the prominence of interesting evaluation approaches. By providing a testbed to easily conduct experiments across many datasets and evaluate in a repeatable, consistent, and more interpretable way, we will be able to track progress toward the goals in NLG research much more clearly. Moreover, we will be able to extend and shape GEM in the future to include more multilingual datasets, which will assist in their adoption across the wider research community.
9 Contribution Statements
GEM is a large effort with a decentralized organization that is split into different task-speciﬁc subgroups. To acknowledge everyone’s contribution, we list the contribution statements below for all groups.
Steering Committee. Antoine Bosselut, Esin Durmus, Varun Prashant Gangal, Sebastian Gehrmann, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu make up the steering committee. Sebastian Gehrmann coordinates and leads the GEM effort. All others provide feedback and discuss larger decisions regarding the direction of

GEM and act as conference organizers for the ACL 2021 workshop.
Summarization. The summarization group members are Chris Emezue, Esin Durmus, Faisal Ladhak, Jiawei Zhou, Juan Diego Rodriguez, Kaustubh Dhole, Khyathi Chandu, Laura Perez, Pawan Sasanka Ammanamanchi, Pedro Henrique Martins, Rubungo Andre Niyongabo, Shashi Narayan, Vikas Raunak, and Yufang Hou. Pedro Henrique Martins organized the group and wrote the data statement for the MLSum dataset. Pawan Sasanka Ammanamanchi was responsible for the XSum data statement, while Vikas Raunak worked on the Wikilingua statement. Shashi Narayan prepared the GEM version of the XSum dataset and trained its baseline models. Juan Diego Rodriguez was responsible for cleaning the MLSum dataset and trained its baseline models. Faisal Ladhak was responsible for the Wikilingua baseline models. Rubungo Andre Niyongabo participated in the discussions and added related papers to the planning document.
Dialog. Sashank Santhanam, Samira Shaikh, Bodhisattwa Prasad Majumder, Harsh Jhamtani, Yangfeng Ji, Tosin Adewumi, and Wanyu Du are part of this group. Tosin Adewumi contributed code for DialoGPT, and Wanyu Du trained baselines for Schema-Guided Dialog. Harsh Jhamtani wrote the data card for Wizards of Wikipedia.
Data2Text. Ondrej Dusek wrote the data cards for E2E NLG and Czech Restaurants data and a TF loader for Czech Restaurants. He also supplied baseline outputs for E2E, Czech Restaurants, and WebNLG. Sebastian Gehrmann supplied baseline outputs for E2E, WebNLG, and CommonGen. Yacine Jernite wrote the data card for CommonGen and the Hugging Face loaders for Czech Restaurants and WebNLG. Teven Le Scao wrote the Hugging Face loader for E2E. Simon Mille and Anastasia Shimorina wrote the data card for WebNLG.
Table2Text. Varun Gangal and Miruna Clinciu are part of this group. Miruna Clinciu was responsible primarily for DART and Varun Gangal for ToTTo while maintaining a close correspondence and understanding between them to ensure all steps, such as code structure, preprocessing primitives, baselines were as uniform as possible.
Simpliﬁcation. Dhruv Kumar, Mounica Maddela, and Wei Xu contributed to the GEM Simpli-

ﬁcation task. Dhruv Kumar created the data cards for the datasets, added Wiki-Auto and Turk/ASSET datasets to TFDS, and integrated the SARI metric (Xu et al., 2016) into the GEM evaluation framework. Mounica Maddela created baselines for the task and added the Turk benchmark corpus to Hugging Face and TFDS. Wei Xu helped in the organization and planning of the task setup.
Automated Evaluation. Ondrej Dusek wrote the base code and included BLEU, Meteor, ROUGE, and referenceless metrics (the latter based on code supplied by Emiel van Miltenburg). He also prepared reference sets for E2E, Czech Restaurants and WebNLG. Sebastian Gehrman included BLEURT and BERTScore and prepared the reference sets. Dhruv Kumar included SARI and adapted the code for source-based metrics. Nishant Subramani helped with code refactoring. Miruna Clinciu , Emiel van Miltenburg and Thibault Sellam provided feedback and participated in discussions.
Human Evaluation. Samira Shaikh was the point of contact for this working group. She led the discussions to make progress on the group goals. She also worked with the group to select the general evaluation criteria as well as the criteria for dialogue and simpliﬁcation tasks. Khyathi Chandu and Miruna Clinciu worked on selecting evaluation criteria for the summarization task and participated in the group discussions. Simon Mille provided support on using the criteria taxonomy and the annotated evaluation sheets for selecting and deﬁning the criteria to use; worked on selecting the D2T criteria. Vitaly Nikolaev and Sashank Santhanam worked on selecting evaluation criteria for dialog and simpliﬁcation tasks. João Sedoc worked with the group to select the evaluation criteria in general as well as the speciﬁc ones for dialog and simpliﬁcation. He also helped to select among annotation interfaces. Anastasia Shimorina worked with the group to select the evaluation criteria and participated in the discussions. Chris Emezue, Sebastian Gehrmann, Khyati Mahajan, and Yufang Hou participated in discussions.
Website and Submission System. Aman Madaan, Moin Nadeem, Hendrik Strobelt, and Sebastian Gehrmann are part of this group. Sebastian Gehrmann developed the website. Aman Madaan wrote the initial version of the result presentation. Hendrik Strobelt leads the visualization effort for

interactive exploration of results.
Model Infrastructure. Yacine Jernite wrote the initial script template for evaluating and ﬁne-tuning Hugging Face models with the CommonGen example. Sebastian Gehrmann generalized the script to work with other datasets. Tosin Adewumi wrote a script for ﬁne-tuning the DialoGPT model for dialogue datasets. Juan Diego Rodriguez worked on the infrastructure to ﬁne-tune mBART on MLSum. Mihir Kale trained all mT5 baselines.
Data and Model Sheets and Statements. Salomey Osei, Pawan Sasanka Ammanamanchi, Juan Diego Rodriguez, Sebastian Gehrmann, Yacine Jernite, and Angelina McMillan-Major are part of this group. The Data Sheet structure was adapted from a combination of designs created for the Hugging Face Datasets library by Angelina McMillan-Major and Yacine Jernite and one written by Sebastian Gehrmann. Juan Diego Rodriguez and Yacine Jernite wrote initial statements for ASSET and CommonGen respectively. The feedback on those was used to improve the structure of the ﬁnal template. Everyone contributed to the model card template.
Challenge Sets. Simon Mille, Emiel van Miltenburg, Kaustubh Dhole, Varun Prashant Gangal, Saad Mahamood, and Laura Perez-Beltrachini proposed and discussed ideas of interest for the data-to-text and the text-to-text tasks. Simon Mille coordinated the group. Emiel van Miltenburg, Saad Mahamood, and Simon Mille worked on the creation of the data-to-text datasets, while Varun Prashant Gangal, Kaustubh Dhole and Laura PerezBeltrachini worked on the text-to-text datasets. Sebastian Gehrmann contributed the ToTTo challenge set.
Crowdsourcing New Data. Chris Emezue, Rubungo Andre Niyongabo, Aremu Anuoluwapo, Khyathi Chandu, Yufang Hou, Samira Shaikh, Varun Prashant Gangal, and Dimitra Gkatzia are members of this group. Khyathi Chandu worked on identifying where the current datasets fall short to motivate the crowdsourcing of data for a new task. Based on the suggestions from collaborators, she wrote two task proposals in the domains of longform text, conversations, and data-to-text that address an array of challenges in generation and easily scale to multiple languages. Samira Shaikh participated in the discussions and gave feedback on the task proposals in the pilot study phase. Dimitra

Gkatzia looked into potential resources for crowdsourcing. Chris Emezue and Rubungo Andre Niyongabo explored potential low-resource African languages for crowdsourcing. We are in the process of piloting the tasks internally.
The authors of this paper not named in the groups participated in initial discussions, participated in the surveys, and provided regular feedback and guidance. Many participants commented on and helped write this paper. We additionally thank all participants of INLG 2019, the Generation Birdsof-a-Feather meeting at ACL 2020, the EvalNLGEval Workshop at INLG 2020, and members of the generation challenge mailing list of SIGGEN for their participation in the discussions that inspired and inﬂuenced the creation of GEM.
References
Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470–6484, Online. Association for Computational Linguistics.
Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoît Sagot, and Lucia Specia. 2020. ASSET: A dataset for tuning and evaluation of sentence simpliﬁcation models with multiple rewriting transformations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4668–4679, Online. Association for Computational Linguistics.
Antonios Anastasopoulos and Graham Neubig. 2020. Should all cross-lingual embeddings speak English? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8658–8679, Online. Association for Computational Linguistics.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65–72. Association for Computational Linguistics.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The ﬁrst surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 217–226, Nancy, France. Association for Computational Linguistics.
Anya Belz, Simon Mille, and David M. Howcroft. 2020. Disentangling the properties of human evaluation methods: A classiﬁcation system to support comparability, meta-evaluation and reproducibility testing. In Proceedings of the 13th International Conference on Natural Language Generation, pages 183–194, Dublin, Ireland. Association for Computational Linguistics.
Emily Bender. 2019. The #benderrule: On naming the languages we study and why it matters. The Gradient.
Emily M. Bender. 2011. On achieving and evaluating language-independence in NLP. Linguistic Issues in Language Technology, 6.
Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–604.
Ondˇrej Bojar, Yvette Graham, and Amir Kamran. 2017. Results of the WMT17 metrics shared task. In Proceedings of the Second Conference on Machine Translation, pages 489–513, Copenhagen, Denmark. Association for Computational Linguistics.
Ondˇrej Bojar, Yvette Graham, Amir Kamran, and Miloš Stanojevic´. 2016. Results of the WMT16 metrics shared task. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 199–231, Berlin,

Germany. Association for Computational Linguistics.
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. CoRR, abs/2006.14799.
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.
Michael A Covington, Congzhou He, Cati Brown, Lorina Naci, and John Brown. 2006. How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale.
Emily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, and Morgan Klaus Scheuerman. 2020. Bringing the people back in: Contesting benchmark machine learning datasets. CoRR, abs/2007.07399.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884–4895, Florence, Italy. Association for Computational Linguistics.
Justin Dieter, Tian Wang, Arun Tejasvi Chaganty, Gabor Angeli, and Angel X. Chang. 2019. Mimic and rephrase: Reﬂective listening in openended dialogue. In Proceedings of the 23rd

Conference on Computational Natural Language Learning (CoNLL), pages 393–403, Hong Kong, China. Association for Computational Linguistics.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342–1352, Vancouver, Canada. Association for Computational Linguistics.
Dheeru Dua, Ananth Gottumukkala, Alon Talmor, Sameer Singh, and Matt Gardner. 2019. ORB: An open reading benchmark for comprehensive evaluation of machine reading comprehension. In EMNLP 2019 MRQA Workshop, page 147.
Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–5070, Online. Association for Computational Linguistics.
Ondˇrej Dušek, David M. Howcroft, and Verena Rieser. 2019. Semantic noise matters for neural natural language generation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 421–426, Tokyo, Japan. Association for Computational Linguistics.
Ondrej Dušek and Filip Jurcıcek. 2016. A contextaware natural language generation dataset for dialogue systems. In RE-WOCHAT: Workshop on Collecting and Generating Resources for Chatbots and Conversational Agents-Development and Evaluation Workshop Programme (May 28 th, 2016), page 6.
Ondˇrej Dušek and Filip Jurcˇícˇek. 2016a. A contextaware natural language generator for dialogue systems. In Proceedings of the 17th Annual

Meeting of the Special Interest Group on Discourse and Dialogue, pages 185–190, Los Angeles. Association for Computational Linguistics.
Ondˇrej Dušek and Filip Jurcˇícˇek. 2016b. Sequenceto-sequence generation for spoken dialogue via deep syntax trees and strings. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 45–51, Berlin, Germany. Association for Computational Linguistics.
Ondˇrej Dušek and Filip Jurcˇícˇek. 2019. Neural generation for Czech: Data and baselines. In Proceedings of the 12th International Conference on Natural Language Generation, pages 563–574, Tokyo, Japan. Association for Computational Linguistics.
Ondrej Dusek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge. Comput. Speech Lang., 59:123–156.
Kawin Ethayarajh and Dan Jurafsky. 2020. Utility is in the eye of the user: A critique of NLP leaderboards. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4846–4853, Online. Association for Computational Linguistics.
Matan Eyal, Tal Baumel, and Michael Elhadad. 2019. Question answering as an automatic evaluation metric for news article summarization. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3938–3948, Minneapolis, Minnesota. Association for Computational Linguistics.
Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir R. Radev. 2020. SummEval: Reevaluating summarization evaluation. CoRR, abs/2007.12626.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages

3558–3567, Florence, Italy. Association for Computational Linguistics.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational Linguistics.
Thiago Castro Ferreira, Claire Gardent, Chris van der Lee, Nikolai Ilinykh, Simon Mille, Diego Moussalem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020). In Proceedings of the 3rd WebNLG Workshop on Natural Language Generation from the Semantic Web (WebNLG+ 2020), Dublin, Ireland (Virtual). Association for Computational Linguistics.
∀, Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Muhammad, Salomon Kabongo Kabenamualu, Salomey Osei, Freshia Sackey, Rubungo Andre Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa Berhe, Mofetoluwa Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Abbott, Iroro Orife, Ignatius Ezeani, Idris Abdulkadir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru, Ghollah Kioko, Murhabazi Espoir, Elan van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris Chinenye Emezue, Bonaventure F. P. Dossou, Blessing Sibanda, Blessing Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp Öktem, Adewale Akinfaderin, and Abdallah Bashir. 2020. Participatory research for low-resourced machine translation: A case study in African languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2144–2160, Online. Association for Computational Linguistics.
Markus Freitag, David Grangier, and Isaac Caswell. 2020. BLEU might be guilty but references are not innocent. In Proceedings of the 2020 Confer-

ence on Empirical Methods in Natural Language Processing (EMNLP), pages 61–71, Online. Association for Computational Linguistics.
Jinlan Fu, Pengfei Liu, and Graham Neubig. 2020. Interpretable multi-dataset evaluation for named entity recognition. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6058– 6069, Online. Association for Computational Linguistics.
Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2020. Go ﬁgure! A meta evaluation of factuality in summarization. CoRR, abs/2010.12834.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124–133, Santiago de Compostela, Spain. Association for Computational Linguistics.
Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. J. Artif. Intell. Res., 61:65–170.
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018. Datasheets for datasets. In Proceedings of the Fifth Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, Sweden.
Tatsunori Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying human and statistical evaluation for natural language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1689–1701, Minneapolis, Minnesota. Association for Computational Linguistics.
Kenneth Heaﬁeld, Hiroaki Hayashi, Yusuke Oda, Ioannis Konstas, Andrew Finch, Graham Neubig, Xian Li, and Alexandra Birch. 2020. Findings of the fourth workshop on neural generation and translation. In Proceedings of the Fourth

Workshop on Neural Generation and Translation, pages 1–9, Online. Association for Computational Linguistics.
Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1693–1701.
David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised deﬁnitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169–182, Dublin, Ireland. Association for Computational Linguistics.
Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LCSTS: A large scale Chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967–1972, Lisbon, Portugal. Association for Computational Linguistics.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 4411–4421. PMLR.
Alfred Inselberg. 1985. The plane with parallel coordinates. Vis. Comput., 1(2):69–91.
Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural CRF model for sentence alignment in text simpliﬁcation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943–7960, Online. Association for Computational Linguistics.

Md. Asifuzzaman Jishan, Khan Raqib Mahmud, and Abul Kalam Al Azad. 2019. Bangla Natural Language Image to Text (BNLIT).
Wendell Johnson. 1944. Studies in language behavior: A program of research. Psychological Monographs, 56(2):1–15.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282–6293, Online. Association for Computational Linguistics.
Mihir Kale and Abhinav Rastogi. 2020. Few-shot natural language generation by rewriting templates. arXiv preprint arXiv:2004.15006.
Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, and Mohamed Coulibali. 2020. NUBIA: NeUral based interchangeability assessor for text generation. In Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 28–37, Online (Dublin, Ireland). Association for Computational Linguistics.
Chris Kedzie, Kathleen McKeown, and Hal Daumé III. 2018. Content selection in deep learning models of summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1818–1828, Brussels, Belgium. Association for Computational Linguistics.
Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, and Daniel S. Weld. 2021. GENIE: A leaderboard for human-inthe-loop evaluation of text generation. CoRR, abs/2101.06561.
Tomáš Kocˇisky`, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328.
Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. 2020. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Association

for Computational Linguistics: EMNLP 2020, pages 4034–4048, Online. Association for Computational Linguistics.
Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203–1213, Austin, Texas. Association for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.
Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020b. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315–7330, Online. Association for Computational Linguistics.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversitypromoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California. Association for Computational Linguistics.
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou. 2018. Visual question generation as dual task of visual question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6116–6124. IEEE Computer Society.
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Bruce Zhang, Rahul Agrawal, Edward Cui,

Sining Wei, Taroon Bharti, Ying Qiao, JiunHung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Rangan Majumder, and Ming Zhou. 2020. XGLUE: A new benchmark dataset for crosslingual pre-training, understanding and generation. CoRR, abs/2004.01401.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823–1840, Online. Association for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.
Tal Linzen. 2020. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210–5217, Online. Association for Computational Linguistics.
Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou, Ming Gong, Pengcheng Wang, Jiusheng Chen, Daxin Jiang, Jiancheng Lv, Ruofei Zhang, Winnie Wu, Ming Zhou, and Nan Duan. 2020a. GLGE: A new general language generation evaluation benchmark. CoRR, abs/2011.11928.
Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020b. Multilingual denoising pre-training for neural machine translation. Trans. Assoc. Comput. Linguistics, 8:726–742.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 285– 294, Prague, Czech Republic. Association for Computational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4):474– 496.
Qingsong Ma, Ondˇrej Bojar, and Yvette Graham. 2018. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 671–688, Belgium, Brussels. Association for Computational Linguistics.
Qingsong Ma, Johnny Wei, Ondˇrej Bojar, and Yvette Graham. 2019. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62–90, Florence, Italy. Association for Computational Linguistics.
Emma Manning, Shira Wein, and Nathan Schneider. 2020. A human evaluation of amr-to-english generation systems. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4773–4786. International Committee on Computational Linguistics.
Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020a. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984–4997, Online. Association for Computational Linguistics.

Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondˇrej Bojar. 2020b. Results of the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 688–725, Online. Association for Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, Online. Association for Computational Linguistics.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. CoRR, abs/1806.08730.
Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, Emily Pitler, and Leo Wanner. 2018. The ﬁrst multilingual surface realisation shared task (SR’18): Overview and evaluation results. In Proceedings of the First Workshop on Multilingual Surface Realisation, pages 1–12, Melbourne, Australia. Association for Computational Linguistics.
Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, and Leo Wanner, editors. 2019. Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019). Association for Computational Linguistics, Hong Kong, China.
Simon Mille, Anya Belz, Bernd Bohnet, Thiago Castro Ferreira, Yvette Graham, and Leo Wanner. 2020. The third multilingual surface realisation shared task (SR’20): Overview and evaluation results. In Proceedings of the Third Workshop on Multilingual Surface Realisation, pages 1–20, Barcelona, Spain (Online). Association for Computational Linguistics.
Emiel van Miltenburg, Desmond Elliott, and Piek Vossen. 2018. Measuring the diversity of automatic image descriptions. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1730–1741, Santa Fe, New Mexico, USA. Association for Computational Linguistics.
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In

Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783–5797, Online. Association for Computational Linguistics.
Shachar Mirkin and Jean-Luc Meunier. 2015. Personalized machine translation: Predicting translational preferences. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025, Lisbon, Portugal. Association for Computational Linguistics.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220–229.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çag˘lar GuÌ‡lçehre, and Bing Xiang. 2016. Abstractive text summarization using sequenceto-sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280– 290, Berlin, Germany. Association for Computational Linguistics.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium. Association for Computational Linguistics.
Jekaterina Novikova, Ondˇrej Dušek, and Verena Rieser. 2017. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206, Saarbrücken, Germany. Association for Computational Linguistics.
Juri Opitz and Anette Frank. 2020. Towards a decomposable metric for explainable evaluation of text generation from amr. arXiv preprint arXiv:2008.08896.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for auto-

matic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to-text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1173–1186, Online. Association for Computational Linguistics.
Laura Perez-Beltrachini and Claire Gardent. 2017. Analysing data-to-text generation benchmarks. In Proceedings of the 10th International Conference on Natural Language Generation, pages 238–242, Santiago de Compostela, Spain. Association for Computational Linguistics.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2020. KILT: a benchmark for knowledge intensive language tasks. CoRR, abs/2009.02252.
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic´, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362–2376, Online. Association for Computational Linguistics.
Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. 2020. Dynasent: A dynamic benchmark for sentiment analysis. CoRR, abs/2012.15349.
Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with entity modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035, Florence, Italy. Association for Computational Linguistics.
Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit

Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and Richard Socher. 2020. DART: open-domain structured data record to text generation. CoRR, abs/2007.02871.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.
Sudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 129–140, New Orleans, Louisiana. Association for Computational Linguistics.
Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8689–8696. AAAI Press.
Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266.
Ehud Reiter. 2018. A structured review of the validity of BLEU. Comput. Linguistics, 44(3).
Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.
Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with

CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–4912, Online. Association for Computational Linguistics.
Viktor Schlegel, Goran Nenadic, and Riza BatistaNavarro. 2020. Beyond leaderboards: A survey of methods for revealing weaknesses in natural language inference data and models. CoRR, abs/2005.14709.
Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. MLSUM: The multilingual summarization corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8051–8067, Online. Association for Computational Linguistics.
Thomas Scialom, Paul-Alexis Dray, Gallinari Patrick, Lamprier Sylvain, Piwowarski Benjamin, Staiano Jacopo, and Wang Alex. 2021. Safeval: Summarization asks for fact-based evaluation. arXiv preprint arXiv:2103.12693.
Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3246–3256, Hong Kong, China. Association for Computational Linguistics.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics.
Claude E Shannon and Warren Weaver. 1963. A mathematical theory of communication.
Eva Sharma, Chen Li, and Lu Wang. 2019. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204–2213, Florence, Italy. Association for Computational Linguistics.

Anastasia Shimorina and Anya Belz. 2021. The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in nlp. arXiv preprint arXiv:2103.09710.
Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk, and William Yang Wang. 2019. What should I ask? using conversationally informative rewards for goal-oriented visual dialog. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6442–6451, Florence, Italy. Association for Computational Linguistics.
Miloš Stanojevic´, Amir Kamran, Philipp Koehn, and Ondˇrej Bojar. 2015. Results of the WMT15 metrics shared task. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 256–273, Lisbon, Portugal. Association for Computational Linguistics.
Simeng Sun, Ori Shapira, Ido Dagan, and Ani Nenkova. 2019. How to compare summarizers without target length? pitfalls, solutions and re-examination of the neural summarization literature. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 21–29, Minneapolis, Minnesota. Association for Computational Linguistics.
Kristina Toutanova, Chris Brockett, Ke M. Tran, and Saleema Amershi. 2016. A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 340–350, Austin, Texas. Association for Computational Linguistics.
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020, Online. Association for Computational Linguistics.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. Superglue: A stickier benchmark for generalpurpose language understanding systems. In

Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.
Sam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for Computational Linguistics.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33.
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simpliﬁcation. Transactions of the Association for Computational Linguistics, 4:401–415.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. CoRR, abs/2010.11934.
Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 109–117, Online. Association for Computational Linguistics.
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020a. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11328–11339. PMLR.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204–2213, Melbourne, Australia. Association for Computational Linguistics.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Xingxing Zhang and Mirella Lapata. 2014. Chinese poetry generation with recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680, Doha, Qatar. Association for Computational Linguistics.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020c. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270–278, Online. Association for Computational Linguistics.
A Task Suggestion Categories
Participants were required to provide information for the following categories when suggesting a dataset for GEM.
1. Dataset Name
2. Reference
3. High-level Task, e.g., data-to-text, or summarization
4. Short Description
5. Challenges, e.g., entity tracking/generation, referring expression generation, surface realization, content selection
6. Communicative goal, e.g., provide speciﬁc information, or entertainment, or accomplish a task
7. Dataset Domain, e.g., Wikipedia, or news articles, Reddit chat, etc)

8. Language(s) 9. Language locale (if known), e.g., en-US, es-
MX 10. Input modality, e.g., text, graph, table, images 11. Input length 12. Output length 13. Output form, e.g., monologue, dialog 14. # Examples in dataset Test split, e.g., i.i.d., or
non-overlap dimension 15. # References per example 16. Data Quality / potential Issues, e.g.,
noisy, clean, biased, code-mixing (different languages/writing systems), (over)normalization 17. License 18. Evaluation strategies (in original paper / papers that use dataset) 19. Why should we use this dataset?
B Considered datasets
The following datasets were proposed to be included in GEM.
1. Alex Context NLG (Dušek and Jurcıcek, 2016; Dušek and Jurcˇícˇek, 2016a)
2. AmbigQA/AmbigNQ (Min et al., 2020) 3. Bangla Natural Language Image to Text (Jis-
han et al., 2019) 4. Big Patent (Sharma et al., 2019) 5. Chinese Poetry (Zhang and Lapata, 2014) 6. CommonGen (Lin et al., 2020) 7. CoQA (Reddy et al., 2019) 8. Czech Restaurant Data (Dušek and Jurcˇícˇek,
2019) 9. DART (Radev et al., 2020) 10. E2E (cleaned) (Novikova et al., 2017; Dušek
et al., 2019) 11. ELI5 (Fan et al., 2019) 12. Hindi Poetry 14 13. LCSTS (Hu et al., 2015) 14. Mimic and Rephrase (Dieter et al., 2019)
14https://www.kaggle.com/shishu1421/hindi-poetrydataset

15. MLSUM (Scialom et al., 2020)

16. MSR Abstractive Text sion (Toutanova et al., 2016)

Compres-

17. MultiWOZ 2.2 (Zang et al., 2020)

18. NarrativeQA (Kocˇisky` et al., 2018)

19. PersonaChat (Zhang et al., 2018)

20. PubMed, Arxiv (Kedzie et al., 2018; Cohan et al., 2018)

21. ROTOWIRE/MLB (Wiseman et al., 2017; Puduppully et al., 2019)

22. Schema-Guided Dialogue (Rastogi et al., 2020)

23. SQUAD Question Generation (Du et al., 2017)

24. SR’11, SR’18, SR’19 (Belz et al., 2011; Mille et al., 2018, 2019)

25. ToTTo (Parikh et al., 2020)

26. Ubuntu Dialogue Generation (Lowe et al., 2015)

27. Visual Question Generation (Shukla et al., 2019; Li et al., 2018)

28. WebNLG (Gardent et al., 2017)

29. WikiAuto + Turk/ASSET (Jiang et al., 2020; Xu et al., 2016; Alva-Manchego et al., 2020)

30. WikiBio (Lebret et al., 2016)

31. WikiSum (Liu et al., 2018)

32. Wizard of Wikipedia (Dinan et al., 2019)

33. Writing Prompts (Fan et al., 2018)

34. XSum (Narayan et al., 2018)

35. WikiLingua (Ladhak et al., 2020)

C Task and Criteria Selection Survey

As part of our selection process, we queried all GEM members about the utility of tasks and selection criteria. The questions below were included in the survey.

• For each suggested task, “Should this task be included in GEM?” on a 5-point Likert scale (1 being strongly against, and 5 strongly in favor).
• We should exclude tasks that are the focus of a shared task in 2021. [yes/no]
• We should exclude tasks that were the focus of a shared task since 2020. [yes/no]

• We should exclude tasks that were ever part of a shared task. [yes/no]
• We should exclude datasets that require paidfor licenses (e.g., LDC or ELRA). [yes/no]
• We should exclude datasets that are not freely available for download. [yes/no]
• We should exclude tasks that require encoding anything but text (e.g., images or graphs). [yes/no]
• We should include # tasks in GEM. [10 points ranging from 2 to 20]
• X% of the tasks should feature non-English language(s). [10 points ranging from 10 to 100%]
• Diversity of tasks is more important than focus on an NLG task (by including multiple datasets for the same task). [10 points from Diversity is more important to Focus is more important]
• We should include noisy and clean datasets. [10 points from only noisy to only clean]
• We should include low- and high-resource datasets. [10 points from only low-resource to only high-resource]
• We should prefer tasks with non-iid test sets or speciﬁc challenge sets. [5-Likert scale from not important to very important]
• We should prefer tasks with test sets with multiple references. [5-Likert scale from not important to very important]
• If we include an NLG task (e.g., simpliﬁcation or data2text), we need multiple datasets for that task. [5-Likert scale from not important to very important]
• We should include a set of tasks with no clear evaluation strategy. [5-Likert scale from not important to very important]
• We should focus on tasks with reliable automatic metrics. [5-Likert scale from not important to very important]

