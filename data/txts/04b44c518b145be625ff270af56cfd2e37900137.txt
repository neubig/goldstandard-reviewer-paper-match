MULTI-CHANNEL END-TO-END NEURAL DIARIZATION WITH DISTRIBUTED MICROPHONES
Shota Horiguchi Yuki Takashima Paola Garc´ıa† Shinji Watanabe‡ Yohei Kawaguchi
Hitachi, Ltd., Japan † CLSP & HLTCOE, Johns Hopkins University, USA
‡ Carnegie Mellon University, USA

arXiv:2110.04694v2 [eess.AS] 28 Mar 2022

ABSTRACT Recent progress on end-to-end neural diarization (EEND) has enabled overlap-aware speaker diarization with a single neural network. This paper proposes to enhance EEND by using multi-channel signals from distributed microphones. We replace Transformer encoders in EEND with two types of encoders that process a multichannel input: spatio-temporal and co-attention encoders. Both are independent of the number and geometry of microphones and suitable for distributed microphone settings. We also propose a model adaptation method using only single-channel recordings. With simulated and real-recorded datasets, we demonstrated that the proposed method outperformed conventional EEND when a multi-channel input was given while maintaining comparable performance with a single-channel input. We also showed that the proposed method performed well even when spatial information is inoperative given multi-channel inputs, such as in hybrid meetings in which the utterances of multiple remote participants are played back from the same loudspeaker.
Index Terms— Speaker diarization, multi-channel, distributed microphones, EEND
1. INTRODUCTION
Meeting transcription is one of the largest application areas of speech-related technologies. One important component of meeting transcription is speaker diarization [1, 2], which gives speaker attributes to each transcribed utterance. In recent years, many endto-end diarization methods have been proposed [3, 4, 5, 6] have achieved comparative accuracy to that of modular-based methods [7, 8]. However, many attempts have been made based on singlechannel recordings, where no spatial information is available. Some meeting transcription systems are based on distributed microphones [9, 10, 11, 12], which enables the ﬂexibility of recording devices and a wide range of sound collection. If we can improve diarization accuracy by extending the diarization methods to distributed microphone settings, it will be compatible with those systems.
Even if multi-channel inputs are given, diarization methods that heavily rely on spatial information are sometimes inoperative. The best examples are direction-of-arrival (DOA) based diarization [13, 14]. Due to COVID-19, meetings are now often held remotely or in a hybrid version of in-person and virtual gatherings. In hybrid meetings, remote attendees’ utterances are played via one loudspeaker, and DOA is no longer a clue to distinguish these speakers. To cope with this situation, spatial information needs to be properly incorporated into speaker-characteristic-based speaker diarization.
In this paper, we propose multi-channel end-to-end neural diarization (EEND) that is invariant to the number and order of chan-

nels for distributed microphone settings. We replaced Transformer encoders in the conventional EEND [4, 6] with two types of multichannel encoders. One is a spatio-temporal encoder [15, 16], in which cross-channel and cross-frame self-attentions are stacked. It was reported in the context of speech separation that the encoder performs well when the number of microphones is large but degrads signiﬁcantly when the number of microphones is small [15]. The other encoder is a co-attention encoder, in which both single- and multi-channel inputs are used and cross-frame co-attention weights are calculated from the multi-channel input. There are only crossframe attentions; thus, its performance does not heavily depend on the number of channels. We further propose to adapt multi-channel EEND only with single-channel real recordings without losing the ability to beneﬁt from spatial information given a multi-channel input during inference. We show that the proposed method can utilize spatial information and outperform the conventional EEND.
2. RELATED WORK
Some multi-channel diarization methods are fully based on DOA estimation [13, 14], but assume that different speakers are not in the same direction, thus are not appropriate for hybrid meetings. Therefore, spatial information needs to be incorporated with singlechannel-based methods as in e.g. [17]. Another possible approach is to combine channel-wise diarization results by using an ensemble method [18, 19], but it does not fully utilize spatial information. Some recent neural-network-based diarization methods utilize spatial information by aggregating multi-channel features. For example, online RSAN [20] uses inter-microphone phase difference features in addition to a single-channel magnitude spectrogram. However, the number of channels is ﬁxed due to the network architecture, making the method less ﬂexible. Moreover, phase-based features are not suited for distributed microphone settings, in which clock drift between channels exists. Multi-channel target-speaker voice activity detection (TS-VAD) [5] combines embeddings extracted from the second from the last layer of single-channel TS-VAD. Although it is ﬂexible in terms of the number of channels because an attentionbased combination is used, it requires an external diarization system that gives an initial i-vector estimation for each speaker.
If we broaden our view to speech processing other than diarization, there are several methods for neural-network-based end-to-end multi-channel speech processing that are invariant to the number of channels, e.g. speech recognition [21, 22, 23], separation [24, 25, 15, 26, 16], and dereverberation [27]. Many use attention mechanisms to work with an arbitrary number of channels. Our proposed method also uses attention-based multi-channel processing.

Cross-frame self-attention FFN
(a) Transformer encoder

Query Key Value Attention weights

Cross-channel self-attention

Cross-frame self-attention

(b) Spatio-temporal encoder

Cross-frame co-attention

Cross-frame self-attention
FFN

FFN
(c) Co-attention encoder Fig. 1: Encoder blocks. Each yellow area is skipped via residual connection.

3. CONVENTIONAL SINGLE-CHANNEL EEND

3.1. Formulation of EEND

In the EEND framework, S speakers’ speech activities are jointly
estimated. Given F -dimensional acoustic features for each T frames X ∈ RF ×T , we ﬁrst apply a linear projection parameterized by W0 ∈ RD×F and b0 ∈ RD followed by layer normalization [28]
LN to obtain D-dimensional frame-wise embeddings

E(0) = LN

W0X + b01T

∈

D×T
R

,

(1)

where 1 is the T -dimensional all-one vector. It is further converted
by N -stacked encoders, where the n-th encoder converts frame-wise embeddings E(n−1) into the same dimensional embeddings E(n):

E(n) = Encoder

E (n−1)

∈

D×T
R

.

(2)

Finally, the frame-wise posteriors of speech activities for S speakers

are estimated. In this paper, we used EEND-EDA [4, 6], with which

the speaker-wise attractor B is ﬁrst calculated using an encoder-

decoder based attractor calculation module (EDA) and then the pos-

teriors Y are estimated as

B = EDA E(N) ∈ RD×S ,

(3)

Y = σ BTE(N) ∈ (0, 1)S×T ,

(4)

where σ (·) is the element-wise sigmoid function. A permutationfree objective is used for optimization as in previous studies [3, 4, 6].

3.2. Transformer encoder

EEND-EDA uses a Transformer encoder [29] without positional en-
codings (Figure 1a) for Encoder in (2). Given Ein ∈ RD×T , the encoder converts it into Eout ∈ RD×T as follows:

E = LN (Ein + MA (Ein, Ein, Ein) ; Θ, Φ) ,

(5)

Eout = LN E + FFN E ; Ψ ,

(6)

where Θ, Φ, and Ψ are sets of parameters, and MA and FFN denote multi-head scaled dot-product attention and a feed-forward network, respectively, each of which is formulated in the following sections.

3.2.1. Multi-head scaled dot-product attention

Given dk-dimensional query Q ∈ Rdk×T , key K ∈ Rdk×T , and dv-dimensional value V ∈ Rdv×T inputs, multi-head scaled dotproduct attention MA is calculated as
V (1)A(1)T  MA (Q, K, V ; Θ, Φ) = WO  ...  + bO1T ∈ Rdv×T ,
V (h)A(h)T
(7) A(i) = softmax Q(i)TK(i) ∈ (0, 1)T ×T , (8)
dk /h

Q(i) = WQ(i)Q + b(Qi)1T ∈ R dhk ×T , (9)

K(i) = WK(i)K + b(Ki)1T ∈ R dhk ×T ,

(10)

V (i) = WV(i)V + b(Vi)1T ∈ R dhv ×T ,

(11)

where h is the number of heads, i ∈ {1, . . . , h} is the head index, and softmax (·) is the column-wise softmax function. The set of parameters Θ and Φ are deﬁned as

Θ :=

WQ(i), b(Qi), WK(i), b(Ki) ,

1≤i≤h

(12)

Φ := {WO, bO} ∪

WV(i), b(Vi) .

(13)

1≤i≤h

3.2.2. Feed-forward network

The feed-forward network FFN consists of two fully connected layers:

FFN E ; Ψ = W2 W1E + b11T + b21T , (14)
+

Ψ := {W1, b1, W2, b2} ,

(15)

where W1 ∈ Rdf ×D and W2 ∈ RD×df are projection matrices, b1 ∈ Rdf and b2 ∈ RD are biases, and [·]+ is the ramp function.

4. MULTI-CHANNEL EEND
To accept multi-channel inputs, we replaced Transformer encoders in EEND-EDA with multi-channel encoders. In this paper, we investigated two types of encoders: spatio-temporal encoder and coattention encoder.

4.1. Spatio-temporal encoder

The spatio-temporal encoder was originally proposed for speech sep-
aration on the basis of distributed microphones [15, 16]. It uses
stacked cross-channel and cross-frame self-attentions in one encoder
block, as illustrated in Figure 1b. In the encoder, frame-wise Cchannel embeddings Ein = (ein,t,c)t,c ∈ RD×T ×C , where ein,t,c ∈ RD, are ﬁrst converted to the same shape of tensor E = et,c t,c ∈ RD×T ×C using cross-channel self-attention as

et,1, . . . , et,C = LN (Ein,t + MA (Ein,t, Ein,t, Ein,t; Θ, Φ)) , (16)

Ein,t := [ein,t,1, . . . , ein,t,C ] .

(17)

The tensor E is then converted to Eout = (eout,t,c)t,c ∈ RD×T ×C by cross-frame self-attention as

[eout,1,c, . . . , eout,T,c] = LN Ec + MA Ec, Ec, Ec; Θ , Φ , (18)

Ec := e1,c, . . . , eT,c .

(19)

In the ﬁnal encoder block, cross-frame self-attention is calculated over the embeddings that are averaged across channels to form E(N) in (3), i.e., the following are used instead of (18) and (19) as
E(N) = LN E + MA E , E , E ; Θ , Φ , (20)

1C E := C Ec (21)
c=1
to calculate speech activities using (3) and (4). All calculations using (16)–(21) do not involve a speciﬁc number of channels or microphone geometry, which makes this encoder independent of the number and geometry of microphones. Note that we did not include feed-forward networks FFN in this encoder following previous studies [15, 16] because we observed performance degradation.

4.2. Co-attention encoder

The spatio-temporal encoder includes cross-channel self-attention,
the performance of which highly depends on the number of channels.
Therefore, we also propose an encoder based only on cross-frame
attention, which is characterized by the use of co-attention. The encoder accepts two inputs: frame-wise embeddings Ein ∈ RD×T and frame-channel-wise embeddings P in = [Pin,1, . . . , Pin,C | Pin,c ∈ RD ×T ]. The proposed encoder converts these inputs to Eout ∈ RD×T and P out = [Pout,1, . . . , Pout,C | Pout,c ∈ RD ×T ] as follows:

E = LN (Ein + MCA (P in, P in, Ein; ΘP , ΦE )) ,

(22)

E = LN E + MA E , E , E ; ΘE, ΦE

(23)

Eout = LN E + FFN E ; ΨE ,

(24)

Pc = LN (Pin,c + MCA (P in, P in, Pin,c; ΘP , ΦP )) , (25)

Pout,c = LN Pc + FFN Pc; ΨP ,

(26)

where ΘP , ΦE, ΘE, ΦE, ΨE, ΨP , ΦP , and ΨP are the sets of parameters. The single-channel input Ein is converted by multihead co-attention MCA in (22), multi-head attention MA in (23), and feed-forward network FFN in (24). Each channel in the multichannel input P in is ﬁrst converted by MCA in (25), the attention weights of which are shared with those in (22), then processed using FFN in (26).

Multi-head co-attention MCA is similar to MA in (7), but the attention weights are calculated using multi-channel inputs as

V (1)A(1)T  MCA (Q, K, V ; Θ, Φ) = WO  ...  + bO1T ∈ Rdv×T ,
V (h)A(h)T

(27)

 Q1(i)T, . . . , QC(i)T

(i)T

(i)T T 

K1 , . . . , KC

A(i) = softmax 

.



C D/h



(28)
Here, Q(ci) and Kc(i) for c ∈ {1, . . . , C} are calculated using (9) and (10) for each channel, and V (i) are calculated using (11). Note that the parameter sets Θ and Φ are shared among channels.

Table 1: Two-speaker conversational datasets.

Dataset

Conversation

Average Overlap Record #Mic #Session duration ratio

SRE+SWBD-train

Simulated Simulated 10

SRE+SWBD-eval

Simulated Simulated 10

SRE+SWBD-eval-hybrid Simulated Simulated 10

CSJ-train

Simulated Recorded 9

CSJ-eval

Simulated Recorded 9

CSJ-dialog

Real Recorded 9

20,000 500 500 100 100 58

88.7 s 88.1 s 88.1 s 113.5 s 102.2 s 755.2 s

34.1 % 34.6 % 34.6 % 11.0 %
9.6 % 17.3 %

Table 2: DERs on SRE+SWBD-eval and SRE+SWBD-hybrid.

SRE+SWBD-eval

SRE+SWBD-eval-hybrid

Method

1ch 2ch 4ch 6ch 10ch 1ch 2ch 4ch 6ch 10ch

1ch + posterior avg. 5.13 4.60 4.31 4.19 4.10 6.07 5.68 5.42 5.38 5.33

Spatio-temporal Spatio-temporal†

32.86 2.97 1.49 1.19 1.03 34.73 10.60 8.65 8.36 8.21 6.34 3.02 1.56 1.28 1.07 8.11 8.23 6.98 6.72 6.40

Co-attention Co-attention†

7.23 2.83 1.85 1.59 1.50 9.03 7.53 6.82 6.51 6.65 4.68 2.52 1.71 1.40 1.23 5.73 5.34 5.05 5.18 5.35

† Channel dropout was used during training.

After the ﬁnal encoder block, two outputs are concatenated as

E(N) = 1
C

Eout

C c=1

Pout,c

∈

(D+D
R

)×T

(29)

to calculate speech activities using (3) and (4).

4.3. Domain Adaptation
EEND performance can be improved by domain adaptation using real recordings. However, the number of real recordings is usually limited, and even more the case when distributed microphones are used. Therefore, it would be useful if multi-channel EEND can be adapted to the target domain only with single-channel recordings. To ensure that adaptation using only single-channel recordings does not lose the ability to beneﬁt from multi-channel recordings, we propose to update only the channel-invariant part of the model. For the spatio-temporal encoder, we freeze the parameters of cross-channel self-attention Θ and Φ in (16). For the co-attention encoder, we freeze the parameters related to multi-channel processing: ΘP in (22) and (25), ΦP in (25), and ΨP in (26).

5. EXPERIMENT
5.1. Datasets
For the experiments, we created three fully simulated two-speaker conversational datasets using NIST Speaker Recognition Evaluation (2004–2006, 2008) (SRE), Switchboard-2 (Phase I–III), and Switchboard Cellular (Part 1, 2) (SWBD). To emulate a reverberant environment, we generated room impulse responses (RIRs) using gpuRIR [30]. Following the procedure in [31], we sampled 200 rooms for each of the three room sizes: small, medium, and large. In each room, a table was randomly placed, 10 speakers were randomly placed around the table, and 10 microphones were randomly placed on the table. To create SRE+SWBD-train and SRE+SWBD-eval, two-speaker conversations were simulated following [3] then RIRs of the randomly selected room and two speaker positions were convolved to obtain a 10-channel mixture. MUSAN corpus [32] was also used to add noise to each mixture. SRE+SWBD-eval-hybrid was created using the same utterances in SRE+SWBD-eval, but two speakers were placed at the same position. This dataset was designed to mimic the part of hybrid meetings, in which multiple speakers’ utterances are played from a single loudspeaker.

Table 3: DERs on CSJ-eval and CSJ-dialog.

CSJ-eval

CSJ-dialog

Method

Adapt 1ch 2ch 4ch 6ch 9ch 1ch 2ch 4ch 6ch

1ch + posterior avg. None 11.17 9.44 8.94 8.89 8.44 28.15 26.01 25.56 24.74 1ch + posterior avg. 1ch 3.27 2.31 2.25 2.05 1.75 22.56 20.82 20.34 19.68

Spatio-temporal Spatio-temporal Spatio-temporal Spatio-temporal

None 10.98 10.20 4.29 3.27 2.63 36.13 45.19 36.48 37.14 1ch 3.44 1.60 1.34 1.07 1.13 20.06 20.02 17.83 16.19 1ch‡ 3.64 1.78 1.64 1.27 1.32 20.57 19.02 17.37 15.49 4ch 3.82 1.06 0.61 0.43 0.39 21.01 15.87 14.21 15.71

Co-attention Co-attention Co-attention Co-attention

None 9.49 3.36 1.42 1.40 0.94 27.96 22.52 19.37 18.23 1ch 2.75 1.41 0.75 0.63 0.52 23.49 22.83 20.70 17.59 1ch‡ 3.26 1.46 0.68 0.48 0.42 22.45 17.90 15.53 14.34 4ch 3.31 1.19 0.57 0.40 0.39 21.42 17.51 14.95 14.21

‡ Adapted only channel-invariant part of each model.

9ch
24.87 20.25
37.63 19.74 18.70 14.20
17.99 15.77 14.05 13.87

95$0XVDJH>*L%@











6SDWLRWHPSRUDO



&RDWWHQWLRQ

           RIFKDQQHOV

Fig. 2: VRAM usage during training with T = 500 and batch size of 64.

We also prepared three real-recorded datasets on the basis of the corpus of spontaneous Japanese (CSJ) [33]: CSJ-train, CSJ-eval, and CSJ-dialog. For CSJ-train and CSJ-eval, 100 two-speaker conversations were simulated using single-speaker recordings in the CSJ training and evaluation sets, respectively. For CSJ-dialog, we directly used the dialog portion of CSJ. To record each session, we distributed nine smartphone devices on a tabletop in a meeting room and four loudspeakers around the table. We played back two speakers’ utterances from two of the four loudspeakers that were randomly selected and recorded them on the smartphone devices. Recorded signals were roughly synchronized to maximize the correlation coefﬁcient and neither clock drift nor frame dropping was compensated.
All the experiments were based on two-speaker mixtures because our scope was investigating multi-channel diarization. Note that EEND-EDA can also be used when the number of speakers is unknown [4, 6].
5.2. Settings
As inputs to a single-channel baseline model [4, 6], 23-dimensional log-mel ﬁlterbanks were extracted for each 10 ms followed by splicing (±7 frames) and subsampling by factor of 10, resulting in 345-dimensional features for each 100 ms. For the spatio-temporal model, we extracted features from each channel in the same manner. For the co-attention model, the 345-dimensional features were averaged across channels to be used as the single-channel input. As the multi-channel input, the log-mel ﬁlterbanks of ±7 frames were averaged followed by subsampling; thus, a 23-dimensional feature was obtained for each 100 ms. We set the embedding dimensionalities as D = 256 and D = 64, i.e., 345-dimensional features were ﬁrst converted to 256 dimensional via (1) and 23-dimensional features were converted to 64 dimensional in the same manner. For each model, the four encoder blocks illustrated in Figure 1 were stacked.
Each model was ﬁrst trained on SRE+SWBD-train for 500 epochs with the Adam optimizer [34] using Noam scheduler [29] with 100,000 warm-up steps. At each iteration, four of ten channels were randomly selected and used for training. The models were then evaluated on SRE+SWBD-eval and SRE+SWBD-eval-hybrid using {1, 2, 4, 6, 10}-channel inputs. Each model was further adapted to CSJ-train for 100 epochs with the Adam optimizer with a ﬁxed learning rate of 1 × 10−5. The adapted models were evaluated on CSJ-eval and CSJ-dialog using {1, 2, 4, 6, 9}-channel inputs. To evaluate the conventional EEND-EDA [4, 6] with multi-channel inputs, we ﬁrst found the optimal speaker permutation between results from each channel by using correlation coefﬁcients of posteriors and then averaged the posteriors among channels. To prevent the models from being overly dependent on spatial information, we also introduce channel dropout, in which multi-channel inputs are randomly dropped to be a single channel. The ratio of channel dropout was

set to 0.1. Each method was evaluated using diarization error rates (DERs) with 0.25 s of collar tolerance.
5.3. Results
Table 2 shows the DERs on SRE+SWBD-eval and SRE+SWBDeval-hybrid. From the results on SRE+SWBD-eval, both spatiotemporal and co-attention models outperformed the single-channel model with posterior averaging. Comparing the two multi-channel models, the spatio-temporal model signiﬁcantly degraded DER with single-channel inputs. Channel dropout eased the situation, but the co-attention model still outperformed the spatio-temporal model when the number of channels was small. In the evaluation of SRE+SWBD-eval-hybrid, the co-attention model always achieved the same or better DERs than the single-channel model. This means that the lack of spatial information does not lead to degradation in diarization performance in the co-attention model because it does not rely on cross-channel self-attention.
Table 3 shows the DERs on CSJ-eval and CSJ-dialog. The evaluation was based on the models trained using channel dropout. Without adaptation, we can see that the co-attention model generalized well. The performance of all models improved through adaptation, regardless of whether the data used for adaptation were 1ch or 4ch. Of course, both spatio-temporal and co-attention models can beneﬁt more from 4ch adaptation; however, it is worth mentioning that they can still utilize spatial information provided by multi-channel inputs even if only 1ch recordings are used for adaptation. By freezing the parameters related to the calculation across channels during 1ch adaptation, the DERs of the co-attention model were reduced especially when four or more microphones were used, while those of the spatio-temporal model were not so improved.
Finally, we show the peak VRAM usage with T = 500 and batch size of 64 in Figure 2. VRAM usage of the co-attention model increased more slowly than the spatio-temporal model as the number of microphones increased because the multi-channel processing part is based on layers with a lower number of units. Thus, the coattention model can be trained using a larger number of channels.
6. CONCLUSION
In this paper, we proposed a multi-channel end-to-end neural diarization method based on distributed microphones. We replaced Transformer encoders in the conventional EEND with two types of multi-channel encoders. Each showed better DERs with multichannel inputs than the conventional EEND on both simulated and real-recorded datasets. We also proposed a model adaptation method using only single-channel recordings, and achieved comparable DERs as when using multi-channel recordings.

7. REFERENCES
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, and O. Vinyals, “Speaker diarization: A review of recent research,” IEEE TASLP, vol. 20, no. 2, pp. 356–370, 2012.
[2] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, “A review of speaker diarization: Recent advances with deep learning,” arXiv:2101.09624, 2021.
[3] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in INTERSPEECH, 2019, pp. 4300–4304.
[4] S. Horiguchi, Y. Fujita, S. Wananabe, Y. Xue, and K. Nagamatsu, “End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,” in INTERSPEECH, 2020, pp. 269–273.
[5] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny, A. Laptev, and A. Romanenko, “Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,” in INTERSPEECH, 2020, pp. 274–278.
[6] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and P. Garcia, “Encoder-decoder based attractor calculation for end-to-end neural diarization,” arXiv:2106.10654, 2021.
[7] F. Landini, J. Profant, M. Diez, and L. Burget, “Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: theory, implementation and analysis on standard tasks,” Computer Speech & Language, vol. 71, pp. 101254, 2022.
[8] T. J. Park, K. J. Han, M. Kumar, and S. Narayanan, “Autotuning spectral clustering for speaker diarization using normalized maximum eigengap,” IEEE Signal Processing Letters, vol. 27, pp. 381–385, 2020.
[9] S. Araki, N. Ono, K. Konoshita, and M. Delcroix, “Meeting recognition with asynchronous distributed microphone array,” in ASRU, 2017, pp. 32–39.
[10] S. Araki, N. Ono, K. Kinoshita, and M. Delcroix, “Meeting recognition with asynchronous distributed microphone array using block-wise reﬁnement of mask-based MVDR beamformer,” in ICASSP, 2018, pp. 5694–5698.
[11] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, and X. Huang, “Meeting transcription using asynchronous distant microphones,” in INTERSPEECH, 2019, pp. 2968–2972.
[12] S. Horiguchi, Y. Fujita, and K. Nagamatsu, “Utterance-wise meeting transcription system using asynchronous distributed microphones,” in INTERSPEECH, 2020, pp. 344–348.
[13] S. Araki, M. Fujimoto, K. Ishizuka, H. Sawada, and S. Makino, “A DOA based speaker diarization system for real meetings,” in HSCMA, 2008, pp. 29–32.
[14] K. Ishiguro, T. Yamada, S. Araki, T. Nakatani, and H. Sawada, “Probabilistic speaker diarization with bag-of-words representations of speaker angle information,” IEEE TASLP, vol. 20, no. 2, pp. 447–460, 2011.
[15] D. Wang, Z. Chen, and T. Yoshioka, “Neural speech separation using spatially distributed microphones,” in INTERSPEECH, 2020, pp. 339–343.

[16] D. Wang, T. Yoshioka, Z. Chen, X. Wang, T. Zhou, and Z. Meng, “Continuous speech separation with ad hoc microphone arrays,” in EUSIPCO, 2021.
[17] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE TASLP, vol. 15, no. 7, pp. 2011–2022, 2007.
[18] A. Stolcke and T. Yoshioka, “DOVER: A method for combining diarization outputs,” in ASRU, 2019, pp. 757–763.
[19] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stolcke, and S. Khudanpur, “DOVER-Lap: A method for combining overlap-aware diarization outputs,” in SLT, 2021, pp. 881–888.
[20] K. Kinoshita, M. Delcroix, S. Araki, and T. Nakatani, “Tackling real noisy reverberant meetings with all-neural source separation, counting, and diarization system,” in ICASSP, 2020, pp. 381–385.
[21] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, “Multichannel end-to-end speech recognition,” in ICML, 2017, pp. 2632–2641.
[22] X. Wang, R. Li, S. H. Mallidi, T. Hori, S. Watanabe, and H. Hermansky, “Stream attention-based multi-array end-to-end speech recognition,” in ICASSP, 2019, pp. 7105–7109.
[23] F.-J. Chang, M. Radfar, A. Mouchtaris, and M. Omologo, “Multi-channel transformer transducer for speech recognition,” in INTERSPEECH, 2021, pp. 296–300.
[24] Y. Luo, E. Ceolini, C. Han, S.-C. Liu, and N. Mesgarani, “FaSNet: Low-latency adaptive beamforming for multi-microphone audio processing,” in ASRU, 2019, pp. 260–267.
[25] Y. Luo, Z. Chen, N. Mesgarani, and T. Yoshioka, “End-to-end microphone permutation and number invariant multi-channel speech separation,” in ICASSP, 2020, pp. 6394–6398.
[26] N. Furnon, R. Serizel, I. Illina, and S. Essid, “Distributed speech separation in spatially unconstrained microphone arrays,” in ICASSP, 2021, pp. 4490–4494.
[27] Z.-Q. Wang and D. Wang, “Multi-microphone complex spectral mapping for speech dereverberation,” in ICASSP, 2020, pp. 486–490.
[28] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” in NIPS 2016 Deep Learning Symposium, 2016.
[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, 2017, pp. 5998–6008.
[30] D. Diaz-Guerra, A. Miguel, and J. R. Beltran, “gpuRIR: A python library for room impulse response simulation with GPU acceleration,” Multimedia Tools and Applications, vol. 80, no. 4, pp. 5653–5671, 2021.
[31] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in ICASSP, 2017, pp. 5220–5224.
[32] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech, and noise corpus,” arXiv:1510.08484, 2015.
[33] K. Maekawa, “Corpus of spontaneous Japanese: Its design and evaluation,” in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[34] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in ICLR, 2015.

