arXiv:2006.16336v2 [cs.CL] 4 Nov 2020

Learning Sparse Prototypes for Text Generation
Junxian He†, Taylor Berg-Kirkpatrick‡, Graham Neubig† †Carnegie Mellon University; ‡University of California San Diego {junxianh,gneubig}@cs.cmu.edu, tberg@eng.ucsd.edu
Abstract
Prototype-driven text generation uses non-parametric models that ﬁrst choose from a library of sentence “prototypes” and then modify the prototype to generate the output text. While effective, these methods are inefﬁcient at test time as a result of needing to store and index the entire training corpus. Further, existing methods often require heuristics to identify which prototypes to reference at training time. In this paper, we propose a novel generative model that automatically learns a sparse prototype support set that, nonetheless, achieves strong language modeling performance. This is achieved by (1) imposing a sparsity-inducing prior on the prototype selection distribution, and (2) utilizing amortized variational inference to learn a prototype retrieval function. In experiments, our model outperforms previous prototype-driven language models while achieving up to a 1000x memory reduction, as well as a 1000x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics and syntax at different granularity as we vary the sparsity of prototype selection, and that certain sentence attributes can be controlled by specifying the prototype for generation.1
1 Introduction
Language models (LMs) predict a probability distribution over text, and are a fundamental technology widely studied in the natural language processing (NLP) community (Bengio et al., 2003; Merity et al., 2018; Dai et al., 2019). Modern LMs are almost exclusively based on parametric recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) or self-attentional (Vaswani et al., 2017; Al-Rfou et al., 2019) neural networks. These models are of interest scientiﬁcally as one of the purest tests of our ability to capture the intricacies of human language mathematically (Linzen et al., 2016; Kuncoro et al., 2017; Petroni et al., 2019). They also have broad downstream applications in generating text in systems such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), or dialog generation (Sordoni et al., 2015), as well as in the unsupervised representation learners that now power many applications in NLP (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019).
However, there has been a recent move towards non-parametric neural LMs (Guu et al., 2018; Khandelwal et al., 2020b) that generate sentences by ﬁrst selecting examples from an external datastore. For instance, Khandelwal et al. (2020b) model the token-level probability at test time by interpolating the language model with a kNN distribution from the nearest context-token pairs in the datastore, while Guu et al. (2018) store external memories on sentence level and feature a prototype-then-edit process of (1) selecting a prototype sentence from a the prototype datastore, and (2) editing this prototype to the ﬁnal desired output. In this paper, we focus on the prototype-then-edit model family which is a lot lighter relatively in terms of memory and time cost at test time.
Intuitively, these non-parametric LMs are attractive because they help remove some of the pressure on the parametric model to memorize the entirety of the language it must model. These intuitive advantages are also borne out in superior performance on language modeling tasks (Guu et al.,
1Code is available at https://github.com/jxhe/sparse-text-prototype.
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

Prototype distribution
θ 0 1 2 3 N

Dirichlet
α hyperparam

Inference Diagram
θ < l a t e x i t s h a 1 _ b a s e 6 4 = " e l t I m + 3 e k J s Y S / U Y b P p 4 y w r r A N 4 = " > A A A 1 q n i c l V v b c t t K d t X M 5 D J R L n M m e c w L K r J r n J S s s n y O a 1 J 5 G l 2 o i 0 V J l E T d b P q 4 Q H A T h I W b 0 E 2 I F M M 8 5 w v y m v x W / i a 7 u 9 H Y u 0 H I p 6 I q S + i 1 d j f 6 s r r 3 I g g P 8 z g S 8 t 2 7 / / 3 V r 3 / z Z 3 / + F 3 / 5 2 7 9 a / + u / + d u / + 9 0 P v / / 7 G 5 F N i w C u g y z O i r u h L y C O U r i W k Y z h L i / A T 4 Y x 3 A 4 f 9 h R / W 0 I h o i z t y 3 k O X x I / T K N x F P g S o c F g m C w G M Y a P / O X X H z b e b b 3 T P 9 7 q x X Z 1 s b F W / f S + / v 6 n / x y M s m C a Q C q D 2 B f i 8 / a 7 X H 5 Z + I W M g h i W 6 4 O p g N w P H v w Q P u N l 6 i c g v i x 0 p 5 f e a 0 R G 3 j g r 8 F 8 q P Y 3 y G g s / E W K e D D E y 8 e V E N D k F t n G f p 3 L 8 r 1 8 W U Z p P J a S B u d F 4 G n s y 8 9 Q M e K O o g E D G c 7 z w g y L C v n r B x C / 8 Q O I 8 r a + / V j / e W e f W O 9 3 p H 3 n 7 n Y P j s + P + 8 f n Z l a e p 9 b a O b O J f N Q y x O U y W 2 I Z 3 6 h c P n s D 7 4 C w L L x t 7 g Z + b a z X i A s Z Q F F E a q k 6 N o j I S N m w c h d M C c E A p P A V Z k v j p a D F A M I a x X C 4 W A 0 i 8 N 1 2 8 / u f l c i U m w H W A w k b t 6 V J b X B G F k 7 q x S 1 V o i 5 J Z b m P 6 W d 4 W M c y k z B I b t K t L K 3 H V u H 0 b 5 r 8 U M b Q R w 5 c i A h s R v B Q x s h E j F Y H L c I S j i 9 U I P d / D e L X o M M a t M v J w b h K 3 D b x W 4 P L z 9 h d s Z T j 2 N r Z V I 8 1 h z 5 a L Q e I X I Q r M L x Y H x 3 f N v u C 1 E 4 J S a o b 0 z / f P 9 X 0 G E m Z S S 3 9 R A P Z e E f 9 m b u y 2 2 d F N y k m W L w a d J t t 5 R L b z d T E o y u e B i B L v E a / L f B I t 3 y j o 3 / H X b G X K O v l z o 1 a u a s k J S P / 7 9 e p q s 6 r a 6 E 1 r 5 O M z z l V r V 9 y 4 X M W 9 c P N G 5 O x 5 J X K m I p 9 X I 1 c D V 2 J G O m j U T q o 7 v W l r + v G 5 O a p m h J p Y h B v o T K O z B j r X 6 L y B J h p N m i r Q a N q M n U o l j i l 2 a b b p 2 R G v B O V O k O p 7 I 2 R k K u J t / G H s 0 9 w 1 w 1 R V F t T W E m I r j S H G e 4 e b 8 0 C f d e Y w x J M a N r 0 4 e 4 L i b Y D 5 b G t 9 g D t V n 1 Y w 3 t h e m H P x P w Z Y W u j t 0 V Y d T 4 F I + v G W d 4 B n r J C Y h 9 S R K t R B i L x p 8 c C 2 e N B s U d P y K b P 3 3 H h f 3 V V 4 N s j D 4 V W F 9 7 b G 4 9 Q f U Z W N H z d + W q m 2 W d e x V z / y p n 7 S w 7 k y y e K 7 0 4 E J x X S + y i z O f L Q 0 Y C f E 1 L 6 y t a 9 a a l / a W j p P P m V 1 8 t q q J 8 b c X e i Z q V P b C 1 P T b H B S A D S b Z O 1 t / L j a I s 0 a a / v H 1 b b 9 1 A N c B F W 5 Z c r g 0 Y z Z h r w 8 a K e d a Z 5 D 4 a l 2 T D O d q p l O W z M 7 X u E / 0 b w 3 G n v 7 9 q 1 f Z t H I m w q V 8 a O x l 2 d C R O j Q T N N 5 7 G N G q t p / u X f K p O S Y o F r G q B h T v Y r 5 f w + y a m i v b m j v F x v C M a c h a G t j Y o V p Q 8 N 1 j 1 A q l r Z N v X 3 7 o k y w d 3 4 c Z m j K J k n L O J E z v a u D v j t Q 1 t T K S H d s U z s t T V n B 2 / v h I O q 2 v n 8 Y 9 J 1 K O 7 9 Y a W V S 0 T D I a u R M f Q o 1 3 V V X 3 1 s U U 7 + p 3 l 5 d v + f W t y O t b 4 C 9 V t c v d r g S H E S x E m u s L t C u Y I C 6 q t o b x 1 l W a F p f G V 5 f V g F I 4 e e O F Z M j C 9 w I l c 8 J / H i x 3 w w o / T g a 8 Y C v 5 r p I F o Z a r j Q J Q r Z X 0 M y y H h H k Q l n H X E R x l m r b h 1 O L T W S J V / p F h E k M r L 4 x f y 2 M c U u z I s F W X w 0 Q e r W 0 0 1 k 0 a J + Y o c s M i Q l c J i B m 5 D I j Y s B l g J i x y 4 y J C V 0 m J G b i M h N i I p e J i P n m M t + I e X C Z B 2 J i l 4 m X W s Z F 4 k U C d y x + d h 3 N 1 W F n V n D T + z Y V 0 h t l 6 R + k p z 4 / o h z n 6 u R x F s Z L q r Z T t + 2 U 7 p q 5 T E Z M 7 j I 5 M Y 8 u 8 0 h M 4 T I F M c J l B D H S Z S Q x U 5 e Z E l O 6 T E n M k 8 s 8 E T N z m R k x c 5 e Z E / P s M s 9 L Y 9 D s B s D M n N X H e 1 l t k o X Z S s M x 2 z Z 1 v 7 X L Y x G V 6 6 t 5 x n F 4 S D D b G 2 V A M N s Y 5 Y h g t i t K I J h t i X J M M N s P Z U g w 2 w z l h G C 2 E 8 o p w W w b l N 8 I Z n u g f C C Y b Y A y J j h m c E J w w m A 2 0 X y G M 4 K Z m M u c Y K b k 8 p F g J u O y I J h p u B Q E C 7 6 o B M v 2 O e H S L Q l m u i 2 f C G a i L W c E M 8 W W c 4 K Z X M t n g q 1 W O z G o 5 1 D 6 I U r R o l s w o m s 9 l 8 E o r / V k B i O / 1 r M Z j A Z b T 2 c w Q m w 9 n 8 G o s f W E B i P J 1 j M a j C 5 b T 2 n k X j y n w S i 0 9 a Q G I 9 P W s x q M V p u n t e U S l 0 s 4 9 + J J D E a 6 r W c x G P 2 2 n s Z g R N x 6 H o N R c u u J D E b O r W c y G E 2 3 n s p g h N 1 6 L o N R d + v J D E b i r W c z G J 2 3 n s 5 g x N 5 6 P o N R / M s n N O 6 F I g p q h 5 L s 0 P 7 Y o W 2 T 7 B K 8 y + A 9 g v c Y v E / w P o M 7 B H c Y f E D w A Y M P C T 5 k 8 B H B R w w + J v i Y w R 8 J / s j g E 4 J P G N w l u M v g U 4 J P G X x G 8 B m D z w k + Z 3 C P 4 B 6 D L w i + Y P A l w Z c M v i L 4 i s F 9 g v s M v i b 4 m s E 3 B N 8 w + J b g W w b f E X z H 4 H u C 7 x n 8 i e B P L x + v r u j A q I 5 p d I f p V 0 u P c b u c 2 3 O 5 P c 7 t u 9 w + 5 z o u 1 + H c g c s d c O 7 Q 5 Q 4 5 d + R y R 5 w 7 d r l j z n 1 0 u Y + c O 3 G 5 E 8 5 1 X a 7 L u V O X O + X c m c u d c e 7 c 5 c 4 5 1 3 O 5 H u c u X O 6 C c 5 c u d 8 m 5 K 5 e 7 4 l z f 5 f q c u 3 a 5 a 8 7 d u N w N 5 2 5 d 7 p Z z d y 5 3 x 7 l 7 l 7 v n 3 C e X s 7 K / 4 R a i f A b 9 O Q I / u 7 6 r 6 5 Z Z C g v 7 e d Z i y d R A g 4 S S R u 2 J F e 7 6 Y f V s t C L 0 0 1 Q L V 9 E s c G g Q s i f a n C B C p k R b E k T I i p R V B 8 m A a P u B C N k O b T o Q I b O h r Q Y i Z D H K q p O s h 9 8 M Q n Z C m w l E y E R o C 4 F I z K b H I G Q Y t F 1 A J G X T a p C M T Z J B y B J o Q 4 A I G Q F t A x C h 9 K + T P y K C r Y N B K N W X 1 W q x t S o N Q m l d J 3 V E K J n r V I 4 I p X C d w B G h x K 3 T N i J t J t V 1 p 6 U f 5 x O 1 3 v p v L c x y W G m m e h B v Q P o E R g 8 s K s p 8 l 7 u o v 9 S t i S y B U O H 6 L 8 F a q U q l F s A G E c H f B I k o T F R V / Z d g q + f 6 W 4 J q I I s F 7 / 9 C i d W W U K w B l V C o I z a o h R K o L a F A x 1 R C c Y Z U Q m F O q I T d Z X 1 F Q X 6 j E o r x g c 3 N Q o m w H v l C C d C W c D L Z L K L 4 M j Y l C y U 6 W 0 L R P V I J B V e w m V o o o d U T t F A i s y W c a D b N K L C S S i i u J y q h s G Z U Q l H N q Y S C e l 5 W 3 z B j + p 0 Z X K d e 1 B m l X J 1 w E a F E q 9 M s I p R e d X J F h J K q T q m I U C r V i R Q R S q A 6 f S J C a V M n T U Q o W e p U i Q i l S J 0 g E a H E q N M i I p Q O d T J E h J K g T o G I U O r T i Q 8 R S n g 6 3 S F C a U 4 n O U Q o u e n U h g i l N J 3 Q E K F E p t M Y I p S + d P J C h J K W T l m I U K r S i Q o R S l A 6 P S F C a U k n J U Q o G e l U h A i l I J 2 A E P n E V p D S x Z B n i 6 R X Z 4 s e y x Z J 1 2 5 9 x X S r 7 V 8 P r t r D i r s y + 1 i r q A + p U O 9 d 7 E M Q + w W g q C Y 7 6 g T C O x o P K M a R e o I K a Z C N o j T E x v x p r B A x r q + T 5 U K o h 7 9 X I F 9 q Y J j F o 1 9 q Z j h b L p p f b k r s n / m m X K f T q j 3 9 8 L o a m j S 2 M x V M / X L X Y q R / u W c x 2 g F y 3 2 K 0 B 2 T H Y r Q L 5 I H F a B / I Q 4 v R T p B H F q O 9 I I 8 t R r t B f r Q Y 7 Q d 5 Y j H a E b J r M d o T 8 t R i t C v k m c V o X 8 h z i 9 H O k D 2 L 0 d 6 Q F x a j 3 S E v L U b 7 Q 1 5 Z j H a I 7 F u M 9 o i 8 t h j t E n l j M d o n 8 t Z i t F P k n c V o r 8 h 7 i 9 F u k Z 8 s Z o w a C v m w 8 P O J Y U P 7 8 T d w P o W E u w w m X Y R 7 D C Z p h P s M J n W E H Q a T Q M I D B p N G w k M G k 0 z C I w a T U s J j B p N Y w o 8 M J r 2 E J w w m y Y R d B p N q w l M G k 3 D C M w a T d s J z B p N 8 w h 6 D S U H h B Y N J R O E l g 0 l H 4 R W D S U p h n 8 G k p v C a w S S o 8 I b B p K n w l s E k q / C O w a S s 8 J 7 B J K 7 w E 4 P t B w E 8 2 i q r J u q H K 0 M m L r F L K G l L 7 B F K 0 h L 7 h G p l v f b 2 9 R c c U w G e 7 w m Q H t 4 6 h p H X 2 f S G E P g K l 5 N I e E / Z N B 4 h h C X w h P 4 6 B L 3 k t P D U i 3 J Z j A 2 p t 8 t g l q O 3 1 N / x 2 m / a O 3 R H E q 0 4 I J Q 0 K w 4 J J c m K I 0 J J s e K Y U B K s + E g o 6 V W c E E p y F V 1 C S a 3 i l F A S q z g j l L Q q z g k l q Y o e o a R U c U E o C V V c E k o 6 F V e E k k x F n 1 B S q b g m l E Q q b g g l j Y p b Q k m i 4 o 5 Q U q i 4 J 5 Q E K j 4 R W j + f S d E N g v 5 g 4 Z s n M 5 U 1 B P I F X f c j g T K N O 1 R C A e 9 S C Y W 7 R y U U 7 D 6 V U E w d K q G I D q i E 4 j m k E o r m i E o o l m M q o U g + U g n F c U I l F E W X S i i G U y q h C M 6 o h I t / T i V c 9 B 6 V c L E v q I S L f E k l X N w r K u G i 9 q m E i 3 l N J V z E G y r h 4 t 1 S C R f t j k q 4 W P d U w k X 6 x O 5 X + a / K e 6 k l A 7 5 k 0 v g w P G j U r t Z v x O L W N u i m 9 x T J S T a V H p o g 7 w k T X Q 6 F a 5 O A f J L j k a r b y 1 o D O n D F H o I 2 U d B w U a B t F D R 8 F G g j B Q 0 n B d p K Q c N L g T Z T 0 H B T o O 0 U N P w U a E M F D U c F 2 l J B w 1 O B N l X Q c F W g b R U 0 f B V o Y w U N Z w X a W k H D W 4 E 2 V 9 B w V 6 D t F T T 8 F W i D B Q 2 H B d p i Q c N j g T Z Z 0 H B Z o G 0 W N H w W a K M F D a c F 2 m p B w 2 u B N l v Q c F u g 7 R Y 0 / B Z o w w U N x w X a c k H D c 4 E 2 X d B w X a B t F z D f h Z 8 f M B H J Y g r e N B 1 B E c / V G 0 4 j X / p e C C k U m I N U O R K o 9 O F U J a T m G 5 i + e k l Q v a h Z J A t d 0 O l Q t Q p J H h U R J k K n f v 3 + 7 n C u k 6 B + Z 0 T d B L N m o 2 3 7 O s n E l / j 5 3 b 2 F E 9 n j k b 1 l W 2 e S b A T x 9 w a i A + q R m N L K f a q g 3 v e C c h n F I 6 g i B 7 p Q 9 7 6 u g c e E z I K J L 9 T L 6 / 5 U Z v p z F R R O D x s v k e c m p u 5 j V W W 1 A y N w 4 k y x J a 5 A A g 8 d G 2 e K q I V A P 1 l z g 2 M / j / 0 A l v X r N 9 0 K W H q v v e r a n d 7 G S 8 d L 7 l 9 c r i v Y G z 7 d J n u 5 5 L m 9 c W o m O Z v j B h k X S / s 0 z i U K C J f 1 8 7 U m F U g a o y p F 4 w i K Z t M i G 8 v E n 1 G k B Z p x m C w y / c a T e f S 2 2 k o e T 9 X o n 9 X z A Z c 9 6 S 7 5 6 0 4 n 3 Z U F v P E L 6 o E q N N u X + M c v c O 2 L j E V e r S z A X l Y S r Q p K o L d Z P C 7 8 R D 2 m m j x l B d p W 4 c + F 9 6 r 7 8 / t X 6 l U f / R 8 / p q l 5 Z V X k u P 5 C v 2 r 2 a g B x z G L s Y 9 L X 3 i 4 m Q N z y q f o 1 x / 0 O i X r l T X l j 0 y i L V u + b Z t N Q 5 0 x t l S M J m 7 p 5 k X m j D F R z T 9 F D l M M o 8 r c a L 1 l n R R K r J / 3 L R f f n d 8 s W M k t B c d t t n H z S 9 d 6 3 c b l i 8 h Z G a 6 H 7 8 y B K x 3 L e 3 D q 5 X 6 h H x n h s + G q z X A G e t c I P w Y t S L 8 0 q m y 9 h t u X t T T K h p i d T B j C Y e P v 4 i T i F P w h v m G U P W + v O Q 5 7 z X J 3 O W f E v q P E i 1 B 3 A v 4 N N d f W 9 Q H V O m k C 8 a m 9 S q x X D 9 O 8 X I v o o q L 5 6 H T A G O f C H u M / i 7 G l Y g P + w b r B M H Y K 5 n O O x P r A X h q n h 0 i 9 w / B M 8 / N f X v / 6 w s d 3 8 n 0 q r F z f v t 7 b f b W 1 f / L T x p 9 3 q f z H 9 d u 0 f 1 / 5 p 7 c 3 a 9 t o f 1 / 6 0 d r T W W 7 t e C 9 b y t f 9 a + + + 1 / / m w + e H y w / 2 H z y b 0 1 7 + q 6 v z D m v P z Y f R / r P T O / w = = < / l a t e x i t >

Prototype sentence
I ordered a burger with fries
Observed sentence
I ordered a pizza with sauce

Edit vector
tn zn
vMF prior

xn

N

tn
q(tn | xn)
xn

zn
q(zn | tn, xn)
N

Figure 1: Left: the proposed generative model to generate data by editing prototypes. Shaded circles denote the observed variables and unshaded denote the latents. Prototypes are sampled from a sparse prototype distribution which itself is a random variable sampled from a Dirichlet prior distribution. Right: the inference diagram of the model, with q(tn|xn) being the prototype retriever and q(zn|tn, xn) being the inverse editor.

2018; Khandelwal et al., 2020b), as well as down-stream applications such as dialogue response generation (Weston et al., 2018; Wu et al., 2019), machine translation (Gu et al., 2018; Bapna & Firat, 2019; Khandelwal et al., 2020a), and code generation (Hashimoto et al., 2018; Hayati et al., 2018). In addition, the prototypes and continuous representations of the edits in prototype-based models lend an element of interpretability to the modeling process. On the down side, however, previous prototype-driven generation methods usually need to store and index a large prototype candidate pool (in general the whole training dataset), leading to signiﬁcant issues with memory and speed efﬁciency at test time.
In this paper, we hypothesize that, in fact, a small set of prototypes is sufﬁcient to achieve the great majority of the gains afforded by such non-parametric models. Intuitively, in a large corpus many sentences look very similar and may be represented by minor transformations of a single prototype sentence. For example, the sentence “I ordered a burger with fries” can serve as the prototype for data samples with the form “I ordered [NOUN PHRASE] with [NOUN PHRASE]”. This is evidenced by Guu et al. (2018)’s observation that 70% of the test set in the Yelp restaurant review corpus (Yelp, 2017) is within word-token Jaccard distance 0.5 of one training sentence.
To take advantage of this intuition, we propose a novel generative model that samples prototypes from a latent prototype distribution, which itself is sampled from a symmetric Dirichlet prior, as shown in Figure 1 (Section 3.1). The Dirichlet prior with appropriate hyperparameters is able to encourage a sparse prototype selection distribution, allowing us to reduce the prototype support set at test time to greatly improve efﬁciency. Moreover, we utilize amortized variational inference (Kingma & Welling, 2013) to train our model, which introduces a learnable prototype retriever to identify prototypes useful for generating each sentence (Section 3.2). This is different from (Guu et al., 2018) where prototypes for each sentence are ﬁxed before training through edit distance heuristics.
We evaluate our approach on the MSCOCO (Lin et al., 2014) and Yelp restaurant review (Yelp, 2017) corpora. Our method is able to improve perplexity over the neural language model baseline by up to 14 points and previous neural editor model by 6 points while achieving over 1000x memory savings and a 1000x speedup at test time (Section 4.2). Interestingly, we ﬁnd that the learned prototypes are able to represent different features when varying sparsity levels – a strong sparsity prior forces the model to share prototypes and the induced prototypes turn out to represent more generic features (e.g. syntactic form of the sentence). On the text generation side, our model is able to generate sentences that resemble the given prototype while allowing for smooth interpolation on the edit space as well (Section 4.3).

2 Background
The prototype-then-edit framework deﬁnes a non-parametric way to augment text generation models. Formally, given a corpus X = {xn}Nn=1,2 the model generates each observed example xn by: (1) retrieving a prototype sequence tn, (2) generating a continuous edit representation zn, and (3)
2Below, we sometimes ignore the subscript to simplify notation when there is no confusion.

2

generating xn conditioned on tn and zn. These intermediate prototype and edit representation variables can depend on extra context in conditional generation tasks (Hodosh et al., 2013; Gu et al., 2018), or are randomly sampled in unconditioned language modeling. In this paper, we focus on the latter, but our methods could likely be applied to the former as well.
For unconditioned language modeling, Guu et al. (2018) deﬁne the data likelihood as:

p(X) =

p(zn)p(tn)pγ (xn|tn, zn)dzn,

(1)

n tn zn

where p(tn) is the prior distribution over prototypes and deﬁned as a uniform distribution over all training examples, p(zn) is a continuous distribution over the edit vector, and pγ (xn|tn, zn) represents a sequence-to-sequence model parameterized by γ. This model is referred to as the neural editor. Guu et al. (2018)’s stated goal of the neural editor is to take direct advantage of training examples to improve language modeling performance while capturing interpretable semantic or syntactic properties in the latent prototype and edit vector variables. However, because the prototypes are selected from the entire training dataset, such a formulation sacriﬁces memory and speed efﬁciency due to the necessity of indexing and searching every training example at test time. In the following section, we detail our approach to mitigate this issue through the learning of sparse prototypes.

3 Method

First we present our our proposed generative model, then we describe the learning and inference techniques for this model class.

3.1 Model Structure

In the previous formulation, Eq. 1, maintaining the entire training dataset at test time is necessary due to assuming a uniform prior over prototypes p(t). Motivated by the hypothesis that a much smaller prototype set would sufﬁce to achieve comparable performance, however, we believe that p(t) can be a sparse distribution where the probability mass concentrates on only a few representative prototypes. Since which training examples are representative as prototypes is unknown in advance, we propose to model the prototype distribution p(t) as a latent variable, endowing the model with freedom to infer a sparse prototype posterior automatically. We deﬁne θ ≡ p(t) and further assume that the latent prototype distribution θ is sampled from a prior distribution pα(θ) (detailed below) which is able to encourage a sparse probability distribution, given appropriate hyperparameters. The graphical model is depicted in Figure 1, which gives the following joint likelihood:

p({xn, tn, zn}Nn=1, θ; α, γ) = pα(θ) p(tn|θ)p(zn)pγ (xn|tn, zn).

(2)

n

The log marginal likelihood of the data, which we will approximate during training is:

log p(X; γ, α) = log pα(θ)

p(tn|θ)p(zn)pγ (xn|tn, zn)dzn dθ.

(3)

θ

n tn zn

This is a general framework for learning sparse prototypes that we refer to as the sparse neural editor, and in this work we speciﬁcally experiment with the following parameterization to instantiate this model class:

Prior over prototype distribution pα(θ): We employ the Dirichlet distribution as pα(θ): pα(θ) ∝ Nk=1 θkαk−1. The support of Dirichlet distribution is the standard N − 1 probability simplex. Here
we use the symmetric Dirichlet distribution which has the same α value for all components since we have no prior knowledge favoring one component over another. α is positive and also referred to as the concentration parameter, where smaller α prefers a sparser prototype distribution θ, with α = 1 equivalent to a uniform distribution over the probability simplex. In our experiments, we often choose α < 1 to encourage sparsity.

Prior over edit vector p(z): We follow Guu et al. (2018) and utilize a von-Mises Fisher (vMF) distribution to model p(z). The vMF distribution places mass on the surface of the unit sphere, and is parameterized by the mean direction vector µ and concentration parameter κ as vMF(·|µ, κ). Thus,

3

information about the edit is captured through the directions of different unit vector samples. Xu & Durrett (2018) empirically show that the vMF distribution has the advantage of overcoming posterior collapse that plagues a large amount of previous work in latent variable models of text (Bowman et al., 2016; He et al., 2019). While Guu et al. (2018) add additional randomness on the norm of edit vectors by multiplying the vMF distribution with another uniform distribution, we sample edit vectors from a uniform vMF distribution directly, which simpliﬁes the model but we nonetheless found sufﬁcient to obtain competitive results. Formally, we deﬁne p(z) = vMF(z|·, 0).
The editor pγ (x|t, z): Generally pγ (x|t, z) can be parameterized by any standard Seq2Seq model with the edit vector z incorporated. To compare with Guu et al. (2018) directly in the experiments, in this work we adopt the same attentional LSTM architecture (Hochreiter & Schmidhuber, 1997). z is utilized to predict the initial hidden state of the decoder and concatenated to the input for the decoder.

3.2 Learning and Inference

Ideally the log marginal likelihood in Eq. 3 should be optimized during training. However, computation is intractable due to marginalization of latent variables, and we resort to amortized variational inference (Kingma & Welling, 2013), optimizing its evidence lower bound (ELBO) instead:
log p(X; γ, α) ≥ LELBO(X; γ, α, φt|x, φz|t,x, λ)

= Eqφt|x (tn|xn)qφz|t,x (zn|tn,xn)[log pγ (xn|tn, zn)]

n

reconstruction log likelihood Lrec

(4)

− Eqφt|x (tn|xn)[DKL(qφz|t,x (zn|tn, xn)||p(zn))]

− Eqλ(θ)[DKL(qφt|x (tn|xn)||p(tn|θ))] − DKL(qλ(θ)||pα(θ)),

where q represents the variational distribution to approximate the model posterior distribution and admits the following factorization form:

q(θ, {tn, zn}Nn=1|X; λ, φt|x, φz|t,x) = qλ(θ) qφt|x (tn|xn)qφz|t,x (zn|tn, xn).

(5)

n

Note that we make conditional independence assumption between θ and other latent variables in q to
simplify the approximate posterior, following common practice in traditional mean ﬁeld variational inference. The inference diagram is depicted in Figure 1. The optimal qλ(θ) to maximize Eq. 4 is a Dirichlet distribution parameterized by λ ∈ RN+ (proof is in Appendix A), i.e., qλ(θ) = Dir(θ; λ).3 And qφt|x (t|x) = Cat(t; fφt|x (x)), the prototype retriever, is a categorical distribution over training examples parameterized by a neural network fφt|x (x). We assume qφz|t,x (z|t, x), the inverse neural editor, is a vMF distribution qφz|t,x (z|t, x) = vMF(gφz|t,x (t, x), κ) where the mean direction parameter is from an encoder g that encodes t and x parameterized by φz|t,x, and the scalar concentration parameter κ is a hyperparameter. Pre-ﬁxing κ results in a constant KL divergence term associated with z and proves to be effective to mitigate the posterior collapse issue (Xu & Durrett, 2018) where x and z become independent. Yet there might be still posterior collapse on t where, for
example, the prototype retriever always predicts a uniform distribution or a degenerate distribution concentrated on a certain prototype regardless of x. To overcome this issue, we follow (Li et al.,
2019) to combine annealing (Bowman et al., 2016) and free-bits techniques (Kingma et al., 2016) and apply them to the term Eqλ(θ)[DKL(qφt|x (tn|xn)||p(tn|θ))].

Notably, the variational distribution family deﬁned in Eq. 5 admits tractable closed-form expressions
of all three KL divergence terms in Eq. 4 (detailed derivations and expressions are in Appendix A). To compute the reconstruction log likelihood Lrec, expectations over z can be efﬁciently approximated by the reparameterization trick for the vMF distribution (Guu et al., 2018). However, the prototype t is discrete and non-differentiable, and summing over all prototypes t to compute Lrec is infeasible due to the evaluation burden of log pγ (x|t, z). Thus, we use the REINFORCE algorithm (Williams, 1992) to compute the gradients of φt|x contributed from Lrec as:

∂Lrec 1 L

(l)

∂ log qφt|x (t(l)|x)

∂φt|x = L l=1 Eqφz|t,x (z|t(l),x) log pγ (x|t , z) − b ∂φt|x , (6)

3qλ(θ) is not symmetric and λ is a vector.

4

where t(l) are samples from qφt|x (t|x). We use an average reward from L samples as the baseline b. The neural parameters γ, φt|x, φz|t,x are updated with stochastic gradient descent to maximize Eq. 4. With respect to the posterior Dirichlet parameter λ, we found in preliminary experiments that classic gradient descent was unable to update it effectively – λ was updated too slowly and the Dirichlet prior became decoupled with the model. Thus, we instead update λ with stochastic variational inference (SVI, Hoffman et al. (2013)) based on the formula of the optimal λ∗ given qφt|x (t|x) (derivations can be found in Appendix A):
λ∗k = α + Nn=1 qφt|x (tn = xk|xn). (7)

It is infeasibly expensive to keep λ optimal under current qφt|x (t|x) at each training step, as it would involve summing over all training examples. Thus we perform SVI, which uses a batch of examples to approximate Eq. 7, leading to the following update form:
λ(kt) = (1 − ρt)λ(kt−1) + ρt α + NB Bi=1 qφt|x (ti = xk|xi) , ρt = (t + σ)−τ , (8)
where B is the batch size, ρt is the step-size at iteration t, τ ∈ (0.5, 1] is the forgetting rate, and σ ≥ 0 is the delay parameter to down-weight early iterations.
We note that our training algorithm is different from Guu et al. (2018) in that we use a learnable prototype retriever qφt|x (t|x) to derive a lower bound as the objective while Guu et al. (2018) directly approximate marginalization over t. They use heuristics to ﬁx the prototype set for each x to be examples similar to x in terms of edit distance, which might produce suboptimal prototypes for the generative model and also does not permit the learning of sparse prototype support.

Sparsity and scalability: After training we expect to be able to infer a sparse prototype distribution θ with most components being almost zero, based on which we can prune and store the entries over a particular probability threshold only, improving memory- and time-efﬁciency at test time. Speciﬁcally, we compute mean of θ under the Dirichlet posterior: Eqλ(θ)[θk] = λk/ i λi, and then take the largest M entries that occupy 90% of the probability mass. At test time, we only maintain these M prototypes and the prototype retriever qφt|x (t|x) is re-normalized accordingly. One issue present during training is that qφt|x (t|x) cannot ﬁt into memory when dealing with large datasets since it is a categorical distribution over all training examples. In this work, we randomly downsample a subset of training data as our prototype library before training if memory is unable to ﬁt all training examples, and learn the sparse prototypes on top of this subsampled corpus. This acts like a rough pre-ﬁltering and in Section 4 we show that it sufﬁces to learn good prototypes and achieve competitive language modeling performance. We leave more advanced techniques to address this issue (e.g. dynamically updating the prototype library) as future work.

Architectures: We now describe the neural architectures we use for the prototype retriever q(t|x) and inverse neural editor q(z|t, x). q(t|x) is deﬁned as:

q(t = xk|x) ∝ exp h(xk, x)/µ 0

if xk = x if xk = x,

h(xk, x) = Embed(xk) W Embed(x), (9)

Edit Operation =
Prototype tn I Data xn I

=

$< l a t e x i t s h a 1 _ b a s e 6 4 = " C k w h K T f P n H G n D G S q 4 d w P X V d X R r M = " > A A A 1 r 3 i c l V v b c t v K l V U y m U m i z E x O J o 9 5 Q U V 2 x T M l q y y f 4 5 q p P E U X 6 m J R E i V R F / v Q x w H B T R A W b k I 3 Q U o M 5 3 n + I a + Z j 5 q / m d 3 d a O z d I O R T U Z U l 9 F q 7 G 3 1 Z 3 X s R h I d 5 H A n 5 5 s 3 / / e S n / / C z f / y n n / / i l + u / + u d / + d d f f / O b f 7 s R 2 b Q I 4 D r I 4 q y 4 G / o C 4 i i F a x n J G O 7 y A v x k G M P t 8 H 5 P 8 b c l F C L K 0 r 5 8 z O F T 4 o d p N I 4 C X y L 0 + Z t f D 2 I Y y y I K J 9 I v i m z 2 + Z u N N 1 t v 9 I + 3 e r F d X W y s V T + 9 z 7 / 5 7 n 8 G o y y Y J p D K I P a F + H 7 7 T S 4 / L f x C R k E M y / X B V E D u B / d + C N / j Z e o n I D 4 t d M + X 3 k t E R t 4 4 K / B f K j 2 N 8 h o L P x H i M R l i Z O L L i W h y C m z j v p / K 8 X 9 9 W k R p P p W Q B u Z G 4 2 n s y c x T 0 + C N o g I C G T / i h R 8 U E f b V C y Z + 4 Q c S J 2 t 9 / a X 6 8 c 4 6 t 9 7 p T v / I 2 + 8 c H J 8 d 9 4 / P z 6 4 8 T a 2 3 d W Q T / 6 p h i M 1 h s s Q 2 v F O / u P c E 3 g e n W n j Z 2 A v 8 3 F y r E R c w h q K I 0 l B 1 a h S V k b B h 4 y i c F o A D S m E W Z E n i p 6 P F A E G 1 V M v F Y g C J 9 6 q L 1 / + + X K 7 E B L g O U N i o P V 1 q i 9 N L b s M u V a E t S m a 5 j e l n e V v E M J M y S 2 z Q r i 6 t x F X j 9 m 2 Y / 1 z E 0 E Y M n 4 s I b E T w X M T I R o x U B C 7 D E Y 4 u V i P 0 f A / j 1 a L D G P f L y M O 5 S d w 2 8 F q B y + + 3 P 2 E r w 7 G 3 s a 0 a a Q 5 7 v l w M E r 8 I U W B + s T g 4 v m v 2 B a + d E J R S M 6 R / v n + u 7 z O Q M J d a + o s C s P e K + K O 5 s d t m R z c p J 1 m + G H S a b O c B 2 c 7 n x a A o n w Y i S r w H v C 7 z S b R 8 p a C / 4 K / 5 y p R 1 8 q d G r V z V k h O Q / t f r 1 d X m V b X R q 9 b I h y e c q 9 a u u H G 5 i n v m 5 o 3 I + d N K 5 F x F P q 1 G r g a u x I x 0 0 K i d V H d 6 1 d b 0 w 1 N z V M 0 I N b E I N 9 C 5 R u c N 9 F G j j w 0 0 0 W j S V I F G 0 2 b s V C p x T L F L 8 0 3 P j n g l K H e C V N 8 b I S N T E W / j D 2 O f 5 q 4 Z p q q y o L a W E F t p D D H e O 9 y c B / q s M 4 c h n t S w 6 c X Z D I r X A S a 1 r f U B 7 l R 9 W s F 4 Y 3 t h z s X / H m B p o b d H W 3 U 8 B S L p x 1 v e A Z 6 x A r O a V E e q U A c h 8 q b F A 9 v i Q b N F T c t Z Z u + 5 8 b a 6 q / B s k I f D q w p v b Y 2 H q T + i K h v f b n y 3 U m 2 z r m O v v u V N f a e H c 2 W S x V e n A x O K 6 X y V W Z z 5 a G n A T o i p f W V r X 7 X U v r S 1 d J 6 c Z X X y 2 q o n x t x d 6 J m p U 9 s z U 9 N s c F I A N J t k 7 W 1 8 u 9 o i z R p r + 9 v V t v 3 U A 1 w E V b l l y u D B j N m G P D 9 o p 5 1 p n k P h q X Z M M 5 2 q m U 5 b M z t e 4 c 9 o 3 h u N v X 7 9 2 i + z a O R N h c r 4 0 d j L M y E i t G m m 6 T z 2 M S N V 7 T / f O 2 V S c k x Q L W N U j K l e x f z d g 6 w a 2 q s b 2 v v R h n D M a Q j a 2 p h Y Y d r Q c N 0 j l I q l b V O v X z 8 r E + y d H 4 c Z m r J J 0 j J O 5 E z v 6 q C v D p Q 1 t T L S H d v U T k t T V v D 2 f j i I u q 2 v H w Z 9 p 9 L O j 1 Z a m V Q 0 D L I a O V O f Q k 1 3 1 d X X F s X U b 6 q 3 V 9 f v u f X t S O s b Y K / V 9 b M d r g Q H U a z E G q s L t C s Y o K 6 q 9 s Z x l h W a 1 l e G 1 5 d V A F L D Z L F i c m S B G 6 H y O Y E f L / a b A a U f R y M e 8 N l c F 8 n C U M u V J k H I 9 g q a W d Y j g l w o 6 5 i L K M 5 S b f t w a r G J L P F K v 4 g w i Y H V N + a v h T F u a V Y k 2 O q L A U I v l n Y 6 i w b t E z N 0 m S E x g c s E x I x c Z k Q M u A w Q M 3 a Z M T G h y 4 T E T F x m Q k z k M h E x X 1 z m C z H 3 L n N P T O w y 8 V L L u E i 8 S O C O x Q + w o 0 d 1 2 J k V 3 P S + T I X 0 R l n 6 B + m p z 4 8 o x 0 d 1 8 j g L 4 y V V 2 6 n b d k p 3 z V w m I y Z 3 m Z y Y B 5 d 5 I K Z w m Y I Y 4 T K C G O k y k p i p y 0 y J K V 2 m J G b m M j N i 5 i 4 z J + b R Z R 6 J e X K Z p 6 U x a H Y D Y G b O 6 u O 9 r D b J w m y l 4 Z h t m 7 r f 2 u W x i M r 1 1 T z j O D w k m O 2 N M i C Y b Y x y R D D b F S U Q z L Z E O S a Y 7 Y c y J J h t h n J C M N s J 5 Z R g t g 3 K L w S z P V D e E 8 w 2 Q B k T H D M 4 I T h h M J t o P s M Z w U z M Z U 4 w U 3 L 5 Q D C T c V k Q z D R c C o I F X 1 S C Z f u c c O m W B D P d l j O C m W j L O c F M s e U j w U y u 5 R P B V q u d G N R z K P 0 Q p W j R L R j R t Z 7 L Y J T X e j K D k V / r 2 Q x G g 6 2 n M x g h t p 7 P Y N T Y e k K D k W T r G Q 1 G l 6 2 n N H L P n t N g F N p 6 U o O R a e t Z D U a r z d P a c o n L J Z x 7 9 i Q G I 9 3 W s x i M f l t P Y z A i b j 2 P w S i 5 9 U Q G I + f W M x m M p l t P Z T D C b j 2 X w a i 7 9 W Q G I / H W s x m M z l t P Z z B i b z 2 f w S j + + R M a 9 0 I R B b V D S X Z o f + z Q t k l 2 C d 5 l 8 B 7 B e w z e J 3 i f w R 2 C O w w + I P i A w Y c E H z L 4 i O A j B h 8 T f M z g 9 w S / Z / A J w S c M 7 h L c Z f A p w a c M P i P 4 j M H n B J 8 z u E d w j 8 E X B F 8 w + J L g S w Z f E X z F 4 D 7 B f Q Z f E 3 z N 4 B u C b x h 8 S / A t g + 8 I v m P w B 4 I / M P g j w R + f P 1 5 d 0 Y F R H d P o D t O v l h 7 j d j m 3 5 3 J 7 n N t 3 u X 3 O d V y u w 7 k D l z v g 3 K H L H X L u y O W O O H f s c s e c e + 9 y 7 z l 3 4 n I n n O u 6 X J d z p y 5 3 y r k z l z v j 3 L n L n X O u 5 3 I 9 z l 2 4 3 A X n L l 3 u k n N X L n f F u b 7 L 9 T l 3 7 X L X n L t x u R v O 3 b r c L e f u X O 6 O c x 9 c 7 g P n P r q c l f 0 N t x D l E + j P E f j Z 9 U 1 d t 8 x S W N j P s x Z L p g Y a J J Q 0 a k + s c N c P q 2 e j F a G f p l q 4 i m a B Q 4 O Q P d H m B B E y J d q S I E J W p K w 6 S A Z E 2 w 9 E y H Z o 0 4 E I m Q 1 t N R A h i 1 F W n W Q 9 / G I Q s h P a T C B C J k J b C E R i N j 0 G I c O g 7 Q I i K Z t W g 2 R s k g x C l k A b A k T I C G g b g A i l f 5 3 8 E R F s H Q x C q b 6 s V o u t V W k Q S u s 6 q S N C y V y n c k Q o h e s E j g g l b p 2 2 E W k z q a 4 7 L f 0 4 n 6 j 1 1 n 9 r Y Z b D S j P V g 3 g D 0 i c w e m B R U b G f D E e q h r k g I k s g V L j + S 7 B W q l K p B b B B R P A 3 Q S I K E 1 V V / y X Y 6 r n + l q A a y G L B + 7 9 Q Y r U l F G t A J R T q i A 1 q o Q R q S y j Q M Z V Q n C G V U J g T K m F 3 W V 9 R k F + o h G K 8 Z 3 O z U C K s R 7 5 Q A r Q l n E w 2 i y i + j E 3 J Q o n O l l B 0 D 1 R C w R V s p h Z K a P U E L Z T I b A k n m k 0 z C q y k E o p r R i U U 1 p x K K K p H K q G g n p b V N 8 y Y f u c G 1 6 k X d U Y p V y d c R C j R 6 j S L C K V X n V w R o a S q U y o i l E p 1 I k W E E q h O n 4 h Q 2 t R J E x F K l j p V I k I p U i d I R C g x 6 r S I C K V D n Q w R o S S o U y A i l P p 0 4 k O E E p 5 O d 4 h Q m t N J D h F K b j q 1 I U I p T S c 0 R C i R 6 T S G C K U v n b w Q o a S l U x Y i l K p 0 o k K E E p R O T 4 h Q W t J J C R F K R j o V I U I p S C c g R D 6 y F a R 0 M e T Z I u n V 2 a L H s k X S t V t f M d 1 q + 9 e D q / a w 4 q 7 M P t Y q 6 k M q 1 H s X + x D E f g E o q s m O O o H w j s Y D i n G k n q B C G m S j K A 2 x M X 8 a K 0 S M 6 + t k u R D q 4 e 8 V y O c a G G b x 6 M e a G c 6 X i + a X m x L 7 Z 7 4 p 1 + m 0 a k 8 / v K 6 G J o 3 t T A V T v 9 y 1 G O l f 7 l m M d o D c t x j t A d m x G O 0 C e W A x 2 g f y 0 G K 0 E + S R x W g v y G O L 0 W 6 Q 7 y 1 G + 0 G e W I x 2 h O x a j P a E P L U Y 7 Q p 5 Z j H a F / L c Y r Q z Z M 9 i t D f k h c V o d 8 h L i 9 H + k F c W o x 0 i + x a j P S K v L U a 7 R N 5 Y j P a J v L U Y 7 R R 5 Z z H a K / K D x W i 3 y I 8 W M 0 Y N h X x Y + P n E s K H 9 + B s 4 n 0 L C X Q a T L s I 9 B p M 0 w n 0 G k z r C D o N J I O E B g 0 k j 4 S G D S S b h E Y N J K e E x g 0 k s 4 X s G k 1 7 C E w a T Z M I u g 0 k 1 4 S m D S T j h G Y N J O + E 5 g 0 k + Y Y / B p K D w g s E k o v C S w a S j 8 I r B J K W w z 2 B S U 3 j N Y B J U e M N g 0 l R 4 y 2 C S V X j H Y F J W + I H B J K 7 w I 4 P t B w E 8 2 i q r J u q H K 0 M m L r F L K G l L 7 B F K 0 h L 7 h G p l v f T 2 9 R c c U w G e 7 w m Q H t 4 6 h p H X 2 f S G E P g K l 5 N I e L N s G o 8 Q w h J 4 Q n 8 d g l 5 y W n j q R b k s x o b U 2 2 U w z 9 F b 6 u 9 4 7 T f t H b o j i V Y c E E q a F Y e E k m T F E a G k W H F M K A l W v C e U 9 C p O C C W 5 i i 6 h p F Z x S i i J V Z w R S l o V 5 4 S S V E W P U F K q u C C U h C o u C S W d i i t C S a a i T y i p V F w T S i I V N 4 S S R s U t o S R R c U c o K V R 8 I J Q E K j 4 S W j + f S d E N g v 5 g 4 Z s n M 5 U 1 B P I F X f c j g T K N O 1 R C A e 9 S C Y W 7 R y U U 7 D 6 V U E w d K q G I D q i E 4 j m k E o r m i E o o l m M q o U j e U w n F c U I l F E W X S i i G U y q h C M 6 o h I t / T i V c 9 B 6 V c L E v q I S L f E k l X N w r K u G i 9 q m E i 3 l N J V z E G y r h 4 t 1 S C R f t j k q 4 W B + o h I v 0 k d 2 v 8 l + V 9 1 J L B n z J p P F h e N C o X a 3 f i M W t b d B N b x b J S T a V H p o g b 4 a J L o f C t U l A P s n x S N X t Z a 0 B H b h i D 0 G b K G i 4 K N A 2 C h o + C r S R g o a T A m 2 l o O G l Q J s p a L g p 0 H Y K G n 4 K t K G C h q M C b a m g 4 a l A m y p o u C r Q t g o a v g q 0 s Y K G s w J t r a D h r U C b K 2 i 4 K 9 D 2 C h r + C r T B g o b D A m 2 x o O G x Q J s s a L g s 0 D Y L G j 4 L t N G C h t M C b b W g 4 b V A m y 1 o u C 3 Q d g s a f g u 0 4 Y K G 4 w J t u a D h u U C b L m i 4 L t C 2 C 5 j v w s 8 P m I h k M Q V v m o 6 g i B / V G 0 4 j X / p e C C k U m I N U O R K o 9 O F U J a T m G 5 i + e k l Q v a h Z J A t d 0 O l Q t Q p J H h U R J k K n f v 3 + 7 v B R J 0 H 9 z o i 6 C W b N R t v 2 d Z K J L / H z u 3 s L J 7 L H I 3 v L t s 4 k 2 Q j i r w 1 E B 9 Q j M a W V + 1 R B v a 8 F 5 T K K R 1 B F D n S h 7 n 1 d A 4 8 J m Q U T X 6 i X 1 / 2 p z P T n K i i c H j Z e I s 9 N T N 3 H q s p q B 0 b g x J l i S 1 y B B B 4 6 N s 4 U U Q u B f r L m B s d + H v s B L O v X b 7 o V s P R e e t W 1 O 7 2 N l 4 6 X 3 L + 4 X F e w N 3 y 6 T f Z y y X N 7 4 9 R M c j b H D T I u l v Z p n E s U E C 7 r 5 2 t N K p A 0 R l W K x h E U z a Z F N p a J P 6 d I C z T j M F l k + o 0 n 8 + h t t Z U 8 n q r R P 6 n n A y 5 7 0 l 3 y 1 5 1 O u i s L e O M X 1 A N V a L Y v 8 Y 9 f 4 N o X G Y u 8 W l m A v a w k W h W U Q G + z e F z 4 i X p M N Z l l B d p W 4 T 8 K 7 0 X 3 h 7 c v 1 K s + + j 9 + T F P z y q r I c f 2 F f t X s x Q D i m M X Y x 6 Q v v V 1 M g L j l U / X r E f c 7 J O q V N + W N T a M s W r 1 v m k 1 D n T O 1 V Y 4 k b O r m R e a N M l D N z a L 7 K I d R 5 G 8 1 X r L O i i R W T / q X i + 4 P b 5 Y t Z J a C 4 r b b O D n T 9 d 6 2 c b l i 8 h Z G a 6 H 7 w y B K x / K x u X V y v 1 C P j P H Y 8 N V m u Q I 8 a 4 U f g h e l X p p V N l / C f M v b m 2 R C T U + m D G A w 8 f b x E 3 E K f x D e M M v u t 9 a d h z z n u T q d s + I / U O N F q D u A f w e b 6 u p r g e q c N I F 4 1 d 6 k V i u G 6 d / P R P R R U H 3 1 O m A M c u A P c Z / F 2 W x Y g H + / b r B M H Y K 5 f M R j f W A v D F P D p V / g + C d 4 + K + v f / 5 m Y 7 v 5 P 5 V W L 2 7 e b m 2 / 2 d q + + G 7 j T 7 v V / 2 L 6 x d r v 1 n 6 / 9 m p t e + 0 / 1 / 6 0 d r T W W 7 t e C 9 a m a 3 9 d + 9 v a / 7 7 b f n f 7 7 o d 3 f z a h P / 1 J V e e 3 a 8 7 P u + j / A Q x z 0 L 4 = < / l a t e x i t >

$< l a t e x i t s h a 1 _ b a s e 6 4 = " C k w h K T f P n H G n D G S q 4 d w P X V d X R r M = " > A A A 1 r 3 i c l V v b c t v K l V U y m U m i z E x O J o 9 5 Q U V 2 x T M l q y y f 4 5 q p P E U X 6 m J R E i V R F / v Q x w H B T R A W b k I 3 Q U o M 5 3 n + I a + Z j 5 q / m d 3 d a O z d I O R T U Z U l 9 F q 7 G 3 1 Z 3 X s R h I d 5 H A n 5 5 s 3 / / e S n / / C z f / y n n / / i l + u / + u d / + d d f f / O b f 7 s R 2 b Q I 4 D r I 4 q y 4 G / o C 4 i i F a x n J G O 7 y A v x k G M P t 8 H 5 P 8 b c l F C L K 0 r 5 8 z O F T 4 o d p N I 4 C X y L 0 + Z t f D 2 I Y y y I K J 9 I v i m z 2 + Z u N N 1 t v 9 I + 3 e r F d X W y s V T + 9 z 7 / 5 7 n 8 G o y y Y J p D K I P a F + H 7 7 T S 4 / L f x C R k E M y / X B V E D u B / d + C N / j Z e o n I D 4 t d M + X 3 k t E R t 4 4 K / B f K j 2 N 8 h o L P x H i M R l i Z O L L i W h y C m z j v p / K 8 X 9 9 W k R p P p W Q B u Z G 4 2 n s y c x T 0 + C N o g I C G T / i h R 8 U E f b V C y Z + 4 Q c S J 2 t 9 / a X 6 8 c 4 6 t 9 7 p T v / I 2 + 8 c H J 8 d 9 4 / P z 6 4 8 T a 2 3 d W Q T / 6 p h i M 1 h s s Q 2 v F O / u P c E 3 g e n W n j Z 2 A v 8 3 F y r E R c w h q K I 0 l B 1 a h S V k b B h 4 y i c F o A D S m E W Z E n i p 6 P F A E G 1 V M v F Y g C J 9 6 q L 1 / + + X K 7 E B L g O U N i o P V 1 q i 9 N L b s M u V a E t S m a 5 j e l n e V v E M J M y S 2 z Q r i 6 t x F X j 9 m 2 Y / 1 z E 0 E Y M n 4 s I b E T w X M T I R o x U B C 7 D E Y 4 u V i P 0 f A / j 1 a L D G P f L y M O 5 S d w 2 8 F q B y + + 3 P 2 E r w 7 G 3 s a 0 a a Q 5 7 v l w M E r 8 I U W B + s T g 4 v m v 2 B a + d E J R S M 6 R / v n + u 7 z O Q M J d a + o s C s P e K + K O 5 s d t m R z c p J 1 m + G H S a b O c B 2 c 7 n x a A o n w Y i S r w H v C 7 z S b R 8 p a C / 4 K / 5 y p R 1 8 q d G r V z V k h O Q / t f r 1 d X m V b X R q 9 b I h y e c q 9 a u u H G 5 i n v m 5 o 3 I + d N K 5 F x F P q 1 G r g a u x I x 0 0 K i d V H d 6 1 d b 0 w 1 N z V M 0 I N b E I N 9 C 5 R u c N 9 F G j j w 0 0 0 W j S V I F G 0 2 b s V C p x T L F L 8 0 3 P j n g l K H e C V N 8 b I S N T E W / j D 2 O f 5 q 4 Z p q q y o L a W E F t p D D H e O 9 y c B / q s M 4 c h n t S w 6 c X Z D I r X A S a 1 r f U B 7 l R 9 W s F 4 Y 3 t h z s X / H m B p o b d H W 3 U 8 B S L p x 1 v e A Z 6 x A r O a V E e q U A c h 8 q b F A 9 v i Q b N F T c t Z Z u + 5 8 b a 6 q / B s k I f D q w p v b Y 2 H q T + i K h v f b n y 3 U m 2 z r m O v v u V N f a e H c 2 W S x V e n A x O K 6 X y V W Z z 5 a G n A T o i p f W V r X 7 X U v r S 1 d J 6 c Z X X y 2 q o n x t x d 6 J m p U 9 s z U 9 N s c F I A N J t k 7 W 1 8 u 9 o i z R p r + 9 v V t v 3 U A 1 w E V b l l y u D B j N m G P D 9 o p 5 1 p n k P h q X Z M M 5 2 q m U 5 b M z t e 4 c 9 o 3 h u N v X 7 9 2 i + z a O R N h c r 4 0 d j L M y E i t G m m 6 T z 2 M S N V 7 T / f O 2 V S c k x Q L W N U j K l e x f z d g 6 w a 2 q s b 2 v v R h n D M a Q j a 2 p h Y Y d r Q c N 0 j l I q l b V O v X z 8 r E + y d H 4 c Z m r J J 0 j J O 5 E z v 6 q C v D p Q 1 t T L S H d v U T k t T V v D 2 f j i I u q 2 v H w Z 9 p 9 L O j 1 Z a m V Q 0 D L I a O V O f Q k 1 3 1 d X X F s X U b 6 q 3 V 9 f v u f X t S O s b Y K / V 9 b M d r g Q H U a z E G q s L t C s Y o K 6 q 9 s Z x l h W a 1 l e G 1 5 d V A F L D Z L F i c m S B G 6 H y O Y E f L / a b A a U f R y M e 8 N l c F 8 n C U M u V J k H I 9 g q a W d Y j g l w o 6 5 i L K M 5 S b f t w a r G J L P F K v 4 g w i Y H V N + a v h T F u a V Y k 2 O q L A U I v l n Y 6 i w b t E z N 0 m S E x g c s E x I x c Z k Q M u A w Q M 3 a Z M T G h y 4 T E T F x m Q k z k M h E x X 1 z m C z H 3 L n N P T O w y 8 V L L u E i 8 S O C O x Q + w o 0 d 1 2 J k V 3 P S + T I X 0 R l n 6 B + m p z 4 8 o x 0 d 1 8 j g L 4 y V V 2 6 n b d k p 3 z V w m I y Z 3 m Z y Y B 5 d 5 I K Z w m Y I Y 4 T K C G O k y k p i p y 0 y J K V 2 m J G b m M j N i 5 i 4 z J + b R Z R 6 J e X K Z p 6 U x a H Y D Y G b O 6 u O 9 r D b J w m y l 4 Z h t m 7 r f 2 u W x i M r 1 1 T z j O D w k m O 2 N M i C Y b Y x y R D D b F S U Q z L Z E O S a Y 7 Y c y J J h t h n J C M N s J 5 Z R g t g 3 K L w S z P V D e E 8 w 2 Q B k T H D M 4 I T h h M J t o P s M Z w U z M Z U 4 w U 3 L 5 Q D C T c V k Q z D R c C o I F X 1 S C Z f u c c O m W B D P d l j O C m W j L O c F M s e U j w U y u 5 R P B V q u d G N R z K P 0 Q p W j R L R j R t Z 7 L Y J T X e j K D k V / r 2 Q x G g 6 2 n M x g h t p 7 P Y N T Y e k K D k W T r G Q 1 G l 6 2 n N H L P n t N g F N p 6 U o O R a e t Z D U a r z d P a c o n L J Z x 7 9 i Q G I 9 3 W s x i M f l t P Y z A i b j 2 P w S i 5 9 U Q G I + f W M x m M p l t P Z T D C b j 2 X w a i 7 9 W Q G I / H W s x m M z l t P Z z B i b z 2 f w S j + + R M a 9 0 I R B b V D S X Z o f + z Q t k l 2 C d 5 l 8 B 7 B e w z e J 3 i f w R 2 C O w w + I P i A w Y c E H z L 4 i O A j B h 8 T f M z g 9 w S / Z / A J w S c M 7 h L c Z f A p w a c M P i P 4 j M H n B J 8 z u E d w j 8 E X B F 8 w + J L g S w Z f E X z F 4 D 7 B f Q Z f E 3 z N 4 B u C b x h 8 S / A t g + 8 I v m P w B 4 I / M P g j w R + f P 1 5 d 0 Y F R H d P o D t O v l h 7 j d j m 3 5 3 J 7 n N t 3 u X 3 O d V y u w 7 k D l z v g 3 K H L H X L u y O W O O H f s c s e c e + 9 y 7 z l 3 4 n I n n O u 6 X J d z p y 5 3 y r k z l z v j 3 L n L n X O u 5 3 I 9 z l 2 4 3 A X n L l 3 u k n N X L n f F u b 7 L 9 T l 3 7 X L X n L t x u R v O 3 b r c L e f u X O 6 O c x 9 c 7 g P n P r q c l f 0 N t x D l E + j P E f j Z 9 U 1 d t 8 x S W N j P s x Z L p g Y a J J Q 0 a k + s c N c P q 2 e j F a G f p l q 4 i m a B Q 4 O Q P d H m B B E y J d q S I E J W p K w 6 S A Z E 2 w 9 E y H Z o 0 4 E I m Q 1 t N R A h i 1 F W n W Q 9 / G I Q s h P a T C B C J k J b C E R i N j 0 G I c O g 7 Q I i K Z t W g 2 R s k g x C l k A b A k T I C G g b g A i l f 5 3 8 E R F s H Q x C q b 6 s V o u t V W k Q S u s 6 q S N C y V y n c k Q o h e s E j g g l b p 2 2 E W k z q a 4 7 L f 0 4 n 6 j 1 1 n 9 r Y Z b D S j P V g 3 g D 0 i c w e m B R U b G f D E e q h r k g I k s g V L j + S 7 B W q l K p B b B B R P A 3 Q S I K E 1 V V / y X Y 6 r n + l q A a y G L B + 7 9 Q Y r U l F G t A J R T q i A 1 q o Q R q S y j Q M Z V Q n C G V U J g T K m F 3 W V 9 R k F + o h G K 8 Z 3 O z U C K s R 7 5 Q A r Q l n E w 2 i y i + j E 3 J Q o n O l l B 0 D 1 R C w R V s p h Z K a P U E L Z T I b A k n m k 0 z C q y k E o p r R i U U 1 p x K K K p H K q G g n p b V N 8 y Y f u c G 1 6 k X d U Y p V y d c R C j R 6 j S L C K V X n V w R o a S q U y o i l E p 1 I k W E E q h O n 4 h Q 2 t R J E x F K l j p V I k I p U i d I R C g x 6 r S I C K V D n Q w R o S S o U y A i l P p 0 4 k O E E p 5 O d 4 h Q m t N J D h F K b j q 1 I U I p T S c 0 R C i R 6 T S G C K U v n b w Q o a S l U x Y i l K p 0 o k K E E p R O T 4 h Q W t J J C R F K R j o V I U I p S C c g R D 6 y F a R 0 M e T Z I u n V 2 a L H s k X S t V t f M d 1 q + 9 e D q / a w 4 q 7 M P t Y q 6 k M q 1 H s X + x D E f g E o q s m O O o H w j s Y D i n G k n q B C G m S j K A 2 x M X 8 a K 0 S M 6 + t k u R D q 4 e 8 V y O c a G G b x 6 M e a G c 6 X i + a X m x L 7 Z 7 4 p 1 + m 0 a k 8 / v K 6 G J o 3 t T A V T v 9 y 1 G O l f 7 l m M d o D c t x j t A d m x G O 0 C e W A x 2 g f y 0 G K 0 E + S R x W g v y G O L 0 W 6 Q 7 y 1 G + 0 G e W I x 2 h O x a j P a E P L U Y 7 Q p 5 Z j H a F / L c Y r Q z Z M 9 i t D f k h c V o d 8 h L i 9 H + k F c W o x 0 i + x a j P S K v L U a 7 R N 5 Y j P a J v L U Y 7 R R 5 Z z H a K / K D x W i 3 y I 8 W M 0 Y N h X x Y + P n E s K H 9 + B s 4 n 0 L C X Q a T L s I 9 B p M 0 w n 0 G k z r C D o N J I O E B g 0 k j 4 S G D S S b h E Y N J K e E x g 0 k s 4 X s G k 1 7 C E w a T Z M I u g 0 k 1 4 S m D S T j h G Y N J O + E 5 g 0 k + Y Y / B p K D w g s E k o v C S w a S j 8 I r B J K W w z 2 B S U 3 j N Y B J U e M N g 0 l R 4 y 2 C S V X j H Y F J W + I H B J K 7 w I 4 P t B w E 8 2 i q r J u q H K 0 M m L r F L K G l L 7 B F K 0 h L 7 h G p l v f T 2 9 R c c U w G e 7 w m Q H t 4 6 h p H X 2 f S G E P g K l 5 N I e L N s G o 8 Q w h J 4 Q n 8 d g l 5 y W n j q R b k s x o b U 2 2 U w z 9 F b 6 u 9 4 7 T f t H b o j i V Y c E E q a F Y e E k m T F E a G k W H F M K A l W v C e U 9 C p O C C W 5 i i 6 h p F Z x S i i J V Z w R S l o V 5 4 S S V E W P U F K q u C C U h C o u C S W d i i t C S a a i T y i p V F w T S i I V N 4 S S R s U t o S R R c U c o K V R 8 I J Q E K j 4 S W j + f S d E N g v 5 g 4 Z s n M 5 U 1 B P I F X f c j g T K N O 1 R C A e 9 S C Y W 7 R y U U 7 D 6 V U E w d K q G I D q i E 4 j m k E o r m i E o o l m M q o U j e U w n F c U I l F E W X S i i G U y q h C M 6 o h I t / T i V c 9 B 6 V c L E v q I S L f E k l X N w r K u G i 9 q m E i 3 l N J V z E G y r h 4 t 1 S C R f t j k q 4 W B + o h I v 0 k d 2 v 8 l + V 9 1 J L B n z J p P F h e N C o X a 3 f i M W t b d B N b x b J S T a V H p o g b 4 a J L o f C t U l A P s n x S N X t Z a 0 B H b h i D 0 G b K G i 4 K N A 2 C h o + C r S R g o a T A m 2 l o O G l Q J s p a L g p 0 H Y K G n 4 K t K G C h q M C b a m g 4 a l A m y p o u C r Q t g o a v g q 0 s Y K G s w J t r a D h r U C b K 2 i 4 K 9 D 2 C h r + C r T B g o b D A m 2 x o O G x Q J s s a L g s 0 D Y L G j 4 L t N G C h t M C b b W g 4 b V A m y 1 o u C 3 Q d g s a f g u 0 4 Y K G 4 w J t u a D h u U C b L m i 4 L t C 2 C 5 j v w s 8 P m I h k M Q V v m o 6 g i B / V G 0 4 j X / p e C C k U m I N U O R K o 9 O F U J a T m G 5 i + e k l Q v a h Z J A t d 0 O l Q t Q p J H h U R J k K n f v 3 + 7 v B R J 0 H 9 z o i 6 C W b N R t v 2 d Z K J L / H z u 3 s L J 7 L H I 3 v L t s 4 k 2 Q j i r w 1 E B 9 Q j M a W V + 1 R B v a 8 F 5 T K K R 1 B F D n S h 7 n 1 d A 4 8 J m Q U T X 6 i X 1 / 2 p z P T n K i i c H j Z e I s 9 N T N 3 H q s p q B 0 b g x J l i S 1 y B B B 4 6 N s 4 U U Q u B f r L m B s d + H v s B L O v X b 7 o V s P R e e t W 1 O 7 2 N l 4 6 X 3 L + 4 X F e w N 3 y 6 T f Z y y X N 7 4 9 R M c j b H D T I u l v Z p n E s U E C 7 r 5 2 t N K p A 0 R l W K x h E U z a Z F N p a J P 6 d I C z T j M F l k + o 0 n 8 + h t t Z U 8 n q r R P 6 n n A y 5 7 0 l 3 y 1 5 1 O u i s L e O M X 1 A N V a L Y v 8 Y 9 f 4 N o X G Y u 8 W l m A v a w k W h W U Q G + z e F z 4 i X p M N Z l l B d p W 4 T 8 K 7 0 X 3 h 7 c v 1 K s + + j 9 + T F P z y q r I c f 2 F f t X s x Q D i m M X Y x 6 Q v v V 1 M g L j l U / X r E f c 7 J O q V N + W N T a M s W r 1 v m k 1 D n T O 1 V Y 4 k b O r m R e a N M l D N z a L 7 K I d R 5 G 8 1 X r L O i i R W T / q X i + 4 P b 5 Y t Z J a C 4 r b b O D n T 9 d 6 2 c b l i 8 h Z G a 6 H 7 w y B K x / K x u X V y v 1 C P j P H Y 8 N V m u Q I 8 a 4 U f g h e l X p p V N l / C f M v b m 2 R C T U + m D G A w 8 f b x E 3 E K f x D e M M v u t 9 a d h z z n u T q d s + I / U O N F q D u A f w e b 6 u p r g e q c N I F 4 1 d 6 k V i u G 6 d / P R P R R U H 3 1 O m A M c u A P c Z / F 2 W x Y g H + / b r B M H Y K 5 f M R j f W A v D F P D p V / g + C d 4 + K + v f / 5 m Y 7 v 5 P 5 V W L 2 7 e b m 2 / 2 d q + + G 7 j T 7 v V / 2 L 6 x d r v 1 n 6 / 9 m p t e + 0 / 1 / 6 0 d r T W W 7 t e C 9 a m a 3 9 d + 9 v a / 7 7 b f n f 7 7 o d 3 f z a h P / 1 J V e e 3 a 8 7 P u + j / A Q x z 0 L 4 = < / l a t e x i t >

—

ordered sweet potato fries

ordered a

burger

?< l a t e x i t s h a 1 _ b a s e 6 4 = " 8 V a w + N t j n Y 8 D D N 1 I d w + Y V 9 A m d X w = " > A A A 1 p 3 i c l V t Z c 9 t K d t Z M t h l N l j v J 4 7 y g I r v G k 5 J V l u 9 1 z V S e R g u 1 W J R E S d T m S 1 8 X C B 6 C s L A J 3 Y R I c Z j n v O c 1 + W H 5 N z n d j c Y 5 D U K + F V X Z Q n / f 6 U Y v X / f 5 C E L D P I 6 E f P f u f 3 / x y 7 / 6 6 7 / 5 2 7 / 7 1 a / X f / P 3 / / C P / / T d b / / 5 R m T T I o D r I I u z 4 m 7 o C 4 i j F K 5 l J G O 4 y w v w k 2 E M t 8 O H P c X f l l C I K E v 7 c p 7 D 5 8 Q P 0 2 g c B b 5 E 6 H 4 A S S 7 n A u S X 7 z b e b b 3 T P 9 7 q x X Z 1 s b F W / f S + / P a H / x y M s m C a Q C q D 2 B f i x + 1 3 u f y 8 8 A s Z B T E s 1 w d T A b k f P P g h / I i X q Z + A + L z Q P V 5 6 r x E Z e e O s w H + p 9 D T K a y z 8 R I h 5 M s T I x J c T 0 e Q U 2 M b 9 O J X j P 3 1 e R G k + l Z A G 5 k b j a e z J z F P D 9 0 Z R A Y G M 5 3 j h B 0 W E f f W C i V / 4 g c R J W l 9 / r X 6 8 s 8 6 t d 7 r T P / L 2 O w f H Z 8 f 9 4 / O z K 0 9 T 6 2 0 d 2 c T f a h h i c 5 g s s Q 3 v 1 C 8 e P I H 3 w S k W X j b 2 A j 8 3 1 2 r E B Y y h K K I 0 V J 0 a R W U k b N g 4 C q c F 4 I B S e A q y J P H T 0 W K A Y A x j u V w s c L G 8 N 1 2 8 / s N y u R I T 4 D p A Y a P 2 d K k t r o j C S d 3 Y p S q 0 R c k s t z H 9 L G + L G G Z S Z o k N 2 t W l l b h q 3 L 4 N 8 1 + K G N q I 4 U s R g Y 0 I X o o Y 2 Y i R i s B l O M L R x W q E n u 9 h v F p 0 G O M + G X k 4 N 4 n b B l 4 r c P n j 9 m d s Z T j 2 N r Z V I 8 1 h z 5 a L Q e I X I Q r M L x Y H x 3 f N v u C 1 E 4 J S a o b 0 z / f P 9 X 0 G E m Z S S 3 9 R A P Z e E f 9 u b u y 2 2 d F N y k m W L w a d J t t 5 R L b z Z T E o y u e B i B L v E a / L f B I t 3 y j o L / j f b G X K O v l z o 1 a u a s k J S P / b 9 e p q s 6 r a 6 E 1 r 5 O M z z l V r V 9 y 4 X M W 9 c P N G 5 O x 5 J X K m I p 9 X I 1 c D V 2 J G O m j U T q o 7 v W l r + v G 5 O a p m h J p Y h B v o T K O z B j r X 6 L y B J h p N m i r Q a N q M n U o l j i l 2 a b b p 2 R G v B O V O k O p 7 I 2 R k K u J t / G H s 0 9 w 1 w 1 R V F t T W E m I r j S H G e 4 e b 8 0 C f d e Y w x J M a N r 0 4 e 4 L i b Y D J b G t 9 g D t V n 1 Y w 3 t h e m H P x P w Z Y W u j t 0 V Y d T 4 F I + v G W d 4 B n r J C Y h 9 S R K t R B i L x p 8 c C 2 e N B s U d P y K b P 3 3 H h f 3 V V 4 N s j D 4 V W F 9 7 b G 4 9 Q f U Z W N 7 z d + W K m 2 W d e x V 9 / z p n 7 Q w 7 k y y e K b 0 4 E J x X S + y i z O f L Q 0 Y C f E 1 L 6 y t a 9 a a l / a W j p P P m V 1 8 t q q J 8 b c X e i Z q V P b C 1 P T b H B S A D S b Z O 1 t f L / a I s 0 a a / v 7 1 b b 9 1 A N c B F W 5 Z c r g 0 Y z Z h r w 8 a K e d a Z 5 D 4 a l 2 T D O d q p l O W z M 7 X u E / 0 b w 3 G n v 7 9 q 1 f Z t H I m w q V 8 a O x l 2 d C R G j P T N N 5 7 G N G q t p / u X f K p O S Y o F r G q B h T v Y r 5 f w + y a m i v b m j v Z x v C M a c h a G t j Y o V p Q 8 N 1 j 1 A q l r Z N v X 3 7 o k y w d 3 4 c Z m j K J k n L O J E z v a u D v j l Q 1 t T K S H d s U z s t T V n B 2 / v h I O q 2 v n 0 Y 9 J 1 K O z 9 b a W V S 0 T D I a u R M f Q o 1 3 V V X 3 1 o U U 7 + p 3 l 5 d v + f W t y O t b 4 C 9 V t c v d r g S H E S x E m u s L t C u Y I C 6 q t o b x 1 l W a F p f G V 5 f V g F I D Z P F i s m R B W 6 E y u c E f r z Y b w a U f h y N e M A X c 1 0 k C 0 M t V 5 o E I d s r a G Z Z j w h y o a x j L q I 4 S 7 X t w 6 n F J r L E K / 0 i w i Q G V t + Y v x b G u K V Z k W C r r w Y I v V r a 6 S w a t E / M 0 G W G x A Q u E x A z c p k R M e A y Q M z Y Z c b E h C 4 T E j N x m Q k x k c t E x H x 1 m a / E P L j M A z G x y 8 R L L e M i 8 S K B O x Y / u I 7 m 6 r A z K 7 j p f Z 0 K 6 Y 2 y 9 P f S U 5 8 f U Y 5 z d f I 4 C + M l V d u p 2 3 Z K d 8 1 c J i M m d 5 m c m E e X e S S m c J m C G O E y g h j p M p K Y q c t M i S l d p i T m y W W e i J m 5 z I y Y u c v M i X l 2 m e e l M W h 2 A 2 B m z u r j v a w 2 y c J s p e G Y b Z u 6 3 9 r l s Y j K 9 d U 8 4 z g 8 J J j t j T I g m G 2 M c k Q w 2 x U l E M y 2 R D k m m O 2 H M i S Y b Y Z y Q j D b C e W U Y L Y N y q 8 E s z 1 Q P h D M N k A Z E x w z O C E 4 Y T C b a D 7 D G c F M z G V O M F N y + U g w k 3 F Z E M w 0 X A q C B V 9 U g m X 7 n H D p l g Q z 3 Z Z P B D P R l j O C m W L L O c F M r u U z w V a r n R j U c y j 9 E K V o 0 S 0 Y 0 b W e y 2 C U 1 3 o y g 5 F f 6 9 k M R o O t p z M Y I b a e z 2 D U 2 H p C g 5 F k 6 x k N R p e t p z R y L 5 7 T Y B T a e l K D k W n r W Q 1 G q 8 3 T 2 n K J y y W c e / E k B i P d 1 r M Y j H 5 b T 2 M w I m 4 9 j 8 E o u f V E B i P n 1 j M Z j K Z b T 2 U w w m 4 9 l 8 G o u / V k B i P x 1 r M Z j M 5 b T 2 c w Y m 8 9 n 8 E o / u U T G v d C E Q W 1 Q 0 l 2 a H / s 0 L Z J d g n e Z f A e w X s M 3 i d 4 n 8 E d g j s M P i D 4 g M G H B B 8 y + I j g I w Y f E 3 z M 4 I 8 E f 2 T w C c E n D O 4 S 3 G X w K c G n D D 4 j + I z B 5 w S f M 7 h H c I / B F w R f M P i S 4 E s G X x F 8 x e A + w X 0 G X x N 8 z e A b g m 8 Y f E v w L Y P v C L 5 j 8 D 3 B 9 w z + R P C n l 4 9 X V 3 R g V M c 0 u s P 0 q 6 X H u F 3 O 7 b n c H u f 2 X W 6 f c x 2 X 6 3 D u w O U O O H f o c o e c O 3 K 5 I 8 4 d u 9 w x 5 z 6 6 3 E f O n b j c C e e 6 L t f l 3 K n L n X L u z O X O O H f u c u e c 6 7 l c j 3 M X L n f B u U u X u + T c l c t d c a 7 v c n 3 O X b v c N e d u X O 6 G c 7 c u d 8 u 5 O 5 e 7 4 9 y 9 y 9 1 z 7 p P L W d n f c A t R P o P + H I G f X d / V d c s s h Y X 9 P G u x Z G q g Q U J J o / b E C n f 9 s H o 2 W h H 6 a a q F q 2 g W O D Q I 2 R N t T h A h U 6 I t C S J k R c q q g 2 R A t P 1 A h G y H N h 2 I k N n Q V g M R s h h l 1 U n W w 6 8 G I T u h z Q Q i Z C K 0 h U A k Z t N j E D I M 2 i 4 g k r J p N U j G J s k g Z A m 0 I U C E j I C 2 A Y h Q + t f J H x H B 1 s E g l O r L a r X Y W p U G o b S u k z o i l M x 1 K k e E U r h O 4 I h Q 4 t Z p G 5 E 2 k + q 6 0 9 K P 8 4 l a b / 2 7 F m Y 5 r D R T P Y g 3 I H 0 C o w c W F R X 7 y X C k a p g L I r I E Q o X r 3 w R r p S q V W g A b R A T / J 0 h E Y a K q 6 t 8 E W z 3 X 3 x J U A 1 k s e P 8 X S q y 2 h G I N q I R C H b F B L Z R A b Q k F O q Y S i j O k E g p z Q i X s L u s r C v I r l V C M D 2 x u F k q E 9 c g X S o C 2 h J P J Z h H F l 7 E p W S j R 2 R K K 7 p F K K L i C z d R C C a 2 e o I U S m S 3 h R L N p R o G V V E J x P V E J h T W j E o p q T i U U 1 P O y + o Y Z 0 + / M 4 D r 1 o s 4 o 5 e q E i w g l W p 1 m E a H 0 q p M r I p R U d U p F h F K p T q S I U A L V 6 R M R S p s 6 a S J C y V K n S k Q o R e o E i Q g l R p 0 W E a F 0 q J M h I p Q E d Q p E h F K f T n y I U M L T 6 Q 4 R S n M 6 y S F C y U 2 n N k Q o p e m E h g g l M p 3 G E K H 0 p Z M X I p S 0 d M p C h F K V T l S I U I L S 6 Q k R S k s 6 K S F C y U i n I k Q o B e k E h M g n t o K U L o Y 8 W y S 9 O l v 0 W L Z I u n b r K 6 Z b b f 9 6 c N U e V t y V 2 c d a R X 1 I h X r v Y h + C 2 C 8 A R T X Z U S c Q 3 t F 4 Q D G O 1 B N U S I N s F K U h N u Z P Y 4 W I c X 2 d L B d C P f y 9 A v l S A 8 M s H v 1 c M 8 P Z c t H 8 c l N i / 8 w 3 5 T q d V u 3 p h 9 f V 0 K S x n a l g 6 p e 7 F i P 9 y z 2 L 0 Q 6 Q + x a j P S A 7 F q N d I A 8 s R v t A H l q M d o I 8 s h j t B X l s M d o N 8 q P F a D / I E 4 v R j p B d i 9 G e k K c W o 1 0 h z y x G + 0 K e W 4 x 2 h u x Z j P a G v L A Y 7 Q 5 5 a T H a H / L K Y r R D Z N 9 i t E f k t c V o l 8 g b i 9 E + k b c W o 5 0 i 7 y x G e 0 X e W 4 x 2 i / x k M W P U U M i H h Z 9 P D B v a j 7 + B 8 y k k 3 G U w 6 S L c Y z B J I 9 x n M K k j 7 D C Y B B I e M J g 0 E h 4 y m G Q S H j G Y l B I e M 5 j E E n 5 k M O k l P G E w S S b s M p h U E 5 4 y m I Q T n j G Y t B O e M 5 j k E / Y Y T A o K L x h M I g o v G U w 6 C q 8 Y T F I K + w w m N Y X X D C Z B h T c M J k 2 F t w w m W Y V 3 D C Z l h f c M J n G F n x h s P w j g 0 V Z Z N V E / X B k y c Y l d Q k l b Y o 9 Q k p b Y J 1 Q r 6 7 W 3 r 7 / g m A r w f E + A 9 P D W M Y y 8 z q Y 3 h M B X u J x E w n v K p v E I I S y B J / T X I e g l p 4 W n X p T L Y m x I v V 0 G s x y 9 p f 6 O 1 3 7 T 3 q E 7 k m j F A a G k W X F I K E l W H B F K i h X H h J J g x U d C S a / i h F C S q + g S S m o V p 4 S S W M U Z o a R V c U 4 o S V X 0 C C W l i g t C S a j i k l D S q b g i l G Q q + o S S S s U 1 o S R S c U M o a V T c E k o S F X e E k k L F P a E k U P G J 0 P r 5 T I p u E P Q H C 9 8 8 m a m s I Z A v 6 L o f C Z R p 3 K E S C n i X S i j c P S q h Y P e p h G L q U A l F d E A l F M 8 h l V A 0 R 1 R C s R x T C U X y k U o o j h M q o S i 6 V E I x n F I J R X B G J V z 8 c y r h o v e o h I t 9 Q S V c 5 E s q 4 e J e U Q k X t U 8 l X M x r K u E i 3 l A J F + + W S r h o d 1 T C x b q n E i 7 S J 3 a / y n 9 V 3 k s t G f A l k 8 a H 4 U G j d r V + I x a 3 t k E 3 v a d I T r K p 9 N A E e U + Y 6 H I o X J s E 5 J M c j 1 T d X t Y a 0 I E r 9 h C 0 i Y K G i w J t o 6 D h o 0 A b K W g 4 K d B W C h p e C r S Z g o a b A m 2 n o O G n Q B s q a D g q 0 J Y K G p 4 K t K m C h q s C b a u g 4 a t A G y t o O C v Q 1 g o a 3 g q 0 u Y K G u w J t r 6 D h r 0 A b L G g 4 L N A W C x o e C 7 T J g o b L A m 2 z o O G z Q B s t a D g t 0 F Y L G l 4 L t N m C h t s C b b e g 4 b d A G y 5 o O C 7 Q l g s a n g u 0 6 Y K G 6 w J t u 4 D 5 L v z 8 g I l I F l P w p u k I i n i u 3 n A a + d L 3 Q k i h w B y k y p F A p Q + n K i E 1 3 8 D 0 1 U u C 6 k X N I l n o g k 6 H q l V I 8 q i I M B E 6 9 e v 3 d 4 d z n Q T 1 O y P q J p g 1 G 2 3 b 1 0 k m v s T P 7 + 4 t n M g e j + w t 2 z q T Z C O I v z U Q H V C P x J R W 7 l M F 9 b 4 V l M s o H k E V O d C F u v d 1 D T w m Z B Z M f K F e X v e n M t O f q 6 B w e t h 4 i T w 3 M X U f q y q r H R i B E 2 e K L X E F E n j o 2 D h T R C 0 E + s m a G x z 7 e e w H s K x f v + l W w N J 7 7 V X X 7 v Q 2 X j p e c v / i c l 3 B 3 v D p N t n L J c / t j V M z y d k c N 8 i 4 W N q n c S 5 R Q L i s n 6 8 1 q U D S G F U p G k d Q N J s W 2 V g m / o w i L d C M w 2 S R 6 T e e z K O 3 1 V b y e K p G / 6 y e D 7 j s S X f J X 3 c 6 6 a 4 s 4 I 1 f U A 9 U o d m + x F 9 + g W t f Z C z y a m U B 9 r K S a F V Q A r 3 N 4 n H h J + o x 1 e Q p K 9 C 2 C n 8 u v F f d n 9 6 / U q / 6 6 D / 8 m K b m l V W R 4 / o L / a r Z q w H E M Y u x j 0 l f e 7 u Y A H H L p + q / O e 5 3 S N Q r b 8 o b m 0 Z Z t H r f N J u G O m d q q x x J 2 N T N i 8 w b Z a C a e 4 o e o h x G k b / V e M k 6 K 5 J Y P e l f L r o / v V u 2 k F k K i t t u 4 + S T r v e + j c s V k 7 c w W g v d n w Z R O p b z 5 t b J / U I 9 M s Z j w 1 e b 5 Q r w r B V + C F 6 U e m l W 2 X w J s y 1 v b 5 I J N T 2 Z M o D B x N v H T 8 Q p / F 5 4 w y x 7 2 F p 3 H v K c 5 + p 0 z o p / Q 4 0 X o e 4 A / h 5 s q q t v B a p z 0 g T i V X u T W q 0 Y p v 9 / I a K P g u q r 1 w F j k A N / i P s s z p 6 G B f g P 6 w b L 1 C F o / i 6 p / g M l w 9 R w 6 R c 4 / g k e / u v r X 7 7 b 2 G 7 + p d L q x c 3 7 r e 1 3 W 9 s X P 2 z 8 e b f 6 K 6 Z f r f 1 u 7 V / X 3 q x t r / 1 x 7 c 9 r R 2 u 9 t e u 1 Y C 1 Z + 6 + 1 / 1 7 7 n w 9 / + H D + 4 e b D n Q n 9 5 S + q O v + y 5 v x 8 8 P 8 P e h D N 2 A = = < / l a t e x i t >

Embed input to inference network

where we prevent selecting the data example itself as the prototype during training to avoid overﬁtting. Embed(·) is a pretrained sentence encoder, W is a linear transformation matrix,

Figure 2: Example of aligned sequences.

and µ is a temperature hyperparameter to control the entropy of qφt|x (t|x), which is critical to

stabilize the Reinforce algorithm at the initial training stage. To ease the computation of encoding

all training examples at each update step, we ﬁx the parameters of Embed(·) and update W only,

which proves to be sufﬁcient in our experiments. We use the average embeddings of the last layer in

pretrained BERT (Devlin et al., 2018)4 as our sentence encoder.

While Guu et al. (2018) uses sum of inserted/deleted word vectors as the mean direction parameter of vMF distribution q(z|t, x), we choose a more powerful encoder following recent advances on

4We use pretrained uncased BERT base from the transformers library (Wolf et al., 2019).

5

representing edits (Yin et al., 2019). Speciﬁcally, a standard difﬁng algorithm is run to compute an alignment of tokens in t and x, and produces two aligned sequences t , x and an additional edit sequence that indicates edit operation + (insertion), − (deletion), ↔ (substitution), and = (equal) at each position. We use ∅ to denote padding. This is illustrated in Figure 2. Word embeddings of all three sequences are concatenated and fed into a single-layer LSTM to obtain the edit representation.
4 Experiments
Our experiments below are designed to (1) examine the efﬁcacy of the proposed sparse neural editor on language modeling, (2) examine the efﬁciency of sparse neural editor on memory savings and speed-up at test time, and (3) demonstrate the interpretable semantics/syntax captured by prototypes and edit vectors.
4.1 Setup
We perform experiments on three different-scale datasets to test our method in different scenarios:
• MSCOCO (Lin et al., 2014): MSCOCO is an image caption dataset and we only focus on its captions as our data. The average length of captions is 12.6. The sentences do not have complex variations and are easy to ﬁnd similar sentences as prototypes. We randomly sample 40K sentences as our training data and 4K as validation and test set respectively. This dataset represents a simple and small-scale setting to test our approach.
• Yelp Medium/Yelp Large: These datasets consist of sentences from Yelp restaurant reviews (Yelp, 2017) preprocessed by Guu et al. (2018), allowing us to perform a direct comparison with their method. The medium and large datasets consist of 1.5M and 17M sentences respectfully, allowing us to test in moderate and relatively large data settings respectively. Note that Yelp Medium is obtained by further ﬁltering Yelp Large to keep sentences that are generally shorter and have less variations, but the test sets for these two are the same.
Baselines: We mainly consider neural language models (NLM) and the neural editor (Guu et al., 2018) as our baseline. It is worth noting that the neural editor model does not have likelihood deﬁned on test sentences that are not similar to any example in the prototype library, thus it is necessary to interpolate with another NLM at test time for smoothing purposes, while our model is able to be used on its own. Note that we only report the neural editor baseline on Yelp Large since their public code is not ready to run on other datasets due to the required pre-built index to retrieve prototypes.
Evaluations: We evaluate language modeling performance with perplexity (PPL). For our model, we approximate the log marginal data likelihood through 1000 importance-weighted latent variable samples (Burda et al., 2015) and compute PPL based on this likelihood. At test time our approach prunes and has access to only M prototypes that occupy 90% probability mass of the posterior prototype distribution as described in Section 3.2. We report M as “#prototypes”. To have an intuitive notion about how similar the prototype and data examples are, we compute average of smoothed sentence-level BLEU scores (Lin & Och, 2004) of the data examples on the validation set with their most likely prototype as the reference. We also report BLEU scores based on part-of-speech sequences (POS-BLEU)5 to view the similarity from a more syntactic perspective. Test speed is evaluated on a single Nvidia 1080 Ti GPU.
Hyperparameters: We try different Dirichlet prior parameters α to control the sparsity and report different sparsity settings for all the datasets. The temperature parameter µ in the prototype retriever q(t|x) is tuned on the MSCOCO validation data and set as 0.3 for all datasets. The concentration parameter κ of the vMF distribution is tuned and set as 30 for MSCOCO and Yelp Medium and 40 for Yelp Large. The number of Reinforce samples L is set as 10 across all datasets. We sample 50K examples for Yelp Medium and 100K examples for Yelp Large as our training prototype library to address the training memory issue discussed in Section 3.2. We employ the same attentional LSTM Seq2Seq model as in Guu et al. (2018) to parameterize pγ (x|t, z) for a direct comparison. Our implementation is based on the fairseq toolkit (Ott et al., 2019) and complete hyperparameter settings can be found in Appendix B.
5POS tagging is performed using the Stanza library (Qi et al., 2020).
6

Table 1: Results on three datasets. Numbers in the parentheses indicate the percentage of prototypes over all training examples. BLEU score is computed by comparing validation examples against their most likely prototypes. POS-BLEU represents the BLEU score on part-of-speech sequences. We also list BLEU scores from random prototype retrieval as a reference point. Results in the starred entry (∗) are obtained by running the public code of the neural editor.

Dataset

Model

PPL↓ #prototypes test speed (sent/s)↑ BLEU POS-BLEU

random retrieval

–

–

– 10.9

31.6

MSCOCO

NLM

20.0

Sparse Neural Editor (α = 10−3) 18.9

Sparse Neural Editor (α = 0.1)

18.6

Sparse Neural Editor (α = 0.2)

19.0

Sparse Neural Editor (α = 0.3)

19.2

– 25 (0%) 778 (2%) 16K (40%) 22K (56%)

3714

–

–

388 13.2

38.8

313 17.2

42.5

250 20.9

46.7

217 22.2

47.9

random retrieval

–

–

–

8.1

17.8

Yelp Medium NLM

74.7

Sparse Neural Editor (α = 10−3) 63.6

– 77 (0%)

Sparse Neural Editor (α = 0.5)

61.9 1.5K (0.1%)

Sparse Neural Editor (α = 10)

63.2 31K (2.1%)

236

–

–

157 12.3

24.7

107 21.6

38.4

95 29.9

48.3

random retrieval

–

–

–

6.6

16.0

Yelp Large

NLM Sparse Neural Editor (α = 0.7) Sparse Neural Editor (α = 10)
Neural Editor (Guu et al., 2018) Neural Editor (our runs)∗ Sparse Neural Editor (α = 0.7)

34.2

–

30.2 2K (0.01%)

30.3 5.5K (0.03%)

Interpolated w/ NLM

26.9 17M (100%) 31.2 17M (100%) 20.2 2K (0.01%)

272

–

–

108 10.5

24.8

98 10.8

25.3

–

–

–

0.1

–

–

108 10.5

24.8

4.2 Results
Results are shown in Table 1. Our sparse neural editor outperforms the NLM baseline across all datasets in terms of PPL, often by a large margin. When interpolated with an NLM at test time, our method outperforms the NLM baseline by 14 PPL points and neural editor by 6.7 PPL points.6 This effect is also observed in (Guu et al., 2018) – prototype-driven language models are especially strong at modeling test sentences that have similar prototypes but relatively weak at modeling others, thus interpolation with a normal NLM is likely to help. Furthermore, in line with the goal of this work to learn a sparse prototype set, our method is able to achieve superior language modeling performance while utilizing only a small fraction of training examples as prototypes. This veriﬁes our hypothesis that a sparse prototype set sufﬁces for such non-parametric language modeling. Also, sparsity learned in our model allows for over a 1000x memory savings and 1000x speed-up7 at test time on Yelp Large compared with a previous neural editor that memorizes all training examples.
Table 1 demonstrates the trend that a smaller Dirichlet hyperparameter α leads to a sparser prototype set, which agrees with our expectation. BLEU scores are also improved as α increases, implying that the less sparse the prototype set the closer the match between the sentence and its prototype. Interestingly, the BLEU score on Yelp Large is a bit low and the model tends to generally favor sparse prototypes. We suspect that this is because it is difﬁcult to learn prototypes that capture ﬁne-grained semantic features and large sentence variations among 17M examples with a limited prototype memory budget, thus the model has to learn prototypes that represent more generic shared features among examples to reach an optimum – for example, the syntactic feature as somewhat reﬂected by the decent POS-BLEU scores.
We want to emphasize that different sparsity may lead to different notions of prototypes and it is hard to judge which one should be preferred – memorizing more prototypes pays cost on language modeling performance and does not necessarily produce better PPL. Also, prototypes that are “less similar” to the examples on the superﬁcial level but capture coarse-grained features may have potentially interesting application on sentence generation since the model is able to generate more diverse output conditioned on prototypes.
6For a fair comparison, we interpolate with the same pretrained NLM from (Guu et al., 2018). 7We include the time to retrieve prototypes for new test sentences when computing test speed. We use Guu et al. (2018)’s public implementation of the neural editor, where the computation of edit distance between all training examples and test sentences to ﬁnd nearest neighbors accounts for much of the runtime. More efﬁcient implementation of this operation, for example through tries, may speed this to some extent.

7

Table 2: Number of matching tokens between examples and their prototypes on the Yelp Medium validation set. Results are reported in cluster of POS tags. Relative changes that are larger than the overall change are bolded.

Model

Overall NOUN DET AUX PRON ADJ VERB CCONJ

Sparse Neural Editor (31K prototypes) 91.2K 14.4K 9.6K 9.3K 9.0K 7.2K 6.4K

5.5K

Sparse Neural Editor (1.5K prototypes) 74.7K 9.9K 8.5K 8.2K 7.3K 5.6K 4.4K

5.0K

Relative Change

-18.1% -31.3% -11.5% -11.8% -18.9% -22.2% -31.3% -9.1%

Table 3: Qualitative examples of prototypes when using denser and sparser prototype supports.

Data Examples

Prototypes

the best corned beef hash i ’ve ever had !

(dense) the best real corned beef hash i ’ve had . (sparse) the chicken satay is the best i ’ve ever had .

the grilled chicken was ﬂavorful , but too ﬂavorful . (dense) the chicken was moist but it lacked ﬂavor . (sparse) my sandwich was good but the chicken was a little plain .

i asked her what time they close and she said <cardinal> o’clock .

(dense) i asked what time they closed <date> , and was told <cardinal> . (sparse) we asked how long the wait was and we were informed it would be <time> .

Table 4: Qualitative examples from the MSCOCO dataset on interpolated sentence generation given the prototype. The ﬁrst row is the given prototype, the second-row and the last-row sentences are obtained by sampling edit vectors from the prior, the rest three sentences are generated by interpolating between the two edit vectors.

Prototype: A man is using a small laptop computer

Prototype: A cat sitting on a sidewalk behind a bush

A man is using his laptop computer with his hands on the keyboard A man is using a laptop computer with his hands on the keyboard A man is using a laptop computer while sitting on a bench A man is using a laptop computer in the middle of a room A young man is using a laptop computer in the middle of a room

A cat laying on top of a wooden bench A cat standing next to a tree in a park Two cats sitting on a bench near a park bench A dog sitting on a bench near a park bench A dog sitting on a bench near a park bench

4.3 Analysis
How do prototypes change when they grow sparser? In Table 1 we notice the BLEU scores usually drop when the prototype set is sparser, implying the learned prototypes change in some way under different sparsity settings. Here we take the Yelp Medium dataset as an example to analyze how prototypes are trained to capture sentence attributes differently in the sparse (α = 0.5) and relatively dense (α = 10) situations. Speciﬁcally, we align prototype and example sequences to minimize edit distance, and focus on words that were aligned between the two sequences. This allows us to obtain a notion of what kind of words are more likely to be kept the same as the prototype during the model’s editing process and how this pattern changes in different sparsity settings. We cluster these matched words in terms of POS tag and report the most common ones.8
Results are shown in Table 2. While the overall number of matching tokens decreases as the prototype set becomes sparser, the content words exhibit a more drastic change (e.g. the nouns, adjectives, and verbs). In contrast, the function words experience a moderate decrease only (e.g. the determiners, auxiliaries, and coordinating conjunctions). This shows that the model tends to learn prototypes that drop ﬁne-grained semantic distinctions but keep the same general syntax when a sparsity constraint is enforced, which is not surprising since it is difﬁcult for a limited number of prototypes to capture large semantic variations in a large dataset. We list qualitative examples in Table 3 to demonstrate this phenomenon, where some content words such as “beef” or “chicken” differ between data examples and prototypes in the sparser setting, yet these words do not change in the dense setting.
Ablation Study on Pre-clustering Prototypes: There is a simple method that is able to endow the neural editor baseline with sparse prototypes – pre-clustering the training examples and using only a subset of the them as the prototype pool during training and test. To compare the effectiveness of this heuristic sparse prototypes and our learned sparse prototypes, we experiment this baseline on MSCOCO dataset and pre-cluster 778 prototypes that correspond to our setting α = 0.1. Concretely, we run k-means to obtain 778 clusters of training sentences embedded by Sentence-BERT (Reimers & Gurevych, 2019) which produces state-of-the-art semantic sentence embeddings.Then for each cluster we identify one prototype whose embedding is the closest to the cluster centroid. We use these 778 prototypes in the neural editor model (Guu et al., 2018) and train with the same objective
8Note that alignment is performed on word sequences instead of POS sequences.
8

Table 5: Comparison with Neural Editor that uses heuristic pre-clustering to select a sparse prototype support. The results are on MSCOCO dataset.

Model

PPL↓ #prototypes BLEU POS-BLEU

NLM

20.0

Neural Editor (pre-cluster) 19.5

Sparse Neural Editor

18.6

–

–

–

778 17.9

40.4

778 17.2

42.5

as Guu et al. (2018).9 Results are shown in Table 5. Our method outperforms the neural editor with the same number of prototypes and presents similar BLEU scores, which demonstrates the superiority of learned prototypes over the heuristically selected prototypes.
Interpolation on the edit vector space: We take our model with 1.5K prototypes on the MSCOCO dataset (i.e. α = 0.1) and perform sentence interpolation in edit vector space. Speciﬁcally, we sample two edit vectors from the uniform vMF prior to produce two sentences for each prototype with beam search decoding, then we perform spherical linear interpolation of the two edit vectors to generate interpolated sentences in-between. Qualitative examples in Table 4 (more examples in Appendix C) show that the edit vectors are able to smoothly capture minor edits over the given prototypes.

5 Conclusion
In this work, we propose a novel generative model that discovers a sparse prototype set automatically by optimizing a variational lower bound of the log marginal data likelihood. We demonstrate its effectiveness on language modeling and its efﬁciency advantages over previous prototype-driven generative models. The framework proposed here might be generalized to automatically discover salient prototypes from a large corpus. New kinds of prototype structure in text might be discovered through either injecting different biases into the model (e.g. sparsity biases in this paper), or incorporating prior knowledge into the prototype library before training. Finally, the approach might be easily extended to conditional generation (e.g. with the edit vectors depending on other data input), and we envision that inducing a sparse prototype set in this case may potentially facilitate controlling text generation through prototypes. We leave exploration in this direction as our future work.

Broader Impact
Our approach is likely to beneﬁt researchers or practitioners who are interested in generating text through machines in practical scenarios such as writing news articles given facts, describing stock trends with stock data, generating headlines, etc. Many such applications have a small number of templates for generated text. On the one hand, our model is able to automatically induce those representative prototypes from a large corpus, helping knowing the salient prototypes or templates to help human writers to start with. On the other hand, our approach can be easily extended to these conditional text generation task directly, for example, with the edit vector depending on the input data instead of from a uniform distribution. Such way potentially allows our model to control the prototypes and directly generate text conditioned on the input data as well. Furthermore, the prototype library may be further explored in other formats in addition to training examples, opening a door to more ﬂexible control under different notions of “prototypes”.
With respect to possible disadvantages from this research from a societal perspective, as a contribution to the widely studied ﬁeld of language modeling, the proposed method inherits some of the risks of the ﬁeld as a whole. These may include models being used maliciously to create fake content (Zellers et al., 2019), or models being used in earnest being manipulated through adversarial attacks to generate undesirable or defamatory content (Wallace et al., 2019). With respect to the latter, as our method is more interpretable due to its use of readable prototypes, we actually expect that it may be more robust to adversarial attacks, and these attacks may be easier for human auditors to detect or defuse when they occur. However, this is speculation, and would have to be conﬁrmed by further experiments.
9The prior over edit vector p(z) and the inverse neural editor q(z|t, x) in sparse neural editor is different from the original neural editor model. For a fair comparison we use p(z) and q(z|t, x) described in this paper for the neural editor baseline here.

9

Acknowledgements
This work was supported in part by the DARPA GAILA project (award HR00111990063), and a gift of computation credits from Amazon AWS. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the U.S. Government or Amazon.
References
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. In Proceedings of AAAI, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR, 2015.
Ankur Bapna and Orhan Firat. Non-parametric adaptation for neural machine translation. In Proceedings of NAACL, 2019.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137–1155, 2003.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of CoNLL, 2016.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of ACL, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. Search engine guided neural machine translation. In Proceedings of AAAI, 2018.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018.
Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In Proceedings of NeurIPS, 2018.
Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, and Graham Neubig. Retrieval-based neural code generation. In Proceedings EMNLP, 2018.
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference networks and posterior collapse in variational autoencoders. In Proceedings of ICLR, 2019.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997.
Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artiﬁcial Intelligence Research, 47:853–899, 2013.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor machine translation. arXiv preprint arXiv:2010.00710, 2020a.
10

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In Proceedings of ICLR, 2020b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive ﬂow. In Proceedings of NeurIPS, 2016.
Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, and Noah A. Smith. What do recurrent neural network grammars learn about syntax? In Proceedings of EACL, 2017.
Bohan Li, Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick, and Yiming Yang. A surprisingly effective ﬁx for deep latent variable modeling of text. In Proceedings of EMNLP, 2019.
Chin-Yew Lin and Franz Josef Och. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of COLING, 2004.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of ECCV, 2014.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4: 521–535, 2016.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In Proceedings of ICLR, 2018.
Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan Cˇ ernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association, 2010.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL (Demo Track), 2019.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of EMNLP, 2014.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of EMNLP, 2019.
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of ACL (Demo Track), 2020.
Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese bertnetworks. In Proceedings of EMNLP, 2019.
Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Proceedings of EMNLP, 2015.
Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive generation of conversational responses. In Proceedings of NAACL, 2015.
11

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of NeurIPS, 2017.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for nlp. arXiv preprint arXiv:1908.07125, 2019.
Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, 2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response generation by context-aware prototype editing. In Proceedings of AAAI, 2019.
Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. In Proceedings of EMNLP, 2018.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In Proceedings of NeurIPS, 2019.
Yelp. Yelp dataset challenge, round 8. 2017. URL https://www.yelp.com/dataset_challenge. Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt.
Learning to represent edits. In Proceedings of ICLR, 2019. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending against neural fake news. In Advances in Neural Information Processing Systems, pp. 9051–9062, 2019.
12

A Derivations of Variational Inference and ELBO

A.1 Derivation of optimal q∗(θ) Here we try to show that the optimal variational distribution over θ, qλ∗ (θ), is a Dirichlet distribution and derive its optimal λ∗ as given in Eq. 7. According to (Bishop, 2006) (Chapter 10.1.1), we have:

qλ∗ (θ) ∝ exp E−θ[log p({xn, tn, zn}Nn=1, θ)] ,

(10)

where E−θ denotes expectation over all latent variables except for θ. We expand Eq. 10 as:

qλ∗ (θ) ∝ exp E−θ[log p({xn, tn, zn}Nn=1, θ)]

∝ exp E−θ[log pα(θ) + log p(tn|θ)]

n

∝ exp E−θ[ log θkα−1 + log θk1(tn=xk)]

k

n

k

∝ exp E−θ[ log θkα−1 + 1(tn = xk) log θk] (11)

k

nk

∝ exp

log θkα−1 +

k

nk

∝ θkα−1+ n q(tn=xk|xn),

k

q(tn = xk|xn) log θk

where 1(·) is the indicator function. We conclude that qλ∗ (θ) has the form of Dirichlet distribution
and the optimal Dirichlet parameter λ∗k = α + n q(tn = xk|xn).

A.2 Derivation of Three KL Divergence Terms
There are three KL divergence terms in our training objective ELBO (Eq. 4). Now we show that all three KL divergence terms can be computed exactly and efﬁciently at training time and we derive their expressions respectively:
(1). Eq(tn|xn)[DKL(q(zn|tn, xn)||p(zn))]: As shown in (Xu & Durrett, 2018), the KL divergence between any vMF distribution with ﬁxed concentration parameter and a uniform vMF distribution is a constant:

DKL(vMF(µ, κ)||vMF(·, 0)) = κ Id/2(κ) + ( d − 1) log κ − d log(2π)

Id/2−1(κ) 2

2

(12)

d

d

− log Id/2−1(κ) + 2 log π + log 2 − log Γ( 2 ),

where d is the number of dimensions, Iv stands for the modiﬁed Bessel function of the ﬁrst kind at order v. Therefore,

Eq(tn|xn)[DKL(q(zn|tn, xn)||p(zn))] = DKL(vMF(µ, κ)||vMF(·, 0)) = const

(13)

13

(2). Eqλ(θ)[DKL(q(tn|xn)||p(tn|θ))]:

Eqλ(θ)[DKL(q(tn|xn)||p(tn|θ))]

= Eqλ(θ) Eq(tn|xn) log q(tn|xn) − log p(tn|θ)

= Eq(tn|xn) log q(tn|xn) − Eqλ(θ)Eq(tn|xn) log θk1(tn=xk)
k

= q(tn|xn) log q(tn|xn) − Eqλ(θ)Eq(tn|xn)

1(tn = xk) log θk

(14)

k

k

= q(tn = xk|xn) log q(tn = xk|xn) − q(tn = xk|xn)Eqλ(θ) log θk

k

k

N (i)
= q(tn = xk|xn) log q(tn = xk|xn) − q(tn = xk|xn)[Ψ(λk) − Ψ( λi)],

k

k

i

where Ψ(·) is the digamma function, and step (i) computes the expectation of log θk over Dirichlet variable θ by using the general fact that the derivative of the log normalization factor with respect to
the natural parameter is equal to the expectation of the sufﬁcient statistic.

(3). DKL(qλ(θ)||pα(θ)):

DKL(qλ(θ)||pα(θ)) = Eqλ(θ) log qλ(θ) − log pα(θ)

= Eqλ(θ) log

θkλk−1/B(λ) − log
k

θkα−1/B(α)
k

N

= − log B(λ) + (λk − 1)[Ψ(λk) − Ψ( λi)]

(15)

k

i

N

+ log B(α) − (α − 1)[Ψ(λk) − Ψ( λi)],

k

i

where B(·) is the multivariate beta function and B(λ) is the normalization factor for Dirichlet distribution parameterized by λ.

B Experimental Details
On MSCOCO dataset, we use a single-layer attentional LSTM seq2seq architecture with word embedding size 100 and hidden state size 400 as pγ (x|t, z), the latent edit vector dimension is 50. This conﬁguration follows the hyperparameters for vMF-VAE (Xu & Durrett, 2018). On Yelp Medium and Yelp Large datasets, we follow (Guu et al., 2018) to use a three-layer attentional LSTM seq2seq architecture as pγ (x|t, z) with word embedding size 300 and hidden state size 256, the edit vector dimension is 128. Skip connections are also used between adjacent LSTM layers. In the inverse editor qφz|t,x (z|t, x), we use a single-layer LSTM to encode three sequences – the aligned prototype, aligned data example, and the edit operation sequence, of which the word embedding size for text sequences and the hidden state size are the same as in pγ (x|t, z), and the word embedding size for edit operation sequence is 10 (since the vocobulary size of edit operations is very small). Across all datasets, we initialize word embeddings in pγ (x|t, z) (both encoder and decoder sides) and qφz|t,x (z|t, x) with GloVe word embeddings (Pennington et al., 2014) following (Guu et al., 2018). All NLM baselines use the same architecture as pγ (x|t, z) in our model for a fair comparison.
With respect to hyperparameter tuning, we tune the temperature parameter µ in the prototype retriver q(t|x) on the MSCOCO validation data in the range of {0.1, 0.3, 0.5, 0.7, 0.9, 1.0}, and set as 0.3 for all datasets. The concentratioon parameter κ of the vMF distribution is tuned in the range of {30, 40, 50} for all datasets. κ is set as 30 for MSCOCO and Yelp Medium and 40 for Yelp Large. We run different α as {0.1, 0.3, 0.5, 0.7, 0.9, 1, 10} for each dataset to obtain prototype set with varying sparsity. On MSCOCO dataset we also add an additional run with α = 0.2 since 0.5 is a large value on this dataset already (90% of training examples are selected as the prototype set when α = 0.5). We apply annealing and free-bits techniques following (Li

14

et al., 2019) to the KL term on prototype variable, Eqλ(θ)[DKL(q(tn|xn)||p(tn|θ))], to mitigate posterior collapse. Speciﬁcally, Eqλ(θ)[DKL(q(tn|xn)||p(tn|θ))] in our training objective becomes β · max{Eqλ(θ)[DKL(q(tn|xn)||p(tn|θ))], c} in practice. This objective means that we can downweight this KL term with β < 1 and optimize it only when this KL is larger than a threshold value c. We increase β from 0 to 1 linearly in the ﬁrst m epochs (annealing). m is tuned in the range of {5, 10} for MSCOCO and {1, 2, 3} for Yelp Medium and Yelp Large.10 c is tuned in the range of {5, 6, 8}. To obtain the reported results in Section 4, m is set as 5 for MSCOCO, 2 for Yelp Medium and 3 for Yelp Large. c is set as 5 for MSCOCO, 6 for Yelp Medium and 8 for Yelp Large. We use Adam (Kingma & Ba, 2014) to optimize the training objective with learning rate 0.001.
C Qualitative Results on Interpolation
As in Section 4.3, here we show more generated examples through interpolation on MSCOCO dataset.

Table 6: Qualitative examples from the MSCOCO dataset on interpolated sentence generation given the prototype. For each example, the ﬁrst row is the given prototype, the second-row and the last-row sentences are obtained by sampling edit vectors from the prior, the rest three sentences are generated by interpolating between the two edit vectors.

Prototype: a horse drawn carriage on the side of a city street

Prototype: A baseball pitcher on the mound having just threw a pitch

Two horses drawn carriage on a city street Two horses standing next to each other on a city street Two horses on the side of a city street Two horses on the side of a city street A brown and white horse drawn carriage on a city street

a baseball player swinging a bat at home plate a man about to hit a ball with his bat a man swinging a bat at the ball during a game a person swinging a bat at the ball during a game A person swinging a bat during a baseball game

Prototype: A man walking on the beach carrying a surfboard
Two people standing next to each other on a beach A person standing on the beach holding a surfboard A man walking along the beach with a surfboard A man walking on the beach with a surfboard A young man walking on the beach with a surfboard

Prototype: A group of people are raising an umbrella on a beach
A group of people are walking on the beach with umbrellas A group of people are walking on the beach next to each other A group of people are walking on the beach with umbrellas A group of people are holding umbrellas on the beach A group of people are walking on the beach

Prototype: there is a white truck that is driving on the road
there are many cows that are standing in the dirt there are many cows that are standing in the dirt the truck is driving down the road in the rain this truck is driving down the road in the rain This truck is pulled up to the side of the road

Prototype: A couple of bags of luggage sitting up against a wall
A large pile of luggage sitting on top of a wall A pile of luggage sitting on top of a wall Two bags of luggage sitting on the ground Two bags of luggage sitting in a room A couple of bags of luggage on a wooden ﬂoor

Prototype: A man riding a sailboat in the ocean next to a shore
A man on a boat in a body of water A man riding a boat on a body of water A man riding a boat in a body of water A man riding a small boat on a body of water A man riding a wave on top of a boat

Prototype: A beer bottle sitting on a bathroom sink next to a mirror
A white cell phone sitting next to a toilet in a bathroom A white bottle of wine sitting next to a toilet A glass of wine sitting next to a toilet in a bathroom A pair of scissors is placed next to a toilet A pair of scissors sitting next to each other on a toilet

Prototype: A little boy sitting on a mattress holding a stuffed animal
A little girl playing with a stuffed animal A little girl playing with a stuffed animal A little boy holding a stuffed animal in his mouth A little girl sitting on a bed with stuffed animals A little girl sitting on a bed with stuffed animals

Prototype: A giraffe has its nose pressed against the trunk of a tree
Two giraffes look at a wire fence to eat Two giraffes look at a fence to eat a couple of giraffes are standing by a fence a close up of a giraffe is eating a carrot a close up of a giraffe has its mouth open

10It is unreasonable to set a large m on Yelp dataset since there are tens of thousands update steps per epoch on Yelp, the annealinig process would be too slow if m is large which usually hurts the language modeling performance (He et al., 2019).
15

