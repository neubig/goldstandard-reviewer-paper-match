Disturbance Decoupling for Gradient-based Multi-Agent Learning with Quadratic Costs
Sarah H. Q. Li1, Lillian Ratliff2, Behc¸et Ac¸ıkmes¸e1

arXiv:2007.07228v2 [cs.GT] 10 Oct 2020

Abstract— Motivated by applications of multi-agent learning in noisy environments, this paper studies the robustness of gradient-based learning dynamics with respect to disturbances. While disturbances injected along a coordinate corresponding to any individual player’s actions can always affect the overall learning dynamics, a subset of players can be disturbance decoupled—i.e., such players’ actions are completely unaffected by the injected disturbance. We provide necessary and sufﬁcient conditions to guarantee this property for games with quadratic cost functions, which encompass quadratic one-shot continuous games, ﬁnite-horizon linear quadratic (LQ) dynamic games, and bilinear games. Speciﬁcally, disturbance decoupling is characterized by both algebraic and graph-theoretic conditions on the learning dynamics, the latter is obtained by constructing a game graph based on gradients of players’ costs. For LQ games, we show that disturbance decoupling imposes constraints on the controllable and unobservable subspaces of players. For two player bilinear games, we show that disturbance decoupling within a player’s action coordinates imposes constraints on the payoff matrices. Illustrative numerical examples are provided.
I. INTRODUCTION
As the application of learning in multi-agent settings gains traction, game theory has emerged as an informative abstraction for understanding the coupling between algorithms employed by individual players (see, e.g., [1]–[3]). Due to scalability, a commonly employed class of algorithms in both games and modern machine learning approaches to multiagent learning is gradient-based learning, in which players update their individual actions using the gradient of their objective with respect to their action. In the gradient-based learning paradigm, continuous quadratic games stand out as a benchmark due to their simplicity and ability to exemplify state-of-the-art multi-agent learning methods such as policy gradient and alternating gradient-descent-ascent [4].
Despite the resurgence of interest in learning in games, a gap exists between algorithmic performance in simulation and physical application in part due to disturbances in measurements [5]. Robustness to environmental noise has been analyzed in a wide variety learning paradigms [6], [7]. Most analysis focuses on independent and identically distributed stochastic noise drawn from a stationary distribution.
In contrast, we study adversarial disturbance without any assumptions on its dynamics or bounds on its magnitude. Though some work exists on the effects of bounded adversarial disturbance in multi-agent learning [8], there is lim-
*This research is partly funded by the following grants: NSF CNS1736582 and ONR N00014-17-1-2623.
1William E. Boeing Department of Aeronautics and Astronautics, University of Washington, Seattle. email: {sarahli,behcet}@uw.edu
2Department of Electrical and Computer Engineering, University of Washington, Seattle. email: ratliffl@uw.edu

ited understanding of how gradient disturbance propagates through the network structure as determined by the coupling of the players’ objectives. Does gradient-based learning fundamentally contribute to or reduce the propagation of disturbance through player actions? Our analysis aims to answer this question for gradient-based multi-agent learning dynamics. The insights we gain provide desiderata to support algorithm synthesis and incentive design, and will lead to improved robustness of multi-agent learning dynamics.
Contributions. The main contribution is providing a novel graph-theoretical perspective for analyzing disturbance decoupling in multi-agent learning settings. For quadratic games, we obtain a necessary and sufﬁcient condition, which can be veriﬁed in polynomial time, that ensures complete decoupling between the corrupted gradient of one player and the learned actions of another player, stated in terms of algebraic and graph-theoretic conditions. The latter perspective leads to greater insight on the types of cost coupling structures that enjoy disturbance decoupling, and hence, provides a framework for designing agent interactions, e.g., via incentive design or algorithm synthesis. Applied to LQ games, a benchmark for multi-agent policy gradient algorithms, we show that disturbance decoupling enforces necessary constraints on the controllable subspace in relation to the unobservable subspace of individual players. Applied to bilinear games, we show that disturbance decoupling enforces necessary constraints on the players’ payoff matrices.
II. RELATED WORK
We study gradient-based learning for N –player quadratic games with continuous cost functions and action sets. Convergence guarantees for gradient-based learning are studied from numerous perspectives including game theory [1], [3], [9], control [10], and machine learning [2], [11].
Convergence guarantees for gradient-based learning dynamics under stochastic noise are studied in [2], [3], [11]. Despite being an important property to understand for adversarial disturbance, how non-stochastic noise propagates through the player network has no guarantees.
Our analysis draws on geometric control [12]–[14]. In [12], algebraic conditions for disturbance decoupling within a single dynamical system is given. In [14], disturbance decoupling for a single structured dynamical system is studied with frequency-based techniques. In this paper, we provide both the algebraic and graph-theoretic conditions for disturbance decoupling of coupled dynamical systems in gradient-based multi-agent learning.

III. CONTINUOUS GAMES AND THE GAME GRAPH MODEL

Let [N ] = {1, 2, . . . , N } denote the index set where

N ∈ N. For a function f ∈ Cr(Rn, R) with r ≥ 2,

Dif = ∂f /∂xi is the partial derivative with respect to xi.

Consider an N -player continuous game (f1, . . . , fN )

where for each i ∈ [N ], fi ∈ Cr(Rn, R) with r ≥ 2 is

player i’s cost function and Rn = Rn1 × . . . × RnN is the

joint action space, with Rni denoting player i’s action space

and n =

N i=1

ni.

Each

player’s

goal

is

to

select

an

action

xi ∈ Rni to minimize its cost fi : Rn → R given the

actions of all other players. That is, player i seeks to solve

the following optimization problem:

min fi(x1, . . . , xi, . . . , xN ).

(1)

xi ∈Rni

:= x

One of the most common characterizations of the outcome of a continuous game is a Nash equilibrium.

Deﬁnition 1 (Nash equilibrium). For an N –player continuous game (f1, . . . , fN ), a joint action x = (x1, . . . , xN ) ∈ Rn is a Nash equilibrium if for each i ∈ [N ],
fi(x ) ≤ fi(x1, . . . , xi−1, xi, xi+1, . . . , xN ), ∀ xi ∈ Rni .

A. Gradient-based learning

We consider a class of simultaneous play, gradient-based
multi-agent learning techniques such that at iteration k, player i receives hi(xk) from an oracle to update its action as follows:

xki +1 = xki − γihi(xk1 , . . . , xkN ),

(2)

where γi > 0 is player i’s step size,

hi(xk) = Difi(xk) + dki

(3)

is player i’s gradient evaluated at the current joint action xk and affected by a player-speciﬁc, arbitrary additive disturbance dki ∈ Rni . In the setting we analyze, dki can modify xki to any other action within Rni .
Under reasonable assumptions on step sizes—e.g., relative to the spectral radius of the Jacobian of hi in a neighborhood of a critical point—it is known that the undisturbed dynamics converge [2], [3]. While such a guarantee cannot be given for arbitrary disturbances as considered in this paper, we provide conditions under which a subset of players still equilibriates and follows the undisturbed dynamics.

B. Quadratic games
For an N –player continuous game (f1, . . . , fN ), behavior of gradient-based learning around a local Nash equilibrium can be approximated by linearizing the learning dynamics, where the linearization corresponds to a quadratic game.

Deﬁnition 2 (Quadratic game). For each i ∈ [N ], fi : Rn → R is deﬁned by

fi(x) = 21 xi Pixi + xi ( j=iPij xj + ri).

(4)

Quadratic games encompass potential games [15] with
Pij = Pji , and zero sum games [16] with Pij = −Pji . We give further examples of quadratic games in Section III-D.

C. Game graph

To highlight how an individual player’s action updates

depend on others’ actions, we associate a directed graph to

the gradient-based learning dynamics deﬁned in (2).

We consider a directed graph ([N ], E), where [N ] is the

index set for the nodes in the graph, and E is the set of

edges. Each node i ∈ [N ] is associated with action xi of the ith player. A directed edge (j, i) points from j to i and

has weight matrix Wij ∈ Rni×nj , such that (j, i) ∈ E if

Wij = 0 element-wise. For each node i, we assume the self loop edge (i, i) always exists and has weight Wii ∈ Rni×ni . The composite matrix W ∈ Rn×n with entries Wij is the

adjacency matrix of the game graph.

On a game graph, we deﬁne a path p = (i, v1, . . . , vk−1, j)

as a sequence of nodes connected by edges. The set of paths

Pikj includes all paths starting at i and ending at j, traversing k + 1 nodes in total. For a path p = (i, v1, . . . , vk−1, j), we

deﬁne its path weight as the product of consecutive edges

on the path, given by Wj,vk−1 . . . Wv1,i =

k−1 l=0

Wvl+1 ,vl

.

In the absence of disturbances di, the update in (2) for a

quadratic game reduces to

xk+1 = W xk − Γr¯,

(5)

where r¯ = r1 . . . rN , Wii = Ini − γiPi, Wij = −γiPij, and Γ = blkdiag(γ1In1 , . . . , γN InN ).

D. Subclasses of games within quadratic games

To both illustrate the breadth of quadratic games and

provide exemplars of the game graph concept, we describe

two important subclasses of games and their game graphs.

1) Finite horizon LQ game: Given initial state z0 ∈ Rm

and horizon T , each player i in an N -player, ﬁnite-horizon

LQ

game

selects

an

action

sequence

(u

0 i

,

.

.

.

,

u

T i

−

1

)

with

uti ∈ Rmi in order to minimize a cumulative state and control

cost subjected to state dynamics:

min 21 Tt=0(zt) Qizt + Tt=−01(uti) Riuti

uti ∈Rmi

(6)

s.t. zt+1 = Azt +

N i=1

Bi uti ,

t = 0, . . . , T

− 1.

The LQ game deﬁned by the collection of optimization
problems (6) for each i ∈ [N ] is equivalent to a one-
shot quadratic game in which each player selects Ui = [(u0i ) , . . . , (uTi −1) ] ∈ Rni with ni = T mi, in order to minimize their cost fi(U ) deﬁned by

21 (

N j=1

Gj

Uj

+H

z0

)

Q¯i(

N j=1

Gj

Uj

+H

z0

)+

1 2

Ui

R¯iUi,

where U = (U1, . . . , UN ) is the joint action proﬁle, and the cost matrices are given by Q¯i = blkdiag{Qi, . . . , Qi},

0

... 0 

I 

 Bi Gi =  .

... .

0 . , H = 

...

,

(7)

 

..

..

..

 


T

AT −1Bi . . . Bi

A

and R¯i = blkdiag{Ri, . . . , Ri}. This follows precisely

from observing that the dynamics are equivalent to Z =

N i=1

GiUi

+ Hz0

where

Z

=

[(z0)

, . . . , (zT )

]

. From

here, it is straight forward to rewrite the optimization problem in (6) as minUi fi(U ). The LQ game is a potential game if and only if Qi = Qj and Ri = Rj for all i, j ∈ [N ].
LQ Game Graph. Suppose each player uses step size γi. Since, Difi(U ) is given by
(Gi Q¯iGi + R¯i)Ui + Gi Q¯i( j=i Gj Uj + Hz0), (8)
the learning dynamics (5) are equivalent to
U k+1 = W U k − Γ[Q¯1G1, . . . , Q¯N GN ] Hz0, (9)
where W = In − M , with M ∈ Rn×n a blockwise matrix having entries Mij = γiGi Q¯iGj if i = j and Mij = γi(Gi Q¯iGi + R¯i) otherwise.
2) Bilinear games: Bilinear games are an important class of games. For instance, a number of game formulations in adversarial learning have a hidden bilinear structure [17]. In evaluating and selecting hyper-parameter conﬁgurations in so-called test suites, pairwise comparisons between algorithms are formulated as bimatrix games [18], [19].
Formally, a two player bilinear game1, a subclass of continuous quadratic games, is deﬁned by f1(x1, x2) = x1 Ax2 and f2(x1, x2) = x1 B x2 where A ∈ Rn1×n2 and B ∈ Rn2×n1 and xi ∈ Rni . Common approaches to learning in games [17], [20], simultaneous and alternating gradient descent both correspond to a linear system.
Game graph for simultaneous gradient play. Players update their strategies simultaneously by following the gradient of their own cost with respect to their choice variable:
xk1+1 = xk1 − γ1Axk2 , xk2+1 = xk2 − γ2Bxk1 (10)

The simultaneous gradient play game graph is given by

Ws = I −γ1A .

(11)

−γ2B I

Game graph for alternating gradient play. In zero-sum bilinear games, it has been shown that alternating gradient play has better convergence properties [20]. Alternating gradient play is deﬁned by
xk1+1 = xk1 − γ1Axk2 , xk2+1 = xk2 − γ2Bxk1+1 (12)
Examining the second player’s update, we see that xk2+1 = (I + γ1γ2BA)xk2 − γ2Bxk1. The game graph in this case is deﬁned by

Wa = I

−γ1A .

(13)

−γ2B I + γ1γ2BA

Remark 1. Convergence of (10) and boundedness of (12) depend on choosing appropriate step sizes γ1 and γ2 [3], [20]. We consider disturbance decoupling for settings such as these where the undisturbed dynamics are convergent.

1The bilinear game formulation and corresponding game graph for different gradient-based learning rules easily extend to an N -player setting, however the results in Sec. IV are presented for two player games.

IV. DISTURBANCE DECOUPLING ON GAME GRAPH
In this section, we derive the necessary and sufﬁcient condition that ensures decoupling of gradient disturbance from the learning trajectory of a subset of players. We emphasize that the condition holds for disturbances with arbitrary magnitudes and functions. This is a useful result because it provides guarantees on both the equilibrium behavior and the learning trajectory under adversarial disturbance.

Deﬁnition 3 (Complete disturbance decoupling). Given initial joint action x0 ∈ Rn, game costs (f1, . . . , fN ), step sizes Γ ∈ Rn×n, suppose that player i’s gradient update
is corrupted as in (3), then for player j = i, action xj is decoupled from the disturbance in player i’s gradient if the
uncorrupted and corrupted dynamics, given respectively by

xk+1 = W xk − Γr¯, yk+1 = W yk − Γr¯ − Γdk (14)

result in identical trajectories for player j when y0 = x0. That is, yjk = xkj holds for all k ≥ 0, dk ∈ Di, where
Di = {d = [d1, . . . , dN ] ∈ Rn | dj = 0, ∀ j = i}.

A. Algebraic condition
We ﬁrst derive an algebraic condition on the joint action space for disturbance decoupling. Deﬁne M⊥ = {x ∈ Rn | x x˜ = 0, ∀ x˜ ∈ M} and let im(A) = {Ax | x ∈ Rn} denote the image of A ∈ Rm×n.

Proposition 1. Consider an N -player quadratic game
(f1, . . . , fN ) as in Deﬁnition 2 under learning dynamics as given by (2), where player i experiences gradient disturbance as given by (3). Let S(i) = {x = [x1, . . . , xN ] ∈ Rn | xj = 0, ∀ j = i} be the joint action subset. For player j = i, the
following statements are equivalent:
(i) Player j is disturbance decoupled from player i. (ii) W kv ∈ S(j)⊥, ∀ v ∈ S(i), ∀ 0 ≤ k < n. (iii) im(W kE) ⊆ im(Y ), ∀ 0 ≤ k < n, where E ∈ Rn×ni
and Y ∈ Rn×(n−nj) are matrices such that im(E) = S(i) and im(Y ) = S(j)⊥.

Proof. For a quadratic game (f1, . . . , fN ), the learning dy-
namics without and with disturbances reduce to the equations in (14). Given initial joint action x0,

xk = W kx0 − W k−1 . . . W 0 Γ r¯ . . . , r¯ , yk = xk − W k−1 . . . W 0 Γ (d0) . . . , (dk−1) .

Then, Deﬁnition 3 is equivalent to M l=−0 1 W M−l−1dl ∈ S(j)⊥ satisﬁed for M ≥ 1 and dl ∈ S(i). Since the condition

holds for all M ≥ 1, it is equivalent to W kdl ∈ S(j)⊥

for all k ≥ 0 and dl ∈ S(i). This is then equivalent

to W kdl ∈ S(j)⊥ for all 0 ≤ k < n and dl ∈ S(i).

To see this equivalence, consider the following result from

Cayley-Hamilton theorem, W k =

n−1 l=0

αlW

l

for

some

αl ∈ R. Thus, for k ≥ n and any d ∈ S(i), W kd =

n−1 l=0

W

lαld

=

n−1 l=0

W

ldˆl

where

dˆl

=

αld

∈

S (i)

for

l = 0, . . . , n − 1, which implies that W kd ∈ S(j)⊥. This

concludes the equivalence.

Finally, we note that (iii) is a restatement of (ii). Furthermore, (iii) can be veriﬁed in polynomial time.

Remark 2. In connection to geometric control theory, con-
dition (iii) of Proposition 1 is equivalent the fact that im([E, . . . , W n−1E]), the smallest W -invariant subspace containing im(E), must be a subset of S(j)⊥ [12, Thm 4.6].

B. Graph-theoretic condition
Next we derive the graph-theoretic condition on the joint action space for disturbance decoupling.

Theorem 1. Consider an N -player quadratic game (f1, . . . , fN ) as in Deﬁnition 2 under learning dynamics as given by (2), where player i experiences gradient disturbance as given by (3). Player j = i is disturbance decoupled if and only if the path weights of paths with length k satisfy

k−1
Wvl+1,vl = 0, ∀ 0 < k < n,
p∈Pikj l=0

(15)

where (vl, vl+1) denotes consecutive nodes on path p = (i, v1, . . . , vk−1, j).

Proof. The result follows from equivalence between Propo-
sition 1 condition (ii) and (15). Note that x ∈ S(i) is equivalent to x = 0 for all = i, and W kx ∈ S(j)⊥ is equivalent to (W kx)j = 0 for all n > k ≥ 0. We prove the result by induction. For k = 0, (W 0x)j = 0 ∀ x ∈ S(i) holds if and only if i = j. For k > 0, (W kx)j = 0 ∀ x ∈ S(i) is equivalent to i = j and (W k)ji = 0. Suppose that for i, j ∈ [N ], (W k)ji is the sum of path weights over all paths of length k, originating at i and ending at j, then (W k+1)ji is the sum of path weights over all paths of length k + 1, originating at i and ending at j. Let W k = M , then (W k+1)ji = q∈[N] MjqWqi, where MjqWqi is the sum of path weights over all paths of length k + 1 from i to j each
of which contains v1 = q. Since we sum over q ∈ [N ], we conclude that (W k+1)ji is the sum of all paths weights of length k + 1 from i to j, i.e., (i, v1, . . . , vk, j) ∈ Pikj+1.
The concept of disturbance decoupling is quite counter-
intuitive: any change in player i’s action does not affect
player j’s action, despite fj being implicitly dependent on xi through the network of player cost functions. As we see
from the proof of Theorem 1, this situation arises when the
dependencies ‘cancel’ each other out, i.e. the sum of path
weights from i to j is always zero for equally lengthed paths.

Example 1 (Disturbance decoupled players). Consider a
4 player quadratic game where xi ∈ R and the game graph is given by Figure 1. Edge weights α, β, γ, and
δ ∈ R, while each self loop has weight wi > 0. Paths of length k ≤ 4 from player 1 to player 4 are enumerated as P114 = {∅}, P124 = {(1, 2, 4), (1, 3, 4)}, and P134 = {(1, 1, 2, 4), (1, 1, 3, 4), (1, 2, 2, 4), (1, 3, 3, 4) , (1, 2, 4, 4), (1, 3, 4, 4)}. To satisfy Theorem 1, the sum of path weights for each P1k4 must be 0 for 0 < k < 4. There are no paths of length one, summation for k = 2 implies the
criteria αγ + βδ = 0, and summation for k = 3 implies

Fig. 1: A simple game graph between four players

the criteria (w1 + w2 + w4)αγ + (w1 + w3 + w4)βδ = 0. If w2 = w3, αγ + βδ = 0 is necessary and sufﬁcient for disturbance decoupling between player 1 and player 4.

Remark 3. Disturbance decoupling is a structural property of the game in terms of disturbance propagation and attenuation. An open research problem is linking this structural property to robust decision making under uncertainties in cost parameters Pi, Pij and step sizes γi.

The following corollary specializes to the class of potential games [15], which arise in many applications [21]–[23].

Corollary 1. Consider an N -player quadratic potential game under learning dynamics as given by (2), where player i experiences gradient disturbance as given by (3). Player i is disturbance decoupled from player j = i if and only if player j is also disturbance decoupled from player i.

Proof. In a potential game graph, Wij = γγji Wji . Therefore, a path p with path weight Wj,vk−1 . . . Wv1,i is equivalent to

γj

γvk−1

γv1

γv Wvk−1j γv Wvk−2vk−1 . . . γi Wi,v1

k−1

k−2

γj = γi Wi,v1 . . . Wvk−1j ,

where γj scales all paths weights from i to j. Since γj, γi >

0,

γj

γi
> 0.

Therefore,

(15)

holds

from

player

i

to

player

j

if

γi

and only if it holds from player j to player i.

Corollary 2. Consider an N -player ﬁnite horizon LQ game as in (6) under learning dynamics as given by (9), where player i experiences gradient disturbance as given by (3), if disturbance decoupling holds between player j and gradient disturbance from player i, then

 Bj  ...

  Qj Bi · · · AT −1Bi = 0. (16) 

Bj (A )T −1

If Qj is positive deﬁnite and T ≥ m, the controllable subspace of (A˜, B˜i) must lie in the unobservable subspace of (B˜j , A˜ ) where A˜ = Q1j/2AQ−j 1/2, B˜i = Q1j/2Bi, and
B˜j = Q1j/2Bj .

Proof. For player j to be disturbance decoupled from player i, edge (i, j) cannot exist, i.e. −γjGj Q¯jGi = 0 from (7). Expanding Gj Q¯j Gi = M ∈ Rnj×ni , Mpq ∈ Rmj×mi is given by Tt=−m1in{p,q}Bj (A )t−pQj At−qBi. We unwrap
these conditions starting from p = T − 1, q = T − 1; in this

case Mpq = Bj QjBi = 0 is necessary. Then we consider
MT −2,T −2 = Bj A Qj ABi + Bj Qj Bi = 0, which implies
that Bj A QjABi is necessary. Subsequently, this implies that all Bj (A )tQjAtBi = 0 is necessary for t ∈ [0, T ). Similarly, we note that MT −1,q = Bj QjAqBi = 0 and Mp,T −1 = Bj (A )pQjBi = 0. From these we can use the rest of M to conclude that Bj (A )pQjAqBi = 0 for any
p, q ∈ [0, T ). This condition is equivalent to (16).

We apply Theorem 1 to two player bilinear games and prove a necessary condition for disturbance decoupling between different coordinates of each player’s action space that is independent of players’ step sizes.

Corollary 3. Consider a two player bilinear game under

learning dynamics (10) and (12), where coordinates x1,i

and x2,i experience gradient disturbance as given by (3).

If j = i and coordinate x1,j is disturbance decoupled

from coordinate x1,i, (A, B) must satisfy

n2 =1

b

iaj

= 0,

where apq and bpq denote the (p, q)th elements of A and

B, respectively. Similarly, if j = i and coordinate x2,j is

disturbance decoupled from coordinate x2,i, (A, B) must

satisfy

n1 =1

bj

a

i

=

0.

Proof. We construct games played by n1 + n2 players with

actions {x1,1, . . . , x1,n1 , x2,1, . . . , x2,n2 } and whose game

graphs are identical to Ws (11) and Wa (13). First consider

disturbance decoupling of x1,j from x1,i. In both learning

dynamics, {x1,1, . . . , x1,n1 } do not have any edges between

players. Therefore, paths between x1,i and x1,j with length 2

is given by P = {(x1,i, x2, , x1,j) | ∈ [n2]}. We sum path

weights over P to obtain

n2 =1

b

iaj

= 0 for disturbance

decoupling of x1,j from x1,i in (10) and (12). A similar

argument follows for disturbance decoupling of x2,j from

x2,i in (10). For disturbance decoupling of x2,j from x2,i

in (12), we note that a edge from x2,i to x2,j exists with

weight γ1γ2(BA)ji when j = i. Disturbance decoupling

requires γ1γ2(BA)ji = 0, therefore

n1 =1

bj

a

i = 0.

Corollary 4. Consider a two player bilinear game under

learning dynamics (10) and (12), where coordinates x1,i and

x2,i experience gradient disturbance as given by (3). If co-

ordinate x2,j is disturbance decoupled from coordinate x1,i,

(A, B) must satisfy bji = 0 and

n2 q=1

bqi

n1 =1

a

q bj

= 0,

where apq and bpq denote the (p, q)th elements of A and

B, respectively. If coordinate x1,j is disturbance decoupled

from coordinate x2,i, (A, B) must satisfy aji = 0 and

n1 q=1

aqi

n2 =1

b

q aj

= 0.

Proof. We construct games played by n1 + n2 players with actions {x1,1, . . . , x1,n1 , x2,1, . . . , x2,n2 } and whose game graphs are identical to Ws (11) and Wa (13). In both learning dynamics, disturbance decoupling requires no direct path
between the decoupled players. Therefore aji = 0 or bji = 0. Consider disturbance decoupling of x1,j from x2,i in (10),
paths of length 3 from x2,i to x1,j without self loops is given by P = {(x2,i, x1,q, x2, , x1,j) | q ∈ [n1], ∈ [n2]}. A path of length 3 with self loops must also include (x2,i, x1,j), whose weight is 0. We sum path weights over p ∈ P to

obtain

n1 q=1

aqi

n2 =1

b

q aj

= 0. A similar argument is

made for disturbance decoupling of x2,j from x1,i in (10).

Consider disturbance decoupling of x2,j from x1,i in (12),

paths of length 2 from x1,i to x2,j without self loops is

given by Q = {(x1,i, x2,q, x2,j) | q ∈ [n2]}. A path of

length 2 with self loops must also include (x1,i, x2,j), whose

weight is 0. Weight of (x2,q, x2,j) is given by γ1γ2(BA)jq

= γ1γ2

n1 =1

bj

a

q. We sum path weights over p

∈

Q to ob-

tain

n2 q=1

bqi

n1 =1

a

q bj

= 0. A similar argument is made

for disturbance decoupling of x1,j from x2,i in (12).

V. NUMERICAL EXAMPLE

We provide an example of disturbance decoupling in a LQ

game. Consider a tug-of-war game in which a single target

z ∈ R2 is controlled by four players. We assume that player i can move z along vector Bi ∈ R2 by ui ∈ R, and that z is

stationary without any player input, i.e., A = I. Starting with

a randomized initial condition z0, at each step t, the target

moves according to the dynamics zt+1 = zt +

4 i=1

Biuti

where B1 = [1, 0] , B2 = [ √12 , √12 ] , B3 = [ √−12 , √12 ] ,

B4 = [0, 1] . Each player i’s cost function is given by

21 z9 − ci 22 +

81 t=0 2

zt − ci 22 + 10

uti

2 2

which describes player i’s objective to move target z towards ci ∈ R2 in a ﬁnite time T = 10 by using minimal amount
of control. By designing the game dynamics to satisfy
Theorem 1, we ensure that player 4’s action is disturbance
decoupled from player 1’s.
Using the equivalent formulation as described in Section III-D.1, Difi(U ) = (Gi Q¯iGi + R¯i)Ui +
j=i Gi Q¯i(GjUj + Hz0 − Ci) where Ci = [ci , . . . , ci ] . Hence, the learning dynamics are U k+1 = W U k + ΓQ¯i[G1, . . . , GN ] [(Hz0 − C1) , . . . , (Hz0 − CN ) ] , where Wij = Gi Q¯iGj = E ⊗ Bi Bj with B1 B2 = B1 B3 = B2 B4 = √12 , B2 B3 = B1 B4 = 0, B3 B4 = − √12 , B1 B1 = B2 B2 = B3 B3 = B4 B4 = 1, and

9 8 7 . . . 1
8 8 7 . . . 1 E = 7 7 7 . . . 1 ∈ R9×9.
 ... . . . 1

1

...

1

To ensure convergence of the undisturbed learning dy-

namics [3], we use uniform step size√s such that Γ = blkdiag(γ1I, . . . , γ4I) with γi = βα , where α = λmin 14 (W + W ) (W + W ) and β = λmax W W with λmax(·) and λmin(·) denoting the maximum and mini-

mum eigenvalues of their arguments, respectively. The asso-

ciated game graph is given in Figure 1, where α = β = γ =

√12 E and δ = − √12 E. A path p = (1, v1, . . . , vk−1, 4) of

length k must have path weight ( √−1 )mδ ( √1 )mγ Ek, where

2

2

mδ (mγ) denotes the number of times the edge with weight

δ (γ) is traversed in p.

Disturbance decoupling between players 1 and 4 is guar-

anteed if all paths of length k ∈ (0, 36) satisfy (15). We

Fig. 2: Left: Trajectory of z with and without disturbances. Players’ preferred destinations are given by triangles. Top right: Players’ game costs during learning. Bottom right: Players’ control error as a function of disturbance magnitude.

can numerically verify that Proposition 1 is satisﬁed or

make the following graph-theoretic observations based on

Theorem 1. First, due to the symmetry within the game

graph, the existence of path p = (1, v1, . . . , vk−1, 4) with

path weight L = ( √−1 )mδ ( √1 )mγ Ek implies the existence

2

2

of path pˆ = (1, vˆ1, . . . , vˆk−1, 4) with path weight Lˆ =

( √−12 )mˆ δ ( √12 )mˆ γ Ek, where mγ = mˆ δ and mδ = mˆ γ .

Second, since edges (3, 4) and (2, 4) form a cut between

player 1 and player 4 in the game graph, any path between

them has the property that mγ + mδ is odd. From these observations, we can conclude that L = −Lˆ. Since each

path p of length k and weight L can be paired with path pˆ of equivalent length k and weight Lˆ = −L, we conclude

that all path sets P1k4 where k > 0 must satisfy Theorem 1.

To numerically verify disturbance decoupling, we simu-

late the uncorrupted learning trajectory of z, shown in the

left plot of Figure 2 in purple. We then inject a random

disturbance into player 1’s gradient updates as given by (3)

with increasing magnitude, and observe its effects on each

player’s action. A sample corrupted trajectory is shown in

the left plot of Figure 2 in brown. In the bottom right plot of

Figure 2, we show the total error in each player’s action from

to the uncorrupted optimal action. We observe that player 4

does not deviate from the optimal action, while player 1’s

action error increases as the disturbance magnitude increases.

We note that these results hold despite the fact that gradient-

based learning no longer converges. In the top right plot of

Figure 2, individual player costs are compared in one round

of gradient-based learning where di ≤ 50 is injected.

Interestingly, despite action remaining uncorrupted, player

4’s cost is disturbance affected. Note that the disturbance

decoupling in actions does not necessarily imply disturbance

decoupling in costs.

VI. CONCLUSION
In this paper, we investigated and characterized the effects of gradient disturbances on an N –player gradient-based learning dynamics. For quadratic games, we deﬁned distur-

bance decoupling for arbitrary disturbances, and showed the
cost coupling structure is crucial in facilitating decoupling
individual player’s action from input disturbance. Our future
work aims to leverage these analysis results to design incen-
tives for players to ensure disturbance decoupling.
REFERENCES
[1] D. Fudenberg, F. Drew, D. K. Levine, and D. K. Levine, The theory of learning in games. MIT press, 1998, vol. 2.
[2] E. Mazumdar, L. J. Ratliff, and S. S. Sastry, “On the convergence of gradient-based learning in continuous games,” SIAM J. Mathematics of Data Science, 2019.
[3] B. Chasnov, L. J. Ratliff, E. Mazumdar, and S. Burden, “Convergence analysis of gradient-based learning in continuous games,” in Proc. 35th Conf. Uncertainty Artif. Intell. (UAI), 2019.
[4] E. Mazumdar, L. J. Ratliff, M. I. Jordan, and S. S. Sastry, “Policygradient algorithms have no guarantees of convergence in continuous action and state multi-agent settings,” Int. Conf. Autonomous Agents and Multi-Agent Systems (AAMAS), 2020.
[5] S. Shalev-Shwartz, O. Shamir, and S. Shammah, “Failures of gradientbased deep learning,” in Int. Conf. Machine Learning. JMLR, 2017, pp. 3067–3075.
[6] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell, “Robust multiagent reinforcement learning via minimax deep deterministic policy gradient,” in AAAI Conf. Artif. Intell., 2019.
[7] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in Proc. Computational Statistics. Springer, 2010, pp. 177– 186.
[8] Q. Jiao, H. Modares, S. Xu, F. L. Lewis, and K. G. Vamvoudakis, “Multi-agent zero-sum differential graphical games for disturbance rejection in distributed control,” Automatica, vol. 69, pp. 24–34, 2016.
[9] L. J. Ratliff, S. A. Burden, and S. S. Sastry, “On the characterization of local nash equilibria in continuous games,” IEEE Trans. Autom. Control, vol. 61, no. 8, pp. 2301–2307, Aug. 2016.
[10] J. S. Shamma and G. Arslan, “Dynamic ﬁctitious play, dynamic gradient play, and distributed convergence to nash equilibria,” IEEE Trans. Autom. Control, vol. 50, no. 3, pp. 312–327, 2005.
[11] Z. Zhou, P. Mertikopoulos, A. L. Moustakas, N. Bambos, and P. Glynn, “Mirror descent learning in continuous games,” in Proc. 56th IEEE Conf. Decision Control, Dec 2017, pp. 5776–5783.
[12] H. L. Trentelman, A. A. Stoorvogel, and M. Hautus, Control theory for linear systems. Springer Science & Business Media, 2012.
[13] W. M. Wonham, “Linear multivariable control,” in Optimal control theory and its applications. Springer, 1974, pp. 392–424.
[14] J.-M. Dion, C. Commault, and J. Van Der Woude, “Generic properties and control of linear structured systems: a survey,” Automatica, vol. 39, no. 7, pp. 1125–1144, 2003.
[15] D. Monderer and L. S. Shapley, “Potential games,” Games Econ. Behav., vol. 14, no. 1, pp. 124–143, 1996.
[16] D. B. Gillies, “Solutions to general non-zero-sum games,” Contributions to the Theory of Games, vol. 4, pp. 47–85, 1959.
[17] E.-V. Vlatakis-Gkaragkounis, L. Flokas, and G. Piliouras, “Poincare´ recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave zero-sum games,” in Adv. Neural Inf. Process. Syst., 2019, pp. 10 450–10 461.
[18] D. Balduzzi, K. Tuyls, J. Perolat, and T. Graepel, “Re-evaluating evaluation,” in Adv. Neural Inf. Process. Syst., 2018, pp. 3268–3279.
[19] D. Balduzzi, W. M. Czarnecki, T. W. Anthony, I. M. Gemp, E. Hughes, J. Z. Leibo, G. Piliouras, and T. Graepel, “Smooth markets: A basic mechanism for organizing gradient-based learners,” in Int. Conf. Representation Learning (ICRL), 2020.
[20] J. P. Bailey, G. Gidel, and G. Piliouras, “Finite regret and cycles with ﬁxed step-size via alternating gradient descent-ascent,” arXiv preprint arXiv:1907.04392, 2019.
[21] D. Paccagnan, B. Gentile, F. Parise, M. Kamgarpour, and J. Lygeros, “Distributed computation of generalized Nash equilibria in quadratic aggregative games with afﬁne coupling constraints,” in Proc. IEEE 55th Conf. Decision and Control, 2016, pp. 6123–6128.
[22] B. Lutati, V. Levit, T. Grinshpoun, and A. Meisels, “Congestion games for v2g-enabled ev charging,” in AAAI Conf. Artif. Intell., 2014.
[23] T. Alpcan and T. Bas¸ar, Network security: A decision and gametheoretic approach. Cambridge University Press, 2010.

