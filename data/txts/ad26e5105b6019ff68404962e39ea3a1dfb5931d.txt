Incentive Design for Temporal Logic Objectives
Yagiz Savas, Vijay Gupta, Melkior Ornik, Lillian J. Ratliff, Ufuk Topcu

arXiv:1903.07752v1 [math.OC] 18 Mar 2019

Abstract— We study the problem of designing an optimal sequence of incentives that a principal should offer to an agent so that the agent’s optimal behavior under the incentives realizes the principal’s objective expressed as a temporal logic formula. We consider an agent with a ﬁnite decision horizon and model its decision-making process as a Markov decision process (MDP). Under certain assumptions, we present a polynomialtime algorithm to synthesize an incentive sequence that minimizes the cost to the principal. We show that if the underlying MDP has only deterministic transitions, the principal can hide its objective from the agent and still realize the desired behavior through incentives. On the other hand, an MDP with stochastic transitions may require the principal to share its objective with the agent. Finally, we demonstrate the proposed method in motion planning examples where a principal changes the optimal trajectory of an agent by providing incentives.
I. INTRODUCTION
Consider a scenario where a principal provides incentives to an agent so that the optimal behavior of the agent under the provided incentives satisﬁes the principal’s objective. If the principal had enough resources to provide arbitrarily large incentives, it would be straightforward to obtain the desired agent behaviour. However, since the resources are limited in practice, it is important to establish the minimum amount of incentives that leads to the desired behavior. In this paper, we are interested in designing a sequence of incentives that minimizes the cost to the principal while guaranteeing the realization of its objective by the agent with maximum probability.
We model the sequential decision-making process of the agent as a Markov decision process (MDP) [1], and assume that the agent’s objective is to maximize its expected total reward at the end of a ﬁnite planning horizon. Although each planning horizon is ﬁnite, the agent plans its future decisions inﬁnitely many times. Examples of such an agent can be a person who plans her schedule on a weekly basis or an autonomous system with a limited computational power which plans its route by considering only a small subset of all possible environment states.
The principal’s objective is described by a syntactically cosafe linear temporal logic (LTL) formula. LTL speciﬁcations are widely used to describe complex tasks for autonomous
Y. Savas and U. Topcu are with the Department of Aerospace Engineering, University of Texas at Austin, TX, USA. E-mail: {yagiz.savas, utopcu}@utexas.edu
V. Gupta is with the Department of Electrical Engineering, University of Notre Dame, IN, USA. E-mail: vgupta@nd.edu
M. Ornik is with the Department of Aerospace Engineering and the Coordinated Science Laboratory, University of Illinois at UrbanaChampaign, IL, USA. E-mail: mornik@illinois.edu
L. J. Ratliff is with the Department of Electrical Engineering, University of Washington, WA, USA. E-mail: ratlifﬂ@uw.edu

robots [2], design security protocols [3] and check the reliability of software [4]. For example, in a navigation scenario, syntactically co-safe LTL formulae allow one to specify tasks such as liveness (eventually visit the region A) or priority (ﬁrst visit the region A and then B).
We assume that the principal is aware of the agent’s reward function and the length of its planning horizon. In many real-world applications, the decision horizon and the reward structure of an agent can be known or at least inferred through observations. For example, a manufacturing company is generally interested in maximizing its proﬁt at the end of a ﬁscal year, and an autonomous car aims to reach its destination within certain time interval.
From a practical point of view, an interesting question is whether an adversarial principal can convince an agent to satisfy its objective through incentives. In such a scenario, if the agent knows the principal’s objective explicitly, it will reject the provided incentives because the resulting behavior under the incentives will serve to the beneﬁt of the enemy. However, if the principal can design an incentive sequence without sharing its objective with the agent, then the incentives may lead to the desired agent behavior. Therefore, it is important to establish the conditions under which the principal can actually hide its objective from the agent.
The contributions of this paper can be summarized as follows. First, we present an algorithm, based on a series of linear optimization problems, to synthesize a sequence of incentives that minimizes the cost to the principal while ensuring that the optimal agent behavior under the provided incentives satisﬁes a syntactically co-safe LTL formula with maximum probability. Second, we present an example scenario where the principal has to share its objective with the agent to induce the desired behavior. Third, we provide sufﬁcient conditions on the structure of the MDP and the length of the agent’s decision horizon under which there exists an optimal incentive design that allows the principal to hide its objective from the agent. Related work. The problem of obtaining desired agent behavior through a sequence of incentives has been extensively studied in the literature. In [5] and [6], the authors present methods to design incentive sequences with limited resources that maximizes the value of the principal’s objective function. They employ techniques from inverse reinforcement learning literature and prove NP-hardness of the considered design problem [5]. The work [7] provides a polynomial-time algorithm to synthesize minimum incentives for inducing a speciﬁc agent policy. Reference [8] considers a bandit model and presents methods to induce desired agent actions under different constraints on the incentives. Although it is quite

different from the problem considered here, the design of feasible incentives that aligns the objectives of an agent and a principal is discussed in [9] from a control theoretic perspective. Unlike the references mentioned above, in this paper, we consider the problem of designing minimum incentives that maximizes the value of the principal’s objective function expressed as a temporal logic formula. We also note that establishing the complexity of the design problem considered in this paper is mentioned as an open problem in [5].
II. PRELIMINARIES
For a set S, we denote its power set and cardinality by 2S and |S|, respectively. Additionally, N={1, 2, . . .}, N0={0, 1, 2, . . .} and R≥0=[0, ∞).
A. Markov Decision Processes
Deﬁnition 1: A Markov decision process (MDP) is a tuple M=(S, s0, A, P, AP, L, R) where S is a ﬁnite set of states, s0∈S is an initial state, A is a ﬁnite set of actions, P:S×A×S→[0, 1] is a transition function such that
s′∈S P(s, a, s′)=1 for all s∈S and a∈A(s) where A(s) denote the available actions in s, AP is a set of atomic propositions, L:S→2AP is a function that labels each state with a subset of atomic propositions, and R:S×A→R is a reward function.
We denote the transition probability P(s, a, s′) by Ps,a,s′.

Deﬁnition 2: For an MDP M, a decision rule d:S×A→[0, 1] is a function such that a∈A(s) d(s, a)=1 for all s∈S. A decision rule d is said to be deterministic if for all s∈S there exists a∈A(s) such that d(s, a)=1, and randomized otherwise. For an MDP M, we denote the set of all (deterministic) decision rules by (DD(M)) D(M).
For an MDP M, a decision-maker, i.e., an agent, chooses a decision rule d∈D(M) at each stage.

Deﬁnition 3: An N -stage policy for an MDP M is a se-
quence π=(d1, d2, . . . , dN ) where N ≤∞ and dt∈D(M) for all t≤N . A stationary policy is a policy such that dt=d1 for all t≤N . A policy is said to be deterministic if dt∈DD(M) for all t, and randomized otherwise. For an MDP M, we
denote the set of all N -stage policies by ΠN (M). For notational simplicity, we denote the set of ∞-stage policies
by Π(M). For an MDP M and a policy π∈Π(M), let µπt (s, a) be the
joint probability of being in state s∈S and taking the action
a∈A(s) at stage t, which is uniquely determined through the
recursive formula

µπt+1(s′, a′) =

P

s,

a

,

s′

µ

π t

(s

,

a

)dt+

1

(s

′

,

a

′

)

(1)

s∈S a∈A(s)

where

µ

π 1

(s

,

a

)=

d1

(s

,

a

)µ

0

(s

)

and

µ0:S→{0, 1}

is

a

func-

tion such that µ0(s0)=1 and µ0(s)=0 for all s∈S\{s0}.

Deﬁnition 4: For an MDP M and a policy π∈Π(M), the

expected residence time in a state-action pair (s, a) is

∞

ξπ(s, a) :=

µ

π t

(s

,

a

).

(2)

t=1

An inﬁnite sequence ̺π=s0s1s2 . . . of states generated in M under a policy π∈Π(M), which starts from the initial state s0 and satisﬁes at∈A(st) dk(st, at)Pst,at,st+1 >0 for all t≥0, is called a path. Any ﬁnite preﬁx of ̺π a ﬁnite
path fragment. We deﬁne the set of all paths and ﬁnite path fragments in M under the policy π by P athsπ(M) and P athsπfin(M), respectively. We use the standard probability measure over the outcome set P athsπ(M) [10].

Deﬁnition 5: An incentive design for an MDP M is a se-
quence Γ=(γ1, γ2, . . .) where γt:S×A→R≥0. A stationary incentive design is a design such that γt=γ1 for all t∈N. For an MDP M, we denote the set of all incentive designs by
Θ(M).

B. Linear temporal logic
We consider syntactically co-safe linear temporal logic (scLTL) formulae to specify tasks and refer the reader to [10], [11] for the syntax and semantics of scLTL.
An scLTL formula is built up from a set AP of atomic propositions, logical connectives such as conjunction (∧) and negation (¬), and temporal modal operators such as until (U) and eventually (♦). An inﬁnite sequence of subsets of AP deﬁnes an inﬁnite word, and an scLTL formula is interpreted over inﬁnite words on 2AP . We denote by w|=ϕ that a word w=w0w1w2 . . . satisﬁes an scLTL formula ϕ.
For an MDP M under a policy π, a path ̺π=s0s1 . . . generates a word w=w0w1 . . . where wk=L(sk) for all k≥0. With a slight abuse of notation, we use L(̺π) to denote the word generated by ̺π. For an scLTL formula ϕ, the set {̺π∈P athsπ(M):L(̺π)|=ϕ} is measurable [10]. Hence, we deﬁne
PrπM(s0 |= ϕ) := PrπM{̺π ∈ P athsπ(M) : L(̺π) |= ϕ}
as the probability of satisfying the scLTL formula ϕ for an MDP M under the policy π∈Π(M).

III. PROBLEM STATEMENT

We consider an agent whose sequential decision-making process is modeled as an MDP M, and a principal that provides the agent a sequence of incentives Γ∈Θ(M).
The agent’s objective is to maximize its expected total reward after N stages. However, since the incentive sequence
offered by the principal might be non-stationary, the agent computes an N -stage policy every N stages. A graphical illustration of the agent’s planning method is shown in Fig. 1. Formally, let N ∈N be a constant, and R(St, At) and γt(St, At) be the random reward and incentive received in stage t≤N . Additionally, let J:=(J0, J1, . . .) be a sequence of objective functions where Jk:ΠN (M)×Θ(M)→R|S| is such that

Jk(π, Γ)(s) := Eπs

N
(R(St, At) + γkN+t(St, At))
t=1

for all s∈S where the expectation is taken over the ﬁnite path fragments that are generated by the policy π∈ΠN (M) and start from the state s. Then, for a given incentive design

Offers (γ1, . . . , γN )

Offers (γN+1, . . . , γ2N )

1

Implements π⋆ N

Implements π⋆ 2N

0

1

for N stages

for N stages

Computes π0⋆

Computes π1⋆

Fig. 1: An illustration of the incentive implementation and the agent’s decision-making process. The principal offers incentives for the next N stages. After receiving the incentive offers, the agent computes and implements its optimal decisions for the next N stages.

Γ∈Θ(M), the agent’s optimal ∞-stage policy is given by π⋆:=(π0⋆, π1⋆, . . .) where πk⋆ is such that

πk⋆ ∈ arg max Jk(π, Γ)(s)

(3)

π∈ΠN (M)

for all s∈S and k∈N0. Note that the agent’s policy πk⋆ maximizes the total reward starting from any s∈S.
The principal’s objective is to design an incentive sequence such that the agent’s optimal policy under the provided incentives satisﬁes an scLTL formula ϕ with maximum probability.
The problem that we consider is the synthesis of an incentive design that minimizes the cost to the principal while realizing its objective. We make the following assumptions:
(i) Agent’s reward function R is known by the principal. (ii) Agent’s decision horizon N is known by the principal. (iii) The principal pays the offered incentives if and only if
the agent takes the incentivized action.

Then, the optimization problem that we are interested in to solve is the following:

∞

min Eπs ⋆

γt(s, a)

(4a)

Γ∈Θ(M)

0

t=1

subject to: π⋆ = (π0⋆, π1⋆, . . .)

(4b)

πk⋆ ∈ arg max Jk(π, Γ)(s) ∀s ∈ S, ∀k ∈ N0
π∈ΠN (M)

(4c)

PrπM⋆ (s0 |= ϕ) = max PrπM(s0 |= ϕ) (4d)
π∈Π(M)

where Γ=(γ1, γ2, . . .).

IV. THE DESIGN OF INCENTIVE SEQUENCES
In this section, we provide a method to synthesize an
incentive design that solves the problem (4a)-(4d). For sim-
plicity, we restrict our attention to reachability speciﬁcations, i.e., ϕ=♦p where p∈AP. The incentive design for general scLTL speciﬁcations is discussed in Section VI.
We ﬁrst partition the states into three disjoint sets as follows. Let B⊆S be the set of all states such that {p}⊆L(s), i.e., the set of states that the principal wants the agent to reach, and S0⊆S be the set of states that have zero probability of reaching the states in B under any policy. More precisely, s∈S0 if PrπM(s |= ♦p)=0 for all π∈Π(M). Finally, we let Sr=S\B ∪ S0 be the set of all states that

are not in B and have nonzero probability of reaching a state in B under some policy. These sets can be found in
time polynomial in the size of the MDP using graph search
algorithms [10]. The agent’s initial state s0∈S can belong to either B,
S0 or Sr. However, we only consider the case s0∈Sr since otherwise the optimal incentive design is trivially γt(s, a)=0 for all t∈N.

A. The cost of control

Recall that the agent’s ﬁrst objective function J0:ΠN (M)×Θ(M)→R|S| is

N
J0(π, Γ)(s) = Eπs (R(St, At) + γt(St, At))
t=1
for all s∈S. Let Vn:S→R be the agent’s value function at stage n such that

N

Vn(s) := max Eπs (R(St, At) + γt(St, At))

π∈ΠN (M)

t=n

for all s∈S, where the expectation is taken over the paths that occupy s at stage n. Then, we have the recursive formula

Vn(s) = max R(s, a) + γn(s, a) + Ps,a,s′ Vn+1(s′)

a∈A(s)

s′ ∈S

for all 1≤n≤N , where VN+1(s)=0 for all s∈S. Let Qn:S×A→R be the agent’s Q-function at stage n such that

Qn(s, a) := R(s, a) + γn(s, a) + Ps,a,s′ Vn+1(s′).
s′ ∈S

By the principle of optimality [1], [12], the agent’s optimal policy π0⋆=(d⋆1, d⋆2, . . . , d⋆N ) is such that, for all 1≤n≤N , dn(s, a′)>0 only if

a′ ∈ arg max Qn(s, a).
a∈A(s)

We recursively deﬁne

Qn(s, a) := R(s, a) + Ps,a,s′ V n+1(s′), (5)
s′ ∈S

V n(s) := max Qn(s, a),

(6)

a∈A(s)

for all s∈S and a∈A(s). For a given ǫ≥0, we ﬁnally deﬁne a real-valued function φǫt:S × A→R≥0 such that

φǫn(s, a) := V0 n(s) − Qn(s, a) + ǫ oifthse∈rwSisre,. a ∈ A(s)

For an arbitrarily small ǫ>0, the value of φǫn(s, a), referred as the cost of control for the state-action pair (s, a), is
the minimum incentive that should be offered to the agent
in order to make the action a∈A(s) uniquely optimal at
stage t. It is worth noting that although the cost of control φǫn(s, a) depends on the stage number n, it is independent of the objective number, i.e., it is the same for all Jk. This is because the agent’s reward function R is stationary,
and therefore, V n(s) and Qn(s, a) do not change with the objective number k as can be seen from (5)-(6).

B. An ǫ-optimal incentive design
To synthesize the minimum incentive sequence, we should specify the actions to be incentivized by the principal at each state for each stage. To this aim, we modify the MDP M by considering the agent’s decision horizon N as another dimension in the state-space.

Deﬁnition 6: For an MDP M and T ={1, 2, . . . , N }, the expanded MDP is a tuple M=(S, s0, A, P, AP, L, R) where
• S=S × T , • s0=(s0, 1) is the initial state, • P:S × A × S→[0, 1] is such that

P (s,n),a,(s′,n′) =  Ps,a,s′ Ps,a,s′ 0

if 1 ≤ n ≤ N − 1 and n′ = n + 1 if n = N and n′ = 1 otherwise,

• L:S→2AP is such that L((s, t))=L(s) for all s∈S and for all t∈T ,
and A, AP and R are as deﬁned for M. We note that the transition function P is deﬁned such that
the agent’s initial state while computing the k-th N stage policy is the state occupied by the agent at kN +1-st stage
on the expanded MDP. Let B∪S0∪Sr be the partition of the states of M such
that if s∈B, then (s, n)∈B for all n∈T , and the sets S0 and Sr are deﬁned similarly. Then, the principal’s objective on M is to induce an agent policy that reaches the set B
with maximum probability. To synthesize an incentive design
under which the optimal agent policy satisﬁes the desired property, we modify the expanded MDP M by making its states s∈B∪S0 absorbing, and denote the resulting MDP by M′. Then, for a given ǫ≥0, we deﬁne the cost of control for a state-action pair on M through the function φǫ:S×A→R≥0 such that

φǫ((s, n), a) := V n(s) − Qn(s, a) + ǫ if s ∈ Sr, a ∈ A(s)

0

otherwise.

Let Ξ(M′)⊆Π(M′) be a subset of the set of ∞-stage policies such that π′∈Ξ(M′) if and only if

π′ ∈ arg max Prπ(s0 |= ϕ),

(7)

π∈Π(M′)

and for ǫ≥0, fǫ : Ξ(M′)→R be a function such that

∞
fǫ(π) := Eπs0 φǫ(St, At) . (8)
t=1

Then, for an arbitrarily small ǫ>0, an ǫ-optimal incentive
sequence can be designed in two steps as follows. Step 1: Compute V n(s) and Qn(s, a) given in (5)-(6),
and construct the cost of control function φǫ. Then for the modiﬁed expanded MDP M′, compute a stationary deterministic policy π=(d, d, . . .) such that

π ∈ arg min fǫ(π).

(9)

π∈Ξ(M′)

Step 2: Let ̺π∈P athsπ(M) be the path followed by the agent. At stage kN where N is the agent’s decision horizon and k∈N0, provide the agent with the incentive sequence {γ1, γ2, . . . , γN } such that
• if ̺π[n]∈B ∪ S0 for all n≤kN

 φǫ((s, n), a) γn(s, a) := ǫ
0

if s ∈ Sr and d((s, n))(a) > 0, if s ∈ Sr and d((s, n))(a) > 0, otherwise,
(10)

• γn(s, a):=0 otherwise.
Under the proposed incentive design (10), the agent’s value function Vn satisﬁes Vn(s)=V n(s)+(N + 1 − n)ǫ for all s∈S, n≤N . Additionally, if ̺π[n]∈B ∪S0 for all n≤kN , then for all s∈S, d((s, n))(a)>0 implies that the agent’s Qfunction satisﬁes

Qn(s, a) = R(s, a) + γn(s, a) + Ps,a,s′ Vn+1(s′)
s′ ∈S
= γn(s, a) + Qn(s, a) + (N − n)ǫ = (N + 1 − n)ǫ + V n(s) > (N − n)ǫ + V n(s) = max Qn(s, a′).
a′∈A(s)\{a}

Consequently, the agent is guaranteed to take the incentivitized actions at each stage until reaching the set B∪S0.
We now show ǫ-optimality of the proposed incentive design. Note that an optimal incentive design, i.e., ǫ=0, does not exist since choosing ǫ=0 in the cost of control function φǫn may not make the incentivized action uniquely optimal for the agent. As a result, the principal may not be able to
control the agent’s actions by offering such incentives.
We need the following technical lemma to state the main
result.
Lemma 1: There exists a policy π∈arg minπ∈Ξ(M′) f0(π) such that ξπ(s, a)<∞ for all s∈Sr and a∈A(s). Proof (Sketch): The problem of synthesizing a policy π such that π∈arg minπ∈Ξ(M′) f0(π) can be recast as a stochastic shortest path (SSP) problem with dead ends and zero-cost loops. Speciﬁcally, the dead ends are the states S0 and zero-cost loops are formed by states Sr. The existence of stationary policies for such SSP problems can be established
by slightly modifying the statement of Theorem 1 in [13]. Since any stationary policy π∈Ξ(M′) is guaranteed to reach the set B ∪ S0 with probability 1 within ﬁnite number of stages, the result follows.
Theorem 1: For any given ǫ>0, there exists ǫ>0 such that

π∈mΞ(iM n ′) fǫ(π) ≤ π∈mΞ(iM n ′) f0(π) + ǫ. Proof: For any policy π∈Ξ(M′) such that ξπ(s, a)<∞ for
all s∈Sr and a∈A(s), we have

fǫ(π) = f0(π) +

ξπ(s, a)ǫ.

(11)

s∈Sr a∈A(s)

Now, for a given ǫ>0, we evaluate both sides of the above equation at π∈arg minπ∈Ξ(M′) f0(π), which satisﬁes the condition ξπ(s, a)<∞ due to Lemma 1. Choosing

ǫ

ǫ=

ξπ(s, a) > 0

s∈Sr a∈A(s)

and taking the minimum of the left hand side of (11) over the set Ξ(M), we conclude the result.
We conclude this section by noticing a remarkable property of the proposed incentive design. Speciﬁcally, to implement the proposed design (10), the principal should use only a simple switch mode which offers the same incentives until the agent reaches the set B∪S0 and shifts all incentives to zero after the agent either satisﬁes the principal’s objective or fails to satisfy it.

V. COMPUTATION OF AN OPTIMAL INCENTIVE DESIGN

In the previous section, we developed a method to syn-

thesize an ǫ-optimal incentive design which require us to

solve a constrained cost minimization problem given in (8).

Speciﬁcally, to solve the incentive design problem (4a)-(4d),

one should synthesize a stationary deterministic policy π

such that

∞

π ∈ arg min Eπs

φǫ(St, At)

(12)

π∈Ξ(M′)

0
t=1

In this section, we develop a method to solve the above optimization problem. For the ease of notation, we consider an scLTL formula of the form ϕ=♦p. The incentive design for general scLTL formulae is discussed in Section VI.

A. Construction of the feasible policy space
To solve the problem (12), we ﬁrst represent the set Ξ(M′) of feasible policies as a set of policies that maximizes the expected total reward with respect to a speciﬁc reward function.
For a given MDP M, we partition the set of states into three disjoint sets B, S0, and Sr as explained in Section IV, and make the states s∈B∪S0 absorbing to form the modiﬁed MDP M′. For the modiﬁed MDP, we deﬁne a reward function r:S×A→R≥0 such that

r(s, a) =

s′∈B P s,a,s′ 0

if s ∈ Sr otherwise.

By making use of the known results, e.g., Theorem 10.100 in [10], it can be easily shown that for any s∈S and π∈Π(M′),

∞

Eπs

r(St, At) = Prπ(s |= ϕ)

t=1

where ϕ=♦p, p∈AP, and {p}⊆L(s′) if and only if s′∈B.
Let x⋆s:=maxπ∈Π(M′) Prπ(s |= ϕ). Then, the problem (12) can be rewritten as

∞

min Eπs

φǫ(St, At)

π∈Π(M′)

0
t=1

(13a)

∞

subject to: Eπs0

r(St, At) = x⋆s0 .

(13b)

t=1

B. Synthesis of an optimal stationary deterministic policy
Using Lemma 1, one can formulate the problem (13a)(13b) as a linear optimization problem and synthesize an optimal stationary policy. First, we compute the maximum probability of satisfying the speciﬁcation ϕ, i.e., x⋆s0 =maxπ∈Π(M′) Prπ(s0 |= ϕ), by solving a linear program (LP) [10] (see Chapter 10). Then we solve the following LP

minimize
λ(s,a)

λ(s, a)φǫ(s, a)
s∈Sr a∈A

(14a)

subject to:

λ(s, a)r(s, a) = x⋆s0
s∈Sr a∈A

(14b)

∀s ∈ Sr,

λ(s, a) −

P s′,a,sλ(s′, a) = α(s)

a∈A(s)

s′∈Sr a∈A(s)

(14c)

∀s ∈ Sr, a ∈ A(s), λ(s, a) ≥ 0

(14d)

where α:S→{0, 1} is a function such that α(s0)=1 and α(s)=0 for all s∈S\{s0}. The variable λ(s, a) denotes the expected residence time in the state-action pair (s, a)
[1], [14]. The constraint (14b) ensures that the probability of satisfying the speciﬁcation ϕ is maximized, and the
constraints (14c) represent the balance between the “inﬂow”
to and “outﬂow” from states. For each s∈Sr and a∈A(s), let λ⋆(s, a) be optimal de-
cision variables in (14a)-(14d). An optimal stationary policy π⋆={d⋆, d⋆, . . .} that solves the problem (13a)-(13b) is then
given by

d⋆(s, a) :=

λ⋆ (s,a) a∈A(s) λ⋆(s,a)
arbitrary

if a∈A(s) λ⋆(s, a) > 0

otherwise

(15)

for s∈Sr, and d⋆(s, a)=1 for an arbitrary a∈A(s) for s∈Sr. We note that a policy constructed through (15) is ran-
domized in general. One can argue that choosing one of the actions a∈A(s) such that d⋆(s, a)>0 deterministically
yields an optimal stationary deterministic policy. However,
the following example illustrates that such an approach may
result in an infeasible policy for the problem (14a)-(14d).
Example 1: Consider the MDP given in Fig. 2, where the cost of control φǫ is such that φǫ(s1, a2)=1 and φǫ(s, a)=0 otherwise. Suppose that the speciﬁcation is ϕ=♦s2, i.e., r(s1, a2)=1 and r(s, a)=0 otherwise. For the LP (14a)(14d), a set of optimal decision variables is given by λ⋆(s0, a1)=2, λ⋆(s1, a1)=1, and λ⋆(s1, a2)=1. Therefore, an optimal policy synthesized through (15) is d⋆(s0, a1)=1, d⋆(s1, a1)=1/2, and d⋆(s1, a2)=1/2. Clearly, if we consider
a1, 0

s0 s1 a2, 1 s2 a1, 0

a1, 0
Fig. 2: An MDP example for which arbitrarily choosing one of the optimal actions and taking it deterministically yields an infeasible policy.

a deterministic policy such that d(s1, a1)=1, the probability of satisfying the speciﬁcation ϕ under this policy is zero. Hence, choosing an arbitrary action a∈A(s) such that d⋆(s, a)>0 deterministically violates the constraint and yields an infeasible policy.⊳
As Example 1 illustrates, a structured approach is required
to synthesize an optimal deterministic policy from the solution of the LP (14a)-(14d). Let υ⋆ be the optimal value of
the LP in (14a)-(14d). To synthesize an optimal deterministic
policy, we ﬁrst solve the following LP,

minimize
λ(s,a)

λ(s, a)
s∈Sr a∈A

(16a)

subject to:

λ(s, a)r(s, a) = x⋆s0
s∈Sr a∈A

(16b)

λ(s, a)φǫ(s, a) = υ⋆

(16c)

s∈Sr a∈A

∀s ∈ Sr,

λ(s, a) −

P s′,a,sλ(s′, a) = α(s)

a∈A(s)

s′∈Sr a∈A(s)

(16d)

∀s ∈ Sr, a ∈ A(s), λ(s, a) ≥ 0.

(16e)

From the optimal decision variables λ⋆(s, a) of (16a)-(16e), an optimal policy π⋆={d⋆, d⋆, . . .} can be generated as follows. Let A⋆(s):={a∈A(s) : λ⋆(s, a)>0}. If A⋆(s)=∅, we choose d⋆(s, a)=1 for an arbitrary a∈A⋆(s), and if A⋆(s)=∅, we choose d⋆(s, a)=1 for an arbitrary a∈A(s).
Proposition 1: A stationary deterministic policy generated from the optimal decision variables λ⋆(s, a) of (16a)-(16e)
is a solution to the problem (13a)-(13b).
A proof of Proposition 1 can be found in Appendix I.
Intuitively, the LP in (16a)-(16e) computes the minimum expected time to reach the set B with probability x⋆s0 with the cost of υ⋆. Therefore, if λ⋆(s, a)>0, by taking the action a∈A(s), the agent has to “get closer” to the set B with nonzero probability. Otherwise, the minimum expected time to reach the set B would be strictly decreased. Consequently, by choosing an arbitrary action a∈A⋆(s), the agent is guaranteed to reach the set B with the desired
probability.

VI. INCENTIVE DESIGN FOR GENERAL SCLTL
SPECIFICATIONS
In previous sections, we have developed methods to synthesize ǫ-optimal incentive designs for reachability speciﬁcations ϕ=♦p. For such speciﬁcations, the principal induces the desired agent behavior by sharing only the incentive sequences with the agent. In other words, the principal does not have to inform the agent explicitly about the speciﬁcation. In this section, we show that for general scLTL formulae, the problem (4a)-(4d) may not have a feasible solution, in which case the principal must share its objective with the agent to induce the desired behavior.
To solve the problem (4a)-(4d) for general scLTL formulae, one needs to utilize the techniques from automata theory [10]. In particular, we use the fact that for any scLTL formula

ϕ built up from AP, we can construct a deterministic ﬁnite automata (DFA) Aϕ=(Q, q0, 2AP , δϕ, F ) where Q is a ﬁnite set of memory states, 2AP is the alphabet, δϕ:Q×2AP→Q is a transition function and F ⊆Q is the set of accepting states [11]. Then, after forming the expanded MDP M for a given MDP M and a decision horizon N as explained in Section
IV-B, one can construct the product MDP which is deﬁned

as follows.

Deﬁnition 7: Let M=(S, s0, A, P, AP, L) be an expanded MDP and Aϕ=(Q, q0, 2AP , δϕ, F ) be a DFA. The product MDP Mp=(Sp, s0p , A, P, AP, Lp, Fp) is a tuple where

• Sp=S×Q,

• s0p = (s0, q) such that q = δ(q0, L(s0)),

• P((s, q), a, (s′, q′))= Ps,a,s′ 0

if q′ = δ(q, L(s′)) otherwise,

• Lp((s, q)) = {q},

• Fp=S × F .

The incentive design problem (4a)-(4d) can now be solved on the product MDP Mp in three steps. First, we partition the states of Mp into three disjoint sets. Let B:=Fp, S0 be the set of states that have zero probability of reaching the set B, and Sr:=Sp\B ∪ S0. Second, we form the modiﬁed product MDP M′p by making all states B ∪ S0 absorbing. Finally, we apply the methods developed in Section IV to synthesize an ǫ-optimal incentive sequence on M′p.
Note that the incentive sequence is designed on the product MDP Mp. Therefore, the principal must share the DFA structure, i.e., it’s objective, with the agent to be able to use

the computed design. However, for the existence of a solution

to the problem (4a)-(4d), the incentive sequence should be designed on the MDP M. The following example illustrates
that the problem (4a)-(4d) may have no feasible solution, even though the existence of an ǫ-optimal incentive sequence on Mp is guaranteed.

Example 2: Consider the MDP given in Fig. 3, where
the numbers next to actions ai represent the transition probabilities, e.g., Ps0,a1,s1 =0.4, and the letters next to state numbers represent labels, e.g., L(s0)=A. Let the agent’s decision horizon be N =3, and the reward function R be such that R(s0, a1)=1 and R(s, a)=0 otherwise. Additionally, let
the principal’s objective be expressed by the scLTL formula
ϕ=♦(B∧♦C), i.e., ﬁrst visit state B and then state C. The maximum probability of satisfying ϕ is x⋆0=0.5, which can be computed by solving an LP [10]. The value x⋆0 is attainable if and only if the agent takes the action a2∈A(s0) with probability 1 after visiting state s2.
The principal should decide on which actions to incen-

a1, 1

a1, 0.2

a2, 1

s3, C

s0, A

a1, 1
s2, B
a1, 0.4

a1, 0.4 s1, A a1, 1

Fig. 3: An MDP example for which there exists no feasible incentive design for the scLTL speciﬁcation ϕ=♦(B∧♦C).

tivize in the ﬁrst three stages t=1, 2, 3 since the agent’s decision horizon is N =3. Clearly, the action a1 should be incentivized for t=1, 2 so that the agent visits state s2. At t=3, the agent will be in state s0 with nonzero probability. Now, if the principal incentivize a1, the agent will take action a2∈A(s0) with probability less then 1 after visiting state s2. On the other hand, if a2 is incentivized, then the agent cannot satisfy the speciﬁcation with probability higher than
0.46. Consequently, no incentive design on the given MDP
can guarantee the satisfaction of the speciﬁcation ϕ with maximum probability. ⊳
We now present a sufﬁcient condition on the structure of
the MDP M which guarantees the existence of an ǫ-optimal incentive design on M.
For the product MDP Mp and a policy π∈Π(Mp), let Msπ,t:={q∈Q: a∈A µπt ((s, q), a)>0} be the set of occupied memory states when the agent is in state s∈S at stage t∈N.

Theorem 2: For an MDP M and a decision horizon N , let

S be the ﬁnite set of states for the expanded MDP. There

exists an ǫ-optimal incentive design on M if there exists an ǫ-
optimal incentive design on Mp such that the agent’s optimal
policy π∈Π(Mp) under the provided incentives satisﬁes |Msπ,t|≤1 for all s∈S and t∈N. Proof: Let Γ be an ǫ-optimal incentive design on Mp with

the desired property. Note that the function in (10) is a

mapping from the expanded MDP M to the MDP M that

preserves ǫ-optimality of the incentive design. Therefore, in

what follows, we construct an incentive mapping from Mp to M that preserves ǫ-optimality of the incentive design Γ,

and conclude the result.

An ǫ-optimality-preserving mapping ψ such that

Γ

′

=

{

γ

′ 1

,

γ

′ 2

,

.

.

.}

:=

ψ

(Γ

)

where

Γ={γ1, γ2, . . .}

is

given

as

follows. For a given t∈N,

• if |Msπ,t|=0, γt′(s, a):=γt((s, q), a) for an arbitrary q∈Q

and for all a∈A(s),

•

if

|Msπ,t|=1,

γ

′ t

(s

,

a

):=

γ

t

((s

,

q

),

a

)

for

q∈Msπ,t

and

for

all a∈A(s).

The following corollary follows from the fact that the

principal can induce a stationary deterministic agent policy

on the product MDP through the methods explained in

Section V.

Corollary 1: For an MDP M, there exists an ǫ-optimal incentive design if Ps,a,s′ ∈{0, 1} for all s, s′∈S and a∈A(s).

Finally, we provide a sufﬁcient condition on the agent’s

decision horizon N that ensures the existence of an ǫ-optimal

incentive design on M. Proposition 2: For an MDP M, there exists an ǫ-optimal

incentive design if the agent’s decision horizon is N =1.

Proof (Sketch): There is a one-to-one correspondence be-

tween the paths of the product MDP Mp and the MDP M

[10]. Therefore, the principal can observe the path followed

by the agent on M, and provide the incentives according to the corresponding path on Mp at each stage. Because N =1,

the principal knows the memory state occupied by the agent

at each stage. Consequently, it becomes possible to map the

incentives from Mp to M at each stage.

VII. NUMERICAL SIMULATIONS
In this section, we demonstrate the proposed incentive design methods on two simple motion planning examples. Considering the availability of off-the-shelf solvers, e.g., Gurobi [15], MOSEK [16], that can efﬁciently solve largescale linear optimization problems, we restrict our attention to small scale examples to better emphasize the properties of the proposed methods. We synthesize the incentive sequences for the following examples through the use of MOSEK [16] solver together with CVXPY [17] interface.
A. Incentives for reachability objectives In this example, we consider a 5×5 grid world envi-
ronment, shown in Fig. 4, and an agent with decision horizon N =1. At each state, the agent has four actions, i.e., A={lef t, right, up, down}, and a transition to the chosen direction occurs with probability 1. If the adjacent state in the chosen direction is the boundary of the environment, the agent stays in its current state. A reward function for the agent is generated by choosing all rewards R(s, a) from the set {0, 1, . . . , 9} uniformly randomly.
The agent starts from the bottom left corner, i.e., Start state in Fig. 4, and aims to maximize its immediate reward at each stage. The principal provides incentives to the agent so that the agent reaches the top right corner, i.e., Target state in Fig. 4.
In the absence of incentives, i.e., γt(s, a)=0 for all t∈N, the agent’s optimal path is shown by blue arrows in Fig. 4. Under its optimal policy, the agent cycles between two states inﬁnitely often. Through the methods explained in Section IV-V, we synthesize an incentive sequence for the agent so that it reaches the target state with probability 1. The agent’s optimal path under the provided incentives is shown by red arrows in Fig. 4. The total cost to the principal is computed as 9+10ǫ units of resources (UR) where ǫ>0 is an arbitrarily small constant.
As can be seen from Fig. 4, under the provided incentives, the agent follows the lowest cost path rather than the shortest
Target
Start
Fig. 4: The motion of an agent on a grid world. The agent’s decision horizon is N =1, and it starts from the Start state. The principal’s objective is to induce an agent policy that reaches the Target state with probability 1. Blue arrows indicate the agent’s optimal policy in the absence of incentives, and red arrows indicate the agent’s optimal policy under the provided incentives.

one to the target state. Speciﬁcally, the shortest path would take 8 stages to reach the target state and cost 12 UR to the principal, whereas the lowest cost path takes 10 stages to reach the target and cost 9 UR. Quantitatively, the proposed incentive design allows the principal to save 25% of the resources that would be paid to the agent if it was to follow the shortest path.
B. Incentives for general scLTL speciﬁcations
In this example, we consider the same grid world environment introduced in the previous example with different state labels. The agent’s decision horizon is N =4, and its objective is to reach the state labeled as C in Fig. 5. The principal’s objective is to induce an agent policy that satisﬁes the scLTL speciﬁcation ϕ=♦(A ∧ ♦(B ∧ ♦C)), i.e., the agent should ﬁrst visit state A, then B, and then C, with probability 1.
The agent receives the reward of 2 for transitioning to the top left state and the reward of 5 for transitioning to the top right state. Its optimal path in the absence of incentives is shown in Fig. 5 with blue arrows (top path). We synthesize an optimal incentive sequence under which the agent’s optimal path is shown in Fig. 5 with red arrows (bottom path).
The total cost of the incentives to the principal is computed as 2+13ǫ units of resources. Speciﬁcally, the principal provides 2+ǫ incentives for the right action in the start state and then ǫ incentives at each stage for desired actions. An interesting property of the incentivized (red) path is that the agent stays in the same state in third stage by taking down action. This is due to the fact that the state s on the left of the state labeled as B has value Vn(s)=0 for all n. Therefore, the principal wants that state to be the agent’s initial state when it computes its second 4-stage policy. By doing so, the principal ensures that the states s′ occupied by the agent in the next 4 stages will always have a value zero, i.e., Vn(s′)=0 if a∈A(s′) µπ4+n(s′, a)>0, and therefore the cost of control will only be ǫ.
C
A
B
Start
Fig. 5: The motion of an agent on a grid world. The agent’s decision horizon is N =4, and it starts from the Start state. The principal’s objective is to induce an agent policy that satisﬁes the scLTL speciﬁcation ϕ=♦(A ∧ ♦(B ∧ ♦C)), i.e., ﬁrst visit A, then B, and then C. The optimal path of the agent in the absence of incentives is shown by blue arrows (top path). Red arrows indicate the agent’s optimal path under the provided incentives (bottom path).
VIII. CONCLUSIONS AND FUTURE DIRECTIONS
We considered a principal-agent model and studied the problem of designing an optimal sequence of incentives that

the principal should offer to the agent in order to induce
a desired agent behavior expressed as a syntactically cosafe linear temporal logic (scLTL) formula. For reachability
objectives, we presented a polynomial-time algorithm to synthesize an incentive design that minimizes the cost to the
principal. By providing an example scenario, we showed that
a feasible incentive design may not exists for general scLTL formulae, and the principal may need to share its objective
with the agent to induce the desired behavior. Furthermore,
we provided sufﬁcient conditions under which the principal can induce the desired behavior without sharing the scLTL
formula with the agent. The results that we present in this paper are obtained under
the assumptions that the agent’s reward function and the
length of its decision horizon are known by the principal. An interesting future direction may be to develop methods
to infer the length of the agent’s decision horizon through
perfect/imperfect observations, or to design an incentive sequence that does not require the knowledge of the length
of the decision horizon.
REFERENCES
[1] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994.
[2] M. Kloetzer and C. Mahulea, “Multi-robot path planning for syntactically co-safe LTL speciﬁcations,” in International Workshop on Discrete Event Systems (WODES), 2016, pp. 452–458.
[3] A. Armando, R. Carbone, and L. Compagna, “LTL model checking for security protocols,” in IEEE Computer Security Foundations Symposium, 2007, pp. 385–396.
[4] L. Tan, O. Sokolsky, and I. Lee, “Speciﬁcation-based testing with linear temporal logic,” in IEEE International Conference on Information Reuse and Integration, 2004, pp. 493–498.
[5] H. Zhang and D. C. Parkes, “Value-based policy teaching with active indirect elicitation.” in AAAI Conference on Artiﬁcial Intelligence, 2008, pp. 208–214.
[6] H. Zhang, Y. Chen, and D. C. Parkes, “A general approach to environment design with one agent.” in International Joint Conference on Artiﬁcal Intelligence, 2009, pp. 2002–2014.
[7] H. Zhang, D. C. Parkes, and Y. Chen, “Policy teaching through reward function learning,” in ACM Conference on Electronic commerce, 2009, pp. 295–304.
[8] Y. Chen, J. Kung, D. C. Parkes, A. D. Procaccia, and H. Zhang, “Incentive design for adaptive agents,” in International Conference on Autonomous Agents and Multiagent Systems, 2011, pp. 627–634.
[9] Y.-C. Ho, P. B. Luh, and G. J. Olsder, “A control-theoretic view on incentives,” Automatica, vol. 18, no. 2, pp. 167–179, 1982.
[10] C. Baier and J.-P. Katoen, Principles of Model Checking. MIT Press, 2008.
[11] C. Belta, B. Yordanov, and E. A. Gol, Formal Methods for DiscreteTime Dynamical Systems. Springer, 2017.
[12] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.
[13] F. Teichteil-Ko¨nigsbuch, “Stochastic safest and shortest path problems.” in AAAI Conference on Artiﬁcial Intelligence, 2012.
[14] K. Etessami, M. Kwiatkowska, M. Y. Vardi, and M. Yannakakis, “Multi-objective model checking of Markov decision processes,” in International Conference on Tools and Algorithms for the Construction and Analysis of Systems, 2007, pp. 50–65.
[15] L. Gurobi Optimization, “Gurobi optimizer reference manual,” 2018. [Online]. Available: http://www.gurobi.com
[16] M. ApS, MOSEK Optimizer API for Python. Version 8.1., 2019. [Online]. Available: https://docs.mosek.com/8.1/pythonapi/index.html
[17] S. Diamond and S. Boyd, “CVXPY: A Python-embedded modeling language for convex optimization,” Journal of Machine Learning Research, vol. 17, no. 83, pp. 1–5, 2016.
[18] R. Serfozo, Basics of Applied Stochastic Processes. Springer, 2009.

APPENDIX I PROOF OF PROPOSITION 1

Note that any deterministic policy constructed from the optimal decision variables λ⋆(s, a) of (16a)-(16e) can only
violate the reachability constraint (14b). In other words, the
constructed policy is guaranteed to minimize the expected
total cost.
We will need the following result to prove Proposition 1. For a given policy π, let Reachπ(s, s′) denote the probability of reaching s′ from s under π. Note that Reachπ(s, s)= a∈A(s) d(s, a)Reachπ((s, a), s) where Reachπ((s, a), s) is the probability of reaching state s from state action pair (s, a) under the policy π. Finally, let ξπ(s):= a∈A(s) ξπ(s, a), and note that for any ξπ(s)<∞, we have [18]

ξπ(s) =

Reachπ(s0, s)

. (17)

1 − a∈A(s) d(s)(a)Reachπ((s, a), s)

To prove the claim of Proposition 1, we show that any
policy that is constructed by choosing actions a∈A(s) such that λ⋆(s, a)>0 deterministically is optimal.
Let π={d, d, . . .} be the stationary randomized policy constructed from λ⋆(s, a) of LP (16) through the formula
(15). Additionally, let π={d, d, . . .} be a stationary randomized policy such that d(s⋆, a⋆)=1, and d(s)=d(s) for all s∈S\{s⋆}. Informally, in state s⋆, we choose one of the
active actions deterministically and do not change the rest
of the policy. We ﬁrst show that Prπ(s0|=ϕ)<x⋆s0 implies
Reachπ((s⋆, a⋆), s⋆)=1. Then by showing that Reachπ((s⋆, a⋆), s⋆)=1 cannot be true, we conclude that Prπ(s0|=ϕ)=x⋆s0 .
As for the ﬁrst claim, suppose for contradiction that Prπ(s0|=ϕ)<x⋆s0 and Reachπ((s⋆, a⋆), s⋆)<1. Note that if Reachπ((s⋆, a⋆), s⋆)<1, then Reachπ(s⋆, s⋆)<1. Therefore, Reachπ(s, s)<1 for all s∈Sr satisfying Reachπ(s⋆, s)>0. Additionally, as d(s′)=d(s′) for all s′ such that Reachπ(s⋆, s′)=0, we have Reachπ(s′, s′)<1.
Consequently, probability of leaving the set Sr is 1. Since all actions that are chosen by policy π satisfy x⋆s=Ps,a,s′ x⋆s′ where x⋆s is the maximum probability of reaching the set B from the state s (see e.g. Chapter 10 in [10]), probability of entering the set B must be equal to x⋆s0 . This raises a contradiction.
As for the second claim, suppose that Reachπ((s⋆, a⋆), s⋆)=1. Then, Reachπ((s⋆, a⋆), s⋆)=1 since π differs from π only in the state s⋆. We now construct a policy πˆ such that dˆ(s)=d(s) for all s∈S\{s⋆}, dˆ(s⋆, a⋆)=0, and

dˆ(s⋆, ai) =

d(s⋆, ai)

.

(18)

a∈A(s⋆)\{a⋆} d(s⋆, ai)

Note that πˆ satisﬁes Prπˆ(s0|=ϕ)=x⋆s0 . By showing that πˆ attains an objective value in (16) that is strictly smaller than the policy π, we will conclude that Reachπ((s⋆, a⋆), s⋆)=1
cannot be possible.

For the ease of notation, let ai:=d(s⋆, ai), Ri:=Reachπ((s⋆, ai), s⋆), and aˆi:=dˆ(s⋆, ai), and Rˆi:=Reachπˆ((s⋆, ai), s⋆). Without loss of generality, we choose a1=a⋆. By the construction of πˆ, it can be
shown that

ξπˆ (s⋆) = (1 − a1)ξπ(s⋆) − a1(R1 − 1)(1 − a1) (19) C

where C:=(1 −

n i=1

aiRi)(1

−

a1

−

n i=2

aiRi)>0.

Note

that ξπˆ (s⋆, ai)(1 − a1)=ξπ(s⋆, ai) due to (18). Then, since

a1>0 and R1=1, we have

ξπˆ (s⋆, ai) ≤ ξπ(s⋆, ai)

(20)

for all ai i=2, 3, . . . , n and ξπˆ (s⋆, a1)<ξπ(s⋆, a1). Conse-
quently, πˆ attains an objective value in (16) that is strictly
smaller than the policy π. Finally, since Reachπ((s⋆, a⋆), s⋆)=1 cannot be
true, Reachπ((s⋆, a⋆), s⋆)=1 cannot be true. If Reachπ((s⋆, a⋆), s⋆)=1 is not true, Prπ(s0|=ϕ)<x⋆s0 is not true. This concludes the proof.

