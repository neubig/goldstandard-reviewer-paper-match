PLLay: Efﬁcient Topological Layer based on Persistence Landscapes

arXiv:2002.02778v4 [cs.LG] 18 Jan 2021

Kwangho Kim Carnegie Mellon University
Pittsburgh, USA kwanghk@cmu.edu
Manzil Zaheer Google Research Mountain View, USA manzilzaheer@google.com
Frederic Chazal Inria
Palaiseau, France frederic.chazal@inria.fr

Jisu Kim Inria
Palaiseau, France jisu.kim@inria.fr
Joon Sik Kim Carnegie Mellon University
Pittsburgh, USA joonsikk@cs.cmu.edu
Larry Wasserman Carnegie Mellon University
Pittsburgh, USA larry@stat.cmu.edu

Abstract
We propose PLLay, a novel topological layer for general deep learning models based on persistence landscapes, in which we can efﬁciently exploit the underlying topological features of the input data structure. In this work, we show differentiability with respect to layer inputs, for a general persistent homology with arbitrary ﬁltration. Thus, our proposed layer can be placed anywhere in the network and feed critical information on the topological features of input data into subsequent layers to improve the learnability of the networks toward a given task. A taskoptimal structure of PLLay is learned during training via backpropagation, without requiring any input featurization or data preprocessing. We provide a novel adaptation for the DTM function-based ﬁltration, and show that the proposed layer is robust against noise and outliers through a stability analysis. We demonstrate the effectiveness of our approach by classiﬁcation experiments on various datasets.
1 Introduction
With its strong generalizability, deep learning has been pervasively applied in machine learning. To improve the learnability of deep learning models, various techniques have been proposed. Some of them have achieved an efﬁcient data processing method through specialized layer structures; for instance, inserting a convolutional layer greatly improves visual object recognition and other tasks in computer vision [e.g., Krizhevsky et al., 2012, LeCun et al., 2016]. On the other hand, a large body of recent work focuses on optimal architecture of deep network [Simonyan and Zisserman, 2015, He et al., 2016, Szegedy et al., 2015, Albelwi and Mahmood, 2016].
In this paper, we explore an alternative way to enhance the learnability of deep learning models by developing a novel topological layer which feeds the signiﬁcant topological features of the underlying data structure in an arbitrary network. The power of topology lies in its capacity which differentiates sets in topological spaces in a robust and meaningful geometric way [Carlsson, 2009, Ghrist, 2008]. It provides important insights into the global "shape" of the data structure via persistent homology
Preprint.

Figure 1: Illustration of PLLay, a novel topological layer based on weighted persistence landscapes. Information in the persistence diagram is ﬁrst encoded into persistence landscapes as a form of vectorized function, and then a deep learning model determines which components of the landscape (e.g., particular hills or valleys) are important for a given task during training. PLLay can be placed anywhere in the network.
[Zomorodian and Carlsson, 2005]. The use of topological methods in data analysis has been limited by the difﬁculty of combining the main tool of the subject, persistent homology, with statistics and machine learning. Nonetheless, a series of recent studies have reported notable successes in utilizing topological methods in data analysis [e.g., Zhu, 2013, Dindin et al., 2020, Nanda and Sazdanovic´, 2014, Tralie and Perea, 2018, Seversky et al., 2016, Gamble and Heo, 2010, Pereira and de Mello, 2015, Umeda, 2017, Liu et al., 2016, Venkataraman et al., 2016, Emrani et al., 2014]
There are at least three beneﬁts of utilizing the topological layer in deep learning; 1) we can efﬁciently extract robust global features of input data that otherwise would not be readily accessible via traditional feature maps, 2) an optimal structure of the layer for a given task can be easily embodied via backpropagation during training, and 3) with proper ﬁltrations it can be applied to arbitrarily complicated data structure even without any data preprocessing.
Related Work. The idea of incorporating topological concepts into deep learning has been explored only recently, mostly via feature engineering perspective where we use some ﬁxed, predeﬁned features that contain topological information [e.g., Dindin et al., 2020, Umeda, 2017, Liu et al., 2016]. Guss and Salakhutdinov [2018], Rieck et al. [2019] proposed a complexity measure for neural network architectures based on topological data analysis. Carlsson and Gabrielsson [2020] applied topological approaches to deep convolutional networks to understand and improve the computations of the network. Hofer et al. [2017] ﬁrst developed a technique to input persistence diagrams into neural networks by introducing their own topological layer. Carrière et al. [2020] proposed a network layer for persistence diagrams built on top of graphs. Poulenard et al. [2018], Gabrielsson et al. [2019], Hofer et al. [2019], Moor et al. [2020] also proposed various topology loss functions and layers applied to deep learning. Nevertheless, all the previous approaches suffer from at least one or more of the following limitations: 1) they rely on a particular parametrized map or ﬁltration, 2) they lack stability results or the stability is limited to a particular type of input data representation, and 3) most importantly, the differentiability of persistent homology is not guaranteed with respect to the layer’s input therefore we can not place the layer in the middle of deep networks in general.
Contribution. This paper presents a new topological layer, PLLay (Persistence Landscape-based topological Layer: see Figure 1 for an illustration), that does not suffer from the above limitations. Our topological layer does not rely on a particular ﬁltration or a parametrized mapping but still shows favorable theoretical properties. The proposed layer is designed based on the weighted persistence landscapes to be less prone to extreme topological distortions. We provide a tight stability bound that does not depend on the input complexity, and show the stability with respect to input perturbations. We also provide a novel adaptation for the DTM function-based ﬁltration, and analyze the stability property. Importantly, we guarantee the differentiability of our layer with respect to the layer’s input.
Reproducibility. The code for PLLay is available at https://github.com/jisuk1/pllay/.
2

2 Background and deﬁnitions

Topological data analysis (TDA) is a recent and emerging ﬁeld of data science that relies on topological tools to infer relevant features for possibly complex data [Carlsson, 2009]. In this section, we brieﬂy review basic concepts and main tools in TDA which we will harness to develop our topological layer in this paper. We refer interested readers to Chazal and Michel [2017], Hatcher [2002], Edelsbrunner and Harer [2010], Chazal et al. [2009, 2016b] for details and formal deﬁnitions.

2.1 Simplicial complex, persistent homology, and diagrams

When inferring topological properties of X, a subset of Rd, from a ﬁnite collection of samples X, we rely on a simplicial complex K, a discrete structure built over the observed points to provide a topological approximation of the underlying space. Two common examples are the Cˇ ech complex and the Vietoris-Rips complex. The Cˇ ech complex is the simplicial complex where k-simplices correspond to the nonempty intersection of k + 1 balls centered at vertices. The Vietoris-Rips (or simply Rips) complex is the simplicial complex where simplexes are built based on pairwise distances
among its vertices. We refer to Appendix A for formal deﬁnitions.
A collection of simplicial complexes F = {Ka ⊂ K : a ∈ R} satisfying Ka ⊂ Kb whenever a ≤ b is called a ﬁltration of K. A typical way of setting the ﬁltration is through a monotonic function on the simplex. A function f : K → R is monotonic if f (ς) ≤ f (τ ) whenever ς is a face of τ . If we let Ka := f −1(−∞, a], then the monotonicity implies that Ka is a subcomplex of K and Ka ⊂ Kb whenever a ≤ b. In this paper, we assume that the ﬁltration is built upon a monotonic function.
Persistent homology is a multiscale approach to represent the topological features of the complex K, and can be represented in the persistence diagram. For a ﬁltration F and for each nonnegative k, we keep track of when k-dimensional homological features (e.g., 0-dimension: connected component, 1-dimension: loop, 2-dimension: cavity,. . .) appear and disappear in the ﬁltration. If a homological feature αi appears at bi and disappears at di, then we say αi is born at bi and dies at di. By considering these pairs (bi, di) as points in the plane, one obtains the persistence diagram deﬁned as follows.

Deﬁnition 2.1 Let R2∗ := {(b, d) ∈ (R ∪ ∞)2 : d > b}. A persistence diagram D is a ﬁnite multiset of {p : p ∈ R2∗}. We let D denote the set of all such D’s.

We will use DX , DX as shorthand notations for the persistence diagram drawn from the simplicial complex constructed on original data source X, X, respectively.
Lastly, we deﬁne the following metrics to measure the distance between two persistence diagrams.

Deﬁnition 2.2 (Bottleneck and Wasserstein distance) Given two persistence diagrams D and D , their bottleneck distance (dB) and q-th Wasserstein distance (Wq) for q ≥ 1 are deﬁned by

dB(D, D ) = inf sup p − γ(p) ∞,
γ∈Γ p∈D¯

1 q

Wq(D, D ) = inf

p − γ(p)

q ∞

,

(1)

γ∈Γ

p∈D¯

respectively, where · ∞ is the usual L∞-norm, D¯ = D ∪ Diag and D¯ = D ∪ Diag with Diag being the diagonal {(x, x) : x ∈ R} ⊂ R2 with inﬁnite multiplicity, and the set Γ consists of all the bijections γ : D¯ → D¯ .

Note that for all q ∈ [1, ∞), dB(DX , DY ) ≤ Wq(DX , DY ) for any given DX , DY . As q tends to inﬁnity, the Wasserstein distance approaches the bottleneck distance. Also, see Appendix B for a further relationship between the bottleneck distance and Wasserstein distance.

2.2 Persistence landscapes
A persistence diagram is a multiset, which is difﬁcult to be used as inputs for machine learning methods (due to the complicated space structure, cardinality issues, computationally inefﬁcient metrics, etc.). Hence, it is useful to transform the persistent homology into a functional Hilbert space, where the analysis is easier and learning methods can be directly applied. One good example is the persistence landscape [Bubenik, 2015, 2018, Bubenik and Dłotko, 2017]. Let D denote a persistence

3

diagram that contains N off-diagonal birth-death pairs. We ﬁrst consider a set of piecewise-linear functions {Λp(t)}p∈D for all birth-death pairs p = (b, d) ∈ D as

Λp(t) = max{0, min{t − b, d − t}}.

Then the persistence landscape λ of the persistence diagram D is deﬁned as a sequence of functions

{λk}k∈N, where

λk(t) = kmaxpΛp(t), t ∈ R, k ∈ N,

(2)

Hence, the persistence landscape is a set of real-valued functions and is easily computable. Advantages for this kind of functional summaries are discussed in Chazal et al. [2014b], Berry et al. [2018].

2.3 Distance to measure (DTM) function

The Distance to measure (DTM) [Chazal et al., 2011, 2016a] is a robustiﬁed version of the distance function. More precisely, the DTM dµ,m0 : Rd → R for a probability distribution µ with parameter m0 ∈ (0, 1) and r ≥ 1 is deﬁned by

dµ,m0 (x) =

1

m0
(δ

1/r
(x))rdm ,

m0 0

µ,m

where δµ,m(x) = inf{t > 0 : µ(B(x, t)) > m} when B(x, t) is an open ball centered at x with radius t. If not speciﬁed, r = 2 is used as a default. In practice, we use a weighted empirical measure

Pn(x) =

ni=1 ni1(Xi = x) ,
i=1 i

with weights i’s for µ. In this case, we deﬁne the empirical DTM by

dˆ (x) = d

(x) =

Xi ∈Nk (x)

i Xi − x r

1/r
,

(3)

m0

Pn ,m0

m0

n i=1 i

where Nk(x) is the subset of {X1, . . . , Xn} containing the k nearest neighbors of x, k is such that

Xi∈Nk−1(x) i < m0

n i=1

i≤

Xi∈Nk(x) i, and i =

Xj ∈Nk(x) j − m0

n j=1 j

if at least one of Xi’s is in Nk(x) and i = i otherwise. Hence the empirical DTM behaves similarly to the k-nearest distance with k = m0n . For i.i.d cases, we typically set i = 1 but the

weights can be ﬂexibly determined in data-driven way. The parameter m0 determines how much

topological/geometrical information should be extracted from the local or global structure. A brief

guideline on DTM parameter selection can be found in Appendix F (see Chazal et al. [2011] for more

details). Since the resulting persistence diagram is less prone to input perturbations and has nice

stability properties, people often prefer using the DTM as their ﬁltration function.

3 A novel topological layer based on weighted persistence landscapes
In this section, we present a detailed algorithm to implement PLLay for a general neural network. Let X, DX , htop denote our input, corresponding persistence diagram induced from X, the proposed topological layer, respectively. Broadly speaking, the construction of our proposed topological layer consists of two steps: 1) computing a persistence diagram from the input, and 2) constructing the topological layer from the persistence diagram.
3.1 Computation of diagram: X → DX
To compute the persistence diagram from the input data, we ﬁrst need to deﬁne the ﬁltration which requires a simplicial complex K and a function f : K → R. There are several options for K and f . We are in general agnostic about which ﬁltration to use since it is in fact problem-dependent; in practice, we suggest using ensemble-like methods that can adapt to various underlying topological structures. One popular choice is the Vietoris-Rips ﬁltration. When there is a one-to-one correspondence between Xi and each ﬁxed grid point Yi, one obvious choice for f could be just interpreting X as a function values, so f (Yi) = Xi. We refer to Chazal and Michel [2017] for more examples.
As described in Section 2.3, one appealing choice for f is the DTM function. Due to its favorable properties, the DTM function has been widely used in TDA [Anai et al., 2019, Xu et al., 2019], and

4

Figure 2: The topological features encoded in the persistence diagram & persistence landscapes for MNIST and ORBIT5k sample. In the MNIST example, two loops (1-dimensional feature) in ‘8’ are clearly identiﬁed and encoded into the 1st and 2nd order landscapes. The ORBIT5k sample shows more involved patterns.

Figure 3: The signiﬁcant point (inside green-dashed circle) in the persistence diagram remains almost unchanged even after corrupting pixels and adding noise to the image.

has a good potential for deep learning application. Nonetheless, to the best of our knowledge, the
DTM function has not yet been adopted in previous studies. In what follows, we detail two common scenarios for the DTM adaptation: when we consider the input X as 1) data points or 2) weights.

• If the input data X is considered as the empirical data points, then the empirical DTM in (3) with weights i’s becomes

• If the input data X is considered as the weights corresponding to ﬁxed points {Y1, . . . , Yn},
then the empirical DTM in (3) with data points Yi’s and weights Xi’s becomes

dˆm0 (x) =

Xi∈Nk(x) i Xi − x r

m0

n i=1 i

1/r
,

dˆm0 (x) =

Xi∈Nk(x) Xi Yi − x r

m0

n i=1

Xi

1/r
,

(4)

(5)

where k and i are determined as in (3).

where k and i are determined as in (3).

Figure 2 provides some real data examples (which will be used in Section 5) of the persistence

diagrams and the corresponding persistence landscapes based on the DTM functions. As shown in

Figure 3, the topological features are expected to be robust to external noise or corruption.

3.2 Construction of topological layer: DX → htop

Our topological layer is deﬁned based on a parametrized mapping which takes the persistence diagram D to be projected onto R, by harnessing persistence landscapes. Our construction is less afﬂicted by the artiﬁcial bending due to a particular transformation procedure as in Hofer et al. [2017], yet still guarantees the crucial information in the persistence diagram to be well preserved as will be seen in Section 4. Insigniﬁcant points with low persistence are likely to be ignored systematically without introducing additional nuisance parameters [Bubenik and Dłotko, 2017].

Let R+0 denote [0, ∞). Given a persistence diagram D ∈ D, we compute the persistence landscape

of order k in (2), λk(t), for k = 1, ..., Kmax. Then, we compute the weighted average λω(t) :=

Kmax k=1

ωk λk (t)

with

a

weight

parameter

ω

=

{ωk }k ,

ωk

>

0,

k ωk = 1. Next, we set a domain

[Tmin, Tmax] and a resolution ν := T /(m − 1), and sample m equal-interval points from [Tmin, Tmax]

to obtain Λω = λω(Tmin), λω(Tmin + ν), ..., λω(Tmax) ∈ R+0 m. Consequently, we have deﬁned a mapping Λω : D → R+0 m which is a (vectorized) ﬁnite-sample approximation of the

weighted persistence landscapes at the resolution ν, at ﬁxed, predetermined locations. Finally, we consider a parametrized differentiable map gθ : R+0 m → R which takes the input Λω and is

differentiable with respect to θ as well. Now, the projection of D with respect to the mapping

5

Algorithm 1 Implementation of single structure element for PLLay Input: persistence diagram D ∈ D

1. compute λk(t) (2) on t ∈ [0, T ] for every k = 1, ..., Kmax

2. compute the weighted average λω(t) :=

Kmax k=1

ωk λk (t),

ωk

> 0,

k ωk = 1

3.

set ν

:=

T m−1

,

and

compute

Λω

= (λω(Tmin), λω(Tmin + ν), ..., λω(Tmax))

∈ Rm

4. for a parametrized differentiable map gθ : Rm → R, deﬁne Sθ,ω = gθ ◦ Λω

Output: Sθ,ω : D → R

Sθ,ω := gθ ◦ Λω deﬁnes a single structure element for our topological input layer. We summarize the procedure in Algorithm 1.
The projection Sθ,ω is continuous at every t ∈ [Tmin, Tmax]. Also, note that it is differentiable with respect to ω and θ, regardless of the resolution level ν. In what follows, we provide some guidelines that might be useful to implement Algorithm 1.
ω: The weight parameter ω can be initialized uniformly, i.e. ωk = 1/Kmax for all k, and will be re-determined during training through the softmax layer in a way that a certain landscape conveying signiﬁcant information has more weight. In general, lower-order landscapes tend to be more signiﬁcant than higher-order landscapes, but the optimal weights may vary from task to task.
θ, gθ: Likewise, some birth-death pairs, encoded in the landscape function, may contain more crucial information about the topological features of the input data structure than others. Roughly speaking, this is equivalent to say certain mountains (or their ridge or valley) in the landscape are especially important. Hence, the parametrized map gθ should be able to reﬂect this by its design. In general, it can be done by afﬁne transformation with scale and translation parameter, followed by an extra nonlinearity and normalization if necessary. We list two possible choices as below.
• Afﬁne transformation: with scale and translation parameter σi, µi ∈ Rm, gθi (Λω) = σi (Λω − µi) and θi = (σi, µi).
• Logarithmic transformation: with same θi = (σi, µi), gθi (Λω) = exp −σi Λω − µi 2 .
Note that other constructions of gθ, θ, ω are also possible as long as they satisfy the sufﬁcient conditions described above. Finally, since each structure element corresponds to a single node in a layer, we concatenate many of them, each with different parameters, to form our topological layer.
Deﬁnition 3.1 (Persistence landscape-based topological layer (PLLay)) For nh ∈ N, let ηi = (θi, ωi) denote the set of parameters for the i-th structure element and let η = (ηi)ni=h1. Given D and resolution ν, we deﬁne PLLay as a parametrized mapping with η of D → Rnh such that
htop : D → Sηi (D; ν) ni=h1 . (6)
Note that this is nothing but a concatenation of nh topological structure elements (nodes) with different parameter sets (thus nh is our layer dimension).
Remark 1 Our PLLay considers only Kmax top landscape functions. For a given persistence diagram, the points near the diagonal are not likely to appear at Kmax top landscape functions, and hence not considered in PLLay. And hence PLLay automatically ﬁlters out the noisy features.
3.3 Differentiability
This subsection is devoted to the analysis of the differential behavior of PLLay with respect to its input (or output from the previous layer), by computing the derivatives ∂∂hXtop . Since ∂∂hXtop = ∂∂DhtX op ◦ ∂∂DXX , this can be done by combining two derivatives ∂∂DXX and ∂∂DhtX op . We have extended Poulenard et al. [2018] so that we can compute the above derivatives for general persistent homology under arbitrary ﬁltration in our setting. We present the result in Theorem 3.1.
6

Theorem 3.1 Let f be the ﬁltration function. Let ξ be a map from each birth-death point (bi, di) ∈ DX to a pair of simplices (βi, δi). Suppose that ξ is locally constant at X, and f (βi) and f (δi) are
differentiable with respect to Xj’s. Then, htop is differentiable with respect to X and

∂htop

∂f (βi) m ∂gθ Kmax ∂λk(lv)

∂f (δi) m ∂gθ Kmax ∂λk(lv)

∂Xj =

∂Xj

∂xl

ωk ∂bi +

∂Xj

∂xl

ωk ∂di .

i

l=1

k=1

i

l=1

k=1

The proof is in Appendix E.1. Note that ∂∂λbik , ∂∂λdki are piecewise constant and are easily computed in explicit forms. Also ∂∂gxθl can be easily realized by an automatic differentiation framework such as tensorflow or pytorch. Our PLLay in Deﬁnition 3.1 is thus trainable via backpropagation at an
arbitrary location in the network. In Appendix D, we also provide a derivative for the DTM ﬁltration.

4 Stability Analysis

A key property of PLLay is stability; its discriminating power should remain stable against nonsystematic noise or perturbation of input data. In this section, we shall provide our theoretical results on the stability properties of the proposed layer. We ﬁrst address the stability for each structure element with respect to changes in persistence diagrams in Theorem 4.1.

Theorem 4.1 Let gθ be · ∞-Lipschitz, i.e. there exists Lg > 0 with |gθ(x) − gθ(y)| ≤ Lg x − y ∞ for all x, y ∈ Rm. Then for two persistence diagrams D, D ,
|Sθ,ω(D; ν) − Sθ,ω(D ; ν)| ≤ LgdB(D, D ).

Proof of Theorem 4.1 is given in Appendix E.2. Theorem 4.1 shows that Sθ,ω is stable with respect to perturbations in the persistence diagram measured by the bottleneck distance (1). It should be noted that only the Lipschitz continuity of gθ is required to establish the result.
Next, Corollary 4.1 shows that under certain conditions our approach improves the previous stability result of Hofer et al. [2017].

Corollary 4.1 For t > 0, let nt ∈ N be satisfying that, for any two diagrams Dt, Dt with dB(D, Dt) ≤ t and dB(D , Dt) ≤ t, either Dt\Dt or Dt\Dt has at least nt points. Then, the ratio of our stability bound in Theorem 4.1 to that in Hofer et al. [2017] is upper bounded by
Cgθ /(1 + (2t/dB(D, D )) × (nt − 1)), where Cgθ is a constant to be speciﬁed in the proof.

See Appendix E.3 for the proof. Corollary 4.1 implies that for complex data structures where each D
contains many birth-death pairs (for ﬁxed t, in general nt grows with the increase in the number of points in D), our stability bound is tighter than that of Hofer et al. [2017] at polynomial rates.

In particular, when we use the DTM function-based ﬁltration proposed in (4) and (5), Theorem 4.1 can be turned into the following stability result with respect to our input X.

Theorem 4.2 Suppose r = 2 is used for the DTM function. Let a differentiable function gθ and

resolution ν be given, and let P be a distribution. For the case when Xj’s are data points, i.e. when (4)

is used as the DTM function of X, let Pn be the empirical distribution deﬁned by Pn =

n i=1
n

iδXi .

i=1 i

For the case when Xj’s are weights, i.e. when (5) is used as the DTM function of X, let Pn be the

empirical distribution deﬁned by Pn = ni=ni1=X1 XiδiYi . Let DP be the persistence diagram of the DTM

ﬁltration of P , and DX be the persistence diagram of the DTM ﬁltration of X. Then,

|Sθ,ω(DX ; ν) − Sθ,ω(DP ; ν)| ≤ Lgm0−1/2W2(Pn, P ).

The proof is given in Appendix E.4. Theorem 4.2 implies that if the empirical distribution Pn induced from the given input X well approximates the true distribution P with respect to the Wasserstein
distance, i.e. having small W2(Pn, P ), then PLLay constructed on observed data is close to the one as if we were to know the true distribution P .

Theorem 4.1 and 4.2 suggest that the topological information embedded in the proposed layer is
robust against small noise, data corruption, or outliers. We have also discussed the stability result for the Vietoris-Rips and the Cˇ ech complex in Appendix C.

7

Accuracy

Accuracy for MNIST data

0.8

0.7

0.6

0.0

0.1

0.2

0.3

Corrupt and noise probability

Sd for MNIST data

0.15 0.10 0.05 0.00

sd

Accuracy

Accuracy for ORBIT5K data

0.9

0.8

0.7

0.6

0.2

0.0

0.1

0.2

0.3

Corrupt and noise probability

Sd for ORBIT5K data

0.15 0.10 0.05 0.00

MLP MLP+S MLP+P CNN CNN+S CNN+P CNN+P(i)

sd

Figure 4: Test accuracy in MNIST and ORBIT5K experiments. PLLay consistently improves the accuracy and the robustness against noise and corruption. In particular, in many cases it effectively reduces the variance of the classiﬁcation accuracy on ORBIT5K.

Model PointNet
PersLay
CNN
CNN+ SLay CNN+ PLLay

Accuracy
0.708 (±0.285)
0.877 (±0.010)
0.915 (±0.088)
0.943 (±0.014)
0.950
(±0.016)

Table 1: Comparison of different methods for ORBIT5K including the current stateof-the-art PersLay. The proposed method achieves the new state-of-the-art accuracy.

5 Experiments
To demonstrate the effectiveness of the proposed approach, we study classiﬁcation problems on two different datasets: MNIST handwritten digits and ORBIT5K. To fairly showcase the beneﬁts of using our proposed method, we keep our network architecture as simple as possible so that we can focus on the contribution from PLLay. In the experiments, we aim to explore the beneﬁts of our layer through the following questions: 1) does it make the network more robust and reliable against noise, etc.? and 2) does it improve the overall generalization capability compared to vanilla models? In order to address both of these questions, we ﬁrst consider the corruption process, a certain amount of random omission of pixel values or points from each raw example (so we will have less information), and the noise process, a certain amount of random addition of uniformly-distributed noise signals or points to each raw example. An example is given in Figure 3. Then we ﬁt a standard multilayer perceptron (MLP) and a convolutional neural network (CNN) with and without the augmentation of PLLay across various noise and corruption rates given to the raw data, and compare the results. The guideline for choosing the TDA parameters in this experiment is described in Appendix F. We intentionally use a small number of training data (∼1000) so that the convergence rates could be included in the evaluation criteria. Each simulation is repeated 20 times. We refer to Appendix G for details about each simulation setup and our model architectures.
MNIST handwritten digits
We classify handwritten digit images from MNIST dataset. Each digit has distinctive topological information which can be encoded into the Persistence Landscape as in Figure 2.
Topological layer. We add two parallel PLLays in Deﬁnition 6 at the beginning of MLP and CNN models, based on the empirical DTM function in (5), where we deﬁne ﬁxed 28 × 28 points on grid and use a set of grayscale values X as a weight vector for the ﬁxed points. We used m0 = 0.05 and m0 = 0.2 for each layer, respectively (referred to MLP+P, CNN+P(i), respectively). Particularly for the CNN model, it is likely that the output of the convolutional layers might carry signiﬁcant information about (smoothed) geometry of the input data shape. So we additionally place another PLLay after each convolutional layer, directly taking the layer output as 2D-function values and using the sublevel ﬁltration (CNN+P).
Baselines. As our baseline methods, we employ 2-layer vanilla MLP, 2-layer CNN, and the topological signature method by Hofer et al. [2017] based on the empirical DTM function proposed in (5) (which we will refer to as SLay). The SLay is augmented at the beginning of MLP and CNN, referred to as MLP+S and CNN+S. See Appendix G.1 for more details.
Result. In Figure 4, we observe that PLLay augmentation consistently improves the accuracy of all the baselines. Interestingly, as we increase the corruption and noise rates, the improvement on
8

CNN increases up to the moderate level of corruption and noise (∼ 15%), then starts to decrease. We conjecture that this is because although DTM ﬁltration is able to robustly capture homological signals as illustrated in Figure 2, if the corruption and noise levels become too much, then the topological structure starts to dissolve in the DTM ﬁltration.
Orbit Recognition
We classify point clouds generated by 5 different dynamical systems from ORBIT5K dataset [Adams et al., 2017, Carrière et al., 2020]. The detailed data generating process is described in Appendix G.2.
Topological layer. The setup remains the same as in the previous MNIST case, except that 1) PLLay at the beginning of each network uses the empirical DTM function in (4), and 2) we set m0 = 0.02.
Baselines & Simulation. All the baseline methods remain the same. For noiseless case, we added PointNet [Charles et al., 2017], a state-of-the-art in point cloud classiﬁcation, and PersLay [Carrière et al., 2020], a state-of-the-art in TDA-utilized classiﬁcation.
Result. In Figure 4, we observe that PLLay improves upon MLP and MLP+S by a huge margin (42% ∼ 60%). In particular, without augmenting PLLay, MLP and MLP+S remain at almost a random classiﬁer, which implies that the topological information is indeed crucial for the ORBIT5K classiﬁcation task, and it would otherwise be very challenging to extract meaningful features. PLLay improves upon CNN or CNN+S consistently as well. Moreover, it appears that CNN suffers from high variance due to the high complexity of ORBIT5K dataset. On the other hand, PLLay can effectively mitigate this problem and make the model more stable by utilizing robust topological information from DTM function. Impressively, for the noiseless case, PLLay has achieved better performance than all the others including the current state-of-the-art PointNet and PersLay by a large margin.
6 Discussion
In this study, we have presented PLLay, a novel topological layer based on the weighted persistence landscape where we can exploit the topological features effectively. We provide the differentiability guarantee of the proposed layer with respect to the layer’s input under arbitrary ﬁltration. Hence, our study offers the ﬁrst general topological layer which can be placed anywhere in the deep learning network. We also present new stability results that verify the robustness and efﬁciency of our approach. It is worth noting that our method and analytical results in this paper can be extended to silhouettes [Chazal et al., 2015, 2014b]. In the experiments, we have achieved the new state-of-the-art accuracy for ORBIT5K dataset based on the proposed method. We expect our work to bridge the gap between modern TDA tools and deep learning research.
The computational complexity depends on how PLLay is used. Computing the DTM is O(n+m log n) when m0 ∝ 1/n and k-d tree is used, where n is the input size and m is the grid size. Computing the persistence diagram is O(m2+ ) for any small > 0 when the simplicial complex K in Section 3.1 grows linearly with respect to the grid size such as cubical complex or alpha complex (Chen and Kerber [2013] and Theorem 4.4, 5.6 of Boissonnat et al. [2018]). Computing the persistence landscape grows linearly with respect to the number of homological features in the persistence diagram, which is the topological complexity of the input and does not necessarily depend on n or m. For our experiments, we consider ﬁxed grids of size 28 × 28 and 40 × 40 as in Appendix G, so the computation is not heavy. Also, if we put PLLay only at the beginning of the deep learning model, then PLLay can be pre-computed and needs not to be calculated at every epoch in the training.
There are several remarks regarding our experiments. First, we emphasize that SLay in Section 5 is rather an intermediate tool designed for our simulation and not completely identical to the topological signature method by Hofer et al. [2017]. For example, SLay combines the method by Hofer et al. [2017] and the DTM function in (4) and (5) that have not appeared in the previous study. So we cannot exclude the possibility that the comparable performance of SLay for certain simulations is due to the contribution by the DTM function ﬁltration. Moreover, for CNN, placing extra PLLay after each convolutional layer appears to bring marginal improvement in accuracy in our experiments. Exploring the optimal architecture with our PLLay, e.g., ﬁnding the most accurate and efﬁcient PLLay network for a given classiﬁcation task, would be an interesting future work.
The source code of PLLay is publicly available at https://github.com/jisuk1/pllay/.
9

Broader Impact
This paper proposes a novel method of adapting tools in applied mathematics to enhance the learnability of deep learning models. Even though our methodology is generally applicable to any complex modern data, it is not tuned to a speciﬁc application that might improperly incur direct societal/ethical consequences. So the broader impact discussion is not needed for our work.
Acknowledgments and Disclosure of Funding
During the last 36 months prior to the submission, Jisu Kim received Samsung Scholarship, and Joon Sik Kim received Kwanjeong Fellowship. Freédéric Chazal was supported by the ANR AI chair TopAI.
References
Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence images: a stable vector representation of persistent homology. J. Mach. Learn. Res., 18:Paper No. 8, 35, 2017. ISSN 1532-4435.
Saleh Albelwi and Ausif Mahmood. Automated optimal architecture of deep convolutional neural networks for image recognition. In 2016 15th IEEE International conference on machine learning and applications (ICMLA), pages 53–60. IEEE, 2016.
Hirokazu Anai, Frédéric Chazal, Marc Glisse, Yuichi Ike, Hiroya Inakoshi, Raphaël Tinarrage, and Yuhei Umeda. Dtm-based ﬁltrations. In 35th International Symposium on Computational Geometry (SoCG 2019), 2019.
S. A. Barannikov. The framed Morse complex and its invariants. In Singularities and bifurcations, volume 21 of Adv. Soviet Math., pages 93–115. Amer. Math. Soc., Providence, RI, 1994.
Eric Berry, Yen-Chi Chen, Jessi Cisewski-Kehe, and Brittany Terese Fasy. Functional summaries of persistence diagrams. arXiv preprint arXiv:1804.01618, 2018.
Jean-Daniel Boissonnat, Frédéric Chazal, and Mariette Yvinec. Geometric and topological inference. Cambridge Texts in Applied Mathematics. Cambridge University Press, Cambridge, 2018. ISBN 978-1-108-41089-2; 978-1-108-41939-0. doi: 10.1017/9781108297806. URL https://doi. org/10.1017/9781108297806.
Peter Bubenik. Statistical topological data analysis using persistence landscapes. The Journal of Machine Learning Research, 16(1):77–102, 2015.
Peter Bubenik. The persistence landscape and some of its properties. arXiv preprint arXiv:1810.04963, 2018.
Peter Bubenik and Paweł Dłotko. A persistence landscapes toolbox for topological statistics. Journal of Symbolic Computation, 78:91–114, 2017.
Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33 of Graduate Studies in Mathematics. American Mathematical Society, Providence, RI, 2001. ISBN 0-82182129-6. doi: 10.1090/gsm/033. URL https://doi.org/10.1090/gsm/033.
Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46(2):255–308, 2009.
Gunnar Carlsson and Rickard Brüel Gabrielsson. Topological approaches to deep learning. In Nils A. Baas, Gunnar E. Carlsson, Gereon Quick, Markus Szymik, and Marius Thaule, editors, Topological Data Analysis, pages 119–146, Cham, 2020. Springer International Publishing. ISBN 978-3-030-43408-3.
Gunnar Carlsson, Afra Zomorodian, Anne Collins, and Leonidas J Guibas. Persistence barcodes for shapes. International Journal of Shape Modeling, 11(02):149–187, 2005.
10

Mathieu Carrière, Frédéric Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, and Yuhei Umeda. Perslay: A neural network layer for persistence diagrams and new graph topological signatures. In Silvia Chiappa and Roberto Calandra, editors, The 23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pages 2786–2796, Online, 26–28 Aug 2020. PMLR. URL http://proceedings.mlr.press/v108/carriere20a.html.
R. Q. Charles, H. Su, M. Kaichun, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 77–85, 2017.
Frédéric Chazal and Bertrand Michel. An introduction to topological data analysis: fundamental and practical aspects for data scientists. arXiv preprint arXiv:1710.04019, 2017.
Frédéric Chazal, David Cohen-Steiner, Marc Glisse, Leonidas J Guibas, and Steve Y Oudot. Proximity of persistence modules and their diagrams. In Proceedings of the twenty-ﬁfth annual symposium on Computational geometry, pages 237–246. ACM, 2009.
Frédéric Chazal, David Cohen-Steiner, and Quentin Mérigot. Geometric inference for probability measures. Foundations of Computational Mathematics, 11(6):733–751, 2011.
Frédéric Chazal, Vin De Silva, and Steve Oudot. Persistence stability for geometric complexes. Geometriae Dedicata, 173(1):193–214, 2014a.
Frédéric Chazal, Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, and Larry Wasserman. Stochastic convergence of persistence landscapes and silhouettes. In Proceedings of the thirtieth annual symposium on Computational geometry, page 474. ACM, 2014b.
Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, and Larry Wasserman. Subsampling methods for persistent homology. In International Conference on Machine Learning, pages 2143–2151, 2015.
Frédéric Chazal, Pascal Massart, Bertrand Michel, et al. Rates of convergence for robust geometric inference. Electronic journal of statistics, 10(2):2243–2286, 2016a.
Frédéric Chazal, Steve Y. Oudot, Marc Glisse, and Vin De Silva. The Structure and Stability of Persistence Modules. SpringerBriefs in Mathematics. Springer Verlag, 2016b. URL https: //hal.inria.fr/hal-01330678.
Chao Chen and Michael Kerber. An output-sensitive algorithm for persistent homology. Comput. Geom., 46(4):435–447, 2013. ISSN 0925-7721. doi: 10.1016/j.comgeo.2012.02.010. URL https://doi.org/10.1016/j.comgeo.2012.02.010.
Vin de Silva and Robert Ghrist. Coverage in sensor networks via persistent homology. Algebraic & Geometric Topology, 7:339–358, 2007. ISSN 1472-2747. doi: 10.2140/agt.2007.7.339. URL https://doi.org/10.2140/agt.2007.7.339.
Meryll Dindin, Yuhei Umeda, and Frédéric Chazal. Topological data analysis for arrhythmia detection through modular neural networks. In Canadian Conference on Artiﬁcial Intelligence, pages 177– 188. Springer, 2020.
Herbert Edelsbrunner and John Harer. Computational topology: an introduction. American Mathematical Soc., 2010.
Herbert Edelsbrunner, David Letscher, and Afra Zomorodian. Topological persistence and simpliﬁcation. In Proceedings 41st Annual Symfposium on Foundations of Computer Science, pages 454–463. IEEE, 2000.
Saba Emrani, Thanos Gentimis, and Hamid Krim. Persistent homology of delay embeddings and its application to wheeze detection. IEEE Signal Processing Letters, 21(4):459–463, 2014.
Brittany T. Fasy, Jisu Kim, Fabrizio Lecci, Clément Maria, David L. Millman, and Vincent Rouvreau. Introduction to the R package TDA. CoRR, abs/1411.1830, 2014. URL http://arxiv.org/ abs/1411.1830.
11

Rickard Brüel Gabrielsson, Bradley J. Nelson, Anjan Dwaraknath, Primoz Skraba, Leonidas J. Guibas, and Gunnar E. Carlsson. A topology layer for machine learning. CoRR, 2019. URL http://arxiv.org/abs/1905.12200.
Jennifer Gamble and Giseon Heo. Exploring uses of persistent homology for statistical analysis of landmark-based shape data. Journal of Multivariate Analysis, 101(9):2184–2199, 2010.
Robert Ghrist. Barcodes: the persistent topology of data. Bulletin of the American Mathematical Society, 45(1):61–75, 2008.
William H Guss and Ruslan Salakhutdinov. On characterizing the capacity of neural networks using algebraic topology. arXiv preprint arXiv:1802.04443, 2018.
Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
Christoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. Deep learning with topological signatures. In Advances in Neural Information Processing Systems, pages 1634–1644, 2017.
Christoph Hofer, Roland Kwitt, Mandar Dixit, and Marc Niethammer. Connectivity-optimized representation learning via persistent homology. Proceedings of the 36th International Conference on Machine Learning, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
Y LeCun, L Bottou, Y Bengio, et al. Lenet-5, convolutional neural networks (2015). Retrieved June, 1, 2016.
Jen-Yu Liu, Shyh-Kang Jeng, and Yi-Hsuan Yang. Applying topological persistence in convolutional neural network for music audio signals. arXiv preprint arXiv:1608.07373, 2016.
Michael Moor, Max Horn, Bastian Rieck, and Karsten M. Borgwardt. Topological autoencoders. CoRR, abs/1906.00722, 2020. URL http://arxiv.org/abs/1906.00722.
Vidit Nanda and Radmila Sazdanovic´. Simplicial models and topological inference in biological systems. In Discrete and topological models in molecular biology, pages 109–141. Springer, 2014.
Cássio MM Pereira and Rodrigo F de Mello. Persistent homology for time series and spatial data clustering. Expert Systems with Applications, 42(15-16):6026–6038, 2015.
Adrien Poulenard, Primoz Skraba, and Maks Ovsjanikov. Topological function optimization for continuous shape matching. Computer Graphics Forum, 37(5):13–25, 2018. doi: 10.1111/cgf. 13487. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13487.
Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, and Karsten Borgwardt. Neural persistence: A complexity measure for deep neural networks using algebraic topology. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ByxkijC5FQ.
Lee M Seversky, Shelby Davis, and Matthew Berger. On time-series topological data analysis: New data and opportunities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 59–67, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.
12

The GUDHI Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 3.3.0 edition, 2020. URL https://gudhi.inria.fr/doc/3.3.0/.
Christopher J Tralie and Jose A Perea. (quasi) periodicity quantiﬁcation in video data, using topology. SIAM Journal on Imaging Sciences, 11(2):1049–1077, 2018.
Yuhei Umeda. Time series classiﬁcation via topological data analysis. Information and Media Technologies, 12:228–239, 2017.
Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, and Pavan Turaga. Persistent homology of attractors for action recognition. In Image Processing (ICIP), 2016 IEEE International Conference on, pages 4150–4154. IEEE, 2016.
X. Xu, J. Cisewski-Kehe, S. B. Green, and D. Nagai. Finding cosmic voids and ﬁlament loops using topological data analysis. Astronomy and Computing, 27:34, Apr 2019. doi: 10.1016/j.ascom. 2019.02.003.
Xiaojin Zhu. Persistent homology: An introduction and a new text representation for natural language processing. In IJCAI, pages 1953–1959, 2013.
Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Computational Geometry, 33(2):249–274, 2005.
13

APPENDIX

A Simplicial complex, Persistent homology, and Distance between sets on metric spaces

Throughout, we will let X denotes a subset of Rd, and X denotes a ﬁnite collection of points from an arbitrary space X.
A simplicial complex can be seen as a high dimensional generalization of a graph. Given a set V , an (abstract) simplicial complex is a set K of ﬁnite subsets of V such that α ∈ K and β ⊂ α implies β ∈ K. Each set α ∈ K is called its simplex. The dimension of a simplex α is dim α = cardα − 1, and the dimension of the simplicial complex is the maximum dimension of any of its simplices. Note that a simplicial complex of dimension 1 is a graph.
When approximating the topology of the underlying space by observed samples, a common choice is the Cˇ ech complex, deﬁned next. Below, for any x ∈ X and r > 0, we let BX(x, r) denote the open ball centered at x and radius r > 0 intersected with X.

Deﬁnition A.1 (Cˇ ech complex) Let X ⊂ X be ﬁnite and r > 0. The (weighted) Cˇ ech complex is the simplicial complex

Cˇ echXX (r) := {σ ⊂ X : ∩x∈σBX(x, r) = ∅}.

(7)

The superscript X will be dropped when understood from the context.

Another common choice is the Vietoris-Rips complex, also referred to as Rips complex, where simplexes are built based on pairwise distances among its vertices.

Deﬁnition A.2 (Vietoris-Rips complex) Let X ⊂ X be ﬁnite and r > 0. The Vietoris-Rips complex RipsX (r) is the simplicial complex deﬁned as

RipsX (r) := {σ ⊂ X : d(xi, xj) < 2r, ∀xi, xj ∈ σ}.

(8)

Note that from (7) and (8), the Cˇ ech complex and Vietoris-Rips complex have the following interleaving inclusion relationship
Cˇ echX (r) ⊂ RipsX (r) ⊂ Cˇ echX (2r).
In particular, when X ⊂ Rd is a subset of a Euclidean space of dimension d, then the constant 2 can be tightened to d2+d1 (e.g., see Theorem 2.5 in de Silva and Ghrist [2007]):

Cˇ echX (r) ⊂ RipsX (r) ⊂ Cˇ echX

2d r . d+1

Persistent homology [Barannikov, 1994, Zomorodian and Carlsson, 2005, Edelsbrunner et al., 2000, Chazal et al., 2014a] is a multiscale approach to represent topological features of the complex K. A ﬁltration F is a collection of subcomplexes approximating the data points at different resolutions, formally deﬁned as follows.
Deﬁnition A.3 (Filtration) A ﬁltration F = {Ka ⊂ K}a∈R is a collection of subcomplexes of K such that a ≤ b implies that Ka ⊂ Kb.
For a ﬁltration F and for each k ∈ N0 = N ∪ {0}, the associated persistent homology P HkF is an ordered collection of k-th dimensional homologies, one for each element of F.
Deﬁnition A.4 (Persistent homology) Let F be a ﬁltration and let k ∈ N0. The associated k-th persistent homology P HkF is a collection of groups {Hk(Ka)}a∈R of each subcomplex Ka in F equipped with homomorphisms {ıak,b}a≤b, where Hk(Ka) is the k-th dimensional homology group of Ka and ıak,b : HkKa → HkKb is the homomorphism induced by the inclusion Ka ⊂ Kb.

14

For the k-th persistent homology P HkF, the set of ﬁltration levels at which a speciﬁc homology appears is always an interval [b, d) ⊂ [−∞, ∞], i.e. a speciﬁc homology is formed at some ﬁltration value b and dies when the inside hole is ﬁlled at another value d > b. To be more formally, the image of a speciﬁc homology class α in Hk(Ka) is nonzero if and only if b ≤ a < d. We often say that α is born at b and dies at d. By considering these pairs as points in the plane, one obtains the persistence diagram as below.
Deﬁnition A.5 (Persistence diagram) Let R2∗ := {(b, d) ∈ (R∪∞)2 : d > b}. Let F be a ﬁltration and let k ∈ N0. The corresponding k-th persistence diagram Dgmk(F ) is a ﬁnite multiset of R2∗, consisting of all pairs (b, d), where [b, d) is the interval of ﬁltration values for which a speciﬁc homology class appears in P HkF. b is called a birth time and d is called a death time.

When topological information of the underlying space is approximated by the observed points, it is often needed to compare two sets with respect to their metric structures. Here we present two distances on metric spaces, Hausdorff distance and Gromov-Hausdorff distance. We refer to Burago et al. [2001] for more details and other distances.
The Hausdorff distance [Burago et al., 2001, Deﬁnition 7.3.1] is on sets embedded in the same metric spaces. This distance measures how two sets are close to each other in the embedded metric space. When S ⊂ X, we denote by Ur(S) the r-neighborhood of a set S in a metric space, i.e. Ur(S) = x∈S BX(x, r).
Deﬁnition A.6 (Hausdorff distance) Let X be a metric space, and X, Y ⊂ X be a subset. The Hausdorff distance between X and Y , denoted by dH (X, Y ), is deﬁned as
dH (X, Y ) = inf{r > 0 : X ⊂ Ur(Y ) and Y ⊂ Ur(X)}.

The Gromov-Hausdorff distance measures how two sets are far from being isometric to each other. To deﬁne the distance, we ﬁrst deﬁne a relation between two sets called correspondence.
Deﬁnition A.7 Let X and Y be two sets. A correspondence between X and Y is a set C ⊂ X × Y whose projections to both X and Y are both surjective, i.e. for every x ∈ X, there exists y ∈ Y such that (x, y) ∈ C, and for every y ∈ Y , there exists x ∈ X with (x, y) ∈ C.

For a correspondence, we deﬁne its distortion by how the metric structures of two sets differ by the correspondence.

Deﬁnition A.8 Let X and Y be two metric spaces, and C be a correspondence between X and Y . The distortion of C is deﬁned by
dis(C) = sup {|dX (x, x ) − dY (y, y )| : (x, y), (x , y ) ∈ C} .

Now the Gromov-Hausdorff distance [Burago et al., 2001, Theorem 7.3.25] is deﬁned as the smallest possible distortion between two sets.

Deﬁnition A.9 (Gromov-Hausdorff distance) Let X and Y be two metric spaces. The GromovHausdorff distance between X and Y , denoted as dGH (X, Y ), is deﬁned as

1

dGH (X, Y

)

=

2

inf
C

dis(C ),

where the inﬁmum is over all correspondences between X and Y .

B Bottleneck distance and Wasserstein distance
Our stability bound in Theorem 4.1 is based on the bottleneck distance, while the stability bound in Hofer et al. [2017] is based on Wasserstein distance. Hence to compare these bounds, we need to understand the relationship between the bottleneck distance and Wasserstein distance. We already know that the Wasserstein distance is lower bounded by the bottleneck distance. Here, we will ﬁnd a tighter lower bound for the ratio of the Wasserstein distance to the bottleneck distance.
Before analyzing the relationship between them, we ﬁrst show a claim.

15

Claim B.1 Let D, D be two persistence diagrams. For t > 0, let nt ∈ N be satisfying the followings: for any two diagrams Dt, Dt with dB(D, Dt) ≤ t and dB(D , Dt) ≤ t, either |Dt\Dt| ≥ nt or |Dt\Dt| ≥ nt holds. Then for any bijection γ : D¯ → D¯ , the number of paired points with being at least 2t apart in L∞ distance is greater or equal to nt, i.e.,
p ∈ D¯ : p − γ(p) ∞ > 2t ≥ nt.

And then, we get a lower bound for the ratio of Wasserstein distance to the bottleneck distance.

Proposition B.1 Let D, D be two persistence diagrams. For t > 0, let nt ∈ N be satisfying the followings: for any two diagrams Dt, Dt with dB(D, Dt) ≤ t and dB(D , Dt) ≤ t, either |Dt\Dt| ≥ nt or |Dt\Dt| ≥ nt holds. Then, the ratio of q-Wasserstein distance to the bottleneck distance is bounded as

Wq(D, D )

2t

q

1 q

dB(D, D ) ≥ 1 + dB(D, D ) (nt − 1) .

C Stability for Vietoris-Rips and Cech ﬁltration

When we use Vietoris-Rips or Cˇ ech ﬁltration, our result can be turned into the stability result with
respect to points in Euclidean space. Let X, Y ⊂ Rd be two bounded sets. The next corollary re-states our stability theorem with respect to points in Rd.

Corollary C.1 Let X, Y be any -coverings of X, Y, and let DX , DY denote persistence diagrams induced from the Vietoris-Rips or Cˇ ech ﬁltration on X, Y respectively. Then we have

|Sθ,ω(DX ; ν) − Sθ,ω(DY ; ν)| ≤ 2Lg (dGH (X, Y) + 2 ) .

(9)

The proof is given in Appendix E.7. Corollary C.1 implies that if we assume our observed data points are sufﬁciently decent quality in the sense that → 0, then our topological layers constructed on those observed points are stable with respect to small perturbations of the true representation under proper persistent homologies. Here, could be interpreted as uncertainty from incomplete sampling. This means the topological information embedded in the proposed layer is robust against small sampling noise or data corruption by missingness.
Moreover, since Gromov-Hausdorff distance is upper bounded by Hausdorff distance, the result in Corollary C.1 also holds when we use dH (X, Y ) in place of dGH (X, Y ) in RHS of (9).
Remark 2 In fact, when we have very dense data that have been well-sampled uniformly over the true representation so that → 0, our result in (9) converges to the following:
|Sθ,ω(DX; ν) − Sθ,ω(DY; ν)| ≤ 2LgdGH (X, Y).

D Differentiability of DTM function

Here we provide a speciﬁc example of computing ∂∂fX(ςj) when f is the DTM ﬁltration which has not been explored in previous approaches. We ﬁrst consider the case of (4) where Xj’s are data points,
as in Proposition D.1. See Appendix E.8 for the proof.

Proposition D.1 When Xj’s and ς satisfy that Xi∈Nk(y) i Xi − yl r are different for each yl ∈ ς, then f (ς) is differentiable with respect to Xj and

∂f (ς) = ∂Xj

j Xj − y r−2 (Xj − y)I(Xj ∈ Nk(y))

r−1

,

dˆm0 (y)

m0

n i=1 i

where I is an indicator function and y = arg maxz∈ς dˆm0 (z). In particular, f is differentiable a.e. with respect to Lebesgue measure on X.

Similarly, we consider the case of (5) where Xj’s are weights, as in Proposition D.2. See Appendix E.9 for the proof.

16

Proposition D.2 When Xj’s and ς satisfy that Yi∈Nk(y) Xi Yi − yl r are different for each yl ∈ ς, then f (ς) is differentiable with respect to Xj and

∂f (ς) = ∂Xj

Yj − y r I(Yj ∈ Nk(y)) − m0 dˆm (y) r

0

r−1

,

r dˆm0 (y)

m0

n i=1

Xi

where y = arg maxy∈ςi dˆm0 (y). In particular, f is differentiable a.e. with respect to Lebesgue measure on X and Y .

Computation of ∂∂hµtoip , ∂∂hςtoip are simpler and can be done in a similar fashion. In the experiments, we set r = 2.

E Proofs

E.1 Proof of Theorem 3.1

For computing ∂∂hXtojp , note that it can be expanded using the chain role as

∂htop = ∂htop ∂bi + ∂htop ∂di ,

(10)

∂Xj i ∂bi ∂Xj i ∂di ∂Xj

and hence we need to compute ∂∂DXX =

, ∂htop
∂bi

∂ htop ∂di

to compute ∂htop .

(bi ,di )∈DX

∂Xj

, ∂bi ∂di
∂Xj ∂Xj

and ∂htop =

(bi,di)∈DX ,Xj ∈X

∂DX

We

ﬁrst

compute

∂DX ∂X

.

Let

K

be

the

simplicial

complex,

and

suppose

all

the

simplices

are

ordered

in

the ﬁltration so that the values of f are nondecreasing, i.e. if ς comes earlier than τ then f (ς) ≤ f (τ ).

Note that the map ξ from each birth-death point (bi, di) ∈ DX to a pair of simplices (βi, δi) is

simply the pairing returned by the standard persistence diagram [Carlsson et al., 2005]. Let γ be the

homological feature corresponding to (bi, di), then the birth simplex βi is the simplex that forms γ in Kbi = f −1(−∞, bi], and the death simplex δi is the simplex that causes γ to collapse in Kdi = f −1(−∞, di]. For example, if γ were to be a 1-dimensional feature, then βi is the edge in Kbi that forms the loop corresponding to γ, and δi is the triangle in Kdi which incurs the loop corresponding to γ can be contracted in Kdi .

Now, f (ξ(bi)) = f (βi) = bi and f (ξ(di)) = f (δi) = di, and from ξ being locally constant on X,

∂bi = ∂f (ξ(bi)) = ∂f (βi) , ∂di = ∂f (ξ(di)) = ∂f (δi) .

(11)

∂Xj

∂Xj

∂Xj ∂Xj

∂Xj

∂Xj

Therefore, the derivatives of the birth value and the death value are the derivatives of the ﬁltration func-

tion evaluated at the corresponding pair of simplices.

And

∂DX ∂X

=

, ∂bi ∂di
∂Xj ∂Xj

(bi,di)∈DX ,Xj ∈X

is the collection of these derivatives, hence applying (11) gives

∂DX = ∂X

∂bi , ∂di ∂Xj ∂Xj

=
(bi,di)∈DX ,Xj ∈X

∂f (βi) , ∂f (δi) ∂Xj ∂Xj

.
ξ−1(βi,δi)∈DX ,Xj ∈X
(12)

Now, we compute ∂∂DhtX op =

, ∂htop
∂bi

∂ htop ∂di

. Computing ∂htop can be done by applying the

(bi ,di )∈DX

∂bi

chain role on htop = Sθ,ω = gθ ◦ Λω as

∂htop ∂Sθ,ω ∂(gθ ◦ Λω)

∂Λω m ∂gθ ∂λω(lν)

=

=

= ∇gθ ◦

=

,

(13)

∂bi

∂bi

∂bi

∂bi

∂xl ∂bi

l=1

where we use xl as the shorthand notation for the input of the function gθ. Then, applying λω(lν) =

Kmax k=1

ωk λk (lν )

to

(13)

gives

∂htop m ∂gθ Kmax ∂λk(lv)

∂bi = ∂xl

ωk ∂bi .

(14)

l=1

k=1

17

Similarly, ∂∂hdtoip can be computed as

∂htop m ∂gθ Kmax ∂λk(lv)

∂di = ∂xl

ωk ∂di .

(15)

l=1

k=1

And therefore, ∂∂DhtX op is the collection of these derivatives from (14) and (15), i.e.,

∂htop = ∂DX

m ∂gθ Kmax ∂λk(lv) m ∂gθ Kmax ∂λk(lv)

∂xl

ωk ∂bi , ∂xl

ωk ∂di

l=1

k=1

l=1

k=1

.
(bi ,di )∈DX

(16)

Hence, ∂∂hXtop can be computed by applying (12) and (16) to (10) as

∂htop = ∂htop ∂bi + ∂htop ∂di ∂Xj i ∂bi ∂Xj i ∂di ∂Xj

∂f (βi) m ∂gθ Kmax ∂λk(lv)

∂f (δi) m ∂gθ Kmax ∂λk(lv)

=

∂Xj

∂xl

ωk ∂bi +

∂Xj

∂xl

ωk ∂di .

i

l=1

k=1

i

l=1

k=1

E.2 Proof of Theorem 4.1

Let D and D be two persistence diagrams and let λ and λ be their persistence landscapes. All the quantities derived from D are denoted by a variable name with the superscript hereafter (e.g., λk(t), Λ ω).
For the stability of the structure element Sθ,ω, we ﬁrst expand the difference between Sθ,ω(D; ν) and Sθ,ω(D ; ν) using Sθ,ω = gθ ◦ Λω as

|Sθ,ω(D; ν) − Sθ,ω(D ; ν)| = gθ Λω − gθ Λ ω .

(17)

Then, RHS of (17) is bounded by applying the Lipschitz condition of the function gθas

gθ Λω − gθ Λ ω ≤ Lg Λω − Λ ω .

(18)

∞

Then for Λω − Λ ω , note that Λω, Λ ω ∈ Rm, the L∞ difference of Λω and Λ ω is bounded

as

∞

Λω − Λ ω = max λω(Tmin + iν) − λω(Tmin + iν)
∞ 0≤i≤m−1

≤ sup λω(t) − λω(t) = m1/2 λω − λω .

(19)

t∈[0,T ]

∞

Now, for bounding λω − λω , we ﬁrst consider the pointwise difference |λω(t) − λω(t)|. For all
∞
t ∈ [0, T ], the difference between λω(t) and λω(t) is bounded as

λω(t) − λω(t) =

1 Kmax ωkλk(t) −
k ωk k=1

1 Kmax k ωk k=1 ωkλk(t)

1 Kmax ≤ k ωk k=1 ωk |λk(t) − λk(t)|

≤

sup

|λk(t) − λk(t)| = max λk − λk ∞ .

1≤k≤Kmax,t∈[0,T ]

1≤k≤Kmax

(20)

And hence λω − λω is bounded by max1≤k≤Kmax λk − λk ∞ as well, i.e.,
∞

λω − λω = sup λω(t) − λω(t) ≤ max λk − λk ∞ .

(21)

∞ t∈[0,T ]

1≤k≤Kmax

18

Then for all k = 1, . . . , Kmax, the ∞-landscape distance λk − λk ∞ is bounded by the bottleneck distance dB(D, D ) from Theorem 13 in Bubenik [2015], i.e.

λk − λk ∞ ≤ dB(D, D ).

(22)

Hence, applying (18), (19), (21), (22) to (17) gives the stated stability result as

|Sθ,ω(D; ν) − Sθ,ω(D ; ν)| = gθ Λω − gθ Λ ω ≤ Lg Λω − Λ ω
∞

≤ Lg λω − λω ≤ Lg max λk − λk ∞

∞

1≤k≤Kmax

≤ LgdB(D, D ).

E.3 Proof of Corollary 4.1

First note that the result of Hofer et al. [2017] used W1 Wasserstein distance with Lr norm for ∀r ∈ N, which will be denoted by W1Lr in this proof. That is,

W

Lr 1

(D

,

D

)

:=

inf

p − γ(p) r

γ

p∈DX

where γ ranges over all bijections D → D (i.e., W1L∞ corresponds to W1 in our deﬁnition 2.2). Then, · r ≥ · ∞ implies that W1Lr is lower bounded by W1, i.e.

W

L 1

r

(D

,

D

)

≥

W1(D,

D

).

(23)

Now, let cK denote the Lipschitz constant in Hofer et al. [2017, Theorem 1] and cgθ denote the

constant term in our result in Theorem 4.1, i.e. cgθ = Lg Tν 1/2. We want to upper bound the ratio

cgθ dB(D,D ) . This directly comes from (23) and Proposition B.1 as

cK

W

Lr 1

(D

,D

)

cgθ dB(D, D ) ≥ cgθ dB(D, D ) ≥ cgθ

1.

cK

W

Lr 1

(D

,

D

)

cK W1(D, D )

cK 1 + dB(D2t,D ) (nt − 1)

Finally, we deﬁne Cgθ,T,ν := cgθcK ,T,ν , and the result follows.
It should be noted that the bound is actually very loose. However, we can still conclude that our bound is tighter than that of Hofer et al. [2017] at polynomial rates.

E.4 Proof of Theorem 4.2

We ﬁrst bound the difference between Sθ,ω(DX ; ν) and Sθ,ω(DP ; ν) using Theorem 4.1 as

|Sθ,ω(DX ; ν) − Sθ,ω(DP ; ν)| ≤ LgdB(DX , DP ).

(24)

It is left to further bound the bottleneck distance dB(DX , DP ). The bottleneck distance between two diagrams DX and DP is bounded by the stability theorem of persistent homology as

dB (DX , DP ) ≤ dPn,m0 − dP,m0 ∞ .

(25)

Then, from r = 2 in the DTM function, the L∞ distance between dPn,m0 and dP,m0 is bounded by the stability of DTM function (Theorem 3.5 from Chazal et al. [2011]) as

dPn,m0 − dP,m0 ∞ ≤ m−0 1/2W2(Pn, P ).

(26)

Hence, combining (24), (25), and (26) altogether gives the stated stability result as

|Sθ,ω(DX ; ν) − Sθ,ω(DP ; ν)| ≤ Lgm−0 1/2W2(Pn, P ).

19

E.5 Proof of Claim B.1
Let γ : D → D be any bijection and let S := p ∈ D¯ : p − γ(p) ∞ > 2t . Then for p ∈ D¯ with p − γ(p) ∞ ≤ 2t, there exists β(p) ∈ R2∗ such that p − β(p) ∞ ≤ t and β(p) − γ(p) ∞ ≤ t. Now, deﬁne two diagrams Dt, Dt as follows:
Dt = S ∪ β(p) : p ∈ D¯\S \Diag, Dt = S ∪ β(p) : p ∈ D¯\S \Diag, where S := γ(p) : p ∈ D¯ . Then, dB(D, Dt) ≤ t and dB(D , Dt) ≤ t from the construction. Hence from the deﬁnition of nt, either |Dt\Dt| ≥ nt or |Dt\Dt| ≥ nt holds. Now, note that Dt\Dt ⊂ S and Dt\Dt ⊂ S . And |S|=|S |, and hence we get the claimed result as
|S| ≥ nt.

E.6 Proof of Proposition B.1

We consider a bijection γ∗ that realizes the q-Wasserstein distance between D and D : i.e. γ∗ =

arginf

p − γ(p) q∞. Then we have that

γ p∈D

dB(D, D )q ≤ sup p − γ∗(p) q∞.

(27)

p∈D

On the other hand, if we let p∗ = argsup p − γ∗(p) ∞, we have
p∈D

Wq(D, D )q =

p − γ∗(p)

q ∞

=

sup

p − γ∗(p)

q ∞

+

p − γ∗(p) q∞.

p∈D

p∈D

p=p∗

Note that from Claim B.1, p ∈ D¯ : p − γ∗(p) ∞ > 2t ≥ nt. And hence Wq(D, D )q can be lower bounded as

Wq(D, D )q = sup

p − γ∗(p)

q ∞

+

p − γ∗(p)

q ∞

(28)

p∈D

p=p∗

≥ sup

p − γ∗(p)

q ∞

+

(2t)q (nt

−

1).

(29)

p∈D

Now, we lower bound the ratio WdBq((DD,,DD ))qq . By (27) and (29), this can be done as follows.

sup Wq(D, D )q ≥ p∈D dB(D, D )q
≥1+

p − γ∗(p)

q ∞

+

(2t)q (nt

−

1)

dB(D, D )q

2t

q

dB(D, D ) (nt − 1).

And hence the ratio of the Wasserstein distance to thw bottleneck distance WdBq((DD,,DD )) is correspondingly lower bounded as

Wq(D, D )

1
Wq(D, D )q q

2t

q

1 q

dB(D, D ) ≥ dB(D, D )q ≥ 1 + dB(D, D ) (nt − 1) .

E.7 Proof of Corollary C.1

The difference between Sθ,ω(DX ; ν) and Sθ,ω(DY ; ν) is bounded by Theorem 4.1 as

|Sθ,ω(DX ; ν) − Sθ,ω(DY ; ν)| ≤ LgdB (DX , DY ) ,

(30)

hence it sufﬁces to show

dB (DX , DY ) < 2 (dGH (X, Y) + 2 ) .

(31)

20

To show (31), we ﬁrst apply the triangle inequality as

dB (DX , DY ) ≤ dB (DX , DX) + dB (DX, DY) + dB (DY, DY ) .

(32)

And note that since X, Y, X, Y are all bounded in Euclidean space, they are totally bounded metric spaces. Thus by Theorem 5.2 in Chazal et al. [2014a], the bottleneck distance between any two diagrams is bounded by Gromov-Hausdorff distance, and in particular,

dB (DX, DY) ≤ 2dGH (X, Y) ,

dB (DX , DX) ≤ 2dGH (X, X) , dB (DY, DY ) ≤ 2dGH (Y, Y ) .

(33)

And then since the Gromov-Hausdorff distance is bounded by the Hausdorff distance,

dGH (X, X) ≤ dH (X, X) , dGH (Y, Y ) ≤ dH (Y, Y ) .

(34)

And the Hausdorff distance between X and X or Y and Y is bounded by by the assumption that X, Y are -coverings of X, Y, respectively, i.e.,

dH (X, X) < , dH (Y, Y ) < .

(35)

Hence combining (32), (33), (34), and (35) gives (31) as

dB (DX , DY ) ≤ dB (DX , DX) + dB (DX, DY) + dB (DY, DY ) ≤ 2 (dGH (X, X) + dGH (X, Y) + dGH (Y, Y )) ≤ 2 (dH (X, X) + dGH (X, Y) + dH (Y, Y )) < 2 (dGH (X, Y) + 2 ) .

Now, the results follows from (30) and (31).

E.8 Proof of Proposition D.1

From (4), note that for any y ∈ ς, dˆm0 (y) is expanded as

dˆm0 (y) =

Xi∈Nk(y) i Xi − y r

m0

n i=1 i

1/r
,

(36)

where k is such that

Xi∈Nk−1(y) i < m0

n i=1

i

≤

Xi∈Nk(y) i, and i =

Xj∈Nk(y) j − m0 nj=1 j for one of Xi’s that is k-th nearest neighbor of y and ωi = ωi

otherwise. Hence, by letting y = arg maxz∈ς dˆm0 (z) applying to (36), the ﬁltration function fX at simplex ς becomes

fX (ς) = dˆX,m0 (y) =

Xi∈Nk(y) i Xi − y r

m0

n i=1 i

1/r
,

(37)

where the notations fX and dˆX,m0 are to clarify the dependency of f on X. And from the condition, dˆm0 (y) > dˆm0 (z) holds for all z ∈ ς. Hence for sufﬁciently small > 0 and for any Z = {Z1, . . . , Zn} with Zj − Xj < , (37) becomes

fZ (ς) = dˆZ,m0 (y) =

Xi∈Nk(y) i Zi − y r

m0

n i=1 i

1/r
.

(38)

Hence by differentiating (38), the derivative of f with respect to X is calculated as

∂f (ς) = ∂Xj
=

Xi∈Nk(y) i Xi − y r

m0

n i=1 i

r1 −1
×

j Xj − y r−2 (Xj − y)I(Xj ∈ Nk(y))

m0

n i=1 i

j Xj − y r−2 (Xj − y)I(Xj ∈ Nk(y))

r−1

.

dˆm0 (y)

m0

n i=1 i

21

E.9 Proof of Proposition D.2

From (5), note that for any y ∈ ς, dˆm0 (y) is expanded as

dˆm0 (y) =

Xi∈Nk(y) Xi Yi − y r

m0

n i=1

Xi

1/r
,

(39)

where k is such that

Yi∈Nk−1(y) Xi < m0

n i=1

Xi

≤

Yi∈Nk(y) Xi, and Xi =

Xj∈Nk(y) Xj − m0 nj=1 Xj for one of Yi’s that is k-th nearest neighbor of y and Xi = Xi

otherwise. Hence, by letting y = arg maxz∈ς dˆm0 (z) and applying to (39), the ﬁltration function fX at simplex ς becomes

fX (ς) = dˆX,m0 (y) =

Xi∈Nk(y) Xi Yi − y r

m0

n i=1

Xi

1/r
,

(40)

where the notations fX and dˆX,m0 are to clarify the dependency of f on X. And from the condition, dˆm0 (y) > dˆm0 (z) holds for all z ∈ ς. Hence for sufﬁciently small > 0 and for any Z = {Z1, . . . , Zn} with Zj − Xj < , (40) becomes

fZ (ς) = dˆZ,m0 (y) =

Xi∈Nk(y) Zi Yi − y r

m0

n i=1

Zi

1/r
.

(41)

Hence by differentiating (41), the derivative of f with respect to X is calculated as

∂f (ς) ∂Xj
=1 r

Xi∈Nk(y) Xi Yi − y r

m0

n i=1

Xi

r1 −1
×

Yj − y r I(Yj ∈ Nk(y)) (m0

n i=1

Xi)

−

m0

(m0

n i=1

Xi)2

Yj − y r I(Yj ∈ Nk(y)) − m0 dˆm (y) r

0

=

r−1

.

r dˆm0 (y)

m0

n i=1

Xi

Xi∈Nk(y) Xi Yi − y r

22

F Guideline for choosing TDA parameters
PLLay has several TDA parameters to choose: Kmax, Tmin, Tmax, m, and m0 if DTM ﬁltration is used. One can try grid search but it could be too time-consuming. More affordable approach is to compute the DTM ﬁltration and the persistence diagram for some data and choose appropriate parameters that can reveal the topological and geometrical information of the data. Figure 5 illustrates one example of the digit 8 in MNIST data. Figure 5(a) shows the contour plot of the chosen data. When using a DTM ﬁltration, we need to choose m0 ﬁrst. DTMs with different m0 values extract different topological and geometrical information. When m0 is small, a DTM ﬁltration aggregates the data more locally, and the geometrical and homological information formed from the local structure is extracted. When m0 is large, a DTM ﬁltration aggregates the data more globally, and the geometrical and homological information formed from the global structure is extracted. From the digit 8, we would ﬁrst like to see the two-loop structure. And if we choose m0 = 0.05, then as can be seen in Figure 5(b) and (c), the 1st persistent homology extracts the two-loop structure, which is more directly expected from the contour plot of the data itself in Figure 5(a). However, if we choose m0 = 0.2, then as can be seen in Figure 5(d) and (e), the two-loop structure disappears, since the two-loop structure is coming from more local geometry of the data. Meanwhile, as the DTM ﬁltration aggregates the data more globally, the global geometry information that three points on the digit 8(top, center, bottom) being close to neighboring points and being centers of local clusters is extracted in the 0th persistent homology. For MNIST data, DTM ﬁltrations with m0 = 0.05 and m0 = 0.2 extract different topological and geometrical information of the data. Hence for MNIST data, we used two parallel PLLays with m0 = 0.05 and m0 = 0.2, respectively. After choosing m0, choosing other TDA parameters Kmax, Tmin, Tmax, m is more straightforward. One can choose parameters so that the desired topological features are well extracted in the landscape. For m0 = 0.05, as can be seen from Figure 5(c), choosing Kmax = 2, Tmin = 0.06, Tmax = 0.3, m = 25 will extract two 1-dimensional features of the persistence diagram in the corresponding landscape. For m0 = 0.2, as can be seen from Figure 5(e), choosing Kmax = 3, Tmin = 0.14, Tmax = 0.4, m = 27 will extract two 1-dimensional features of the persistence diagram in the corresponding landscape.
G Experiment Details.
All the experiments were implemented using GUDHI The GUDHI Project [2020] and Tensorflow library in Python and TDA package Fasy et al. [2014] in R. We use mean and standard deviation across 20 runs of simulations with different network initializations. We remark that the basic purpose of our experiment design is to highlight the prospects and possibilities of using topological layer, not to win state-of-the-art performances.
23

0

5

10

15

20

25 0

5

10

15

20

25

(a) Digit 8 in MNIST data.

Persistence diagram
1.00 +∞

0.75

0.140

0.50 0.25 0.120

Death

0.00

−0.25

−0.50

−0.75

−1.00

−1.0

−0.5

0.0

0.5

1.0

(b) Contour plot of DTM ﬁltration, m0 = 0.05.

0.100
0.080 0 1
0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14
Birth (c) Persistence Diagram of DTM ﬁltration, m0 = 0.05.

Persistence diagram
1.00 +∞
0.75 0.190
0.50
0.180 0.25

Death

0.00

0.170

−0.25

−0.50

−0.75

−1.00

−1.0

−0.5

0.0

0.5

1.0

(d) Contour plot of DTM ﬁltration, m0 = 0.2.

0.160

0.150

0

0.140

1

0.14

0.15

0.16

0.17

0.18

0.19

Birth

(e) Persistence Diagram of DTM ﬁltration, m0 = 0.2.

Figure 5: One example of the digit 8 in MNIST data, its contour plots and persistence diagrams of DTM ﬁltration at m0 = 0.05 and m0 = 0.2. When m0 = 0.05, DTM ﬁltration aggregates more locally, and the 1st persistent homology extracts two loop structures of the digit 8. When m0 = 0.2, DTM ﬁltration aggregates the digit 8 more globally, and the 0th persistent homology extracts three connected component structures of the digit 8.

24

MLP MLP+S MLP+P
CNN CNN+S CNN+P CNN+P(i)

0.00 0.8683 (0.0063) 0.8597 (0.0087) 0.8791 (0.0062) 0.8506 (0.0261) 0.8544 (0.0194) 0.8790 (0.0151) 0.8635 (0.0189)

0.05 0.8425 (0.0061) 0.8322 (0.0086) 0.8538 (0.0061) 0.8367 (0.0246) 0.8058 (0.1081) 0.8541 (0.0218) 0.8391 (0.0153)

Corruption and noise probability

0.10

0.15

0.20

0.25

0.8133 0.7850 0.7441 0.6997

(0.0087) (0.0086) (0.0098) (0.0090)

0.8060 0.7749 0.7364 0.6844

(0.0152) (0.0147) (0.0177) (0.0187)

0.8227 0.7910 0.7511 0.7045

(0.0103) (0.0121) (0.0109) (0.0087)

0.8030 0.7872 0.7541 0.7315

(0.0315) (0.0340) (0.0319) (0.0447)

0.7988 0.7938 0.7649 0.7055

(0.0252) (0.0326) (0.0215) (0.1268)

0.8364 0.8209 0.7855 0.7551

(0.0214) (0.0217) (0.0247) (0.0289)

0.8113 0.7985 0.7671 0.7391

(0.0250) (0.0275) (0.0179) (0.0302)

0.30 0.6514 (0.0124) 0.6372 (0.0213) 0.6507 (0.0120) 0.6778 (0.0506) 0.6884 (0.0372) 0.7044 (0.0230) 0.6841 (0.0936)

0.35 0.5732 (0.0155) 0.5637 (0.0161) 0.5753 (0.0135) 0.6245 (0.0478) 0.6281 (0.0407) 0.6355 (0.0404) 0.6364 (0.0355)

Table 2: Test accuracy in MNIST experiments. In each cell, the top number corresponds to the average accuracy of the model at the corruption and noise probability, and the bottom number corresponds to the 1 standard deviation of the accuracies. At each column, the model with the best accuracy is bolded.

G.1 MNIST handwritten digits.
For MNIST handwritten digits, we use MNIST dataset. Raw input data is a 784 dimensional vector (reshaped from 28 by 28) of real values, each value being the pixel intensity. We use 1000 random samples for the training set and 10000 samples for the test set. Cross-entropy loss was used to train the network for 100 epochs, using Adam optimizer with mini-batches of size 16.
Topological layer. For MLP+P and CNN+P(i), we use two parallel PLLays at the beginning of MLP and CNN models with 32 nodes each and afﬁne transformation, which are concatenated to the raw input to either MLP or CNN. We used the empirical DTM ﬁltration in (5), where we deﬁne ﬁxed 28 × 28 points on grid on [−1, 1]2 and use X as a weight vector for the ﬁxed points. For one PLLay, we used m0 = 0.05, Kmax = 2, Tmin = 0.06, Tmax = 0.3, m = 25, and for the other PLLay, we used m0 = 0.2, Kmax = 3, Tmin = 0.14, Tmax = 0.4, m = 27. For CNN+P, we additionally use one PLLay after the convolutional layer, with Kmax = 3, Tmin = 0.05, Tmax = 0.95, m = 18.
Baselines. For the baselines, models were designed to have simple structures for quick comparisons:
• Vanilla MLP: one hidden layer with 64 units with ReLU activations.
• CNN: two convolution layers followed by two fully connected layers.
• SLay: for comparison with PLLay, two SLays are used with 10 nodes each, which are concatenated to the raw input to either MLP or CNN. We used the value ν = 0.005 and ν = 0.01 for the hyperparameter of each SLay, respectively.
Result. The Accuracy results for MNIST data in Figure 4 is represented with 1 standard errors in Table 2 and Figure 6. In Figure 6, the results for MLP, MLP+S, MLP+P are in Figure 6(a), and the results for CNN, CNN+S, CNN+P, CNN+P(i) are in Figure 6(b). We can see that PLLay consistently improves the accuracies of all baselines. In particular from Table 2 and Figure 6(b), the improvement on CNN is 1.7% ∼ 2.8% when the corruption and noise is 0% ∼ 5%, and then the improvement goes up to 3.3% when the corruption and noise becomes 10% ∼ 15%, and then starts to decrease as the corruption and noise further increases. As discussed in Section 5, this is because although the DTM ﬁltration can robustly capture homological signals up to a moderate amount of corruption and noise, as seen in Figure 2, when the corruption and noise become too much, the topological structure starts to dissolve in the DTM ﬁltration. Also, the accuracies for CNN+P are consistently higher than the accuracies for CNN+P(i), meaning that adding PLLay in the middle of the network indeed further improves the accuracy.

25

Accuracy for MNIST data, MLP based
0.9

Accuracy

0.8

0.7

0.6

0.0 0.9

0.1

0.2

0.3

Corrupt and noise probability

(a) Test accuracy in MNIST data for MLP, MLP+S, MLP+P. Accuracy for MNIST data, CNN based

MLP MLP+S MLP+P

Accuracy

0.8
CNN CNN+S CNN+P CNN+P(i) 0.7

0.6

0.0

0.1

0.2

0.3

Corrupt and noise probability

(b) Test accuracy in MNIST data for CNN, CNN+S, CNN+P, CNN+P(i).
Figure 6: Test accuracy in MNIST experiments. PLLay contributes to consistent improvement in accuracy and robustness against noise and corruption. In particular, the improvement on CNN increases up to the moderate level of corruption and noise (∼ 15%), and then start to decrease.

26

MLP MLP+S MLP+P
CNN CNN+S CNN+P CNN+P(i)

0.00 0.2000 (0.0014) 0.2054 (0.0126) 0.8082 (0.0103) 0.9466 (0.0116) 0.9412 (0.0182) 0.9511 (0.0140) 0.9449 (0.0343)

0.05 0.2001 (0.0031) 0.2028 (0.0129) 0.7906 (0.0082) 0.9247 (0.0152) 0.8881 (0.1612) 0.9249 (0.0308) 0.9319 (0.0290)

0.10 0.1997 (0.0020) 0.2171 (0.0364) 0.7660 (0.0115) 0.9053 (0.0195) 0.8142 (0.1900) 0.9095 (0.0329) 0.8965 (0.0471)

Noise probability

0.15

0.20

0.1994 0.1998

(0.0029) (0.0009)

0.2171 0.2121

(0.0364) (0.0236)

0.7456 0.7181

(0.0104) (0.0100)

0.8791 0.8224

(0.0255) (0.1474)

0.8142 0.8197

(0.1900) (0.1473)

0.8941 0.8619

(0.0305) (0.0366)

0.8873 0.8577

(0.0143) (0.0349)

0.25 0.2003 (0.0010) 0.2159 (0.0301) 0.6942 (0.0130) 0.8323 (0.0298) 0.7777 (0.1875) 0.8480 (0.0173) 0.8285 (0.0515)

0.30 0.2004 (0.0016) 0.2115 (0.0193) 0.6545 (0.0110) 0.7963 (0.0331) 0.6580 (0.2622) 0.8087 (0.0396) 0.7954 (0.0516)

0.35 0.1999 (0.0011) 0.2057 (0.0180) 0.6218 (0.0102) 0.7401 (0.1293) 0.7195 (0.1778) 0.7668 (0.0319) 0.7543 (0.0553)

Table 3: Test accuracy in ORBIT5K experiments. In each cell, the top number corresponds to the average accuracy of the model at the noise probability, and the bottom number corresponds to the 1 standard deviation of the accuracies. At each column, the model with the best accuracy is bolded.

G.2 Orbit recognition.

For orbit recognition, we use ORBIT5K dataset [Adams et al., 2017, Carrière et al., 2020], a synthetic
dataset used as a benchmark in Topological Data Analysis. It consists of a point cloud generated by the following discrete dynamical system: given an initial point (x1, y1) ∈ [0, 1]2 and a parameter r > 0, we generate a point cloud {(xn, yn) ∈ [0, 1]2 : n = 1, . . . , N } as

xn+1 = xn + ryn(1 − yn)

mod 1,

yn+1 = yn + rxn+1(1 − xn+1) mod 1.

For comparison with Adams et al. [2017], Carrière et al. [2020], we use parameters r = 2.5, 3.5, 4.0, 4.1, 4.3, with random initialization of (x1, y1) and N = 1000 points in each simulated orbit. We generated 1000 orbits per each value of r, and randomly split the 5000 observations in 70% − 30% training-test sets as in Carrière et al. [2020]. Cross-entropy loss was used to train the network for 100 epochs, using Adam optimizer with mini-batches of size 16. For the noiseless case, the experiment for PointNet is repeated 5 times, and the experiment result for PersLay is from Carrière et al. [2020].
Topological layer. For MLP+P and CNN+P(i), we use one PLLay at the beginning of MLP and CNN models with 64 nodes and afﬁne transformation, which is solely used as the input to MLP or concatenated to the raw input to CNN. We used the empirical DTM ﬁltration in (4), where we deﬁne ﬁxed 40 × 40 points on grid on [0.0125, 0.9875]2 and use X as the empirical data points. We used m0 = 0.01, Kmax = 2, Tmin = 0.03, Tmax = 0.1, m = 17. For CNN+P, we additionally use one PLLay after the convolutional layer, with Kmax = 2, Tmin = 0.05, Tmax = 0.95, m = 18.
Baselines. For the baselines, models were designed to have simple structures for quick comparisons:

• Vanilla MLP: one hidden layer with 32 units with ReLU activations.
• CNN: two convolution layers followed by two fully connected layers.
• SLay: for comparison with PLLay, one SLay is used with 16 nodes, which is concatenated to the raw input to either MLP or CNN. We used the value ν = 0.01 for the hyperparameter of SLay.

Result. The accuracy results for ORBIT5K data in Figure 4 is represented with 1 standard errors in Table 3 and Figure 7. In Figure 7, the results for MLP, MLP+S, MLP+P are in Figure 7(a), and the results for CNN, CNN+S, CNN+P, CNN+P(i) are in Figure 7(b). From Figure 7(a), we observe that PLLay improves over MLP and MLP+S by a huge margin (42% ∼ 60%). In particular, without PLLay, MLP and MLP+S remain at random classiﬁers, which implies that the topological information is indeed critical for ORBIT5K. In Figure 7(b), PLLay improves over CNN or CNN+S consistently

27

as well. Moreover, due to the high complexity of ORBIT5K, CNN suffers from high variance at corruption and noise probability 0.2, 0.35, while PLLay can effectively reduce the variance at those simulations and make the models more stable by utilizing robust topological information from the DTM function. Also, the accuracies for CNN+P are almost always higher than the accuracies for CNN+P(i), meaning that adding PLLay in the middle of the network indeed further improves the accuracy.
28

Accuracy for ORBIT5K data, MLP based
0.8

Accuracy

0.6
MLP MLP+S MLP+P 0.4

0.2 0.0
1.0

0.1

0.2

0.3

Corrupt and noise probability

(a) Test accuracy in ORBIT5K data for MLP, MLP+S, MLP+P. Accuracy for ORBIT5K data, CNN based

Accuracy

0.8 CNN CNN+S CNN+P CNN+P(i)
0.6

0.4

0.0

0.1

0.2

0.3

Corrupt and noise probability

(b) Test accuracy in ORBIT5K data for CNN, CNN+S, CNN+P, CNN+P(i).
Figure 7: Test accuracy in ORBIT5K experiments. PLLay contributes to consistent improvement in accuracy and robustness against noise and corruption. In particular in (b), when the corruption and noise probability is 0.1, 0.25, 0.35, PLLay effectively reduces the variance of classiﬁcation accuracy.

29

