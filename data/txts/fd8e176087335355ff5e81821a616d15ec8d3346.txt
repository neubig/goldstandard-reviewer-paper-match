arXiv:2110.03484v3 [cs.LG] 14 Mar 2022

Published as a conference paper at ICLR 2022
CREATING TRAINING SETS VIA WEAK INDIRECT SUPERVISION
Jieyu Zhang1,2, Bohan Wang1,3, Xiangchen Song4, Yujing Wang1, Yaming Yang1, Jing Bai1, Alexander Ratner2,5 1Microsoft Research Asia 2University of Washington 3University of Science and Technology of China 4Carnegie Mellon University 5Snorkel AI, Inc. {jieyuz2, ajratner}@cs.washington.edu {yujwang, yayaming, jbai}@microsoft.com wbhfy@mail.ustc.edu.cn xiangchensong@cmu.edu
ABSTRACT
Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent Weak Supervision (WS) frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks are restricted to supervision sources that share the same output space as the target task. To extend the scope of usable sources, we formulate Weak Indirect Supervision (WIS), a new research problem for automatically synthesizing training labels based on indirect supervision sources that have different output label spaces. To overcome the challenge of mismatched output spaces, we develop a probabilistic modeling approach, PLRM, which uses user-provided label relations to model and leverage indirect supervision sources. Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with a generalization bound. On both image and text classiﬁcation tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%.
1 INTRODUCTION
One of the greatest bottlenecks of using modern machine learning models is the need for substantial amounts of manually-labeled training data. In real-world applications, such manual annotations are typically time-consuming, labor-intensive and static. To reduce the efforts of annotation, researchers have proposed Weak Supervision (WS) frameworks (Ratner et al., 2016; 2018; 2019; Fu et al., 2020) for synthesizing labels from multiple weak supervision sources, e.g., heuristics, knowledge bases, or pre-trained classiﬁers. These frameworks have been widely applied on various machine learning tasks (Dunnmon et al., 2020; Fries et al., 2021; Safranchik et al., 2020; Lison et al., 2020; Zhou et al., 2020; Hooper et al., 2021; Zhan et al., 2019; Varma et al., 2019) and industrial data (Bach et al., 2019). Among them, data programming (Ratner et al., 2016), one representative example that generalizes many approaches in the literature, represents weak supervision sources as labeling functions (LFs) and synthesizes training labels using Probabilistic Graphical Model (PGM).
Given both the increasing popularity of WS and the general increase in open-source availability of machine learning models and tools, there is a rising tide of available supervision sources that WS frameworks and practitioners could potentially leverage, including pre-trained machine learning models or prediction APIs (Chen et al., 2020; d’Andrea & Mintz, 2019; Yao et al., 2017). However, existing WS frameworks only utilize weak supervision sources with the same label space as the target task. This incompatibility largely limits the scope of usable sources, necessitating manual effort from domain experts to provide supervision for unseen labels. For example, consider target task of classifying {“dog”, “wolf ”, “cat”, “lion”} and a set of three weak supervision sources (e.g. trained classiﬁers or expert heuristics) with disjoint output spaces {“caninae”, “felidae”}, {“domestic animals”, “wild animals”} and {“husky”, “bengal cat”} respectively. We call these types of sources indirect supervision sources. For concreteness, we follow the general convention of data programming (Ratner et al., 2016) and refer to these sources as indirect labeling functions (ILFs).
1

Published as a conference paper at ICLR 2022
Despite their apparent utility, existing weak supervision methods could not directly leverage such ILFs, as their output spaces have no overlap with the target one.
In this paper, we formulate a novel research problem that aims to leverage such ILFs automatically, minimizing the manual efforts to develop and deploy new models. We refer to this as the Weak Indirect Supervision (WIS) setting, a new Weak Supervision paradigm which leverages ILFs, along with the relational structures between individual labels, to automatically create training labels.
The key difﬁculty of leveraging ILFs is due to the mismatched label spaces. To overcome this, we introduce pairwise relations between individual labels to the WIS setup, which are often available in structured sources (e.g. off-the-shelf Knowledge Bases (Miller, 1995; Sinha et al., 2015; Dong et al., 2020) or large scale label hierarchies (Murty et al., 2017; The Gene Ontology Consortium, 2018; Partalas et al., 2015) for various domains), or can be provided by subject matter experts in far less time than generating entirely new sets of weak supervision sources. For example, in the aforementioned example, we could rely on a biological species ontology to see that the unseen labels “dog” and “cat” are both subsumed by the seen label “domestic animals”. Based on the label relations, we can automatically leverage the supervision sources as ILFs. Notably, previous work (Qu et al., 2020) also leveraged a label relation graph but was focused on relation extraction task in a few-shot learning setting, while You et al. (2020) proposed to learn label relations given data for each label in a transfer learning scenario. In contrast, we aim to solve the target task directly and without clean labeled data.
The remaining questions are (1) how to synthesize labels based on pair-wise label relations and ILFs? and (2) How can we know whether, given a set of ILFs and label relations, the unseen labels are distinguishable or not? To address the ﬁrst question, we develop a probabilistic label relation model (PLRM), the ﬁrst PGM for WIS which aggregates the output of ILFs and models the label relations as dependencies between random variables. In turn, we use the learned PLRM to produce labels for training an end model. Furthermore, we derive the generalization error bound of PLRM based on assumptions similar to previous work (Ratner et al., 2016).
The second question presents an important stumbling block when dealing with unseen labels, as we may not be able to distinguish the unseen labels given existing label relations and ILFs, resulting in an unsatisfactory synthesized training set. To address this issue, we formally introduce the notion of distinguishability in WIS setting and theoretically establish an equivalence between: (1) the distinguishability of the label relation structure as well as the ILFs, and (2) the capability of PLRM to distinguish unseen labels. This result then leads to a simple sanity test for preventing the model from failing to distinguish unseen labels. In preliminary experiments, we observe a signiﬁcant drop in model performance when the condition is violated.
In experiments, we make non-trivial adaptations for baselines from related settings to the new WIS problem. On both text and image classiﬁcation tasks, we demonstrate the advantages of PLRM over adapted baselines. Finally, in a commercial advertising system where developers need to collect annotations for new ads tags, we illustrate how to formulate the training label collection as a WIS problem and apply PLRM to achieve an effective performance.
Summary of Contributions. Our contributions are summarized as follows:
• We formulate Weak Indirect Supervision (WIS), a new research problem which synthesizes training labels based on indirect supervision sources and label relations, minimizing human efforts of both data annotation and weak supervision sources construction;
• We develop the ﬁrst model for WIS, the Probabilistic Label Relation Model (PLRM) with comparable statistical efﬁciency to previous WS frameworks and standard supervised learning;
• We introduce a new notion of distinguishability in WIS setting, and provide a simple test of the distinguishability of PLRM for unseen labels by theoretically establishing the connection between the label relation structures and distinguishability;
• We showcase the potential of the WIS formulation and the effectiveness of PLRM in a commercial advertising system for synthesizing training labels of new ads tags. On academic image and text classiﬁcation tasks, we demonstrate the advantages of PLRM over baselines by quantitative experiments. Overall, PLRM outperforms baselines by a margin of 2%-9%.
2

Published as a conference paper at ICLR 2022

2 RELATED WORK

Table 1: Comparisons between the proposed weak indirect supervision (WIS) and related machine learning tasks. Compared to normal and weakly supervised learning, WIS handles mismatched train and test label spaces. WIS is similar in spirit to indirect supervision (IS) and zero-shot learning (ZSL), but distinct in that WIS only takes as input weak or noisy labels and a simple set of logical label relations, and aims to output a training data set rather than a trained model, affording complete modularity in which ﬁnal model class is used.

Task
Supervised Learning (SL) Weak Supervision (WS) Indirect Supervision (IS) Zero-Shot Learning (ZSL)
Weak Indirect Supervision (WIS)

Label Type
Clean Labels Noisy Sources Clean Labels Clean Labels
Noisy Sources

Ytrain = Ytest

Label Information
– – Label Trans. Matrix Label Embed. / Attribute
Label Relation

When Label Info. is Required
– – Training Training & Test
Training

We brieﬂy review related settings. The comparison between WIS and related tasks is in Table 1.
Weak Supervision: We draw motivation from recent work which model and integrate weak supervision sources using PGMs (Ratner et al., 2016; 2018; 2019; Fu et al., 2020) and other methods (Guan et al., 2018; Khetan et al., 2018) to create training sets. While they assume supervision sources share the same label space as the new tasks, we aim to leverage indirect supervision sources with mismatched label spaces in a labor-free way.
Indirect Supervision: Indirect supervision arises more generally in latent-variable models for various domains (Brown et al., 1993; Liang et al., 2013; Quattoni et al., 2004; Chang et al., 2010; Zhang et al., 2019). Very recently, Raghunathan et al. (2016) proposed to use the linear moment method for indirect supervision, wherein the transition between desired label space Y and indirect supervision space O is known, as well as the ground truth of indirect supervisions for training. In contrast, both are unavailable in WIS. Theoretically, Wang et al. (2020) developed a uniﬁed framework for analyzing the learnability of indirect supervision with shared or superset label spaces, while we focus on disjoint label spaces and the consequent unique challenge of distinguishability of unseen classes.
Zero-Shot Learning: Zero-Shot Learning (ZSL) (Lampert et al., 2009; Wang et al., 2019) aims to learn a classiﬁer that is able to generalize to unseen classes. The WIS problem differentiates from ZSL by (1) in ZSL setting, the training and test data belong to seen and unseen classes, respectively, and training data is labeled, while for WIS, both training and test data belong to unseen classes and unlabeled; (2) ZSL tends to render a classiﬁer that could predict unseen classes given certain label information, e.g., label attributes (Romera-Paredes & Torr, 2015), label descriptions (Srivastava et al., 2018) or label similarities (Frome et al., 2013), while WIS aims to provide training labels for unlabeled training data, allowing users to train any machine learning models, and the label relations are used only in synthesizing training labels.

3 PRELIMINARY: WEAK SUPERVISION
We ﬁrst describe the Weak Supervision (WS) setting. A glossary of notations used is in App. A.
Deﬁnitions and notations. We assume a k-way classiﬁcation task, and have an unlabeled dataset D consisting of m data points. Denote by Xi ∈ X the individual data point and Yi ∈ Y = {y1, . . . , yk} the unobserved interested label of Xi. We also have n sources, each represented by a labeling function (LF) and denoted by λj. Each λj outputs a label Yˆij ∈ Yλj = {yˆ1j, . . . , yˆkjλj } on Xi, where Yλj is the label space associated with λj and |Yλj | = kλj . We denote the concatenation of LFs’ output as Yˆi = [Yˆi1, Yˆi2, . . . , Yˆin], and the union set of LFs’ label spaces as Yˆ with |Yˆ| = kˆ. Note that kˆ is not necessarily equal to the sum over kλj , since LFs may have overlapping label spaces. We call yˆ ∈ Yˆ seen label and y ∈ Y desired labels. In WS settings, we have Y ⊂ Yˆ. Notably, we assume all the involved labels come from the same semantic space.
The goal of WS. The goal is to infer the training labels for the dataset D based on LFs, and to use them to train an end discriminative classiﬁer fW : X → Y, all without ground truth training labels.

3

Published as a conference paper at ICLR 2022

Bengal Cat

Cat

Felidae

Domestic Animal

Lion Wild Animal Wolf

Canine

Dog Husky

Label Graph

Canine Felidae Domestic Animals Wild Animals Husky Bengal Cat

…

def labeling rule: return label
< l a t e x i t s h a 1 _ b a s e 6 4 = " / 2 W W k R Y r M M s z g R L W + x I d y M J R c 1 I = " > A A A B 8 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s w U U Z c F N 6 6 k g n 1 I O 5 R M J t O G J p k h y Q h l 6 F e 4 c a G I W z / H n X 9 j O p 2 F t h 4 I H M 4 5 l 9 x 7 g o Q z b V z 3 2 y m t r W 9 s b p W 3 K z u 7 e / s H 1 c O j j o 5 T R W i b x D x W v Q B r y p m k b c M M p 7 1 E U S w C T r v B 5 G b u d 5 + o 0 i y W D 2 a a U F / g k W Q R I 9 h Y 6 X H A b T T E Q 2 9 Y r b l 1 N w d a J V 5 B a l C g N a x + D c K Y p I J K Q z j W u u + 5 i f E z r A w j n M 4 q g 1 T T B J M J H t G + p R I L q v 0 s X 3 i G z q w S o i h W 9 k m D c v X 3 R I a F 1 l M R 2 K T A Z q y X v b n 4 n 9 d P T X T t Z 0 w m q a G S L D 6 K U o 5 M j O b X o 5 A p S g y f W o K J Y n Z X R M Z Y Y W J s R x V b g r d 8 8 i r p N O r e Z b 1 x f 1 F r 3 h V 1 l O E E T u E c P L i C J t x C C 9 p A Q M A z v M K b o 5 w X 5 9 3 5 W E R L T j F z D H / g f P 4 A b L W Q N A = = < / l a t e x i t >
1

def labeling rule: return label

def labeling rule: return label

< l a t e x i t s h a 1 _ b a s e 6 4 = " t j Q b C w f / n j h U 6 F s j F W C k k U m x w w U = " > A A A B 8 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s w U U Z c F N 6 6 k g n 1 I O 5 R M J t O G J p k h y Q h l 6 F e 4 c a G I W z / H n X 9 j O p 2 F t h 4 I H M 4 5 l 9 x 7 g o Q z b V z 3 2 y m t r W 9 s b p W 3 K z u 7 e / s H 1 c O j j o 5 T R W i b x D x W v Q B r y p m k b c M M p 7 1 E U S w C T r v B 5 G b u d 5 + o 0 i y W D 2 a a U F / g k W Q R I 9 h Y 6 X H A b T T E w 8 a w W n P r b g 6 0 S r y C 1 K B A a 1 j 9 G o Q x S Q W V h n C s d d 9 z E + N n W B l G O J 1 V B q m m C S Y T P K J 9 S y U W V P t Z v v A M n V k l R F G s 7 J M G 5 e r v i Q w L r a c i s E m B z V g v e 3 P x P 6 + f m u j a z 5 h M U k M l W X w U p R y Z G M 2 v R y F T l B g + t Q Q T x e y u i I y x w s T Y j i q 2 B G / 5 5 F X S a d S 9 y 3 r j / q L W v C v q K M M J n M I 5 e H A F T b i F F r S B g I B n e I U 3 R z k v z r v z s Y i W n G L m G P 7 A + f w B b j m Q N Q = = < / l a t e x i t >
2 Indirect Labeling Functions

< l a t e x i t s h a 1 _ b a s e 6 4 = " A K + 4 F + N k 7 x H 8 t W K b T O v g m T 7 b F K I = " > A A A B 8 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s w U U Z c F N 6 6 k g n 1 I O 5 R M J t O G J p k h y Q h l 6 F e 4 c a G I W z / H n X 9 j O p 2 F t h 4 I H M 4 5 l 9 x 7 g o Q z b V z 3 2 y m t r W 9 s b p W 3 K z u 7 e / s H 1 c O j j o 5 T R W i b x D x W v Q B r y p m k b c M M p 7 1 E U S w C T r v B 5 G b u d 5 + o 0 i y W D 2 a a U F / g k W Q R I 9 h Y 6 X H A b T T E Q z m s 1 t y 6 m w O t E q 8 g N S j Q G l a / B m F M U k G l I R x r 3 f f c x P g Z V o Y R T m e V Q a p p g s k E j 2 j f U o k F 1 X 6 W L z x D Z 1 Y J U R Q r + 6 R B u f p 7 I s N C 6 6 k I b F J g M 9 b L 3 l z 8 z + u n J r r 2 M y a T 1 F B J F h 9 F K U c m R v P r U c g U J Y Z P L c F E M b s r I m O s M D G 2 o 4 o t w V s + e Z V 0 G n X v s t 6 4 v 6 g 1 7 4 o 6 y n A C p 3 A O H l x B E 2 6 h B W 0 g I O A Z X u H N U c 6 L 8 + 5 8 L K I l p 5 g 5 h j 9 w P n 8 A y S m Q c Q = = < / l a t e x i t >
n

Unlabeled Data

Y< l a t e x i t s h a 1 _ b a s e 6 4 = " g G F W U Y b 8 2 Z T 3 a X 9 z O T R e e s 8 y C u w = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k q M e C F 4 8 t 2 F Z p Q 9 l s J + 3 a z S b s b o R S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B V c G 8 / 7 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o p Z N M M W y y R C T q P q Q a B Z f Y N N w I v E 8 V 0 j g U 2 A 5 H N z O / / Y R K 8 0 T e m X G K Q U w H k k e c U W O l x k O v X P F c b w 6 y S v y c V C B H v V f + 6 v Y T l s U o D R N U 6 4 7 v p S a Y U G U 4 E z g t d T O N K W U j O s C O p Z L G q I P J / N A p O b N K n 0 S J s i U N m a u / J y Y 0 1 n o c h 7 Y z p m a o l 7 2 Z + J / X y U x 0 H U y 4 T D O D k i 0 W R Z k g J i G z r 0 m f K 2 R G j C 2 h T H F 7 K 2 F D q i g z N p u S D c F f f n m V t C 5 c / 9 K t N q q V m p v H U Y Q T O I V z 8 O E K a n A L d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f t C e M 0 g = = < / l a t e x i t >

Y¯< l a t e x i t s h a 1 _ b a s e 6 4 = " 8 l Z f x I c X K Q / k Q y 2 c O T y l f a M q f 2 c = " > A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V o h 6 L X j x W s F + 0 a 8 m m 2 T Y 0 y S 5 J V i h L f 4 U X D 4 p 4 9 e d 4 8 9 + Y b f e g r Q 8 G H u / N M D M v i D n T x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 S h R h D Z J x C P V C b C m n E n a N M x w 2 o k V x S L g t B 1 M b j O / / U S V Z p F 8 M N O Y + g K P J A s Z w c Z K 3 X 6 A V d q d P X q D c s W t u n O g V e L l p A I 5 G o P y V 3 8 Y k U R Q a Q j H W v c 8 N z Z + i p V h h N N Z q Z 9 o G m M y w S P a s 1 R i Q b W f z g + e o T O r D F E Y K V v S o L n 6 e y L F Q u u p C G y n w G a s l 7 1 M / M / r J S a 8 9 l M m 4 8 R Q S R a L w o Q j E 6 H s e z R k i h L D p 5 Z g o p i 9 F Z E x V p g Y m 1 H J h u A t v 7 x K W h d V 7 7 J a u 6 9 V 6 j d 5 H E U 4 g V M 4 B w + u o A 5 3 0 I A m E B D w D K / w 5 i j n x X l 3 P h a t B S e f O Y Y / c D 5 / A K C a k E 4 = < / l a t e x i t >

1

Y¯< l a t e x i t s h a 1 _ b a s e 6 4 = " / F J i O g B e N a C 3 A 4 e I p g M Q n F 9 L P a Y = " > A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 k t p X o s e v F Y w X 7 R r i W b Z t v Q J L s k W a E s / R V e P C j i 1 Z / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e E H O m j e t + O 7 m N z a 3 t n f x u Y W / / 4 P C o e H z S 0 l G i C G 2 S i E e q E 2 B N O Z O 0 a Z j h t B M r i k X A a T u Y 3 M 7 9 9 h N V m k X y w U x j 6 g s 8 k i x k B B s r d f s B V m l 3 9 l g Z F E t u 2 V 0 A r R M v I y X I 0 B g U v / r D i C S C S k M 4 1 r r n u b H x U 6 w M I 5 z O C v 1 E 0 x i T C R 7 R n q U S C 6 r 9 d H H w D F 1 Y Z Y j C S N m S B i 3 U 3 x M p F l p P R W A 7 B T Z j v e r N x f + 8 X m L C a z 9 l M k 4 M l W S 5 K E w 4 M h G a f 4 + G T F F i + N Q S T B S z t y I y x g o T Y z M q 2 B C 8 1 Z f X S a t S 9 m r l 6 n 2 1 V L / J 4 s j D G Z z D J X h w B X W 4 g w Y 0 g Y C A Z 3 i F N 0 c 5 L 8 6 7 8 7 F s z T n Z z C n 8 g f P 5 A 6 I e k E 8 = < / l a t e x i t >

2

Yˆ< l a t e x i t s h a 1 _ b a s e 6 4 = " k f B l L C 0 C L T 9 J I / U V y 5 D w h N 5 C l d Q = " > A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K U I 9 B L x 4 j m B f J G m Y n k 2 T I 7 O w y 0 y u E J V / h x Y M i X v 0 c b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V x F I Y d N 1 v J 7 e 2 v r G 5 l d 8 u 7 O z u 7 R 8 U D 4 8 a J k o 0 4 3 U W y U i 3 A m q 4 F I r X U a D k r V h z G g a S N 4 P x 7 c x v P n F t R K Q e c B J z P 6 R D J Q a C U b R S u z u i m L a n j 1 6 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 1 H L A m 5 Q i a p M R 3 P j d F P q U b B J J 8 W u o n h M W V j O u Q d S x U N u f H T + c F T c m a V P h l E 2 p Z C M l d / T 6 Q 0 N G Y S B r Y z p D g y y 9 5 M / M / r J D i 4 9 l O h 4 g S 5 Y o t F g 0 Q S j M j s e 9 I X m j O U E 0 s o 0 8 L e S t i I a s r Q Z l S w I X j L L 6 + S x k X Z u y x X 7 i u l 6 k 0 W R x 5 O 4 B T O w Y M r q M I d 1 K A O D E J 4 h l d 4 c 7 T z 4 r w 7 H 4 v W n J P N H M M f O J 8 / r O 6 Q V g = = < / l a t e x i t >

1

Yˆ< l a t e x i t s h a 1 _ b a s e 6 4 = " P m y x B n H S Z R H e 1 D 8 7 A l o n O U J j 7 U o = " > A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y h A 8 h d 0 Q 1 G P Q i 8 c I 5 k W y h t n J J B k y M 7 v M 9 A p h y V d 4 8 a C I V z / H m 3 / j J N m D J h Y 0 F F X d d H e F s e A G P e / b W V v f 2 N z a z u 3 k d / f 2 D w 4 L R 8 c N E y W a s j q N R K R b I T F M c M X q y F G w V q w Z k a F g z X B 8 O / O b T 0 w b H q k H n M Q s k G S o + I B T g l Z q d 0 c E 0 / b 0 s d w r F L 2 S N 4 e 7 S v y M F C F D r V f 4 6 v Y j m k i m k A p i T M f 3 Y g x S o p F T w a b 5 b m J Y T O i Y D F n H U k U k M 0 E 6 P 3 j q n l u l 7 w 4 i b U u h O 1 d / T 6 R E G j O R o e 2 U B E d m 2 Z u J / 3 m d B A f X Q c p V n C B T d L F o k A g X I 3 f 2 v d v n m l E U E 0 s I 1 d z e 6 t I R 0 Y S i z S h v Q / C X X 1 4 l j X L J v y x V 7 i v F 6 k 0 W R w 5 O 4 Q w u w I c r q M I d 1 K A O F C Q 8 w y u 8 O d p 5 c d 6 d j 0 X r m p P N n M A f O J 8 / r n K Q V w = = < / l a t e x i t >

2

Yˆ< l a t e x i t s h a 1 _ b a s e 6 4 = " x a H e Q P K G O A 0 g Y X E L K L P 2 A d C S / H g = " > A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x q U I 9 B L x 4 j m B d J D L O T 2 W T I z O 4 y 0 y u E J V / h x Y M i X v 0 c b / 6 N k 2 Q P G i 1 o K K q 6 6 e 7 y Y y k M u u 6 X k 1 t Z X V v f y G 8 W t r Z 3 d v e K + w c N E y W a 8 T q L Z K R b P j V c i p D X U a D k r V h z q n z J m / 7 4 Z u Y 3 H 7 k 2 I g r v c R L z n q L D U A S C U b R S u z u i m L a n D + f 9 Y s k t u 3 O Q v 8 T L S A k y 1 P r F z + 4 g Y o n i I T J J j e l 4 b o y 9 l G o U T P J p o Z s Y H l M 2 p k P e s T S k i p t e O j 9 4 S k 6 s M i B B p G 2 F S O b q z 4 m U K m M m y r e d i u L I L H s z 8 T + v k 2 B w 1 U t F G C f I Q 7 Z Y F C S S Y E R m 3 5 O B 0 J y h n F h C m R b 2 V s J G V F O G N q O C D c F b f v k v a Z y V v Y t y 5 a 5 S q l 5 n c e T h C I 7 h F D y 4 h C r c Q g 3 q w E D B E 7 z A q 6 O d Z + f N e V + 0 5 p x s 5 h B + w f n 4 B q / 2 k F g = < / l a t e x i t >

3

Label Model

Probabilistic Labels

Cat Lion Dog Wolf End Model

Input

Figure 1: An example of WIS problem: the input consists of an unlabeled dataset, a label graph, and n indirect labeling functions (ILFs). The ILFs represent weak supervision sources such as pretrained classiﬁers, knowledge bases, heuristic rules, etc. We can see that the ILFs cannot predict desired labels i.e., {“dog”, “wolf ”, “cat”, “lion”}. To address this, a label graph is given; here we only visualize the subsuming relation. Finally, a label model, instantiated as a PGM, takes the ILF’s outputs and produces probabilistic labels in the target output space, which are in turn used to train an end machine learning model that can generalize beyond them.

4 WEAK INDIRECT SUPERVISION

Now, we introduce the new Weak Indirect Supervision (WIS) problem. Unlike the standard WS setting, we only have indirect labeling functions (ILFs) instead of LFs, and an additional label graph is given. The goal of WIS remains the same as WS. An example of WIS problem is in Fig. 1.
Indirect Labeling Function. In WIS, we only have indirect labeling functions (ILFs), which cannot directly predict any desired labels, i.e., Yˆ ∩ Y = ∅. Therefore, we refer to the desired labels as unseen labels. To make it possible to leverage the ILFs, a label graph is given, which encodes pair-wise label relations between different seen and unseen labels.
Label Graph. Concretely, a label graph G = (V, E) consists of (1) a set of all the labels as nodes, i.e., V = Yˆ ∪ Y, and (2) a set of pair-wise label relations as typed edges, i.e., E = {(yi, yj, tyiyj )|tyiyj ∈ T , i < j, ∀yi, yj ∈ V}. Here, T is the set of label relation types and, similar to Deng et al. (2014), there are four types of label relations: exclusive, overlapping, subsuming, subsumed, notated by to, te, tsg, tsd, respectively. Notably, for any ordered pair of labels (yi, yj), their label relation should fall into one of the four types. The rationale behind these label relations is that when treating each label as a set, there are four unique set relations and each corresponds to one deﬁned label relation respectively as shown in Fig. 2. For convenience, we denote the set of non-exclusive neighbors of a given label y in Yˆ as N (y, Yˆ), i.e., N (y, Yˆ) = {yˆ ∈ Yˆ|tyyˆ = te}.

Label relation: Set relation

Exclusive

< l a t e x i t s h a 1 _ b a s e 6 4 = " Q q I K p l T 5 X x 6 / r l 9 7 + e S s Y 2 p s x n k = " > A A A C A X i c b V C 7 S k N B E N 0 b j c b 4 u m o j 2 C w m g o 3 h X g t N o 0 R t t I t g H p C E s H c z S Z b s f b A 7 V w g h N v 6 K j Y U i t j Z + g 5 1 / 4 + Z R a O K B g c M 5 M 8 z M 8 S I p N D r O t 5 V Y W E w u L a d W 0 q t r 6 x u b 9 t Z 2 W Y e x 4 l D i o Q x V 1 W M a p A i g h A I l V C M F z P c k V L z e 1 c i v 3 I P S I g z u s B 9 B w 2 e d Q L Q F Z 2 i k p r 2 b z V 7 U O Y v o J T 2 j d f A j 7 G t A m s 0 2 7 Y y T c 8 a g 8 8 S d k k w h l f 9 M n h 9 5 x a b 9 V W + F P P Y h Q C 6 Z 1 j X X i b A x Y A o F l z B M 1 2 M N E e M 9 1 o G a o Q H z Q T c G 4 w + G 9 M A o L d o O l a k A 6 V j 9 P T F g v t Z 9 3 z O d P s O u n v V G 4 n 9 e L c Z 2 v j E Q Q R Q j B H y y q B 1 L i i E d x U F b Q g F H 2 T e E c S X M r Z R 3 m W I c T W h p E 4 I 7 + / I 8 K R / n 3 J O c c 2 v S u C E T p M g e 2 S e H x C W n p E C u S Z G U C C c P 5 I m 8 k F f r 0 X q 2 3 q z 3 S W v C m s 7 s k D + w P n 4 A B r u W 7 A = = < / l a t e x i t >
A

\

B

=;

Subsuming

< l a t e x i t s h a 1 _ b a s e 6 4 = " z O k S 5 B k n z F 9 r K 9 h L 8 6 q 5 C p V O M O s = " > A A A B + 3 i c b V D L T g J B E O z 1 i f h a 8 e h l I p h 4 I r s e 1 J u o F 7 1 h I o 8 E C J k d G p g w O 7 v M z B o J 4 V e 8 e N A Y r / 6 H 8 e b f O D w O C l b S S a W q O 9 1 d Q S y 4 N p 7 3 7 S w t r 6 y u r a c 2 0 p t b 2 z u 7 7 l 6 m r K N E M S y x S E S q G l C N g k s s G W 4 E V m O F N A w E V o L e 9 d i v P K D S P J L 3 Z h B j I 6 Q d y d u c U W O l p p v J 5 S 7 r O o k 1 G o n 9 P r n K 5 Z p u 1 s t 7 E 5 B F 4 s 9 I 9 u I T J i g 2 3 a 9 6 K 2 J J i N I w Q b W u + V 5 s G k O q D G c C R + l 6 o j G m r E c 7 W L N U 0 h B 1 Y z i 5 f U S O r N I i 7 U j Z k o Z M 1 N 8 T Q x p q P Q g D 2 x l S 0 9 X z 3 l j 8 z 6 s l p n 3 e G H I Z J w Y l m y 5 q J 4 K Y i I y D I C 2 u k B k x s I Q y x e 2 t h H W p o s z Y u N I 2 B H / + 5 U V S P s n 7 p 3 n v z s s W b q d p Q A o O 4 B C O w Y c z K M A N F K E E D B 7 h C V 7 g 1 R k 5 z 8 6 b 8 z 5 t X X J m M / v w B 8 7 H D 7 b B l F 4 = < / l a t e x i t >
A

%

B

Subsumed

< l a t e x i t s h a 1 _ b a s e 6 4 = " W u J y 3 h L U Y G k w k R + T K O x Q + P q p w Y g = " > A A A B + 3 i c b V D L T g J B E O z 1 i f h a 8 e h l I p h 4 I r s e 1 J u o F 7 1 h I o 8 E C J k d G p g w O 7 v M z B o J 4 V e 8 e N A Y r / 6 H 8 e b f O D w O C l b S S a W q O 9 1 d Q S y 4 N p 7 3 7 S w t r 6 y u r a c 2 0 p t b 2 z u 7 7 l 6 m r K N E M S y x S E S q G l C N g k s s G W 4 E V m O F N A w E V o L e 9 d i v P K D S P J L 3 Z h B j I 6 Q d y d u c U W O l p p v J 5 S 7 r O g k 0 G o n 9 P r n K 5 Z p u 1 s t 7 E 5 B F 4 s 9 I 9 u I T J i g 2 3 a 9 6 K 2 J J i N I w Q b W u + V 5 s G k O q D G c C R + l 6 o j G m r E c 7 W L N U 0 h B 1 Y z i 5 f U S O r N I i 7 U j Z k o Z M 1 N 8 T Q x p q P Q g D 2 x l S 0 9 X z 3 l j 8 z 6 s l p n 3 e G H I Z J w Y l m y 5 q J 4 K Y i I y D I C 2 u k B k x s I Q y x e 2 t h H W p o s z Y u N I 2 B H / + 5 U V S P s n 7 p 3 n v z s s W b q d p Q A o O 4 B C O w Y c z K M A N F K E E D B 7 h C V 7 g 1 R k 5 z 8 6 b 8 z 5 t X X J m M / v w B 8 7 H D 6 D v l F A = < / l a t e x i t >
A

$

B

< l a t e x i t s h a 1 _ b a s e 6 4 = " W b e h n k O m V K 1 p Y Y 8 r p V P P b / 9 E D e Y = " > A A A C I X i c b V D L T g J B E J z 1 i f h C P X q Z C C Y e C N n 1 g B w F L 3 r T R N S E J W R 2 a G D C 7 O w 6 0 2 t C i B c / x I s / 4 E f o w Y P G e D N + h X / g A C Y q W M k k 1 V X d 6 e k K Y i k M u u 6 7 M z U 9 M z s 3 n 1 p I L y 4 t r 6 x m 1 t b P T J R o D l U e y U h f B M y A F A q q K F D C R a y B h Y G E 8 6 B 7 M P D P r 0 A b E a l T 7 M V Q D 1 l b i Z b g D K 3 U y J R y u b L P W U w r v o J L H 8 I Y e w Y w T 8 u + i t A 3 S W A r W s k P / J + 6 n M s 1 M l m 3 4 A 5 B J 4 n 3 T b L 7 2 c / H m + p 9 8 b i R e f O b E U 9 C U M g l M 6 b m u T H W + 0 y j 4 B K u 0 3 5 i I G a 8 y 9 p Q s 1 S x E E y 9 P 7 z w m m 5 b p U l b k b Z P I R 2 q v y f 6 L D S m F w a 2 M 2 T Y M e P e Q P z P q y X Y K t X 7 Q s U J g u K j R a 1 E U o z o I C 7 a F B o 4 y p 4 l j G t h / 0 p 5 h 2 n G 0 Y a a t i F 4 4 y d P k r P d g l c s u C c 2 j S M y Q o p s k i 2 y Q z y y R / b J I T k m V c L J L X k g z + T F u X O e n F f n b d Q 6 5 X z P b J A / c D 6 + A D i P p g 8 = < / l a t e x i t >
A

\

B

Overlap 6= ;, A 6⇢ B, B

6⇢ A

A

B

AB

BA

A

B

Figure 2: The one-to-one mapping between label relations and set relations.

5 PROBABILISTIC LABEL RELATION MODEL
One of the key difﬁculties in both WS and WIS is that we do not observe the true label Yi. Following prior work (Ratner et al., 2016; 2019; Fu et al., 2020), we use a latent variable Probabilistic Graphical Model (PGM) for estimating Yi based on the Yˆi output by ILFs. Speciﬁcally, the PGM is instantiated
4

Published as a conference paper at ICLR 2022

as a factor graph model. This standard technique lets us describe the family of generative distributions in terms of M known dependencies/factor functions {φ}, and an unknown parameter Θ ∈ RM as PΘ(·) ∝ exp(Θ Φ(·)), where Φ is the concatenation of {φ}. However, the unique challenge for WIS is that the dependencies {φ} between Yi and Yˆi are unknown due to the mismatches of label spaces. We overcome these by leveraging the label graph G to build the dependencies for the PGM.

5.1 A BASELINE PGM FOR WIS

In prior work (Ratner et al., 2016; Bach et al., 2017), the PGM for WS is governed by accuracy

dependencies:

φAy,cjc(Y, Yˆ j ) := 1{Y = Yˆ j = y}

which is deﬁned for each λj and y ∈ Yλj ∩ Y. However, in WIS, the ILFs cannot predict desired label y ∈ Y. As a simple baseline approach to start, we leverage the coarse-grained exclusive/non-exclusive label relation to build a corresponding "accuracy" factor. Speciﬁcally, for an ILF λj and one label
yˆ ∈ Yλj , given a desired label y ∈ Y, if yˆ and y have non-exclusive label relation, i.e., yˆ ∈ N (y, Yλj )
we expect a certain portion of data assigned yˆ should be labeled as y. Thus, we treat Yˆ j = yˆ as a pseudo indicator of Y = y and add a pseudo accuracy dependency between them:

φAy,cyˆc,j (Y, Yˆ j ) := 1{Y = y ∧ Yˆ j = yˆ}

We call the PGM governed by pseudo accuracy dependencies Weak Supervision with Label Graph (WS-LG). Notably, it can be treated as a simple adaptation of PGM for WS (Ratner et al., 2016; 2019; Fu et al., 2020) to the WIS problem. However, such a naïve adaptation might have two drawbacks:
1. It does not model speciﬁc dependencies ILFs with different undesired labels. For example, two ILFs outputting “Husky” and “bulldog” respectively would be naively modeled the same as if they both output “Dog”.
2. It can only directly model exclusive/non-exclusive label relations, ignoring the prior knowledge encoded in other relation types, i.e., subsuming, subsumed, or overlapping. For example, given an unseen label “Dog” and some ILFs outputting “Husky” or “Domestic Animals”, WS-LG would treat all ILFs as indicators of “Dog”. However, we know a “Husky” is of course a “Dog” (subsumed relation) while a “Domestic Animals” is not necessarily a “Dog” (subsuming relation).

5.2 PROBABILISTIC LABEL RELATION MODEL
To more directly model the full range and nuance of label relations, we propose a new probabilistic label relation model (PLRM). In PLRM, we explicitly model both (1) the dependency between ILF outputs and the true labels in their output spaces, i.e. their direct accuracy, and (2) the dependencies between these labels and the target unseen labels, as separate dependency types, thus explicitly incorporating the full label relation graph into our model and learning its corresponding weights.
Concretely, we augment the WS-LG model with (1) latent variables representing the assignment of the data to each seen label, and (2) label relation dependencies which capture ﬁne-grained label relations between these output labels and desired labels. To model seen label in Yˆ, we introduce a binary latent random vector Y¯ = [Y¯ 1, . . . , Y¯ kˆ], where Y¯ i indicating whether the data should be assigned yˆi. Then, for ILF λj that could predict yˆi, we have accuracy dependency:
φAyˆc,cj (Y¯ i, Yˆ j ) := 1{Y¯ i = 1 ∧ Yˆ j = yˆi} i
To model ﬁne-grained label relations, for a desired label y ∈ Y and seen label yˆi ∈ Yˆ, we add label relation dependencies. We enumerate the label relation dependencies corresponding to the four label relation types, i.e., exclusive, overlapping, subsuming, subsumed, as follows:
φey,yˆ (Y, Y¯ i) := − 1{Y = y ∧ Y¯ i = 1} i
φoy,yˆ (Y, Y¯ i) := 1{Y = y ∧ Y¯ i = 1} i
φsyg,yˆ (Y, Y¯ i) := − 1{Y = y ∧ Y¯ i = 1} i
φsyd,yˆ (Y, Y¯ i) := − 1{Y = y ∧ Y¯ i = 0} i

5

Published as a conference paper at ICLR 2022

The above dependencies encode the prior knowledge of the label relations, but also allow the model to learn corresponding parameters. For example, an exclusive label relation dependency φe outputs -1 when two exclusive labels are activated at the same time for the same data, otherwise 0, which reﬂects our prior knowledge of the exclusive label relation; and the corresponding parameter can be treated as the strength of the label relation. Likewise, for any pair of seen labels, we add label relation dependency following the same convention. Finally, we specify the model as:

PΘ(Y, Y¯ , Yˆ ) ∝ exp Θ Φ(Y, Y¯ , Yˆ ) .

(1)

Recall that Y is the unobserved true label, Y¯ is the binary random vector, each of whose binary value Y¯ i reﬂects whether the data should be assigned seen label yˆi ∈ Yˆ, and Yˆ is the concatenated outputs of ILFs.
Learning Objective. We estimate the parameters Θˆ by minimizing the negative log marginal likelihood PΘ(Yˆ ) for observed ILF outputs Yˆ1:m:

m

Θˆ = arg min − log PΘ(Y, Y¯ , Yˆi) .

(2)

Θ

i=1

Y,Y¯

We follow Ratner et al. (2016) to optimize the objective using stochastic gradient descent.

Training an End Model. Let pΘˆ (Y | Yˆ ) be the probabilistic label (i.e. distribution) predicted by learned PLRM. We then train an end model fW : X → Y parameterized by W , by minimizing the empirical noise-aware loss (Ratner et al., 2019) with respect to Θˆ over m unlabeled data points:

Wˆ = arg min 1

m
E

ˆ (Y, fW (Xi)),

(3)

Wm

Y ∼pΘˆ (Y |Yi)

i=1

where (Y, fW (Xi)) is a standard cross entropy loss.

Generalization Error Bound. We extend previous results from (Ratner et al., 2016) to bound both the expected error of learned parameter Θˆ and the expected risk for Wˆ . All the proof details and description of assumptions can be found in Appendix.
Theorem 1. Suppose that we run stochastic gradient descent to produce Θˆ and Wˆ based on Eqs. (2) and (3), respectively, and that our setup satisﬁes certain assumptions (App D.2). Let |D| be the size of the unlabeled dataset. Then we have

2
E Θˆ − Θ∗ ≤ O

M log |D|

,

E

(Wˆ ) − (W ∗) ≤ χ + O H

log |D|

.

|D|

|D|

Interpreting the Bound. By Theorem 1, the two errors decrease by the rate O˜(1/|D|) and O˜(1/|D|1/2) respectively as |D| increases. This shows that although we trade computational efﬁciency for the reduction of human efforts by using complex dependencies and more latent variables,
we maintain comparable statistical efﬁciency as previous WS frameworks and supervised learning
theoretically.

6 DISTINGUISHABILITY OF UNSEEN LABELS

One unique challenge of WIS is that there may exist pairs of unseen labels which cannot be distinguished by the learned model. For example, as shown in Fig. 3, where “Dog” is a seen label for which LFs could predict for and “Husky” and “Bulldog” are unseen labels for which we want to generate training labels; however, we could not distinguish between “Husky” and “Bulldog” even though the LFs make correct predictions of seen label “Dog”, because both “Husky” and “Bulldog” share the same label relation to “Dog”.
To tackle this issue, we theoretically connect the distinguishability of unseen labels to the label relation structures and provide a testable

Exclusive

Dog

Subsuming

Husky

Bulldog

Figure 3: Example of indistinguishable unseen labels “Husky” and “Bulldog”.

6

Published as a conference paper at ICLR 2022

condition for the distinguishability. Intuitively, same label relation structures could lead to indistinguishable unseen labels as shown in Fig. 3; however, it turns out to be challenging to prove that different label relation structures could guarantee the distinguishability with respect to the model. To illustrate, we formally deﬁne the distinguishability as below.
Deﬁnition 1 (Distinguishability). For any model PΘ(Y, Y¯ , Yˆ ) with parameters Θ, any pair of unseen labels yi, yj ∈ Y are distinguishable w.r.t. the model, if for a.e. Θ > 0 (element-wisely), there does NOT exist such a Θ˜ > 0 that, for ∀Y¯ , Yˆ , the following equations hold

PΘ(Y = yi|Y¯ , Yˆ ) = PΘ˜ (Y = yj|Y¯ , Yˆ ), PΘ(Y = yj|Y¯ , Yˆ ) = PΘ˜ (Y = yi|Y¯ , Yˆ ), (4)

PΘ(Y = y|Y¯ , Yˆ ) = PΘ˜ (Y = y|Y¯ , Yˆ ), ∀y ∈ Y/{yi, yj},

(5)

PΘ(Yˆ ) = PΘ˜ (Yˆ ).

(6)

From the deﬁnition, we can see that the opposite of distinguishability, i.e., indistinguishability, describes an undesired model: for any learned parameter Θ > 0, we can always ﬁnd another Θ˜ which optimizes the loss equally well (Eq. (6)), but Eqs. (4-5) implies whenever PΘ predict yi, PΘ˜ will predict yj instead, which reﬂects that the model cannot distinguish the two unseen labels. Note that the notion of distinguishability is different from the identiﬁability in PGMs: the generic identiﬁability (Allman et al., 2015), the strongest notion of identiﬁability, requires the model to be identiﬁable up to label swapping, while the distinguishability aims to avoid the label swapping.
However, distinguishability is hard to verify since Eqs. (4-5) and (6) need to hold for any possible conﬁguration of Y¯ , Yˆ , and any pair of unseen labels. Fortunately, for the proposed PLRM, we prove that distinguishability is equivalent to the asymmetry of the label relation structures when two conditions hold. To state the required conditions, we ﬁrst introduce the notations of consistency and informativeness to characterize the label graph and ILFs.
Consistency. We discuss the consistency of a label graph to avoid an ambiguous or unrealistic label graph. We interpret semantic labels ya, yb as sets A, B, and then connect the label relations to the set relations (Fig. 2). Given the set interpretations, we deﬁne the consistency of label graph as:
Deﬁnition 2 (Consistent Label Graph). A label graph G = (Y, E) is consistent if the induced set relations are consistent.
For example, assume Y = {ya, yb, yc}, and tab = tbc = tca = tsg. From tab, tbc, we can observe that A B C, which contradicts to C A implied by tca = tsg. Thus, G is inconsistent.
Informativeness. In addition, we try to describe what kind of ILF is desired. Intuitively, an ILF is uninformative if it always "votes" for one of the desired labels. For example, if the desired label space Y is {“Dog”, “Bird”}, then for an ILF λ1 outputting {“Husky”, “Bulldog”}, we know “Dog” is non-exclusive to “Husky” and “Bulldog”, while “Bird” exclusive to both. In such case, λ1 can hardly provide information to help distinguish “Dog” from “Bird”, because it always votes for “Dog”. On the other hand, a binary classiﬁer of “Husky”, i.e., λ2, is favorable since it could output “Not a Husky” to avoid consistently voting for “Dog”. We can see an undesired ILF always votes for a single desired label. To formally describe this, we deﬁne an informative ILF as:
Deﬁnition 3 (Informative ILF). An ILF λj is informative if, for ∀y ∈ Y, there exists Xi ∈ D s.t. the output of λj on Xi is not in N (y, Yλj ), i.e., Yˆij ∈ N (y, Yλj ).
Testable Conditions for Distinguishability. Based on the introduced notations, we prove the necessary and sufﬁcient condition for learned PLRM being able to distinguish unseen labels:
Theorem 2. For PLRM induced from a consistent label graph, as well as informative ILFs, for any pair of yi, yj ∈ Y, they are indistinguishable, if and only if tik = tjk for ∀yk ∈ Yˆ.
Theorem 2 provides users with a testable condition: for any pair of unseen labels yi, yj, there should exist at least one seen label yk such that yk has different label relations to yi and yj, i.e., tik = tjk, so that PLRM is able to distinguish yi and yj. In preliminary experiments, we observe the violation of this condition causes a dramatic drop in overall performance (about 10 points). Notably, based on Theorem 2, users could theoretically guarantee the distinguishability of a pair of unseen labels by adding only one seen label and corresponding ILFs to break the symmetry.

7

Published as a conference paper at ICLR 2022

7 EXPERIMENTS

We demonstrate the applicability and performance of our method on image classiﬁcation tasks derived from ILSVRC2012 (Russakovsky et al., 2015) and text classiﬁcation tasks derived from LSHTC-3 (Partalas et al., 2015). Both datasets have off-the-shelf label relation structure (Deng et al., 2014; Partalas et al., 2015), which are directed acyclic graphs (DAGS) and from which we could query pairwise label relations. Indeed, there is a one-to-one mapping between a DAG structure of labels and a consistent label graph (See App. E.1 for an example). The ILSVRC2012 dataset consists of 1.2M training images from 1,000 leave classes; for non-leave classes, we follow Deng et al. (2014) to aggregate images belonging to its descendent classes as its data points. The LSHTC-3 dataset consists of 456,886 documents and 36,504 labels organized in a DAG.

7.1 SETUP
For each dataset, we randomly sample 100 different label graphs, each of which consists of 8 classes, and use each label graph to construct a WIS task. For each label graph, we treat 3 of the sampled classes as unseen classes and the other 5 as seen classes. The distinguishable condition in Sec. 6 is ensured for all the WIS tasks, and the performance drop when it is violated can be found in App. G.1. We sample data belonging to unseen classes for our experiments and split them into train and test set. For image classiﬁcation tasks, we follow Mazzetto et al. (2021b;a) to train a branch of image classiﬁers as supervision sources of seen classes. For text classiﬁcation tasks, we made keyword-based labeling functions as supervision sources of seen classes following Zhang et al. (2021); each of the labeling functions returns its associated label when a certain keyword exists in the text, otherwise abstains. Notably, all the involved supervision sources are "weak" because they cannot predict the desired unseen classes. Experimental details and additional results are in App. F.

7.2 COMPARED METHODS AND RESULTS
In addition to the WS-LG baseline, which is an adaptation of Data Programming (Ratner et al., 2019) to WIS task, and PLRM, we also include the following baselines. Note that all compared methods input the same data, ILFs, and label relations throughout our experiments for fair comparisons.

Label Relation Majority Voting (LR-MV). We modify the majority voting method based on the
label’s non-exclusive neighbors: we replace yˆ predicted by any ILF with the set of desired labels N (yˆ, Y), i.e., the desired labels with non-exclusive relation to yˆ, then aggregate the modiﬁed votes.

Weighted Label Relation Majority Voting (W-LR-MV). LR-MV only leverages exclusive/non-

exclusive label relations. To leverage ﬁne-grained label relations, W-LR-MV attaches a weight to

each replaced label. Speciﬁcally, if the ILF’s output yˆ is replaced with its ancestor label y (subsumed

relation),

then

the

weight

of

y

equals

1,

while

for

the

other

relations,

the

weight

is

|Y

1 ∗ (yˆ)|

,

where

Y∗(yˆ) = {y ∈ Y(yˆ)|tyyˆ = tsd}.

For the above methods, we compare the performance of (1) directly applying included models on the test set and (2) the end models (classiﬁers) trained with inferred training labels.

Zero-Shot Learning (ZSL). It is non-trivial to apply ZSL methods, because ZSL assumes label attributes for all classes and a labeled training set of seen classes, while WIS input an unlabeled
dataset of unseen classes, label relations and ILFs. Fortunately, the Direct Attribute Prediction
(DAP) (Lampert et al., 2013) method is able to make predictions solely based on attributes without labeled data, by training attribute classiﬁer p(ai|x) for each attribute ai. Therefore we include it in our experiments. The details of applying DAP can be found in App. F.2.

Evaluation Results. For a fair comparison, we ﬁx the network architecture of the classiﬁers for all the methods. For image classiﬁcation, we use ResNet-32 (He et al., 2016) and for text classiﬁcation, we use logistic regression with pre-trained text embedding (Reimers & Gurevych, 2019). The overall results for both datasets can be found in Table 2. From the results, we can see that PLRM consistently outperforms baselines. The advantages of PLRM show the effect of not just leveraging the label graph, as the baselines do, but modeling the accuracy of ILFs and the strengths of label relations

8

Published as a conference paper at ICLR 2022

Table 2: Averaged evaluation results over 100 WIS tasks derived from LSHTC-3 and ILSVRC2012.

Method

DAP

Label Model

LR-MV W-LR-MV
WS-LG

PLRM

End Model

LR-MV W-LR-MV
WS-LG

PLRM

LSHTC-3

Accuracy F1-score

42.90 ± 13.53 35.98 ± 15.73

58.86 ± 10.50 59.28 ± 10.47 62.60 ± 10.12

54.33 ± 11.10 54.55 ± 11.36 57.50 ± 11.19

64.65 ± 11.30 60.01 ± 13.39

67.17 ± 12.25 66.57 ± 11.73 70.69 ± 13.05

62.49 ± 13.95 61.80 ± 13.24 67.36 ± 14.24

72.32 ± 13.18 69.37 ± 14.41

ILSVRC2012

Accuracy F1-score

33.25 ± 3.68 29.13 ± 4.63

46.88 ± 10.66 41.39 ± 10.80 53.68 ± 7.62

40.11 ± 16.44 30.19 ± 16.94 52.15 ± 7.94

56.18 ± 7.35 54.94 ± 7.44

49.60 ± 12.80 42.61 ± 12.46 56.56 ± 9.68

42.83 ± 18.17 31.34 ± 18.20 54.57 ± 11.17

58.38 ± 8.27 56.83 ± 8.49

as PLRM does. The reported results have high variance, which actually indicates the 100 different WIS tasks are diverse and have varying difﬁculty. Also, we can see the end models are much better than directly applying the label models on the test set; this shows that the end models are able to generalize beyond the training labels produced by label models.

7.3 REAL-WORLD APPLICATION
In this section, on a commercial advertising system (CAS), we showcase how to reduce human annotation efforts of new labeling tasks by formulating them as WIS problems. In a CAS, ads tagging (classiﬁcation) is a critical application for understanding the semantics of ads copy. When new ads and tags are added to the system, manual annotations need to be collected for training a new classiﬁer. As tags are commonly organized as taxonomies, the label relations between existing and new tags are readily available or can be trivially ﬁgured out by humans; Existing classiﬁers and the heuristic rules previously used for annotating existing tags could serve as ILFs. Therefore, given (1) an unlabeled dataset of new tags, (2) the label relations, and (3) ILFs, we formulate it as a WIS problem.
On such WIS formulation, we apply our method and baselines, to synthesize training labels of new tags. Speciﬁcally, we have two WIS tasks where the tags are under the “Car Accessories” and “Furniture” categories respectively. For both tasks, we have 3 new tags and leverage 5 existing tags related to the new ones with given relations. On a test set, we evaluate the performance of DAP and the quality of labels produced by label models, as shown in Table 3. Note that since we re-use the existing labeling sources tailored for existing tags as ILFs and obtain label relations from an existing taxonomy, we achieve these results without any manual annotation or creation of new labeling functions. This demonstrates the potential of the proposed WIS task in real-world scenarios.
Table 3: Evaluation on product tagging with new tags.

Category Car Accessories
Furniture

Metric
F1 Accuracy
F1 Accuracy

DAP
50.62 52.83
30.81 33.60

LR-MV
68.68 68.17
64.70 72.53

W-LR-MV
68.06 67.67
61.45 72.13

WS-LG
66.85 66.33
70.59 74.51

PLRM
76.37 75.83
80.57 82.02

8 CONCLUSION
We propose Weak Indirect Supervision (WIS), a new research problem which leverages indirect supervision sources and label relations to synthesize training labels for training machine learning models. We develop the ﬁrst method for WIS called Probabilistic Label Relation Model (PLRM) with the generalization error bound of both PLRM and end model. We provide a theoretically-principled sanity test to ensure the distinguishability of unseen labels. Finally, we provide experiments to demonstrate the effectiveness of PLRM and its advantages over baselines on both academic datasets and industrial scenario.
9

Published as a conference paper at ICLR 2022
Reproducibility Statement. All the assumptions and proofs of our theory can be found in App. C & D. Examples and illustrations of label graph are in App. E. Experimental details can be found in App. F. Additional experiments are in App. G.
REFERENCES
Elizabeth S Allman, John A Rhodes, Elena Stanghellini, and Marco Valtorta. Parameter identiﬁability of discrete bayesian networks with hidden variables. Journal of Causal Inference, 3(2):189–205, 2015.
Stephen H. Bach, Bryan He, Alexander J. Ratner, and Christopher Ré. Learning the structure of generative models without labeled data. In Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Sydney, Australia, 2017.
Stephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Chris Ré, and Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Proceedings of the 2019 International Conference on Management of Data, SIGMOD ’19, pp. 362–375, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450356435. doi: 10.1145/3299869.3314036. URL https://doi.org/10.1145/3299869.3314036.
Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311, 1993.
Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser, and Dan Roth. Structured output learning with indirect supervision. In ICML, pp. 199–206, 2010.
Lingjiao Chen, Matei Zaharia, and James Zou. Frugalml: How to use ml prediction apis more accurately and cheaply. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Carlos d’Andrea and André Mintz. Studying the live cross-platform circulation of images with computer vision api: An experiment based on a sports media event. International Journal of Communication, 13(0), 2019. ISSN 1932-8036.
Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut Neven, and Hartwig Adam. Large-scale object classiﬁcation using label relation graphs. In European conference on computer vision, pp. 48–64. Springer, 2014.
Xin Luna Dong, Xiang He, Andrey Kan, Xian Li, Yan Liang, Jun Ma, Yifan Ethan Xu, Chenwei Zhang, Tong Zhao, Gabriel Blanco Saldana, et al. Autoknow: Self-driving knowledge collection for products of thousands of types. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2724–2734, 2020.
Jared A. Dunnmon, Alexander J. Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert, Hersh Sagreiya, Roger Goldman, Christopher Lee-Messer, Matthew P. Lungren, Daniel L. Rubin, and Christopher Ré. Cross-modal data programming enables rapid medical machine learning. Patterns, 1(2):100019, 2020. ISSN 2666-3899. doi: https://doi.org/10.1016/j.patter. 2020.100019. URL https://www.sciencedirect.com/science/article/pii/ S2666389920300192.
Jason A Fries, Ethan Steinberg, Saelig Khattar, Scott L Fleming, Jose Posada, Alison Callahan, and Nigam H Shah. Ontology-driven weak supervision for clinical entity classiﬁcation in electronic health records. Nature Communications, 12(1), 2021.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26, pp. 2121–2129. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/ 7cce53cf90577442771720a370c3c723-Paper.pdf.
10

Published as a conference paper at ICLR 2022
Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher Ré. Fast and three-rious: Speeding up weak supervision with triplet methods. In Proceedings of the 37th International Conference on Machine Learning (ICML 2020), 2020.
Melody Y. Guan, Varun Gulshan, Andrew M. Dai, and Geoffrey E. Hinton. Who said what: Modeling individual labelers improves classiﬁcation. In AAAI, 2018.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.
Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis Langlotz, and Christopher Re. Cut out the annotator, keep the cutout: better segmentation with weak supervision. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=bjkX6Kzb5H.
Ashish Khetan, Zachary C. Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=H1sUHgb0Z.
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 951–958. IEEE, 2009.
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classiﬁcation for zeroshot visual object categorization. IEEE transactions on pattern analysis and machine intelligence, 36(3):453–465, 2013.
Percy Liang, Michael I Jordan, and Dan Klein. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446, 2013.
Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, and Samia Touileb. Named entity recognition without labelled data: A weak supervision approach. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1518–1533, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.139. URL https://www. aclweb.org/anthology/2020.acl-main.139.
Alessio Mazzetto, Cyrus Cousins, Dylan Sam, Stephen H Bach, and Eli Upfal. Adversarial multi class learning under weak supervision with performance guarantees. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7534–7543. PMLR, 18–24 Jul 2021a. URL https://proceedings.mlr.press/v139/mazzetto21a.html.
Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal, and Stephen Bach. Semi-supervised aggregation of dependent weak supervision sources with performance guarantees. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 3196–3204. PMLR, 13–15 Apr 2021b. URL https://proceedings.mlr.press/v130/ mazzetto21a.html.
George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): 39–41, 1995.
Shikhar Murty, Pat Verga, L. Vilnis, and A. McCallum. Finer grained entity typing with typenet. AKBC Workshop, 2017.
Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artières, George Paliouras, Éric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, and Patrick Gallinari. LSHTC: A benchmark for large-scale text classiﬁcation. CoRR, abs/1503.08581, 2015.
Meng Qu, Tianyu Gao, Louis-Pascal Xhonneux, and Jian Tang. Few-shot relation extraction via bayesian meta-learning on relation graphs. In International Conference on Machine Learning, pp. 7867–7876. PMLR, 2020.
11

Published as a conference paper at ICLR 2022
Ariadna Quattoni, Michael Collins, and Trevor Darrell. Conditional random ﬁelds for object recognition. Advances in neural information processing systems, 17:1097–1104, 2004.
Aditi Raghunathan, Roy Frostig, John Duchi, and Percy Liang. Estimation from indirect supervision with linear moments. In International Conference on Machine Learning (ICML), 2016.
A. J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, and C. Ré. Data programming: Creating large training sets, quickly. In Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, 2016.
A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Ré. Training complex models with multi-task weak supervision. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Honolulu, Hawaii, 2019.
Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. Snorkel: Rapid training data creation with weak supervision. In Proceedings of the 44th International Conference on Very Large Data Bases (VLDB), Rio de Janeiro, Brazil, 2018.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084.
Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In International conference on machine learning, pp. 2152–2161. PMLR, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211–252, December 2015. ISSN 0920-5691. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/ s11263-015-0816-y.
Esteban Safranchik, Shiying Luo, and Stephen Bach. Weakly supervised sequence tagging from noisy rules. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 5570–5578, 2020.
Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In WWW, 2015.
Shashank Srivastava, Igor Labutov, and Tom Mitchell. Zero-shot learning of classiﬁers from natural language quantiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 306–316, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1029. URL https: //www.aclweb.org/anthology/P18-1029.
The Gene Ontology Consortium. The Gene Ontology Resource: 20 years and still GOing strong. Nucleic Acids Research, 47(D1):D330–D338, 11 2018. ISSN 0305-1048. doi: 10.1093/nar/ gky1055. URL https://doi.org/10.1093/nar/gky1055.
Paroma Varma, Frederic Sala, Shiori Sagawa, Jason Alan Fries, Daniel Y. Fu, Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James Priest, and Christopher Ré. Multiresolution weak supervision for sequential data. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 192–203, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 93db85ed909c13838ff95ccfa94cebd9-Abstract.html.
Kaifu Wang, Qiang Ning, and Dan Roth. Learnability with indirect supervision signals. Advances in Neural Information Processing Systems 32, 2020.
Wei Wang, Vincent W Zheng, Han Yu, and Chunyan Miao. A survey of zero-shot learning: Settings, methods, and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10 (2):1–37, 2019.
12

Published as a conference paper at ICLR 2022
Yuanshun Yao, Zhujun Xiao, Bolun Wang, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao. Complexity vs. performance: Empirical analysis of machine learning as a service. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, pp. 384–397, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450351188. doi: 10.1145/3131365.3131372. URL https://doi.org/10.1145/3131365.3131372.
Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang. Co-tuning for transfer learning. Advances in Neural Information Processing Systems, 33, 2020.
Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. Generating multi-agent trajectories using programmatic weak supervision. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rkxw-hAcFQ.
Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. Wrench: A comprehensive benchmark for weak supervision. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.
Yivan Zhang, Nontawat Charoenphakdee, and Masashi Sugiyama. Learning from indirect observations, 2019.
Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, and Xiang Ren. Nero: A neural rule grounding framework for label-efﬁcient relation extraction. The Web Conference, 2020.
13

Published as a conference paper at ICLR 2022

SUPPLEMENTARY MATERIALS FOR “CREATING TRAINING SETS VIA WEAK INDIRECT SUPERVISION”
The supplementary materials are organized as follows. In Appendix A, we provide a glossary of variables and symbols used in this paper. In Appendix B, we provide the details of PLRM model. In Appendix C and D, we provide the detailed proofs of Theorem 2 and Theorem 1 respectively. In Appendix E, we provide the detailed examples and illustrations of label graph in WIS. In Appendix F and G, we provide experimental details and additional experiment resulst respectively.

A GLOSSARY OF SYMBOLS

Table 4: Glossary of variables and symbols used in this paper.

Symbol
Xi m Yi y Y k
λj n Yˆij Yˆi yˆj
Yλj
kλj Yˆ kˆ
K
Y¯ i Y¯
G E T te to tsg tsd N (y, Yˆ)
φ Φ M θ Θ Θˆ Θ∗ W Wˆ W∗

Simpliﬁed
Yj kj

Used for
The i-th data point, Xi ∈ X Number of data points The true desired label of the i-th data point, Yi ∈ Y A semantic label, e.g., "dog" The set of desired labels, Y = {y1, y2, . . . , yk} Cardinality of Y, i.e., k = |Y|
The j-th Indirect labeling function (ILF) Number of ILF The output label of j-th ILF on i-th data point, Yˆij ∈ Yλj The concatenation of ILFs’ output, Yˆi = [Yˆi1, Yˆi2, . . . , Yˆin] A semantic label in the label space of λj Label label space of ILF λj, Yλj = {yˆ1j, yˆ2j, . . . , yˆkjλj } Cardinality of the output space of ILF λ, i.e., kλj = |Yλj | Union set of all the Yλj , Yˆ = {yˆ1, yˆ2, . . . , yˆkˆ} Cardinality of the Yˆ, i.e., kˆ = |Yˆ| Total number of labels, i.e., K = kˆ + k
Latent binary variable indicating whether the data should be assigned yˆi ∈ Yˆ. Concatenation of all latent binary variable, Y¯ = [Y¯ 1, . . . , Y¯ kˆ]
Label graph, G = (Yˆ ∪ Y, E) The set of label relations, E = {(yi, yj, tyiyj )|tyiyj ∈ T , i < j, ∀yi, yj ∈ V} The set of label relation types, T = {te, to, tsd, tsg} Exclusive label relation Overlap label relation Subsuming label relation Subsumed label relation the set of non-exclusive neighbors of a given label y in Yˆ
A single dependency, or, factor function Concatenation of all individual dependency Number of total dependencies A single parameter of the PGM Concatenation of all parameters of the PGM, Θ ∈ RM The learned parameters The golden parameters The parameter of an end model
The learned parameters The golden parameters

14

Published as a conference paper at ICLR 2022

B DETAILS OF THE PLRM

We use Y, Y¯ , and Yˆ to represent random vector. Then, we give the formal form of the PLRM as:

PΘ(Y, Y¯ , Yˆ ) ∝ exp Θ Φ(Y, Y¯ , Yˆ ) .

(7)

Recall that Y is the unobserved true label, Y¯ is the binary random vector, each of whose binary value Y¯ i reﬂects whether the data should be assigned seen label yˆi ∈ Yˆ, and Yˆ is the concatenated outputs
of ILFs. Speciﬁcally, we enumerate Φ as below:

1. (Pseudo accuracy dependency): ∀j ∈ [n], y ∈ Y/{unknown}, yˆ ∈ Yλj , we have
φAy,cyˆc,j(Y, Yˆ j) := 1{Y = y ∧ Yˆ j = yˆ ∧ yˆ ∈ N (y, Yλj )}1
2. (Accuracy dependency): ∀j ∈ [n], yˆi ∈ Yˆ ∩ Yj we have
φAyˆc,cj (Y¯ i, Yˆ j ) := 1{Y¯ i = 1 ∧ Yˆ j = yˆi} i
3. (Label relation dependency between seen labels): ∀yˆi, yˆj ∈ Yˆ, i < j (a) if tyˆiyˆj = te, we have
φeyˆ ,yˆ (Y¯ i, Y¯ j) := − 1{Y¯ i = 1 ∧ Y¯ j = 1} ij
(b) if tyˆiyˆj = to, we have
φoyˆ ,yˆ (Y¯ i, Y¯ j) := 1{Y¯ i = 1 ∧ Y¯ j = 1} ij
(c) if tyˆiyˆj = tsg, we have
φsyˆg,yˆ (Y¯ i, Y¯ j) := − 1{Y¯ i = 0 ∧ Y¯ j = 1} ij
(d) if tyˆiyˆj = tsd, we have
φsyˆd,yˆ (Y¯ i, Y¯ j) := − 1{Y¯ i = 1 ∧ Y¯ j = 0} ij
4. (Label relation dependency between desired and seen labels): ∀y ∈ Y/{unknown}, yˆi ∈ Yˆ (a) if tyyˆi = te, we have
φey,yˆ (Y, Y¯ i) := − 1{Y = y ∧ Y¯ i = 1} i
(b) if tyyˆi = to, we have
φoy,yˆ (Y, Y¯ i) := 1{Y = y ∧ Y¯ i = 1} i
(c) if tyyˆi = tsg, we have
φsyg,yˆ (Y, Y¯ i) := − 1{Y = y ∧ Y¯ i = 1} i
(d) if tyyˆi = tsd, we have
φsyd,yˆ (Y, Y¯ i) := − 1{Y = y ∧ Y¯ i = 0} i
And example of our PLRM is shown in Fig. 4, where square with difference colors corresond to different dependency/factor functions in PLRM.

<latexitsha1_base64="gGFWUYb82ZT3aX9zOTRees8yCuw=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0ikqMeCF48t2FZpQ9lsJ+3azSbsboRS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8MBVcG8/7dgpr6xubW8Xt0s7u3v5B+fCopZNMMWyyRCTqPqQaBZfYNNwIvE8V0jgU2A5HNzO//YRK80TemXGKQUwHkkecUWOlxkOvXPFcbw6ySvycVCBHvVf+6vYTlsUoDRNU647vpSaYUGU4EzgtdTONKWUjOsCOpZLGqIPJ/NApObNKn0SJsiUNmau/JyY01noch7Yzpmaol72Z+J/XyUx0HUy4TDODki0WRZkgJiGzr0mfK2RGjC2hTHF7K2FDqigzNpuSDcFffnmVtC5c/9KtNqqVmpvHUYQTOIVz8OEKanALdWgCA4RneIU359F5cd6dj0VrwclnjuEPnM8ftCeM0g==</latexit>
Y

Y¯< l a t e x i t s h a 1 _ b a s e 6 4 = " 8 l Z f x I c X K Q / k Q y 2 c O T y l f a M q f 2 c = " > A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V o h 6 L X j x W s F + 0 a 8 m m 2 T Y 0 y S 5 J V i h L f 4 U X D 4 p 4 9 e d 4 8 9 + Y b f e g r Q 8 G H u / N M D M v i D n T x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 S h R h D Z J x C P V C b C m n E n a N M x w 2 o k V x S L g t B 1 M b j O / / U S V Z p F 8 M N O Y + g K P J A s Z w c Z K 3 X 6 A V d q d P X q D c s W t u n O g V e L l p A I 5 G o P y V 3 8 Y k U R Q a Q j H W v c 8 N z Z + i p V h h N N Z q Z 9 o G m M y w S P a s 1 R i Q b W f z g + e o T O r D F E Y K V v S o L n 6 e y L F Q u u p C G y n w G a s l 7 1 M / M / r J S a 8 9 l M m 4 8 R Q S R a L w o Q j E 6 H s e z R k i h L D p 5 Z g o p i 9 F Z E x V p g Y m 1 H J h u A t v 7 x K W h d V 7 7 J a u 6 9 V 6 j d 5 H E U 4 g V M 4 B w + u o A 5 3 0 I A m E B D w D K / w 5 i j n x X l 3 P h a t B S e f O Y Y / c D 5 / A K C a k E 4 = < / l a t e x i t >

1

Y¯< l a t e x i t s h a 1 _ b a s e 6 4 = " / F J i O g B e N a C 3 A 4 e I p g M Q n F 9 L P a Y = " > A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 k t p X o s e v F Y w X 7 R r i W b Z t v Q J L s k W a E s / R V e P C j i 1 Z / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e E H O m j e t + O 7 m N z a 3 t n f x u Y W / / 4 P C o e H z S 0 l G i C G 2 S i E e q E 2 B N O Z O 0 a Z j h t B M r i k X A a T u Y 3 M 7 9 9 h N V m k X y w U x j 6 g s 8 k i x k B B s r d f s B V m l 3 9 l g Z F E t u 2 V 0 A r R M v I y X I 0 B g U v / r D i C S C S k M 4 1 r r n u b H x U 6 w M I 5 z O C v 1 E 0 x i T C R 7 R n q U S C 6 r 9 d H H w D F 1 Y Z Y j C S N m S B i 3 U 3 x M p F l p P R W A 7 B T Z j v e r N x f + 8 X m L C a z 9 l M k 4 M l W S 5 K E w 4 M h G a f 4 + G T F F i + N Q S T B S z t y I y x g o T Y z M q 2 B C 8 1 Z f X S a t S 9 m r l 6 n 2 1 V L / J 4 s j D G Z z D J X h w B X W 4 g w Y 0 g Y C A Z 3 i F N 0 c 5 L 8 6 7 8 7 F s z T n Z z C n 8 g f P 5 A 6 I e k E 8 = < / l a t e x i t >

2

< l a t e x i t s h a 1 _ b a s e 6 4 = " k f B l L C 0 C L T 9 J I / U V y 5 D w h N 5 C l d Q = " > A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K U I 9 B L x 4 j m B f J G m Y n k 2 T I 7 O w y 0 y u E J V / h x Y M i X v 0 c b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V x F I Y d N 1 v J 7 e 2 v r G 5 l d 8 u 7 O z u 7 R 8 U D 4 8 a J k o 0 4 3 U W y U i 3 A m q 4 F I r X U a D k r V h z G g a S N 4 P x 7 c x v P n F t R K Q e c B J z P 6 R D J Q a C U b R S u z u i m L a n j 1 6 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 1 H L A m 5 Q i a p M R 3 P j d F P q U b B J J 8 W u o n h M W V j O u Q d S x U N u f H T + c F T c m a V P h l E 2 p Z C M l d / T 6 Q 0 N G Y S B r Y z p D g y y 9 5 M / M / r J D i 4 9 l O h 4 g S 5 Y o t F g 0 Q S j M j s e 9 I X m j O U E 0 s o 0 8 L e S t i I a s r Q Z l S w I X j L L 6 + S x k X Z u y x X 7 i u l 6 k 0 W R x 5 O 4 B T O w Y M r q M I d 1 K A O D E J 4 h l d 4 c 7 T z 4 r w 7 H 4 v W n J P N H M M f O J 8 / r O 6 Q V g = = < / l a t e x i t >
Yˆ

1

< l a t e x i t s h a 1 _ b a s e 6 4 = " P m y x B n H S Z R H e 1 D 8 7 A l o n O U J j 7 U o = " > A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y h A 8 h d 0 Q 1 G P Q i 8 c I 5 k W y h t n J J B k y M 7 v M 9 A p h y V d 4 8 a C I V z / H m 3 / j J N m D J h Y 0 F F X d d H e F s e A G P e / b W V v f 2 N z a z u 3 k d / f 2 D w 4 L R 8 c N E y W a s j q N R K R b I T F M c M X q y F G w V q w Z k a F g z X B 8 O / O b T 0 w b H q k H n M Q s k G S o + I B T g l Z q d 0 c E 0 / b 0 s d w r F L 2 S N 4 e 7 S v y M F C F D r V f 4 6 v Y j m k i m k A p i T M f 3 Y g x S o p F T w a b 5 b m J Y T O i Y D F n H U k U k M 0 E 6 P 3 j q n l u l 7 w 4 i b U u h O 1 d / T 6 R E G j O R o e 2 U B E d m 2 Z u J / 3 m d B A f X Q c p V n C B T d L F o k A g X I 3 f 2 v d v n m l E U E 0 s I 1 d z e 6 t I R 0 Y S i z S h v Q / C X X 1 4 l j X L J v y x V 7 i v F 6 k 0 W R w 5 O 4 Q w u w I c r q M I d 1 K A O F C Q 8 w y u 8 O d p 5 c d 6 d j 0 X r m p P N n M A f O J 8 / r n K Q V w = = < / l a t e x i t >
Yˆ

2

< l a t e x i t s h a 1 _ b a s e 6 4 = " x a H e Q P K G O A 0 g Y X E L K L P 2 A d C S / H g = " > A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x q U I 9 B L x 4 j m B d J D L O T 2 W T I z O 4 y 0 y u E J V / h x Y M i X v 0 c b / 6 N k 2 Q P G i 1 o K K q 6 6 e 7 y Y y k M u u 6 X k 1 t Z X V v f y G 8 W t r Z 3 d v e K + w c N E y W a 8 T q L Z K R b P j V c i p D X U a D k r V h z q n z J m / 7 4 Z u Y 3 H 7 k 2 I g r v c R L z n q L D U A S C U b R S u z u i m L a n D + f 9 Y s k t u 3 O Q v 8 T L S A k y 1 P r F z + 4 g Y o n i I T J J j e l 4 b o y 9 l G o U T P J p o Z s Y H l M 2 p k P e s T S k i p t e O j 9 4 S k 6 s M i B B p G 2 F S O b q z 4 m U K m M m y r e d i u L I L H s z 8 T + v k 2 B w 1 U t F G C f I Q 7 Z Y F C S S Y E R m 3 5 O B 0 J y h n F h C m R b 2 V s J G V F O G N q O C D c F b f v k v a Z y V v Y t y 5 a 5 S q l 5 n c e T h C I 7 h F D y 4 h C r c Q g 3 q w E D B E 7 z A q 6 O d Z + f N e V + 0 5 p x s 5 h B + w f n 4 B q / 2 k F g = < / l a t e x i t >
Yˆ

3

Label Relation Dependency True Accuracy Dependency Pseudo Accuracy Dependency

Figure 4: PLRM.

1When yˆ ∈/ N (y, Yλj ), φAy,cyˆc,j is always zero and will not occur in the model. Here we use this form for the sack of rigorous representation.

15

Published as a conference paper at ICLR 2022

C PROOF OF THEOREM 2

C.1 SIMPLIFYING THE NOTATION

To simplify the indexing of dependencies, we use Φ1 to represent the concatenation of φ which involves both Y and Y¯ , Φ2 to represent the concatenation of φ which involve both Y and Λˆ , and Φ3 to represent the concatenation of remaining φ which do not involve Y.

Speciﬁcally, Φ1 consists of k × kˆ components corresponding to the dependency between the k desired labels and the kˆ seen labels. We use the subscript i, j to denote the dependency function between the
desired label yi and seen label yˆj, i.e.,

Φ1i,j = φ?y ,yˆ , ij

where ? is the corresponding relation.

Similarly, Φ2 consists of k × (

n j=1

kj

)

components

corresponding

to

the

dependency

between

the

k desired labels and the kj seen labels output by the ILF λj (j ∈ [n]), and we use Φ2i,j,l to denote the

dependency of yi and yˆlj, and Φ2i,j = (Φ2i,j,l)kl=j 1to denote the dependency of yi and yˆj.

According to Φ1, Φ2, and Φ3, we also divide the parameter Θ into Θ1 (with elements being Θ1i,j correspondingly), Θ2 (with elements being Θ2i,j = (Θ2i,j,l)kl=j 1 correspondingly), and Θ3, and the
joint probability is then given as:

exp (Θ1)TΦ1(Y, Y¯ ) + (Θ2) Φ2(Y, Yˆ ) + (Θ3)TΦ3(Y¯ , Yˆ )

PΘ(Y, Y¯ , Yˆ ) =

(8)

Y ,Y¯ ,Yˆ exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ ) + (Θ3)TΦ3(Y¯ , Yˆ )

Also, for notation convenience, we adopt following simpliﬁcations:
1. ∀yi ∈ Y → ∀i ∈ [k] since |Y| = k, similarly, ∀yˆi ∈ Yj → ∀i ∈ [kj] and ∀yˆi ∈ Yˆ → ∀i ∈ [kˆ];
2. ∀λj → ∀j ∈ [n] since we have n ILFs in total; 3. φytyii,yyjj → φtyi,yj where t = tyiyj and can be seen from the subscript of the dependency.

C.2 PROPOSITIONS AND LEMMAS

First, we state some propositions and lemmas that will be useful in the proof to come.

Proposition 1 (Multi-class classiﬁcation). For a multi-class classiﬁcation task, ∀yi, yj ∈ Y, we have

ty y = te. Similarly, ∀yˆa, yˆb ∈ Yˆ, we have tyˆ yˆ = te.

ij

ab

Lemma 1. For a consistent label graph G and ∀yˆl ∈ Yˆ, ∀yi, yj ∈ Y, if ty yˆ = to, we have il
tyj yˆl = tsg .

Proof. For ∀yi, yj ∈ Y, based on Proposition 1, we know tyiyj = te, which implies (1) the intersection of the sets labeled by yi and yj is empty. For ∀yˆl ∈ Yˆ, if ty yˆ = to, we have (2)
il
the intersection of the sets labeled by yi and yˆl is not empty. If tyjyˆl = tsg, which implies (3) yj yˆl. Based on (2)(3), we have yi ∩ yj = ∅, which is contradictory to (1). Thus, we prove when tyiyˆl = to, tyj yˆl = tsg.
Lemma 2. For an informative ILF λj and given any yd ∈ Y, there exists some yˆl ∈ Yj, such that, Φ2d,j,l(yd, yˆ) = 0, ∀l ∈ [kj ].

Proof. Because ILF λj is informative, we know there exists one yˆa ∈ Yj such that yˆa is exclusive to yd, i.e., yˆa ∈/ N (yd, Yj). Therefore, for any yˆl ∈ Yˆ, either yˆa = yˆl, or yˆl = yˆa ∈/ Yj, which leads to
the conclusion by the deﬁnition of Φ2d,j,l = φAy cc,yˆ ,j. dl

16

Published as a conference paper at ICLR 2022

C.3 DEFINITIONS

Before the main proof, we connect the indistinguishablity of label relation structure with the dependency structure of PLRM by introducing the concept of symmetry as follows:
Deﬁnition 4 (Symmetry). For yi, yj ∈ Y, we say yi and yj have symmetric dependency structure if the following equation holds:

Φ1i,l = Φ1j,l, ∀l ∈ kˆ;

Φ2i,a,b = Φ2j,a,b, ∀a ∈ [n], b ∈ [ka].

(9)

Based on the construction of PLRM, we know that for ∀yi, yj ∈ Y, ∀yˆb ∈ Yˆ, ty yˆ = ty yˆ (the

ib

jb

statement in Theorem 2) is equivalent to yi and yj have symmetric dependency structure.

C.4 EQUIVALENT STATEMENT OF THEOREM 2
Our main result states that asymmetric is equivalent to distinguishable as in the following theorem, which can readily be seen to be identical to Theorem 2 in the main body of the paper:
Theorem 3. For a probability model deﬁned as Eq. (8) induced from a consistent label graph and informative ILFs, for any pair of yi, yj ∈ Y, yi and yj are distinguishable if and only if they have asymmetric dependency structure.

C.5 PROOF OF THE NECESSITY IN THEOREM 3: NECESSARY CONDITION
We ﬁrst prove that for any yi, yj ∈ Y, yi and yj have asymmetric dependency structure is the necessary condition of that they are distinguishable.

Proof of Theorem 3. We prove this theorem by reduction to absurdity. Suppose yi and yj are symmetric. Then, by Eq. (8), the distribution of Y condition on any Y¯ and Yˆ can be calculated as follows:
PΘ(Y = yi|Y¯ , Yˆ ) = PΘ(yi, Y¯ , Yˆ ) . PΘ(Y¯ , Yˆ )
On the other hand, applying Y = yi in the deﬁnition of Φ2 leads to
Φ2r,a,l(yi, ·) = 0, ∀r ∈ [k], r = i, ∀a ∈ [n], ∀l ∈ [ka].
We further separate Φ1 into (Φ1i )ki=1, where Φ1i collects all the dependency in Φ1 with yi involved, i.e.,
Φ1i = (Φ1i,j )kjˆ=1, with the corresponding parameters respectively denoted as Θ1i with Θ1 = (Θ1i )ki=1. Similarly, Φ2 is also divided into (Φ2i )ki=1 following the same routine and Θ2 is respectively divided into (Θ2i )ki=1. Speciﬁcally, if yi and yj are symmetric, we further have
Φ1i = Φ1j , Φ2i = Φ2j .

Based on the notation, PΘ(Y = yi|Y¯ , Yˆ ) can then be represented as

PΘ(Y = exp

= yi|Y¯ , Yˆ )

exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ ) + (Θ3)TΦ3(Y¯ , Yˆ )

Y

k

k

(Θ1l )TΦ1l (yi, Y¯ ) + (Θ2l )TΦ2l (yi, Yˆ ) + (Θ3)TΦ3(Y¯ , Yˆ )

l=1

l=1

17

Published as a conference paper at ICLR 2022

which further leads to

PΘ(Y = yi|Y¯ , Yˆ )

exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )
Y

k

= exp

(Θ1l )TΦ1l (yi, Y¯ ) + (Θ2i )TΦ2i (yi, Yˆ )

(10)

l=1

which is independent of Θ3. Similarly,

PΘ(Y = yj|Y¯ , Yˆ )

exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

Y

k

= exp

(Θ1l )TΦ1l (yi, Y¯ ) + (Θ2j )TΦ2j (yj , Yˆ )

(11)

l=1

and ∀l ∈ [k]/{i, j},

PΘ(Y = yl|Y¯ , Yˆ )

exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

Y

k

= exp

(Θ1l )TΦ1l (yi, Y¯ ) + (Θ2l )TΦ2l (yl, Yˆ )

(12)

l=1

Let Θ˜ be deﬁned as follows:

Θ˜ 1i = Θ1j , Θ˜ 1j = Θ1i , Θ˜ 1l = Θ1l , ∀l ∈/ {i, j},

Θ˜ 2i = Θ2j , Θ˜ 2j = Θ2i , Θ˜ 2l = Θ2l , ∀l ∈/ {i, j}, and
Θ˜ 3 = Θ3.

We then have

PΘ(Y = yi|Y¯ , Yˆ ) PΘ˜ (Y = yj|Y¯ , Yˆ )
Y exp (Θ˜ 1) Φ1(Y , Y¯ ) + (Θ˜ 2) Φ2(Y , Yˆ ) =
Y exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

· exp ((Θ1i )T(Φ1i (yi, Y¯ ) − Φ1j (yj, Y¯ )) + (Θ1j )T(Φ1j (yi, Y¯ ) − Φ1i (yj, Y¯ )) + (Θ2i )T(Φ2i (yi, Yˆ ) − Φ2j (yj, Yˆ )))

Y exp (Θ˜ 1) Φ1(Y , Y¯ ) + (Θ˜ 2) Φ2(Y , Yˆ )

=

.

Y exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

Similarly,

PΘ(Y = yj|Y¯ , Yˆ ) PΘ˜ (Y = yi|Y¯ , Yˆ )
Y exp (Θ˜ 1) Φ1(Y , Y¯ ) + (Θ˜ 2) Φ2(Y , Yˆ ) =
Y exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

· exp ((Θ1j )T(Φ1j (yj, Y¯ ) − Φ1i (yi, Y¯ )) + (Θ1i )T(Φ1i (yj, Y¯ ) − Φ1j (yi, Y¯ )) + (Θ2j )T(Φ2j (yj, Yˆ ) − Φ2i (yi, Yˆ )))

Y exp (Θ˜ 1) Φ1(Y , Y¯ ) + (Θ˜ 2) Φ2(Y , Yˆ )

=

.

Y exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

18

Published as a conference paper at ICLR 2022

and ∀l ∈ [k]/{i, j}, PΘ(Y = yl|Y¯ , Yˆ ) = PΘ˜ (Y = yl|Y¯ , Yˆ )

Y exp (Θ˜ 1) Φ1(Y , Y¯ ) + (Θ˜ 2) Φ2(Y , Yˆ ) .
Y exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

Similarly, we have

PΘ(Y = unknown|Y¯ , Yˆ ) = PΘ˜ (Y = unknown|Y¯ , Yˆ )

Y exp (Θ˜ 1) Φ1(Y , Y¯ ) + (Θ˜ 2) Φ2(Y , Yˆ ) .
Y exp (Θ1) Φ1(Y , Y¯ ) + (Θ2) Φ2(Y , Yˆ )

Therefore, we have

PΘ(Y = yi|Y¯ , Yˆ ) = PΘ(Y = yj|Y¯ , Yˆ ) = PΘ(Y = y|Y¯ , Yˆ ) , ∀y ∈ Y/{y , y }.

PΘ˜ (Y = yj|Y¯ , Yˆ ) PΘ˜ (Y = yi|Y¯ , Yˆ ) PΘ˜ (Y = y|Y¯ , Yˆ )

ij

Since

PΘ(Y = yi|Y¯ , Yˆ ) + PΘ(Y = yj|Y¯ , Yˆ ) + PΘ(Y = yl|Y¯ , Yˆ ) = 1,

l=i,j

and PΘ˜ (Y = yj|Y¯ , Yˆ ) + PΘ˜ (Y = yi|Y¯ , Yˆ ) + PΘ˜ (Y = yl|Y¯ , Yˆ ) = 1,
l=i,j

we obtain that PΘ(Y = yi|Y¯ , Yˆ ) = PΘ˜ (Y = yj|Y¯ , Yˆ ) PΘ(Y = yj|Y¯ , Yˆ ) = PΘ˜ (Y = yi|Y¯ , Yˆ ) PΘ(Y = yl|Y¯ , Yˆ ) = PΘ˜ (Y = yl|Y¯ , Yˆ ),
which indicates yi and yj indistinguishable, and leads to a contradictory.

The proof is completed.

C.6 PROOF OF THEOREM 3: SUFFICIENT CONDITION
We then prove that for any yi, yj ∈ Y, yi and yj have asymmetric dependency structure is the sufﬁcient condition of that they are distinguishable.

Proof. We use the same notations (Θ1i )ki=1, (Θ2i )ki=1, and Θ3 in Appendix C.5 to denote the separation of the parameter Θ. Let Θ be any parameter satisfying that there exists a parameter Θ˜ , such that Eq. (4-5) holds. By Eqs. (10), (11), and Eq. (12) together with Eqs. (4-5), we have ∀r ∈ [k], r = i, j,

exp ((Θ1i )TΦ1i (yi, Y¯ ) + (Θ1j )TΦ1j (yi, Y¯ ) + (Θ2i )TΦ2i (yi, Yˆ )) exp ((Θ˜ 1i )TΦ1i (yj, Y¯ ) + (Θ˜ 1j )TΦ1j (yj, Y¯ ) + (Θ˜ 2j )TΦ2j (yj, Yˆ ))

exp ((Θ1i )TΦ1i (yj , Y¯ ) + (Θ1j )TΦ1j (yj , Y¯ ) + (Θ2j )TΦ2j (yj , Yˆ ))

= exp ((Θ˜ 1)TΦ1(yi, Y¯ ) + (Θ˜ 1)TΦ1(yi, Y¯ ) + (Θ˜ 2)TΦ2(yi, Yˆ ))

i

i

j

j

i

i

exp ((Θ1i )TΦ1i (yr, Y¯ ) + (Θ1j )TΦ1j (yr, Y¯ )) exp ((Θ1i )TΦ1i (yj, Y¯ ) + (Θ1j )TΦ1j (yi, Y¯ ))

= exp ((Θ˜ 1)TΦ1(yr, Y¯ ) + (Θ˜ 1)TΦ1(yr, Y¯ )) = exp ((Θ˜ 1)TΦ1(yj, Y¯ ) + (Θ˜ 1)TΦ1(yi, Y¯ )) .

i

i

j

j

i

i

j

j

By simple rearranging, we have
((Θ1i )TΦ1i (yi, Y¯ ) + (Θ1j )TΦ1j (yi, Y¯ ) + (Θ2i )TΦ2i (yi, Yˆ ) + (Θ2j )TΦ2j (yi, Yˆ )) − ((Θ˜ 1i )TΦ1i (yj , Y¯ ) + (Θ˜ 1j )TΦ1j (yj , Y¯ ) + (Θ˜ 2i )TΦ2i (yj , Yˆ ) + (Θ˜ 2j )TΦ2j (yj , Yˆ )) =((Θ1i )TΦ1i (yj , Y¯ ) + (Θ1j )TΦ1j (yj , Y¯ ) + (Θ2i )TΦ2i (yj , Yˆ ) + (Θ2j )TΦ2j (yj , Yˆ )) − ((Θ˜ 1i )TΦ1i (yi, Y¯ ) + (Θ˜ 1j )TΦ1j (yi, Y¯ ) + (Θ˜ 2i )TΦ2i (yi, Yˆ ) + (Θ˜ 2j )TΦ2j (yi, Yˆ )) =((Θ1i )TΦ1i (yj, Y¯ ) + (Θ1j )TΦ1j (yi, Y¯ )) − ((Θ˜ 1i )TΦ1i (yj, Y¯ ) + (Θ˜ 1j )TΦ1j (yi, Y¯ )). (13)

19

Published as a conference paper at ICLR 2022

By the equality between the second term and the third term in Eq. (13), we obtain that (Θ1j )TΦ1j (yi, Y¯ ) − (Θ˜ 1i )TΦ1i (yj , Y¯ )
=((Θ1j )TΦ1j (yj, Y¯ ) + (Θ2j )TΦ2j (yj, Yˆ )) − ((Θ˜ 1i )TΦ1i (yi, Y¯ ) + (Θ˜ 2i )TΦ2i (yi, Yˆ )). (14)
We further set Y¯ in Eq. (14) respectively to el (the one hot vector with its l-th position being 1) and 0 for any ﬁxed l ∈ [kˆ], i.e.,
((Θ1j )TΦ1j (yi, el) − (Θ1j )TΦ1j (yi, 0)) − ((Θ˜ 1i )TΦ1i (yj , el) − (Θ˜ 1i )TΦ1i (yj , 0)) =((Θ1j )TΦ1j (yj , el) − (Θ1j )TΦ1j (yj , 0)) − ((Θ˜ 1i )TΦ1i (yi, el) − (Θ˜ 1i )TΦ1i (yi, 0)), which by simple rearranging further leads to Θ1j,l(Φ1j,l(yj , 1) − Φ1j,l(yj , 0) − Φ1j,l(yi, 1)) = Θ˜ 1i,l(Φ1i,l(yi, 1) − Φ1i,l(yi, 0) − Φ1i,l(yj , 1)).

Since Θ1j,l, Θ˜ 1i,l > 0, and by deﬁnition we have

|Φ1j,l(yj , 1) − Φ1j,l(yj , 0) − Φ1j,l(yi, 1)| = 1,

and |Φ1i,l(yi, 1) − Φ1i,l(yi, 0) − Φ1i,l(yj , 1)| = 1,
we obtain Θ1j,l = Θ˜ 1i,l, and

Φ1j,l(yj , 1) − Φ1j,l(yj , 0) − Φ1j,l(yi, 1) = Φ1i,l(yi, 1) − Φ1i,l(yi, 0) − Φ1i,l(yj , 1).

(15)

Therefore, either tyjyˆl ∈ {to, tsd, tsg} and tyiyˆl ∈ {to, tsd, tsg}, or tyjyˆl = te and tyiyˆl = te, which by deﬁnition further indicates that Φ2i = Φ2j (recall the way we build dependency between Y and Yˆ ).

As l is arbitrarily picked, we then have Θ1j is equal to Θ˜ 1i component-wisely.

By the equality between the ﬁrst term and the third term in Eq. (13) and following exact the same routine, we also have Θ˜ 1j = Θ1i .

On the other hand, for any r ∈ [kˆ], ﬁxing Y¯ and Yˆ s (∀s = r), and setting Yˆr = yˆlr (l ∈ kr, yˆlr ∈ N (yj, Yl)) in Eq. (14), we have

(Θ1j )TΦ1j (yj , Y¯ ) + (Θ˜ 1i )TΦ1i (yj , Y¯ ) + Θ2j,r,lΦ2j,r,l(yj , yˆlr) + Θ2j,sΦ2j,s(yj , Y s)
s=r

=(Θ1j )TΦ1j (yi, Y¯ ) + (Θ˜ 1i )TΦ1i (yi, Y¯ ) + Θ˜ 2i,r,lΦ2i,r,l(yi, yˆlr) +

Θ˜

2 i,s

Φ

2 i,s

(

yi

,

Yˆ

s

).

s=r

On the other hand, by Lemma 2, there exists some p, s.t., yˆpr ∈/ N (yj, Yr) (which by Φ2i = Φ2j further leads to yˆpr ∈/ N (yi, Yr)). Setting Yˆr = yˆlr leads to

(Θ1j )TΦ1j (yj , Y¯ ) + (Θ˜ 1i )TΦ1i (yj , Y¯ ) + Θ2j,sΦ2j,s(yj , Y s)
s=r

=(Θ1j )TΦ1j (yi, Y¯ ) + (Θ˜ 1i )TΦ1i (yi, Y¯ ) +

Θ˜

2 i,s

Φ

2 i,s

(

yi

,

Yˆ

s

).

s=r

Subtracting the above two equations leads to Θ2j,a,l = Θ˜ 2i,a,l. Since a and l are arbitrarily picked, we conclude that Θ2j = Θ˜ 2i . Following the same routine, we also have Θ2i = Θ˜ 2j . Therefore, by applying Θ1j = Θ˜ 1i , Θ1i = Θ˜ 1j , Θ2j = Θ˜ 2i , and Θ2i = Θ˜ 2j in Eq. (13), we have
(Θ1i )TΦ1i (yi, Y¯ ) − (Θ1i )TΦ1j (yj , Y¯ ) = (Θ1i )TΦ1i (yj , Y¯ ) − (Θ1i )TΦ1j (yi, Y¯ ), (Θ1j )TΦ1j (yj , Y¯ ) − (Θ1j )TΦ1i (yi, Y¯ ) = (Θ1j )TΦ1j (yi, Y¯ ) − (Θ1j )TΦ1i (yj , Y¯ ).

20

Published as a conference paper at ICLR 2022

Let Y¯ = 1kˆ (i.e., the kˆ-dimension all 1 vector), we have

(Θ1i )T((Φ1i (yi, 1kˆ) − Φ1i (yj , 1kˆ)) − ((Φ1j (yj , 1kˆ) − Φ1j (yi, 1kˆ)))) = 0,

(16)

(Θ1j )T((Φ1i (yi, 1kˆ) − Φ1i (yj , 1kˆ)) − ((Φ1j (yj , 1kˆ) − Φ1j (yi, 1kˆ)))) = 0.

(17)

Since yi and yj are asymmetric, we have that there exists l, such that tyiyˆl = tyjyˆl . Concretely, by Eq. (15), we have tyiyˆl ∈ {to, tsd, tsg}, tyjyˆl = {to, tsd, tsg}, and tyiyˆl = tyjyˆl . On the other hand,
Φ1i,l(yi, 1) − Φ1i,l(yj , 1)) = Φ1j,l(yj , 1) − Φ1j,l(yi, 1), if and only if tyiyˆl = to, tyjyˆl = tsg, or tyjyˆl = to, tyiyˆl = tsg, which contradicts Lemma 1.

Therefore,

Φ1i,l(yi, 1) − Φ1i,l(yj , 1)) = Φ1j,l(yj , 1) − Φ1j,l(yi, 1).

In this case, solutions of Θ1i , Θ1j subject to respectively Eqs. (16) and (17) lie along a zero-measure set.

The proof is completed.

D PROOF OF THEOREM 1
D.1 LEARNING ALGORITHM
We ﬁrst present the algorithm for producing Θˆ and Wˆ in Algorithm 1.
Algorithm 1 WIS Require: Step size η, dataset D ⊂ X , and initial parameter Θ0.
Θˆ → Θ0. for all X ∈ D do
Independently sample (Y, Y¯ , Yˆ ) from πΘˆ , and (Y , Y¯ , Yˆ ) from πΘˆ conditionally given Yˆ = Yˆ (X). Θˆ ← Θˆ + η(Φ(Y, Y¯ , Yˆ ) − Φ(Y , Y¯ , Yˆ )). Compute Wˆ as described in (3) using Θˆ . output (Θˆ , Wˆ )

D.2 ASSUMPTIONS

First, the problem distribution π∗ needs to be accurately modeled by some distribution Θ∗ in the family that we are trying to learn:

∃Θ∗ s.t. ∀(Y, Yˆ ), p(X,Y )∼π∗ (Y, Yˆ ) = pθ∗ (Y, Yˆ ).

(18)

Secondly, given an example (X, Y ) ∼ π∗, we assume Y is independent of X given Yˆ (X):

(X, Y ) ∼ π∗ ⇒ Y ⊥ X | Yˆ (X).

(19)

This assumption encodes the idea that while the ILFs can be arbitrarily dependent on the features,
they provide sufﬁcient information to accurately identify the true label vector. Then, for any Θ, accurately learning Θ from data distribution is possible. That is, there exists an unbiased estimator Θˆ (D) which is a function of the dataset D of i.i.d from πΘ, such that, for any Θ and some c > 0,

Cov(Θˆ (D)) I .

(20)

2c|D|

And we are reasonably certain in our guess of latent variables, i.e., Y and Y¯ . That is, for any Θ, Θ∗,

k

kˆ

1 2

EYˆ ∗∼Θ∗

(ni + kˆ)Var(Y,Y¯ ,Yˆ )∼πΘ (1Y =yi |Yˆ = Yˆ ∗)2 + (mi + K − 1)Var(Y,Y¯ ,Yˆ )∼πΘ (Y¯ i|Yˆ = Yˆ ∗)2

i=1

i=1

≤ √ c . (21) 2M

21

Published as a conference paper at ICLR 2022

We also assume that the output of the last layer of end model hW has bounded ∞ norm, that is, for any possible parameter W ,

hW ∞ ≤ H.

(22)

Finally, we assume that solving Eq. (3) has bounded generalization risk such that for some χ > 0,

solution Wˆ satisﬁes

EWˆ Θˆ (Wˆ ) − min Θˆ (W ) ≤ χ.

(23)

W

D.3 PROOF OF THEOREM 1

To begin with, we state two basic lemmas needed for proofs throughout this section: Lemma D.1. Let x1, x2 be two binary random variable. Then we have variance of product of x1 and x2 can be bounded as
Var [x1x2] ≤ Var [x1] + Var [x2] .
Lemma D.2. Let Y be a random vector and · s be the spectral norm. Then we have
Cov(Y, Y ) s ≤ Var(Yi).
i

Then, we borrow two lemmas from (Ratner et al., 2016), which are slightly different from the original ones but can be easily proved following the same derivations: Lemma D.3. [Lemma D.1 in (Ratner et al., 2016)] Given a family of maximum-entropy distributions
πΘ(Y, Y¯ , Yˆ ) = 1 exp (ΘTΦ(Y, Y¯ , Yˆ )). ZΘ
If we let J be the maximum expected log-likelihood objective, under another distribution π∗, for the event associated with the observed labeling function values Yˆ ,
J (Θ) = E(Y ∗,Y¯ ∗,Yˆ ∗)∼π∗ log P(Y,Y¯ ,Yˆ )∼πΘ Yˆ = Yˆ ∗ ,
then its Hessian can be calculated as ∇2J (Θ) = E(Y ∗,Y¯ ∗,Yˆ ∗)∼π∗ Cov(Y,Y¯ ,Yˆ )∼πΘ φ(Y, Y¯ , Yˆ ) | Yˆ = Yˆ ∗ −Cov(Y,Y¯ ,Yˆ )∼πΘ (φ(Y, Y¯ , Yˆ )).

Lemma D.4. [Lemma D.4 in (Ratner et al., 2016)] Suppose that we are looking at a WIS maximum likelihood estimation problem and the objective function J(Θ) is strongly concave with concavity parameter c > 0. If we run stochastic gradient descent using unbiased samples from a true distribution πΘ∗ , then if we set step size as

η = c 2, 4
and run (using a fresh sample at each iteration) for T steps, where

2

2 Θ0 − Θ∗ 2

T = c2 2 log

.

We can bound the expected parameter estimation error with

E

Θˆ − Θ∗

2
≤M

2,

(24)

where M is the dimension of Θ.

Based on Lemma D.4, in order to obtain the optimization error with respect to the estimated Θˆ produced by Algorithm 1, we only need to show that the WIS object function J(Θ)2 is strongly concave. We prove this through the following lemma, which is a non-trivial extension of Lemma D.3
in (Ratner et al., 2016) given the fact that we have multiple latent variables and relatively complex
dependency structures with comparison to (Ratner et al., 2016):

2Note that, in the Eq. (2) of the main body of the paper, we are minimizing −J(Θ), which is equivalent to maximizing J(Θ) as discussed here.

22

Published as a conference paper at ICLR 2022

Lemma D.5. [Extension of Lemma D.3 in (Ratner et al., 2016)] With conditions (20) and (21), the WIS objective function J(Θ) is strongly concave with strong convexity c.
We then come to bound the generalization error of Wˆ produced by Algorithm 1, using the following non-trivial extension of Lemma D.5 in (Ratner et al., 2016):
Lemma D.6. [Extension of Lemma D.5 in (Ratner et al., 2016)] Suppose that conditions (18)-(23) hold. Let Wˆ be the learned parameters of the end model produced by Algorithm 1, and (W ∗) be the minimum of cross entropy loss function . Then, we can bound the expected risk with
E (Wˆ ) − (W ∗) ≤ χ + 4cH .

Finally, we conclude Lemmas (D.4), (D.5) and (D.6) as the following theorem, which is identical to the Theorem 1 in the main body of the paper:
Theorem 4 (Extension of Theorem 2 in (Ratner et al., 2016)). Suppose that we run Algoirthm 1 on a WIS speciﬁcation to produce Θˆ and Wˆ , and all conditions of Lemmas (D.5) and (D.6) are satisﬁed. Then, for any > 0, if we set the step size to be

η= c 2 4
and the input dataset D is large enough such that

2

2 Θ0 − Θ∗ 2

|D| > c2 2 log

,

then we can bound the expected parameter error and the expected risk as:

E

Θˆ − Θ∗

2
≤M

2,

E

(Wˆ ) − (W ∗) ≤ χ + 4cH .

D.4 PROOFS OF LEMMAS
Lemma D.1. Let x1, x2 be two binary random variable. Then we have variance of product of x1 and x2 can be bounded as
Var [x1x2] ≤ Var [x1] + Var [x2] .

Proof. Joint distribution of x1 and x2 can be listed as the following table: (where p1 + p2 + p3 + p4 = 1)
x1/x2 0 1 0 p1 p2 1 p3 p4

Then we have while

Var [x1x2] = p4 − p24 = p4(p1 + p2 + p3),

Var [X1] + Var [X2] = (p2 + p4)(p1 + p3) + (p3 + p4)(p1 + p2) ≥ p4(p1 + p2 + p3).

The proof is completed.
Lemma D.2. Let Y be a random vector and · s be the spectral norm. Then we have
Cov(Y, Y ) s ≤ Var(Yi).
i

23

Published as a conference paper at ICLR 2022

Proof. By deﬁnition of spectral norm, we have Cov(Y, Y ) s = max xTCov(Y, Y )x
x 2≤1

Where x is a constant vector. And by Cauchy-Schwarz inequality,

xTCov(Y, Y )x = E xT(Y − E [Y ])(Y − E [Y ])Tx ≤ E x 2 Y − E [Y ] 2 .

Because x is a constant vector and x ≤ 1,

max E x 2 Y − E [Y ] 2
x 2≤1
= max x 2 E Y − E [Y ] 2
x 2≤1

= max x 2
x 2≤1
= Var(Yi).
i

Var(Yi)
i

The proof is completed.

Lemma D.5. [Extension of Lemma D.3 in (Ratner et al., 2016)] With conditions (20) and (21), the WIS objective function J(Θ) is strongly concave with strong convexity c.

Proof. By Lemma D.3, hessian matrix of J can be decomposed as follows: ∇2J (Θ) = EYˆ ∗∼πΘ∗ Cov(Y,Y¯ ,Yˆ )∼πΘ Φ(Y, Y¯ , Yˆ ) | Yˆ = Yˆ ∗ −Cov(Y,Y¯ ,Yˆ )∼πΘ (Φ(Y, Y¯ , Yˆ )).

Basically, to prove that J(Θ) is strongly concave with strong convexity c, we need to show for a real number c > 0,
∇2J(Θ) cI.

We calculate each term separately: for the ﬁrst term

A = EYˆ ∗∼πΘ∗ Cov(Y,Y¯ ,Yˆ )∼πΘ Φ(Y, Y¯ , Yˆ ) | Yˆ = Yˆ ∗ ,

since A is symmetric, for any real number c, A cI, if and only if its spectral norm where A s equals to the eigenvalue of A with largest absolute value.
Since by deﬁnition, vector function Φ(Y, Y¯ , Yˆ ) can be represented as:

A s ≤ c,







Φ(Y,

Y¯

,

Yˆ

)

=

 









φAcc j (Y, Yˆ j )



yi,yˆl ,j

i∈[k],j∈[n],yˆj ∈N (yi,Yj )

l



φ

Acc yˆ ,j

(Y¯

i

,

Yˆ

j

)

 

i

j ∈[n],yˆi ∈Yj

,

φtyˆ ,yˆ (Y¯ i, Y¯ j )

 

ij

i,j∈[kˆ]



φty ,yˆ (Y, Y¯ j )



ij

i∈[k],j∈[kˆ]

24

Published as a conference paper at ICLR 2022

by Lemma D.2, we have A can be further bounded by





kn

A ≤ EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j=1 yˆj ∈N (yi,Yj )
l

 φAcc j (Y, Yˆ j ) | Yˆ = Yˆ ∗ 
yi,yˆl ,j


n

+ EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

j=1 yˆi∈Yj



φ

Acc yˆ ,j

(Y¯

i

,

Yˆ

j

)

|

Yˆ

=

Yˆ ∗



i





+ EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

1≤i,j≤kˆ


k

kˆ

+ EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j=1

φtyˆ ,yˆ (Y¯ i, Y¯ j ) | Yˆ = Yˆ ∗  ij
 φty ,yˆ (Y, Y¯ j) | Yˆ = Yˆ ∗ 
ij

=A1 + A2 + A3 + A4,

where


kn

A1 =EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j=1 yˆj ∈N (yi,Yj )
l

φAcc j (Y, Yˆ j ) | Yˆ = Yˆ ∗
yi,yˆl ,j


n

A2 =EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

j=1 yˆl∈Yj



φ

Acc yˆ ,j

(Y

,

Yˆ

j

)

|

Yˆ

=

Yˆ ∗

 ;

l





  ;

A3 =EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

1≤i,j≤kˆ

φtyˆ ,yˆ (Y¯ i, Y¯ j ) | Yˆ = Yˆ ∗  ; ij


k

kˆ

A4 =EYˆ ∗∼πΘ∗ 

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j=1

 φty ,yˆ (Y, Y¯ j) | Yˆ = Yˆ ∗  .
ij

We then bound the four terms respectively. As for A1, for ﬁxed Yˆ ∗, we have

kn
Var(Y,Y¯ ,Yˆ )∼πΘ
i=1 j=1 yˆj ∈N (yi,Yj )
l

φAcc j (Y, Yˆ j ) | Yˆ = Yˆ ∗
yi,yˆl ,j

kn

=

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j=1 yˆj ∈N (yi,Yj )
l

1Y =y ∧Yˆ j=yˆj | Yˆ = Yˆ ∗

i

l


k

=

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j∈[n],yˆj ∈N (yi,Yj ),(Yˆ ∗)j =yˆj

l

l


1Y =yi | Yˆ = Yˆ ∗ 


k

=

Var(Y,Y¯ ,Yˆ )∼πΘ

i=1 j∈[n],yˆj ∈N (yi,Yj ),(Yˆ ∗)j =yˆj

l

l


1Y =yi | Yˆ = Yˆ ∗ 

k
≤ niVar(Y,Y¯ ,Yˆ )∼πΘ 1Y =yi | Yˆ = Yˆ ∗ ,
i=1
where ni is the number of ILFs whose label space contains label that is non-exclusive to label yi, i.e., ni = |{j ∈ [n]|N (yi, Yj) = ∅}|.

Therefore, we have
k
A1 ≤ 1 niEYˆ ∗∼πΘ∗ Var(Y,Y¯ ,Yˆ )∼πΘ Y =yi | Yˆ = Yˆ ∗ .
i=1

25

Published as a conference paper at ICLR 2022

Similarly, for A2, we have

kˆ
A2 ≤ miEYˆ ∗∼πΘ∗ Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ i | Yˆ = Yˆ ∗ ,
i=1
where mi is the number of ILFs whose label space contains the label yˆi.
As for A3, for ﬁxed Yˆ ∗ and any yˆi, yˆj ∈ Yˆ, we further separate the proof into subcases by tyˆ yˆ ij
which is simpliﬁed as t:
(1). t = to. In this case,

Var(Y,Y¯ ,Yˆ )∼π φtyˆ ,yˆ (Y¯ i, Y¯ j ) | Yˆ = Yˆ ∗

Θ

ij

=Var(Y,Y¯ ,Yˆ )∼πΘ 1Y¯ i=Y¯ j | Yˆ = Yˆ ∗

=Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ iY¯ j | Yˆ = Yˆ ∗

(∗)
≤ Var ¯ ˆ

Y¯ i | Yˆ = Yˆ ∗ + Var ¯ ˆ

Y¯ j | Yˆ = Yˆ ∗ ,

(Y,Y ,Y )∼πΘ

(Y,Y ,Y )∼πΘ

where Eq. (∗) is due to Lemma D.1. (2). t = te. Similarly,

Var(Y,Y¯ ,Yˆ )∼π φtyˆ ,yˆ (Y¯ i, Y¯ j ) | Yˆ = Yˆ ∗

Θ

ij

=Var(Y,Y¯ ,Yˆ )∼πΘ −1Y¯ i=Y¯ j =1 | Yˆ = Yˆ ∗

=Var(Y,Y¯ ,Yˆ )∼πΘ 1Y¯ i=Y¯ j =1 | Yˆ = Yˆ ∗

=Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ iY¯ j | Yˆ = Yˆ ∗

≤Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ i | Yˆ = Yˆ ∗ + Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ j | Yˆ = Yˆ ∗ ,

(3). t = tsg. In this case,

Var(Y,Y¯ ,Yˆ )∼πΘ =Var(Y,Y¯ ,Yˆ )∼πΘ =Var(Y,Y¯ ,Yˆ )∼πΘ ≤Var(Y,Y¯ ,Yˆ )∼πΘ =Var(Y,Y¯ ,Yˆ )∼πΘ

φtyˆ ,yˆ (Y¯ i, Y¯ j ) | Yˆ = Yˆ ∗ ij
−1Y¯ i=1,Y¯ j=0 | Yˆ = Yˆ ∗ (1 − Y¯ i)Y¯ j | Yˆ = Yˆ ∗ 1 − Y¯ i | Yˆ = Yˆ ∗ + Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ j | Yˆ = Yˆ ∗ Y¯ i | Yˆ = Yˆ ∗ + Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ j | Yˆ = Yˆ ∗ ,

(4). t = tsd. Similar to (3).,

Var(Y,Y¯ ,Yˆ )∼πΘ =Var(Y,Y¯ ,Yˆ )∼πΘ =Var(Y,Y¯ ,Yˆ )∼πΘ ≤Var(Y,Y¯ ,Yˆ )∼πΘ =Var(Y,Y¯ ,Yˆ )∼πΘ

φtyˆ ,yˆ (Y¯ i, Y¯ j ) | Yˆ = Yˆ ∗ ij
−1Y¯ i=0,Y¯ j=1 | Yˆ = Yˆ ∗ (1 − Y¯ j)Y¯ i | Yˆ = Yˆ ∗ 1 − Y¯ j | Yˆ = Yˆ ∗ + Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ i | Yˆ = Yˆ ∗ Y¯ i | Yˆ = Yˆ ∗ + Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ j | Yˆ = Yˆ ∗ ,

26

Published as a conference paper at ICLR 2022

Combining (1), (2), (3), and (4), we have
kˆ
A3 ≤ (kˆ − 1)EYˆ ∗∼πΘ∗ Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ i | Yˆ = Yˆ ∗ ,
i=1

As for A4, by similar discussion of A3,

kˆ

k

A4 ≤ kEYˆ ∗∼πΘ∗ Var(Y,Y¯ ,Yˆ )∼πΘ Y¯ i | Yˆ = Yˆ ∗ + 1 kˆEYˆ ∗∼πΘ∗ Var(Y,Y¯ ,Yˆ )∼πΘ Y =yi | Yˆ = Yˆ ∗ .

i=1

i=1

Combining estimation of A1, A2, A3, A4, and by condition (21) we have

As

≤A1 + A2 + A3 + A4


k

kˆ



≤EYˆ ∗∼πΘ∗  (ni + kˆ)VarY,Yˆ (1Y =yi |Yˆ = Yˆ ∗) + (mi + K − 1)VarY,Yˆ (Y¯ i|Yˆ = Yˆ ∗)

i=1

i=1

1


k

kˆ

2

≤EYˆ ∗∼πΘ∗  (ni + kˆ)Var2Y,Yˆ (Y |Yˆ = Yˆ ∗) + (mi + K − 1)Var2Y,Yˆ (Y¯ i|Yˆ = Yˆ ∗)

i=1

i=1

1

k

K

2

· (ni + kˆ) + (mi + K − 1)

i=1
c

√

i=1

≤ √ · 2M ≤ c,

2M

which further leads to

A cI.

For the second term B = Cov(Y,Y¯ ,Yˆ )∼πΘ (Φ(Y, Y¯ , Yˆ )),

B = E(Y,Y¯ ,Yˆ )∼πΘ (Φ(Y, Y¯ , Yˆ ) − E(Y,Y¯ ,Yˆ )∼πΘ [Φ(Y, Y¯ , Yˆ )])2

 
= EY,Y¯ ,Yˆ ∼πΘ Φ(Y, Y¯ , Yˆ ) −

Y ,Y¯ ,Yˆ Φ(Y , Y¯ , Yˆ ) exp ΘT Φ(Y , Y¯ , Yˆ ) Y ,Y¯ ,Yˆ exp ΘT Φ(Y , Y¯ , Yˆ )

2
 


 
= EY,Y¯ ,Yˆ ∼πΘ ∇Θ log exp ΘT Φ(Y, Y¯ , Yˆ )



− ∇Θ log 

exp

Y ,Y¯ ,Yˆ

ΘT Φ(Y , Y¯ , Yˆ )

2
 


= E(Y,Y¯ ,Yˆ )∼πΘ ∇Θ log πΘ(Y, Y¯ , Yˆ ) 2 ,

where E(Y,Y¯ ,Yˆ )∼πΘ ∇Θ log πΘ(Y, Y¯ , Yˆ ) 2 bound and the condition (20),

is the Fisher Information of Θ. By the Cramér-Rao

I 2c|D|

Cov(Θˆ )

DE(Y,Yˆ )∼π

2 −1

∇Θ log πΘ(Y, Yˆ )

,

Θ

which further leads to B = E(Y,Y¯ ,Yˆ )∼π ∇Θ log πΘ(Y, Y¯ , Yˆ ) 2 2cI.
Θ

The proof is completed by putting estimation of terms A and B together.

27

Published as a conference paper at ICLR 2022

Lemma D.6. [Extension of Lemma D.5 in (Ratner et al., 2016)] Suppose that conditions (18)-(23) hold. Let Wˆ be the learned parameters of the end model produced by Algorithm 1, and (W ∗) be the minimum of cross entropy loss function . Then, we can bound the expected risk with
E (Wˆ ) − (W ∗) ≤ χ + 4cH .
Proof. We begin by rewriting objective of expected loss minimization problem using law of total expectation as follows:
(W ) =E(X,Y )∼π∗ E(X,Y )∼π∗ [H(Y, σ(h(X, W )))|X] =E(X ,Y )∼π∗ E(X,Y )∼π∗ [H(Y, σ(h(X, W )))|X = X ] =E(X ,Y )∼π∗ E(X,Y )∼π∗ [H(Y, σ(h(X , W )))|X = X ]
and by our conditional independence assumption (condition (19)), we have P(Y |X = X ) = P(Y |Yˆ (X) = Yˆ (X )),
which further leads to (W ) =E(X ,Y )∼π∗ E(X,Y )∼π∗ H(Y, σ(h(X , W ))) Yˆ (X) = Yˆ (X ) =E(X ,Y )∼π∗ E(Y,Yˆ )∼πΘ∗ H(Y, σ(h(X , W ))) Yˆ = Yˆ (X )
On the other hand, if we are minimizing the model with learned parameter Θˆ , we will be actually minimizing
Θˆ (W ) = E(X ,Y )∼π∗ E(Y,Yˆ )∼πΘˆ H(Y, σ(h(X , W ))) Yˆ = Yˆ (X ) ,
where for any X , E(Y,Yˆ )∼πΘˆ H(Y, σ(h(X , W ))) Yˆ = Yˆ (X ) can be further calculated as
E(Y,Yˆ )∼πΘˆ H(Y, σ(h(X , W ))) Yˆ = Yˆ (X )
k
= log (σ(h(X , W ))l) P(Y,Yˆ )∼πΘˆ (Y = yl|Yˆ = Yˆ (X )).
l=1
For simpliﬁcation, we rewrite P(Y,Yˆ )∼πΘˆ (Y = yl|Yˆ = Yˆ (X )) as follows with slight abuse of notations:

P(Y,Yˆ )∼π (Y = yl|Yˆ = Yˆ (X )) = Pπˆ (yl|Yˆ (X )),

Θˆ

Θ

and similarly

E(X ,Y )∼π∗ = Eπ∗ ,

Let lX = arg minl log (σ(h(X , W ))l). The difference between the loss functions will be

| Θˆ (W ) −

(W )| = =

k

Eπ∗

log σ(h(X , W ))l

l=1

Eπ∗ log σ(h(X , W ))lX



PπΘ∗ (yl|Yˆ (X )) − Pπ ˆ (yl|Yˆ (X )) Θ

PπΘ∗ (yl |Yˆ (X )) − Pπ ˆ (yl |Yˆ (X ))

X

ΘX



+ Eπ∗ 

log σ(h(X , W ))l

l=lX

PπΘ∗ (yl|Yˆ (X )) − Pπ ˆ (yl|Yˆ (X ))  . Θ

28

Published as a conference paper at ICLR 2022

Furthermore,

Eπ∗ log σ(h(X , W ))lX 

PπΘ∗ (yl |Yˆ (X )) − Pπˆ (yl |Yˆ (X ))

X

ΘX



+ Eπ∗ 

log (σ(h(X , W ))l) PπΘ∗ (yl|Yˆ (X )) − Pπˆ (yl|Yˆ (X ))  Θ

l=lX







= Eπ∗ log σ(h(X , W ))lX 

−

PπΘ∗ (yl|Yˆ (X )) +

Pπˆ (yl|Yˆ (X ))

Θ

l=lX

j=lX



+ Eπ∗ 

log (σ(h(X , W ))l) PπΘ∗ (yl|Yˆ (X )) − Pπˆ (yl|Yˆ (X ))  Θ

l=lX





= Eπ∗ 
l=lX


log (σ(h(X , W ))l) − log σ(h(X , W ))lX

PπΘ∗ (yl|Yˆ (X )) − Pπˆ (yl|Yˆ (X ))  Θ 

= Eπ∗ 

h(X , W )l − h(X , W )l PπΘ∗ (yl|Yˆ (X )). − Pπˆ (yl|Yˆ (X ))  .

(25)

X

Θ

l=lX

Let h¯(l1, l2) = h(X , W )l1 − h(X , W )l2 .
By Eq. (22), we have for any l ∈ [k],
0 ≤ h¯(l, lX ) ≤ 2H.

For any ﬁxed X , deﬁne gX (Θ) as follows:

gX (Θ) =

h¯(l, lX )PπΘ (yl|Yˆ (X )),

(26)

l=lX

based on which we have

Θˆ (W ) − (W ) ≤ Eπ∗ gX (Θˆ ) − gX (Θ∗)

By First Mean Value Theorem, gX (Θˆ ) − gX (Θ∗) = ∇gX (ξ), Θˆ − Θ∗ ≤ Θˆ − Θ∗

∇gX (ξ) .

We then bound ∇gX (ξ) element-wisely: (1). For any i ∈ [k], j ∈ [n], yˆlj ∈ N (yi, Yj), if i = lX , Yˆ j(X ) = yˆlj,

∂gX (ξ) ∂θAcc j
yi,yˆl ,j

=

h¯(l, l ) ∂Pπξ (yl|Yˆ (X ))

X

∂θAcc j

l=lX

yi,yˆl ,j

=−

h¯(l, lX )Pπξ (yi|Yˆ (X ))Pπξ (yl|Yˆ (X ))

l=lX

=

h¯(l, lX )Pπξ (yi|Yˆ (X ))Pπξ (yl|Yˆ (X ))

l=lX

≤2HPπξ (yi|Yˆ (X ))(1 − Pπξ (yi|Yˆ (X )))

=2HVar 1Y =yi |Yˆ (X ) .

29

Published as a conference paper at ICLR 2022

If i = lX , Yˆ j(X ) = yˆlj,

∂gX (ξ) ∂θAcc j
yi,yˆl ,j

=

h¯(l, l ) ∂Pπξ (yl|Yˆ (X ))

X

∂θAcc j

l=lX

yi,yˆl ,j

=−

h¯(l, lX )Pπξ (yi|Yˆ (X ))Pπξ (yl|Yˆ (X ))

l∈/{i,lX }

+ h¯(i, lX ) Pπξ (yi|Yˆ (X )) − Pπξ (yi|Yˆ (X ))Pπξ (yi|Yˆ (X ))



 ≤ max

h¯(l, lX )Pπξ (yi|Yˆ (X ))Pπξ (yl|Yˆ (X )),

l∈/{i,lX }

h¯(i, lX ) Pπξ (yi|Yˆ (X )) − Pπξ (yi|Yˆ (X ))Pπξ (yi|Yˆ (X ))

≤2HPπξ (yi|Yˆ (X ))(1 − Pπξ (yi|Yˆ (X )))

=2HVar 1Y =yi |Yˆ (X ) .

If Yˆ j(X ) = yˆlj,

∂gX (ξ) ∂θAcc j
yi,yˆl ,j

=

h¯(l, l ) ∂Pπξ (yl|Yˆ (X ))

X

∂θAcc j

l=lX

yi,yˆl ,j

= 0.

(2). For j ∈ [n], yˆr ∈ Yj, if Yˆ j(X ) = yˆr,

∂gX (ξ) ∂ θyAˆ c,cj
r

=

h¯(l, l ) ∂Pπξ (yl|Yˆ (X ))

X

∂ θyAˆ c,cj

l=lX

r

=

h¯(l, lX ) Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X )) .

l=lX

Let
f1(l) = Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) f2(l) = Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X )),

and
B1 = {l : f1(l) ≥ f2(l), l = lX }, B2 = {l : f1(l) < f2(l), l = lX }.

30

Published as a conference paper at ICLR 2022

Therefore,

h¯(l, lX ) Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X ))
l=lX

=

h¯(l, lX ) (f1(l) − f2(l))

l=lX

= h¯(l, lX ) (f1(l) − f2(l)) + h¯(l, lX ) (f1(l) − f2(l))

l∈B1

l∈B2

≤ max

h¯(l, lX ) (f1(l) − f2(l))

t=1,2

l∈BT

= max

h¯(l, lX ) (f1(l) − f2(l)) , h¯(l, lX ) (f2(l) − f1(l)) .

l∈B1

l∈B2

On the other hand,

h¯(l, lX ) (f1(l) − f2(l))

l∈B1

= h¯(l, lX ) Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X ))
l∈B1

≤2H

Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X ))

l∈B1

=2H Pπξ (Y = yl, ∃l ∈ B1, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl, ∃l ∈ B1|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X ))

=2H Pπξ (Y = yl, ∃l ∈ B1, Y¯ r = 1|Yˆ (X ))Pπξ (Y¯ r = 0|Yˆ (X ))

− Pπξ (Y = yl, ∃l ∈ B1, Y¯ r = 0|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X ))

≤2H Pπξ (Y¯ r = 1|Yˆ (X ))Pπξ (Y¯ r = 0|Yˆ (X ))

=2HVar Y¯ r|Yˆ (X ) .

Similarly, we have

h¯(l, lX ) (f2(l) − f1(l)) − h¯(l, lX )

l∈B1

l∈B2

≤2HVar Y¯ r|Yˆ (X ) .

Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) + Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X ))

Conclusively, we have

∂gX (ξ) ≤ 2HVar Y¯ r|Yˆ (X ) . ∂ θyAˆ c,cj
r

If Yˆ j = yˆr, similar to (1), we have

∂gX (ξ) = 0. ∂ θyAˆ c,cj
r

31

Published as a conference paper at ICLR 2022

(3). For any yˆi, yˆj ∈ Yˆ, by the deﬁnition of φtyˆ ,yˆ , there exists (a, b) ∈ {0, 1}2, such that ij
φtyˆ ,yˆ (a, b) = 0. Similar to (2), let ij

f3(l) = Pπξ (Y = yl, Y¯ i = a, Y¯ j = b|Yˆ (X )) f4(l) = Pπξ (Y = yl|Yˆ (X ))Pπξ (Y¯ i = a, Y¯ j = b|Yˆ (X ))
and
B3 = {l : f3(l) ≥ f4(l), l = lX }, B4 = {l : f3(l) < f4(l), l = lX }

we have

∂gX (ξ) = max ∂θytˆ ,yˆ
ij

h¯(l, lX ) (f3(l) − f4(l)) , h¯(l, lX ) (f4(l) − f3(l))

l∈B3

l∈B4

≤2HVar φtyˆ ,yˆ (Y¯ i, Y¯ j)|Yˆ (X ) ij

(∗)
≤ 2H

Var Y¯ i|Yˆ (X )

+ Var Y¯ j|Yˆ (X )

,

where inequality (∗) comes from Lemma D.1.

(4). For any yi ∈ Y, yˆr ∈ Yˆ, by the deﬁnition of φty ,yˆ , there exists a ∈ {0, 1}, yj ∈ Y, s.t.,

ir

φty ,yˆ (yj, a) = 0. We further divide the proof into two cases: φty ,yˆ (yi, a) = 0, and φty ,yˆ (yi, a) =

0. i r

ir

ir

(4a). If φty ,yˆ (yi, a) = 0, we have tyiyˆr = tsg and consequently a = 1. Similar to (1-3)., we have ir

∂gX (ξ) =

h¯(l, l

∂Pπξ (yl|Yˆ (X )) (•)

)

=

k
h¯(l, l

) ∂Pπξ (yl|Yˆ (X ))

∂θyt ,yˆ

X

∂θyt ,yˆ

X

∂θyt ,yˆ

ir

l=lX

ir

l=1

ir

= h¯(l, lX ) Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
l=i
− h¯(i, lX )Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) ,

where Eq. (•) is due to Let

f5(l) = Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) f6(l) = Pπξ (Y = yl|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))

and

B5 = {l : f5(l) ≥ f6(l), B6 = {l : f5(l) < f6(l),

l = i}, l = i}

Then we have

∂gX (ξ) ≤ max

h¯(l, lX ) (f5(l) − f6(l)) , h¯(l, lX ) (f6(l) − f5(l)) + h¯(i, lX )f6(i) .

∂θyt ,yˆ ir

l∈B5

l∈B6

32

Published as a conference paper at ICLR 2022

On one hand,
h¯(l, lX ) (f5(l) − f6(l))
l∈B5
= h¯(l, lX ) Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
l∈B5
≤ 2H Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
l∈B5
=2H Pπξ (Y = yl, ∃l ∈ B5, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yl, ∃l ∈ B5|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
=2H Pπξ (Y = yl, ∃l ∈ B5, Y¯ r = 1|Yˆ (X ))(1 − Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )))
− Pπξ (Y = yl, ∃l ∈ B5, Y¯ r = 0|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) ≤2HPπξ (Y = yi, Y¯ r = 1|Yˆ (X ))(1 − Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))) =2HVarπξ φty ,yˆ (Y, Y¯ r)|Yˆ (X )
il
≤2HVarπξ 1Y =yi |Yˆ (X ) + 2HVarπξ Y¯ r|Yˆ (X ) .

On the other hand,
h¯(l, lX ) (f6(l) − f5(l)) + h¯(i, lX )f6(i)
l∈B6
= − h¯(l, lX ) Pπξ (Y = yl, Y¯ r = 1|Yˆ (X )) + Pπξ (Y = yl|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
l∈B6
+ h¯(i, lX )Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) ≤2H −Pπξ (Y = yl, ∃l ∈ B6, Y¯ r = 1|Yˆ (X ))(1 − Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )))
− Pπξ (Y = yl, ∃l ∈ B6, Y¯ r = 0|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) + Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
≤2H Pπξ (Y = yl, ∃l ∈ B6, Y¯ r = 0|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
+ Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
≤2H Pπξ (Y¯ r = 0|Yˆ (X ))Pπξ (Y¯ r = 1|Yˆ (X )) + Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))
≤2HVarπξ 1Y =yi |Yˆ (X ) + 2HVarπξ Y¯ r|Yˆ (X ) .

Therefore, in this case, we have

∂gX (ξ) ≤ 2HVarπ 1Y =y |Yˆ (X ) + 2HVarπ Y¯ r|Yˆ (X ) .

∂θyt ,yˆ

ξ

i

ξ

ir

(4b). If φty ,yˆ (yi, a) = 0, similar to (4a)., we have ir

∂gX (ξ) = − h¯(l, lX )Pπ (Y = yl|Yˆ (X ))Pπ (Y = yi, Y¯ r = 1|Yˆ (X ))

∂θyt ,yˆ ir

l=i

ξ

ξ

+ h¯(i, lX ) Pπξ (Y = yi, Y¯ l = 1|Yˆ (X )) − Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) .

33

Published as a conference paper at ICLR 2022

Since
h¯(i, lX ) Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) − Pπξ (Y = yi|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X )) =h¯(i, lX )Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))Pπξ (Y = yi|Yˆ (X )) ≥0,

we have

∂gX (ξ) ∂θyt ,yˆ
il

≤ max h¯(i, lX )Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))Pπξ (Y = yi|Yˆ (X )) ,



h¯(l, lX )Pπξ (Y = yl|Yˆ (X ))Pπξ (Y = yi, Y¯ r = 1|Yˆ (X ))

l=i



≤2HVar 1Y =yi |Yˆ (X ) .

Combining (4a). and (4b)., we have that

∂gX (ξ) ≤ 2H Varπ 1Y =y |Yˆ (X ) + Varπ Y¯ l|Yˆ (X ) .

∂θyt ,yˆ

ξ

i

ξ

il

Combining (1-4)., we then have

∇gX (ξ) 2

kn

2

≤4H 2

(|N (yi, Yj)| − 1) Varπ 1Y =y |Yˆ (X )

(27)

ξ

i

i=1 j=1

+ 4H2

Varπ Y¯ r|Yˆ (X ) 2 ξ

j∈[n],yˆr ∈Yj

+ 4H2

Varπ Y¯ i|Yˆ (X ) + Varπ Y¯ j|Yˆ (X ) 2

ξ

ξ

i,j∈[kˆ]

+ 4H2

Varπ 1Y =y |Yˆ (X ) + Varπ Y¯ l|Yˆ (X ) 2

ξ

i

ξ

i∈[k],j∈[kˆ]


k

kˆ



≤8H2  (ni + kˆ)Varπξ (1Y =yi |Yˆ = Yˆ ∗)2 + (mi + K − 1)Varπξ (Y¯ i|Yˆ = Yˆ ∗)2 .

i=1

i=1

(28)

Therefore, by Eqs. (25), (26), and (28), and Assumption Eq. (21), we have





| (W ) − Θˆ (W )| = Eπ∗ 

h¯(l, lX ) Pπ ∗ (Y = yl|Yˆ (X )) − Pπˆ (Y = yl|Yˆ (X )) 

Θ

Θ

l=lX

= Eπ∗ gX (Θ∗) − gX (Θˆ )

≤ Eπ∗ ≤ √2cH
M

∇gX (ξ) Θ∗ − Θˆ .

Θ∗ − Θˆ

Now, we apply the assumption that we are able to solve the empirical problem, producing an estimate
Wˆ that satisﬁes E Θˆ (Wˆ ) − Θˆ (WΘ∗ ) ≤ χ,

34

Published as a conference paper at ICLR 2022

where WΘˆ∗ is the true solution to Therefore,

WΘˆ∗ = arg min Θ(W ).
W

E (Wˆ ) − (W ∗) = E Θˆ (Wˆ ) − Θˆ (WΘˆ∗ ) + Θˆ (WΘˆ∗ ) − Θˆ (Wˆ ) + (Wˆ ) − (W ∗)

(∗)
≤ χ+E

Θˆ (W

∗ ˆ

)

−

Θˆ (Wˆ ) +

(Wˆ ) −

(W ∗)

Θ

≤ χ + 4cH √1 E Θˆ − Θ∗ + E M
≤ χ + 4cH √1 E Θˆ − Θ∗ , M

Θˆ (WΘˆ∗ ) − Θˆ (Wˆ ) +

Θˆ (Wˆ ) −

Θˆ (W ∗)

where Eq. (∗) comes from condition (23).

With Eqs. (20) and (21), we have Eq. (24) by Lemma D.5, i.e.,

E Θˆ − Θ∗ 2 ≤ E Θˆ − Θ∗ 2 ≤ ε2M.

We can now bound this using the result of Lemma D.6, which results in E (Wˆ ) − (W ∗) ≤ χ + 4cH .
The proof is completed.

E EXAMPLES AND ILLUSTRATIONS
E.1 LABEL GRAPH AND LABEL HIERARCHY
Fig 5 shows the mapping between a label hierarchy and the corresponding label graph. Indeed, given the order of labels, any label structure represented as a (directed acyclic graph) DAG can be converted to exact one consistent label graph based on the four types of label relations.

Canidae

Domestic Animal

Dog

Cat

Husky

Cat
Domestic Animal

Exclusive Overlap Subsuming Subsumed

Canidae

Dog Husky

Figure 5: The illustration of mapping between a DAG of labels and a label graph.

E.2 AN EXAMPLE OF INCONSISTENT LABEL GRAPH
Fig. 6 shows an example of an inconsistent label graph. We can see that the label graph is unrealistic and ambiguous because “Husky” subsumes “Canidae”, but (1) “Canidae” subsumes “Dog” and (2) “Dog” subsuems “Husky” combined imply that “Husky” should be subsumed by “Canidae”. Also, from the example, we can see that label graph induced from cyclic label hierarchy must be inconsistent.
35

Published as a conference paper at ICLR 2022

Canidae

Canidae

Exclusive Dog Husky Overlap Dog Husky
Subsuming Subsumed
Figure 6: The illustration of inconsistent label graph.

E.3 ENUMERATION OF INCONSISTENT TRIANGLE LABEL GRAPH
For a triangle label graph G, we list all inconsistent label relation structures. The consistency of larger label graph with more labels can be veriﬁed by checking the consistency of every triangle inside. One example proof of {Exclusive, Overlap, Subsuming} can be found in Lemma 1.

Table 5: Enumeration of Inconsistent Label Relation Triplets.

label relation Triplets

tab

tbc

tac

Overlap Overlap Overlap Overlap Exclusive Exclusive Exclusive Exclusive Exclusive Subsuming Subsuming Subsuming Subsuming Subsuming Subsuming Subsuming Subsumed Subsumed Subsumed Subsumed Subsumed Subsumed Subsumed

Subsumed Subsumed Subsuming Exclusive Subsumed Overlap Subsuming Subsuming Subsuming Exclusive Subsumed Overlap Overlap Subsuming Subsuming Subsuming Overlap Subsumed Subsumed Subsumed Exclusive Exclusive Exclusive

Subsuming Exclusive Subsumed Subsumed Subsuming Subsuming Subsuming Subsumed Overlap Subsumed Exclusive Subsumed Exclusive Exclusive Subsumed Overlap Subsuming Exclusive Subsuming Overlap Subsuming Subsumed Overlap

E.4 AN EXAMPLE OF INDISTINGUISHABLE LABEL GRAPH
Fig. 7 shows an example label graph with indistinguishable label relation structure. Again, red labels represent desired unseen labels, while gray labels are undesired and seen. We can see that unseen label “Husky” and “Bulldog” have indistinguishable label relation structures because for all seen labels, their label relations are equal. For example, seen label “Dog” subsumes both “Husky” and “Bulldog”. In contrast, for “Husky” and “Bengal Cat”, seen label “Cat” subsumes the latter but exclusive to the former, which indicates that “Husky” and “Bengal Cat” have distinguishable label relation structure. Note that “Bengal Cat” and “Persian Cat” also have indistinguishable label relation structure, but the former is unseen desired label while the latter is seen and can be predicted by some ILF(s). We are only interested in the distinguishablity of a pair of unseen labels.
36

Published as a conference paper at ICLR 2022

In practice, users could "break the symmetry" by adding new ILFs with new labels. For example, if we add an ILF that could predict “Arctic Animals”, then the new seen label “Arctic Animals” will be added into label graph as shown in Fig. 8. We know that “Arctic Animals” subsumes “Husky” but not “Bulldog”, so we break the indistinguishable label relation structure of “Husky” and “Bulldog” successfully.

Domestic Animal

Dog

Cat

Husky

Bulldog

Bengal Cat

Persian Cat

Husky

Exclusive Overlap Subsuming Subsumed

Bulldog
Bengal Cat

…

…

…

Dog
Domestic Animal
Cat
Persian Cat

Figure 7: An example of an indistinguishable label relation structure (“Husky” and “Bulldog”).

Dog

…

Domestic Animal

Husky

Domestic Animal

…

Arctic

Dog

Cat

Animals

Husky

Bulldog

Bengal Cat

Persian Cat

Exclusive Overlap Subsuming Subsumed

Bulldog
Bengal Cat

…

Cat
Persian Cat

…

Arctic Animals
Figure 8: An example of ﬁxing an indistinguishable label relation structure (“Husky” and “Bulldog”) by adding a new label (“Arctic Animals”).

F EXPERIMENTAL DETAILS
F.1 DATASET
Large scale Text Classiﬁcation Dataset3: LSHTC-3 (Partalas et al., 2015), a large scale hierarchical text classiﬁcation dataset, which consists of 456,886 documents and 36,504 categories organized in a label hierarchy. We ﬁlter out the documents with multiple labels, and preserve categories with more than 500 documents. We use a pre-trained sentence transformer (Reimers & Gurevych, 2019) to obtain document embeddings for classiﬁcation. We follow Zhang et al. (2021) to generate 5 keyword-based labeling functions for each seen label as ILFs. Large scale Image Classiﬁcation Dataset4: ILSVRC2012 (Russakovsky et al., 2015), a large scale image classiﬁcation dataset, which consists of 1.2M training images from 1000 object classes based
3http://lshtc.iit.demokritos.gr/ 4http://image-net.org/challenges/LSVRC/2012/index#data
37

Published as a conference paper at ICLR 2022

on ImageNet. Following Deng et al. (2014) we use WordNet as the label hierarchy, and because all the images are assigned to leave labels in WordNet, for each non-leave label, we aggregate images belonging to its descendants as its data points (Deng et al., 2014). For weak supervision sources creation, we follow Mazzetto et al. (2021b;a) to train 10 image classiﬁers as ILFs. We randomly sampling 2 or 3 exclusive seen labels from the label graph as well as 500 images for each label to train a ResNet-32 classiﬁer.

F.2 DESCRIPTION OF APPLYING DAP
To apply DAP, we use both label relations and ILFs to construct attributes for both unseen classes and unlabeled data points. Then, we train the attribute classiﬁers, which in turn are used to predict unseen labels on the test set as in Lampert et al. (2013). To construct attributes for unseen labels and data points, we leverage the outputs of ILFs and label relations.
First, based on the label relations and basic logistic rules, we enumerate all the possible assignments of seen labels given a data point. For example, if label A is subsumed by label B, then for a data point, when it belongs to label A, it must also belong to B; And if label A and B are exclusive, then one data cannot belong to both at the same time. Let s ∈ S denote one possible label assignment and S is the set of all possible s. Then we deﬁne the attribute as a vector of |S| dimension where each dimension corresponds to one s.
Second, we deﬁne the attribute of unseen labels. For an unseen label A and a label assignment s, if A is not exclusive to any label in s then we set the corresponding attribute as = 1 for label A, other wise 0. The intuition is that, if A is not exclusive to labels in s, it’s likely that when a data belongs to assignment s, it also belongs to label A. For each data point, we use the labels assigned by ILFs to build their attributes. If a data belongs to assignment s then its corresponding attribute as = 1, otherwise 0.
Then, we can train attribute classiﬁer p(a|x) for each attribute based on data point attributes. During inference, we use unseen label attribute as well as attribute classiﬁer as in Lampert et al. (2013):

|S| p(acm|x)

f (x) = arg max
c

p(ac |x)

(29)

m=1 m

F.3 HYPER-PARAMETERS

For

the

training

of

PGMs,

we

set

the

learning

rate

to

be

1 n

where

n

is

the

number

of

training

data.

For

training logistic regression model, we use the default parameters in scikit-learn library. For training

ResNet model, we set batch size as 256 and use Adam optimizer with learning rate being 1e-3 and

weight decay being 5e-5.

F.4 HARDWARE AND IMPLEMENTATION DETAILS
All experiments ran on a machine with an Intel(R) Xeon(R) CPU E5-2678 v3 with a 512G memory and a GeForce GTX 1080Ti-11GB GPU.
All the code was implemented in Python. We use the standard implementation of the logistic regression model from Python scikit-learn library5 and the ResNet model from torchvision library6.
Our code will be released upon the acceptance.

F.5 DATASET DETAILS OF REAL-WORLD APPLICATIONS
We list the tags we used in the real-world application (Sec. 7.3) and examples of label relations we query from the existing product category taxonomy.
5https://scikit-learn.org/stable/modules/generated/sklearn.linear_model. LogisticRegression.html
6https://pytorch.org/docs/stable/torchvision/models.html

38

Published as a conference paper at ICLR 2022

Table 6: The tags and examples of label relations of “Car Accessories” category.

new unseen tags: existing tags:
label relation examples:

“Performance Modifying Parts”, “Vehicle Tires & Tire Parts”, “Car Engines & Engine Parts”
“Car Modiﬁcation Parts”, “Car Parts & Accessories” “Car & Truck Tires”, “Replacement Car Parts”, “Car & Truck Wheels”
“Replacement Car Parts” subsumes “Car Engines & Engine Parts” “Car & Truck Tires” is subsumed by “Vehicle Tires & Tire Parts”

Table 7: The tags and examples of label relations of “Furniture Accessories” category.

new unseen tags: existing tags:
label relation examples:

“Clothing & Shoe Storage”, “Living Room Furniture”, “Beds & Headboards”
“Coffee Tables & End Tables”, “Entertainment & Media Centers” “Bedroom Furniture”, “Sofas & Chairs”, “Mattresses”
“Bedroom Furniture” subsumes “Beds & Headboards” “Sofas & Chairs” is subsumed by “Living Room Furniture”

G ADDITIONAL EXPERIMENTS

G.1 PERFORMANCE DROP WHEN THE DISTINGUISHABLE CONDITION IS VIOLATED
To validate the effectiveness of the distinguishable condition, we drive another 100 WIS tasks from LSHTC-3 dataset where each task has at least one pair of unseen labels sharing exactly the same label relation structure. In Table 8, we report the performance drop on the averaged evaluation results over the 100 WIS tasks with comparison to the numbers in Table 2. Although the two sets of WIS tasks are different and therefore are not individually comparable, the averaged performance drop does indicates that the violation of the distinguishable condition results in undesirable synthesized training labels, which implicitly demonstrates the effectiveness of the distinguishable condition.

Table 8: Performance drop on averaged evaluation results over 100 WIS tasks derived from LSHTC-3 when the distinguishable condition is violated.

Method

Label Model

LR-MV W-LR-MV
WS-LG

PLRM

End Model

LR-MV W-LR-MV
WS-LG

PLRM

Accuracy
-11.49 -11.51 -9.28
-9.66
-16.14 -15.27 -13.13
-13.39

F1-score
-13.83 -13.47 -8.63
-9.63
-17.08 -15.97 -13.78
-14.09

39

