Transformer Memory as a DiÔ¨Äerentiable Search Index

arXiv:2202.06991v2 [cs.CL] 16 Feb 2022

Yi Tay Vinh Q. Tran Mostafa Dehghani Jianmo Ni Dara Bahri Harsh Mehta Zhen Qin Kai Hui Zhe Zhao Jai Gupta Tal Schuster
William W. Cohen Donald Metzler
Google Research

Abstract
In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the DiÔ¨Äerentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identiÔ¨Åers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI signiÔ¨Åcantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.
1 Introduction
Information retrieval (IR) systems map a user query q ‚àà Q to a ranked list of relevant documents {d1, . . . , dn} ‚äÜ D, typically represented by integers or short strings called document identiÔ¨Åers (docids). The most widely used IR approaches are based on pipelined retrieve-then-rank strategies. For retrieval, approaches based on inverted indexes or nearest neighbor search are common where contrastive learning based dual encoders (DEs) (Gillick et al., 2018; Karpukhin et al., 2020; Ni et al., 2021) are the present state-of-the-art. This paper proposes an alternative architecture, wherein a sequence-to-sequence (seq2seq) learning system (Sutskever et al., 2014) is used to directly map a query q to a relevant docid j ‚àà Y. This proposal is shown in the bottom half of Figure 1, for a sequence-to-sequence encoder-decoder architecture.

We call this proposed architecture a diÔ¨Äerentiable search index (DSI), and implement it with a large pre-trained Transformer (Vaswani et al., 2017) model, building on the recent success of large generative language models (LMs) (Brown et al., 2020; RaÔ¨Äel et al., 2019; Devlin et al., 2018; Thoppilan et al., 2022; Du et al., 2021). In this proposed architecture, all information of the corpus is encoded within the parameters of the Transformer language model.
At inference time, the trained model take as input a text query q and outputs a docid j. If desired, beam search can be used to produce a ranked list of potentially-relevant docids. As we show, this process can work surprisingly well when trained properly. In our experiments it can consistently outperform DE baselines, sometimes drastically: for a base-sized T5 model, Hits@1 on the smallest corpus is improved by more than 20 points, from 12.4% for a DE to 33.9% for DSI; and on a corpus 30√ó larger, performance is improved by nearly 7 points. These gains increase when larger models are used: for an 11B-parameter T5 model, Hits@1 performance improves by more than 25 points over DE on the small corpus, and more than 15 points on the large corpus. DSI also performs extremely well in a zero-shot setting, e.g., improving Hits@1 by 14 points over BM25.
In addition to these quantitative gains, the DSI architecture is much simpler than a DE (see Table 1). A DE system Ô¨Åxes a search procedure (MIPS) and learns internal representations that optimize performance for that search procedure; in contrast, a DSI system contains no special-purpose Ô¨Åxed search procedure, instead using standard model inference to map from encodings to docids.
Of particular interest to the machine learning community, as Table 1 shows, in DSI all aspects of retrieval are mapped into well-understood ML tasks. This may lead to new potential approaches to solving long-standing IR problems. As one example, since indexing is now a special case of model training, in-

1

query123

Learning to encode

Who is the author of tipping the velvet?

doc456

‚Ä¶ As part of her research ‚Ä¶ Sarah Waters came across the title of her first book, Tipping the Velvet. ‚Ä¶

doc137
‚Ä¶. Keeley Hawes is known for her roleas Kitty Butler in Tipping the Velvet, ‚Ä¶

query123 Who is the author of tipping the velvet?
doc456
‚Ä¶ As part of her research ‚Ä¶ Sarah Waters came across the title of her first book, Tipping the Velvet. ‚Ä¶
doc137
‚Ä¶. Keeley Hawes is known for her roleas Kitty Butler in Tipping the Velvet, ‚Ä¶

Encoder

ùëâ$'%

ùëâ!"#

Contrastive loss during training

ùëâ$%&

query123

Retrieve

Encoder

ùëâ$'%

Maximal Inner Product Search
ùëâ!"# ùëâ'(%

Rank List
1. doc456 2. doc283

‚Ä¶

‚Ä¶

Learning to encode and retrieve ùëâ$'%

Encoder

ùëâ!"#

Decoder

ùëâ$%&

doc456 doc456 doc137

Beam Search

‚Ä¶

Rank List
1. doc456 2. doc283

Figure 1: Top, an overview of dual encoders (DE), that has two independent steps, encoding and retrieval. In

DEs, the queries qi and documents dj are mapped to vectors vi and vj in a common space. In the retrieval

stage, the documents relevant to a given query can be found by performing a maximal inner product search

(MIPS)

to

Ô¨Ånd

the

k

documents

dj

with

largest

inner

product

with

the

query

v

T i

v

j

.

Bottom,

the

same

task

solved with a diÔ¨Äerentiable search index (DSI), which uniÔ¨Åes the encoding and retrieval. In DSI, the docids

(represented as strings) are generated directly from the query using a seq-to-seq model, and a rank list of k

document is created incorporating the beam search mechanism during decoding.

crementally updating an index becomes a special case of model updating (Sun et al., 2020).
In this paper, DSI is applied to moderate-sized corpora (from 10k to 320k documents), all of which are derived from one challenging retrieval task, and we leave the important question of the scaling DSI to larger corpora to future work. The task considered is retrieving supporting passages given questions from the Natural Questions (NQ) dataset, a challenging task for lexical models.
While the idea of DSI is simple, there are a number of ways it can be realized, some of which work surprisingly well, and some of which work surprisingly poorly. Below we explore a number of variations of the DSI architecture.
Document representation. We explore several approaches to representing documents, including a ‚Äúnaive‚Äù approach of using the document‚Äôs full text, as well as variants of the bag-of-words representation used by traditional IR engines.
Docid representation. We look at several ways to represent docids. In addition to naively representing integers as text strings, we also consider unstructured atomic docids, where each document is assigned a unique token, and some simple baselines for constructing structured semantic docids that describe how to navigate to a document through a hierarchical clustering of the corpus. Structured docids‚Äîeither semantically structured via clustering, or naively structured

as tokenized integers‚Äîscale better to large corpora, since the size of the vocabulary used in the decoder is made larger.
Indexing. A trainable IR system traditionally has two phases: indexing a corpus (i.e., memorizing information about each document), and learning how to eÔ¨Äectively retrieve from the index. In DSI, the index is stored in the model parameters, and indexing is simply another kind of model training. Figure 1 suggests one approach to indexing a corpus: namely, to train on (1) examples (dj, j) that pair document dj with its docid j, in addition to (2) examples (q, j) that pair a query q with a relevant docid j. In this setup the examples of type (1) are ‚Äúindexing‚Äù examples.
While it is clear that examples of type (2) alone do not provide enough information for a system to generalize to novel retrievals, there are many alternatives to examples of type (1) that might plausibly ‚Äúteach‚Äù a model about the associations between documents and docids. We explore a number of these below, and show that some plausible-seeming techniques perform very poorly. We also explore a number of alternative multi-task optimization and curriculum learning schemes for combining these types of examples.
EÔ¨Äects of model and corpus size. Since recent results suggest that some properties of large LMs emerge only for very large model sizes Brown et al. (2020), we explore the performance of DSI for a range of

2

Table 1: Information retrieval requires a series of decisions, associated with the subproblems of document representation, indexing, and retrieval. Structured-document variants of DSI are also sensitive to a fourth decision, namely how docids are represented.

doc/query rep. docid rep. indexing
retrieval (top-1)

BM25 or TFIDF sparse vdj vector in R|V |
‚àí
build inverted index mapping
each term t ‚Üí {dj1 , . . . , djk } approximate sparse matmul
to Ô¨Ånd argmaxj vTq vdj

Dual Encoder (DE) dense vdj vector in Rd
‚àí
build table mapping
each docvec vdj ‚Üí j approximate MIPS to Ô¨Ånd argmaxj vTq vdj

DiÔ¨Äerentiable Search Index (DSI) Various (see Section 3.1.2) Various (see Section 3.2)
train model (see Section 3.1.1) to map dj ‚Üí j
run trained model to Ô¨Ånd argmaxj Pr(j|q)

model sizes and corpus sizes of 10k, 100k, and 320k documents.
Summary. We show that even naive representations for documents and docids, coupled with appropriate training procedures to Ô¨Åne-tune modern large LMs, can perform surprisingly well; we present two improved docid representations, unstructured docids and semantically-structured docids, which improve the naive representation choice. We show that there is substantial variation in performance among indexing/training strategies and we show that performance of DSI signiÔ¨Åcantly and consistently improves with model scale. To our knowledge this is the Ô¨Årst case of generative indexing improving performance over strong baselines for a well-studied document retrieval task.
2 Related Work
De Cao et al. (2020) describe a related sequence-tosequence system called autoregressive entity linking, in which documents mentioning an entity‚Äîperhaps implicitly, e.g., by posing a question to which that entity is an answer‚Äîare mapped to a canonical name of that entity. In the case of Wikipedia, canonical entity names correspond to page titles, so this could be viewed as a sort of document retrieval. This approach has been adapted to other purposes, such as generating knowledge base triples in canonical form (Josifoski et al., 2021). The task we consider is different from those considered in autoregressive entity linking: our goal is to retrieve a document containing the answer, rather than a document whose title is the answer. More importantly, in autoregressive entity linking the generation target is a semantically meaningful name, whereas we allow targets to be arbitrary docids. This makes our approach applicable to general retrieval tasks, but raises new questions about docid representation and indexing strategies.
In autoregressive entity linking, generation is con-

strained to return an output from a Ô¨Åxed set. It would be feasible to constrain DSI generation outputs to be valid docids. Although we do not use this technique, the degree to which this might improve performance is a worthwhile question.
There is a large body of work on retrieval augmented generation, i.e., retrieving auxiliary documents to enhance language models (Borgeaud et al., 2021; Guu et al., 2020). These techniques are useful for many tasks including question-answering, but rely on traditional retrieval methods such as DEs. Here we use generation to replace a retrieval process, rather than using retrieval to augment a generation process.
Alternatively, text generation can be used to improve retrieval by improving the document or query representations used by a standard IR system. For instance, in Nogueira et al. (2019) before being indexed, a document is passed to a text-to-text model which generates additional text that could be useful in retrieval (in particular, questions likely to be asked about that document are generated). Alternatively, generation could be used to augment a query (Mao et al., 2020). These approaches are used as preprocessing steps for traditional retrieval systems, not as an alternative retrieval method, like DSI.
Dual encoders (Dehghani et al., 2017; Gillick et al., 2018; Ni et al., 2021; Karpukhin et al., 2020) are a well-established paradigm for retrieval. The key idea is produce query and document embeddings independently and perform a similarity retrieval in vector space across all embedding pairs. Query and candidate documents are produced by a sequence encoder and training is performed using a form of contrastive loss.
Notably, there been also prior work on learned indexes (Kraska et al., 2018; Abu-Libdeh et al., 2020) that train neural networks to learn B-Tree Indexes. Neural networks in these line of work typically operate on standard data structures, as opposed to learning an index specialized for search. To this end, our work is indeed in similar spirit of parameterizing a non-

3

discrete operation or process, but solves an entirely diÔ¨Äerent problem with a diÔ¨Äerent modeling paradigm (e.g., pretrained Transformers).
The interpretation of a large Transformer model as a memory store have been investigated in prior work. (Roberts et al., 2020) demonstrated success on a closed-book QA task whereby they train T5 models to retrieve facts that are encoded within the parameters of the model during pretraining. However, diÔ¨Äerent from CBQA, the presented problem here in this paper is to retrieve full documents based on docids instead of generating direct answers. Meanwhile, (Petroni et al., 2019) also investigated language models as knowledge bases and found that pretrained LMs may already contain relational knowledge. (Geva et al., 2020) analyzes the knowledge encoded within Transformer feedforward layers. There have been also works that demonstrate the relation of Transformers to associative memory and hopÔ¨Åeld networks (Ramsauer et al., 2020), which reinforce the notion that Transformers should intuitively serve as a good associative memory store or search index.
There have been also a myriad of interesting work that explores the intersection of large language models and search. (Metzler et al., 2021) outlines a highlevel vision of what a next generation search system would look like. WebGPT (Nakano et al., 2021) enabled GPT models (Brown et al., 2020) to search and navigate the web using imitation learning. LaMDA (Thoppilan et al., 2022) Ô¨Ånetunes the language model to ‚Äòcall‚Äô an external information retrieval system. While this retrieval system is presently decoupled from the main language model, one could envision the retriever to also be jointly learned within the same model as an improvement to facilitate end-to-end training - a capability that is enabled by proposals presented in this paper.
Finally, this work is also closely related to the theme of model uniÔ¨Åcation where uniÔ¨Åed multi-task models have been recently popular due to immense potential (RaÔ¨Äel et al., 2019; Khashabi et al., 2020; Aribandi et al., 2021). Hence, the proposed DSI presents an opportunity to integrate discrete and disjoint search operations into end-to-end uniÔ¨Åed models - a unique capability that was not possible before. We are excited by the potential and possibilities that lies ahead.
3 DiÔ¨Äerentiable Search Index
The core idea behind the proposed DiÔ¨Äerentiable Search Index (DSI) is to fully parameterize traditionally multi-stage retrieve-then-rank pipelines within a single neural model. To do so, DSI models must

support two basic modes of operation:
‚Ä¢ Indexing: a DSI model should learn to associate the content of each document dj with its corresponding docid j. This paper utilizes a straightforward sequence-to-sequence (seq2seq) approach that takes document tokens as input and generates identiÔ¨Åers as output.
‚Ä¢ Retrieval: Given an input query, a DSI model should return a ranked list of candidate docids. Here, this is achieved with autoregressive generation.
Following these two operations, a DSI model can be trained to index a corpus of documents and optionally Ô¨Åne-tune on an available set of labeled data (queries and labeled documents), and thereafter used to retrieve relevant documents‚Äîall within a single, uniÔ¨Åed model. As opposed to retrieve-then-rank approaches, this type of model allows for simple end-to-end training and can easily be used as a diÔ¨Äerentiable subcomponent of a larger, more complex neural model.
3.1 Indexing Strategies
We investigate various indexing strategies that are meant to learn associations between documents and their identiÔ¨Åers. We train our model to predict docids given a sequence of document tokens. This allows our model to learn which identiÔ¨Åer belongs to which document and can be thought of as a diÔ¨Äerentiable take on traditional search indexes. We consider various alternatives and ablate these settings in subsequent sections. The Ô¨Ånal strategy employed was Inputs2Targets with direct indexing.
3.1.1 Indexing Method
This section discusses the indexing task variants that we consider.
Inputs2Target We frame this as a seq2seq task of doc tokens ‚Üí docid. As its name suggests, this binds the docids to the document tokens in a straightforward inputs-to-targets fashion. The advantage here is that the identiÔ¨Åer is the denoising target, which puts it in closer proximity to the loss function. Since the retrieval task is also concerned with predicting identiÔ¨Åers, this formulation allows the network to follow a similar input-target balance in terms of sequence length. A potential weakness is that the document tokens are not denoising targets and therefore there is no opportunity for general pre-training on document tokens.

4

Targets2Inputs This formulation considers the opposite of the above, i.e., generating document tokens from identiÔ¨Åers, i.e., docid ‚Üí doc tokens. Intuitively, this is equivalent to training an autoregressive language model that is conditioned on the docid.
Bidirectional This formulation trains both Inputs2Targets and Targets2Inputs within the same co-training setup. A preÔ¨Åx token is prepended to allow the model to know which direction the task is being performed in.
Span Corruption We also explored a setup that performs span corruption-based denoising (RaÔ¨Äel et al., 2019) with the inclusion of docid tokens. In this approach, we concatenate the identiÔ¨Åer to the document tokens as a preÔ¨Åx that can be randomly masked as spans in the span corruption objective. This method has the advantage of (1) also performing general pre-training during indexing and (2) achieving a good balance of docids as denoising targets and inputs.
3.1.2 Document Representation Strategies
In the previous section, we explored ‚Äúhow to index‚Äù. This section investigates ‚Äúwhat to index?‚Äù, i.e., how to best represent doc tokens. We state our options here and carefully ablate them in our experiments later. The best option in the end was the direct indexing method.
Direct Indexing This strategy represents a document exactly. We take the Ô¨Årst L tokens of a document, with sequential order preserved, and associate them with the docid.

3.2 Representing Docids for Retrieval
Retrieval within seq2seq-based DSI models is accomplished by decoding docids given an input query. How to do this decoding in an eÔ¨Äective way largely depends on how docids are represented in the model. The remainder of this section explores a number of possible ways for representing docids and how to handle decoding for each.
3.2.1 Unstructured Atomic IdentiÔ¨Åers
The most naive way to represent documents is assign each an arbitrary (and possibly random) unique integer identiÔ¨Åer. We refer to these as unstructured atomic identiÔ¨Åers. With these identiÔ¨Åers, an obvious decoding formulation is to learn a probability distribution over the identiÔ¨Åers. In this case, models are trained to emit one logit for each unique docid (|Ndocuments|). This is analogous to the output layer in standard language models, but extended to include docids. To accommodate this, we extend the output vocabulary of a standard language model as follows:
O = Softmax([Wtokens; Wdocs]T hlast)
where [; ] is the row-wise concatenation operator, Wtokens ‚àà Rdmodel√ó|Ntokens| and Wdocs ‚àà R . dmodel√ó|Ndocuments| hlast is the last layer‚Äôs hidden state (‚àà Rdmodel ) of the decoder stack. To retrieve the top-k documents for a given query, we simply sort the output logits and return the corresponding indices. This is also reminiscent of standard listwise learning to rank where all documents are considered at once.

Set Indexing Documents may contain repeated terms and/or non-informative words (e.g., stopwords). This strategy de-duplicates repeated terms using the default Python set operation and removes stopwords from the document. The rest of the document after Ô¨Åltering is passed into the model in similar fashion to the direct index.
Inverted Index This strategy maps chunked documents (contiguous blocks of tokens) instead of entire documents directly to the docid. We randomly subsample a single contiguous chunk of k tokens and associate them with the docid. The key advantage of this approach is to allow looking beyond the Ô¨Årst k tokens.

3.2.2 Naively Structured String IdentiÔ¨Åers
We also consider an ostensibly absurd approach that treats unstructured identiÔ¨Åers, i.e., arbitrary unique integers, as tokenizable strings. We refer to these as naively structured identiÔ¨Åers.
In this formulation, retrieval is accomplished by decoding a docid string sequentially one token at a time. This eliminates the need for the large softmax output space that comes with unstructured atomic identiÔ¨Åers. It also eliminates the need to learn embeddings for each individual docid.
When decoding, beam search is used to obtain the predicted best docid. With this strategy, it is less straightforward to obtain a top-k ranking. One could exhaustively comb through the entire docid space

5

and obtain the likelihood of each docid given the query. Instead, we use the partial beam search tree to construct top-k retrieval scores. We Ô¨Ånd this approximation to be quite eÔ¨Écient and eÔ¨Äective in practice.

3.2.3 Semantically Structured IdentiÔ¨Åers
All of the approaches for representing docids thus far assumed that the identiÔ¨Åers are assigned in an arbitrary manner. While exploring the limits of arbitrary identiÔ¨Åers is quite interesting, it is only intuitive that imbuing the docid space with semantic structure can lead to better indexing and retrieval capabilities. As such, this section explores semantically structured identiÔ¨Åers. SpeciÔ¨Åcally, we aim to automatically create identiÔ¨Åers that satisfy the following properties: (1) the docid should capture some information about the semantics of its associated document, (2) the docid should be structured in a way that the search space is eÔ¨Äectively reduced after each decoding step. This results in identiÔ¨Åers where semantically similar documents share identiÔ¨Åer preÔ¨Åxes. In this work, we treat this as a fully unsupervised preprocessing step. However, as part of future work it may be possible to integrate and automatically learn semantic identiÔ¨Åers in a fully end-to-end manner. To construct identiÔ¨Åers with this property, we employ a simple hierarchical clustering process over document embeddings to induce a decimal tree (or more generally, a trie). Given a corpus to be indexed, all documents are clustered into 10 clusters. Each document is assigned an identiÔ¨Åer with the number of their cluster from 0-9. For every cluster containing more than c documents, the algorithm is applied recursively, with the next level‚Äôs result (the remaining suÔ¨Éx of the identiÔ¨Åer) appended to the existing identiÔ¨Åer. For clusters with c documents or less, each element is assigned an arbitrary number from 0 to at most c ‚àí 1 and likewise its digits are appended to the existing identiÔ¨Åer. Although this speciÔ¨Åc process induces a decimal tree, it is possible to induce similar types of tries using any number of other reasonable strategies. In practice, we simply apply k-means over embeddings generated by a small 8-layer BERT model, with c = 100. We include pseudo-code for this process in Algorithm 1.
3.3 Training and Optimization
The DSI models that we train are optimized for seq2seq cross entropy loss and are trained with

Figure 2: Visual example of a hierarchical clustering process used to assign semantically structured identiÔ¨Åers. During inference, beam search navigates this trie to decode the correct docid.
Algorithm 1 Generating semantically structured identiÔ¨Åers
Input: Document embeddings X1:N , where Xi ‚àà Rd Output: Corresponding docid strings J1:N function GenerateSemanticIDs(X1:N )
C1:10 ‚Üê Cluster(X1:N , k = 10) J ‚Üê empty list for i = 0 to 9 do
Jcurrent ‚Üê [i] ‚àó |Ci+1| if |Ci+1| > c then
Jrest ‚ÜêGenerateSemanticIDs(Ci+1) else
Jrest ‚Üê [0, . . . , |Ci+1| ‚àí 1] end if Jcluster ‚ÜêelementwiseStrConcat(Jcurrent, Jrest) J ‚Üê J.appendElements(Jcluster) end for J ‚Üê reorderToOriginal(J, X1:N , C1:10) return J end function
teacher forcing.We explored two main strategies for training DSI models. The Ô¨Årst and more straightforward strategy is to Ô¨Årst train a model to perform indexing (memorization), followed by a Ô¨Åne-tuning stage where the trained model is used to map queries to docids (e.g., retrieval). The second strategy is to train them together in a multi-task setup. To this end, we frame co-training tasks in similar fashion to T5-style co-training (e.g., using task prompts to differentiate them). The latter performed signiÔ¨Åcantly better, especially when the proportion of indexing to retrieval task examples is high. Hence, we adopted multi-task learning as the default strategy.
Here, we make the observation that our setup is unique and unlike traditional multi-task learning or transfer learning. In typical multi-task setups, two tasks have shared commonalities that could improve the performance of both tasks if they were learned

6

together. However, in our setup, the retrieval task is completely dependent on the indexing task. In particular, without the indexing task, the identiÔ¨Åers leveraged by the retrieval task would be completely meaningless. Hence, in order to solve task B (retrieval), the model needs to learn task A (indexing) well enough. This problem setup presents unique and largely unexplored research challenges that might be of interest to the ML community.

4 Experiments
In this section, we discuss our experimental setup, datasets used and baselines compared. We also discuss experimental results, Ô¨Åndings and eÔ¨Äect of various strategies discussed in earlier sections of the paper. Since this is fairly new concept, this work aims to put forth a proof-of-concept and seeks to answer research questions instead of making a ‚Äòsotaeesque‚Äô comparison. We leave extensive comparisons on other setups and baselines to future work.

4.1 Dataset
We conduct our experiments on the challenging Natural Questions (NQ) (Kwiatkowski et al., 2019) dataset. NQ consists of 307K query-document pairs, where the queries are natural language questions and the documents are Wikipedia articles. Given a question, the retrieval task is to identify the Wikipedia article that answers it. For evaluating how DSI models perform at diÔ¨Äerent scales, we construct three sets from NQ to form our testbed, namely NQ10K, NQ100K, and NQ320K denoting diÔ¨Äerent numbers of total documents. NQ320K is the full NQ set and uses its predetermined training and validation split for evaluation purposes. For all datasets, we use the same docid space/budget of 320K tokens for all unstructured atomic and naively structured identiÔ¨Åer experiments. Semantically structured identiÔ¨Åers are generated separately for each dataset so as to prevent leakage of semantic information from larger splits into smaller ones. Text is lowercased. Table 2 reports the statistics of these datasets.

Table 2: Statistics of NQ datasets used in our experi-

ments. Dataset

|D| Train Pairs Val Pairs Vdoc out

NQ10K 10K

8K

2K 320K

NQ100K 86K

80K

20K 320K

NQ320K 228K

290K

17K 320K

Metrics We evaluate our models on Hits@N where N={1, 10}. This metric reports the proportion of correct documents ranked in the top N predictions.
4.2 Implementation Details
All DSI models are initialized using standard pretrained T5 (RaÔ¨Äel et al., 2019) model conÔ¨Ågurations. The conÔ¨Ågurations names and corresponding number of model parameters are: Base (0.2B), Large (0.8B), XL (3B) and XXL (11B). For unstructured atomic identiÔ¨Åers runs, we initialize the identiÔ¨Åers randomly as new parameters and only Ô¨Ånetune the weights during the indexing stage. We use the Jax (T5X1) implementation for our experiments. The DSI models are trained for a maximum of 1M steps using a batch size of 128. Our training hardware consists of 128-256 TPUv4 chips for models above 1B parameters and 64-128 TPUv3 or TPUv4 chips otherwise. As an estimate, models above 1B parameters typically take about at least a full day for convergence for NQ320K. We tune the learning rate amongst {0.001, 0.0005} and linear warmup amongst {10K, 100K, 200K, 300K} and/or none. Semantically structured identiÔ¨Åers are generated using an 8-layer BERT (Devlin et al., 2018) model2, and the default k-means clustering in scikit-learn. Based on our early ablation experiments of various DSI setting, the main results presented use direct indexing (L = 32) and the Inputs2Targets indexing strategy. We present results for all the docid representation methods. Following the main results, we present our ablation studies.
4.3 Baselines
For baselines, we use T5-based dual encoders implemented by (Ni et al., 2021). We use the gensim3 package for computing BM25 scores. For the T5-based dual encoders, we train with contrastive learning on the NQ pairs until convergence (‚âà 10K steps) and obtain top-k nearest neighbors with a system similar to ScaNN4 (Guo et al., 2020). For zero-shot retrieval, we also compare with a state-of-the-art unsupervised baseline, Sentence T5 (Ni et al., 2021) which have been specially pre-trained with a similarity learning task.
1https://github.com/google-research/t5x 2https://tfhub.dev/google/collections/bert 3https://radimrehurek.com/gensim 4https://github.com/google-research/ google-research/tree/master/scann

7

Table 3: Experimental results on NQ document retrieval. DSI outperforms BM25 and Dual Encoder baselines. Among all the Docid representation methods, Semantic String Docids perform the best.

Model BM25 T5 T5 T5 T5 DSI DSI DSI DSI DSI DSI DSI DSI DSI DSI DSI DSI

Size Base Large XL XXL Base Large XL XXL Base Large XL XXL Base Large XL XXL

Params 220M 800M 3B 11B 250M 800M 3B 11B 250M 800M 3B 11B 250M 800M 3B 11B

Method Dual Encoder Dual Encoder Dual Encoder Dual Encoder Atomic Docid Atomic Docid Atomic Docid Atomic Docid Naive String Docid Naive String Docid Naive String Docid Naive String Docid Semantic String Docid Semantic String Docid Semantic String Docid Semantic String Docid

NQ10K

Hits@1 Hits@10

12.4

33.5

16.2

48.6

18.8

55.7

20.8

59.6

22.1

61.6

13.0

38.4

31.3

59.4

40.1

76.9

39.4

77.0

28.1

48.0

34.7

60.5

44.7

66.4

46.7

77.9

33.9

57.3

37.5

65.1

41.9

67.1

48.5

72.1

NQ100K

Hits@1 Hits@10

20.9

46.4

18.7

55.2

22.3

60.5

23.3

63.2

24.1

64.5

23.8

58.6

17.1

52.3

19.0

55.3

25.3

67.9

18.7

44.6

21.2

50.7

24.0

55.1

27.5

62.4

19.0

44.9

20.4

50.2

22.4

52.2

26.9

59.5

NQ320K

Hits@1 Hits@10

11.6

34.4

20.5

58.3

22.4

63.3

23.9

65.8

24.3

67.3

20.7

40.9

6.9

27.3

28.1

61.9

24.0

55.1

6.7

21.0

13.3

19.9

16.7

58.1

23.8

55.9

27.4

56.6

35.6

62.6

39.1

66.8

40.4

70.3

Table 4: Experimental results on Zero-Shot NQ document retrieval. DSI outperforms BM25, T5 embeddings and SentenceT5, the state-of-the-art for unsupervised similarity modeling. Among Docid representation method, the Atomic Docid performs the best on zero-shot learning.

Model BM25 T5 SentenceT5 DSI DSI DSI

Size XXL Large XXL XXL XXL

Method Dual Encoder Dual Encoder Atomic Docid Naive String Docid Semantic String Docid

NQ10K

Hits@1 Hits@10

12.4

33.5

0.3

1.3

17.6

50.7

25.7

60.1

43.4

67.4

43.9

68.8

NQ100K

Hits@1 Hits@10

20.9

46.4

1.9

8.0

17.4

50.8

23.0

57.3

17.4

41.5

11.4

26.6

NQ320K

Hits@1 Hits@10

11.5

33.7

1.1

5.9

16.9

51.0

25.1

56.6

9.2

22.6

13.9

31.1

4.4 Experimental Results
Table 3 reports retrieval results for NQ10K, NQ100K, and NQ320K with Ô¨Ånetuning and Table 4 reports zero-shot retrieval results. For zero-shot retrieval, the model is only trained on the indexing task and not the retrieval task, so the model sees no labeled query ‚Üí docid data points.
4.4.1 Supervised Finetuning Results
Our results show that DSI outperforms DE across all dataset sizes. On the small dataset (NQ10K), the performance gap between DSI and DE is large, e.g., the best DSI variant outperforms DE by 2 times. On NQ100K, the gap becomes less prominent with the best DSI model (unstructured atomic identiÔ¨Åers) outperforming DE by +5% Hits@1 and Hits@10. On the large dataset (NQ320K), the best DSI model (struc-

tured semantic identiÔ¨Åers) outperform the best DE model by +66% relative Hits@1 and +4.5% Hits@10.
4.4.2 Zero-Shot Results
Table 4 reports results on zeros-shot retrieval. Recall that zero-shot retrieval is performed by only performing indexing and not the retrieval task. In other words, the model does not see any annotated query or document pairs. Generally, the best result is obtained by DSI with unstructured atomic identiÔ¨Åers on both NQ100K and NQ320K. The best performance on all NQ datasets outperform well-established unsupervised retrieval baselines such as BM25. Moreover, DSI outperforms unsupervised representation learning methods such as SentenceT5 (Ni et al., 2021), which is trained to learn similarity-aware representations via contrastive learning. We also note that raw T5 embeddings perform extremely poorly and do not

8

produce reasonable results on the task of unsupervised retrieval. Given that it is generally diÔ¨Écult for an unsupervised neural method to outperform BM25, we Ô¨Ånd these early results very encouraging.

4.4.3 Document IdentiÔ¨Åers
One key research question in this paper is the crucial choice of how to represent docids. Generally, we Ô¨Ånd that structured semantic identiÔ¨Åers are helpful and improve over unstructured identiÔ¨Åers. When comparing naive versus semantic string identiÔ¨Åers, it seems imperative to use semantic identiÔ¨Åers if possible. This is intuitive, since imbuing the target space with semantic structure can facilitate greater ease of optimization and additional unsupervised representation learning methods as external knowledge. The competitiveness of unstructured atomic identiÔ¨Åers is somewhat mixed and we had some diÔ¨Éculty optimizing such models. We hypothesize that this could possibly be because of the the newly initialized softmax layer and that training such a system from scratch would mitigate these issues. However, we defer this line of investigation to future work. In lieu of the instability and high variance of the unstructured atomic identiÔ¨Åers, the performance is not consistent across the diÔ¨Äerent datasets. Moreover, these docids might also run into intermittent nonconvergence which we trace back to an optimization related quirk. However, we also note that unstructured atomic identiÔ¨Åers perform the best, by a wide margin, on the zero-shot retrieval setup and achieve performance often more than double than that of beam decoding methods.
4.4.4 Indexing Strategies
In this section, we explore the eÔ¨Äect of diÔ¨Äerent indexing methods (Section 3.1.1). We run experiments on NQ100K with the diÔ¨Äerent indexing strategies described earlier. Models are trained using the Naive Docid method. Without indexing, the model achieves 0% Hits@1. This is intuitive, since the Docids are not meaningful without the indexing task. Secondly, the Inputs2Targets and Bidirectional formulation performs the best, with the bidirectional method performing slightly worse (13.5 vs 13.2) compared to the former. Finally, the accuracy with Targets2Inputs and Span Corrpution with Docids yield no meaningful results (0% accuracy). This goes to show that there can be huge variance across indexing strategies whereby some strategies work reasonably well and some completely do not work at all.

Figure 3: Scaling plots for DSI vs. DE across model sizes. Performance refers to the Hits@1 metric.
Figure 4: Performance of diÔ¨Äerent document representations.
4.4.5 Document Representations In this section, we explore the performance of the different document representation strategies described in Section 3.1.2. Figure 4 reports the results on NQ320K. Overall, we Ô¨Ånd that the direct indexing approach works the best. We also Ô¨Ånd that it is diÔ¨Écult to train the inverted index method since the docid is repeatedly exposed to diÔ¨Äerent tokens. We also Ô¨Ånd that shorter document lengths seem to work well where performance seems to substantially dip beyond 64 tokens suggesting that it might be harder to optimize or eÔ¨Éciently memorize when there are a larger number of document tokens. Finally, we also Ô¨Ånd that there was no additional advantage in applying set processing or stopwords preprocessing to the document tokens.
4.4.6 Scaling Laws Another interesting insight is how the scaling law of DSI diÔ¨Äers from Dual Encoders. Understanding the scaling behaviour of Transformers have garnered

9

Figure 5: EÔ¨Äect of multi-task ratio of indexing to retrieval examples.
signiÔ¨Åcant interest in recent years (Kaplan et al., 2020; Tay et al., 2021; Abnar et al., 2021). We Ô¨Ånd that the gain in retrieval performance obtained from increasing model parameterization in DE seems to be relatively small. Conversely, the scaling properties of DSI seems to be more optimistic. Figure 3 plots the scaling behaviour (log scale) of three methods (DE and DSI with naive and semantic IDs). DSI (naive) strongly beneÔ¨Åts from scale going from base to XXL and seems to still have headroom for improvement. Meanwhile, DSI (semantic) starts oÔ¨Ä equally competitive as DE base but performs much better with scale. DE models, unfortunately are more or less plateaued at smaller parameterization.
4.4.7 Interplay Between Indexing and Retrieval
Our early experiments showed that Ô¨Årst learning the indexing task and then learning the retrieval task in a sequential manner results in mediocre performance. There, we focused on exploring good ratios r for co-training the indexing and retrieval tasks together using multi-task learning. Figure 5 shows the eÔ¨Äect of modifying the ratio of indexing to retrieval samples. We Ô¨Ånd the optimization process is signiÔ¨Åcantly inÔ¨Çuenced by the interplay between the indexing and retrieval tasks. Setting r too high or low generally resulted in poor performance. We Ô¨Ånd that a rate of 32 generally performed well.
5 Conclusion
This paper proposed the DiÔ¨Äerentiable Search Index (DSI), a new paradigm for learning an end-to-end

search system in a uniÔ¨Åed manner, paving the way for next generation search (Metzler et al., 2021). We deÔ¨Åne novel indexing and retrieval tasks that encode the relationship between terms and docids completely within the parameters of a Transformer model. The paper proposed a number of diÔ¨Äerent ways to represent documents and docids, and explored diÔ¨Äerent model architectures and model training strategies. Experiments conducted on the Natural Questions data set show that DSI performs favorably against common baselines such as BM25 and dual encoders, both in a standard Ô¨Åne-tuning setup as well as in a zero-shot setup. Although the models and results presented here are promising, there is a great deal of potential future research that can be explored based on this work. For example, it would be interesting to explore alternative strategies for representing documents and docids, including end-to-end strategies for learning semantic identiÔ¨Åers. Another possible research direction is to explore how such models can be updated for dynamic corpora, where documents may be added or removed from the system. We are also interested in exploring and leveraging mixture-of-expert models (Du et al., 2021; Fedus et al., 2021; Lepikhin et al., 2020) for scaling the memory capacity of DSI. Finally it may also be interesting to further investigate DSI as an unsupervised representation learning method and/or memory store for other language models to exploit.
6 Acknowledgements
The authors would like to thank Fernando Pereira, Huaixiu Steven Zheng, Sebastian Ruder, Adam D. Lelkes, Ian Wetherbee and Dani Yogatama for feedback and discussions.
References
Abnar, S., Dehghani, M., Neyshabur, B., and Sedghi, H. Exploring the limits of large scale pre-training. arXiv preprint arXiv:2110.02095, 2021.
Abu-Libdeh, H., Altƒ±nbu¬®ken, D., Beutel, A., Chi, E. H., Doshi, L., Kraska, T., Ly, A., Olston, C., et al. Learned indexes for a google-scale disk-based database. arXiv preprint arXiv:2012.12501, 2020.
Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H. S., Mehta, S. V., Zhuang, H., Tran, V. Q., Bahri, D., Ni, J., et al. Ext5: Towards extreme multitask scaling for transfer learning. arXiv preprint arXiv:2111.10952, 2021.

10

Borgeaud, S., Mensch, A., HoÔ¨Ämann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
De Cao, N., Izacard, G., Riedel, S., and Petroni, F. Autoregressive entity retrieval. arXiv preprint arXiv:2010.00904, 2020.
Dehghani, M., Zamani, H., Severyn, A., Kamps, J., and Croft, W. B. Neural ranking models with weak supervision. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 65‚Äì74, 2017.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam: EÔ¨Écient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021.
Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and eÔ¨Écient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.
Gillick, D., Presta, A., and Tomar, G. S. End-toend retrieval in continuous space. arXiv preprint arXiv:1811.08008, 2018.
Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., and Kumar, S. Accelerating largescale inference with anisotropic vector quantization. In International Conference on Machine Learning, 2020. URL https://arxiv.org/abs/1908. 10396.
Guu, K., Lee, K., Tung, Z., and Pasupat, P. REALM: Retrieval-Augmented Language Model Pre-Training. In Proceedings of ICML 2020, 2020.

Josifoski, M., De Cao, N., Peyrard, M., and West, R. Genie: Generative information extraction. arXiv preprint arXiv:2112.08340, 2021.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
Karpukhin, V., OgÀòuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. UniÔ¨Åedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020.
Kraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pp. 489‚Äì504, 2018.
Kwiatkowski, T., Palomaki, J., RedÔ¨Åeld, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin Kenton Lee, J., Toutanova, K., Jones Matthew Kelcey, L., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural Questions: a Benchmark for Question Answering Research. In Transactions of the ACL, 2019.
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
Mao, Y., He, P., Liu, X., Shen, Y., Gao, J., Han, J., and Chen, W. Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553, 2020.
Metzler, D., Tay, Y., Bahri, D., and Najork, M. Rethinking search: making domain experts out of dilettantes. In ACM SIGIR Forum, volume 55, pp. 1‚Äì27. ACM New York, NY, USA, 2021.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
Ni, J., A¬¥ brego, G. H., Constant, N., Ma, J., Hall, K. B., Cer, D., and Yang, Y. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021.

11

Nogueira, R., Yang, W., Lin, J., and Cho, K. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019.
Petroni, F., Rockt¬®aschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.
RaÔ¨Äel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.
Ramsauer, H., Sch¬®aÔ¨Ç, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., Gruber, L., Holzleitner, M., Pavlovi¬¥c, M., Sandve, G. K., et al. HopÔ¨Åeld networks is all you need. arXiv preprint arXiv:2008.02217, 2020.
Roberts, A., RaÔ¨Äel, C., and Shazeer, N. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.
Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generalization under distribution shifts. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9229‚Äì9248. PMLR, 13‚Äì18 Jul 2020. URL https://proceedings.mlr.press/ v119/sun20b.html.
Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014.
Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., and Metzler, D. Scale eÔ¨Éciently: Insights from pre-training and Ô¨Åne-tuning transformers. arXiv preprint arXiv:2109.10686, 2021.
Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998‚Äì 6008, 2017.
12

