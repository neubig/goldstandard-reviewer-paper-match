A Survey on Multimodal Disinformation Detection
Firoj Alam1∗ , Stefano Cresci2 , Tanmoy Chakraborty3 , Fabrizio Silvestri4 , Dimiter Dimitrov5 , Giovanni Da San Martino6 , Shaden Shaar1 , Hamed Firooz7 , Preslav Nakov1 1Qatar Computing Research Institute, HBKU, Doha, Qatar, 2IIT-CNR, Pisa, Italy, 3IIIT-Delhi, India, 4Sapienza University of Rome, Italy, 5Soﬁa University, 6University of Padova, Italy, 7Facebook AI
{ﬁalam, sshaar, pnakov}@hbku.edu.qa, s.cresci@iit.cnr.it, tanmoy@iiitd.ac.in, fsilvestri@diag.uniroma1.it, mitko.bg.ss@gmail.com, dasan@math.unipd.it, mhﬁrooz@fb.com

arXiv:2103.12541v1 [cs.MM] 13 Mar 2021

Abstract
Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufﬁcient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the deﬁnition of disinformation – (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.
1 Introduction
The proliferation of online social media has encouraged individuals to freely express their opinions and emotions. On one hand, the freedom of speech has led the massive growth of online content which, if systematically mined, can be used for citizen journalism, public awareness, political campaigning, etc. On the other hand, its misuse has given rise to the hostility in online media, resulting false information in the form of fake news, hate speech, propaganda, cyberbullying, etc. It also set the dawn of the Post-Truth Era, where appeal to emotions has become more important than the truth. More recently, with the emergence of the COVID-19 pandemic, a new blending of medical and political false information has given rise to the ﬁrst global infodemic.
∗Contact Author

The term “fake news” was declared Word of the Year 2017 by Collins dictionary. However, this term is very generic, and may mislead people and fact-checking organizations to focus only on veracity. Recently, several international organizations such as the EU have redeﬁned the term to make it more precise: disinformation, which refers to information that is – (i) fake and also (ii) spreads deliberately to deceive and harm others. The latter aspect of the disinformation (i.e., harmfulness) is often ignored, but it is equally important. In fact, the reason behind disinformation emerging as an important issue, is because the news was weaponized.
Disinformation often spreads as text. However, the Internet and social media allow the use of different modalities, which can make a disinformation message attractive as well as impactful, e.g., a meme or a video, is much easier to consume, attracts much more attention, and spreads further than simple text [Zannettou et al., 2018; Hameleers et al., 2020; Li and Xie, 2020].
Yet, multimodality remains under-explored when it comes to the problem of disinformation detection. [Bozarth and Budak, 2020] performed a meta-review of 23 fake news models and the data modality they used, and found that 91.3% leveraged text, 47.8% used network structure, 26.0% used temporal data, and only a handful leveraged images or videos. Moreover, while there has been research on trying to detect whether an image or a video has been manipulated, there has been less research in a truly multimodal setting [Zlatkova et al., 2019].
In this paper, we survey the research on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. We further argue for the need to cover multiple modalities in the same framework, while taking both factuality and harmfulness into account.
While there have been a number of surveys on “fake news” [Cardoso Durier da Silva et al., 2019; Kumar and Shah, 2018], misinformation [Islam et al., 2020], fact-checking [Kotonya and Toni, 2020], truth discovery [Li et al., 2016], and propaganda detection [Da San Martino et al., 2020], none of them had multimodality as their main focus. Moreover, they targeted either factuality (most surveys above), or harmfulness (the latter survey), but not both. Therefore, the current survey is organized as follows. We analyze literature covering various aspects of multimodality: image, speech, video, and network and temporal. For each modality, we review the relevant lit-

Harmful Factual

Image
Speech
Video
Network and Temporal
Multimodal
Figure 1: Our envision of multimodality to interact with harmfulness and factuality in this survey.
erature on the aspect of harmfulness, and factuality. Figure 1 shows how we envision the multimodality aspect of misinformation interacts with harmfulness and factuality. We also highlight major takeaways and limitations of existing studies, and suggest future directions in this domain.
2 Multimodal Factuality Prediction
In this section, we focus on the ﬁrst aspect of disinformation – factuality. Automatic detection of factual claims is important to debunk the spread of misleading information. It is crucial to identify factuality of the statements that can mislead people. A large body of work has been devoted to automatically detecting factuality from textual content to debunk misleading information. Such claims are expressed and disseminated with other modalities (e.g., image, speech, video) and propagated through different social networks. This section summarizes relevant studies that explore all modalities except text-only modality as text is commonly used with other modalities. 2.1 Image Text with visual content (e.g., images) spreads faster, gets 18% more clicks, 89% more likes, and 150% more retweets [Zhang et al., 2018]. Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address visual content with text for predicting misleading information [Volkova et al., 2019], fake images [Gupta et al., 2013], images shared with misinformation in political groups [Garimella and Eckles, 2020], and fauxtographic detection [Zhang et al., 2018;

Wang et al., 2020b]. [Garimella and Eckles, 2020] manually annotated a sample
of 2,500 images collected from public WhatsApp groups, and labeled as misinformation, not misinformation, misinformation already fact-checked, and unclear. They identiﬁed different types of images such as out-of-context old images, doctored images, memes (funny and misleading image), and other types of fake images. Image related features that are exploited in this study include images with faces and dominant colors. The authors also found that violent and graphic images spread faster.
[Nakamura et al., 2020] developed a multimodal dataset containing one million posts including text, image, metadata, and comments collected from Reddit. The dataset was labeled with 2, 3, and 6-ways labels. [Volkova et al., 2019] proposed models for detecting misleading information using images and text. They developed a corpus of 500,000 Twitter posts, consisting of images labeled with six classes: disinformation, propaganda, hoaxes, conspiracies, clickbait, and satire. They then compared how different combinations of models (i.e., textual, visual, and lexical characteristics of the text) performed. Fauxtography has also been commonly used in social media in different forms such as a fake image with false claims, a true image with false claims. [Zlatkova et al., 2019] investigated factuality of claims with respect to images and compared the performance of different feature groups between text and images. Another notable work in this direction is FauxBuster [Zhang et al., 2018], which uses content in social media (e.g., comments) in addition to the content in the images and the texts. [Wang et al., 2020b] analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement, speciﬁcally in Twitter and Reddit. Their ﬁndings reveal that doctored images are often used as memes to mislead or as a means of satire and they have ‘clickbait’ power, which drives engagement.
2.2 Speech
There has been effort to use acoustic signals to predict factuality from political debates [Kopev et al., 2019; Shaar et al., 2020], left-center-right bias prediction in political debates [Dinkov et al., 2019], and on deception detection in speech [Hirschberg et al., 2005]. [Kopev et al., 2019] reported that the acoustic signal consistently help to improve the performance compared to using textual and metadata features only. Similarly, [Dinkov et al., 2019] reported that the use of speech signals improves the performance political bias (i.e., left, center, right) in political debates.
Moreover, a large body of work was done in the direction of deception detection using acoustic signals. [Hirschberg et al., 2005] created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech. Different sets of information was used in their experiments such as acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, ﬁlled pauses, and paralinguistic information (e.g., speaker information, gender, ﬁeld-pause, laughter). Using the same corpus, an evaluation campaign was organized [Kaya and Karpov, 2016], where different multimodal approaches were proposed, such as fusion of acoustic and lexical sources [Kaya and Karpov, 2016], acoustic,

prosodic, lexical, and phonotactics [Levitan et al., 2016].
2.3 Video
In addition to textual, imagery, and speech content, the information in video plays a signiﬁcant role in capturing cues of deceptive behavior. Such cues in videos (e.g., facial expression, gestures) have been investigated in several studies [Pe´rez-Rosas et al., 2015; Krishnamurthy et al., 2018; Soldner et al., 2019] for deception detection. [Pe´rez-Rosas et al., 2015] developed a real-life courtroom trial dataset, which includes 61 deceptive and 60 truthful videos. They explored the use of n-gram features from transcripts and non-verbal features (i.e., facial expressions, eyebrows, eyes, mouth openness, mouth lips, and head movements, hand gestures) to develop models to discriminate between liars and truth-tellers. [Krishnamurthy et al., 2018] used textual, audio, and visual features for deception detection. They used a 3D CNN to extract visual features from each frame, spatiotemporal features, and facial expressions such as smile, fear, or stress. The textual features consisted of word-embeddings, the audio features used the Interspeech 2013 ComParE feature set, and the micro-expressions were frowning, smiling, eyebrows raising, etc. [Soldner et al., 2019] developed a multimodal deception dataset using TV shows and experimented with textual (i.e., n-grams, part-of-speech, psycholinguistic, and word embeddings features), visual (i.e., facial expressions), and dialog features.
Relevant evaluation campaigns include the MediaEval Benchmark [Boididou et al., 2016], where the focus of the tasks was automatic detection of manipulated and misleading social media content.
2.4 Network and Temporal Information
The rationale for leveraging network information stems from early work by [Shao et al., 2018; Vosoughi et al., 2018], which showed that propagation and interaction networks of fake news are deeper and wider than those of real news. [Vosoughi et al., 2018] also found that fake information spread faster than factual one, thus advocating for the use of temporal information.
Propagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network) [Zhou and Zafarani, 2019]. [Shu et al., 2020] tackled the fake news classiﬁcation task by proposing an approach based on hierarchical propagation networks. At both micro- and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features.
[Shu et al., 2019] provided one of the most thorough multimodal frameworks to fake news classiﬁcation. They used news content embeddings and user embeddings, both obtained via non-negative matrix factorization, as well as user-news interactions embeddings and publisher-news relation embeddings. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Furthermore, the best results are obtained when

network and content data are used in conjunction, indicating that the two provide complementary information.
[Vosoughi et al., 2017] proposed Rumor Gauge, a system that jointly exploits temporal and propagation in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text (i.e., handcrafted features), user (i.e., those who shared the rumor) and network propagation (i.e., structural characteristics of the retweet cascade). The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. As a rumor spreads, each time series is fed to a Hidden Markov Model that provides a “local” veracity score. Local scores are then aggregated to predict the “global” veracity of the rumor at each time step. Experiments revealed that the network features yield the largest contribution; textual and user-derived features followed, providing comparable contributions. Results by [Vosoughi et al., 2017] and [Kwon et al., 2017] also demonstrated that the contribution of the different data modalities change through time.
To mitigate the “cold start” problem of propagation-based early detection of fake news, [Liu and Wu, 2018] proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Finally, the two representations were concatenated and fed to a multi-layer feedforward network that predicted the ﬁnal class label. Their experiments showed that the proposed system was capable of detecting fake news with 0.8 accuracy after only 5 minutes since the news started spreading. The increased efﬁcacy at the early stage of news propagation depends on the choice to rely heavily on user information.
[Zannettou et al., 2018] analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propagate. Finally, [Van-Hoang et al., 2020] proposed Factual News Graph (FANG) to exploit social structure and engagement patterns of users for fake news detection.
3 Multimodal Harmful Content Detection
In this section, we focus on the second aspect of disinformation – harmfulness. Once again, we do not discuss text-only models as text is almost always present in multimodal approaches.
It is essential to ﬁlter or ﬂag harmful content on social media platforms. The harmful content includes child abuse material, violent and extreme content, hate speech, graphic content, sexual content, cruel and insensitive material, and spam content. In recent years, the ability to recognize harmful content within online communities has received a lot of attention from researchers and policymakers that aim to keep users safe in the digital world. Studies in this direction include detecting harmful contents in network science [Ribeiro et al., 2018], natural language processing [Waseem et al., 2017; Schmidt and Wiegand, 2017;

Fortuna and Nunes, 2018] and computer vision [Yang et al., 2019a; Vijayaraghavan et al., 2019; Gomez et al., 2020; Dimitrov et al., 2021].
3.1 Image
Among different harmful contents, cyberbullying is one of the major growing problems, affecting teens signiﬁcantly. [Hosseinmardi et al., 2015] investigated Instagram images and their associated comments for detecting cyberbullying and online harassment. They developed a crowd-sourced dataset of manually labeled images and their associated comments.
Hate speech is another important problem that spread over social media. “Hateful Memes Challenge” is an important milestone to advance the research on this topic and the tasks is to detect hateful memes [Kiela et al., 2020]. [Das et al., 2020] proposed different approaches for hatefulness detection in memes such as (i) extract caption and include this information with multimodal model, (ii) use sentiment as an additional feature with multimodal representations. For hate speech detection, [Yang et al., 2019a] reported that augmenting text with image embedding immediately boosts performance. [Vijayaraghavan et al., 2019] proposed methods for interpreting multimodal hate speech detection models, where the modalities consist of text and socio-cultural information rather than images. Concurrently, [Gomez et al., 2020] introduced a larger dataset (150,000 tweets) for multimodal hate speech detection, which consists of six categories such as no attacks to any community, racist, sexist, homophobic, religion based attacks and attacks to other communities.
Propaganda is another topic which has been explored in multimodal setting. [Seo, 2014] showed how Twitter was by Israel Defense Forces and Hamas as a propaganda tool during the 2012 Gaza conﬂict to build international support for their own side. [Dimitrov et al., 2021] addressed the detection of persuasion techniques in memes. They collected a dataset of 950 Facebook memes on multiple relevant topics such as politics, COVID-19, vaccines and gender equality. They used both textual and visual content of a meme to identify if it contains one or more of the 22 propaganda techniques deﬁned in their work. Their analysis of the dataset showed that while propaganda is not always factually false or harmful, most memes are used to damage the reputation of a person or a group of people. [Glenski et al., 2019] explored multilingual multimodal content for deception detection to distinguish between disinformation, propaganda, conspiracy, hoax, and clickbait.
3.2 Speech
Cues in spoken content can represent harmful behaviors and those cues can be used to automatically detect such content. Due to the lack of data, studies using speech-only modality are comparatively lower than other modalities even if it has a major role in many context. For example, for detecting violent content such as screaming and gunshots, the speech modality can play a signiﬁcant role, which other modalities might not be able to offer. This is important as journalists most often post user-generated content without verifying then, which can lead to serious consequences. [Giannakopoulos, 2009] studied the audio segmentation approaches for segmenting violent (e.g.,

gunshots, screams) and non-violent (e.g., music, speech) content in movies. The studies related to violent content detection using acoustic features also include [Esra et al, 2013], where the focus was ﬁnding violent content in movies.
[Junwei et al., 2017] proposed Localized Self-Paced Reranking (LSPaR) for detecting gunshots and explosion in videos using acoustic features. [Soni and Singh, 2018] investigated audio, visual and textual features for cyberbullying detection. Their ﬁndings suggest that audio and visual features are associated with the occurrence of cyberbullying, and both these features complement textual features.
3.3 Video
There are multiple studies on detecting cyberbullying in videobased social networks such as vine [Raﬁq et al., 2015] and YouTube [Dadvar and Eckert, 2018]. These studies show that although the percentage of cyberbullying in video sessions is quite low, automatic detection of these types of content is very challenging. [Wang et al., 2020a] utilized textual, visual, and other meta-information to detect social media posts with bullying topics. Their proposed method was evaluated on publicly-available multi-modal cyberbullying datasets. [Abd Kadir et al., 2016] investigated the relationship between emotion and propaganda techniques in Youtube videos. Their ﬁndings suggest that propaganda techniques in Youtube videos affect emotional responses.
Content (e.g., Youtube videos) can also be attacked by hateful users via posting hateful comments through a coordinated effort. [Mariconti et al., 2019] investigated whether a video is likely to be attacked using different modalities such as metadata, audio transcripts, and thumbnails.
There has been a recent interest from different government agencies to stop the spread of violent content. [Constantin et al., 2020] developed a multimodal dataset, which consists of more than 96 hours of Hollywood and YouTube videos and high variability of content. Their study suggests that multimodal approaches with audio and image perform better than other settings.
3.4 Network and Temporal Information
The use of network data for predicting factuality was motivated by results showing different propagation patterns for fake and real content. These results are lacking for harmful content. However, the aim to harm in social media is often pursued via coordinated actions, for instance by groups of users (e.g., social bots and trolls [Cresci, 2020]) that target certain characters or minorities. These collaborative harmful actions, perpetrated to increase the efﬁcacy of the harm, allow to exploit networks for detecting harmful campaigns.
[Chatzakou et al., 2019] focused on detecting cyberbullying and cyberaggression by training machine learning models for detecting: (i) bullies, (ii) aggressors, (iii) spammers, and (iv) normal users on Twitter. To solve these tasks, they leveraged a combination of 38 features extracted from user proﬁles, the textual content of their posts, and network information (e.g., user degree and centrality measures in the social graph).
Orthogonal and synergy to the detection of disinformation, scholars have recently focused on the novel task of detecting Coordinated Inauthentic Behavior (CIB) [Nizzoli et al., 2020].

CIB is deﬁned as coordinated activities that aim to mislead and manipulate others.1
The threat of CIB does not necessarily stem from the content being spread – which may well be true and factual, but rather from the shared intent to mislead and manipulate, which may cause harm. Detecting CIB typically involves analyzing both interaction networks to detect suspicious coordination, as well as the coordinated users and the content they shared to detect inauthentic users and harmful content [Nizzoli et al., 2020]. Given the importance of coordination in CIB, analyses typically start from the available network data by applying community detection algorithms, and subsequently moving to the analysis of textual data (e.g., social media posts or news articles). [Weber and Neumann, 2020] also considered the timings of actions to detect coordination, thus leveraging both the network and temporal data modalities. Despite the promising initial results, the detection of CIB is still in its infancy, and thus, additional data modalities such as images and audio are yet to be studied.
4 Modeling Techniques
In this section, we discuss modeling techniques for both factuality and harmfulness.
To combine multiple modalities there has been several approaches: (i) early-fusion, where low-level features from different modalities are learned, fused and fed into a single prediction model [Jin et al., 2017b; Yang et al., 2018; Zhang et al., 2019; Singhal et al., 2019; Zhou et al., 2020; Kang et al., 2020]; (ii) late-fusion, where unimodal decisions are fused with some mechanisms such as averaging, voting [Agrawal et al., 2017; Qi et al., 2019], and (iii) hybrid-fusion, where a subset of learned features are passed to the ﬁnal classiﬁer (early-fusion) and the remaining modalities are fed to the classiﬁer later (late-fusion) [Jin et al., 2017a]. Within these fusion strategies, the learning setup can also be divided into unsupervised, semi-supervised, supervised and self-supervised methods.
[Dimitrov et al., 2021] investigated different fusion strategies (e.g., early- and late-fusion and self-supervised models) for propaganda detection. For different fusion strategies, the authors used VisualBERT [Li et al., 2019], MMBT [Kiela et al., 2019] and ViLBERT [Lu et al., 2019]. Their ﬁndings suggest that self-supervised joint learning models, such as MMBT, ViLBERT, VisualBERT perform better in increasing order, respectively, compared to the other fusion methods. As a part of “Hateful Memes Challenge” to classify hateful vs. not-hateful memes, several of such models have also been investigated [Kiela et al., 2020]. In addition, Kiela et al. also experimented with other models such as Gated Multimodal Unit (GMU) [Arevalo et al., 2017] and ConcatBERT [Kiela et al., 2020]. These models learn individual and non-overlapping training objectives for each modality.
Attempts to design unsupervised models are limited. [Mu¨ller-Budack et al., 2020] introduced Cross-modal Consistency Veriﬁcation Tool (CCVT) to check the coherence between images and associated texts. [Yang et al., 2019b]
1http://about.fb.com/news/2018/12/ inside-feed-coordinated-inauthentic-behavior/

deﬁned trust of news and credibility of users who spread the news and used Bayesian learning to iteratively update these quantities. News with low trustworthiness is returned as fake news. [Gangireddy et al., 2020] proposed GTUT, a graphbased approach that exploits the underlying bipartite network of users and news articles to detect the dense communities of fake news and fraud users.
Due to the scarcity of labeled data, a few studies attempted to design semi-supervised methods by leveraging an ample amount of unlabelled data. [Helmstetter and Paulheim, 2018; Gravanis et al., 2019] presented weak-supervision in fake news detection. [Guacho et al., 2018] presented a tensordecomposition based semi-supervised method for fake content detection. [Dong et al., 2020] developed a deep semisupervised model via two-path learning (one path utilizes a limited labeled data, the other path explores the unlabelled data) for timely fake news detection. [Paka et al., 2021b] presented Cross-SEAN, a cross-stitch based semi-supervised end-to-end neural attention model for COVID-19 fake news detection. They further extended it by combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of fake news [Paka et al., 2021a].
Within the supervised learning setup, two other types of learning method have also been explored for disinformation detection such as adversarial learning and autoencoder based. Adversarial learning models for fake news detection include EANN [Wang et al., 2018], an event adversarial neural network to detect emerging and time-critical fake news, and SAME [Cui et al., 2019], a sentiment-aware multimodal embedding method which, along with multiple modalities, leverages the sentiment expressed by readers in their comments. [Qi et al., 2019] proposed MAVE, a multimodal variational autoencoder that jointly encodes text and images and uses the intermediate representation for fake news detection.
5 Lessons Learned
1. A lot of progress has been made on the problem, but the two components in the deﬁnition of disinformation (falseness and harmfulness) have been considered in isolation. There is a need to combine factuality and intentional harmfulness into the detection model.
2. Most multimodal datasets cover just two modalities, or put together modalities mechanically. Moreover, no multimodal dataset looks at both aspects of disinformation: factuality and harmfulness.
3. In the early phase of (dis)information spreading, user and content features are those that provide the highest contribution for detecting factuality. Indeed, at that time few interactions with content are available and the propagation network is small and sparse. As information spreads, the contribution of content-derived features remains constant, while propagation-derived features become richer and more informative. In summary, early prediction of factuality and veracity must necessarily rely heavily on users and content – be it text, image, video or audio. Instead, analyses carried out at later times beneﬁt more from network propagation and temporal data.

6 Major Challenges
Recently, several initiatives were undertaken by major companies and government entities to combat disinformation in social media.2 However, automatic detection of misleading and harmful content poses diverse challenges. Below we offer a non-exhaustive list of what is to be addressed in the future.
1. Models Combining Multiple Multimodalities. The major challenge is to devise a mechanism to combine multiple modalities in a systematic way so that one modality complements others. Current research primarily adopts fusion mechanisms, which might not bring the complementarity across modalities.
2. Datasets. There is a need for truly multimodal datasets that combine multiple modalities such as text, image, speech, video, temporal, user proﬁle, and network. In addition, we also need to focus on the dataset from multiple platforms (e.g., news, posts from Twitter, Reddit, Gab, and Instagram) as different data sources present different types of styles, and focus on diverse topics. For example, Gab promotes the free ﬂow of information, which can hinder content moderation.
3. Contextualization. Existing methods of disinformation detection are mostly non-contextualized, i.e., the broader context of a news in terms of the responses of the readers and how the users perceive them are not captured. We argue that the response thread under a news, the underlying social network among users, the propagation dynamics of the news and its mentions across social media need suitable integration to better capture the overall perspective on the news.
4. Meta Information. Along with the news and the context, other information such as the authenticity of the news, the credibility of the authors of the news, the factuality of the news also play important roles for disinformation detection. Moreover, whether the disinformation attack is a coordinated effort or an individual activity would also help understanding its severity.
5. Fine-grained Detection. Existing disinformation detection models are mostly binary classiﬁers – given a news, they detect whether it is a disinformation or not. We argue that such systems may not be enough to provide actionable results. Therefore, rather than a binary classiﬁcation, one could cast the problem as a multi-class classiﬁcation task or even a regression task. This would also help prioritize disinformation for reactive measurements.
6. Bias, Region, and Cultural Awareness. The performance of most of the existing systems is limited to the underlying dataset, particularly to the demography and the underlying cultural aspects. For instance, a model trained on an Indian political dataset may not generalize to a US health related dataset [Fortuna et al., 2021].
7. Transparent and Accountable Models. The detection models should be designed in such a way that their outcomes are unbiased and more accountable to ethical considerations.
2For example, https://digi.org.au/disinformation-code/

8. Disinformation on Evolving Topics. Often claims or harmful content are disseminated based on the current event; information about COVID-19 and its vaccine are examples of such use cases. Existing models might fail on such use cases, zero-shot or few-shot learning might be a future avenue to explore.
9. Common-sense Reasoning. The models for disinformation detection should present the outcome in such a way that a practitioner can interpret it and understand why a piece of information is ﬂagged as disinformation, what is the related real news based on which the judgement was made, which part of the information was counterfeit, etc. There is also a lack of datasets containing disinformation with explanations and the corresponding real information.
10. Personalization. The way of perceiving a news is subjective. Therefore, a disinformation detection system should adhere to the preferences of individual readers. We posit that the detection system should be personalized at the level of an individual person to the level of a group/community.
7 Future Forecasting
Based on the challenges mentioned in the previous section, we forecast the following research directions:
1. Explainability. Model interpretation remains largely unexplored. This can be addressed in future studies to understand the general capability of the models. Providing evidence of why certain claims are false is also important. Current methods of disinformation detection lack such capability, and as of present, justiﬁcation is written manually by human fact-checkers.
2. Beyond Content and Network Signals. Current stateof-the-art solutions in multimodal factuality prediction and harmful content detection are primarily based on content signals and network structure. However, the information in these signals is limited and does not include personal preferences or cultural desires. In the near future, we envision multimodal techniques for disinformation detection that would go beyond content and network signals and would include signals like common sense and information about the user. Moreover, multimodal models will become larger with more heterogeneous signals as input, and they would be pre-trained on wider variety of tasks to shelter both aspects of disinformation: factuality and harmfulness.
8 Conclusion
We surveyed the state of the art in multimodal disinformation detection based on prior work on different modalities, focusing on disinformation, i.e., information that is both false and intents to do harm. We argued for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness in the same framework. Finally, we discussed current challenges and we forecast some future research directions.

References
[Abd Kadir et al., 2016] Shamsiah Abd Kadir et al. Emotion and techniques of propaganda in youtube videos. In IJST, 2016.
[Agrawal et al., 2017] Taruna Agrawal et al. Multimodal detection of fake social media use through a fusion of classiﬁcation and pairwise ranking systems. In EUSIPCO, 2017.
[Arevalo et al., 2017] John Arevalo et al. Gated multimodal units for information fusion. In ICLR, 2017.
[Boididou et al., 2016] Christina Boididou et al. Verifying multimedia use at MediaEval 2016. MediaEval, 2016.
[Bozarth and Budak, 2020] Lia Bozarth and Ceren Budak. Toward a better performance evaluation framework for fake news classiﬁcation. In ICWSM, 2020.
[Cardoso Durier da Silva et al., 2019] Fernando Cardoso Durier da Silva et al. Can machines learn to detect fake news? A survey focused on social media. In HICSS, 2019.
[Chatzakou et al., 2019] Despoina Chatzakou et al. Detecting cyberbullying and cyberaggression in social media. TWEB, 2019.
[Constantin et al., 2020] M. G. Constantin et al. Affect in multimedia: Benchmarking violent scenes detection. TAC, 2020.
[Cresci, 2020] Stefano Cresci. A decade of social bot detection. CACM, 2020.
[Cui et al., 2019] Limeng Cui et al. SAME: Sentiment-aware multimodal embedding for detecting fake news. In ASONAM, 2019.
[Da San Martino et al., 2020] Giovanni Da San Martino et al. A survey on computational propaganda detection. In IJCAI, 2020.
[Dadvar and Eckert, 2018] Maral Dadvar and Kai Eckert. Cyberbullying detection in social networks using deep learning based models; a reproducibility study. arXiv:1812.08046, 2018.
[Das et al., 2020] Abhishek Das et al. Detecting hate speech in multi-modal memes. arXiv:2012.14891, 2020.
[Dimitrov et al., 2021] Dimiter Dimitrov et al. Task 6 at SemEval2021: Detection of persuasion techniques in texts and images. In SemEval, 2021.
[Dinkov et al., 2019] Yoan Dinkov et al. Predicting the leading political ideology of youtube channels using acoustic, textual, and metadata information. In Interspeech, 2019.
[Dong et al., 2020] Xishuang Dong et al. Two-path deep semisupervised learning for timely fake news detection. In TCSS, 2020.
[Esra et al, 2013] Acar Esra et al. Violence detection in hollywood movies by the fusion of visual and mid-level audio cues. In MM, page 717–720. ACM, 2013.
[Fortuna and Nunes, 2018] Paula Fortuna and Se´rgio Nunes. A survey on automatic detection of hate speech in text. CSUR, 2018.
[Fortuna et al., 2021] Paula Fortuna et al. How well do hate speech, toxicity, abusive and offensive language classiﬁcation models generalize across datasets? IPM, 2021.
[Gangireddy et al., 2020] Siva Charan Reddy Gangireddy et al. Unsupervised fake news detection: A graph-based approach. In HT, 2020.
[Garimella and Eckles, 2020] Kiran Garimella and Dean Eckles. Images and misinformation in political groups: Evidence from WhatsApp in India. arXiv:2005.09784, 2020.
[Giannakopoulos, 2009] Theodoros Giannakopoulos. Study and application of acoustic information for the detection of harmful content, and fusion with visual information. University of Athens, 2009.

[Glenski et al., 2019] Maria Glenski et al. Multilingual multimodal digital deception detection and disinformation spread across social platforms. arXiv:1909.05838, 2019.
[Gomez et al., 2020] Raul Gomez et al. Exploring hate speech detection in multimodal publications. In WACV, 2020.
[Gravanis et al., 2019] Georgios Gravanis et al. Behind the cues: A benchmarking study for fake news detection. Expert Systems with Applications, 2019.
[Guacho et al., 2018] Gisel Bastidas Guacho et al. Semi-supervised content-based detection of misinformation via tensor embeddings. In ASONAM, 2018.
[Gupta et al., 2013] Aditi Gupta et al. Faking Sandy: Characterizing and identifying fake images on Twitter during hurricane Sandy. In WWW, 2013.
[Hameleers et al., 2020] Michael Hameleers et al. A picture paints a thousand lies? the effects and mechanisms of multimodal disinformation and rebuttals disseminated via social media. Pol. Comm., 2020.
[Helmstetter and Paulheim, 2018] Stefan Helmstetter and Heiko Paulheim. Weakly supervised learning for fake news detection on Twitter. In ASONAM, 2018.
[Hirschberg et al., 2005] Julia Hirschberg et al. Distinguishing deceptive from non-deceptive speech. In Interspeech, 2005.
[Hosseinmardi et al., 2015] Homa Hosseinmardi et al. Detection of cyberbullying incidents on the Instagram social network. arXiv:1503.03909, 2015.
[Islam et al., 2020] Md Raﬁqul Islam et al. Deep learning for misinformation detection on online social networks: a survey and new perspectives. SNAM, 2020.
[Jin et al., 2017a] Zhiwei Jin et al. Multimodal fusion with recurrent neural networks for rumor detection on microblogs. In MM, 2017.
[Jin et al., 2017b] Zhiwei Jin et al. Novel visual and statistical image features for microblogs news veriﬁcation. TMM, 2017.
[Junwei et al., 2017] Liang Junwei et al. Temporal localization of audio events for conﬂict monitoring in social media. In ICASSP, 2017.
[Kang et al., 2020] SeongKu Kang et al. Multi-modal component embedding for fake news detection. In IMCOM, 2020.
[Kaya and Karpov, 2016] Heysem Kaya and Alexey A Karpov. Fusing acoustic feature representations for computational paralinguistics tasks. In Interspeech, 2016.
[Kiela et al., 2019] Douwe Kiela et al. Supervised multimodal bitransformers for classifying images and text. arXiv:1909.02950, 2019.
[Kiela et al., 2020] Douwe Kiela et al. The hateful memes challenge: Detecting hate speech in multimodal memes. arXiv:2005.04790, 2020.
[Kopev et al., 2019] Daniel Kopev et al. Detecting deception in political debates using acoustic and textual features. In ASRU, 2019.
[Kotonya and Toni, 2020] Neema Kotonya and Francesca Toni. Explainable automated fact-checking: A survey. In COLING, 2020.
[Krishnamurthy et al., 2018] Gangeshwar Krishnamurthy et al. A deep learning approach for multimodal deception detection. arXiv:1803.00344, 2018.
[Kumar and Shah, 2018] Srijan Kumar and Neil Shah. False information on web and social media: A survey. arXiv:1804.08559, 2018.

[Kwon et al., 2017] Sejeong Kwon et al. Rumor detection over varying time windows. PLoS ONE, 2017.
[Levitan et al., 2016] Sarah Ita Levitan et al. Combining acousticprosodic, lexical, and phonotactic features for automatic deception detection. In Interspeech, 2016.
[Li and Xie, 2020] Yiyi Li and Ying Xie. Is a picture worth a thousand words? an empirical study of image content and social media engagement. JMR, 2020.
[Li et al., 2016] Yaliang Li et al. A survey on truth discovery. In SIGKDD, 2016.
[Li et al., 2019] L. Li et al. VisualBERT: A simple and performant baseline for vision and language. arXiv:1908.03557, 2019.
[Liu and Wu, 2018] Yang Liu and Yi-Fang Wu. Early detection of fake news on social media through propagation path classiﬁcation with recurrent and convolutional networks. In AAAI, 2018.
[Lu et al., 2019] Jiasen Lu et al. VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv:1908.02265, 2019.
[Mariconti et al., 2019] Enrico Mariconti et al. “You Know What to Do”: Proactive detection of YouTube videos targeted by coordinated hate attacks. HCI (CSCW), 2019.
[Mu¨ller-Budack et al., 2020] Eric Mu¨ller-Budack et al. Multimodal analytics for real-world news using measures of cross-modal entity consistency. arXiv:2003.10421, 2020.
[Nakamura et al., 2020] Kai Nakamura et al. r/Fakeddit: A new multimodal benchmark dataset for ﬁne-grained fake news detection. In LREC, 2020.
[Nizzoli et al., 2020] Leonardo Nizzoli et al. Coordinated behavior on social media in 2019 UK general election. arXiv:2008.08370, 2020.
[Paka et al., 2021a] W. Paka et al. Combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of COVID-19 fake tweets. In PAKDD, 2021.
[Paka et al., 2021b] William Scott Paka et al. Cross-SEAN: A crossstitch semi-supervised neural attention model for COVID-19 fake news detection. arXiv:2102.08924, 2021.
[Pe´rez-Rosas et al., 2015] Vero´nica Pe´rez-Rosas et al. Deception detection using real-life trial data. In ICMI, 2015.
[Qi et al., 2019] Peng Qi et al. Exploiting multi-domain visual information for fake news detection. arXiv:1908.04472, 2019.
[Raﬁq et al., 2015] Rahat Ibn Raﬁq et al. Careful what you share in six seconds: Detecting cyberbullying instances in Vine. In ASONAM, 2015.
[Ribeiro et al., 2018] Manoel Ribeiro et al. Characterizing and detecting hateful users on Twitter. In ICWSM, 2018.
[Schmidt and Wiegand, 2017] Anna Schmidt and Michael Wiegand. A survey on hate speech detection using natural language processing. In SocialNLP, 2017.
[Seo, 2014] Hyunjin Seo. Visual propaganda in the age of social media: An empirical analysis of Twitter images during the 2012 Israeli–Hamas conﬂict. Visual Communication Quarterly, 2014.
[Shaar et al., 2020] Shaden Shaar et al. That is a known lie: Detecting previously fact-checked claims. In ACL, 2020.
[Shao et al., 2018] Chengcheng Shao et al. The spread of lowcredibility content by social bots. Nature communications, 2018.
[Shu et al., 2019] Kai Shu et al. Beyond news contents: The role of social context for fake news detection. In WSDM, 2019.

[Shu et al., 2020] Kai Shu et al. Hierarchical propagation networks for fake news detection: Investigation and exploitation. In ICWSM, 2020.
[Singhal et al., 2019] Shivangi Singhal et al. SpotFake: A multimodal framework for fake news detection. In BigMM, 2019.
[Soldner et al., 2019] Felix Soldner et al. Box of lies: Multimodal deception detection in dialogues. In NAACL-HLT, 2019.
[Soni and Singh, 2018] Devin Soni and Vivek K Singh. See no evil, hear no evil: Audio-visual-textual cyberbullying detection. HCI (CSCW), 2018.
[Van-Hoang et al., 2020] Nguyen Van-Hoang et al. FANG: Leveraging social context for fake news detection using graph representation. In CIKM, 2020.
[Vijayaraghavan et al., 2019] Prashanth Vijayaraghavan et al. Interpretable multi-modal hate speech detection. In ICML, 2019.
[Volkova et al., 2019] Svitlana Volkova et al. Explaining multimodal deceptive news prediction models. ICWSM, 2019.
[Vosoughi et al., 2017] Soroush Vosoughi et al. Rumor gauge: Predicting the veracity of rumors on Twitter. TKDD, 2017.
[Vosoughi et al., 2018] Soroush Vosoughi et al. The spread of true and false news online. Science, 2018.
[Wang et al., 2018] Yaqing Wang et al. EANN: Event adversarial neural networks for multi-modal fake news detection. In SIGKDD, 2018.
[Wang et al., 2020a] Kaige Wang et al. Multi-modal cyberbullying detection on social networks. In IJCNN, 2020.
[Wang et al., 2020b] Yuping Wang et al. Understanding the use of fauxtography on social media. arXiv:2009.11792, 2020.
[Waseem et al., 2017] Zeerak Waseem et al. Understanding abuse: A typology of abusive language detection subtasks. In ACL Workshops, 2017.
[Weber and Neumann, 2020] Derek Weber and Frank Neumann. Who’s in the gang? revealing coordinating communities in social media. arXiv:2010.08180, 2020.
[Yang et al., 2018] Yang Yang et al. TI-CNN: Convolutional neural networks for fake news detection. arXiv:1806.00749, 2018.
[Yang et al., 2019a] Fan Yang et al. Exploring deep multimodal fusion of text and photo for hate speech classiﬁcation. In ACL Workshops, 2019.
[Yang et al., 2019b] Shuo Yang et al. Unsupervised fake news detection on social media: A generative approach. In AAAI, 2019.
[Zannettou et al., 2018] Savvas Zannettou et al. On the origins of memes by means of fringe web communities. In IMC, 2018.
[Zhang et al., 2018] Daniel Yue Zhang et al. Fauxbuster: A contentfree fauxtography detector using social media comments. In BigData, 2018.
[Zhang et al., 2019] Huaiwen Zhang et al. Multi-modal knowledgeaware event memory network for social media rumor detection. In MM, 2019.
[Zhou and Zafarani, 2019] X. Zhou and R. Zafarani. Network-based fake news detection: A pattern-driven approach. SIGKDD, 2019.
[Zhou et al., 2020] Xinyi Zhou et al. SAFE: Similarity-aware multimodal fake news detection. arXiv:2003.04981, 2020.
[Zlatkova et al., 2019] Dimitrina Zlatkova et al. Fact-checking meets fauxtography: Verifying claims about images. In EMNLP, 2019.

