Crowdsourced Labeling for Worker-Task Specialization Model
Doyeon Kim and Hye Won Chung School of Electrical Engineering KAIST
Email: {highlowzz, hwchung}@kaist.ac.kr

arXiv:2004.00101v2 [cs.HC] 9 Jun 2021

Abstract—We consider crowdsourced labeling under a d-type worker-task specialization model, where each worker and task is associated with one particular type among a ﬁnite set of types and a worker provides a more reliable answer to tasks of the matched type than to tasks of unmatched types. We design an inference algorithm that recovers binary task labels (up to any given recovery accuracy) by using worker clustering, worker skill estimation and weighted majority voting. The designed inference algorithm does not require any information about worker/task types, and achieves any targeted recovery accuracy with the best known performance (minimum number of queries per task). 1
I. INTRODUCTION
We consider the problem of crowdsourced labeling, which has diverse applications in image labeling, video annotation, and character recognition [1]–[3]. Workers in the crowdsourcing system are given simple tasks and asked to provide a binary label to each assigned task. Since workers may provide incorrect labels to some of the tasks and worker reliabilities are usually unknown, the main challenge in the crowdsourced labeling is to infer true labels from noisy answers collected from workers of unknown reliabilities.
To resolve such challenges and to design inference algorithms with provable performance guarantees, many previous works considered a simple yet meaningful error model for workers’ answers. One of the most widely studied model is the single-coin Dawid-Skene model [4], where each worker is modeled by his/her own reliability level and the worker provides a correct answer to any task with probability depending on the worker’s reliability level, regardless of the types of assigned tasks. For such a model, various inference algorithms were proposed to ﬁrst estimate the worker reliabilities from the collected answers and to use them to infer correct labels by using expectation maximization (EM) [5]–[7], message passing [8], or spectral method [9], [10]. However, this error model does not capture some realistic scenarios where worker’s ability to provide a correct label could change depending on the types of the assigned tasks and the workers’ expertise [11]–[14].
1This work was supported in part by National Research Foundation of Korea under Grant 2017R1E1A1A01076340; in part by the Ministry of Science and ICT, South Korea, under the ITRC support program under Grant IITP-2021-2018-0-01402; and in part by the Institute of Information and Communications Technology Planning & Evaluation (IITP) grant funded by the Korea Government MSIT under Grant 2020-0-00626.

In this work, we consider a d-type specialization model, which was introduced in [15]. This model assumes that each worker and each task is associated with a single type (among d different types), and a worker provides an answer better than a random guess if the task type matches the worker type and otherwise, the worker just provides a random guess. The inference algorithm proposed in [15] is composed of two stages. At the ﬁrst stage, the workers are clustered based on similarity on their answers, and at the second stage the task label is estimated by ﬁrst ﬁnding a cluster of the matched type and aggregating the answers only from the chosen cluster while ignoring the answers from other clusters.
In this work, we generalize the d-type specialization model to the case where a worker provides an answer better than a random guess with probability q ∈ [1/2, p) even when the worker type and the task type does not match. When the types are matched, the answer is correct with higher probability p ∈ (q, 1]. Different from the algorithm in [15], we do not throw away the answers from the cluster of unmatched type but use the answers with proper weights to achieve the optimal accuracy in the label estimation. We propose two algorithms in this paper. Our ﬁrst algorithm does not require any information on the worker/task types but the parameters (p, q), and it achieves the best known performance, regardless of the regimes of the reliability parameters (p, q) or the number of types d. We then propose our second algorithm which does not require even (p, q) values. In this algorithm, the parameters are estimated from the workers’ answers and used to estimate the correct labels. We empirically show that our second algorithm achieve as good performance as the ﬁrst algorithm in diverse parameter regimes. Furthermore, we empirically demonstrate that under the generalized d-type specialization model our two proposed algorithms outperform the state-of-the-art inference algorithms developed for the Dawid-Skene model.
II. PROBLEM FORMULATION
In this work, we consider a d-type specialization model for crowdsourced labeling. We assume that there exists m binary tasks and n workers. Denote the set of tasks and the set of workers by T and W, respectively. Let Wz denote the set of workers of type z ∈ [d]. For i ∈ T , let ai ∈ {−1, 1} denote the true label of the i-th task, and let ti, wj ∈ [d] denote the type of the i-th task and that of the j-th worker, respectively, where [d] := {1, . . . , d}. We assume that the type of each task

and the type of each worker are uniformly distributed over [d]. The set of workers assigned to task i is denoted by Ni. Let mij be the j-th worker’s answer to the task i. If task i is not assigned to worker j, then mij = 0, and if it is assigned

mij = ai with probability fij,

(1)

−ai with probability 1 − fij.

We assume that mij ’s are independent for all i, j. The d-type specialization model we consider further assumes that

fij = p, if ti = wj,

(2)

q, o.w.

where p > q ≥ 1/2. Different from [15] where the value q

was ﬁxed to 1/2, here we consider a general q ∈ [1/2, p).

For i ∈ T , let aˆi ∈ {−1, 1} denote the inferred label of the

i-th task. The performance metric we consider is the expected

fraction

of

errors

in

the

inferred

labels,

i.e.,

E[

1 m

m i=1

½(aˆi

=

ai)]

=

1 m

m i=1

È(aˆi

=

ai).

We

aim

to

minimize

the

number

of queries per task, achieving

m1 m È(aˆi = ai) ≤ αc, for some αc ∈ (0, 1). (3) i=1

III. PERFORMANCE BASELINES

In this section, we ﬁrst review performance baselines of previous works and outline our contributions.

A. Oracle Weighted Majority Voting and Majority Voting

As the ﬁrst performance baseline, we consider a general weighted majority voting, which aggregates answers with weights to generate the label estimate. For weighted majority voting, the decision is given by





aˆWi MV = sign  µij mij  ,

(4)

j∈Ni

where µij is the weight for the answer from the j-th worker to the i-th task. By using Hoeffding’s inequality (or Corollary 5 in [16]), it can be shown that the weighted majority voting
guarantees

È(aˆWMV = ai) ≤ exp − γW2 MV |Ni|

(5)

i

2

where γWMV = j∈Ni µij (2fij − 1) (6) µi∗ 2 · |Ni|

for µi∗ = (µi1, . . . , µi|Ni|). By Cauchy-Schwarz inequality,

the weight µij that maximizes γWMV is µij ∝ (2fij − 1).

When we choose Ni ⊂ W at random, effectively, 1/d

fraction of answers are given with ﬁdelity fij = p and the rest

with fij = q. Thus, when {fij} is known, i.e., when the task

types {ti} and the worker types {wj} as well as the reliability

parameters (p, q) are known at the inference algorithm, by

choosing µij ∝ (2fij − 1) the ora√cle weighted majority voting

can achieve (5) with γ∗ = (2p−1)2+√(d−1)(2q−1)2 . The

WMV

d

required number of queries per task to achieve (3) for the oracle weighted majority voting is thus

2d

1

Loracle = (2p − 1)2 + (d − 1)(2q − 1)2 ln αc . (7)

As another baseline, we can consider the simple ma-
jority voting that aggregates all the answers with equal
weights, i.e., aˆMi V = sign j∈Ni mij . The majority voting
gives È(aˆMi V = ai) ≤ exp − γ2M2V |Ni| where γMV =
((2p−1)+(dd−1)(2q−1)) . To achieve the targeted recovery accuracy (3) with the majority voting, the required number of
queries per task is

2d2

1

Lmv = ((2p − 1) + (d − 1)(2q − 1))2 ln αc . (8)

We can easily check the Loracle in (7) is less than or equal to Lmv in (8). However, the oracle result is achievable when the worker types and the task types as well as reliability parameters (p, q) are all known to the inference algorithm.

B. Inference Algorithm from [15]: Clustering and Majority Voting from the Workers of a Matched Cluster
We review the algorithm in [15], proposed for the d-type specialization model with p > q = 1/2. The parameters ζ, r, and l of this algorithm can be chosen later to guarantee the recovery condition (3).

Algorithm [15]: This algorithm is composed of two stages.

• Stage 1 (Clustering Workers by Types): Let S ⊂ T represent

randomly chosen r tasks from the set T . Assign each task

in S to all n workers. Given the answers mij for i ∈ S,

cluster workers sequentially: for a worker j ∈ [n] if there

exists a cluster of workers Q ⊂ [j − 1] such that for each

j′ ∈ Q

1 ½(mij = mij′ ) > ζ,

(9)

r i∈S

then assign j to Q; otherwise, create a new cluster containing j. Let {V1, . . . , Vc} be the resulting clusters of [n] workers. For each task i ∈ T \S and cluster z ∈ [c], assign task i to l workers sampled uniformly at random from the set Vz. The total number of workers assigned to task i is lc. • Stage 2 (Type Matching and Majority Voting): For each task i ∈ T , ﬁnd a cluster of the matched type by

z∗(i) = arg max

mij ,

(10)

z∈[c] j∈Ni∩Vz

and estimate the label for the task i by the majority voting from the answers only from the set Vz∗(i):





aˆi = sign 

mij  .

j∈Ni ∩Vz∗(i)

(11)

The main idea of this algorithm is to cluster workers by ﬁnding subsets of workers having similarity (larger than some

threshold ζ) in their answers for the initially assigned |S| = r

tasks. After assigning the rest of the tasks T \S to total

lc workers from c clusters, the ﬁnal decision is made by

the majority voting from the answers only from one cluster

believed to be composed of workers having the same type as

the task. The parameters ζ, r, and l of this algorithm can be

chosen to guarantee the recovery condition (3). We note that the choice of ζ, which is 12 + (p−dq)2 in [15], requires a prior knowledge of the model parameter p, q.

We can easily generalize the analysis of this original al-

gorithm to a general q ≥ 1/2 by selecting a proper choice

of ζ, r, n and l, and can show that the required number of

queries

per

task

1 m

(nr

+

ld(m

−

r))

to

achieve

the

recovery

condition (3) can be bounded as

Ltype = min

2d

6d + 3 2d 6d

(p−q)2 + (2q−1)2 ln αc , (p−q)2 ln αc

2

2

2

(12)

when r = 2(pd−2q)4 ln 3n2(nα−c 1) , n ≥ max 8d ln α3dc , Ltype ,

m ≥ cn3, and l = 21d Ltype for some constant c > 0.

Remark 1 (Our contributions): When q = 1/2 and

d is large, the clustering-based algorithm can guarantee the

recovery condition (3) with the number of queries per task scaling as (2p−d 1)2 ln αdc , whereas the majority voting requires (2pd−21)2 ln α1c queries per task. This demonstrates the beneﬁt of using the clustering-based algorithm for q = 1/2. The

gain comes from aggregating a selected subset of answers

from a matched cluster; in contrast, even though the majority

voting aggregates almost d times large number of answers,

since (d − 1)l answers are just random guesses, these an-

swers degrade the overall inference performance, especially

when d is large. On the other hand, for any q > 1/2,

the clustering-based algorithm requires much more number of queries (p−q)2+d(2q−1)2 ln αdc compared to that of majority voting ((2p−1)+(dd−2 1)(2q−1))2 ln α1c ≈ (2q−1 1)2 ln α1c , since the clustering-based algorithm does not utilize the (d−1)l answers

from unmatched clusters even though these answers can still

provide some useful information about the true task label when

q > 1/2. Motivated by this observation, in the next section

we propose two new algorithms, still based on clustering,

but that aggregates the answers from all the clusters with

proper weights. In particular, our second algorithm uses a

new clustering method based on semideﬁnite programming

(SDP) [17]–[20], which does not require the knowledge of

the reliability parameters p, q, and we also suggest estimators

pˆ, qˆ calculated from the clustering result, which then can be

used for weighted majority voting of workers’ answers.

IV. MAIN RESULTS
A. First Algorithm: When Parameters (p, q) are Known
We ﬁrst consider the case when (p, q) are known so that we can use the optimal weighted majority voting after the clustering step in Stage 1 of Algorithm [15]. With general q ∈ [1/2, p), for the optimal weighted majority voting Stage 2 of Algorithm [15] should be changed as below.

Algorithm 1 (for the known (p, q) case): This algorithm is composed of two stages. Stage 1 for worker clustering is

the same as that of Algorithm [15], which is summarized in Section III-B. Stage 2 is modiﬁed as below.

• Stage 2 (Type Matching and Weighted Majority Voting): For each task i ∈ T , ﬁnd a cluster of the matched type z∗(i)

by (10) and set the weights µij for answers mij , j ∈ Ni,

by

µij = 2p − 1, for j ∈ Vz∗(i),

(13)

2q − 1, for j ∈ Ni\Vz∗(i).

Estimate the label for the task i by the weighted majority voting (4) with weights (13) based on the worker clustering and the type matching.
Theorem 1: With Algorithm 1, for any αc ∈ (0, 1), when m ≥ cn3 for some constant c > 0, the recovery of task labels is guaranteed with the expected accuracy (3), with the number of queries per task

2d

6d + 3

LAlg1 = (p − q)2/2 + γu ln αc

(14)

where (2(2p − 1)(2q − 1) + (d − 2)(2q − 1)2)2
γu = 2((2p − 1)2 + (d − 1)(2q − 1)2) . (15)

Remark 2: Note that Algorithm 1 guarantees the recovery

condition (3) with a reduced number LAlg1 of queries per task

compared to that of Algorithm [15] in (12). Especially, the

gap increases as q(< p) increases. Compared to the required

number (8) of queries for majority voting, we can see that

the proposed algorithm requires the same order Θ ln αdc

(ignoring the ln d overhead) of queries when q > 1/2 and

d → ∞, while that of Algorithm [15] required Θ d ln αdc

queries per task.

Proof: With the two-stage algorithm, the workers are ﬁrst

clustered, and for a given task, the cluster of the matched

type is inferred. We ﬁrst analyze the clustering error. For any

two workers (a, b) having the same type, È(mia = mib|wa =

wb)

=

p2+(1−p)2 d

+ (d−1)(q2d+(1−q)2) ,

while

for

two

workers

of

different types,

È(mia

=

mib|wa

=

wb)

=

2(pq

+(1−p)(1−q d

))

+

(d−2)(q2+(1−q)2) . By setting ζ in (9) as the mean of the two

values, dwe can bound È( 1r i∈S ½(mia = mib) < ζ|wa =

wb) ≤ exp − 2(pd−2q)4 r and È( 1r i∈S ½(mia = mib) ≥

ζ|wa = wb) ≤ exp − 2(pd−2q)4 r by using Chernoff bound.

By union bound, the clustering error is then bounded by

È(Clustering error) ≤ n exp − 2(p − q)4 r . (16)

2

d2

We also need to guarantee that the number of workers per
type is at least l. Since the number of workers per type is distributed by Binomial(n, d1 ), by using the Chernoff bound and the union bound,

È(∪z∈[d]{|Vz| ≤ l}) ≤ d exp − 1 1 − ld 2 n . (17)

2

nd

Next, we bound the type matching error. Let Siz :=
j∈Ni∩Wz ½(mij = +1). Note that Siz is distributed by
Binomial(|Ni ∩ Wz|, p) if ti = z and ai = 1; Binomial(|Ni ∩ Wz|, q) if ti = z and ai = 1; Binomial(|Ni ∩ Wz|, 1 − p) if ti = z and ai = −1; and Binomial(|Ni ∩ Wz|, 1 − q) if ti = z and ai = −1. Therefore, if Siz is concentrated around its mean by 12 (p−q), then (10) provides the correctly matched type. By the union bound over z ∈ [d], the type matching error
is thus bounded above by

È(z∗(i) = ti) ≤ 2d exp − (p − q)2l .

(18)

2

We then analyze the label estimation error. When the clustering

is perfect but the type matching is wrong, the weight deﬁned

in (13) is not equal to the desired weight µij = 2fij − 1, and

the estimation error is bounded above by the case when the

weight is higher (µij = 2p − 1) for a cluster that is incorrectly matched to the task, and lower (µij = 2q − 1) for the cluster
having the same type as the task, i.e., È aˆWi MV = ai ≤
exp (−γul) where for γu in (15). On the other hand, when

the clustering and type matching is all correct, the estimation

error for aˆW i MV is equal to that of the oracle weighted majority voting, exp(−γml) for γm = (2p−1)2+(d2−1)(2q−1)2 . It can be shown that exp(−γml) ≤ exp(−((p − q)2/2 + γu)l). By

combining the above analysis, the expected fraction of label

errors E

1 m

m i=1

½(aˆi

=

ai)

is bounded above by

n

2(p − q)4

1

ld 2 n

2 exp − d2 r + d exp − 2 1 − n d

(p − q)2l

+ (2d + 1) exp − 2

· exp(−γul).

(19)

To limit the fraction of errors to αc, we can choose r = 2(pd−2q)4 ln 3n2(nα−c 1) , l = (p−q)21/2+γu ln 6dα+c 3 and n ≥

max 8d ln α3dc , (p−q)22d/2+γu ln 6dα+c 3 . The total number of

queries

per

task

is

1 m

(nr

+

ld(m

−

r))

≤

ld +

nmr ,

and

the

second term is dominated by the ﬁrst term when m ≥ cn3 for

some constant c > 0. Thus, the total number of queries per

task is bounded by LAlg1 in (14).

◦

B. Second Algorithm: When Parameters (p, q) are Unknown
In this section, we propose a new algorithm that does not require the knowledge of reliability parameters (p, q). For the purpose, we change both the clustering algorithm in Stage 1 and the weighted majority voting in Stage 2 of Algorithm 1.
Algorithm 2 (for the unknown (p, q) case):
• Stage 1 (Clustering Workers by Types): – Data preparation: after assigning each of |S| = r tasks to all n workers, construct a data matrix S ∈ {−1, 1}r×n, and deﬁne the similarity matrix A = ST S while zeroing out the diagonal term of A.

– Parameter estimation for within-cluster and cross-cluster

edge densities: compute and ﬁnd the largest two eigen-

values of A. Denote them by λ1 and λ2. Set pˆc =

λ1 +(d−1)λ2 n−d

and qˆc =

λ1

−λ2 n

.

– Clustering Based on SDP (Algorithm 1 in [20]): With a

tuning

parameter

λ

=

pˆc

+qˆc 2

,

solve

the

SDP

problem

max A − λ1n×n, X
X∈Rn×n

subject to X O; In, X = n;

(20)

0 ≤ Xij ≤ 1, ∀i, j ∈ [n].

Employ the approximate k-medoids clustering algorithm (Algorithm 1 in [21]) on the optimal solution Xˆ SDP of SDP to extract an explicit clustering, {V1, . . . , Vd}.

• Stage 2 (Type Matching and Weighted Majority Voting): for each task i ∈ T , ﬁnd the cluster of matched type z∗(i)
by (10).

– Randomly split each cluster: for each z ∈ [d], randomly split the workers in Vz into Vz(1) and Vz(2) with probability

β and 1 − β respectively, where β > 0 is a small enough probability. Let W(1) = ∪dz=1Vz(1), W(2) = ∪dz=1Vz(2), and (Vz(1))c = W(1)\Vz(1) for z ∈ [d]. – Estimate p and q: for z∗(i) in (10), deﬁne M(i) := Ni ∩ Vz(1∗)(i) and U (i) := Ni ∩ (Vz(1∗)(i))c, i.e, M(i) (U (i)) is the set of workers in W(1) who answered for the task i

and are believed to have the matched (unmatched) type.

Deﬁne

pˆ =

1 m

m i=1

pˆi

and

qˆ =

1 m

m i=1

qˆi

where



½(mij = 1)

½(mij = −1) 





pˆi = max  j∈M(i|)M(i)| , j∈M(i)|M(i)|  ,











½(mij = 1)

½(mij = −1) 





qˆi = max  j∈U(i)|U (i)| , j∈U(i) |U (i)|  .









– Set the weights µij as in (13) by replacing p by pˆ and q by qˆ, and estimate the label for the task i by the weighted
majority voting aˆWi MV = sign j∈Ni∩W(2) µij mij .

Remark 3: We remark that Algorithm 2 does not require any prior information about reliability parameters (p, q) nor

the task/worker types. Stage 1 of Algorithm 2 clusters workers

by applying SDP to the similarity matrix with the tuning parameter λ chosen from the data, and Stage 2 of Algorithm 2

ﬁrst ﬁnds a matched cluster and uses this information to obtain the estimates (pˆ, qˆ) of the model parameters (p, q), which then

can be used for the weighted majority voting.

The performance of the clustering algorithm is guaranteed

by the lemma below.

Lemma 2: Suppose the tuning parameter λ in the SDP (20)

obeys the bound 14 rpm + 34 rpu ≤ λ ≤ 34 rpm + 14 rpu where pm := ((2p − 1)2 + (d − 1)(2q − 1)2)/d and pu := (2(2p − 1)(2q − 1) + (d − 2)(2q − 1)2)/d. Then, there is a universal

constant c1 > 0 such that Stage 1 of Algorithm 2 achieves

the strong consistency with probability at least 1 − 4n−1 if

r

≥

c1

d2(ln n)2 (p −p )2

.

mu

Mean of Error Fraction of Inferred Tasks

m = 1e+05, n = 60, p = 0.98, q = 0.50, d = 3 0.08
Majority Voting

0.07

Oracle Weighted Majority Voting Algorithm [15]

0.06

Algorithm 1 (p,q are known) Algorithm 2 (p,q are unknown)

0.05

0.04

0.03

0.02

0.01

0

18

20

22

24

26

28

Total Queries

Mean of Error Fraction of Inferred Tasks

10-4 2.5
m = 1e+05, n = 60, p = 0.98, q = 0.80, d = 3

2

Majority Voting

Oracle Weighted Majority Voting

Algorithm [15]

1.5

Algorithm 1 (p,q are known)

Algorithm 2 (p,q are unknown)

1

0.5

0

18

20

22

24

26

28

Total Queries

Mean of Error Fraction of Inferred Tasks

Fig. 1. Comparisons of label recovery accuracy for ﬁve different algorithms.

0.06 0.05 0.04 0.03 0.02

m = 2e+04, n = 50, p = 0.95, q = 0.55, d = 4
MV Oracle WMV EM [4] Variational [6] KOS [8] Ratio-Eigen [9] SpecEM [10] Proposed (Alg.2)

0.01

0

25

30

35

40

45

50

Total Queries

Fig. 2. Comparison of the proposed algorithm with the state-of-the-art algorithms designed for the single-coin Dawid-Skene model.

V. NUMERICAL RESULTS
We provide simulation results to show that the proposed algorithms outperform other baselines in diverse parameter regimes. In Fig. 1, we compare our algorithm (Alg.1 and 2) with majority voting, oracle weighted majority voting, and Alg. [15] in terms of the error fraction in inferred tasks over the number of queries per task when d = 3. The result is averaged over 30 times Monte Carlo simulations. When q = 1/2 (left ﬁgure), Alg. 1 becomes the same as Alg. [15] and these algorithms outperform the majority voting. We can observe that Alg. 2, which uses the estimates (pˆ, qˆ), achieves as good performance as that of Alg. 1. When q > 1/2 (right ﬁgure), our algorithms show the best performance.
In Fig. 2, the performance of the proposed algorithm is compared with that of the state-of-the-art algorithms developed for the single-coin Dawid-Skene model, which assumes that the worker reliability does not change depending on the task type. The state-of-the-art algorithms perform worse than the proposed algorithm when the data is collected assuming the worker-type specialization model, which may reﬂect more realistic scenarios in diverse crowdsourcing applications.

VI. CONCLUSIONS
We considered crowdsourced labeling under a d-type specialization model with general reliability parameters p > q ≥ 1/2. When (p, q) values are known but not the types of tasks/workers, our proposed algorithm (Alg. 1) recovers binary tasks up to any given accuracy (1 − αc) ∈ (0, 1) with the number of queries per task scales as Θ(d ln αdc ) when q = 1/2 and as Θ(ln αdc ) when q > 1/2. We also

proposed an algorithm (Alg. 2) that does not require any
information about reliability parameters nor the task/worker
types, and empirically showed that this algorithm achieves as
good performance as the algorithm with the known reliability parameters (p, q).
REFERENCES
[1] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy, “Learning from crowds,” Journal of Machine Learning Research, vol. 11, no. Apr, pp. 1297–1322, 2010.
[2] L. Von Ahn, B. Maurer, C. McMillen, D. Abraham, and M. Blum, “reCAPTCHA: Human-based character recognition via web security measures,” Science, vol. 321, no. 5895, pp. 1465–1468, 2008.
[3] P. Welinder, S. Branson, P. Perona, and S. J. Belongie, “The multidimensional wisdom of crowds,” in Advances in neural information processing systems, 2010, pp. 2424–2432.
[4] A. P. Dawid and A. M. Skene, “Maximum likelihood estimation of observer error-rates using the em algorithm,” Journal of the Royal Statistical Society: Series C (Applied Statistics), vol. 28, no. 1, pp. 20– 28, 1979.
[5] C. Gao and D. Zhou, “Minimax optimal convergence rates for estimating ground truth from crowdsourced labels,” arXiv preprint arXiv:1310.5764, 2013.
[6] Q. Liu, J. Peng, and A. T. Ihler, “Variational inference for crowdsourcing,” Advances in neural information processing systems, vol. 25, pp. 692–700, 2012.
[7] D. Zhou, J. C. Platt, S. Basu, and Y. Mao, “Learning from the wisdom of crowds by minimax entropy,” 2012.
[8] D. R. Karger, S. Oh, and D. Shah, “Budget-optimal task allocation for reliable crowdsourcing systems,” Operations Research, vol. 62, no. 1, pp. 1–24, 2014.
[9] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi, “Aggregating crowdsourced binary ratings,” in Proceedings of the 22nd international conference on World Wide Web, 2013, pp. 285–294.
[10] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan, “Spectral methods meet EM: A provably optimal algorithm for crowdsourcing,” in Advances in neural information processing systems, 2014, pp. 1260–1268.
[11] H. W. Chung, J. O. Lee, and A. O. Hero, “Fundamental limits on data acquisition: Trade-offs between sample complexity and query difﬁculty,” in 2018 IEEE International Symposium on Information Theory (ISIT), 2018, pp. 681–685.
[12] D. Kim and H. W. Chung, “Crowdsourced classiﬁcation with xor queries: An algorithm with optimal sample complexity,” in 2020 IEEE International Symposium on Information Theory (ISIT), 2020, pp. 2551– 2555.
[13] D. Kim and H. W. Chung, “Binary classiﬁcation with xor queries: Fundamental limits and an efﬁcient algorithm,” IEEE Transactions on Information Theory, pp. 1–1, 2021.
[14] N. B. Shah, S. Balakrishnan, and M. J. Wainwright, “A permutationbased model for crowd labeling: Optimal estimation and robustness,” IEEE Transactions on Information Theory, 2020.
[15] D. Shah and C. Lee, “Reducing crowdsourcing to graphon estimation, statistically,” in International Conference on Artiﬁcial Intelligence and Statistics, 2018, pp. 1741–1750.
[16] H. Li and B. Yu, “Error rate bounds and iterative weighted majority voting for crowdsourcing,” arXiv preprint arXiv:1411.4086, 2014.
[17] B. P. Ames, “Guaranteed clustering and biclustering via semideﬁnite programming,” Mathematical Programming, vol. 147, no. 1, pp. 429– 465, 2014.
[18] B. Hajek, Y. Wu, and J. Xu, “Achieving exact cluster recovery threshold via semideﬁnite programming,” IEEE Transactions on Information Theory, vol. 62, no. 5, pp. 2788–2797, 2016.
[19] R. K. Vinayak and B. Hassibi, “Similarity clustering in the presence of outliers: Exact recovery via convex program,” in 2016 IEEE International Symposium on Information Theory (ISIT). IEEE, 2016, pp. 91–95.
[20] J. Lee, D. Kim, and H. W. Chung, “Robust hypergraph clustering via convex relaxation of truncated MLE,” IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 3, pp. 613–631, 2020.
[21] Y. Fei and Y. Chen, “Exponential error rates of SDP for block models: Beyond grothendieck’s inequality,” IEEE Transactions on Information Theory, vol. 65, no. 1, pp. 551–571, 2018.

