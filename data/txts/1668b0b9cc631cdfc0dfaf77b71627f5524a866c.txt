An Accelerated Directional Derivative Method for Smooth Stochastic Convex Optimization

Pavel Dvurechensky

Eduard Gorbunov August 20, 2020

Alexander Gasnikov ∗

arXiv:1804.02394v2 [math.OC] 21 Sep 2020

Abstract
We consider smooth stochastic convex optimization problems in the context of algorithms which are based on directional derivatives of the objective function. This context can be considered as an intermediate one between derivative-free optimization and gradient-based optimization. We assume that at any given point and for any given direction, a stochastic approximation for the directional derivative of the objective function at this point and in this direction is available with some additive noise. The noise is assumed to be of an unknown nature, but bounded in the absolute value. We underline that we consider directional derivatives in any direction, as opposed to coordinate descent methods which use only derivatives in coordinate directions. For this setting, we propose a non-accelerated and an accelerated directional derivative method and provide their complexity bounds. Our non-accelerated algorithm has a complexity bound which is similar to the gradient-based algorithm, that is, without any dimension-dependent factor. Our accelerated algorithm has a complexity bound which coincides with the complexity bound of the accelerated gradient-based algorithm up to a factor of square root of the problem dimension. We extend these results to strongly convex problems.
1 Introduction
Zero-order or derivative-free optimization considers problems of minimization of a function using only, possibly noisy, observations of its values. This area of optimization has a long history, starting as early as in 1960 [64, 34], see also [17, 67, 23]. Even an older area of optimization, which started in 19th century [19], considers ﬁrst-order methods which use the information about the gradient of the objective function. In this paper, we choose an intermediate class of problems. Namely, we assume that at any given point and for any given direction, a noisy stochastic approximation for the directional derivative of the objective function at this point in this direction is available. We underline that we consider directional derivatives in any direction, as opposed to coordinate descent methods which rely only on derivatives in coordinate directions. We refer to the class of optimization
∗This paper was published in European Journal of Operational Research (DOI: https://doi.org/10.1016/j. ejor.2020.08.027). P. Dvurechensky (pavel.dvurechensky@wias-berlin.de) is with Weierstrass Institute for Applied Analysis and Stochastics and Institute for Information Transmission Problems RAS. E. Gorbunov (eduard.gorbunov@phystech.edu, eduardgorbunov.github.io) is with Moscow Institute of Physics and Technology and National Research University Higher School of Economics. A. Gasnikov (gasnikov@yandex.ru) is with Moscow Institute of Physics and Technology, National Research University Higher School of Economics and Institute for Information Transmission Problems RAS
1

methods, which use directional derivatives of the objective function, as directional derivative methods. Unlike well developed areas of derivative-free and ﬁrst-order stochastic optimization methods, the area of directional derivative optimization methods for stochastic optimization problems is not suﬃciently covered in the literature. This class of optimization methods can be motivated by at least three situations.
The ﬁrst one is connected to Automatic Diﬀerentiation [71]. Assume that the objective function is given as a computer program, which performs elementary arithmetic operations and elementary functions evaluations. Automatic Diﬀerentiation allows to calculate the gradient of this objective function and the additional computational cost is no more than ﬁve times larger than the cost of the evaluation of the objective value. The drawback of this approach is that it requires to store in memory the result of all the intermediate operations, which can require large memory amount. On the contrary, calculation of the directional derivative is easier than the calculation of the full gradient and requires the same memory amount as the calculation of the value of the objective [49]. Since a random vector can be a part of the program input or some randomness can be used during the program execution, stochastic optimization problems can also be considered.
Importantly, automatic calculation of the directional derivative does not require the objective function to be smooth. This fact motivates the study of directional derivative methods in connection to Deep Learning. Indeed, learning problem is often stated as a problem of minimization of a loss function. A non-smooth activation function, called rectiﬁer, is frequently used in Deep Learning as a building block for the loss function. Formally speaking, this non-smoothness does not allow to use Automatic Diﬀerentiation in the form of backpropagation to calculate the gradient of the objective function. At the same time, directional derivatives can be calculated by properly modiﬁed backpropagation.
The second motivating situation is connected to quasi-variational inequalities, which are used in modelling of diﬀerent phenomena, such as sandpile formation and growth [63], determination of lakes and river networks [6], and superconductivity [5]. It happens that directional derivatives can be calculated for such problems [54] as a solution to some auxiliary problem. Since this subproblem can not always be solved exactly, the noise in the directional derivative naturally arises. If the considered physical phenomenon takes place in some random media, stochastic optimization can be a natural approach to use.
The third motivating situation is connected to derivative-free stochastic optimization. In this situation a gradient approximation, based on the diﬀerence of stochastic approximations for the values of the objective in two close points, can be considered as a noisy directional derivative in the direction given by the diﬀerence of these two points [33]. In this case, derivative-free stochastic optimization can be considered as a particular case of directional derivative stochastic optimization.
Motivated by potential presence of non-stochastic noise in the problem, we assume that the noise in the directional derivative consists of two parts. Similar to stochastic optimization problems, the ﬁrst part is of a stochastic nature. On the opposite, the second part is an additive noise of an unknown nature, but bounded in the absolute value. More precisely, we consider the following optimization problem

min f (x) := Eξ[F (x, ξ)] = F (x, ξ)dP (x) ,

(1)

x∈Rn

X

where ξ is a random vector with probability distribution P (ξ), ξ ∈ X , and for P -almost every ξ ∈ X , the function F (x, ξ) is closed and convex. Moreover, we assume that, for P almost every

2

ξ, the function F (x, ξ) has gradient g(x, ξ), which is L(ξ)-Lipschitz continuous with respect to the

Euclidean norm and there exists L2 0 such that EξL(ξ)2 L2 < +∞. Under this assumptions,

Eξg(x, ξ) = ∇f (x) and f has L2-Lipschitz continuous gradient with respect to the Euclidean norm.

Also we assume that

Eξ[ g(x, ξ) − ∇f (x) 22] σ2,

(2)

where · 2 is the Euclidean norm. Finally, we assume that an optimization procedure, given a point x ∈ Rn, direction e ∈ S2(1)
and ξ independently drawn from P , can obtain a noisy stochastic approximation f (x, ξ, e) for the
directional derivative g(x, ξ), e :

f (x, ξ, e) = g(x, ξ), e + ζ(x, ξ, e) + η(x, ξ, e),

Eξ(ζ(x, ξ, e))2 ∆ζ , ∀x ∈ Rn, ∀e ∈ S2(1),

|η(x, ξ, e)| ∆η, ∀x ∈ Rn, ∀e ∈ S2(1), a.s. in ξ,

(3)

where S2(1) is the Euclidean sphere or radius one with the center at the point zero and the values ∆ζ, ∆η are controlled and can be made as small as it is desired. Note that we use the smoothness of F (·, ξ) to write the directional derivative as g(x, ξ), e , but we do not assume that the whole stochastic gradient g(x, ξ) is available.
It is well-known [50, 25, 31, 36] that, if the stochastic approximation g(x, ξ) for the gradient of f is available, an accelerated gradient method has complexity bound O max L2/ε, σ2/ε2 ,
where ε is the target optimization error. The question, to which we give a positive answer in this paper, is as follows.
Is it possible to solve a smooth stochastic optimization problem with the same ε-dependence in the complexity and only noisy observations of the directional derivative?

1.1 Related work
We ﬁrst consider the related work on directional derivative optimization methods and, then, a closely related class of derivative-free methods with two-point feedback, the latter meaning that an optimization method uses two function value evaluations on each iteration. Since all the considered methods are randomized, we compare oracle complexity bounds in terms of expectation, that is, a number of directional derivatives or function values evaluations which is suﬃcient to achieve an error ε in the expected optimization error Ef (xˆ) − f ∗, where xˆ is the output of an algorithm and f ∗ is the optimal value of f .

1.1.1 Directional derivative methods
Deterministic smooth optimization problems. In [60], the authors consider the Euclidean case and propose a non-accelerated and an accelerated directional derivative method for smooth convex problems with complexity bounds O(nL2/ε) and O(n L2/ε) respectively. Also they propose a non-accelerated and an accelerated method for problems with µ-strongly convex objective and prove complexity bounds O(nL2/µ log2(1/ε)) and O(n L2/µ log2(1/ε)) respectively. For a more general case of problems with additional bounded noise in directional derivatives, but also for the Euclidean case, an accelerated directional derivative method was proposed in [33] and a bound O(n L2/ε) was proved.

3

We also should mention coordinate descent methods. In the seminal paper [57], a random coordinate descent for smooth convex and µ-strongly convex optimization problems were proposed and O(L/ε) and O(L/µ log2(1/ε)) complexity bounds were proved, where L is an eﬀective Lipschitz constant of the gradient varying from n to some average over coordinates coordinate-wise Lipschitz constant. In the same paper, an accelerated version of random coordinate descent was proposed for convex problems and O(n L/ε) complexity bound was proved. Papers [52, 35, 53, 65] generalize accelerated random coordinate descent for diﬀerent settings, including µ-strongly convex problems, and [61, 3, 41] provide a O( L/ε) and O( L/µ log2(1/ε)) complexity bounds, where L is an eﬀective Lipschitz constant of the gradient varying from n to some average over coordinates coordinate-wise Lipschitz constant, and, in the best case, is dimension-independent. An accelerated random coordinate descent with inexact coordinate-wise derivatives was proposed in [33] with O(n L/ε) complexity bound and also a uniﬁed view on directional derivative methods, coordinate descent and derivative-free methods.
Stochastic optimization problems. A directional derivative method for non-smooth stochastic convex optimization problems was introduced in [60] with a complexity bound O(n2/ε2). A random coordinate descent method for non-smooth stochastic convex and µ- strongly convex optimization problems were introduced in [24] with complexity bounds O(n/ε2) and O(n/µε) respectively.

1.1.2 Derivative-free methods

Deterministic smooth optimization problems. A non-accelerated and an accelerated derivative-

free method for this type of problems were proposed in [60] for the Euclidean case with the bounds

O(nL2/ε) and O(n L2/ε) respectively. The same paper proposed a non-accelerated and an accel-

erated method for µ-strongly convex problems with complexity bounds O(nL2/µ log2(1/ε)) and

O(n L2/µ log2(1/ε)) respectively. A non-accelerated derivative-free method for deterministic

problems with additional bounded noise in function values was proposed in [15] together with

O(nL2/ε) bound and application to learning parameter of a parametric PageRank model, see also

[38, 37]. Deterministic problems with additional bounded noise in function values were also con-

sidered in [33], where several accelerated derivative-free methods, including Derivative-Free Block-

Coordinate Descent, were proposed and a bound O(n L/ε) was proved, where L depends on the

method and, in some sense, characterizes the average over blocks of coordinates Lipschitz constant

of the derivative in the block. Mixed ﬁrst-order/zero-order setting is considered in [13]. After our

paper appeared as a preprint, the papers [10, 16] studied derivative-free quasi-Newton methods

for problems with noisy function values, and the paper [11] reported theoretical and empirical

comparison of diﬀerent gradient approximations for zero-order methods.

Stochastic optimization problems. Most of the authors in this group solve a more general

problem of bandit convex optimization and obtain bounds on the so-called regret. It is well known

[20] that a bound on the regret can be converted to a bound on the expected optimization error.

Non-smooth stochastic optimization problems were considered in [60], where an O(n2/ε2) complex-

ity bound was proved for a derivative-free method. This bound was improved by [26, 40, 39, 66, 8, 46]

to1

O(n2/q Rp2 /ε2 ),

where

p

∈

{1, 2},

1 p

+

1 q

=

1

and

Rp

is

the

radius

of

the

feasible

set

in

the

p-

norm · p. For non-smooth µp-strongly convex w.r.t. to p-norm problems, the authors of [39, 8]

proved a bound O(n2/q/(µpε)). A version of these methods for non-smooth saddle-point problems

1O hides polylogarithmic factors (ln n)c, c > 0.

4

is developed in [14]. Intermediate, partially smooth problems with a restrictive assumption of boundedness of E g(x, ξ) 2,
were considered in [26], where it was proved that a proper modiﬁcation of Mirror Descent algorithm with derivative-free approximation of the gradient gives a bound O(n2/qRp2/ε2) for convex problems, improving upon the bound O(n2/ε2) of [1]. For strongly convex w.r.t 2-norm problems, the authors of [1] obtained a bound O(n2/ε), which was later extended for µp-strongly convex problems and improved to O(n2/q/(µpε)) in [39].
In the fully smooth case, without the assumption that E g(x, ξ) 2 < +∞, papers [43, 42] proposed a derivative-free algorithm for the Euclidean case with the bound
nL2R2 nσ2 O max ε , ε2 .

In [45], the authors proposed a non-accelerated and an accelerated derivative-free method with the

bounds

 2

2



 n q L2Rp2 n q σ2Rp2 

O max

ε , ε2  ,





 O max n 12 + 1q


2



L2Rp2 n q σ2Rp2 

, ε

ε2





respectively, where Rp characterizes the distance in p-norm between the starting point of the algorithm and a solution to (1), p ∈ {1, 2} and q ∈ {2, ∞} is the conjugate to p, given by the identity p1 + 1q = 1.
The authors of [21] combine accelerated derivative-free optimization with accelerated variance
reduction technique for ﬁnite-sum convex problems in the Euclidean setup.
Other works. For a recent review of derivative-free optimization see [51] and for a review of
stochastic optimization, including derivative-free optimization, see [62].

1.2 Our contributions
As we have seen above, only two results on directional derivative methods for non-smooth stochastic convex optimization are available in the literature, and, to the best of our knowledge, nothing is known about directional derivative methods for smooth stochastic convex optimization, even in the well-developed area of random coordinate descent methods. Our main contribution consists in closing this gap in the theory of directional derivative methods for stochastic optimization and considering even more general setting with additional noise of an unknown nature in the directional derivative.
Our methods are based on two proximal setups [9] characterized by the value2 p ∈ {1, 2} and its conjugate q ∈ {2, ∞}, given by the identity p1 + 1q = 1. The case p = 1 corresponds to the choice of 1-norm in Rn and corresponding prox-function which is strongly convex with respect to this norm (we provide the details below). The case p = 2 corresponds to the choice of the Euclidean 2-norm in Rn and squared Euclidean norm as the prox-function. As our main contribution, we propose an Accelerated Randomized Directional Derivative (ARDD) algorithm for smooth stochastic optimization based on noisy observations of directional derivative of the
2Strictly speaking, we are able to consider all the intermediate cases p ∈ [1, 2], but we are not aware of any proximal setup which is compatible with p ∈/ {1, 2}

5

objective. Our method has the complexity bound



2



 1 + 1 L2Rp2 n q σ2Rp2 

O max n 2 q

, ε

ε2

,

(4)





where Rp characterizes the distance in p-norm between the starting point of the algorithm and a solution to (1).
As our second contribution, we propose a non-accelerated Randomized Directional Derivative (RDD) algorithm with the complexity bound

 2

2



 n q L2Rp2 n q σ2Rp2 

O max

ε , ε2  .

(5)





Interestingly, for this method when p = 1 and q = ∞, we obtain complexity bound which depends on

the dimension n only logarithmically despite we use only noisy directional derivative observations.

Let us comment on the comparison between the accelerated and non-accelerated method. In the

regime of small variance σ2 in both bounds the dominating term is the ﬁrst one. If p = 1, q = ∞

and L2Rp2 < nε, then the bound for the non-accelerated method is smaller than that of for the

accelerated. In this regime it is preferred to use the non-accelerated method.

Note √that, in the case of (1) having a sparse solution, our bounds for p = 1 allow to gain a factor of n in the complexity of the accelerated method and a factor of n in the complexity of the

non-accelerated method in comparison to the Euclidean case p = 2. Indeed, sparsity of a solution

x∗ means that

x∗ 1 = O(1) ·

x∗ 2 and, if the starting point is zero, we obtain R12 =

x∗

2 1

=

O(1) ·

x∗

2 2

=

O(1)R22.

Hence,

the

bounds

for

p

=

1

and

p

=

2

can

be

compared

only

based

on

the

corresponding powers of n, the latter being smaller for the case p = 1, q = ∞.

We underline here that our methods are based on random directions drawn from the uniform

distribution on the unit Euclidean sphere and our results for p = 1 can not be obtained by random

coordinate descent.

As our third contribution, we extend the above results to the case when the objective function is

additionally known to be µp-strongly convex w.r.t. p-norm. For this case, we propose an accelerated

and a non-accelerated algorithm which respectively have complexity bounds

1 + 1 L2

µpRp2

n

2 q

σ

2

2
n q L2

µpRp2

n

2 q

σ

2

O max n 2 q

µp log2

, ε µpε

, O max

µp log2

, ε µpε

. (6)

In the regime of small variance σ2 in both bounds the dominating term is the ﬁrst one. If p = 1, q = ∞ and Lµp2 < n, then the bound for the non-accelerated method is smaller than that of for the accelerated. In this regime of relatively well-conditioned problems it is preferred to use the non-accelerated method.
As our ﬁnal contribution, we consider derivative-free smooth stochastic convex optimization with inexact values of the stochastic approximations for the function values as a particular case of optimization using noisy directional derivatives. This allows us to obtain the complexity bounds of [45] as a straightforward corollary of our results in this paper. At the same time we obtain new complexity bounds for the strongly convex case which, to the best of our knowledge, were not known in the literature.

6

Note that our results for accelerated and non-accelerated methods are somewhat similar to the ﬁnite-sum minimization problems of the form
m
min fi(x),
x∈Rn i=1
where fi are convex smooth functions. For such problems accelerated methods have complexity O(m + mL/ε) and non-accelerated methods have complexity O(m + L/ε) (see, e.g. [2] for a nice review on the topic)√. As we see, acceleration allows to take the square root of the second term but for the price of m and the two bounds can not be directly compared without additional assumptions on the value of mε.
Special note on [45, 70]. One of the novelties and insights in the approach of this paper in comparison to [45, 70] is to realize that gradient-free methods are a particular case of directional derivative methods with inexact oracle. Unlike these papers, in the current paper we need to account for two types of inexactness. One is stochastic with bounded second moment and the second is bounded a.s. This is a more complicated assumption than the one in [45, 70] and we have to assume that the error values can be controlled, unlike [45, 70]. Moreover, since the oracle returns diﬀerent information, we have to construct our stochastic approximation of the gradient diﬀerently, which also changes the proof technique. We also analyze in this paper the case of strongly convex objective values, which was not done in [45, 70].
1.3 Paper organization
The rest of the paper is organized as follows. In Section 2, both for convex and strongly convex problems, we introduce our algorithms, state their convergence rate theorems and corresponding complexity bounds. Section 3 is devoted to proof of the convergence rate theorem for our accelerated method and convex objective functions. Section 4 is devoted to proof of the convergence rate theorem for our non-accelerated method and convex objective functions. In Section 5 we provide the proofs for the case of strongly convex objective function. Finally, in Section 6 we provide numerical experiments with two types of objective functions: worst case functions for ﬁrst-order methods [56] and least squares problem.

2 Algorithms and main results
In this section, we provide our non-accelerated and accelerated directional derivative methods both for convex and strongly convex problems together with convergence theorems and corresponding complexity bounds. The proofs are rather technical and postponed to next sections.

2.1 Preliminaries

We start by introducing necessary objects and technical results. Proximal setup. Let p ∈ [1, 2] and x p be the p-norm in Rn deﬁned as

n

x

p p

=

|xi|p,

i=1

x ∈ Rn,

7

· q be its dual, deﬁned by g q = max g, x , x p ≤ 1 , where q ∈ [2, ∞] is the conjugate
x
number to p, given by p1 + 1q = 1, and, for q = ∞, by deﬁnition x ∞ = max |xi|.
i=1,...,n
We choose a prox-function d(x) which is continuous, convex on Rn and is 1-strongly convex on
Rn with respect to · p, i.e., for any x, y ∈ Rn d(y)−d(x)− ∇d(x), y−x ≥ 12 y−x 2p. Without loss of generality, we assume that min d(x) = 0. We deﬁne also the corresponding Bregman divergence
x∈Rn
V [z](x) = d(x) − d(z) − ∇d(z), x − z , x, z ∈ Rn. Note that, by the strong convexity of d,

V [z](x) ≥ 21 x − z 2p, x, z ∈ Rn. (7) For the case p = 1, we choose the following prox-function [9]

en(κ−1)(2−κ)/κ ln n 2

1

d(x) = 2

x κ,

κ=1+ ln n

(8)

and, for the case p = 2, we choose the prox-function to be the squared Euclidean norm

d(x) = 21 x 22. (9)
Main technical lemma. In our proofs of complexity bounds, we rely on the following lemma. The proof is rather technical and is provided in the appendix.

Lemma 2.1. Let e ∈ RS2(1), i.e be a random vector uniformly distributed on the surface of

the unit Euclidean sphere in Rn, p ∈ [1, 2] and q be given by p1 + 1q = 1. Then, for n 8 and

ρn

=

min{q

−

1,

16

ln

n

−

8}

n

2 q

−1

,

Ee

e

2 q

≤

ρn,

(10)

Ee s, e 2 e 2q ≤ 6nρn s 22, ∀s ∈ Rn. (11)

Stochastic approximation of the gradient. Based on the noisy stochastic observations (3) of the directional derivative, we form the following stochastic approximation of ∇f (x)
∇mf (x) = 1 m f (x, ξi, e)e, (12) m
i=1
where e ∈ RS2(1), ξi, i = 1, ..., m are independent realizations of ξ, m is the batch size.

2.2 Algorithms and main results for convex problems
Our Accelerated Randomized Directional Derivative (ARDD) method is listed as Algorithm 1.

Theorem 2.2. Let ARDD method be applied to solve problem (1). Then

E[f (yN )] − f (x∗)

384ΘpNn22ρnL2 + n4LN

·

σ2 m

+

61N 24L

∆ζ

+

122N 3L

∆2η

√

√

2

√2

22

+ 12 N22nΘp 2∆ζ + 2∆η + 12nNρn2 L2 2∆ζ + 2∆η ,

(13)

8

Algorithm 1 Accelerated Randomized Directional Derivative (ARDD) method

Input: x0 —starting point; N 1 — number of iterations; m 1 — batch size. Output: point yN .
1: y0 ← x0, z0 ← x0. 2: for k = 0, . . . , N − 1. do 3: αk+1 ← 96nk2+ρ2nL2 , τk ← 48αk+11n2ρnL2 = k+2 2 . 4: Generate ek+1 ∈ RS2(1) independently from previous iterations and ξi, i = 1, ..., m – inde-
pendent realizations of ξ.

5: Calculate

∇mf (xk+1) = 1 m f (xk+1, ξi, e)e. m
i=1

6: xk+1 ← τkzk + (1 − τk)yk. 7: yk+1 ← xk+1 − 2L12 ∇mf (xk+1).

8: zk+1 ← argmin
z∈Rn
9: end for

αk+1n

∇mf (xk+1), z − zk

10: return yN

+ V [zk] (z) .

where Θp = V [z0](x∗) is deﬁned by the chosen proximal setup and E[·] = Ee1,...,eN ,ξ1,1,...,ξN,m [·].
Before we proceed to the non-accelerated method, we give the appropriate choice of the ARDD method parameters N , m, and accuracy of the directional derivative evaluation ∆ζ, ∆η. These values are chosen such that the r.h.s. of (13) is smaller than ε. For simplicity we omit numerical constants and summarize the obtained values of the algorithm parameters in Table 1 below. The last row represents the total number N m of oracle calls, that is, the number of directional derivative evaluations, which was advertised in (4). Note that the bound (13) allows also to choose the accuracy of the directional derivative evaluation ∆ζ, ∆η decreasing with N . This is done by making each term with ∆ζ or ∆η in the r.h.s. to be of the same order as the ﬁrst term.

p=1

N m ∆ζ ∆η O-le calls

O n ln nεL2Θ1

O max 1, lnnn · εσ3/22 · ΘL21

O min n(ln n)2L22Θ1, nεΘ21 , √nε 23ln n · ΘL21

O

min

√ n ln nL

√ Θ

,

√ε

3
, √ε4

· 4 L2

2

1 nΘ1 4 n ln n

Θ1

O max

, n ln nL2Θ1 σ2Θ1 ln n

ε

ε2

p=2

O n2L2Θ2
ε

O max 1, εσ3/22 · ΘL22

O min n3L22Θ2, nεΘ22 , εn32 · ΘL22 O min n 32 L2√Θ2, √nεΘ2 , √ε 34n · 4 ΘL22

O max

, n2L2Θ2 σ2Θ2n

ε

ε2

Table 1: Algorithm 1 parameters for the cases p = 1 and p = 2.

Our Randomized Directional Derivative (RDD) method is listed as Algorithm 2.

9

Algorithm 2 Randomized Directional Derivative (RDD) method

Input: x0 —starting point; N 1 — number of iterations; m 1 — batch size.
Output: point x¯N .
1: for k = 0, . . . , N − 1. do 2: α ← 48nρ1nL2 . 3: Generate ek+1 ∈ RS2 (1) independently from previous iterations and ξi, i = 1, ..., m – inde-
pendent realizations of ξ.

4: Calculate

∇mf (xk) = 1 m f (xk, ξi, e)e. m
i=1

5: xk+1 ← argmin αn ∇mf (xk), x − xk
x∈Rn

6: end for

N −1

7:

return

x¯N

←

1 N

xk

k=0

+ V [xk] (x) .

Theorem 2.3. Let RDD method be applied to solve problem (1). Then

E[f (x¯N )] − f (x∗)

√ 384nρNnL2Θp + L22 σm2 + 12nL2 ∆ζ + 34Ln2 ∆2η + 8 N2nΘp

√ 2∆ζ + 2∆η

+ 3LN2ρn

√

2

2∆ζ + 2∆η ,

(14)

where Θp = V [z0](x∗) is deﬁned by the chosen proximal setup and E[·] = Ee1,...,eN ,ξ1,1,...,ξN,m [·].

Before we proceed, we give the appropriate choice of the RDD method parameters N , m, and accuracy of the directional derivative evaluation ∆ζ, ∆η. These values are chosen such that the r.h.s. of (14) is smaller than ε. For simplicity we omit numerical constants and summarize the obtained values of the algorithm parameters in Table 2 below. The last row represents the total number N m of oracle calls, that is, the number of directional derivative evaluations, which was advertised in (5). Note that the bound (14) allows also to choose the accuracy of the directional derivative evaluation ∆ζ, ∆η decreasing with N . This is done by making each term with ∆ζ or ∆η in the r.h.s. to be of the same order as the ﬁrst term.

2.3 Extensions for strongly convex problems

In this subsection, we assume additionally that f is µp-strongly convex w.r.t. p-norm. Our algo-

rithms and proofs rely on the following fact. Let x∗ be some ﬁxed point and x be a random point

such that Ex

x − x∗

2 p

Rp2, then

Exd x − x∗ Ωp , (15)

Rp

2

where Ex denotes the expectation with respect to random vector x and Ωp is deﬁned as follows. For p = 1 and our choice of the prox-function (8), Ωp = en(κ−1)(2−κ)/κ ln n = O(ln n) for our choice

10

p=1

p=2

N m ∆ζ ∆η O-le calls

O L2Θε1 ln n

O max 1, εσL22 O min (lnnn)2 L22Θ1, nεΘ21 , εLn2 O min l√nnn L2√Θ1, √nεΘ1 , εLn2

O max L2Θ1 ln n , σ2Θ1 ln n

ε

ε2

O nL2εΘ2 O max 1, εσL22 O min nL22Θ2, nεΘ22 , εLn2 O min √nL2√Θ2, √nεΘ2 , εLn2 O max nL2εΘ2 , nσε22Θ2

Table 2: Algorithm 2 parameters for the cases p = 1 and p = 2.

of κ = 1 + ln1n , see [55, 47]. For p = 2 and our choice of the prox-function (9), Ωp = 1. Our Accelerated Randomized Directional Derivative method for strongly convex problems (ARDDsc)
is listed as Algorithm 3.

Theorem 2.4. Let f in problem (1) be µp-strongly convex and ARDDsc method be applied to

solve this problem. Then

Ef (uK ) − f ∗

µpRp2 · 2−K + 2∆.
2

(18)

√ where ∆ = 6214NL20 ∆ζ + 1232LN2 0 ∆2η + 12 2Nn02Rp2Ωp

√ 2∆ζ + 2∆η

+ 12nNρ0n2 L2

√

2

2∆ζ + 2∆η . Moreover,

under an appropriate choice of ∆ζ and ∆η s.t. 2∆ ε/2, the oracle complexity to achieve

ε-accuracy of the solution is

1 + 1 L2Ωp

µpRp2

n

2 q

σ

2

Ω

p

O max n 2 q

µp log2

, ε

µpε

.

Despite we have linear convergence in terms of the iterations number, the number of the oracle evaluations corresponds to sublinear convergence. The reason is that we consider general stochastic optimization problem, rather than ﬁnite-sum problems for which the linear convergence rate is achievable in terms of the oracle evaluations [2]. Our oracle complexity corresponds to the lower complexity bounds [55] for general stochastic convex optimization.
Before we proceed to the non-accelerated method, we give the appropriate choice of the accuracy of the directional derivative evaluation ∆ζ, ∆η for ARDDsc to achieve an accuracy ε of the solution. These values are chosen such that the r.h.s. of (18) is smaller than ε. For simplicity we omit numerical constants and summarize the obtained values of the algorithm parameters in Table 3 below. The last row represents the total number of oracle calls, that is, the number of directional derivative evaluations, which was stated in (6).
Our Randomized Directional Derivative method for strongly convex problems (RDDsc) is listed as Algorithm 4.

11

Algorithm 3 Accelerated Randomized Directional Derivative method for strongly convex functions

(ARDDsc)

Input: x0 —starting point s.t. convexity parameter.

x0 − x∗

2 p

≤

Rp2;

K

1 — number of iterations; µp – strong

Output: point uK.

1: Set

N0 = 8aL2Ωp , (16) µp

where a = 384n2ρn. 2: for k = 0, . . . , K − 1 do 3: Set

8bσ2N02k

2

2 −k 4∆

−k

mk := max 1, L2µpR2

,

Rk := Rp2

+

1−2

µp

,

(17)

p

where b = n4 . 4: Set dk(x) = Rk2d x−Rkuk . 5: Run ARDD with starting point uk and prox-function dk(x) for N0 steps with batch size mk.

6: Set uk+1 = yN0 , k = k + 1. 7: end for
8: return uK

Algorithm 4 Randomized Directional Derivative method for strongly convex functions (RDDsc)

Input: x0 —starting point s.t. convexity parameter.

x0 − x∗

2 p

≤

Rp2;

K

1 — number of iterations; µp – strong

Output: point uK.

1: Set

N0 = 8aL2Ωp , (19) µp

where a = 384nρn. 2: for k = 0, . . . , K − 1 do 3: Set

8bσ22k

2

2 −k 4∆

−k

mk := max 1, L2µpR2

,

Rk := Rp2

+

1−2

µp

,

(20)

p

where b = 2 4: Set dk(x) = Rk2d x−Rkuk . 5: Run RDD with starting point uk and prox-function dk(x) for N0 steps with batch size mk. 6: Set uk+1 = yN0 , k = k + 1. 7: end for
8: return uK

12

∆ζ ∆η O-le calls

p=1

O min ε O min √ε 4
O max

L2µ1 , ε2 n(ln n)2L22Ω1 , ε · µ1

n ln nΩ1

R12 µ21

nΩ1

L2 µ1

,

√

√

ε n ln nL2 Ω1

,

√ ε

·

n ln nΩ1

R1 µ1

µ1 nΩ1

n ln nµL1 2Ω1 log2 µ1εR12 , σ2Ωµ11εln n

p=2

O min ε

L2µ2 , ε2 n3L22Ω2 , ε · µ2

n2 Ω2

R22 µ22

nΩ2

√ L µ √n3L √Ω √

O min

ε 4 n22Ω22 , ε

2 R2 µ2

2,

ε·

µ2 nΩ2

O max n Lµ2Ω2 2 log2 µ2εR22 , nσµ22Ωε 2

Table 3: Algorithm 3 parameters for the cases p = 1 and p = 2.

Theorem 2.5. Let f in problem (1) be µp-strongly convex and RDDsc method be applied to

solve this problem. Then

Ef (uK ) − f ∗

µpRp2 · 2−K + 2∆.
2

(21)

√

where ∆ =

n 12L

∆ζ

+

4n 3L

∆2η + 8

2nRp2 Ωp N

2

2

0

√ 2∆ζ + 2∆η

+ 3LN20ρn

√

2

2∆ζ + 2∆η . Moreover, under

an appropriate choice of ∆ζ and ∆η s.t. 2∆ ε/2, the oracle complexity to achieve ε-accuracy

of the solution is

2
n q L2Ωp

µpRp2

n

2 q

σ

2

Ωp

O max

µp

log2

, ε

µpε

.

Despite we have linear convergence in terms of the iterations number, the number of the oracle evaluations corresponds to sublinear convergence. The reason is that we consider general stochastic optimization problem, rather than ﬁnite-sum problems for which the linear convergence rate is achievable in terms of the oracle evaluations [2]. Our oracle complexity corresponds to the lower complexity bounds [55] for general stochastic convex optimization.
Before we proceed, we give the appropriate choice of the accuracy of the directional derivative evaluation ∆ζ, ∆η for RDDsc to achieve an accuracy ε of the solution. These values are chosen such that the r.h.s. of (21) is smaller than ε. For simplicity we omit numerical constants and summarize the obtained values of the algorithm parameters in Table 4 below. The last row represents the total number of oracle calls, that is, the number of directional derivative evaluations, which was stated in (6).

p=1

p=2

∆ζ ∆η O-le calls

O min

εL2 , ε2 (ln n)2L22 , ε µ1

n

nR12 µ21

nΩ1

O min

εLn2 , ε √lnnRnL1µ21 , ε nµΩ11

O max L2Ωµ11ln n log2 µ1εR12 , σµ21Ωε1

O min O min

εL2 , ε2 nL22 , ε µ2

n

R22µ22 nΩ2

√
εLn2 , ε Rn2µL22 ,

ε nµΩ22

O max nLµ22Ω2 log2 µ2εR22 , nσµ22Ωε 2

Table 4: Algorithm 4 parameters for the cases p = 1 and p = 2.

13

2.4 Corollaries for derivative-free optimization

In this subsection, following [45], we consider derivative-free smooth stochastic optimization in the
two-point feedback situation. We assume that an optimization procedure, given a pair of points (x, y) ∈ R2n , can obtain a pair of noisy stochastic realizations (f (x, ξ), f (y, ξ)) of the objective value f , where

f (x, ξ) = F (x, ξ) + Ξ(x, ξ), |Ξ(x, ξ)| ∆, ∀x ∈ Rn, a.s. in ξ,

(22)

and ξ is independently drawn from P . Based on these observations of the objective value, we form the following stochastic approxi-
mation of ∇f (x)

∇mf t(x) = 1 m f (x + te, ξi) − f (x, ξi) e =

m

t

i=1

gm(x, ξm), e

1m + m (ζ(x, ξi, e) + η(x, ξi, e)) e,
i=1
(23)

where e ∈ RS2(1), ξi, i = 1, ..., m are independent realizations of ξ, m is the batch size, t is some

m

small

positive

parameter

which

we

call

smoothing

parameter,

gm(x, ξm)

:=

1 m

g(x, ξi), and

i=1

ζ(x, ξi, e) = F (x + te, ξi) − F (x, ξi) − g(x, ξi), e , η(x, ξi, e) = Ξ(x + te, ξi) − Ξ(x, ξi) , i = 1, ..., m.

t

t

By Lipschitz smoothness of F (·, ξ), we have |ζ(x, ξ, e)|

L(ξ)t 2

for

all

x

∈

Rn

and

e

∈

S2(1).

Hence, Eξ(ζ(x, ξ, e))2 L224t2 for all x ∈ Rn and e ∈ S2(1). At the same time, from (22), we have

that |η(x, ξ, e)|

2∆ t

for

all

x

∈

Rn,

e

∈

S2(1)

and

a.s.

in

ξ.

Applying

Theorem

2.2

and

Theorem

2.3 with ∆ζ = L224t2 and ∆η = 2t∆ , we reproduce respectively the result of Theorem 2 and Theorem

3 in [45]. Applying Theorem 2.4 and Theorem 2.5 with ∆ζ = L224t2 and ∆η = 2t∆ , we obtain also

complexity bounds (6) for derivative-free smooth stochastic strongly convex optimization, which

was not yet done in the literature.

3 Proof of main result for ARDD method

We divide the proof of Theorem 2.2 into two large steps. First, to simplify the derivations, we prove this theorem assuming two additional inequalities which connect noisy stochastic approximation of the gradient (12) with the true gradient and function values. This result is stated as Lemma 3.1. Then, in Lemma 3.2, we show that our approximation of the gradient (12) indeed satisﬁes these two inequalities.

Lemma 3.1. Let {xk, yk, zk}, k 0 be generated by ARDD method. Assume that there exist numbers δ1 > 0,δ2 > 0 such that, for all k 0

E ∇mf (xk+1), zk − x∗

1 E [ ∇f (xk+1), zk − x∗ ] − δ1E [ zk − x∗ ]

(24)

n

and

E

∇mf (xk+1)

2 q

96ρnL2 (E[f (xk+1)] − E[f (yk+1)]) + δ2,

(25)

14

where expectation is taken w.r.t. all randomness and x∗ is a solution to (1). Then

E[f (yN )] − f (x∗)

√ 384ΘpNn22ρnL2 + 12nN 22Θp δ1 + 24ρNnL2 δ2 + 12Nρn2L2 δ12,

(26)

where Θp = V [z0](x∗) is deﬁned by the chosen proximal setup and the expectation is taken w.r.t. all randomness.

This result is proved below in subsection 3.1.

Lemma 3.2. Let {xk, yk, zk}, k with

0 be generated by ARDD method. Then (24) and (25) hold

∆ζ 2∆η

δ1 = √ + √

(27)

2n

n

and

96ρn σ2

2

δ2 = n · m + 61ρn∆ζ + 976ρn∆η.

(28)

This result is proved below in subsection 3.2.

Proof of Theorem 2.2. Combining Lemma 3.1 and Lemma 3.2, we obtain (13).

3.1 Proof Lemma 3.1
The following lemma estimates the progress in step 8 of ARDD method (and in step 5 of RDD method), which is a Mirror Descent step.

Lemma 3.3. Assume that z+ = argmin αn ∇mf (x), v − z + V [z] (v) . Then, for any ﬁxed
v∈Rn
u ∈ Rn,

αnE ∇mf (x), z − u

α2 n2
2E

∇mf (x)

2 q

+ E [V [z](u)] − E [V [z+](u)] ,

(29)

where expectation is taken w.r.t. all randomness.

Proof. For all u ∈ Rn, we have

αn ∇mf (x), z − u = αn ∇mf (x), z − z+ + αn ∇mf (x), z+ − u

x
αn ∇mf (x), z − z+

+

−∇V [z](z+), z+ − u

=y αn ∇mf (x), z − z+

z

(30)

+V [z](u) − V [z+](u) − V [z](z+)

αn ∇mf (x), z − z+

− 12

z − z+

2 p

+V [z](u) − V [z+](u) { α22n2 ∇mf (x) 2q + V [z](u) − V [z+](u),

where x follows from the deﬁnition of z+, whence ∇V [z](z+) + αn∇mf t(x), u − z+ 0 for all

u ∈ Rn; y follows from the ”‘magic identity”’ Fact 5.3.3 in [9] for the Bregman divergence; z

follows from (7); and { follows from the Fenchel inequality ζ s, z

− 21

z

2 p

≤

ζ2 2

s

2q .

Taking full

expectation we get (29).

Now we prove the following lemma which estimates the one-iteration progress of the whole algorithm.

15

Lemma 3.4. Let {xk, yk, zk, αk, τk}, k sumptions of Lemma 3.1,

0 be generated by ARDD method. Then, under as-

48n2ρnL2αk2+1E[f (yk+1)] − (48n2ρnL2αk2+1 − αk+1)E [f (yk)] −E [V [zk](x∗)] + E[V [zk+1](x∗)] − αk+1δ1nE [ zk − x∗ p] − α2k+21n2 δ2 αk+1f (x∗),

(31)

where expectation is taken w.r.t. all randomness, x∗ is a solution to (1).

Proof. Combining (24), (25) and (29), we obtain

αk+1E [ ∇f (xk+1), zk − x∗ ] 48α2n2ρnL2 (E [f (xk+1)] − E [f (yk+1)])

α2 n2

(32)

+E [Vzk (x∗)] − E[V [zk+1](x∗)] + αk+1δ1nE [ zk − x∗ p] +

k+1
2

δ2.

Further,

αk+1 (E [f (xk+1)] − f (x∗)) αk+1E [ ∇f (xk+1), xk+1 − x∗ ] = αk+1E [ ∇f (xk+1), xk+1 − zk ] + αk+1E [ ∇f (xk+1), zk − x∗ ] =x (1−τkτ)αk+1 E [ ∇f (xk+1), yk − xk+1 ] + αk+1E [ ∇f (xk+1), zk − x∗ ]
k
y (1−τkτk)αk+1 (E [f (yk)] − E [f (xk+1)]) + αk+1E [ ∇f (xk+1), zk − x∗ ] (32) (1−τkτk)αk+1 (E [f (yk)] − E [f (xk+1)]) + 48α2n2ρnL2 (E [f (xk+1)] − E [f (yk+1)])
+E [Vzk (x∗)] − E[V [zk+1](x∗)] + αk+1δ1nE [ zk − x∗ p] + α2k+21n2 δ2 =z (48αk2+1n2ρnL2 − αk+1)E [f (yk)] − 48αk2+1n2ρnL2E[f (yk+1)]
+αk+1E[f (xk+1)] + E [Vzk (x∗)] − E[V [zk+1](x∗)] + αk+1δ1nE [ zk − x∗ p] + α2k+21n2 δ2.
Here x is since xk+1 := τkzk + (1 − τk)yk ⇔ τk(xk+1 − zk) = (1 − τk)(yk − xk+1), y follows from the convexity of f and the inequality 1 − τk 0 and z is since τk = 48αk+11n2ρnL2 . Rearranging the terms, we obtain the statement of the lemma.

We are now ready to ﬁnish the proof of Lemma 3.1. Proof of Lemma 3.1. Note that 48n2ρnL2αk2+1 − αk+1 + 192n21ρnL2 = 48n2ρnL2αk2. That is,
48n2ρnL2αk2+1 − αk+1 + 192n21ρnL2 = 19(2kn+2ρ2)n2L2 − 96nk2+ρ2nL2 + 192n21ρnL2 = k2+41k92+n42−ρ2nkL−2 4+1 = 19(2kn+2ρ1)n2L2 = 48n2ρnL2αk2 .
Telescoping (31) for k = 0, 1, 2, . . . , l − 1 for l N we have3

l−1
48n2ρnL2αl2E[f (yl)] + 192n21ρnL2 E[f (yk)] − V [z0](x∗) + E[V [zl](x∗)]
k=1

l−1

l−1

−ζ1 αk+1E[ u − zk p] − ζ2 αk2+1

k=0

k=0

l−1
αk+1f (u),
k=0

where we denoted

ζ1 := δ1n,

n2 ζ2 := 2 δ2.

3Note that α1 = 96n22ρnL2 = 48n21ρnL2 and therefore 48n2ρnL2α12 − α1 = 0.

(33) (34)

16

√
We deﬁne Θ := V [z0](x∗), Rk := E[ x∗ − zk p]. Also, from (7), we have that ζ1α1R0 ≤ 48n22ΘρnζL1 2 .

l−1

√

l−1

To simplify the notation, we deﬁne Bl := ζ2 αk2+1 + Θ + 48n22ΘρnζL1 2 . Since αk+1 = 192l(nl2+ρ3n)L2

k=0

k=0

and, for all i = 1, . . . , N , f (yi) f (x∗), we obtain from (33)

192(ln+21ρ)n2L2 E[f (yl)]

f (x∗)

192(ln+23ρ)nlL2 − 192nl−2ρ1nL2

l−1
+ Bl − E[V [zl](x∗)] + ζ1 αk+1Rk,
k=1

0 192(ln+21ρ)n2L2 (E[f (yl)] − f (x∗)) Bl − E[V [zl](x∗)] + ζ1 l−1 αk+1Rk,
k=1

(35)

which gives

l−1

E[V [zl](x∗)] Bl + ζ1 αk+1Rk.

(36)

k=1

Moreover,

1 2

(E[

zl − x∗

p])2

21 E[ zl − x∗ 2p]

(36)

l−1

E[V [zl](x∗)] Bl + ζ1 αk+1Rk,

k=1

(37)

whence,

√

l−1

Rl

2 · Bl + ζ1 αk+1Rk.

k=1

(38)

√

Applying Lemma B.1 for a0 = ζ2α12 + Θ + 48n22ΘρnζL1 2 , ak = ζ2αk2+1, b = ζ1 for k = 1, . . . , N − 1, we

obtain

l−1
Bl + ζ1 αk+1Rk

√Bl + √2ζ1 · 96n2lρ2nL2 2 , l = 1, . . . , N

(39)

k=1

Since V [z](x∗) 0, by inequality (35) for l = N and the deﬁnition of Bl, we have

19(2Nn+2ρ1n)2L2 (E[f (yN )] − f (x∗))

√

√

B + 2ζ ·

N2

2 x 2B + 4ζ2 ·

N4

N

1 96n2ρnL2

N

1 (96n2ρnL2)2

l−1

√

= 2ζ2 αk2+1 + 2Θ + 24n22ΘρnζL1 2 + 4ζ12 · (96n2Nρn4 L2)2

k=0

y
2Θ +

√ 2Θζ1

+ 2ζ2(N+1)3 + 4ζ2 ·

N4

24n2ρnL2 (96n2ρnL2)2

1 (96n2ρnL2)2

(40)

where x is due to the fact that ∀a, b ∈ R (a + b)2

N −1
2a2 + 2b2 and y is because αk2+1 =
k=0

N +1
(96n2ρ1nL2)2 k2
k=2

(96n2ρ1nL2)2 · (N +1)(N +62)(2N +3)

(96n2ρ1nL2)2 · (N +1)2(N6+1)3(N +1) = (96(nN2+ρn1L)32)2 .

Dividing (40) by 19(2Nn+2ρ1n)2L2 and substituting ζ1, ζ2 from (34), we obtain

E[f (yN )] − f (x∗)

384Θn2ρnL2

+

√
12 2Θ ζ

+

(N +1)ζ2

+

N 4ζ12

(N +1)2

(N +1)2

1 √

24n2ρnL2

12n2ρnL2(N +1)2

384ΘNn22ρnL2 + 12nN 22Θ δ1 + 24ρNnL2 δ2 + 12Nρn2L2 δ12.

17

3.2 Proof Lemma 3.2

We start with the following technical result which connects our noisy approximation (12) of the stochastic gradient with the stochastic gradient itself and also with ∇f .

Lemma 3.5. For all x, s ∈ Rn, we have

Ee

∇mf (x)

2 q

m

12ρn n

gm(x, ξm)

2 2

+

ρn m

ζ(x, ξi)2 + 16ρn∆2η,

i=1

(41)

m

2 1m

2 1m

2

2

Ee ∇ f (x) 2 2n g (x, ξm) 2 − 2m ζ(x, ξi) − 8∆η,

(42)

i=1

Ee ∇mf (x), s

m

1 n

gm(x, ξm), s

− s√p
2m n

|ζ(x, ξi)| − 2∆√η ns p ,

i=1

(43)

m

Ee

∇f (x), e

e − ∇mf (x)

2 2

2 n

∇f (x) − gm(x, ξm)

2 2

+

1 m

ζ(x, ξi)2 + 16∆2η,

(44)

i=1

m

where

gm(x, ξm)

:=

1 m

g(x, ξi), ζ(x, ξi) and ∆η are deﬁned in (3).

i=1

Proof. First of all, we rewrite ∇mf (x) as follows

∇mf (x) =

gm(x, ξm), e + 1 m θ(x, ξi, e) e, m
i=1

where

θ(x, ξi, e) = ζ(x, ξi) + η(x, ξi, e), i = 1, ..., m.

By (3), we have

|θ(x, ξi, e)| ≤ |ζ(x, ξi)| + ∆η.

(45)

Proof of (41).

y 12ρn n
where x (45) and

m

2

Ee

∇mf (x)

2 q

=

Ee

gm(x, ξm), e + m1 θ(x, ξi, e) e

i=1

q

x

m

2

2Ee

gm(x, ξm), e

e

2 q

+

2Ee

1 m

θ(x, ξi, e)e

i=1

q

m

gm(x, ξm)

2 2

+

2ρn m

(|ζ(x, ξi)| + ∆η)2

i=1

m

12ρn n

gm(x, ξm)

2 2

+

ρn m

ζ(x, ξi)2 + 16ρn∆2η,

i=1

(46)

holds since

x+y

2 q

2

x

2 q

+

2

y

2 q

,

∀x,

y

∈

Rn;

y

follows

from

inequalities

(10),(11),

the fact that, for any a1, a2, . . . , am > 0, it holds that

m

2

ai

i=1

m
m a2i .
i=1

18

Proof of (42).

x1
Ee

gm(x, ξm), e e 2

2

2

where x follows from (45) e ∈ S2(1) and Lemma B.10
Proof of (43).

Ee

∇mf (x)

2 2

=

Ee

− m1 m (|ζ(x, ξi)| + ∆η)2 y 21n
i=1

and inequality

x+y

2 2

1 2

in [15], stating that, for any s

m

2

gm(x, ξm), e + m1 θ(x, ξi, e) e

i=1

2

m

gm(x, ξm)

2 2

−

1 2m

ζ(x, ξi)2 − 8∆2η,

i=1

(47)

x

2 2

−

y

2 2

,

∀x,

y

∈

Rn;

y

follows

from

∈

Rn,

E

s, e

2

=

1 n

s

22.

m

Ee ∇mf (x), s

= Ee

gm(x, ξm), e e, s

+

Ee

1 m

θ(x, ξi, e) e, s

i=1

x

m

1 n

gm(x, ξm), s

− m1

(|ζ(x, ξi)| + ∆η) Ee| e, s |

(48)

i=1

y n1 gm(x, ξm), s − 2ms√pn m |ζ(x, ξi)| − 2∆√η ns p

i=1

where x follows from Ee[n g, e e] = g, ∀g ∈ Rn and (45); y follows from Lemma B.10 in [15], since E| s, e | ≤ E s, e 2, and the fact that x 2 x p for p 2.
Proof of (44).

m

2

Ee

∇f (x), e

e − ∇mf (x)

2 2

= Ee

∇f (x), e e −

gm(x, ξm), e

e

−

1 m

θ(x, ξi, e)e

i=1

2

x

2

m

2

2Ee

∇f (x) − gm(x, ξm), e e

+ 2Ee

1 m

θ(x, ξi, e)e

(49)

2

i=1

2

y

m

2 n

∇f (x) − gm(x, ξm)

2 2

+

1 m

ζ(x, ξi)2 + 16∆2η,

i=1

where x holds since

x+y

2 2

in [15], and (45).

2 x 22+2 y 22, ∀x, y ∈ Rn; y follows from e ∈ S2(1) and Lemma B.10

We continue by proving the following lemma which estimates the progress in step 7 of ARDD, which is a gradient step.

Lemma 3.6. Assume that y = x − 2L12 ∇mf (x). Then,

gm(x, ξm)

2 2

≤ 8nL2(f (x) − Eef (y)) + 8

∇f (x) − gm(x, ξm)

2 2

m
+ 5n ζ(x, ξi)2 + 80n∆2,

(50)

m

η

i=1

where gm(x, ξm) is deﬁned in Lemma 3.5, ζ(x, ξi) and ∆η are deﬁned in (3).

Proof. Since ∇mf (x) is collinear to e, we have that, for some γ ∈ R, y − x = γe. Then, since e 2 = 1,
∇f (x), y − x = ∇f (x), e γ = ∇f (x), e e, y − x = ∇f (x), e e, y − x .

19

From this and L2-smoothness of f we obtain

f (x) +

f (y) f (x) + ∇f (x), e e, y − x + L22 ||y − x||22

∇mf (x), y − x + L2||y − x||22 + ∇f (x), e e − ∇mf (x), y − x − L22 ||y − x||22

x
f (x) +

∇mf (x), y − x

+ L2||y − x||2 +

1

∇f (x), e e − ∇mf (x) 2,

2 2L2

2

where x follows form the Fenchel inequality s, z − ζ2 z 22 ≤ 21ζ s 22. Using y = x − 2L12 ∇mf (x), we get
4L12 ∇mf (x) 22 f (x) − f (y) + 2L12 ∇f (x), e e − ∇mf (x) 22
Taking the expectation in e and applying (42), (44), we obtain

m
4L12 21n gm(x, ξm) 22 − 21m ζ(x, ξi)2 − 8∆2η
i=1

4L12 Ee ∇mf (x) 22

f (x) − Eef (y) + 2L12

f (x) − Eef (y) + 2L12 Ee ∇f (x), e e − ∇mf (x) 22

2

∇f (x) − gm(x, ξm)

2 + t2

m
ζ(x, ξi)2 + 16∆2

,

n

2m

η

i=1

Rearranging the terms, we obtain the statement of the lemma.

We are now ready to ﬁnish the proof of Lemma 3.2.

Proof of Lemma 3.2. Taking the expectation w.r.t. all randomness4 of (43) and using inequality

(3)

E[|ζ(x, ξi)|] E[|ζ(x, ξi)|2]

∆ζ ,

√ we obtain inequality (24) with δ1 = 2√∆nζ + 2√∆nη . Combining (41) and (50), taking the full ex-

pectation and using E[ ∇f (x) − gm(x, ξ) 22]

δ2

=

96ρn n

·

σ2 m

+

61ρn∆ζ

+

976ρn ∆2η .

σm2 , which follows from (2), we obtain (25) with

4 Proof of main result for RDD method
As in the previous section, we divide the proof of Theorem 2.3 into large steps. First, to simplify the derivations, we prove this theorem assuming two additional inequalities which connect or noisy stochastic approximation of the gradient (12) with the true gradient and function values. Then we show that our approximation of the gradient (12) indeed satisﬁes these two inequalities.

Lemma 4.1. Let {xk, yk, zk}, k 0 be generated by RDD method. Assume that there exist numbers δ1 > 0,δ2 > 0 such that, for all k 0

E ∇mf (xk), xk − x∗

1 E [ ∇f (xk), xk − x∗ ] − δ1E [ xk − x∗ p]

(51)

n

E

∇mf (xk)

2 q

48ρnL2 (E [f (xk)] − f (x∗)) + δ2,

(52)

n

4Note that we use s = zk − x∗ which does not depend on ξ1, ξ2, . . . , ξm from the (k + 1)-th iterate and it does not depend on ek+1. Therefore we can use tower property of mathematical expectation and take ﬁrstly conditional expectation w.r.t. ξ1, . . . , ξm and after that take full expectation.

20

where expectation is taken w.r.t. all randomness and x∗ is a solution to (1). Then

E[f (x¯N )] − f (x∗)

√ 384nρNnL2Θp + 12ρnnL2 δ2 + 8n N2Θp δ1 + 3Ln2Nρn δ12,

(53)

where Θp = V [z0](x∗) is deﬁned by the chosen proximal setup and the expectation is taken w.r.t. all randomness.

This result is proved below in subsection 4.1.

Lemma 4.2. Let {xk, yk, zk}, k with

0 be generated by RDD method. Then (51) and (52) hold

∆ζ 2∆η

δ1 = √ + √

(54)

2n

n

and

24ρn σ2

2

δ2 = n · m + ρn∆ζ + 16ρn∆η.

(55)

This result is proved below in subsection 4.2.

Proof of Theorem 2.3. Combining Lemma 4.1 and Lemma 4.2, we obtain (14).

4.1 Proof Lemma 4.1
Combining (29), (51) and (52) we get

αE [ ∇f (xk), xk − x∗ ]

24α2nρnL2 (E [f (xk)] − f (x∗)) + αδ1nE [

xk − x∗

p]

+

α2 n2 2

δ2

+E [V [xk](x∗)] − E [V [xk+1](x∗)] ,

whence due to convexity of f we have

(α − 24α2nρnL2) (E[f (xk)] − f (x∗))

αδ1nE [

xk − x∗

p]

+

α2 n2 2

δ2

α

(56)

4

+E[V [xk](x∗)] − E[V [xk+1](x∗)],

because α = 48nρ1nL2 . Summing (56) for k = 0, . . . , l − 1, where l N we get

0 N4α (E[f (x¯l)] − f (x∗)) α22n2l δ2 + αδ1n l−1 E[ xk − x∗ p]

k=0
+ V [x0](x∗) −E[V [xl](x∗)],

(57)

Θp

where x¯l d=ef 1l l−1 xk. From the previous inequality we get
k=0

1 2

(E[

xl − x∗

p])2

21 E[ xl − x∗ 2p]

E[V [xl](x∗)]

Θp + l · α22n2 δ2 + αδ1n l−1 E[ xk − x∗ p],
k=0

(58)

21

whence ∀l N we obtain

√

α2n2

l−1

E[ xk − x∗ p] 2 Θp + l · 2 δ2 + αδ1n E[ xk − x∗ p]. (59)

k=0

Denote Rk = E[ x∗ − xk p] for k = 0, . . . , N . Applying Lemma B.2 for a0 = Θp + αδ1nE[ x0 −

x∗ p]

Θp + αn

2Θpδ1, ak

=

α2 n2 2

δ2

,

b

=

nδ1

for

k

=

1, . . . , N

−

1

we

have

for

l

=

N

Nα 4

(E[f

(x¯N

)]

−

f

(x∗))

α2 n2

√

2

Θp + N · 2 δ2 + αn 2Θpδ1 + 2nδ1αN

x
2Θp + N α2n2δ2 + 2αn 2Θpδ1 + 4n2δ12α2N 2,

whence

E[f (x¯N )] − f (x∗)

because α = 48nρ1nL2 .

√ 384nρNnL2Θp + 12ρnnL2 δ2 + 8n N2Θp δ1 + 3Ln2Nρn δ12,

4.2 Proof Lemma 4.2

Taking mathematical expectation w.r.t. all randomness from the (43) we obtain5 inequality (51)

√

(3)

with δ1 = 2√∆nζ + 2√∆nη , because E[|ζ(x, ξi)|]

E[|ζ(x, ξi)|2]

∆ζ. Combining (41) and

gm(x, ξm)

2 2

2

∇f (x)

2 2

+

2

∇f (x) − gm(x, ξm)

2 2

4L2 (E[f (x)] − f (x∗)) + 2 ∇f (x) − gm(x, ξm) 22,

E[ ∇f (x) − gm(x, ξm) 22]

σ2 m

and

taking

full

mathematical

expectation

we

obtain

(52)

with

δ2

=

24ρn n

·

σ2 m

+ ρn∆ζ

+

16

ρn

∆

2 η

.

5 Proofs for strongly convex problems

5.1 Accelerated algorithm

Lemma 5.1. Assume that we start ARDD Algorithm 1 from a random point x0 such that Ex0 x∗ − x0 2p Rp2, use the function Rp2d x−Rpx0 as the prox-function and run ARDD for N0 iterations. Then

E[f (yN0)] − f ∗

aL2Rp2Ωp bσ2N0

N2

+

+ ∆,

mL2

0

5Note that we use s = xk − x∗ which does not depend on ξ1, ξ2, . . . , ξm from the (k + 1)-th iterate and it does not depend on ek+1. Therefore we can use tower property of mathematical expectation and take ﬁrstly conditional expectation w.r.t. ξ1, . . . , ξm and after that take full expectation.
22

where a = 384n2ρn, b = n4 ,

61N0

122N0 2 12

∆ = 24L2 ∆ζ + 3L2 ∆η +

2nRp2Ωp N02

∆ζ 2 + 2∆η

+ N02 12nρnL2

2
∆ζ 2 + 2∆η

and the expectation is taken with respect to all the randomness.

Proof. Note that Rp2d x−Rpx0 is strongly convex with constant 1 w.r.t · p. Since 0 = arg min d(x), we have, for the prox-function d¯(x) = Rp2d x−Rpx0 and corresponding Bregman divergence V¯ [x0](x),
Θp = V¯ [x0](x∗) = d¯(x∗) − d¯(x0) − ∇d¯(x0), x∗ − x0 = d¯(x∗) ≤ Rp22Ωp . Applying Theorem 2.2 an taking additional expectation w.r.t to x0, we ﬁnish the proof of the lemma.

Proof of Theorem 2.4. We prove by induction that

E uk − x∗ 2p ≤ Rk2 = Rp22−k + 4∆ 1 − 2−k .

(60)

µp

For k = 0, this inequality obviously holds. Let us assume that it holds for some k ≥ 0 and prove the induction step. Applying Lemma 5.1 at the step k of Algorithm 3, we obtain that

Ef (uk+1) − f ∗ = Ef (yN0 ) − f ∗

aL2Rk2Ωp + bσ2N0 + ∆.

N02

mk L2

By deﬁnition of N0, we have

aL2 Rk2 Ωp N02

aL2Rk2Ωp = µpRk2 .

8aL2Ωp

8

µp

By deﬁnition of mk, we have

8bσ2N0

8bσ2N0

8bσ2N0

mk L2µpRp22−k L µ R22−k + 4∆ (1 − 2−k) = L2µpRk2

2p p

µp

and Hence,

bσ2N0 mk L2

bσ2N0 = µpRk2 .

L2

8bσ2N0
2

8

L2 µp Rk

Ef (uk+1) − f ∗

µpRk2 + ∆ = µp R22−k + 4∆ 1 − 2−k

4

4

p

µp

+∆

= µ2p Rp22−(k+1) + 4µ∆p 1 − 2−(k+1)

= µpRk2+1 .
2

Since f is strongly convex, we have

E

uk+1 − x∗

2 p

2 (Ef (uk+1) − f ∗) µp

Rk2+1.

23

This ﬁnishes the induction step and, as a byproduct, we obtain inequality (18).
It remains to estimate the complexity. To make the right hand side of (18) smaller than ε it is suﬃcient to choose K = log2 µpεRp2 . To estimate the total number of oracle calls, we write

K −1
Number of calls = N0mk
k=0

K −1
N0
k=0

8bσ2N02k 1 + L2µpR2
p

8bσ2N022K KN0 + L2µpR2
p

8aL2Ωp

µpRp2

8bσ2 8aL2Ωp µpRp2

µp

log2 ε + L2µpR2 ·

µp

· ε

p

8aL2Ωp

µpRp2 64abσ2Ωp

µp log2 ε + µpε

1 + 1 L2Ωp

µpRp2

n

2 q

σ

2

Ω

p

= O max n 2 q

µp log2

, ε

µpε

,

where

we

used

that

a

=

384n2ρn,

b

=

4 n

and

ρn

is

given

in

Lemma

2.1.

5.2 Non-accelerated algorithm

Lemma 5.2. Assume that we start RDD Algorithm 2 from a random point x0 such that Ex0 x∗ − x0 2p Rp2, use the function Rp2d x−Rpx0 as the prox-function and run RDD for N0 iterations. Then

E[f (yN0)] − f ∗

aL2Rp2Ωp bσ2

+

+ ∆,

N0

mL2

√

where a = 192nρn, b = 2, ∆ =

n 12L

∆ζ + 34Ln

∆2η+ 8

2nRp2 Ωp N

2

2

0

√ 2∆ζ + 2∆η

and the expectation is taken with respect to all the randomness.

+ 3LN20ρn

√

2

2∆ζ + 2∆η

Proof. Note that Rp2d x−Rpx0 is strongly convex with constant 1 w.r.t · p. Since 0 = arg min d(x), we have, for the prox-function d¯(x) = Rp2d x−Rpx0 and corresponding Bregman divergence V¯ [x0](x),
Θp = V¯ [x0](x∗) = d¯(x∗) − d¯(x0) − ∇d¯(x0), x∗ − x0 = d¯(x∗) ≤ Rp22Ωp .
Applying Theorem 2.3 an taking additional expectation w.r.t to x0, we ﬁnish the proof of the lemma.

Proof of Theorem 2.5. We prove by induction that

E uk − x∗ 2p ≤ Rk2 = Rp22−k + 4∆ 1 − 2−k .

(61)

µp

24

For k = 0, this inequality obviously holds. Let us assume that it holds for some k ≥ 0 and prove the induction step. Applying Lemma 5.2 at the step k of Algorithm 4, we obtain that

Ef (uk+1) − f ∗ = Ef (yN0 ) − f ∗

aL2Rk2Ωp + bσ2 + ∆.

N0

mk L2

By deﬁnition of N0, we have By deﬁnition of mk, we have

aL2 Rk2 Ωp N0

aL2Rk2Ωp = µpRk2 .

8aL2Ωp

8

µp

8bσ2

8bσ2

8bσ2

mk L2µpRp22−k L µ R22−k + 4∆ (1 − 2−k) = L2µpRk2

2p p

µp

and Hence,

bσ2 mk L2

bσ2 = µpRk2 .

L2

8bσ2
2

8

L2 µp Rk

Ef (uk+1) − f ∗

µpRk2 + ∆ = µp R22−k + 4∆ 1 − 2−k

4

4

p

µp

+∆

= µ2p Rp22−(k+1) + 4µ∆p 1 − 2−(k+1)

= µpRk2+1 .
2

Since f is strongly convex, we have

E

uk+1 − x∗

2 p

2 (Ef (uk+1) − f ∗) µp

Rk2+1.

This ﬁnishes the induction step and, as a byproduct, we obtain inequality (21).
It remains to estimate the complexity. To make the right hand side of (21) smaller than ε it is suﬃcient to choose K = log2 µpεRp2 . To estimate the total number of oracle calls, we write

K −1
Number of calls = N0mk
k=0

K −1
N0
k=0

8bσ22k 1 + L2µpR2
p

8bσ2N02K KN0 + L2µpR2
p

8aL2Ωp

µpRp2

8bσ2 8aL2Ωp µpRp2

µp

log2 ε + L2µpR2 ·

µp

· ε

p

8aL2Ωp

µpRp2 64abσ2Ωp

µp log2 ε + µpε

2
n q L2Ωp

µpRp2

n

2 q

σ

2

Ω

p

= O max

µp

log2

, ε

µpε

,

where we used that a = 192nρn, b = 2 and ρn is given in Lemma 2.1.

25

6 Numerical experiments
In this section we numerically test our methods on the “worst in the world” function from [56] and least squares problem. In these problems there is no noise of type η(x, ξ, e) from (3) since one can compute directional derivatives with machine precision. Moreover, for both examples one can compute exact functional values, therefore, using small enough smoothing parameter t (see (23)) it is possible to approximate directional derivatives via ﬁnite diﬀerences with high enough accuracy. That is, for the problems we consider in this section the diﬀerence between directional derivative oracle and derivative-free oracle is negligible to inﬂuence the behaviour of our methods. Taking it into account we consider only derivative-free oracle in the experiments and compare our methods with RSGF from [42].

6.1 Nesterov’s function

We start with numerical tests on Nesterov’s function

L

n−1 2

2

2

L

f (x) = 8

x1 +

(xi − xi+1) + xi − 4 x1

(62)

i=0

which is convex, L-smooth and attains its minimal value f ∗ = L8 −1 + n+1 1 at such x∗ =

(x

∗ 1

,

.

.

.

,

x

∗ n

)

that

x∗i

=

1−

i n+1

for i = 1, . . . , n [56].

We take the starting point x0 such that

all coordinates expect the ﬁrst one coincides with corresponding coordinates of x∗ and we take 10

as the ﬁrst coordinate of x0. We also choose L = 10, t = 10−8 and consider n = 100, 1000, 5000. The results can be found in Figure 1. In these settings x0 − x∗ 1 = x0 − x∗ 2 and our theory

establishes (see Tables 1 and 2) better complexity bounds for the case when p = 1 then for the

Euclidean case especially for big n. The experiments conﬁrm this claim: as one can see in Fig-

ure 1, the choice of 1 proximal setup becomes more beneﬁcial than standard Euclidean setup for

n = 1000 and n = 5000 to reach good enough accuracy. Indeed, our choice of the starting point

and L implies that f (x0) − f (x∗) ≈ 200 and for n = 1000 and n = 5000 ARDD with 1 proximal setup (ARDD NE in Figure 1) make f (xN ) − f (x∗) of order 10−3 − 10−5 faster than ARDD with p = 2 (ARDD E in Figure 1) and RDD with p = 1 (RDD NE in Figure 1) ﬁnds such xN that

f (xN ) − f (x∗) is of order 10−3 faster than its Euclidean counterpart (RDD E in Figure 1). Finally,

all of our methods outperform RSGF on the considered problem.

To perform mirror descent step for p = 1 we apply relations obtained in Appendix B from [45].

See other details connected with parameters tuning in C of this work.

6.2 Least squares problem

In this subsection we consider least squares problem:

1

2 1r1

2

min
x∈Rn

f (x) = 2r

Ax − b

2= r

2 (Aix − bi) .

(63)

i=1

Here A is r × n real matrix, b ∈ Rr and Ai denotes the i-th row of A. Clearly, f (x) is convex and

smooth function.

Moreover, each summand fi(x) =

1 2

(Aix

−

bi)2

is also convex and L2.i-smooth

26

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

Nesterov's function, n = 100, L = 10

10 1

ARDD_E ARDD_NE

RDD_E

10 3

RDD_NE RSGF

10 5

10 7

10 9

Nesterov's function, n = 1000, L = 10

ARDD_E

10 1

ARDD_NE RDD_E

RDD_NE

10 3

RSGF

10 5

10 7

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

100

Nesterov's function, n = 5000, L = 10
ARDD_E

ARDD_NE

10 2

RDD_E RDD_NE

RSGF

10 4

f(xk) f(x * ) f(x0) f(x * )

10 6

10 8

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e7

Figure 1: ARDD, RDD and RSGF applied to minimize Nesterov’s function (62). We use E and
NE to deﬁne 2 and 1 proximal setups respectively (see (8) and (9) for the details). In the plot for n = 5000 number of oracle calls is divided by 107.

function with L2,i =

Ai

22.

One can consider (63) as (1) with F (x, ξ) = fξ(x) =

1 2

(Aξ x

−

bξ )2

where ξ is uniformly distributed on {1, 2, . . . , r}. Then, by deﬁnition of L2 we have

L2 =

EξL22,ξ =

1r r i=1

Ai

2 2

=

A √

F

r

(64)

where A F denotes Frobenius norm of matrix A. In our preliminary experiments elements of A and b were sampled independently from the
standard normal distribution and then matrix A was normalized by its 2-norm. In particular, we choose r = 300 and n = 400 which implies that f (x) is just convex but not strongly convex and f (x∗) = 0. Moreover, we compute the solution x∗ as A+b where A+ denotes Moore-Penrose inverse of A and choose the starting point x0 as x∗ and 100 to the ﬁrst component. In our tests the suboptimality of the starting point, i.e. f (x0) − f (x∗), was approximately 3. The results can be found in Figure 2. We want to notice that in these preliminary experiments with stochasticity
in functional values in experiments with ARDD it was needed to tune not only αk+1 that appears in the mirror descent step, but also the stepsize for the gradient step, see the details in C.

27

f(xk) f(x * ) f(x0) f(x * )

Least squares problem, r = 300, n = 400

10 1

ARDD_E ARDD_NE

10 3

RDD_E RDD_NE

10 5 RSGF

10 7

10 9

10 11

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e8

Figure 2: ARDD, RDD and RSGF applied to solve least squares problem (63). We use E and NE
to deﬁne 2 and 1 proximal setups respectively (see (8) and (9) for the details). For all methods batch size m equals 50. By oracle call we mean one computation of functional value of a summand. Number of oracle calls is divided by 108.

7 Conclusion
In this paper we propose four novel directional derivative methods for smooth stochastic convex and strongly convex optimization with corollaries for derivative-free optimization. These methods are able to work with Euclidean and non-Euclidean proximal setups. We prove complexity results showing that in non-Euclidean case complexities of our methods outperform state-of-the-art results for directional derivative and derivative-free methods in terms of the dependence on the dimension of the problem under assumption that 1 and 2 norms of x0 − x∗ are close to each other, e.g. when x0 = 0 and x∗ is sparse. Moreover, we analyze our methods under general assumptions on the noisy oracle and provide bounds for the admissible noise levels. Since we use mini-batches, we are able to separate iteration complexity and sample complexity, the former being up to a dimensiondependent factor the same as for accelerated gradient method in the standard deterministic fullgradient setting. This makes our methods amenable to parallel computation setting [29] and leads to acceleration in this setting compared to standard stochastic gradient methods [26]. Finally, we conduct several experiments providing numerical justiﬁcations of the obtained results.
Using an additional “light-tail” assumption that Eξ[exp( g(x, ξ) − ∇f (x) 22/σ2)] exp(1) and techniques of [44] our algorithms and analysis can be extended to obtain results in terms of probability of large deviations. For example, in the case of controlled noise levels ∆ζ, ∆η this means that an algorithm outputs a point xˆ which satisﬁes P{f (xˆ) − f (x∗) ε} 1 − δ, where δ ∈ (0, 1) is the conﬁdence level, for the price of extra ln 1δ factor in N and m. As directions of future research we would like to point a primal-dual extension for problems with linear constraints in the spirit of [32, 22, 4, 7, 30, 27, 59], an extension with line-search to adapt to an unknown value of L2 using the techniques in [18, 12, 28], an extension for the case of intermediate smoothness [58, 48] or interpolation between accelerated and non-accelerated methods [36, 31], as well as extension to

28

a more general type of inexactness called inexact model of the objective [69, 68].

Acknowledgements
The research is supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) No. 075-00337-20-03, project No. 0714-2020-0005.

A Proof of Lemma 2.1

Here we prove that, for e ∈ RS2 (1)

E[ e 2q]

min{q

−

1,

16

ln

n

−

8}

n

2 q

−1

,

(65)

E[ s, e 2 e 2q] 6 s 22 min{q − 1, 16 ln n − 8}n 2q −2.

(66)

We start with proving the following inequality which could be rough for big q:

E[ e 2q]

(q

−

1)n

2 q

−1,

2

q < ∞.

(67)

We have

E[ e 2q] = E

2

n

qx

n

|ek |q

E

|ek |q

k=1

k=1

2

q

=y

(nE[|e2 |q ])

2 q

,

(68)

2
where x is due to probabilistic version of Jensen’s inequality (function ϕ(x) = x q is concave, because q 2) and y is because mathematical expectation is linear and components of vector e are identically distributed.
Moreover, due to Poincare lemma, we have

d

ξ

e=

,

(69)

ξ12 + · · · + ξn2

where ξ is Gaussian random vector which mathematical expectation is zero vector and covariance matrix is identical. Then

E[|e2|q] = E

|ξ2|q
q

(ξ12+...+ξn2 ) 2

n

− 2q

n

= ··· |x2|q

x2k

· 1 n · exp − 12 x2k dx1 . . . dxn.

Rn

k=1

(2π) 2

k=1

Consider spherical coordinates:

x1 = r cos ϕ sin θ1 . . . sin θn−2, x2 = r sin ϕ sin θ1 . . . sin θn−2, x3 = r cos θ1 sin θ2 . . . sin θn−2, x4 = r cos θ2 sin θ3 . . . sin θn−2,
...
xn = r cos θn−2, r > 0, ϕ ∈ [0, 2π), θi ∈ [0, π], i = 1, n − 2.

29

The Jacobian of mapping is

det

∂(x1, . . . , xn)

= rn−1 sin θ1(sin θ2)2 . . . (sin θn−2)n−2.

∂(r, ϕ, θ1, θ2, . . . , θn−2)

Then mathematical expectation E[|e2|q] could be rewritten in the following form:

E[|e2 |q ]

=

···

rn−1| sin ϕ|q| sin θ1|q+1| sin θ2|q+2 . . . | sin θn−2|q+n−2

r>0, ϕ∈[0,2π),

θi∈[0,π], i=1,n−2

− r2

·e

2 n

dr

.

.

.

dθn−2

(2π) 2

= 1 n Ir · Iϕ · Iθ1 · Iθ2 · . . . · Iθn−2 ,
(2π) 2

where

Ir = +∞rn−1e− r22 dr,

0

2π

π

Iϕ = | sin ϕ|qdϕ = 2 | sin ϕ|qdϕ,

0

0

π

Iθi = | sin θi|q+idθi, i = 1, n − 2.

0

Now we are going to compute these integrals. Start with Ir:

Ir = +∞rn−1e− r22 dr = /r = √2t/ = +∞(2t) n2 −1e−tdt = 2 n2 −1Γ( n2 ).

0

0

To compute other integrals it is useful to consider the following integral (α > 0):

π

π

π | sin ϕ|αdϕ = 2 2 | sin ϕ|αdϕ = 2 2(sin2 ϕ) α2 dϕ = /t = sin2 ϕ/

0

0

0

=

1

t α−1 2

(1

−

t)− 12

dt

=

B( α+1 ,

1) =

Γ( α+2 1 )Γ( 12 )

√ Γ( α+1 ) = π 2.

0 2 2 Γ( α+2 2 ) Γ( α+2 2 )

From this we obtain

E[|e2|q] = 1 n Ir · Iϕ · Iθ1 · Iθ2 · . . . · Iθn−2

(2π) 2

=

1

·

2

n 2

−1

Γ(

n

)

·

√ Γ( q+1 ) 2π 2

·

√ Γ( q+2 ) π2

·

√ Γ( q+3 ) π2

·

...

·

√ Γ( q+n−1 )

π

2

n
(2π) 2

2 Γ( q+2 2 ) Γ( q+2 3 ) Γ( q+2 4 )

Γ( q+2n )

= √1π · Γ(Γn2()qΓ+(nq+2) 1 ) .

2

Now, we want to show that ∀ q 2

1 Γ( n2 )Γ( q+2 1 )

√· π

Γ( q+n )

2

q
q−1 2 .
n

At the beginning show that (71) holds for q = 2 (and arbitrary n):

1 √

·

Γ( n2 )Γ( 2+2 1 )

−

1

=

1 √

· Γ( n2 ) · 12 Γ( 21 ) − 1 = 1 − 1 = 0

0.

π Γ( 2+2n )

n

π

n2 Γ( n2 )

n nn

30

(70) (71)

Consider the function

1 Γ( n )Γ( q+1 )

q
q−1 2

fn(q) = √π ·

2

2

Γ( q+n )

−

n

2

where q 2. Also consider ψ(x) = d(ln(dΓx(x))) with x > 0 which is called (digamma function). For gamma function it holds
Γ(x + 1) = xΓ(x), x > 0.

Taking natural logarithm from it and taking derivative w.r.t. x:

ln Γ(x + 1) = ln Γ(x) + ln x, d(ln(Γd(xx+1))) = d(ln(dΓx(x))) + x1 ,

which could be written in digamma-function-notation:

1

ψ(x + 1) = ψ(x) + .

(72)

x

One can show that digamma function is monotonically increases when x > 0. To prove this fact

we are going to show that

Γ (x) 2 < Γ(x)Γ (x).

(73)

That is,

x +∞
<
0

e−

t 2

t

x−1 2

2

+∞

dt ·

0

e−

t 2

t

x−1 2

ln t

+∞

2

(Γ (x))2 =

e−t ln t · tx−1dt

0

+∞

+∞

2
dt =

e−ttx−1dt ·

ettx−1 ln2 tdt,

0

0

Γ(x)

Γ (x)

where x follows from Cauchy-Schwartz inequality (the equality cannot occur because functions

e−

t 2

t

x−1 2

and

e−

t 2

t

x−1 2

ln t

are

linearly

independent).

From

(73)

follows

that

d2(ln Γ(x)) Γ (x)

dx2

= Γ(x)

Γ (x) (Γ (x))2 (73) = Γ(x) − (Γ(x))2 > 0,

which shows that digamma function increases.

Now we show that fn(q) decreases on the interval [2, +∞). To obtain it is suﬃcient to consider

ln(f (q)):

= ln

Γ( n ) √2

+ ln

Γ

q+1

π

2

− ln Γ q+2n

ln(fn(q)) − 2q (ln(q − 1) − ln n) ,

d(ln(dfnq (q))) = 12 ψ q+2 1 − 12 ψ q+2n − 12 ln(q − 1) − 2(qq−1) + 12 ln n.

We are going to show that d(ln(dfnq (q))) < 0 for q 2. Let k = n2 (the closest integer which is no

31

greater than n2 ). Then ψ q+2n > ψ k − 1 + q+2 1 and ln n ln(2k + 1), whence

d(ln(fn(q)))

dq

< 12 ψ q+2 1 − ψ k − 1 + q+2 1 − 12 ln(q − 1) − 2(qq−1) + 12 ln(2k + 1)

(72)
=

1

ψ

q+1

k−1
−

1

− ψ q+1

2 2 i=1 q+2 1 +k−i−1 2

− 2(qq−1) + 12 ln 2qk−+11

x − 1 k−1 2 − 1 + 1 ln

2

q−1+2k−2i q−1 2

i=1

2k+1 q−1

= − 12 q−2 1 + q+2 1 + q+2 3 + . . . + q+22k−3 + 12 ln 2qk−+11

y
<

−

1

ln

q+2k−1

2

q−1

+ 21 ln 2qk−+11

z − 1 ln 2k+1

2

q−1

+ 21 ln 2qk−+11

= 0,

where x and z is because q

2,

y

is

due

to

estimation

of

integral

of

1 x

by

integral

of

g(x)

=

q−11+2i , x ∈ [q − 1 + 2i, q − 1 + 2i + 2], i = 0, 2k − 1 which is no less than f (x):

q+2k−1

2

2

2

2

+

+

+...+

>

q−1 q+1 q+3

q + 2k − 3

1

q + 2k − 1

dx = ln

.

x

q−1

q−1

So, we shown that d(ln(dfnq (q))) < 0 for q 2 arbitrary natural number n. Therefore for any ﬁxed number n the function fn(q) decreases as q increase, which means that fn(q) fn(2) = 0, i.e., (71)
holds. From this and (68),(70) we obtain that ∀ q 2

E[||e||2q ] (68) (nE[|e2|q]) 2q (70),(71) (q − 1)n 2q −1.

(74)

However, inequality (74) is useless when q is big (with respect to n). Consider left hand side of (74) as function of q and ﬁnd its minimum for q 2. Consider hn(q) = ln(q − 1) + 2q − 1 ln n (it is logarithm of the right hand side of (74)). Derivative of h(q) is

dh(q) dq

=

1 q−1

−

2

ln q2

n

,

1 q−1

−

2 ln n q2

=

0,

q2 − 2q ln n + 2 ln n = 0.

If n 8, then the point where the function obtains its minimum on the set [2, +∞) is q0 = ln n 1 + 1 − ln2n (for the case n 7 it turns out that q0 = 2; further without loss of generality we assume n 8). Therefore for all q > q0 it is more useful to use the following estimation:

E[||e||2]

x
<

E[||e||2

]

(74)

(q0

− 1)n q20 −1

y

(2

ln

n

−

1)n

2 ln n

−1

q

q0

(75)

= (2 ln n − 1)e2 n1 (16 ln n − 8) n1 (16 ln n − 8)n 2q −1,

where x is due to e q < e q0 for q > q0, y follows from q0 (74) and (75) together we obtain (65).

2 ln n, q0

ln n. Putting estimations

32

Now we are going to prove (66). Firstly, we want to estimate Jensen’s inequality (q 2)

E[ e 4q]. Due to probabilistic

 E[||e||4q] = E 

2

n

2q

|ek |q



k=1

2

n

2q

E

|ek |q

k=1

x
E

n
n |ek|2q
k=1

(70),(71) 4
nq

2q
2q−1 2 n

2

q =y

2
n2E[|e2|2q] q

2

q

=

(2q

−

1)2n

4 q

−2,

where x is because

n

2

xk

k=1

n
n x2k for x1, x2, . . . , xn ∈ R and y follows from that mathe-
k=1

matical expectation is linear and components of the random vector e are identically distributed.

From this we obtain

E[||e||4q ]

(2q

−

1)n

2 q

−1.

(76)

Consider the right hand side of the inequality (76) as a function of q and ﬁnd its minimum for q 2.

Consider hn(q) = ln(2q − 1) + 2q − 1 ln n (logarithm of the right hand side (76)). Derivative of

h(q) is

dh(q) dq

=

2 2q−1

−

2

ln q2

n

,

2 2q−1

−

2 ln n q2

=

0,

q2 − 2q ln n + ln n = 0.

If n 3, the the point where the function obtains its minimum on the set [2, +∞) is q0 = ln n 1 + 1 − ln1n (for the case n 2 it turns out that q0 = 2; further without loss of generality we assume that n 3). Therefore for all q > q0:

E[||e||4]

x
<

E[||e||4

]

(76)

(2q0

− 1)n q20 −1

y

(4

ln

n

−

1)n

2 ln n

−1

q

q0

(77)

= (4 ln n − 1)e2 n1 (32 ln n − 8) n1 (32 ln n − 8)n 2q −1,

where x is due to e q < e q0 for q > q0, y follows from q0 (76) and (77) together we get inequality

2 ln n, q0

ln n. Putting estimations

E[||e||4q ]

min{2q

−

1,

32

ln

n

−

8}n

2 q

−1.

(78)

Now we are going to ﬁnd E[ s, e 4], where s ∈ Rn is some vector. Let Sn(r) be a surface area of
n-dimensional Euclidean sphere with radius r and dσ(e) be unnormalized uniform measure on ndimensional Euclidean sphere. From this it follows that Sn(r) = Sn(1)rn−1, SSn−n(11()1) = nn√−π1 ΓΓ(( nn++2 21 )) .
2
Besides, let ϕ be the angle between s and e. Then

π

E[ s, e 4] = Sn1(1) s, e 4dσ(ϕ) = Sn1(1) ||s||42 cos3 ϕSn−1(sin ϕ)dϕ

S

0

= ||s||4 Sn−1(1)

π
cos4 ϕ sinn−2 ϕdϕ = ||s||4 ·

n√−1 Γ( n+2 2 )

π
cos4 ϕ sinn−2 ϕdϕ.

2 Sn(1)

2 n π Γ( n+1 )

0

20

(79)

33

Compute the integral:

π

π

2

cos4 ϕ sinn−2 ϕdϕ = 2 cos4 ϕ sinn−2 ϕdϕ = /t = sin2 ϕ/

0

0

π 2

5 n−1

3 1 1 n−1

√ n−1

=

n−3

3

t 2 (1 − t) 2 dt

=

B( n−1 ,

5)

=

Γ( 2 )Γ(

2

)=

2 · 2 Γ( 2 )Γ(

2

)=

3

·

πΓ( 2 ) .

0 2 2 Γ( n+2 4 ) n+2 2 ·Γ( n+2 2 ) n+2 2Γ( n+2 2 )

From this and (79) we obtain

E[ s, e 4] = ||s||4 · n√−1 Γ( n+2 2 ) ·

3

√ π

Γ(

n−

1

)

·

2

2 n π Γ( n+2 1 ) n+2 2Γ( n+2 2 )

= ||s||4 · 3(n−1) · Γ( n−2 1 ) = 3||s||42 x 3||s||42 .

2 2n(n+2) n−2 1 Γ( n−2 1 ) n(n+2) n2

To prove (66), it remains to use (78), (80) and Cauchy-Schwartz inequality ((E[XY ])2 E[Y 2]):

x
E[ s, e 2||e||2]

E[ s, e 4] · E[||e||4]

√3||s||2

min{2q

−

1,

32

ln

n

−

8}n

2 q

−2.

q

q

2

(80) E[X2] ·

B Technical Results
Lemma B.1. Let a0, . . . , aN−1, b, R1, . . . , RN−1 be non-negative numbers such that

√ Rl 2 ·

l−1

l−1

ak + b αk+1Rk

k=0

k=1

l = 1, . . . , N,

where αk+1 = 96nk2+ρ2nL2 for all k ∈ N. Then for l = 1, . . . , N

l−1

l−1

ak + b αk+1Rk

k=0

k=1



2

l−1

√

l2



ak + 2b · 96n2ρnL2  .

k=0

(81) (82)

Proof. For l = 1 it is trivial inequality. Assume that (82) holds for some l < N and prove it for l + 1. From the induction assumption and (81) we obtain

√ Rl 2

l−1 ak + √2b · 96n2lρ2nL2 ,
k=0

(83)

34

whence

l

l

l−1

l−1

ak + b αk+1Rk = ak + b αk+1Rk + al + bαl+1Rl

k=0

k=1

k=0

k=1

2

x l−1 ak + √2b · 96n2lρ2nL2 + al + √2bαl+1 l−1 ak + √2b · 96n2lρ2nL2

k=0

k=0

l
= ak + 2
k=0

l−1 ak · √2b 96n2lρ2nL2 + 2b2 (96n2lρ4nL2)2 + √2bαl+1
k=0

l−1 ak + √2b · 96n2lρ2nL2
k=0

l
= ak + 2
k=0

l−1 √ ak · 2b
k=0

96n2lρ2nL2 + αl2+1

+ 2b2

yl
ak + 2
k=0

l ak · √2b 96(nl+2ρ1n)2L2 + 2b2 (96n(l2+ρ1n)L42)2 =
k=0

(96n2lρ4nL2)2 + αl+1 · 96n2lρ2nL2

2

l

√

(l+1)2

ak + 2b · 96n2ρnL2 ,

k=0

l−1
where x follows from the induction assumption and (83), y is because ak
k=0

96n2lρ2nL2 + αl2+1 = 1922l2n+2ρl+nL2 2

(96n2lρ4nL2)2 + αl+1 · 96n2lρ2nL2

l4+(l+2)l2 (96n2ρnL2)2

96(nl+2ρ1n)2L2 , (96n(l2+ρ1n)L42)2 .

l
ak and
k=0

Lemma B.2. Let a0, . . . , aN−1, b, R1, . . . , RN−1 be non-negative numbers such that

√

l−1

l−1

Rl 2 ·

ak + bα Rk l = 1, . . . , N.

(84)

k=0

k=1

Then for l = 1, . . . , N

l−1

l−1

ak + bα Rk

k=0

k=1



2

l−1

√



ak + 2bαl .

k=0

(85)

Proof. For l = 1 it is trivial inequality. Assume that (85) holds for some l < N and prove it for l + 1. From the induction assumption and (84) we obtain

√

l−1

√

Rl 2

ak + 2bαl ,

(86)

k=0

35

whence

yl
ak + 2
k=0

l

l

l−1

l−1

ak + bα Rk = ak + bα Rk + al + bαRl

k=0

k=1

k=0

k=1

2

x

l−1

√

√

l−1

√

ak + 2bαl + al + 2bα

ak + 2bαl

k=0

k=0

l
= ak + 2
k=0

l−1 √

√

ak · 2bαl + 2b2α2l2 + 2bα

k=0

l−1

√

ak + 2bαl

k=0

l
= ak + 2
k=0

l−1 √ ak · 2bα l + 21 + 2b2α2 l2 + l
k=0

l

√

ak · 2bα(l + 1) + 2b2α2(l + 1)2 =

2

l

√

ak + 2bα(l + 1) ,

k=0

k=0

l−1
where x follows from the induction assumption and (86), y is because ak
k=0

l
ak .
k=0

C Parameters tuning

In our analysis it is needed to choose αk+1 = 96nk2+ρ2nL2 for ARDD and α = 48nρ1nL2 . However, one can tune these parameters in order to achieve better convergence rate in practice. In our experiments

we choose αk+1 = γ · 96nk2+ρ2nL2 , α = γ · 48nρ1nL2 and tune numerical factor γ. In [42] authors

prove convergence results for stepsize6 α = √ 1 min √1 , √D˜ where D˜ is some numerical

n+4

4L n+4 σ N

constant, therefore, in our experiments with RSGD we use stepsizes α = γ· √ 1 min √1 , √1

n+4

4L n+4 N

where we also tune numerical factor γ.

C.1 Nesterov’s function
One can ﬁnd our numerical results with tuning stepsizes for each method in Figures 3-5. Our tests with Nesterov’s function show that for this problem ARDD E and RDD work better with γ ∈ [32, 64] and RSGF shows the best performance with γ ∈ [4, 10]. Interestingly, ARDD and RDD with p = 1 require to choose γ signiﬁcantly larger (of order 103 − 104) than for Euclidean methods in order to get competitive or even better convergence rate. Moreover, ARDD E, RDD E and RSGF disconverge for γ ≥ 64, 200, 20 respectively. So, our empirical observation is as follows: ARDD and RDD with non-Euclidean proximal setup are able to converge with signiﬁcantly larger stepsizes than its Euclidean counterpart.
We summarize best options for γ that we use in the experiments presented in Section 6 in Table 5.

C.2 Least squares problem
In addition to the tuning of γ in ARDD we also tried diﬀerent options for L2: instead of L2 from (64) we tried β · A√ F with diﬀerent β. We tried β = 0.001, 0.01, 0.1, 1, 2, 5 and 10, but the best
r 6If σ = 0, then one should ignore the second term in the minimum.

36

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

Nesterov's function, n = 100, L = 10 ARDD_E 1.0

100

Nesterov's function, n = 100, L = 10 ARDD_NE 1.0

100

Nesterov's function, n = 100, L = 10 RDD_E 1.0

10 1

ARDD_E 2.0 ARDD_E 4.0 ARDD_E 8.0

10 2

ARDD_NE 100.0 ARDD_NE 500.0 ARDD_NE 1000.0

10 1

RDD_E 2.0 RDD_E 4.0 RDD_E 8.0

10 3

ARDD_E 16.0

ARDD_NE 2000.0

10 2

RDD_E 16.0

10 5

ARDD_E 32.0

10 4

ARDD_NE 3000.0

10 3

RDD_E 32.0 RDD_E 64.0 RDD_E 128.0

10 7

10 6

10 4

RDD_E 150.0 RDD_E 175.0

10 9

10 8

10 5 10 6

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

100

Nesterov's function, n = 100, L = 10 RDD_NE 1.0

100

Nesterov's function, n = 100, L = 10 RSGF 1.0

10 1

RDD_NE 100.0 RDD_NE 500.0

10 1

RSGF 2.0 RSGF 4.0

10 2

RDD_NE 1000.0 RDD_NE 2000.0

10 2

RSGF 10.0

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

10 3

RDD_NE 3000.0 RDD_NE 5000.0 RDD_NE 10000.0

10 3

10 4

RDD_NE 12000.0

10 4

10 5

10 5

10 6

10 6

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

Figure 3: Stepsize tuning for ARDD, RDD and RSGF applied to minimize Nesterov’s function (62). We use E and NE to deﬁne 2 and 1 proximal setups respectively (see (8) and (9) for the details). Numbers in labels in upper right corners denote diﬀerent choices of γ that are used.

n = 100 n = 1000 n = 5000

ARDD E 32 32 32

ARDD NE 2000 2000 1000

RDD E 32 64 64

RDD NE 12000 3000 3000

RSGF 10 4 10

Table 5: The optimal choices of γ for ARDD, RDD and RSGF applied to minimize Nesterov’s function (62) for diﬀerent dimension n.

results were obtained for β = 0.01. One can ﬁnd our numerical results with tuning γ in Figure 6. Besides m = 50 we tried diﬀerent batch sizes. In general, the behaviour of the considered methods was similar after proper parameters tuning.
References
[1] Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In COLT 2010 - The 23rd Conference on Learning Theory, 2010.
[2] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 1200–1205, New York, NY, USA, 2017. ACM. arXiv:1603.05953.
[3] Zeyuan Allen-Zhu, Zheng Qu, Peter Richtarik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1110–1119, New York, New York, USA, 20–22 Jun 2016. PMLR. First appeared in arXiv:1512.09103.

37

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

Nesterov's function, n = 1000, L = 10

ARDD_E 1.0

10 1

ARDD_E 2.0 ARDD_E 4.0

ARDD_E 8.0

10 3

ARDD_E 16.0 ARDD_E 32.0

10 5

Nesterov's function, n = 1000, L = 10

ARDD_NE 1.0

10 1

ARDD_NE 1000.0 ARDD_NE 2000.0

10 3

10 5

100

Nesterov's function, n = 1000, L = 10
RDD_E 1.0

10 1

RDD_E 2.0 RDD_E 4.0

10 2

RDD_E 8.0 RDD_E 16.0

RDD_E 32.0

10 3

RDD_E 64.0 RDD_E 128.0

10 4

RDD_E 150.0 RDD_E 175.0

10 7
0

10 7

10 5

10 6

250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

100

Nesterov's function, n = 1000, L = 10
RDD_NE 1.0

100

Nesterov's function, n = 1000, L = 10
RSGF 1.0

10 1

RDD_NE 100.0 RDD_NE 1000.0

10 1

RSGF 2.0 RSGF 4.0

10 2

RDD_NE 2000.0 RDD_NE 3000.0

10 2

RSGF 10.0

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

10 3

10 3

1100 54 10 4

10 6

10 5

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

10 6 0 250000 50N00u00m7b50e00r0 o10f00o00r0a1c25l0e00c01a50ll0s00017500002000000

Figure 4: Stepsize tuning for ARDD, RDD and RSGF applied to minimize Nesterov’s function (62). We use E and NE to deﬁne 2 and 1 proximal setups respectively (see (8) and (9) for the details). Numbers in labels in upper right corners denote diﬀerent choices of γ that are used.

[4] A. S. Anikin, A. V. Gasnikov, P. E. Dvurechensky, A. I. Tyurin, and A. V. Chernov. Dual approaches to the minimization of strongly convex functionals with a simple structure under aﬃne constraints. Computational Mathematics and Mathematical Physics, 57(8):1262–1276, 2017.
[5] John W. Barrett and Leonid Prigozhin. A quasi-variational inequality problem in superconductivity. Mathematical Models and Methods in Applied Sciences, 20(5):679–706, 2010.
[6] John W. Barrett and Leonid Prigozhin. Lakes and rivers in the landscape: A quasi-variational inequality approach. Interfaces and Free Boundaries, 16(2):269–296, 2014.
[7] Anastasia Bayandina, Pavel Dvurechensky, Alexander Gasnikov, Fedor Stonyakin, and Alexander Titov. Mirror descent and convex optimization problems with non-smooth inequality constraints. In Pontus Giselsson and Anders Rantzer, editors, Large-Scale and Distributed Optimization, chapter 8, pages 181–215. Springer International Publishing, 2018. arXiv:1710.06612.
[8] Anastasia Bayandina, Alexander Gasnikov, and Anastasia Lagunovskaya. Gradient-free twopoints optimal method for non smooth stochastic convex optimization problem with additional small noise. Automation and remote control, 79(7), 2018. arXiv:1701.03821.
[9] Aaron Ben-Tal and Arkadi Nemirovski. Lectures on Modern Convex Optimization (Lecture Notes). Personal web-page of A. Nemirovski, 2015.
[10] Albert S. Berahas, Richard H. Byrd, and Jorge Nocedal. Derivative-free optimization of noisy functions via quasi-Newton methods. SIAM Journal on Optimization, 29(2):965–993, 2019.
[11] Albert S. Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and empirical comparison of gradient approximations in derivative-free optimization. arXiv:1905.01332, 2019.
38

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

Nesterov's function, n = 5000, L = 10

ARDD_E 1.0

10 1

ARDD_E 2.0 ARDD_E 4.0

ARDD_E 8.0

10 3

ARDD_E 16.0 ARDD_E 32.0

10 5

10 7

100

Nesterov's function, n = 5000, L = 10
ARDD_NE 1.0

ARDD_NE 100.0

10 2

ARDD_NE 1000.0 ARDD_NE 2000.0

10 4

10 6

10 8

100

Nesterov's function, n = 5000, L = 10
RDD_E 1.0

10 1

RDD_E 8.0 RDD_E 32.0

RDD_E 64.0

10 2

RDD_E 150.0 RDD_E 175.0

10 3

10 4

10 5

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0 10 6 0.0 0.2 0.4 0.6 0.8 1.0

Number of oracle calls

1e7

Number of oracle calls

1e7

Number of oracle calls

1e7

100

Nesterov's function, n = 5000, L = 10
RDD_NE 1.0

100

Nesterov's function, n = 5000, L = 10
RSGF 1.0

10 1

RDD_NE 100.0 RDD_NE 1000.0 RDD_NE 2000.0

10 1

RSGF 2.0 RSGF 4.0

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

10 2 10 3

RDD_NE 3000.0 RDD_NE 10000.0

10 2

RSGF 10.0

10 4

10 3

10 5

10 4

10 6

10 5

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e7

0.0

0.2Numbe0.r4 of ora0c.6le calls0.8

1.0 1e7

Figure 5: Stepsize tuning for ARDD, RDD and RSGF applied to minimize Nesterov’s function
(62). We use E and NE to deﬁne 2 and 1 proximal setups respectively (see (8) and (9) for the details). Number of oracle calls is divided by 107. Numbers in labels in upper right corners denote
diﬀerent choices of γ that are used.

[12] Albert S. Berahas, Liyuan Cao, and Katya Scheinberg. Global convergence rate analysis of a generic line search algorithm with noise. arXiv:1910.04055, 2019.
[13] Aleksandr Beznosikov, Eduard Gorbunov, and Alexander Gasnikov. Derivative-free method for composite optimization with applications to decentralized distributed optimization. IFACPapersOnLine, 2020. Accepted, arXiv:1911.10645.
[14] Aleksandr Beznosikov, Abdurakhmon Sadiev, and Alexander Gasnikov. Gradient-free methods for saddle-point problem. In A. Kononov and et al., editors, Mathematical Optimization Theory and Operations Research 2020, Cham, 2020. Springer International Publishing. accepted, arXiv:2005.05913.
[15] Lev Bogolubsky, Pavel Dvurechensky, Alexander Gasnikov, Gleb Gusev, Yurii Nesterov, Andrei M Raigorodskii, Aleksey Tikhonov, and Maksim Zhukovskii. Learning supervised pagerank with gradient-based and gradient-free optimization methods. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4914–4922. Curran Associates, Inc., 2016. arXiv:1603.00717.
[16] Raghu Bollapragada and Stefan M. Wild. Adaptive sampling quasi-Newton methods for derivative-free stochastic optimization. arXiv:1910.13516, 2019.
[17] R.P. Brent. Algorithms for Minimization Without Derivatives. Dover Books on Mathematics. Dover Publications, 1973.
[18] C. Cartis and K. Scheinberg. Global convergence rate analysis of unconstrained optimization methods based on probabilistic models. Mathematical Programming, 169(2):337–375, Jun 2018.

39

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

100 10 2 10 4 10 6 10 8 10 10

Least 0.0

squares problem, r = 300, n = 400
ARDD_E 0.0001 ARDD_E 0.001 ARDD_E 0.01 ARDD_E 0.1 ARDD_E 0.2 ARDD_E 0.5 ARDD_E 0.7 ARDD_E 1.0 ARDD_E 2.0

10 1 10 3 10 5 10 7 10 9 10 11

Least squares problem, r = 300, n = 400
ARDD_NE 0.1 ARDD_NE 1.0 ARDD_NE 5.0 ARDD_NE 10.0 ARDD_NE 20.0 ARDD_NE 50.0 ARDD_NE 75.0 ARDD_NE 100.0 ARDD_NE 125.0 ARDD_NE 200.0

Least squares problem, r = 300, n = 400

10 1

RDD_E 1.0 RDD_E 5.0

RDD_E 10.0

10 3

RDD_E 20.0 RDD_E 50.0

RDD_E 100.0

10 5

RDD_E 150.0 RDD_E 175.0

RDD_E 200.0

10 7

RDD_E 500.0 RDD_E 700.0

RDD_E 1000.0

10 9

RDD_E 2000.0

0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e8

0.0

0.2Numbe0.r4 of ora0c.6le calls0.8

1.0 1e8

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e8

Least squares problem, r = 300, n = 400
RDD_NE 1.0

100

Least squares problem, r = 300, n = 400
RSGF 1.0

10 1

RDD_NE 2000.0 RDD_NE 4000.0

10 1

RSGF 100.0 RSGF 1000.0

RDD_NE 8000.0 RDD_NE 16000.0

10 2

RSGF 2000.0 RSGF 4000.0

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

10 3

RDD_NE 32000.0

10 3

RDD_NE 64000.0 RDD_NE 128000.0

10 4

10 5

RDD_NE 200000.0 RDD_NE 400000.0

10 5

10 6

10 7

10 7

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e8

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 1.01e8

Figure 6: Stepsize tuning for ARDD, RDD and RSGF applied to solve least squares problem (63).
We use E and NE to deﬁne 2 and 1 proximal setups respectively (see (8) and (9) for the details). For all methods batch size m equals 50. By oracle call we mean one computation of functional value of a summand. Number of oracle calls is divided by 108.

[19] Augustin Cauchy. M´ethode g´en´erale pour la r´esolution des syst´emes d’´equations simultan´ees. Comptes rendus hebdomadaires des s´eances de l’Acad´emie des sciences, 55:536–538, 1847.
[20] Nicol`o Cesa-bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of online learning algorithms. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 359–366. MIT Press, 2002.
[21] Yuwen Chen, Antonio Orvieto, and Aurelien Lucchi. An accelerated DFO algorithm for ﬁnitesum convex functions. In Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2020. (accepted), arXiv:2007.03311.
[22] Alexey Chernov, Pavel Dvurechensky, and Alexander Gasnikov. Fast Primal-Dual Gradient Method for Strongly Convex Minimization Problems with Linear Constraints, pages 391–403. Springer International Publishing, Cham, 2016.
[23] A. Conn, K. Scheinberg, and L. Vicente. Introduction to Derivative-Free Optimization. Society for Industrial and Applied Mathematics, 2009.
[24] Cong D. Dang and Guanghui Lan. Stochastic block mirror descent methods for nonsmooth and stochastic optimization. SIAM J. on Optimization, 25(2):856–881, April 2015.
[25] Olivier Devolder. Stochastic ﬁrst order methods in smooth convex optimization. CORE Discussion Paper 2011/70, 2011.
[26] John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Trans. Information Theory, 61(5):2788–2806, 2015. arXiv:1312.2139.

40

[27] Darina Dvinskikh, Eduard Gorbunov, Alexander Gasnikov, Pavel Dvurechensky, and Cesar A. Uribe. On primal and dual approaches for distributed stochastic convex optimization over networks. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 7435–7440, 2019. arXiv:1903.09844.
[28] Darina Dvinskikh, Aleksandr Ogaltsov, Alexander Gasnikov, Pavel Dvurechensky, and Vladimir Spokoiny. On the line-search gradient methods for stochastic optimization. IFACPapersOnLine, 2020. Accepted, arXiv:1911.08380.
[29] P. E. Dvurechensky, A. V. Gasnikov, and A. A. Lagunovskaya. Parallel algorithms and probability of large deviation for stochastic convex optimization problems. Numerical Analysis and Applications, 11(1):33–37, Jan 2018. arXiv:1701.01830.
[30] Pavel Dvurechensky, Darina Dvinskikh, Alexander Gasnikov, Csar A. Uribe, and Angelia Nedi´c. Decentralize and randomize: Faster algorithm for Wasserstein barycenters. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, NeurIPS 2018, pages 10783–10793. Curran Associates, Inc., 2018. arXiv:1806.03915.
[31] Pavel Dvurechensky and Alexander Gasnikov. Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. Journal of Optimization Theory and Applications, 171(1):121–145, 2016.
[32] Pavel Dvurechensky, Alexander Gasnikov, Evgenia Gasnikova, Sergey Matsievsky, Anton Rodomanov, and Inna Usik. Primal-dual method for searching equilibrium in hierarchical congestion population games. In Supplementary Proceedings of the 9th International Conference on Discrete Optimization and Operations Research and Scientiﬁc School (DOOR 2016) Vladivostok, Russia, September 19 - 23, 2016, pages 584–595, 2016. arXiv:1606.08988.
[33] Pavel Dvurechensky, Alexander Gasnikov, and Alexander Tiurin. Randomized similar triangles method: A unifying framework for accelerated randomized optimization methods (coordinate descent, directional search, derivative-free method). arXiv:1707.08486, 2017.
[34] Vaclav Fabian. Stochastic approximation of minima with improved asymptotic speed. Ann. Math. Statist., 38(1):191–200, 02 1967.
[35] Olivier Fercoq and Peter Richt´arik. Accelerated, parallel, and proximal coordinate descent. SIAM Journal on Optimization, 25(4):1997–2023, 2015. First appeared in arXiv:1312.5799.
[36] A. V. Gasnikov and P. E. Dvurechensky. Stochastic intermediate gradient method for convex optimization problems. Doklady Mathematics, 93(2):148–151, Mar 2016.
[37] A. V. Gasnikov, P. E. Dvurechensky, M. E. Zhukovskii, S. V. Kim, S. S. Plaunov, D. A. Smirnov, and F. A. Noskov. About the power law of the pagerank vector component distribution. Part 2. The Buckley–Osthus model, veriﬁcation of the power law for this model, and setup of real search engines. Numerical Analysis and Applications, 11(1):16–32, 2018.
[38] A. V. Gasnikov, E. V. Gasnikova, P. E. Dvurechensky, A. A. M. Mohammed, and E. O. Chernousova. About the power law of the pagerank vector component distribution. Part 1.
41

Numerical methods for ﬁnding the pagerank vector. Numerical Analysis and Applications, 10(4):299–312, 2017.
[39] A. V. Gasnikov, E. A. Krymova, A. A. Lagunovskaya, I. N. Usmanova, and F. A. Fedorenko. Stochastic online optimization. single-point and multi-point non-linear multi-armed bandits. convex and strongly-convex case. Automation and Remote Control, 78(2):224–234, Feb 2017. arXiv:1509.01679.
[40] A. V. Gasnikov, A. A. Lagunovskaya, I. N. Usmanova, and F. A. Fedorenko. Gradient-free proximal methods with inexact oracle for convex stochastic nonsmooth optimization problems on the simplex. Automation and Remote Control, 77(11):2018–2034, Nov 2016. arXiv:1412.3890.
[41] Alexander Gasnikov, Pavel Dvurechensky, and Ilnura Usmanova. On accelerated randomized methods. Proceedings of Moscow Institute of Physics and Technology, 8(2):67–100, 2016. In Russian, ﬁrst appeared in arXiv:1508.02182.
[42] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. arXiv:1309.5549.
[43] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1):267–305, 2016. arXiv:1308.6594.
[44] Eduard Gorbunov, Darina Dvinskikh, and Alexander Gasnikov. Optimal decentralized distributed algorithms for stochastic convex optimization. arXiv preprint arXiv:1911.07363, 2019.
[45] Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov. An accelerated method for derivative-free smooth stochastic convex optimization. arXiv:1802.09022, 2018.
[46] Xiaowei Hu, Prashanth L.A., Andrs Gyrgy, and Csaba Szepesvari. (bandit) convex optimization with biased noisy gradient oracles. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 819–828, Cadiz, Spain, 09–11 May 2016. PMLR.
[47] Anatoli Juditsky and Yuri Nesterov. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stochastic Systems, 4(1):44–80, 2014.
[48] Dmitry Kamzolov, Pavel Dvurechensky, and Alexander V. Gasnikov. Universal intermediate gradient method for convex problems with inexact oracle. Optimization Methods and Software, 0(0):1–28, 2020. arXiv:1712.06036.
[49] K. Kim, Yu. Nesterov, V. Skokov, and B. Cherkasskii. Eﬀektivnii algoritm vychisleniya proisvodnyh i ekstremalnye zadachi (eﬃcient algorithm for calculation of derivatives and extreme problems). Ekonomika i matematicheskie metody, 20(2):309–318, 1984.
[50] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365–397, Jun 2012. Firs appeared in June 2008.
42

[51] Jeﬀrey Larson, Matt Menickelly, and Stefan M. Wild. Derivative-free optimization methods. Acta Numerica, 28:287404, 2019.
[52] Yin Tat Lee and Aaron Sidford. Eﬃcient accelerated coordinate descent methods and faster algorithms for solving linear systems. In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS ’13, pages 147–156, Washington, DC, USA, 2013. IEEE Computer Society. First appeared in arXiv:1305.1922.
[53] Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3059–3067. Curran Associates, Inc., 2014. First appeared in arXiv:1407.1296.
[54] Boris S. Mordukhovich and Jiri V. Outrata. Coderivative analysis of quasivariational inequalities with applications to stability and optimization. SIAM Journal on Optimization, 18(2):389–412, 2007.
[55] A.S. Nemirovsky and D.B. Yudin. Problem Complexity and Method Eﬃciency in Optimization. J. Wiley & Sons, New York, 1983.
[56] Yurii Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004.
[57] Yurii Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012. First appeared in 2010 as CORE discussion paper 2010/2.
[58] Yurii Nesterov. Universal gradient methods for convex optimization problems. Mathematical Programming, 152(1):381–404, 2015.
[59] Yurii Nesterov, Alexander Gasnikov, Sergey Guminov, and Pavel Dvurechensky. Primal-dual accelerated gradient methods with small-dimensional relaxation oracle. Optimization Methods and Software, pages 1–28, 2020. arXiv:1809.05895.
[60] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Found. Comput. Math., 17(2):527–566, April 2017. First appeared in 2011 as CORE discussion paper 2011/16.
[61] Yurii Nesterov and Sebastian U. Stich. Eﬃciency of the accelerated coordinate descent method on structured optimization problems. SIAM Journal on Optimization, 27(1):110–123, 2017. First presented in May 2015 http://www.mathnet.ru:8080/PresentFiles/11909/7_ nesterov.pdf.
[62] Warren B. Powell. A uniﬁed framework for stochastic optimization. European Journal of Operational Research, 275(3):795 – 821, 2019.
[63] Leonid Prigozhin. Variational model of sandpile growth. European Journal of Applied Mathematics, 7(3):225235, 1996.
[64] H. H. Rosenbrock. An automatic method for ﬁnding the greatest or least value of a function. The Computer Journal, 3(3):175–184, 1960.
43

[65] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 64–72, Bejing, China, 22–24 Jun 2014. PMLR. First appeared in arXiv:1309.2375.
[66] Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with twopoint feedback. Journal of Machine Learning Research, 18:52:1–52:11, 2017. First appeared in arXiv:1507.08752.
[67] James C. Spall. Introduction to Stochastic Search and Optimization. John Wiley & Sons, Inc., New York, NY, USA, 1 edition, 2003.
[68] Fedor Stonyakin, Alexander Tyurin, Alexander Gasnikov, Pavel Dvurechensky, Artem Agafonov, Darina Dvinskikh, Dmitry Pasechnyuk, Sergei Artamonov, and Victorya Piskunova. Inexact relative smoothness and strong convexity for optimization and variational inequalities by inexact model. arXiv:2001.09013, 2020.
[69] Fedor S. Stonyakin, Darina Dvinskikh, Pavel Dvurechensky, Alexey Kroshnin, Olesya Kuznetsova, Artem Agafonov, Alexander Gasnikov, Alexander Tyurin, C´esar A. Uribe, Dmitry Pasechnyuk, and Sergei Artamonov. Gradient methods for problems with inexact model of the objective. In Michael Khachay, Yury Kochetov, and Panos Pardalos, editors, Mathematical Optimization Theory and Operations Research, pages 97–114, Cham, 2019. Springer International Publishing. arXiv:1902.09001.
[70] E. A. Vorontsova, A. V. Gasnikov, E. A. Gorbunov, and P. E. Dvurechenskii. Accelerated gradient-free optimization methods with a non-euclidean proximal operator. Automation and Remote Control, 80(8):1487–1501, 2019.
[71] R. E. Wengert. A simple automatic derivative evaluation program. Commun. ACM, 7(8):463– 464, August 1964.
44

