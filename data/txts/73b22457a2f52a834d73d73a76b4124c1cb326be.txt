arXiv:2201.03398v2 [cs.GT] 6 Apr 2022

Multiplayer Performative Prediction: Learning in Decision-Dependent Games∗
Adhyyan Narang†, Evan Faulkner∗, Dmitriy Drusvyatskiy‡, Maryam Fazel∗, Lillian J. Ratliﬀ∗
April 8, 2022
Abstract
Learning problems commonly exhibit an interesting feedback mechanism wherein the population data reacts to competing decision makers’ actions. This paper formulates a new game theoretic framework for this phenomenon, called multi-player performative prediction. We focus on two distinct solution concepts, namely (i) performatively stable equilibria and (ii) Nash equilibria of the game. The latter equilibria are arguably more informative, but can be found eﬃciently only when the game is monotone. We show that under mild assumptions, the performatively stable equilibria can be found eﬃciently by a variety of algorithms, including repeated retraining and the repeated (stochastic) gradient method. We then establish transparent suﬃcient conditions for strong monotonicity of the game and use them to develop algorithms for ﬁnding Nash equilibria. We investigate derivative free methods and adaptive gradient algorithms wherein each player alternates between learning a parametric description of their distribution and gradient steps on the empirical risk. Synthetic and semi-synthetic numerical experiments illustrate the results.
1 Introduction
Supervised learning theory and algorithms crucially rely on the training and testing data being generated from the same distribution. This assumption, however, is often violated in contemporary applications because data distributions may “shift” in reaction to the decision maker’s actions. Indeed, supervised learning algorithms are increasingly being trained on data that is generated by strategic or even adversarial agents, and deployed in environments that react to the decisions that the algorithm makes. In such settings, the model learned on the training data may be unsuitable for downstream inference and prediction tasks.
The method most commonly used in machine learning practice to address such distributional shifts is to periodically retrain the model to adapt to the changing distribution [Diethe et al., 2019, Wu et al., 2020]. Consequently, it is important to understand when such retraining heuristics converge and what types of solutions they ﬁnd. Despite the ubiquity of retraining heuristics in practice, one should be aware that training without consideration of strategic eﬀects or decisiondependence can lead to unintended consequences, including reinforcing bias. This is a concern for applications with potentially signiﬁcant social impact, such as predictive policing [Lum and
∗Drusvyatskiy’s research was supported by NSF DMS-1651851 and CCF-2023166 awards. Ratliﬀ’s research was supported by NSF CNS-1844729 and Oﬃce of Naval Research YIP Award N000142012571. Fazel’s research was supported in part by awards NSF TRIPODS II-DMS 2023166, NSF TRIPODS-CCF 1740551, and NSF CCF 2007036.
†Department of Electrical and Computer Engineering, University of Washington, Seattle ‡Department of Mathematics, University of Washington, Seattle
1

Isaac, 2016], criminal sentencing [Angwin et al., 2016, Courtland, 2018], pricing equity in ride-share markets [Chen et al., 2015], and loan or job procurement [Bartlett et al., 2019].
Optimization over decision-dependent probabilities has classical roots in operations research; see for example the review article of [Hellemo et al., 2018] and references therein. The more recent work of [Perdomo et al., 2020], motivated by the strategic classiﬁcation literature [Dong et al., 2018, Hardt et al., 2016, Miller et al., 2020], sets forth an elegant framework—aptly named performative prediction—for modeling decision-dependent data distributions in machine learning settings. There is now extensive research that develops algorithms for performative prediction by leveraging advances in convex optimization [Brown et al., 2020, Drusvyatskiy and Xiao, 2020, Mendler-Du¨nner et al., 2020, Miller et al., 2021, Perdomo et al., 2020].
The existing strategic classiﬁcation and performative prediction literature focuses solely on the interplay between a single learner and the population that reacts to the learner’s actions. However, learning algorithms in practice are often deployed alongside other algorithms which may even be competing with one another. Concrete examples to keep in mind are those of college admissions and loan procurement, wherein the applicants may tailor their proﬁle to make them more desirable for the college of their choice, or the loan with the terms (such as interest rate) that match the applicant’s current socio-economic and ﬁscal situation. In these cases, there are multiple competing learners (colleges, banks) and the population reacts based on the admissions policies of all the colleges (or banks) simultaneously. Examples of this type are widespread in applications; we provide further motivating vignettes in Section 3.
1.1 Contributions
We formulate the ﬁrst game theoretic model for decision-dependent learning in the presence of competition, called multi-player performative prediction.1 This is a new class of stochastic games that model a variety of machine learning problems arising in many practical applications. The model captures, as a special case, important problems including strategic classiﬁcation in settings with multiple decision-making entities that model learning when each entity’s data distribution depends on the action taken. It opens up an entire new class of problems that can be studied to determine important socio-economic implications of using machine learning algorithms (including classiﬁers and predictors) in settings where the “data” generated for training is produced by strategic users that react based on their own internal preferences.
We focus on two solution concepts for such games: (i) performatively stable equilibria and (ii) Nash equilibria. The former arises naturally when decision-makers employ na¨ıve repeated retraining algorithms. This is very common practice, and hence it is important to understand the equilibrium to which such algorithms converge and precisely when they do so. We show that performatively stable equilibria are sure to exist and to be unique under reasonable smoothness, convexity, and Lipschitz assumptions (Section 4). Moreover, repeated training and the (stochastic) gradient methods succeed at ﬁnding such equilibrium strategies. The ﬁnite time eﬃciency estimates (or iteration complexity) we obtain reduce to state-of-the art guarantees in the single player setting.
In some applications, a performatively stable equilibrium may be a poor solution concept and instead a Nash equilibrium may be desirable. In particular, as machine learning algorithms become more sophisticated, in the sense that at the time of learning decision-dependence is taken into consideration, a more natural equilibrium concept is a Nash equilibrium. Aiming towards algorithms for ﬁnding Nash equilibria, we develop transparent conditions ensuring strong monotonicity of the game (Section 5). Assuming that the game is strongly monotone, we then discuss a number
1A preliminary version of this paper appeared in the Proceedings of the 25th International Conference on Artiﬁcial Intelligence and Statistics, 2022.
2

of algorithms for ﬁnding Nash equilibria (Section 6). In particular, derivative-free methods are immediately applicable but have a high sample complexity O( d22 ) [Bravo et al., 2018, Drusvyatskiy et al., 2021]. Seeking faster algorithms, we introduce an additional assumption that the data distribution depends linearly on the performative eﬀects of all the players. When the players know explicitly how the distribution depends on their own performative eﬀects, but not those of their competitors, a simple stochastic gradient method is applicable and comes equipped with an eﬃciency guarantee of O( dε ). Allowing players to know their own performative eﬀects may be unrealistic in some settings. Consequently, we propose an adaptive algorithm in the setting when the data distribution has an amenable parametric description. In the algorithm, the players alternate between estimating the parameters of the distribution and optimizing their loss, again with only empirical samples of their individual gradients given the estimated parameters. The sample complexity for this algorithm, up to variance terms, matches the rate O( dε ) of the stochastic gradient method.
Finally, we present illustrative numerical experiments using both a synthetic example to validate the theoretical bounds, and a semi-synthetic example generated using data from multiple ride-share platforms (Section 7).
1.2 Related Work
Performative Prediction. The multiplayer setting in the present paper is inspired by the single player performative prediction framework introduced by [Perdomo et al., 2020], and further reﬁned by [Mendler-Du¨nner et al., 2020] and [Miller et al., 2021]. These works introduce the notions of performative optimality and stability, and show that repeated retraining and stochastic gradient methods convergence to a stable point. Subsequently, [Drusvyatskiy and Xiao, 2020] showed that a variety of popular gradient-based algorithms in the decision-dependent setting can be understood as the analogous algorithms applied to a certain static problem corrupted by a vanishing bias. In general, performative stability does not imply performative optimality. Seeking to develop algorithms for ﬁnding performatively optimal points, [Miller et al., 2021] provide suﬃcient conditions for the prediction problem to be convex. For decision-dependent distributions described as location families, [Miller et al., 2021] additionally introduce a two-stage algorithm for ﬁnding performatively optimal points. The paper [Izzo et al., 2021] instead focuses on algorithms that estimate gradients with ﬁnite diﬀerences. The performative prediction framework is largely motivated by the problem of strategic classiﬁcation [Hardt et al., 2016]. This problem has been studied extensively from the perspective of causal inference [Bechavod et al., 2020, Miller et al., 2020] and convex optimization [Dong et al., 2018].
Another line of work in performative prediction has focused on the setting in which the environment evolves dynamically in time or experiences time drift. This line of work is more closely related to reinforcement learning wherein a decision maker attempts to maximize their reward over time given that the stochastic environment depends on their decision. In particular, [Brown et al., 2020] formulate a time-dependent performative prediction problem such that the decisionmaker seeks to optimize the stationary reward—i.e., the reward under the ﬁxed point distribution induced by the player’s decision. Repeated retraining algorithms seeking the performatively stable solution to this problem are studied. In contrast, [Ray et al., 2022] study performative prediction in geometrically decaying environments, and provide conditions and algorithms that lead to the performatively optimal solution. The papers [Cutler et al., 2021] and [Wood et al., 2021] study performative prediction problems wherein the environment is drifting not only due to the action of the decision maker but also in time. These two papers analyze the tracking eﬃciency of the proximal stochastic gradient method and projected gradient descent under time drift.
3

Gradient-Based Learning in Continuous Games. There is a broad and growing literature on learning in games. We focus on the most relevant subset: gradient-based learning in continuous games. In his seminal work, [Rosen, 1965] showed that convex games which are diagonal strictly convex admit a unique Nash equilibrium and the gradient method converges to it. There is a large literature extending this work to more general games. For instance, [Ratliﬀ et al., 2016] provide a characterization of Nash equilibria in non-convex continuous games, and show that continuous time gradient dynamics locally converge to Nash; building on this work, [Chasnov et al., 2020] provide local convergence rates that extend to global rates when the game admits a potential function or is strongly monotone.
Under the assumption of strong monotonicity, the iteration complexity of stochastic and derivative-free gradient methods has also been obtained [Bravo et al., 2018, Drusvyatskiy et al., 2021, Mertikopoulos and Zhou, 2019]. Relaxing strong monotonicity to monotonicity, Tatarenko and Kamgarpour [2019, 2020] show that the stochastic gradient and derivative free gradient methods— i.e., where players use a single-point query of the loss to construct an estimate of their individual gradient of a smoothed version of their loss function—converge asymptotically. The approach to deal with the lack of strong monotonicity is to add a regularization term that decays to zero asymptotically. The update players employ in this regularized game is then analyzed as a stochastic gradient method with an additional bias term. We take a similar perspective to Tatarenko and Kamgarpour [2019, 2020] and [Drusvyatskiy and Xiao, 2020] in the analysis of all the algorithms we study—namely, we view the updates as a stochastic gradient method with additional bias.
Stochastic programming. Stochastic optimization problems with decision-dependent uncertainties have appeared in the classical stochastic programming literature, such as [Ahmed, 2000, Dupacov´a, 2006, Jonsbr˚aten et al., 1998, Rubinstein and Shapiro, 1993, Varaiya and Wets, 1988]. We refer the reader to the recent paper [Hellemo et al., 2018], which discusses taxonomy and various models of decision dependent uncertainties. An important theme of these works is to utilize structural assumptions on how the decision variables impact the distributions. Consequently, these works sharply deviate from the framework explored in [Mendler-Du¨nner et al., 2020, Miller et al., 2021, Perdomo et al., 2020] and from our paper. Time-varying problems have also been studied under the title “nonstationary optimization problems” in, e.g., [Ermoliev, 1988, Gaivoronskii, 1978], where it is assumed that the time varying functions converges to a limit but there is no explicit assumption on decision or state-feedback.
2 Notation and Preliminaries
This section records basic notation that we will use. A reader that is familiar with convex games and the Wasserstein-1 distance between probability measures may safely skip this section. Throughout, we let Rd denote a d-dimensional Euclidean space, with inner produce ·, · and the induced norm
x = x, x . The projection of a point y ∈ Rd onto a set X ⊂ Rd will be denoted by
projX (y) = argmin x − y .
x∈X
The normal cone to a convex set X at x ∈ X , denoted by NX (x) is the set
NX (x) = {v ∈ Rd : v, y − x ≤ 0 ∀y ∈ X }.
4

2.1 Convex Games and Monotonicity

Fix an index set [n] = {1, . . . , n}, integers di for i ∈ [n], and set d =

n i=1

di.

We will always

decompose vectors x ∈ Rd as x = (x1, . . . , xn) with xi ∈ Rdi. Given an index i, we abuse notation

and write x = (xi, x−i), where x−i denotes the vector of all coordinates except xi. A collection of

functions Li : Rd → R and sets Xi ⊂ Rdi, for i ∈ [n], induces a game between n players, wherein

each player i seeks to solve the problem

min Li(xi, x−i).

(1)

xi∈Xi

Deﬁne the joint action space X = X1 × · · · × Xn. A vector x ∈ Rd is called a Nash equilibrium of the game (1) if the condition

xi ∈ argmin Li(xi, x−i)
xi∈Xi

holds for each i ∈ [n].

Thus x is a Nash equilibrium if each player i has no incentive to deviate from xi when the strategies of all other players remain ﬁxed at x−i.
We use ∇iL(x) to denote the derivative of L(·) with respect to xi. With this notation, we deﬁne
the vector of individual gradients

H(x) := (∇1L1(x), . . . , ∇nLn(x)).

This map arises naturally from writing down ﬁrst-order optimality conditions corresponding to (1) for each player. Namely, we say that (1) is a C1-smooth convex game if the sets Xi are closed and convex, the functions Li(·, x−i) are convex (i.e., Li is convex in xi when x−i are ﬁxed), and the partial gradients ∇iLi(x) exist and are continuous. Thus, the Nash equilibria x are characterized by the inclusion
0 ∈ H(x ) + NX (x ).
Generally speaking, ﬁnding global Nash equilibria is only possible for “monotone” games. A C1-smooth convex game is called α-strongly monotone (for α ≥ 0) if it satisﬁes
H(x) − H(x ), x − x ≥ α x − x 2 for all x, x ∈ Rd.

If this condition holds with α = 0, the game is simply called monotone. It is well-known from [Rosen, 1965] that α-strongly monotone games (with α > 0) over convex, closed and bounded strategy sets X admit a unique Nash equilibrium x , which satisﬁes
H(x), x − x ≥ α x − x 2 for all x ∈ X .

2.2 Probability Measures and Gradient Deviation
To simplify notation, we will always assume that when taking expectations with respect to a measure that the expectation exists and that integration and diﬀerentiation operations may be swapped whenever we encounter them. These assumptions are completely standard to justify under uniform integrability conditions.
We will be interested in random variables taking values in a metric space. Given a metric space Z with metric d(·, ·), the symbol P(Z) will denote the set of Radon probability measures µ on Z with a ﬁnite ﬁrst moment Ez∼µ[d(z, z0)] < ∞ for some z0 ∈ Z. We measure the deviation between two measures µ, ν ∈ Z using the Wasserstein-1 distance:

W1(µ, ν) = sup E [h(X)] − E [h(Y )] ,

(2)

h∈Lip1 X∼µ

Y ∼ν

5

where Lip1 denotes the set of 1-Lipschitz continuous functions h : Z → R. Fix a function g : Rd×Z → R and a measure µ ∈ P(Z), and deﬁne the expected loss
fµ(x) = E g(x, z).
z∼µ
The following standard result shows that the Wasserstein-1 distance controls how the gradient ∇fµ(x) varies with respect to µ; see, for example, Drusvyatskiy and Xiao [2020, Lemmas 1.1, 2.1] or [Perdomo et al., 2020, Lemma C.4] for a short proof.
Lemma 1 (Gradient deviation). Fix a function g : Rd × Z → R such that g(·, z) is C1-smooth for all z ∈ Z and the map z → ∇xg(x, z) is β-Lipschitz continuous for any x ∈ Rd. Then for any measures µ, ν ∈ P(Z), the estimate holds:
sup ∇fµ(x) − ∇fν(x) ≤ β · W1(µ, ν).
x

3 Decision-Dependent Games

We model the problem of n decision-makers, each facing a decision-dependent learning problem, as an n-player game. Each player i ∈ [n] seeks to solve the decision-dependent optimization problem

min Li(xi, x−i) where Li(x) := E i(x, zi).

(3)

xi∈Xi

zi∼Di(x)

Throughout, we suppose that each set Xi lies in the Euclidean space Rdi and we set d =

n i=1

di.

The loss function for the i’th player is denoted as i : Rd × Zi → R, where Zi is some metric space

and Di(x) ∈ P(Zi) is a probability measure that depends on the joint decision x ∈ X and the player

i ∈ [n]. Observe that the random variable zi in the objective function of player i is governed by the

distribution Di(x), which crucially depends on the strategies x = (x1, . . . , xn) chosen by all players.

This is worth emphasizing: the parameters chosen by one player have an inﬂuence on the data seen

by all other players. This is one of the critical ways in which the problems for the diﬀerent players

are strategically coupled. The other is directly through the loss function i which also depends on

the joint decision x. These two sources of strategic coupling are why the game theoretic abstraction

naturally arises. It is worth keeping in mind that in most practical settings (see the upcoming

Vignettes 1 and 2), the loss functions i(x, zi) depend only on xi, that is i(x, zi) ≡ i(xi, zi). If

this is the case, we will call the game separable (which refers to separable losses, not distributions).

Thus, for separable games, the coupling among the players is due entirely to the distribution Di(x)

that depends on the actions of all the players.

Remark 1. The decision-dependence in the distribution map may encode the reaction of strategic users in a population to the announced joint decision x; hence, in these cases there is also a “game” between the decision-makers and the strategic users in the environment—a game with a diﬀerent interaction structure known as a Stackelberg game [Von Stackelberg, 2010]. This level of strategic interaction between decision-maker and strategic user is abstracted away to an aggregate level in Di(xi, x−i). The game between a single decision maker and the strategic user population has been studied widely (cf. Section 1.2). We leave it to future work to investigate both layers of strategic interaction simultaneously.

We assume that each player has full information of the other players’ parameters. This is a reasonable assumption in our setup: if the data population (e.g., strategic users) are able to respond to the players’ deployed decisions xi, the other players must be able to respond to these decisions as well. In essense, these decisions are publicly announced. The following vignettes based on practical applications motivate diﬀerent types of strategic coupling.

6

Vignette 1 (Multiplayer forecasting). Players have the same decision-dependent data distribution— namely, D ≡ Di ≡ Dj for all i, j ∈ [n]. Multiple mapping applications forecast the travel time between diﬀerent locations, yet the realized travel time is collectively inﬂuenced by all their forecasts. The decision-dependent players are the mapping applications (ﬁrms). The decision xi each player makes is the rule for recommending routes. Users choose routes, which then impact the realized travel time zi ≡ z ∼ D on the m diﬀerent road segments in the network observed by all ﬁrms.

Vignette 2. Players have diﬀerent distributions—i.e., Di ≡ Dj for all i, j ∈ [n]. (a) Multiplayer Strategic Classiﬁcation. Multiple universities classify students as accepted or rejected using applicant data, where each applicant designs their application to ﬁt desiderata of multiple universities. The data zi ∼ Di(x) is an application that university i receives, and as a decision-dependent player, each university i designs a classiﬁcation rule xi to determine which applicants are accepted. The goal of a university is to accept qualiﬁed candidates, and diﬀerent types of universities predominently cater to diﬀerent populations (e.g., liberal arts versus science and engineering), yet students may apply to multiple programs across many universities thereby resulting in distinct distributions Di that depend on the joint decision rule x.
(b) Revenue Maximization via Demand Forecasting. In a ride-share market, multiple platforms forecast demand for rides (respectively, supply of drivers) at diﬀerent locations in order to optimize their revenue by using the forecast to set prices. In most ride-share markets, drivers and passengers participate in multiple platforms by, e.g., “price shopping”. Hence, the forecasted demand zi ∼ Di(x) for platform i depends on their own decision xi as well as their competitors’ decisions x−i.

Prior formulations of decision dependent learning do not readily extend to the settings described in the vignettes without a game theoretic model. There are two natural solution concepts for the game (3). The ﬁrst is the classical notion of Nash equilibrium; we repeat the deﬁnition here for ease of reference.

Deﬁnition 1 (Nash Equilibrium). A vector x ∈ X is a Nash equilibrium of the game (3) if the

inclusion

xi ∈ argmin Li(xi, x−i)
xi∈Xi

holds for each i ∈ [n].

As previously observed, generally speaking, ﬁnding Nash equilibria is only possible for monotone games. The game (3) can easily fail to be monotone even if the game is separable and the loss functions i(·, z) are strongly convex. In Section 5, we develop suﬃcient conditions for (strong) monotonicity and use them to analyze algorithms for ﬁnding Nash equilibria. The suﬃcient conditions we develop, which are in line with those in the single player setting [Miller et al., 2021], are necessarily quite restrictive.
On the other hand, there is an alternative solution concept, which is more amenable to numerical methods, and reduces to performatively stable points of [Perdomo et al., 2020] in the single player setting. The idea is to decouple the eﬀects of a decision x on the integrand (x, z) and on the distribution D(x). Setting notation, any vector y ∈ X induces a static game (without performative eﬀects) wherein the distribution for player i is ﬁxed at Di(y), that is:

min Lyi (xi, x−i)
xi∈Xi

where

Lyi (xi, x−i) := E i(xi, x−i, zi).
zi∼Di(y)

G(y)

Notice that this is a parametric family of static games, indexed by y ∈ X .

7

Deﬁnition 2 (Performatively stable equilibria). A strategy vector x ∈ X is a performatively stable equilibrium of the game (3) if it is a Nash equilibrium of the static game G(x ) (with game G(·) as deﬁned above).

The diﬀerence between performatively stable equilibria and Nash equilibria of is that the governing distribution for player i is ﬁxed at Di(x ). Performatively stable equilibria have a clear intuitive meaning: each player i has no incentive to deviate from x having access only to data drawn from D(x ). Notice that if the game (3) is separable—the typical setting—the static game G(y) is entirely decoupled for any y in the sense that the problem that player i aims to solve depends only on xi and not on x−i. This enables a variety of single player optimization techniques to extend to the computation of performatively stable equilibria.
Nash and performatively stable equilibria are typically distinct, even in the single player setting as explained in [Perdomo et al., 2020]. This distinction is worth highlighting. Under mild smoothness assumptions, the chain rule directly implies the following expression for the gradient of player i’th objective:

d

∇iLi(xi, x−i) = E [∇i i(xi, x−i, zi)] +

E [ i(xi, x−i, zi)]

.

(4)

zi∼Di(x)

dui zi∼Di(ui,x−i)

ui=xi

Pi

Qi

If x is a Nash equilibrium of the game (3), then equality 0 = ∇iLi(xi, x−i) = Pi + Qi holds for all i ∈ [n]. On the other hand, provided the loss functions i are convex, x is a performatively stable equilibrium precisely when Pi = 0 for all i ∈ [n]. This clearly shows that the two solution concepts are typically distinct, since performative stability in essence ignores the term Qi on the right side of (4). It is an open question as to how these equilibrium concepts compare in terms of their eﬃciency relative to the social optimum. We explore this empirically in Section 7.
In the rest of the paper we impose the following assumption that is in line with the performative prediction literature.

Assumption 1 (Convexity and smoothness). There exist constants α > 0 and βi > 0 such that for each i ∈ [n], the following hold:
1. For any y ∈ X , the game G(y) is α-strongly monotone. 2. Each loss i(xi, x−i, zi) is C1-smooth in xi and the map zi → ∇i i(x, zi) is βi-Lipschitz
continuous for any x ∈ X .

It is worth noting that in the setting where the losses are seperable, the game G(y) is α-strongly monotone as long as each expected loss Ez∼Di(y) i(xi, zi) is α-strongly convex in xi. Assumption 1 alone does not imply convexity of the objective functions Li(xi, x−i) in xi nor monotonicity of the game (3) itself. Suﬃcient conditions for convexity and strong monotonicity of the game are given in Section 5.
Next, we require the distributions Di(x) to vary in a Lipschitz way with respect to x.
Assumption 2 (Lipschitz distributions). For each i ∈ [n], there exists γi > 0 satisfying

W1(Di(x), Di(y)) ≤ γi · x − y

for all x, y ∈ X .

In this case, we deﬁne the constant ρ :=

n i=1

(

βi γi α

)2

.

We will see in Theorem 1 that the constant ρ is fundamentally important for algorithms, since it characterizes settings in which algorithms can be expected to work. We end this section with some

8

convenient notation that will be used throughout. To this end, ﬁx two vectors x = (x1, . . . , xn) ∈ X and z = (z1, . . . , zn) ∈ Z1 × . . . × Zn. We then set

gi(x, zi) := ∇i i(x, zi) and g(x, z) := (g1(x, z1), . . . , gn(x, zn)).

Taking expectations deﬁne

Gi,y(x) := E gi(x, zi) and Gy(x) := (G1,y(x), . . . , Gn,y(x)).

(5)

zi∼Di(y)

Thus Gy(·) is the vector of individual gradients corresponding to the game G(y). Notice that we may write
Gy(x) := E g(x, z)
z∼Dπ (y)
where Dπ(y) := D1(y) × . . . × Dn(y) is the product measure. The following is a direct consequence of Lemma 1.

Lemma 2 (Deviation in the vector of individual gradients). Suppose Assumptions 1 and 2 hold. Then for every x, y, y ∈ X and index i ∈ [n], the estimates hold:

Gi,y(x) − Gi,y (x) ≤ βiγi · y − y ,

Gy(x) − Gy (x) ≤

n
βi2γi2 · y − y .
i=1

Proof. Using Lemma 1 and the standing Assumptions 1 and 2 we compute

Gi,y(x) − Gi,y (x)

= E ∇i i(x, zi) − E ∇i i(x, zi)

zi∼Di(y)

zi∼Di(y )

≤ βi · W1(Di(y), Di(y ))

≤ βiγi · y − y .

Therefore, we deduce

n

n

Gy(x) − Gy (x) 2 =

Gi,y(x) − Gi,y (x) 2 ≤ βi2γi2 · y − y 2.

i=1

i=1

The proof is complete.

4 Algorithms for Finding Performatively Stable Equilibria
The previous section isolated two solution concepts for decision dependent games: Nash equilibria and performatively stable equilibria. In this section, we discuss existence of the latter and algorithms for ﬁnding these. We discuss three algorithms: repeated retraining, the repeated gradient method, and the repeated stochastic gradient method. While the ﬁrst two are largely conceptual, the repeated stochastic gradient method is entirely implementable.
9

4.1 Repeated Retraining
Observe that performatively stable equilibria of (3) are precisely the ﬁxed points of the map

Nash(x) := {x ∈ X : x is a Nash equilibrium of the game G(x)}.

A natural algorithm is therefore repeated retraining, which is simply the ﬁxed point iteration

xt+1 = Nash(xt).

(6)

In the single player settings, this algorithm was studied in Perdomo et al. [2020]. Unrolling notation, given a current decision vector xt, the updated decision vector xt+1 is the Nash equilibrium of the
game wherein each player i ∈ [n] seeks to solve

min E i(yi, y−i, zi).

(7)

yi∈Xi zi∼Di(xt)

It is important to notice that since xt is ﬁxed, the game (8) is strongly monotone in light of
Assumption 1. Thus, in iteration t, the players play a Nash equilibrium (i.e., a best response) in this game induced by the prior joint decision xt. Importantly, despite the fact that xt+1 is a Nash equilibrium of a game in iteration t, the collective decision xt+1 is not a Nash equilibrium for the
multiplayer performative prediction problem (3). In fact, players have an incentive to change their action since it is possible that by changing xti, the change it induces in the distribution Di(·) reduces their expected loss.
The following theorem shows that in the regime ρ < 1, the game (3) admits a unique performa-
tively stable equilibrium and repeated retraining converges linearly.

Theorem 1 (Repeated retraining). Suppose Assumptions 1-2 hold and that we are in the regime ρ < 1. Then the game (3) admits a unique performatively stable equilibrium x and the repeated retraining process (6) converges linearly:

xt+1 − x ≤ ρ · xt − x

for all t ≥ 0.

Proof. We will show that the map Nash(·) is Lipschitz continuous with parameter ρ. To this end, consider two points x and x and set y := Nash(x) and y := Nash(x ). Note that ﬁrst order optimality conditions for y and y guarantee

Gx(y), y − y ≤ 0 and Gx (y ), y − y ≤ 0.

Strong monotonicity of the game G(x) therefore ensures

α · y − y 2 ≤ Gx(y) − Gx(y ), y − y ≤ Gx (y ) − Gx(y ), y − y ≤ Gx (y ) − Gx(y )) · y − y

n

≤

γi2βi2 · x − x · y − y ,

i=1

where the last inequality follows from Lemma 2. Dividing through by y − y guarantees that Nash(·) is indeed a contraction with parameter ρ. The result follows immediately from the Banach ﬁxed point theorem.

10

Theorem 1 shows that the interesting parameter regime is ρ < 1, since outside of this setting performative equilibria may even fail to exist. It is worth noting that when the game (3) is separable, each iteration of repeated retraining (6) becomes

min E i(yi, zi).

(8)

yi∈Xi zi∼Di(xt)

That is, the optimization problems faced by the players are entirely independent of each other. In
the separable case, the regime when repeated retraining succeeds can be slightly enlarged from ρ < 1 to ni=1 βαiγii 2 < 1, where αi is the strong convexity constant of the loss for player i. This is the content of the following theorem, whose proof is a slight modiﬁcation of the proof of Theorem 1.

Theorem 2 (Improved contraction for separable games). Suppose that the game (3) is separable and that each loss function Lyi (xi) = Ezi∼Di(y) i(xi, zi) is αi-strongly convex in xi for every y ∈ X . Suppose moreover that Assumptions 1 and 2 hold, and that we are in the regime ni=1 βαiγii 2 < 1.
The game (3) admits a unique performatively stable equilibrium x and the repeated retraining
process converges linearly:

xt+1 − x ≤

n

2

βαiγii · xt − x

i=1

for all t ≥ 0.

Proof. We will show that the map Nash(·) is Lipschitz continuous with parameter ni=1 βαiγii 2. To this end, consider two points x and x and set y := Nash(x) and y := Nash(x ). Note that ﬁrst order optimality conditions for y and y guarantee

Gi,x(y), yi − yi ≤ 0 and

Gi,x (y ), yi − yi ≤ 0 for all i ∈ [n].

Set

v

=

(

α1−1

,

.

.

.

,

α

−1 n

)

and

let

denote the Hadamard product between two vectors. Strong

convexity of the loss functions therefore ensures

y − y 2 ≤ αi−1 Gi,x(y) − Gi,x(y ), yi − yi
i
≤ αi−1 Gi,x (y ) − Gi,x(y ), yi − yi
i
= v (Gx (y ) − Gx(y )), y − y ≤ v (Gx (y ) − Gx(y )) · y − y
≤ n βi2γi2 · x − x · y − y , i=1 αi2

where the last inequality follows from Lemma 1 and the standing Assumptions 1 and 2. Dividing

through by y − y guarantees that Nash(·) is indeed a contraction with parameter The result follows immediately from the Banach ﬁxed point theorem.

n βi2γi2 .
i=1 α2i

11

4.2 Repeated Gradient Method
Repeated retraining is largely a conceptual algorithm since in each iteration it requires computation of the exact Nash equilibrium of a stochastic game (8). A more realistic algorithm would instead take a single gradient step on the game (8). With this in mind, given a step-size parameter η > 0, the repeated gradient method repeats the updates:
xt+1 = projX (xt − ηGxt (xt)).
More explicitly, in iteration t, each player i ∈ [n] performs the update

xti+1 = projXi xt − ηt

E

∇i

i

(x

t i

,

xt−

i

,

zi

)

.

zi∼Di(xt)

This algorithm is largely conceptual since each player needs to compute an expectation; nonetheless we next show that this process converges linearly under the following additional smoothness assumption.

Assumption 3 (Smoothness). For every y ∈ X , the vector of individual gradients Gy(x) is LLipschitz continuous in x.

The following is the main result of this section.

Theorem 3 (Repeated gradient method). Suppose that Assumptions 1-3 hold and that we are in

the

regime

ρ

<

1√ .
2+ 2

Then

the

iterates

xt

generated

by

the

repeated

gradient

method

with

η

=

α L2

converge linearly to the performatively stable equilibrium x —that is, the following estimate holds:





t+1

1

α2ρ

t

x −x ≤

α2 + L2  x − x

for all t ≥ 0.

(9)

1 + L2

Proof. Using the triangle inequality, we estimate

xt+1 − x

= projX (xt − ηGxt(xt)) − x ≤ projX (xt − ηGx (xt)) − x ≤ projX (xt − ηGx (xt)) − x

+ projX (xt − ηGxt(xt)) − projX (xt − ηGx (xt)) + η Gxt(xt)) − Gx (xt))

≤ projX (xt − ηGx (xt)) − x + η

βi2γi2 · xt − x ,

i

(10)

where the last inequality follows from Lemma 2. The rest of the argument is standard. We will

simply show that the ﬁrst term on the on right-side is a fraction of xt − x∗ . To this end, set yt+1 = projX (xt − ηGx (xt)). Since the function x → 12 xt − ηGx (xt) − x 2 is 1-strongly convex and yt+1 is its minimizer over X , we deduce

1 yt+1 − x 2 ≤ 1 xt − ηGx (xt) − x 2 − 1 xt − ηGx (xt) − yt+1 2.

2

2

2

Expanding the right hand side yields

1 yt+1 − x

2 ≤ 1 xt − x

2 − η Gx (xt), yt+1 − x

1 −

yt+1 − xt

2

2

2

2

1 =

xt − x

2 − η Gx (yt+1), yt+1 − x

(11)

2

− η Gx (xt) − Gx (yt+1), yt+1 − x

1 −

yt+1 − xt

2.

2

12

Strong convexity of the loss functions ensures

η Gx (yt+1), yt+1 − x ≥ η Gx (yt+1) − Gx (x ), yt+1 − x ≥ αη yt+1 − x 2.

(12)

Young’s inequality in turn implies

t

t+1 t+1

Gx (xt) − Gx (yt+1) 2 η2L2 yt+1 − x 2

η| Gx (x ) − Gx (y ), y − x | ≤

2L2

+ 2

xt − yt+1 2 η2L2 yt+1 − x 2

(13)

≤

+

,

2

2

where the last inequality follows from Assumption 3. Putting the estimates (11)-(13) together yields

Rearranging gives

1 yt+1 − x 2 ≤ 1 xt − x 2 − 2αη − η2L2 yt+1 − x 2.

2

2

2

yt+1 − x

2

≤

1 1+2αη−η2L2

xt − x

2. Returning to (10) we therefore conclude

xt+1 − x

 ≤

1 +η
1 + 2αη − η2L2


βi2γi2 xt − x .
i

Plugging

in

η

=

α L2

yields

the

claimed

estimate

(9).

An

elementary

argument

shows

that

in

the

assumed regime ρ < 1√ , the term in the parentheses in (9) is indeed smaller than one.

2+ 2

4.3 Repeated Stochastic Gradient Method

As observed earlier, the repeated gradient method is still largely a conceptual algorithm since an expectation has to be computed in every iteration. We next analyze an implementable algorithm that approximates the expectation in each step of gradient with an unbiased estimator. Namely, in each iteration t of the repeated stochastic gradient method, each player i ∈ [n] performs the update:

Sample zit ∼ Di(xt) Set xti+1 = projXi xti − η∇i i(xti, xt−i, zit) .

We will analyze the method under the following standard variance assumption.

Assumption 4 (Finite variance). There exists a constant σ ≥ 0 satisfying

E Gx(x) − g(x, z) 2 ≤ σ2
z∼Dπ (x)

for all x ∈ X .

Convergence analysis for the repeated stochastic gradient method will follow from the following simple observation. Noticing the equality Gx(x) = Ez∼Dπ(x) g(x, z), Lemma 2 directly implies that

Gx(x) − Gx (x) ≤ αρ x − x .

That is, we may interpret the repeated stochastic gradient method as a standard stochastic gradient algorithm applied to the static problem G(x ) with a bias that is linearly bounded by the distance to x . With this realization, we can forget about dynamics and simply analyze the stochastic gradient method on a static problem with this special form of bias. Appendix A does exactly that. In particular, the following is a direct consequence of Theorem 8 in Appendix A.
In the following theorem, let Et denote the conditional expectation with respect to the σ-algebra generated by (xl)l=1,...,t.

13

Theorem 4 (One-step improvement). Suppose that Assumptions 1-4 hold and that we are in the regime ρ < 1. Then with any η < α(81L−2ρ) , the repeated stochastic gradient method generates a sequence xt satisfying

Et xt+1 − x

2 1 + 2ηαρ + 2η2α2ρ2 t

≤

1+ρ

x −x

1 + 2ηα( 2 )

2

4η2σ2

+

1+ρ ,

1 + 2ηα( 2 )

where x is the performatively stable equilibrium of the game (3).

Proof. This follows directly from applying Theorem 8 in Appendix A with G(x) = Gx (x), gt = g(xt, zt), Ct = D = 0, and B = αρ.

With Theorem 4 at hand, it is straightforward to obtain global eﬃciency estimates under a variety of step-size choices. One particular choice, highlighted by [Ghadimi and Lan, 2013], is the step-decay schedule that periodically cuts η by a fraction. The resulting algorithm and its convergence guarantees are summarized in the following corollary.

Corollary 1 (Repeated stochastic gradient method with a step-decay schedule). Suppose that

Assumptions

1-4

hold

and

we

are

in

the

regime

ρ<

12 .

Set

η0

:=

α(1−ρ) 4

·

min{1,

1 2L2

}.

Consider

running the repeated stochastic gradient method in k = 0, . . . , K epochs, for Tk iterations each, with

constant step-size ηk = 2−kη0, and such that the last iterate of epoch k is used as the ﬁrst iterate in

epoch k + 1. Fix a target accuracy ε > 0 and suppose we have available a constant R ≥ x0 − x 2.

Set

T0 = (1−1ρ0)αη0 log( 2εR ) , Tk = (110−lρo)gα(4η)k for k ≥ 1,

and

K=

1 + log2

40η0σ2 (1−ρ)αε

.

The ﬁnal iterate x produced satisﬁes E x − x 2 ≤ ε, while the total number of iterations of the repeated stochastic gradient method is at most

L2

2R

σ2

O (1 − ρ)α2 · log ε + (1 − ρ)2α2ε .

Proof. Consider a sequence x0, x1, . . . , xt generated by the stochastic gradient method with a ﬁxed

step-size η ≤ η0. Using Theorem 4 together with the tower rule for conditional expectations, we

deduce

t+1

2 1 + 2ηαρ + 2η2α2ρ2 t

2

4η2σ2

E x −x ≤

1+ρ E x − x +

1+ρ .

(14)

1 + 2ηα( 2 )

1 + 2ηα( 2 )

Our choice of η0 ensures

1 + 2ηαρ + 2η2α2ρ2 1 + 2ηα · 1+43ρ

≤

1+ρ = 1 −

2ηα( 1−4 ρ )

1−ρ

1+ρ ≤ 1 − 10 ηα.

1 + 2ηα( 1+2 ρ )

1 + 2ηα · 2

1 + 2ηα( 2 )

Therefore iterating (14) we obtain the estimate

E xt+1 − x 2 ≤ (1 − ψ(η))t+1 x0 − x 2 + Γη,
where we set ψ(η) = cη with c = 11−0ρ α and Γ = α4(10−σ2ρ) . The result now follows directly from [Drusvyatskiy and Xiao, 2020, Lemma B.2].

The eﬃciency estimate in Corollary 1 coincides with the standard eﬃciency estimate for the stochastic gradient method on static problems, up to multiplication by (1 − ρ)−2.

14

5 Monotonicity of Decision-Dependent Games

Our next goal is to develop algorithms for ﬁnding true Nash equilibria of the game (3). As the ﬁrst step, this section presents suﬃcient conditions for the game to be monotone along with some examples. We note, however, that the suﬃcient conditions we present are strong, and necessarily so because the game (3) is typically not monotone. When specialized to the single player setting n = 1, the suﬃcient conditions we derive are identical to those in [Miller et al., 2021] although the proofs are entirely diﬀerent.
First, we impose the following mild smoothness assumption.

Assumption 5 (Smoothness of the distribution). For each index i ∈ [n] and point x ∈ X , the map ui → Ezi∼D(ui,x−i) i(x, zi) is diﬀerentiable at ui = xi and its derivative is continuous in x.

Under Assumption 5, the chain rule implies the following expression for the derivative of player i’s loss function

d

d

∇iLi(xi, x−i) =

E [ i(ui, x−i, zi)]

+

E [ i(xi, x−i, zi)]

.

dui zi∼Di(xi,x−i)

ui=xi dui zi∼Di(ui,x−i)

ui=xi

To simplify notation, deﬁne

d

Hi,x(y) :=

E

i(y, zi)

.

dui zi∼D(ui,x−i)

ui=xi

Therefore, we may equivalently write

∇iLi(xi, x−i) = Gi,x(x) + Hi,x(x)
where Gi,x(x) is deﬁned in (5). Stacking together the individual partial gradients Hi,x(y) for each player, we set Hx(y) = (H1,x(y), . . . , Hn,x(y)). Therefore the vector of individual gradients for (3) is simply the map D(x) := Gx(x) + Hx(x). Thus the game (3) is monotone, as long as D(x) is a monotone mapping.
The suﬃcient conditions we present in Theorem 5 are simply that we are in the regime ρ < 12 and that the map x → Hx(y) is monotone for any y. The latter can be understood as requiring that for any y ∈ X , the auxiliary game wherein each player aims to solve

min

E

i(y, zi).

xi∈X zi∼Di(xi,x−i)

is monotone. In the single player setting n = 1, this simply means that the function x → Ezi∼Di(x) (y, zi) is convex for any ﬁxed y ∈ X , thereby reducing exactly to the requirement in [Miller et al., 2021, Theorem 3.1].
The proof of Theorem 5 crucially relies on the following useful lemma.

Lemma 3. Suppose that Assumptions 1, 2, 5 hold. For any x ∈ X , the map Hx(y) is Lipschitz

continuous in y with parameter

n i=1

βi2γi2.

Proof. Fix three points x, x , y ∈ X . Player i’s coordinate of Hx (x) − Hx (y) is simply

d

Hi,x (x) − Hi,x (y) =

E ( i(x, zi) − i(y, zi))

.

dui zi∼Di(ui,x−i)

ui=xi

15

Setting γ(s) = y + s(x − y) for any s ∈ (0, 1), the fundamental theorem of calculus ensures

1

i(x, zi) − i(y, zi) =

∇i i(γ(s), zi), x − y ds.

s=0

Therefore diﬀerentiating, taking an expectation, and using the Cauchy-Schwarz inequality we deduce

1d

Hi,x (x) − Hi,x (y) ≤

E ∇i i(γ(s), zi)

· x − y ds.

s=0 dui zi∼Di(ui,x−i)

ui=xi

(15)

Now for any s ∈ (0, 1), Lemma 2 guarantees that the map ui → Ezi∼Di(ui,x−i) ∇i i(γ(s), zi) is Lipschitz continuous with parameter βiγi and therefore its derivative is upper-bounded in norm by βiγi. We therefore deduce that the right hand side of (15) is upper bounded by βiγi x − y . Applying this argument to each player leads to the claimed Lipschitz constant on Hx(y) with respect to x.

Theorem 5 (Monotonicity of the decision-dependent game). Suppose that Assumptions 1, 2, and

5

hold,

and

that

we

are

in

the

regime

ρ

<

1 2

and

the

map

x

→

Hx(y)

is

monotone

for

any

y.

The

game (3) is strongly monotone with parameter (1 − 2ρ) α.

Proof. Fix an arbitrary pair of points x, x ∈ X . Expanding the following inner product, we have

D(x) − D(x ), x − x = Gx(x) − Gx (x ), x − x + Hx(x) − Hx (x ), x − x .

(16)

We estimate the ﬁrst term as follows:

Gx(x) − Gx (x ), x − x = Gx (x) − Gx (x ), x − x + Gx(x) − Gx (x), x − x

n

1/2

≥α x−x 2−

βi2γi2

· x−x 2

(17)

i=1

= (1 − ρ) α x − x 2,

(18)

where (17) follows from Assumption 1 and Lemma 2. Next, we estimate the second term on the right side of (16) as follows:

Hx(x) − Hx (x ), x − x = Hx (x) − Hx (x ), x − x + Hx(x) − Hx (x), x − x

≥ Hx (x) − Hx (x ), x − x

(19)

≥ − Hx (x) − Hx (x ) · x − x

(20)

≥−

n

1/2

βi2γi2

x − x 2,

i=1

(21)

where (19) follows from the assumption that the map x → Hx(y) is monotone and (21) follows from Lemma 3. Combining (16), (18), and (21) completes the proof.

The following two examples of multiplayer performative prediction problems illustrate settings where the mapping x → Hx(y) is indeed monotone and therefore Theorem 5 may be applied to deduce monotonicity of the game.

16

Example 1 (Revenue Maximization in Ride-Share Markets). Consider a ride share market with two ﬁrms that each would like to maximize their revenue by setting the price xi. The demand zi that each ride share ﬁrm sees is inﬂuenced not only by the price they set but also the price that their competitor sets. Suppose that ﬁrm i’s loss is given by
i(xi, zi) = −zi xi + λ2i xi 2
where λi ≥ 0 is some regularization parameter. Moreover, let us suppose that the random demand zi takes the semi-parametric form zi = ζi + Aixi + A−ix−i, where ζi follows some base distribution Pi and the parameters Ai and A−i capture price elasticities to player i’s and its competitor’s change in price, respectively. The mapping x → Hx(y) is monotone. Indeed, observe that i-th component of Hx(y) is given by

Hi,x(y) = E Ai ∇zi i(yi, ζi + Aixi + A−ix−i) = −Ai yi.
ζi∼Pi

Hence, the map x → Hx(y) is constant and is therefore trivially monotone.
The next example is a multiplayer extension of Example 3.2 in [Miller et al., 2021] which models the single player decision-dependent problem of predicting the ﬁnal vote margin in an election contest.

Example 2 (Strategic Prediction). Consider two election prediction platforms. Each platform seeks to predict the vote margin. Not only can predicting a large margin in either direction dissuade people from voting, but people may look at multiple platforms as a source for information. Features θ such as past polling averages are drawn i.i.d. from a static distribution Pθ. Each platform observes a sample drawn from the conditional distribution

zi|θ ∼ ϕi(θ) + Aixi + A−ix−i + wi,

where ϕi : Rdi → R is an arbitrary map, the parameter matrices Ai ∈ Rmi×di and A−i ∈ Rmi×d−i

are ﬁxed, and wi is a zero-mean random variable. Each player seeks to predict zi by minimizing the

loss

1 i(xi, zi) =

zi − θ

xi

2.

2

We claim that the map x → Hx(y) is monotone as long as

√

n − 1 · max A−iAi op ≤ min λmin(Ai Ai),

i∈[n]

i∈[n]

where λmin denotes the minimal eigenvalue. The interpretation of this condition is that the performative eﬀects due to interaction with competitors are small relative to any player’s own performative eﬀects. To see the claim, set A¯i to be the matrix satisfying A¯ix = Aixi + A−ix−i and observe that the i-th component of Hx(y) is given by

Hi,x(y) = E Ai ∇zi i(yi, ϕi(θ) + A¯ix + wi)
θ,wi
= E Ai (ϕi(θ) + A¯ix − θ yi + wi)
θ,wi
= Ai Aixi + Ai A−ix−i + E Ai (ϕi(θ) − θ
θ,wi

yi + wi).

Therefore, the map Hi,x(y) is aﬃne in x. Consequently, monotonicity of x → Hi,x(y) is equivalent to monotonicity of the linear map x → V (x) + W (x), where V is the block diagonal matrix V (x) =

17

Diag(A1 A1, . . . , An An)x and we deﬁne the linear map W (x) = (A1 A−1x−1, . . . , An A−nx−n). The minimal eigenvalue of V is simply mini∈[n] λmin(Ai Ai). Let us estimate the operator norm of W . To this end, set s := maxi Ai A−i op and for any x we compute

n
W (x) 2 =
i=1

n
Ai A−ix−i 2 ≤ s2 x−i 2 = (n − 1)s2 x 2.
i=1

Thus, under the stated assumptions, the operator norm of W is smaller than the minimal eigenvalue of V , and therefore the sum V + W is monotone.

6 Algorithms for Finding Nash Equilibria

In contrast to Section 4, in this section we analyze algorithms that converge to the Nash equilibrium of the n–player performative prediction game (3) when the game is strongly monotone. Recall that the Nash equilibrium x of this game is characterized by the relation

xi ∈ argmin E

i(xi, x−i, zi)

xi∈Xi zi∼Di(xi,x−i)

∀i ∈ [n].

It is important to stress the distinction between the performatively stable equilibria studied in Section 4 and the Nash equilibrium x of the game (3): namely, for the latter concept the distribution Di explicitly depends on the optimization variable xi versus being ﬁxed at x = (xi , x−i). Theorem 5 (cf. Section 5) gives suﬃcient conditions under which the multiplayer performative prediction game (3) is strongly monotone and hence, admits a unique Nash equilibrium.
In the following subsections, we study natural learning dynamics—namely, variants of gradient play as it is referred to in the literature on learning and games—for continuous games seeking Nash equilibrium in diﬀerent information settings. Speciﬁcally, we study gradient-based learning methods where players update using an estimate of their individual gradient consistent with the information available to them. It is important to contrast the gradient updates in Section 4 with the updates considered in this section: the Nash-seeking algorithms studied in this section all use gradient estimates of the individual gradient

d

∇iLi(xi, x−i) = E [∇i i(x, zi)] +

E [ i(x, zi)]

zi∼Di(x)

dui zi∼Di(ui,x−i)

ui=xi

(22)

for each player i ∈ [n], whereas performatively stable equilibrium seeking algorithms of Section 4
are deﬁned such that the gradient update only uses the ﬁrst term on the right hand side of (22).
The main diﬃculty with applying gradient-based methods is that estimation of the second
term on the right hand side of (22), without some parametric assumptions on the distributions
Di. Consequently, we start in Section 6.1 with derivative free methods, wherein each player only has access to loss function queries. This does not require players to have any information on the distribution Di, but results in a slow algorithm, roughly with complexity O( dε22 ). In practice, the players may have some information on Di, and it’s reasonable that they would exploit this information during learning. Hence, in Section 6.2 we impose a speciﬁc parametric assumption of the distributions and study stochastic gradient play2 under the assumption that each player knows
their own “inﬂuence” parameter on the distribution. The resulting algorithm enjoys eﬃciency on the order of O( 1ε ). Section 6.3 instead develops a variant of a stochastic gradient method wherein
2This method is known as stochastic gradient play in the game theory literature; here we refer to as the stochastic gradient method to be consistent with the naming convention of other methods in the paper.

18

each player adaptively learns their inﬂuence parameters, and uses their current estimate of those
parameters to optimizing their loss function by taking a step along the direction of their individual gradient; the resulting process has eﬃciency on the order of O( dε ).

6.1 Derivative Free Method for Performative Prediction Games

As just alluded to, the ﬁrst information setting we consider for multiplayer performative prediction is the “bandit feedback” setting, where players have oracle access to queries of their loss function only, and therefore are faced with the problem of creating an estimate of their gradient from such queries. This setting requires the least assumptions on what information is available to players. In the optimization literature, when a ﬁrst order oracle is not available, derivative free or zeroth order methods are typically applied. Derivative free methods have been extended to games [Bravo et al., 2018, Drusvyatskiy et al., 2021]. The results in this section are direct consequences of the results in these papers. We concisely spell them out here in order to compare them with the convergence guarantees discussed in the following two sections.
The derivative free (gradient) method we consider proceeds as follows. Fix a parameter δ > 0. In each iteration t, each player i ∈ [n] performs the update:

Sample vit ∈ Si 

 Sample zit ∼ Di(xt + δvt)

  

.

(23)

Set xti+1 = proj(1−δ)Xi xti − ηt dδi i(xt + δvt, zit)vit 

Recall that Si denotes the unit sphere with dimension di. The reason for projecting onto the set (1 − δ)Xi is simply to ensure that in the next iteration t + 1, the strategy played by player i remains in Xi. We state the convergence guarantees of the method informally here because they are meant only as a baseline result. The formal statement for derivative free methods in general games can be found in Drusvyatskiy et al. [2021].3

Proposition 1 (Convergence rate of the derivative free method). Consider an n–player decision-
dependent game (3). Under reasonable smoothness and bounded variance assumptions, algorithm (23) with appropriately chosen parameters δ and ηt will ﬁnd a point x satisfying E[ x − x 2] ≤ ε after at most O( dε22 ) iterations.
The rate O( dε22 ) can be extremely slow in practice. Therefore in the rest of the paper, we focus on stochastic gradient based methods, which enjoy signiﬁcantly better eﬃciency guarantees (at cost
of access to a richer oracle).

6.2 Stochastic Gradient Method in Performative Prediction Games
In practice, often players have some information regarding their data distribution Di and can leverage this during learning. Stochastic gradient play—which we refer to as the stochastic gradient method to be consistent with the rest of the paper—is a natural learning algorithm commonly adopted in the literature on learning in games for settings where players have an unbiased estimate of their individual gradient. To apply the stochastic gradient method to multiplayer performative prediction, players need oracle access to the gradient of their loss with respect to their choice
3Though Theorem 2 in Drusvyatskiy et al. [2021] is stated for deterministic games, it applies verbatim whenever the value of the loss function for each each player is replaced by an unbiased estimator of their individual loss functions—our setting.

19

variable, which requires some knowledge of how the distribution Di depends on the joint action proﬁle x. To this end, let us impose the following parametric assumption, which we have already encountered in Example 1.

Assumption 6 (Parametric assumption). For each index i ∈ [n], there exists a probability measure Pi and matrices Ai and A−i satisfying

zi ∼ Di(x)

⇐⇒

zi = ζi + Aixi + A−ix−i for ζi ∼ Pi.

The mean and covariance of ζi are deﬁned as µi := Eζi∼Pi[ζi] and Σi := Eζi∼Pi[(ζi − µi)(ζi − µi) ], respectively.

Assumption 6 is very natural and generalizes an analogous assumption used in the single player setting in [Miller et al., 2021]. It asserts that the distribution used by player i is a “linear perturbation” of some base distribution Pi. We can interpret the matrices Ai and A−i as quantifying the performative eﬀects of player i’s decisions and the rest of the players’ decisions, respectively, on the distribution Di governing player i’s data.
Under Assumption 6, we may write player i-th loss function as

Li(x) = E i(x, ζi + Aixi + A−ix−i).
ζi∼Pi

(24)

Under mild smoothness assumptions, diﬀerentiating (24) using the chain rule, we see that the gradient of the i-th player’s loss is simply

∇iLi(x) = E [∇i i(x, zi) + Ai ∇zi i(x, zi)].
zi∼Di(x)

(25)

Therefore, given a point x, player i may draw zi ∼ Di(x) and form the vector

wi(x, zi) = ∇i i(x, zi) + Ai ∇zi i(x, zi).

By deﬁnition, wi(x, zi) is an unbiased estimator of ∇iLi(x), that is

E wi(x, z) = ∇iLi(x).
zi∼Di(x)
With this notation, the stochastic gradient method proceeds as follows: in each iteration t ≥ 0 each player i ∈ [n] performs the update:

Sample zit ∼ Di(xt) Set xti+1 = projXi xti − ηt · wi(xt, zit) . (26)
Let us look at the computation that is required in each iteration. Evaluation of the vector wi(x, zi) requires evaluation of both ∇i i(x, zi) and ∇zi i(x, zi), and knowledge of the matrix Ai. When the game is separable, it is very reasonable that each player can explicitly compute ∇i i(xi, zi) and ∇zi i(xi, zi) assuming oracle access to queries zi from the environment which does depend on x−i and xi. Moreover, the matrix Ai depends only on the performative eﬀects of player i, and in this section we will suppose that it is indeed known to each player. In the next section, we will develop an adaptive algorithm wherein each player i ∈ [n] simultaneously learns Ai and A−i while optimizing their loss.
In order to apply standard convergence guarantees for stochastic gradient play, we need to assume that (i) the vector of individual gradients is Lipschitz continuous and (ii) that the variance of w(x, zi) is bounded. Let us begin with the former.

20

Assumption 7 (Smoothness). Suppose that the map (∇1L1(x), ∇2L2(x), . . . , ∇nLn(x)) is LLipschitz continuous.
The constant L may be easily estimated from the smoothness parameters of each individual loss function i(x, z) and the magnitude of the matrices Ai and A−i; this is the content of the following lemma. In what follows, we deﬁne the mixed partial derivative ∇i,zi i(x, zi) = (∇i i(x, zi), ∇zi i(x, zi)). Recall that ∇i i(xi, x−i, zi) denotes the partial derivative of i with respect to the xi argument and ∇zi i(xi, x−i, zi) denotes the partial derivative with respect to zi.
Lemma 4 (Suﬃcient conditions for Assumption 7.). Suppose that Assumption 6 holds and that there exist constants ξi ≥ 0 such that for each index i the map (x, zi) → ∇i,zi i(x, zi) is ξi-Lipschitz continuous. Then Assumption 7 holds with

n

L=

ξi2 max{1,

Ai

2 op

}

·

(1

+

A¯i

2op).

i=1

Proof. Let A¯i be a matrix satisfying A¯ix = Aixi + A−ix−i. Observe that we may write

∇iLi(x) = E V ∇i,z i(x, ζi + A¯ix)

ζi,0 ∼Pi

i

where

V= I 0 . 0 Ai

Therefore, we deduce

∇iLi(x) − ∇iLi(x )

≤ V op E ∇i,z i(x, ζi + A¯ix) − ∇i,z i(x , ζi + A¯ix )

ζi∼Pi

i

i

≤ max{1, Ai op} · ξi · E (x, ζi + A¯ix) − (x , ζi + A¯ix )
ζi∼Pi

= max{1, Ai op} · ξi · x − x 2 + A¯i(x − x ) 2

≤ max{1, Ai op} · ξi ·

1+

A¯i

2 op

·

x−x

.

This completes the proof.

Next we assume a ﬁnite variance bound.

Assumption 8 (Finite variance). Suppose that there exists a constant σ > 0 satisfying

E w(x, z) − E w(x, z ) 2 ≤ σ2

z∼Dπ (x)

z ∼Dπ(x)

∀x ∈ X .

Let us again present a suﬃcient condition for Assumption 8 to hold in in terms of the variance of the individual gradients ∇i,zi (x, zi). The proof is immediate and we omit it.

Lemma 5 (Suﬃcient conditions for Assumption 8). Suppose that there exist constants s1, s2 ≥ 0 such that for all x ∈ X and i ∈ [n] the estimates hold:

E ∇i i(x, zi) − E ∇i

zi∼Di(x)

zi∼Di(x)

E ∇zi i(x, zi) − E ∇zi

zi∼Di(x)

zi∼Di(x)

i(x, zi) i(x, zi)

2 ≤ s21 2 ≤ s22

Then Assumption 8 holds with σ2 =

n i=1

2(s21

+

Ai 2ops22).

21

The following is now a direct consequence of standard convergence guarantees for stochastic gradient methods.

Theorem 6 (Stochastic gradient play). Consider an n-player performative prediction game (3).

Suppose that Assumptions 6-8 hold and that the game is α-strongly monotone with α > 0. Then a

single

step

of

the

stochastic

gradient

method

(26)

with

any

constant

η≤

α 2L2

satisﬁes

E[ xt+1 − x 2] ≤ 1 E[ xt − x 2] + 2η2σ2 , (27)

1 + αη

1 + ηα

where x is the Nash equilibrium of the game (3).

Proof. This is immediate from Theorem 8 in Appendix A with B ≡ Ct ≡ D ≡ 0.

Analogous to the analysis of the stochastic repeated gradient method, applying a step-decay schedule on η yields the following corollary. The proof follows directly from the recursion (27) and the generic results on step decay schedules; e.g. [Drusvyatskiy and Xiao, 2020, Lemma B.2].

Corollary 2 (Stochastic gradient method with a step-decay schedule). Suppose that the assumptions

of Theorem 6 hold. Consider running stochastic gradient method in k = 0, . . . , K epochs, for Tk

iterations

each,

with

constant

step-size

ηk

=

α 2L2

·

2−k ,

and

such

that

the

last

iterate

of

epoch

k

is

used as the ﬁrst iterate in epoch k + 1. Fix a target accuracy ε > 0 and suppose we have available a

constant R ≥ x0 − x 2. Set

T0 = α2η0 log( 2εR ) , Tk = 2 lαoηgk(4) for k ≥ 1, and K = 1 + log2 2ηα0εσ2 .

The ﬁnal iterate x produced satisﬁes E gradient play called is at most
O

x−x∗ 2 ≤ ε, while the total number of iterations of stochastic

L2

2R

σ2

α2 · log ε + α2ε .

6.3 Adaptive Gradient Method in Performative Prediction Games
Throughout this section, we continue working under the parametric Assumption 6. An apparent deﬁciency of the stochastic gradient method discussed in Section 6.2 is that each player i needs to know the matrix Ai that governs the performative eﬀect of the player on the distribution. In typical settings, the matrix Ai may be unknown to the player, but it might be possible to estimate it from data. Inspired by methods in adaptive control to simultaneously estimate the parameters of the system and optimize the control input, we propose the adaptive gradient method outlined in Algorithm 1.4 In each iteration, each player simultaneously estimates their distribution parameters and myopically optimizes their individual loss via stochastic gradient method on the current estimated loss. More precisely, the algorithm maintains two sequences: (i) xt that eventually converges to the Nash equilibrium x , and (ii) estimates Aˆti that dynamically estimates the unknown matrix A¯i. In each iteration t, the algorithm draws samples zit ∼ Di(xt), and each player i takes the gradient step
xti+1 = projXi xti − ηt((∇i i(xt, zit) + (Aˆtii) ∇zi i(xt, zit)) ,
4We remark that the word “adaptive” here refers to adaptively estimating the model parameters, and is diﬀerent from its meaning in methods like AdaGrad, where it is the algorithm’s stepsize that is being adapted.

22

where Aˆtii denotes the submatrix of Aˆti whose columns are indexed by player i’s action space. Next, in order to update Aˆt, the algorithm draws a sample qit ∼ Di(xt + ut) where ut is a user-speciﬁed noise sequence. Observe that conditioned on ut, the equality holds:

E[qit − zit | ut] = A¯iut.

Therefore, a good strategy for forming a new estimate Aˆti+1 of A¯i from Aˆti is to take a gradient step

on the least squares objective

1 min

qt − zt − Biut

2.

Bi 2 i i

Explicitly, this gives the update

Aˆti+1 = Aˆti + νt(qit − zit − Aˆtiut)(ut) .

Analogous to estimation in adaptive control or machine learning, we exploit noise injection ut to ensure suﬃcient exploration of the parameter space. In particular, the noise vector needs to be suﬃciently isotropic. We impose the following assumption.

Assumption

9

(Injected

Noise).

The

injected

noise

vector

ut

=

(

ut1

,

.

.

.

,

u

t n

)

∈

Rd

is

a

zero-mean

random vector that is independent of xt, and independent of the injected noise at any previous

queries to the environment by any player. Moreover, there exists constants cl, R > 0 and cu,i > 0 for each i ∈ [n] such that for all t ≥ 0 and i ∈ [n] the random vector vi := uti satisﬁes

0 ≺ cl · I E[vivi ], E vi 2 ≤ cu,i, and E[ vi 2vivi ] R2E[vivi ].

In the simple Gaussian case where ut ∼ N (0, Id), we may set5

cl = 1, cu,i = di, and R2 = 3 max di.
i∈[n]

Analyzing the convergence of Algorithm 1 amounts to decomposing the analysis into convergence of the stochastic gradient method on the estimated losses induced by the sequence of Aˆti, and convergence of the estimation error E Aˆti − A¯i 2. The former analysis proceeds in an analogous fashion to that of Theorem 6 in Section 6.2. For the latter, we leverage the injected noise to ensure
there is suﬃcient exploration. The following lemma establishes a one-step improvement guarantee on estimation of A¯i. Throughout, we set Aˆt := (Aˆt1, . . . , Aˆtn) and let · F denote the Frobenius norm. We also let Et be the conditional expectation with respect to the σ-algebra generated by (xl, ul)l=1,...,t.

Lemma

6

(Estimation

error).

Suppose

that

Assumptions

6

and

9

hold

and

choose

νt

∈

(0,

2 R2

).

Then the matrices Aˆti generated by Algorithm 1 satisfy the estimate:

12 Et Aˆti+1 − A¯i 2F ≤ 1 − clνt(22 − νtR2) Aˆti − A¯i 2F + νt2tr(Σi)cu,i. (28)

Therefore when setting νt =

2
2

for all t ≥ 0, the estimate holds:

cl(t+ 2cR )

l

max

(1 + 2R2 ) Aˆ1 − A¯ 2 , 8

n i=1

tr(Σi)cu,i

E

Aˆt − A¯

2 F

≤

cl

F

c2l

.

t + 2R2

cl

5For the justiﬁcation of the expression for R2, see [Dieuleveut et al., 2017, Section 2.1].

23

Algorithm 1: Adaptive Gradient Method

1 Input: Stepsizes {ηt}t≥1, {νt}t≥1; initial x1 ∈ Rd, Aˆ1i ∈ Rm×d; 2 for t = 1, . . . , t do

3 for i ∈ [n] do

4

Query the environment: Draw samples zit ∼ Di(xt) and qit ∼ Di(xt + ut);

5

Individual gradient update:

xti+1 = projXi xti − ηt(∇i i(xt, zit) + (Aˆtii) ∇zi i(xt, zit)) ,

6

where Aˆtii denotes the submatrix of Aˆti whose columns are indexed by player i.

7

Estimation update: Aˆti+1 = Aˆti + νt(qit − zit − Aˆtiuti)(uti)

8 end

9 end

Proof. This follows from a standard estimate for online least squares, which appears as Lemma 10
in Appendix C. Namely, let G1 be the σ-algebra generated by x1, . . . , xt and let G2 be the σ-algebra generated by G1 ∪ {ut}. Set b = qit − zit, y = uti, B = Aˆti, V = A¯i, v = uti, λ1 = cl, and λ2 = cu,i.
Let us upper bound the variance E[ V y − b 2 | G2]. To this end, let w and w be drawn i.i.d from Pi. Observe that conditioned on uti, the random vector A¯iuti − (qit − zit) has the same distribution
as w − w . Let us compute

E w − w 2 = tr(E((w − w )(w − w ) ) = 2tr(Σi).

Therefore, we may set σ2 = 2tr(Σi). An application of Lemma 10 completes the proof of (28). Summing up (28) for i = 1, . . . , n and using the tower rule for for conditional expectations yields:

n

E

Aˆt+1 − A¯

2 F

≤ (1 − νtcl(2 − νt2R2))E

Aˆt − A¯

2 F

+ 2νt2

tr(Σi)cu,i.

i=1

Noting

νt

≤

1 R2

,

we

deduce

1 − νtcl(2 − νt2R2)

≤

1 − νtcl.

The

result

follows

directly

from

plugging

in the value of νt and using Lemma 8 in Appendix B.

Next we show that the direction of motion of Algorithm 6.3 is well-aligned with the direction of motion of the stochastic gradient method. To this end, deﬁne the true (stochastic) vector of individual gradients
vt := (∇i i(xt, zit) + Ai ∇zi i(xt, zit))i∈[n],
and its estimator that is used by the algorithm

vˆt := (∇i i(xt, zit) + (Aˆtii) ∇zi i(xt, zit))i∈[n].

We make the following Lipschitzness assumption on the loss i(x, zi) in the variable zi.

Assumption 10 (Lipschitz continuity in z). Suppose that there exists a constant δ > 0 such that for all x ∈ X , the estimate holds:

E
z∼Dπ (x)

n
∇ i(x, zi) 2 ≤ δ.
i=1

24

Lemma 7. Suppose that Assumptions 6 and 10 hold. Then for each t ≥ 1 and i ∈ [n], the estimate

holds:

Et vˆt − vt

≤δ

Aˆt − A¯

2 F

.

Proof. Notice that we may write vˆt − vt = Btwt, where Bt is the block diagonal matrix with blocks Aˆtii − Ai and we set wt = (∇zi i(xt, zit))ni=1. Using H¨older’s inequality we estimate:

Et vˆt − vt

= Et Btwt

≤

Bt F · Et wt

≤δ

Aˆt − A¯

2 F

,

as claimed.

In light of Lemmas 6 and 7, we may interpret Algorithm 6.3 as an approximation to the stochastic gradient method with a bias that tends to zero; we may then simply invoke generic convergence guarantees for biased stochastic gradient methods, which we record in Theorem 8 of Appendix A. We will make use of the following assumption.

Assumption 11 (Finite variance). Suppose that there exists σ > 0 such that for all x ∈ X , the variance bound holds:

E ∇i,zi i(xt, zit) − E ∇i,zi i(xt, zi) 2 ≤ σ2.

zi∼Di(xt)

zi∼Di(xt)

The end result is the following theorem, which in particular implies a O(d/t) rate of convergence when ut are standard Gaussian. See the discussion after the theorem.

Theorem 7 (Convergence of the adaptive method). Suppose that Assumptions 6, 7, 9, and 10 hold and that the game (3) is α-strongly monotone. Deﬁne the constant k0 = 1 + 8αL22 and q0 = 2cRl2 and set ηt = α(t+k20−2) and νt = cl(t+2 q0) for all t ≥ 0. Then for all t ≥ 1, the iterates generated by Algorithm 1 satisfy

E xt − x

max 2≤

1 2

α2

(1

+

k0

)

x1 − x

2, 32(1 + 2 A¯ 2F )σ2 + 8δ2Z max{ 11++kq00 , 1} α2(t + k0)

+ max 12 α2(1 + k0)3/2 x1 − x 2, 64σ2Z max{ 11++kq00 , 1} . α2(t + k0)3/2

where we set Z = max

(1 + 2R2 ) Aˆ1 − A¯ 2 , 8

n i=1

tr(Σi)cu,i

.

cl

F

c2l

Proof. We will apply the standard convergence guarantees in Theorem 8 of Appendix A for biased stochastic gradient methods. Using Lemma 7 we estimate the gradient bias:

Et[vˆt] − Et[vt]

= Et vˆt − vt

≤δ

Aˆt − A¯

2 F

.

Next, we estimate the variance: Et[ vˆit − E vˆit 2] = Et

I0

tt

2 t

0 Aˆtii (∇i,zi i(x , zi ) − zi∼DEi(xt) ∇i,zi i(x , zi)) .

Summing these inequalities over i ∈ [n], we deduce E[ vˆit − E vˆit 2] ≤ max{1, Aˆt 2op}σ2.

25

Recalling the deﬁnition of Z and q0 and applying Theorem 8 in Appendix A we deduce

Et xt+1 − x

2

1

t

2

2ηt2(max{1, Aˆt 2op})σ2

2ηtδ2

Aˆt − A¯

2 F

≤ 1 + ηtα x − x +

1 + ηtα

+ α

1 ≤

xt − x

2 + 2η2(1 +

Aˆt

2 )σ2 + 2ηtδ2

Aˆt − A¯

2 F

1 + ηtα

t

F

α

1 ≤

xt − x

2 + 2η2(1 + 2

A¯

2 )σ2 + 2ηtδ2

Aˆt − A¯

2 F

1 + ηtα

t

F

α

+ 4ηt2σ2

Aˆt − A¯

2 F

,

where the last inequality follows from the algebraic expression Aˆt 2 ≤ 2 A¯ 2 + 2 Aˆt − A¯ 2. Taking expectations and using the tower rule, we compute

E xt+1 − x

2≤

1

E xt − x

2 + 2η2(1 + 2

A¯

2 )σ2 + 2ηtδ2 E

Aˆt − A¯

2 F

1 + ηtα

t

F

α

+ 4ηt2σ2 E

Aˆt − A¯

2 F

≤ 1 E xt − x 2 + 2η2(1 + 2 A¯ 2 )σ2 + 2ηtδ2Z + 4ηt2σ2Z ,

1 + ηtα

t

F

α(t + q0) t + q0

where the last inequality follows from Lemma 6. Now our choice ηt = α(t+k20−2) ensures 1+1ηtα = 1 − t+2k0 . Therefore we deduce

t+1

2

2

t

2

8(1 + 2

A¯

2 F

)

σ

2

E x − x ≤ 1 − t + k0 E x − x + α2(t + k0 − 2)2

16σ2Z 4δ2Z (29)

+

α2(t

+

q0)(t

+

k0

−

2)2

+

α2(t

+

q0)(t

+

k0

−

. 2)

We now aim to apply Lemma 9 in Section B. To this end, we need to upper bound the last three terms in (29) so that the denominators are of the form (t + k0)p for some power p. To this end, note the following elementary estimates:

t + k0 ≤ k0 + 1 ≤ 2 t + k0 − 2 k0 − 1

and

(t + k0)2

≤ k0 + 1 · t + k0 ≤ c(k0 + 1) ≤ 2c

(t + q0)(t + k0 − 2) k0 − 1 t + q0 k0 − 1

where c = maxt≥1{ tt++kq00 } = max{ 11++kq00 , 1}. Combining these estimates with (29), we obtain

E xt+1 − x 2 ≤ 1 − 2 t + k0

t

2

32(1 + 2

A¯

2 F

)σ2

+

8δ 2 Z c

64σ2Z · c

x −x +

α2(t + k0)2

+ (α2(t + k0)3) .

Applying Lemma 9 in Section B, we conclude:

E xt − x

2

max

1 2

α2

(1

+

k0

)

x1 − x

2,

32(1 + 2

A¯

2 F

)σ2

+

8δ 2 Z c

≤

α2(t + k0)

max +

1 2

α2(1

+

k0)3/2

x1 − x

α2(t + k0)3/2

2, 64σ2Zc .

This completes the proof.

26

In particular, consider the Gaussian case ut ∼ N (0, Id) in the setting when di = Cid for some numerical constants Ci, and when the traces tr(Σi) ≡ tr(Σ) are equal for all i ∈ [n]. Then the eﬃciency estimate in Theorem 7 becomes

E xt − x 2

 max

L2 x1 − x

2,

A¯

2 F

σ2

+

δ2

max

d

Aˆ1 − A¯

2 F

,

tr(Σ)

= O

α2t + L2

n i=1

cu,i

max{ dLα22 , 1}

max +

L3 x1 − x

2, ασ2 max

d

Aˆ1 − A¯

2 F

,

tr(Σ)

(α2t + L2)3/2

n i=1

cu,i

max{ dLα22 , 1}



.

Consequently, treating all terms besides d and t as constants, yields the rate O( dt ).

7 Numerical Examples
Section 5 introduced two examples motivated by practical problems including revenue maximization in ride-share markets and interactions between election prediction platforms. In this section, we explore each of these examples. For the former, we create semi-synthetic experiments using real data from a ride-share market in Boston, MA that includes two well-known platforms (Uber and Lyft). We demonstrate how the eﬀects of modeling competition impacts revenue generation and demand. For the latter, we create synthetic experiments that explore the gap between the social cost at the social optimum, performatively stable equilibrium and the Nash equilibrium. We start with the purely synthetic example in Section 7.1, and then move on to the semi-synthetic example in Section 7.2.

7.1 Multiplayer Performative Prediction with Strategic Data Sources

Recall in Section 5 in Example 2 we introduced a performative prediction game motivated by multiple election platforms. The decision problem that player i faces is such that a set of features are drawn i.i.d. from a static distribution Pθ, and player i observes a sample drawn from the conditional distribution
zi|θ ∼ ϕi(θ) + Aixi + A−ix−i + wi,

where ϕi : Rdi → R is an arbitrary map, the parameter matrices Ai ∈ Rmi×di and A−i ∈ Rmi×d−i

are ﬁxed, and wi is a zero-mean random variable. Each player seeks to predict zi by minimizing the

loss

1 i(xi, zi) =

zi − θ

xi

2.

2

We showed in Example 2 that the map x → Hx(y) is monotone as long as

√

n − 1 · max A−iAi op ≤ min λmin(Ai Ai), .

i∈[n]

i∈[n]

As noted in Example 2, the interpretation is that the performative eﬀects due to interaction with competitors are small relative to any player’s own performative eﬀects. We enforce this condition on the parameters selected for the examples we explore numerically.
We randomly generate problem instances—namely, the parameters Ai, A−i for i ∈ [n]—by using scipy.sparse.random which allows for the sparsity of the matrix to be set in addition to randomly

27

xt x 2 xt x 2

SGM

AGM

100

10 2

10 4

10 6
0 200 40it0eration60s0 800 1000
(a) Error to Nash Equilibrium

100

RSGM

10 1

10 2

10 3

10 4

0 200 40it0eration60s0 800 1000
(b) Error to Performatively Stable Equilibrium

Figure 1: Strategic Prediction. (a) Iteration complexity of the norm-squared error to the Nash equilibrium for both the stochastic gradient method (SGM) and the adaptive gradient method (AGM) for a randomly generated problem instance where mi = 10 and di = 2 for each i ∈ {1, 2}. The sample complexity of AGM empirically matches that of SGM even for fairly large problem instances. (b) Iteration complexity of the norm-squared error to the performatively stable equilibrium for the stochastic repeated gradient method (SRGM) for a randomly generated problem instance where mi = 100 and di = 2 for each i ∈ {1, 2}. The empirical rates matches the theory in Section 4 and Section 6.

generating the matrix parameters. We set d = 5 and m = 100 for the experiments in Figure 1. These values can be changed in the provided code, resulting in similar conclusions regarding the convergence rate. In Figure 1a we show the iteration complexity of the norm-square error to the Nash equilibrium for both the stochastic gradient method and the adaptive gradient method. Analogously, in Figure 1b, we show the iteration complexity of the norm-square error to the performatively stable equilibrium for the stochastic repeated gradient method. The mean and ±1 standard deviation are depicted with darker lines and the individual sample trajectories are shown using lighter shade lines of the same color indicated in the legend for the two methods. For both Nash-seeking methods (stochastic gradient and adaptive gradient method), we use a constant step size η = 0.01 for both methods for the stochastic gradient update step. Additionally, we use the step size ν = 2/(t + 6d) for the estimation update in the adaptive gradient method. We see that their iteration complexities are empirically the same. For the performatively stable equilibrium seeking algorithm, we use a constant step-size η = 0.1.
7.2 Revenue Maximization: Competition in Ride-Share Markets
The next numerical example we explore is semi-synthetic competition between two ride-share platforms seeking to maximize their revenue given that the demand they experience is inﬂuenced by their own prices as well as their competitors. We use data from a prior Kaggle competition to set up the semi-synthetic simulation environment.6
Game Abstraction. The abstraction for the game can be described as follows. Consider a ride-share market with two platforms that each seek to maximize their revenue by setting the price
6The data used in this paper is publicly available (https://www.kaggle.com/brllrb/ uber-and-lyft-dataset-boston-ma).
28

xt x 2 xt x 2

103

AGM

SGM

DFM

102

101

100

10 1

10 2 0

2000 40i0te0ratio6n0s00 8000 10000

(a) Error to Nash Equilibrium

103

RSGM

101

10 1

10 3
0 500 1000ite1ra5t0i0ons2000 2500 3000
(b) Error to Performatively Stable Equilibrium

Figure 2: Competition in Ride-Share Markets: Experiment 1. (a) Iteration complexity for Nash seeking algorithms including derivative free gradient method (DFM), stochastic gradient method (SGM) and adaptive gradient method (AGM). (b) Iteration complexity for the repeated stochastic gradient method (RSGM), a performatively stable equilibrium seeking algorithm. The plots demonstrate that the sample complexities of AGM and SGM are nearly identical and outperform DFM as expected.

xi. The vector of demands zi ∈ Rmi containing demand information for mi locations that each ride-share platform sees is inﬂuenced not only by the prices they set but also the prices that their competitor sets. Suppose that platform i’s loss is given by
i(xi, zi) = − 21 zi xi + λ2i xi 2
where λi ≥ 0 is some regularization parameter, and xi ∈ Rmi represents the vector of price diﬀerentials from some nominal price for each of the mi locations. Observe that this game is separable since the losses i do not explicitly depend on x−i. Moreover, let us suppose that the random demand zi takes the semi-parametric form zi = ζi + Aixi + A−ix−i, where ζi follows some base distribution Pi and the parameters Ai and A−i capture price elasticities to player i’s and its competitor’s change in price, respectively; naturally, the price elasticity for player i to its own price changes is negative while the price elasticity for player i’s demand given changes in its competitors actions is positive. Namely, we have that Ai 0 and A−i 0 capturing that an increase in player i’s prices results in a decrease in demand, while an increase in its competitor’s prices results in a increase in demand. Moreover, we showed in Example 1 that the mapping x → Hx(y) is trivially monotone. Hence, the game between ride-share platforms is strongly monotone and admits a unique Nash equilibrium. Throughout the remainder of this section we set λ1 = λ2 = 1.
Semi-Synthetic Data Construction. There are eleven locations that we consider in our simulation, and each element in xi represents the price diﬀerence (set by platform i) from a nominal price. We aggregate the rides into bins of $5 increments; this is done by taking the raw data and rounding the price to the nearest bin as follows 5 · p5 where p is the price of the ride. Then, for each bin j we have a diﬀerent base empirical distribution Pi,j for each player i ∈ {1, 2} which is just the collection of rides for that bin.
29

Loss Revenue price of anarchy (PoA)

0 SO 2000 4000 6000 8000 10000 12000 14000 16000 Lyft Uber Total

NE Lyft Uber Total

(a) Loss

PS Lyft Uber Total

16000 SO 14000 12000 10000 8000 6000 4000 2000
0 Lyft Uber Total

NE Lyft Uber Total

(b) Revenue

PS Lyft Uber Total

0.8

0.6

0.4

0.2

0.0 PS

NE

(c) PoA

Figure 3: Competition in Ride-Share Markets: Experiment 2. (a) Loss and (b) revenue at the social optimum obtained via stochastic gradient descent on the social cost (sum of players’ costs), Nash equilibrium obtained via the stochastic gradient method, and performatively stable equilibrium obtained via the repeated stochastic gradient method. The overall (sum of both players) loss and revenue are worse at the performatively stable equilibrium. Note that the loss is the negative of the revenue plus some small (λ1 = λ2 = 1) regularization term. (c) Average price of anarchy (PoA) at the Nash equilibrum versus the performatively stable equilibrium. A value closer to one is better, and hence the Nash equilibrium (by a small margin) has a better PoA.

For each bin, we estimate these price elasticity matrices Ai and A−i from the data using the heuristic that a 50% increase in price by any ﬁrm leads to a 75% decrease in demand. With this heuristic we use the average base demand for each location and price bin pair to estimate both the diagonal elements of Ai and A−i. In the experiments presented, our semi-synthetic model is such that there is no correlation between locations; however, in the provided code base, we have additional experiments that estimate the correlation between locations and explore the eﬀects of positive and negative correlations on equilibrium outcomes.7 We further note that the results presented in this section are for the $10 nominal price bin, however, in the repository of code it is easy to adjust this parameter to any of the other price bins. The conclusions we draw are similar across the bins.
Experiment 1: Numerical Comparison of Iteration Complexity. To validate the theory developed in the previous sections, we show the iteration complexity of the Nash seeking algorithms (Figure 2a), and the performatively stable equilibrium seeking algorithms (Figure 2b). We run each algorithm from twenty random initial conditions, and compute the error between the trajectory of the algorithm and the Nash equilibrium (respectively, performatvely stable equilibrium). In Figure 2a (respectively, Figure 2b), we show the mean of these error trajectories and plus and minus one standard deviation. All the raw trajectories are also shown using a less opaque trajectory. For the stochastic gradient method, we use a constant step size η = 5e-5 for the gradient update, and for the adaptive gradient method we use the step size schedule ηt = η0/t for the gradient update and νt = ν0/t for the estimation update where η0 = 5e-5 and ν0 = 1e-4. For the derivative free method, we use a constant query radius δ = 5 and step size schedule ηt = 2/t. The plots in Figure 2a demonstrate the that empirical sample complexity of the adaptive gradient method and the stochastic gradient method are nearly identical, and outperform the derivative free method as expected. Figure 2b simply illustrates the convergence rate as predicted by the theory in Section 4 for the repeated stochastic gradient method.
7The data and code used in this paper are publicly available (https://github.com/ratlifflj/ performativepredictiongames).
30

Experiment 2: Social Eﬃciency of Diﬀerent Equilibrium Concepts. As noted in the preceding sections, in the study of equilibrium for games, it is important to understand the eﬃciency of diﬀerent equilibrium concepts. The typical benchmark for eﬃciency is the cost at the social optimum. The social cost is deﬁned as the sum of all the players individual costs:
n
S(x) = Li(x).
i=1
We ﬁnd the unique socially optimal equilibrium xso by running stochastic gradient descent on the social cost (with η = 0.001). Let xne be the Nash equilibrium of the game (L1, L2) and let xps be the performatively stable equilibrium of the game G(xps), using the notation from Section 4 for the game induced by xps.
To compute the Nash equilibrium and the perfomatively stable equilibrium we run the stochastic gradient method and the repeated stochastic gradient method with step-size η = 0.001, respectively. In Figure 3 we show the loss at each of the equilibrium concepts for each player, and the total loss. For the ride-share game, both the Nash equilibrium and the performatively stable equilibrium are unique, and hence this set is a singleton.
The price anarchy is a common metric for equilibrium eﬃciency and is deﬁned as the ratio of the social cost at the worst case competitive equilibrium concept relative to the social cost at the social optimum—namely, it is given by
PoA(x) = maxx∈Eq(G) S(x) , S (xso )
where Eq(G) denotes the set of equilibria for the game G. An equilibrium concept is said to be more eﬃcient the closer this quantity is to one.
Given the stochastic nature of the problem at hand, we deﬁne the empirical price of anarchy as the ratio of the corresponding empirical social costs, and denote it as PoA(x). Hence, in comparing equilibrium concepts—i.e., Nash versus performatively stable equilibrium—the equilibrium with the social cost closest to the social cost at the social optimum is desirable. The empirical price of anarchy for the Nash equilibrium of the ride-share game is PoA(xne) = 0.899 while the empirical price of anarchy for the peformatively stable equilibrium is PoA(xps) = 0.829, where we compute the empirical social cost at the corresponding equilibrium. This is also illustrated in Figure 3c. These ratios are fairly close, indicating that it is an interesting direction of future research to better understand the analytical properties of eﬃciency the diﬀerent equilibrium concepts.
In Figure 3b, we also show the revenue for each of the equilibrium concepts. The revenue shares an analogous story to the loss: the total revenue at the Nash equilibrium is closer to the social optimum, but the performatively stable equilibrium is not far oﬀ.
Experiment 3: Eﬀect of Ignoring Performativity. We study the impact of players ignoring performative eﬀects due to competition in the data distribution. In Figure 5, we explore the eﬀects of players either being completely myopic—i.e., git = λixti − 12 ζit—or partially myopic—i.e., git = −(Ai − λiI) xti − 12 ζit—on the change in revenue, demand and average price (across locations) from nominal at the Nash equilibrium. Recall that players employing the stochastic gradient method use the gradient estimate git = −(Ai − λiI) xti − 12 (ζit + A−ixt−i); we refer to this as the non-myopic case since all performative eﬀects are considered. Even when the players are myopic or partially myopic, the environment, however, does have these performative eﬀects, meaning that
31

Change in Revenue Change in Demand Avg. Change in Price Change in Revenue Change in Demand Avg. Change in Price Change in Revenue Change in Demand Avg. Change in Price

Uber Lyft

5

5

4

4

3

3

2

2

1

1

© Mapbox © OpenStreetMap

0

0

(a) Price change by location.

Uber Lyft

© Mapbox © OpenStreetMap

80 80

60 60

40 40

20 20

0

0

−20 −20

−40 −40

−60 −60

−80 −80

(b) Demand change by location.

Uber Lyft

30

30

20

20

10

10

0

0

−10 −10

© Mapbox © OpenStreetMap

−20 −20

(c) Revenue change by location.

Figure 4: Competition in Ride-Share Markets: Experiment 2. Eﬀects due to myopic decision-making. Change in (a) price, (b) demand and (c) revenue from nominal by location for $10 nominal price bin due to ignoring performative eﬀects: players run stochastic gradient descent, and the image shows the change in demand (respectively, revenue) when both players model decision-dependence as compared to when they both do not model decision-dependence. The size of the circles shows the magnitude of the change, while the color indicates the raw value. The majority of locations see a decrease in demand, but due to an increase in price at the Nash equilibrium relative to the myopic outcome, there is an increase in revenue for both players.

4000 3000 2000 1000
0 1000 2000
Lyft Uber

250 200 150 100 50
0 Lyft Uber

0 1 2 3 4 5 6
Lyft Uber

(a) Both Myopic

1000 800 600 400 200
0 Lyft Uber

0 5 10 15 20 25 30 35 40
Lyft Uber

1.2 1.0 0.8 0.6 0.4 0.2 0.0 Lyft Uber

(d) Both Partially Myopic

6000 4000 2000
0 2000 4000
Lyft Uber

250 200 150 100 50
0 50
Lyft Uber

0 1 2 3 4 5 6 Lyft Uber

(b) Uber Myopic Only

700 600 500 400 300 200 100
0 Lyft Uber

20 0 20 40 60 Lyft Uber

1.2 1.0 0.8 0.6 0.4 0.2 0.0 Lyft Uber

(e) Uber Partially Myopic Only

10000 7500 5000 2500
0 2500 5000
Lyft Uber

300 200 100
0 100
Lyft Uber

0 1 2 3 4 5 6
Lyft Uber

(c) Lyft Myopic Only

800 600 400 200
0 Lyft Uber

20 0 20 40 60
Lyft Uber

1.2 1.0 0.8 0.6 0.4 0.2 0.0 Lyft Uber

(f) Lyft Partially Myopic Only

Figure 5: Competition in Ride-Share Markets: Experiment 3. Eﬀects of players being
(a)–(c) myopic or (d)–(f) partially myopic relative to Nash (not myopic, and consider competition).
Positive changes in revenue indicate the Nash equilibrium is better for that player. When a player is myopic, they do not consider any performative eﬀects in their update—i.e., git = λixti − 21 ζit—and when a player is partially myopic, they consider their own performative eﬀects, but not those of their competitor—i.e., git = −(Ai − λiI) xti − 12 ζit. In (a)–(c), we observe that when at least one player is completely myopic, then at least one player is worse oﬀ at the Nash equilibrium. In (d)–(f)
we observe that when at least one player is partially myopic, the Nash equilibrium always is better
for both players.

Change in Revenue Change in Demand Avg. Change in Price Change in Revenue Change in Demand Avg. Change in Price Change in Revenue Change in Demand Avg. Change in Price

zi = ζi + Aixi + A−ix−i and hence, the myopic player is in this sense ignoring or unaware of the fact that the data distribution is reacting to its competition’s decisions. To compute the equilibrium
32

outcomes we run the stochastic gradient method with a constant step-size of η = 0.001. In Figure 5 (a)–(c), we observe that when at least one player is completely myopic, then at least
one player is worse oﬀ at the Nash equilibrium in the sense that their revenue is lower. Interestingly, the player that is worse oﬀ at the Nash is the non-myopic player. In Figure 5 (d)–(f), on the other hand, we observe that when at least one player is partially myopic, the Nash equilibrium always is better for both players in the sense that their individual revenues are higher at the Nash.
The values in Figure 5 represent the total demand and revenue changes, and average price change across locations. It is also informative to examine the per-location changes. Focusing in on the setting considered in Figure 5 (d), we examine the per-location price, revenue and demand. We see that the relative change depends on the location, however, the majority of locations see a decrease in demand, yet an increase in price and hence, revenue. This suggests that modeling performative eﬀects due to competition can be beneﬁcial for both players.

8 Discussion
The new class of games in this paper motivates interesting future work at the intersection of statistical learning theory and game theory. For instance, it is of interest to extend the present framework to handle more general parametric forms of the distributions Di. Many multiplayer performative prediction problems exhibit a hierarchical structure such as a governing body that presides over an institution; hence, a Stackelberg variant of multiplayer performative prediction is of interest. Along these lines, the multiplayer performative prediction problem is also of interest for mechanism design problems arising in applications such as recommender systems. For instance, the recommendations that platforms select at the Nash equilibrium inﬂuence the preferences of consumers (data-generators). A mechanism designer (e.g., the government) can place constraints on platforms to prevent them from manipulating users’ preferences to make their prediction tasks easier.

References

Shabbir Ahmed. Strategic planning under uncertainty: Stochastic integer programming approaches. University of Illinois at Urbana-Champaign, 2000.

Julia Angwin, Jeﬀ Larson, Surya Mattu, and Lauren Kirchner.

Machine

bias.

Propublica, 2016.

URL https://www.propublica.org/article/

machine-bias-risk-assessments-in-criminal-sentencing.

Robert Bartlett, Adair Morse, Richard Stanton, and Nancy Wallace. Consumer-lending discrimination in the ﬁntech era. Technical report, National Bureau of Economic Research, 2019.

Yahav Bechavod, Katrina Ligett, Zhiwei Steven Wu, and Juba Ziani. Causal feature discovery through strategic modiﬁcation. arXiv preprint arXiv:2002.07024, 3, 2020.

Mario Bravo, David S Leslie, and Panayotis Mertikopoulos. Bandit learning in concave n-person games. arXiv preprint arXiv:1810.01925, 2018.

Gavin Brown, Shlomi Hod, and Iden Kalemaj. Performative prediction in a stateful world. arXiv preprint arXiv:2011.03885, 2020.

33

Benjamin Chasnov, Lillian Ratliﬀ, Eric Mazumdar, and Samuel Burden. Convergence analysis of gradient-based learning in continuous games. In Ryan P. Adams and Vibhav Gogate, editors, Proceedings of The 35th Uncertainty in Artiﬁcial Intelligence Conference, volume 115 of Proceedings of Machine Learning Research, pages 935–944. PMLR, 22–25 Jul 2020.
Le Chen, Alan Mislove, and Christo Wilson. Peeking beneath the hood of uber. In Proceedings of the 2015 Internet Measurement Conference, IMC ’15, page 495–508, 2015. doi: 10.1145/2815675. 2815681.
R Courtland. Bias detectives: the researchers striving to make algorithms fair. Nature, pages 357–360, 2018. doi: 10.1038/d41586-018-05469-3.
Joshua Cutler, Dmitriy Drusvyatskiy, and Zaid Harchaoui. Stochastic optimization under time drift: iterate averaging, step decay, and high probability guarantees. arXiv preprint arXiv:2108.07356, 2021.
Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. Continual learning in practice. arXiv preprint arXiv:1903.05202, 2019.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for least-squares regression. The Journal of Machine Learning Research, 18(1): 3520–3570, 2017.
Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classiﬁcation from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 55–70, 2018.
Dmitriy Drusvyatskiy and Lin Xiao. Stochastic optimization with decision-dependent distributions. arXiv preprint arXiv:2011.11173, 2020.
Dmitriy Drusvyatskiy, Maryam Fazel, and Lillian J. Ratliﬀ. Improved rates for derivative free gradient play in monotone games. arXiv:2111.09456, 2021.
Jitka Dupacov´a. Optimization under exogenous and endogenous uncertainty. University of West Bohemia in Pilsen, 2006.
Yu. Ermoliev. Stochastic quasigradient methods. In Yu. Ermoliev and R. J-B Wets, editors, Numerical Techniques for Stochastic Optimization, chapter 6, pages 141–185. Springer, 1988.
A. A. Gaivoronskii. Nonstationary stochastic programming problems. Cybernetics, 14:575–579, 1978.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 23(4):2061–2089, 2013.
Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classiﬁcation. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111–122, 2016.
Lars Hellemo, Paul I Barton, and Asgeir Tomasgard. Decision-dependent probabilities in stochastic programs with recourse. Computational Management Science, 15(3):369–395, 2018.
34

Zachary Izzo, Lexing Ying, and James Zou. How to learn when data reacts to your model: performative gradient descent. arXiv preprint arXiv:2102.07698, 2021.
Tore W Jonsbr˚aten, Roger JB Wets, and David L Woodruﬀ. A class of stochastic programs withdecision dependent random elements. Annals of Operations Research, 82:83–106, 1998.
Kristian Lum and William Isaac. To predict and serve? Signiﬁcance, 13(5):14–19, 2016.
Celestine Mendler-Du¨nner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimization for performative prediction. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4929–4939. Curran Associates, Inc., 2020.
Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoﬀ functions. Mathematical Programming, 173(1):465–507, 2019.
John Miller, Smitha Milli, and Moritz Hardt. Strategic classiﬁcation is causal modeling in disguise. In International Conference on Machine Learning, pages 6917–6926. PMLR, 2020.
John Miller, Juan C Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the performative risk. arXiv preprint arXiv:2102.08570, 2021.
Juan Perdomo, Tijana Zrnic, Celestine Mendler-Du¨nner, and Moritz Hardt. Performative prediction. In International Conference on Machine Learning, pages 7599–7609. PMLR, 2020.
Lillian J Ratliﬀ, Samuel A Burden, and S Shankar Sastry. On the characterization of local nash equilibria in continuous games. IEEE transactions on automatic control, 61(8):2301–2307, 2016.
Mitas Ray, Lillian J. Ratliﬀ, Dmitriy Drusvyatskiy, and Maryam Fazel. Decision-dependent learning in geometrically decaying environments. In Proceedings of the AAAI International Conference on Artiﬁcial Intelligence (AAAI), 2022.
J Ben Rosen. Existence and uniqueness of equilibrium points for concave n-person games. Econometrica: Journal of the Econometric Society, pages 520–534, 1965.
Reuven Y Rubinstein and Alexander Shapiro. Discrete event systems: Sensitivity analysis and stochastic optimization by the score function method, volume 13. Wiley, 1993.
Tatiana Tatarenko and Maryam Kamgarpour. Learning nash equilibria in monotone games. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 3104–3109. IEEE, 2019.
Tatiana Tatarenko and Maryam Kamgarpour. Bandit online learning of nash equilibria in monotone games. arXiv preprint arXiv:2009.04258, 2020.
Pravin Varaiya and RJ-B Wets. Stochastic dynamic optimization approaches and computation. 1988.
Heinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media, 2010.
Killian Wood, Gianluca Bianchin, and Emiliano Dall’Anese. Online projected gradient descent for stochastic optimization with decision-dependent distributions. IEEE Control Systems Letters, 2021.
35

Yinjun Wu, Edgar Dobriban, and Susan Davidson. Deltagrad: Rapid retraining of machine learning models. In International Conference on Machine Learning, pages 10355–10366. PMLR, 2020.

Appendix

A Stochastic gradient method with bias

In this section, we consider a variational inequality

0 ∈ G(x) + NX (x),

(30)

where X ⊂ Rd is a closed convex set and G : Rd → Rd is an L-Lipschitz continuous and α-strongly

monotone map. We will analyze the stochastic gradient method, which in each iteration performs

the update:

xt+1 = projX (xt − ηgt),

(31)

where η > 0 is a ﬁxed stepsize and gt is a sequence of random variables, which approximates G(xt). In particular, it will be crucial for us to allow gt to be a biased estimator of G(xt). Formally, we

make the following assumption on the randomness in the process. Throughout, x denotes the

unique solution of (30).

Assumption 12 (Stochastic framework). Suppose that there exists a ﬁltered probability space (Ω, F , F, P) with ﬁltration F = (Ft)t≥0 such that F0 = {∅, Ω}. Suppose moreover that gt is Ft+1measurable and there exist constants B, σ ≥ 0 and Ft-measurable random variables Ct, σt ≥ 0 satisfying the bias/variance bounds

(Bias) (Variance)

Etgt − G(xt)] ≤ Ct + B xt − x∗ , Et gt − Et[gt] 2 ≤ σt2 + D2 xt − x 2,

where Et = E[· | Ft] denotes the conditional expectation.

The following is a one-step improvement guarantee for the stochastic gradient method in the two conceptually distinct cases Ct = 0 and B = 0. In the case Ct = 0 , the bias Etgt − G(xt) shrinks as the iterates approach x . The theorem shows that as long as B/α < 1, with a suﬃciently small stepsize η, the method can converge to an arbitrarily small neighborhood of x . In the case B = 0, one can only hope to convergence to a neighborhood of the minimizer whose radius depends on {Ct}t≥0.

Theorem 8 (One step improvement). The following are true.

• (Benign bias) Suppose Ct ≡ 0 for all t. Set ρ := B/α and suppose that we are in the regime ρ ∈ (0, 1). Then with any η < α(81L−2ρ) , the stochastic gradient method (31) generates a sequence xt satisfying

t+1

2 1 + 2ηαρ + 4η2D2 + 2η2α2ρ2 t

2

4η2σt2

Et x − x ≤

1+ρ

x −x +

1+ρ . (32)

1 + 2ηα( 2 )

1 + 2ηα( 2 )

•

(Oﬀset

bias)

Suppose

B

≡ 0.

Then

with

any

η

≤

α 4L2

,

the

stochastic

gradient

method

(31)

generates a sequence xt satisfying

Et xt+1 − x 2 ≤ 1 + 2η2D2 xt − x 2 + 2η2σt2 + 2ηCt2 . (33)

1 + ηα

1 + ηα α(1 + ηα)

36

Moreover, in the zero bias setting B ≡ Ct ≡ 0, the estimate (33) holds in the slightly wider

parameter

regime

η

≤

α 2L2

.

Proof. We begin with the ﬁrst claim. To this end, suppose Ct ≡ 0 for all t. Set ρ := B/α and
suppose that we are in the regime ρ ∈ (0, 1). Fix three constants ∆1, ∆2, ∆3 > 0 to be speciﬁed later. Noting that xt+1 is the minimizer of the 1-strongly convex function x → 12 xt − ηgt − x 2 over X , we deduce

1 xt+1 − x 2 ≤ 1 xt − ηgt − x 2 − 1 xt − ηgt − xt+1 2.

2

2

2

Expanding the squares on the right hand side and combining terms yields

1 xt+1 − x 2

2≤ 1 2 1
= 2

xt − x xt − x

2 − η gt, xt+1 − x

1 −

xt+1 − xt

2

2

2 − η gt, xt − x

1 −

xt+1 − xt

2 − η gt, xt+1 − xt

.

2

Setting µt := Et[gt], we successively compute

1 Et

xt+1 − x

2

2 ≤ 1 xt − x

2 − η Etgt, xt − x

1 − Et

xt+1 − xt

2 − ηEt

gt, xt+1 − xt

2

2

1 ≤

xt − x

2 − η µt, xt − x

1 − Et

xt+1 − xt

2 − ηEt

gt, xt+1 − xt

2

2

1 =

xt − x

2 − ηEt G(xt+1), xt+1 − x

1 − Et

xt+1 − xt

2

2

2

+ η Et gt − µt, xt − xt+1 +η Et µt − G(xt+1), x − xt+1 ].

(34)

P1

P2

Taking into account strong monotonicity of G, we deduce G(xt+1), xt+1 − x ≥ α xt+1 − x therefore

1 + 2ηα Et

xt+1 − x

2 ≤ 1 xt − x

2

−

1 Et

xt+1 − xt

2 + η(P1 + P2).

2

2

2

2 and (35)

Using Young’s inequality, we may upper bound P1 and P2 by:

P1 ≤ σt2 + D2 xt − x 2 + ∆1Et xt+1 − xt 2 . (36)

2∆1

2

Next, we decompose P2 as

P2 = µt − G(xt), x − xt + Et µt − G(xt), xt − xt+1 + Et G(xt) − G(xt+1), x − xt+1 . (37)

We bound each of the three products in turn. The ﬁrst bound follows from our assumption on the

bias:

µt − G(xt), x − xt ≤ B xt − x 2.

(38)

The second bound uses Young’s inequality and our assumption on the bias:

Et µt − G(xt), xt − xt+1 ≤ ∆2 µt − G(xt) 2 + Et xt − xt+1 2

2

2∆2

(39)

≤ ∆2B2 xt − x 2 + Et xt − xt+1 2

2

2∆2

37

The third inequality uses Young’s inequality and Lipschitz continuity of G:

Et G(xt) − G(xt+1), x − xt+1 ≤ ∆3 G(xt) − G(xt+1) 2 + Et x − xt+1 2

2

2∆3

(40)

≤ ∆3L2 xt − xt+1 2 + Et x − xt+1 2

2

2∆3

Putting together all the estimates (35)-(40) yields

1 + 2ηα − 2η∆−3 1 Et xt+1 − x 2 ≤ 1 + ηD2∆−1 1 + 2ηB + η∆2B2 xt − x 2

2

2

(41)

1 − η∆1 − η∆−2 1 − η∆3L2

t+1 t 2 ησt2

−

Et x − x + .

2

2∆1

Let us now set

∆−3 1 = (1 −2ρ)α ,

1 ∆1 = 4η ,

∆−2 1 := η−1 − ∆1 − ∆3L2.

Notice

∆1

≤

1 2η

− ∆3L2

by

our

assumption

that

η

≤

α(81L−2ρ) .

In

particular,

this

implies

∆−2 1

≥

21η .

Notice that our choice of ∆2 ensures that the the fraction multiplying Et xt+1 − xt 2 in (41) is zero.

We therefore deduce

Et xt+1 − x

2 1 + ηD2∆−1 1 + 2ηB + η∆2B2 t

≤

1 + 2ηα − 2η∆−1

x −x

3

1 + 2ηαρ + 4η2D2 + 2η2α2ρ2 t

≤

1+ρ

x −x

1 + 2ηα( 2 )

2

ησt2

+ ∆ (1 + 2ηα − 2η∆−1)

1

3

2

4η2σt2

+

1+ρ ,

1 + 2ηα( 2 )

thereby completing the proof of (32).

We next prove the second claim. To this end, suppose B = 0. All of the reasoning leading up to

and including (36) is valid. Continuing from this point, using Young’s inequality, we upper bound

P2 by:

P ≤ Et µt − G(xt+1) 2 + ∆2Et xt+1 − x 2 .

(42)

2

2∆2

2

Next observe

Et µt − G(xt+1) 2 ≤ 2Et µt − G(xt) 2 + 2Et G(xt) − G(xt+1) 2, ≤ 2Ct2 + 2L2 xt − xt+1 2 (43)

and therefore

P2 ≤ 2Ct2 + 2L2 xt − xt+1 2 + ∆2Et xt+1 − x 2

(44)

2∆2

2

Putting the estimates (35), (36), and (44) together yields:

1 + η(2α − ∆2) Et xt+1 − x 2 ≤ 1 + ηD2∆−1 1 xt − x 2

2

2

(45)

ησt2 2ηCt2∆−2 1 1 − 2ηL2∆−2 1 − η∆1

t+1

t2

++

−

Et x − x

2∆1

2

2

Setting

∆2

=α

and

∆1

= η−1 −

2L2 α

ensures

that

the

last

term

on

the

right

is

zero.

Notice

that

our

assumption

η

≤

α 4L2

ensures

∆1

≥

21η .

Rearranging

(45)

directly

yields

(33).

In

the

case

B

≡

Ct

≡

0,

instead of (43) we may simply use the bound Et µt − G(xt+1) 2 = Et G(xt) − G(xt+1) ≤ L2 xt −

xt+1 2. Continuing in the same way as above yields the improved estimate.

38

B Technical results on sequences

The following lemma is standard and follows from a simple inductive argument.

Lemma 8. Consider a sequence Dt ≥ 0 for t ≥ 1 and constants t0 ≥ 0, a > 0 satisfying

Dt+1 ≤ (1 − t+2t0 )Dt + (t+at0)2

(46)

Then the estimate holds: Dt ≤ max{(1 + t0)D1, a} ∀t ≥ 1. (47) t + t0
We will need the following more general version of the lemma.

Lemma 9. Consider a sequence Dt ≥ 0 for t ≥ 1 and constants t0 ≥ 0, a, b > 0 satisfying

Dt+1 ≤ (1 − t+2t0 )Dt + (t+at0)2 + (t+bt0)3 .

(48)

Then the estimate holds:

max{(1 + t0)D1/2, a} max{(1 + t0)3/2D1/2, b}

Dt ≤

t + t0

+ (t + t0)3/2

∀t ≥ 1.

(49)

Proof. Clearly the recursion (48) continues to hold with a and b replaced by the bigger quantities max{(1 + t0)D1/2, a} and max{(1 + t0)D1/2, b}, respectively. Therefore abusing notation, we will make this substitution. As a consequence, the claimed estimate (49) holds automatically for the base case t = 1. As an inductive assumption, suppose the claim (49) is true for Dt. Set s = t + t0. We then deduce

2

ab

Dt+1 ≤

1− s

Dt + s2 + s3

2 ≤ 1−
s

ab s + s3/2

ab + s2 + s3

11

1

21

≤ a s − s2 + b s3/2 − s5/2 + s3 .

Elementary

algebraic

manipulations

show

1 s

−

1 s2

≤

s+1 1 .

Deﬁne

the

function

φ(s)

=

1 s3/2

−

2 s5/2

+

1 s3

−

(1+s1)3/2 .

Elementary

calculus

shows

that

φ

is

increasing

on

the

interval

s ∈ [1, ∞).

Since

φ

tends to zero as s tends to inﬁnity, it follows that φ is negative on the interval [1, ∞). We therefore

conclude

Dt+1

≤

a s+1

+

b (1+s)3/2

as

claimed.

C Online Least Squares

In this appendix section, we record basic and well-known results on estimation in online least squares, following [Dieuleveut et al., 2017].

Lemma 10. Fix a probability space (Ω, F , P) with two sub-σ-algebras G1 ⊂ G2 ⊂ F . Deﬁne the

function

1 f (B) =

By − b

2,

2

where B : Ω → Rm1×m2, b : Ω → Rm1, and y : Ω → Rm2 are random variables. Suppose moreover that there exist random variables V : Ω → Rm1×m2 and σ : Ω → R satisfying the following.

39

1. B, V , and σ1 are G1-measurable.

2. y is G2-measurable. 3. The estimates, E[b | G2] = V y and E[ V y − b 2 | G2] ≤ σ2 ,hold.

4. There exist constants λ1, λ2, R > 0 satisfying λ1I E[yy | G1], E[ y 2 | G1] ≤ λ2, and E[ y 2yy | G1] R2 E[yy ].

Then

for

any

constant

ν

∈

(0,

2 R2

),

the

gradient

step

B+

=

B

−

ν(By

−

b)y

satisﬁes the bound:

1

+

2

1 − λ1ν(2 − νR2)

2 ν2σ2λ2

2 E[ B − V F | G1] ≤

2

B−V F +

. 2

Proof. Expanding the squared norm yields:

12 B+ − V 2F = 12 B − V − ν(By − b)y Taking conditional expectations, we conclude

2F = 21 B − V 2F − ν B − V, (By − b)y

ν2

2

+ (By − b)y 2

F.

1 E[

B+ − V

2

2F | G2] = 12 1
= 2

B−V B−V

2 F

−ν

B

− V, (By − E[b

|

G2])y

ν2 + 2 E[ (By − b)y

2

2 ν2 2

2

F − ν (B − V )y F + 2 y E[ By − b F | G2].

Next, observe

2 F

|

G2]

(50)

By − b

2 F

=

(B − V )y 2 +

V y − b 2 + 2 By − V y, V y − b .

Taking the conditional expectation E[· | G2], the last term vanishes, and therefore we deduce

E[

By − b

2 F

|F

]≤

(B − V )y 2 + σ2. Combining this with (50) we compute

1

+

2

1

2

2 ν2 2

2 ν2σ2 2

2 E[ B − V F | G2] ≤ 2 B − V F − ν (B − V )y F + 2 y

(B − V )y + 2

y.

Taking expectations with respect to G1 and using the tower rule, we deduce

1

+

2

1

2

2

ν2

2

2

2 E[ B − V F | G1] ≤ 2 B − V F − ν E[ (B − V )y F | G1] + 2 E[ y (B − V )y | G1]

+ ν2σ2λ2 . 2

Observe next

E[ y 2 (B − V )y 2 | G1] = (B − V )(B − V ) , E[ y 2yy

| G1]]

≤ R2E[

(B − V )y

2 F

|

G1],

and therefore

1

+

2

1

2

ν2R2

2

ν 2 σ 2 λ2

2 E[ B − V F | G1] ≤ 2 B − V F − (ν − 2 ) E[ (B − V )y F | G1] + 2

Note that ν ≥ ν22R2 . Next we estimate

E[

(B − V )y

2 F

|

G1]

=

tr((B

−V)

(B − V ) E[yy

| G1]]

≥ λ1

B−V

2 F

.

This completes the proof.

40

