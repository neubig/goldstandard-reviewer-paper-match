Description-Driven Task-Oriented Dialog Modeling
Jeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu, Mingqiu Wang, Harrison Lee, Abhinav Rastogi, Izhak Shafran, Yonghui Wu Google Research
{jeffreyzhao, raghavgupta, yuancao}@google.com

arXiv:2201.08904v1 [cs.CL] 21 Jan 2022

Abstract
Task-oriented dialogue (TOD) systems are required to identify key information from conversations for the completion of given tasks. Such information is conventionally speciﬁed in terms of intents and slots contained in taskspeciﬁc ontology or schemata. Since these schemata are designed by system developers, the naming convention for slots and intents is not uniform across tasks, and may not convey their semantics effectively. This can lead to models memorizing arbitrary patterns in data, resulting in suboptimal performance and generalization. In this paper, we propose that schemata should be modiﬁed by replacing names or notations entirely with natural language descriptions. We show that a language description-driven system exhibits better understanding of task speciﬁcations, higher performance on state tracking, improved data efﬁciency, and effective zero-shot transfer to unseen tasks. Following this paradigm, we present a simple yet effective DescriptionDriven Dialog State Tracking (D3ST) model, which relies purely on schema descriptions and an “index-picking” mechanism. We demonstrate the superiority in quality, data efﬁciency and robustness of our approach as measured on the MultiWOZ (Budzianowski et al., 2018), SGD (Rastogi et al., 2020), and the recent SGD-X (Lee et al., 2021b) benchmarks.
1 Introduction
The design of a task-oriented dialog (TOD) system conventionally starts with deﬁning a schema specifying the information required to complete its tasks — usually, a list of relevant slots and intents. These slots and intents often appear as abbreviated notations, such as train-leaveat and hotel-internet, to indicate the domain of a task and the information it captures.
Models are trained using these schemata will be heavily dependent on these abbreviations. This

is especially true for decoder-only or sequence-tosequence (seq2seq) TOD models, which are often trained with supervision to predict dialogue belief states as sequences of these notations. For example, a sequence such as train-leaveat=3:00pm, hotel-internet=no, or sequences of similar structure, are the target output for TOD models described by Hosseini-Asl et al. 2020 and Zhao et al. 2021.
This has several disadvantages. First, the element notations convey little semantic (and possibly ambiguous) meaning for the requirements of the slot (Du et al., 2021), potentially harming language understanding. Second, task-speciﬁc abstract schema notations make it easy for a model to overﬁt on observed tasks and fail to transfer to unseen ones, even if there is sufﬁcient semantic similarity between the two. Finally, creating notations for each slot and intent complicates the schema design process.
In this paper, we advocate for TOD schemata with intuitive, human-readable, and semanticallyrich natural language descriptions, rather than the abbreviated notations that have become customary when designing TOD models. Instead of “hotel-internet”, we assert that it is more natural to describe this slot as “whether the hotel has internet”. This would be easier for both the designer of the TOD system when specifying the task ontology, and we also argue that it plays an important role in improving model quality and data efﬁciency.
To this end, we present a simple yet effective approach: Description-Driven Dialog State Tracking (D3ST). Here, schema descriptions are indexed and concatenated as preﬁxes to a seq2seq model, which then learns to predict active schema element indices and corresponding values. In addition, an index-picking mechanism reduces the chance of the model overﬁtting to speciﬁc schema descriptions. We demonstrate its superior performance

measured on benchmarks including MultiWOZ (Budzianowski et al., 2018; Zang et al., 2020; Han et al., 2021; Ye et al., 2021) and Schema-Guided Dialogue (SGD, (Rastogi et al., 2020)), as well as strong few- and zero-shot transfer capability to unseen tasks. We also show evidence that, under this very general setting, natural language descriptions lead to better quality over abbreviated notations.
2 Related Work
In recent years, there has been increasing interest in leveraging language prompts for data efﬁciency and quality improvement for dialogue modelling.
Inclusion of task descriptions: One line of research focuses on providing descriptions or instructions related to the dialogue tasks. Shah et al. (2019) utilized both slot descriptions and a small number of examples of slot values for learning slot representations for spoken language understanding. Similar to our work, Lin et al. (2021b); Lee et al. (2021a) provided slot descriptions as extra inputs to the model and have shown quality improvement as well as zero-shot transferability. Mi et al. (2021) extended the descriptions to a more detailed format by including task instructions, constraints and prompts altogether, demonstrating advantages of providing more sophisticated instructions to the model. However, unlike our approach, they predict slot values one-by-one in turn, which becomes increasingly inefﬁcient as the number of slots increases, and is also prone to oversampling slot values since most slots are inactive at any stage during a dialogue. In contrast, our work predicts all states in a single pass, and is hence more efﬁcient.
Prompting language models: Powerful language models like GPT (Radford et al., 2019; Brown et al., 2020) demonstrated impressive fewshot learning ability even without ﬁne-tuning. It is therefore natural to consider leveraging these models for few-shot dialogue modeling. Madotto et al. (2020) applied GPT-2 by priming the model with examples for language understanding, state tracking, dialogue policy and language generation tasks respectively, and in Madotto et al. (2021) this approach has been extended to systematically evaluate on a set of diversiﬁed tasks using GPT-3 as backbone. Unlike these works in which the language models are frozen, we ﬁnetune the models on downstream tasks. Budzianowski and Vulic´ (2019); Baolin Peng (2020) on the other hand, applied GPT-2 for few-shot and transferable response

generation with given actions, whereas our work focuses mainly on state tracking.
Describe task with questions: Another line of research casts state tracking as a question answering (QA) or machine reading (MR) problem (Gao et al., 2020; Namazifar et al., 2020; Li et al., 2021; Lin et al., 2021a), in which models are provided questions about each slot and their values are predicted as answers to these questions. The models are often ﬁnetuned on extractive QA or MR datasets, and by converting slot prediction into QA pairs the models are able to perform zero-shot state tracking on dialogue datasets. Their question generation procedure however, is more costly than using schema descriptions, which we adopt in our work.
3 Methodology
D3ST uses a seq2seq model for dialogue state tracking, and relies purely on descriptions of schema items to instruct the model.
3.1 Model
We choose to use seq2seq for modeling for the following reasons: ﬁrst, seq2seq is a general and versatile architecture that can easily handle different formats of language instructions; second, seq2seq has been shown to be an effective approach for DST (Zhao et al., 2021); and third, seq2seq as a generic model architecture can be easily initialized from a publicly available pretrained checkpoint.
For D3ST, we used the T5 (Raffel et al., 2020) model and the associated pretrained checkpoints of different sizes: Base (220M parameters), Large (770M parameters), and XXL (11B parameters).
3.2 Description-Driven Modeling
D3ST relies solely on schema descriptions for dialogue state tracking. An example of D3ST is provided in Figure 1.
Given a set of descriptions corresponding to slots and intents speciﬁed by a schema, let dsilot, i = 1 . . . N and dijntent, j = 1 . . . M be the descriptions for slots and intents respectively, where N and M are the numbers of slots and intents. Let uutsr and ustys be the user and system utterance at turn t respectively.
Input The input to the encoder consists of the slot descriptions, intent descriptions, and conversation context concatenated into a single string. The slot descriptions have the following format:
0 : ds0lot . . . I : dsSlot

Similarly, the intent descriptions have the following format:
i0 : di0nt . . . iJ : diJnt
Note that 0 . . . I and i0 . . . iJ are the indices we assign to each of the slot and intent descriptions respectively. Here, “i” is a literal character to differentiate intent indices from those for slots. To prevent the model from memorizing association between a speciﬁc index:description pair, we randomize the assignment of indices to descriptions for each example during training. Such a dynamic construction forces the model to consider descriptions rather than treating inputs as constant strings to make generalizable predictions. The conversation context consists of all turns of the conversations concatenated together, with leading [user] and [sys] tokens before each user and system utterance, signalling the speaker of each utterance.
[usr] uu0sr [sys] us0ys . . . [usr] uuTsr [sys] usTys
Output The decoder generates a sequence of dialogue states in the format
[states] as0 : vs0 . . . asM : vsM [intents] ai0 . . . aiN
where asm is the index of the mth active slot and there are M active slots in all, vsm is its corresponding value. ain is the index of the nth active intent and N is the number of active intents. This way the model learns to identify active schema elements with abstract indices, as we randomize the element order during training. Note that inactive elements are not generated.
Handling categorical slots Some slots are categorical, that is, they have pre-deﬁned candidate values for the model to choose from. For example “whether the hotel provides free wifi or not” could have the categorical values “yes” and “no”. To improve categorical slot prediction accuracy, we enumerate possible values together with their slot descriptions. That is, assuming the ith slot is categorical and has k values va . . . vk, its corresponding input format is
i : dsilot ia) va . . . ik) vk
in which ia) . . . ik) are indices assigned to each of the values.1 Assuming this slot is active with its third value (vc) being mentioned, then the corresponding prediction has the format i : ic).
1One may also adopt a) . . . k) as value indices or even completely discard indexing for categorical values, however

3.3 Properties
From the formulation described in Section 3.2, we expect our proposed approach to have the following properties. First, the model relies fully on the understanding of schema descriptions for the identiﬁcation of active slots and intents. Second, the model learns to pick indices corresponding to the active slots, intents or categorical values, instead of generating these schema elements. This “indexpicking” mechanism, based on schema description understanding, reduces the chance of the model memorizing training schemata and makes it easier for the model to zero-shot transfer to unseen tasks. Finally, unlike previous work which also takes advantage of schema descriptions (for example Lin et al., 2021b; Lee et al., 2021a) but generates values for each slot in turn (even if a slot is inactive), our approach enables predicting multiple active (and only active) slot-value pairs together with intents with a single decoding pass, making the inference procedure more efﬁcient.
We also note that the sequence of schema descriptions prepended to the conversation context plays a similar role as instructions for speciﬁc tasks (Wei et al., 2021; Mishra et al., 2021). Providing more detailed human-readable descriptions enables the language model understand task requirements better, and leads to improved few-shot performance, as will be seen in experimental results.
4 Experiments
We design our experiments to answer the following questions:
1. What is the quality of the D3ST model, when all training data is available?
2. How does the description type for schema deﬁnition, including human-readable natural descriptions, abbreviated or even random notations, affect model quality?
3. How data-efﬁcient is D3ST in the lowresource or zero-shot regimes, and how do different description types affect efﬁciency?
4. How robust is the model to different wordings of the human-readable descriptions?
we found this shared indexing across categorical slots can sometimes cause selection ambiguity when some values (like “true” or “false”) are shared by multiple categorical slots. We therefore apply slot-speciﬁc indices ia) . . . ik) to constrain index-picking within the ith slot value range.

0:departure location of train 1:destination location of train 2:day of the train 2a) monday 2b) tuesday 2c) wednesday i1:look for a train i2:change ticket [user] i need to find a spot on a train on wednesday, can you help me find one? [system] yes i can. where are you going and what time would like to arrive or depart? [user] i’m leaving from london kings cross and going to cambridge. could you choose a train and give me the station it leaves from?

seq2seq

[states] 0:london kings cross 1:cambridge 2:2c [intents] i1

Figure 1: An example of D3ST. Red: Indexed schema description sequence as preﬁx; Blue: Conversation context; Green: State prediction sequence. See Section 3 for details. Best viewed in color.

4.1 Setup
Datasets We conduct experiments on the MultiWOZ 2.1-2.4 (Budzianowski et al., 2018; Zang et al., 2020; Han et al., 2021; Ye et al., 2021) and SGD (Rastogi et al., 2020) datasets. The MultiWOZ dataset is known to contain annotation errors in multiple places and previous work adopted different data pre-processing procedures, so we follow the recommended procedure2 of using the TRADE (Wu et al., 2019) script to pre-process MultiWOZ 2.1. However, we do not apply any pre-processing to 2.2-2.4 for reproducibility and fair comparison with existing results. We use Joint Goal Accuracy (JGA) as the evaluation metric, which measures the percentage of turns across all conversations for which all states are correctly predicted by the model.
Training setup We use the open-source T5 code base3 and the associated T5 1.1 checkpoints.4 We consider models of the size base (250M parameters), large (800M) and XXL (11B) initialized from the corresponding pretrained checkpoints, and ran each experiment on 64 TPU v3 chips (Jouppi et al., 2017). For ﬁne-tuning, we use batch size 32 and use constant learning rate of 1e − 4 across all experiments. The input and output sequence lengths are 1024 and 512 tokens, respectively.
Descriptions We use the slot and intent descriptions included in the original MultiWOZ and SGD datasets as inputs (dsilot and diint described in Section 3.2) to the model. For MultiWOZ, we include schema descriptions across all do-
2https://github.com/budzianowski/ multiwoz#dialog-state-tracking
3https://github.com/google-research/ text-to-text-transfer-transformer
4https://github.com/google-research/ text-to-text-transfer-transformer/blob/ main/released_checkpoints.md

mains as model preﬁx and set the input length limit to 2048. To avoid ambiguity between descriptions from different domains, we also add domain names as part of the descriptions. For example for the hotel-parking slot, the description is “hotel-parking facility at the hotel”. For SGD, we include descriptions from domains relevant to each turn as suggested by the standard evaluation.
4.2 Main Results
Table 1 gives the model quality when the entire training datasets are used for ﬁne-tuning. We show that D3ST is close to, or at the state-of-the-art across all benchmarks, illustrating the effectiveness of the proposed approach. We also see that increasing the model size signiﬁcantly improves the quality.
Note however that not all results are directly comparable, and we discuss some notable incongruities. The best result on SGD is from paDST, but this model has signicant advantages. paDST uses a data augmentation procedure by back-translating between English and Chinese, as well as special handcrafted rules for model predictions. In contrast, our models only train on the default SGD dataset, and do not apply any handcrafted rules whatsoever. While paDST has signiﬁcantly higher JGA compared to the similarly-sized D3ST Large. D3ST XXL is on par, making up for its lack of data augmentation and handcrafted rules with a much larger model.
One other notable comparison can be made. DaP also relies on slot descriptions and is ﬁnetuned from a T5 Base model, making it directly comparable to our D3ST Base model, which exhibits better performance on SGD and MultiWOZ. One additional advantage of D3ST is that it predicts all slots at

Model Transformer-DST (Zeng and Nie, 2021) SOM-DST (Kim et al., 2020) TripPy (Heck et al., 2020) SAVN (Wang et al., 2020) SimpleTOD# (Hosseini-Asl et al., 2020) Seq2seq (Zhao et al., 2021) DaP (seq) (Lee et al., 2021a) DaP (ind) (Lee et al., 2021a) D3ST (Base) D3ST (Large) D3ST (XXL)

Pretrain. Model (# Params.) BERT Base (110M) BERT Base (110M) BERT Base (110M) BERT Base (110M) DistilGPT-2 (82M) T5 Base (220M) T5 Base (220M) T5 Base (220M) T5 Base (220M) T5 Large (770M) T5 XXL (11B)
(a) JGA on MultiWOZ 2.1-2.4.

MW2.1 55.35 51.2 55.3 54.5 50.3/55.7 52.8
56.7 54.2 54.5 57.8

MW2.2 -
57.6 51.2 57.6 56.1 54.2 58.7

MW2.3 -
55.5 63.0 58.0 51.3 59.3
59.1 58.6 60.8

MW2.4 -
66.8 59.6 60.1
67.1
72.1 70.8 75.9

Model

Pretrain. Model (# Params.) JGA Intent

SGD baseline (Rastogi et al., 2020)

BERT Base (110M)

25.4 90.6

DaP (ind) (Lee et al., 2021a)

T5 Base (220M)

71.8 90.2

SGP-DST (Ruan et al., 2020)

T5 Base (220M)

72.2 91.8

paDSTI (Ma et al., 2020)

XLNet Large (340M)

86.5 94.8

D3ST (Base)

T5 Base (220M)

72.9 97.2

D3ST (Large)

T5 Large (770M)

80.0 97.1

D3ST (XXL)

T5 XXL (11B)

86.4 98.8

(b) JGA, active intent accuracy and requested slot F1 on SGD.

Req slot 96.5 97.8 99.0 98.5 98.9 99.1 99.4

Table 1: Results on MultiWOZ and SGD datasets with full training data. “-” indicates no public number is available. Best results are marked in bold. #: SimpleTOD results are retrieved from the 2.3 website https: //github.com/lexmen318/MultiWOZ-coref, in which two numbers are reported for 2.1 (one produced by the 2.3 author, the other by the original SimpleTOD paper). !: No data pre-processing applied for MultiWOZ 2.1. I: Data augmentation and special rules applied.

once in a single inference pass. In contrast, the independent (ind) decoding variant of DaP does inference once for every slot, similar to most other baselines, and is thus far less efﬁcient. This is not scalable in TOD, especially with schemata becoming increasingly large in terms of the number of slots, intents, and domains.
4.3 Comparison of Description Types
We now study whether the quality of D3ST is sensitive to the schema description types. For this, we run the same experiment as in Section 4.2 with D3ST Large and XXL, but using three different types of descriptions: human-readable language descriptions, schema element names (abbreviations) as deﬁned in the original schema, and random strings. The random string descriptions are generated by simply randomly permuting the character sequences of the original element names. This experiment is designed to check how a model with only memorization capability without any understanding of schema element semantics does on seen and unseen schemas. An example of all three description type comparisons can be found in Ap-

pendix A.
Type Language Name Random

M2.1 54.5 57.8 55.1 57.5 20.1 57.6

M2.2 55.9 58.7 55.8 57.9 9.0 56.1

M2.3 58.6 60.8 59.6 60.4 12.1 59.3

M2.4 70.8 75.9 72.2 75.4 16.9 73.6

SGD 80.0 86.4 73.7 79.7 37.4 64.8

Table 2: Comparison between D3ST models using different types of descriptions on MultiWOZ and SGD. “Language”, “Name” and “Random” correspond to using detailed language description, schema element name and random strings respectively. Each type contains two rows, corresponding to the results given by “large” and “XXL” models. Note that the "Random" experiments for "large" models had trouble converging, and we instead report their JGA at 85k steps.

Table 2 compares the performance with different description types. It can be seen that using language descriptions consistently outperforms other types, aligned with our expectation that natural and human-readable descriptions contain richer semantics and are aligned with the pretraining objective, enabling LM to perform better. Element names are less readable than full descriptions, but still retain

some semantics: they preform well but fall short of full descriptions. On the other hand, using random strings performs worst on average, even on MultiWOZ where the training and test schema are the same (and the model is allowed to memorize descriptions from training). With random strings, there is the extra challenge of identifying the correct slot id for each value to predict, since each example has a random shufﬂing of the slot ids. Indeed, we observed that training "large" models on random names is hard to converge, and instead of reporting their ﬁnal results, we stopped these experiments early and reported their JGA at 85k steps. The XXL models did not encounter the same issue; we suspect that it was easier for larger models to memorize slot name permutations.
In constrast to MultiWOZ, SGD requires models to generalize to unseen tasks and domains in the evaluation datasets. Here, using random strings undermined quality signiﬁcantly. In general, meaningless inputs hurt performance and lead to less generalization. We therefore suggest instructing the model with semantically rich representations, in particular, language descriptions.
One more observation we make is that, on large MultiWOZ models, using element names had better JGA than using a full language description. This trend does not hold on SGD, and also reverses when trained with XXL. We hypothesize that this is a result of input sequence length: on MultiWOZ we feed slots descriptions from all domains as preﬁx, and when full language description is utilized, the input sequence becomes excessively long. Using element names shortens the length, making a moderate-size model easier to learn. In contrast, input sequence lengths on SGD are lower than that on MultiWOZ, since only active domains are provided as part of the input.
4.4 Data Efﬁciency
Properly designed preﬁxes or prompts have been shown to signiﬁcantly improve an LM’s data efﬁciency (Radford et al., 2019; Liu et al., 2021; Wei et al., 2021). We investigate how different types of description preﬁxes vary in performance in low-resource regimes by running experiments with large and XXL models on SGD with 0.16% (10-shot), 1%, and 10% of training data. For the 0.16% experiment, we randomly select 10 samples from each training domain to increase the domain diversity, totalling 260 examples. For other exper-

iments the samples are uniformly sampled across the entire training set. We sample from three random seeds for each experiment.

Type Language Name

0.18% 6.1 ± 0.7 51.0 ± 0.2 5.0 ± 0.2 47.7 ± 0.5

1% 36.7 ± 2.0 79.4 ± 0.4 28.0 ± 2.7 74.9 ± 1.4

10% 73.1 ± 0.2 83.0 ± 0.1 69.7 ± 0.3 78.6 ± 0.7

Table 3: Data efﬁciency of D3ST using natural language and element name descriptions, trained and evaluated on SGD. Each description type contains two rows, corresponding to the results given by “large” and “XXL” models. The metric is JGA.

The results are given in Table 3. From the table we have the following observations:
• Using human-readable language descriptions consistently outperforms other types of representations, indicating better data efﬁciency with semantically-rich descriptions.
• With just 0.18% of the data, XXL models can already reach more than half of their full quality (from Table 1). At 1%, we observe quality close to using 100% data. Increasing to 10% only yielded marginal gains.
• Larger models are much more data efﬁcient than smaller ones, as can be seen from the big gap between “large” and “XXL” models.
4.5 Zero-shot Transfer to Unseen Tasks
To assess our approach’s zero-shot transfer ability to unseen tasks, we conduct the following set of experiments: MultiWOZ cross-domain transfer Following a setup similar to TransferQA (Lin et al., 2021a) and T5DST (Lin et al., 2021b), we run the “leaveone-out” cross-domain zero-shot transfer evaluation on MultiWOZ 2.1.5 For each domain, we train a model on examples excluding that domain, and evaluate it on examples including it. Table 4a shows our results in comparison with the baselines.6 It can be seen that our approach achieves
5For zero-shot evaluation, Lin et al. (2021a) and Lin et al. (2021b) experimented on MultiWOZ 2.1 and 2.0 respectively. While our models are trained and evaluated on MultiWOZ 2.1, we include results from both of them for comparison.
6When skipping the train domain, we postprocess predictions for slots train-departure and train-destination by ignoring the sufﬁx "train station". This is semantically correct and improves JGA.

the best cross-domain transfer performance with signiﬁcant gains across almost all domains. SGD unseen service transfer The SGD benchmark contains numerous services and some domains only present in the test set. We present the results for zero-shot transfer to these domains and services in Table 4b. Note that D3ST Base has worse JGA on unseen domains when fairly compared to DaP and SGP-DST. However, D3ST has superlative JGA on seen domains, even better than paDST (with data augmentation and hand-crafted rules). In addition, increasing the size of D3ST further increases both seen and especially unseen JGA, indicating better generalization. At XXL, JGA on unseen domains is almost equal to paDST. Cross-dataset transfer In this setup, we evaluate if a model trained on one dataset can be directly applied to another dataset. To this end, we train a model on SGD then directly evaluate on the MultiWOZ 2.4 test set, and vice versa7. In both cases we use the XXL model from Section 4.2, and report the numbers in Table 4.
Despite obvious schema differences and domain mismatch between MultiWOZ and SGD, our model trained on MultiWOZ already achieves zero-shot quality on SGD close to the BERT-baseline (Rastogi et al., 2020) with 25.4% JGA. Our model trained on SGD and evaluated on MultiWOZ shows similarly strong zero-shot results. Both results are much lower than the state of the art for both datasets however, due to differing biases deﬁned in schemata between the two datasets, and from latent knowledge that isn’t captured from a schema alone. Qualitative Evaluation In addition to quantitatively evaluating zero-shot transfer, we qualitatively examined examples of D3ST transferring to novel domains. We handcrafted a few dialogues for domains very different from the ones seen in the SGD dataset (e.g. conference submission, internet provider, e-commerce retailer). We designed the dialogues to be as stylistically realistic as possible for customer service scenarios. We tasked the XXL model trained on SGD (from Table 1) with inferring their dialogue states, and share one example in Table 5. More examples can be found
7Note that the SGD dataset deﬁnes the services that will occur in each dialogue, whereas MultiWOZ expects models to be able to predict any of its domains for all dialogues. To make it compatible between SGD and MultiWOZ for cross-task zero-shot transfer, we limit the schema preﬁx for MutliWOZ to domains that appear in the current dialogue.

Domain
Attraction Hotel Restaurant Taxi Train Avg

D3ST 56.4 21.8 38.2 78.4 38.7 46.7

JGA TransferQA
31.3 22.7 26.3 61.9 36.7 35.8

T5DST 33.1 21.2 21.7 64.6 35.4 35.2

(a) Cross-domain (leave-one-out) transfer on MultiWOZ.

Model

JGA Overall Seen Unseen

SGD Baseline 25.4 41.2 20.0

DaP (ind)

71.8 83.3 68.0

SGP-DST

72.2 87.9 66.9

Team14L

77.3 90.0 73.0

paDSTI

86.5 92.4 84.6

D3ST (base)

72.9 92.5 66.4

D3ST (large) 80.0 93.8 75.4

D3ST (XXL) 86.4 95.8 83.3

(b) JGA on seen versus unseen services for SGD. L and I have the same meaning as in Table 1.

Transfer SGD→MultiWOZ MultiWOZ→SGD

JGA 28.9 23.1

(c) Cross-dataset transfer b/w SGD and MultiWOZ 2.4.

Table 4: Zero-shot transfer evaluation results from three different setups.

in Table A2 of Appendix B. We observe that the model performs surprisingly well across all of our handcrafted dialogues, even though the domains are very different from the training data.
4.6 Robustness to Variations of Descriptions
Since there are many ways to provide descriptions for a given schema, a natural question to raise about this approach is how robust the model is against different choices of descriptions. The recently proposed SGD-X benchmark (Lee et al., 2021b) is designed speciﬁcally for the study of this problem. SGD-X contains ﬁve variations of the original SGD, each one using a different set of schema descriptions provided by different crowd-source workers. To assess the robustness of D3ST, we use the large and XXL models evaluated in Section 4.2 and decode test sets from each of the ﬁve variants of SGD-X. A robust model is expected to have smaller ﬂuctuations in predictions across schema variants for the same dialogue context, as measured by Schema Sensitivity SS(JGA) deﬁned in Lee et al. (2021b),. which calculates the average variation coefﬁcient of JGA at turn level. A lower SS(JGA) value implies less ﬂuctuation and more robustness.
We compare the robustness of models using dif-

Domain: Conference Submission
Input: 0:name of the conference 1:title of the paper 2:the first author of the paper 3:research areas for the paper 4:email for openreview account i1:submit a paper to a conference i2:check if a paper has been accepted [user] hi, i’d like to submit a paper for a conference [system] that’s great. which conference would you like to submit to? [user] i’d like to submit to acl 2022 [system] ok. could you share the title of your paper and the name of your first author? [user] the paper is "description-driven task-oriented dialog modeling", and the first author is grace hopper [system] great, thank you. note that this year, we require all paper authors to be registered on openreview. could you give the email for your openreview account? [user] sure, its gracehopper@gmail.com
Prediction:[states] 0:acl 2022 1:description-driven task-oriented dialog modeling 2:grace hopper 4:gracehopper@gmail.com [intents] i1
Table 5: An example of D3ST performing zero-shot transfer to a hypothetical "Conference Submission" domain. The predicted dialogue state is entirely correct. Boldface and color were added for visual clarity.

ferent prompt types in Table 6. From the numbers we see that using the most human-readable natural language descriptions not only achieves the highest average accuracy over all SGD-X test set variants, but also enjoys the smallest SS(JGA) at the same model size. This indicates that description-driven models are more robust. On the other hand, using element names and random names have progressively lower mean accuracy and higher sensitivity to schema changes.

Size Orig v1 v2 v3 v4 v5 Avg v1-5 SS(JGA)

large 80.0 79.9 79.4 76.5 71.9 69.1 75.3

0.26

XXL 86.4 85.5 85.1 73.9 75.5 68.9 77.8

0.27

(a) Natural language description

Size Orig v1 v2 v3 v4 v5 Avg v1-5 SS(JGA)

large 73.7 72 69.5 66.4 61.1 65.7 66.9

0.37

XXL 79.7 80.8 76.6 74.2 61.2 72.3 73.0

0.35

(b) Element name description

Size Orig v1 v2 v3 v4 v5 Avg v1-5 SS(JGA)

large 37.4 29.3 34.6 28.0 25.2 25.0 28.4

0.74

XXL 64.8 67.8 68.8 72.9 58.1 68.1 67.1

0.51

(c) Random description

Table 6: Robustness comparison for various description types. SS(JGA) refers to schema sensitivity for JGA.

5 Conclusion
We advocate using human-readable language descriptions in place of abbreviated or arbitrary notations for schema deﬁnition in TOD modeling. We

believe this schema representation contains more meaningful information for a strong LM to leverage, leading to better performance and improved data efﬁciency. To this end, we propose a simple and effective DST model named “DescriptionDriven Dialogue State Tracking” (D3ST), which relies fully on schema descriptions and an indexpicking mechanism to indicate active slots or intents. Our experiments verify the effectiveness of description-driven dialogue modeling in the following ways. First, D3ST achieves superior quality on MultiWOZ and SGD. Second, using language descriptions outperforms abbreviations or arbitrary notations. Third, the description driven approach improves data-efﬁciency, and enables effective zero-shot transfer to unseen tasks and domains. Fourth, using language for schema description improves model robustness as measured by the SGD-X benchmark.
References
Chunyuan Li Xiujun Li Jinchao Li Michael Zeng Jianfeng Gao Baolin Peng, Chenguang Zhu. 2020. Fewshot natural language generation for task-oriented dialog.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.
Paweł Budzianowski and Ivan Vulic´. 2019. Hello, it’s GPT-2 - how can I help you? towards the use of pretrained language models for task-oriented dialogue systems. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 15–22, Hong Kong. Association for Computational Linguistics.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašic´. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016–5026, Brussels, Belgium. Association for Computational Linguistics.

Xinya Du, Luheng He, Qi Li, Dian Yu, Panupong Pasupat, and Yuan Zhang. 2021. QA-driven zeroshot slot ﬁlling with weak supervision pretraining. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 654–664, Online. Association for Computational Linguistics.
Shuyang Gao, Sanchit Agarwal, Di Jin, Tagyoung Chung, and Dilek Hakkani-Tur. 2020. From machine reading comprehension to dialogue state tracking: Bridging the gap. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 79–89, Online. Association for Computational Linguistics.
Ting Han, Ximing Liu, Ryuichi Takanobu, Yixin Lian, Chongxuan Huang, Dazhen Wan, Wei Peng, and Minlie Huang. 2021. Multiwoz 2.3: A multidomain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation.
Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Gasic. 2020. TripPy: A triple copy strategy for value independent neural dialog state tracking. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 35–44, 1st virtual meeting. Association for Computational Linguistics.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue.
Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, and Pierre-luc et al. Cantin. 2017. Indatacenter performance analysis of a tensor processing unit. SIGARCH Comput. Archit. News, 45(2):1–12.
Sungdong Kim, Sohee Yang, Gyuwan Kim, and SangWoo Lee. 2020. Efﬁcient dialogue state tracking by selectively overwriting memory.
Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021a. Dialogue state tracking with a language model using schema-driven prompting. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Harrison Lee, Raghav Gupta, Abhinav Rastogi, Yuan Cao, Bin Zhang, and Yonghui Wu. 2021b. Sgd-x: A benchmark for robust generalization in schemaguided dialogue systems.
Shuyang Li, Jin Cao, Mukund Sridhar, Henghui Zhu, Shang-Wen Li, Wael Hamza, and Julian McAuley. 2021. Zero-shot generalization in dialog state tracking through generative question answering. In Proceedings of the 16th Conference of the European

Chapter of the Association for Computational Linguistics: Main Volume, pages 1063–1074, Online. Association for Computational Linguistics.
Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, and Pascale Fung. 2021a. Zero-shot dialogue state tracking via cross-task transfer.
Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, and Rajen Subba. 2021b. Leveraging slot descriptions for zero-shot cross-domain dialogue StateTracking. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5640–5648, Online. Association for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing.
Yue Ma, Zengfeng Zeng, Dawei Zhu, Xuan Li, Yiying Yang, Xiaoyuan Yao, Kaijie Zhou, and Jianping Shen. 2020. An end-to-end dialogue state tracking system with machine reading comprehension and wide & deep classiﬁcation.
Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, and Pascale Fung. 2021. Few-shot bot: Promptbased learning for dialogue systems.
Andrea Madotto, Zihan Liu, Zhaojiang Lin, and Pascale Fung. 2020. Language models as few-shot learner for task-oriented dialogue systems.
Fei Mi, Yitong Li, Yasheng Wang, Xin Jiang, and Qun Liu. 2021. Cins: Comprehensive instruction for fewshot learning in task-oriented dialog systems.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions.
Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur, and Dilek Hakkani-Tür. 2020. Language model is all you need: Natural language understanding as question answering.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-totext transformer. Journal of Machine Learning Research, 21(140):1–67.

Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(05):8689–8696.
Yu-Ping Ruan, Zhen-Hua Ling, Jia-Chen Gu, and Quan Liu. 2020. Fine-tuning bert for schema-guided zeroshot dialogue state tracking.
Darsh Shah, Raghav Gupta, Amir Fayazi, and Dilek Hakkani-Tur. 2019. Robust zero-shot cross-domain slot ﬁlling with example values. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5484–5490, Florence, Italy. Association for Computational Linguistics.
Yexiang Wang, Yi Guo, and Siqi Zhu. 2020. Slot attention with value normalization for multi-domain dialogue state tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3019–3028, Online. Association for Computational Linguistics.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners.
Chien-Sheng Wu, Andrea Madotto, Ehsan HosseiniAsl, Caiming Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808–819, Florence, Italy. Association for Computational Linguistics.
Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2021. Multiwoz 2.4: A multi-domain taskoriented dialogue dataset with essential annotation corrections to improve state tracking evaluation.
Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 109–117, Online. Association for Computational Linguistics.
Yan Zeng and Jian-Yun Nie. 2021. Jointly optimizing state operation prediction and value generation for dialogue state tracking.
Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, and Yonghui Wu. 2021. Effective sequence-tosequence dialogue state tracking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7486–7493, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

A Example of Description Types
An example of the different description types for a single example can be found in Table A1.
B Zero-shot Transfer to Novel Domains
Qualitative examples showcasing zero-shot transfer to novel domains can be found in Table A2.

Language

0:playback device on which the song is to be played 0a) bedroom speaker 0b) tv 0c) kitchen speaker 1=name of the artist the song is performed by 2=name of the song 3=album the song belongs to 4=genre of the song i0=search for a song based on the name and optionally other attributes i1=play a song by its name and optionally artist [user] i want to find a movie. [system] what is your location. [user] santa rosa. i want to see it at 3rd street cinema. [system] i found 3 movies. does hellboy, how to train your dragon: the hidden world or the upside interest you? [user] how to train your dragon: the hidden world is perfect. can you find me some songs from the album summer anthems. [system] i found 1 song you may like. how about no other love from the album summer anthems by common kings? [user] that would be great. [system] play the song now? [user] play it on the bedroom device.

Name

0:music_2-genre 1:music_2-playback_device 1a) bedroom speaker 1b) kitchen speaker 1c) tv 2:music_2-album 3:music_2-artist 4:music_2-song_name i0:music_2-playmedia i1:music_2-lookupmusic [user] i want to find a movie. [system] what is your location. [user] santa rosa. i want to see it at 3rd street cinema. [system] i found 3 movies. does hellboy, how to train your dragon: the hidden world or the upside interest you? [user] how to train your dragon: the hidden world is perfect. can you find me some songs from the album summer anthems. [system] i found 1 song you may like. how about no other love from the album summer anthems by common kings? [user] that would be great. [system] play the song now? [user] play it on the bedroom device.

Random

0:e-e_ciugs2mrn 1:psuekc_l-2imceyibaca_dv 1a) bedroom speaker 1b) kitchen speaker 1c) tv 2:umm2uisc_bal- 3:satriti_2-sumc 4:_-onassng2_cemmui i0:aeusmmci2-adipl_y i1:miiu_2olosckucp-ums [user] i want to find a movie. [system] what is your location. [user] santa rosa. i want to see it at 3rd street cinema. [system] i found 3 movies. does hellboy, how to train your dragon: the hidden world or the upside interest you? [user] how to train your dragon: the hidden world is perfect. can you find me some songs from the album summer anthems. [system] i found 1 song you may like. how about no other love from the album summer anthems by common kings? [user] that would be great. [system] play the song now? [user] play it on the bedroom device.

States [states] 1:1a 2:summer anthems 4:no other love [intents] i0

Table A1: Examples of the same SGD dialogue with different description types. "Language" uses a detailed natural language description, "Name" uses the schema element name, and "Random" is generated from a random shufﬂing of the slot name. Note that the categorical slot value enumeration is unaffected in "Random", and that all three description types would have the same target slots and intents.

Domain Internet Provider

Inputs

0:email address of the account 1:whether professional help is needed for internet installation 1a) true 1b) false 2:whether to bundle services on the same plan 2a) true 2b) false 3:download speed of the internet plan 4:whether services are for residential or business use 4a) residential 4b) business 5:the address to provide services to i0=buy or change an internet plan i1:file a formal complaint [user] hi there - my internet contract is up for renewal, and i’m interested in exploring other plan options. [system] happy to help. is this for your home or for a business? [user] home [system] what’s the email associated with your account? [user] noamchomsky@hotmail.com [system] thanks. your current plan is 25 mbps download speed for $53 / month. the two other plans are 50 mbps for $63 / month and 100 mbps for $73 / month. would you interested in either of those? [user] i’m interested in upgrading to the 50 mbps plan. [system] great. for $10 / month more, would you like to include our basic cable plan? [user] no thanks. i’ll need to talk this over with my partner. thank you for your help.

States [states] 0:noamchomsky@hotmail.com 3:50 mbps 4:4a [intents] i0 Domain E-Commerce Retailer

Inputs

0:phone number associated with the customer’s account 1:a coupon code to apply to the purchase 2:the reason for the product return 2a) accidental purchase 2b) malfunction 2c) preference 3:the retail product to purchase or to be returned 4:date the product was purchased 5:identifier associated with the purchase i0:return a product i1:purchase a product [system] hi how can i help you today? [user] hello - i recently purchased a glow in the dark ball that i’d like to return. [system] no problem. i’m happy to help. can you provide the order number or date of purchase please? [user] 1ozdl3v260lkq, and i purchased it last week on nov 1, 2021 [system] thanks. and what’s the reason for the return? [user] the ball seems to be broken. it doesn’t actually glow in the dark. [system] sorry to hear about that. we’ll process the return and you should receive a refund within 10 business days. is there anything else i can do for you? [user] no, thanks for your help!

States [states] 2:2b 3:glow in the dark ball 4:nov 1, 2021 5:1ozdl3v260lkq [intents] i0

Table A2: Two more examples of D3ST trained on SGD performing zero-shot transfer to novel domains. The only error is in the "Internet Provider" example, where the model misses that the slot for "whether to bundle services on the same plan" should be false. We hypothesize that "bundle" is industry jargon that the model fails to associate with the dialogue context.

