arXiv:1012.2098v7 [stat.ME] 8 Aug 2013

Multinomial Inverse Regression for Text Analysis
Matt Taddy
taddy@chicagobooth.edu The University of Chicago Booth School of Business
ABSTRACT: Text data, including speeches, stories, and other document forms, are often connected to sentiment variables that are of interest for research in marketing, economics, and elsewhere. It is also very high dimensional and difﬁcult to incorporate into statistical analyses. This article introduces a straightforward framework of sentiment-sufﬁcient dimension reduction for text data. Multinomial inverse regression is introduced as a general tool for simplifying predictor sets that can be represented as draws from a multinomial distribution, and we show that logistic regression of phrase counts onto document annotations can be used to obtain low dimension document representations that are rich in sentiment information. To facilitate this modeling, a novel estimation technique is developed for multinomial logistic regression with very high-dimension response. In particular, independent Laplace priors with unknown variance are assigned to each regression coefﬁcient, and we detail an efﬁcient routine for maximization of the joint posterior over coefﬁcients and their prior scale. This ‘gamma-lasso’ scheme yields stable and effective estimation for general high-dimension logistic regression, and we argue that it will be superior to current methods in many settings. Guidelines for prior speciﬁcation are provided, algorithm convergence is detailed, and estimator properties are outlined from the perspective of the literature on non-concave likelihood penalization. Related work on sentiment analysis from statistics, econometrics, and machine learning is surveyed and connected. Finally, the methods are applied in two detailed examples and we provide out-of-sample prediction studies to illustrate their effectiveness.
Taddy is an Associate Professor of Econometrics and Statistics and Neubauer Family Faculty Fellow at the University of Chicago Booth School of Business, and this work was partially supported by the IBM Corporation Faculty Research Fund at Chicago. The author thanks Jesse Shapiro, Matthew Gentzkow, David Blei, Che-Lin Su, Christian Hansen, Robert Gramacy, Nicholas Polson, and anonymous reviewers for much helpful discussion.

1 Introduction

This article investigates the relationship between text data – product reviews, political speech,

ﬁnancial news, or a personal blog post – and variables that are believed to inﬂuence its com-

position – product quality ratings, political afﬁliation, stock price, or mood polarity. Such

language-motivating observable variables, generically termed sentiment in the context of this

article, are often the main object of interest for text mining applications. When, as is typical,

large amounts of text are available but only a small subset of documents are annotated with

known sentiment, this relationship yields the powerful potential for text to act as a stand-in for

related quantities of primary interest. On the other hand, language data dimension (i.e., vocab-

ulary size) is both very large and tends to increase with the amount of observed text, making the

data difﬁcult to incorporate into statistical analyses. Our goal is to introduce a straightforward

framework of sentiment-preserving dimension reduction for text data.

As detailed in Section 2.1, a common statistical treatment of text views each document as

an exchangeable collection of phrase tokens. In machine learning, these tokens are usually just

words (e.g., tax, pizza) obtained after stemming for related roots (e.g., taxation, taxing, and

taxes all become tax), but richer tokenizations are also possible: for example, we ﬁnd it useful

to track common n-gram word combinations (e.g. bigrams pay tax or cheese pizza and trigrams

such as too much tax). Under a given tokenization each document is represented as xi =

[xi1, . . . , xip] , a sparse vector of counts for each of p tokens in the vocabulary. These token

counts, and the associated frequencies fi = xi/mi where mi =

p j=1

xij

,

are

then

the

basic

data

units for statistical text analysis. In particular, the multinomial distribution for xi implied by an

assumption of token-exchangeability can serve as the basis for efﬁcient dimension reduction.

Consider n documents that are each annotated with a single sentiment variable, yi (e.g.,

restaurant reviews accompanied by a one to ﬁve star rating). A naive approach to text-sentiment

prediction would be to ﬁt a generic regression for yi|xi. However, given the very high dimen-

sion of text-counts (with p in the thousands or tens of thousands), one cannot efﬁciently estimate

this conditional distribution without also taking steps to simplify xi. We propose an inverse re-

gression (IR) approach, wherein the inverse conditional distribution for text given sentiment is

used to obtain low dimensional document scores that preserve information relevant to yi.

As an introductory example, consider the text-sentiment contingency table built by col-

2

lapsing token counts as xy = i:yi=y xi for each y ∈ Y, the support of an ordered discrete sentiment variable. A basic multinomial inverse regression (MNIR) model is then
xy ∼ MN(qy, my) with qyj = pexp[αj + yϕj] , for j = 1, . . . , p, y ∈ Y (1) l=1 exp[αl + yϕl]
where each MN is a p-dimensional multinomial distribution with size my = i:yi=y mi and probabilities qy = [qy1, . . . , qyp] that are a linear function of y through a logistic link. Under conditions detailed in Section 3, the sufﬁcient reduction (SR) score for fi = xi/mi is then

zi = ϕ fi ⇒ yi ⊥⊥ xi, mi | zi.

(2)

Hence, given this SR projection, full xi is ignored and modeling the text-sentiment relationship becomes a univariate regression problem. This article’s examples include linear, E[yi] = β0+β1zi, quadratic, E[yi] = β0+β1zi+β2zi2, and logistic, p(yi < a) = (1 + exp[β0 + β1zi])−1, forms for this forward regression, and SR scores should be straightforward to incorporate into alternative regression models or structural equation systems. The procedure rests upon assumptions that allow for summary tables wherein the text-sentiment relationship of interest can be modeled as a logistic multinomial, but when such assumptions are plausible, as we ﬁnd common in text analysis, they introduce information that should yield signiﬁcant efﬁciency gains.
In estimating models of the type in (1), which involve many thousands of parameters, we propose use of fat-tailed and sparsity-inducing independent Laplace priors for each coefﬁcient ϕj. To account for uncertainty about the appropriate level of variable-speciﬁc regularization, each Laplace rate parameter λj is left unknown with a gamma hyperprior. Thus, for example,

λj −λ |ϕ | rs s−1 −rλ

π(ϕj, λj) = 2 e j j Γ(s) λj e j , s, r, λj > 0,

(3)

independent for each j under a Ga(s, r) hyperprior speciﬁcation. This departure from the usual shared-λ model is motivated in Section 3.3.
Fitting MNIR models is tough for reasons beyond the usual difﬁculties of high dimension regression – simply evaluating the large-response likelihood is expensive due to the normalization in calculating each qi. As surveyed in Section 4, available cross-validation (e.g., via

3

solution paths) and fully Bayesian (i.e., through Monte-Carlo marginalization) methods for estimating ϕj under unknown λj are prohibitively expensive. A novel algorithm is proposed for ﬁnding the joint posterior maximum (MAP) estimate of both coefﬁcients and their prior scale. The problem is reduced to log likelihood maximization for ϕ with a non-concave penalty, and it can be solved relatively quickly through coordinate descent. For example, given the prior in (3), the log likelihood implied by (1) is maximized subject to (i.e., minus) cost constraints

c(ϕj) = s log(1 + |ϕj|/r)

(4)

for each coefﬁcient. This provides a powerful new estimation framework, which we term the gamma-lasso. The approach is very computationally efﬁcient, yielding robust SR scores in less than a second for documents with thousands of unique tokens. Indeed, although a full comparison is beyond the scope of this paper, we ﬁnd that the proposed algorithm can also be far superior to current techniques for high-dimensional logistic regression in the more common large-predictor (rather than large-response) setting.
This article thus includes two main methodological contributions. First, Section 3 introduces multinomial inverse regression as an IR procedure for predictor sets that can be represented as draws from a multinomial, and details its application to text-sentiment analysis. This includes full model speciﬁcation and general sufﬁciency results, guidelines on how text data should be handled to satisfy the MNIR model assumptions, and our independent gammaLaplace prior speciﬁcation. Second, Section 4 develops a novel approach to estimation in very high dimensional logistic regression. This includes details of coordinate descent for joint MAP estimation of coefﬁcients and their unknown variance, and an outline of estimator properties from the perspective of the literature on non- concave likelihood penalization. As background, Section 2 brieﬂy surveys the literature on text mining and sentiment analysis, and on dimension reduction and inverse regression.
The following section describes language pre-processing and introduces two datasets that are used throughout to motivate and illustrate our methods. Performance comparison and detailed results for these examples are then presented in Section 5. Both example datasets, along with all implemented methodology, are available in the textir package for R.

4

1.1 Data processing and examples
Text is usually initially cleaned according to some standard information retrieval criteria, and we refer the reader to Jurafsky and Martin (2009) for an overview. In this article, we simply remove a limited set of stop words (e.g., and or but) and punctuation, convert to lowercase, and strip sufﬁxes from roots according to the Porter stemmer (Porter, 1980). The main data preparation step is then to parse clean text into informative language tokens; as mentioned in the introduction, counts for these tokens are the starting point for statistical analysis. Most commonly (see, e.g., Srivastava and Sahami, 2009) the tokens are just words, such that each document is treated as a vector of word-counts. This is referred to as the bag-of-words representation, since these counts are summary statistics for language generated by exchangeable draws from a multinomial ‘bag’ of word options.
Despite its apparent limitations, the token-count framework can be made quite ﬂexible through more sophisticated tokenization. For example, in the N -gram language model words are drawn from a Markov chain of order N (see, e.g., Jurafsky and Martin, 2009). A document is then summarized by its length-N word sequences, or N -gram tokens, as these are sufﬁcient for the underlying Markov transition probabilities. Our general practice is to count common unigram, bigram, and trigram tokens (i.e., words and 2-3 word phrases). Another powerful technique is to use domain-speciﬁc knowledge to parse for phrases that are meaningful in the context of a speciﬁc ﬁeld. Talley and O’Kane (2011) present one such approach for tokenization of legal agreements; for example, they use any conjugation of the word act in proximity of God to identify a common Act of God class of carve-out provisions. Finally, work such as that of Poon and Domingos (2009) seeks to parse language according to semantic equivalence.
Thus while we focus on token-count data, different language models are able to inﬂuence analysis through tokenization rules. And although separation of parsing from statistical modeling limits our ability to quantify uncertainty, it has the appealing effect of allowing text data from various sources and formats to all be analyzed within a multinomial likelihood framework.
1.1.1 Ideology in political speeches
This example originally appears in Gentzkow and Shapiro (GS; 2010) and considers text of the 109th (2005-2006) Congressional Record. For each of the 529 members of the United States
5

House and Senate, GS record usage of phrases in a list of 1000 bigrams and trigrams. Each

document corresponds to a single person. The sentiment of interest is political partisanship,

where party afﬁliation (Republican, Democrat, or Independent) provides a simple indicator

and a higher-ﬁdelity measure is calculated as the two-party vote-share from each speaker’s

constituency (congressional district for representatives; state for senators) obtained by George

W. Bush in the 2004 presidential election. Note that token vocabulary in this example is inﬂu-

enced by sentiment: GS built contingency tables for bigram and trigram usage by party, and

kept the top 1000 ‘most partisan’ phrases according to ranking of their Pearson χ2-test statistic.

Deﬁne phrase frequency lift for a given group as f¯jG/f¯j, where f¯jG is mean frequency for

phrase j in group G and f¯j =

n i=1

fij /n

is

the

average

across

all

documents.

The

following

tables show top-ﬁve lift phrases used at least once by each party.

DEMOCRATIC FREQUENCY LIFT

congressional.hispanic.caucu medicaid.cut
clean.drinking.water earth.day
tax.cut.beneﬁt

2.163 2.154 2.154 2.152 2.149

REPUBLICAN FREQUENCY LIFT

ayman.al.zawahiri america.blood.cent million.budget.request million.illegal.alien temporary.worker.program

1.850 1.849 1.847 1.846 1.845

1.1.2 On-line restaurant reviews

This dataset, which originally appears in the topic analysis of Maua´ and Cozman (2009), con-

tains 6260 user-submitted restaurant reviews (90 word average) from www.we8there.com.

The reviews are accompanied by a ﬁve-star rating on four speciﬁc aspects of quality – food,

service, value, and atmosphere – as well as the overall experience. After tokenizing the text

into bigrams (based on a belief that modiﬁers such as very or small would be useful here), we

discard phrases that appear in less than ten reviews and documents which do not use any of the

remaining phrases. This leaves a dataset of 6147 review counts for a token vocabulary of 2978

bigrams. Top-ﬁve lift phrases that occur at least once in both positive (overall experience > 3)

and negative (overall experience < 3) reviews are below.

NEGATIVE FREQUENCY LIFT

food poison food terribl one worst spoke manag
after left

5.402 5.354 5.339 5.318 5.285

POSITIVE FREQUENCY LIFT

worth trip everi week melt mouth
alway go onc week

1.393 1.390 1.389 1.389 1.389

6

2 Background
This section brieﬂy reviews the relevant literatures on sentiment analysis and inverse regression. Additional background is in the appendices and material speciﬁc to estimation is in Section 4.
2.1 Analysis of sentiment in text
As already outlined, we use sentiment to refer to any variables related to document composition. Although broader than its common ‘opinion polarity’ usage, this deﬁnition as ‘sensible quality’ ﬁts our need to refer to the variety of quantities that may be correlated with text.
Much of existing work on sentiment analysis uses word frequencies as predictors in generic regression and classiﬁcation algorithms, including support vector machines, principle components (PC) regression, neural networks, and penalized least-squares. Examples from this machine learning literature can be found in the survey by Pang and Lee (2008) and in the collection from Srivastava and Sahami (2009). In the social sciences, research on ideology in political text includes both generic classiﬁers (e.g., Yu et al., 2008) and analysis of contingency tables for individual terms (e.g., Laver et al., 2003) (machine learning researchers, such as Thomas et al., 2006, have also made contributions in this area). In economics, particularly ﬁnance, it is more common to rely on weighted counts for pre-deﬁned lists of terms with positive or negative tone; examples of this approach include Tetlock (2007) and Loughran and McDonald (2011) (again, machine learners such as Bollen et al., 2011, have also studied prediction for ﬁnance).
These approaches all have drawbacks: generic regression does nothing to leverage the particulars of text data, independent analysis of many contingency tables leads to multiple-testing issues, and pre-deﬁned word lists are subjective and unreliable. A more promising strategy is to use text-speciﬁc dimension reduction based upon the multinomial implied by exchangeability of token-counts. For example, a topic model treats documents as drawn from a multinomial distribution with probabilities arising as a weighted combination of ‘topic’ factors. Thus xi ∼ MN(ωi1θ1 + . . . + ωiKθK, mi), where topics θk = [θk1 · · · θkp] and weights ωi are probability vectors. This framework, also known as latent Dirichlet allocation (LDA), has been widely used in text analysis since its introduction by Blei et al. (2003).
The low dimensional topic-weight representation (i.e., ωi) serves as a basis for sentiment
7

analysis in the original Blei et al. article, and has been used in this way by many since. The approach is especially popular in political science, where work such as that of Grimmer (2010) and Quinn et al. (2010) investigates political interpretation of latent topics (these authors restrict ωik ∈ {0, 1} such that each document is drawn from a single topic). Recently, Blei and McAuliffe (2007) have introduced supervised LDA (sLDA) for joint modeling of text and sentiment. In particular, they augment topic model with a forward regression yi = f (ωi), such that token counts and sentiment are connected through shared topic-weight factors.
Finally, our investigation was originally motivated by a desire to build a model-based version of the speciﬁc slant indices proposed by Gentzkow and Shapiro (2010), which are part of a general political science literature on quantifying partisanship through weighted-term indices (e.g., Laver et al., 2003). Appendix A.1 shows that the GS indices can be written as summation of phrase frequencies loaded by their correlation with measured partisanship (e.g., vote-share), such that slant is equivalent to ﬁrst-order partial least-squares (PLS; Wold, 1975).
2.2 Inverse regression and sufﬁcient reduction
This article is based on a notion that, given the high dimension of text data, it is not possible to efﬁciently estimate conditional response y|x without ﬁnding a way to simplify x. The same idea motivates many of the techniques surveyed above, including LDA and sLDA, PLS/slant, and PC regression. A framework to unify techniques for dimension reduction in regression can be found in Cook’s 2007 overview of inverse regression, wherein inference about the multivariate conditional distribution x|y is used to build low dimension summaries for x.
Suppose that vi is a K-vector of response factors through which xi depends on yi (i.e., vi is a possibly random function of yi). Then Cook’s linear IR formulation has xi = Φvi + i, where Φ = [ϕ1 · · · ϕK] is a p × K matrix of inverse regression coefﬁcients and i is p-vector of error terms. Under certain conditions on var( i), detailed by Cook, the projection zi = Φ xi provides a sufﬁcient reduction (SR) such that yi is independent of xi given zi. As this implies p(xi|Φ xi, yi) = p(xi|Φ xi), SR corresponds to the classical deﬁnition of sufﬁciency for ‘data’ xi and ‘parameter’ yi, but is conditional on unknown Φ that must be estimated in practice. When such estimation is feasible, the reduction of dimension from p to K should make these SR projections easier to work with than the original predictors.
8

Many approaches to dimension reduction can be understood in context of this linear IR model: PC directions arise as SR projections for the maximum likelihood solution when vi is unspeciﬁed (see, e.g., Cook, 2007) and, following our discussion in A.1, the ﬁrst PLS direction is the SR projection for least-squares ﬁt when vi = yi. A closely related framework is that of factor analysis, wherein one seeks to estimate vi directly rather than project xi into its lower dimensional space. By augmenting estimation with a forward model for yi|vi researchers are able to build supervised factor models; see, e.g., West (2003).
The innovation of inverse regression, from Cook’s 2007 paper and in earlier work including Li (1991) and Bura and Cook (2001), is to investigate the SR projections that result from explicit speciﬁcation for vi as a function of yi. Cook’s principle ﬁtted components are derived for a variety of functional expansions of yi, Li et al. (2007) interprets PLS within an IR framework, and the sliced inverse regression of Li (1991) deﬁnes vi as a step-function expansion of yi. Since in each case the vi are conditioned upon, these IR models are more restrictive than the random joint forward-inverse speciﬁcation of supervised factor models. But if the IR model assumptions are satisﬁed then its parsimony should lead to more efﬁcient inference.
Instead of a linear equation, dimension reduction for text data is based on multinomial models. Following the topic model factor speciﬁcation, LDA is akin to PC analysis for multinomials and sLDA is the corresponding supervised factor model. However, existing work on non-Gaussian inverse regression relies on conditional independence; for example, Cook and Li (2009) use single-parameter exponential families to model each xij|vi. To our knowledge, noone has investigated SR projections based on the multinomial predictor distributions that arise naturally for text data. Hence, we seek to build a multinomial inverse regression framework.

3 Modeling

The subject-speciﬁc multinomial inverse regression model has, for i = 1, . . . , n:

xi ∼ MN(qi, mi) with qij =

eηij p eηil , j = 1, . . . , p, where ηij = αj + uij + viϕj. (5)
l=1

This generalizes (1) with the introduction of K-dimensional response factors vi and subject effects ui = [ui1 · · · uip] . Section 3.1 derives sufﬁcient reduction results for projections zimi =

9

Φ xi, where Φ = [ϕ1, · · · ϕp]. Section 3.2 then describes application of these results in text analysis and outlines situations where (5) can be replaced with a collapsed model as in (1). Finally, 3.3 presents prior speciﬁcation for these very high dimensional regressions.

3.1 Sufﬁcient reduction in multinomial inverse regression

This section establishes classical sufﬁciency-for-y (conditional on IR parameters) for projections derived from the model in (5). The main result follows, due to use of a logit link on ηi = [ηi1 · · · ηip] , from factorization of the multinomial’s natural exponential family parametrization.

PROPOSITION 3.1. Under the model in (5), conditional on mi and ui yi ⊥⊥ xi | vi ⇒ yi ⊥⊥ xi | Φ xi.

Proof. Setting αij = αj + uij and suppressing i, the likelihood is

m x

exp [x η − A(η)] =

mx ex α exp [(x Φ)v − A(η)] = h(x)g(Φ x, v), where A(η) = m log pj=1 eηj . Hence, the

usual sufﬁciency factorization (e.g., Schervish, 1995, 2.21) implies p(x|Φ x, v) = p(x|Φ x),

and v is independent of x given Φ x. Finally, p(y|x, Φ x) = vp(y|v)dP(v|Φ x) = p(y|Φ x).

Second, it is standard in text analysis to control for document size by regressing yi onto frequencies rather than counts. Fortunately, our sufﬁcient reductions survive this transformation.
PROPOSITION 3.2. If yi ⊥⊥ xi | Φ xi, mi and p(y | xi) = p(yi | fi), then yi ⊥⊥ xi | zi = Φ fi.
Proof. We have that each of f and [Φ f , m] are sufﬁcient for y in p(x|y) = MN(q, m)p(m|y). Under conditions of Lehmann and Sheffe´ (1950, 6.3), there exists a minimal sufﬁcient statistic T (x) and functions g and g˜ such that g(f ) = T (x) = g˜(Φ f , m). Having g˜ vary with m, while g(f ) does not, implies that the map Φ f has introduced such dependence. But since m cannot be recovered from f , this must be false. Thus g˜ = g˜(Φ f ), and z = Φ f is sufﬁcient for y.

3.2 MNIR for sentiment in text: collapsibility and random effects
For text-sentiment response factor speciﬁcation, we focus on untransformed vi = yi and discretized vi = step(yi) along with their analogues for multivariate sentiment. The former is appropriate for categorical sentiment (e.g., political party, or 1-5 star rating) and, for reasons

10

discussed below, the latter is used with continuous sentiment (e.g., vote-share is rounded to the nearest whole percentage, and in general one can bin and average y by quantiles). Regardless, our methods apply under generic v(yi) including, e.g., the expansions of Cook (2007).
Given this setting of discrete vi, MNIR estimation can often be based on the collapsed counts that arise by aggregating within factor level combinations. For example, since sums of multinomials with equal probabilities are also multinomial, given shared intercepts (i.e., uij = 0) and writing the support of vi as V, the likelihood for the model in (5) is exactly the same as that from, for v ∈ V with xv = i:vi=v xi and mv = i:vi=v mi,

eηvj

xv ∼ MN(qv, mv), where qvj = p eηvl and ηvj = αj + vϕj.

(6)

l=1

Since pooling documents in this way leaves only as many ‘observations’ as there are levels in the support of vi, it can lead to dramatically less expensive estimation.
Under the marginal model of (6), Φ is the population average effect of v on x. One needs to be careful in when and how estimates from this model are used in SR projection, since conditional document-level validity of these results is subject to the usual collapsibility requirements for analysis of categorical data (e.g., Bishop et al., 1975). In particular, omitted variables must be conditionally independent of xi given vi; this can usually be assumed for sentiment-related variables (e.g., a congress person’s voting record is ignored given their party and vote-share). Covariates that act on xi independent of vi should be included in MNIR, as part of the equation for subject effects ui (e.g., although it is not considered in this article, it might be best to condition on geography when regressing political speech onto partisanship). The sufﬁcient reduction result of (3.1) is then conditional on these sentiment-independent variables, such that they (or their SR projection) may need to be used as inputs in forward regression.
It is often unreasonable to assume that known factors account for all variation across documents, and treating the ui of (5) as random effects independent of vi provides a mechanism for explaining such heterogeneity and understanding its effect on estimation. Omitting ui ⊥⊥ vi tends to yield estimated Φ that is attenuated from its correct document-speciﬁc analogue (Gail et al., 1984), although the population-average estimators can be reliable in some settings; for example, Zeger et al. (1985) show consistency for the stationary distribution effect of covari-

11

ates when the ui encode temporal dependence (such as that between consecutive tokens in an N -gram text model). When their inﬂuence is considered negligible, it is common to simply ignore the random effects in estimation. In this article we also consider modeling euij as independent gamma random variables, and use this to motivate a prior in 3.3 for the marginal random effects in a collapsed table. Another option would be to incorporate latent topics into MNIR and parametrize ui through a linear factor model; this is especially appealing since SR projections onto estimated factor scores could then be used in forward regression.
This last point – on random effects and forward regression – is important: when Φ is estimated with random effects, Section 3.1 only establishes sufﬁciency of zi conditional on ui. Marginal sufﬁciency would follow from p(vi|ui, Φ xi) = p(vi|Φ xi), which for ui ⊥⊥ vi requires ui ⊥⊥ Φ xi. Thus, information about vi from this marginal dependence is lost when (as is usually necessary) ui is omitted in regression of vi onto zi. Section 5 shows that random effects in MNIR can be beneﬁcial even if they are then ignored in forward regression. However, SR projection onto parametric representations of ui is an open research interest.
It is clear that there are many relevant issues to consider when assessing an MNIR model, and it is helpful to have our sentiment regression problem placed within the well studied framework of contingency table analysis (e.g., Agresti, 2002, is a general reference). Ongoing work centers on inference according to speciﬁc dependence structures or random effect parametrizations. However, as illustrated in Section 5, even very simple MNIR models – measuring population average effects – allow SR projections that are powerful tools for forward prediction.
3.3 Prior speciﬁcation
To complete the MNIR model, we provide prior distributions for the intercepts α, loadings Φ, and possible random effects U = [uv1 · · · uvd] , where d is the number of points in V.
First, each phrase intercept is assigned an independent standard normal prior, αj ∼ N(0, 1). This serves to identify the logistic multinomial model, such that there is no need to specify a null category, and we have found it diffuse enough to accomodate category frequencies in a variety of text and non-text examples. Second, we propose independent Laplace priors for each ϕjk, with coefﬁcient-speciﬁc precision (or ‘penalty’) parameters λjk, such that π(ϕjk) = λjk/2 exp(−λjk|ϕjk|) for j = 1 . . . p and k = 1 . . . K. The implied prior stan-
12

√ dard deviation for ϕjk is 2/λjk. Each λjk is then assigned a conjugate gamma hyperprior Ga(λjk; s, r) = rs/Γ(s)λsjk−1e−rλjk, yielding the joint gamma-Laplace prior introduced in (3). Hyperprior shape, s, and rate, r, imply expectation s/r and variance s/r2 for each λjk.
As an example speciﬁcation, consider variation in empirical token probabilities by level of the logical variables ‘party = republican’ for congressional speech and ‘rating > 3’ for we8there reviews. Standard deviation of ﬁnite log(qˆtrue,j/qˆfalse,j) across tokens is 1.9 and 1.4 respectively, and given variables normalized to have var(v) = 1 these deviations in log-odds correspond to a jump of two in v (from approximately -1 to 1). Hence, a coefﬁcient standard deviation of around 0.7, implying E[λjk] = 2, is at the conservative (heavy penalization) end of the range indicated by informal data exploration, recommending the exponential Ga(1, 1/2) as a penalty prior speciﬁcation. In Section 5 we also consider shapes of 1/10 and 1/100, thus decreasing E[λjk] by two orders of magnitude, and ﬁnd performance robust to these changes.
The above models have, with s ≤ 1, hyperprior densities for ϕjk that are increasing as the penalty approaches zero (i.e., at MLE estimation). This strategy has performed well in many applications, both for text analysis and otherwise, when dimension is not much larger than 103. However, in examples with vocabulary sizes reaching 105 and higher, it is useful to increase both shape and rate for fast convergence and to keep the number of non-zero term loadings manageably small. As an informal practical recipe, if estimated Φ is less sparse than desired and you suspect overﬁt, increase s. Following the discussion in 4.3 on hyperprior variance and algorithm convergence, if the optimization is taking too long or getting stuck in a minor mode, multiply both s and r by a constant to keep E[λjk] unchanged while decreasing var[λjk].
Finally, we use exp[uij] ∼ Ga(1, 1) independent for each i and j as an illustrative random effect model. Considering euij as a multiplier on relative odds, its mode at zero assumes some tokens are inappropriate for a given document, the mean of one centers the model on a shared intercept, and the fat right tail allows for occasional large counts of otherwise rare tokens. Counts are not immediately collapsable in the presence of random effects, but assumptions on the generating process for xi unconditional on mi can be used to build a prior model for their effect on aggregated counts: if each xij is drawn independent from a Poisson Po(eαj+uij+viϕj ) with exp[uij] ∼ Ga(1, 1), and nv = i 1[vi=v], then xvj ∼ Po(eαj+uvj+vϕj ) with exp[uvj] i∼nd Ga(nv, 1). For convenience, we use a log-Normal approximation to the gamma and specify
13

uv,j

∼

N(log

(n

v

)

−

0.5σ

2 v

,

σv2

)

with

σv2

=

log(nv +1)−log(nv).

Note

that

σv2

→

0

as

nv

grows,

leading to static uv,j whose effect is equivalent to multiplying both numerator and denominator

of exp[ηv,j]/ l exp[ηv,l] by a constant. Thus modeling random effects is unnecessary under

our assumed model after aggregating large numbers of observations.

3.3.1 Motivation for independent gamma-Laplace priors
One unique aspect of this article’s approach is the use of independent gamma-Laplace priors for each regression coefﬁcient ϕjk. Part of the speciﬁcation should not be surprising: the Laplace provides, as a scale-mixture of normal densities, a widely used robust alternative to the conjugate normal prior (e.g., Carlin et al., 1992). It also encourages sparsity in Φ through a sharp density spike at ϕjk = 0, and MAP inference with ﬁxed λjk is equivalent to likelihood maximization under an L1-penalty in the lasso estimation and selection procedure of Tibshirani (1996). Similarly, conjugate gamma hyperpriors are a common choice in Bayesian inference for lasso regression (e.g., Park and Casella, 2008).
However, our use of independent precision for each coefﬁcient, rather than a single shared λ, is a departure from standard practice. We feel that this provides a better representation of prior utility, and it avoids the overpenalization that can occur when inferring a single coefﬁcient precision on data with a large proportion of spurious regressors. In their recent work on the Horseshoe prior, Carvalho et al. (2010) illustrate general practical and theoretical advantages of an independent parameter variance speciﬁcation. As detailed in Section 4, our model also yields an estimation procedure, labeled the gamma-lasso, that corresponds to likelihood maximization under a speciﬁc nonconcave penalty; the estimators thus inherit properties deemed desirable by authors in that literature (beginning from Fan and Li, 2001).
Finally, given the common reliance on cross-validation (CV) for lasso penalty selection, it is worth discussing why we choose to do otherwise. First, our independent λjk penalties would require a CV search of impossibly massive dimension. Moreover, CV is just an estimation technique and, like any other, is sensitive to the data sample on which it is applied. As an illustration, Section 5.1 contains an example of CV-selected penalty performing far worse in out-of-sample prediction than those inferred under a wide range of gamma hyperpriors. CV is also not scaleable: repeated training and validation is infeasible on truly large applications (i.e.,

14

when estimating the model once is expensive). That said, one may wish to use CV to choose s or r in the hyperprior; since results are less sensitive to these parameters than they are to a ﬁxed penalty, a small grid of search locations should sufﬁce.

4 Estimation

Following our model speciﬁcation in Section 3, the full posterior distribution of interest is

np

K

p(Φ, α, λ, U | X, V) ∝

q

xij ij

π

(u

ij

)N(α

j

;

0,

σα2

)

GL(ϕjk, λjk)

(7)

i=1 j=0

k=1

where qij = exp[ηij]/

p l=1

exp[ηil]

with

ηij

=

αj

+

uij

+

K k=1

vik ϕj k

and

GL

is

our

gamma-

Laplace joint coefﬁcient-penalty prior Laplace(ϕjk; λjk)Ga(λjk; r, s). We only consider here uij = 0 or uij i∼nd N(0, σi2) for π(uij), although sentiment-independent covariates can also be

included trivially as additional dimensions of vi. Note that ‘i’ denotes an observation, but that

in MNIR this will often be a combination of documents after the aggregation of Section 3.2.

Bayesian analysis of logistic regression typically involves posterior simulation, e.g. through

Gibbs sampling with latent variables (Holmes and Held, 2006) or Metropolis sampling with

posterior-approximating proposals (Rossi et al., 2005). Despite recent work on larger datasets

and sparse signals (e.g., Gramacy and Polson, 2012), our experience is that these methods are

too slow for text analysis applications. Even the more modest goal of posterior maximiza-

tion presents considerable difﬁculty: unlike the usual high-dimension logistic regression exam-

ples, where K is big and p is small, our large response leads to a likelihood that is expensive

to evaluate (due to normalization of each qi) and has a dense information matrix (from 4.2,

∂2 log LHD/∂ϕjk =

n i=1

mi vi2k qij (1

−

qij ),

which

will

not

be

zero

unless

vik

is).

As

a

result,

commonly used path algorithms that solve over a grid of shared λ values (e.g., Friedman et al.,

2010, as implemented in glmnet for R) do not work even for the small examples of this article.

We are thus motivated to develop super efﬁcient estimation for sparse logistic regression.

The independent gamma-Laplace priors of Section 3.3 are the ﬁrst crucial aspect of our ap-

proach: it remains necessary to choose hyperprior s and r, but results are robust enough to

misspeciﬁcation that basic defaults can be applied. Section 4.1 derives the gamma-lasso (GL)

15

non-concave penalty that results from MAP estimation under this prior. Second, Section 4.2 describes a coordinate descent algorithm for fast negative log posterior minimization wherein the GL penalties are incorporated at no extra cost over standard lasso regression. Lastly, 4.3 considers conditions for posterior log concavity and convergence.

4.1 Gamma-lasso penalized regression

Our estimation framework relies upon recognition that optimal λjk can always be written as a function of ϕjk, and thus does not need to be explicitly solved for in the joint objective.

PROPOSITION 4.1. MAP estimation for Φ and λ under the independent gamma-Laplace prior model in (7) is equivalent to minimization of the negative log likelihood for Φ subject to costs

pK

c(Φ) =

c(ϕjk), where c(ϕjk) = s log(1 + |ϕjk|/r)

(8)

j=1 k=1

Proof. Under conjugate gamma priors, the conditional posterior mode for each λjk given ϕjk is available as λ(ϕjk) = s/(r + |ϕjk|). Any joint maximizing solution [Φˆ , λˆ] for (7) will thus consist of λˆjk = λ(ϕˆjk); otherwise, it is always possible to increase the posterior by replacing λˆjk. Taking the negative log of (3) and removing constant terms, the inﬂuence of a GL(λjk, ϕjk) prior on the negative log posterior is −s log(λjk)+(r +|ϕjk|)λjk, which becomes −s log [(s/r)/(1 + |ϕjk|/r)] + s ∝ s log(1 + |ϕjk|/r) after replacing λjk with λ(ϕjk).

The implied penalty function is drawn in the left panel of Figure 2. Given its shape – everywhere concave with a sharp spike at zero – our gamma-lasso estimation ﬁts within the general framework of nonconcave penalized likelihood maximization as outlined in Fan and Li (2001) and studied in many papers since. In particular, c(ϕjk) can be seen as a reparametrization of the ‘log-penalty’ described in Mazunder et al. (2011, eq. 10), which is itself introduced in Friedman (2008) as a generalization of the elastic net. Viewing estimation from the perspective of this literature is informative. Like the standard lasso, singularity at zero in c(ϕjk) causes some coefﬁcients to be set to zero. However, unlike the lasso, the gamma-lasso has gradient c (ϕjk) = sign(ϕjk)s/(r + |ϕjk|) which disappears as |ϕjk| → ∞, leading to the property of unbiasedness for large coefﬁcients listed by Fan and Li (2001) and referred to as Bayesian ro-

16

bustness by Carvalho et al. (2010). Other results from this literature apply directly; for example, in most problems it should be possible to choose s and r to satisfy requirements for the strong oracle property of Fan and Peng (2004) conditional on their various likelihood conditions.
It is important to emphasize that, despite sharing properties with cost functions that are purpose-built to satisfy particular notions of optimality, c(ϕjk) occurs simply as a consequence of proper priors in a principled Bayesian model speciﬁcation. To illustrate the effect of this penalty, Figure 1 shows MAP coefﬁcients for a simple logistic regression under changes to data and parameterization. In each case, gamma-lasso estimates threshold to zero before jumping to solution paths that converge to the MLE with increasing evidence. Figure 2 illustrates how these solution discontinuities arise due to concavity in the minimization objective, an issue that is discussed in detail in Section 4.3. Note that although the univariate lasso thresholds at larger values than the gamma-lasso, in practice we often observe greater sparsity under GL penalties since large signals are less biased and single coefﬁcients are allowed to account for the effect of multiple correlated inputs. In contrast, standard lasso estimates also ﬁx some estimates at zero but lead to continuous solution paths that never converge to the MLE.

4.2 Negative log posterior minimization by coordinate descent

Taking negative log and removing constant factors, maximization equates with minimization of l(Φ, α, U) + pj=1(αj/σα)2 − log π(U) + c(Φ), where l is the strictly convex

n

p

l(Φ, α, U) = − xi(α + Φ vi + ui) − mi log

exp(αj + ϕj vi + uij) . (9)

i=1

j=1

Full parameter-set moves for this problem are prohibitively expensive in high-dimension due to (typically dense) Hessian storage requirements. Hence, feasible algorithms make use of coordinate descent (CD), wherein the optimization cycles through updates for each parameter conditional on current estimates for all other parameters (e.g., Luenberger and Ye, 2008). Although conditional minima for logistic regression are not available in closed-form, one can bound the CD objectives with an easily solvable function and optimize that instead. In such bound-optimization (also known as majorization; Lange et al., 2000) for, say, l(θ), each move θt−1 → θt proceeds by setting new θt as the minimizing argument to bound b(θ), where b is such

17

phi −4 −2 0 2 4 −4 −2 0 2 4 −4 −2 0 2 4

s = 0.1, r = 1/2

s = 0.5, r = 1/2

s = 1, r = 1/2

−3 −1 0 1 2 3

−3 −1 0 1 2 3

−3 −1 0 1 2 3

x'v
Figure 1: Maximizing solutions for univariate logistic regression log posteriors L(ϕ) = x vϕ − i log [1 + eϕvi] − pen(ϕ), given v = [−1, −1, 1, 1] . The dotted line is the MLE, with pen(ϕ) = 0,
the dashed line is lasso pen(ϕ) = s|ϕ|/r, and the solid line is gamma-lasso pen(ϕ) = s log(1 + |ϕ|/r).

x'v = 1.6

x'v = 2

−log[ LHD ] + penalty 2.0 2.2 2.4 2.6 2.8

−log[ LHD ] + penalty 2.75 2.80 2.85 2.90

2

1

0

s=1, r=.5 s=1.5, r=.75

−4 −2 0

2

4

phi

0.0 0.5 1.0 1.5 phi

0

1

2

3

phi

Figure 2: The left panel shows gamma-lasso penalty s log(1 + |ϕ|/r) for [s, r] of [1, 1/2] (solid) and [3/2, 3/4] (dashed). The right two plots show the corresponding minimization objectives, negative log likelihood plus GL penalty, near a solution discontinuity in the simple logistic regression of Figure 1.

penalty

0.04

0.4

0.2

0.02

L( phi ) − solved objective 0.00 0.10 0.20 0.30

0.0

0.00

q
−0.15 −0.05 0.05 phi [ chicken wing ]

q

1.35

1.45

1.55

phi [ first date ]

q

−0.10 0.00

0.10

phi [ ate here ]

Figure 3: Coordinate objective functions at convergence in regression of we8there reviews onto overall rating. Solid lines are the true negative log likelihood and dashed lines are bound functions with δ = 0.1. Both are shown for new ϕj as a difference over the minimum at estimated ϕj (marked with a dot).

18

that previous estimate θt−1 minimizes b(θ) − l(θ). Algorithm monotonicity is then guaranteed through the inequality l(θt) = b(θt) + l(θt) − b(θt) ≤ b(θt−1) − [b(θt−1) − l(θt−1)] = l(θt−1).
Using θ to denote a new value for a parameter currently estimated at θ, a quadratic bound for each element of (9) conditional on all others is available through Taylor expansion as

b(θ ) = l(Φ, α, U) + gl(θ)(θ − θ) + 1 (θ − θ)2Hθ

(10)

2

where gl(θ) = ∂l/∂θ is the current coordinate gradient and Hθ is an upper bound on curvature at the updated estimate, hl(θ ) = ∂2l/∂θ 2. Quadratic bounding is also used in the logistic

regression CD algorithms of Krishnapuram et al. (2005) and Madigan et al. (2005): the former

makes use of a loose static bound on hl, while the latter updates Hθ after each iteration to

obtain tighter bounding in a constrained trust-region {θ ∈ θ ± δ} for speciﬁed δ > 0. We have

found that dynamic trust region bounding can lead to an order-of-magnitude fewer iterations,

and Appendix A.2 derives Hθ as the least upper bound on hl(θ ) for θ within δ of θ.

In implementing this approach, coordinate-wise gradient and curvature for ϕjk are

∂l

n

∂2l

n

gl(ϕjk) = ∂ϕjk = − i=1 vik(xij − miqij) and hl(ϕjk) = ∂ϕ2jk = i=1 mivi2kqij(1 − qij), (11)

and similar functions hold for random effects and intercepts but with covariates of one and without summing over i for random effects. Then under normal, say N(µθ, σθ2), priors for θ = uij or αi, the negative log posterior bound is B(θ ) = b(θ ) + 0.5(θ − µθ)2/σθ2 which is minimized in {θ ± δ} at θ = θ − sgn(∆θ)min{|∆θ|, δ} with ∆θ = [gl(θ) + (θ − µθ)/σθ2] / [Hθ + 1/σθ2].
Although the GL penalty on ϕjk is concave and lacks a derivative at zero, coordinate-wise updates are still available in closed form. Suppressing the jk subscript, each coefﬁcient update under GL penalty requires minimization of B(ϕ ) = gl(ϕ)(ϕ −ϕ)+ 21 (ϕ −ϕ)2Hϕ +s log(1+ |ϕ |/r) within the trust region {ϕ ∈ ϕ ± δ : sgn(ϕ ) = sgn(ϕ)}. This is achieved by ﬁnding the roots of B (ϕ ) = 0 and, when necessary, comparing to the bound evaluated at zero where B is undeﬁned. Setting B (ϕ ) = 0 yields the quadratic equation

ϕ 2 + (sgn(ϕ)r − ϕ˜) ϕ + s − sgn(ϕ)rϕ˜ = 0

(12)

Hϕ

19

with characteristic (sgn(ϕ)r + ϕ˜)2 − 4s/Hϕ, where ϕ˜ = ϕ − gl(ϕ)/Hϕ would be the updated coordinate for an MLE estimator. From standard techniques, for {ϕ : sgn(ϕ) = sgn(ϕ )} this function will have at most one real minimizing root – that is, with Hϕ > s/ (r + |ϕ |)2. Hence, each coordinate update is to ﬁnd this root (if it exists) and compare B(ϕ ) to B(0). The minimizing value (0 or possible root ϕ ) dictates our parameter move ∆ϕ, and this move is truncated at sgn(∆ϕ)δ if it exceeds the trust region. Finally, when ϕ = 0, repeat this procedure for both sgn(ϕ) = ±1; at most one direction will lead to a nonzero solution.
As it is inexpensive to characterize roots for B (ϕ ), the gamma-lasso does not lead to any noticeable increase in computation time over standard lasso algorithms (e.g., Madigan et al., 2005). Crucially, tests for decreased objective can performed on the bound function, instead of the full negative log posterior. Figure 3 shows objective and bound functions around the converged solution for three phrase loadings from regression of we8there reviews onto overall rating. With δ = 0.1, B provides tight bounding throughout this neighborhood. Behavior around the origin is most interesting: the solution for chicken wing, a low-loading negative term, is at B (ϕ ) = 0 just left of the singularity at zero, while ate here falls in the sharp point at zero. The neighborhood around ﬁrst date, a high-loading term, is everywhere smooth.
4.3 Posterior log concavity and algorithm convergence
Since the gamma-lasso penalty is everywhere concave, our minimization objective is not guaranteed to be convex. This is illustrated by the right two plots of Figure 2, where a very lowinformation likelihood (four observations) can be combined with a relatively diffuse prior on λ (s = 1, r = 1/2) to yield concavity near zero. The effect of this is benign when the gradient is the same direction on either side of the origin (as in the right panel of 2), but in other cases it will lead to local minima at zero away from the true global solution (as in the center panel). Such non-convexity is the cause of the discontinuities in the solution paths of Figure 1.
From the second derivative of l(ϕjk) + c(ϕjk), the conditional objective for ϕjk will be concave only if hl(ϕjk = 0) < s/r2 – that is, if prior variance on λjk is greater than the negative log likelihood curvature at ϕjk = 0. In our experience, this problem is rare: the likelihood typically overwhelms penalty concavity and real examples behave like those shown in Figure 3. Moreover, although it is possible to show stationary limit points for CD on such
20

nonconvex functions (e.g. Mazunder et al., 2011), we advocate avoiding the issue through prior speciﬁcation. In particular, hyperprior shape and rate can be raised to decrease var(λjk) while keeping E[λjk] unchanged. Although this may require more prior information than desired, it is the amount necessary to have both fast MAP estimation and estimator stability. If you want to use more diffuse priors, you should pay the computational price of marginalization and mean inference (as in, e.g., Gramacy and Polson, 2012).
5 Examples
We now apply our framework to the datasets of Section 1.1. The implemented software is available as the textir package for R, with these examples included as demos. Section 5.1 examines out-of-sample predictive performance, and is followed by individual data analyses.
5.1 A comparison of text regression methods
Our prediction performance study considers three text analyses: both constituent percentage vote-share for G.W. Bush (bushvote) and Republican party membership (gop) regressed onto speech for a member of the 109th US congress, and a user’s overall rating (overall) regressed onto the content of their we8there restaurant review. In each case, we report root mean square error or misclassiﬁcation rate over 100 training and validation iterations. Full results and study details are provided in Appendix A.3, and performance for a subset of models is plotted in Figure 4. Here, we focus on some main comparisons that can be drawn from the study.
MNIR is considered under three different hyperprior speciﬁcations, with rate r = 1/2 and shapes of s = 1/100, 1/10, and 1. Response factors are vi = yi for gop and overall, and vi is set as yi rounded by whole number for bushvote (note that instead setting vi = yi here leads to no discernable improvement). In each case, MNIR is ﬁt for observations binned by factor level. We consider models both with and without independent random effects. As predicted, performance is unaffected by random effects for discrete yi, where we are collapsing together hundreds of observations. However, they do improve out-of-sample performance by approximately 1.5% for bushvote, where only a small number of speakers are binned at each whole percentage point. Hence, detailed MNIR results are reported with random effects included only
21

for bushvote. Finally, resulting SR scores zi = ϕ fi are incorporated into a variety of forward regression models: linear E[yi] = α + βzi and quadratic E[yi] = α + β1zi + β2zi2 for bushvote, logistic E[yi] = exp[α + βzi]/(1 + exp[α + βzi]) for gop, and linear and proportional-odds logistic p(yi ≤ c) = exp[αc − βzi]/(1 + exp[αc − βzi]), c = 1 . . . 5, for overall.
Performance is very robust to changes in the MNIR hyperprior. Figure 4 shows little difference between otherwise equivalent models using the conservative default s = 1 and the lowest expected penalty s = 1/100; results for s = 1/10 are squeezed in-between. In congressional speech examples s = 1/100 has a slight edge; phrases here have already been pre-selected for partisanship and are thus largely relevant to the sentiment. On the other hand, s = 1 is the best performing shape for the we8there example, where phrases were only ﬁltered by a minimum document threshold. Looking at forward regressions, the problem speciﬁc quadratic bushvote (see Section 5.2 for justiﬁcation) and proportional odds overall (accounting for ordinal response) forward regressions provide lower average out-of-sample error rates at the price of slightly higher variability across iterations, when compared to simple linear forward regression.
As comparators, we consider text-speciﬁc LDA (both supervised and standard topic models) as well as an assortment of generic regression techniques: lasso penalized linear (bushvote and overall) and binary logistic (gop) regression, with penalty either optimized under our gamma hyperpriors (gop), marginalized in MCMC (bushvote), or tuned through CV (all examples); ﬁrst-direction PLS (bushvote and overall); and support vector machines (gop). In every comparison, gamma-lasso MNIR provides higher quality predictions with lower runtimes. The only similar predictive performance was for LDA with 25 and 50 topics in the bushvote example, at 15-50 times higher computational cost. Note that, given the size of real text analysis applications, we view the speed and scaleability of MNIR as a primary strength and only considered feasible alternatives, with short Gibbs runs for 50 topic sLDA and the Bayesian lasso (7-9 min) at the very high end of our runtimes. Moreover, both sLDA and CV lasso occasionally fail to converge (these runs were excluded); this never happened for MNIR.
Among comparators, the multinomial topic models outperform generic alternatives. Interestingly, LDA combined with simple regression outperforms sLDA in both congress examples. Again, this is probably due to pre-selection of phrases: topics are relevant to ideology regardless of supervision, and the extra parameters in sLDA are not worth their cost in degrees of
22

freedom. Moreover, the simpler LDA models can be ﬁt with the MAP estimation of Taddy (2012b), whereas sLDA is applied here through a slow-to-converge Gibbs sampler (we note that the original sLDA paper uses a variational EM algorithm). However, in the we8there data, the extra machinery of sLDA offers a clear improvement over unsupervised LDA, as should be the case in many text applications. Finally, in an important side comparison, binary logistic regressions were ﬁt for gop regressed onto phrase frequencies using both CV and independent gamma hyperpriors for the lasso penalty. The scaleable, low-cost, gamma-lasso yields large performance improvements over a CV optimized model, regardless of hyperprior speciﬁcation.
5.2 Application: partisanship and ideology in political speeches
For the data of Section 1.1.1, we have two sentiment metrics of interest: an indicator for party membership, and each speaker’s constituent vote-share for Bush in 2004. Since the two independents caucused with Democrats, the former metric can be summarized in gop as a two-party partisanship. Following the political economy notion that there should be little discrepancy between voter and representative beliefs, bushvote provides a measure of ideology as expressed in support for G.W. Bush (and lack of support for John Kerry) in the context of that election.
Figure 5 shows MNIR ﬁt in separate models for each of gop and bushvote, as studied in Section 5.1. For partisanship, ﬁt with s = 1/100 and r = 1/2, a simple univariate logistic forward regression yields clear discrimination between parties; 8.5% (45 speakers) are misclassiﬁed under a maximum probability rule. In the bushvote MNIR, ﬁt under the same hyperprior but with inclusion of random effects, the resulting SR scores zi = ϕ fi increase quickly with vote-share at low (mostly Democrat) values and more slowly for high (mostly Republican) values. This motivates our quadratic forward regression for bushvote onto SR score, the predictive mean of which is plotted in Figure 5 (with R2 of 0.5). However, looking at the SR scores colored by party (red for Republicans, blue Democrats, green independents) shows that this curvature could instead be explained through different forward regression slopes by level of gop, implying that the relationship between language and ideology is party-dependent.
Given the above, a more useful model might consider text reduction that allows interaction between party and ideology. For example, we can build orthogonal bivariate sentiment factors as gop and bushvote minus the gop-level means, say votediff (again, rounded to the nearest
23

109th Congress Vote−Shares

RMSE (on % of vote)

13

12

qq

qq

q

11

q q

10

q

qq

q

mnir1mQnir3Qmnir1lda50mnir3lda25lda10 ldas5lda10slda5

time (in seconds)

200 60

q q
qqq

15

q

qq

5q

qqq

q q

qqq

qq

q

1 mnmir3nir3mQnir1Qmnir1 lda5lda10lda25slda5lda5s0lda10

109th Congress Party Membership

Misclassification %

40

30

20 q

q

q

10

q q q qq
q q
qq q

time (in seconds)

120 30 5 1 1/4

qqqq q q q
q qq
q q

mnir1 mnir3 lda1la0sso100 slda10 lassol5assoCV

mnir1 mnilra3sso100 lassol5assoCV lda10 slda10

we8there Overall Ratings

RMSE (on overall rating)

1.30

q q

1.25

1.20

q q

1.15

qqq

q

q

qq

1.10 q

1.05

qq q

mnir3mpnoir1pomnir1mnir3sldas5lda1s0lda25 lda5lda10lda25

time (in seconds)

240 60
10

q q
q qq
q

1

1/4

q

q

qq q

mnir3mnmirn1ir3mpnoir1po lda5lda10lda25sldas5lda1s0lda25

Figure 4: Out-of-sample performance and run-times for select models. For MNIR, ‘Q’ indicates quadratic and ‘po’ proportional-odds logistic forward regressions, while λj prior ‘1’ is Ga(0.01, 0.5) and ‘3’ is Ga(1, 0.5). We annotate with the number of topics for (s)LDA, and for binary Lasso regressions with either CV or the rate in an exponential penalty prior. Full details are in the appendix.

24

whole percentage). Figure 6 shows ﬁtted values for such a model, including random effects and with hyperprior shape increased to s = 1/10 to reﬂect a preference for smaller conditional coefﬁcients. In detail, with zgop and zvotediﬀ the two dimensions of SR scores from MNIR x ∼ MN(q(vgop, vvotediﬀ), m), normalized for ease of interpretation, the ﬁtted forward model is

E[bushvote] = 51.9 + 6.2zgop + 5.2zvotediﬀ − 1.9zgopzvotediﬀ .

(13)

Thus a standard deviation increase in either SR direction implies a 5-6% increase in expected vote-share, and each effect is dampened when the normalized SR scores have the same sign.
The right panel of Figure 6 shows ﬁtted expected counts qjm against true nonzero counts in our bivariate MNIR model ﬁt; with random effects to account for model misspeciﬁcation, there appears to be no pattern of overdispersion. The only clear outlier in forward regression is Chaka Fattah (D-PA) with a standardized residual of -5.2; he uttered a token in our sample only twice: once each for rate.return and billion.dollar. Finally, Figure 7 plots response factor loadings for a select group of tokens. Among other lessons, we see that racial identity rhetoric (african.american.latino, black.caucu) points towards the left wing of the Democratic party, while discussion of hate crimes is indicative of a moderate Republican. A few large loadings are driven by single observations: for example, violent.sexual.predator contributes more than 0.1% of speech for only Byron Dorgan, a Democratic Senator in Bush-supporting North Dakota. However, this is not the rule and most term loadings affect many speakers.

5.3 Application: on-line restaurant reviews
For the data of Section 1.1.2, our sentiment consists of ﬁve correlated restaurant ratings (each on a ﬁve point scale) that accompany every review. The left panel of Figure 8 shows MNIR for review content regressed onto the single overall response factor, as studied in Section 5.1. The true overall rating has high correlation (0.7) with our SR scores, despite considerable overlap between scores across rating levels. The right plot of Figure 8 shows probabilities for each increasing overall rating category, as estimated in the proportional-odds logistic forward regression, p(overall ≤ c) = exp[αc − βzoverall]/(1 + exp[αc − βzoverall]). Again, zoverall is normalized here to have mean zero and standard deviation of one in our sample. This model has β = 2.3,

25

Probability of a Republican 0.0 0.2 0.4 0.6 0.8 1.0

Vote Share 10 30 50 70

qq q

qq

qq

qq

qq

qq

q qqqqqq

qq

q

qqq

qqqq

qqqq

qq q

q

qq

qqq

q q

qq

Democrat/Indep. Republican

−2 −1 0 1 2 3

Party

normalized SR Score

Figure 5: Separate MNIR ﬁts for congressional speech onto each of party and vote-share. The right shows probabilities that each speaker is Republican and the left shows SR scores against bushvote.

Vote Share 10 30 50 70

qq

fitted count

100 10 1 0.1
0.01 0.001

qq q

q q q qq

q

q

qq

q

q

q qqq qqqq q

qq

qq

q

q

q q

q

qq

q

q

qq

qq

q

qq

q q

qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq

qq

qq q qq

qqq

qq q

qqq

q qq

q qq

qqqq

q qqqq

q qq

qq qq

qqqqqq

qqqq q

qqqq q

q qqq

qqqqqqqqq

q

q

q

qqqqqq

qqqqqq

qqqq q

qqqq

qqqq qq

qqqqqq qqqqqq qqqq qqqqq q q q

qq

qqqqq

qqqqq q

qqqqq q

qqqqqqq qqq qq

qqqqq q

qqqqqqq

qqq

q

qqqqq

qqq

qqq

q

20 30 40 50 60 70

1 2 4 8 17 41 98 266

forward regression fitted values

observed count

Figure 6: Bivariate ideology and partisanship MNIR. The left plot shows ﬁtted values for a forward regression that interacts SR scores, and the right shows ﬁtted vs observed token counts in MNIR.

votediff Loadings −2 −1 0 1 2

violent.sexual.predator warren.buffett

tax.haven democrat.white.house

jefferson.county

deep.sea.coral

illegal.alien death.tax.repeal

massive.tax.cuntational.wildlife property.right

medic.liability.reform

illegal.immigranntear.earth.object

african.american.latino black.caucu

marriage.protection.amendment commonly.prescribed.drug

hate.crime.law change.heart.mind

−1

0

1

2

3

4

gop Loadings
Figure 7: Select congressional speech term loadings in bivariate MNIR with party and vote-share.
26

normalized SR Score −4 −2 0 2 4
Probability of Rating 0.0 0.2 0.4 0.6 0.8 1.0

1

2

3

4

5

Overall Rating

−2

−1

0

1

normalized SR Score

Figure 8: Sufﬁcient reduction and forward model ﬁt for inverse regression of we8there reviews onto the corresponding overall rating. The left plot shows SR score by true review rating, and the right shows proportional-odds logistic regression probabilities for each rating-level as a function of these SR scores.

Overall
plan.return feel.welcom
best.meal select.includ finest.restaur steak.chicken
love.restaur ask.waitress good.work can.enough
after.left come.close open.lunch warm.friend spoke.manag definit.recommend expect.wait great.time chicken.beef room.dessert price.great
seafood.restaur friend.atmospher
sent.back ll.definit anyon.look most.popular order.wrong
delici.food fresh.seafood

Food
again.again mouth.water
francisco.bay high.recomend
cannot.wait best.servic kept.secret food.poison outstand.servic
far.best food.awesom
best.kept everyth.menu
excel.price keep.come hot.fresh best.mexican best.sushi pizza.best
food.fabul melt.mouth each.dish absolut.wonder
foie.gras menu.chang
food.bland noth.fanci back.time food.excel worth.trip

Service
cozi.atmospher
servic.terribl servic.impecc
attent.staff time.favorit servic.outstand servic.horribl dessert.great
terribl.servic never.came experi.wonder time.took waitress.come servic.except final.came new.favorit servic.awesom sever.minut
best.dine veri.rude peopl.veri poor.servic
ask.check real.treat never.got
non.exist flag.down
tabl.ask
least.minut won.disappoint

Value
big.portion around.world
chicken.pork
perfect.place place.visit mahi.mahi veri.reason babi.back low.price peanut.sauc
wonder.time garlic.sauc great.can
absolut.best place.best year.alway over.price dish.well few.place authent.mexican wether.com especi.good
like.sit open.until great.too
open.daili
best.valu just.great
fri.littl portion.huge

Atmosphere
walk.down
great.bar atmospher.wonder
dark.wood food.superb atmospher.great
alway.go bleu.chees realli.cool recommend.everyon great.atmospher
wonder.restaur love.atmospher
bar.just expos.brick back.drink
fri.noth great.view chicken.good bar.great person.favorit great.decor french.dip pub.food coconut.shrimp
go.up
servic.fantast gas.station pork.loin place.friend

Figure 9: High-loading phrases in each direction for regression of we8there reviews onto aspect ratings. Green tokens are positive, black are negative, and size is proportional to the absolute value of the loading.

27

implying that the odds of being at or above any given rating level are multiplied by e2.3 ≈ 10 for every standard deviation increase in the SR score.
Looking to explore aspect-speciﬁc factors, Figure 9 shows top-30 absolute value loadings in MNIR for review token-counts onto all ﬁve dimensions of sentiment. Inﬂuential terms on either side of the rating spectrum can be easily connected with elements of a good or bad meal: plan.return, best.meal, and big.portion are good, while sent.back, servic.terribl, and food.bland are bad. The largest loadings appear to be onto overall and food aspects, with service slightly less important and loadings for value and atmosphere quickly decreasing in size. This would indicate that the reviews focus on these elements in that order.
6 Discussion
The promising results of Section 5 reinforce a basic idea: a workable inverse speciﬁcation can introduce information that leads to more efﬁcient estimation. Given the multinomial model as a natural inverse distribution for token-counts, analysis of sentiment in text presents an ideal setting for inverse regression. While the approach of not jointly modeling a corresponding forward regression falls short of full Bayesian analysis, such inference would signiﬁcantly complicate estimation and detract from our goal of providing a fast default method for supervised document reduction. We are happy to take advantage of parametric hierarchical Bayesian inference for the difﬁcult MNIR estimation problem, and suggest that application appropriate techniques for low-dimensional forward regression should be readily available.
Although the illustrative applications in this article are quite simple, the methods scale to far larger datasets. Collapsing observations across sentiment factors for MNIR yields massive computational gains: training data need only include token counts tabled by sentiment level, and as an example, in Taddy (2012a) this allows MNIR runs of only a few seconds for 1.6 million twitter posts scored as positive or negative. Moreover, we see no reason why gamma-lasso logistic regression, which was developed speciﬁcally for large response settings, should not be viewed as an efﬁcient option in generic penalized regression. Finally, current collaborations that use MNIR for text analysis include study of partisanship in the US congressional record from 1873 to present, and an attempt to quantify the economic content of news in 20 years of
28

Wall Street Journal editions. In each case, we are considering a more rigorous treatment of the identiﬁcation of single sentiment dimensions and controlling for related endogenous variables; this work shows MNIR’s promise as the basis for a variety of text related inference goals.
29

Appendix

A.1 Slant and Partial Least Squares

The GS slant index for document i is zislant =

p j=1

bj (fij

−

aj )/

p j=1

b2j ,

with

parameters

obtained through ordinary least-squares (OLS) as [aj, bj] = arg mina,b ni=1[fij −(a+byi)]2 for

j = 0 . . . p. Since bj = cov(fj, y)/var(y), slant is equivalent (up to a uniform shift and scale for

all index values) to a weighted sum of term frequencies loaded by their covariance with y. This

is also the ﬁrst direction in partial least-squares; see Frank and Friedman (1993) for statistical

properties of PLS and its relationship to OLS, and Hastie et al. (2009) for a common version

of the algorithm. Using the usual normalization applied in PLS, an improved slant measure is

given by zislant =

p j=1

fij

cor(fj

,

yi

).

For

vote-share

regressed

onto

congressional

speech

in

the data of Section 1.1.1, this change increases within-sample R2 from 0.37 to 0.57.

Given Fˆ = [ˆf1 · · · ˆfp] as a normalized covariate matrix with mean-zero and variance-one

columns, a PLS algorithm which highlights its inverse regression structure is as follows.

1. Set the initial response factor v0 = y = [y1 . . . yn] , and for k = 1, . . . , K: - Loadings are ϕk = cor(Fˆ, vk−1) = [cor(ˆf1, vk−1) . . . cor(ˆfp, vk−1)] . - The kth PLS direction is zk = ϕkFˆ. - The new response factors are vk = vk−1 − [zkvk−1/(zkzk)]zk.
2. Set yˆ as OLS ﬁtted values for regression of y onto Z, where Z = [z1 · · · zK].

An extra step to normalize and orthogonalize zk with respect to [z1 · · · zk−1] recovers orthonor-
mal directions, as in the original PLS algorithm. Moreover, loading calculations replaced by ϕkj = arg minϕ ni=1[fij − (a + ϕvki)]2 will only scale zk by the variance of vk and lead to the same forward ﬁt, such that PLS can be viewed as stagewise inverse regression.

A.2 Trust-region bound for logistic multinomial likelihood

The bounding used here is essentially the same as in Genkin et al. (2007) but for introduction

of dependence upon vik that is missing from their version. We describe the bound for updates

to ϕjk, but it applies directly to αj or uij simply by replacing covariate values with one.

Given a trust region of ϕjk ± δ, the upper bound on hl(ϕjk) =

n i=1

vi2k mi qij (1

−

qij )

is

Hjk =

n i=1

vi2k mi /Fij ,

where

each

Fij

is

a

lower

bound

on

1/(qij

−

qi2j )

=

2

+

eηij +δvik /Eij

+

Eij/eηij+δvik , with Eij = pl=1 eηil − eηij . This target is convex in δ with minimum at eδvik =

Eij/eηij , such that



eηij −|vik|δ if Eij < eηij −|vik|δ

F

= eij + Eij + 2 where e

 
= eηij+|vik|δ if E

> eηij +|vik|δ

ij Eij eij

ij

ij



 Eij

otherwise.

We use unique δjk and update δjk = max{δjk/2, 2|ϕjk − ϕjk|} after each iteration.

30

A.3 Out-of-Sample Prediction Study Details
Each model was ﬁt to 100 random data subsets and used to predict on the left-out sample. Tables report average root mean square error (RMSE) or percent misclassiﬁed (MC%), the percentage worse than best on this metric, and run-time in seconds (including count collapsing in MNIR).
We use R package implementations: lda for SLDA (Chang, 2011); glmnet for CV lasso regression (Friedman et al., 2010); monomvn for Bayesian lasso (Gramacy, 2012); kernlab for SVM (Karatzoglou et al., 2004); textir for MNIR, LDA, PLS, and gamma-lasso regression; and arm (Gelman et al., 2012) for the forward regression models that accompany MNIR and LDA. Penalty prior in MNIR is Ga(s, 1/2), (s)LDA Dirichlet precisions are 1/K for topic weights and 1/p for token probabilities, and sLDA assumes a forward error variance of 25% of marginal response variance. Unless otherwise speciﬁed, we apply package defaults. (S)LDA and MNIR use token counts; all others regress onto token frequencies.

Vote Share: Congressional speech with two-party vote share (%) as continuous response, training on 200 and predicting on 329. Constant mean RMSE is 13.4. MNIR models were ﬁt with random effects; models without random effects are an average of 1.5% worse on RMSE but 20% faster. Bayes lasso uses a Ga(2,1/10) prior on λ and was run for 200 MCMC iterations after a burn-in of 100 (refer to monomvn for details).

MNIR & Quadratic

MNIR & Linear

LDA & Linear

Supervised LDA

Lasso

PLS

s = 10−2 10−1 1 s = 10−2 10−1 1 K = 2 5 10 25 50 K = 2 5 10 25 50 CV Bayes K=1

RMSE

10.7 10.7 10.8 10.9 10.9 10.9 11.7 11.3 11.1 10.9 10.9 12.9 12.1 11.7 12.3 15.1 13.7 15.7 15.9

% Worse

0

0

0

1

1

2

9

6

4

2

2

21 13 9 15 41 28 46 49

Run Time 2.2

2.3 2.1

2.2

2.3 2.1 1.2 2.4 6.2 29 112 43 75 128 288 508 0.9 410 0.1

Party Classiﬁcation: Congressional speech data with ‘Republican’ as binary response, training on 200 and predicting on 329. Null model misclassiﬁcation rate is 46%. MNIR models were ﬁt without random effects which lead to the same misclassiﬁcation but 40% longer average run-times. Lasso and gamma-lasso are applied in binary logistic regressions, with shape one and rate r for the latter, and SVM uses Gaussian kernels with misclassiﬁcation cost C (refer to kernlab for details). LDA led to complete separation and SLDA failed to converge for K > 10.

MNIR & Logistic s = 10−2 10−1 1

LDA & Logistic Supervised LDA K = 2 5 10 K = 2 5 10

Lasso

Gamma-Lasso

SVM

CV r = 5 25 50 100 C=1 100 1000

MC%

11

11 12 20 15 15 33 20 18 24 19 17 16 15 37 32 32

% Worse

0

0 2 76 36 30 188 75 54 115 68 49 42 35 224 182 180

Run Time 0.3

0.4 0.3 1.1 2.5 6.3 44 77 126 1.0 0.6 0.5 0.5 0.5 3.1 3.5 3.4

Restaurant Rating: We8there reviews with ordinal rating response, training on 2000 and predicting on 4166. Constant mean RMSE is 1.35. Reported MNIR models were ﬁt without random effects which lead to equivalent predictive performance but 15% longer average run-times.

MNIR & POLR s = 10−2 10−1 1

MNIR & Linear s = 10−2 10−1 1

K =2

LDA & POLR 5 10 25

Supervised LDA 50 K = 2 5 10 25

Lasso PLS 50 CV K = 1

RMSE

1.08 1.08 1.07 1.09 1.09 1.10 1.19 1.17 1.20 1.23 1.23 1.15 1.13 1.14 1.15 1.16 1.24 1.25

% Worse

1

1

0

2

2

2

12 10 12 15 15

8

5 6 7 8 16

17

Run Time 0.6

0.6 0.5

0.3

0.4 0.3 2.5 13.4 28 61 167 53 90 154 341 651 54

2.2

References
Agresti, A. (2002). Categorical Data Analysis (2nd ed.). Wiley.
Bishop, Y., S. Fienberg, and P. Holland (1975). Discrete Multivariate Analysis. MIT Press.
Blei, D. M. and J. D. McAuliffe (2007). Supervised topic models. In Neural Information Processing Systems, Volume 21.
Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent Dirichlet allocation. Journal of Machine Learning Research 3, 993–1022.
Bollen, J., H. Mao, and X.-J. Zeng (2011). Twitter mood predicts the stock market. Journal of Computational Science 2, 1–8.
Bura, E. and D. Cook (2001). Estimating the structural dimension of regressions via parametric inverse regression. Journal of the Royal Statistical Society, Series B 63, 393–410.
Carlin, B. P., N. G. Polson, and D. S. Stoffer (1992). A Monte-Carlo approach to nonnormal and nonlinear state-space modeling. Journal of the American Statistical Association 87, 493–500.
Carvalho, C. M., N. G. Polson, and J. G. Scott (2010). The horseshoe estimator for sparse signals. Biometrika 97, 465–480.
Chang, J. (2011). lda: Collapsed Gibbs sampling methods for topic models. R package version 1.3.1.
Cook, R. D. (2007). Fisher lecture: Dimension reduction in regression. Statistical Science 22, 1–26.
Cook, R. D. and L. Li (2009). Dimension reduction in regressions with exponential family predictors. Journal of Computational and Graphical Statistics 18, 774–791.
Fan, J. and R. Li (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association 96, 1348–1360.
Fan, J. and H. Peng (2004). Nonconcave penalized likelihood with a diverging number of parameters. The Annals of Statistics 32, 928–961.
Frank, I. E. and J. H. Friedman (1993). A statistical view of some chemometrics regression tools. Technometrics 35, 109–135.
Friedman, J. H. (2008). Fast sparse regression and classiﬁcation. Technical Report, Dept. of Statistics, Stanford University.
Friedman, J. H., T. Hastie, and R. Tibshirani (2010). Regularization paths for generalized linear models via coordinate descent.
Gail, M. H., S. Wieand, and S. Piantadosi (1984). Biased estimates of treatment effect in randomized experiments with nonlinear regressions and omitted covariates. Biometrika 71, 341–444.
Gelman, A., Y.-S. Su, M. Yajima, J. Hill, M. G. Pittau, J. Kerman, and T. Zheng (2012). arm: Data Analysis Using Regression and Multilevel/Hierarchical Models. R package version 1.5-03.
Genkin, A., D. D. Lewis, and D. Madigan (2007). Large-scale Bayesian logistic regression for text categorization. Technometrics 49, 291–304.
32

Gentzkow, M. and J. Shapiro (2010). What drives media slant? Evidence from U.S. daily newspapers. Econometrica 78, 35–72.
Gramacy, R. B. (2012). monomvn: Estimation for multivariate normal and Student-t data with monotone missingness. R package version 1.8-10.
Gramacy, R. B. and N. G. Polson (2012). Simulation-based regularized logistic regression. Bayesian Analysis 7, 1–24.
Grimmer, J. (2010). A Bayesian hierarchical topic model for political texts: Measuring expressed agendas in senate press releases. Political Analysis 18, 1–35.
Hastie, T., R. Tibshirani, and J. H. Friedman (2009). The Elements of Statistical Learning. Springer.
Holmes, C. C. and L. Held (2006). Bayesian auxiliary variable models for binary and multinomial regression. Bayesian Analysis 1, 145–168.
Jurafsky, D. and J. H. Martin (2009). Speech and Language Processing (2nd ed.). USA: Prentice Hall.
Karatzoglou, A., A. Smola, K. Hornik, and A. Zeileis (2004). kernlab – an S4 package for kernel methods in R. Journal of Statistical Software 11(9), 1–20.
Krishnapuram, B., L. Carin, M. A. T. Figueiredo, and A. J. Hartemink (2005). Sparse multinomial logistic regression: Fast algorithms and generalization bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence 27, 957–969.
Lange, K., D. R. Hunter, and I. Yang (2000). Optimization transfer using surrogate objective functions. Journal of Computational and Graphical Statistics 9, 1–20.
Laver, M., K. Benoit, and J. Garry (2003). Extracting policy positions from political texts using words as data. American Political Science Review 97, 311–332.
Lehmann, E. L. and H. Sheffe´ (1950). Completeness, similar regions, and unbiased estimation – part 1. Sankhya¯: The Indian Journal of Statistics 10, 305–340.
Li, K. (1991). Sliced inverse regression for dimension reduction. Journal of the American Statistical Association 86, 316–327.
Li, L., R. D. Cook, and C.-L. Tsai (2007). Partial inverse regression. Biometrika 94, 615–625.
Loughran, T. and B. McDonald (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. Journal of Finance 66, 35–65.
Luenberger, D. G. and Y. Ye (2008). Linear and Nonlinear Programming (3rd ed.). Springer.
Madigan, D., A. Genkin, D. D. Lewis, and D. Fradkin (2005). Bayesian multinomial logistic regression for author identiﬁcation. In AIP Conference Proceedings, Volume 803.
Maua´, D. D. and F. G. Cozman (2009). Representing and classifying user reviews. In ENIA ’09: VIII Enconro Nacional de Inteligeˆncia Artiﬁcial, Brazil.
Mazunder, R., J. H. Friedman, and T. Hastie (2011). Sparsenet: Coordinate descent with nonconvex penalties. Journal of the American Statistical Association 106, 1125–1138.
33

Pang, B. and L. Lee (2008). Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval 1-2, 1–135.
Park, T. and G. Casella (2008). The Bayesian lasso. Journal of the American Statistical Association 103, 681–686.
Poon, H. and P. Domingos (2009). Unsupervised semantic parsing. In Proceedings of the Conference on EMNLP.
Porter, M. F. (1980). An algorithm for sufﬁx stripping. Program 14, 130–137.
Quinn, K., B. Monroe, M. Colaresi, M. Crespin, and D. Radev (2010). How to analyze political attention with minimal assumptions and costs. American Journal of Political Science 54, 209–228.
Rossi, P. E., G. M. Allenby, and R. McCulloch (2005). Bayesian Statistics and Marketing. Wiley.
Schervish, M. J. (1995). Theory of Statistics. Springer.
Srivastava, A. N. and M. Sahami (Eds.) (2009). Text Mining: Classiﬁcation, Clustering, and Applications. CRC Press.
Taddy, M. (2012a). Design and analysis of a text mining experiment. arXiv:1206.3776v1.
Taddy, M. (2012b). On estimation and selection for topic models. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2012).
Talley, E. L. and D. O’Kane (2011). The measure of a MAC: A machine-learning protocol for analyzing force majeure clauses in M&A agreements. Journal of Institutional and Theoretical Economics 168, 181–201.
Tetlock, P. (2007). Giving content to investor sentiment: The role of media in the stock market. Journal of Finance 62, 1139–1168.
Thomas, M., B. Pang, and L. Lee (2006). Get out the vote: Determining support or opposition from congressional ﬂoor-debate transcripts. In Emperical Methods in Natural Language Processing.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B 58, 267–288.
West, M. (2003). Bayesian factor regression models in the “large p, small n” paradigm. In Bayesian Statistics, Volume 7. Oxford Univeresity Press.
Wold, H. (1975). Soft modeling by latent variables: The nonlinear iterative partial least squares approach. In Perspectives in Probability and Statistics, Papers in Honour of M.S. Bartlett. Academic Press.
Yu, B., S. Kaufmann, and D. Diermeirer (2008). Classifying party afﬁliation from political speech. Journal of Information Technology and Politics 5, 33–49.
Zeger, S. L., K.-Y. Liang, and S. G. Self (1985). The analysis of binary longitudinal data with timeindependent covariates. Biometrika 72, 31–38.
34

