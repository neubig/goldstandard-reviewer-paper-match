IGA: An Intent-Guided Authoring Assistant

Simeng Sun1∗ Wenlong Zhao1 Varun Manjunatha2 Rajiv Jain2

Vlad Morariu2 Franck Dernoncourt2 Balaji Vasan Srinivasan2 Mohit Iyyer1

University of Massachusetts Amherst1

Adobe Research2

{simengsun,wenlongzhao,miyyer}@cs.umass.edu

{vmanjuna, rajijain, morariu, dernonco, balsrini}@adobe.com

arXiv:2104.07000v2 [cs.CL] 19 Sep 2021

Abstract
While large-scale pretrained language models have signiﬁcantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to ﬁne-grained author speciﬁcations. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to speciﬁc rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We ﬁne-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to ﬁll in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations conﬁrm the quality of IGA’s generated outputs, while a smallscale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing.
1 Introduction
Writing can be both an exhilarating, creative experience as well as a frustrating slog. Can recent advances in neural language modeling help improve the human writing experience? Pretrained Transformer language models (Radford et al., 2019) have improved writing aids such as email “autocomplete” (Chen et al., 2019), while commercial tools such as Grammarly and Microsoft Editor can rewrite full sentences to increase clarity.1
Few existing writing assistants provide support for the underlying cognitive process of writing (Greer et al., 2016). In this paper, we explore more advanced writing assistance functions:
∗Most of the work done during an internship at Adobe. 1As these systems are not open-sourced, it is unclear how exactly they are implemented.

CONTEXT: Sarah didn't come to school today,

CAUSE homework

EFFECT

news

CONTRAST exam

DESCRIPTION <NA>

BIOGRAPHY <NA>

IDIOM

<NA>

REPHRASE <NA>

because she had to get her homework done.
therefore there is no more big news!
but still took the exam and had it done. it was an unplanned break. David, the principal, believes she will miss tomorrow's class. She was busy making ends meet. Sarah did not arrive at school today.

Figure 1: General concept overview of our IntentGuided Authoring assistant IGA. Given context, by specifying different writing intents, the system generates output satisfying the intent. In addition to well-
formed sentence fragments, keywords can also be part of user input, serving as arguments for the intents, and are preserved in the output.

speciﬁcally, we build an authoring assistant capable of following ﬁne-grained user directives (e.g., add descriptive text, use idiomatic language, or paraphrase a clunky bit of wording). Our system, the Intent-Guided Assistant (IGA), combines controllable text generation with text inﬁlling (Zhu et al., 2019; Keskar et al., 2019a; Lewis et al., 2020; Donahue et al., 2020); more speciﬁcally, we adapt the tag-based control of Keskar et al. (2019b) to include a set of rhetorical directives that our model learns to inﬁll with relevant and ﬂuent text. Our system can handle the following authorguided tags: cause, effect, concession (contrast), description, biography, idiom, and rephrase. User input to IGA can be as simple as a list of keywords and does not have to include well-formed text (Figure 1).
We train IGA in supervised fashion by creating a large multi-domain dataset in which spans corresponding to particular directives are replaced with a single tag: for the input “It was raining <description> trees", the ground-

truth completion could be “It was raining , the trees were swaying and the wind was oppressive." To build our dataset, we use heuristics based on lexical and syntactic choice to isolate spans corresponding to each directive. For the above example, we extract the ﬁrst simple declarative clause, then highlight the span that contains words (such as “oppressive") in a large list of adjectives, and ﬁnally extract keywords such as “trees" using a keyword extractor. At inference time, our model can ﬂexibly take any tag as input: given “It was raining <contrast> trees", for example, our model inserts a contrastive clause to produce “It was raining but still the trees were not wet".
To evaluate the effectiveness and usability of our AI-assisted writing paradigm, we design IGA to be interactive, in the spirit of human-AI coauthoring. In addition to automatic and crowdsourced evaluations that demonstrate IGA’s output quality, we perform a user study in which participants make use of our system for creative storytelling (Section 6). Our results show most users prefer writing with assistance from IGA compared to writing from scratch or with a non-controllable inﬁlling model.
Our contributions are as follows:
1. We design IGA, an authoring assistant capable of controlled text generation based on explicit rhetorical directives speciﬁed by the author.
2. To train IGA, we create a large dataset (75M tokens) of text heuristically-labeled with author intent, sourced from multiple repositories. This dataset is made publicly available to facilitate future research on AI-enabled author assistance.2
3. We validate the usefulness of IGA through automatic and crowdsourced evaluations as well as a user study involving creative writing.
2 Related Work
2.1 Theories of writing
Numerous studies within the humanities focus on modeling the process of effective writing (Flower and Hayes, 1981; Rohman, 1965; Grabe and Kaplan, 1998). We base the design of IGA on the
2Dataset and models can be found at https://github.com/SimengSun/ IGA-An-Intent-Guided-Authoring-Assistant

widely cited and reproducible “cognitive process theory of writing” of Flower and Hayes (1981), which was made more comprehensive in the review work by Becker (2006). This theory posits that writing is a non-linear process that consists broadly of three steps : planning, translating and reviewing. The planning phase involves accessing one’s knowledge of the topic and target audience to formulate a rough outline of the eventual output. The actual rendering of the text on paper is called translating, while the reviewing phase consists of making edits or revisions to the output. All of these steps happen concurrently, under the inﬂuence of a monitor. In our work, a human author and a language model jointly participate in the planning and translating phase, while the human (by means of an editable output interface) reviews and monitors the process.
2.2 Inﬁlling language models
The actual implementation of IGA relies on controllable text inﬁlling via language modeling. The ability of large-scale language models to generate ﬂuent and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to inﬁll text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives speciﬁed by an author.
2.3 Controllable Text Generation
IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al. (2020) combine a large language model with an attribute discriminator to generate text that obeys certain sentiments or topics. Meanwhile, expanding the control codes proposed in Keskar et al. (2019b), Krause et al. (2020) develop a model that can generalize to new control codes, while

Fine-tuning example construction
(1) Identify key words and target spans
(2) Replace target spans with tags
(3) Concatenate target spans and special tokens

The wind blew over the farm, the rain came down and she heard ominous pings on the roof.
The wind blew over the farm, the rain came down and she heard ominous pings on the roof.
The wind blew over the farm, the rain came down and <descrip> pings <descrip> roof.
The wind blew over the farm, the rain came down and <descrip> pings <descrip> roof. <sep> she heard ominous <answer> on the <answer> <eos>
The wind blew over the farm, the rain came down and <descrip> pings <descrip> roof. <sep>

(3) Post-processing: Infill model output back to spans
(2) Post-processing: Identify tags
(1) Fine-tuned GPT-2 generates output prefixed with input and <sep>
Inference time

Figure 2: On the left, we show how each example is constructed for ﬁne-tuning. On the right, we show how the ﬁnal output is constructed by post-processing the output of a ﬁne-tuned GPT-2 model at inference time.

the Megatron-CNTRL model (Xu et al., 2020) control the output with predicted keyword. In contrast to these works, IGA focuses on ﬁnegrained, intra-sentential controlled inﬁlling. Previous work has also explored controlling stylistic parameters (Ficler and Goldberg, 2017) and syntactic structures (Iyyer et al., 2018; Goyal and Durrett, 2020).
3 Intent-Guided Assistant
IGA extends text inﬁlling models with ﬁnegrained rhetorical control. Speciﬁcally, we build on the Inﬁlling Language Model (ILM) of Donahue et al. (2020), which ﬁne-tunes an off-theshelf language model such as GPT-2 on a dataset of text with masked spans. To continue with our running example, the input to this model is the sequence “It was raining <mask> trees", and a target output is “and getting harder to see the". At inference time, the blanks are substituted with the words predicted by the LM and combined with the input in a post-processing step to generate the ﬁnal output: “It was raining and getting harder to see the trees".
Building on this framework, we ﬁne-tune an off-the-shelf GPT-2 medium model3 on our dataset (described in Section 4) created for generating text conditioned on author intent. Instead of replacing spans with a generic <mask> token as in ILM, we replace spans with more ﬁne-grained tags corresponding to rhetorical directives. For ﬁne-tuning, we concatenate a tag-replaced sentence with the ground-truth spans that should be inﬁlled using a special separator token <sep>, as in Donahue et al. (2020). If multiple tags are used
3This model has 431M parameters.

in the input, the multiple ground-truth spans following the <sep> token are separated by special <answer> tokens. At inference time, the model is fed a tag-replaced sentence and the <sep> token, from which it generates span(s) to inﬁll the input tags. In a post-processing step, we replace the tags with the generated answer spans. Figure 2 summarizes the ﬁne-tuning process (left) and the inference process (right). During inference time, we use top-k sampling (Fan et al., 2018) with k ﬁxed to 40.
To sum up, unlike the standard GPT-2 model which only supports strict left-to-right generation, both ILM and IGA are capable of text-inﬁlling. Moreover, IGA has ﬁner control over author writing intents than ILM, which further narrows down the output generation space. Although providing GPT-2 and ILM with speciﬁc discourse markers may result in output conforming to certain intents, they are less ﬂexible than IGA, which can sample from multiple discourse cues.
4 Dataset

Train Dev Test
Train Dev Test

PARA
148,621 500 500
CNTRA
200,000 500 500

BIO
200,000 500 500
DESCP
198,760 500 500

CAUSE
200,000 500 500
IDIOM
176,738 500 500

EFFECT
108,328 500 500
Total
1,232,447 3500 3500

Table 1: Number of examples per tag and in total.

The novelty of IGA lies not in its model architecture but the way in which we supervise it to enable controlled ﬁne-grained text inﬁlling. We construct the ﬁne-tuning dataset primarily by heuris-

Tag PARA BIO
CAUSE EFFECT CNTRA DESCP IDIOM

System input and output
Input: <paraphrase> The pandemic has caused very serious problems. <paraphrase> Output: The pandemic has brought severe economic, social and political effects that have seriously affected many countries. Input: Oria, <biography> , mentions that technically only humans can cry in response to emotional state. Output: Oria, a psychologist specializing in the study of human emotion, mentions that technically only humans can cry in response to emotional state. Input: This is a really good book <cause> plot <cause> . Output: This is a really good book because the plot is always so well written and never predictable. Input: Janet had suffered a traumatic brain injury in a car accident in 1988, <effect>. Output: Janet had suffered a traumatic brain injury in a car accident in 1988, thus had no memory of who she was or what happened. Input: The castle was built in 1865, <contrast> . Output: The castle was built in 1865, though a full-scale modern restoration has been underway for the past four years. Input: <description> individual <description> beliefs <description> wrong . Output: There are individual and social beliefs that should lead us to be skeptical of the facts and the wrong. Input: This report only shows the <idiom> , as many details can only be uncovered if you sign the document. Output: This report only shows the tip of the iceberg , as many details can only be uncovered if you sign the document.

Table 2: Example output of each tag from IGA.

tically mining the NEWSROOM corpus (Grusky et al., 2018), the largest available summarization dataset with 1.3 million news articles. We also collect partial data from ParaNMT-50M (Wieting and Gimpel, 2018), WikiLarge (Zhang and Lapata, 2017) for “sentence embellishment” writing intent, and PoMo (Kang et al., 2019) to extract postmodiﬁer that comes after an entity. Our dataset (statistics shown in Table 1) contains 75M tokens with a mean example length of 60.5 words tokenized with NLTK (Bird et al., 2009).
Choosing a collection of tags: Before we start collecting data, we conduct an internal survey with potential users of our system to determine what writing assistance functions they would most beneﬁt from. We surveyed nine NLP researchers about their opinions on the ideal functionality of an authoring assistant. After removing simple functions such as generating synonyms, antonyms, adjectives, and adverbs, which are already implemented in existing writing assistant tools, we condense the most requested writing intents into seven tags (described in detail below; examples provided in Table 2). Only one of them (PARA) is heavily constrained by semantic content, and the rest in-

volve open-ended generation loosely constrained by keywords and intent.
4.1 Data collection for each writing intent
CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually extracted 16 markers, many from the discourse marker list in Sileo et al. (2019), and then mine NEWSROOM (Grusky et al., 2018) to ﬁnd sentences that match any of the markers. For all mined examples, we also preserve the previous sentence as the context of the matched sentence. Simple declarative clauses that start with matched discourse markers are extracted through shift-reduce constituency parser ZPar (Zhang and Clark, 2011). The YAKE algorithm is later applied to those clauses for keyword extraction.
EFFECT: As a conjugate writing intent of CAUSE, EFFECT is used when one needs to describe the result or consequence. It usually cooccurs with the words/phrases such as ‘therefore’ and ‘as a result’. Similar to CAUSE, we manually select 15 discourse markers that signify EFFECT,

mine sentences from NEWSROOM, and highlight spans of interest using the same process as before. Speciﬁcally, we extract declarative clauses based on the constituent labels returned from the parser. Then, we decide the intent according to the starting markers of those clauses. Inside each clause, we run YAKE, an unsupervised keyword extraction algorithm, to extract keywords for the clause. All markers are included in Appendix B.

CNTRA: Comparison is a commonly used writing technique that encompasses two separate intents: concession and contrast. Concession refers to the unexpectedness of a consequence (Webber et al.): words/phrases such as “although” and “even though” raise an expectation curbed by the rest of the sentence. Contrast is often confused with concession, but its markers include “by comparison”, “in contrast”, etc. We manually select 31 concession markers and six comparison markers, and mine the Newsroom corpus with these to obtain labeled data.

DESCP: Descriptive details are important for creative writing and can help embellish written output. To curate data for this tag, we ﬁrst collect 27K descriptive adjectives based on morphological rules.4 We then mine sentences from NEWSROOM and extract simple declarative clauses that contain the matched adjective(s). The spans are ﬁltered to be greater than ﬁve tokens.

IDIOM: Idiomatic language makes writing more vivid and imaginative. We collect 3000 idioms online5 and mine sentences from NEWSROOM that match any of the idioms. In order to include variants of the raw idiom, e.g. “apple of one’s eye”, we apply regular expressions to match various possessive forms.

BIO: Biographical post-modiﬁers are com-

monly used to provide a brief summary of a pre-

viously mentioned named entity. For example,

“co-founder of Microsoft Corporation” ﬁlls in the

blank span of the sentence “Bill Gates,

,

has pursued a number of philanthropic endeav-

ors”. PoMo (Kang et al., 2019) is an existing

dataset that aligns with this writing intent. It con-

4Adjectives are extracted from English word frequency list https://norvig.com/ngrams/count_1w.txt
5Idioms are extracted from https:// 7esl.com/english-idioms/ and https: //www.phrases.org.uk/meanings/
phrases-and-sayings-list.html

tains sentences with post-modiﬁers and facts about named entities extracted from Wikidata. We use the textual data in PoMo, replace the post-modiﬁer with <biography> and treat the ground-truth postmodiﬁer as target span.
PARA: Paraphrasing is a common method by which authors improve their draft writing (Flower and Hayes, 1981; Tufte, 2006). Unlike sentence simpliﬁcation, the intent of our <paraphrase> tag is to paraphrase with improved writing quality, similar to embellishment. We construct parallel data for this tag by combining ParaNMT-50M (Wieting and Gimpel, 2018), a large corpus consisting of back-translated sentence pairs, with WikiLarge, a sentence simpliﬁcation dataset with parallel simple and complex sentences. The original sentence in ParaNMT-50M and complex sentence in WikiLarge are treated as targets, while the back-translated sentence and the simpliﬁed sentence are used as the source. We use BLEURT (Sellam et al., 2020) to ﬁlter noisy pairs from ParaNMT-50M,6 discarding pairs whose word-level edit distance is less than ﬁve. To further encourage complex paraphrases, we require the reference sentence to have more lowfrequency words than the candidate sentence.
5 Evaluation against references
As an initial comparison of IGA and ILM, we evaluate the generated outputs of each model against reference completions from our dataset, both automatically and through a crowdsourced evaluation. We acknowledge that this type of evaluation (especially using automatic metrics) is limited for open-ended generation tasks like ours (Fan et al., 2018; Akoury et al., 2020; Rashkin et al., 2020), which is why we also conduct an in-depth user study in Section 6. While results of these evaluations cannot reﬂect how practical IGA can be used as an authoring assistant, they do indicate that IGA is more constrained than ILM and produces output that better fulﬁlls the writing intents.
5.1 Automatic evaluation
We compare IGA with ILM on automatic metrics such as ROUGE (Lin, 2004) and self-BLEU (Zhu et al., 2018) following Rashkin et al. (2020), computing both scores against reference completions
6We set BLEURT threshold to be (0.7, 0.9) to avoid semantically dissimilar sentences and sentences without too much change.

BIO CAUSE EFFECT CNTRA DESCP IDIOM

ROUGE-2
ILM IGA
10.4 9.9 4.1 9.0 5.2 6.6 4.2 4.9 2.1 2.2 33.7 37.8

BLEU-2
ILM IGA
47.7 44.2 35.0 37.1 37.2 37.8 32.3 34.6 23.4 23.6 62.3 64.5

Length
ILM IGA
6.3 6.0 10.1 10.2 13.2 13.4 10.3 10.3 8.9 8.9 3.0 2.7

Table 3: ROUGE-2, self-BLEU2, and total number of inﬁlled tokens of each example on test set.

STRAP IGA

BLEURT
0.31 0.53

BLEU
21.79 33.90

s-BLEU
19.50 13.97

i-BLEU
9.4 15.46

Table 4: BLEURT, BLEU, self-BLEU, iBLEU scores comparison of STRAP (Krishna et al., 2020) and IGA on PARA validation set.

from the validation set.7 On all but the BIO tag, IGA achieves higher ROUGE and self-BLEU scores than ILM (Table 3), which shows that IGA outputs have higher coverage and lower diversity, respectively, without differing considerably in length. This result indicates that IGA is indeed conditioning its output on the tags to produce more constrained outputs. Since the inﬁlled spans of BIO are strictly post-modiﬁers that follow a very speciﬁc structure (i.e., enclosed by two commas), the superior performance of ILM indicates that it memorizes this simple form of construction without requiring a separate tag input.
PARA is the only substitution-based tag in our system and is not supported by ILM. Therefore, we compare performance of PARA with the stateof-the-art paraphraser STRAP released by Krishna et al. (2020) with the default nucleus sampling p = 0.6. We compute BLEURT scores to check semantic similarity, as well as BLEU (Papineni et al., 2002), self-BLEU (Sun and Zhou, 2012), and iBLEU (Sun and Zhou, 2012) with α = 0.8 to check the diversity of output. Table 4 indicates that IGA outperforms STRAP in all dimensions. We hypothesize that this is primarily because the diverse paraphraser in STRAP normalizes (and often simpliﬁes) stylized text, while our PARA tag is associated with complex, embellished paraphrases during ﬁne-tuning.
7All automatic metrics are computed only on the inﬁlled spans, excluding the context.

5.2 Intrinsic crowdsourced evaluation
The above automatic evaluations can only tell us so much about IGA’s capabilities. Many of our tags (e.g., DESCP, CAUSE) are open-ended, which results in a large space of acceptable outputs. Thus, measuring similarity to ground-truth span completions is not as suitable for our task as it is for more constrained tasks such as machine translation or summarization. In this section, we shift to human evaluation as a way to learn more about the behavior and usefulness of IGA.
We begin with a small-scale intrinsic evaluation to get a sense of the generation quality and adequacy of an output in fulﬁlling a writing intent. We randomly select 50 examples from the test set of each tag and generate outputs using both ILM and IGA for each example. For each output, three Mechanical Turkers are shown the gold completion as well as the generated text and asked to choose which is more ﬂuent, coherent, and adequate at fulﬁlling the author intent, using a ﬁve-point scale to measure ﬁne-grained preference.8 The results of this task are inconclusive: although IGA outputs for most tags are more often preferred than those from ILM across these dimensions (see Appendix A for speciﬁcs), especially in terms of adequacy, the subjective nature of the task yields low agreement among annotators.9 In general, annotators found the task difﬁcult and most often chose to express no preference.
6 User study
Due to the limitations of the previous evaluations, we launch a user study in the same spirit as Clark et al. (2018) to understand the interactive behavior of real users. We measure whether human authors beneﬁt from AI-assisted writing, and whether they prefer intent-guided generation to the uncontrolled ILM model. We design an interactive web demo that allows participants to write with the help of each model, logging their behavior (e.g., queries to the model, edits made on generated text) and self-reported feedback.
Our interactive demo is inspired by markup
8We employ workers with an approval rate higher than 96% and total approved HITS greater than 1000. Each rater is rewarded for $0.1 per HIT.
9The best Fleiss κ (Fleiss, 1971) across all tags was only slightly over 0.2, which indicates slight agreement (Landis and Koch, 1977).

Figure 3: User interface screenshot. The bottom-left Assistant textbox accepts intent marked-up sentences. Model output can be added to the top-right Main textbox through the ‘+’ button. All edits and sentences without machine assistance happen in the Main textbox.

language editors such as Overleaf10 or Lyx11; a screenshot of the interface is shown in Figure 3. In the textbox to the left, users type sentences with any of our supported tags. After clicking Generate, the model’s output will be displayed on the right hand side12. By default, three samples are shown to the user, although they can increase the number of samples if they wish. After a user selects a sample, it is appended to the existing text in the top-right textbox, which contains all of the text the user has already written. Users can then edit the sample (or completely delete it) and write continuations directly into the top-right textbox. This process repeats every time the user decides to use a tag to obtain model-generated text. On the backend, the input fed to our IGA model is the concatenation of content in the main textbox (i.e., context) and the input in the assistant box, truncated at 300 tokens.
6.1 User study design
We recruited twelve computer science graduate students for our user study, seven of whom are native English speakers.13 Nine of the twelve participants in the user study had creative writing experience in English prior to the evaluation, three participants had taken creative writing classes, and one was trained in media writing. We asked each participant to write short stories in response to prompts selected from WritingPrompts (Fan et al.,
10https://www.overleaf.com/ 11https://www.lyx.org/ 12It takes ∼1s to display three model outputs. 13Each participant is rewarded with a $30 Amazon gift card.

2018), a large dataset of stories written by users on Reddit. This task is suitable for our user study because creative writing requires diverse rhetorical directives while also not placing as much of an emphasis on world knowledge on the part of the participant (unlike writing a news article, for instance).
We ask each participant to write responses to three different prompts, where for each prompt they use one of three different writing modes: (1) BASE: writing from scratch without any AI assistance, (2) ILM: writing assisted only with the <mask> tag, and (3) IGA: writing assisted with multiple tags. To study how often users use intentguided generation instead of uncontrolled generation when given a choice, we also include the <mask> tag in the IGA mode by simply producing outputs from the ILM model. We randomize the order of modes across subjects to mitigate respondent fatigue (Lavrakas, 2008) (e.g., one participant may write their responses using BASE, ILM, IGA while another might use IGA, BASE, ILM).
Each evaluation session lasts for approximately one hour. Before each session, the participant is instructed to read a tutorial document, which describes the system’s layout and the usage of each tag. During each evaluation session, they ﬁrst go over an interactive tutorial to experiment with each tag, either with provided examples or examples they invent themselves, and then start the main writing tasks. The purpose of the tutorial is to thoroughly familiarize participants with each model so they do not have to learn on the ﬂy.
During the AI-assisted writing phase, we do not

require participants to write every sentence with the tags, or even use the system at all if they choose not to (e.g., they can write their whole response from scratch). We do require them to write at least ten sentences in response to each prompt. In each evaluation session, we record the following metrics to understand how the participants interact with the systems:
1. # of clicks on the Generate button, which takes the user-tagged sentence and outputs multiple (sampled) completions
2. # of clicks on the + button, which adds a sampled completion to the Main textbox
3. # of sentences written without any assistance
4. # of model-generated tokens that were kept and deleted by the author in the Main textbox
5. # of novel tokens inserted by an author within a model-generated completion.
We report the average number of tokens and sentences in the responses (Table 5), the average number of clicks per sentence (Table 6), tag usage of all AI-assisted output (Figure 4), and unigram precision, recall, and F1-scores of each intent tag (Table 7). In summary, users interact more frequently with IGA than ILM, generating more content (∼ 3 more sentences per session and ∼ 30 more tokens in each response), and more of their sentences on average are AI-assisted with IGA (8.0 compared to 6.6). Additionally, IGA generations are far less likely to be edited than those from ILM: Table 7 shows 69% of the generated tokens are preserved in ILM mode, compared to ∼ 87% in IGA mode (averaged across all tags). Interestingly, when equipped with the intent-based tags in IGA mode, the output of the uncontrolled <mask> tag, the second most often used tag in IGA mode as shown in Figure 4, is more likely to be accepted by users than in the ILM (80 vs. 68). This is likely because when the users use the <mask> tag under IGA, they indeed have no clear intents, and are thus more likely to accept intent-free generation.
6.2 Survey feedback
After each session, we also collect feedback from subjects through a post-session survey. The ﬁrst part of our survey asks participants to recall their experience with IGA mode and evaluate various aspects (Table 11). We refer readers to Appendix C for more detailed description. Participants are

Mode
BASE ILM IGA

# Tokens
215 170 198

# Sents
14.2 11.4 12.8

# AI-assisted Sents
6.6 8.0

Table 5: Average # of tokens, sentences, and sentences assisted by AI systems (per response).

CAUSE IDIOM
MASK

13% 10%

16%

9%

7%

17%

6%

22%

BIO PARA CNTRA EFFECT DESCP

Figure 4: DESCP and the uncontrolled MASK were the most commonly used IGA tags from our user study.

mildly satisﬁed with the model performance (3.4 / 5) and are interested in using the system for future WritingPrompt tasks (3.6 / 5), but they are polarized on how easy the system is to learn (3.6 / 5 with a standard deviation of 1.4)
We also ask them to choose the writing mode they prefer the most and explain their preference. Out of 12 participants, seven prefer IGA writing mode, four prefer ILM, and only one prefers writing from scratch. The majority of participants favoring the experience of either ILM or IGA demonstrates the potential of AI-assisted writing, especially for open-ended creative tasks like story writing. The most common reason that users prefer the intent-guided generation of IGA is because it provides ﬁne-grained control over the generated output. The four participants who prefer ILM remark that the system is much simpler to use because it has only one tag (<mask>). As one participant comments : “Once I became more comfortable with the remainder of the tags, I think it would be easier for me to write, and therefore more enjoyable. So short-term I would enjoy ILM and then long-term IGA. As someone who struggled with ﬁguring out what to write next for short stories in elementary school, I wish this existed then!".
In the ﬁnal portion of the survey, we ask them to rate the quality of each tag in IGA. If they did not use a certain tag in their writing, they are asked to give a rating for it by recalling their experience of using that tag during the tutorial mode. Table 8

Model Generate Add Gen/Sent. Add/Sent.

ILM

13.6 6.3 1.21

0.56

IGA

16.3 7.4 1.42

0.62

Table 6: We log every time a subject clicks the Generate and + buttons, averaged by response and by # of total sentences.

Tag

Pre. Rec. F1 Tag

Pre. Rec. F1

PARA BIO CAUSE CNTRA

93 92 92 DESCP 89 100 100 100 EFFECT 93 81 79 79 IDIOM 95 80 84 82 MASK 80

89 89 93 93 90 92 81 80

MASK(ILM) 69 68 68 -

-

--

Table 7: Unigram precision, recall, and F1 of model output in comparison with the ﬁnal text participant submitted. Smaller precision/recall indicates more deletion/insertion operations of participants.

shows that while participants have polarized view about the IDIOM tag, they are overall satisﬁed with the output of PARA and DESCP.

Tag

Avg. Std. Tag

Avg. Std.

PARA 3.7 0.6 DESCP 4.0 0.8

BIO

3.7 1.0 EFFECT 3.1 0.8

CAUSE 3.2 0.8 IDIOM 3.1 1.4

CNTRA 3.3 1.2 MASK 3.6 1.3

Table 8: Post-survey tag-speciﬁc ratings on a 5-point Likert scale (1 is negative, 5 is positive).

7 Limitations
Although our user study demonstrates that subjects prefer IGA over competing models, it has many limitations. First, NLP researchers are not the right group to ideate the set of writing intents, and in the user study, computer science graduate students are not representative enough as the target users. A more ideal setup is to conduct both the ideation of intents and user study with expert users, preferably English students or teachers. This sort of study could be done on platforms like Upwork. To validate the usefulness of the existing intents in IGA, we also need to conduct interviews with writing professionals and inquire about new prospective intents for future development.
8 Conclusion
In this paper, we introduce a new approach to interactive human-AI co-authoring by means of

an Intent-Guided Authoring Assistant (IGA). Our model is able to inﬁll around author-provided keywords, sentence fragments, and rhetorical instructions with ﬂuent and coherent text. We conduct a small-scale user study which shows that our method has advantages over baseline methods on a creative writing task.
Ethics statement
Our data collection is for research purposes only, and thus consistent with the terms of use of all source corpora we mined. For the evaluation process, we strive to compensate the Mechanical Turk workers as well as participants of our user study with competitive payments.
The intended use of IGA is for creative writing. Although generating factually-correct output is not a major focus of creative writing tasks, IGA often hallucinates facts about real-world entities, a phenomenon that raises ethical concerns and has become an increasing focus in text generation research (Maynez et al., 2020; Wang and Sennrich, 2020). The model can on rare occasions produce offensive outputs, due in large part to GPT-2’s pretraining corpora. One potential way to reduce the toxicity of output is to apply profanity ﬁlter as a post-processing step before ﬁnal output is returned.
Acknowledgements
We thank the reviewers for the thoughtful comments. We thank Andrew Drozdov, Katherine Thai, Nicholas Monath and other UMass computer science graduate students for helping us with the user study. We thank UMass NLP group for the great advice on the initial draft of this paper. MI was partially supported by award IIS-1955567 from the National Science Foundation (NSF).
References
Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470–6484, Online. Association for Computational Linguistics.
A. Becker. 2006. A review of writing model research based on cognitive processes.

Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.
Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. 2019. Skeleton-toresponse: Dialogue generation guided by retrieval memory. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1219–1228, Minneapolis, Minnesota. Association for Computational Linguistics.
Mia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang, Andrew M. Dai, Zhifeng Chen, Timothy Sohn, and Yonghui Wu. 2019. Gmail smart compose: Real-time assisted writing. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, page 2287–2295, New York, NY, USA. Association for Computing Machinery.
Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A. Smith. 2018. Creative writing with a machine in the loop: Case studies on slogans and stories. In 23rd International Conference on Intelligent User Interfaces, IUI ’18, page 329–340, New York, NY, USA. Association for Computing Machinery.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations.
Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to ﬁll in the blanks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2492– 2501, Online. Association for Computational Linguistics.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational Linguistics.

Jessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation, pages 94–104, Copenhagen, Denmark. Association for Computational Linguistics.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.
Linda Flower and John R. Hayes. 1981. A cognitive process theory of writing. College Composition and Communication, 32(4):365–387.
Tanya Goyal and Greg Durrett. 2020. Neural syntactic preordering for controlled paraphrase generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 238–252, Online. Association for Computational Linguistics.
W. Grabe and R. Kaplan. 1998. Theory and practice of writing: An applied linguistic perspective. College Composition and Communication, 49:301.
Nick Greer, Jaime Teevan, and Shamsi Iqbal. 2016. An introduction to technological support for writing. Technical Report MSR-TR-2016-1.
Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708–719, New Orleans, Louisiana. Association for Computational Linguistics.
Yichen Huang, Yizhe Zhang, Oussama Elachqar, and Yu Cheng. 2020. INSET: Sentence inﬁlling with INter-SEntential transformer. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2502–2515, Online. Association for Computational Linguistics.
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1875–1885, New Orleans, Louisiana. Association for Computational Linguistics.
Jun Seok Kang, Robert Logan, Zewei Chu, Yang Chen, Dheeru Dua, Kevin Gimpel, Sameer Singh, and Niranjan Balasubramanian. 2019. PoMo: Generating entity-speciﬁc post-modiﬁers in context. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 826–838, Minneapolis, Minnesota. Association for Computational Linguistics.

Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. 2019a. CTRL - A Conditional Transformer Language Model for Controllable Generation. arXiv preprint arXiv:1909.05858.
Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019b. CTRL: A conditional transformer language model for controllable generation. CoRR, abs/1909.05858.
Ben Krause, Akhilesh Deepak Gotmare, B. McCann, N. Keskar, Shaﬁq R. Joty, R. Socher, and Nazneen Rajani. 2020. Gedi: Generative discriminator guided sequence generation. ArXiv, abs/2009.06367.
Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737–762, Online. Association for Computational Linguistics.
J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159–174.
Paul J Lavrakas. 2008. Encyclopedia of survey research methods. Sage publications.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.
Yi Liao, Xin Jiang, and Qun Liu. 2020. Probabilistically masked language model capable of autoregressive generation in arbitrary word order. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 263–274, Online. Association for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, Online. Association for Computational Linguistics.
Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long

and Short Papers), pages 2267–2277, Minneapolis, Minnesota. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Jianfeng Gao. 2020. PlotMachines: Outlineconditioned generation with dynamic plot state tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4274–4295, Online. Association for Computational Linguistics.
D. Gordon Rohman. 1965. Pre-writing the stage of discovery in the writing process. College Composition and Communication, 16(2):106–112.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics.
Damien Sileo, Tim Van De Cruys, Camille Pradel, and Philippe Muller. 2019. Mining discourse markers for unsupervised sentence representation learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3477–3486, Minneapolis, Minnesota. Association for Computational Linguistics.
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion transformer: Flexible sequence generation via insertion operations. volume 97 of Proceedings of Machine Learning Research, pages 5976–5985, Long Beach, California, USA. PMLR.
Hong Sun and Ming Zhou. 2012. Joint learning of a dual SMT system for paraphrase generation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 38–42, Jeju Island, Korea. Association for Computational Linguistics.
Virginia Tufte. 2006. Artful Sentences: Syntax as Style. Graphics Press.
Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 58th Annual

Meeting of the Association for Computational Linguistics, pages 3544–3552, Online. Association for Computational Linguistics.
Bonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi. The penn discourse treebank 3.0 annotation manual.
Sean Welleck, Kianté Brantley, Hal Daumé Iii, and Kyunghyun Cho. 2019. Non-monotonic sequential text generation. volume 97 of Proceedings of Machine Learning Research, pages 6716–6726, Long Beach, California, USA. PMLR.
John Wieting and Kevin Gimpel. 2018. ParaNMT50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462, Melbourne, Australia. Association for Computational Linguistics.
Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, and Bryan Catanzaro. 2020. MEGATRON-CNTRL: controllable story generation with external knowledge using large-scale language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 2831–2845. Association for Computational Linguistics.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 9054–9065. Curran Associates, Inc.
Xingxing Zhang and Mirella Lapata. 2017. Sentence simpliﬁcation with deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594, Copenhagen, Denmark. Association for Computational Linguistics.
Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and Bill Dolan. 2020. POINTER: Constrained progressive text generation via insertion-based generative pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8649–8670, Online. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.
Wanrong Zhu, Zhiting Hu, and Eric Xing. 2019. Text inﬁlling.

Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. CoRR, abs/1802.01886.
A. Çelikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. ArXiv, abs/2006.14799.

Appendix
A Mechanical Turk experiment
We randomly select 50 examples from test set of each tag and get output from ILM and IGA respectively. Each example includes the gold reference and the model output. Each example was assigned to three Mechanical Turk workers who have approval rate higher than 96% and number of approved HITS greater than 1000. Each worker was asked to rate the ﬂuency (FL), coherence (CH) and the adequacy (ADQ) of the inﬁlled content. The ﬁrst two dimensions are common in natural language generation evaluation, which judge the grammaticality and how well the system output ﬁts into the provided context (Çelikyilmaz et al., 2020). The last quality dimension ADQ measures how well the inﬁlled content alone fulﬁll the target author intent. The rating is on a 5-point Likert scale. To increase inter annotator agreement, we collapsed 1 and 2 to 1, 4 and 5 to 3 and change 3 to 2, thus the reported value in 9 is reported on a 3-point scale.

BIO CAUSE EFFECT CNTRA DESCP IDIOM

FL 1.97 1.82 1.83 1.83 2.06 1.93∗

ILM
CH 1.91 1.88 1.73∗ 1.79 2.04∗ 1.81∗

ADQ
1.99 1.67∗ 1.79∗ 1.82∗ 2.07∗ 1.75∗

FL 2.13 2.00 1.94 1.97 2.07 1.88∗

IGA
CH 1.99 1.99 1.88∗ 1.87 1.95∗ 1.84∗

ADQ
2.05
1.99 1.96∗ 1.94∗ 2.06∗ 1.84∗

Table 9: Ratings of intrinsic crowdsourced evaluation. We collapse the 5-point Likert scale to 3-point scale with 1 (prefer reference), 2 (no preference), 3 (prefer generated text). Fleiss κ greater than 0.2 is marked with ∗.

In general, we ﬁnd it’s hard to get high agreement from the Turkers in terms of ﬂuency except for IDIOM . Annotators believe the ILM has better ﬂuency mostly because some spans are inﬁlled with clauses rather than short idioms, which leads raters to give higher ﬂuency scores.

B Discourse markers used for data extraction
We display discourse markers used for extracting ﬁne-tuning example in Table 10.

albeit admittedly although
but but then but still concede that regardless
still though
because because of
due to on the strength of
as a result consequently as a consequence that being so

CNTRA

despite even as even after even before even if even so even then nevertheless nonetheless no matter

even though even when even while even with however in any case in any event in spite of whatever
whether

CAUSE

as a consequence of owing to
by reason of in the wake of

insofar as not only because of
on account of as a result of

EFFECT

accordingly for this reason for that reason on this account

on that account inevitably hence in the end

yet by comparison
by contrast conversely in contrast on the contrary on the other hand
on grounds of by dint of thanks to by virtue of
thereby therefore
thus

Table 10: Example discourse markers used for mining ﬁne-tuning example.

Dimension Avg. Std. Dimension Avg. Std.

Fluency

3.8 0.4 Quality

2.9 0.8

Relevance 3.6 0.8 Satisfy

3.4 0.8

Coherence 3.3 0.9 Use Again 3.6 1.0

Interesting 3.1 1.0 Easy to learn 3.6 1.4

Inspiration 3.5 0.8 -

-

-

Table 11: Post-survey general ratings. Ratings are on 5-point Likert scale with 5 being positive experience, 3 neutral, and 1 negative.

C Post-survey rating
The ﬁrst section of our survey asks participants to recall their experience with IGA mode and evaluate various aspects presented in Table 11. Besides commonly asked dimensions, such as ﬂuency, relevance, coherence, and general quality of system output, we also ask them how often the system generate output that’s interesting (interesting) and that inspires them to write (Inspiration). They are also asked to rate whether they are satisﬁed with the system output (satisfy), whether they would like to use the system again (Use again) for the WritingPrompt task, and how easy it is to learn the system (Easy to learn). In general, participants are mildly satisﬁed with the model performance, but understandably, have polarized views on how easy it is to learn this system with standard deviation of 1.4.
D Fine-tuning example
We display ﬁne-tuning example of each tag (intent) in Table 12.

Intent Tag PARA BIO
CAUSE EFFECT CNTRA
DESCP IDIOM

Example <sub> the growth potential has consistently declined in this period . <sub> <sep> The growth potential has been steadily declining throughout this period . <answer> Roger Stone , a Republican strategist , said , “ Issues that were extremely successful for us in the 80 ’s are n’t on the radar screen anymore . ” But Robert Teeter , <biography> , insists that the frictions and tensions are simply the growing pains of a governing coalition . <sep> the Republican polltaker <answer> I gawped in astonishment . This morning I read that the University of Exeter has had to employ social media operators to deal with inquiries , <cause> increasing <cause> email , considering it too slow and unwieldy . <sep> because <answer> numbers of students will not use <answer> “ I view military prisons as the overlooked campaign of 1864 ; prisons , their management and questions of exchange are taking up a massive part of the bureaucratic part of the war . ” <effect> Civil War <effect> . <sep> In the end , most <answer> POWs survived <answer> Part of being able to extend the network effect of your status update is having the right desktop client for broadcasting updates as well as keeping a lookout on relevant updates from other users . <concession> perfect <concession> user , we highly recommend the new Seesmic Desktop for managing multiple accounts and tracking custom search results . <sep> Though we believe the <answer> desktop client is unique to each <answer> It’s because, contrary to what we’ve been told by satirists, sneering cynics and other such detritus, he is in fact a deeply witty and humane man. <description> and he looks like a chimp . <sep> He ’s intelligent , perceptive <answer> As the Senators prepare to face the Montreal Canadiens in Game 3 of their playoff series Sunday night ( CBC , 7 p.m . ET ) at Scotiabank Place , the Ottawa coach had his audience of assembled media <idiom> as he tried to deﬂect any talk about a war of words . <sep> in stitches <answer>

Table 12: Example of each writing intent tag

