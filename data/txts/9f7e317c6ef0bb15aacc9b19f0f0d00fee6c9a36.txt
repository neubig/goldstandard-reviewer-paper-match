What Neural Networks Memorize and Why: Discovering the Long Tail via Inﬂuence Estimation

Vitaly Feldman * † Apple

Chiyuan Zhang* Google Research, Brain Team

arXiv:2008.03703v1 [cs.LG] 9 Aug 2020

Abstract
Deep learning algorithms are well-known to have a propensity for ﬁtting the training data very well and often ﬁt even outliers and mislabeled data points. Such ﬁtting requires memorization of training data labels, a phenomenon that has attracted signiﬁcant research interest but has not been given a compelling explanation so far. A recent work of Feldman [Fel19] proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a signiﬁcant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given.
In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the inﬂuence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled inﬂuence and memorization values can be estimated much more efﬁciently. Our experiments demonstrate the signiﬁcant beneﬁts of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in [Fel19].
1 Introduction
Perhaps the most captivating aspect of deep learning algorithms is their ability to generalize to unseen data. The models used in deep learning are typically overparameterized, making it easy to perfectly ﬁt the training dataset without any generalization. In fact, the standard training algorithms do ﬁt the training data very well, typically achieving 95-100% accuracy, even when the accuracy on the test dataset is much more modest. In particular, they usually ﬁt obvious outliers (such as images with no discernible features of their class) and mislabeled examples. The only way for a training algorithm to ﬁt an example whose label cannot be predicted based on the rest of the dataset is to memorize1 the label. Further, it is now well-known that standard deep learning algorithms achieve high training accuracy even on large and randomly labeled datasets [ZBHRV17].
This propensity for label memorization is not explained by the standard approach to understanding of generalization. At a high level, the standard approach upper-bounds the generalization error by the sum of an upper bound on the generalization gap controlled by a model complexity (or stability) parameter and the empirical error. Fitting of outliers and mislabeled examples does not improve the generalization error. Therefore, to avoid “overﬁtting”, the balance between the complexity parameter and the empirical error is supposed to be tuned in a way that prevents label memorization. Memorization is also generally thought of (and taught in ML courses) as being the opposite of generalization.
This disconnect between the classical theory and modern practice was highlighted in the work of Zhang et al. [ZBHRV17] and generated a large wave of research interest in the topic of generalization for deep learning. The bulk of this research focuses on ﬁnding new ways to control the generalization gap or showing that training algorithms
*Equal contribution. †Part of this work done while the author was at Google Research, Brain Team. 1This notion of label memorization is deﬁned rigorously in eq. (1) (Sec. 1.1).
1

induce a form of implicit regularization. These results have lead to tighter theoretical bounds and, in some cases, bounds that show correlation with the actual generalization gap (see [NBMS17; NK19; JNMKB20] for analyses of numerous measures). Yet, fundamentally, these works still follow the same classical approach to generalization that cannot explain memorization. Another line of research focuses on the generalization error of algorithms that ﬁt the data perfectly (referred to as interpolating) [WOBM17; BRT18; BHM18; LR18; BMM18; RZ19; BLLT19; BHX19; HMRT19; MVS19]. At a high level, these works show that under certain conditions interpolating algorithms achieve (asymptotically) optimal generalization error. However, under the same conditions there also exist standard non-interpolating algorithms that achieve the optimal generalization error (e.g. via appropriate regularization). Thus these works do not explain why interpolating algorithms are used in the ﬁrst place.
A recent work of Feldman [Fel19] proposes a new explanation for the beneﬁts of memorization. The explanation suggests that memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed, namely, rare and atypical instances make up a signiﬁcant fraction of the data distribution. Moreover, in such distributions useful examples from the “long tail” (in the sense that memorizing them improves the generalization error) can be statistically indistinguishable from the useless one, such as outliers and mislabeled ones. This makes memorization of useless examples (and the resulting large generalization gap) necessary for achieving close-to-optimal generalization error. We will refer to this explanation as the long tail theory.
In [Fel19] the need for memorization and statistical indistinguishability of useful from useless examples are theoretically demonstrated using an abstract model. In this model the data distribution is a mixture of subpopulations and the frequencies of those subpopulations are chosen from a long-tailed prior distribution. Subpopulation are presumed to be distinct enough from each other that a learning algorithm cannot achieve high accuracy on a subpopulation without observing any representatives from it. The results in [Fel19] quantify the cost of not memorizing in terms of the prior distribution and the size of the dataset n. They also show that the cost is signiﬁcant for the prototypical long-tailed distributions (such as the Zipf distribution) when the number of samples is smaller than the number of subpopulations.
While it has been recognized in many contexts that modern datasets are long-tailed [ZAR14; VHP17; BS19], it is unclear whether this has any relationship to memorization by modern deep learning algorithms and (if so) how signiﬁcant is the effect. The theoretical explanation in [Fel19] is based on a generative prior distribution and therefore cannot be directly veriﬁed. This leads to the question of how the long tail theory can be tested empirically.

1.1 Overview
In this work we develop approaches for empirically validating the long tail theory. The starting point for such validation is examining which training examples are memorized and what is the utility of all the memorized examples as a whole. To make it more concrete we recall the deﬁnition of label memorization from [Fel19]. For a training algorithm A operating on a dataset S = ((x1, y1), . . . , (xn, yn)) the amount of label memorization by A on example (xi, yi) ∈ S is deﬁned as

mem(A, S, i) := Pr [h(xi) = yi] − Pr [h(xi) = yi],

(1)

h←A(S)

h←A(S \i )

where S\i denotes the dataset S with (xi, yi) removed and probability is taken over the randomness of the algorithm A (such as random initialization). This deﬁnition captures and quantiﬁes the intuition that an algorithm memorizes the label yi if its prediction at xi based on the rest of the dataset changes signiﬁcantly once (xi, yi) is added to the dataset.
The primary issue with this deﬁnition is that estimating memorization values with standard deviation of σ requires running A(S\i) on the order of 1/σ2 times for every example. As a result, this approach requires Ω(n/σ2) training runs which translates into millions of training runs needed to achieve σ < 0.1 on a dataset with n =50,000 examples. We bypass this problem by proposing a closely-related estimator that looks at the expected memorization of the label of xi on a random subset of S that includes m ≤ n of examples. It can be seen as mem(A, S, i) smoothed by the random subsampling of the dataset and is also related to the Shapley value of example i for accuracy on itself. Most importantly, for m bounded away from n and 1 this memorization value can be estimated with standard deviation σ for every i at the same time using just O(1/σ2) training runs.
We compute memorization value estimates on the MNIST, CIFAR-100 and ImageNet datasets and then estimate the marginal effect of memorized examples on the test accuracy by removing those examples from the training dataset.

2

We ﬁnd that, aside from the MNIST dataset,2 a signiﬁcant fraction of examples have large memorization estimates. The marginal utility of the memorized examples is also signiﬁcant, in fact higher than a random set of examples of the same size. For example, on the ImageNet ≈ 32% of examples have memorization estimates ≥ 0.3 and their marginal utility is ≈ 3.4% (vs. ≈ 2.6% for a random subset of 32% of examples). In addition, by visually examining the memorization estimates, we see that examples with high memorization scores are a mixture of atypical examples and outliers/mislabeled examples (whereas examples with low memorization estimates are much more typical). All of these ﬁndings are consistent with the long tail theory.
A more important prediction of the theory is that memorization is necessary since each memorized representative of a rare subpopulation signiﬁcantly increases the prediction accuracy on its subpopulation. We observe that this prediction implies that there should exist a substantial number of memorized training examples each of which signiﬁcantly increases the accuracy on an example in the test set. Further, of the test examples that are inﬂuenced signiﬁcantly, most are inﬂuenced signiﬁcantly only by a single training example. The uniqueness is important since, according to the theoretical results, such unique representatives of a subpopulation are the ones that are hard to distinguish from outliers and mislabeled examples (and thus memorizing them requires also memorizing useless examples).
To ﬁnd such high-inﬂuence pairs of examples we need to estimate the inﬂuence of each training example (xi, yi) on the accuracy of the algorithm A at each test example (xj, yj):

infl(A, S, i, j) := Pr [h(xj) = yj] − Pr [h(xj) = yj].

(2)

h←A(S)

h←A(S \i )

As with memorization values, estimating the inﬂuence values for all pairs of examples is clearly not computationally feasible. A famous proxy for the classical leave-one-one inﬂuence is the inﬂuence function [CW82]. Computing this function for deep neural networks has been studied recently by Koh and Liang [KL17] who proposed a method based on assumptions of ﬁrst and second order optimality. Alternative proxies for measuring inﬂuence have been studied in [YKYR18; PLSK20].
We propose and use a new estimator for inﬂuence based on the same subsampling as our memorization value estimator. Its primary advantages are that it is a natural smoothed version of the inﬂuence value itself and, as we demonstrate visually, provides reliable and relatively easy to interpret estimates. We then locate all train-test pairs of examples from the same class (xi, yi) and (xj, yj) such that our estimate of the memorization value of (xi, yi) is sufﬁciently large (we chose 0.25 as the threshold) and our estimate of the inﬂuence of (xi, yi) on the accuracy at (xj, yj) is signiﬁcant (we chose 0.15 as the threshold). See Sec. 2 for the details of the estimator and the justiﬁcation of the threshold choice. Overall, we found a substantial number of such pairs in the CIFAR-100 and the ImageNet. For example we found 1641 pairs satisfying these criteria in the ImageNet. In these pairs there are 1462 different test examples (comprising 2.92% of the test set) of which 1298 are inﬂuenced signiﬁcantly by only a single training example.
These quantitative results of our experiments clearly support the key ideas of the long tail theory. To further investigate the ﬁndings, we visually inspect the high-inﬂuence pairs of examples that were found by our methods. This inspection shows that, in most cases, the pairs have an easy-to-interpret visual similarity and provide, we believe, the most compelling evidence for the long tail theory.
In addition to our main experiments, we investigate several natural related questions (albeit only on CIFAR-100). In the ﬁrst set of experiments we look at how much the results of our experiments depend on the choice of the architecture. We ﬁnd that, while the architecture deﬁnitely plays a role (as it does for the accuracy) there are strong correlations between sets of memorized examples and high-inﬂuence pairs for different architectures. These experiments also give a sense of how much our results are affected by the randomness in the estimation and training processes and the resulting selection bias.
Finally, as our inﬂuence and memorization estimators are still very computationally intensive, we consider a faster way to compute these values. Speciﬁcally, instead of training the entire network on each random subset of S, we train only the last layer over the representation given by the penultimate layer of the network trained once on the entire dataset. The resulting estimator is much more computationally efﬁcient but it fails completely at detecting memorized examples and gives much worse inﬂuence estimates. In addition to being a potentially useful negative result, this

2We include the MNIST dataset primarily as a comparison point, to show that memorization plays a much smaller role when the variability of the data is low (corresponding to a low number of subpopulations in a mixture model) and the number of examples per class is high.

3

experiment provides remarkable evidence that most of the memorization effectively happens in the deep representation and not in the last layer.
1.2 Related Work
For a more detailed comparison of the long tail theory with the standard approaches to understanding of generalization and work on interpolating methods we refer the reader to [Fel19].
Memorization of data has been investigated in the context of privacy-preserving ML. Starting from [SSSS17], multiple works have demonstrated that the output of the trained neural network can be used to perform successful membership inference attacks, that is to infer with high accuracy whether a given data point is part of the training set. An important problem in this area is to ﬁnd learning algorithms that are more resistant to such attacks on privacy. Our results suggest that reducing memorization will also affect the accuracy of learning.
Arpit et al. [Arp+17] examine the relationship between memorization of random labels and performance of the network on true labels. The work demonstrates that using various regularization techniques, it is possible to reduce the ability of the training algorithm to ﬁt random labels without signiﬁcantly impacting its test accuracy on true labels. The explanation proposed for this ﬁnding is that memorization is not necessary for learning. However memorization is used informally to refer to ﬁtting the entire randomly labeled dataset. Even with regularization, the algorithms used in this work memorize a signiﬁcant fraction of randomly labeled examples and ﬁt the true training data (nearly) perfectly.
Carlini et al. [CEP19] consider different ways to measure how “prototypical” each of the data points is according to several metrics and across multiple datasets. They examine 5 different metrics and draw a variety of insights about the metrics from the visualisations of rankings along these metrics. They also discuss “memorized exceptions”, “uncommon submodes” and outliers informally. While the memorization value we investigate also identiﬁes atypical examples and outliers, it is not directly related to these metrics. This work also brieﬂy reports on an unsuccessful attempt to ﬁnd individual training examples that inﬂuence individual test examples on MNIST. As our experiments demonstrate, such high-inﬂuence pairs are present in MNIST and therefore this result conﬁrms that ﬁnding them via the direct leave-one-out method while ensuring statistical signiﬁcance is computationally infeasible even on MNIST.
The use of random data subsamples is standard in data analysis, most notably in bagging, bootstrapping and cross validation. In these applications the results from random subsamples are aggregated to estimate the properties of the results of data analysis as a whole whereas we focus on the properties of individual samples. Concurrent work of Jiang et al. [JZTM20] uses data subsamples to estimate the regularity (or easiness) of each training example. This score (referred to as the empirical consistency score) coincides with the second term in our subsampled memorization value estimator (Alg. 1, line 5). The value of this estimator is typically equal to one minus our memorization estimate since ﬁtting hard-to-predict training examples requires memorizing their labels. The score was derived independently of our work and its computation in [JZTM20] builds on the experimental framework developed in this work. The focus in [JZTM20] is on the investigation of several proxies for the consistency score and their experimental results are otherwise unrelated to ours.
Toneva et al. [TSCTBG19] investigate a “forgetting” phenomenon in which an example that was predicted correctly at some point during training becomes misclassiﬁed. They empirically demonstrate that examples that are never “forgotten” tend to have low marginal utility. The notion of memorization we consider is not directly related to such “forgetting”.
2 Estimation and Selection Procedures
In this section we describe how our memorization and inﬂuence estimators are deﬁned and computed. We also describe the selection criteria for high-inﬂuence pairs of examples.
Memorization and inﬂuence estimators: Our goal is to measure the label memorization by an algorithm A on a (training) dataset S and example (xi, yi) (eq. (1)) and the inﬂuence of a training example (xi, yi) on a test example (xj, yj). Both of these values are a special case of measuring the inﬂuence of (xi, yi) on the expected accuracy at some example z = (x, y) or
4

infl(A, S, i, z) := Pr [h(x) = y] − Pr [h(x) = y].

(3)

h←A(S)

h←A(S \i )

Clearly, mem(A, S, i) = infl(A, S, i, (xi, yi)), that is memorization corresponds to the inﬂuence of example i on the accuracy on itself (or self-inﬂuence).
As discussed in Sec. 1.1, directly estimating the inﬂuence of all n training examples within standard deviation σ requires training on the order of n/σ2 models. Thus we propose a closely-related inﬂuence value that looks at the expected inﬂuence of an example (xi, yi) relative to a dataset that includes a random subset of S of size m < n. More formally, for a set of indices I ⊆ [n] ([n] is deﬁned as the set {1, . . . , n}), let SI = (xi, yi)i∈I be the dataset consisting of examples from S with indices in I. For a set of indices J ⊆ [n], let P (J, m) denote the uniform distribution over all subsets of J of size m. Then we deﬁne:

inflm(A, S, i, z) :=

E

infl(A, SI∪{i}, i, z) ,

I∼P ([n]\{i},m−1)

where by sampling a random subset of size m − 1 that excludes index i we ensure that SI∪{i} is uniform over all subsets of size m that include the index i.
We now show that subsampled inﬂuence values can be estimated with standard deviation σ by training just O(1/σ2) models.
Lemma 2.1. There exists an algorithm that for every dataset S ∈ (X × Y )n, learning algorithm A, m ∈ [n] and integer t, runs A t times and outputs estimates (µi)i∈[n] such that for every i ∈ [n] and p = min(m/n, 1 − m/n),

E (inflm(A, S, i, z) − µi)2 ≤ 1 + 1 + e−pt/16 ,

pt (1 − p)t

2

where the expectation is with respect to the randomness of A and the randomness of the estimation algorithm.

We include the proof in Sec. A. The estimator exploits the fact that by training models on random subsets of size m we will, with high probability, obtain many models trained on subsets that include index i for every i and also many subsets that exclude i. By linearity of expectation, this gives an estimate of inflm(A, S, i, z). Alg. 1 describes the resulting algorithm for estimating all the memorization and inﬂuence values. We use k ∼ [t] to denote index k being chosen randomly and uniformly from the set [t] (the probabilities for such sampling are computed by enumerating over all values of k).

Algorithm 1 Memorization and inﬂuence value estimators

Require: Training dataset: S = ((x1, y1), . . . , (xn, yn)), testing dataset Stest = ((x1, y1), . . . , (xn , yn )), learning algorithm A, subset size m, number of trials t.

1: Sample t random subsets of [n] of size m: I1, I2, . . . , It.

2: for k = 1 to t do

3: Train model hk by running A on STk . 4: for i = 1 to n do

5: memm(A, S, i) := Prk∼[t][hk(xi) = yi | i ∈ Ik] − Prk∼[t][hk(xi) = yi | i ∈ Ik].

6: for j = 1 to n do

7:

inflm(A, S, i, j) := Prk∼[t][hk(xj) = yj | i ∈ Ik] − Prk∼[t][hk(xj) = yj | i ∈ Ik].

8: return memm(A, S, i) for all i ∈ [n]; inflm(A, S, i, j) for all i ∈ [n], j ∈ [n ].

The larger the subset size parameter m, the closer is our estimator to the original infl(A, S, i, z). At the same time, we need to ensure that we have a sufﬁcient number of random datasets that exclude each example (xi, yi). To roughly balance these requirements in all of our experiments we chose m = 0.7 · n. Due to computational constraints we chose the number of trials to be t = 2000 for ImageNet and t = 4000 for MNIST/CIFAR-100.
We remark that for m = n/2 our inﬂuence value is closely-related to the Shapley value of example (xi, yi) for the function that measures the expected accuracy of the model on point z. This follows from the fact that for functions that

5

are symmetric (do not depend on the order of examples) the Shapley value is equal to the expected marginal utility of example i relative to the random and uniform subset of all examples. For sufﬁciently large n, such subsets have size close to n/2 with high probability. The Shapley value is itself a natural and well-studied measure of the contribution of each point to the value of a function and thus provides an additional justiﬁcation for the use of our subsampled inﬂuence values.
Selection of high-inﬂuence pairs of examples: To ﬁnd the high-inﬂuence pairs of examples we select all pairs of examples (xi, yi) ∈ S and (xj, yj) ∈ Stest for which memm(A, S, i) ≥ θmem; inflm(A, S, i, j) ≥ θinfl and yi = yj. The last condition is used since the long tail theory only explains improvement in accuracy from examples in the same subpopulation and allows to reduce the noise in the selected estimates.
Selection of pairs that is based on random estimates introduces selection bias into the estimates of values that are close to the threshold value. The choice of θmem is less important and is almost unaffected by bias due to a relatively small set of estimates. We have chosen θmem = 0.25 as a signiﬁcant level of memorization. In choosing θinfl we wanted to ensure that the effect of this selection bias is relatively small. To measure this effect we ran our selection procedure with various thresholds on CIFAR-100 twice, each based on 2000 trials. We chose θinfl = 0.15 as a value for which the The Jaccard similarity coefﬁcient between the two selected sets is ≥ 0.7. In these two runs 1095 and 1062 pairs were selected, respectively with ≈ 82% of the pairs in each set also appearing in the other set. We have used the same thresholds for all three datasets for consistency. More details on the consistency of our selection process can be found in Sec. 3.5.
3 Empirical Results
In this section we describe the results of the experiments based on the methods and parameters speciﬁed in Sec. 2. We use ResNet50 in both ImageNet and CIFAR-100 experiments, which is a Residual Network architecture widely used in the computer vision community [HZRS16]. Full details of the experimental setup and training algorithms are given in Sec. B.
3.1 Examples of memorization value estimates
In Fig. 1 we show examples of the estimated subsampled memorization values around 0, 0.5, and 1, respectively. Additional examples can be found at [FZ20]. These examples suggest that the estimates reﬂect our intuitive interpretation of label memorization. In particular, some of the examples with estimated memorization value close to 0 are clearly typical whereas those with value close to 1 are atypical, highly ambiguous or mislabeled.
3.2 Marginal utility of memorized examples
Fig. 2 demonstrates the signiﬁcant effect that the removal of memorized examples from the dataset has on the test set accuracy. One could ask whether this effect is solely due to the reduction in number of available training examples as a result of the removal. To answer this question we include in the comparison the accuracy of the models trained on the identical number of examples which are chosen randomly from the entire dataset. Remarkably, memorized examples have higher marginal utility than the identical number of randomly chosen examples. The likely reason for this is that most of the randomly chosen examples are easy and have no marginal utility.
3.3 Estimation of inﬂuence and marginal utility of high-inﬂuence examples
We compute the estimated inﬂuences and select high-inﬂuence pairs of examples as described in Sec. 2. Overall we found 35/1015/1641 pairs in MNIST/CIFAR-100/ImageNet. In Fig. 3 we give histograms of the number of such pairs for every level of inﬂuence. The number of unique test examples in these pairs is 33/888/1462 (comprising 0.33%/8.88%/2.92% of the test set). Of those 31/774/1298 are inﬂuenced (above the 0.15 threshold) by a single training example. On CIFAR-100 and ImageNet this conﬁrms the importance of the subpopulations in the long tail that have unique representatives for the generalization error. The results on the MNIST are consistent with the fact that it is a
6

0.0

0.5

1.0

0.0

0.0

0.5

0.5

1.0

1.0

Figure 1: Examples of memorization values from ImageNet class “bobsled” (top), CIFAR-100 class “bee” (bottom left) and MNIST class 2, 3, 5, 6 (bottom right).

trainset fraction accuracy

0.72

0.70

0.68

remove memorized

remove random

1.00

0.75

0.50 0.2 0.4 0.6 0.8 1.0 memorization value threshold

(a) ImageNet

trainset fraction accuracy

0.750 0.725 0.700 0.675
1.00 0.75 0.50 0.2 0.4 0.6 0.8 1.0
memorization value threshold
(b) CIFAR-100

ftrraaictnisoent

accuracy

0.9925 0.9920 0.9915 0.9910 0.919.0005
0.98 0.2 0.4 0.6 0.8 1.0 memorization value threshold
(c) MNIST

Figure 2: Effect on the test set accuracy of removing examples with memorization value estimate above a given threshold and the same number of randomly chosen examples. Fraction of the training set remaining after the removal is in the bottom plots. Shaded area in the accuracy represents one standard deviation on 100 (CIFAR-100, MNIST) and 5 (ImageNet) trials.

7

count count count

800 825

700

600

500

400 381

300

200

168

100

93 58 43 23 19 13 6 5 5 1 1 0 0 0

0 0.2

0.4

0.6

0.8

1.0

influence

(a) ImageNet

500 509

400

300

200 196

100

101

48 26 20 19 24 11 12 10 10 4 7 7 5 6

0 0.2

0.4

0.6

0.8

1.0

influence

(b) CIFAR-100

18 17.5

15.0

12.5

10.0

9

7.5

6

5.0

2.5

1

1

0.0 0.2

0.4

0.6

0.8

1.0

influence

(c) MNIST

Figure 3: Histogram of the inﬂuence estimates for all the pairs from the ImageNet, CIFAR-100 and MNIST datasets that were selected according to Algorithm 1 and criteria described in Sec. 2.

much easier dataset with low variation among the examples (in particular, a smaller number of subpopulations) and much larger number of examples per class.
Next we examine the marginal utility of the training examples in high-inﬂuence pairs. Denote the set of all the unique training and testing examples in the high-inﬂuence pairs by Sh ⊆ S and Sh ⊆ S , respectively. To evaluate the marginal utility of the high-inﬂuence training examples Sh, we train a ResNet50 model on the CIFAR-100 full training set S, and on S \ Sh, respectively. Over 100 random runs, the two settings result in 76.06 ± 0.28% and 73.52 ± 0.25% accuracy on the test set, respectively, giving 2.54 ± 0.2% difference. When restricted to the set of highly-inﬂuenced test examples Sh, the two settings have 72.14 ± 1.32% and 45.38 ± 1.45% accuracy, respectively. Note that the difference in accuracy on these examples contributes 2.38 ± 0.17% to the total difference in accuracy. This difference is within one standard deviation of the entire difference in accuracy which means that the high inﬂuences that we detected capture the marginal utility of Sh. This shows that there is a large number of memorized training examples for which the only beneﬁt of memorization is the large increase in accuracy on individual test examples. This is well aligned with modeling used in the long tail theory where the label memorization on representatives of rare subpopulations signiﬁcantly increases the accuracy on those subpopulations [Fel19].
3.4 Examples of high-inﬂuence pairs
Remarkably, in most cases our estimates of inﬂuence are easy to interpret by humans. Very high inﬂuence scores (greater than 0.4) almost always correspond to near duplicates or images from a set of photos taken together. These are artifacts of the data collection methods and are particularly prominent in CIFAR-100 which has numerous near-duplicate images [BD20]. Naturally, such examples beneﬁt the most from memorization. Examples in pairs with lower inﬂuences (more than 80% of inﬂuences we found are below 0.4) are visually very similar but in most cases are not from the same set. We include examples of various inﬂuences for MNIST, CIFAR-100 and ImageNet in Figs. 4, 5 and 6, respectively. Additional examples can be found at [FZ20]. To select the presented examples, we ﬁrst sort the training examples in the high-inﬂuence pairs by the highest inﬂuence they have on a test example and then pick 3 consecutive sets each of 5 training examples with indices spread evenly in the sorted order (in particular, without any cherry-picking, see Sec.C for the exact description).
3.5 Estimation consistency and comparison of different architectures
We study the consistency of our estimation of memorization and inﬂuence under subset resampling (and randomness of the training algorithm) and also across different neural network architectures on CIFAR-100. All estimates are based on 2000 trials. To measure the consistency we consider Jaccard similarity coefﬁcient of the sets of examples that have memorization/inﬂuence estimate above a certain threshold and also average difference in estimates of examples in these sets. In particular, for memorization estimates memm(A, S, i) and memm(A , S, i), where A and A are training algorithms using two neural network architectures, we compare the estimates at each threshold θmem in the following

8

train test

train test

train test

Figure 4: Examples of inﬂuence estimates for memorized examples from MNIST. Left column is the memorized examples in the training set and their memorization estimates (above). For each training example 4 most inﬂuenced examples in the test set are given together with the inﬂuence estimates (above each image).

train test

train test

Figure 5: Additional examples of inﬂuence estimates for memorized examples from CIFAR-100. Format is as in Fig.4. 9

train test
Figure 6: Additional examples of inﬂuence estimates for memorized examples from ImageNet. Format is as in Fig.4. 10

way: let Imem(θmem) := {i : memm(A, S, i) ≥ θmem}, Imem(θmem) := {i : memm(A , S, i) ≥ θmem}, then
Dmem(θmem) := meani∈Imem(θmem)∪Imem(θmem) |memm(A, S, i) − memm(A , S, i)| (4)
measures the difference in memorization estimates between A and A at θmem. Similarly, the discrepancy for inﬂuence estimation at θinfl is measured by comparing inflm(A, S, i) and inflm(A , S, i) over the union of the two subsets:

Iinfl(θinfl) := {(i, j) : inflm(A, S, i, j) ≥ θinfl, memm(A, S, i) ≥ 0.25} and

Iinfl(θinfl) := {(i, j) : inflm(A , S, i, j) ≥ θinfl, memm(A , S, i) ≥ 0.25}.
Note we have an extra constraints of memorization estimate being above 0.25, which is used when we select highinﬂuence pairs.
Jaccard similarity coefﬁcient for these sets is deﬁned as:

J (θ ) := |Imem(θmem) ∩ Imem(θmem)|

mem mem

|Imem(θmem) ∪ Imem(θmem)|

and similarly,

J (θ ) := |Iinfl(θinfl) ∩ Iinfl(θinfl)| .

infl infl

|Iinfl(θinfl) ∪ Iinfl(θinfl)|

We compare ResNet50 with ResNet50 (independent runs with the same architecture), ResNet18, Inception [Sze+15], and DenseNet100 [HLVDMW17] in Fig. 7. The results show consistency in the estimation of both memorization and inﬂuence across different architectures.
We ﬁrst note that comparison of two different runs of the same architecture gives a sense of the accuracy of our estimates and the effect of selection bias. For memorization, the high consistency and almost non-existent selection bias are apparent. (Jaccard similarity is not very reliable when sets are relatively small and, as a result, we see a drop near threshold 1.0 even though there is almost no difference in the estimates). For inﬂuence estimates there is a clear drop in accuracy and consistency around threshold 0.1 which appears to be primarily due to the selection bias. At the same time, the average difference in estimates is still below 0.015. Our choice of inﬂuence threshold being 0.15 was in part guided by ensuring that Jaccard similarity for the chosen threshold is sufﬁciently high (above 0.7).
The plots also show high similarity between memorized examples and high-inﬂuence pairs computed for the different architectures. The difference in the produced estimates appears to be closely correlated with the difference in the accuracy of the architectures. This is expected since both memorization and inﬂuence estimates rely directly on the accuracy of the models. This suggests that our memorization estimates may not be very sensitive to variations in the architectures as long as they achieve similar accuracy.

3.6 Does the last layer sufﬁce for memorization?
Finally, we explore a natural approach to speed up the computation of our estimator. We train a ResNet50 model on the full CIFAR-100 training set and take the output of the penultimate layer as the representation for each example. Then, when training with subsets of size m, we start from the pre-trained representations and only learn a fresh linear classiﬁer on top of that from a random initialization. This reduces the training time by a factor of 720. The intuition is that, if label memorization mostly happens at the ﬁnal layer, then we could derive similar inﬂuence estimates much faster. In principle, this could be true, as the ﬁnal layer in many classiﬁcation networks is itself overparameterized (e.g. the ﬁnal layer of ResNet50 for CIFAR-100 has more than 200k parameters).
Our experimental results suggest that this intuition is wrong. Speciﬁcally, the 4,000 linear models trained using 70% training data achieve 75.8 ± 0.1% accuracy on the test set. In comparison, the ResNet50 model trained on the full training set, which is used to generate the representations, achieves 75.9% test accuracy, and the 4,000 ResNet50 models trained on 70% training data achieve only 72.3 ± 0.3% test accuracy. Moreover, there are only 38 training examples with memorization estimates above 0.25 using linear models, compared with 18,099 examples using full ResNet50 models. This suggests that most of the memorization is already present in the representation before reaching the ﬁnal layer. Namely, trained representations of memorized examples are close to those of other examples from the

11

Jaccard similarity

1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3
0.0 0.2 0.4 0.6 0.8 1.0 memorization threshold
1.0
0.8
0.6
0.4
0.2
0.0 0.0 0.2 0.4 0.6 0.8 1.0 influence threshold

average estimation difference

average estimation difference

0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000
0.0

ResNet50 (1062, 72%) ResNet18 (864, 70%) Inception (599, 67%) DenseNet100 (576, 65%)
0.2 0.4 0.6 0.8 1.0 memorization threshold

0.25

ResNet50 (1062, 72%)

ResNet18 (864, 70%)

0.20

Inception (599, 67%) DenseNet100 (576, 65%)

0.15

0.10

0.05

0.00 0.0 0.2 0.4 0.6 0.8 1.0 influence threshold

Jaccard similarity

Figure 7: Consistency of the estimation of memorization (top) and inﬂuence (bottom) across different architectures on CIFAR-100. In the average estimation difference we plot Dmem(θmem) and Dinfl(θinfl). Jaccard similarity plots are for Jmem(θmem) and Jinfl(θinfl). All the architectures are compared to ResNet50 — with the “ResNet50” entry being comparison between two independent runs of the same architecture. The numbers in the legend indicate the number of high-inﬂuence pairs selected by each architecture according to θinfl = 0.15 and θmem = 0.25, and the average test accuracy (with 70% training set), respectively.

12

Jaccard similarity average estimation difference

0.6 0.5 0.4 0.3 0.2 0.1 0.0
0.0 0.2 0.4 0.6 0.8 1.0 influence threshold

1.0 0.8 0.6 0.4 0.2 0.0
0.0 0.2 0.4 0.6 0.8 1.0 influence threshold

Figure 8: Consistency of the estimation of inﬂuence between ResNet50 and linear models trained on the penultimate layer representations computed on the entire CIFAR-100 dataset.

same class. Despite the lack of memorization, we still found 457 high-inﬂuence pairs of examples (as before, those with inﬂuence estimates above 0.15). In most of these pairs we see no visual similarity (although there is still a signiﬁcant fraction of those that are visually similar). In Fig. 8 we quantify the large discrepancy in estimates obtained using this technique and training of the full ResNet50 model. Speciﬁcally, we compare the inﬂuence estimates using the Jaccard similarity and average difference in estimate but without the additional constraint of having memorization value above 0.25 (since it is satisﬁed only by 38 examples).
4 Discussion
Our experiments provide the ﬁrst empirical investigation of memorization and its effects on accuracy that are based on formally deﬁned and intuitive criteria. The results reveal that, in addition to outliers and mislabeled examples, neural networks memorize training examples that signiﬁcantly improve accuracy on visually similar test examples. These pairs of examples are visually atypical and most train and test examples only appear in a single pair. This, we believe, strongly supports the long tail theory and, together with the results in [Fel19], provides the ﬁrst rigorous and compelling explanation for the propensity of deep learning algorithms to memorize seemingly useless labels: it is a result of (implicit) tuning of the algorithms for the highest accuracy on long-tailed and mostly noiseless data.
Our work demonstrates that accuracy of a learning algorithm on long tailed data distributions depends on its ability to memorize the labels. As can be seen from the results, the effect on the accuracy of not memorizing examples depends on the number of available examples and the data variability (a formal analysis of this dependence can be found in [Fel19]). This means that the effect on accuracy will be higher on an under-represented subpopulation. The immediate implication is that techniques that limit the ability of a learning system to memorize will have a disproportionate effect on under-represented subpopulations. Techniques aimed at optimizing the model size (e.g. model compression) or training time are likely to affect the ability of the learning algorithm to memorize data. This effect is already known in the context of differential privacy (which formally limits the ability of an algorithm to memorize data) [BPS19].
The experiments in Section 3.6 demonstrate that most of memorization happens in the representations derived by training a DNN. A natural direction for future work is to derive a detailed understanding of the process of memorization by a training algorithm.
The primary technical contribution of our work is the development of inﬂuence and memorization estimators that are simple to implement, computationally feasible, and essentially as accurate as true leave-one-out inﬂuences. While several other approaches for inﬂuence estimation exist, we believe that our approach provides substantially easier to interpret results. Unlike some of the existing techniques [KL17; YKYR18; PLSK20] it is also completely model-agnostic and is itself easy to explain. In addition to understanding of deep learning, inﬂuence estimation is useful for interpretability and outlier detection and, we hope, our estimator will ﬁnd applications in these areas.

13

Computing our estimator with high accuracy relies on training thousands of models and thus requires signiﬁcant computational resources. A natural direction for future work is ﬁnding proxies for our estimator that can be computed more efﬁciently. To simplify future research in this direction and other applications of our estimator, we provide the computed values for CIFAR-100 and ImageNet at [FZ20].

References

[Aba+15] [Arp+17]
[BD20] [BHM18]
[BHX19] [BLLT19] [BMM18]
[BPS19] [BRT18] [BS19] [CEP19] [CW82] [Fel19]
[FZ20] [HLVDMW17]
[HMRT19] [HZRS16]

M. Abadi et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorﬂow.org. 2015. URL: https://www.tensorflow.org/.
D. Arpit, S. Jastrzkebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio, et al. “A closer look at memorization in deep networks”. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org. 2017, pp. 233–242.
B. Barz and J. Denzler. “Do We Train on Test Data? Purging CIFAR of Near-Duplicates”. In: Journal of Imaging 6.6 (2020), p. 41.
M. Belkin, D. J. Hsu, and P. Mitra. “Overﬁtting or perfect ﬁtting? risk bounds for classiﬁcation and regression rules that interpolate”. In: Advances in Neural Information Processing Systems. 2018, pp. 2300–2311.
M. Belkin, D. Hsu, and J. Xu. “Two models of double descent for weak features”. In: arXiv preprint arXiv:1903.07571 (2019).
P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. “Benign Overﬁtting in Linear Regression”. In: arXiv preprint arXiv:1906.11300 (2019).
M. Belkin, S. Ma, and S. Mandal. “To Understand Deep Learning We Need to Understand Kernel Learning”. In: ICML. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 541–549. URL: http://proceedings.mlr.press/v80/belkin18a.html.
E. Bagdasaryan, O. Poursaeed, and V. Shmatikov. “Differential privacy has disparate impact on model accuracy”. In: Advances in Neural Information Processing Systems. 2019, pp. 15453–15462.
M. Belkin, A. Rakhlin, and A. B. Tsybakov. “Does data interpolation contradict statistical optimality?” In: arXiv preprint arXiv:1806.09471 (2018).
R. Babbar and B. Scho¨lkopf. “Data scarcity, robustness and extreme multi-label classiﬁcation”. In: Machine Learning (2019). N. Carlini, U´ . Erlingsson, and N. Papernot. “Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications”. In: arXiv preprint arXiv:1910.13427 (2019).
R. D. Cook and S. Weisberg. Residuals and inﬂuence in regression. New York: Chapman and Hall, 1982.
V. Feldman. “Does Learning Require Memorization? A Short Tale about a Long Tail”. In: CoRR abs/1906.05271 (2019). Extended abstract in STOC 2020. arXiv: 1906.05271. URL: http://arxiv. org/abs/1906.05271.
V. Feldman and C. Zhang. Additional Material. https : / / pluskid . github . io / influence memorization/. 2020.
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. “Densely connected convolutional networks”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, pp. 4700–4708.
T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation”. In: arXiv preprint arXiv:1903.08560 (2019).
K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learning for Image Recognition”. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016, pp. 770–778.

14

[JNMKB20] [JZTM20] [KL17] [LR18] [MVS19] [NBMS17] [NK19] [PLSK20] [RZ19] [SSSS17] [Sze+15] [TSCTBG19] [VHP17] [WOBM17] [YKYR18] [ZAR14] [ZBHRV17]

Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. “Fantastic Generalization Measures and Where to Find Them”. In: ICLR. 2020. URL: https://openreview.net/forum?id= SJgIPJBFvH.
Z. Jiang, C. Zhang, K. Talwar, and M. C. Mozer. “Characterizing Structural Regularities of Labeled Data in Overparameterized Models”. In: CoRR abs/2002.03206 (2020). arXiv: 2002.03206. URL: https://arxiv.org/abs/2002.03206.
P. W. Koh and P. Liang. “Understanding black-box predictions via inﬂuence functions”. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org. 2017, pp. 1885–1894.
T. Liang and A. Rakhlin. “Just interpolate: Kernel” ridgeless” regression can generalize”. In: arXiv preprint arXiv:1808.00387 (2018).
V. Muthukumar, K. Vodrahalli, and A. Sahai. “Harmless interpolation of noisy data in regression”. In: arXiv preprint arXiv:1903.09139 (2019).
B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. “Exploring generalization in deep learning”. In: Advances in Neural Information Processing Systems. 2017, pp. 5947–5956.
V. Nagarajan and J. Z. Kolter. “Uniform convergence may be unable to explain generalization in deep learning”. In: Advances in Neural Information Processing Systems. 2019, pp. 11611–11622.
G. Pruthi, F. Liu, M. Sundararajan, and S. Kale. Estimating Training Data Inﬂuence by Tracking Gradient Descent. 2020. arXiv: 2002.08484 [cs.LG].
A. Rakhlin and X. Zhai. “Consistency of Interpolation with Laplace Kernels is a High-Dimensional Phenomenon”. In: COLT. Vol. 99. PMLR, 2019, pp. 2595–2623. URL: http://proceedings.mlr. press/v99/rakhlin19a.html.
R. Shokri, M. Stronati, C. Song, and V. Shmatikov. “Membership Inference Attacks Against Machine Learning Models”. In: 2017 IEEE Symposium on Security and Privacy, SP 2017. 2017, pp. 3–18.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. “Going deeper with convolutions”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2015, pp. 1–9.
M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, and G. J. Gordon. “An Empirical Study of Example Forgetting during Deep Neural Network Learning”. In: ICLR. OpenReview.net, 2019. URL: https://openreview.net/forum?id=BJlxm30cKm.
G. Van Horn and P. Perona. “The devil is in the tails: Fine-grained classiﬁcation in the wild”. In: arXiv preprint arXiv:1709.01450 (2017).
A. J. Wyner, M. Olson, J. Bleich, and D. Mease. “Explaining the success of adaboost and random forests as interpolating classiﬁers”. In: The Journal of Machine Learning Research 18.1 (2017), pp. 1558–1590.
C.-K. Yeh, J. Kim, I. E.-H. Yen, and P. K. Ravikumar. “Representer point selection for explaining deep neural networks”. In: Advances in Neural Information Processing Systems. 2018, pp. 9291–9301.
X. Zhu, D. Anguelov, and D. Ramanan. “Capturing long-tail distributions of object subcategories”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014, pp. 915– 922.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. “Understanding deep learning requires rethinking generalization”. In: ICLR. 2017. URL: https://openreview.net/forum?id=Sy8gdB9xx.

15

A Proof of Lemma 2.1

Proof. Observe that, by linearity of expectation, we have:

inflm(A, S, i, z) =

E

infl(A, SI∪{i}, i, z)

I∼P ([n]\{i},m−1)

=

E

Pr [h(x) = y] − Pr [h(x) = y]

I∼P ([n]\{i},m−1) h←A(SI∪{i})

h←A(SI )

=

Pr

[h(x) = y] −

Pr

[h(x) = y]

I∼P ([n]\{i},m−1), h←A(SI∪{i})

I∼P ([n]\{i},m−1), h←A(SI )

By deﬁnition, the distribution of I ∪ {i} for I sampled from P ([n] \ {i}, m − 1) is the same as the distribution of J sampled from P ([n], m) and conditioned on the index i being included in the set of indices J. As a result, the ﬁrst term that we need to estimate is equal to

αi,1 :=

Pr

[h(x) = y] =

Pr

[h(x) = y | i ∈ J].

I∼P ([n]\{i},m−1), h←A(SI∪{i})

J∼P ([n],m), h←A(SJ )

This implies that instead of sampling from P ([n] \ {i}, m − 1) for every i separately, we can use samples from P ([n], m), select the samples for which J contains i and evaluate this term on them.
Speciﬁcally, given J1, . . . , Jt/2 sampled randomly from P ([n], m) we use A to train models h1, . . . , ht/2 on each of the dataset SJ1 , . . . , SJt/2 . Now for every i, we can estimate αi,1 as

µi,1 = |{k ∈ [t/2] : i ∈ Jk, hk(x) = y}| , |{k ∈ [t/2] : i ∈ Jk}|

or set µi,1 = 1/2 if the denominator is equal to 0. Similarly, the distribution of I sampled from P ([n] \ {i}, m − 1) is the same as the distribution of J sampled
from P ([n], m − 1) and conditioned on the index i being excluded from J. Therefore the second term that we need to estimate is equal to:

αi,2 :=

Pr

[h(x) = y] =

Pr

[h(x) = y | i ∈ J].

I∼P ([n]\{i},m−1), h←A(SI )

J∼P ([n],m−1), h←A(SJ )

This means that we can estimate the second term analogously by sampling Jt/2+1, . . . , Jt from P ([n], m − 1), using A to train models ht/2+1, . . . , ht on each of the resulting subsets and then estimating the second term as

µi,2 = |{t/2 + 1 ≤ k ≤ t : i ∈ Jk, hk(x) = y}| , |{t/2 + 1 ≤ k ≤ t : i ∈ Jk}|

or set µi,2 = 1/2 if the denominator is equal to 0. The ﬁnal estimator is deﬁned for every i ∈ [n] as µi = µi,1 − µi,2.

We now compute the expected squared error of each of the terms of this estimator. For µi,1 we consider two cases.

The case in which the denominator |{k ∈ [t/2]

:

i ∈ Jk}| is equal to at least

mt 4n

and the case in which the denominator

is less than m4nt . In the ﬁrst case we are effectively estimating the mean of a Bernoulli random variable using the

empirical mean of least m4nt independent samples. The expectation of each of the random variables is exactly equal to

αi,1 and thus the squared error is exactly the variance of the empirical mean. For a Bernoulli random variable this means

that it is equal to at most 4nαi,1m(1t−αi,1) ≤ mnt . For the second case, note that for every k, i ∈ Jk with probability m/n.

Therefore the multiplicative form of the Chernoff bound for the sum of t/2 independent Bernoulli random variables

implies

that

the

probability

of

this

case

is

at

most

e−

mt 16n

.

Also

note

that

in

this

case

we

are

either

estimating

the

mean

using < m4nt independent samples or using the ﬁxed value 1/2. In both cases the squared error is at most 1/4. Thus

E[(αi,1 − µi,1)2] ≤ e− 1m6nt + n . 4 mt

An analogous argument for the second term gives

e−

(n−m+1)t 16n

E[(αi,2 − µi,2)2] ≤

+

n .

4

(m − n + 1)t

16

By combining these estimates we obtain that

E[(inflm(A, S, i, z) − µi)2] ≤ E[(αi,1 − µi,1)2] + E[(αi,2 − µi,2)2]

n

n

e−

mt 16n

e−

(n−m+1)t 16n

≤+

+

+

mt (m − n + 1)t 4

4

1

1

e−pt/16

≤+

+

,

pt (1 − p)t

2

where we used that p = min(m/n, 1 − m/n).

Remark A.1. In practice, models trained on random subsets of size m − 1 are essentially identical to models trained on random subsets of size m. Thus, in our implementation we improve the efﬁciency by a factor of 2 by only training models on subsets of size m. Our estimator also beneﬁts from the fact that for most examples, the variance of each sample αi,1(1 − αi,1) (or α2,1(1 − α2,1)) is much smaller than 1/4. Finally, it is easy to see that the estimator is also strongly concentrated around inflm(A, S, i, z) and the concentration result follows immediately from the concentration of sums of independent Bernoulli random variables.

B Details of the Experimental Setup
We implement our algorithms with Tensorﬂow [Aba+15]. We use single NVidia® Tesla P100 GPU to for most of the training jobs, except for ImageNet, where we use 8 P100 GPUs with single-node multi-GPU data parallelization.
We use ResNet50 [HZRS16] in both ImageNet and CIFAR-100 experiments, which is a Residual Network architecture widely used in the computer vision community [HZRS16]. Because CIFAR-100 images (32 × 32) are smaller than ImageNet images (224 × 224), for CIFAR-100 we replace the ﬁrst two layers (a convolution layer with 7 × 7 kernel and 2 × 2 stride, and a max pooling layer with 3 × 3 kernel and 2 × 2 stride) with a single convolution layer with 3 × 3 kernel and 1 × 1 stride. We use data augmentation with random padded (4 pixels for CIFAR-100 and 32 pixels for ImageNet) cropping and random left-right ﬂipping during training. For MNIST, we use a simpliﬁed Inception [Sze+15] model as described in [ZBHRV17].
We use stochastic gradient descent (SGD) with momentum 0.9 to train the models. For ImageNet, we use batch size 896 and base learning rate 0.7. During the 100 training epochs, the learning rate is scheduled to grow linearly from 0 to the maximum value (the base learning rate) during the ﬁrst 15 epochs, then it remains piecewise constant, with a 10× decay at epoch 30, 60 and 90, respectively. Our implementation achieves ≈ 73% top-1 accuracy when trained on the full training set.
We also use SGD with momentum 0.9 for CIFAR-100 training. To achieve faster training, we use slightly larger batch size (512) and base learning rate (0.4) than usual. During the 160 training epochs, the learning rate is scheduled to grow linearly from 0 to the maximum value (base learning rate) in the ﬁrst 15% iterations, and then decay linearly back to 0 in the remaining iterations. Our implementation achieves ≈ 76% top-1 accuracy when trained on the full training set. In the experiment on the estimation consistency, we also trained CIFAR-100 on a number of different architectures. ResNet18 and Inception are trained using exactly the same hyper-parameter conﬁguration as described above. For DenseNet, we halved the batch size and learning rate due to higher memory load of the architecture. The linear models on pre-computed hidden representations are also trained using the same hyper-parameter as ResNet50, except they train for only 40 epochs due to fast convergence.
For MNIST, we use SGD with momentum 0.9 and the same learning rate scheduler as the CIFAR-100 experiment, with base learning rate 0.1. We train the models for 30 epochs with batch size 256.
Our ImageNet training jobs takes about half an hour for each training epoch. On CIFAR-100, the training time per epoch is about: 1 minute and 30 seconds for ResNet50, 17 seconds for ResNet18, 45 seconds for DenseNet100, 14 seconds for Inception, and 0.5 second for Linear models on pre-computed hidden representations. Our training time on MNIST is about 7 seconds per epoch.
Our architectures and training algorithms are not state-of-the-art since state-of-the-art training is signiﬁcantly more computationally intensive and it would not be feasible for us to train thousands of models.

17

C Selection Procedure for Examples of Inﬂuence Estimates
For our inﬂuence ﬁgures, to avoid cherry-picking, we select the training examples to include as follows. We ﬁrst sort the training examples in the selected high-inﬂuence pairs by the highest inﬂuence they have on a test example. We then pick 3 consecutive sets each of 5 training examples with indices spread evenly in the sorted order. The exact Python code is included below. n_copies = 3 n_egs = 5 idx_sort_selected = np.argsort(-max_infl_of_train_selected) base_idxs = np.linspace(0, len(idx_train_selected) - n_copies, n_egs).astype(np.int) for i_copy in range(n_copies):
idxs_to_depict = [idx_train_selected[idx_sort_selected[x + i_copy]] for x in base_idxs]
visualize_tr_examples_and_influence(idxs_to_depict, n_test_egs=4)
18

