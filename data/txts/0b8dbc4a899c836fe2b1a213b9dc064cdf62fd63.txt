What-if I ask you to explain: Explaining the effects of perturbations in procedural text
Dheeraj Rajagopal1, Niket Tandon2, Bhavana Dalvi2, Peter Clark2, Eduard Hovy1
1Carnegie Mellon University 2Allen Institute for Artiﬁcial Intelligence {dheeraj,hovy}@cs.cmu.edu {niket,bhavanad,peterc}@allenai.org

arXiv:2005.01526v2 [cs.CL] 8 Oct 2020

Abstract
Our goal is to explain the effects of perturbations in procedural text, e.g., given a passage describing a rabbit’s life cycle, explain why illness (the perturbation) may reduce the rabbit population (the effect). Although modern systems are able to solve the original prediction task well (e.g., illness results in less rabbits), the explanation task - identifying the causal chain of events from perturbation to effect - remains largely unaddressed, and is the goal of this research. We present QUARTET, a system that constructs such explanations from paragraphs, by modeling the explanation task as a multitask learning problem. QUARTET constructs explanations from the sentences in the procedural text, achieving ∼ 18 points better on explanation accuracy compared to several strong baselines on a recent process comprehension benchmark. On an end task on this benchmark, we show a surprising ﬁnding that good explanations do not have to come at the expense of end task performance, in fact leading to a 7% F1 improvement over SOTA.
1 Introduction
Procedural text is common in natural language (in recipes, how-to guides, etc.) and ﬁnds many applications such as automatic execution of biology experiments (Mysore et al., 2019), cooking recipes (Bollini et al., 2012) and everyday activities (Yang and Nyberg, 2015). However, the goal of procedural text understanding in these settings remains a major challenge and requires two key abilities, (i) understanding the dynamics of the world inside a procedure by tracking entities and what events happen as the narrative unfolds. (ii) understanding the dynamics of the world outside the procedure that can inﬂuence the procedure.
While recent systems for procedural text comprehension have focused on understanding the dynamics of the world inside the process, such as tracking

Figure 1: Given a procedural text, the task is to explain the effect of the perturbation using the input sentences.
entities and answering questions about what events happen, e.g., (Tandon et al., 2018; Bosselut et al., 2018; Henaff et al., 2017), the extent to which they understand the inﬂuences of outside events remains unclear. In particular, if a system fully understands a process, it should be able to predict what would happen if it was perturbed in some way due to an event from the outside world. Such counterfactual reasoning is particularly challenging because, rather than asking what happened (described in text), it asks about what would happen in an alternative world where the change occurred.
Recently, Tandon et al. (2019) introduced the WIQA dataset that contains such problems, requiring prediction of the effect of perturbations in a procedural text. They also presented several strong models on this task. However, it is unclear whether those high scores indicate that the mod-

els fully understand the described procedures, i.e., that the models have knowledge of the causal chain from perturbation to effect. To test this, Tandon et al. (2019) also proposed an explanation task. While the general problem of synthesizing explanations is hard, they proposed a simpliﬁed version in which explanations were instead assembled from sentences in the input paragraph and qualitative indicators (more/less/unchanged). Although they introduced this explanation task and dataset, they did not present a model to address it. We ﬁll this gap by proposing the ﬁrst solution to this task.
We present a model, QUARTET (QUAlitative Reasoning wiTh ExplanaTions) that takes as input a passage and a perturbation, and its qualitative effect. The output contains the qualitative effect and an explanation structure over the passage. See Figure 1 for an example. The explanation structure includes up to two supporting sentences from the procedural text, together with the qualitative effect of the perturbation on the supporting sentences (more of or less of in Figure 1). QUARTET models this qualitative reasoning task as a multitask learning problem to explain the effect of a perturbation.
Our main contributions are:
• We present the ﬁrst model that explains the effects of perturbations in procedural text. On a recent process comprehension benchmark, QUARTET generates better explanations compared to strong baselines.
• On an end task on this benchmark, we show a ﬁnding that good explanations do not have to come at the expense of end task performance, in fact leading to a 7% F1 improvement over SOTA. (refer §6). Prior work has found that optimizing for explanation can hurt end-task performance. Ours is a useful datapoint showing that good explanations do not have to come at the expense of end-task performance1.
2 Related work
Procedural text understanding: Machine reading has seen tremendous progress. With machines reaching human performance in standard QA benchmarks (Devlin et al., 2018; Rajpurkar et al., 2016), more challenging datasets have been proposed (Dua et al., 2019) that require background knowledge, commonsense reasoning (Talmor et al., 2019) and visual reasoning (Antol et al., 2015;
1All the code will be publicly shared upon acceptance

Zellers et al., 2018). In the context of procedural text understanding which has gained considerable amount of attention recently, (Bosselut et al., 2018; Henaff et al., 2017; Dalvi et al., 2018) address the task of tracking entity states throughout the text. Recently, (Tandon et al., 2019) introduced the WIQA task to predict the effect of perturbations.
Understanding the effects of perturbations, speciﬁcally, qualitative change, has been studied using formal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense reasoning. However, existing models do not explain the effects of perturbations.
Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reﬂect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017).
Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions.
Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018), and multihop reading comprehension (HotpotQA dataset) (Yang et al., 2018). In contrast to these datasets, we explain the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial modiﬁcation for the following reasons: (i) HotpotQA models are not trained to predict the qualitative structure (more or less of chosen explanation sentences in Figure 1). (ii) HotpotQA involves reasoning over named entities, whereas the current task focuses on common nouns and actions (models that work well on named entities need to be adapted to common nouns and actions (Sedghi and Sabharwal, 2018)). (iii) explanation paragraphs in HotpotQA are not

ears less protected → (MORE/+) sound enters the ear → (MORE/+) sound hits ear drum → (MORE/+) more sound detected blood clotting disorder → (LESS/-) blood clots → (LESS/-) scab forms → (MORE/+) less scab formation breathing exercise → (MORE/+) air enters lungs → (MORE/+) air enters windpipe → (MORE/+) oxygen enters bloodstream squirrels store food → (MORE/+) squirrels eat more → (MORE/+) squirrels gain weight → (MORE/+) hard survival in winter less trucks run → (LESS/-) trucks go to reﬁneries → (LESS/-) trucks carry oil → (MORE/+) less fuel in gas stations coal is expensive → (LESS/-) coal burns → (LESS/-) heat produced from coal → (LESS/-) electricity produced legible address → (MORE/+) mailman reads address → (MORE/+) mail reaches destination → (MORE/+) on-time delivery more water to roots → (MORE/+) root attract water → MORE/+) roots suck up water → (LESS/-) plants malnourished in a quiet place → (LESS/-) sound enters the ear → (LESS/-) sound hits ear drum → (LESS/-) more sound detected eagle hungry → (MORE/+) eagle swoops down → (MORE/+) eagle catches mouse → (MORE/+) eagle gets more food
Table 1: Examples of our model’s predictions on the dev. set in the format: “qp → di xi → dj xj → de qe”. Supporting sentences xi, xj are compressed e.g., “the person has his ears less protected” → “ears less protected”

procedural while the current input is procedural in nature with a speciﬁc chronological structure.
Another line of work provides more structure and organization to explanations, e.g., using scene graphs in computer vision (Ghosh et al., 2019). For elementary science questions, Jansen et al. (2018) uses a science knowledge graph. These approaches rely on a knowledge structure or graph but knowledge graphs are incomplete and costly to construct for every domain (Weikum and Theobald, 2010). There are trade-offs between unstructured and structured explanations. Unstructured explanations are available abundantly while structured explanations need to be constructed and hence are less scalable (Camburu et al., 2018). Generating free-form (unstructured) explanations is difﬁcult to evaluate (Cui et al., 2018; Zhang et al., 2019), and adding qualitative structure over them is nontrivial. Taking a middle ground between free-form and knowledge graphs based explanations, we infer a qualitative structure over the sentences in the paragraph. This retains the rich interpretability and simpler evaluation of structured explanations as well as leverages the large-scale availability of sentences required for these explanation.
It is an open research problem whether requiring explanation helps or hurts the original task being explained. On the natural language inference task (e-SNLI), Camburu et al. (2018) observed that models generate correct explanations at the expense of good performance. On the Cos-E task, recently Rajani et al. (2019) showed that explanations help the end-task. Our work extends along this line in a new task setting that involves perturbations and enriches natural language explanations with qualitative structure.

3 Problem deﬁnition
We adopt the problem deﬁnition described in Tandon et al. (2019), and summarize it here.
Input: 1. Procedural text with steps x1 . . . xK. Here, xk denotes step k (i.e., a sentence) in a procedural text comprising K steps. 2. A perturbation qp to the procedural text and its likely candidate effect qe.
Output: An explanation structure that explains the effect of the perturbation qp:
qp → dixi → djxj → deqe
• i: step id for the ﬁrst supporting sentence.
• j: step id for the second supporting sentence.
• di ∈ {+ − • }: how step id i is affected. • dj ∈ {+ − • }: how step id j is affected. • de ∈ {+ − • }: how qe is affected.
See Figure 1 for an example of the task, and Table 1 for examples of explanations.
An explanation consists of up to two (i.e., zero, one or two) supporting sentences i, j along with their qualitative directions di, dj. If there is only one supporting sentence, then j = i. If de = • , then i =Ø, j =Ø (there is no valid explanation for no-effect).
While there can be potentially many correct explanation paths in a passage, the WIQA dataset consists of only one gold explanation considered best by human annotators. Our task is to predict that particular gold explanation.
Assumptions: In a procedural text, steps x1 . . . xK are chronologically ordered and have a forward ﬂowing effect i.e., if j > i then more/increase of xi will result in more/increase of xj. Prior work on procedural text makes a similar assumption (Dalvi et al., 2018). Note that

this assumption does not hold for cyclic processes, and cyclic processes have already been ﬂattened in WIQA dataset. We make the following observations based on this forward-ﬂow assumption.
a1: i <= j (forward-ﬂow order) a2: dj = di (forward-ﬂow assumption)2
a3: For the WIQA task, de is the answer label because it is the end node in the explanation structure.
a4: If di = • then answer label = • (since qp does not affect qe, there is no valid explanation.)
a5: 1 ≤ i ≤ K; if di = •, then i = Ø (see a4)
a6: i ≤ j ≤ K; if de = •, then j = Ø (see a4)
This assumption reduces the number of predictions, removing dj and answer label (see a2, a3). Given x1 . . . xK, qp, qe the model must predict four labels: i, j, di, de .
4 QUARTET model
We can solve the problem as a classiﬁcation task, predicting four labels: i, j, di, de. If these predictions are performed independently, it requires several independent classiﬁcations and this can cause error propagation: prediction errors that are made in the initial stages cannot be ﬁxed and can propagate into larger errors later on (Goldberg, 2017).
To avoid this, QUARTET predicts and explains the effect of qp as a multitask learning problem, where the representation layer is shared across different tasks. We apply the widely used parameter sharing approach, where a single representation layer is followed by task speciﬁc output layers (Baxter, 1997). This reduces the risk of overﬁtting to a single task and allows decisions on i, j, di, de to inﬂuence each other in the hidden layers of the network. We ﬁrst describe our encoder and then the other layers on top, see Figure 2 for the model architecture.
Encoder: To encode x1 . . . xK and question q we use the BERT architecture (Devlin et al., 2018) that has achieved state-of-the-art performance across several NLP tasks (Clark et al., 2019),
2Note that this does not assume all sentences have the same directionality of inﬂuence. For example, a paragraph could include both positive and negative inﬂuences: “Predators arrive. Thus the rabbit population falls...”. Rather, the dj = di assumption is one of narrative coherence: the more predators arrive, the more the rabbit population falls. That is, within a paragraph, we assume enhancing one step will have enhanced effects (both positive or negative effects) on future steps - a property of a coherently authored paragraph.

where the question q = qp ⊕ qe (⊕ stands for concatenation). We start with a byte-pair tokenization (Sennrich et al., 2015) of the concatenated passage and question (x1 . . . xK ⊕ q) . Let [xk] denote the byte-pair tokens of sentence xk. The text is encoded as [CLS] [x1] [unused1] [SEP] [x2] [unused2] [SEP] .. [q] [SEP]. Here, [CLS] indicates a special classiﬁcation token. [SEP] and [unused1..K] are special next sentence prediction tokens.
These byte-pair tokens are passed through a 12layered Transformer network, resulting in a contextualized representation for every byte-pair token. In this contextualized representation, the vector u = [u1, ...uK, uq] where uk denotes the encoding for [xk], and uq denotes question encoding. Let El be the embedding size resulting from lth transformer layer. In that lth layer, [u1, ...uK] ∈ RK∗El. The hidden representation of all transformer layers are initialized with weights from a self-supervised pre-training phase, in line with contemporary research that uses pre-trained language models (Devlin et al., 2018).
To compute the ﬁnal logits, we add a linear layer over the different transformer layers in BERT that are individual winners for individual tasks in our multitask problem. For instance, out of the total 12 transformer layers, lower layers (layer 2) are the best predictors for [i, j] while upper layers (layer 10 and 11) are the best performing predictors for [di, de]. Zhang et al. (2019) found that the last layer is not necessarily the best performing layer. Different layers seem to learn complementary information because their fusion helps. Combining different layers by weighted averaging of the layers has been attempted with mixed success (Zhang et al., 2019; Clark et al., 2019). We observed the same trend for simple weighted transformation. However, we found that learning a linear layer over concatenated features from winning layers improves performance. This is probably because there is very different information encoded in a particular dimension across different layers, and the concatenation preserves it better than simple weighted averaging.
Classiﬁcation tasks: To predict the ﬁrst supporting sentence xi, we obtain a softmax distribution si ∈ RK over [u1, ...uK]. From the forward-ﬂow assumption made in the problem deﬁnition section earlier, we know that i ≤ j, making it possible to model this as a span prediction xi:j. Inline with standard span based prediction models (Seo et al.,

Figure 2: QUARTET model. Input: Concatenated passage and question using standard BERT word-piece tokenization. Representation Layer: The input is encoded using BERT transformer. We obtain [CLS] and sentence level representations. Prediction: From the sentence level representation, we use an MLP to model the distributions for i and j (using attended sentence representation). From [CLS] representation, we use MLP for di (and dj, since di = dj) and de distributions. Output: Softmax to predict {i, j, di, dj, de}

2017), we use an attended sentence representation (si [u1, ...uK]) ⊕ ([u1, ...uK]) ∈ RK∗2El to predict a softmax distribution sj ∈ RK to obtain xj. Here, denotes element-wise multiplication and ⊕ denotes concatenation.
For classiﬁcation of di (and dj, since di = dj), we use the representation of the ﬁrst token (i.e., CLS token ∈ REl) and a linear layer followed by softmax to predict di ∈ { + − • }. Classiﬁcation of de is performed in exactly the same manner.
The network is trained end-to-end to minimize the sum of cross-entropy losses for the individual classiﬁcation tasks i, j, di, de. At prediction time, we leverage assumptions (a4, a5, a6) to generate consistent predictions.
5 Experiments
Dataset: We train and evaluate QUARTET on the recently published WIQA dataset 3 comprising of 30,099 questions from 2107 paragraphs with explanations (23K train, 5K dev, 2.5K test). The perturbations qp are either linguistic variation (17% examples) of a passage sentence (these are called in-para questions) or require commonsense reasoning to connect to a passage sentence (41% examples) (called, out-of-para questions). Explanations are supported by up to two sentences from the pas-
3WIQA dataset link: http://data.allenai.org/wiqa/

sage: 52.7% length 2, 5.5% length 1, 41.8% length 0. Length zero explanations indicate that de =• (called, no-effect questions), and ensure that random guessing on explanations gets low score on the end task.

Metrics: We evaluate on both explainability and

the downstream end task (QA). For explainabil-

ity, we deﬁne explanation accuracy as the aver-

age accuracy of the four components of the ex-

planation:

accexpl

=

1 4

∗

i∈{i,j,di,de} acc(i) and

accqa = acc(de) (by assumption a3). The QA task

is measured in terms of accuracy.

Hyperparameters: QUARTET ﬁne-tunes BERT, allowing us to re-use the same hyperparameters as BERT with small adjustments in the recommended range (Devlin et al., 2018). We use the BERT-baseuncased version with a hidden size of 768. We use the standard adam optimizer with a learning rate 1e-05, weight decay 0.01, and dropout 0.2 across all the layers4. All the models are trained on an NVIDIA V-100 GPU.

Models: We measure the performance of the following baselines (two non-neural and three neural). • RANDOM: Randomly predicts one of the three labels {+ − • } to guess [di, de]. Supporting sentences i and j are picked randomly from |avgsent|
4Hyperparameter search details in appendix §9.1

sentences.

• MAJORITY: Predicts the most frequent label (no

effect i.e. de=• in the case of WIQA dataset.) • qeONLY : Inspired by existing works (Gururan-

gan et al., 2018), this baseline exploits annotation

artifacts (if any) in the explanation dataset by re-

training QUARTET using only qe while hiding the permutation qp in the question. • HUMAN upper bound (Krippendorff’s alpha inter-

annotator values on [i, j, di]) on explainability reported in (Tandon et al., 2019)5.

• TAGGING: We can reduce our task to a

structured prediction task. An explanation

i, j, di, de requires span prediction xi:j and

labels on that span. So, for example, the

explanation i = 1, j = 2, di =+, dj =− for input x1 · x5 can be expressed as a tag

sequence:

B-CORRECT E-OPPOSITE O

O O. Explanation i = 2, j = 4, di =+, dj =− would be expressed as: O B-CORRECT I-CORRECT E-OPPOSITE O. When de

= • , then the tag sequence will O O O O

O. This BIEO tagging scheme has seven

labels T = {B-CORRECT, I-CORRECT,

B-OPPOSITE, I-OPPOSITE,

E-CORRECT, E-OPPOSITE, O}.

Formulating as a sequence tagging task allows

us to use any standard sequence tagging model

such as CRF as baseline. The decoder invalidates

sequences that violate assumptions (a3 - a6). To

make the encoder strong and yet comparable to our

model, we use exactly the same BERT encoder as

QUARTET. For each sentence representation uk, we predict a tag ∈ T . A CRF over these local pre-

dictions additionally provides global consistency.

The model is trained end-to-end by minimizing the

negative log likelihood from the CRF layer.

• BERT-NO-EXPL: State-of-the-art BERT model

(Tandon et al., 2019) that only predicts the ﬁnal

answer de, but cannot predict the explanation. • BERT-W/-EXPL: A standard BERT based ap-

proach to the explanation task that predicts the

explanation structure. This model minimizes only

the cross-entropy loss of the ﬁnal answer de, pre-

dicting an explanation that provides the best an-

swer accuracy.

• DATAAUG: This baseline is adapted from Asai

and Hajishirzi (2020), where a RoBERTa model

is augmented with symbolic knowledge and uses

an additional consistency-based regularizer. Com-

5https://allenai.org/data/wiqa

pared to our model, this approach uses a more robustly pre-trained BERT (RoBERTa) with dataaugmentation optimized for QA Accuracy. • QUARTET: our model described in §4 that optimizes for the best explanation structure.

5.1 Explanation accuracy
QUARTET is also the best model on explanation accuracy. Table 2 shows the performance on [i, j, di, de]. QUARTET also outperforms baselines on every component of the explanation. QUARTET performs better at predicting i than j. This trend correlates with human performance- picking on the second supporting sentence is harder because in a procedural text neighboring steps can have similar effects.
We found that the explanation dataset does not contain substantial annotation artifacts for the qeONLY model to leverage (qeONLY < MAJORITY)
Table 1 presents canonical examples of QUARTET dev predictions.

RANDOM qe O N LY MAJORITY TAGGING BERT-W/-EXPL QUARTET HUMAN

acci 12.50 32.77 41.80 42.26 38.66 69.24 75.90

accj 12.50 32.77 41.80 37.03 38.66 65.97 66.10

accdi 33.33 33.50 41.80 56.74 69.20 75.92 88.20

accde 33.33 44.82 41.80 58.34 75.06 82.07 96.30

accexpl 22.91 36.00 41.80 48.59 55.40 73.30 81.63

Table 2: Accuracy of the explanation structure (i, j, di, de). Overall explanation accuracy is accexpl. (Note that BERT-NO-EXPL and DATAAUG do not produce explanations).

We also tried a simple bag of words and embedding vector based alignment between qp and xi in order to pick the most similar xi. These baselines perform worse than random, showing that aligning qp and xi involves commonsense reasoning that the these models cannot address.
6 Downstream Task
In this section, we investigate whether a good explanation structure leads to better end-task performance. QUARTET advocates explanations as a ﬁrst class citizen from which an answer can be derived.
6.1 Accuracy on a QA task
We compare against the existing SOTA on WIQA no-explanation task. Table 3 shows that QUARTET improves over the previous SOTA BERT-NO-EXPL

by 7%, achieving a new SOTA results. Both these models are trained on the same dataset6. The major difference between BERT-NO-EXPL and QUARTET is that BERT-NO-EXPL solves only the QA task, whereas QUARTET solves explanations, and the answer to the QA task is derived from the explanation. Multi-tasking (i.e., explaining the answer) provides the gains to QUARTET.

RANDOM MAJORITY qe O N LY TAGGING BERT-NO-EXPL BERT-W/-EXPL DATAAUG QUARTET HUMAN

QA accuracy 33.33 41.80 44.82 58.34 75.19 75.06 78.50 82.07 96.30

Table 3: QUARTET improves accuracy on the QA (end task) by 7% points.

All the models get strong improvements over RANDOM and MAJORITY. The least performing model is TAGGING. The space of possible sequences of correct labels is large, and we believe that the current training data is sparse, so a larger training data might help. QUARTET avoids this sparsity problem because rather than a sequence it learns on four separate explanation components.
Table 4 presents the accuracy based on question types. QUARTET achieves large gains over BERTNO-EXPL on the most challenging out-of-para questions. This suggests that QUARTET improves the alignment of qp and xi that involves some commonsense reasoning.
6.2 Correlation between QA and Explanation
QUARTET not only improves QA accuracy but also the explanation accuracy. We ﬁnd that QA accuracy (accde in Table 2) is positively correlated (Pearson coeff. 0.98) with explanation accuracy (accexpl). This shows that if a model is optimized for explanations, it leads to better performance on end-task.
6We used the same code and parameters as provided by the authors of WIQA-BERT. The WIQA with-explanations dataset has about 20% fewer examples than WIQA withoutexplanations dataset [http://data.allenai.org/wiqa/] This is because the authors removed about 20% instances with incorrect explanations (e.g., where turkers didn’t have an agreement). So we trained both QUARTET and WIQA-BERT on exactly the same vetted dataset. This helped to increase the score of WIQA-BERT by 1.5 points.

Model
RANDOM MAJORITY qe O N LY BERT-NO-EXPL BERT-W/-EXPL QUARTET

in-para
33.33 00.00 20.38 71.40 72.83 73.49

out-of para 33.33 00.00 20.85 53.56 58.54 65.65

no-effect
33.33 100.0 78.41 90.04 92.03 95.30

overall
33.33 41.80 44.82 75.19 75.06 82.07

Table 4: QUARTET improves accuracy over SOTA BERT-NO-EXPL across question types.

Thus, with this result we establish that (at least on our task) models can make better predictions when forced to generate a sensible explanation structure. An educational psychology study (Dunlosky et al., 2013) hypothesizes that student performance improves when they are asked to explain while learning. However, their hypothesis is not conclusively validated due to lack of evidence. Results in Table 2 hint that, at least on our task, machines that learn to explain, ace the end task.

7 Error analysis
We analyze our model’s errors (marked in red) over the dev set, and observe the following phenomena.
1. Multiple explanations: As mentioned in Section 3, more than one explanations can be correct. 22% of the incorrect explanations were reasonable, suggesting that overall explanation accuracy scores might under-estimate the explanation quality. The following example illustrates that while gathering firewood is appropriate when fire is needed for survival, one can argue that going to wilderness is less precise but possibly correct.

Gold: need ﬁre for survival → (MORE/+)

gather ﬁrewood → (MORE/+) build ﬁre for warmth

→ (MORE/+) extensive camping trip Pred: need ﬁre for survival →

(MORE/+)

go to wilderness → (MORE/+) build ﬁre for warmth

→ (MORE/+) extensive camping trip

2. i, j errors: Fig. 3 shows that predicted and gold distributions of i and j are similar. Here, sentence id = −1 indicates no effect. The model has learned from the data to never predict j < i without any hard constraints.
The model is generally good at predicting i, j and in many cases when the model errs, the explanation seems plausible. Perhaps for the same underlying reason, human upper bound is not high on i (75.9%) and on j (66.1%). We show an exam-

in the following passage, the model incorrectly predicts • (no effect) because it fails to draw a connection between sleep and noise:

Figure 3: Gold vs. predicted distribution of i & j resp.

ple where i, j are incorrectly predicted (in red), but

sound plausible.

Gold: ear is not clogged by infection →

(OPP/-)

sound hits ear

→

(OPP/-)

electrical impulse reaches brain → (OPP/-) more

sound detected Pred: ear is not clogged by infection →

(OPP/-)

sound hits ear

→

(OPP/-)

drum converts sound to electrical impulse → (OPP/-)

more sound detected

3. di, de errors: When the model incorrectly predicts di, a major source of error is when ‘−’ is misclassiﬁed. 70% of the ‘−’ mistakes, should have been classiﬁed as ‘+’. A similar trend is observed for de but the misclassiﬁcation of ‘− is less skewed. Table 5 shows the confusion matrix of di and of de in { + − • } .

•

+−

• 1972 91 47

+ 295 883 358

− 226 492 639

•

+−

• 1972 89 49

+ 261 909 295

− 252 346 830

Table 5: Confusion matrix for di (left) and de overall (right). (gold is on x-axis, predicted on y-axis.)

The following example shows an instance where ‘−’ is misclassiﬁed as ‘+’. It implies that there is more scope for improvement here.
Gold: less seeds fall to the ground → (OPP/-) seed falls to the ground → (OPP/-) seeds germinate → (MORE/+) fewer plants Pred: less seeds fall to the ground → (OPP/-) seed falls to the ground → (OPP/-) seeds germinate → (OPP/-) fewer plants
4. in-para vs. out-of-para: The model performs better on in-para questions (typically, linguistic variations) than out-of-para questions (typically, commonsense reasoning). Also see empirical evidence of this in Table 4.
The model is challenged by questions involving commonsense reasoning, especially to connect qp with xi in out-of-para questions. For example,

Pack up your camping gear, food. Drive to your campsite. Set up your tent. Start a ﬁre in the ﬁre pit. Cook your food in the ﬁre. Put the ﬁre out when you are ﬁnished. Go to sleep. Wake up ...
qp: less noise from outside qe: you will have more energy

Analogous to i and j, the model also makes more errors between labels ‘+’ and ‘−’ in out-of-para questions compared to in-para questions (39.4% vs 29.7%) – see Table 6.

•+− + 29 295 78 − 49 130 259

•

+−

+ 266 588 280

− 177 362 380

Table 6: Confusion matrix di for in-para & out-of-para

(Tandon et al., 2019) discuss that some in-para questions may involve commonsense reasoning similar to out-of-para questions. The following is an example of an in-para question where the model fails to predict di correctly because it cannot ﬁnd the connection between protected ears and amount of sound entering.

Gold: ears less protected → (MORE/+) sound enters ear → (MORE/+) sound hits ear drum → (MORE/+) more sound detected Pred: ears less protected → (OPP/-) sound enters the ear → (OPP/-) sound hits ear drum → (MORE/+) more sound detected
5. Injecting background knowledge: To study whether additional background knowledge can improve the model, we revisit the out-of-para question that the model failed on. The model fails to draw a connection between sleep and noise, leading to an incorrect (no effect) ‘•’ prediction.
By adding the following relevant background knowledge sentence to the paragraph “sleep requires quietness and less noise”, the model was able to correctly change probability mass from de = ‘•’ to ‘+’. This shows that providing commonsense through Web paragraphs and sentences is a useful direction.
Pack up your camping gear, food ... Sleeping requires quietness and less noise. Go to sleep. Wake up ...
qp: less noise from outside qe: you will have more energy

8 Assumptions and Generality
QUARTET makes two simplifying assumptions: (1) explanations are assembled from the provided sentences (question + context), rather than generated, and (2) explanations are chains of qualitative, causal inﬂuences, describing how an end-state is inﬂuenced by a perturbation. Although these (helpfully) bound this work, the scope of our solution is still quite general: Assumption (1) is a common approach in other work on multihop explanation (e.g., HotpotQA), where authoritative sentences support an answer. In our case, we are the ﬁrst to apply the same idea to chains of inﬂuences. Assumption (2) bounds QUARTET to explaining the effects of qualitative, causal inﬂuences. However, this still covers a large class of problems, given the importance of causal and qualitative reasoning in AI. The WIQA dataset provides the ﬁrst large-scale dataset that exempliﬁes this class: given a qualitative inﬂuence, assemble a causal chain of events leading to a qualitative outcome. Thus QUARTET offers a general solution within this class, as well as a speciﬁc demonstration on a particular dataset.
9 Conclusion
Explaining the effects of a perturbation is critical, and we have presented the ﬁrst system that can do this reliably. QUARTET not only predicts meaningful explanations, but also achieves a new state-of-the-art on the end-task itself, leading to an interesting ﬁnding that models can make better predictions when forced to explain. Our work opens up new directions for future research: 1) Can additional background context from the Web improve explainable reasoning? 2) Can such structured explanations be applied to other NLP tasks? We look forward to future progress in this area.
Acknowledgements
We thank Harsh Jhamtani, Keisuke Sakaguchi, Vidhisha Balachandran, Dongyeop Kang, members of the AI2 Aristo group, and the anonymous reviewers for their insightful feedback.
References
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. ICCV.

Akari Asai and Hannaneh Hajishirzi. 2020. Logicguided data augmentation and regularization for consistent question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5642–5650, Online. Association for Computational Linguistics.
Jonathan Baxter. 1997. A bayesian/information theoretic model of learning to learn via multiple task sampling. Machine Learning, 28(1):7–39.
Mario Bollini, Stefanie Tellex, Tyler Thompson, Nicholas Roy, and Daniela Rus. 2012. Interpreting and executing recipes with a cooking robot. In ISER.
Antoine Bosselut, Omer Levy, Ari Holtzman, Corin Ennis, Dieter Fox, and Yejin Choi. 2018. Simulating action dynamics with neural process networks. ICLR.
Oana-Maria Camburu, Tim Rockta¨schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In NeurIPS.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What does bert look at? an analysis of bert’s attention. arXiv preprint arXiv:1906.04341.
Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and Serge J. Belongie. 2018. Learning to evaluate image captioning. CVPR.
Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau Yih, and Peter Clark. 2018. Tracking state changes in procedural text: A challenge dataset and models for process comprehension. NAACL.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. NAACL.
John Dunlosky, Katherine A. Rawson, Elizabeth Marsh, Mitchell J. Nathan, and Daniel T. Willingham. 2013. Improving students’ learning with effective learning techniques: Promising directions from cognitive and educational psychology. Psychological science.
Kenneth D. Forbus. 1984. Qualitative process theory. Artiﬁcial Intelligence, 24:85–168.
Shalini Ghosh, Giedrius Burachas, Arijit Ray, and Avi Ziskind. 2019. Generating natural language explanations for vqa using scene graphs and visual attention. arXiv preprint arXiv:1902.05715.
Yoav Goldberg. 2017. Neural network methods for natural language processing. Synthesis Lectures on HLT.

Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In NAACL, pages 107–112.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2017. Tracking the world state with recurrent entity networks. In ICLR.
Peter Jansen, Elizabeth Wainwright, Steven Marmorstein, and Clayton Morrison. 2018. Worldtree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. In LREC 2018.
David Lewis. 2013. Counterfactuals. John Wiley & Sons.
Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. arXiv.
Sheshera Mysore, Zach Jensen, Edward Kim, Kevin Huang, Haw-Shiuan Chang, Emma Strubell, Jeffrey Flanigan, Andrew McCallum, and Elsa Olivetti. 2019. The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures. arXiv preprint arXiv:1905.06939.
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. 2018. Multimodal explanations: Justifying decisions and pointing to the evidence. CVPR.
Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. Rise: Randomized input sampling for explanation of black-box models. arXiv preprint arXiv:1806.07421.
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. ACL.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In EMNLP.
Hanie Sedghi and Ashish Sabharwal. 2018. Knowledge completion for generics using guided tensor factorization. ACL.
Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. ICCV.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention ﬂow for machine comprehension. ICLR.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL.
Niket Tandon, Bhavana Dalvi Mishra, Joel Grus, Wentau Yih, Antoine Bosselut, and Peter Clark. 2018. Reasoning about actions and state changes by injecting commonsense knowledge. EMNLP.
Niket Tandon, Bhavana Dalvi Mishra, Keisuke Sakaguchi, Antoine Bosselut, and Peter Clark. 2019. Wiqa: A dataset for ”what if...” reasoning over procedural text. EMNLP.
Gerhard Weikum and Martin Theobald. 2010. From information to knowledge: harvesting entities and relationships from web sources. In PODS.
Daniel S Weld and Johan De Kleer. 2013. Readings in qualitative reasoning about physical systems. Morgan Kaufmann.
Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. arXiv preprint arXiv:1908.04626.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP.
Zi Yang and Eric Nyberg. 2015. Leveraging procedural knowledge for task-oriented search. In SIGIR.
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018. From recognition to cognition: Visual commonsense reasoning. ArXiv, abs/1811.10830.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675.

Appendix
9.1 Hyperparameter Tuning QUARTET ﬁne-tunes BERT, allowing us to re-use the same hyperparameters as BERT with small adjustments in the recommended range (Devlin et al., 2018). We use the BERT-base-uncased version with a hidden size of 768. We found the best hyperparameter settings by searching the space using the following hyperparameters.
1. weight decay = { 0.1, 0.01, 0.05 }
2. dropout = {0.1, 0.2, 0.3 }
3. learning rate = {1e-05, 2e-05, 5e-05}

