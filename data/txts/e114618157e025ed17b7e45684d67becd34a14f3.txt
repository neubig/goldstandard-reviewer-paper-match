Lower Bounds on the Total Variation Distance Between Mixtures of Two Gaussians

Sami Davies∗

Arya Mazumdar† Soumyabrata Pal‡ March 11, 2022

Cyrus Rashtchian§

arXiv:2109.01064v2 [math.PR] 9 Mar 2022

Abstract
Mixtures of high dimensional Gaussian distributions have been studied extensively in statistics and learning theory. While the total variation distance appears naturally in the sample complexity of distribution learning, it is analytically diﬃcult to obtain tight lower bounds for mixtures. Exploiting a connection between total variation distance and the characteristic function of the mixture, we provide fairly tight functional approximations. This enables us to derive new lower bounds on the total variation distance between two-component Gaussian mixtures with a shared covariance matrix.

1 Introduction

Let N (µ, Σ) denote the d-dimensional Gaussian distribution with mean µ ∈ Rd and positive deﬁnite

covariance matrix Σ ∈ Rd×d. A k-component mixture of d-dimensional Gaussian distributions

is a distribution of the form f =

k i=1

wi

·

N (µi, Σi).

Such

a

mixture

is

deﬁned

by

k

triples

{

(w

i

,

µi

,

Σi

)}

k i=1

,

where

wi

∈

R+

with

k i=1

wi

=

1

are

the

mixing

weights,

µi

∈

Rd

are

the

means,

and Σi ∈ Rd×d are the covariance matrices. Mixtures of Gaussian distributions have been studied

intensively due to their broad applicability to statistical problems [2, 10, 11, 21, 22, 28, 29, 31, 32].

The variational distance (a.k.a., the total variation (TV) distance) between two distributions

f, f with same sample space Ω and sigma algebra S is deﬁned as follows:

f − f TV sup f (A) − f (A) .
A∈S
The minimum pairwise TV distance of a class of distributions appears naturally in the expressions of statistical error rates related to the class, most notably in the Neyman-Pearson approach to hypothesis testing [25, 30], as well as in the sample complexity results in density estimation [13]. In particular, in these applications, a lower bound on the total variation distance between two candidate distributions is an essential part of the algorithm design and analysis.
Tight bounds are known for the total variation distance between single Gaussians; however, they have only recently been derived as closed form functions of the distribution parameters [5, 14]. The functional form of the TV distance bound is often much more useful in practice because it can be directly evaluated based on only the means and covariances of the distribution. This has
∗Northwestern University. sami@northwestern.edu †Haliciogˇlu Data Science Institute, UC San Diego. arya@ucsd.edu ‡Computer Science Department, University of Massachusetts Amherst. soumyabratap@umass.edu §Department of Computer Science & Engineering, UC San Diego. crashtchian@eng.ucsd.edu

1

opened up the door for new applications to a variety of areas, such as analyzing ReLU networks [34], distribution learning [3, 4], private distribution testing [7, 8], and average-case reductions [6].
Inspired by the wealth of applications for single Gaussian total variation bounds, we investigate deriving analogous results for mixtures with two components. As our main contribution, we complement the single Gaussian results and derive tight lower bounds for pairs of mixtures containing two equally weighted Gaussians with shared variance. We also present our results in a closed form in terms of the gap between the component means and certain statistics of the covariance matrix. The total variation distance between two distributions can be upper bounded by other distances/divergences (e.g., KL divergence, Hellinger distance) that are easier to analyze. In contrast, it is a key challenge to develop ways to lower bound the total variation distance. The shared variance case is important because it presents some of the key diﬃculties in parameter estimation and is widely studied [12, 35]. For example, mean estimation with shared variance serves as a model for the sensor location estimation problem in wireless or physical networks [23, 26, 33].
The lower bound on total variation distance can be applicable in several contexts. In binary hypothesis testing, it gives a suﬃcient condition to bound from above the total probability of error of the best test [28]. Hypothesis testing in Gaussian mixture models has been of interest, cf. [1, 9]. Furthermore, parameter learning in Gaussian mixture models is a core topic in density estimation [13]. Our bound can provide a suﬃcient condition on the learnability of the class of two component Gaussian mixtures in terms of the precision of parameter recovery and gap between the component of mixtures. Indeed, performance of various density estimation techniques, such as the Scheﬀé estimator or the minimum distance estimator, depends crucially on a computable lower bound in total variation distance between candidate distributions [13]. Furthermore, our lower bound implies that, for the class of distributions we consider, if a pair of distributions is close in variational distance, then the distributions have close parameters. This type of implication is integral to arguments in outlier-robust moment estimation algorithms and clustering [4, 20].
We obtain lower bounds on the total variation distance by examining the characteristic function of the mixture. This connection has been previously used in [24] in the context of mixture learning, but it required strict assumptions on the mixtures having discrete parameter values, i.e., Gaussians with means that belong to a scaled integer lattice. It is not clear how to generalize their techniques to noninteger means. As a ﬁrst step towards that generalization, we analyze unrestricted two-component one-dimensional mixtures by applying a novel and more direct analysis of the characteristic function. Then, in the high-dimensional setting, we obtain a new TV distance lower bound by projecting and then using our one-dimensional result. By carefully choosing and analyzing the one-dimensional projection (which depends on the mixtures), we exhibit nearly-tight bounds on the TV distance of d-dimensional mixtures for any d ≥ 1.

1.1 Results
Let F be the set of all d-dimensional, two-component, equally weighted mixtures

F = fµ ,µ = 1 N (µ0, Σ) + 1 N (µ1, Σ) | µ0, µ1 ∈ Rd, Σ ∈ Rd×d ,

01 2

2

where Σ ∈ Rd×d is a positive deﬁnite matrix. When d = 1, we use the notation fµ0,µ1 ∈ F and simply denote the variance as σ2 ∈ R. Our main result is the following nearly-tight lower bound on
the TV distance between pairs of d-dimensional two-component mixtures with shared covariance.

Theorem 1. For fµ0,µ1 , fµ0,µ1 ∈ F , deﬁne sets S1 = {µ1 − µ0, µ1 − µ0}, S2 = {µ0 − µ0, µ1 − µ1}, S3 = {µ0 − µ1, µ1 − µ0} and vectors v1 = argmaxs∈S1||s||2, v2 = argmaxs∈S2||s||2, v3 =

2

argmaxs∈S3 ||s||2. Let λΣ,U maxu:||u||2=1,u∈U uT Σu with U being the span of the vectors v1, v2, v3. If v1 2 ≥ min( v2 2, v3 2)/2 and λΣ,U = Ω(||v1||2), then

v1 2 min( v2 2, v3 2)

fµ0,µ1 − fµ0,µ1 TV = Ω min 1,

λΣ,U

,

and otherwise, we have that fµ0,µ1 − fµ0,µ1 TV = Ω min 1, min( v2 2, v3 2)/ λΣ,U .

Notice from the deﬁnitions of v1, v2, v3 that U is contained within the subspace spanned by the

unknown mean vectors µ0, µ1, µ0, µ1. Furthermore, λΣ,U as deﬁned in Theorem 1 can always be bounded from above by the largest eigenvalue of the matrix Σ, and as we will show in Section 1.3,

this upper bound characterizes the TV distance between mixtures in several instances.

In some cases, it is simpler to work with z = Σ−1/2x instead of the original samples x ∈ Rd.

Note

that

if

x

∼

1 2

N

(µ0

,

Σ)

+

1 2

N

(µ1

,

Σ),

then

z

∼

1 2

N

(Σ−1/2µ0,

I

)

+

1 2

N

(Σ−1/2

µ1

,

I

),

for

I

the

d-dimensional identity matrix. Overall, if we scale the distribution by Σ−1/2, then by the invariance

property of TV distance (see, for instance, Section 5.3 in [13]), Theorem 1 implies the following.
For fµ0,µ1 , fµ0,µ1 ∈ F and S1, S2, S3 as above, the scaled vectors are vi = argmaxs∈Si ||Σ−1/2s||2, for i ∈ [3]. If Σ−1/2v1 2 ≥ min( Σ−1/2v2 2, Σ−1/2v3 2)/2 and ||Σ−1/2v1||2 = O(1), then

fµ0,µ1 − fµ0,µ1 TV = Ω min 1, Σ−1/2v1 2 min( Σ−1/2v2 2, Σ−1/2v3 2) ,

and otherwise, fµ0,µ1 − fµ0,µ1 TV = Ω min 1, min( Σ−1/2v2 2, Σ−1/2v3 2) . In the special case of one component Gaussians, i.e., µ0 = µ1 and µ0 = µ1, we recover a result by
Devroye et al. (see the lower bound in [14, Theorem 1.2], setting Σ1 = Σ2). In the one-dimensional setting, our next theorem shows a novel lower bound on the total variation distance between any two distinct two-component one-dimensional Gaussian mixtures from F.
Theorem 2. Without loss of generality, for fµ0,µ1, fµ0,µ1 ∈ F , suppose µ0 ≤ min(µ1, µ0, µ1) and µ0 ≤ µ1. Further, let δ1 = max{|µ0 − µ1|, |µ0 − µ1|} and δ2 = max{|µ0 − µ0|, |µ1 − µ1|}. If [µ0, µ1] ⊆ [µ0, µ1] and σ = Ω(δ1), then we have that
||fµ0,µ1 − fµ ,µ ||TV ≥ Ω(min(1, δ1δ2/σ2)), 01
and otherwise, ||fµ0,µ1 − fµ0,µ1 ||TV ≥ Ω(min(1, δ2/σ)).

1.2 Related Work

Let I denote the d-dimensional identity matrix. Statistical distances between a pair of k-component

d-dimensional Gaussian mixtures f =

k i=1

k−1N (µi,

I)

and

f

=

k i=1

k−1N

(µi,

I)

with

shared,

known component covariance I have been studied in [15, 35]. For a k-component Gaussian mixture

f=

k i=1

k−1N (µi, I),

let

M

(f )

=

i=1 k−1µ⊗i where x⊗ is the -wise tensor product of x. We

denote the Kullback-Leibler divergence, Squared Hellinger divergence, and χ2-divergence of f, f

by ||f − f ||KL , ||f − f ||H2, and ||f − f ||χ2 respectively. We write ||M ||F to denote the Frobenius norm of the matrix M . Prior work shows the following.

Theorem 3 (Theorem 4.2 in [15]). Consider mixtures f =

k i=1

k−1N

(µi,

I

)

and

f

=

k i=1

k−1

N

(µi

,

I

)

where ||µi||2 ≤ R, ||µi||2 ≤ R, for all i ∈ [k] and constant R ≥ 0. For any distance D ∈ {H2, KL, χ2},

we have

f −f

D =Θ

max ≤2k−1

M

(f ) − M

(f

)

2 F

.

3

This bound alone does not give a guarantee for the TV distance. However it is well-known that,

fµ0,µ1 − fµ0,µ1 TV ≥ fµ0,µ1 − fµ0,µ1 H2 . (1)

We can use this in conjunction with Theorem 3 to get a lower bound on TV distance, but it is suboptimal for many canonical instances. For example, consider one-dimensional Gaussian mixtures

1

1

1

1

f = N (u, 1) + N (−u, 1) and f = N (2u, 1) + N (−2u, 1).

(2)

2

2

2

2

Using Eq. (1) and Theorem 3, we have that ||f − f ||TV = Ω(u4). On the other hand, by using our result (Theorem 2), we obtain the improved bound ||f − f ||TV = Ω(u2). The improvement becomes more signiﬁcant as u becomes smaller. Also, the prior result in Theorem 3 assumes that the means of the two mixtures f, f are contained in a ball of constant radius, limiting its applicability.
The TV distance between Gaussian mixtures with two components when d = 1 has been recently studied in the context of parameter estimation [16, 19, 27, 18]. The TV distance guarantees in these papers are more general, as they do not need the component covariances to be same. However, the results and their proofs are tailored towards the case when both the mixtures have zero mean. They do not apply when considering the TV distance between two mixtures with distinct means. Theorems 1 and 2 hold for all mixtures with shared component variances, without assumptions on the means.
Further, our bound can be tighter than these prior results, even in the case when the mixtures have zero mean. Consider again the pair of mixtures f, f deﬁned in Eq. (2) above. In [27, 19], the authors show that ||f − f ||TV = Ω(u4); see, e.g., Eq. (2.7) in [27]. Notice that this is the same bound that can be recovered from Theorem 3, and as we mentioned before, this bound is loose. By using Theorem 2, we obtain the improved bound ||f − f ||TV = Ω(u2). Now consider a more general pair of mixtures, where for u, v ≥ 0, we deﬁne

1

1

1

1

f = N (u, 1) + N (−u, 1) and f = N (v, 1) + N (−v, 1).

(3)

2

2

2

2

In [16] (see the proof of Lemma G.1 part (b)), the authors show that ||f − f ||TV = Ω((u − v)2). Notice that for the previous example in Eq. (2) with v = 2u, the result in [16] leads to the bound ||f − f ||TV = Ω(u2), which is the same bound that can be obtained from Theorem 2. However, for any small ε > 0, by setting v = u + ε, we see that the bound in [16] reduces to ||f − f ||TV = Ω(ε2). On the other hand, by using Theorem 2, we obtain ||f − f ||TV = Ω(u · ε + ε2). Whenever u ε, our result provides a much larger and tighter lower bound. On the other hand, whenever u < ε, our
bound coincides with that of [16].

1.3 Tightness of the TV distance bound

Our bounds on the TV distance are tight up to constant factors. For example, let u ∈ Rd be a
d-dimensional vector satisfying ||u||2 < 1. Consider the mixtures f = 0.5N (u, I) + 0.5N (−u, I) and f = 0.5N (2u, I) + 0.5N (−2u, I). Considering the notation of Theorem 1, we have v1 = 2u and v2 = u, and the ﬁrst bound in the theorem implies that ||f − f ||TV ≥ Ω(||u||22). On the other hand, we use the inequality ||f − f ||TV ≤ 2 ||f − f ||H2 in conjunction with Theorem 3. In the notation of Theorem 3, note that M1(f ) − M1(f ) = 0, and we can upper bound the max over ∈ {2, 3} by
the sum of the two terms to say that

f −f

TV ≤ O

max

M

(f ) − M

(f

)

2 F

∈{2,3}

≤ O(||u ⊗ u||F + ||u ⊗ u ⊗ u||F ) = O(||u||22 + ||u||32).

4

Since ||u||2 < 1, we see that ||u||22 is the dominating term on the RHS, and ||f − f ||TV = Θ(||u||22). As a result, our TV distance bound in Theorem 1 is tight as a function of the means for this example.
Our bounds are tight in other instances too. Consider the second parts of Theorems 1 and 2 when samples are pre-multiplied with Σ−1/2. Here, we can use the triangle inequality to derive a
simple upper bound on the TV distance,

1

fµ0,µ1 − fµ0,µ1

≤ min TV 2

N (µ0, Σ) − N (µ0, Σ) TV + N (µ1, Σ) − N (µ1, Σ) TV , N (µ1, Σ) − N (µ0, Σ) TV + N (µ0, Σ) − N (µ1, Σ) TV .

Then, we can use tight bounds on the TV distance between single Gaussians. In the one-dimensional setting, Theorem 1.3 in [14] shows that ||fµ0,µ1 − fµ0,µ1||TV = O(max(1, δ2/σ)), recalling that δ2 = max{|µ0 − µ0|, |µ1 − µ1|}. In the high dimensional setting, Theorem 1.2 in [14] shows that
1 fµ0,µ1 − fµ0,µ1 TV = O max 1, λmin(Σ) · min ( v2 2, v3 2) ,
recalling v2 and v3 from the deﬁnitions in Theorem 1 and letting λmin(Σ) be the minimum eigenvalue of Σ. Again by the invariance property, TV distance remains the same if the samples are premultiplied by Σ−1/2. With this transformation, the component co-variance matrix is I and

fµ0,µ1 − fµ0,µ1 TV = O max 1, min Σ−1/2v2 2, Σ−1/2v3 2 .

It follows that the second parts of Theorems 1 and 2 are tight up to constants when samples are pre-multiplied with Σ−1/2.

On the other hand, when comparing with the Hellinger distance upper bound of [15], our lower

bound on the TV distance is not tight in the following case. Deﬁne two d-dimensional mixtures

f = 0.5N (u, I)+0.5N (−u, I) and f = 0.5N (4u, I)+0.5N (−2u, I). The mean of f is u. Applying

Theorem 3, we get an upper bound of

f −f

H2 = O( u 2 +

u

2 2

+

u 32) = O( u 2), when

u 2 1. In contrast, Theorem 1 only gives a lower bound of Ω( u 22), which can be much smaller

than u 2. It would be an interesting open direction to derive tight bounds on this instance. We do

not know if this is an inherent limitation of either of the bounds, and it may be possible to extend

our results to capture the u 2 term, or tighten the upper bound.

1.4 Preliminaries
We use Ω(·), O(·), and Θ(·) to hide absolute constants. For vectors u, v ∈ Rd, we let u, v denote the Euclidean inner product. We use the characteristic function of a distribution, deﬁned below.
Deﬁnition 1. The characteristic function Cf : R → C of a distribution f is Cf (t) = R eitxf (x)dx. If X is a random variable with distribution f , then Cf (t) = EX∼f [eitX ]. The characteristic function of a two-component, one-dimensional mixture fµ0,µ1 ∈ F is Cfµ0,µ1 (t) = 12 e−σ2t2/2(eitµ0 + eitµ1 ). The characteristic function can be used to bound the TV distance with the following lemma.
Lemma 2 ([24]). For distributions f, f on a shared sample space Ω ⊆ R,
1 f − f TV ≥ 2 st∈uRp |Cf (t) − Cf (t)|.
5

Organization. The rest of the paper is organized as follows. In Section 2, we give a brief high level overview of our results. In Section 3, we provide the main parts of the proof of our TV distance result for one dimensional mixtures, which is based on elementary complex analysis. Subsequently, in Section 4, we provide the proof of the TV distance lower bound in the high dimensional case.
2 Technical Overview
In one dimension, we lower bound the TV distance as follows. For fµ0,µ1, fµ0,µ1 ∈ F , suppose µ0 is the smallest mean. Recall that δ1 = max{|µ0 − µ1|, |µ0 − µ1|} and δ2 = max{|µ0 − µ0|, |µ1 − µ1|}. If [µ0, µ1] ⊆ [µ0, µ1], then ||fµ0,µ1 − fµ0,µ1 ||TV ≥ Ω(min(1, δ1δ2/σ2)) and otherwise, ||fµ0,µ1 − fµ0,µ1||TV ≥ Ω(min(1, δ2/σ)). The latter case corresponds to when either both means from one mixture are smaller than another, i.e., µ0 ≤ µ1 ≤ µ0, µ1, or the mixtures’ means are interlaced, i.e., µ0 ≤ µ0 ≤ µ1 ≤ µ1.
We use Lemma 2 to lower bound the TV distance between mixtures fµ0,µ1, fµ0,µ1 ∈ F by the modulus of a complex analytic function:
4 fµ0,µ1 − fµ0,µ1 TV ≥ sutp e− σ22t2 eitµ0 + eitµ1 − eitµ0 − eitµ1 . (4)
Let h(t) = eitµ0 + eitµ1 − eitµ0 − eitµ1 . A lower bound on ||fµ0,µ1 − fµ ,µ ||TV can be obtained by 01
taking t = 1/(cσ) for c constant, so that e−σ2t2/2 is not too small. Then, it remains to bound |h(t)| at the chosen value of t. In some cases, we will have to choose the constant c very carefully, as terms in h(t) can cancel out due to the periodicity of the complex exponential function. For instance, if µ0 = 0, µ1 = 200σ, µ0 = σ, and µ1 = 201σ with σ = 2π, then |h(1)| = 0.
It is reasonable to wonder whether there is a simple, global way to lower bound Eq. (4). We could reparameterize the function h(t) as the complex function g(z) = zµ0 + zµ1 − zµ0 − zµ1, where z = eit, then study |g(z)|, for z in the disc with center 0 and radius 1 in the complex plane. However, we are unaware of a global way to bound |g(z)| here due to the fact that (i) g(z) is not analytic at 0 when the means are non-integral and (ii) there is not a clear, large lower bound for g(z) anywhere inside the unit disc. These two facts obstruct the use of either the Maximum Modulus Principle or tools from harmonic measure to obtain lower bounds. Instead, we use a series of lemmas to handle the diﬀerent ways that |h(t)| can behave. The techniques include basic complex analysis and Taylor series approximations of order at most three.
Let fµt 0,µ1 be the distribution of the samples obtained according to fµ0,µ1 and projected onto the direction t ∈ Rd. We have (see Lemma 6 for a proof)
fµt 0,µ1 ≡ N (µT0 t2, tT Σt) + N (µT1 t2, tT Σt) , fµt 0,µ1 ≡ N (µ0T t2, tT Σt) + N (µ1T t2, tT Σt) .
By the data processing inequality for f -divergences (see Theorem 5.2 in [13]), we have fµ0,µ1 − fµ0,µ1 TV ≥ supt∈Rd fµt 0,µ1 − fµt 0,µ1 TV. Using our lower bound on the TV distance between onedimensional mixtures (Theorem 2), we obtain a lower bound on fµ0,µ1 − fµ0,µ1 TV by choosing t ∈ Rd carefully. This leads to Theorem 1.
3 Lower Bound on TV Distance of 1-Dimensional Mixtures
Consider distinct Gaussian mixtures fµ0,µ1, fµ0,µ1 ∈ F . Without loss of generality we will also let µ0 ≤ min(µ1, µ0, µ1) be the smallest unknown parameter, and let µ1 ≥ µ0. We maintain these assumptions throughout this section, and we will prove Theorem 2.
6

Figure 1: Layout of the means for Theorem 2. The means can be ordered in diﬀerent ways, which aﬀects the analysis of lower bounding |eitµ0 + eitµ1 − eitµ0 − eitµ1| in Lemma 3. For a ﬁxed t, the order aﬀects (i) whether the real or imaginary part of eitµ0 + eitµ1 − eitµ0 − eitµ1 has large modulus
and (ii) whether the terms from µ0 and µ1 or µ0 and µ1 dominate.

Eq. (4) implies that we can lower bound ||fµ0,µ1 − fµ0,µ1||TV by the modulus of a complex analytic function with parameter t. Then, we can optimize the bound by choosing t = Θ(1/σ) and lower
bounding the term in the absolute value signs.
We deﬁne the following parameters relative to the means to simplify some bounds:

δ1 = max(|µ0 − µ1|, |µ0 − µ1|) δ3 = µ0 + µ1 − µ0 − µ1

δ2 = max(|µ0 − µ0|, |µ1 − µ1|) δ4 = min( µ0 − µ0 , µ1 − µ1 ).

We ﬁrst consider t such that t(µ1 − µ0), t(µ1 − µ0), t(µ1 − µ0) ≤ π4 , which is covered in Lemma 3. Lemma 3. For t > 0 with t(µ1 − µ0), t(µ1 − µ0), t(µ1 − µ0) ∈ [0, π4 ], if µ0, µ1 ∈ [µ0, µ1], then

eitµ0 + eitµ1 − eitµ0 − eitµ1 ≥ max

t2(δ1 − δ4)δ4 tδ3

,√

2

42

√ and otherwise, when µ1 > µ1, eitµ0 + eitµ1 − eitµ0 − eitµ1 ≥ tδ2/(2 2).

See Figure 1 for an illustration of the diﬀerent ways that the means can be ordered. The lemma follows from straightforward calculations that only use Taylor series approximations, trigonometric identities, and basic facts about complex numbers. We include the proof in Appendix A.
Recall that we will choose t = Θ(1/σ) to cancel the exponential term in Eq. (4). Therefore, Lemma 3 handles the case when all the means are within some interval of size Θ(σ).
Next, we prove that when the separation between the mixtures is substantially fair apart—when either |µ0 − µ0| or |µ1 − µ1| is at least 2σ—we have a constant lower bound on the TV distance. Recall that it is without loss of generality to assume that µ0 is the smallest parameter and µ1 > µ0. A similar result as the following two lemmas has been observed previously (e.g., [17]) but we provide a simple and self-contained proof.

Lemma 4. If max(|µ0 − µ0| , |µ1 − µ1|) ≥ 2σ, then it follows that ||fµ0,µ1 − fµ0,µ1||TV ≥ Ω(1).
Proof. Assume that |µ0 − µ0| ≥ 2σ, where the case |µ1 − µ1| ≥ 2σ is analogous. Recall from the deﬁnition of TV distance that

||fµ0,µ1 − fµ0,µ1 ||TV Asu⊆pΩ fµ0,µ1 (A) − fµ0,µ1 (A)

≥ Pr [X ≤ µ0 + σ] − Pr [Y ≤ µ0 + σ].

X ∼fµ0 ,µ1

Y ∼fµ0,µ1

For a random variable X ∼ fµ0,µ1, let E denote the event that we choose the component with mean µ0, i.e., if X denotes X conditioned on E, then we have X ∼ N (µ0, σ2). Since the mixing

7

weights are equal, we have Pr(E) = Pr(Ec) = 1/2, where Ec is the complement of E. Therefore,

Pr(X ≥ µ0 + σ) ≤ Pr(E) Pr(X ≥ µ0 + σ | E) + Pr(Ec) = 1 2

∞

e− (t−µ0)2 2σ2

1

√ dt +

µ0+σ 2πσ

2

1∞

t−µ

e− (t−µ0)2 2σ2

1 1 e− 21 1

≤

0√

dt + ≤ · √ + .

(5)

2 µ0+σ σ

2πσ

2 2 2π 2

Recall that µ0 ≤ µ0, µ0 ≤ µ1 and |µ0 − µ0| ≥ 2σ. Again, for a random variable Y ∼ fµ ,µ , let E

c denotes µ

01
is chosen). Then,

denote the event that the component with mean µ0 is chosen (and E

1

Pr(Y

≤ µ0 + σ) = Pr(E ) Pr(Y ≤ µ0 + σ | E ) + Pr(E c) Pr(Y

=a Pr(E ) Pr(Y ≤ µ0 − σ | E ) + Pr(E c) Pr(Y

=b Pr(E ) Pr(Y ≥ µ0 + σ | E ) + Pr(E c) Pr(Y

c 1 e− 21 1 e− 12

e− 12

≤ ·√ + ·√ =√

2 2π 2 2π 2π

≤ µ0 + σ ≤ µ1 − σ ≥ µ1 + σ

| E c) | E c) | E c)

where in step (a), we used the fact that µ0 − σ ≥ µ0 + σ and µ1 − σ ≥ µ0 + σ; in step (b), we used the symmetry of Gaussian distributions; in step (c), we used the same analysis as in (5). By plugging
this in the deﬁnition of TV distance, we have

1 ||fµ0,µ1 − fµ0,µ1 ||TV ≥ X∼Pfµr0,µ1 [X ≤ µ0 + σ] − Y ∼Pfµr0,µ1 [Y ≤ µ0 + σ] ≥ 2 −

9 ≥ 0.137.
8πe

If Lemma 4 does not apply, then we case on whether max(|µ0 − µ1| , |µ0 − µ1|) is large or not. If max(|µ0 − µ1| , |µ0 − µ1|) < 100σ, we use Lemma 3—exactly how will be explained later—and otherwise we use the following lemma. Recall that δ2 = max(|µ0 − µ0|, |µ1 − µ1|).

Lemma 5. If max(|µ0 − µ1| , |µ0 − µ1|) ≥ 100σ and max(|µ0 − µ0| , |µ1 − µ1|) ≤ 2σ, then

sup e− σ22t2 eitµ0 + eitµ1 − eitµ0 − eitµ1 ≥ π2δ2 .

t

240eσ

We defer the proof of Lemma 5 to Appendix A. Using Lemmas 3, 4, and 5, we prove Theorem 2.

Proof of Theorem 2. Using Lemma 2, we see that

2 fµ0,µ1 − fµ ,µ

≥ sup e− σ22t2 eitµ0 + eitµ1 − eitµ0 − eitµ1 .

0 1 TV

t

2

Case 1: Consider the case when µ0, µ1, µ0, µ1 are in an interval of size at most 100σ, i.e.,

max µ1 − µ0 , |µ1 − µ0| , µ0 − µ0 ≤ 100σ.

(6)

Recall δ1 = max{|µ0 − µ1|, |µ0 − µ1|}, δ2 = max(|µ0 − µ0|, |µ1 − µ1|), δ3 = |µ0 + µ1 − µ0 − µ1|, δ4 = min(|µ0 − µ0| , |µ1 − µ1|). For t = π/400σ, 0 ≤ t max |µ1 − µ0| , |µ1 − µ0| , |µ0 − µ0| ≤ π4 . We have assumed that µ0 ≤ min(µ1, µ0, µ1) and µ0 ≤ µ1. This implies that µ0 ≤ µ0 ≤ µ1 ≤ µ1 in

8

the subcase when µ0, µ1 ∈ [µ0, µ1]. This also implies that δ1 = |µ1 − µ0| ≥ 2δ4, a fact we will use later. The inequality in Eq. (6) implies that the above value of t = π/400σ satisﬁes the conditions of
Lemma 3. Then, when µ0, µ1 ∈ [µ0, µ1], the ﬁrst part of the lemma implies that

π2(δ1 − δ4)δ4 πδ3 2 fµ0,µ1 − fµ0,µ1 TV ≥ max 640000eσ2 , 3200√2eσ .

Now we observe that δ3 ≥ δ2 − δ4. To see this, assume without loss of generality that δ2 = |µ0 − µ0| and δ4 = |µ1 − µ1|. By the triangle inequality, we have that δ3 = |µ0 + µ1 − µ0 − µ1| ≥ |µ0 − µ0| − |µ1 − µ1| = δ2 − δ4. We spl√it up the calculations based on the value of δ3. If δ3 ≥ δ22 , then ||fµ0,µ1 − fµ0,µ1||TV ≥ πδ2/(12800 2eσ). On the other hand, if δ3 ≤ δ22 , then since δ3 ≥ δ2 − δ4, we have that δ4 ≥ δ22 . Coupled with the fact that δ1 ≥ 2δ4 (hence δ4 ≤ δ1/2 implying δ1 − δ4 ≥ δ1/2), we have that ||fµ0,µ1 − fµ ,µ ||TV ≥ π2δ1δ2/(5120000eσ2). Putting these together, we have
01

fµ0,µ1 − fµ0,µ1 TV ≥ min

π2δ1δ2

πδ2

,

√

5120000eσ2 12800 2eσ

π2δ1δ2 = 5120000eσ2

For the case when both of µ0, µ1 are not in [µ0, µ1], we have µ1 > µ1 (recall that µ0 is the smallest mean and µ0 ≤ µ1), and we can use the second part of Lemma 3 to conclude that

e− σ22t2 itµ itµ itµ itµ

πδ2

2 fµ0,µ1 − fµ0,µ1 TV ≥ sutp 2

e 0 +e 1 −e 0 −e 1 ≥

√.

1600 2eσ

Case 2: Next, consider when δ2 = max(|µ0 − µ0|, |µ1 − µ1|) ≥ 2σ. Lemma 4 implies that fµ0,µ1 − fµ0,µ1 TV ≥ Ω(1).

Case 3: Now, we consider the only remaining case, when δ1 = max(|µ0 − µ1| , |µ0 − µ1|) ≥ 100σ and δ2 ≤ max(|µ0 − µ0| , |µ1 − µ1|) ≤ 2σ. This case satisﬁes the conditions of Lemma 5, and therefore, we have that

e− σ22t2 itµ itµ itµ itµ

π2δ2

2 fµ0,µ1 − fµ0,µ1 TV ≥ sutp 2

e 0 +e 1 −e 0 −e 1 ≥

,

240eσ

thus proving the theorem.

4 Lower Bound on TV Distance of d -Dimensional Mixtures

We lower bound the TV distance of high-dimensional mixtures in F and prove Theorem 1. For any direction t ∈ Rd, we denote the projection of the distributions fµ0,µ1 and fµ0,µ1 on t by fµt 0,µ1 and f t , respectively. The next lemma allows us to precisely deﬁne the projected mixtures.
µ0,µ1

Lemma

6.

For

a

random

variable

x

∼

1 2

N

(µ0

,

Σ)

+

1 2

N

(µ1

,

Σ),

for

any

t

∈

Rd,

tT x ∼ N ( µ0, t , tT Σt) + N ( µ1, t , tT Σt) .

2

2

Proof. A linear transformation of a multivariate Gaussian is also a Gaussian. For x ∼ N (µ0, Σ), we see that t, x ∼ N ( µ0, t , tT Σt) by a computation of the mean and variance. Similarly, for x ∼ N (µ1, Σ), we have t, x ∼ N ( µ1, t , tT Σt). Putting these together, the claim follows.

9

From Lemma 6, we can exactly deﬁne the one-dimensional mixtures
fµt 0,µ1 = N ( µ0, t2 , tT Σt) + N ( µ1, t2 , tT Σt) , fµt 0,µ1 = N ( µ0, t2 , tT Σt) + N ( µ1, t2 , tT Σt) . By using the data processing inequality, or the fact that variational distance is non-increasing under all mappings (see, for instance, Theorem 5.2 in [13]), it follows that

fµ0,µ1 − fµ0,µ1

≥ sup
TV t∈Rd

fµt 0,µ1 − fµt 0,µ1

.
TV

Let H be the set of permutations on {0, 1}. The following lemma has two cases based on whether the interval deﬁned by one pair of mean’s projections is contained in the interval deﬁned by the other pair’s projections.
√ Lemma 7. Let t ∈ Rd be any vector. If tT Σt = Ω max (| t, µ0 − µ1 |, | t, µ0 − µ1 |) , and either µ0, t , µ1, t ∈ [ µ0, t , µ1, t ] or µ0, t , µ1, t ∈ [ µ0, t , µ1, t ], then ||fµt 0,µ1 − fµt 0,µ1 ||TV is at least

1

Ω min

1, tT Σt · max | t, µ0 − µ1 |, | t, µ0 − µ1 |

min max | t, µ0 − µσ(0) |, | t, µ1 − µσ(1) |
σ∈H

.

Otherwise, we have that ||fµt 0,µ1 − fµt 0,µ1 ||TV is at least

1

Ω

min

1,

√ tT Σt

· min max(|
σ∈H

t, µ0

− µσ(0)

|, |

t, µ1

− µσ(1)

|)

.

Proof. The proof follows directly from Theorem 2. Note that in Theorem 2, we assumed the ordering
of the means without loss of generality, i.e., µ0 ≤ min(µ1, µ0, µ1) and µ0 < µ1. However, taking a minimum over the set of permutations in H allows us to restate the theorem in its full generality.

Now we are ready to provide the proof of Theorem 1.

4.1 Proof of Theorem 1
Let S1 = {µ1 − µ0, µ1 − µ0}, S2 = {µ0 − µ0, µ1 − µ1}, S3 = {µ0 − µ1, µ1 − µ0},
and v1 = argmaxs∈S1 ||s||2, v2 = argmaxs∈S2 ||s||2, v3 = argmaxs∈S3 ||s||2.
We consider two cases below. Depending on the norm of v1, we modify our choice of projection direction. In the ﬁrst case, we do not have a guarantee on the ordering of the means, so we use the ﬁrst part of Lemma 7. In the second case, we can use the better bound in the second part of the lemma after arguing about the arrangement of the means.

Case 1 (2 ||v1||2 ≥ min(||v2||2 , ||v3||2) and λΣ,U = Ω(||v1||2)): We start with a lemma that shows the existence of a vector z that is correlated with {v1, v2, v3}. We use z to deﬁne the direction
t to project the means on, while roughly preserving their pairwise distances.

Lemma 8. For v1, v2, v3 deﬁned above, there exists a vector z ∈ Rd such that ||z||2 ≤ 10, z belongs

to

the

subspace

spanned

by

v1, v2, v3,

and

|

z, v

|≥

||v||2 6

for

all

v ∈ {v1, v2, v3}.

10

Proof. We use the probabilistic method. Let u1, u2, u3 be orthonormal vectors forming a basis of the subspace spanned by v1, v2 and v3; hence, we can write the vectors v1, v2 and v3 as a linear combination of u1, u2, u3. Let us deﬁne a vector z randomly generated from the subspace spanned by v1, v2, v3 as follows. Let p, q, r be independently sampled according to N (0, 1). Then, deﬁne z = pu1 + qu2 + ru3. By this construction, we have that z, v ∼ N (0, ||v||22) for all vectors v ∈ {v1, v2, v3}, and further, ||z||22 = p2 + q2 + r2. Hence, for any v ∈ {v1, v2, v3}, we have

Pr (| z, v | ≤ ||v||2/6) ≤ ≤

||v||2 6

e−x2/2||v||22

dx

− ||v6||2 2π||v||22

||v||2 6
− ||v6||2

1

dx ≤

2π||v||22

3

||v||2

1 ≤√ .

2π||v||22 3 2π

Also, we can bound the norm of z by bounding p, q, r. We see that

∞ e−x2/2

1 ∞ xe−x2/2

e−12.5

Pr(p > 5) ≤

√ dx ≤

√ dx ≤

.

5

2π

55

2π

5

Similarly, Pr(p < −5) ≤ e−12.5/5. Applying the same calculations to q and r √and taking a union bound, we must have that with positive probability ||z||2 ≤ p2 + q2 + r2 ≤ 75 ≤ 10 and | z, v | ≥ ||v||2/6 for all v ∈ {v1, v2, v3}, implying there exists a vector z that satisﬁes the claim.

For this case, we will use the ﬁrst part of Lemma 7. Let z be the vector guaranteed by Lemma 8. Setting t = √ z , then Lemma 8 implies that
zT Σz

| z, v |

| t, v | = √

≥

√||v||2

zT Σz 6 zT Σz

for all v ∈ {v1, v2, v3}.

Recall that we deﬁned λΣ,U

max ||u|| =1 uT Σu to be the maximum amount a unit norm 2 u∈span(v1,v2,v3)

vector u belonging to the span of the vectors v1, v2, v3 is stretched by the matrix Σ. Note that λΣ,U

i√s also upper bounded by the maximum eigenvalue of Σ. Now, using the fact that ||z||2 ≤ 10 and zT Σz ≤ λΣ,U ||z||2 ≤ 10 λΣ,U , we obtain

| t, v | ≥ ||v||2 60 λΣ,U

for all v ∈ {v1, v2, v3}.

√ The part of Lemma 7 that we use depends on whether tT Σt = Ω max (| t, v1 |)

(7) or not. However,

the second part of the lemma is stronger and implies the ﬁrst part. Therefore, we simply use the lower bound in the ﬁrst part of the lemma, and we see that

fµt 0,µ1 − fµt 0,µ1 TV

= Ω min 1, max | t, µ0 − µ1 |, | t, µ0 − µ1 | min max | t, (µ0 − µσ(0)) |, | t, (µ1 − µσ(1)) |
σ∈H

(a)

(b)

v1 2 min( v2 2, v3 2)

= Ω min 1, | t, v1 | min(| t, v2 | , | t, v3 |) = Ω min 1,

,

λΣ,U

wherein step (a), we used the following facts (from deﬁnitions):

max | t, µ0 − µ1 |, | t, µ0 − µ1 | ≥ | t, v1 |

(8)

max | t, (µ0 − µ0) |, | t, (µ1 − µ1) | ≥ | t, v2 |

(9)

max | t, (µ0 − µ1) |, | t, (µ1 − µ0) | ≥ | t, v3 |

(10)

and in step (b), we used Eq. (7) for each v ∈ {v1, v2, v3}.

11

Case 2 (2 ||v1||2 ≤ min(||v2||2 , ||v3||2) or λΣ,U = O(||v1||2)): For this case, we will use the second part of Lemma 7. The random choice of t in Case 1 would have been suﬃcient for using the

second part of Lemma 7 when c ||v1||2 ≤ min(||v2||2 , ||v3||2) or λΣ,U = O(||v1||2) for some large constant c but with a deterministic choice of t that is described below, we can show that c = 2 is suﬃcient. Let t = √ v , where
vT Σv

v2

sv3

v = ||v2||2 + ||v3||2 with s = argmaxu∈{−1,+1} v2, uv3 .

Notice that we must have s v2, v3 > 0 from the deﬁnition of s. Then we see that

| v, v1 | =

v2, v1 + s v3, v1

||v2||2

||v3||2

≤ 2 ||v1||2 ≤ min(||v2||2 , ||v3||3)

s v2, v3 | v, v2 | = ||v2||2 + ||v3||2 ≥ ||v2||2 (11)

v2, v3

s v2, v3

| v, v3 | = ||v2||2 + s ||v3||2 = ||v2||2 + ||v3||2 ≥ ||v3||2 . (12)

The ﬁrst inequality follows the norm bound on v1 for this case, the second inequality uses that the deﬁnition of v and s imply that the second term in the sum is non-negative, and the third inequality uses the same logic and the fact that s ∈ {−1, 1}.
We just showed that | v, v1 | ≤ min(| v, v2 | , | v, v3 |), and hence t, v1 ≤ min( t, v2 , t, v3 ). This implies that the interval deﬁned by one pair of projected means is not contained within the interval deﬁned by the other pair of projected means. This means we can use the second p√art of Lemma 7. Furthermore, we also have tT Σt = 1. Finally, since ||v||2 ≤ 2, note that
vT Σv ≤ 2 λΣ,U . Using Lemma 7 with our choice of t, we see that

fµt 0,µ1 − fµt 0,µ1 TV

= Ω min 1, min max(| t, (µ0 − µσ(0)) |, | t, (µ1 − µσ(1) )|)
σ∈H

(a)
= Ω min 1, min | t, v2 | , | t, v3 |

(b)
=Ω

min

1, min(||v2||2 , ||v3||2)

.

λΣ,U

In step (a), we used Eq. (9) and (10), while in step (b), we used Eq. (11) and (12). The remaining

case is when √

λΣ,U = O(||v1||2). The second part of Lemma 7 applies because we observe that

tT Σt = O max (| t, µ0 − µ1 |, | t, µ0 − µ1 |) . To see this, recall that tT Σt = 1, and hence,

max | t, µ0 − µ1 |, | t, µ0 − µ1 | ≥ | t, v1 | = √zT v1 ≥ √||v1||2 ≥ ||v1||2 = Ω(1) = Ω
zT Σz 6 zT Σz 6 λΣ,U

tT Σt .

Next, recall that Lemma 8 implies that | t, v2 | ≥ ||v2||/6 and | t, v3 | ≥ ||v3||/6. Then, using the second part of Lemma 7, we have that

fµt 0,µ1 − fµt 0,µ1 TV

= Ω min 1, min max(| t, (µ0 − µσ(0)) |, |tT (µ1 − µσ(1))|)
σ∈H

(a)
= Ω min 1, min | t, v2 | , | t, v3 |

(b)
=Ω

min

1, min(||v2||2 , ||v3||2)

.

λΣ,U

12

Again in step (a), we used Eq. (9) and (10) while in step (b), we used Eq. (11) and (12). This completes the proof of Theorem 1.
5 Conclusion and Open Questions
We demonstrated the use of complex analytic tools to prove new lower bounds on the total variation distance between any two Gaussian mixtures with two equally weighted components and shared component variance. For a pair of mixtures with shared component variance, we provide guarantees on the total variation distance as a function of the largest gap (among the two mixtures) between the component means. Although intuitive, such a characterization was missing despite a vast literature on the total variation distance between mixtures of Gaussians with two components. We also extended our results to high dimensions and showed an elegant way via characteristic functions to reduce the problem to the one-dimensional setting. Finally, we should also point out that our lower bounds hold for all pairs of Gaussian mixtures with shared component covariance matrix without any assumptions on the component means; this was not the case in the prior results, which either needed the means to be bounded or the means of both mixtures to be zero.
The complex analytic tools in this work are elementary, and there is room for development. These tools may be helpful in proving bounds on statistical distance between more diverse distributions. For example, our analytic techniques do extend to mixtures of two Gaussians with shared covariance and certain non-equal mixing weights. To give a speciﬁc instance, for a mixture with weights c0 and c1, we could replace Lemma 3 so the lower bound only gains an additional multiplicative factor of min{c0, c1} when µ1 > µ1. We avoided stating our results in full generality of the mixing weights to not complicate our techniques and results. It would be useful and interesting to provide matching upper bounds on the total variation distance of two-component mixtures for all instances as a function of the means and covariance (generalizing the results for single Gaussians [14]). Extending our results to more general mixtures with k components and unknown component variances/weights (e.g., for Gaussian or even other families of distributions, such as those studied in [24]) will be of signiﬁcant interest to both the statistics and machine learning communities.
Acknowledgement: The work of A. Mazumdar and S. Pal is supported in part by NSF awards 2133484, 2127929, and 1934846.
References
[1] Murray Aitkin and Donald B Rubin. Estimation and hypothesis testing in ﬁnite mixture models. Journal of the Royal Statistical Society: Series B (Methodological), 47(1):67–75, 1985.
[2] Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Symposium on Theory of Computing, 2001.
[3] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of Gaussian mixtures via compression schemes. Journal of the ACM, 67(6):1–42, 2020.
[4] Ainesh Bakshi, Ilias Diakonikolas, Samuel B. Hopkins, Daniel Kane, Sushrut Karmalkar, and Pravesh K. Kothari. Outlier-robust clustering of gaussians and other non-spherical mixtures. In 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020, pages 149–159. IEEE, 2020.
13

[5] SS Barsov and Vladimir V Ul’yanov. Estimates of the proximity of gaussian measures. In Sov. Math., Dokl, volume 34, pages 462–466, 1987.
[6] Matthew Brennan and Guy Bresler. Optimal average-case reductions to sparse pca: From weak assumptions to strong hardness. In Conference on Learning Theory, pages 469–470. PMLR, 2019.
[7] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. arXiv preprint arXiv:1905.13229, 2019.
[8] Clément L Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou. Private identity testing for high-dimensional distributions. arXiv preprint arXiv:1905.11947, 2019.
[9] Jiahua Chen, Pengfei Li, et al. Hypothesis test for normal mixture models: The em approach. Annals of Statistics, 37(5A):2523–2542, 2009.
[10] Sanjoy Dasgupta. Learning mixtures of Gaussians. In Foundations of Computer Science, pages 634–644, 1999.
[11] Sanjoy Dasgupta and Leonard J Schulman. A two-round variant of EM for Gaussian mixtures. In Proceedings of the Sixteenth conference on Uncertainty in artiﬁcial intelligence, pages 152–159, 2000.
[12] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suﬃce for mixtures of two Gaussians. In Conference on Learning Theory, pages 704–710, 2017.
[13] Luc Devroye and Gábor Lugosi. Combinatorial methods in density estimation. Springer Science & Business Media, 2012.
[14] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018.
[15] Natalie Doss, Yihong Wu, Pengkun Yang, and Harrison H Zhou. Optimal estimation of high-dimensional Gaussian mixtures. arXiv preprint arXiv:2002.05818, 2020.
[16] Avi Feller, Evan Greif, Nhat Ho, Luke Miratrix, and Natesh Pillai. Weak separation in mixture models and implications for principal stratiﬁcation. arXiv preprint arXiv:1602.06595, 2016.
[17] Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Symposium on Theory of Computing, 2015.
[18] Philippe Heinrich and Jonas Kahn. Strong identiﬁability and optimal minimax rates for ﬁnite mixture estimation. The Annals of Statistics, 46(6A):2844–2870, 2018.
[19] Nhat Ho and XuanLong Nguyen. Convergence rates of parameter estimation for some weakly identiﬁable ﬁnite mixtures. The Annals of Statistics, 44(6):2726–2755, 2016.
[20] Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Symposium on Theory of Computing, 2018.
[21] Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.
14

[22] Daniel M Kane. Robust learning of mixtures of gaussians. arXiv preprint arXiv:2007.05912, 2020.
[23] Petri Kontkanen, Petri Myllymaki, Teemu Roos, Henry Tirri, Kimmo Valtonen, and Hannes Wettig. Topics in probabilistic location estimation in wireless networks. In 2004 IEEE 15th International Symposium on Personal, Indoor and Mobile Radio Communications, volume 2, pages 1052–1056. IEEE, 2004.
[24] Akshay Krishnamurthy, Arya Mazumdar, Andrew McGregor, and Soumyabrata Pal. Algebraic and analytic approaches for parameter learning in mixture models. In Proc. 31st International Conference on Algorithmic Learning Theory (ALT), volume 117, pages 468–489, 2020.
[25] Erich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science & Business Media, 2006.
[26] Hui Liu, Houshang Darabi, Pat Banerjee, and Jing Liu. Survey of wireless indoor positioning techniques and systems. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 37(6):1067–1080, 2007.
[27] Tudor Manole and Nhat Ho. Uniform convergence rates for maximum likelihood estimation under two-component gaussian mixture models. arXiv preprint arXiv:2006.00704, 2020.
[28] Ankur Moitra. Algorithmic aspects of machine learning. Cambridge University Press, 2018.
[29] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians. In Foundations of Computer Science, 2010.
[30] Jerzy Neyman and Egon Sharpe Pearson. Contributions to the theory of testing statistical hypotheses. University of California Press, 2020.
[31] Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71–110, 1894.
[32] D Michael Titterington, Adrian FM Smith, and Udi E Makov. Statistical analysis of ﬁnite mixture distributions. Wiley, 1985.
[33] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
[34] Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer relu networks. Advances in neural information processing systems, 32:8107–8117, 2019.
[35] Yihong Wu and Pengkun Yang. Optimal estimation of gaussian mixtures via denoised method of moments. Annals of Statistics, 48(4):1981–2007, 2020.
A Missing Proofs from Section 3
Here, we provide the proofs for Lemmas 3 and 5. Recall that we have indexed the means such that µ0 ≤ min(µ1, µ0, µ1) and µ0 < µ1.
Proof of Lemma 3. We use case analysis on diﬀerent orderings of the means and their separations.
15

Claim 1. For any t > 0 such that t(µ1 − µ0), t(µ1 − µ0), t(µ1 − µ0) ∈ [0, π4 ], when µ1 > µ1, eitµ0 + eitµ1 − eitµ0 − eitµ1 ≥ t√δ2 . 22
Proof. Assume that µ1 − µ1 ≥ µ0 − µ0, and recall that δ2 = max(|µ0 − µ0|, |µ1 − µ1|). First, we factor out the lowest common exponent to see that
eitµ0 + eitµ1 − eitµ0 − eitµ1 = 1 + eit(µ1−µ0) − eit(µ0−µ0) − eit(µ1−µ0) .

Let us denote φ1 = µ1 − µ0, φ0 = µ0 − µ0 and φ1 = µ1 − µ0. The following inequalities hold:

1 + eitφ1 − eitφ0 − eitφ1 ≥ sin(tφ1) − sin(tφ0) − sin(tφ1)

[|z| ≥ |Im(z)|]

≥ − sin(tφ1) + sin(tφ0) + sin(tφ1)

≥ − sin(tφ1) + sin(t(φ1 + (φ1 − φ1)))

= 2 sin t φ1 − φ1 cos t φ1 + φ1 − φ1

2

2

1 ≥ √ tδ2.
22

In the last line, we use that cos

t

φ1

+

φ1−φ1 2

≥ 1/√2 and sin t φ1−2 φ1

follows from the fact that

[Remove | · |] [sin(tφ0) ≥ 0]
≥ tδ42 , where the former

0 ≤ t φ1 + φ1 −2 φ1 = t(φ1 2+ φ1) = 12 t(µ1 − µ0) + t(µ1 − µ0) ≤ π4

and the latter follows from sin(x) ≥ x/2 for x ∈ R. If µ0 − µ0 > µ1 − µ1, then we can use a similar string of inequalities by using the fact that

eitµ0 + eitµ1 − eitµ0 − eitµ1 = 1 + eit(µ0−µ1) − eit(µ0−µ1) − eit(µ1−µ1) .

We denote φ0 = µ0 − µ1, φ0 = µ0 − µ1 and φ1 = µ1 − µ1. Note that all the φ are negative and φ0 > φ0 > φ1. The following holds:

eitφ0 + 1 − eitφ0 − eitφ1 ≥ sin(tφ0) − sin(tφ0) − sin(tφ1)

[|z| ≥ |Re(z)|]

= − sin(t|φ0|) + sin(t|φ0|) + sin(t|φ1|)

= − sin(t|φ0|) + sin(t|φ0|) + sin(t|φ1|)

≥ − sin(t|φ0|) + sin(t(|φ0| + (|φ0| − |φ0|)))

= 2 sin t(|φ0| − |φ0|) cos t |φ0| + |φ0| − |φ0|

2

2

1 ≥ √ tδ2.
22

In the last line, we use that sin

t(|φ0|−|φ0|) 2

≥ tδ42 and cos

t

|φ0|

+

|φ0|−|φ0| 2

[sin(·) odd] [Remove | · |] [sin(t|φ0|) ≥ 0]
√
≥ 22 .

Claim 2. For t > 0 such that t(µ1 − µ0), t(µ1 − µ0), t(µ1 − µ0) ∈ [0, π4 ], if both µ0, µ1 ∈ [µ0, µ1], then

eitµ0 + eitµ1 − eitµ0 − eitµ1 ≥ max

t2(δ1 − δ4)δ4 tδ3 ,√

.

2

42

16

√ Proof. First, we show the left hand side of the inequality in the claim statement is at least tδ3/(4 2).
Assume that µ0 − µ0 = δ2, recalling that δ2 = max(|µ0 − µ0|, |µ1 − µ1|). We factor out the lowest common exponent to see that
eitµ0 + eitµ1 − eitµ0 − eitµ1 = 1 + eit(µ1−µ0) − eit(µ0−µ0) − eit(µ1−µ0) .

Let us denote φ1 = µ1 − µ0, φ0 = µ0 − µ0 and φ1 = µ1 − µ0. To prove following inequalities, we need two facts. We use Fact I that ∂∂x (sin(x − y) − sin(x)) = cos(x − y) − cos(x) ≥ 0 for π4 ≥ x ≥ y ≥ 0. In particular, taking x = φ1 and y = µ1 − µ1, the inequality is increasing with respect to φ1, so so we can lower bound the function at φ1 = µ1 − µ1 + φ0. Additionally, we use Fact II that − sin(x + y) + 2 sin(x) ≥ sin((x − y)/2) cos(y/2) for 0 ≤ y ≤ x ≤ π/4, for the choice of x = φ0 and y = µ1 − µ1. Then, we have that

1 + eitφ1 − eitφ0 − eitφ1 ≥ sin(tφ1) − sin(tφ0) − sin(tφ1)

≥ − sin(tφ1) + sin(tφ0) + sin(tφ1)

≥ − sin(tφ1) + sin(tφ0) + sin(t(φ1 − (µ1 − µ1)))

≥ − sin(t(µ1 − µ1 + φ0)) + 2 sin(tφ0)

≥ sin t(−µ1 + µ1 + µ0 − µ0) cos t(µ1 − µ1)

2

2

≥ t√δ3 . 42

[|z| ≥ |Im(z)|] [Remove | · |]
[Fact I above] [Fact II above] [sin(x) ≥ x/2;
√ cos(·) ≥ 2/2]

Now, we assume that µ0 − µ0 < µ1 − µ1 and factor out eitµ1:

eitµ0 + eitµ1 − eitµ0 − eitµ1 = eit(µ0−µ1) + 1 − eit(µ0−µ1) − eit(µ1−µ1) .

As in the proofs of other claims, we let φ0 = µ0 − µ1, φ0 = µ0 − µ1 and φ1 = µ1 − µ1. Then, we have

eitφ0 + 1 − eitφ0 − eitφ1 ≥ sin(tφ0) − sin(tφ0) − sin(tφ1)

≥ − sin(t|φ0|) + sin(t|φ0|) + sin(t|φ1|)

≥ − sin(t|φ0|) + sin(t|φ0|) + sin(t|φ1|)

≥ − sin(t|φ0|) + sin(t|φ0| − |µ0 − µ0|) + sin(t|φ1|)

≥ − sin(t(|φ1| + |µ0 − µ0|)) + 2 sin(t|φ1|)

≥ sin t(|φ1| − |µ0 − µ0|) cos t|µ0 − µ0|

2

2

t

t

≥ √ · (|φ1| − |µ0 − µ0|) = √ · δ3.

42

42

[|z| ≥ |Im(z)|] [sin(·) odd] [Remove | · |]
[Fact I above] [Fact II above] [sin(x) ≥ x/2;
√ cos(·) ≥ 2/2]

In the application of Fact I, we let |φ0| be as small as possible, choosing |φ0| = |φ1| + |µ0 − µ0|. Next, we show the left hand side of the inequality in the claim statement is at least t2(δ1 − δ4)δ4/2.
Assume that µ0 − µ0 ≤ µ1 − µ1. First, we factor out the lowest common exponent to see

eitµ0 + eitµ1 − eitµ0 − eitµ1 = 1 + eit(µ1−µ0) − eit(µ0−µ0) − eit(µ1−µ0) .

17

Again, we denote φ1 = µ1 − µ0, φ0 = µ0 − µ0 and φ1 = µ1 − µ0. The following holds:

1 + eitφ1 − eitφ0 − eitφ1 ≥ |Re(1 + eitφ1 − eitφ0 − eitφ1 )|

= 1 + cos(tφ1) − cos(tφ0) − cos(tφ1)

≥ −1 − cos(tφ1) + cos(tφ0)

+ cos(t(φ1 − (µ1 − µ1)))

≥ −1 − cos(tφ1) + cos(tφ0) + cos(t(φ1 − φ0))

≥ t2(φ1 − φ0)φ0 ≥ t2(δ1 − δ4)δ4 .

2

2

[|z| ≥ |Re(z)|]
[Remove | · |] [µ1 − µ1 ≥ φ0] [Fact III below]

Recall that δ1 = max(|µ0 − µ1|, |µ0 − µ1|),and δ4 = min(|µ0 − µ0| , |µ1 − µ1|), so in the above, δ4 = µ0 − µ0 = φ0. The last line uses Fact III that −1 − cos(x) + cos(y) + cos(x − y) ≥ (x − y)y/2 for 0 ≤ y ≤ x ≤ π/4.
When µ0 − µ0 > µ1 − µ1 we use the same trick as in the previous claims and factor out eitµ1 instead of eitµ0. In particular the following holds:
eitµ0 + eitµ1 − eitµ0 − eitµ1 = eitµ0−µ1 + 1 − eit(µ0−µ1) − eit(µ1−µ1) .
Again, we denote φ0 = µ0 − µ1, φ0 = µ0 − µ1 and φ1 = µ1 − µ1. Note that all the φ are negative and φ0 > φ0 > φ1. The following holds:

eitφ0 + 1 − eitφ0 − eitφ1 ≥ |Re(eitφ0 + 1 − eitφ0 − eitφ1 )|

= cos(tφ0) + 1 − cos(tφ0) − cos(tφ1)

= cos(t|φ0|) + 1 − cos(t|φ0|) − cos(t|φ1|)

≥ − cos(t|φ0|) − 1 + cos(t(|φ0| − |µ0 − µ0|))

+ cos(t(|φ1|))

≥ − cos(t|φ0|) − 1 + cos(t(|φ0| − |φ1|))

+ cos(t|φ1|)

≥ t2(|φ0| − |φ1|)|φ1| ≥ t2(δ1 − δ4)δ4 .

2

2

[|z| ≥ |Re(z)|] [cos(·) even] [Remove | · |]
[µ0 − µ0 ≥ |φ1|] [Fact III above]

Proof of Lemma 5
Proof of Lemma 5. Deﬁne α and β such that µ0 − µ0 = ασ and |µ1 − µ1| = βσ; note that by assumption α, β ≤ 2. For x ∈ R, we use the notation x to denote the unique value such that x = 2πkcσ + xσ, where k ∈ Z is a integer and 0 ≤ x < 2πc. We prove this lemma with two cases, when µ1 > µ1 and when µ1 ≤ µ1. Without loss of generality, we assume that |µ0 − µ1| ≥ 100σ. Also, recall our assumption on the ordering of the unknown parameters that µ0 ≤ min(µ1, µ0, µ1) and µ0 ≤ µ1.
18

Case 1 (µ1 > µ1): Here, we will choose t = cσ, where

µ1 − µ0

c= 2πσ

µ1−µ0

.

80σ/π

Then substituting in t = 1/cσ, we see that eitx = eit2πkcσeitxσ = ei2πkeix/c = eix/c.

From the choice of c and the fact that x ≤ x and x/2 ≤ x for x ≥ 1, we see 40/π2 ≤ c ≤ 80/π2. As before, let φ1 = µ1 − µ0, φ0 = µ0 − µ0 and φ1 = µ1 − µ0. We prove that the following hold:

φ1 = 0
π2α ≤ φ0 = α ≤ π2α ≤ π2 80 c c 40 20 π2β ≤ φ1 = β ≤ π2β ≤ π2 . 80 c c 40 20

We prove these statements in order. To see that φ1 = 0, the deﬁnitions of c and φ1 imply that φ1 = 2πσk 2πσ φ1φπ1/(80σ) + φ1σ, for k = φ1π/(80σ) and φ1 = 0.
Next, since ασ = φ0, we can write α/c = 2πk + φ0/c, and it would follow that α/c = φ0/c if φ0 < 2πσc = φ1/ φ1π/(80σ) . Indeed this is the case, since
φ1/ φ1π/(80σ) ≥ 80σ/π > 2σ > φ0.

Using the fact that φ1 = 0, we will show φ1/c = β/c. We break up φ1 into φ1 + µ1 − µ1, writing

φ1

φ1

φ1 = βσ + φ1 = βσ + k

φ1π/(80σ)

+ φ1σ = βσ + k

, φ1π/(80σ)

for k = φ1π/(80σ) . If β < 2πc, then this choice of k is correct for the deﬁnition of φ1, and it follows that φ1 = β. This is indeed the case as 2πc = σ φ1πφ/1(80σ) ≥ 80/π > β.
We use our lower bounds on φ0/c and φ1/c and the fact that φ1 = 0 in the following:

e− σ2t2 2

eitµ0 + eitµ1 − eitµ0 − eitµ1

=

e− σ2t2 2

1 + eitφ1 − eitφ0 − eitφ1

≥ e− 2c12 Im(1 + eiφ1/c − eiφ0/c − eiφ1/c)

= e− 2c12 sin(φ1/c) − sin(φ0/c) − sin(φ1/c)
= e− 2c12 (sin(φ0/c) + sin(φ1/c)) ≥ e−1 max(sin(φ0/c), sin(φ1/c)) ≥ e−1 max(sin(π2α/80), sin(π2β/80)) ≥ π2δ2 .
160e

19

Case 2 (µ1 ≤ µ1): First we consider the case when β ≤ α. Since µ1 − µ0 ≥ 100σ, we must have µ1 − µ0 ≥ µ1 − µ0 − (µ1 − µ1) ≥ 100σ − 2σ = 98σ. We choose t = cσ for

c=

µ1 − µ0

.

3πσ/2 + 2πσ µ810−σ/µπ0

As before, for any x ∈ R we write x = 2πkcσ + xσ where k ∈ Z is a positive integer and

0

<

x

<

2πc.

From

the

choice

of

c

and

the

fact

that

µ1

− µ0

≥

98σ,

25 π2

≤

c

≤

80 π2

.

Here

we

denote

φ1 = µ1 − µ1, φ0 = µ0 − µ0 and φ1 = µ1 − µ0; note this is diﬀerent than our previous φ deﬁnitions.

We will show the following set of inequalities and equalities:

φ1 = 3π c2 π2α ≤ φ0 = α ≤ π2α ≤ π2 80 c c 25 12 π2β ≤ φ1 = β ≤ π2β ≤ π2 . 80 c c 25 12

To see that φ1 = 3π/2, observe ﬁrst that φ1/(cσ) = 2πk+φ1/c; then we can simplify φ1/(cσ) and write φ1/(cσ) = 3π/2 + 2π φ1π/(80σ) . Together these imply that 3π/2 + 2π φ1π/(80σ) = 2πk + φ1/c. Taking k = φ1π/(80σ) , it follows that φ1 = 3π/2.
Additionally, since ασ = φ0, we can write α/c = 2πk + φ˜0/c. It follows that α/c = φ˜0/c if

φ < 2πσc = 2πσ

φ1

=

φ1

.

0

3πσ/2 + 2πσ φ1π/(80σ) 3/4 + φ1π/(80σ)

Indeed this is the case, since if φ1π/(80σ) < 1/4,

φ1

>φ >φ

3/4 + φ1π/(80σ)

1

0

and if φ1π/(80σ) ≥ 1/4,

φ1

>

φ1

> 80σ/(4π) > 6σ > φ .

3/4 + φ1π/(80σ) 4 φ1π/(80σ)

0

A similar line of reasoning shows that φ1/c = β/c. Here β/c = 2πk + φ1/c, so it remains to show

φ1 < 2πσc =

φ1

.

3/4 + φ1π/(80σ)

Indeed this is the case, since if φ1π/(80σ) < 1/4,

φ1

> φ > 98σ > 2σ > µ1 − µ = φ1,

3/4 + φ1π/(80σ)

1

1

and if φ1π/(80σ) ≥ 1/4, then

φ1

>

φ1

> 80σ/(4π) > 6σ > φ1.

3/4 + φ1π/(80σ) 4 φ1π/(80σ)

20

Setting t = 1/cσ, the following calculation holds if β ≤ α:

e− σ2t2 2

eitµ0 + eitµ1 − eitµ0 − eitµ1

=

e− σ2t2 2

1 + eit(φ1+φ1) − eitφ0 − eitφ1

= e− 2c12

1

+

ei

φ1 c

ei

φ1 c

−

ei

φ0 c

−

ei

φ1 c

≥ e− 2c12

Im(1

+

ei

φ1 c

ei

φ1 c

−

ei

φ0 c

−

ei

φ1 c

)

≥ e− c12 (−1 + cos φ˜1 + sin φ˜0 )

c

c

=

e−

1 2c2

(−1

+

cos

β

+

sin

α )

c

c

≥

e−

1 2c2

(α/c

−

(α/c)3/6

−

(β/c)2/2)

≥

e−

1 2c2

(α/c

−

(α/c)3/6

−

(α/c)2/2)

≥ e− 2c12 α ≥ π2δ2 . 3c 240e

The fourth to last inequality follows because sin x ≥ x − x63 and cos x ≥ 1 − x22 . The third to last

inequality

follows

because

β

≤

α

and

α/c

<

1.

In

the

ﬁnal

step,

we

re-used

the

fact

that

25 π2

≤

c

≤

80 π2

.

If α < β, then we can do a very similar proof by choosing t = cσ for

c=

µ1 − µ0

.

3πσ/2 + 2πσ µ810−σ/µπ0

From

the

choice

of

c

and

the

fact

that

µ1 −µ0

≥

98σ,

25 π2

≤

c

≤

80 π2

.

Here

we

denote

φ1

=

µ1 −µ1, φ0

=

µ0 − µ1 and φ0 = µ0 − µ0. From the same explanations as in the case when β ≤ α, we see that

|φ0| = 3π c2

π2β ≤ |φ1| = β ≤ π2β ≤ π2 80 c c 25 12

π2α ≤ |φ0| = α ≤ π2α ≤ π2 . 80 c c 25 12

We obtain the same bound as in the case of β ≤ α by factoring out eitµ1 and using a similar

calculation:

e− σ2t2 2

eitµ0 + eitµ1 − eitµ0 − eitµ1

=

e− σ2t2 2

eit(µ0−µ1) + 1 − eit(µ0−µ1) − eit(µ1−µ1)

=

e− σ2t2 2

eit(φ0+φ0) + 1 − eitφ0 − eitφ1

= e− 2c12

1

+

ei

φ0 c

ei

φ0 c

−

ei

φ0 c

−

ei

φ1 c

≥ e− 2c12 Im

1

+

ei

−|φ0 | c

ei

−|φ0 | c

−

ei

−|φ0 | c

−

ei

−|φ1 | c

≥ e− c12 = e− c12

−1 + cos |φ˜0| + sin |φ˜1|

c

c

α

β

−1 + cos + sin ,

c

c

21

and the rest of the proof follows as before, just swapping α and β. 22

