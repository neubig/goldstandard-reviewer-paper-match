Learning from Crowds by Modeling Common Confusions
Zhendong Chu, Jing Ma, Hongning Wang
Department of Computer Science, University of Virginia {zc9uy, jm3mr, hw5x}@virginia.edu

arXiv:2012.13052v2 [cs.LG] 12 Jun 2021

Abstract
Crowdsourcing provides a practical way to obtain large amounts of labeled data at a low cost. However, the annotation quality of annotators varies considerably, which imposes new challenges in learning a high-quality model from the crowdsourced annotations. In this work, we provide a new perspective to decompose annotation noise into common noise and individual noise and differentiate the source of confusion based on instance difﬁculty and annotator expertise on a per-instance-annotator basis. We realize this new crowdsourcing model by an end-to-end learning solution with two types of noise adaptation layers: one is shared across annotators to capture their commonly shared confusions, and the other one is pertaining to each annotator to realize individual confusion. To recognize the source of noise in each annotation, we use an auxiliary network to choose from the two noise adaptation layers with respect to both instances and annotators. Extensive experiments on both synthesized and real-world benchmarks demonstrate the effectiveness of our proposed common noise adaptation solution.
Introduction
The availability of large amounts of labeled data is often a prerequisite for applying supervised learning solutions in practice. Crowdsourcing makes it possible to collect massive labeled data in both time- and cost-efﬁcient manner (Buecheler et al. 2010). However, because of varying and unknown expertise of annotators, crowdsourced labels are usually noisy, which naturally lead to an important research problem: how to train an accurate learning model with only crowdsourced annotations?
The ﬁrst step to estimate an accurate learning model from crowdsourced annotations is to properly model the generation of such data. In this work, we focus on the crowdsourced classiﬁcation problem. The seminal work from Dawid and Skene (1979) (known as the DS model) assumes that each annotator has his/her own class-dependent confusion when providing annotations to instances. This is modeled by an annotator-speciﬁc confusion matrix, whose entries are the probability of ﬂipping one class into another. The DS model has become the cornerstone of most learning from crowds solutions; and mainstream solutions perform label aggregation prior to classiﬁer training: their key difference lies on
Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.

different label aggregation methods based on the DS model (Venanzi et al. 2014; Zhang et al. 2014; Whitehill et al. 2009). Recent developments focus more on uniﬁed solutions, where variants of the Expectation-Maximization (EM) algorithm are proposed to integrate label aggregation and classiﬁer training (Albarqouni et al. 2016; Cao et al. 2019; Raykar et al. 2010). Typically, such solutions treat the classiﬁer’s predictions as latent variables, which are then mapped to the observed crowdsourced labels using individual confusion matrices of annotators. Rodrigues and Pereira (2018) further fuse label inference and classiﬁer training in an endto-end approach using neural networks, where the gradient from label aggregation is directly propagated to estimate the annotators’ confusion matrices. Tanno et al. (2019) propose a similar solution but encourage the annotator confusion matrix to be close to an identity matrix by trace regularization.
All existing DS-model-based solutions assume noise in crowdsourced labels is only caused by individual annotators’ expertise. However, it is not uncommon that different annotators would share common confusion about the labels. For example, when a bird in an image is too small, every annotator has a chance to confuse it with an airplane because of the background sky. We hypothesize that on an instance the annotator is conﬁdent about, he/she is more likely to use his/her expertise to provide a label (i.e., introducing individualized noise), while he/she would use common sense to label those unconﬁdent ones. We empirically evaluate this hypothesis on two public crowdsourcing datasets, one for image labeling and one for music genre classiﬁcation (more details of the datasets can be found in the Experiment Section), and visualize the results in Figure 1. On both datasets, there are quite some commonly made mistakes across annotators. For example, on the image labeling dataset LabelMe, 61.0% annotators mistakenly labeled street as inside city and 44.1% of them mislabeled open country as forest; on the music classiﬁcation dataset, 63.6% annotators mislabeled metal as rock and 38.6% of them mislabeled disco as pop. The existence of such shared confusions across annotators directly affects label aggregation: the majority of annotators are not necessarily correct, as their mistakes are no longer independent (e.g., those large off-diagonal entries in Figure 1). This is against the fundamental assumption in the DS model, and strongly urges new noise modeling to better handle real-world crowdsourced data.
Moving beyond the independent noise assumption in the

(a) LabelMe

(b) Music

Figure 1: Analysis of commonly made mistakes across annotators on two real-world crowdsourcing datasets. The value of each entry in the heatmap denotes the percentage of annotators with this confusion pair (e.g., mistakenly label street as inside city on LabelMe dataset).

family of DS models (Dawid and Skene 1979; Rodrigues and Pereira 2018), we decompose annotation noise into two sources, common noise and individual noise, and differentiate the source of noise based on both annotators and instances. We refer to the annotation confusions shared across annotators as common noise, and model it by a global confusion matrix shared by all annotators. In the meanwhile, we also maintain annotator-speciﬁc confusion matrices for individual noise modeling. We still treat ground-truth labels of instances as latent variables, but map them to noisy annotations by two parallel confusion matrices, to capture these different sources of noise. We determine the choice of confusion matrices on a per-instance-annotator basis, by explicitly modeling of annotator expertise and instance difﬁculty (Whitehill et al. 2009; Yin et al. 2017). To leverage the power of representation learning to model annotator expertise and instance difﬁculty, we realize all our model components using neural networks. In particular, we model the two types of confusion matrices as two parallel noise adaptation layers (Goldberger and Ben-Reuven 2016). For each annotator-instance pair, the classiﬁer ﬁrst maps the instance to a latent class label, then an auxiliary network decides which noise adaptation layer to map the latent class label to the observed annotation. Cross-entropy loss is counted on the predicted annotations for end-to-end training of these components. We name this approach CoNAL - learning from crowds with common noise adaptation layers. Extensive experiments show considerable improvement of our new noise modeling approach against a rich set of baselines on two synthesized datasets, including a fully synthesized dataset and one based on CIFAR-10 dataset with various settings of noise generation, as well as two real-world datasets, e.g., LabelMe for image classiﬁcation, and Music for music genre classiﬁcation.
Related Works
Several existing studies focused on modeling the different roles of instance and annotator in crowdsourced data. Whitehill et al. (2009) model the accuracy of each annotation, which depends on instance difﬁculty and annotator expertise, to weigh each instance in ﬁnal majority vote. Welinder et al. (2010) model each annotator as a multi-dimensional classiﬁer and consider instance difﬁculty as single dimension latent variable. Zhou et al. (2012) propose a minimax

entropy principle on a probability distribution over annotators, instances and annotations, in which by minimizing entropy instance confusability and annotator expertise are naturally inferred. Khetan and Oh (2016) and Shah, Balakrishnan, and Wainwright (2016) consider generalized DS models which model the instance difﬁculty. Instead of simply using a single scalar to model instance difﬁculty and annotator expertise as in previous works, we model them by learning their corresponding representations via an auxiliary network, which can better capture the shared statistical pattern across observed annotations.
Our method is closely related to several existing DS-based models considering relations among annotators; but it is also clearly distinct from them. Kamar, Kapoor, and Horvitz (2015) use a global confusion matrix to capture the identical mistakes by all annotators, and it is designed to replace the individual matrix when observations of an annotator are rare. Moreover, the choice of confusion matrix in this solution only depends on the number of annotations an annotator provided. This unnecessarily reﬂects the annotator expertise, as the task assignment is typically out of their control in crowdsourcing. Venanzi et al. (2014) and Imamura, Sato, and Sugiyama (2018) cluster annotators to generate their own confusion matrices from a shared communitywide confusion matrix. However, the above approaches still assume a single underlying noise source, and thus they do not consider the difference between global (or communitylevel) and individual confusions. Li, Rubinstein, and Cohn (2019) explore the correlation of annotation across annotators by classifying them into auxiliary subtypes under different ground-truth classes. However, the characteristics of each annotator are missing since they are only represented by a speciﬁc subtype. In our work, we still characterize individual annotators by modeling their own confusions.
Common Confusion Modeling in Crowdsourced Data
In this section, we formulate our problem-solving framework for training classiﬁers directly from crowdsourced labels, based on the insight of common confusion modeling across annotators. We ﬁrst describe the notations and our probabilistic modeling of the noisy annotation process, considering both common and individual confusions. This probabilistic model of noisy annotations is the basis of the endto-end neural solution we develop in this paper.
Notations and Probabilistic Modeling
Assume we have N instances labeled by R annotators out of C possible classes. We deﬁne xi as the feature vector of the i-th instance and yir as its label provided by the r-th annotator. Denote zi as the unobservable ground-truth label for the i-th instance, which is considered as a latent variable sampled from a multinomial distribution parameterized by {p(zi = c|xi)}Cc=1. For simplicity, we collectively deﬁne X = {xi}Ni=1, Y = {yir}Ni=,1R,r=1 and Z = {zi}Ni=1. The ﬁnal goal of learning from crowds is to obtain the classiﬁer P (Z|X) only with crowdsourced annotations Y .
Similar to the DS-based models (see Figure 2a for reference), the confusion of the r-th annotator is measured

by an annotator-speciﬁc confusion matrix πr, in which the (z, z )-element πzr,z denotes the probability that annotator r

xi xi zi zi

xi xi zi zi sir sirπr πr

will label the true label z as z . Aside from individual confusion, the key assumption of our solution is that annotation mistakes can also be introduced by common confusion,

NN

yir yir

πr πr
RR

NN

yir yir

πg πg

RR

which is modeled by a globally shared confusion matrix πg

(a) DS model.

(b) Common noise model.

across all annotators. We deﬁne the confusion matrices set

as Π = {π1:R, πg}. We associate a Bernoulli random vari-

Figure 2: Graphical model presentations of DS model and

able sri ∼ B(ωir) with each annotation yir to differentiate the source of noise on it: sri =1 if the confusion is caused by the common noise, where wir is the probability of the global confusion matrix being chosen by annotator r on instance i

our common noise model. Remarks. This result extends the known lower bound re-

(see Figure 2b). Denote the set of parameters governing the

sult of DS models (Imamura, Sato, and Sugiyama 2018).

generation of sri across all annotations as Ω. Suggested by the successful practice in modeling crowd-

Lower bound on the error rate measures the difﬁculty of a crowdsourcing problem. Theorem 1 suggests the proposed

sourced data, we also impose the following two commonly

decomposition has the potential to further reduce the lower

made assumptions: 1) each annotator provides their annota-

bound, i.e., to obtain better inferred true labels. To under-

tions independently (Dawid and Skene 1979); and 2) each

stand this result, we should ﬁrst note that the lower bound

annotation is independent from the instance’s features given

mainly depends on the KL distance between the class dis-

the ground-truth labels (Yan et al. 2014; Rodrigues and

tributions conditioned on different ground-truth classes, as

Pereira 2018). We should note the ﬁrst assumption is not

deﬁned in F (ρi, Π, Ω), i.e., how two different classes will

contradicting to our common confusion modeling: as the an-

be confused with other classes. The more different they are

notators can independently choose the shared common noise

(i.e., a larger KL distance), the easier one can differentiate

model to generate their annotations, the resulting observed

the two from the observed noisy labels. For example, con-

annotations are no longer independent across annotators. As

sider a crowdsourced dataset where an annotator labels a set

a result, the complete data likelihood of observed annota-

of instances as airplane; but among them, 50% cases should

tions under our model can be deﬁned as,

be bird, and the other 50% should be spacecraft. Intuitively,

NRC

without any additional knowledge, it is hard to determine

p (Y, Z|X, Π, Ω) =

p (yir|zi; Π, ωir)p(zi|xi) ,

i=1 r=1 z=1
(1)

p (yir|zi; Π, ωir) = ωirp (yir|zi, πg) + (1 − ωir) p (yir|zi, πr) .

the true label when he/she labels an instance as airplane. And this is asserted by Theorem 1: If we only used a single confusion matrix for this annotator, the conditional class distributions for bird and spacecraft will be pushed closer, because their entries on airplane are close. This causes a

Based on the above imposed problem structure, we derive an information-theoretical lower bound about the resulting noise modeling quality. Let Zˆ be the estimated true

smaller KL term in F (ρi, Π, Ω) between bird and spacecraft (e.g., setting ωir=0 for all instances in annotator r). But if we knew that the confusion between bird and airplane is caused

labels of all instances. Noise modeling quality is measured

by the error rate given by L(Zˆ, Z) = N1

N i=1

I

(zˆi

=

zi),

where I(·) is an indicator function. Given the ground-truth

instance-speciﬁc class distribution ρi = {ρic}Cc=1 and con-

fusion matrices Π, we have the following theorem about the

by common noise, and the confusion between spacecraft and airplane is caused by individual noise, these mistakes could be attributed to two confusion matrices separately, which eliminates the misleading similarity between the conditional probabilities for bird and spacecraft caused by airplane.

lower bound of minimax error rate of our model.
Theorem 1. The minimax error rate of our model is lower bounded by

infZˆsupZ∈[C]N E L(Zˆ, Z)

(2)

End-to-end Learning Framework
To apply our noise modeling in crowdsourced data, we need to estimate the confusion matrices Π together with the classiﬁer. Instead of building a vanilla tabular model for them,

1

N

log 2

≥ N 2log C F (ρi, Π, Ω) − N 2log C ,

i=1

we realize them using neural models, to take advantage of the power of representation learning. In particular, we map the output of the classiﬁer to noisy annotations by two types of confusion layers, which we refer to as noise adaptation

RC C

layers (Goldberger and Ben-Reuven 2016). We also intro-

F (ρi, Π, Ω) =H(ρi) −

ρicρic ωir KL(πcg∗ πcg ∗) duce an auxiliary network that takes both annotator and in-

r=1 c=1 c =1

stance as input to predict the choice of these two noise adap-

+ (1 − ωir) KL (πcr∗ πcr ∗) .

tation layers. Since we treat the ground-truth label of an instance as a latent variable, the Expectation Maximization

where H(ρi) = −

C c=1

ρiclogρic

is

the

entropy

of

ground-truth class distribution and πc∗ is the c-th row in con-

(EM) algorithm becomes a natural choice for model learning, as typically done in literature (Albarqouni et al. 2016; Rodrigues and Pereira 2018; Bertsekas 2014). For the in-

fusion matrix π. The proof and further discussion of Theo-

tegrity of work, we provide the derived EM algorithm in

rem 1 is provided in Appendix A.

Appendix B for interested readers. However, the EM-based

algorithm has several clear drawbacks in our solution: 1) In crowdsourced data, because the annotators typically only label a small proportion of instances, EM-based algorithm becomes very sensitive to the initialization of model parameters. It can easily cause instability issues in training a neural network model. 2) In every EM iteration, we need to retrain the neural network, which causes a huge overhead when handling large networks. Instead, we take an end-toend approach to jointly perform latent variable inference and model parameter estimation. We deﬁne cross-entropy loss on the observed annotations and use error back-propagation to update the classiﬁer’s output and the network parameters simultaneously.

xi
e1:R
input

Classi er
θ
Aux.Net
Wa
backbone model

Wgfi

fi

× (1 − ωi1:R)

+=

W1:Rfi ωi1:R

× ωi1:R

h1:R

i

parallel noise adaptation layers

predicted anno. dist.

Figure 3: Overview of our framework for classiﬁcation with 3 classes and R annotators.

We construct a neural network classiﬁer with non-linear intermediate layers and a softmax output layer. The probability distribution of the predicted true label zi given the instance feature vector xi is thus speciﬁed as pθ(zi|xi), where θ is the network parameter set including the softmax layer. We denote the immediate output of the classiﬁer as fi = f (xi) ∈ RC . We then use noise adaptation layers to map the classiﬁer’s output into noisy annotations, which are implemented by introducing additional softmax output layers on top of the output layer of the classiﬁer (see overview in Figure 3). The weight matrices of the noise adaptation layers resemble confusion matrices Π in a probabilistic sense. The output of the noise adaptation layer is thus the probability distribution of predicted annotation pW (yˆir|f (xi)), where W is the parameter set of the noise adaptation layer.
We consider two types of noise adaptation layers: one individual noise adaptation layer for every annotator parameterized by W r, and a common noise adaptation layer shared across all annotators parameterized by W g. The ﬁnal probability distribution of annotations is obtained as,
p(yˆir|xi) = ωir pW g (yˆir|f (xi)) + (1 − ωir) pW r (yˆir|f (xi)).
where ωir governs the distribution that the mistake of annotator r on instance i is caused by common confusion πg, denoted by the noise source indicator sri .
As sri is unobservable, we introduce an auxiliary network to model sri ∼ B(ωir) by parameterizing it over annotator expertise and instance difﬁculty, both of which are modeled via learnt representations by the auxiliary network. Specifically, as in our problem setup, every instance is associated

with raw features, the auxiliary network takes instance feature xi as input for learning instance i’s embedding vi. The same can be applied to annotator r, if any raw feature er is available about the annotator, otherwise we use its onehot encoding as input for learning annotator embedding ur. Then ωir can be obtained as follows,

vi = W vxi + bv, ur = W uer + bu, ωir = σ(ur vi). (3)

where (W v, bv) and (W u, bu) are weight matrices and bias terms for annotator and instance embeddings, and σ is a sigmoid function. To simplify our notations, we collectively refer the parameters in this auxiliary network as W a. To avoid the magnitude of learnt u and v becoming extremely large or small, which causes numerical issues in estimating ωir, we normalize the learnt annotator and instance embeddings before computing their inner product.
Based on the above full speciﬁcations of our probabilistic modeling using neural networks, we are ready to estimate the network parameters. We can easily verify that, maximizing the likelihood of observed annotations given the input feature vectors as deﬁned in Eq (1) is equivalent to minimizing the cross-entropy loss between the observed annotations and predicted annotation distributions,

g

1:R

a

1N R C r

r

L(θ, W , W , W ) = − N

yijlog pj(yˆi |xi).

i=1 r=1 j=1

where yirj = 1 if yir = j; otherwise yirj = 0; and pj(yˆir|xi) refers to the j-th entry of the predicted annotation distribution. All parameters can be trained by back-propagation using gradient descent techniques, such as Adam (Kingma and Ba 2014) and SGD (Goodfellow, Bengio, and Courville 2016). Once trained, in the testing phase, we can directly use the classiﬁer to make predictions on new instances.
The gradient ﬂow in back-propagation reveals how our common confusion modeling handles crowdsourced data. In the context of classiﬁcation, we can simply view the introduced noise adaptation layer as performing a projection of gradients; and with a slight abuse of notations, we denote the output of our noise adaptation layers as hri = ωirW gfi + (1 − ωir)W rfi. Under the chain rule, the gradients are naturally decoupled with respect to different sources of noise,

∂L R ∂L ∂hri R r ∂L g

r ∂L r

∂fi = r=1 ∂hri ∂fi = r=1 ωi ∂hri W +(1−ωi ) ∂hri W .

(4)

It clearly shows confusion matrices reshape the gradients,

which informs the classiﬁer layer what the true label should

be on an instance given its noisy annotations. The impor-

tance of each confusion matrix in shaping the classiﬁer is determined by ωir, which infers the source of noise based on annotator expertise and instance difﬁculty.

The gradients in Eq (4) also suggest a potential bottle-

neck of our proposed solution: if the common and individ-

ual noise adaptation layers are unidentiﬁable, we cannot cor-

rectly attribute the noise, which is the key for our solution to

Figure 4: Results on CIFAR-10 dataset.

perform according to Theorem 1. To avoid this, we add 2norm on the difference between the common and individual noise adaptation layers as a regularization term, to enforce them to be different. This presents our ﬁnal loss function,

g

1:R

a

1N R C r

r

L(θ, W , W , W ) = − N

yijlog pj(yˆi |xi)

i=1 r=1 j=1

R

−λ

Wg −Wr 2

r=1

where λ is a hyper-parameter to control regularization.

Experiments
We evaluate our method on both synthesized and real-world datasets. We consider a rich set of related solutions as our baselines, which can be divided into two categories: 1) Methods with simple noise models. DL-MV: it learns a neural network classiﬁer with labels aggregated by majority voting. DL-CL (Rodrigues and Pereira 2018): it learns a neural classiﬁer with designated layers to ﬁt individual annotator confusions (so-called crowd layer). Anno-Reg (Tanno et al. 2019): it improves DL-CL by imposing additional trace regularization on individual confusion matrices. Doctor Net (Guan et al. 2018): it learns a neural network for every annotator’s annotations and aggregates the networks’ output by weighted majority voting. Max-MIG (Cao et al. 2019): it jointly estimates a neural classiﬁer and a label aggregation network using an information-theoretical loss function. 2) Methods with complex noise models. DL-GLAD: it learns a neural classiﬁer with labels aggregated by GLAD (Whitehill et al. 2009), where annotator ability and instance difﬁculty are modeled. DL-WC: it learns a neural classiﬁer with labels aggregated by WC (Imamura, Sato, and

Sugiyama 2018), where similar annotators are clustered to share the same confusion matrix. AggNet (Albarqouni et al. 2016): an EM-based deep model considering annotator sensitivity and speciﬁcity.
Experiments on Synthesized Datasets
We evaluate the proposed method under various settings of synthesized data. Particularly, we demonstrate the effectiveness of our model with different (1) common confusion types; (2) common noise strength, which is deﬁned as the sum of off-diagonal entries in the common confusion matrix; and (3) proportion of common noise, which reﬂects the percentage of annotations introduced by common confusion. Datasets description. We generate synthesized crowdsourced data on two datasets, where we directly manipulate the number of annotators and annotation generation under a variety of settings. On the Synthetic dataset, we completely synthesized everything. We ﬁrst sample a mean vector for every class and then sample instance features from a multivariate Gaussian distribution parameterized by this mean vector. In particular, we randomly generate 10,000 instances with 6 classes, which are split into a 8,000-instance training set, a 1,000-instance validation set and a 1,000-instance testing set. The CIFAR-10 dataset is generated based on the CIFAR-10 image classiﬁcation dataset (Krizhevsky, Hinton et al. 2009). It consists of 60,000 32 × 32 color images from 10 classes, which are split into a 40,000-instance training set, a 10,000-instance validation set and a 10,000-instance testing set. Image features are used to train the neural classiﬁer on this dataset. In both datasets, each instance in the training set is labeled by averaging 3 randomly selected annotators out of 30 in total. Synthesizing annotations. We consider two representative noise patterns in common noise: (1) Asymmetric confusion. Every class is mapped to another uniformly chosen class

(a) common noise

(b) annotator 1

(c) annotator 2

(d) annotator 3

(e) common noise

(f) annotator 1

(g) annotator 2

(h) annotator 3

Figure 5: Comparison between ground truth confusion matrices and learned ones on CIFAR-10 dataset. The top row is the result of asymmetric common noise. The bottom row is the result of symmetric common noise.

on both datasets. (2) Symmetric confusion. On Synthetic dataset, two random classes are paired and ﬂipped into each other. And on CIFAR-10 dataset, we manually paired similar classes (e.g., bird and airplane) to be ﬂipped with each other. For individual confusion matrices, we use asymmetric confusion. We generate one global confusion matrix, and one individual confusion matrix for every annotator. In our experiments, the range of common noise strength is set to [0.4, 0.8], while the individual noise strength of annotators is ﬁxed to 0.7. In both noise generation patterns, the noise strength is evenly distributed among the chosen off-diagonal entries.
To control the source of noise in each annotation, i.e., sri , we randomly generate a set of annotator features u, which are not disclosed to the learners. Given instance feature vector vi and annotator feature vector ur, we compute ωir by Eq (3) with the ground-truth weight matrices (W u, bu) and (W v, bv). These weight matrices are not disclosed to the learner. The bias terms are used to control the average proportion of common noise across annotations into a range of [0.3, 0.7]. When we generate annotation yir for instance i by annotator r, we ﬁrst sample sri ∼ B(ωir). If sri = 1, the common confusion matrix πg will be used; otherwise, individual confusion matrix πr will be used. Then we sample yir from the chosen confusion matrix based on the true label zi of this instance. We also include a special case that the proportion is 0, where there is no common confusion.
In our experiments, when studying the inﬂuence of common noise strength on the learnt classiﬁer, the average proportion of common noise is controlled to be around 0.5. When studying the inﬂuence of the proportion of common noise in each annotation, the common and individual noise strength is controlled to 0.4 and 0.7 respectively. Backbone networks & training details. On the Synthetic dataset, we apply a simple network with only one fully connected (FC) layer (with 128 units and ReLU activations), along with a softmax output layer, using 50% dropout. On the CIFAR-10 dataset, we follow the setting of Cao et al. (2019) to use VGG-16 as the backbone network. We trained the network using the Adam optimizer (Kingma and Ba 2014) with default parameters and learning rate searched

from {0.02, 0.01, 0.005}. The dimension of annotator and instance embedding is chosen from {20, 40, 60, 80}. The regularization term λ is searched from {10−4, 10−5, 10−6}. All experiments are repeated 5 times with different random seeds. Model selection is achieved by choosing the model with the highest accuracy on the validation set. We report mean and standard deviation of test accuracy on the ﬁve runs. To make the comparisons fair, all the evaluated methods used the same backbone networks. We implement our framework with PyTorch, and run it on a CentOS system with one NVIDIA 2080Ti GPU with 10 GB memory. Results. We report the results on the CIFAR-10 dataset in Figure 4, where our solution demonstrated consistent improvement against all baselines across all settings. The observation on the Synthetic dataset is similar, and we present the results in Appendix C due to space limit. All the baselines assumed single source of noise, i.e., annotator-speciﬁc noise; as a result, they are heavily inﬂuenced when noise become complicated, e.g., a large proportion of mistakes from common confusion and the strength of common noise is strong. Our solution is less sensitive to the environment by decomposing and separately modeling the confusion. When there is no common confusion, the empirical result shows no signiﬁcant difference between our solution and baselines in this extreme setting, which should also be expected. But we argue that this extreme setting rarely holds in reality, as annotators always share some commonsense about the world.
All models are inﬂuenced by symmetric common noise, which directly makes the swapped classes similar. Based on the lower bound provided in Theorem 1, similar conditional class distributions in the confusion matrices will make the problem more difﬁcult, so that the degeneration of all methods are expected under symmetric confusion. In the most extreme case where the proportion of common noise is set to 0.7 and the common noise strength is set to 0.6, nearly 42% annotations are pairwise ﬂipped. However, our method can still outperform baselines with a large margin. Mix-MIG is believed to be robust to correlated mistakes if high-quality annotator exists. However, our experiments show that common confusion poisoned the classiﬁer obtained in Max-MIG even though every annotator is of high quality (individual

LabelMe Music

DL-MV DL-CL Doctor Net Anno-Reg Max-MIG 79.83±0.34 83.27±0.52 82.12±0.43 82.77±0.48 85.33±0.61 72.53±0.41 81.46±0.53 76.58±0.47 79.12±0.36 81.37±0.33

DL-GLAD DL-WC AggNet 83.12±0.34 82.74±0.33 84.75±0.27 77.82±0.37 75.76±0.24 81.92±0.41

CoNAL 87.12±0.55 84.06±0.42

Table 1: Test accuracy on two real-world crowdsourcing datasets.

noise strength is set to 0.7). DL-CL and Anno-Reg failed because they could not differentiate the source of noise, such that the gradients from the modeled annotations cannot be properly adjusted to update the classiﬁer. Both Doctor Net and DL-MV are based on majority vote, so that they fail when the annotations across annotators are no longer independent, i.e., caused by the common confusion. Compared to methods with complex noise models, DL-GLAD directly models the annotation accuracy, which is not suitable for class-dependent confusion. DL-WC clusters correlated annotators to share confusion matrix, which can reduce the inﬂuence of common confusion. But the expertise of each annotator is missing, which leads to its bad performance. AggNet shows the advantage of directly learning from annotations rather than from aggregated labels. But it still assumes the only noise source thus cannot handle common noise well.
To understand how accurate our solution can distinguish common and individual noise, we report the learnt weights of noise adaptation layers against the ground-truth confusion matrices on the CIFAR-10 dataset in Figure 5. In this experiment, we set the common noise strength to 0.7 and the proportion of common noise to 0.5. We can ﬁnd that in most cases the ground-truth common noise pattern is well recovered, especially under the asymmetric noise pattern.
Experiments on Real-world Datasets
Datasets description. We consider two real-world datasets. LabelMe (Rodrigues and Pereira 2018; Russell et al. 2008) is an image classiﬁcation dataset, consists of 2,688 images from 8 classes, where 1,000 of them are labeled by annotators from Amazon Mechanical Turk (AMT)1 and the remainings are used for validation and testing. Each image is labeled by an average of 2.5 annotators, with a mean accuracy of 69.2%. Standard data augmentation techniques are used on training data, including horizontal ﬂips, rescaling and shearing, to enrich the training set to 10,000 images. Music (Rodrigues, Pereira, and Ribeiro 2014) is a music genre classiﬁcation dataset, consisting of 1,000 samples of songs with 30 seconds length from 10 music genres, where 700 of them are labeled by AMT annotators and the rest are used for testing. Each sample is labeled by an average of 4.2 annotators, with a mean annotation accuracy of 73.2%. Backbone networks & training details. For LabelMe dataset, we followed the setting of Rodrigues and Pereira (2018): we apply a pre-trained VGG-16 network followed by a FC layer with 128 units and ReLU activations, and a softmax output layer, using 50% dropout. For Music dataset, we use the same FC layer and softmax layer as LabelMe. Batch normalization (Ioffe and Szegedy 2015) is performed
1https://www.mturk.com/

in each layer. Other hyper-parameters are the same as the synthesized experiments.
Results. As reported in Table 1, CoNAL achieved new stateof-the-art performance on both real-world datasets. In particular, we looked into the accuracy on classes where commonly made mistakes across annotators are observed (see Figure 1). For example, for open country on LabelMe, its accuracy in CoNAL is 67.21%, while the best baseline MaxMIG only achieved 54.19%. The good performance aligns with our analysis in Theorem 1, by differentiating common and individual confusions, it is easier to ﬁnd the true labels. We provide the visualization of the learned confusion matrices and the training and testing accuracy plots on real-world datasets in Appendix C.
Inﬂuence of the regularization term λ. We studied the inﬂuence of different λ in Table 2. The results show by enforcing the noise adaptation layers to be different, the performance is improved on both datasets. The value of λ also matters, and 10−5 achieves best performance empirically.

λ

0

10−4

10−5

10−6

LabelMe 85.68±0.38 86.61±0.41 87.12±0.55 86.26±0.47

Music 82.14±0.31 83.52±0.25 84.06±0.42 82.98±0.37

Table 2: Model performance under different λ.

Conclusion & Future works
In this paper, we study the problem of learning from crowds with noisy annotations. Aside from the widely employed independent noise assumptions across annotators, we decompose annotation noise into common and individual confusions. We used neural networks to realize our probabilistic modeling of crowdsourced data, and estimate each component in our solution in an end-to-end fashion. Extensive empirical evaluations conﬁrm the advantage of our solution in learning from complicated real-world crowdsourced data. Our solution is also ﬂexible: it can be easily applied to any existing neural classiﬁers by simply connecting with the proposed noise adaptation layers. In our current solution, all annotators share the same global confusion matrix. An interesting extension is to consider group-wise confusion, where we keep a shared confusion matrix for each annotator group, and identify the groups by optimization. It is also worthwhile to extend the solution to a proactive setting, e.g., probe annotators for more annotations so as to improve common confusion modeling.

Acknowledgments
We thank our anonymous reviewers for their helpful comments. This work was supported by NSF 1718216, 1553568, and Department of Energy DE-EE0008227.
Ethics statement
Our study focuses on tackling an urgent problem in this deep learning era: learning from crowds. High-quality labels are needed for real-world deep learning applications; however, they are typically difﬁcult and expensive to collect in practice. Hence, we propose to directly learn from labels given by non-expert annotators, considering both common mistakes and individualized mistakes. On the one hand, industrial applications will beneﬁt from this work since nonexpert labels are both cost- and time-effective to enable deployment of deep learning systems. On the other hand, our work also has academic impact. Our method can be applied to new research problems where high-quality labeled data is rare but crowdsourced labels are easy to obtain, such as medical image classiﬁcation.
The potential issue of common noise modeling is it might open the door for adversarial annotators. When previously modeled independently, they need to provide a large number of annotations to poison a learner. But if an attacker gets access to common noise, he/she only needs to provide a few annotations consistent with the common noise to amplify the inﬂuence of common noise. This will also make other ordinary annotators inadvertently contribute to the attack. Another potential issue of learning from crowds is when modeling annotator expertise, we are learning an annotator proﬁle, which has risk in disclosing their privacy, especially in privacy sensitive annotation problems. Data masking or distortion (e.g., differential privacy) is needed to protect annotators’ privacy.
References
Albarqouni, S.; Baur, C.; Achilles, F.; Belagiannis, V.; Demirci, S.; and Navab, N. 2016. Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images. IEEE transactions on medical imaging 35(5): 1313–1321.
Bertsekas, D. P. 2014. Constrained optimization and Lagrange multiplier methods. Academic press.
Buecheler, T.; Sieg, J. H.; Füchslin, R. M.; and Pfeifer, R. 2010. Crowdsourcing, open innovation and collective intelligence in the scientiﬁc method: a research agenda and operational framework. In The 12th International Conference on the Synthesis and Simulation of Living Systems, Odense, Denmark, 19–23 August 2010, 679–686. MIT Press.
Cao, P.; Xu, Y.; Kong, Y.; and Wang, Y. 2019. Max-mig: an information theoretic approach for joint learning from crowds. arXiv preprint arXiv:1905.13436 .
Dawid, A. P.; and Skene, A. M. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics) 28(1): 20–28.

Goldberger, J.; and Ben-Reuven, E. 2016. Training deep neural-networks using a noise adaptation layer. In International Conference on Learning Representations.
Goodfellow, I.; Bengio, Y.; and Courville, A. 2016. Deep learning. MIT press.
Guan, M. Y.; Gulshan, V.; Dai, A. M.; and Hinton, G. E. 2018. Who said what: Modeling individual labelers improves classiﬁcation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Imamura, H.; Sato, I.; and Sugiyama, M. 2018. Analysis of minimax error rate for crowdsourcing and its application to worker clustering model. arXiv preprint arXiv:1802.04551 .
Ioffe, S.; and Szegedy, C. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 .
Kamar, E.; Kapoor, A.; and Horvitz, E. 2015. Identifying and accounting for task-dependent bias in crowdsourcing. In Third AAAI Conference on Human Computation and Crowdsourcing. Citeseer.
Khetan, A.; and Oh, S. 2016. Achieving budget-optimality with adaptive schemes in crowdsourcing. In Advances in Neural Information Processing Systems, 4844–4852.
Kingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple layers of features from tiny images. Citeseer.
Li, Y.; Rubinstein, B.; and Cohn, T. 2019. Exploiting worker correlation for label aggregation in crowdsourcing. In International Conference on Machine Learning, 3886–3895.
Raykar, V. C.; Yu, S.; Zhao, L. H.; Valadez, G. H.; Florin, C.; Bogoni, L.; and Moy, L. 2010. Learning from crowds. Journal of Machine Learning Research 11(Apr): 1297–1322.
Rodrigues, F.; Pereira, F.; and Ribeiro, B. 2014. Gaussian process classiﬁcation and active learning with multiple annotators. In International conference on machine learning, 433–441.
Rodrigues, F.; and Pereira, F. C. 2018. Deep learning from crowds. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Russell, B. C.; Torralba, A.; Murphy, K. P.; and Freeman, W. T. 2008. LabelMe: a database and web-based tool for image annotation. International journal of computer vision 77(1-3): 157–173.
Shah, N. B.; Balakrishnan, S.; and Wainwright, M. J. 2016. A permutation-based model for crowd labeling: Optimal estimation and robustness. arXiv preprint arXiv:1606.09632 .
Tanno, R.; Saeedi, A.; Sankaranarayanan, S.; Alexander, D. C.; and Silberman, N. 2019. Learning from noisy labels by regularized estimation of annotator confusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 11244–11253.

Venanzi, M.; Guiver, J.; Kazai, G.; Kohli, P.; and Shokouhi, M. 2014. Community-based bayesian aggregation models for crowdsourcing. In Proceedings of the 23rd international conference on World wide web, 155–164.
Welinder, P.; Branson, S.; Perona, P.; and Belongie, S. J. 2010. The multidimensional wisdom of crowds. In Advances in neural information processing systems, 2424– 2432.
Whitehill, J.; Wu, T.-f.; Bergsma, J.; Movellan, J. R.; and Ruvolo, P. L. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in neural information processing systems, 2035– 2043.
Yan, Y.; Rosales, R.; Fung, G.; Subramanian, R.; and Dy, J. 2014. Learning from multiple annotators with varying expertise. Machine learning 95(3): 291–327.
Yin, L.; Han, J.; Zhang, W.; and Yu, Y. 2017. Aggregating crowd wisdoms with label-aware autoencoders. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, 1325–1331.
Zhang, Y.; Chen, X.; Zhou, D.; and Jordan, M. I. 2014. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In Advances in neural information processing systems, 1260–1268.
Zhou, D.; Basu, S.; Mao, Y.; and Platt, J. C. 2012. Learning from the wisdom of crowds by minimax entropy. In Advances in neural information processing systems, 2195– 2203.

A. Proof of Theorem 1
Proof. In our setting, the ground-truth class distribution ρi depends on the instance features. Then the minimax error rate of the crowdsourcing problem can be lower bounded by the following,

infZˆsupZ∈[C]N E L(Zˆ, Z) ≥ N 2l1ogC N R(ρi, Π )

i=1

(5)

− log2 N 2logC

where

RC C

R(ρi, Π ) = H(ρi) −

ρicρic KL(πcr∗ πcr∗)

r=1 c=1 c =1
(6) and Π = {π r}Rr=1 denotes the set of annotator-level confusion matrices. We use π to differentiate with our deﬁned

individual confusion matrix in the main paper. The proof of

Eq (5) is similar to (Imamura, Sato, and Sugiyama 2018).

Based on our new noise generation assumption, the annota-

tion noise can be decomposed by common noise and indi-

vidual noise. Thus we can further bound the minimax error

rate under this noise assumption.

Under our new noise assumption, we can evaluate the

confusion matrix on a per-instance-annotator basis. Specif-

ically, in each annotation, the effective confusion matrix is

a weighted combination of the global and individual confusion matrices, where the weight is wir. In a mixture model, the Kullback–Leibler divergence can be decomposed ac-

cordingly by,

KL(πcr∗

πcr∗) = KL(ωirπcg∗ + (1 − ωir)πcr∗

ωirπcg ∗ + (1 − ωir)πcr ∗)

≤ KL(ωri ωri ) + ωir KL(πcg∗ πcg ∗)

+ (1 − ωir) KL(πcr∗ πcr ∗)

(7)

= ωir KL(πcg∗ πcg ∗)

+ (1 − ωir) KL(πcr∗ πcr ∗)

(8)

where ωri = (ωir, 1 − ωir). The inequality can be derived by the log-sum inequality. Substitute Eq (8) back to Eq (6), we can get the new term F (ρ, Π, Ω) in Theorem 1. Plug it
back into Eq (5), we can get the reﬁned result in our Theo-
rem 1,

infZˆ supZ∈[C]N E

L(Zˆ, Z)

1

N

≥ N 2log C F (ρi, Π, Ω)

i=1

− log 2 N 2log C

Corollary 1.1. When N ≥ maxF2(lρogi2,Π,Ω) , increasing the number of instances N will decrease the error rate bound.

Proof.

1

N

log 2

N 2log C F (ρi, Π, Ω) − N 2log C

i=1

≤ maxF (ρi, Π, Ω) − log 2 ,

N log C

N 2log C

When the gradient of the upper bound is less than 0, the upper bound will decrease when N is growing. This can be achieved by setting N by the following,

− 1 maxF (ρi, Π, Ω) + 2log2 ≤ 0

N2

log C

N 3log C

⇒N ≥

2log2

maxF (ρi, Π, Ω)

Remarks. The corollary shows when the number of instances is growing, the label aggregation quality gets improved. Also, we need to point out the structure of confusion matrices Π is more important than the number of classes C in this lower bound. With a larger KL distance between every pair of rows in Π, we can expect an improved error lower bound.
B. EM algorithm for learning from crowds by
modeling common confusions
The EM algorithm is a generic solution for aggregating crowdsourced labels in classic crowdsourcing problems (Dawid and Skene 1979; Imamura, Sato, and Sugiyama 2018), and it can also be used under our common confusion assumption. Though we have pointed out the main drawbacks of EM-based algorithms in our solution framework, we still list the procedures of using EM algorithm in our problem for interested readers. In particular, we demonstrate a two-step solution, where the latent indicator sri is drawn from a Bernoulli distribution directly parameterized by ωir and the ground-truth label zi is drawn from a multinomial distribution pθ(zi|xi) parameterized by θ, which is essentially the soft-classiﬁer we are estimating from the crowdsourced data. Once the ground-truth labels {zi}Ni=1 on instances are inferred, we estimate the parameters θ in pθ(zi|xi) by treating the inferred labels as ground-truth. When the instance features are unavailable, we can use another multinomial distribution p(zi|ρ) to replace pθ(zi|xi), where ρ = {ρc}Cc=1 is the corresponding Dirichelet prior, to perform answer aggregation by EM as well.
Under our common confusion assumption, the conditional probability p(yir|zi) can be written as

p(yir|zi; Π, ωir) =

p(

sri

|

wir

)p

(y

r i

|zi

,

sri

,

π

r

)

sri ={0,1}

Based on this conditional probability, we derive the EM
procedure to infer the ground-truth labels as follows. In the
E-step, we estimate hidden ground-truth label zi and latent indicator sri . The posterior q(zi) and q(sri ) are obtained using Bayes’ rule,

Figure 6: Results on Synthetic dataset.

R

q(zi = c) ∝ pθ0 (zi = c|xi) p(yir|zi = c; Π0, ωir0),

r=1

q(sri

=

1)

∝

ωir0

p

(y

r i

|zi

,

π0g

)

,

q(sri = 0) ∝ (1 − ωir0) p(yir|zi, π0r).

where θ0, ω0 and Π0 are the current estimated parameters. In the M-step, we update the parameters of neural network θ, proportion of common noise ω and confusion matrices Π. The proportion of common noise and confusion matrices have closed-form solutions by using the Lagrange multiplier method (Bertsekas 2014),

ωir = q(sri = 1)

πcg,l =

N i=1

R r=1

q(zi

=

c)q(sri

=

1)I(yir

=

l)

Ni=1 Rr=1 q(zi = c)q(sri = 1) ,

πcr,l =

N i=1

q(zi

=

c)q(sri

=

0)I(yir

=

l)

N i=1

q(zi

=

c)q(sri

=

0)

To update the neural network parameter θ, we follow the approach in (Goldberger and Ben-Reuven 2016; Albarqouni et al. 2016) and use the inferred posterior of groundtruth q(zi) as the target. Speciﬁcally, we compute the crossentropy loss and backpropagate the error using stochastic gradient optimization techniques such as Adam (Kingma and Ba 2014). For the generic setting where instance features are unavailable, we can update the class distribution ρ using its closed-form solution,

1N ρc = N q(zi = c)
i=1

C. Additional experiment results
Results on Synthetic dataset. Figure 6 presents the test accuracy on Synthetic dataset under the same settings as we described in Section 3. We report mean and standard deviation of test accuracy on ﬁve runs. The results align with our analysis in Section 3. Under the asymmetric confusion, our proposed approach is robust to the settings of common noise strength and the proportion of common noise. Under the symmetric confusion, all methods’ performance is inﬂuenced (becomes worse); however, CoNAL still outperforms baselines with a large margin by differentiating the source of noise.
Figure 7 shows the learnt weights of noise adaptation layers against the ground-truth confusion matrices on the Synthetic dataset. We set the common noise strength to 0.7 and the average proportion of common noise around 0.5. We reconstruct the confusion matrix from the learnt weights by normalizing them using softmax on each row. From the results, we can clearly observe most confusion matrices (especially the confusion matrix for common noise) are well recovered under both confusion settings. Visualization of learnt confusion matrices on real-world datasets. We provide visualization of learnt confusion matrices on both real-world datasets in Figure 8 and 9. We can clearly observe these two types of learnt confusion matrices, i.e., for common confusion and individual confusion, capture different mistake patterns across annotators. For example, on LabelMe dataset, the commonly made mistake from inside city to street was covered by the learnt common confusion. The same observation is also obtained on the Music dataset, such as the common mistake from jazz to blues is reﬂected in our learnt common confusion matrix. On the other hand, the individual noise on annotators captures their own speciﬁc mistakes. For example, on LabelMe dataset, both annotator 1 and 2 confused about tall building and inside city; but this mistake does not appear in annotator 3, 4 and

(a) common noise

(b) annotator 1

(c) annotator 2

(d) annotator 3

(e) common noise

(f) annotator 1

(g) annotator 2

(h) annotator 3

Figure 7: Comparison between ground truth confusion matrices and learned ones on Synthetic dataset. The top row is the result of asymmetric common noise. The bottom row is the result of symmetric common noise.

5, nor the global confusion matrix.
We also notice some low-quality annotators in the Music dataset, such as annotator 1 (with ground-truth annotation accuracy of 0.182) and annotator 5 (with ground-truth annotation accuracy of 0.108), whose annotations are almost random. By separately modeling the annotation noise at a per-annotation basis, our solution reduces the inﬂuence from such low-quality annotators in learning the common noise model and maintains the quality of inferred true labels overall.
We visualized the distribution of inferred proportion of common noise (i.e., ωir) across annotations to better understand how CoNAL differentiates the source of noise in individual annotations. We rank the instances by their average ωir over all annotators who have labeled this instance in a descending order. Then we count the frequency of ground-truth labels in the top 50% and bottom 50% instances respectively and report the results in Figure 10. The larger the average ωir in an instance is, the more likely the annotators made similar mistakes on it (i.e., the common confusion matrix can better explain the observed annotations on this instance). On LabelMe dataset, we can observe that annotators tend to make similar annotations on forest (with ground-truth annotation entropy 0.660) and mountain (with ground-truth annotation entropy 0.192), but make their own mistakes on open country (with ground-truth annotation entropy 1.287) and inside city (with ground-truth annotation entropy 1.116). In other words, the annotations on forest and mountain are much more consistent than those on open country and inside city. This observation can also be explained by the learnt confusion matrices. From the learnt global confusion matrix, we can observe confusion patterns in open country and inside city are quite scattered, and different annotators (e.g., all those ﬁve visualized annotators) have distinct confusions. While for forest and mountain, the global confusion matrix correctly maps them to the correct annotation, and individual annotators might occasionally make their own mistakes, e.g., annotator 3. Similar observations are also obtained on the Music dataset, where annotators tend to make similar mistakes on hiphop and reggae, and make their own distinct mistakes on jazz and rock.

Discussion about overparameterized models. To prove that the improved performance of our solution comes from its unique modeling of crowdsourced data other than simply an increased number of parameters to ﬁt, we compare our model with the overparameterized DL-CL (Rodrigues and Pereira 2018), which has a similar structure as ours to capture individual confusions, but without the notion of modeling common confusion. Rodrigues and Pereira (2018) discussed that simply adding more parameters can make the output of the learnt classiﬁer lose its interpretability as a shared ground-truth estimate across annotators, so that they only used one softmax layer for each annotator upon the classiﬁer’s output layer. We add another softmax layer for each annotator, to introduce more parameters but avoid losing the interpretability of the bottleneck layer, we name it as DL-CL_Over.

Model

DL-CL DL-CL_Over CoNAL

#Params in NAL R × C2 2 × R × C2 (R + 1) × C2

LabelMe 83.27±0.52 82.34±0.34 87.12±0.55

Music 81.46±0.53 80.47±0.27 84.06±0.42

Table 3: Comparison with the overparameterized model.
We present the results on real-world datasets, along with the number of parameters in the noise adaptation layers (NAL). Even though DL-CL_Over has the most number of parameters to ﬁt, its performance did not increase but decreased, which indicates that blindly adding more parameters will not help model crowdsourced data. Our model adds a global noise adaptation layer, which has fewer parameters than DL-CL_Over. The results prove the advantage of our model comes from its unique design to annotation confusions, but not simply more parameters to ﬁt.

coast forest highway inside city mountain open country street tall building coast forest highway inside city mountain open country street tall building

common noise annotator 3

annotator 1 annotator 4

annotator 2 annotator 5

Figure 8: Learnt global confusion matrix and individual confusion matrices of 5 annotators on LabelMe dataset.

blues classical country
disco hiphop
jazz metal
pop reggae
rock blues classical country disco hiphop jazz metal pop reggae rock

common noise annotator 3

annotator 1 annotator 4

annotator 2 annotator 5

Figure 9: Learnt global confusion matrix and individual confusion matrices of 5 annotators on Music dataset.

Frequency

LabelMe

Music

Top 50% Bottom 50%

0.20

0.15

0.15

0.10

0.10

0.05

0.05

0.00 coast forest ighwayinsicdiety untain ountrsytreet talllding 0.00 blues lassicalountrydisco hiphop jazz metal pop reggae rock

h

mo open c

bui

cc

Figure 10: ω-label distribution on real-world datasets. We rank the instances by average ω over all annotators and visualize the ground-truth label distribution of top 50% and bottom 50% instances.

