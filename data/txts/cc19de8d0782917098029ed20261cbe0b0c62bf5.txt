arXiv:2010.15300v1 [cs.CL] 29 Oct 2020

Uncovering Latent Biases in Text: Method and Application to Peer Review
Emaad Manzoor∗, Nihar B. Shah† Carnegie Mellon University
∗emaad@cmu.edu, †nihars@cs.cmu.edu
Abstract
Quantifying systematic disparities in numerical quantities such as employment rates and wages between population subgroups provides compelling evidence for the existence of societal biases. However, biases in the text written for members of diﬀerent subgroups (such as in recommendation letters for male and non-male candidates), though widely reported anecdotally, remain challenging to quantify. In this work, we introduce a novel framework to quantify bias in text caused by the visibility of subgroup membership indicators. We develop a nonparametric estimation and inference procedure to estimate this bias. We then formalize an identiﬁcation strategy to causally link the estimated bias to the visibility of subgroup membership indicators, provided observations from time periods both before and after an identity-hiding policy change. We identify an application wherein “ground truth” bias can be inferred to evaluate our framework, instead of relying on synthetic or secondary data. Speciﬁcally, we apply our framework to quantify biases in the text of peer reviews from a reputed machine learning conference before and after the conference adopted a double-blind reviewing policy. We show evidence of biases in the review ratings that serves as “ground truth”, and show that our proposed framework accurately detects these biases from the review text without having access to the review ratings.
1. Introduction
Societal biases against individuals based on race, gender, and other attributes can lead to disparities in hiring (Bertrand and Mullainathan, 2004), wages (Blau and Kahn, 2017) and incarceration rates (Alesina and La Ferrara, 2014), among other socioeconomic outcomes. Uncovering evidence of such biases informs the creation of policies to eliminate long-standing gaps between diﬀerent population subgroups.
Many important socioeconomic outcomes are inﬂuenced by written feedback, such as academic hiring that is inﬂuenced by reference letters, and employee appraisals that rely on managerial performance reviews. Previous studies have reported biases in the text of such feedback (Mitchell and Martin, 2018; Madera et al., 2019; Correll et al., 2020). Mitchell and Martin (2018) ﬁnd that student evaluations of female faculty teaching were more likely to focus on communication ability. Madera et al. (2019) ﬁnd that academic letters of recommendation for women were more likely to contain “doubt-raising” phrases. In the context of managerial feedback provided during employee appraisals, Correll et al. (2020) ﬁnd that “women were more likely to receive vague feedback that did not oﬀer speciﬁc details of what they had done well and what they could do to advance”.
Such biases in text can have severe economic consequences. For example, diﬀerences in the media framing of natural disasters were found to be associated with large disparities in the amount of allocated foreign aid (Str¨omberg, 2007; Kweiﬁo-Okai, 2014). Similarly, online reviews of Asian and Indian restaurants declared them “unauthentic” when they deﬁed the negative stereotypes of uncleanliness, leading to signiﬁcant losses in revenue (Kay, 2019).
1

Biases in text are more prevalent than those in numerical feedback due to the absence of enforced structure (Mackenzie et al., 2019), and are more diﬃcult to detect due to their subtle manner of expression (Morstatter et al., 2018). In addition, the inherently unstructured nature of text makes it challenging to quantify the biases it contains. Without the ability to quantify and provide credible evidence of such biases, society remains at risk of widening socioeconomic gaps fueled by the unchecked expression of prejudices in written feedback.
In this work, we propose a framework to quantify biases in text, provided data from time periods both before and after an identity-hiding policy change. Our proposed framework extends the diﬀerence-in-diﬀerences causal inference methodology introduced by Card and Krueger (1994) (which compares diﬀerences in numerical quantities over time) to handle diﬀerences in unstructured text over time. We motivate and evaluate our framework in the setting of scholarly peer review, which is a key mechanism of feedback and quality assurance in scientiﬁc research. Speciﬁcally, we assemble a dataset of peer reviews from the International Conference of Learning Representations (ICLR), which switched from single-blind to double-blind reviewing in 2018. We test for biases in the peer review text, which is prone to prejudice due to the lack of enforced structure. Importantly, our dataset enables estimating “ground truth” biases using the peer review ratings (a numerical quantity). The goal of our proposed framework is to quantify biases in the peer review text that are consistent with the “ground truth”, without having access to the review ratings.
Our main contributions are:
I. We formalize bias as a causal estimand — the disparity in the peer review text between the subgroups caused by the visibility of author identities — that relies on a weaker assumption than “no unobserved confounders”. We propose a nonparametric estimation and inference procedure to quantify this bias. Our procedure makes no assumptions on the data-generating process of the peer review text and requires no feature engineering.
II. We apply our proposed framework to quantify the bias in the text of peer reviews from the International Conference on Learning Representations (ICLR). We detect a statistically signiﬁcant bias with respect to the authors’ aﬃliation, but ﬁnd no evidence of bias with respect to the authors’ perceived gender.
III. Our chosen application is motivated by the opportunity to evaluate our proposed framework on “ground truth” biases derived from review ratings. Speciﬁcally, we evaluate our proposed framework by comparing the estimated biases in the peer review text with the biases in the peer review ratings estimated using the diﬀerence-in-diﬀerences methodology (Card and Krueger, 1994; Angrist and Pischke, 2008). We show that the biases in the peer review text estimated using our proposed framework are consistent with the “ground truth”.
In Appendix B, we also evaluate an alternative measure of disparity in the text proposed by Gentzkow et al. (2019), and this empirical evaluation reveals that our proposed measure of disparity has greater statistical power. Finally, although presented in the context of peer review, our proposed framework can also be applied to other settings such as testing for biases in feedback provided by employers when reviewing employees (Goldin and Rouse, 2000) and evaluating potential hires (Capowski, 1994).
Replication code and data for this work is available at https://emaadmanzoor.com/biases-in-text/.
2

2. Related Work
There is recently an increasing focus on designing methods for improving the fairness and quality of peer review, and our work contributes to this important line of literature (Garg et al., 2010; Roos et al., 2011; Cabanac and Preuss, 2013; Lee, 2015; Xu et al., 2019; Kang et al., 2018; Wang and Shah, 2019; Stelmakh et al., 2019a; Noothigattu et al., 2020; Kobren et al., 2019; Fiez et al., 2020; Stelmakh et al., 2020; Jecmen et al., 2020). Our work complements previous studies that investigated biases in peer review (Goldberg, 1968; Ceci and Peters, 1982; Swim et al., 1989; Lloyd, 1990; Blank, 1991a; Garfunkel et al., 1994; Snodgrass, 2006; Ross et al., 2006; Budden et al., 2008; Webb et al., 2008; Walker et al., 2015; Okike et al., 2016; Seeber and Bacchelli, 2017; Bernard, 2018; Tomkins et al., 2017; Stelmakh et al., 2019b; Salimi et al., 2020). In contrast with our work, the aforementioned studies focused on biases in numerical quantities such as acceptance rates and review ratings caused by visible author identities.
Our approach compares single-blind and double-blind reviews for gender and aﬃliation-based population subgroups. A number of previous studies have quantiﬁed biases using the peer-review ratings provided for diﬀerent subgroups under single-blind and double-blind reviewing policies. Blank (1991b) conducts a randomized control trial and ﬁnds no signiﬁcant diﬀerence in the review ratings for male and female authors under single and double-blind reviewing. Ross et al. (2006) compare single and double-blind reviewing in diﬀerent years at a medical conference and ﬁnd that the association between abstract acceptance and whether the authors were aﬃliated to institutions in the USA reduces signiﬁcantly when reviewing is double-blind. Madden and DeWitt (2006) compare single and double-blind reviewing for the SIGMOD conference in diﬀerent years and ﬁnd that the mean number of accepted of papers by a “proliﬁc” author remained largely similar before and after the switch to double-blind reviewing. In contrast, Tung (2006) ﬁnd a signiﬁcant reduction in the median number of accepted papers by a “proliﬁc” author after SIGMOD switched to double-blind reviewing. More recently, Tomkins et al. (2017) conduct a semi-randomized controlled trial with the WSDM 2017 conference and do not ﬁnd a signiﬁcant association between a paper’s single-blind review rating and whether its authors were women, or whether its authors were aﬃliated to institutions in the USA. Salimi et al. (2020) compare single and double-blind reviewing in several conferences and ﬁnd a signiﬁcant eﬀect of institutional prestige on review ratings when reviewing was single-blind (and none when reviewing was double-blind). In the context of single-blind journal peer review, Thelwall et al. (2020) ﬁnd that reviewers are biased in favor of authors from their home country. Our work complements these studies by focusing on biases in the review text, instead of on biases in numerical quantities.
Our proposed framework is closely related to the bias discovery method proposed by Field and Tsvetkov (2020), who train a machine learning model to identify diﬀerences in online comments addressed towards men and women. Their method also relies on the idea that text which is predictive of gender is likely to contain bias. They show that their method can detect gender bias on a labeled dataset from a diﬀerent domain than the one their model was trained on. However, the method proposed by Field and Tsvetkov (2020) crucially depends on the “no unobserved confounders” assumption. This assumption is restrictive and unlikely to hold in practice. In contrast, our proposed framework relies on a weaker assumption that remains valid under a large class of unobserved confounders (though requiring additional data, from two diﬀerent time periods). We also overcome a key limitation in the (Field and Tsvetkov, 2020) by evaluating our proposed framework on “ground truth” derived from the same peer review process used for bias estimation, instead of on secondary datasets or tasks.
3

3. Problem Deﬁnition
We assume the availability of peer reviews from a conference in two diﬀerent years, with author identities (their names, emails, and aﬃliations) visible to reviewers during the peer review process in exactly one of the two years. Our goal is to quantify biases in the text of the peer reviews written for papers belonging to two pre-speciﬁed subgroups based on a selected identifying attribute of their authors.
Previous studies have reported systematic disparities in the text written for diﬀerent population subgroups. For example, Madera et al. (2019) report that recommendation letters for women are more likely to contain “doubt-raising” language then those for men. However, such disparities by themselves are not suﬃcient evidence of bias (Rathore and Krumholz, 2004). While disparities are observed diﬀerences in the review text, biases are observed diﬀerences that are caused by author identity visibility, and not other factors.
In a counterfactual universe where the author identities are hidden, the bias must be zero (since its cause no longer exists) but the observed disparity can be nonzero. For example, if we partition papers into subgroups based on their ﬁrst author’s aﬃliation country, disparities in the review text could also arise due to country-speciﬁc preferences for diﬀerent research topics. When deﬁning bias, our goal is to separate the disparity caused by author identity visibility from the disparity caused by other factors. We now formalize bias as a causal estimand with the potential outcomes framework (Imbens and Rubin, 2015), and derive an expression that relates bias to the disparities observed in diﬀerent time periods.
Consider papers submitted to a conference in the years tSB and tDB, where the conference employed single-blind reviewing in year tSB and double-blind reviewing in year tDB. We partition the papers into two subgroups G0 and G1 based on a selected identifying attribute of their authors (such their aﬃliation or perceived gender). Our goal is to formalize bias as the disparity in the review text for papers in each subgroup caused by the visibility of this identifying attribute to reviewers during the peer review process.
We denote by ∆t the observed disparity in the review text in year t ∈ {tSB, tDB}. While we propose a careful nonparametric formulation of ∆t in Section 4, ∆t can be viewed as any measure of the diﬀerence in the review text in year t between subgroups G0 and G1. We denote by ∆St B and ∆Dt B the counterfactual disparities in the review text in year t that would have been observed had author identities been visible to and hidden from reviewers, respectively. Only one of the quantities ∆Dt B and ∆St B is visible in each year. When t = tSB and reviewing was single-blind, ∆DtSBB is unobserved and quantiﬁes the disparity in year tSB had reviewing been double-blind instead. When t = tDB and reviewing was double-blind, ∆StDBB is unobserved and quantiﬁes the disparity in year tDB had reviewing been single-blind instead.
We deﬁne the bias (our causal estimand) as a diﬀerence in counterfactual disparities:
bias = ∆DtDBB − ∆StDBB . (1)
Eq. (1) subtracts the disparity ∆StDBB (caused by both author identity visibility and other factors) from the disparity ∆DtDBB (caused by other factors only) to isolate the disparity caused by author identity visibility only. Note that the bias could also have been deﬁned as ∆DtSBB − ∆StSBB (the change in the disparity that would have been observed had reviewing in year tSB been double-blind instead). Either deﬁnition is valid and applicable to our framework (after minor algebraic changes).
4

Note that our deﬁnition of bias does not restrict how the subgroups are deﬁned. As such, the bias with respect to subgroups based on diﬀerent identifying attributes that lead to the same partitions G0 and G1 will be identical, and must be interpreted based on domain expertise. For example, the bias with respect to subgroups based on the authors’ seniority would be similar to the bias with respect to subgroups based on the authors’ “fame” if senior authors are likely to have had more time to publish, engage with their community and develop connections than junior authors. In practice, we recommend deﬁning subgroups based on hypotheses grounded in socioeconomic theory, and disentangling the biases with respect to diﬀerent subgroup deﬁnitions using institutional knowledge.
4. Proposed Framework
Our goal is to estimate the bias deﬁned in Eq. (1) given the peer reviews from a conference in years tSB and tDB, with author identities visible to reviewers during the peer review process in year tSB and hidden in year tDB. In this section, we ﬁrst provide an identiﬁcation proof to link the causal estimand in Eq. (1) (that contains unobservable counterfactual quantities) with an empirical estimand (that contains observable quantities only). We then propose a nonparametric estimation and inference procedure to estimate the bias from the available peer review data.
4.1 Identiﬁcation
The bias as deﬁned in Eq. (1) contains the unobservable counterfactual disparity ∆StDBB (the disparity in year tDB had reviewing been single-blind), and cannot be estimated without further assumptions; this is the fundamental problem of causal inference (Holland, 1986). The process of linking a causal estimand deﬁned in terms of unobserved counterfactual quantities with an empirical estimand deﬁned in terms of observed quantities is called identiﬁcation, and relies on one or more identiﬁcation assumptions. We make the following identiﬁcation assumption:
Assumption 1. The disparity in t = tDB had author identities been visible is equal to the disparity in t = tSB when author identities were indeed visible: ∆StDBB = ∆StSBB.
Assumption 1 implies that the change in disparity from year tSB to tDB was caused only by the author identities being hidden in year tDB, and not other factors. Assumption 1 remains valid in the presence unobserved confounders that aﬀect the review text and (i) that do not vary from tSB to tDB, or (ii) that vary from tSB to tDB but aﬀect the review text for both subgroups identically (such as a more critical reviewer pool in year tDB). Hence, it is less restrictive than the “no unobserved confounders” assumption in prior work (Field and Tsvetkov, 2020). We further discuss the validity of Assumption 1 for our setting in Appendix A.
Let ∆t be the observed disparity in year t ∈ {tSB, tDB}. Given Assumption 1, we link the bias (that contains an unobservable counterfactual disparity) with an empirical estimand (that contains observable disparities only) with the following identiﬁcation proof :
bias (=i) ∆DtDBB − ∆StDBB (=ii) ∆DtDBB − ∆StSBB (i=ii) ∆tDB − ∆tSB (2)
where the equation (i) follows from the deﬁnition in Eq. (1), equation (ii) follows from Assumption 1, and equation (iii) follows from the fact that reviewing was indeed double-blind in year tDB (∆DtDBB = ∆tDB ) and single-blind in year tSB (∆StSBB = ∆tSB ).
5

4.2 Estimation and Inference
Having deﬁned the bias in the review text in terms of observable disparities in Eq. (2), we now focus on estimating this bias from the available peer review data in years tSB and tDB. Formalizing the disparities ∆tSB and ∆tDB in the text in a manner that is both substantively meaningful and that permits estimation and inference is non-trivial. In this section, we formalize the disparities in the text and propose a nonparametric procedure to estimate them from the peer reviews in years tSB and tDB.
Intuitively, the disparity ∆t in the review text in year t is a measure of how the text of the reviews written for G0 diﬀer from those written for G1. A simple approach to quantify the disparity is to select a “feature” of the review text (such as its “politeness”), annotate the review text based on this feature (either manually or via natural language processing methods) and then compare the value of this feature in the reviews for papers in each of the two subgroups G0 and G1. However, the disparities and bias quantiﬁed in this manner are sensitive to feature selection and annotation.
In contrast, we propose measuring the ∆t nonparametrically, without any feature selection and annotation. We rely on the intuition that if the text of the reviews written for G0 diﬀers systematically from the text of those written for G1, a binary machine-learning classiﬁer should be able to distinguish between the reviews written for each subgroup using the review text. Hence, we could use any measure of the performance (such as the accuracy, precision, or recall) of such a classiﬁer as a measure of the disparity ∆t.
However, disparities in the review text may also be caused by diﬀerences in the research topics pursued by each subgroup. To “control for” subgroup diﬀerences in research topics, we rely on the following intuition: subgroup diﬀerences in research topics should be reﬂected in the text of their paper abstracts. Hence, we quantify subgroup diﬀerences in research topics by the ability of a binary machine-learning classiﬁer to distinguish between the papers belonging to each subgroup using their abstract text.
We now deﬁne the disparity in the review text based on the intuition discussed previously. Let f (·) be a binary classiﬁer mapping a paper’s review text to its subgroup and g(·) be a binary classiﬁer mapping a paper’s abstract text to its subgroup. Let perf(f ; t) and perf(g; t) be the chosen measures of classiﬁcation performance of f (·) and g(·) respectively, such as their area under the ROC curve (AUC), accuracy or precision. We measure the disparity in the review text as the ratio of the performances of the two classiﬁers on the peer review data in year t:

∆t = perf(f ; t)/perf(g; t).

(3)

Normalizing perf(f ; t) by perf(g; t) as in Eq. (3) “controls for” subgroup diﬀerences in research topics: if perf(f ; t) is high due to subgroup diﬀerences in research topics, perf(g; t) will also be high. While any binary classiﬁer may be used for f (·) and g(·), poor classiﬁers are more likely to underestimate the bias (due to equally poor classiﬁcation performance in both tSB and tDB).
In Section 6, we report results with multinomial Naive Bayes classiﬁers for f (·) and g(·) and the AUC as our chosen measure of classiﬁcation performance. We estimate the value of perf(f ; t) and perf(g; t) using k-fold cross-validation. To eliminate any dependence on the choice of cross-validation folds, we repeat the bias estimation procedure many times with the data belonging to each fold randomized uniformly in each iteration. We use the empirical distribution of bias estimates from these iterations to construct conﬁdence intervals on the estimated bias.
6

Year

2017 2018 2019 2020
0

Rejected

Accepted

40% 37% 35% 31%

Num10b0e0r of pap2e0r0s0

Figure 1: Papers submitted, accepted and rejected from ICLR 2017 through 2020. The proportion of papers accepted in each year is reported to the right of each bar.

Year
2017 2018 2019 2020

Subgroup Deﬁnition

Aﬃliation-based

Gender-based

24.1% 24.1% 26.0% 30.5%

25.5% 28.5%
— —

Table 1: Proportion of submitted papers belonging to subgroup G1 in each year for diﬀerent subgroup deﬁnitions.

A ﬁnal issue we address is that perf(f ; t) and perf(g; t) can diﬀer in tSB and tDB due to diﬀerences in the sample size (number of reviews) or due to diﬀerences in the proportion of papers belonging to each subgroup in tSB and tDB. Hence, when estimating ∆tDB we downsample the available peer review data in year tDB1 such that (i) the number of peer reviews or abstracts is equal to that in tSB, (ii) the proportion of abstracts or peer reviews written for papers in subgroup G0 is equal to that in tSB, and (iii) the proportion of abstracts or peer reviews written for papers in subgroup G1 is equal to that in tSB. As with the cross-validation folds, the downsampling is randomized uniformly in each iteration.
5. Data
We assemble a dataset of 16,880 peer reviews from the OpenReview platform for all the 5,638 papers submitted to the International Conference on Learning Representations (ICLR) from 2017 to 2020. Each paper receives 3 peer reviews on average (with a standard deviation of 0.3). Each peer review contains textual comments and a numerical rating, from 1 to 10 in ICLR 2017–2019 and in {1, 3, 6, 8} in ICLR 2020. Fig. 1 reports the number of papers submitted and the proportion of papers accepted in each year.
1. We do this because the number of reviews in year tDB is greater than that in year tSB. In general, we downsample the data in the year having a greater number of reviews.
7

We investigate the existence of biases in the peer review text with respect to two types of author attributes: (i) the country of their aﬃliation, and (ii) their perceived gender. Aﬃliation and gender biases have been a recurring theme in prior work on improving fairness in peer review (Blank, 1991a; Ross et al., 2006; Tomkins et al., 2017; Salimi et al., 2020). We focus on testing for aﬃliation and gender bias in the review text to complement prior ﬁndings, though biases with respect to other attributes may also exist.
ICLR was co-founded by researchers aﬃliated with institutions in the USA and Canada. In addition, the general, program and area chairs of ICLR were almost exclusively aﬃliated with institutions in the USA and Canada since its inception in 2013. Motivated by the possibility of “in-group bias” (Taylor and Doria, 1981), we test for reviewer biases caused by visible author identities in favor of (or against) papers having at least one author with an aﬃliation in the USA or Canada. We partition the submitted papers into two subgroups, G0 and G1, based on the countries of the aﬃliations of their authors. We allocate all papers having at least one author aﬃliated to a university, organization, or company in the USA or Canada to G0, and all other papers to G1. Author aﬃliations are extracted from the submitted paper PDFs in ICLR 2017, and from the authors’ registered emails on the OpenReview platform in ICLR 2018, 2019 and 2020. The goal of this aﬃliation-based partitioning is to quantify the extent to which reviewers are biased by visible author aﬃliations.
In addition to aﬃliation-based subgroups, we consider subgroups based on the authors’ gender as perceived by the reviewer (and not self-reported). Since we do not observe how reviewers infer gender from authors’ names, we approximate the perceived gender of each author using the following protocol. We ﬁrst use historical self-reported gender records from the U.S. Social Security Administration to compute the probability of an author’s ﬁrst name being reported as male.2 If this probability is greater than 90%, we annotate the author’s perceived gender as male. If this probability is less than 10%, we annotate the author’s perceived gender as non-male. If this probability is between 10% and 90%, an external human annotator manually infers the gender of the author (male or non-male) using visible information on their homepage and Google Scholar proﬁle (found with a Google search). We expect our annotation protocol to approximate the authors’ gender as perceived by reviewers3.
We allocate all papers having at least one author perceived to be non-male to G1, and those with all authors perceived to be male to G0. The goal of this gender-based partitioning is to quantify the extent to which reviewers are biased in favor of (or against) papers having at least one author perceived to be non-male. Table 1 reports the proportion of papers submitted to ICLR in each year that belong to subgroup G1 for aﬃliation-based and gender-based subgroup deﬁnitions. Since our external human annotations of gender only span ICLR 2017 and 2018, our analyses of bias with gender-based subgroups excludes data from ICLR 2019 and 2020.
A key policy change during this period is ICLR’s switch to double-blind reviewing from 2018 onwards. We exploit this policy change to estimate the bias while eliminating the impact of a large class of unobserved confounders, as discussed in Section 4. We also exploit this policy change in Section 6 to construct a “ground truth” measure of bias in the peer review ratings using the diﬀerence-in-diﬀerences methodology. We then evaluate whether the biases estimated by our
2. We do this using the gender package available at https://github.com/ropensci/gender. 3. We designed our annotation protocol to approximate how reviewers perceive the gender of an author from their
name without knowledge of the author’s self-reported gender. As such, our annotations may contradict authors’ gender and cause unintended psychological harm. To prevent this, we do not plan to make our gender annotations public. However, we have described our annotation protocol in suﬃcient detail for replication.
8

proposed framework are consistent with the presence and absence of the “ground truth” bias in each year.
Since ICLR permits non-anonymized submissions to arXiv and other preprint servers while the paper is under review, it is likely that some author identities were visible even during the double-blind reviewing process in ICLR 2018, 2019 and 2020. Hence, we expect our bias estimates to be conservative (attenuated towards zero).
6. Evaluation
We now evaluate the ability of our proposed framework to detect biases in the text of peer reviews. Evaluating the validity of causal estimates is challenging in general due to the lack of “ground truth” causal eﬀects to compare with, which can only be obtained using randomized control trials that are often expensive and time-consuming. Hence, as in prior work (Field and Tsvetkov, 2020), evaluation is typically carried out using secondary tasks or semi-synthetic datasets such as the IBM Causal Inference Benchmark (Shimoni et al., 2018).
Instead of relying on secondary tasks or semi-synthetic datasets that may not represent peer reviewer behavior in the real world, we construct “ground truth” bias estimates based on the ratings provided by peer reviewers. Speciﬁcally, we apply the diﬀerence-in-diﬀerences methodology to quantify the presence or absence of biases in the review ratings using peer reviews from each consecutive pair of years between ICLR 2017 and 2020. We then apply our proposed framework to estimate biases in the review text.
We evaluate whether (i) the estimated bias in the review text is statistically signiﬁcant when the estimated bias in the review ratings is statistically signiﬁcant, and (ii) the estimated bias in the review text is statistically insigniﬁcant when the estimated bias in the review ratings is statistically insigniﬁcant. The underlying intuition is that the rating of a review must also be reﬂected in its text (with language expressing praise or criticism, for example). Hence, an accurate textual bias estimation framework must be able to detect biases (when present) using the review text without having access to the review ratings.
We begin in Section 6.1 with a detailed discussion on estimating the “ground-truth” aﬃliation bias using the diﬀerence-in-diﬀerences methodology and review ratings. We then estimate and evaluate the aﬃliation bias in the review text in Section 6.2. In Section 6.3, we estimate and evaluate the bias due to perceived gender. In Appendix A, we further discuss the validity of the identiﬁcation assumptions used in our evaluation. In Appendix B, as a comparative baseline, we evaluate an alternate measure of disparity in the text proposed by (Gentzkow et al., 2019) and empirically show that our proposed measure of disparity has greater statistical power.
6.1 Constructing “ground truth” aﬃliation biases from peer review ratings
We ﬁrst provide descriptive evidence in Fig. 2 of reviewer bias in favor of papers having at least one author with an aﬃliation in the USA or Canada (subgroup G0) in ICLR 2017, when reviewing was single-blind. Fig. 2a shows that the median review rating for papers in G0 was 1 unit higher than that for papers in G1 in ICLR 2017. This median ratings disparity disappears after the switch to double-blind reviewing in ICLR 2018. Similarly, Fig. 2b shows that the mean review rating for papers in G0 was 0.665 units higher than that for papers in G1 in ICLR 2017, and 0.297 units
9

Median Rating Mean Rating

6 5 4 3
2017

G0

G1

2018Year2019 2020

6 5 4 3 2017

G0

G1

2018Year2019 2020

(a) Median review rating for each subgroup

(b) Mean review rating for each subgroup

Figure 2: Ratings disparities in ICLR 2017 through 2020. The ratings disparity is quantiﬁed by the diﬀerence in (a) median, and (b) mean ratings for papers in G0 and G1.

higher in ICLR 2018. Thus, switching to double-blind reviewing coincided with a reduction in mean ratings disparity of 0.369 units.
We now estimate the ratings bias using the diﬀerence-in-diﬀerences causal inference methodology (Card and Krueger, 1994; Angrist and Pischke, 2008). The diﬀerence-in-diﬀerences methodology can be viewed as an analogue of our proposed framework to quantify biases in numerical quantities, instead of biases in unstructured text. This framework has been previously used to quantify gender biases in peer review ratings and hiring decisions (Blank, 1991b; Goldin and Rouse, 2000), among several other settings.
The diﬀerence-in-diﬀerences methodology, like our proposed framework, requires peer reviews in two years tSB (with single-blind reviewing) and tDB (with double-blind reviewing). Let rij be the rating that reviewer i gave to paper j, let Tj ∈ {tSB, tDB} be the year in which paper j was submitted, and let Sj ∈ {G0, G1} be subgroup that paper j belongs to. The diﬀerence-in-diﬀerences methodology deﬁnes the ratings disparity ∆rtating in each year t ∈ {tSB, tDB} as:

∆rtating = E[rij |Sj = G0, Tj = t]

− E[rij|Sj = G1, Tj = t]

(4)

where the expectations are over all papers j and their respective reviewers i. The ratings bias γ is deﬁned as the diﬀerence in the ratings disparities between the year with single-blind reviewing and the year with double-blind reviewing:
γ = ∆rtDatBing − ∆rtSaBting (5)
Interpreting γ as a ratings bias – the diﬀerence in mean subgroup ratings caused by visible author identities – requires the parallel trends identiﬁcation assumption. This assumption states that, had the conference never switched to double-blind reviewing, the change in expected rating for subgroup G1 from tSB to tDB would have been equal that for subgroup G0 from tSB to tDB. It is a special case of Assumption 1 when the disparity in each year is deﬁned as a diﬀerence in mean subgroup ratings, as in Eq. (4). The parallel trends assumption is less restrictive than the “no unobserved confounders” assumption. We discuss the validity of this assumption in our setting in Appendix A.

10

Years tSB, tDB
2017, 2018 Placebo Tests
2018, 2019 2019, 2020

“Ground truth” bias in review ratings Estimated bias in the review text

Bias

p-value

95% CI

Bias

p-value

95% CI

-0.369 (0.164) 0.024 (-0.690, -0.047) -0.166 (0.055) 0.002 (-0.270, -0.063)

0.138 (0.112) 0.118 (0.099)

0.219 0.236

(-0.082, 0.358) (-0.077, 0.313)

-0.070 (0.068) 0.012 (0.043)

0.308 0.781

(-0.195, 0.072) (-0.082, 0.083)

Table 2: “Ground truth” and estimated bias with respect to aﬃliation. “Ground truth” diﬀerencein-diﬀerence estimates of the bias in the review ratings (left) and bias in the review text estimated by our proposed framework (right). Standard errors reported in brackets. Estimates in each row are computed using ICLR peer reviews in consecutive years tSB and tDB. Estimates in bold are statistically signiﬁcant at the 5% level.

The ratings bias γ in Eq. (5) is typically estimated using a “two-way ﬁxed-eﬀects” regression (Imai and Kim, 2020) on peer reviews from the years tSB and tDB:

rij = ρ + αI[Tj = tDB] + βI[Sj = G0]

(6)

+ γI[Tj = tDB] × I[Sj = G0] + ij

where the coeﬃcients ρ, α, β and γ are estimated using ordinary least squares (OLS). The error term ij is assumed to be Gaussian with zero-mean, which enables deriving asymptotic conﬁdence intervals and p-values for the estimates.
We estimate the ratings bias using the two-way ﬁxed-eﬀects regression model in Eq. (6) on peer reviews from ICLR 2017 (tSB) and 2018 (tDB). Recall that Fig. 2b reports a change in the mean ratings disparity from ∆r2a0t1i7ng = 0.665 to ∆r2a0t1i8ng = 0.297, for a total of ∆r2a0t1i8ng − ∆r2a0t1i7ng = −0.369 units after switching to double-blind reviewing. The estimated bias in the ﬁrst row of Table 2 (left) mirrors this. In addition, the conﬁdence intervals and p-value indicate that the estimated bias is statistically signiﬁcant (p = 0.024).
Recall that reviewing in ICLR was double-blind in the years 2018, 2019 and 2020. Hence, as “placebo tests”, we also estimate γ using the two-way ﬁxed-eﬀects regression model in Eq. (6) using peer reviews in the year pair (tSB = 2018, tDB = 2019), and the year pair (tSB = 2019, tDB = 2020). The estimates are reported in the second and third rows of Table 2 (left). The ratings bias estimated using either of these year pairs is statistically insigniﬁcant. This is consistent with the fact that reviewing was double-blind during both years in the pair, and lends support to the validity of the parallel trends assumption.
Table 2 (left) thus comprises the “ground truth” presence and absence of bias for each year pair, which we expect our proposed framework to uncover from the review text without having access to the review ratings.

6.2 Estimating and evaluating aﬃliation bias in the review text
We now estimate the bias in the review text using equations (1) and (3). We use multinomial Naive Bayes classiﬁers with add-one smoothing for f (·) and g(·) on frequencies of unigrams and bigrams in the review and abstract text respectively. We use the area under the ROC curve (AUC) for both perf(f ; t) and perf(g; t), estimated using 10-fold cross-validation. We downsample the reviews and abstracts in year tDB to equalize the sample sizes and subgroup proportions in tSB and tDB, as
11

Source
Ratings Text

Bias
-0.073 (0.160) -0.468 (0.335)

p-value
0.647 0.163

95% CI
(-0.386, 0.240) (-0.862, 0.198)

Table 3: “Ground truth” and estimated bias with respect to perceived gender. Estimated bias in the review text and review ratings with respect to the authors’ perceived gender using ICLR peer reviews in the years 2017 and 2018. Standard errors reported in brackets.

described in Section 4. We repeat the bias estimation procedure 1,000 times with downsampling and the cross-validation folds randomized uniformly in each iteration. We use the empirical distribution of bias estimates from these iterations to construct conﬁdence intervals on the estimated bias. We compute the p-value from the conﬁdence intervals using the analytical method proposed by Altman and Bland (2011).
The estimated biases in the review text for the year pair (tSB = 2017, tDB = 2018) and the placebo year pairs (tSB = 2018, tDB = 2019) and (tSB = 2019, tDB = 2020) are reported in Table 2 (right). The ﬁrst row of Table 2 (right) reports a statistically signiﬁcant (p = 0.002) estimated bias corresponding to a reduction of 0.166 units (and hence, a negative estimate) in the classiﬁcation performance ratio (see Eq. 3) from ICLR 2017 to 2018. The second and third rows of Table 2 (right) report statistically insigniﬁcant bias estimates using peer reviews in the double-blind year pairs (tSB = 2018, tDB = 2019) and (tSB = 2019, tDB = 2020). The biases in the review text in each year pair estimated using our proposed framework are consistent with the presence and absence of “ground truth” ratings bias in each year pair reported in Table 2 (left). This validates the eﬀectiveness of our proposed framework.

6.3 Estimating and evaluating bias with respect to the authors’ perceived gender
Diﬀerent types of biases may be expressed in the text against population subgroups deﬁned in diﬀerent ways (such as by aﬃliation, race or gender). Our proposed framework does not rely on linguistic feature-engineering targeted at any speciﬁc type of bias. Hence, we evaluate the ability of our proposed framework to test for biases with subgroups deﬁned based on the authors’ gender as perceived by the reviewer (and not their self-reported gender). We detailed our gender-based subgroup deﬁnition and our gender annotation protocol earlier in Section 5.
Since our manual gender annotations only span ICLR 2017 and 2018, we report the estimated bias in the review ratings and text using ICLR 2017 and 2018 in Table 3. The “ground truth” bias in the review ratings (estimated using the diﬀerence-in-diﬀerences methodology as in Section 6.1) is statistically insigniﬁcant. The bias in the review text estimated using our proposed framework is also statistically insigniﬁcant, and hence, consistent with the “ground truth”.
In summary, given our data and choice of gender-based subgroups, we cannot reject the null hypotheses of there being no bias in the review ratings and text against papers with at least one author perceived to be non-male. It is, however, important to note that failing to reject the null hypothesis does not conﬁrm the absence of gender bias.

12

7. Conclusion and Discussion
Our work addresses an important yet relatively overlooked medium through which biases can harm society — that of text-based communication. We propose a framework to nonparametrically estimate biases expressed in text, which is robust to a larger class of unobserved confounders than prior work. We evaluate our approach in the setting of scholarly peer-review, wherein the “ground truth” bias can be inferred, and show that our proposed framework detects bias in the peer review text that is consistent with the “ground truth”. Our framework can be used by policymakers to formulate more eﬀective bias-mitigation policies that improve the equitability of hiring, promotion and other socioeconomic processes.
More generally, our work extends the diﬀerence-in-diﬀerences methodology to accommodate unstructured text as the “outcome”. It operates on text observed in two time periods associated with two population subgroups before and after a (potentially identity-hiding) policy change, such as switching to age-blind recruitment (Capowski, 1994), blind performance reviews (Goldin and Rouse, 2000) or blind grading (Hanna and Linden, 2012). Our proposed framework quantiﬁes the causal eﬀect of the policy change on the diﬀerence in the text associated with each population subgroup. As such, our work also contributes to the nascent literature on causal inference from text (Roberts et al., 2018; Egami et al., 2018; Pryzant et al., 2018; Sridhar and Getoor, 2019; Keith et al., 2020) with “text as the outcome”.
As a policymaking tool, one should take care in using our proposed framework appropriately. Our evaluation study in Section 6 is a detailed example of how the estimates and conﬁdence intervals from our proposed framework are to be interpreted, both when they are statistically signiﬁcant and insigniﬁcant. We expect that, with this example, users of our proposed framework are motivated to employ similar care when interpreting the bias estimates in their setting.
A correct use of our proposed framework would entail carefully considering the validity of the underlying identiﬁcation assumption (Assumption 1). If an unobserved confounder exists that violates Assumption 1, our estimates will quantify the change in disparity from tSB to tDB caused by a combination of hiding author identities and the unobserved confounder, which cannot be interpreted as bias. While this assumption is empirically untestable (since it involves unobserved counterfactual quantities), placebo tests can be used to empirically support its validity. However, note that while the failure of a placebo test implies that the identiﬁcation assumption does not hold, the success of a placebo test does not conﬁrm that the identiﬁcation assumption holds.
In our peer review setting, a potential confounder is an increase in research funding only for the institutions comprising G1 from ICLR 2017 to 2018, and not for those comprising G0. In Fig. 2, note that the subgroup disparity in the mean and median review ratings decreases from ICLR 2017 to 2018 (the mean and median review ratings for the two subgroups become more similar in 2018). However, also note that the reduction in disparity is due to a decrease in the mean and median ratings for subgroup G0 in 2018. Had research funding for the institutions comprising G1 increased, we would have expected an increase in the mean or median ratings for subgroup G1 in 2018. Hence, the temporal trends in ratings in Fig. 2 contradict the hypothesis of confounding due to an increase in research funding for the institutions comprising G1. In Appendix A, we detail this argument further and show how a combination of substantive reasoning, empirical tests and external evidence must be used to assess the validity of the identiﬁcation assumption.
13

Acknowledgments
The work of Nihar Shah was supported by an NSF CAREER Award CIF: 1942124.
References
Alberto Alesina and Eliana La Ferrara. A test of racial bias in capital sentencing. American Economic Review, 2014.
Douglas G Altman and J Martin Bland. How to obtain the p value from a conﬁdence interval. BMJ: British Medical Journal (Online), 2011.
Joshua D Angrist and J¨orn-Steﬀen Pischke. Mostly harmless econometrics: An empiricist’s companion. Princeton university press, 2008.
Christophe Bernard. Gender bias in publishing: Double-blind reviewing as a solution? Eneuro, 2018. Marianne Bertrand and Sendhil Mullainathan. Are Emily and Greg more employable than Lakisha and
Jamal? a ﬁeld experiment on labor market discrimination. American economic review, 2004. Rebecca M Blank. The eﬀects of double-blind versus single-blind reviewing: Experimental evidence from the
american economic review. The American Economic Review, 1991a. Rebecca M Blank. The Eﬀects of Double-Blind versus Single-Blind Reviewing: Experimental Evidence from
The American Economic Review. American Economic Review, December 1991b. Francine D Blau and Lawrence M Kahn. The gender wage gap: Extent, trends, and explanations. Journal of
Economic Literature, 2017. Amber E. Budden, Tom Tregenza, Lonnie W. Aarssen, Julia Koricheva, Roosa Leimu, and Christopher J.
Lortie. Double-blind review favours increased representation of female authors. Trends in Ecology and Evolution, 2008. URL http://www.sciencedirect.com/science/article/pii/S0169534707002704. Guillaume Cabanac and Thomas Preuss. Capitalizing on order eﬀects in the bids of peer-reviewed conferences to secure reviews by expert referees. Journal of the Association for Information Science and Technology, 2013. Genevieve Capowski. Ageism: The new diversity issue. Management Review, 1994. David Card and Alan B Krueger. Minimum wages and employment: A case study of the fast-food industry in new jersey and pennsylvania. The American Economic Review, 1994. Stephen J Ceci and Douglas P Peters. Peer review: A study of reliability. Change: The Magazine of Higher Learning, 1982. Shelley Correll, Katherine Weisshaar, Alison Wynn, and JoAnne Wehner. Inside the black box of organizational life: The gendered language of performance assessment. American Sociological Review, 2020. Naoki Egami, Christian J Fong, Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. How to make causal inferences using texts. arXiv preprint arXiv:1802.02163, 2018. Anjalie Field and Yulia Tsvetkov. Unsupervised discovery of implicit gender bias. arXiv preprint arXiv:2004.08361, 2020. T Fiez, N Shah, and L Ratliﬀ. A SUPER* algorithm to optimize paper bidding in peer review. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2020. Joseph M Garfunkel, Martin H Ulshen, Harvey J Hamrick, and Edward E Lawson. Eﬀect of institutional prestige on reviewers’ recommendations and editorial decisions. JAMA, 1994. N. Garg, T. Kavitha, A. Kumar, K. Mehlhorn, and J. Mestre. Assigning papers to referees. Algorithmica, 2010. Matthew Gentzkow, Jesse M Shapiro, and Matt Taddy. Measuring group diﬀerences in high-dimensional choices: Method and application to congressional speech. Econometrica, 2019. Philip Goldberg. Are women prejudiced against women? Trans-action, 1968.
14

Claudia Goldin and Cecilia Rouse. Orchestrating impartiality: The impact of” blind” auditions on female musicians. American economic review, 2000.
Government of Canada. Canada’s science vision. 2020. URL https://www.ic.gc.ca/eic/site/131.nsf/ eng/h_00000.html.
Rema N Hanna and Leigh L Linden. Discrimination in grading. American Economic Journal: Economic Policy, 2012.
Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 1986.
Kosuke Imai and In Song Kim. On the use of two-way ﬁxed eﬀects regression models for causal inference with panel data. 2020.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.
Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei Fang. Mitigating manipulation in peer review via randomized reviewer assignments. In ICML Workshop on Incentives in Machine Learning, 2020.
Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. A dataset of peer reviews (peerread): Collection, insights and nlp applications. arXiv preprint arXiv:1804.09635, 2018.
Sara Kay. Yelp reviewers’ authenticity fetish is white supremacy in action. Eater New York, 2019. URL https://ny.eater.com/2019/1/18/18183973/authenticity-yelp-reviews-white-supremacy-trap.
Katherine Keith, David Jensen, and Brendan O’Connor. Text and causal inference: A review of using text to remove confounding from causal estimates. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.
Ari Kobren, Barna Saha, and Andrew McCallum. Paper matching with local fairness constraints. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019.
Carla Kweiﬁo-Okai. Media distortion and western bias – why do some disasters attract more cash? The Guardian, 2014. URL https://www.theguardian.com/global-development/2014/dec/ 02/students-speak-media-distortion-western-bias-disasters.
Carole J Lee. Commensuration bias in peer review. Philosophy of Science, 2015.
Margaret E Lloyd. Gender factors in reviewer recommendations for manuscript publication. Journal of applied behavior analysis, 1990.
Lori Mackenzie, JoAnne Wehner, and Shelley J. Correll. Why most performance evaluations are biased, and how to ﬁx them. Harvard Business Review, 2019. URL https://hbr.org/2019/01/ why-most-performance-evaluations-are-biased-and-how-to-fix-them.
Samuel Madden and David DeWitt. Impact of double-blind reviewing on sigmod publication rates. ACM SIGMOD Record, 2006.
Juan M Madera, Michelle R Hebl, Heather Dial, Randi Martin, and Virgina Valian. Raising doubt in letters of recommendation for academia: gender diﬀerences and their impact. Journal of Business and Psychology, 2019.
Kristina MW Mitchell and Jonathan Martin. Gender bias in student evaluations. PS: Political Science & Politics, 2018.
Fred Morstatter, Liang Wu, Uraz Yavanoglu, Stephen R Corman, and Huan Liu. Identifying framing bias in online news. ACM Transactions on Social Computing, 2018.
National Science Foundation. National patterns of r&d resources: 2017–18 data update. 2020. URL https://ncses.nsf.gov/pubs/nsf20307.
Ritesh Noothigattu, Nihar Shah, and Ariel Procaccia. Loss functions, axioms, and peer review. In ICML Workshop on Incentives in Machine Learning, 2020.
Kanu Okike, Kevin T Hug, Mininder S Kocher, and Seth S Leopold. Single-blind vs double-blind peer review in the setting of author prestige. Jama, 2016.
15

Reid Pryzant, Sugato Basu, and Kazoo Sone. Interpretable neural architectures for attributing an ad’s performance to its writing style. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018.
Saif S Rathore and Harlan M Krumholz. Diﬀerences, disparities, and biases: clarifying racial variations in health care use, 2004.
Margaret E Roberts, Brandon M Stewart, and Richard A Nielsen. Adjusting for confounding with text matching. 2018.
Magnus Roos, J¨org Rothe, and Bj¨orn Scheuermann. How to calibrate the scores of biased reviewers by quadratic programming. In AAAI Conference on Artiﬁcial Intelligence, 2011.
Joseph S Ross, Cary P Gross, Mayur M Desai, Yuling Hong, Augustus O Grant, Stephen R Daniels, Vladimir C Hachinski, Raymond J Gibbons, Timothy J Gardner, and Harlan M Krumholz. Eﬀect of blinded peer review on abstract acceptance. Jama, 2006.
Babak Salimi, Harsh Parikh, Moe Kayali, Lise Getoor, Sudeepa Roy, and Dan Suciu. Causal relational learning. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, 2020.
Marco Seeber and Alberto Bacchelli. Does single blind peer review hinder newcomers? Scientometrics, 2017. Yishai Shimoni, Chen Yanover, Ehud Karavani, and Yaara Goldschmnidt. Benchmarking framework for
performance-evaluation of causal inference analysis. arXiv preprint arXiv:1802.05046, 2018. Richard Snodgrass. Single-versus double-blind reviewing: an analysis of the literature. ACM Sigmod Record,
2006. Dhanya Sridhar and Lise Getoor. Estimating causal eﬀects of tone in online debates. In Proceedings of the
28th International Joint Conference on Artiﬁcial Intelligence. AAAI Press, 2019. Ivan Stelmakh, Nihar Shah, and Aarti Singh. PeerReview4All: Fair and accurate reviewer assignment in peer
review. In Conference on Algorithmic Learning Theory, 2019a. Ivan Stelmakh, Nihar Shah, and Aarti Singh. On testing for biases in peer review. In NeurIPS, 2019b. Ivan Stelmakh, Nihar Shah, and Aarti Singh. Catch me if i can: Detecting strategic behaviour in peer
assessment. arXiv, 2020. David Str¨omberg. Natural disasters, economic development, and humanitarian aid. Journal of Economic
perspectives, 2007. Janet Swim, Eugene Borgida, Geoﬀrey Maruyama, and David G Myers. Joan mckay versus john mckay: Do
gender stereotypes bias evaluations? Psychological Bulletin, 1989. Donald M Taylor and Janet R Doria. Self-serving and group-serving bias in attribution. The Journal of
Social Psychology, 1981. Mike Thelwall, Liz Allen, Eleanor-Rose Papas, Zena Nyakoojo, and Verena Weigert. Does the use of open,
non-anonymous peer review in scholarly publishing introduce bias? Evidence from the F1000Research postpublication open peer review publishing model. Journal of Information Science, page 0165551520938678, 2020. Andrew Tomkins, Min Zhang, and William D Heavlin. Reviewer bias in single-versus double-blind peer review. Proceedings of the National Academy of Sciences, 2017. Anthony KH Tung. Impact of double blind reviewing on sigmod publication: a more detail analysis. ACM SIGMOD Record, 2006. Richard Walker, Beatriz Barros, Ricardo Conejo, Konrad Neumann, and Martin Telefont. Bias in peer review: a case study. F1000Research, 2015. Jingyan Wang and Nihar B Shah. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS, 2019. Thomas J Webb, Bob O’Hara, and Robert P Freckleton. Does double-blind review beneﬁt female authors? Trends in Ecology & Evolution, 2008. Yichong Xu, Han Zhao, Xiaofei Shi, and Nihar Shah. On strategyproof conference review. In IJCAI, 2019.
16

Appendix A. Validity of the identiﬁcation assumptions
Our evaluation in Section 6 relies on Assumption 1 (required by our proposed framework) and the parallel trends assumption (required by the diﬀerence-in-diﬀerences methodology used to construct the “ground truth”). Both assumptions are robust to unobserved confounders that (i) do not change from tSB to tDB, or (ii) change from tSB to tDB, but aﬀect the peer review text or rating for both subgroups to the same extent. For example, if the peer reviewers were harsher in tDB than in tSB by the same amount for G0 and G1, the average peer review ratings and the “positivity” of the peer review text would be lower in tDB for both G0 and G1 by the same amount. Hence, having harsher reviewers in tDB would have no eﬀect on the review text or ratings disparities that comprise the bias in Eqs. (1) and (5).
If an unobserved confounder exists that violates these assumptions, our estimates will quantify the change in disparity from tSB to tDB caused by a combination of hiding author identities and the unobserved confounder. Neither assumption is empirically testable (since each involves unobserved counterfactual quantities). Placebo tests can empirically support (though not conﬁrm) the validity of these assumptions. Hence, the identiﬁcation assumptions must be argued for substantively (with external evidence if possible), and care must be taken when interpreting the estimates as biases (the change in disparity caused by hiding author identities).
In our setting, a potential confounder is an increase in research funding only for the institutions comprising G1 from ICLR 2017 to 2018, and not for those comprising G0. Recall that papers in G1 were rated lower on average than those in G0 in ICLR 2017. A funding increase of this type could result in an improvement in the average quality of papers in G1 from 2017 to 2018, with no change in the average quality of papers in G0. This would, in turn, cause a reduction in the subgroup disparity from ICLR 2017 to 2018. Hence, if such a confounder was present, it would be impossible to attribute the change in disparity from ICLR 2017 to 2018 entirely to the switch to double-blind reviewing.
We argue against the existence of such a confounder. In Fig. 2, we note that the disparity in the mean and median review ratings for each subgroup decreases from ICLR 2017 to 2018. However, we also note that this reduction is due to a decrease in the mean and median ratings for subgroup G0. Had the aforementioned confounder been present, we would expect the disparity to reduce due to an increase in the mean or median ratings for subgroup G1. Hence, this contradicts the claim that an increase in research funding only for the institutions comprising G1 confounds our bias estimates.
Another potential confounder is a decrease in research funding for the institutions comprising G0, which would lead to a reduction in disparity from ICLR 2017 to 2018 (if the quality of papers in G0 decreased due to lesser funding). However, federal research funding in the USA increased in every year from 1953 to 2018 (National Science Foundation, 2020), and federal research funding in Canada increased in every year since 2013 (Government of Canada, 2020). Thus, it is unlikely that a decrease in research funding for the institutions comprising G0 confounds our bias estimates.
Appendix B. Comparison with an alternate measure of disparity in the text
Recall from Eq. (3) that our proposed framework quantiﬁes disparity in the text using the performance of binary classiﬁers in predicting subgroup membership. We now evaluate an alternate method to measure disparity in the text proposed by Gentzkow et al. (2019). We estimate the bias in the review text using the disparities derived from this method for each pair of consecutive years
17

Years tSB, tDB
2017, 2018 2018, 2019 2019, 2020

Bias
-0.468 (0.335) 0.549 (0.376) -0.501 (0.384)

p-value
0.163 0.145 0.193

95% CI
(-0.862, 0.198) (-0.025, 0.864) (-0.835, 0.091)

Table 4: Aﬃliation bias estimated using (Gentzkow et al., 2019). Estimated aﬃliation biases in the review text using ICLR peer reviews in consecutive years tSB and tDB. The disparity in the text is estimated using the method proposed by Gentzkow et al. (2019). Standard errors are reported in brackets.

from ICLR 2017 to 2020, and compare these estimates with the “ground truth” biases estimated in Section 6.1.
Gentzkow et al. (2019) propose measuring subgroup disparities in text by estimating a regularized Poisson generative model of word frequencies as a function of subgroup membership. Speciﬁcally, the disparity for each word in the review (or abstract) text is estimated using an L1-regularized Poisson regression of the word frequency on the subgroup membership indicator. The magnitude of the regularization penalties are chosen to minimize the Bayesian Information Criterion (BIC) of the estimated model. The overall disparity in the review (or abstract) text is computed by aggregating all word-level disparities. Conﬁdence intervals are constructed by repeated estimation using random subsampling without replacement.
We estimate the bias in the review text using equations (1) and (3), with perf(f ; t) and perf(g; t) replaced by disparities in the review and abstract text (respectively) estimated using the method proposed by Gentzkow et al. (2019). As in Section 6.2, we downsample the reviews and abstracts in year tDB and repeat the bias estimation procedure 1,000 times to construct conﬁdence intervals. The estimated biases are reported in Table 4. For any year pair, with disparities in the text estimated using the method proposed by Gentzkow et al. (2019), we cannot reject the null hypothesis of there being no bias in the review text.
Hence, compared to our proposed method to measure disparity in the text, the method proposed by Gentzkow et al. (2019) lacks statistical power for the data used in our setting. We attribute this to two factors. First, the frequencies of most words used in the reviews and abstracts are low, which would lead to noisy estimates of word-level disparities, and hence, noisy overall disparity estimates. Second, the Poisson regression imposes strong distributional assumptions on how the text is generated that are unlikely to hold in all settings.

18

