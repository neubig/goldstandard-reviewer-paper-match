DiscreTalk: Text-to-Speech as a Machine Translation Problem
Tomoki Hayashi1,2, Shinji Watanabe3
1Human Dataware Lab. Co. Ltd., Japan 2Nagoya University, Japan
3Johns Hopkins University, USA
hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp

arXiv:2005.05525v1 [cs.CL] 12 May 2020

Abstract
This paper proposes a new end-to-end text-to-speech (E2ETTS) model based on neural machine translation (NMT). The proposed model consists of two components; a nonautoregressive vector quantized variational autoencoder (VQVAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model. Index Terms: speech synthesis, text-to-speech, end-to-end, vector quantization, machine translation
1. Introduction
With the improvement of deep learning techniques, the presence of end-to-end text-to-speech (E2E-TTS) models has been growing not only in the research Ô¨Åeld but also as a production system [1]‚Äì[6]. Compared to the conventional statistical parametric speech synthesis (SPSS) systems [7]‚Äì[9], the E2E-TTS models do not require the language expert knowledge and the alignments between text and speech, making it possible to train the system with only the pairs of text and speech. Thanks to the neural vocoders [10]‚Äì[15], the E2E-TTS models can achieve the quality comparable to professionally recorded speech.
Although E2E-TTS models have achieved excellent performance with a simple training scheme, it still depends on the human-designed acoustic features, e.g., Mel-spectrogram [2] and speech parameters such as F0 [3]. To achieve the best performance, we must carefully tune the hyperparameters of these acoustic features such as the number of points in fast Fourier transform (FFT), a window size, the analysis range of frequency, and the normalization method. Hence, current E2ETTS models are not in a fully end-to-end manner.
Recently, the discretization of sequential information in a fully-data-driven manner by vector quantized variational autoencoder (VQ-VAE) [16] has been getting attention. The VQVAE model, which consists of encoder and decoder networks,

can convert an arbitrary length sequential input into a downsampled sequence of the discrete symbols and precisely reconstruct the input from the discrete symbol sequence even if the input is a raw waveform of speech. One of the applications using VQVAE for speech processing is a voice conversion, where it models speech waveforms directly and utilizes additional speaker ID embedding to condition the decoder to control the speaker characteristics [16], [17]. Tjandra et al. have extended to a more challenging task, cross-lingual voice conversion by combining with a sequence-to-sequence (Seq2Seq) model [18]. Another exciting idea is automatic speech recognition (ASR) or TTS without target text [19], [20], where VQ-VAE models the acoustic feature space to obtain the discretized unit similar to phoneme instead of the corresponding text [21], [22]. Henter et al. have utilized VQ-VAE to obtain the embedding to control the speech speaking style in an unsupervised manner [23]. Kumar et al. have applied the non-autoregressive generative adversarial network (GAN)-based VQ-VAE model for music generation [15]. Thus, VQ-VAE has excellent potential to obtain meaningful representations in an unsupervised manner even for the extreme long sequence, such as speech waveforms.
This paper proposes a novel E2E-TTS framework based on VQ-VAE and neural machine translation (NMT), which is a fully-end-to-end model without human-designed acoustic features. The proposed model consists of two components; the VQ-VAE model that learns a mapping from a speech waveform into a sequence of discrete symbols and the Transformer-NMT model trained to estimate the discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use techniques developed in NMT and ASR. Furthermore, we can avoid an over smoothing problem of the predicted features, which is one of the common issues in TTS. The contributions of this paper are summarized as follows:
‚Ä¢ We propose a non-autoregressive GAN-based VQ-VAE model with the multi-resolution short-time Fourier transform (STFT) loss inspired by great successes of GAN-based neural vocoders [14], [15]. The proposed VQ-VAE‚Äôs decoder can decode the sequence of discrete symbols into a speech waveform much faster than the real-time while keeping the reasonable quality.
‚Ä¢ We introduce advanced decoding techniques such as beamsearch, shallow fusion with a language model (LM), and subword unit, commonly used in NMT and ASR Ô¨Åelds. With the ASR evaluation metric, we can investigate the effectiveness of these techniques more intuitively.
‚Ä¢ The experimental results of the subjective evaluation with mean opinion score (MOS) on naturalness show that the proposed method outperforms the conventional E2E-TTS

Subword sequences SentencePiece encode Discrete symbol sequences VQ-Codebook quantization
VQ-Encoder
Speech
Corpus

Cross entropy loss

Posterior

Softmax

Linear

eTnTrcTaroarndansensfrosfofbromrlmromecerkerr eennccooddeerrbblolocckk

dTeTrcTaroarndansensfrosfofbromrlmormecerkerr eennccooddeerrbblolocckk

Positional encoding
Embedding

Positional encoding
Embedding

Previous subword Text

Training phase Synthesis phase
Beam-search Predicted subword
sequences
SentencePiece decode
Predicted discrete symbol sequences
VQ-Codebook look up
VQ-Decoder
Speech

Figure 1: Overview of the proposed method.
model (Transformer-TTS [5] with Parallel WaveGAN [14]), achieving the performance comparable to the reconstruction of the VQ-VAE model.

2. VQ-VAE
The VQ-VAE model consists of three components: an encoder, a decoder, and a shared codebook. The encoder Enc(¬∑) is a nonlinear mapping function to encode T -length arbitrary sequential input X = {x1, x2, ..., xT } into a downsampled N -length sequence of vectors Z = {z1, ..., zn, ..., zN }(N < T ) in the latent space. Then, the quantization function Q(¬∑) converts vector zn to i-th centroid vector ei in the codebook based on the distance between them as follows:
Q(zn) = z(vnq) = ei where i = argmin zn ‚àí ej . (1)
j

The decoder Dec(¬∑) is another non-linear mapping function to reconstruct the input waveform from the sequence of quantized vectors. The whole network is trained using the following objective function:

L = Lrec + Lcb + ŒªcmLcm,

(2)

Lrec = Dec(Q(Enc(X))) ‚àí X 22,

(3)

Lcb = sg[Enc(X)] ‚àí Zvq 22,

(4)

Lcm = sg[Zvq] ‚àí Enc(X) 22,

(5)

where Lrec, Lcb, and Lcm represent reconstruction loss, codebook loss, and commitment loss, respectively. Œªcm represents a constant coefÔ¨Åcient for balancing between codebook loss and commitment loss. Zvq represents the sequence {z(v1q), ..., z(vNq )} and sg[¬∑] represents stop-gradient operation to prevent gradient from Ô¨Çowing its argument.

3. Method
3.1. Overview
The overview of the proposed model is shown in Fig 1, which is divided into training and synthesis phases. In the training phase, at Ô¨Årst, we train the non-autoregressive GAN-based VQVAE model using speech waveforms in the corpus (Section 3.2). Next, the VQ-encoder converts all of the speech waveforms into sequences of the discrete symbols, i.e., centroid IDs in the VQcodebook. The discrete symbol sequence is encoded into a se-

Generator
Speech

Reconstruction loss
Decoder Commit. loss Codebook loss Codebook
Encoder

Parameter update

√ó ùúÜ*+

Œ£ Gwra.rd.ti.enGts

ùúÜ%& √ó √ó

ùúÜ'() Œ£

Adversarial loss Fake
Discriminator loss Real Fake or Real?
Feature match. loss
Discriminator K Avg pool
Discriminator 2 Avg pool
Discriminator 1

‚Ä¶

‚Ä¶
Discriminator

Parameter update

Gradients w.r.t. D

Figure 2: The proposed VQ-VAE training Ô¨Çow.

quence of subword units by SentencePiece [24]. Then, we train the Transformer-NMT model [25] using text as the inputs and the subword sequence as the targets (Section 3.3).
In the synthesis phase, the Transformer-NMT model estimates the subword sequence from a given text using beamsearch [26]. The SentencePiece model decodes the estimated sequence of subword units into that of the original discrete symbols. Then, the VQ-codebook replaces each discrete symbol with the embedding. Finally, the VQ-decoder converts the embedding sequence into a speech waveform. In the following sections, we explain each component in detail.

3.2. Non-autoregressive GAN-based VQ-VAE

Let us introduce a speech waveform x = {x1, x2, ..., xT } as the inputs of the VQ-VAE model. To boost up the reconstruction performance, we use a multi-resolution STFT loss [14] as the reconstruction function and additionally introduce an adversarial objective function based on MelGAN [15]. Let us describe the VQ-VAE model as a generator network G(¬∑) (= Dec(Q(Enc(¬∑))), and introduce K discriminator networks Dk(¬∑) (k = 1, 2, ..., K), each of which has the same structure. As the network structure, we use the MelGAN‚Äôs generator as the decoder and the MelGAN‚Äôs discriminator as the encoder and each discriminator. The objective function in Eq. (2) and the reconstruction function in Eq. (3) are modiÔ¨Åed as follows:

L = LG = Lrec + Lcb + ŒªcmLcm + ŒªadvLadv, (6)

Lrec = M1 M m=1 L(mma)g + L(sm c ) , (7)

L(mma)g = 1 log |STFT(x)| ‚àí log |STFT(G(x))| 1, (8) F

L(m) = |STFT(x)| ‚àí |STFT(G(x))| F , (9)

sc

|STFT(x)| F

Ladv = K1 K k=1 (1 ‚àí Dk(G(x)))2 + ŒªfmL(fmk) , (10) L(fmk) = L1 Ll=1 Dk(l)(x) ‚àí Dk(l)(G(x)) 1, (11)

where Lsc, Lmag, Ladv, and Lfm represent spectral convergence loss, magnitude loss [27], adversarial loss and feature matching loss [15], respectively. M and m represents the number of the STFT loss functions and its index, respectively.
¬∑ 1 and ¬∑ F represent L1 and Frobenius norm, respectively. |STFT(¬∑)| and F represent STFT magnitude and the number of

elements in the magnitude, respectively. Dk(l)(¬∑) and L represent the l-th layer‚Äôs outputs of k-th discriminator and the number of layers, respectively. The objective function of the discriminator LD is deÔ¨Åned as follows:
LD = K1 K k=1 (1 ‚àí Dk(x))2 . (12)
We summarize the Ô¨Çow of the training in Fig. 2, which updating the generator and the discriminator alternately.
3.3. Transformer-NMT
We use an autoregressive Transformer-based encoder-decoder model [25], which consists of self-attention layers. The model learns the mapping from the character or the phoneme sequence to that of subword units of discrete symbols converted by SentencePiece [24]. Note that unlike a regular NMT problem, the length of the input and output differ signiÔ¨Åcantly since the length of the output is similar to that of acoustic features. We use cross-entropy loss to optimize the network with the label smoothing technique [28].
4. Experimental Evaluation
4.1. Experimental condition
To demonstrate the performance of the proposed model, we conducted an experimental evaluation using the JSUT corpus [29]. The JSUT corpus includes 10 hours of a single female Japanese speech. We used 7,196, 250, and 250 utterances for training, validation, and evaluation, respectively. The speech was downsampled to 24k Hz and the input text was converted into a phoneme sequence using Open JTalk [30]. To check the performance of the proposed method, we compared the following Ô¨Åve models:
1. Target: The target speech. We downsampled to 24k Hz and trimmed the silence at the beginning and the end of utterances by the force alignment with Julius [31].
2. Baseline: The baseline system using Transformer-TTS [5] with Parallel WaveGAN [14]. We used the open-source toolkit ESPnet-TTS [32] to build this system.
3. Reconst: Reconstructed speech by the proposed VQ-VAE. 4. Proposed (Raw): The proposed model with raw discrete
symbols as the target of the NMT model. 5. Proposed (SW): The proposed model with subword units.
For the proposed models, we built several models using different downsampling factor (DSF) in the VQ-VAE model (DSF128 or DSF256) and the different number of subword units (SW256 or SW512)1. All of the generated samples are available online [33]2. The detailed training condition of the VQ-VAE and Transformer-NMT models is shown in Table 1. While we used a Ô¨Åxed batch size with randomly cropped speech for the VQ-VAE model, using a dynamic batch making depending on the length of each sequence for the Transformer-NMT model. All of the models were trained with a single GPU (Titan V) with open-source E2E speech processing toolkit ESPnet [35].
4.2. Objective evaluation results
We conducted the objective evaluation using the ASR-based objective measure character error rate (CER) and the token error
1256 is the same as the number of centroids in codebook, but that of active centroids is much smaller than predeÔ¨Åned size.
2We are also planning to publish the codes as an open-source.

Table 1: Training conditions.

VQ-VAE training condition

Sampling rate # Centroids Centroids dimension
Downsampling scales
Upsampling scales
Batch size Batch length Optimizer Learning rate Gradient clip # Iterations (Œªcm, Œªfm, Œªadv)

24,000 Hz 256 128 [4, 4, 4, 4] (for DSF256) [4, 4, 4, 2] (for DSF128) [8, 8, 2, 2] (for DSF256) [8, 8, 4, 2] (for DSF128) 16 8,192 RAdam [34] 1e-4 (for G) & 5e-5 (for D) 10.0 (for G) & 1.0 (for D) 5,000,000 (0.25, 25.0, 4.0)

Transformer-NMT training condition

# Encoder blocks # Decoder blocks Feed-forward units Attention dimension # Attention heads Dropout-rate Batch size Optimizer # Warmup steps Gradient clip # Epochs Label smoothing weith

6 6 2,048 256 4 0.1 96 (in average) Noam [25] 8,000 5 2,000 0.1

rate (TER) of the NMT models. As the ASR model, we used the Transformer-ASR model trained on the corpus of spontaneous Japanese (CSJ) [36]. The objective evaluation result is shown in Table 23, where ‚ÄúBeam‚Äù represents the beam-size of beam-search in decoding.
First, we focus on the CER result in Table 2. A comparison between the different DSFs shows that the reconstruction with the small DSF achieved the performance comparable to the baseline while the use of the large DSF greatly affected the intelligibility. From audio samples available in [33], the model with a large DSF generated slurred speech while keeping high signal-to-noise (SN) ratio. The authors highly recommend listening to the samples to understand the difference. Thus, there was a trade-off between the size of DSF and speech articulation. From a comparison between the target types, the use of the subword was effective, especially in the case of the small DSF. This is because the same discrete symbol repeatedly appeared in the sequence due to high time resolution, and the subword can summarize these successive symbols. Note that since the use of the subword made the length of the target sequence smaller, it can also reduce the training time. However, the use of a large number of subword units made the target symbols sparse, decreasing the performance. Therefore, we need to tune the number of subword units according to the size of the training data. Focusing on the effectiveness of beam-search, it led to a slight improvement of the performance, but a large beam-size did not always bring the improvement.
Next, we focus on the TER result of the NMT models in Table 2. Interestingly, the predicted sequence is totally different from the ground-truth, and we could not Ô¨Ånd the meaningful
3Note that ASR result includes Hiragana, Katakana, and Kanji. It contains many homophonic words (e.g., Á†ÇÁ≥ñ and ‰ΩêËó§) and transcription mismatch (e.g., „Åù„Çå„Åª„Å©„Å´ and „Åù„ÇåÁ®ã„Å´).

Table 2: Character error rates calculated by the ASR model and token error rates of the NMT models.

Method

Baseline Reconst (DSF256) Proposed (DSF256, + Beam=3 + Beam=5 + Beam=10 Proposed (DSF256, + Beam=3 + Beam=5 + Beam=10 Reconst (DSF128) Proposed (DSF128, + Beam=3 + Beam=5 + Beam=10 Proposed (DSF128, + Beam=3 + Beam=5 + Beam=10 Proposed (DSF128, + Beam=3 + Beam=5 + Beam=10

Raw) SW256) Raw) SW256) SW512)

Target

CER [%]
15.1 22.9 23.8 23.1 23.5 22.9 24.7 23.8 22.9 23.5 14.8 20.8 21.3 21.9 21.2 18.6 18.2 19.1 19.0 19.5 19.1 19.6 19.3
12.1

TER [%]
87.2 87.2 87.4 87.3 92.4 92.3 92.2 92.2 91.5 92.0 92.4 92.6 93.3 93.0 93.0 93.2 94.8 94.8 94.9 94.8
-

correlation between the CER of the ASR model and the TER of the NMT model. The training process also tended to be overÔ¨Åtting in the early stage. One of the possible reasons is that speech included various speaking styles or intonations, even for the same words. Therefore, the conversion from the text to the sequence of discrete symbols became a one-to-many problem.
Finally, we investigated the performance of shallow fusion with the LM for the subword sequence of the discrete symbols (VQ-LM). We built three long short-term memory (LSTM)based VQ-LMs with 1,024 units and a different number of layers (1, 2, and 4) for the subword sequence (DSF128, SW256). The training curve showed the same tendency as the NMT models, which tended to overÔ¨Åt in the early stage, and the deeper model brought lower training perplexity (10.6, 7.6, and 5.3, respectively). To check the effectiveness of the fusion, we used the best perplexity VQ-LM model and changed the weight for its score in decoding from 0.1 to 0.3 while Ô¨Åxing beam-size to 3. The result is shown in Table 3. From the results, we could not conÔ¨Årm the improvement. One of the reasons is that the training data was the same for NMT models and VQ-LMs, and the amount of training data was relatively small while the length of each sequence was long. We need to investigate the case where the training data of VQ-LM is much bigger than NMT models; in other words, we have many untranscribed utterances.
4.3. Subjective evaluation results
We conducted a subjective evaluation using mean opinion score (MOS) on naturalness. We used the ‚ÄúVOICE ACTRESS‚Äù subset in the JSUT corpus (= 100 utterances) for the subjective evaluation. The number of subjects is 45, and that of evaluation samples per each subject is 160 (= 20 samples √ó 8 models). Each subject rated the naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. We instructed subjects to work in a quiet room and use headphones. We used the Likert single stimulus test available in WebMUSHRA [37].

Table 3: Effectiveness of shallow fusion with the VQ-LM.

LM weight
0.1 0.2 0.3
0.0

CER [%]
18.4 21.0 21.4
18.2

TER [%]
93.2 93.8 94.2
93.0

Table 4: Mean opinion score on naturalness, where CI represents conÔ¨Ådence interval. The beam-size is Ô¨Åxed to 1.

Method
Baseline Reconstruction (DSF256) Proposed (DSF256, Raw) Proposed (DSF256, SW256) Reconstruction (DSF128) Proposed (DSF128, Raw) Proposed (DSF128, SW256)
Target

MOS ¬± 95% CI
3.48 ¬± 0.08 3.36 ¬± 0.07 3.25 ¬± 0.07 3.27 ¬± 0.07 3.99 ¬± 0.06 3.39 ¬± 0.08 3.93 ¬± 0.06
4.32 ¬± 0.05

The subjective result is shown in Table 4. A comparison between the different DSFs shows that the small DSF led to better naturalness than the large one in the reconstruction condition. In the case of the large DSF, both raw and subword models are almost the same naturalness, comparable to the reconstruction condition. However, in the case of the small DSF, there is a large difference between raw and subword models. While the subword model achieved the performance comparable to the reconstruction condition, the raw model is worse than the reconstruction. The reason for the difference is that due to the lengthened target sequence, the raw model with the small DSF sometimes failed to generate speech, leading long pause and word deletions. Hence, the use of the subword unit is effective, especially in the case of the small DSF. Compared to the other model, our best model outperformed the baseline with a signiÔ¨Åcant difference (signiÔ¨Åcance level of 5 %).
The results represent that the VQ-VAE model determined the upper bound of the performance, and there was less gap between the reconstruction and synthesis conditions than the conventional E2E-TTS models. Also, while the smaller DSF led to better reconstruction performance, but the smaller one made the training of NMT models difÔ¨Åcult. Therefore, if we can improve the reconstruction performance of the VQ-VAE model while keeping a reasonable DSF size, it is expected that the quality of the proposed method is further improved.
5. Conclusions
This paper proposed a novel E2E-TTS framework consisting of the VQ-VAE and NMT models. The experimental evaluation with the JSUT corpus showed that 1) the proposed model outperforms the conventional Transformer-TTS with Parallel WaveGAN in naturalness, achieving the performance comparable to the reconstruction condition, 2) the use of subword unit is effective, especially in the case of the small downsampling factor, and 3) there is a trade-off between the resolution of discrete symbols and speech articulation.
In future work, we will consider the attention constraint for Transformer-NMT to make the generation more stable, extend this framework to a multi-speaker model, apply to the largescale corpus to clarify the effectiveness of VQ-LMs, and make it fully-non-autoregressive by using connectionist temporal classiÔ¨Åcation [38].

6. References
[1] Y. Wang, R. Skerry-Ryan, D. Stanton, et al., ‚ÄúTacotron: Towards end-to-end speech synthesis,‚Äù in Proc. Interspeech, 2017.
[2] J. Shen, R. Pang, R. J. Weiss, et al., ‚ÄúNatural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions,‚Äù in ICASSP, 2018, pp. 4779‚Äì4783.
[3] W. Ping, K. Peng, A. Gibiansky, et al., ‚ÄúDeep Voice 3: Scaling text-to-speech with convolutional sequence learning,‚Äù in ICLR, 2018.
[4] W. Ping, K. Peng, and J. Chen, ‚ÄúClariNet: Parallel wave generation in end-to-end text-to-speech,‚Äù in ICLR, 2019.
[5] N. Li, S. Liu, Y. Liu, et al., ‚ÄúClose to human quality TTS with Transformer,‚Äù ArXiv preprint arXiv:1809.08895, 2018.
[6] Y. Ren, Y. Ruan, X. Tan, et al., ‚ÄúFastspeech: Fast, robust and controllable text to speech,‚Äù in NIPS, 2019, pp. 3165‚Äì3174.
[7] P. Taylor, Text-to-speech synthesis., 2009.
[8] H. Zen, K. Tokuda, and A. W. Black, ‚ÄúStatistical parametric speech synthesis,‚Äù Speech Communication, vol. 51, no. 11, 2009.
[9] K. Tokuda, Y. Nankaku, T. Toda, et al., ‚ÄúSpeech synthesis based on hidden Markov models,‚Äù Proceedings of the IEEE, vol. 101, no. 5, pp. 1234‚Äì1252, 2013.
[10] A. v. d. Oord, S. Dieleman, H. Zen, et al., ‚ÄúWaveNet: A Generative Model for Raw Audio,‚Äù ArXiv:1609.03499, 2016.
[11] Z. Wu and S. King, ‚ÄúInvestigating gated recurrent networks for speech synthesis,‚Äù in ICASSP, 2016, pp. 5140‚Äì 5144.
[12] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda, ‚ÄúSpeaker-dependent WaveNet vocoder.,‚Äù in Proc. Interspeech, 2017.
[13] R. Prenger, R. Valle, and B. Catanzaro, ‚ÄúWaveGlow: A Ô¨Çow-based generative network for speech synthesis,‚Äù in ICASSP, 2019, pp. 3617‚Äì3621.
[14] R. Yamamoto, E. Song, and J.-M. Kim, ‚ÄúParallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,‚Äù in ICASSP, 2020, pp. 6199‚Äì6203.
[15] K. Kumar, R. Kumar, T. de Boissiere, et al., ‚ÄúMelgan: Generative adversarial networks for conditional waveform synthesis,‚Äù in NIPS, 2019, pp. 14 881‚Äì14 892.
[16] A. van den Oord, O. Vinyals, et al., ‚ÄúNeural discrete representation learning,‚Äù in NIPS, 2017, pp. 6306‚Äì6315.
[17] S. Ding and R. Gutierrez-Osuna, ‚ÄúGroup latent embedding for vector quantized variational autoencoder in nonparallel voice conversion,‚Äù Proc. Interspeech, pp. 724‚Äì 728, 2019.
[18] A. Tjandra, S. Sakti, and S. Nakamura, ‚ÄúSpeech-tospeech translation between untranscribed unknown languages,‚Äù ArXiv preprint arXiv:1910.00795, 2019.
[19] E. Dunbar, X. N. Cao, J. Benjumea, et al., ‚ÄúThe zero resource speech challenge 2017,‚Äù in Proc. ASRU, 2017, pp. 323‚Äì330.
[20] E. Dunbar, R. Algayres, J. Karadayi, et al., ‚ÄúThe zero resource speech challenge 2019: TTS without T,‚Äù ArXiv preprint arXiv:1904.11469, 2019.

[21] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord, ‚ÄúUnsupervised speech representation learning using wavenet autoencoders,‚Äù IEEE/ACM transactions on audio, speech, and language processing, vol. 27, no. 12, pp. 2041‚Äì2053, 2019.
[22] A. Baevski, S. Schneider, and M. Auli, ‚ÄúVq-wav2vec: Self-supervised learning of discrete speech representations,‚Äù ArXiv preprint arXiv:1910.05453, 2019.
[23] G. E. Henter, J. Lorenzo-Trueba, X. Wang, and J. Yamagishi, ‚ÄúDeep encoder-decoder models for unsupervised learning of controllable speech synthesis,‚Äù ArXiv preprint arXiv:1807.11470, 2018.
[24] T. Kudo and J. Richardson, ‚ÄúSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,‚Äù ArXiv preprint arXiv:1808.06226, 2018.
[25] A. Vaswani, N. Shazeer, N. Parmar, et al., ‚ÄúAttention is all you need,‚Äù in NIPS, 2017, pp. 5998‚Äì6008.
[26] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, ‚ÄúHybrid CTC/attention architecture for end-toend speech recognition,‚Äù IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240‚Äì1253, 2017.
[27] S. O¬® . Arƒ±k, H. Jun, and G. Diamos, ‚ÄúFast spectrogram inversion using multi-head convolutional neural networks,‚Äù IEEE Signal Processing Letters, vol. 26, no. 1, pp. 94‚Äì98, 2018.
[28] R. Mu¬®ller, S. Kornblith, and G. E. Hinton, ‚ÄúWhen does label smoothing help?‚Äù In NIPS, 2019, pp. 4696‚Äì4705.
[29] R. Sonobe, S. Takamichi, and H. Saruwatari, ‚ÄúJSUT corpus: Free large-scale Japanese speech corpus for end-to-end speech synthesis,‚Äù ArXiv preprint arXiv:1711.00354, 2017.
[30] Open JTalk (version 1.11), http://open- jtalk. sourceforge.net, 2018.
[31] A. Lee and T. Kawahara, ‚ÄúRecent development of opensource speech recognition engine julius,‚Äù in Proc. APSIPA, 2009, pp. 131‚Äì137.
[32] T. Hayashi, R. Yamamoto, K. Inoue, et al., ‚ÄúESPnetTTS: UniÔ¨Åed, reproducible, and integratable open source end-to-end text-to-speech toolkit,‚Äù in ICASSP, 2020, pp. 7654‚Äì7658.
[33] DiscreTalk audio sample, https://kan-bayashi. github.io/DiscreTalk, 2020.
[34] L. Liu, H. Jiang, P. He, et al., ‚ÄúOn the variance of the adaptive learning rate and beyond,‚Äù ArXiv preprint arXiv:1908.03265, 2019.
[35] S. Watanabe, T. Hori, S. Karita, et al., ‚ÄúESPnet: Endto-end speech processing toolkit,‚Äù in Proc. Interspeech, 2018, pp. 2207‚Äì2211.
[36] K. Maekawa, ‚ÄúCorpus of spontaneous japanese: Its design and evaluation,‚Äù in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[37] M. SchoefÔ¨Çer, S. Bartoschek, F.-R. Sto¬®ter, et al., ‚ÄúwebMUSHRA-a comprehensive framework for webbased listening tests,‚Äù Journal of Open Research Software, vol. 6, no. 1, 2018.
[38] A. Graves, S. Ferna¬¥ndez, F. Gomez, and J. Schmidhuber, ‚ÄúConnectionist temporal classiÔ¨Åcation: Labelling unsegmented sequence data with recurrent neural networks,‚Äù in ICML, 2006, pp. 369‚Äì376.

