arXiv:1306.2558v1 [cs.AI] 11 Jun 2013

The Eﬀect of Biased Communications On Both Trusting and Suspicious Voters

William W. Cohen Carnegie Mellon University Department of Machine Learning

David P. Redlawsk Rutgers University Department of Political Science

Douglas Pierce Rutgers University Department of Political Science

November 5, 2021

Abstract
In recent studies of political decision-making, apparently anomalous behavior has been observed on the part of voters, in which negative information about a candidate strengthens, rather than weakens, a prior positive opinion about the candidate. This behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational “motivated reasoning”. We consider scenarios in which this eﬀect arises in a model of rational decision making which includes the possibility of deceptive information. In particular, we will consider a model in which there are two classes of voters, which we will call trusting voters and suspicious voters, and two types of information sources, which we will call unbiased sources and biased sources. In our model, new data about a candidate can be eﬃciently incorporated by a trusting voter, and anomalous updates are impossible; however, anomalous updates can be made by suspicious voters, if the information source mistakenly plans for an audience of trusting voters, and if the partisan goals of the information source are known by the suspicious voter to be “opposite” to his own. Our model is based on a formalism introduced by the artiﬁcial intelligence community called “multi-agent inﬂuence diagrams”, which generalize Bayesian networks to settings involving multiple agents with distinct goals.
1 Introduction
Historically, political decision-making has been modeled in a number of ways. Models that propose rational decision making on the part of voters must account for the fact that voters frequently have diﬃculty in responding to factual surveys on political issues. One resolution to this diﬃculty is to model candidate evaluation as an online learning process, in which a tally representing candidate aﬀect is incremented in response to external information [13], after which the information itself is discarded. However, in a number of recent studies of political decision-making, apparently anomolous behavior has been observed on the part of voters, in which negative information about a candidate k strengthens (rather than weakens) a prior positive opinion held about k [5, 20] .
This behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational “motivated reasoning” [11]. In motivated reasoning models, a voter will (1) evaluate the aﬀect of new informationi.e., its positive or negative emotional charge, then (2) compare this to the aﬀect predicted by current beliefs, and ﬁnally (3) react, where congruent information (i.e., information consistent with predicted aﬀect) is processed quickly and easily, and incongruent
1

Biased Communications to Trusting and Suspicious Voters

2

information is processed by a slower stop-and-think process. Stop-and-think processing may include steps such as counter-arguing, discounting the validity of the information, or bolstering existing aﬀect by recalling previously-assimilated information [14, 20].
Some evidence for the motivated reasoning hypothesis comes from hman-subject experiments using a dynamic process tracing environment (DPTE), in which data relevant to a mock election is presented as a dynamic stream of possibly relevant news items. In DPTE experiments, detailed hypotheses about political reasoning can be tested, for instance by varying the frequency and amount of incongruent information presented to voters in the mock election. Experimental evidence shows, for instance, that both political sophisticates and novices spend more time processing negative information about a liked candidate, and novices also spend longer processing positive information about a disliked candidate [20]. Most intriguingly, small to moderate amounts of incongruent information—e.g., negative information about a liked candidate—actually reinforce the prior positive view of the candidate [21].
This apparently anomalous eﬀect—whereby information has the inverse of the expected impact on a voter—appears to be inconsistent with rational decision-making. In this paper, we show analytically that this “anomalous” eﬀect can occur in a model of rational decision making which includes the possibility of deceptive information. The model makes another interesting prediction: it justiﬁes as computationally eﬀective and eﬃcient a heuristic of pretending to believe information from a possibly-deceptive source if that source’s political preferences are the same as the voters.
In particular, we will consider a model in which there are two classes of voters, which we will call trusting voters and suspicious voters, and two types of information sources, which we will call unbiased sources and biased sources. Information from an unbiased source is modeled simply as data Dk that probabilistically inform a voter about a candidate k’s positions. Trusting voters are voters that treat information about a candidate as coming from an unbiased source. We show that, in our model, new data about a candidate can be eﬃciently incorporated by a trusting voter, and anomalous updates (in which “negative” information increases support) are impossible.
Biased sources are information sources j who plan their communications with a goal in mind (namely, encouraging trusting voters to vote in a particular way). To do this, j will access some data Ck which is communicated to them only (not to voters directly) and release some possibly-modiﬁed version Bk of Ck, concealing the original Ck. Bk is chosen based on the utility to j of the probable eﬀect of Bk on a trusting voter i.
We then introduce suspicious voters. Unlike trusting voters, who behave as if communications were from unbiased sources, suspicious voters explicitly model the goal-directed behavior of biased sources.
The behavior of rational suspicious voters depends on circumstances—depending on the assumptions made, diﬀerent eﬀects are possible. If the partisan goals of the biased source j and a suspicious voter i are aligned, then a suspicious voter can safely act as if the information is correct—i.e., perform the same updates as a naive voter. Intuitively, this is because j is choosing information Bk strategically to inﬂuence a naive voter to achieve j’s partisan goals, and since i’s goals are the same, it is strategically useful for i to “play along” with the deception; this intutition can be supported rigorously in our model. If the partisan goals of j are unknown, then a rational suspicious voter i may discount or ignore the information Bk; again, this intuition can be made rigorous, if appropriate assumptions are made. Finally, if the partisan goals of j are known to be “opposite” those of i, then a rational suspicious voter may display the “anomalous” behavior discussed above: information Bk that would cause decreased support for a suspicious voter will cause increased support for i. Intuitively, this occurs because i recognizes that j may be attempting to decrease support for candidate k, and since i and j have “opposite” partisan alignments, it is rational for i to instead increase support.
In short, in this model, a negative communication about k can have the eﬀect opposite to one’s initial expectation; however, the apparent paradox is not due to motivated reasoning, but simply to imprecise planning on the part of j. In particular, j’s communication was planned by a biased source with the aim of inﬂuencing trusting voters, while in fact, i is a suspicious voter.
Below, we will ﬁrst summarize related work, and then ﬂesh out these ideas more formally. Our model will be based on a formalism introduced by the artiﬁcial intelligence community called “multiagent inﬂuence diagrams”, which generalize Bayesian networks to settings involving multiple agents with

Biased Communications to Trusting and Suspicious Voters

3

distinct goals.
2 Related work
This work is inspired for recent work on motivated reasoning and hot cognition in political contexts (for recent overviews, see [9, 19]). There is strong experimental evidence that information processing of political information involves emotion, and there recent research has sought to either collect empirical evidence for [20, 5], and and build models that explicate [12], the mechanisms behind this phenomenon.
The models of this paper are not intended to dispute role of emotion in political decision making. Indeed, our models reﬂect situations in which one party deliberately withholds or distorts information to manipulate a second party, and introspection clearly suggests that such situations will typically invoke an emotional response. However, work in social learning (e.g., [22]) and information cascades (e.g., [23]) shows that behaviors (such as “following the herd”) which appear to be driven by non-rational emotions may in fact be strategies that are lead to results that are evolutionarily desirable (if not always “rational” from the individual’s perspective.) Hence, the identiﬁcation of emotional aspects to decision-making does not preclude rational-agent explanations; rather, it raises the question of why these mechanisms exist, what evolutionary pressures might cause them to arise, and whether or not those pressures are still relevant in modern settings. This paper makes a step toward these long-term goals by identifying cases in which behavior explainable by motivated reasoning models is also rational, for instance in the result of Theorem 2.
The explanation suggested here for anomalous, motivated-reasoning-like updating is based on a voter recognizing that a source may be biased, and correcting for that bias. While this explanation of anomalous updates is (to our knowledge) novel, it is certainly recognized that trust in the source of information is essential in political persuasion, and that a voter’s social connextions strongly inﬂuences political decisionmaking (e.g., [1]). More generally, empirical studies of persuasion substantiate a role of conﬁrmatory bias and prior beliefs [7], and show that in non-political contexts (e.g., in investing), sophisticated consumers of information adjust for credibility, while inexperienced agents under-adjust.
The results of this paper are also related to models of media choice—for instance, research in which the implications of a presumed tendency of voters to seek conﬁrmatory news is explored mathematically [4, 2, 8, 25]. Other analyses show why preferences for unbiased news lead to economic incentives to distort the news [3]. This paper does not address these issues, but does contribute by providing a rational-agent model for why such a conﬁrmatory bias exists: in particular, Proposition 3 describes a strategy from using information from biased information sources with similar preferences as a voter. We notice that this strategy is both simple and computationally inexpensive, and might be preferable on these grounds to more complex strategies to “de-noise” biased information from sources with unknown preferences.
A further connection is to formal work on “talk games”, such as Crawford and Sobel’s model of strategic communication [6]. In this model, a well-informed but possibly deceptive “sender” sends a message to a “receiver” who (like our voter) takes an action that aﬀects both herself and the sender. Variants of this model have explored cases in which information can only be withheld or disclosed, and disclosed information may be veriﬁed by the receiver [15]; cases where the receiver uses approximate “coarse” reasoning [16]; and cases where there is a mixed population of strategic and naive recievers, all of whom obtain information from senders acting strategically [17].
The analysis goals in “talk games” is diﬀerent from the goals of this paper. whereas we investigate whether speciﬁc counter-intuitive observed behavior can arise in a plausible (not necessarily temporally stable) situation, this prior work primarily nalyzes the communication eﬃciency of a a system in equilibrium, Some of the results obtained for talk games are reminiscent of results shown here: for instance, Crawford and Sobel show that in equilibrium, signaling is more informative when the sender and reciever’s preferences are more similar. However, other results are less intuitive: for instance, in some models there is no deception at equilibrium [17]. We note that while equilibria are convenient to analyze, there is no particular reason to believe that natural political discourse reaches an equilibria.
Game theory has a long history in analyses of politics; in particular, writing in 1973, Shubik discusses possible applications of game theory to analysis of misinformation [24]. The tools used used in this

Biased Communications to Trusting and Suspicious Voters

4

Draft—to check: subscripts for C, B, R

i

a voter

j

a pundit

k

a candidate

Ti, Tj

“target positions” for voter i and source j

Tk

the position of a candidate k

dom(T )

the set of values taken on by random variable T

Sik, Sjk

similarity of a target position and a candidate

Dk

data about candidate k

Yik

the vote of i for k

Ui

utility function for i

Cjk

data about k that is known only to j

Bjk

biased variant of data Cjk about k that has been modiﬁed by j

Rjk

reputational cost to pundit j of modifying Cjk to Bjk

P TrV(X|Y ) a conditional probability computed using the trusting voter model

P BiP(X|Y ) a conditional probability computed using the biased pundit model

P SuV(X|Y ) a conditional probability computed using the suspicious voter model

Table 1: Notation used in the Paper

paper arose from more recent work in artiﬁcial intelligence [10, 18], speciﬁcally analysis of multi-agent problem solving tasks, in which one agent explicitly models the goals and knowledge of another in settings involving probabilistic knowledge. One small contribution of this paper is introduction of a new set of mathematical techniques, which (to our knowledge) have not been previously used for analysis of political decision-making. We note however that while these tools are convenient, they are not absolutely necessary to obtain our results.
3 Modeling trusting voters
3.1 A model
Consider the inﬂuence diagram on the left-hand side of Figure 1. In this model i is a voter, and k is a candidate. A voter i has a preference Ti: for example, Ti might be a member of the set
dom(T ) = {goodLiberal, evilLiberal, goodConservative, evilConservative}
Likewise Tk is candidate k’s actual position, which is also an element of dom(T ). Sik measures how similar two positions are. Yik is a measure of i’s support for k, which is chosen by voter i to maximize i’s utility. The utility Ui for voter i is a function of how appropriate Yik is given Sik. Finally, Dk is some data about k, generated probabilistically according to the value of Tk. As an example, this might be a statement made by k. The notation used in this diagram (and elsewhere below) is summarized in Table 1.
The shapes in the nodes in the diagrams indicate the type of the variable: diamonds for utilities, circles for random variables, and squares for decision variable, which an agent (in this case, voter i) will choose in order to maximize utility. The dotted arrow lines leading into a decision variable indicates information available when the decision is made. The arrows leading into a random variable node indicate “parents”—variables on which the value of the variable is conditioned. This sort of diagram is called an inﬂuence diagram, and the general version we will use later (in which multiple agents may exist) is called a MAID (Multi-Agent Inﬂuence Diagram) [18, 10].
More precisely the model deﬁnes a probability distribution generated by this process:

Biased Communications to Trusting and Suspicious Voters

5

Figure 1: Model of a trusting voter, in MAID and Bayes Net notation

• Pick ti ∼ P (Ti), where P (Ti) is a prior on voter preferences.
• Pick tk ∼ P (Tk), where P (Tk) is a prior on candidate positions.
• Pick dk ∼ P (Dk|Tk = tk). Equivalently, we could let dk = fD(tk, D), where fD is a function and D is a random variable chosen independently of all other random variables in the model.
• Pick sik ∼ P (Skij |Tk = tk, Tj = tj ). Equivalently, we could let sik = fS(ti, tk, S), where fS is a function and S is a random variable, again chosen independently of all other random variables in the model.
• Allow voter i to pick yik, based on a user-chosen probability distribution Pτ (Yik|Sik = sik), or equivalently computed using fY (sik, Y ).
• Pick utility ui from P (U |yik, sik)—or equivalently, computed as ui = fU (yik, sik, U ).
The user can choose any distribution Pτ (Y |S), but we will henceforth assume that she will make the optimal choice—i.e., the probability distribution Pτ will be chosen by i to maximize the expected utility ui, where ui will be picked from P (U |yik, Sik)P r(Sik)—or equivalently computed as ui = s P (Sik = s ) ∗ fU (yik, s , U ). In this speciﬁc case, the conversion is based on the observation that

P TrV(Yik = y|Sik = s) = P y = argmaxy

fu(y , s, U )d U

(1)

U

and yields the Bayes network on the righthand side of Figure 1. Here Yik is simply a random variable conditioned on Sik, where the form of the dependency depends on Equation 1. Notice that the link from Sik to Yik is deterministic. We call this the trusting voter model, since voter i trusts the validity of the information Dk.
MAIDs (and their single-agent variants, inﬂuence diagram networks) have a number of advantages as a formalism. They provide a compact and natural computational representation for situations which are otherwise complex to describe - in particular, situations in which agents have limited knowledge

Biased Communications to Trusting and Suspicious Voters

6

d goodLiberal goodConserv evilLiberal evilConserv

P (Ti = d) 0.4 0.4 0.1 0.1

d goodLiberal goodConserv evilLiberal evilConserv

P (Tk = d) 0.29 0.69 0.01 0.01

ti goodLiberal goodLiberal goodLiberal goodLiberal goodConserv goodConserv goodConserv evilLiberal evilLiberal evilConserv

tk goodLiberal goodConserv evilLiberal evilConserv goodConserv evilLiberal evilConserv evilLiberal evilConserv evilConserv

sik|ti, tk 5+ 1+
−2 + −5 +
5+ −5 + −2 +
5+ −5 +
5+

tk goodLiberal goodLiberal goodConserv goodConserv evilLiberal evilLiberal evilConserv evilConserv

ck safety-net motherhood guns motherhood safety-net chthulu guns chthulu

P (ck|tk) 0.4 0.6 0.3 0.7 0.9 0.1 0.8 0.2

P (Yi = 1|Sik = s): see Eq 1

Table 2: Conditional probability tables (CPT) for a sample example of the trusting voter model. In the table for sik|ti, tk, is drawn unformly from the set {−1, 0, +1} and for pairs ti, tk not shown, sik|ti, tk = sik|tk, ti.

of the game structure, or mutually inconsistent beliefs, but act rationally in accordance with these beliefs. In particular, MAIDs relax the assumption usually made in Bayesian games that players’ beliefs are consistent, and supports an explicit process in which one player can model another player’s strategy. MAIDs also support an expressive structured representation for a player’s beliefs. Together these features make them appropriate for modeling “bounded rationality” situations of the sort we consider here. Further discussion of MAIDs, and their formal relation to other formalisms for games and probability distributiobns, can be found elsewhere [10, 18].
Next, we will explore some simpliﬁcations of Eq. 1) If fu is deterministic, then Eq. 1 simpliﬁes to the following choice of yik given sik:
yik = fyTrV(sik) = argmaxy fu(y , sik)
If only a distribution P (Sik = s) is known, then voter i’s optimal strategy is to let
yik = argmaxy fu(y , s)P (Sik = s)
s
In any case, however, the model has similar properties: once we assume that i uses an optimal strategy, then the MAID becomes an ordinary Bayes net, deﬁning a joint distribution over the variables Ti, Tk, Sik, and Yik. We will henceforth use P TrV to denote probabilities computed in this model, and reserve the non-superscripted P (A|B) and P (A) for conditional (respectively prior) probability distribution that are assumed to be available as background information.
Example 1 To take a concrete example, igure 2 shows the conditional probability tables for a small example, where candidates and target positions have values like evilLiberal and goodConserv, and the value of a communication Ck is a name of something that a candidates might support (e.g.,motherhood or guns).
3.2 Implications of the model
3.2.1 New information about a candidate is easy to process
It is obvious how to use the trusting voter model to compute Yik if Tk and Ti are known. However, a more reasonable situation is that Dk and Ti are known to i, but the true position of the candidate can only be inferred, indirectly, from Dk. Fortunately, using standard Bayes network computations, we can also easily compute a distribution over Yik given Dk.
First, note that we can marginalize over Tk and compute
P (Dk = d) = P (Dk = d|t )P (Tk = t )
t

Biased Communications to Trusting and Suspicious Voters

7

Figure 2: Model of a trusting voter with m multiple independent data items about the candidate, in Bayes Net notation

and that using Bayes’ rule

P (Tk = t|Dk = d) = P (Dk = d|Tk = t)P (Tk = t) = P (Dk = d|Tk = t) P (Tk = t) (2)

P (Dk = d)

P (Dk = d)

This gives a simple rule for i to use in choosing her vote Yik given Dk:

P TrV(Y = y|Ti = ti, Dk = dk)

(3)

=

P TrV(Y = y|Sik = s) P (Sik = s|Ti = ti, Tk = t)P (Tk = t|Dk = dk)

(4)

s

t

= P TrV(Y = y|Sik)P (Sik|ti, Tk)P (Tk|dk)

The last line uses a simpliﬁed notation from the Bayes net community, where sums used to marginalize are omitted, and the event X = x is replaced with x when the variable X is clear from context.
This procedure can be extended easily to the case of multiple data items Dk,1, . . . , Dk,m about the candidate, each independently generated from P (Dk|Tk), as shown in Figure 2. It can be shown that

P (Tk = t|dk,1, . . . , dk,m) =

P (dk, |Tk = t) P (Tk = t) P (dk, )

Put another way, we can deﬁne P (Tk = t|dk,1, . . . , dk,m) recursively as follows:
P (dk,m|Tk = t) P (Tk = t|dk,1, . . . , dk,m) = P TrV(dk,m) · P (Tk = t|dk,1, . . . , dk,m−1)
Hence, voter i can quickly update beliefs about Tk incrementally with each new piece of information dk, and then (as before) use Equation 3 to update her vote.

Biased Communications to Trusting and Suspicious Voters

8

This incremental update property is worth emphasizing—while it may be complicated (if not computationally complex) to compute P TrV(Y |S), or it may be diﬃcult for a voter to establish her preferences
ti, absorbing new information in the trusting voter model is straightforward, and consists of two “natural” steps: estimating the candidate’s position Tk given the information dk, ; and then updating her support yik for candidate k, given the updated estimate of the candidate’s position.

3.2.2 Positive information increases support
In a number of recent studies of political decision-making, negative information about a candidate k has been observed to strengthen (rather than weaken) a voter’s support for k. We will show that under fairly reasonable assumptions this eﬀect can not occur with the trusting voter model. In particular, we will assume that support Yik increases monotonically with the similarity Sik of the candidate’s position Tk and the voter’s preference Ti.
Recall that in the trusting voter model Yik is a deterministic function of Sik, deﬁned as

fY TrV(sik) = argmaxy

fu(y , s, U )d U

(5)

U

If fY TrV has the property that

∀s1 > s2, fY TrV(s1) ≥ fyTrV(s2)

then we will say that i’s support for k increases monotonically with sik. This is one assumption needed for our result.
We also need to precisely deﬁne “negative” information. We say that dk is strictly negative about k to voter i if there is some partition of dom(Sik) into triples (a1, b1, δ1), . . . ,(am, bm, δm) so that

• For every triple (a , b , δ ), a < b , δ > 0, P (Sik = a |dk) = P (Sik = a ) + δ , and P (Sik = b |dk) = P (Sik = b ) − δ . In other words, learning Dk = dk shifts some positive probability mass δ from the larger similarity value b to the smaller similarity value a .

• For all s ∈ dom(Sik) that are not in any triple, P (sik = s|dk) = P (sik = s). In other words, the probability mass of values s not in any triples is unchanged.

To illustrate this, consider Figure 3, which illustrates a plausible example of strictly negative information. Strictly positive information is deﬁned analogously.
We can now state formally the claim that negative information will reduce support. An analogous statement holds for strictly positive information.

Theorem 1 Let EP [X] denote the expected value of X in probability distribution P . In a trusted voter model P TrV, if voter i’s support for k increases monotonically with sik and dk is strictly negative about
k to voter i, then EP TrV [Yik|Dk = dk] < EP TrV [Yik].

Proof. Let S = dom(Sik) − {a1, b1, . . . , am, bm}, where the a , b ’s are the triples guaranteed by the strictly-negative property of dk.

EP TrV [Yik|Dk = dk]

=

y · P TrV(Yik = y|Dk = dk)

y

=

fyTrV(s)P TrV(Sik = s|Dk = dk)

s

m

m

=

fyTrV(s)P TrV(Sik = s|dk) + fyTrV(a )P TrV(Sik = a |dk) + fyTrV(b )P TrV(Sik = b |dk)

s ∈S

=1

=1

Looking at the three terms of the ﬁnal sum in turn, clearly

fyTrV(s)P TrV(Sik = s|dk) =

fyTrV(s)P TrV(Sik = s)

s ∈S

s ∈S

Biased Communications to Trusting and Suspicious Voters

9

Figure 3: An illustration of strictly negative information for a voter i

and the last two terms can be written as

m
fyTrV(a )P TrV(Sik = a |dk) + fyTrV(b )P TrV(Sik = b |dk)

=1

m

=

fyTrV(a ) P TrV(Sik = a ) + δ + fyTrV(b ) P TrV(Sik = b ) − δ

=1

m

=

fyTrV(a )P TrV(Sik = a ) + fyTrV(b )P TrV(Sik = b ) + δ (fyTrV(a ) − fyTrV(b ))

=1

m

≤

fyTrV(a )P TrV(Sik = a ) + fyTrV(b )P TrV(Sik = b )

=1

with the last step holding because (a) δ > 0 and (b) a < b . (Recall that from the monotonicity of fy, if a < b then fyTrV(a ) ≤ fyTrV(b )). Combining these gives that

EP TrV [Yik|Dk = dk]

m

m

=

fyTrV(s)P TrV(Sik = s|dk) + fyTrV(a )P TrV(Sik = a |dk) + fyTrV(b )P TrV(Sik = b |dk)

s ∈S

=1

=1

m

≤

fyTrV(s)P TrV(Sik = s) + fyTrV(a )P TrV(Sik = a ) + fyTrV(b )P TrV(Sik = b )

s ∈S

=1

=

fyTrV(s)P TrV(Sik = s) = EP TrV [Yik]

s∈dom(Sik )

This concludes the proof.

Biased Communications to Trusting and Suspicious Voters

10

Figure 4: Model of a biased pundit, in MAID and Bayes net notation
4 Modeling biased pundits and suspicious voters
4.1 Biased pundits
We now consider a new model, as shown in Fig 4. In this model there is a pundit j, who, like a voter, has a target candidate position Tj. Pundit j observes a private datapoint Ck from candidate k—perhaps based on a private communication or research—and then publishes a “biased version” Bk of Ck. However, Bk is chosen under the assumption that some trusting voter i will react to Bk as if it were Dk in the trusting voter model of Figure 1. Speciﬁcally, we assume that Bk will provoke a vote Yik according to the trusting voter model. The utility assigned by j to this outcome is a function of i’s vote Yik, the similarity of Sjk of Tk to j’s target Tj, and a “reputational cost” Rjk, which is a function of Ck and Bk. For instance, Rjk might be zero if bk = ck, and otherwise some measure of how embarrassing it might be to j if his deception of replacing ck with bk were discovered.
More precisely the model deﬁnes a probability distribution generated by this process:
• Pick tj ∼ P (Tj), where P (Tj) is a prior on pundit preferences. • Pick tk ∼ P (Tk), where P (Tk) is a prior on candidate positions. • Pick ck ∼ P (Dk|Tk = tk), or equivalently, ck = fD(tk, D). (Notice that we assume ck is chosen from
the same conditional distribution P (Dk|Tk) used in the trusting voter model to chose Dk—we’re using a diﬀerent variable here to emphasize the diﬀerent role it will play.)
• Allow pundit j to pick bk, based some user-chosen distribution Pσ(Bk|Ck = ck). • Pick rik ∼ P (Rjk|Bk = bk, Ck = ck), or equivalently, rjk = fR(bk, ck, R). • Show bk to a trusting voter i, presenting it as a sample from P (Dk|Tk = tk), and allow user i to
pick yi according to the trusting voter model. • Pick ui ∼ P (Uj |Rjk = rjk, Sjk = sjk, Yik = yik), or equivalently, let uj = fU (rjk, sjk, yjk, U ).

Biased Communications to Trusting and Suspicious Voters

11

To distinguish the two utility functions, we will henceforth use fU TrV for the utility function fU (s, y) used in the trusting voter model, and use fU BiP for the utility function fU (r, s, y) deﬁned above. As
below, we will assume pundit j will pick bk, based on available estimates of Sij and knowledge of Ck, to
maximize the expected utility uj. This can be computed as

uj = P TrV(Yik = y|Dk = bk)P (Sjk = s|Dk = ck)P (Rjk = r|bk = ck)fU BiP(r, s, y, U )
r,s,y

where P TrV(Yik|Dk = bk) is estimated using the trusting voter model; P (Sjk|Dk = ck) is computed as in Section 3.2.1, using P (DCk|Tk) and P (Sjk|Tj, Tk); and P (Rjk = r|bk, ck) is computed using the given probability function of reputational cost r, as a function of the unbiased data ck and the biased version bk that is released.
Since pundit j picks Bk to maximize utility, then as before, we can convert this MAID to a Bayes net. Speciﬁcally, Bk depends on the parents Ck and Sjk as follows.
P BiP(Bk = b|Ck = c, Sjk = s) =

P b = argmaxb P TrV(Yik = y|Dk = bk)P (Rjk = r|b , ck) fU BiP(r, s, y, U )d U

r,y

U

This can be simpliﬁed, if we assume that fU BiP and fR are deterministic:

P BiP(Bk = b|Ck = c, Sjk = s) = P

b = argmaxb

P TrV(Yik = y|Dk = bk)fU BiP(fR(b , c), s, y)
y
(6)

4.2 Suspicious voters
Finally, we introduce a model for a “suspicious voter”. Intuitively, this model is simple. As before, we assume that j chooses bk according to the biased pundit model of Eq. 6—i.e., that j believes i to be a trusting voter. The suspicious voter will then attempt to reason with this correctly, and ﬁnd the vote Yi maximizing i’s true utility, given then information revealed by j’s choice of bk. The MAID and Bayes net versions of this model are shown in Figure 5 and Figure 6 respectively, and probabilities computed in this model will be writted as P SuV.
This suspicious voter model does not have a simple closed-form solution for P SuV(S), as in the trusting voter model. The generative process for the suspicious voter model is identical to the biased pundit model, except that after bk is chosen according to Eq. 6, i will pick a value of Yik from a distribution P SuV(Yik|Bk = bk), which is chosen to maximize the expected value of ui. We will deﬁne

P SuV(Yik = y|Sik = s) = P y = argmaxy

fu(y , s, U )d U
U

or, assuming determinacy,

yik = fySuV(sik) = argmaxy fu(y , sik)

This leads to a more complex inference problem for voters. Although the process of computing P (Sik|Ti, Tk) is unchanged, relative to the trusting voter model, a suspicious voter cannot estimate a distribution over Tk using dk, as in Eq. 2, because dk is not known. Instead i only has indirect evidence about dk in the form of bk.
However, voter i can use this indirect evidence to compute

P SuV(Tk|Bk = bk) = P TrV(Tk|Dk = c) · P BiP(Bk = b|Ck = c) · P TrV(Dk = c)

(7)

c

Biased Communications to Trusting and Suspicious Voters

12

Figure 5: Model of a suspicious voter in MAID notation Figure 6: Model of a suspicious voter in Bayes net notation

Biased Communications to Trusting and Suspicious Voters

13

where P BiP(b|c) = tj P BiP(b|c, tj)P (tj) is the probability, in the biased pundit model, of pundit j publishing Bk = b when Ck = c. We can now break the summation over c into two cases:

P SuV(Tk|Bk = bk)

= P TrV(Tk|Dk = b) · P TrV(Dk = b) · P BiP(Bk = b|Ck = b)

(8)

+

P TrV(Tk|Dk = c) · P TrV(Dk = c) · P BiP(Bk = b|Ck = c)

(9)

c=b

In the term in line 8, j is not altering the original input ck, so we say he is being accurate. In line 9, so we say that j is being deceptive. In order to exploit the information in bk using Eq 8-9 to optimize her utility, the suspicious voter must assess the probability of deception, and adjust inferences about candidate k accordingly—a potentially diﬃcult inference problem.

4.3 Implications of the suspicious voter model

To simplify further analysis, we will make some additional assumptions.

• We assume a deterministic reputational cost function fR in the biased pundit model.

• We assume the reputational cost of being accurate is zero, i.e., that ∀c, fR(c, c) = 0, and that the reputational cost of being deceptive is greater than zero, i.e., ∀b = c, fR(b, c) > 0.
• We assume deterministic utility functions fU TrV and fU BiP.

• We assume the utility for pundits is the same as the utility for voters, minus the reputational cost

of altering c to b—i.e., that

fU BiP(r, s, y) ≡ fU TrV(s, y) − r

Given these assumptions, some observations can now be made.

Proposition 1 If the prior P (Tj) such that P BiP(b|c) = P BiP(b |c ) for all communications b, b , c and c , then for all bk, P SuV(Tk|bk) = P SuV(Tk) = P TrV(Tk).
In other words, if P BiP(b|c) is constant, then a biased pundit’s publications bk convey no information to i. This can be seen immediately by inspection of Eq. 7. Notice that requiring that P BiP(b|c) does not
imply that any individual pundits simply publish information b uniformly at random, without regard to c—instead, it says that if one averages over all pundits and considers tj P BiP(b|c, tj)P (tj), then the cumulative probability of seeing any particular b is constant, and independent of c.
More generally, one can make this observation.

Proposition 2 If the prior P (Tj) such that (1) P BiP(c|c) = α for all c, and (2) P BiP(b|c) = P BiP(b |c) for all communications b = c and b = c , then for all bk,

P SuV(Tk|bk) = αP TrV(Tk|bk) + (1 − α)P TrV(Tk)

In other words, if all deceptions are equally likely, but publications are accurate with ﬁxed probability α, then a suspicious voter’s update to Tk is simply a mixture of her prior belief P SuV(Tk) = P TrV(Tk) and the belief a trusting voter would have, P TrV(Tk|bk), with mixing coeﬃcient α. Again, this proposition can be be veriﬁed immediately by inspection of lines 8-9.
A ﬁnal observation is that when a biased pundit’s preference tj is the same as a voter’s preference ti, and this is known to both i and j, then even a suspicious voter will obtain high utility by simply believing j’s publication bk. In particular i’s utility from adopting the belief that D = bk is just as high as if i had observed ck itself.
Proposition 3 If tj = ti, and bk is a publication from j under the biased pundit model, then the expected utility to i of voting according to P TrV(Tk|Dk = bk) is at least as large as the expected utility to i of voting according to P TrV(Tk|Dk = ck).

Biased Communications to Trusting and Suspicious Voters

14

This proposition seems plausible if we recall that bk was chosen to maximize the utility to j of i’s belief in bk in the trusting voter model—thus, since the utility to i is the same as the utility to j, it seems reasonable that adopting this belief is also useful to i. To establish it more formally, let us deﬁne EU (b|c, ti) to be the expected utility to agent (either i or j), absent reputational costs, of having i adopt the belief in the trusting-voter model that Dk = b when in fact Dk = c, if Ti = ti. In other words, we deﬁne

EU (b|c, ti) ≡

P TrV(Tk = tk|Dk = c)P (S k = s k|t , tk)P TrV(Yik = yik|Ti = ti, Dk = b)f TrVu(s k, y)

s k ,tk ,yik

Note that the weighting in the factors P TrV(Tk = tk|Dk = c)P TrV(S k = s k|t , tk) holds for both i and j, because here we care about the true distribution over Tk, as deduced from c. The weighting in the factor P TrV(Yik = y|Ti = ti, Dk = b) arises because for both i and j, utility is based on i’s estimated support yik for k given i’s known preference ti.
If we assume that ti = tj = t, then this simpliﬁes to

EUi(b|c, ti) = EUj(b|c, ti) =

P TrV(Tk = tk|Dk = c)P TrV(S = s|t, tk)P TrV(Yik = yik|Ti = t, Dk = b)fu(s, y)

s,tk ,yik

and hence we see that in this case, the functions for i and j are indeed the same. Since j has chosen b to maximize EU (b|c) − fR(b, c), and fR is never negative, clearly EUj(b|c, ti) ≥ EUj(b|c, ti), and so EUi(b|c, ti) ≥ EUi(b|c, ti) as well.
As noted in Section 2, there are a number of papers analyzing media bias in which voters are assumed prefer “good news” (i.e., new biased towards their favored candidates) leading to fragmentation and specialization as media companies diﬀerentiate by providing news biased for their readers. The results above suggest a rational reason for picking a news source j with the same partisan preferences as one’s self: in particular, this sort of news is computionally easier to process. Similarly, biased sources with unknown preferences are “less informative”, in the sense that new information leads to changes in support (relative to unbiased sources, or well-aligned partisan sources).

4.4 Suspicious voters and “irrationality”
Finally, we address the question of whether suspicous voters can behave in the counter-intuitive manner discussed in the introduction—whether information about the candidate k that is negative (as interpreted by a trusting voter) can increase support for a suspicous voter. We will show that this is possible.
Theorem 2 It can be the case that information b will decrease i’s support for k in the trusted voter model, and increase i’s support for k in the suspicous voter model: i.e., it may be that EP TrV [Yik|Dk = b] < EP TrV [Yik] but EP SuV [Yik|Dk = b] > EP SuV [Yik].
Intuitively, this happens when i believes strongly that j is being deceptive, and i has diﬀerent candidate preferences from j. The theorem asserts the existence of such behavior, so we are at liberty to make additional assumptions in the proof (preferably, ones that could be imagined to hold in reality).
In the proof, we assume that j’s preferences Tj are known to i. This is plausible since context may indicate, for instance, that j is a strong conservative. This does not aﬀect the basic model, since we allow the case of an arbitrary prior on Tj.
We also assume that all information about candidates is either strictly positive for i and strictly negative for j, or else strictly negative for i and strictly positive for j. To see how this is possible, ﬁrst imagine a hypercube-like space of candidate positions, as in Figure 3, and assume that Ti and Tj are on opposite corners of the cube. The cube might indicate, for example, positions on the environment, abortion, and increased military spending, with i preferring the liberal position on all three and j preferring the conservative positions. Then assume that all information indicates the probability of the candidate’s position along these three axis; in this case the assumption is satisﬁed.
Given these two assumptions, the statement of the theorem holds. First, we need a slightly stronger version of Theorem 1. Informally, this states that if i has partial knowledge of b, but does know that b is strictly negative for i, then i’s support for k will be weakened.

Biased Communications to Trusting and Suspicious Voters

15

Corollary 1 Suppose H is a probability distribution over items of information b, and also b is strictly negative for i for every b with non-zero probability in H. Let P TrV(Yik|H) denote
P TrV(Yik|Dk = b)PH (b).
b
Then EP TrV [Yik|H] < EP TrV [Yik].
Proof. For any item s ∈ dom(Sik), it is clear that
P (Y |E) = P (Y |b)PH (b)
b
and also, by marginalization over the (unrelated) variable H, we see that P (Y ) = b P (Y )PH (b). Since for all b with non-zero probability under H we have that E[Y |b] < E[Y ], the result holds. This concludes the proof.
We can now prove Theorem 2. Proof. Suppose information b that is strictly negative for i is observed by i, and consider again the formula for P SuV(Tk|Bk = bk) given on lines 8 and 9. This shows that the suspicous voter will reason by cases. In one case, j is being accurate, and the change in probability for Tk is in the same direction as in the trusted voter model, and as noted above, this will lead to a belief update that decreases support for k. However, this change is down-weighted by the factor P BiP(Bk = b|Ck = b)P TrV(Dk = b), which can be interpreted as the probability that b was really observed times the probability that j chooses to report accurately. We will assume that P TrV(Dk = b) is very small, so that
P SuV(Tk|Bk = bk) ≈ P TrV(Tk|Dk = c) · P TrV(Dk = c) · P BiP(Bk = b|Ck = c)
c=b
In this case, j is being deceptive. Reasoning is this case is complicated by the fact that i must consider all inputs c that could have
observed, and compute the product of P TrV(Dk = c), the prior probability of c, and also P BiP(Bk = b|Ck = c), the probability of b being reported in place of c by j. However, since the preferences of i and j are opposite, the latter is quite informative: in particular, since b was deceptively chosen by j to be negative for i, then b must prefer that i give weaker support for k, implying that c is actually negative for j, and hence positive for i. This holds for every c that could have let to the deceptive report b. By Corollary 1, the net change in support for i in the suspicous voter model positive. This concludes the proof.
5 Concluding Remarks
To summarize, we propose a model in which there are two classes of voters, trusting voters and suspicious voters, and two types of information sources, unbiased sources and biased sources. Information from an unbiased source is modeled simply as observations Dk that probabilistically inform a voter about a candidate k’s positions, and trusting voters are voters that treat information about a candidate as coming from an unbiased source. We show that reasoning about new information is computationally easy for trusting voters, and that trusting voters behave intuitively: in particular, negative information about candidate k (according to a particular deﬁnition) will decrease support for k, and positive information will increase support.
Biased sources are information sources j who plan their communications in order to encourage trusting voters to vote in a particular way). To do this, they report some possibly-modiﬁed version Bk of a private observation Ck, concealing the original Ck. In the model, Bk is chosen based on the utility to j of the probable eﬀect of Bj on a trusting voter i.
Finally, suspicious voters model the behavior of biased sources. In general this is complicated to do, however, some special cases lead to simple inference algorithms. For instance, under one set of

Biased Communications to Trusting and Suspicious Voters

16

assumptions, all information from biased sources can be ignored. In another set of assumptions, a suspicious voter will make the same sort of updates to her beliefs as a trusting voter, but simply make them less aggressively, discounting the information by a factor related to the probability of deception.
Another interesting tractible reasoning case for suspicious voters is when the biased information source j has the same latent candidate preferences as the voter i. In this case, suspicious voters can act the same way a trusting voter would—even if the information acted on is false, it is intended to achieve a result that is desirable to i (as well as j).
These results are of some interest in light of the frequently-observed preference for partisan voters to collect information from similarly partisan information channels. The results suggest a possibly explanation for this, in terms of information content. The optimal way to process information from a possibly-deceptive unknown source, or a source with a known-to-be-diﬀerent partisan alignment, is to either discount it, ignore it, or else employ complicated (and likely computationally complex) reasoning schemes. However, reports from partisan source with the same preferences as a voter can be acted on as if they were trusted—even if the reports are actually deceptive.
Finally, we show rigorously that a suspicious voter can, in some circumstances, increase support for a candidate k after receiving negative information about k. Speciﬁcally, information that would decrease support for a trusting voter might increase support for a suspicious voter—if she believes the source has diﬀerent candidate preferences, and if she believes the source is being deceptive. This behavior mimics behavior attributed elsewhere to “motivated reasoning”, but does not arise from “irrationality”—instead it is a result of the voters correct identiﬁcation of, and compensation for, an ineﬀective attempt at manipulation on the half the information source.
We should note that this hypothesis does not suggest that emotion is not present in such situations— in fact, it seems likely that reports viewed as deceptive would indeed provoke strong emotional responses. It does suggest that some of the emotion associated with these counterintuitive updates may be associated with mechanisms that have an evolutionary social purpose, rather than being a result of some imperfect adaption of humans to modern life.
It seems plausible that additional eﬀects can be predicted from the suspicious-voter model. For instance, although we have not made this conjecture rigorous, if a biased source j does not know i’s political preferences, then deceptive messages b will likely tend to be messages that would be interpreted as negative by most voters. For instance, j might assert that the candidate violates some cultural norm, or holds an extremely unpopular political views. (Or , on the other hand, assert the candidate has a property that almost all voters agree is “good”. Arguably, most information from deceptive partisan sources would be of this sort, rather than discussion of stands on widely-disagreed-on issues (like gay marriage or abortion in the US).
As they stand, however, the results do suggest a number of speciﬁc predictions about how information might be processed in a social setting. First, information provided by persons believed to have political alignments similar to voter i will be more easily assimilated, and have more eﬀect on the view of i, than information provided by persons believed to have diﬀerent political alignments. Second, information provided by persons with political alignments similar to voter i will be more assimilated in roughly the same speed (and with the same impact) as information from a believed-to-be-neutral source. Third, information provided by persons with political alignments diﬀerent from voter i may lead to counterintuitive updates, while information from similarly-aligned sources or neutral sources will not. An important topic for future work would be testing these predictions, for instance using the DPTE methodology.
References
[1] P.A. Beck, R.J. Dalton, S. Greene, and R. Huckfeldt. The social calculus of voting: Interpersonal, media, and organizational inﬂuences on presidential choices. American Political Science Review, 96(01):5773, 2002.
[2] D. Bernhardt, S. Krasa, and M. Polborn. Political polarization and the electoral eﬀects of media bias. Journal of Public Economics, 92(5-6):10921104, 2008.

Biased Communications to Trusting and Suspicious Voters

17

[3] Jeremy Burke. Unfairly balanced: Unbiased news coverage and information loss. In Annual Meeting of the American Political Science Association, Chicago, IL, 2007.
[4] M. Burke, E. Joyce, T. Kim, V. Anand, and R. Kraut. Introductions and requests: Rhetorical strategies that elicit online community response. In Proceedings of the The third communities and technologies conference, New York, NY, page 2140, 2007.
[5] Andrew J. W. Civettini and David P. Redlawsk. Voters, emotions, and memory. Political Psychology, 30(1):125–151, 2009.
[6] V. P Crawford and J. Sobel. Strategic information transmission. Econometrica: Journal of the Econometric Society, page 14311451, 1982.
[7] S. DellaVigna and M. Gentzkow. Persuasion: Empirical evidence. Annual Review of Economics, 2:643–670, 2010.
[8] J. Duggan and C. Martinelli. A spatial theory of media slant and voter choice. The Review of Economic Studies, 78(2):640, 2011.
[9] D.A. Graber and J.M. Smith. Political communication faces the 21st century. Journal of Communication, 55(3):479, 2005.
[10] D. Koller and B. Milch. Multi-agent inﬂuence diagrams for representing and solving games. Games and Economic Behavior, 45(1):181221, 2003.
[11] Z Kunda. The case for motivated reasoning. Psychological Bulletin, 108(3):480–498, 1990.
[12] M. Lodge, C. Taber, and C. Weber. First steps toward a Dual-Process accessibility model of political beliefs, attitudes, and behavior. Feeling politics: Emotion in political information processing, page 1130, 2006.
[13] Milton Lodge, Kathleen M. McGraw, and Patrick Stroh. An impression-driven model of candidate evaluation. The American Political Science Review, 83(2):pp. 399–419, 1989.
[14] Milton Lodge and Charles Taber. Three Steps toward a Theory of Motivated Political Reasoning, pages 183–213. Cambridge University Press, 2000.
[15] P. Milgrom and J. Roberts. Relying on the information of interested parties. The RAND Journal of Economics, page 1832, 1986.
[16] S. Mullainathan, J. Schwartzstein, and A. Shleifer. Coarse thinking and persuasion. The Quarterly Journal of Economics, 123(2):577, 2008.
[17] M. Ottaviani and F. Squintani. Naive audience and communication bias. International Journal of Game Theory, 35(1):129150, 2006.
[18] A. Pfeﬀer. Networks of inﬂuence diagrams: A formalism for representing agents beliefs and decisionmaking processes. Journal of Artiﬁcial Intelligence Research, 33:109147, 2008.
[19] D. P Redlawsk. Feeling politics: Emotion in political information processing. Palgrave Macmillan, 2006.
[20] David P. Redlawsk. Hot cognition or cool consideration? testing the eﬀects of motivated reasoning on political decision making. The Journal of Politics, 64(04):1021–1044, 2002.
[21] David P. Redlawsk, Andrew J. W. Civettini, and Karen M. Emmerson. The aﬀective tipping point: Do motivated reasoners ever get it? Political Psychology, 31(4):563–593, 2010.
[22] L. Rendell, R. Boyd, D. Cownden, M. Enquist, K. Eriksson, M. W. Feldman, L. Fogarty, S. Ghirlanda, T. Lillicrap, and K. N. Laland. Why copy others? insights from the social learning strategies tournament. Science, 328(5975):208, 2010.
[23] M. J Salganik, P. S Dodds, and D. J Watts. Experimental study of inequality and unpredictability in an artiﬁcial cultural market. Science, 311(5762):854, 2006.
[24] Martin Shubik. Game theory and political science. Paper No. 351, 1973.
[25] D.F. Stone. Ideological media bias. Journal of Economic Behavior & Organization, 78:256–271, 2011.

