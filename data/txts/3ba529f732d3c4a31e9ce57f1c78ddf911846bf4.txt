arXiv:2109.11377v2 [cs.LG] 11 Oct 2021

WRENCH: A Comprehensive Benchmark for Weak Supervision
Jieyu Zhang1,2, Yue Yu3, Yinghao Li3, Yujing Wang1, Yaming Yang1, Mao Yang1, Alexander Ratner2
1Microsoft Research Asia 2University of Washington 3Georgia Institute of Technology {jieyuz2, ajratner}@cs.washington.edu
{yujwang, yayaming, maoyang}@microsoft.com {yueyu, yinghaoli}@gatech.edu
Abstract
Recent Weak Supervision (WS) approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a signiﬁcant "hidden" source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classiﬁcation and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efﬁcacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.
1 Introduction
One of the major bottlenecks for deploying modern machine learning models in real-world applications is the need for substantial amounts of manually-labeled training data. Unfortunately, obtaining such manual annotations is typically time-consuming and labor-intensive, prone to human errors and biases, and difﬁcult to keep updated in response to changing operating conditions. To reduce the efforts of annotation, recent weak supervision (WS) frameworks have been proposed which focus on enabling users to leverage a diversity of weaker, often programmatic supervision sources [77, 78, 76] to label and manage training data in an efﬁcient way. Recently, WS has been widely applied to various machine learning tasks in a diversity of domains: scene graph prediction [9], video analysis [23, 94], image classiﬁcation [12], image segmentation [35], autonomous driving [98], relation extraction [36, 109, 57], named entity recognition [84, 53, 50, 45, 27], text classiﬁcation [79, 102, 87, 88], dialogue system [63], biomedical [43, 19, 64], healthcare [20, 17, 21, 82, 95, 83], software engineering [75], sensors data [24, 39], E-commerce [66, 105], and multi-agent systems [104].
In a WS approach, users leverage weak supervision sources, e.g., heuristics, knowledge bases, and pretrained models, instead of manually-labeled training data. In this paper, we use the data programming formalism [78] which abstracts these weak supervision sources as labeling functions, which are user-deﬁned programs that each provides labels for some subset of the data, collectively generating a large but potentially overlapping set of votes on training labels. The labeling functions may have varying error rates and may generate conﬂicting labels on certain data points. To address these issues, researchers have developed modeling techniques which aggregate the noisy votes of labeling functions
35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.

Unlabeled Data
def lf_1(x): return heuristic_1(x)
def lf_2(x): return dist_sup(x, KB)
def lf_3(x): return re.find(p, x) Labeling Functions
Input

Label Model

Probabilistic Labels
Two-stage Method

End Model

Label Model

Joint Model

End Model

One-stage Method

Figure 1: An overview of WS pipeline.

to produce training labels (often referred to as a label model) [78, 76, 22, 94], which often build on prior work in modeling noisy crowd-worker labels, e.g. [14]. Then, these training labels (often conﬁdence-weighted or probabilistic) are in turn used to train an end model which can generalize beyond the labels for downstream tasks. These two-stage methods mainly focus on the efﬁciency and effectiveness of the label model, while maintaining the maximal ﬂexibility of the end model. Recent approaches have also focused on integrating semi- or self-supervised approaches [102]; we view these as modiﬁed end models in our benchmarking framework. In addition to these two-stage methods, researchers have also explored the possibility of coupling the label model and the end model in an end-to-end manner [79, 45, 38]. We refer to these one-stage methods as joint models. An overview of WS pipeline can be found in Fig.1.
Despite the increasing adoption of WS approaches, a common benchmark platform is still missing, leading to an evaluation space that is currently rife with custom and/or private datasets, weak supervision sources that are highly varied and in often hidden and uncontrolled ways, and basic evaluation protocols that are highly variable. Several thematic issues are widespread in the space:

• Private and/or custom datasets: Due to the lack of standardized benchmark datasets, researchers often construct their own datasets for comparison. In particular, WS approaches are often practically motivated by real-world use cases where labeled data is difﬁcult to generate, resulting in datasets are often based on real production need and therefore are not publicly avaiable.
• Hidden weak supervision source variance: Unlike traditional supervised learning problems, WS datasets vary not just in the unlabeled data X, but also crucially in the labels Y and weak supervision sources they derive from (see Fig. 2). This latter degree of variance has a major effect on the performance of WS approaches; however it is often poorly documented and controlled for. For example, it is not uncommon to have two datasets with completely different weak supervision sources bear the exact same name (usually deriving from the source of the unlabeled data X) in experimental results, despite being entirely different datasets from a WS perspective.
• End-to-end evaluation protocol: WS approaches involve more complex (e.g. two-stage) pipelines, requiring greater (yet often absent) care to normalize and control evaluations. For example, it is not uncommon to see signiﬁcant variance in which stage of a two-stage pipeline performance numbers are reported for, what type of training labels are produced, etc [78, 102].

To address these issues and contribute a resource to the growing WS community, we developed Weak Supervision Benchmark (WRENCH), a benchmark platform for WS with 22 diverse datasets from the literature, a range of standardized real, synthetic, and procedurally generated weak supervision sources, and a modular, extendable framework for execution and evaluation of WS approaches, along with initial implementations of recent popular WS methods. WRENCH includes:

• A diverse (and easily extensible) set of 22 real-world datasets for two canonical, annotationintensive machine learning problems, classiﬁcation and sequence tagging, including datasets used in existing WS studies and new ones we contribute.
• A range of real (user-generated) weak supervision sources, and new synthetic and procedural weak supervision source generators, enabling systematic study on the effect of different supervision source types on the performances of WS methods, e.g. with respect to accuracy, variance, sparsity, conﬂict and overlap, correlation, and more.

2

• A modular, extensible Python codebase for standardization of implementation, evaluation, and ablation of WS methods, including standardized evaluation scripts for prescribed metrics, uniﬁed interfaces for publicly available methods, and re-implementations of some other popular ones.
To demonstrate the utility of WRENCH, we analyze the effect of a range of weak supervision attributes using WRENCH’s procedural weak supervision generation suite, illustrating the effect of various salient factors on WS method efﬁcacy (Sec. 5). We also conduct extensive experiments to render a comprehensive evaluation of popular WS methods (Sec. 6), exploring more than 120 compared methods and their variants (83 for classiﬁcation and 46 for sequence tagging). We plan to continuously update WRENCH with more datasets and methods as the ﬁeld advances. We welcome contributions and expect the scope and breadth of WRENCH to increase over time.
2 Related Work
Weak Supervision. Weak supervision builds on many previous approaches in machine learning, such as distant supervision [69, 34, 89], crowdsourcing [26, 42], co-training methods [6], patternbased supervision [28], and feature annotation [65, 103]. Speciﬁcally, weak supervision methods take multiple noisy supervision sources and an unlabeled dataset as input, aiming to generate training labels to train an end model (two-stage method) or directly produce the end model for the downstream task (one-stage method) without any manual annotation. Weak supervision has been widely applied on both classiﬁcation [78, 76, 22, 102, 79] and sequence tagging [53, 72, 84, 50, 45] to help reduce human annotation efforts.
Weak Supervision Sources Generation. To further reduce the efforts of designing supervision sources, many works propose to generate supervision sources automatically. Snuba [93] generates heuristics based on a small set of labeled datasets. IWS [7] and Darwin [25] interactively generate labeling functions based on user feedback. TALLOR [46] and GLaRA [108] automatically augment an initial set of labeling functions with new ones. Different from existing works that optimize the task performance, the procedural labeling function generators in WRENCH facilitate the study of the impact of different weak supervision sources. Therefore, we assume access to a fully-labeled dataset and generate diverse types of weak supervision sources.
The Scope of this Benchmark. We are aware that there are numerous works on learning with noisy or distantly labeled data for various tasks, including relation extraction [59, 69, 89], sequence tagging [51, 56, 73, 86], image classiﬁcation [29, 48, 70] and visual relation detection [101, 106]. There are also several benchmarks targeting on this topic [30, 37, 80, 100, 10] with different noise levels and patterns. However, these studies mainly concentrate on learning with single-source noisy labels and cannot leverage complementary information from multiple annotation sources in weak supervision. Separately, there are several works [3, 38, 62, 68, 67] leveraging additional clean, labeled data for denoising multiple weak supervision sources, while our focus is on benchmarking weak supervision methods that do not require any labeled data. So we currently do not include these methods in WRENCH, that being said, we plan to gradually incorporate them in the future.
3 Background: Weak Supervision
We ﬁrst give some background on weak supervision (WS) at a high level. In the WS paradigm, multiple weak supervision sources are provided which assign labels to data, which may be inaccurate, correlated, or otherwise noisy. The goal of a WS approach is the same as in supervised learning: to train an end model based on the data and weak supervision source labels. This can be broken up into a two-stage approach–separating the integration and modeling of WS from the training of the end model–or tackled jointly as a one-stage approach.
3.1 Problem Setup
We more formally deﬁne the setting of WS here. We are given a dataset containing n data points X = [X1, X2, . . . , Xn] with i-th data point denoted by Xi ∈ X . Let m be the number of WS sources {Sj}m j=1, each assigning a label λj ∈ Y to Xi to vote on its respective Yi or abstaining (λj = −1). We deﬁne the propensity of one source Sj as p(λj = −1). For concreteness, we follow the general convention of WS [78] and refer to these sources as labeling functions (LFs) throughout the paper. In WRENCH, we focus on two major machine learning tasks:
3

Table 1: Statistics of all the tasks, domains and datasets in-

cluded in WRENCH.

Train Dev Test

Task (↓)

Domain (↓) Dataset (↓)

#Label #LF #Data #Data #Data

Income Class.

Tabular Data Census [40, 3]

2

83 10,083 5,561 16,281

Sentiment Class.

Movie Review

IMDb [61, 79] Yelp [107, 79]

2

5 20,000 2,500 2,500

2

8 30,400 3,800 3,800

Spam Class.

Review

Youtube [1]

Text Message SMS [2, 3]

2

10 1,586 120 250

2

73 4,571 500 500

Topic Class.

News

AGNews [107, 79]

4

9 96,000 12,000 12,000

Question Class. Web Query TREC [49, 3]

6

68 4,965 500 500

Relation Class.

News Biomedical Web Text Chemical

Spouse [11, 77] CDR [13, 77] SemEval [31, 109] ChemProt [41, 102]

2

9 22,254 2,811 2,701

2

33 8,430 920 4,673

9

164 1,749 200

692

10

26 12,861 1,607 1,607

Image Class.

Video

Commercial [22] Tennis Rally [22] Basketball [22]

2

4 64,130 9,479 7,496

2

6 6,959 746 1,098

2

4 17,970 1,064 1,222

News

CoNLL-03 [85, 53]

4

16 14,041 3250 3453

Web Text

WikiGold [5, 53] OntoNotes 5.0 [96]

4

16 1,355 169 170

18

17 115,812 5,000 22,897

Sequence Tagging

BC5CDR [47, 50]

2

Biomedical NCBI-Disease [16, 50]

1

9

500

500 500

5

592

99

99

Review

Laptop-Review [74, 50] 1

MIT-Restaurant [55]

8

3 2,436 16 7,159

609 800 500 1,521

Movie

MIT-Movies [54]

12

7 9,241 500 2,441

Figure 2: Box plots: The coverage, overlap, conﬂict and accuracy of LFs in collected datasets. We can see the LFs have diverse properties across datasets.

Classiﬁcation: for each Xi, there is an unobserved true label denoted by Yi ∈ Y. A label matrix L ∈ Rn×m is obtained via applying m LFs to the dataset X = [X1, X2, . . . , Xn]. We seek to build an end model fw : X → Y to infer the labels Yˆ for each X ∈ X.

Sequence tagging: each Xi ∈ X is a sequence of tokens [xi,1, xi,2, . . . , xi,t], where t is the length of Xi, with an unobserved true label list denoted by Yi = [yi,1, yi,2, . . . , yi,t] where yi,j ∈ Y. For each sequence Xi with its associated label matrix Li ∈ Rn×t, we aim to produce an sequence tagger model fw : X → Y which infers labels Yˆ = [yˆ1, yˆ1, . . . , yˆt] for each sequence.
It is worth noting that, different from the semi-supervised setting, and some recent WS work, where some ground-truth labeled data is available [3, 62, 38, 67, 68], we consider the setting where we train the end model without observing any ground truth training labels. However, we note that WRENCH can be extended in future work to accommodate these settings as well.
3.2 Two-stage Method
Two-stage methods usually decouple the process of training label models and end models. In the ﬁrst stage, a label model is used to combine the label matrix L with either probabilistic soft labels or one-hot hard labels, which are in turn used to train the desired end model in the second stage. Most studies focus on developing label models while leaving the end model ﬂexible to the downstream tasks. Existing label models include Majority Voting (MV), Probabilistic Graphical Models (PGM) [14, 78, 76, 22, 53, 84, 50], etc.. Note that prior crowd-worker modeling work can be included and subsumed by this set of approaches, e.g. [14].
3.3 One-stage Method
One-stage methods attempt to effectively train a label model and end model simultaneously [79, 45]. Speciﬁcally, they usually design a neural network for aggregating the prediction of labeling functions while utilizing another neural network for ﬁnal prediction. We refer to the model designed for one-stage methods as a joint model.
4 Wrench Benchmark Platform
We propose the ﬁrst benchmark platform, WRENCH, for weak supervision (WS). Speciﬁcally, WRENCH includes the following components:

A collection of 22 real-world datasets. We collect 22 publicly available real-world datasets and the corresponding user-provided LFs from the literature. The statistics of the datasets is in Table 1. The datasets cover a wide range of topics, including both generic domains such as web text, news, videos and specialized ones including biomedical and chemical publications. The corresponding LFs have various forms, such as key words [86], regular expressions [3], knowledge bases [51] and human-provided rules [22]. Some relevant statistics of the LFs is in Fig. 2; the box plots demonstrate

4

Table 2: The initial set of methods included in WRENCH. A brief introduction of each method can be found in App. C. We plan to add more methods in near future.

Task Classiﬁcation Sequence Tagging

Module Label Model
End Model
Joint Model Label Model End Model Joint Model

Method
Majority Voting Weighted Majority Voting Dawid-Skene [14] Data Progamming [78] MeTaL [76] FlyingSquid [22]
Logistic Regression Multi-Layer Perceptron Neural Network BERT [15] RoBERTa [58] COSINE-BERT [102] COSINE-RoBERTa [102]
Denoise [79]
Hidden Markov Model [53] Conditional Hidden Markov Model [50]
LSTM-CNNs-CRF [60] BERT [15]
Consensus Network [45]

Abbr.
MV WMV DS DP MeTaL FS
LR MLP B R BC RC
Denoise
HMM CHMM
LSTM-CNNs-CRF BERT
ConNet

that the LFs have diverse properties across datasets, enabling more thorough comparisons among WS approaches. The description of each dataset and detailed statistics are in App. B.
A range of procedural labeling function generators. In addition to the manually-created LFs coupled with each dataset, WRENCH provides a range of procedural labeling function generators for the ﬁrst time, giving users ﬁne-grain control over the space of weak supervision sources. It facilitates researchers to evaluate and diagnose WS methods on (1) synthetic datasets or (2) real-world datasets with procedurally generated LFs. Based on the generators, users could study the relationship between different weak supervision sources and WS method performances. The details of the generators and the provided studies are in Sec. 5. Notably, the uniﬁed interface of WRENCH allows users to add more generators covering new types of LFs easily.
Abundant baseline methods and extensive comparisons. WRENCH provides uniﬁed interfaces for a range of publicly available and popular methods. A summary of models currently included in WRENCH is in Table 2. With careful modularization, users could pick any label model and end model to form a two-stage WS method, while also choosing to use soft or hard labels for training the end model, leading to more than 100 method variants. We conduct extensive experiments to offer a systematic comparison over all the models and possible variants on the collected 22 datasets (Sec. 6). Another beneﬁt of this modularity is that other approaches can be easily contributed, and we plan to add more models in the future.
5 Labeling Function Generators
In addition to user-generated labeling functions collected as part of the 22 benchmark datasets in WRENCH, we provide two types of weak supervision source generators in WRENCH in order to enable ﬁne-grain exploration of WS method efﬁcacy across different types of weak supervision: (1) synthetic labeling function generators, which directly generate labels from simple generative label models; (2) procedural labeling function generators, which automatically generate different varieties of real labeling functions given an input labeled dataset. In this section, we introduce the generators in detail and provide some sample studies to demonstrate the efﬁcacy of these generators in enabling ﬁner-grain exploration of the relationship between weak supervision source qualities and WS method performances. For simplicity, in this section we constrain our study to binary classiﬁcation tasks and the details on implementation and parameter can be found in App. E.3.
5.1 Synthetic Labeling Function Generator
The synthetic labeling function generators are independent of the input data X; instead, they directly generate the labeling function output labels. We provide one initial synthetic generator to start in WRENCH’s ﬁrst release, which generates labels according to a classic model where the LF outputs are conditionally independent given the unseen true label Y [14, 78]. For this model, WRENCH provides users with control over two important dimensions: accuracy and propensity. In addition, users can control the variance of the LF accuracies and propensities via the respective radius of accuracy and propensity parameters. For example, the accuracy of each LF could be chosen to be uniformly sampled from [a − b, a + b], where a is the mean accuracy and b is the radius of accuracy, resulting in

5

(a)

(b)

Figure 3: Label models performance (AUC) on synthetic LFs with varying (a) radius of LF’s accuracy and (b) propensity. We can see that when the radius of LF’s accuracy is large or the propensity of LFs is small, the label model performance are more divergent.

a variance of 1b22 . We construct the synthetic label generators to be extensible, for example, to include more controllable parameters and more complex models.

Based on this generator, we study different dimensions of LFs and found that the comparative performance of label models are largely dependent on the variance of accuracy and propensity of LFs. First, we ﬁx other dimensions and vary the radius of LF’s accuracy, and generate Y and LFs for binary classiﬁcation. As shown in Fig. 3(a), we can see that the performance of label models diverge when we increase the variance of LFs’ accuracy by increasing the radius of accuracy. Secondly, we vary the propensity of LFs. From the curves in Fig. 3(b), we can see that if we increase the propensity of LFs, the label models’ performance keep increasing and converge eventually, while when the propensity is lower, the label models perform differently. These observations indicate the importance of the dimensions of LFs, which could lead to the distinct comparative performance of label models.

5.2 Procedural Labeling Function Generator

The procedural labeling function generator class in WRENCH requires the input of a labeled dataset (X, Y ), i.e. with data features and ground truth training labels. The procedural generators create a pool of candidate LFs based on a given feature lexicon. Each candidate LF S consists of a single or composite feature from the provided lexicon and a label. The ﬁnal set of generated LFs is those candidate LFs whose individual parameters (e.g. accuracies and propensities) and group parameters (e.g. correlation and data-dependency) meet user-provided thresholds. These procedurally-generated LFs mimic the form of user-provided ones, but enable ﬁne-grain control over the LF attributes.

In this section, we provide an example study of how label models perform with different types of LFs on two real-world text classiﬁcation datasets, Yelp and Youtube, to demonstrate the utility of these procedural generators. For simplicity, we adopt (n, m)-gram features, where n and m are the minimum and maximum length of gram respectively and are input by users. Speciﬁcally, a candidate LF S consists of one label value y and an (n, m)-gram feature f ; for each data point, if the feature f exists, then S assigns label y, otherwise returns abstain (λ = −1). We generate and examine three sets of LFs, namely, the LFs with highest (1) accuracies, (2) pairwise correlations and (3) data-dependencies (Fig. 4). For (2), the correlations of LFs are measured by the conditional mutual information of a pair of candidate LFs given the ground truth labels Y . We are interested in (2) because existing works often assume the LFs are independent conditional on Y [78, 4], however, users can hardly generate perfectly conditionally independent LFs; therefore, it is of great importance to study how label models perform when LFs are not conditionally independent. The reason for (3) is that previous studies typically assume the LFs are uniformly accurate across the dataset [78, 4, 76, 22], however, in practice, this is another often violated assumption–e.g. speciﬁc LFs are often more accurate on some subset of data than the other. Thus, we measure the data-dependency of LFs by the variance of accuracies of LF over clusters of data and pick LFs with the highest data dependency.

The results are in Fig. 4. First, in the case of top-k accurate LFs (Fig. 4(a)&(d)), the label models perform similarly, however, for the other two types of LFs, there are large gaps between label model performance and the superiority of recently-proposed methods, i.e., DP, MeTaL, FS, can be clearly seen. Secondly, even within the same type of LFs, one label model can result in varying performance on different datasets; for example, when correlated LFs are generated (Fig. 4(b)&(e)), the DS model performs much better on Yelp than Youtube compared to the MV model. These observations further

6

(a)

(b)

(c)

(d)

(e)

(f)

Figure 4: Label models performance (AUC) on Youtube and Yelp with varying types of procedural LFs, namely, top-k accurate, correlated, or data-dependent LFs. We can see that the dependency properties of LF (correlated and data-dependent) have a major effect on the comparative performance of label models.

conﬁrm that the LFs have a major effect on the efﬁcacy of different WS approaches, and it is critical to provide a benchmark suite for WS with varied datasets and varying types of LFs.

6 Benchmark Experiments

To demonstrate the utility of WRENCH in providing fair and rigorous comparisons among WS methods, we conduct extensive experiments on the collected real-world datasets with real labeling functions. Here, we consider all the possible ways to compose a two-stage method using the initial models that we implement in WRENCH (Table 2), ablating over the choice of soft and hard labels, as well as considering the one-stage methods listed.
6.1 Classiﬁcation
6.1.1 Evaluation Protocol
We evaluate the performance of (1) the label model directly applied on test data; (2) the end model trained with labels provided by label model for two-stage methods; (3) the end model trained within a joint model for one-stage methods; and (4) the "gold" method, namely training an end model with ground truth labels, with different end models. We include all the possible two-stage methods as well as the variants using soft or hard labels in our comparison, leading to 83 methods in total.
For each dataset, we adopt the evaluation metric used in previous work. For LR and MLP applied on textual datasets, we use a pre-trained BERT model to extract the features. Note that for the Spouse dataset, we do not have the ground truth training labels, so we do not include gold methods for it. In addition, due to privacy issues, for the video frame classiﬁcation datasets (i.e., Commerical, Tennis Rally and Basketball), we only have access to the features extracted by pre-trained image classiﬁer instead of raw images, thus, we choose LR and MLP as end models.
6.1.2 Evaluation Results
Due to the space limit, we defer the complete results as well as the standard deviations to the App.F, while only presenting the top 3 best WS methods and the gold method with the best end model for each dataset in Table 3. From the table, we could observe a diversity of the best WS methods on different datasets. In other words, there is no such method that could consistently outperform others. This observation demonstrates that it remains challenging to design a generic method that works for diverse tasks. For textual datasets, it is safe to conclude that ﬁne-tuning a large pretrain

7

Table 3: Classiﬁcation. The performance of the best gold method and top 3 best weak supervision methods for each dataset. EM and LM stand for the end model and label model respectively. Underline indicates using the soft label for training end model. Datasets with * are non-textual data on which BERT/RoBERTa are not applicable. Each metric value is averaged over 5 runs. The detailed results and average performance can be found in App. F.

Dataset IMDb Yelp Youtube SMS AGNews TREC Spouse CDR SemEval ChemProt Commerical* Tennis Rally* Basketball* Census*

Metric Acc. Acc. Acc. F1 Acc. Acc. F1 F1 Acc. Acc. F1 F1 F1 F1

Best Gold

EM Value

R 93.25

R 97.13

B 97.52

B 96.96

R 91.39

R 96.68

–

–

R 65.86

B 95.43

B 89.76

MLP 91.69

LR 82.73

MLP 64.97

MLP 67.13

Top 1 EM LM RC MeTaL RC FS BC MV RC WMV RC DS RC DP BC FS
– MeTaL BC DP BC DP
Denoise MLP FS MLP FS LR MeTaL

Value 88.86 95.45 98.00 98.02 88.20 82.36 56.52 69.61 88.77 61.56 91.34 83.77 43.18 58.16

EM RC RC RC RC RC RC – – BC RC LR MLP MLP MLP

Top 2 LM FS FS MV MeTaL MV MeTaL MeTaL DP MV MV MV MeTaL WMV MeTaL

Value 88.48 95.33 97.60 97.71 88.15 79.84 46.62 63.51 86.80 59.43 90.62 83.70 40.73 57.84

EM RC RC RC RC RC BC RC RC RC RC MLP LR MLP MLP

Top 3 LM MV DS MV WMV WMV DP MV DP DP MV MV FS DP MeTaL

Value 88.48 95.01 97.60 97.27 88.11 78.72 46.28 61.40 86.73 59.32 90.55 83.68 40.70 57.66

language model is the best option of the end model, and COSINE could successfully improve the performance of ﬁne-tuned language models. Moreover, ﬁne-tuning a pre-trained language model is, not surprisingly, much better than directly applying label model on test data in most cases, because it is well-known that large pre-trained language models like BERT can easily adapt to new tasks with good generalization performance.
6.2 Sequence Tagging
6.2.1 Evaluation Protocol
Same as the evaluation scheme on classiﬁcation tasks, we evaluate the performance of (1) the label models; (2) the end models trained by predictions from the label models; (3) the joint models; and (4) the end models trained by gold labels on the training set. Note that following previous works [45, 84, 86], we adopt hard labels in order to ﬁt end models which contain CRF layers. To adapt label models designed for classiﬁcation tasks to sequence tagging tasks, we split each sequence by tokens and reformulate it as a token-level classiﬁcation task. We discuss the detailed procedure on adapting label model for sequence tagging tasks in App. D. However, these models neglect the internal dependency between labels within the sequence. In contrast, HMM and CHMM take the whole sequence as input and predict the label for tokens in the whole sequence. For the end model with LSTM/BERT, we run experiments with two settings: (1) stacking a CRF layer on the top of the model, (2) using a classiﬁcation head for token-level classiﬁcation; and the best performance is reported.
Following the standard protocols, we use entity-level F1-score as the metric [53, 60] and use BIO schema [90, 50, 53], which labels the beginning token of an entity as B-X and the other tokens inside that entity as I-X, while non-entity tokens are marked as O. For methods that predict token-level labels (e.g.MV), we transform token-level predictions to entity-level predictions when calculating the F1 score. Since BERT tokenizer may separate a word into multiple subwords, for each word, we use the result of its ﬁrst token as its prediction.
6.2.2 Evaluation Results
Table 4 demonstrates the main result of different methods on sequence tagging tasks. For label models, we conclude that considering dependency relationships among token-level labels during learning generally leads to better performance, as HMM-based models achieve best performance on 7 of 8 datasets. One exception is the MIT-Restaurants dataset, where weak labels have very small coverage. In this case, the simple majority voting-based methods achieve superior performance compared with other complex probabilistic models. For end models, surprisingly, directly training a neural model with weak labels does not guarantee the performance gain, especially for LSTM-based

8

Table 4: Sequence Tagging. Comparisons among different methods. The number stands for the F1 score. Each metric value is averaged over 5 runs. red and blue indicate the best and second best result for each end model respectively, and gray is the best weak supervision method. The ﬁrst 8 rows with end model as – indicates
directly apply label models on test data. The detailed results are in App. F.2.

End Model (↓) Label Model (↓)

MV

WMV

DS

–

DP

MeTaL

FS

HMM

CHMM

LSTM-CNN

Gold
MV WMV
DS DP MeTaL FS HMM CHMM

LSTM-ConNet

BERT

Gold
MV WMV
DS DP MeTaL FS HMM CHMM

BERT-ConNet

CoNLL-03
60.36 60.26 46.76 62.43 60.32 62.49 62.18 63.22
87.46
66.33 64.60 50.60 67.15 65.05 66.49 66.18 66.67
66.02
89.41
67.08 65.96 54.04 67.66 66.34 67.54 68.48 68.30
67.83

WikiGold
52.24 52.87 42.17 54.81 52.09 58.29 56.36 58.89
80.45
58.27 55.39 40.61 57.89 56.31 60.49 62.51 61.34
58.04
87.21
63.17 61.28 49.09 62.91 61.74 66.58 64.25 65.16
64.18

BC5CDR
83.49 83.49 83.49 83.50 83.50 56.71 71.57 83.66
78.59
74.75 74.31 75.37 74.79 74.66 54.49 64.07 74.54
72.04
82.49
77.93 77.76 77.57 77.67 77.80 62.89 68.70 77.98
72.87

NCBI-Disease
78.44 78.44 78.44 78.44 78.44 40.67 66.80 78.74
79.39
72.44 72.21 72.86 72.50 72.42 44.90 59.12 72.15
63.04
84.05
77.93 78.53 78.69 78.18 79.02 46.50 65.52 78.20
71.40

Laptop-Review
73.27 73.27 73.27 73.27 64.36 28.74 73.63 73.26
71.25
63.52 63.02 63.96 62.59 63.87 28.35 62.57 62.28
50.36
81.22
71.12 71.60 71.41 71.46 71.80 38.57 71.51 71.17
67.32

MIT-Restaurant
48.71 48.19 46.81 47.92 47.66 13.86 42.65 47.34
79.18
41.70 41.27 41.21 41.62 41.48 13.09 37.90 41.59
39.26
78.85
42.95 42.62 42.26 42.27 42.26 13.80 39.51 42.79
42.37

MIT-Movies
59.68 60.37 54.06 59.92 56.60 43.04 60.56 61.38
87.07
62.41 61.79 55.99 62.29 62.10 45.77 61.94 62.97
60.46
87.56
63.71 63.44 58.89 63.92 64.19 49.79 63.38 64.58
64.12

Ontonotes 5.0
58.85 57.58 37.70 61.85 58.27 5.31 55.67 64.06
79.52
61.92 59.22 44.92 63.82 60.43 43.25 59.43 63.71
60.58
84.11
63.97 61.63 48.55 65.16 63.08 49.63 61.29 66.03
60.36

Average
64.38 64.31 57.84 65.27 62.66 38.64 61.88 66.32
79.83
62.47 61.37 55.58 62.83 61.85 44.51 59.17 62.97
58.73
84.36
65.62 63.92 58.87 65.97 65.61 49.11 62.59 66.50
63.81

model which is trained from scratch. Such a phenomenon arrives when the quality of LFs is poor (e.g. MIT-Restaurants, LaptopReview). Under this circumstance, the weak labels generated through LFs are often noisy and incomplete [51, 56], and the end model can easily overﬁt to them. As a result, there is still a signiﬁcant performance gap between the results trained by gold labels and weak labels, which motivates the future research on designing methods robust against the induced noise.
7 Discussion and Recommendation
• Correctly categorization of method and comparing it to right baselines are critical. As stated in Sec. 3, weak supervision methods could be categorized into label model, end model and joint model. However, we observed that in previous work, researchers, more or less, did not clearly categorize their method and compare it to inappropriate baselines. For example, COSINE is an end model but in the original paper, the authors coupled COSINE with MV (a label model) and compared it with another label model, MeTaL1, without coupling MeTaL with an end model. This comparison is hardly fair and effective.
• Strong weakly supervised models rely on high-quality supervision sources. From the result shown in table 3 and 4, we observe that both label model and end model perform well only when the quality of the overall labeling functions is reasonably good. For those datasets which have very noisy labeling functions (e.g.Basketball) or very limited coverage (e.g.MIT-Restaurants), there is still a large gap between the performance of fully-supervised model and weakly-supervised model. Such phenomenon illustrates that it is still necessary to check the quality of initial labeling functions before applying weak supervision models for new tasks. Otherwise, directly adopting these models may not lead to satisfactory results, and may even hurt the performance.
• When the end models become deeper, using soft label may be a good idea. Based on the average performance of models across tasks, we observe that using soft labels to train the end model is better than hard labels in most cases, especially when the end model become deeper (from logistic regression to pretrained language model). We think this is relevant to the idea of "label smoothing" [71], which prevents the deep models from overﬁtting to (noisy) training data.
• Uncovered data should be used when training end models. A common practice of weak supervision is to train an end model using only covered data2, i.e., the subset of data which receive at least one weak signal. However, the superiority of COSINE suggests that those uncovered data should also be used in training an end model; this inspires future direction of exploring new end model training strategy combined with semi-supervised learning techniques.
1The Snorkel baseline in their paper. 2https://www.snorkel.org/use-cases/01-spam-tutorial

9

• For sequence tagging tasks, selecting appropriate tagging scheme is important. As studied in App. D.2, choosing different tagging schema can cause up to 10% performance in terms of F1 score. This is mainly because when adopting more complex tagging schema (e.g., BIO), the label model could predict incorrect label sequences, which may hurt ﬁnal performance especially for the case where the number of entity types is small. Under this circumstance, it is recommended to use IO schema during model training. For other datasets including more types of entities, there is no clear winners for different schemes.
• For classiﬁcation tasks, MeTaL and MV are the most worth-a-try label models and for end model, deeper is better. According to the model performance averaged over datasets, we ﬁnd MeTaL and MV are the best label models when using different end models or directly applying label models on test set. For the choices of end model, not surprisingly, deeper model is better.
• For sequence tagging tasks, CHMM gains an advantage over other baselines in terms of label model. CHMM generally outperforms other label models and achieves highest average score. We remark that CHMM is the only label model that combines the outputs of labeling function with data feature (i.e. BERT embeddings). The superiority of CHMM indicates that developing data-dependent label model will be a promising direction for the future research. For the end model, pre-trained language models are more suitable end models, as it can capture general semantics and syntactic information [81] which will beneﬁt the downstream tasks.
8 Conclusion and Future Work
We introduce WRENCH, a comprehensive benchmark for weak supervision. It includes 22 datasets for classiﬁcation and sequence tagging with a wide range of domains, modalities, and sources of supervision. Through extensive comparisons, we conclude that designing general-purpose weak supervision methods still remains challenging. We believe that WRENCH provides an increasingly needed foundation for addressing this challenge. In addition, WRENCH provides procedural labeling function generators for systematic study of various types of weak supervision sources. Based on the generators, we study a range of aspects of weak supervision, in order to help understand the weak supervision problem and motivate future research directions.
For future work, we plan to include more weak supervision methods and novel tasks, covering more aspects of weak supervision. Speciﬁcally, we plan to incorporate following aspects:
(1) Learning the dependency structure of supervision sources. The dependency structure among supervision sources is frequently ignored in applications of weak supervision. However, as reported in [8], this unawareness and consequent dependency structure misspeciﬁcation could result in a serious performance drop. To address this, several approaches have been proposed [4, 91, 92], but a benchmark for this purpose is missing. To complement this, we plan to add more datasets with varying dependency structure for benchmarking the dependency structure learning in weak supervision.
(2) Active generation and repurposing of supervision sources. To further reduce human annotation efforts, very recently, researchers turn to active generation [93, 46, 108, 7, 25] and repurposing [27] of supervision sources. In the future, we plan to incorporate these new tasks and methods into WRENCH to extend its scope.
(3) More applications of weak supervision. WRENCH focus on two applications of weak supervision: classiﬁcation and sequence tagging. To unleash the potential of weak supervision and push the community to move forward, we plan to add more applications into WRENCH in the future.
Finally, the model performance prediction based on properties of dataset and supervision sources is of great importance yet challenging and open. We believe the labeling function generator in WRENCH and the new proposed measurements of supervision sources, i.e., correlation and data-dependency, would contribute to this goal.
References
[1] T. C. Alberto, J. V. Lochter, and T. A. Almeida. “TubeSpam: Comment Spam Filtering on YouTube”. In: ICMLA. 2015, pp. 138–143. DOI: 10.1109/ICMLA.2015.37.
[2] Tiago A Almeida, José María G Hidalgo, and Akebo Yamakami. “Contributions to the study of SMS spam ﬁltering: new collection and results”. In: DocEng. 2011, pp. 259–262.
10

[3] Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. “Learning from Rules Generalizing Labeled Exemplars”. In: ICLR. 2020. URL: https://openreview. net/forum?id=SkeuexBtDr.
[4] Stephen H. Bach, Bryan D. He, Alexander J. Ratner, and C. Ré. “Learning the Structure of Generative Models without Labeled Data”. In: Proceedings of machine learning research 70 (2017), pp. 273–82.
[5] Dominic Balasuriya, Nicky Ringland, Joel Nothman, Tara Murphy, and James R Curran. “Named entity recognition in wikipedia”. In: Workshop on The People’s Web Meets NLP. 2009, pp. 10–18.
[6] A. Blum and Tom. Mitchell. “Combining labeled and unlabeled data with co-training”. In: COLT. 1998.
[7] Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski. “Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling”. In: ICLR. 2021.
[8] Salva Rühling Cachay, Benedikt Boecking, and Artur Dubrawski. “Dependency Structure Misspeciﬁcation in Multi-Source Weak Supervision Models”. In: ICLR Workshop on Weakly Supervised Learning (2021).
[9] Vincent S Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein, Christopher Re, and Li Fei-Fei. “Scene graph prediction with limited labels”. In: ICCV. 2019, pp. 2580–2590.
[10] Zewei Chu, Karl Stratos, and Kevin Gimpel. “NATCAT: Weakly Supervised Text Classiﬁcation with Naturally Annotated Resources”. In: AKBC. 2021. URL: https://openreview. net/forum?id=kmVA04ltlG_.
[11] D. Corney, M. Albakour, Miguel Martinez-Alvarez, and Samir Moussa. “What do a Million News Articles Look like?” In: NewsIR@ECIR. 2016.
[12] Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau, and Xu Chu. “Goggles: Automatic image labeling with afﬁnity coding”. In: SIGMOD. 2020, pp. 1717– 1732.
[13] Allan Peter Davis, Cynthia J Grondin, Robin J Johnson, Daniela Sciaky, Benjamin L King, Roy McMorran, Jolene Wiegers, Thomas C Wiegers, and Carolyn J Mattingly. “The comparative toxicogenomics database: update 2017”. In: Nucleic acids research 45 (2017), pp. D972– D978.
[14] A. P. Dawid and A. M. Skene. “Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm”. In: Journal of the Royal Statistical Society 28.1 (1979), pp. 20–28. ISSN: 00359254, 14679876. URL: http://www.jstor.org/stable/2346806.
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019, pp. 4171–4186. DOI: 10.18653/v1/N19-1423.
[16] Rezarta Islamaj Dog˘an, Robert Leaman, and Zhiyong Lu. “NCBI disease corpus: a resource for disease name recognition and concept normalization”. In: Journal of biomedical informatics 47 (2014), pp. 1–10.
[17] Jared A. Dunnmon, Alexander J. Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert, Hersh Sagreiya, Roger E. Goldman, Christopher Lee-Messer, Matthew P. Lungren, Daniel L. Rubin, and Christopher Ré. “Cross-Modal Data Programming Enables Rapid Medical Machine Learning”. In: Patterns 1.2 (2020), p. 100019. DOI: 10.1016/j.patter.2020. 100019. URL: https://doi.org/10.1016/j.patter.2020.100019.
[18] Eibe Frank and Ian H Witten. “Generating Accurate Rule Sets Without Global Optimization”. In: ICML. 1998, pp. 144–151.
[19] Jason Fries, Sen Wu, Alex Ratner, and Christopher Ré. “Swellshark: A generative model for biomedical named entity recognition without labeled data”. In: arXiv preprint arXiv:1704.06360 (2017).
[20] Jason Alan Fries, E. Steinberg, S. Khattar, S. Fleming, J. Posada, A. Callahan, and N. Shah. “Ontology-driven weak supervision for clinical entity classiﬁcation in electronic health records”. In: Nature Communications 12 (2021).
[21] Jason Alan Fries, P. Varma, V. Chen, K. Xiao, H. Tejeda, Priyanka Saha, Jared A Dunnmon, H. Chubb, S. Maskatia, M. Fiterau, S. Delp, E. Ashley, Christopher Ré, and J. Priest. “Weakly supervised classiﬁcation of aortic valve malformations using unlabeled cardiac MRI sequences”. In: Nature Communications 10 (2019).
11

[22] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher Ré. “Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods”. In: ICML. 2020, pp. 3280–3291.
[23] Daniel Y. Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang, Anh Truong, Avanika Narayan, Maneesh Agrawala, Christopher Ré, and Kayvon Fatahalian. “Rekall: Specifying Video Events using Compositions of Spatiotemporal Labels”. In: arXiv preprint arXiv:1910.02993 (2019).
[24] Jonathan Fürst, Mauricio Fadel Argerich, Kalyanaraman Shankari, Gürkan Solmaz, and Bin Cheng. “Applying Weak Supervision to Mobile Sensor Data: Experiences with TransportMode Detection”. In: AAAI Workshop. 2020.
[25] Sainyam Galhotra, Behzad Golshan, and Wang-Chiew Tan. “Adaptive Rule Discovery for Labeling Text Data”. In: SIGMOD. 2021, pp. 2217–2225. ISBN: 9781450383431.
[26] Huiji Gao, Geoffrey Barbier, and Rebecca Goolsby. “Harnessing the Crowdsourcing Power of Social Media for Disaster Relief”. In: IEEE Intelligent Systems 26 (2011), pp. 10–14.
[27] Karan Goel, Laurel J. Orr, Nazneen Fatema Rajani, Jesse Vig, and Christopher Ré. “Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems”. In: NAACL-HLT. 2021, pp. 205–213.
[28] S. Gupta and Christopher D. Manning. “Improved Pattern Learning for Bootstrapped Entity Extraction”. In: CoNLL. 2014.
[29] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. “Co-teaching: Robust training of deep neural networks with extremely noisy labels”. In: NeurIPS. Vol. 31. 2018. URL: https://proceedings.neurips.cc/ paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf.
[30] Michael A Hedderich, Dawei Zhu, and Dietrich Klakow. “Analysing the Noise Model Error for Realistic Noisy Label Data”. In: AAAI. Vol. 35. 2021, pp. 7675–7684.
[31] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. “SemEval2010 task 8: Multi-way classiﬁcation of semantic relations between pairs of nominals”. In: Semeval. 2010, pp. 33–38.
[32] Geoffrey E. Hinton. “Training Products of Experts by Minimizing Contrastive Divergence”. In: Neural Comput. 14.8 (Aug. 2002), pp. 1771–1800. ISSN: 0899-7667. DOI: 10.1162/ 089976602760128018. URL: https://doi.org/10.1162/089976602760128018.
[33] Sepp Hochreiter and Jürgen Schmidhuber. “Long short-term memory”. In: Neural computation 9 (1997), pp. 1735–1780.
[34] R. Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. “KnowledgeBased Weak Supervision for Information Extraction of Overlapping Relations”. In: ACL. 2011.
[35] Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis Langlotz, and Christopher Re. “Cut out the annotator, keep the cutout: better segmentation with weak supervision”. In: ICLR. 2020.
[36] Chenghao Jia, Yongliang Shen, Yechun Tang, Lu Sun, and Weiming Lu. “Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data”. In: NAACL-HLT. 2021.
[37] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. “Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels”. In: ICML. Vol. 119. 2020, pp. 4804–4815. URL: http:// proceedings.mlr.press/v119/jiang20c.html.
[38] Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng, and Ahmed Hassan Awadallah. “Self-Training with Weak Supervision”. In: NAACL-HLT. 2021, pp. 845–863. DOI: 10.18653/v1/2021.naacl-main.66. URL: https://www.aclweb.org/anthology/ 2021.naacl-main.66.
[39] Saelig Khattar, Hannah O’Day, Paroma Varma, Jason Fries, Jennifer Hicks, Scott Delp, Helen Bronte-Stewart, and Chris Re. “Multi-frame weak supervision to label wearable sensor data”. In: ICML Time Series Workshop. 2019.
[40] Ron Kohavi. “Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree hybrid.” In: KDD. Vol. 96. 1996, pp. 202–207.
12

[41] Martin Krallinger, Obdulia Rabal, Saber A Akhondi, et al. “Overview of the BioCreative VI chemical-protein interaction Track”. In: BioCreative evaluation Workshop. Vol. 1. 2017, pp. 141–146.
[42] R. Krishna, Yuke Zhu, O. Groth, Justin Johnson, K. Hata, J. Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, D. Shamma, Michael S. Bernstein, and Li Fei-Fei. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations”. In: IJCV 123 (2016), pp. 32–73.
[43] Volodymyr Kuleshov, Jialin Ding, Christopher Vo, Braden Hancock, Alexander J. Ratner, Yang I. Li, C. Ré, S. Batzoglou, and M. Snyder. “A machine-compiled database of genomewide association studies”. In: Nature Communications 10 (2019).
[44] John D Lafferty, Andrew McCallum, and Fernando CN Pereira. “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data”. In: ICML. 2001, pp. 282– 289.
[45] Ouyu Lan, Xiao Huang, Bill Yuchen Lin, He Jiang, Liyuan Liu, and Xiang Ren. “Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling”. In: ACL. 2020, pp. 2134–2146. DOI: 10.18653/v1/2020.acl-main.193. URL: https://www.aclweb. org/anthology/2020.acl-main.193.
[46] Jiacheng Li, Haibo Ding, Jingbo Shang, Julian McAuley, and Zhe Feng. “Weakly Supervised Named Entity Tagging with Learnable Logical Rules”. In: ACL. 2021, pp. 4568–4581. DOI: 10.18653/v1/2021.acl-long.352. URL: https://aclanthology.org/2021.acllong.352.
[47] Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. “BioCreative V CDR task corpus: a resource for chemical disease relation extraction”. In: Database 2016 (2016).
[48] Junnan Li, Caiming Xiong, and Steven Hoi. “MoPro: Webly Supervised Learning with Momentum Prototypes”. In: ICLR. 2021. URL: https://openreview.net/forum?id=0EYBhgw80y.
[49] Xin Li and Dan Roth. “Learning question classiﬁers”. In: COLING. 2002.
[50] Yinghao Li, Pranav Shetty, Lucas Liu, Chao Zhang, and Le Song. “BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition”. In: ACL. 2021, pp. 6178–6190.
[51] Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao Zhang. “Bond: Bert-assisted open-domain named entity recognition with distant supervision”. In: KDD. 2020, pp. 1054–1064.
[52] Pierre Lison, Jeremy Barnes, and Aliaksandr Hubin. “skweak: Weak Supervision Made Easy for NLP”. In: arXiv preprint arXiv:2104.09683 (2021).
[53] Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, and Samia Touileb. “Named Entity Recognition without Labelled Data: A Weak Supervision Approach”. In: ACL. 2020, pp. 1518– 1533.
[54] Jingjing Liu, Panupong Pasupat, Scott Cyphers, and Jim Glass. “Asgard: A portable architecture for multilingual dialogue systems”. In: ICASSP. IEEE. 2013, pp. 8386–8390.
[55] Jingjing Liu, Panupong Pasupat, Yining Wang, Scott Cyphers, and Jim Glass. “Query understanding enhanced by hierarchical parsing structures”. In: Workshop on ASRU. IEEE. 2013, pp. 72–77.
[56] Kun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang, and Sheng Gao. “Noisy-Labeled NER with Conﬁdence Estimation”. In: NAACL. 2021, pp. 3437–3445.
[57] Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji, and Jiawei Han. “Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach”. In: EMNLP. 2017, pp. 46–56.
[58] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. “Roberta: A robustly optimized bert pretraining approach”. In: arXiv preprint arXiv:1907.11692 (2019).
[59] Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing Zhu, Songfang Huang, Rui Yan, and Dongyan Zhao. “Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix”. In: ACL. 2017, pp. 430–439. DOI: 10.18653/v1/P17-1040. URL: https://www.aclweb.org/anthology/P17-1040.
13

[60] Xuezhe Ma and Eduard Hovy. “End-to-end Sequence Labeling via Bi-directional LSTMCNNs-CRF”. In: ACL. 2016, pp. 1064–1074.
[61] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. “Learning Word Vectors for Sentiment Analysis”. In: ACL. 2011, pp. 142–150. URL: https://www.aclweb.org/anthology/P11-1015.
[62] Ayush Maheshwari, Oishik Chatterjee, Krishnateja Killamsetty, Ganesh Ramakrishnan, and Rishabh Iyer. “Semi-Supervised Data Programming with Subset Selection”. In: Findings of ACL. 2021, pp. 4640–4651.
[63] Neil Mallinar, Abhishek Shah, Rajendra Ugrani, Ayush Gupta, Manikandan Gurusankar, Tin Kam Ho, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy, Robert Yates, Chris Desmarais, and Blake McGregor. “Bootstrapping Conversational Agents with Weak Supervision”. In: AAAI. 2019, pp. 9528–9533. DOI: 10 . 1609 / aaai . v33i01 . 33019528. URL: https : //doi.org/10.1609/aaai.v33i01.33019528.
[64] Emily K. Mallory, Matthieu de Rochemonteix, Alexander J. Ratner, Ambika Acharya, Christoper M Re, R. Bright, and R. Altman. “Extracting chemical reactions from text using Snorkel”. In: BMC Bioinformatics 21 (2020).
[65] Gideon S. Mann and A. McCallum. “Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data”. In: J. Mach. Learn. Res. 11 (2010), pp. 955–984.
[66] Jose Mathew, Meghana Negi, Rutvik Vijjali, and Jairaj Sathyanarayana. “DeFraudNet: An End-to-End Weak Supervision Framework to Detect Fraud in Online Food Delivery”. In: ECML-PKDD. 2021.
[67] A. Mazzetto, C. Cousins, D. Sam, S. H. Bach, and E. Upfal. “Adversarial Multiclass Learning under Weak Supervision with Performance Guarantees”. In: ICML. 2021.
[68] A. Mazzetto, D. Sam, A. Park, E. Upfal, and S. H. Bach. “Semi-Supervised Aggregation of Dependent Weak Supervision Sources With Performance Guarantees”. In: AISTATS. 2021.
[69] Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. “Distant supervision for relation extraction without labeled data”. In: ACL. 2009, pp. 1003–1011. URL: https : / / www . aclweb.org/anthology/P09-1113.
[70] Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec. “Coresets for Robust Training of Deep Neural Networks against Noisy Labels”. In: NeurIPS 33 (2020).
[71] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. “When does label smoothing help?” In: Advances in Neural Information Processing Systems. Vol. 32. Curran Associates, Inc., 2019. URL: https : / / proceedings . neurips . cc / paper / 2019 / file / f1748d6b0fd9d439f71450117eba2725-Paper.pdf.
[72] An Thanh Nguyen, Byron Wallace, Junyi Jessy Li, Ani Nenkova, and Matthew Lease. “Aggregating and Predicting Sequence Labels from Crowd Annotations”. In: ACL. 2017, pp. 299–309. DOI: 10 . 18653 / v1 / P17 - 1028. URL: https : / / www . aclweb . org / anthology/P17-1028.
[73] Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, and Xuan-Jing Huang. “Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning”. In: ACL. 2019, pp. 2409–2419.
[74] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. “SemEval-2014 Task 4: Aspect Based Sentiment Analysis”. In: SemEval. 2014, pp. 27–35. DOI: 10.3115/v1/S14-2004. URL: https://www.aclweb. org/anthology/S14-2004.
[75] Nikitha Rao, Chetan Bansal, and Joe Guan. “Search4Code: Code Search Intent Classiﬁcation Using Weak Supervision”. In: MSR. ACM/IEEE. May 2021. URL: https://www. microsoft . com / en - us / research / publication / search4code - code - search intent-classification-using-weak-supervision/.
[76] A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Ré. “Training Complex Models with Multi-Task Weak Supervision”. In: AAAI. 2019, pp. 4763–4771.
[77] Alexander J Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. “Snorkel: Rapid training data creation with weak supervision”. In: VLDB. Vol. 11. 2017, p. 269.
[78] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Ré. “Data programming: Creating large training sets, quickly”. In: NeurIPS. Vol. 29. 2016, pp. 3567– 3575.
14

[79] Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao Zhang. “Denoising Multi-Source Weak Supervision for Neural Text Classiﬁcation”. In: Findings of EMNLP. 2020, pp. 3739–3754.
[80] Sebastian Riedel, Limin Yao, and Andrew McCallum. “Modeling relations and their mentions without labeled text”. In: ECML-PKDD. Springer. 2010, pp. 148–163.
[81] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. “A primer in bertology: What we know about how bert works”. In: TACL 8 (2020), pp. 842–866.
[82] Khaled Saab, Jared Dunnmon, Roger E. Goldman, Alexander Ratner, Hersh Sagreiya, Christopher Ré, and Daniel L. Rubin. “Doubly Weak Supervision of Deep Learning Models for Head CT”. In: MICCAI. 2019, pp. 811–819.
[83] Khaled Kamal Saab, Jared A Dunnmon, Christopher Ré, D. Rubin, and C. Lee-Messer. “Weak supervision as an efﬁcient approach for automated seizure detection in electroencephalography”. In: NPJ Digital Medicine 3 (2020).
[84] Esteban Safranchik, Shiying Luo, and Stephen Bach. “Weakly supervised sequence tagging from noisy rules”. In: AAAI. Vol. 34. 2020, pp. 5570–5578.
[85] Erik Tjong Kim Sang and Fien De Meulder. “Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition”. In: CoNLL. 2003, pp. 142–147.
[86] Jingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren, Teng Ren, and Jiawei Han. “Learning named entity tagger using domain-speciﬁc dictionary”. In: EMNLP. 2018, pp. 2054–2064.
[87] Kai Shu, Subhabrata (Subho) Mukherjee, Guoqing Zheng, Ahmed H. Awadallah, Milad Shokouhi, and Susan Dumais. “Learning with Weak Supervision for Email Intent Detection”. In: SIGIR. ACM. 2020.
[88] Kai Shu, Guoqing Zheng, Yichuan Li, Subhabrata (Subho) Mukherjee, Ahmed H. Awadallah, Scott Ruston, and Huan Liu. “Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News”. In: ECML-PKDD (2020).
[89] Shingo Takamatsu, Issei Sato, and H. Nakagawa. “Reducing Wrong Labels in Distant Supervision for Relation Extraction”. In: ACL. 2012.
[90] Erik F. Tjong Kim Sang. “Introduction to the CoNLL-2002 Shared Task: LanguageIndependent Named Entity Recognition”. In: COLING. 2002. URL: https://www.aclweb. org/anthology/W02-2024.
[91] P. Varma, Bryan D. He, Payal Bajaj, Nishith Khandwala, I. Banerjee, D. Rubin, and Christopher Ré. “Inferring Generative Model Structure with Static Analysis”. In: Advances in neural information processing systems 30 (2017), pp. 239–249.
[92] P. Varma, Frederic Sala, Ann He, Alexander J. Ratner, and C. Ré. “Learning Dependency Structures for Weak Supervision Models”. In: ICML. 2019.
[93] Paroma Varma and Christopher Ré. “Snuba: Automating weak supervision to label training data”. In: VLDB. Vol. 12. NIH Public Access. 2018, p. 223.
[94] Paroma Varma, Frederic Sala, Shiori Sagawa, Jason Fries, Daniel Fu, Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James Priest, and Christopher Ré. “Multi-Resolution Weak Supervision for Sequential Data”. In: NeurIPS. Vol. 32. 2019. URL: https : / / proceedings . neurips . cc / paper / 2019 / file / 93db85ed909c13838ff95ccfa94cebd9-Paper.pdf.
[95] Yanshan Wang, S. Sohn, Sijia Liu, F. Shen, Liwei Wang, E. Atkinson, S. Amin, and H. Liu. “A clinical text classiﬁcation paradigm using weak supervision and deep representation”. In: BMC Medical Informatics 19 (2019).
[96] Ralph Weischedel, Sameer Pradhan, Lance Ramshaw, Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, et al. “Ontonotes release 5.0”. In: Linguistic Data Consortium (2011).
[97] Lloyd R Welch. “Hidden Markov models and the Baum-Welch algorithm”. In: IEEE Information Theory Society Newsletter 53 (2003), pp. 10–13.
[98] Zhenzhen Weng, P. Varma, Alexander Masalov, Jeffrey M. Ota, and C. Ré. “Utilizing Weak Supervision to Infer Complex Objects and Situations in Autonomous Driving Data”. In: IEEE Intelligent Vehicles Symposium (2019), pp. 119–125.
[99] Shanchan Wu and Yifan He. “Enriching pre-trained language model with entity information for relation classiﬁcation”. In: CIKM. 2019, pp. 2361–2364.
15

[100] [101] [102]
[103] [104] [105] [106] [107]
[108] [109]

Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. “Learning From Massive Noisy Labeled Data for Image Classiﬁcation”. In: CVPR. 2015.
Yuan Yao, Ao Zhang, Xu Han, Mengdi Li, Cornelius Weber, Zhiyuan Liu, Stefan Wermter, and Maosong Sun. “Visual Distant Supervision for Scene Graph Generation”. In: arXiv preprint arXiv:2103.15365 (2021).
Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. “FineTuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach”. In: NAACL-HLT. 2021, pp. 1063–1077. URL: https : / / www . aclweb.org/anthology/2021.naacl-main.84.
Omar Zaidan and Jason Eisner. “Modeling Annotators: A Generative Approach to Learning from Annotator Rationales”. In: EMNLP. 2008.
Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. “Generating MultiAgent Trajectories using Programmatic Weak Supervision”. In: ICLR. 2019. URL: https: //openreview.net/forum?id=rkxw-hAcFQ.
Danqing Zhang, Zheng Li, Tianyu Cao, Chen Luo, Tony Wu, Hanqing Lu, Yiwei Song, Bing Yin, Tuo Zhao, and Qiang Yang. “QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction”. In: CIKM. 2021.
Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, and Shih-Fu Chang. “PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN”. In: ICCV. 2017.
Xiang Zhang, Junbo Zhao, and Yann LeCun. “Character-level Convolutional Networks for Text Classiﬁcation”. In: NeurIPS. 2015, pp. 649–657. URL: http://papers.nips. cc / paper / 5782 - character - level - convolutional - networks - for - text classification.pdf.
Xinyan Zhao, Haibo Ding, and Zhe Feng. “GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised Named Entity Recognition”. In: EACL. 2021, pp. 3636–3649. URL: https://aclanthology.org/2021.eacl-main.318.
Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, and Xiang Ren. “Nero: A neural rule grounding framework for label-efﬁcient relation extraction”. In: The Web Conference. 2020, pp. 2166–2176.

16

A Key Information
A.1 Dataset Documentations
The dataset is provided in json format; there are three json ﬁles corresponding to the train, validation and test split. Each data point contains the following ﬁelds:
• id: unique identiﬁer for the example; • label: the label of the example; • weal_labels: the output of the labeling functions; • data: a dictionary contains the raw data;
Details of each dataset can be found in App. B.
A.2 Intended Uses
WRENCH is intended for researchers in machine learning and related ﬁelds to innovate novel methods for the weak supervision problem and data scientists to apply machine learning algorithms which require manual annotations.
A.3 Hosting and Maintenance Plan
WRENCH codebase is hosted and version-tracked via GitHub. It will be permanently available under the link https://github.com/JieyuZ2/wrench. The download link of all the datasets can be found in the Github repository. WRENCH is a community-driven and open-source initiative. We are committed and has resources to maintain and actively develop WRENCH for at minimum the next ﬁve years. We plan to grow WRENCH by including new learning tasks and datasets. We welcome external contributors.
A.4 Licensing
We license our work using Apache 2.03. All the datasets are publicly released by previous work.
A.5 Author Statement
We the authors will bear all responsibility in case of violation of rights.
A.6 Limitations
Weak supervision is an increasing ﬁeld, and there are important tasks and datasets yet to be included in WRENCH. However, WRENCH is an ongoing effort and we plan to continuously include more datasets and tasks in the future.
A.7 Potential Negative Societal Impacts
WRENCH does not involve human subjects research and does not contain any personally identiﬁable information. Possible misuse may lead to negative outcomes, such as direct usage of the model predictions to detect spam message without prior rigorous validation of the model performance.
B Real-world Datasets
B.1 Detailed Statistics and Visualization
We provide the detailed statistics of real-world datasets in Table 5-6. We also visualize the dataset statistics in Fig. 5, where each value is normalized to [0, 1] range across datasets.
3https://www.apache.org/licenses/LICENSE-2.0
17

Table 5: Detailed statistics of classiﬁcation datasets included in WRENCH.

Task (↓)

Domain (↓) Dataset (↓)

Avr. over LFs

Train Dev Test

#Label #LF Ovr. %Coverage %Coverage %Overlap %Conﬂict %Accuracy #Data #Data #Data

Income Classiﬁcation Tabular Data Census [40, 3]

2

83

99.13

5.41

5.34

1.50

78.74

10,083 5,561 16,281

Sentiment Classiﬁcation Movie Review

IMDb [61, 79] Yelp [107, 79]

2

5

2

8

87.58 82.78

23.60

11.60

4.50

18.34

13.58

4.94

69.88 73.05

20,000 2,500 30,400 3,800

2,500 3,800

Spam Classiﬁcation

Review

Youtube [1]

Text Message SMS [2, 3]

2

10

2

73

87.70 40.52

16.34

12.49

7.14

0.72

0.29

0.01

83.16 97.26

1,586 120 250 4,571 500 2719

Topic Classiﬁcation

News

AGNews [107, 79]

4

9

69.08

10.34

5.05

2.43

81.66

96,000 12,000 12,000

Question Classiﬁcation Web Query TREC [49, 3]

6

68

95.13

2.55

1.82

0.84

75.92

4,965 500 500

News

Spouse [11, 77]

2

9

Relation Classiﬁcation

Biomedical Web Text

CDR [13, 77] SemEval [31, 109]

2

33

9

164

Chemical

ChemProt [41, 102] 10

26

25.77 90.72 100.00 85.62

3.75

1.66

0.65

–

22,254 2,811 2,701

6.27

5.36

3.21

75.27

8,430 920 4,673

0.77

0.32

0.14

97.69

1,749 200 692

5.93

4.40

3.95

46.65

12,861 1,607 1,607

Image Classiﬁcation

Video

Commercial [22] Tennis Rally [22] Basketball [22]

2

4

2

6

2

4

100.00 100.00 100.00

54.51 66.86 49.24

53.09 66.86 48.46

12.51 16.76 18.50

91.33 81.70 62.04

64,130 6,959 17,970

9,479 746 1,064

7,496 1,098 1,222

Table 6: Detailed statistics of sequence tagging datasets included in WRENCH.

Domain (↓) Dataset (↓)

Avr. over LFs #Label #LF Ovr. %Coverage %Coverage %Overlap %Conﬂict %Precision

Train #Data

Dev #Data

Test #Data

News

CoNLL-03 [85, 52]

4

16

79.51

23.71

4.30

1.44

72.19

14,041 3250 3453

Web Text

WikiGold [5, 52] OntoNotes 5.0 [96]

4

16

18

17

69.68 66.79

20.30

3.65

1.61

65.87

1,355 169 170

12.45

1.55

0.54

54.84

115,812 5,000 22,897

Biomedical BC5CDR [47, 50]

2

9

NCBI-Disease [16, 50]

1

5

86.62 77.15

16.75

1.77

0.17

88.23

500

500 500

21.16

1.40

0.18

74.88

592

99

99

Review

Laptop-Review [74, 50] 1

3

MIT-Restaurant [55, 3]

8

16

70.62 47.84

29.37

1.65

0.25

70.30

2,436 609 800

2.87

0.37

0.06

76.65

7,159 500 1,521

Movie Query MIT-Movies [54]

12

7

64.14

16.60

5.29

0.97

75.10

9,241 500 2,441

B.2 Classiﬁcation Datasets
Census [40]. This UCI dataset is extracted from the 1994 U.S. census. It lists a total of 13 features of an individual such as age, education level, marital status, country of origin etc. The primary task on it is binary classiﬁcation - whether a person earns more than 50K or not. The train data consists of 32,563 records. The labeling functions are generated synthetically by [3] as follows: We hold out disjoint 16k random points from the training dataset as a proxy for human knowledge and extract a PART decision list [18] from it as labeling functions.
SMS [2]. This dataset contains 4,571 text messages labeled as spam/not-spam, out of which 500 were held out for validation and 2719 for testing. The labeling functions are generated manually by [3], including 16 keyword-based and 57 regular expression-based rules.
AGNews [107]. This dataset is a collection of more than one million news articles. It is constructed by [79] choosing the 4 largest topic classes from the original corpus. The total number of training samples is 96K and both validation and testing are 12K. The labeling functions are also generated by [79], including 9 keyword-based rules.
Yelp [107]. This dataset is a subset of Yelp’s businesses, reviews, and user data for binary sentiment classiﬁcation. It is constructed by [79], including 30.4K training samples, 3.8K validation samples and 3.8K testing samples. The labeling functions are also generated by [79], including 7 heuristic rules on keywords and 1 third-party model on polarity of sentiment.
Youtube [1]. This dataset is a public set of comments collected for spam detection. It has ﬁve datasets composed of 1,586 real messages extracted from ﬁve videos. The number of validation samples is 120 and that of testing samples is 250. The labeling functions are generated manually by [77], including 5 keyword-based, 1 regular expression-based, 1 heuristic, 1 complex preprocessors, and 2 third-party model rules.
IMDb [61]. This is a dataset for binary sentiment classiﬁcation containing a set of 20,000 highly polar movie reviews for training, 2,500 for validation and 2,500 for testing. It is constructed by [79]. The labeling functions are also generated by [79], including 4 heuristic rules on keywords and 1 heuristic rules on expressions.
TREC [49]. This dataset contains 4,965 labeled questions in the training set, 500 for validation set and another 500 for the testing set. It has 6 classes. The labeling functions are generated by [3], including 68 keyword-based rules.
Spouse [11]. This dataset is constructed by [77] and to identify mentions of spouse relationships in a set of news articles from the Signal Media [11]. It contains 22,254 training samples 2,811 validation

18

Figure 5: Visualization of all statistics of datasets.
samples and 2,701 testing samples. The labeling functions are generated by Snorkel4. Note that the gold labels for the training set is not available. Therefore, we are unable to calculate the accuracy for labeling functions on the training set.
CDR [47]. This dataset is constructed by [77], where the task is to identify mentions of causal links between chemicals and diseases in PubMed abstracts. It has 8,430 training samples 920 validation samples and 4,673 testing samples. The labeling functions can be found in Snorkel tutorial5.
SemEval [31]. This relation classiﬁcation dataset is constructed by [109] with 9 relation types. The size of the training, validation and test set are 1,749, 200 and 692 respectively. The labeling functions are generated by [109], including 164 heuristic rules.
ChemProt [41]. This is a 10-way relation classiﬁcation dataset constructed by [102], containing 12,861 training samples, 1,607 validation samples and 1,607 testing samples. The labeling functions are generated by [102], including 26 keyword-based rules.
Basketball, Commercial, Tennis Rally [22]. These datasets are video frame classiﬁcation datasets collected by [22]. All the labeling functions are the same as previous work [22]. Due to privacy issues, we only have access to the features extracted by a ResNet-101 model pre-trained on ImageNet.
B.3 Sequence Tagging Datasets
CoNLL-03 [85]. This is a well-known open-domain NER dataset from the CoNLL 2003 Shared Task. It consists of 1393 English news articles and is annotated with 4 entity types: person, location, organization, and miscellaneous6. Note that different papers [73, 50, 53] use different weak supervision sources with varying quality. In our study, we use the labeling function generated by [53] for fair comparison. (We use BTC, core_web_md, crunchbase_cased, crunchbase_uncased, full_name_detector, geo_cased, geo_uncased, misc_detector, wiki_cased, wiki_uncased, multitoken_crunchbase_cased, multitoken_crunchbase_uncased, multitoken_geo_cased, multitoken_geo_uncased, multitoken_wiki_cased, multitoken_wiki_uncased as weak supervision sources)
BC5CDR [47]. This dataset accompanies the BioCreative V CDR challenge and consists of 1,500 PubMed articles and is annotated with chemical and disease mentions. The labeling functions are selected from [50]. (We use DictCore-Chemical, DictCore-Chemical-Exact, DictCore-Disease, DictCore-Disease-Exact, Element, Ion, or Isotope, Organic Chemical, Antibiotic, Disease or Syndrome, PostHyphen, ExtractedPhrase as weak supervision sources.)
NCBI-Disease [16]. This dataset includes 793 PubMed abstracts annotated with disease mentions only. The labeling functions are the same as [50].
Laptop-Review [74]. This dataset is from the SemEval 2014 Challenge, Task 4 Subtask 1 and consists of 3,845 sentences with laptop-related entity mentions. The labeling functions are selected from [50]. (We use CoreDictionary, ExtractedPhrase, ConsecutiveCapitals as weak supervision sources.)
4https://github.com/snorkel-team/snorkel-tutorials/tree/master/spouse 5https://github.com/snorkel-team/snorkel-extraction/tree/master/tutorials/cdr 6In the original dataset, it has -DOCSTART- lines to separate documents, but these lines are removed here.
19

Wikigold [5]. This dataset contains a set of Wikipedia articles (40k tokens) randomly selected

from a 2008 English dump and manually annotated with the four CoNLL-03 entity types. Since

the label type of Wikigold is the same as CoNLL-03, we also use the labeling function pro-

vided in [53]. (We use BTC, core_web_md, crunchbase_cased, crunchbase_uncased,

full_name_detector, geo_cased, geo_uncased, misc_detector, wiki_cased,

wiki_uncased, multitoken_crunchbase_cased, multitoken_crunchbase_uncased,

multitoken_geo_cased,

multitoken_geo_uncased,

multitoken_wiki_cased,

multitoken_wiki_uncased as weak supervision sources).

MIT-Restaurant [55]. This is a slot-ﬁlling dataset includeing sentences about restaurant search
queries. It contains 8 entity types with 9180 examples. We follow the data split by [3] and use regular
expression in their paper as weak supervision. Besides, we also extract restaurant names and cuisines from yelp database7 to augment the labeling function.8

MIT-Movies [54]. This dataset includes sentences on movie search queries with 12 entity types. For this dataset, we curate the weak supervision via several class-related keywords, semantic patterns based on regular expressions (listed in table 7) and knowledge-base matching. Speciﬁcally, we collect the movie-related information on JsonMC9, Movies-dataset10, IMDB11 and Wikidata12. There are 7 weak supervision sources in total.

Ontonotes 5.0 [96]. This is a ﬁne-grained NER dataset with text documents from multiple domains, including broadcast conversation, P2.5 data and Web data. It consists of 113 thousands of training data and is annotated with 18 entity types. We adopt a set of the weak supervision sources presented in Skweak13 [52] including money_detector, date_detector, number_detector, company_type_detector, full_name_detector, crunchbase_cased, crunchbase_uncased, geo_cased, geo_uncased, misc_detector, wiki_cased, wiki_uncased (12 in total). Since some of the weak supervision sources in Skweak only have coarse-level annotation (e.g. they can only label for entity types listed in CoNLL-03: person, location, organization, and miscellaneous), we follow the method used in [51] to use SPARQL to query the categories of an entity in the knowledge ontology in wikipedia. Apart from it, we also extracted multi-token phrases from wikidata knowledge base and match with the corpus. Finally, we include several class-related keywords, and regular expressions (listed in table 8) as the labeling functions. As a result, there are 17 weak supervision sources. To control the size for the validation set, we only use the ﬁrst 5000 sentences as the validation set and put others into the test set.

C Compared Methods

C.1 Classiﬁcation
C.1.1 Label Model
MV / WMV: We adopt the classic majority voting (MV) algorithm as one label model, as well as its extension weighted majority voting (WMV) where we reweight the ﬁnal votes by the label prior. Notably, the abstaining LF, i.e., λj = −1 won’t contribute to the ﬁnal votes.
DS [14]: Dawid-Skene (DS) model estimates the accuracy of each LF with expectation maximization (EM) algorithm by assuming a naive Bayes distribution over the LFs’ votes and the latent ground truth.
DP [78]: Data programming (DP) models the distribution p(L, Y ) as a factor graph. It is able to describe the distribution in terms of pre-deﬁned factor functions, which reﬂects the dependency of any subset of random variables. The log-likelihood is optimized by SGD where the gradient is estimated by Gibbs sampling, similarly to contrastive divergence [32].

7https://www.yelp.com/dataset 8In [3, 38, 102], they treat this dataset as token-level classiﬁcation problem and use token-level F1 score for
evaluation, which is in conﬂict with the original evaluation protocol of the dataset [55]. 9https://github.com/jsonmc/jsonmc 10https://github.com/randfun/movies-dataset 11https://www.imdb.com/ 12https://www.wikidata.org/ 13https://github.com/NorskRegnesentral/skweak

20

Labeling Functions Table 7: Examples of labeling functions on MIT-Movies.

LF #1: Key words:

[‘tom hanks’, ‘jennifer lawrence’, ‘tom cruises’, ‘tom cruse’, ‘clint eastwood’, ‘whitney houston’, ‘leonardo dicaprio’, ‘jennifer aniston’, ‘kristen stewart’] → ACTOR [‘’(18|19|20)\d2’, ‘’(18|19|20)\d2s’, ‘last two years’, ‘last three years’] → YEAR [‘batman’, ‘spiderman’, ‘rocky’, ‘twilight’, ‘titanic’, ‘harry potter’, ‘ice age’, ‘speed’, ‘transformers’, ‘lion king’, ‘pirates of the caribbean’]→ TITLE
[‘i will always love you’, ‘endless love’, ‘take my breath away’, ‘hes a tramp’, ‘beatles’, ‘my heart will go on’, ‘theremin’, ‘song’, ‘music’] → SONG [‘comedy’,‘comedies’, ‘musical’, ‘romantic’, ‘sci fi’, ‘horror’, ‘cartoon’, ‘thriller’, ‘action’, ‘documentaries’, ‘historical’, ‘crime’, ‘sports’, ‘classical’]→ GENRE [‘x’, ‘g’, ‘r’, ‘pg’, ‘rated (x|g|r)’, ‘pg rated’, ‘pg13’, ‘nc17’] → RATING [‘(five|5) stars’, ‘(highest|lowest) rated’, ‘(good|low) rating’, ‘mediocre’] → RATINGS_AVERAGE [‘classic’, ‘recommended’, ‘top’, ‘review’, ‘awful’, ‘reviews’, ‘opinions’, ‘say about’, ‘saying about’, ‘think about’] → REVIEW [‘trailers’, ‘trailer’, ‘scene’, ‘scenes’, ‘preview’, ‘highlights’] → TRAILER [‘007’, ‘james bond’, ‘ron weasley’, ‘simba’, ‘peter pan’, ‘santa clause’, ‘mr potato head’, ‘buzz lightyear’, ‘darth bader’, ‘yoda’, ‘dr evil’] → CHARACTER

LF #2: Context Patterns (The NOUN matched in * will be recognized as the entity with the target type):

[‘movie named *’, ‘film named *’, ‘movie called’, ‘film called *’, ‘movie titled *’,

‘film titled *’] → CHARACTER

[‘(starring|starred|stars) *’, ‘featured *’] → ACTOR

[‘(about|about a|about the)

*’,‘set in *’] → PLOT

[‘directed by *’,‘produced by *’] → DIRECTOR

[‘a rating of *’] → RATINGS_AVERAGE

[‘during the *’,‘in the *’] → YEAR

[‘the song *’,‘a song *’] → SONG

[‘pg *’,‘nc *’] → RATING

LF #3: Regex Patterns (The entity matched in underlined * will be recognized as the entity with the target

type):

[’(is|was|has) [\w\s]* (ever been in|in|ever in) (a|an) [\w]* (movie|film)’] → ACTOR [’(show me|find) (a|the) [^\w]* (from|for)’] → TRAILER [‘(a|an) [\w]* (movie|film)’] → GENRE [‘(did|does) [^\w|\s]* direct’] → DIRECTOR [‘(past|last) ([0-9]*|[^\w]) years’] → YEAR [^\d{1} stars"] → RATINGS_AVERAGE

MeTaL [76]: MeTal models the distribution via a Markov Network and recover the parameters via a matrix completion-style approach. Notably, it requires label prior as input.
FS [22]: FlyingSquid (FS) models the distribution as a binary Ising model, where each LF is represented by two random variables. A Triplet Method is used to recover the parameters and therefore no learning is needed, which makes it much faster than data programming and MeTal. Notably, FlyingSquid is designed for binary classiﬁcation and the author suggested applying a one-versus-all reduction repeatedly to apply the core algorithm. The label prior is also required.
C.1.2 End Model
LR: We choose Logistic Regression (LR) as an example of linear model.
MLP: For non-linear model, we take Multi-Layer Perceptron Neural Networks (MLP) as an example.
BERT [15] / RoBERTa [58]: It is also interested how recent large scale pretrained language models perform as end models for textual datasets, so we include both BERT[15] and RoBERTa [58], shortened as B and R respectively. Notably, these pretrained language models can only work for textual datasets, and for text relation classiﬁcation task, we adopt the R-BERT [99] architecture.

21

Labeling Functions Table 8: Examples of labeling functions on Ontonotes.
LF #1: Key words:
[‘Taiwan’, ‘Hong Kong’, ‘China’, ‘Japan’, ‘America’, ‘Germany’, ‘US’, ‘Singapore"] → GPE [‘World War II’, ‘World War Two’,’nine eleven’, ‘the Cold War’, ‘World War I’, ‘Cold War’, ‘New Year’, ‘Chinese New Year’, ‘Christmas"] → EVENT [‘the White House’, ‘Great Wall’, ‘Tiananmen Square’, ‘Broadway"]→ FAC
[ ‘Chinese’, ‘Asian’, ‘American’, ‘European’, ‘Japanese’, ‘British’, ‘French’, ‘Republican"] → NORP [‘English’, ‘Mandarin’, ‘Cantonese"]→ LANG [‘Europe’, ‘Asia’, ‘North America’, ‘Africa‘, ‘South America’] → LOC [‘WTO’, ‘Starbucks’, ‘mcdonald’, ‘google’, ‘baidu’, ‘IBM’, ‘Sony’, ‘Nikon’] → ORG [’toyota’, ‘discovery’, ‘Columbia’, ‘Cocacola’, ‘Delta’, ‘Mercedes’, ‘bmw’] → PRODUCT [‘this year’, ‘last year’, ‘last two years’, ‘last three years’, ‘recent years’, ‘today’, ‘tomorrow’, ‘(18|19|20)\{d}2’] → DATE [‘(this|the|last) (morning|evening|afternoon|night)’] → TIME [·first’, ‘second’, ‘third‘, ‘fourth‘, ‘fifth’, ‘sixth’, ‘seventh‘, ‘ninth’, ‘firstly’, ‘secondly’, ‘1 st’, ‘2 nd’, ‘(3|...|9) th’] → ORDINAL
LF #2: Numerical Patterns (The numerical value end with the terms in the list below will be recognized as the entity with the target type):
‘dollars’, ‘dollar’, ‘yuan’, ‘RMB’, ‘US dollar’, ‘Japanese yen’, ’HK dollar’, ‘Canadian dollar’, ‘Australian dollar’, ‘lire’, ‘francs’] → MONEY [‘the (18|19|20)\{d}2s’, ‘the (past|last|previous) (one|two|three|four|five|six|seven|eight|nine|ten) (decade|day|month|year)’, ‘the [0-9\w]+ century’] → DATE [‘tons’, ‘tonnes’, ‘barrels’, ‘m’, ‘km’, ‘mile’, ‘miles’, ‘kph’, ‘mph’, ‘kg’, ‘°C’, ‘dB’, ‘ft’, ‘gal’, ‘gallons’, ‘g’, ‘kW’, ‘s’, ‘oz’, ‘m2’, ‘km2’, ‘yards’, ‘W’, ‘kW’, ‘kWh’, ‘kWh/yr’, ‘Gb’, ‘MW’, ‘kilometers’,"square meters’, ‘square kilometers’, ‘meters’, ‘liters’, ‘litres’, ‘g’, ‘grams’, ‘tons/yr’, ’pounds’, ’cubits’, ’degrees’, ’ton’, ’kilograms’, ’inches’, ’inch’, ’megawatts’, ’metres’, ’feet’, ’ounces’, ’watts’, ’megabytes’, ’gigabytes’, ’terabytes’, ’hectares’, ’centimeters’, ’millimeters’] → QUANTITY [‘year’, ‘years’, ‘ago’, ‘month’, ‘months’, ‘day’, ‘days’, ‘week’, ‘weeks’, ‘decade’] → DATE [‘%’, ’percent’] → PERCENT [‘AM’, ‘PM’, ‘p.m’, ‘a.m’, ‘hours’, ‘minutes’, ‘EDT’, ‘PDT’, ‘EST’, ‘PST"] → TIME
LF #3: Regex Patterns (The term matched in * will be recognized as the entity with the target type):
‘(NT|US)$ [\w,]* (billion|million|trillion)’, ‘[0-9.]+ (billion yuan|yuan|million yuan)’,"[0-9.]+ (billion yen|yen|million yen)’, ‘[0-9.]+ (billion US dollar|US dollar|million US dollar)’,"[0-9.]+ (billion HK dollar|HK dollar|million HK dollar)’, ‘[0-9.]+ (billion francs|francs|million francs)’, ‘[0-9.]+ (billion cent|cent|million cent)’, ‘[0-9.]+ (billion marks|marks|million marks)’, ‘[0-9.]+ (billion Swiss francs|Swiss francs|million Swiss francs)’, ‘$ [0-9.,]+ (billion|million|trillion)’, ‘about $ [0-9.,]+"] → MONEY [ ‘the (18|19|20)\d2s’, ‘the (past|last|previous) (one|two|three|four|five|six|seven|eight|nine|ten) (decade|day|month|year)’, ‘the [0-9\w]+ century"] → DATE [‘(t|T)he [\w ]+ Standard’, ‘(t|T)he [\w ]+ Law’, ‘(t|T)he [\w ]+ Act’, ‘(t|T)he [\w ]+ Constitution’,‘set in *’] → LAW [‘[0-9.]+ (%|percent)’] → PERCENT [‘\d{1}:\d{2} (am|a.m.|pm|p.m.)’, ‘\d{2}:\d{2} (am|a.m.|pm|p.m.)’] → TIME
COSINE [102]: COSINE uses self-training and contrastive learning to bootstrap over unlabeled data for improving a pretrained language model-based end model. We denote the BERT-based COSINE and the RoBERTa-based COSINE by BC and RC respectively.
C.1.3 Joint Model
Denoise [79]: Denoise adopts an attention network to aggregate over weak labels, and use a neural classiﬁer to leverage the data feature. These two components are jointly trained in an end-to-end manner.
22

C.2 Sequence Tagging
C.2.1 Label Models
HMM [53]: Hidden Markov models [53] represent true labels as latent variables and inferring them from the independently observed noisy labels through unsupervised learning with expectationmaximization algorithm [97].
CHMM [50]: Conditional hidden Markov model (CHMM) [50] substitutes the constant transition and emission matrices by token-wise counterpart predicted from the BERT embeddings of input tokens. The token-wise probabilities are representative in modeling how the true labels should evolve according to the input tokens.
C.2.2 End Model
LSTM-CNNs-CRF [60]: LSTM-CNNs-CRF encodes character-level features with convolutional neural networks (CNNs), and use bi-directional long short-term memory (LSTM) network [33] to model word-level features. A conditional random ﬁeld (CRF) layer [44] is stacked on top of LSTM to impose constraints over adjacent output labels.
BERT: It use the pre-trained BERT [15] to leverage the pre-trained context knowledge stored in BERT. In our experiments, we use BERT-base-cased for other datasets as our encoder, and we stack a linear layer to predict token labels. We also run experiment on stacking a CRF layer on the top of the model, and report the best performance.
C.2.3 Joint Model
ConNet [45]: Consensus Network (ConNet) adopts a two-stage training approach for learning with multiple supervision signals. In the decoupling phase, it trains BiLSTM-CNN-CRF [60] with multiple parallel CRF layers for each labeling source individually. Then, the aggregation phase aggregates the CRF transitions with attention scores and outputs a uniﬁed label sequence.

D Adapting Label Model for Sequence Tagging Problem

D.1 Label Correction Technique

One of the main difference between sequence tagging and classiﬁcation is that for sequence tagging, there is a speciﬁc type ‘O’ which indicates the token does not belong to any pre-deﬁned types. Therefore, if one token cannot be matched with all labeling functions, it will be automatically labeled as type ‘O’. In our study, we use a label correction technique to differentiate the type ‘O’ with Abstain as follows:

li,c  li,c = Abstain (-1) O

if Li,c = O; if Li,c = O and ∃ c ∈ [1, n] s.t. Li,c = O; otherwise

We have also tried on another choice that regard the weak label for tokens that cannot be matched with any labeling functions as ‘O’. The comparison of results is in table 9.
From the table, it is clear that when regarding unmatched token as type O, it leads to a drastic decrease in the ﬁnal performance. Speciﬁcally, the recall of the model is much lower since most of the tokens will be recognized as O when without label modiﬁcation. One exception is in DS method, as it achieve better performance on 4 out of 8 datasets without label modiﬁcation. However, when without label modiﬁcation is better, the performance gain between the two methods is between 0.56% – 5.66% in terms of F1 score. In contrast, when using label modiﬁcation is better, the gain on F1 score is much larger, i.e., between 4.76% – 42.34%. Therefore, using label modiﬁcation is generally a better way to adapt label models for classiﬁcation to sequence tagging problems, and we use this technique in our experiments by default.

D.2 Comparision of IO and BIO Tagging Schema

We also compare the performance of label model with IO and BIO tagging scheme, as both of them have been adopted in previous studies [84, 53, 50]. The comparision result is shown in table 9. From the result, we ﬁnd that for datasets that when the number of entity types is small (e.g. BC5CDR,

23

Table 9: Sequence Tagging. The comparison of label models with or without label modiﬁcation and IO/BIO tagging scheme. The number stands for the F1 score (Precision, Recall) with standard deviation in the bracket under each value. Each metric value is averaged over 5 runs.

End Model (↓) IO Schema, with Label Modiﬁcation BIO Schema, with Label Modiﬁcation IO Schema, without Label Modiﬁcation

Label Model (↓) MV WMV DS DP
MeTaL FS MV
WMV DS DP
MeTaL FS MV
WMV DS DP
MeTaL FS

CoNLL-03
60.36(59.06/61.72) (0.00)
60.26(59.03/61.54) (0.00)
46.76(45.29/48.32) (0.00)
62.43(61.62/63.26) (0.22)
60.32(59.07/61.63) (0.08)
62.49(63.25/61.76) (0.00)
61.73 (59.70/63.89) (0.00)
60.93 (58.46/63.62) (0.00)
47.29 (45.52/49.20) (0.00)
63.62 (61.83/65.52) (0.10)
61.69 (59.57/63.95) (0.08)
61.97 (62.34/61.59) (0.00)
8.10 (85.41/4.25) (0.00)
0.00 (0.00/0.00) (0.00)
49.24 (49.84/48.65) (0.00)
7.74 (84.50/4.05) (0.00)
6.59 (88.72/3.42) (0.08)
50.77 (81.11/36.95) (0.00)

WikiGold
52.24(48.95/56.00) (0.00)
52.87(50.74/55.20) (0.00)
42.17(40.05/44.53) (0.00)
54.81(53.10/56.64) (0.13)
52.09(50.31/54.03) (0.23)
58.29(62.77/54.40) (0.00)
55.30 (51.02/59.73) (0.00)
54.54 (50.57/59.20) (0.00)
40.82 (37.50/44.80) (0.00)
55.40 (51.86/59.46) (0.07)
54.63 (51.07/58.73) (0.04)
57.21 (61.73/53.33) (0.00)
8.14 (88.89/4.27) (0.00)
0.00 (0.00/0.00) (0.00)
41.38 (42.86/40.00) (0.00)
8.65 (94.44/4.53) (0.00)
7.57 (92.50/3.95) (0.04)
45.44 (83.57/31.20) (0.00)

BC5CDR
83.49(91.69/76.64) (0.00)
83.49(91.66/76.66) (0.00)
83.49(91.66/76.66) (0.00)
83.50(91.69/76.65) (0.00)
83.50(91.66/76.67) (0.00)
56.71(88.03/41.83) (0.00)
81.71 (88.22/76.09) (0.00)
80.45 (86.04/75.54) (0.00)
81.54 (87.32/76.48) (0.00)
83.08 (90.16/77.03) (0.09)
80.64 (86.44/75.65) (0.12)
56.77 (91.29/41.19) (0.00)
0.04 (100.00/0.02) (0.00)
0.00 (0.00/0.00) (0.00)
71.84 (89.25/60.12) (0.00)
0.04 (100.00/0.02) (0.00)
0.01 (20.00/0.00) (0.00)
19.27 (88.91/10.80) (0.00)

NCBI-Disease
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
40.67(72.24/28.30) (0.00)
72.37 (82.74/64.30) (0.00)
76.50 (89.61/66.84) (0.00)
77.54 (91.15/67.47) (0.00)
76.69 (89.56/67.05) (0.03)
76.37 (89.26/66.73) (0.03)
42.66 (88.67/28.09) (0.00)
6.68 (80.49/3.48) (0.00)
0.00 (0.00/0.00) (0.00)
56.69 (83.23/42.98) (0.00)
6.73 (100.00/3.48) (0.00)
6.73 (100.00/3.48) (0.00)
42.87 (93.93/27.77) (0.00)

Laptop-Review
73.27(88.86/62.33) (0.00)
73.27(88.86/62.33) (0.00)
73.27(88.86/62.33) (0.00)
73.27(88.86/62.33) (0.00)
64.36(83.21/53.63) (17.81)
28.74(60.59/18.84) (0.00)
67.43 (79.01/58.80) (0.00)
62.09 (70.57/55.43) (0.00)
72.25 (86.53/62.02) (0.00)
62.15 (70.70/55.44) (0.00)
64.79 (77.84/55.43) (0.01)
60.89 (73.01/52.20) (0.00)
29.71 (70.29/18.84) (0.00)
0.46 (30.00/0.46) (0.00)
29.91 (77.56/18.53) (0.00)
30.45 (79.35/18.84) (0.00)
30.20 (78.71/18.68) (0.00)
29.81 (78.95/18.38) (0.00)

MIT-Restaurant
48.71(74.25/36.24) (0.00)
48.19(73.73/35.80) (0.00)
46.81(71.71/34.75) (0.00)
47.92(73.24/35.61) (0.00)
47.66(73.40/35.29) (0.00)
13.86(84.10/7.55) (0.00)
47.55 (72.91/35.29) (0.00)
47.07 (72.33/34.91) (0.00)
35.60 (52.71/26.88) (0.00)
46.93 (72.15/34.78) (0.02)
46.69 (72.22/34.50) (0.03)
13.29 (86.31/7.20) (0.00)
0.00 (0.00/0.00) (0.00)
0.00 (0.00/0.00) (0.00)
41.26 (77.45/28.12) (0.00)
0.00 (0.00/0.00) (0.00)
0.00 (0.00/0.00) (0.00)
0.00 (0.00/0.00) (0.00)

MIT-Movies
59.68(69.92/52.05) (0.00)
60.37(70.98/52.52) (0.00)
54.06(63.64/46.99) (0.00)
59.92(70.65/52.01) (0.43)
56.60(72.28/47.70) (7.71)
43.04(77.73/29.75) (0.00)
59.78 (70.38/51.95) (0.00)
60.22 (70.88/52.35) (0.00)
55.86 (65.76/48.54) (0.00)
60.14 (70.60/52.39) (0.05)
60.24 (70.90/52.37) (0.11)
42.27 (77.22/29.09) (0.00)
8.93 (81.29/4.72) (0.00)
0.00 (0.00/0.00) (0.00)
51.10 (73.27/39.24) (0.00)
8.73 (83.10/4.61) (0.00)
8.17 (82.22/4.30) (0.00)
26.36 (83.18/15.66) (0.00)

Ontonotes 5.0
58.85(54.17/64.40) (0.00)
57.58(53.15/62.81) (0.00)
37.70(34.33/41.82) (0.00)
61.85(57.44/66.99) (0.19)
58.27(54.10/63.14) (0.48)
5.31(2.87/35.74) (0.00)
57.89 (52.68/64.24) (0.00)
56.81 (51.85/62.84) (0.00)
39.25 (36.55/42.39) (0.00)
61.66 (56.55/67.85) (0.04)
58.75 (53.65/62.68) (0.38)
8.01 (4.60/31.06) (0.00)
0.00 (0.00/0.00) (0.00)
0.00 (0.00/0.00) (0.00)
40.83 (42.68/39.14) (0.00)
0.00 (0.00/0.00) (0.00)
0.00 (0.00/0.00) (0.00)
27.28 (70.04/16.94) (0.00)

NCBI-Disease), using IO schema leads to higher F1 score. For other datasets, there is no clear winner, as IO schema excels on Ontonotes and MIT-Restaurants datasets while BIO performs better on the others. To conclude, the optimal tagging scheme is highly data-dependent and we use IO tagging schema in our experiments.
E Implementation Details
E.1 Hardware and Implementation
Our models are implemented based on Python and PyTorch. For gradient-based optimization, we adopt AdamW Optimizer and linear learning rate scheduler; and we early stop the training process based on the evaluation metric values on validation set. For all the compared methods, we either re-implement them based on ofﬁcial released code or create an interface for calling their ofﬁcial implementations. For ﬁne-tuning pre-trained language models, we use the dumps provided by HuggingFace14.
We use a pre-trained BERT model15 to extract features for textual classiﬁcation datasets. For text classiﬁcation dataset, we use the outputting embedding of the [CLS] token as data feature; for relation classiﬁcation, we follow the R-BERT [99] to use the concatenation of embeddings of [CLS] and the two entity tokens as data feature. Other features, e.g., TF-IDF feature, or other pre-trained language models are also supported in WRENCH.
All experiments are run on CPUs or 64 Nvidia V100 GPUs (32GB VRAM) on Microsoft Azure.
E.2 Hyper-parameter Search Space
For each model, we use grid search to ﬁnd the best hyer-parameters on validation set. For each trial, we repeat 3 runs with different initializations and for ﬁnal evaluation, we repeat 5 runs with different initializations. The search space is based on the suggestions in original paper and can be found in Table 10.
E.3 Parameters for studies in Sec. 5
Fig. 3 (a) We generate 10 labeling functions; 5 for positive label and 5 for negative label. The mean accuracy, mean propensity and radius of propensity is set to 0.75, 0.1, 0.0, respectively.
Fig. 3 (b) We generate 10 labeling functions; 5 for positive label and 5 for negative label. The mean accuracy, radius of accuracy and radius of propensity is set to 0.75, 0.1, 0.0, respectively.
14https://huggingface.co/models 15https://huggingface.co/bert-base-cased

24

Fig. 4 The minimum propensity of candidate LFs is 0.1. The minimum accuracy is set to be the label prior plus 0.1, e.g., for LFs labeling positive label, the minimum accuracy is P (y = 1) + 0.1. For (n, m)-gram features, n is set to 1 and m is 2.
F Additional Results
F.1 Classiﬁcation The detailed comparisons over the collected classiﬁcation datasets are in Table 11-12. F.2 Sequence Tagging The detailed comparisons over the collected sequence tagging datasets are in Table 13.
25

Table 10: The hyper-parameters and search space. Note that the ConNet shares search space of other parameters with its backbone, i.e., LSTM-CRF/BERT-CRF.

Model MeTal DP LogReg MLP BERT COSINE
Denoise
LSTM-CRF
BERT-CRF HMM CHMM ConNet

Hyper-parameter
lr weight_decay num_epoch
lr weight_decay num_epoch
batch_size lr weight_decay
batch_size lr weight_decay ffn_num_layer ffn_hidden_size
batch_size lr
batch_size lr weight_decay T ξ λ µ γ
batch_size lr weight_decay alpha c1 c2 c3 ffn_num_layer ffn_hidden_size
batch_size lr weight_decay dropout word_feature_extractor word_embed_dimension LSTM/GRU_hidden_size num_hidden_layer LSTM/GRU_hidden_size num_hidden_layer char_feature_extractor char_embed_dimension
batch_size lr lr_crf weight_decay weight_decay_crf
γ num_epoch
batch_size nn_lr hmm_lr num_pretrain_epoch num_epoch
n_steps_phase1

Description
learning rate weight decay the number of training epochs
learning rate weight decay the number of training epochs
the input batch_size learning rate weight decay
the input batch_size learning rate weight decay the number of MLP layers the hidden size of MLP layers
the input batch_size learning rate
the input batch_size learning rate weight decay the period of updating model the conﬁdent threshold the weight for conﬁdent regularization the weight for contrastive regularization the margin for contrastive regularization
the input batch_size learning rate weight decay momentum term for temporal ensembling coefﬁcient of denoiser loss coefﬁcient of classiﬁer loss coefﬁcient of unsupervised self-training loss the number of MLP layers the hidden size of MLP layers
the input batch_size learning rate weight decay dropout ratio the word feature extractor layers the embedding dimension of word the hidden size of LSTM/GRU layers the number of LSTM/GRU layers the hidden size of LSTM/GRU layers the number of LSTM/GRU layers the character feature extractor layers the embedding dimension of character
the input batch_size learning rate learning rate for the CRF layer weight decay weight decayfor the CRF layer
redundency factor the number of training epochs
the input batch_size learning rate of NN learning rate of HMM the number of pre-training epochs the number of training epochs
the number of training steps of phase1

Range
1e-5,1e-4,1e-3,1e-2,1e-1 1e-5,1e-4,1e-3,1e-2,1e-1 5,10,50,100,200
1e-5,5e-5,1e-4 1e-5,1e-4,1e-3,1e-2,1e-1 5,10,50,100,200
32,128,512 1e-5,1e-4,1e-3,1e-2,1e-1 1e-5,1e-4,1e-3,1e-2,1e-1
32,128,512 1e-5,1e-4,1e-3,1e-2,1e-1 1e-5,1e-4,1e-3,1e-2,1e-1 2 100
16,32 2e-5,3e-5,5e-5
32 1e-6,1e-5 1e-4 50,100,200 0.2,0.4,0.6,0.8 0.01,0.05,0.1 1 1
32,128,512 1e-4,1e-3,1e-2 0.0 0.6 0.1,0.3,0.5,0.7,0.9 0.1,0.3,0.5,0.7,0.9 1-c2-c1 2 100
16,32,64 1e-2,5e-3,1e-3 1e-8 0.0,0.5 LSTM,GRU 100 200 1 200 1 CNN 30
16,32,8 2e-5,3e-5,5e-5 1e-3,5e-3,1e-2 1e-6 1e-8
0,0.1,0.3,0.5,0.7,0.9 50
16,64,128 1e-3,5e-4,1e-4 1e-2,5e-3,1e-3 2,5 50
200,500,1000

26

Table 11: Classiﬁcation: detailed comparison. Each metric value is averaged over 5 runs. Underline indicates using soft label for training end model. red and blue indicate the best and second best result for each end model respectively, and gray is the best weak supervision method in this table.

End

Label

Model (↓) Model (↓)

MV

WMV

–

DS

DP

MeTaL FS Gold MV

MV

WMV

WMV

LR

DS

DS

DP

DP

MeTaL

MeTaL

FS FS Gold MV

MLP

MV WMV WMV
DS

DS

DP DP MeTaL

MeTaL FS

FS

Denoise

IMDb (Acc.)
71.04 (0.00)
71.04 (0.00)
70.60 (0.00)
70.96 (0.00)
70.96 (0.59)
70.36 (0.00)
81.56 (0.20)
76.93 (0.45)
77.26 (0.14)
76.63 (0.22)
77.03 (0.38)
76.54 (0.30)
77.15 (0.36)
76.90 (0.61)
76.86 (0.27)
76.30 (0.28)
77.18 (0.20)
76.74 (0.79)
76.84 (0.34)
81.79 (0.32)
77.14 (0.13)
77.10 (0.37)
76.66 (0.40)
76.90 (0.24)
76.64 (0.37)
77.18 (0.38)
76.42 (0.51)
76.77 (0.38)
76.35 (0.37)
77.61 (0.36)
76.78 (15.99)
77.35 (0.42)
76.22 (0.37)

Yelp (Acc.)
70.21 (0.00)
68.50 (0.00)
71.45 (0.00)
69.37 (0.03)
68.30 (0.43)
68.68 (0.00)
89.16 (0.27)
86.21 (0.27)
86.33 (0.19)
85.23 (0.21)
86.11 (0.20)
85.43 (0.20)
85.91 (0.14)
85.38 (0.33)
84.98 (0.36)
86.32 (0.22)
86.41 (0.22)
86.63 (0.17)
86.48 (0.29)
89.19 (0.31)
84.24 (1.19)
84.91 (1.28)
79.17 (5.31)
85.45 (1.21)
86.00 (0.29)
86.06 (0.23)
83.98 (1.88)
80.91 (1.56)
85.61 (0.54)
85.19 (0.16)
84.50 (1.33)
83.95 (0.81)
71.56 (15.80)

Youtube (Acc.)
84.00 (0.00)
78.00 (0.00)
83.20 (0.00)
82.00 (2.02)
84.00 (0.00)
76.80 (0.00)
94.24 (0.41)
90.72 (1.42)
93.36 (0.93)
88.80 (0.25)
92.64 (0.41)
88.32 (0.82)
88.88 (0.93)
90.00 (0.80)
89.92 (0.93)
89.84 (0.78)
88.00 (2.01)
87.68 (0.78)
88.72 (0.53)
94.00 (0.44)
89.44 (0.74)
90.16 (0.60)
88.16 (0.86)
92.48 (0.16)
88.00 (0.98)
87.44 (0.82)
90.00 (0.25)
90.08 (1.11)
88.88 (1.30)
87.44 (0.90)
86.32 (1.35)
85.20 (0.91)
76.56 (19.24)

SMS (F1)
23.97 (0.00)
23.97 (0.00)
4.94 (0.00)
23.78 (0.89)
7.06 (0.00)
0.00 (0.00)
93.79 (0.61)
90.77 (1.02)
90.07 (2.62)
90.25 (0.48)
90.08 (1.20)
90.32 (1.66)
89.88 (0.87)
25.42 (0.65)
43.33 (7.04)
89.13 (0.88)
90.76 (0.83)
66.04 (5.54)
63.75 (5.16)
94.45 (0.59)
89.03 (0.82)
91.91 (0.73)
90.73 (1.00)
91.20 (1.46)
88.63 (0.48)
88.82 (0.61)
26.16 (3.35)
27.15 (2.98)
88.07 (0.29)
91.10 (0.97)
71.81 (4.99)
37.54 (16.98)
91.69 (1.42)

AGNews (Acc.)
63.84 (0.00)
64.00 (0.00)
62.76 (0.00)
63.90 (0.08)
62.27 (0.27)
60.98 (0.00)
86.51 (0.28)
82.69 (0.05)
82.69 (0.14)
82.88 (0.26)
82.84 (0.05)
82.95 (0.07)
82.92 (0.21)
82.04 (0.14)
83.21 (0.27)
83.16 (0.05)
83.36 (0.30)
82.43 (0.21)
82.86 (0.19)
87.69 (0.18)
83.37 (0.27)
83.41 (0.20)
83.62 (0.16)
83.54 (0.18)
83.45 (0.22)
83.86 (0.23)
83.05 (0.22)
83.71 (0.17)
83.78 (0.19)
83.77 (0.21)
83.43 (0.22)
82.65 (0.22)
83.45 (0.11)

TREC (Acc.)
60.80 (0.00)
57.20 (0.00)
50.00 (0.00)
64.20 (0.51)
57.60 (0.00)
31.40 (0.00)
68.56 (1.15)
57.56 (4.99)
62.68 (4.56)
52.88 (4.50)
63.84 (7.60)
47.16 (1.30)
50.00 (2.54)
64.08 (4.41)
52.96 (3.11)
59.52 (1.82)
54.64 (3.98)
34.24 (1.99)
35.56 (4.93)
66.04 (4.05)
61.40 (3.10)
63.88 (4.49)
59.76 (2.14)
63.48 (5.37)
47.28 (1.65)
49.92 (1.03)
68.40 (1.41)
55.52 (2.83)
56.32 (4.41)
63.80 (1.25)
28.48 (1.00)
25.60 (3.72)
56.20 (6.73)

Spouse (F1)
20.81 (0.00)
20.53 (0.00)
15.53 (0.00)
21.12 (0.08)
46.62 (0.00)
34.30 (0.00)
– –
23.99 (0.98)
22.45 (2.79)
20.24 (2.81)
23.23 (2.08)
19.01 (2.33)
17.07 (2.22)
24.75 (1.11)
22.80 (3.68)
21.77 (0.76)
22.17 (1.43)
28.69 (1.96)
31.69 (2.14)
– –
21.52 (0.99)
22.59 (0.66)
18.71 (2.10)
19.70 (0.88)
17.13 (0.35)
16.42 (0.59)
21.65 (0.49)
23.77 (0.94)
20.84 (0.64)
21.17 (0.49)
30.55 (2.06)
30.37 (2.72)
22.47 (7.50)

CDR (F1)
60.31 (0.00)
52.12 (0.00)
50.43 (0.00)
63.51 (0.07)
69.61 (0.01)
20.18 (0.00)
63.09 (0.36)
54.44 (0.54)
56.69 (0.65)
53.62 (0.98)
55.58 (1.70)
51.84 (0.50)
49.88 (1.44)
56.24 (0.57)
55.90 (0.74)
56.52 (0.57)
57.80 (0.42)
48.68 (0.60)
55.53 (0.77)
63.02 (0.48)
56.42 (0.86)
57.66 (1.09)
53.77 (1.17)
57.21 (0.52)
51.96 (0.41)
51.14 (0.45)
56.69 (1.31)
45.32 (22.67)
56.58 (0.46)
58.17 (0.21)
49.20 (0.40)
49.33 (1.30)
56.54 (0.37)

SemEval (Acc.)
77.33 (0.00)
71.00 (0.00)
71.00 (0.00)
71.00 (0.00)
71.00 (0.00)
31.83 (0.00)
93.23 (0.31)
82.83 (1.91)
85.73 (1.08)
72.70 (4.31)
83.87 (2.13)
72.80 (2.20)
72.97 (1.10)
72.80 (3.23)
84.00 (2.35)
75.90 (3.99)
79.73 (2.67)
31.83 (0.00)
40.13 (3.48)
93.33 (0.24)
83.13 (1.50)
85.53 (1.07)
72.37 (0.74)
83.77 (2.93)
73.60 (1.01)
72.93 (2.46)
72.83 (2.26)
79.23 (0.31)
73.00 (1.04)
74.27 (2.87)
31.83 (0.00)
32.50 (1.17)
80.83 (1.31)

ChemProt (Acc.)
49.04 (0.00)
52.08 (0.00)
37.59 (0.00)
47.42 (0.29)
51.96 (0.00)
43.31 (0.00)
77.96 (0.25)
55.84 (0.65)
56.73 (0.33)
54.91 (0.36)
56.63 (0.49)
49.25 (1.51)
48.31 (2.19)
52.94 (0.65)
55.15 (0.54)
54.60 (0.41)
55.68 (0.59)
47.26 (0.22)
48.21 (1.35)
80.15 (0.55)
56.04 (0.59)
55.83 (0.63)
54.64 (0.58)
56.52 (0.79)
48.10 (0.64)
44.64 (0.43)
52.88 (1.59)
55.52 (0.77)
55.02 (0.75)
55.52 (0.82)
46.46 (0.46)
48.23 (1.22)
53.96 (0.38)

Commercial (F1)
85.28 (0.00)
83.80 (0.00)
88.24 (0.00)
77.29 (0.00)
88.20 (0.00)
77.31 (0.00)
91.01 (0.12)
90.62 (0.08)
90.25 (0.28)
89.94 (0.15)
90.34 (0.17)
89.77 (0.18)
89.88 (0.22)
87.44 (0.17)
87.51 (0.18)
90.00 (0.06)
90.15 (0.12)
87.18 (0.19)
89.21 (0.34)
91.69 (0.07)
90.42 (0.27)
90.55 (0.27)
88.59 (0.58)
90.07 (0.38)
88.73 (0.60)
89.86 (0.16)
88.40 (0.37)
88.68 (0.24)
89.73 (0.11)
89.86 (0.08)
88.20 (0.37)
89.59 (0.09)
91.34 (0.16)

Tennis Rally (F1)
81.00 (0.00)
82.61 (0.00)
80.65 (0.00)
82.55 (0.00)
82.52 (0.04)
82.29 (0.00)
82.73 (0.65)
83.59 (0.07)
82.15 (0.23)
83.57 (0.00)
82.38 (0.22)
83.57 (0.00)
83.59 (0.04)
83.57 (0.00)
83.68 (0.00)
83.68 (0.00)
83.57 (0.00)
83.64 (0.05)
83.68 (0.00)
81.48 (0.50)
81.85 (0.16)
82.23 (0.14)
83.56 (0.03)
82.64 (0.14)
83.59 (0.04)
83.59 (0.04)
83.57 (0.00)
83.66 (0.04)
83.70 (0.04)
83.56 (0.03)
83.57 (0.12)
83.77 (0.09)
82.34 (2.46)

Basketball (F1)
16.33 (0.00)
13.13 (0.00)
13.79 (0.00)
17.39 (0.00)
13.13 (0.00)
17.25 (0.00)
62.82 (1.57)
26.31 (4.60)
30.67 (8.52)
23.48 (16.23)
26.65 (8.40)
24.37 (11.77)
20.45 (11.09)
24.69 (1.70)
24.94 (2.70)
4.66 (4.96)
25.62 (17.00)
31.13 (2.25)
25.41 (7.07)
64.97 (13.65)
39.40 (4.82)
39.84 (21.02)
38.75 (17.47)
40.73 (11.98)
22.79 (12.00)
34.81 (19.01)
37.50 (3.76)
40.70 (7.20)
36.74 (18.93)
36.35 (14.00)
38.53 (9.83)
43.18 (7.79)
33.73 (3.43)

Census (F1)
32.80 (0.00)
9.99 (0.00)
47.16 (0.00)
22.66 (0.02)
44.48 (2.34)
15.33 (0.00)
67.12 (0.52)
47.96 (4.23)
51.56 (2.59)
23.94 (14.25)
30.12 (12.97)
49.70 (0.24)
50.10 (0.39)
15.71 (15.36)
21.02 (13.55)
57.39 (0.78)
58.16 (0.72)
26.53 (15.68)
21.37 (15.09)
67.13 (0.16)
54.62 (3.78)
56.73 (3.77)
39.04 (4.10)
50.86 (8.55)
51.19 (1.30)
50.06 (0.53)
47.54 (6.59)
54.57 (4.21)
57.66 (0.32)
57.84 (0.83)
21.93 (0.21)
39.03 (1.76)
43.71 (3.51)

Average 56.91 53.43 53.38 55.51 58.40 45.00 80.91 67.89 69.19 64.22 67.24 65.07 64.79 60.14 61.88 66.34 68.09 58.48 59.25 81.15 69.14 70.17 66.25 69.61 64.79 65.48 63.51 63.26 68.02 68.98 58.69 57.74 65.76

27

Table 12: Comparisons on textual datasets among pre-trained language model-based methods.

End Model (↓) Label Model (↓)

Gold

MV

MV

WMV

WMV

B

DS

DS

DP

DP

MeTaL

MeTaL

FS

FS

MV
MV
WMV
WMV
DS BC
DS
DP

DP MeTaL MeTaL

FS FS

Gold

MV

MV

WMV

WMV

R

DS

DS

DP

DP

MeTaL

MeTaL

FS

FS

MV MV

WMV WMV

DS RC
DS

DP DP MeTaL

MeTaL

FS FS

IMDb (Acc.)
91.58 (0.31)
79.73 (2.60)
79.91 (2.23)
81.32 (1.35)
80.70 (1.39)
80.25 (2.23)
78.79 (1.59)
80.35 (2.16)
80.82 (1.29)
80.02 (2.46)
81.23 (1.23)
82.26 (1.41)
81.20 (1.01)
82.98 (0.05)
83.14 (0.42)
83.69 (0.04)
83.28 (0.12)
91.54 (0.54)
80.48 (0.0)
84.58 (0.08)
82.73 (0.03)
83.47 (0.12)
83.83 (0.14)
84.40 (0.00)
82.64 (0.19)
93.25 (0.30)
85.76 (0.70)
86.17 (1.31)
86.06 (0.88)
86.03 (1.03)
84.74 (1.41)
86.85 (0.72)
86.26 (1.02)
84.86 (0.58)
84.98 (1.07)
87.23 (0.97)
86.95 (0.58)
87.10 (1.06)
88.22 (0.22)
88.48 (0.00)
87.46 (0.05)
88.00 (0.00)
88.01 (0.56)
87.77 (0.05)
87.91 (0.15)
87.30 (0.66)
86.46 (0.11)
88.86 (0.14)
87.65 (0.06)
88.48 (0.00)

Yelp (Acc.)
95.48 (0.53)
82.26 (3.50)
85.64 (2.52)
81.40 (5.06)
81.19 (3.74)
88.59 (1.25)
88.57 (2.01)
81.17 (4.36)
82.90 (3.69)
86.92 (3.52)
88.29 (1.57)
87.76 (1.30)
88.86 (0.92)
89.22 (0.05)
89.64 (0.03)
90.40 (0.65)
87.87 (0.00)
90.84 (0.30)
91.12 (0.11)
88.44 (0.03)
91.02 (0.13)
89.76 (0.00)
90.68 (0.05)
89.05 (0.07)
91.18 (0.03)
97.13 (0.26)
89.91 (1.76)
87.87 (1.18)
82.27 (4.11)
86.06 (3.97)
92.30 (1.75)
92.06 (1.20)
89.59 (2.87)
85.73 (3.49)
89.08 (3.71)
92.22 (1.14)
92.08 (2.63)
94.34 (0.89)
94.23 (0.20)
91.06 (0.39)
92.53 (0.06)
93.16 (0.03)
94.19 (0.18)
95.01 (0.25)
94.09 (0.06)
94.40 (0.37)
93.11 (0.01)
93.95 (0.00)
95.45 (0.10)
95.33 (0.06)

Youtube (Acc.)
97.52 (0.64)
95.36 (1.71)
93.68 (0.47)
89.92 (1.51)
93.76 (2.10)
92.88 (0.78)
89.36 (2.56)
93.84 (1.61)
93.60 (0.98)
92.32 (1.44)
92.48 (0.99)
91.84 (2.10)
91.60 (2.18)
98.00 (0.00)
95.44 (0.20)
93.44 (0.20)
97.20 (0.00)
94.16 (0.20)
93.04 (0.20)
96.32 (0.16)
94.80 (0.00)
94.88 (0.53)
94.72 (0.16)
94.80 (0.00)
96.16 (0.20)
95.68 (1.42)
96.56 (0.86)
95.60 (0.80)
92.96 (1.73)
95.52 (0.99)
93.52 (1.39)
92.96 (1.53)
95.60 (0.80)
94.48 (1.17)
94.56 (0.65)
94.08 (1.70)
93.84 (1.57)
93.20 (3.19)
97.60 (0.00)
97.60 (0.00)
95.60 (0.00)
97.20 (0.00)
96.24 (0.41)
95.52 (0.30)
96.80 (0.00)
95.60 (0.00)
97.04 (0.20)
96.00 (0.00)
95.20 (0.00)
96.80 (0.00)

SMS (F1)
96.96 (0.66)
94.56 (1.88)
94.85 (1.16)
91.79 (2.67)
95.02 (1.26)
91.98 (1.00)
93.06 (1.30)
29.97 (2.33)
31.96 (2.87)
92.28 (2.01)
90.43 (2.64)
11.62 (11.39)
7.32 (5.35)
97.01 (0.00)
96.85 (0.31)
95.95 (0.35)
96.34 (0.31)
93.90 (0.05)
95.37 (0.08)
33.70 (0.00)
36.44 (0.00)
95.62 (0.31)
93.75 (0.00)
62.27 (0.17)
63.54 (4.71)
96.31 (0.58)
94.17 (2.88)
95.06 (1.66)
92.96 (1.71)
93.96 (1.11)
94.10 (1.72)
93.17 (0.89)
28.25 (2.83)
46.66 (11.89)
93.28 (1.57)
93.00 (1.42)
10.72 (10.15)
18.20 (3.93)
96.67 (0.37)
96.82 (0.29)
98.02 (0.38)
97.27 (0.36)
96.79 (0.27)
97.10 (0.31)
31.71 (0.29)
64.22 (0.00)
97.71 (0.00)
96.18 (0.00)
82.24 (0.93)
65.65 (0.00)

AGNews (Acc.)
90.78 (0.49)
86.27 (0.53)
86.62 (0.28)
85.49 (0.63)
86.66 (0.44)
86.69 (0.35)
86.59 (0.38)
85.36 (0.92)
86.55 (0.08)
86.77 (0.29)
86.82 (0.23)
86.29 (0.49)
85.51 (0.62)
87.03 (0.00)
87.14 (0.00)
86.25 (0.01)
87.22 (0.00)
87.19 (0.0)
87.06 (0.01)
86.98 (0.39)
86.67 (0.00)
87.26 (0.02)
87.41 (0.01)
87.16 (0.16)
86.57 (0.00)
91.39 (0.38)
86.88 (0.98)
87.14 (0.18)
86.70 (0.51)
86.99 (0.37)
87.16 (0.58)
86.82 (0.29)
86.81 (0.42)
87.65 (0.37)
87.18 (0.45)
86.87 (0.37)
86.69 (0.29)
86.17 (0.78)
88.15 (0.30)
88.04 (0.06)
87.83 (0.13)
88.11 (0.05)
88.20 (0.11)
87.21 (0.0)
87.53 (0.03)
88.04 (0.00)
87.85 (0.02)
87.43 (0.01)
87.73 (0.12)
87.23 (0.00)

TREC (Acc.)
96.24 (0.61)
66.56 (2.31)
66.56 (1.20)
54.64 (4.85)
66.00 (2.33)
46.36 (3.39)
48.40 (0.95)
68.64 (3.57)
68.40 (2.41)
58.28 (1.95)
62.44 (2.96)
27.60 (0.00)
30.96 (4.04)
76.56 (0.08)
68.56 (1.13)
60.48 (0.10)
70.88 (1.14)
53.36 (0.29)
51.72 (1.17)
78.72 (0.43)
72.40 (0.00)
61.80 (0.00)
71.20 (0.36)
27.60 (0.00)
36.20 (0.00)
96.68 (0.82)
66.28 (1.21)
66.16 (1.25)
58.88 (0.92)
63.64 (1.94)
48.32 (1.50)
50.12 (1.99)
72.12 (4.58)
66.80 (0.85)
60.04 (1.18)
65.60 (1.67)
30.44 (3.48)
28.84 (2.48)
77.96 (0.34)
74.28 (0.75)
70.28 (1.09)
72.08 (1.01)
59.40 (0.42)
57.96 (0.15)
82.36 (0.08)
74.00 (0.77)
71.64 (0.59)
79.84 (0.23)
38.80 (0.33)
33.80 (0.00)

Spouse (F1)
– –
19.56 (1.22)
19.43 (0.95)
19.74 (5.48)
19.34 (2.87)
16.42 (0.60)
16.23 (0.04)
18.66 (1.55)
28.74 (7.63)
17.26 (0.73)
17.18 (0.23)
33.63 (18.57)
9.14 (18.29)
32.39 (3.41)
42.71 (5.47)
36.27 (3.01)
32.49 (1.54)
23.33 (0.70)
24.76 (0.57)
30.71 (9.78)
33.83 (0.00)
35.84 (6.73)
27.23 (2.80)
56.52 (0.32)
53.46 (0.13)
– –
17.99 (1.99)
21.68 (8.32)
16.14 (1.40)
17.43 (1.21)
16.57 (0.25)
16.93 (0.52)
17.62 (4.24)
17.71 (2.27)
16.42 (2.79)
20.80 (7.13)
0.00 (0.00)
0.00 (0.00)
40.50 (1.23)
46.28 (1.59)
20.76 (1.44)
30.07 (2.35)
21.34 (1.19)
28.75 (1.03)
28.86 (10.02)
21.74 (0.00)
23.99 (8.47)
21.89 (4.72)
16.06 (0.15)
0.00 (0.00)

CDR (F1)
65.39 (1.18)
57.16 (0.83)
58.89 (0.50)
53.60 (3.25)
57.53 (0.46)
50.01 (0.30)
50.49 (0.48)
58.48 (0.73)
57.94 (0.29)
58.48 (0.90)
56.72 (3.26)
4.29 (8.59)
35.25 (5.75)
58.99 (0.09)
59.26 (0.17)
58.29 (0.18)
59.55 (0.09)
52.09 (0.03)
51.73 (0.04)
60.46 (0.11)
58.47 (0.16)
59.33 (0.04)
59.14 (0.04)
48.89 (0.08)
55.69 (0.03)
65.86 (0.60)
55.07 (3.47)
54.96 (5.42)
42.37 (21.19)
54.88 (3.82)
50.77 (0.12)
50.85 (0.37)
54.42 (5.32)
57.78 (0.79)
53.68 (4.00)
59.19 (0.35)
0.00 (0.00)
0.00 (0.00)
60.38 (0.05)
61.13 (0.12)
56.27 (0.10)
58.66 (0.46)
51.37 (0.60)
52.25 (0.25)
61.40 (0.07)
59.86 (0.17)
58.29 (0.39)
60.16 (0.16)
38.14 (6.62)
0.00 (0.00)

SemEval (Acc.)
95.43 (0.65)
83.93 (1.74)
85.03 (0.83)
70.97 (0.24)
82.47 (1.29)
71.67 (0.66)
71.70 (0.81)
71.07 (0.33)
83.93 (0.83)
71.47 (0.57)
70.80 (0.87)
31.83 (0.00)
31.83 (0.00)
86.80 (0.46)
86.13 (0.19)
82.90 (0.08)
86.70 (0.22)
72.50 (0.00)
72.83 (0.00)
75.77 (1.33)
88.77 (0.13)
79.20 (2.33)
81.20 (0.64)
31.83 (0.00)
31.83 (0.00)
93.23 (1.83)
84.00 (0.84)
84.13 (0.59)
67.47 (6.93)
82.87 (2.49)
69.67 (1.18)
70.80 (0.61)
70.57 (0.83)
72.60 (20.40)
70.73 (0.68)
70.27 (0.88)
31.83 (0.00)
31.83 (0.00)
86.20 (0.07)
83.93 (0.20)
72.77 (0.48)
84.67 (0.00)
71.70 (0.07)
77.03 (0.52)
75.17 (0.95)
86.73 (0.08)
70.90 (0.08)
84.20 (0.16)
31.83 (0.00)
31.83 (0.00)

ChemProt (Acc.)
89.76 (0.88)
56.09 (1.08)
57.32 (0.98)
55.40 (1.02)
55.66 (1.36)
44.37 (0.53)
45.71 (1.46)
54.00 (1.41)
57.00 (1.20)
55.48 (1.33)
56.17 (0.66)
45.66 (0.45)
49.53 (1.14)
58.47 (0.08)
58.01 (0.02)
56.10 (0.42)
57.93 (0.00)
49.65 (0.68)
49.43 (1.15)
57.51 (0.02)
61.56 (0.06)
55.46 (0.12)
57.85 (0.26)
48.10 (0.60)
49.35 (0.00)
86.98 (1.49)
56.85 (1.91)
57.31 (1.07)
46.56 (11.71)
55.57 (0.78)
45.69 (0.86)
46.96 (0.38)
39.91 (9.33)
56.18 (1.12)
54.59 (0.77)
42.02 (11.91)
39.95 (6.50)
39.43 (8.74)
59.43 (0.00)
59.32 (0.06)
55.58 (0.34)
58.31 (0.09)
46.75 (0.27)
49.23 (0.11)
52.86 (0.06)
55.96 (0.06)
53.32 (0.19)
56.89 (0.11)
48.60 (0.11)
39.89 (0.00)

Average 91.02 72.15 72.79 68.43 71.83 66.92 66.89 64.15 67.18 69.93 70.26 50.28 51.12 76.75 76.69 74.38 75.95 70.86 69.75 69.32 70.67 74.26 74.70 63.06 64.66 90.72 73.35 73.61 67.24 72.30 68.28 68.75 64.12 67.05 70.45 71.13 47.25 47.91 78.93 78.69 73.71 76.75 71.40 72.78 69.87 72.79 74.03 76.54 62.17 53.90

28

Table 13: Sequence Tagging. The detailed results for different methods. The number stands for the F1 score (Precision, Recall) with standard deviation in the bracket under each value. Each metric value is averaged over 5 runs. red and blue indicate the best and second best result for each end model respectively, and gray is the best
weak supervision method.

End Model (↓) –

Label Model (↓) MV WMV DS DP
MeTaL FS

HMM

CHMM

LSTM-CNN-MLP

Gold MV WMV DS DP MeTaL FS HMM CHMM

LSTM-CNN-CRF

Gold MV WMV DS DP MeTaL FS HMM CHMM

LSTM-ConNet

BERT-MLP

Gold MV WMV DS DP MeTaL

FS

HMM

CHMM

BERT-CRF

Gold MV WMV DS DP MeTaL FS HMM

CHMM

BERT-ConNet

CoNLL-03
60.36(59.06/61.72) (0.00)
60.26(59.03/61.54) (0.00)
46.76(45.29/48.32) (0.00)
62.43(61.62/63.26) (0.22)
60.32(59.07/61.63) (0.08)
62.49(63.25/61.76) (0.00)
62.18(66.42/58.45) (0.00)
63.22(61.93/64.56) (0.26)
87.46(87.72/87.19) (0.35)
66.33(67.54/65.19) (0.52)
64.60(65.31/63.91) (0.73)
50.60(49.35/51.90) (0.72)
66.98(68.95/65.13) (0.50)
65.05(66.20/63.94) (0.73)
66.49(69.13/64.05) (0.25)
64.85(69.48/60.81) (1.41)
66.67(67.75/65.64) (0.25)
86.80(86.80/86.80) (0.74)
65.97(67.14/64.85) (0.81)
63.76(63.76/63.76) (1.06)
49.74(48.66/50.91) (1.41)
67.15(67.47/66.83) (0.69)
64.48(65.77/63.29) (0.85)
66.21(68.71/63.90) (0.79)
66.18(70.66/62.24) (1.27)
66.58(67.05/66.17) (0.75)
66.02(67.98/64.19) (0.95)
89.41(90.06/88.76) (0.21)
67.08(68.35/65.86) (0.71)
65.96(66.88/65.07) (0.52)
59.48(65.12/55.91) (0.90)
67.66(68.82/66.55) (0.73)
66.34(67.46/65.28) (1.29)
67.54(69.81/65.43) (1.32)
68.48(71.04/66.17) (0.16)
68.30(69.10/67.54) (0.44)
87.38(87.70/87.06) (0.34)
66.63(67.68/65.62) (0.85)
64.38(66.55/62.35) (1.09)
53.89(54.10/53.68) (1.42)
65.48(66.76/64.28) (0.37)
65.11(66.87/63.45) (0.69)
67.34(70.05/64.83) (0.75)
67.49(71.26/64.14) (0.89)
66.72(67.17/66.27) (0.41)
67.83(69.37/66.40) (0.62)

WikiGold
52.24(48.95/56.00) (0.00)
52.87(50.74/55.20) (0.00)
42.17(40.05/44.53) (0.00)
54.81(53.10/56.64) (0.13)
52.09(50.31/54.03) (0.23)
58.29(62.77/54.40) (0.00)
56.36(61.51/52.00) (0.00)
58.89(55.71/62.45) (0.97)
80.45(80.80/80.11) (1.46)
58.27(56.65/60.00) (0.67)
53.86(54.01/53.76) (0.48)
40.01(38.00/42.24) (3.13)
57.87(58.34/57.44) (1.27)
56.31(55.94/56.69) (1.64)
59.80(65.71/54.88) (1.65)
60.87(67.45/55.47) (0.95)
61.34(60.49/62.24) (1.79)
79.79(79.79/79.79) (0.49)
57.04(54.91/59.36) (1.33)
55.39(54.30/56.53) (0.95)
40.61(38.31/43.20) (1.89)
57.89(57.56/58.24) (1.99)
55.37(54.26/56.53) (1.69)
60.49(65.46/56.27) (3.30)
62.51(71.09/55.79) (1.21)
59.90(57.38/62.67) (2.86)
58.04(61.10/55.36) (1.60)
87.21(87.88/86.56) (0.65)
63.17(64.15/62.29) (2.15)
61.28(63.88/59.25) (2.18)
54.04(54.24/53.91) (1.66)
62.91(64.44/61.49) (1.23)
61.74(61.55/61.97) (1.84)
66.58(72.23/61.76) (1.40)
64.25(68.96/60.16) (1.65)
65.16(63.45/66.99) (0.67)
86.78(87.27/86.29) (0.84)
62.09(61.89/62.29) (1.06)
59.96(60.33/59.73) (1.08)
48.89(46.80/51.20) (1.59)
61.09(61.07/61.12) (1.53)
58.94(61.53/56.75) (3.22)
66.44(72.86/61.17) (1.40)
63.31(70.95/57.33) (1.02)
63.06(62.12/64.11) (1.91)
64.18(72.17/57.92) (1.71)

BC5CDR
83.49(91.69/76.64) (0.00)
83.49(91.66/76.66) (0.00)
83.49(91.66/76.66) (0.00)
83.50(91.69/76.65) (0.00)
83.50(91.66/76.67) (0.00)
56.71(88.03/41.83) (0.00)
71.57(93.48/57.98) (0.00)
83.66(91.76/76.87) (0.04)
78.02(79.80/76.34) (0.20)
74.56(79.70/70.05) (0.21)
74.29(79.95/69.39) (0.16)
74.21(80.15/69.11) (0.34)
74.26(79.09/70.00) (0.33)
74.37(79.86/69.62) (0.20)
54.37(77.11/42.00) (0.42)
64.07(79.54/53.67) (0.38)
74.29(79.55/69.70) (0.11)
78.59(80.90/76.41) (0.70)
74.75(79.90/70.22) (0.60)
74.31(79.59/69.69) (0.63)
75.37(80.88/70.56) (0.28)
74.79(80.48/69.91) (0.68)
74.66(79.95/70.03) (0.88)
54.49(77.53/42.02) (0.47)
63.68(77.69/53.98) (0.71)
74.54(80.49/69.44) (0.50)
72.04(77.71/67.18) (0.54)
82.49(86.67/78.70) (0.28)
77.93(84.26/72.50) (0.43)
77.76(84.63/71.94) (0.47)
49.09(46.69/51.79) (0.20)
77.67(83.87/72.33) (0.40)
77.80(83.77/72.63) (0.21)
62.89(79.81/52.02) (1.39)
68.70(81.86/59.20) (0.82)
77.98(83.74/72.98) (0.13)
79.65(79.48/79.83) (0.29)
74.93(75.84/74.04) (0.32)
75.32(77.59/73.18) (0.39)
75.42(76.91/74.00) (0.32)
75.08(76.78/73.47) (0.55)
75.32(76.71/73.99) (0.20)
59.38(71.35/50.94) (1.30)
67.37(75.17/61.12) (0.70)
75.21(76.61/73.88) (0.41)
72.87(73.25/72.60) (0.91)

NCBI-Disease
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
78.44(93.04/67.79) (0.00)
40.67(72.24/28.30) (0.00)
66.80(96.79/51.00) (0.00)
78.74(93.21/68.15) (0.10)
79.41(80.94/77.97) (0.54)
70.54(80.95/62.51) (0.79)
70.94(81.81/62.64) (0.81)
70.71(81.38/62.53) (0.67)
70.75(80.69/63.06) (1.35)
71.23(80.59/63.82) (0.78)
42.70(71.11/30.52) (1.03)
57.80(81.75/44.71) (0.60)
71.45(80.90/64.01) (0.72)
79.39(80.34/78.48) (0.59)
72.44(82.56/64.60) (1.44)
72.21(83.05/63.89) (1.33)
72.86(82.69/65.15) (1.01)
72.50(82.83/64.48) (0.86)
72.42(83.41/64.01) (1.44)
44.90(74.39/32.19) (1.15)
59.12(84.26/45.55) (1.15)
72.15(82.51/64.12) (1.42)
63.04(74.55/55.16) (12.69)
84.05(84.08/84.03) (0.29)
77.93(85.84/71.38) (0.73)
78.53(86.44/71.97) (0.85)
77.57(84.62/71.62) (0.55)
78.18(85.91/71.74) (0.71)
79.02(85.98/73.12) (0.59)
46.50(72.13/34.34) (1.32)
65.52(87.25/52.52) (1.44)
78.20(85.04/72.40) (0.71)
80.64(81.50/79.83) (0.27)
72.87(83.57/64.63) (0.62)
73.23(83.77/65.07) (0.71)
72.91(82.60/65.30) (0.52)
72.86(81.54/65.85) (0.92)
74.16(82.66/67.31) (1.02)
44.12(73.05/31.62) (1.15)
61.43(84.29/48.36) (1.60)
72.96(81.31/66.25) (0.93)
71.40(80.30/64.56) (1.81)

Laptop-Review
73.27(88.86/62.33) (0.00)
73.27(88.86/62.33) (0.00)
73.27(88.86/62.33) (0.00)
73.27(88.86/62.33) (0.00)
64.36(83.21/53.63) (17.81)
28.74(60.59/18.84) (0.00)
73.63(89.30/62.63) (0.00)
73.26(88.79/62.36) (0.13)
69.83(74.51/65.73) (0.51)
62.32(74.03/53.81) (1.24)
61.73(72.92/53.54) (0.66)
63.12(74.62/54.70) (0.59)
61.02(72.90/52.47) (0.76)
62.43(74.18/53.91) (0.47)
27.08(54.14/18.13) (3.20)
62.53(74.62/53.81) (0.48)
62.18(73.86/53.72) (0.84)
71.25(76.37/66.80) (1.80)
63.52(75.14/55.01) (0.96)
63.02(75.36/54.15) (0.98)
63.96(75.27/55.62) (0.79)
62.59(74.16/54.15) (0.83)
63.87(75.34/55.44) (1.53)
28.35(54.68/19.20) (1.61)
62.57(74.21/54.09) (0.30)
62.28(74.15/53.69) (0.69)
50.36(63.04/42.73) (7.74)
81.22(83.67/78.96) (1.20)
69.88(75.99/64.84) (1.17)
71.60(76.96/66.95) (0.68)
78.69(86.37/72.27) (0.88)
70.86(77.70/65.15) (1.10)
71.80(76.17/67.93) (0.81)
36.87(63.98/26.09) (1.96)
71.51(75.86/67.66) (0.58)
70.58(77.41/64.87) (0.48)
79.15(81.26/77.18) (0.77)
71.12(76.74/66.34) (1.83)
71.09(76.16/66.68) (0.73)
70.19(76.49/64.87) (1.46)
71.46(76.42/67.14) (0.86)
71.24(76.66/66.62) (1.14)
38.57(61.91/28.09) (2.55)
70.28(76.41/65.08) (0.71)
71.17(76.66/66.43) (0.95)
67.32(73.60/62.14) (1.24)

MIT-Restaurant
48.71(74.25/36.24) (0.00)
48.19(73.73/35.80) (0.00)
46.81(71.71/34.75) (0.00)
47.92(73.24/35.61) (0.00)
47.66(73.40/35.29) (0.00)
13.86(84.10/7.55) (0.00)
42.65(71.44/30.40) (0.00)
47.34(73.05/35.02) (0.57)
77.80(78.42/77.19) (0.28)
41.30(63.57/30.59) (0.37)
40.60(62.48/30.07) (0.26)
39.86(61.98/29.37) (0.23)
41.06(63.10/30.43) (0.17)
40.97(62.58/30.46) (0.60)
13.09(74.74/7.17) (0.30)
37.47(60.36/27.17) (0.50)
40.97(63.11/30.33) (0.17)
79.18(79.68/78.69) (0.20)
41.70(63.92/30.95) (0.21)
41.27(63.20/30.64) (0.31)
41.21(63.02/30.61) (0.34)
41.62(63.43/30.97) (0.47)
41.48(63.09/30.90) (0.45)
12.74(71.00/7.00) (0.13)
37.90(62.48/27.20) (0.56)
41.59(63.67/30.89) (0.40)
39.26(61.74/28.78) (0.46)
78.85(78.74/78.96) (0.33)
41.89(62.80/31.43) (0.59)
42.40(62.88/31.98) (0.57)
71.41(76.00/67.38) (0.72)
42.06(62.49/31.70) (0.31)
42.07(62.69/31.66) (0.74)
13.52(71.73/7.47) (0.74)
38.10(61.40/27.62) (0.57)
42.10(62.88/31.64) (0.27)
78.83(79.14/78.53) (0.44)
42.95(63.18/32.54) (0.43)
42.62(63.56/32.06) (0.23)
42.26(62.65/31.89) (0.78)
42.27(62.81/31.86) (0.53)
42.26(62.82/31.84) (0.49)
13.80(72.63/7.62) (0.23)
39.51(62.49/28.90) (0.72)
42.79(63.19/32.35) (0.22)
42.37(62.88/31.95) (0.72)

MIT-Movies
59.68(69.92/52.05) (0.00)
60.37(70.98/52.52) (0.00)
54.06(63.64/46.99) (0.00)
59.92(70.65/52.01) (0.43)
56.60(72.28/47.70) (7.71)
43.04(77.73/29.75) (0.00)
60.56(75.04/50.76) (0.00)
61.38(73.00/52.96) (0.10)
86.18(87.05/85.33) (0.40)
61.50(72.78/53.25) (0.22)
61.31(72.62/53.05) (0.25)
54.59(67.94/45.63) (0.45)
61.05(72.66/52.64) (0.45)
61.02(72.44/52.71) (0.32)
45.77(75.50/32.84) (0.61)
61.55(76.16/51.65) (0.24)
62.05(73.57/53.65) (0.23)
87.07(87.49/86.64) (0.19)
62.41(74.45/53.72) (0.28)
61.79(73.72/53.19) (0.42)
55.99(68.63/47.29) (0.50)
62.29(74.31/53.63) (0.22)
62.10(73.97/53.52) (0.48)
45.62(77.19/32.38) (0.44)
61.94(76.85/51.89) (0.44)
62.97(74.82/54.36) (0.30)
60.46(75.61/50.38) (0.90)
87.56(87.57/87.54) (0.21)
63.20(73.74/55.35) (0.70)
65.26(71.65/60.89) (0.47)
41.14(61.70/30.86) (0.56)
63.28(73.36/55.64) (0.35)
63.00(73.02/55.40) (0.28)
49.37(75.67/36.64) (0.34)
63.07(76.72/53.55) (0.53)
63.68(73.49/56.18) (0.29)
87.03(87.12/86.94) (0.91)
63.71(73.46/56.25) (0.23)
63.44(72.85/56.19) (0.29)
58.89(69.67/51.01) (0.34)
63.92(73.05/56.84) (0.36)
64.19(73.30/57.10) (0.52)
49.79(75.45/37.17) (1.03)
63.38(76.46/54.15) (0.81)
64.58(74.77/56.84) (0.74)
64.12(74.03/56.56) (0.51)

Ontonotes 5.0
58.85(54.17/64.40) (0.00)
57.58(53.15/62.81) (0.00)
37.70(34.33/41.82) (0.00)
61.85(57.44/66.99) (0.19)
58.27(54.10/63.14) (0.48)
5.31(2.87/35.74) (0.00)
55.67(57.95/53.57) (0.00)
64.06(59.70/69.09) (0.07)
79.52(79.91/79.14) (0.40)
60.04(56.89/63.57) (0.53)
58.47(55.46/61.83) (0.09)
41.13(40.55/41.75) (0.98)
62.68(59.51/66.21) (0.06)
59.15(55.99/62.68) (0.45)
41.54(44.13/39.48) (2.06)
58.17(61.13/55.49) (0.23)
63.70(60.58/67.17) (0.45)
79.42(80.14/78.72) (2.76)
61.92(59.47/64.59) (0.55)
59.22(56.76/61.91) (0.31)
44.92(42.91/47.16) (1.96)
63.82(61.11/66.80) (0.29)
60.43(57.99/63.08) (0.31)
43.25(46.46/40.56) (0.53)
59.43(61.53/57.47) (0.62)
63.71(61.40/66.20) (0.36)
60.58(59.43/61.83) (0.46)
84.11(83.11/85.14) (0.55)
63.86(61.26/66.71) (0.60)
62.90(72.32/55.66) (0.48)
58.62(68.60/51.19) (2.14)
65.16(63.15/67.33) (0.74)
63.08(60.94/65.36) (0.46)
49.63(57.13/43.91) (2.49)
61.13(63.55/58.93) (0.47)
66.03(63.42/68.87) (0.29)
83.86(82.32/85.45) (0.18)
63.97(61.15/67.08) (0.58)
61.29(58.70/64.14) (0.32)
48.55(46.97/50.26) (1.23)
65.09(63.27/67.04) (0.31)
62.13(60.26/64.13) (0.40)
42.45(44.03/41.51) (5.05)
61.29(63.86/58.93) (0.78)
65.26(61.99/68.91) (0.20)
60.36(57.81/63.21) (0.61)

Average 64.38(72.50/59.65) 64.31(72.65/59.33) 57.84(66.07/52.90) 65.27(73.71/60.16) 62.66(72.14/57.48) 38.64(63.95/34.77) 61.18(76.49/52.10) 66.32(74.65/61.43) 79.83(81.14/78.63) 61.86(69.01/57.37) 60.72(68.07/56.02) 54.28(61.75/49.66) 61.96(69.41/57.17) 61.32(68.47/56.73) 43.85(66.45/36.13) 58.41(71.31/50.35) 62.83(69.98/58.31) 80.19(81.44/79.04) 62.47(69.69/57.91) 61.37(68.72/56.72) 55.58(62.55/51.31) 62.83(70.17/58.13) 61.85(69.22/57.10) 44.51(66.93/36.69) 59.17(72.35/51.03) 62.97(70.19/58.44) 58.73(67.65/53.20) 84.36(85.22/83.58) 65.62(72.05/61.29) 61.63(59.19/64.29) 45.32(42.74/48.24) 65.97(72.47/61.49) 65.61(71.45/61.67) 49.11(70.31/40.96) 62.59(73.33/55.72) 66.50(72.32/62.68) 82.91(83.22/82.64) 64.78(70.44/61.10) 63.92(69.94/59.93) 58.87(64.52/55.28) 64.66(70.21/60.95) 64.17(70.10/60.15) 47.73(67.67/40.37) 61.76(72.61/54.75) 65.22(70.48/61.88) 63.81(70.43/59.42)

29

