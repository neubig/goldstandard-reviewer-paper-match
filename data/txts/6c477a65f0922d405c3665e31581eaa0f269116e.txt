arXiv:1412.5836v3 [cs.CL] 21 Mar 2015

Accepted as a workshop contribution at ICLR 2015
INCORPORATING BOTH DISTRIBUTIONAL AND RELATIONAL SEMANTICS IN WORD REPRESENTATIONS
Daniel Fried∗ Department of Computer Science University of Arizona Tucson, Arizona, USA dfried@email.arizona.edu
Kevin Duh Graduate School of Information Science Nara Institute of Science and Technology Ikoma, Nara, JAPAN kevinduh@is.naist.jp
ABSTRACT
We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which ﬂexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.
1 INTRODUCTION
We are interested in algorithms for learning vector representations of words. Recent work has shown that such representations can capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various Natural Language Processing systems (Turian et al., 2010; Wang & Manning, 2013; Socher et al., 2013a; Collobert et al., 2011).
Although many kinds of representation learning algorithms have been proposed so far, they are all essentially based on the same premise of distributional semantics (Harris, 1954). For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations using the context window around the word. Intuitively, these algorithms learn to map words with similar context to nearby points in vector space.
However, distributional semantics is by no means the only theory of word meaning. Relational semantics, exempliﬁed by WordNet (Miller, 1995), deﬁnes a graph of relations such as synonymy and hypernymy (Cruse, 1986) between words, reﬂecting our world knowledge and psychological predispositions. For example, a relation like “dog is-a mammal” describes a precise hierarchy that complements the distributional similarities observable from corpora.
We believe both distributional and relational semantics are valuable for word representations, and investigate combining these approaches into a uniﬁed representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011). Its advantages include (a) ﬂexibility in incorporating arbitrary objectives, and (b) relative ease of implementation. We show that ADMM effectively optimizes the joint objective and present preliminary results on several tasks.
2 DISTRIBUTIONAL AND RELATIONAL OBJECTIVES
Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi ∈ Rd, the word’s embedding. An n-length sequence of words (i1, i2, . . . , in) is represented as a vector x by concatenating the vector embeddings for each word, x = [wi1 ; wi2 . . . ; win ]. This vector x is then scored by feeding it through a two-layer neural network with h hidden nodes: SNLM (x) = u (f (Ax + b)), where A ∈ Rh×(nd), b ∈ Rh, u ∈ Rh
∗Currently at the University of Cambridge.
1

Accepted as a workshop contribution at ICLR 2015

are network parameters and f is the sigmoid f (t) = 1/(1 + e−t) applied element-wise. The model is trained using noise contrastive estimation (NCE) (Mnih & Kavukcuoglu, 2013), where training text is corrupted by random replacement of random words to provide an implicit negative training example, xc. The hinge-loss function, comparing positive and negative training example scores, is:

LNLM (x, xc) = max(0, 1 − SNLM (x) + SNLM (xc))

(1)

The word embeddings, w, and other network parameters are optimized with backpropagation using stochastic gradient descent (SGD) over n-grams in the training corpus.

Relational Semantics Objective: We investigate three different objectives, each modeling relations from WordNet. The Graph Distance loss, LGD, enforces the idea that words close together in the WordNet graph should have similar embeddings in vector space. First, for a word pair (i, j), we deﬁne a pairwise word similarity W ordSim(i, j) as the normalized shortest path between the words’ synonym sets in the WordNet relational graph (Leacock & Chodorow, 1998). Then, we encourage the cosine similarity between their embeddings vi and vj to match that of W ordSim(i, j):

vi · vj

2

LGD(i, j) = ||vi||2||vj||2 − [a × W ordSim(i, j) + b] (2)

where a and b are parameters that scale W ordSim(i, j) to be of the same range as the cosine similarity. Training proceeds by SGD: word pairs (i, j) are sampled from the WordNet graph, and both the word embeddings v and parameters a, b are updated by gradient descent on the loss function.

A different approach directly models each WordNet relation as an operation in vector space. These models assign scalar plausibility scores to input tuples (vl, R, vr), modeling the plausibility of a relation of type R between words vl and vr. In both of the relational models we consider, each type of relationship (for example, synonymy or hypernymy) has a distinct set of parameters used to
represent the relationship as a function in vector space. The TransE model of Bordes et al. (2013) represents relations as linear translations: if the relationship R holds for two words vl and vr, then their embeddings vl, vr ∈ Rd should be close after translating vl by a relation vector R ∈ Rd:

ST ransE (vl, R, vr) = −||vl + R − vr||2

(3)

Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors and a non-linearity function. The scoring function for a input tuple is:

SNT N (vl, R, vr) = U f vl WRvr + VR vvrl + bR (4)

where U ∈ Rh, WR ∈ Rd×d×h, VR ∈ Rh×2d and bR ∈ Rk are parameters for relationship R. As in the NLM, parameters for these relational models are trained using NCE (producing a noisy example for each training example by randomly replacing one of the tuples’ entries) and SGD, using the hinge loss as deﬁned in Eq. 1, with SNLM replaced by the ST ransE or SNT N scoring function.

Joint Objective Optimization by ADMM: We now describe an ADMM formulation for joint op-
timization of the above objectives. Let w be the set of word embeddings {w1, w2, . . . wN } for the distributional objective, and v be the set of word embeddings {v1, v2, . . . vN } for the relational objective, where N and N are the vocabulary size of the corpus and WordNet, respectively. Let
I be the set of N words that occur in both. Then we deﬁne a set of vectors y = {y1, y2, . . . yN }, which correspond to Lagrange multipliers, to penalize the difference (wi − vi) between sets of embeddings for each word i in the joint vocabulary I, producing a Lagrangian penalty term:

ρ

LP (w, v) =

yi (wi − vi) +

(wi − vi) (wi − vi)

(5)

2

i∈I

i∈I

In the ﬁrst term, y has same dimensionality as w and v, so a scalar penalty is maintained for each entry in every embedding vector. This constrains corresponding w and v vectors to be close to each other. The second residual penalty term with hyperparameter ρ is added to avoid saddle points; ρ can be viewed as a step-size during the update of y.

This augmented Lagrangian term (Eq. 5) is added to the sum of the loss terms for each objective (Eq. 1 and Eq. 2). Let θ = (u, A, b) be the parameters of the distributional objective, and φ be the parameters of the relational objective. The ﬁnal loss function we optimize becomes:

L = LNLM (w, θ) + LGD(v, φ) + LP (w, v)

(6)

2

Accepted as a workshop contribution at ICLR 2015

L L M +L GD

0.45
0.40 NLM loss + WordNet loss
0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00

600

800

Figure 1: Analysis of ADMM behavior by training iteration, for varying ρ. Left: Joint loss, LNLM + LGD, on the training data . Right: Normalized residual magnitude, averaged across embeddings.

Knowledge Base Analogy Test Parsing

NLM 42
76.03

GD 41
75.90

GD+NLM 41
76.18

TransE 82.87
37 75.86

TransE+NLM 83.10 38 76.01

NTN 80.95
36 75.85

NTN+NLM 81.27 41 76.14

Table 1: Results summary: Accuracy on knowledge base completion, MaxDiff accuracy on Analogy Test, and Label Arc Score Accuracy on Dependency Parsing for single- and joint-objective models.

The ADMM algorithm proceeds by repeating the following three steps until convergence: (1) Perform SGD on w and θ to minimize LNLM + LP , with all other parameters ﬁxed. (2) Perform SGD on v and φ to minimize LGD + LP , with all other parameters ﬁxed. (3) For all embeddings i corresponding to words in both the n-gram and relational training sets, update the constraint vector yi := yi + ρ(wi − vi). Since LNLM and LGD share no parameters, Steps (1) and (2) can be optimized easily using the single-objective NCE and SGD procedures, with additional regularization term ρ (wi − vi).
3 PRELIMINARY EXPERIMENTS & DISCUSSIONS
The distributional objective LNLM is trained using 5-grams from the Google Books English corpus1, containing over 180 million 5-gram types. The top 50k unigrams by frequency are used as the vocabulary, and each training iteration samples 100k n-grams from the corpus. For training LGD, we sample 100k words from WordNet and compute the similarity of each to 5 other words in each ADMM iteration. For training LT ransE and LNT N , we use the dataset of Socher et al. (2013b), presenting the entire training set of correct and noise-contrastive corrupted examples one instance at a time in randomized order for each iteration.
We ﬁrst provide an analysis of the behavior of ADMM on the training set, to conﬁrm that it effectively optimizes the joint objective. Fig. 1(left) plots the learning curve by training iteration for various values of the ρ hyperparameter. We see that ADMM attains a reasonable objective value relatively quickly in 100 iterations. Fig. 1(right) shows the averaged difference between the resulting sets of embeddings w and v, which decreases as desired.2
Next, we compare the embeddings learned with different objectives on three standard benchmark tasks (Table 1). First, the Knowledge Base Completion task (Socher et al., 2013b) evaluates the models’ ability to classify relationship triples from WordNet as correct. Triples are scored using the relational scoring functions (Eq.3 and 4) with the learned model parameters. The model uses a development set of data to determine a plausibility threeshold, and classiﬁes triples with a higher score than the threshold as correct, and those with lower score as incorrect. Secondly, the SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012). Given a set of four or ﬁve word pairs, the model selects the pairs that most and least represent a particular relation (deﬁned by a set of example word pairs) by comparing the cosine similarity of the vector difference between words in each pair. Finally, the Dependency Parsing task on the SANCL2012 data (Petrov & McDonald, 2012) evaluates the accuracy of parsers trained on news domain adapted for web domain. We incorporate the embeddings as additional features in
1Berkeley distribution: tomato.banatao.berkeley.edu:8080/berkeleylm_binaries/ 2The reason for the peak around iteration 50 in Fig. 1 is that the embeddings begin with similar random initializations, so initially differences are small; as ADMM starts to see more data, w and v diverge, but converge eventually as y become large.

3

Accepted as a workshop contribution at ICLR 2015
a standard maximum spanning tree dependency parser to see whether embeddings improve generalization of out-of-domain words. The evaluation metric is the labeled attachment score, the accuracy of predicting both correct syntactic attachment and relation label for each word. For both Knowledge Base and Parsing tasks, we observe that joint objective generally improves over single objectives: e.g. TransE+NLM (83.10%) > TransE (82.87%) for Knowledge Base, GD+NLM (76.18%) > GD (75.90%) for Parsing. The improvements are not large, but relatively consistent. For the Analogy Test, joint objectives did not improve over the single objective NLM baseline. We provide further analysis as well as extended descriptions of methods and experiments in a longer version of the paper here: http://arxiv.org/abs/1412.4369.
ACKNOWLEDGMENTS
This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported by the Flinn Scholarship during the course of this work. We thank Haixun Wang, Jun’ichi Tsujii, Tim Baldwin, Yuji Matsumoto, and several anonymous reviewers for helpful discussions at various stages of the project.
REFERENCES
Bengio, Yoshua, Ducharme, Re´jean, Vincent, Pascal, and Jauvin, Christian. A neural probabilistic language models. JMLR, 2003.
Bordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto, Weston, Jason, and Yakhnenko, Oksana. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pp. 2787–2795, 2013.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1–122, 2011.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.
Cruse, Alan D. Lexical Semantics. Cambridge Univ. Press, 1986.
Harris, Zellig. Distributional structure. Word, 10(23):146–162, 1954.
Jurgens, David A, Turney, Peter D, Mohammad, Saif M, and Holyoak, Keith J. Semeval-2012 task 2: Measuring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pp. 356–364. Association for Computational Linguistics, 2012.
Leacock, Claudia and Chodorow, Martin. Combining local context and WordNet similarity for word sense identiﬁcation. WordNet: An Electronic Lexical Database, pp. 265–283, 1998.
Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geoffrey. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 746–751, Atlanta, Georgia, June 2013a. Association for Computational Linguistics. URL http://www. aclweb.org/anthology/N13-1090.
Mikolov, Toma´s˘, Sutskever, Ilya, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Distributed representations of words and phrases and their compositionality. In NIPS, 2013b.
Miller, George A. WordNet: A lexical database for English. Communications of the ACM, 38(11): 39–41, 1995.
Mnih, Andriy and Kavukcuoglu, Koray. Learning word embeddings efﬁciently with noisecontrastive estimation. In Advances in Neural Information Processing Systems 26 (NIPS 2013), 2013.
4

Accepted as a workshop contribution at ICLR 2015
Petrov, Slav and McDonald, Ryan. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), 2012.
Schwenk, Holger. Continuous space language models. Computer Speech and Language, 21(3): 492–518, July 2007. ISSN 0885-2308. doi: 10.1016/j.csl.2006.09.003. URL http://dx. doi.org/10.1016/j.csl.2006.09.003.
Socher, Richard, Bauer, John, Manning, Christopher D., and Ng, Andrew Y. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 455–465, Soﬁa, Bulgaria, August 2013a. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ P13-1045.
Socher, Richard, Chen, Danqi, Manning, Christopher D., and Ng, Andrew Y. Reasoning with neural tensor networks for knowledge base completion. In NIPS, 2013b.
Turian, Joseph, Ratinov, Lev-Arie, and Bengio, Yoshua. Word representations: A simple and general method for semi-supervise learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 384–394, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P10-1040.
Wang, Mengqiu and Manning, Christopher D. Effect of non-linear deep architecture in sequence labeling. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pp. 1285–1291, Nagoya, Japan, October 2013. Asian Federation of Natural Language Processing. URL http://www.aclweb.org/anthology/I13-1183.
5

