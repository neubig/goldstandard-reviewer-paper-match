arXiv:1605.03933v1 [cs.DS] 12 May 2016

Competitive analysis of the top-K ranking problem

Xi Chen ∗

Sivakanth Gopi † Jieming Mao ‡ May 13, 2016

Jon Schneider §

Abstract

Motivated by applications in recommender systems, web search, social choice and

crowdsourcing, we consider the problem of identifying the set of top K items from noisy

pairwise comparisons. In our setting, we are non-actively given r pairwise comparisons

between each pair of n items, where each comparison has noise constrained by a very

general noise model called the strong stochastic transitivity (SST) model. We analyze

the competitive ratio linear time algorithm

of algor for the

ithms top-K

f

or pr

the top-K problem oblem which has a

. In com

particular, we p petitive ratio of

rO˜es(e√nnt

a );

i.e. to solve any instance of top-K, our algorithm needs at most O˜(√n) times as

many samples needed as the best possible algorithm for that instance (in contrast, all previous known algorithms for the top-K problem have competitive ratios of Ω˜ (n) or

worse). We competitive

further show ratio at least

tΩ˜h(a√t nth).is

is

tight:

any

algorithm

for

the

top-K

problem

has

∗Stern School of Business, New York University, email: xchen3@stern.nyu.edu †Department of Computer Science, Princeton University, email: sgopi@cs.princeton.edu ‡Department of Computer Science, Princeton University, email: jiemingm@cs.princeton.edu §Department of Computer Science, Princeton University, email: js44@cs.princeton.edu
1

Contents

Contents

2

1 Introduction

3

1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

2 Preliminaries and Problem Setup

5

3 Main Results

7

3.1 Main Techniques and Overview . . . . . . . . . . . . . . . . . . . . . . . . . 8

4 Lower bounds on the sample complexity of domination

10

5 Domination in the well-behaved regime

12

5.1 5.2

CO˜o(√unnt)in-cgomalpgoertiitthivme aalngdormithamx sal.go.ri.th.m.

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

12 17

5.2.1 Combining counting and max . . . . . . . . . . . . . . . . . . . . . . 18

5.2.2 The sum of cubes algorithm . . . . . . . . . . . . . . . . . . . . . . . 19

6 D6.1omAinnatO˜io(√nni)n-ctohmepgeteintievreaallgroergitimhme . . . . . . . . . . . . . . . . . . . . . . . .

21 21

6.2 Acount and Amax have unbounded competitive ratios . . . . . . . . . . . . . . 28

7 Reducing top-K to domination

31

8 Hardness of domination and top-K

35

8.1 A hard distribution for domination . . . . . . . . . . . . . . . . . . . . . . . 36

8.2 Proof of hardness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

8.3 Proving hardness for Top-K . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

References

47

A Probability and Information Theory Preliminaries

49

2

1 Introduction

The problem of inferring a ranking over a set of n items, such as documents, images, movies,

or URL links, is an important problem in machine learning and ﬁnds many applications

in recommender systems, web search, social choice, and many other areas. One of the

most popular forms of data for ranking is pairwise comparison data, which can be easily

collected via, for example, crowdsourcing, online games, or tournament play. The problem

of ranking aggregation from pairwise comparisons has been widely studied and most work

aims at inferring a total ordering of all the items (see, e.g., [NOS12]). However, for some

applications with a large number of items (e.g., rating of restaurants in a city), it is only

necessary to identify the set of top K items. For these applications, inferring the total global

ranking order unnecessarily increases the complexity of the problem and requires signiﬁcantly

more samples.

In the basic setting for this problem, there is a set of n items with some true underlying

ranking. For possible pair (i, j) of items, an analyst is given r noisy pairwise comparisons

between those two items, each independently ranking i above j with some probability pij.

From this data, the analyst wishes to identify the top K items in the ranking, ideally using as

few samples r as is necessary to be correct with suﬃciently high probability. The noise in the

pairwise comparisons (i.e. the probabilities pij) is constrained by the choice of noise model.

Many existing models - such as the Bradley-Terry-Luce model (BTL) [BT52, Luc59], the

Thurstone model [Thu27], and their variants - are parametric comparison models, in that

each probability pij is of the form f (si, sj), where si is a ‘score’ associated with item i.

While these parametric models yield many interesting algorithms with provable guarantees

[CS15, JKSO13, STZ16], the models enforce strong assumptions on the probabilities of

incorrect pairwise comparisons that might not hold in practice [DM59, ML65, Tve72, BW97].

A more general class of pairwise comparison model is the strong stochastic transitivity

(SST) model, which subsumes the aforementioned parameter models as special cases and has

a wide range of applications in psychology and social science (see, e.g., [DM59, ML65, Fis73]).

The SST model only enforces the following coherence assumption: if i is ranked above j, then

pil ≥ pjl for all other items l. [SBGW15] pioneered the algorithmic and theoretical study

of ranking aggregation under SST models. For top-K ranking problems, [SW15] proposed

a counting-based algorithm, which simply orders the items by the total number of pairwise

comparisons won. For a certain class of instances, this algorithm is in fact optimal; any

algorithm with a constant probability of success on these instances needs roughly at least

as many samples as this counting algorithm. However, this does not rule out the existence

of other instances where the counting algorithm performs asymptotically worse than some

other algorithm.

In this paper, we petitive analysis. We

study algorithms give an algorithm

for the which,

top-K on any

problem from the instance, needs at

smtaonsdt pO˜o(in√tno)f

comtimes

as many samples as the best possible algorithm for that instance to succeed with the same

probability. We further show that algorithm needs at least

tΩ˜hi(s√rnes)utlitmisestigahs tm: afonryasnaymaplgleosriatshmth,ethbeerset

are instances where possible algorithm.

In contrast, the counting algorithm of [SBGW15] sometimes requires Ω(n) times as many

3

samples as the best possible algorithm, even when the probabilities pij are bounded away from 1.
Our main technical tool is the introduction of a new decision problem we call domination, which captures the diﬃculty of solving the top K problem while being simpler to directly analyze via information theoretic techniques. The domination problem can be thought of as a restricted one-dimensional variant of the top-K problem, where the analyst is only given the outcomes of pairwise comparisons that involve item i or j, and wishes to determine whether i is ranked above j. Our proof of the above claims proceeds by proving analogous competitive ratio results for the domination problem, and then carefully embedding the domination problem as part of the top-K problem.
1.1 Related Work
The problem of sorting a set of items from a collection of pairwise comparisons is one of the most classical problems in computer science and statistics. Many works investigate the problem of recovering the total ordering under noisy comparisons drawn from some parametric model. For the BTL model, Negahban et al. [NOS12] propose the RankCentrality algorithm, which serves as the building block for many spectral ranking algorithms. Lu and Boutilier [LB11] give an algorithm for sorting in the Mallows model. Rajkumar and Agarwal [RA14] investigate which statistical assumptions (BTL models, generalized lownoise condition, etc.) guarantee convergence of diﬀerent algorithms to the true ranking.
More recently, the problem of top-K ranking has received a lot of attention. Chen and Suh [CS15], Jang et al. [JKSO13], and Suh et al. [STZ16] all propose various spectral methods for the BTL model or a mixture of BTL models. Eriksson [Eri13] considers a noisy observation model where comparisons deviating from the true ordering are i.i.d. with bounded probability. In [SW15], Shah and Wainwright consider the general SST models and propose the counting-based algorithm, which motivates our work. The top-K ranking problem is also related to the best K arm identiﬁcation in multi-armed bandit [BWV13, JMN+14, ZCL14]. However, in the latter problem, the samples are i.i.d. random variables rather than pairwise comparisons and the goal is to identify the top K distributions with largest means.
This paper and the above references all belong to the non-active setting: the set of data provided to the algorithm is ﬁxed, and there is no way for the algorithm to adaptively choose additional pairwise comparisons to query. In several applications, this property is desirable, speciﬁcally if one is using a well-established dataset or if adaptivity is costly (e.g. on some crowdsourcing platforms). Nonetheless, the problems of sorting and top-K ranking are incredibly interesting in the adaptive setting as well. Several works [Ail11, JN11, KMS07, BM08] consider the adaptive noisy sorting problem with (noisy) pairwise comparisons and explore the sample complexity to recover an (approximately) correct total ordering in terms of some distance function (e.g,., Kendall’s tau). In [WMJ13], Wauthier et al. propose simple weighted counting algorithms to recovery an approximate total ordering from noisy pairwise comparisons. Dwork et al. [DKNS01] and Ailon et al. [ACN08] consider a related Kemeny optimization problem, where the goal is to determine the total ordering that minimizes the
4

sum of the distances to diﬀerent permutations. More recently, the top-K ranking problem in the active setting has been studied by Braverman et al. [BMW16] where they consider the sample complexity of algorithms that use a constant number of rounds of adaptivity. All of this work takes place in much more constrained noise models than the SST model. Extending our work to the active setting is an interesting open problem.
2 Preliminaries and Problem Setup
Consider the following problem. An analyst is given a collection of n items, labelled 1 through n. These items have some true ordering deﬁned by a permutation π : {1, . . . , n} → {1, . . . , n} such that for 1 ≤ u < v ≤ n, the item labelled π(u) has a better rank than the item labelled π(v) (i.e., the item with label i has a better rank than the item j if and only if π−1(i) < π−1(j)). The analyst’s goal is to determine the set of the top K items, i.e., {π(1), . . . , π(k)}.
The analyst receives r samples. Each sample consists of pairwise comparisons between all pairs of items. All the pairwise comparisons are independent with each other. The outcomes of the pairwise comparison between any two items is characterized by the probability matrix P ∈ [0, 1]n×n. For a pair of items (i, j), let Xi,j ∈ {0, 1} be the outcome of the comparison between the item i and j, where Xi,j = 1 means i is preferred to j (denoted by i ≻ j) and Xi,j = 0 otherwise. Further, let B(z) denote the Bernoulli random variable with mean z ∈ [0, 1]. The outcome Xi,j follows B(Pπ−1(i),π−1(j)), i.e.,
Pr(Xi,j = 1) = Pr(i ≻ j) = P . π−1(i),π−1(j)
The probability matrix P is said to be strong stochastic transitive (SST) if it satisﬁes the following deﬁnition.
Deﬁnition 2.1. The n × n probability matrix P ∈ [0, 1]n×n is strong stochastic transitive (SST) if
1. For 1 ≤ u < v ≤ n, Pu,l ≥ Pv,l for all l ∈ [n].
2. P is shifted-skew-symmetric (i.e., P − 0.5 is skew-symmetric) where Pv,u = 1 − Pu,v and Pu,u = 0.5 for u ∈ [n].
The ﬁrst condition claims that when the item i has a higher rank than item j (i.e., π−1(i) < π−1(j)), for any other item k, we have
Pr(i ≻ k) = Pπ−1(i),π−1(k) ≥ Pr(j ≻ k) = P . π−1(j),π−1(k)
Remark 2.1. Many classical parametric models such that BTL [BT52, Luc59] and Thurstone (Case V) [Thu27] models are special cases of SST. More speciﬁcally, parametric models assume a score vector w1 ≥ w2 ≥ . . . ≥ wn. They further assume that the comparison probability Pu,v = F (wu − wv), where F : R → [0, 1] is a non-decreasing function and F (t) = 1 − F (−t) (e.g., F (t) = 1/(1 + exp(−t)) in BTL models). By the property of F , it is easy to verify that Pu,v = F (wu − wv) satisfy the conditions in Deﬁnition 2.1.
5

Under the SST models, we can formally deﬁne the top-K ranking problem as follows. The top-K ranking problem takes the inputs n, k, r that are known to the algorithm and the SST probability matrix P that is unknown to the algorithm.
Deﬁnition 2.2. Top-K(n, k, P, r) is the following algorithmic problem:
1. A permutation π of [n] is uniformly sampled.
2. The algorithm is given samples Xi,j,l for i ∈ [n], j ∈ [n], l ∈ [r], where each Xi,j,l is sampled independently according to B(Pπ−1(i),π−1(j)). The algorithm is also given the value of k, but not π or the matrix P.
3. The algorithm succeeds if it correctly outputs the set of labels {π(1), ..., π(k)} of the top k items.
Remark 2.2. We note that [SW15] considers a slightly diﬀerent observation model in which each pair is queried r times. For each query, one can obtain a comparison result with the probability pobs ∈ (0, 1] and with probability 1 − pobs, the query is invalid. In this model, each pair will be compared r · pobs times on expectation. When pobs = 1, it reduces to our model in Deﬁnition 2.2, where we observe exactly r comparisons for each pair. Our results can be easily extended to deal with the observation model in [SW15] by replacing r with the eﬀective sample size, r · pobs. We omit the details for the sake of simplicity.
Our primary metric of concern is the sample complexity of various algorithms; that is, the minimum number of samples an algorithm A requires to succeed with a given probability. To this end, we call the triple S = (n, k, P) an instance of the Top-K problem, and write rmin(S, A, p) to denote the minimum value such that for all r ≥ rmin(S, A, p), A succeeds on instance S with probability p when given r samples. When p is omitted, we will take p = 34; i.e., rmin(S, A) = rmin(S, A, 34 ).
Instead of working directly with Top-K, we will spend most of our time working with a problem we call Domination, which captures the core of the diﬃculty of the Top-K problem. Domination is formally deﬁned as follows.
Deﬁnition 2.3. Domination(n, p, q, r) is the following algorithmic problem:
1. p = (p1, · · · , pn) and q = (q1, · · · , qn) are two vectors of probabilities that satisfy 1 ≥ pi ≥ qi ≥ 0 for all i ∈ [n]. p, q are not given to the algorithm.
2. A random bit B is sampled from B( 12 ). Samples Xi,j, Yi,j (for i ∈ [n], j ∈ [r]) are generated as follows:
(a) Case B = 0: each Xi,j is independently sampled according to B(pi) and each Yi,j is independently sampled according to B(qi).
(b) Case B = 1: each Xi,j is independently sampled according to B(qi) and each Yi,j is independently sampled according to B(pi).
6

The algorithm is given the samples Xi,j and Yi,j, but is not given the bit B or the values of p and q.
3. The algorithm succeeds if it correctly outputs the value of the hidden bit B.
As before, we are interested in the sample complexity of algorithms for Domination. We call the triple C = (n, p, q) an instance of Domination, and write rmin(C, A, p) to be the minimum value such that for all r ≥ rmin(C, A, p), A succeeds at solving Domination(n, p, q, r) with probability at least p (similarly, we let rmin(C, A) = rmin(C, A, 43 )).

3 Main Results

There are at least two main approaches one can take to analyze the sample complexity of

problems like Top-K and Domination. The ﬁrst (and more common) is to bound the

value of rmin(S, A) by some explicit function f (S) of the instance S. This is the approach

taken by [SW15]. They show that for some simple function f (roughly, the square of the

reciprocal of the absolute diﬀerence of the sums of the k-th and (k + 1)-th rows of the

matrix P i.e. 1/ Pk − Pk+1 21), there is an algorithm A such that for all instances S,

rmin(S, A) = O(f (S)); moreover this is optimal in the sense that there exists an instance S

such that for all algorithms A, rmin(S, A) = Ω(f (S)). While this is a natural approach, it

leaves open the question of what the correct choice of f should be; indeed, diﬀerent choices

of f give rise to diﬀerent ‘optimal’ algorithms A which outperform each other on diﬀerent

instances.

In this paper, we take the second approach, which is to compare the sample complexity

of an algorithm on an instance to the sample complexity of the best possible algorithm on

that instance. Formally, let rmin(S, p) = infA rmin(S, A, p) and let rmin(S) = rmin(S, 43 ). An ideal algorithm A would satisfy rmin(S, A) = Θ(rmin(S)) for all instances S of Top-K;

more generally, we are interested in bounding the ratio between rmin(S, A) and rmin(S). We

call this ratio the competitive ratio of the algorithm, and say that an algorithm is f (n)-

competitive if rmin(S, A) ≤ f (n)rmin(S). (We likewise deﬁne all the corresponding notions

for Domination).

O˜(√Inn)o-ucormmpaeitnitiuvpep(erresbtaotuenmdenretsoufltC, owreollgairvye

a linear-time 7.5):

algorithm

for

Top-K

which

is

Theorem 3.1. There is an algorithm A for Top-K such that A runs in time O(n2r) and on every instance S of Top-K on n items,
√ rmin(S, A) ≤ O( n log n)rmin(S). In our main lower bound result, we show that up to logarithmic factors, this √n competitive ratio is optimal (restatement of Theorem 8.1):

Theorem 3.2. For any algorithm A for Top-K, there exists an instance S of Top-K on

n items such that

rmin(S, A) ≥ Ω

√n log n

rmin(S).

7

In comparison, for the counting algorithm A′ of [SW15], there exist instances S such that rmin(S, A′) ≥ Ω˜ (n)rmin(S). For example, consider the instance S = (n, k, P) with

1
2

1 2

+

ε

···

···

1 2

+

ε

1 − ε

... 

2

P

=

 

...

... ... ...



...

 

 ...

 21 + ε

21 − ε · · · · · · 21 − ε 21

It is straightforward to show that with Θ(log n/ε2) samples, we can learn all pairwise
comparisons correctly with high probability by taking a majority vote, and therefore even sort all the elements correctly. This implies that rmin(S) = O(log n/ε2). On the other hand, we show in Corollary 5.4 that rmin(S, A′) = Ω(n/ε2) when ε < 1/10.

3.1 Main Techniques and Overview

We prove our main results by ﬁrst proving similar results for Domination which we deﬁned

in Deﬁnition 2.3. Intuitively Domination captures the main hardness of Top-K while

being much simpler to analyze. Once we prove upper bound and lower bounds for the

sample complexity of Domination, we will use reductions to prove analogous results for

Top-K.

We begin in Section 4, by proving a general lower bound on the sample complexity of

domination. Explicitly, for a given instance C = (n, p, q) of Domination, we show that

rmin(C) ≥ Ω(1/I(p, q)) where I(p, q) is the amount of information we can learn about the

bit B from one sample of pairwise comparison in each of the coordinates.

In Section 5, we proceed to design algorithms for Domination restricted to instances

C = (n, p, q) where δ ≤ pi, qi ≤ 1 − δ for some constant 0 < δ ≤ 1/2. In this regime

I(p, q) = Θ(1/ p − q 22), which makes it easier to argue our algorithms are not too bad

compared with the optimal one. We ﬁrst consider an algorithm we call the counting algo-

rithm Acount (Algorithm 1), which is a Domination analogue of the counting algorithm proposed by [SW15]. We show that Acount has a competitive ratio of Θ˜ (n). Intuitively, the

main reason Acount fails is that Acount tries to consider samples from diﬀerent coordinates

equally important even when they are sampled from a very unbalanced distribution (for

example, p1 = q1, p2 = q2, ..., pn = qn). We then consider another algorithm we call the max algorithm Amax (Algorithm 2) which simply ﬁnds i′ = maxi | rj=1(Xi,j − Yi,j)| and outputs B according the sign of rj=1(Xi′,j − Yi′,j). We show Amax also has a competitive ratio of Θ˜ (n). Interestingly, Amax fails for a diﬀerent reason from Acount, namely that Amax

does not use the information fully from all coordinates when the samples are sampled from

a very balanced distribution. In fact, Acount performs well whenever Amax fails and vice

versa. We therefore show how combine Acount and Amax in two diﬀerent ways to get two

new new

algorithms: Acomb algorithms have a

(Algorithm competitive

3r)atainodofAO˜cu(b√e (nA),lgwohriitchhmis4t)i.ghWt ebyshTowhetohreamt b8o.t2h.

of

these

8

In Section 6, we design algorithms for Domination in the general regime. In this

regime, I(p, q) can be much larger than p − q 22, particularly for values of pi and qi very

close to 0 or 1. In these corner cases, the counting algorithm Acount and max algorithm

Amax can fail very badly; we will show that even for ﬁxed n, their competitive ratios can

grow arbitrarily large (Lemma 6.6 and Lemma 6.7). One main reason for this failure is

that, even when |pi − qi| < |pj − qj|, samples from coordinate i could convey much more

information than the samples from coordinate j (consider, for example, pi = ε/2, qi = 0,

and pj = 1/2 + ε, qj (Algorithm 5) which

=has1/a2)c.omTapkeitnitgivtehirsatiniotoofacO˜co(√unnt),

we in

design a new algorithm Acoup the general regime. The new

algorithm still combines features from both Acount and Amax, but also better estimates the

importance of each coordinate. To estimate how much information each coordinate has, the

new algorithm divides the samples into Θ(log n) groups and checks how often samples from

coordinate i are consistent with themselves. If one coordinate has a large proportion of the

total information, it uses samples from that coordinate to decide B, otherwise it takes a

majority vote on samples from all coordinates.

ratiIonoSf eO˜ct(i√onn)7,,thwues

return to Top-K proving Theorem

and 3.1.

present an algorithm that has a Our algorithm works by reducing

competitive the Top-K

problem to several instances of the Domination problem (see Theorem 6.5). At a high

level, the algorithm tries to ﬁnd the top k rows by pairwise comparisons of rows, each of

which can be thought of as an instance of Domination. We use algorithm Acoup to solve

these Domination instances. Since we only need to make at most n2 comparisons, if Acoup

outputs

the

correct

answer

with

at

least

1−

ε n2

probability

for

each

comparison,

then

by

union bound all the comparisons will be correct with probability at least 1 − ε. However,

to ﬁnd the top k rows, we do not actually need to compare all the rows to each other;

Lemma 7.1 shows that we can ﬁnd the top k rows with high probability while making only

O(n) comparisons. Using this lemma, we get a linear time algorithm for solving Top-K.

Finally in Lemma 7.4, we extend the lower bound for Domination proved in Lemma 4.2

to show a lower bound on the number of samples any algorithm would need on a speciﬁc

instance of Top-K. Combining these results, we prove Theorem 3.1.

Finally, in Section 8, we show that the algorithms for both Domination and Top-K

presented in the previous sections have the optimal competitive ratio (up to polylogarithmic

ifnacsttaonrsc)e. CSpoefcdiﬁocmalilnya, twioenshwohwertehramt ifno(rCa,nAy)a≥lgoΩ˜ri(t√hmn)rAmsino(lCvi)ng(TDhoeomrienmat8i.o2)n. ,Wtheeraecceoxmistpsliashn

this by constructing a distribution C over instances of Domination such that each instance

i(nTthheeorseumpp8o.r5t) obfutthiasndyisatlrgiobruittihomn ctahnatbysuscocleveeddsboyvearntahlegoernitthirme dwisitthriblouwtiosanmrepqleuicroems pΩ˜l(e√xinty)

tinimSeesctmioonre7)satmo pshleosw(Tanheaonreamlog8o.u7s).Ω˜W(√ent)helonweemr bbeodunDdofomrinTaotpio-Kn

in Top-K (Theorem

(similarly 8.1).

as

9

4 Lower bounds on the sample complexity of domination

We start by establishing lower bounds on the number of samples rmin(C) needed by any algorithm to succeed with constant probability on a given instance C = (n, p, q) of Domination. This is controlled by the quantity I(p, q), which is the amount of information we can learn about the bit B given one sample of pairwise comparison between each of the coordinates of p and q.

Deﬁnition 4.1. Given 0 ≤ p, q ≤ 1, deﬁne

I(p, q) = (p(1 − q) + q(1 − p)) 1 − H p(1 − q) . p(1 − q) + q(1 − p)
Given p = (p1, · · · , pn) ∈ [0, 1]n, q = (q1, · · · , qn) ∈ [0, 1]n, deﬁne
n
I(p, q) = I(pi, qi).
i=1
Lemma 4.2. Let C = (n, p, q) be an instance of Domination. Then rmin(C) ≥ 0.05/I(p, q).

Proof. The main idea is to bound the mutual information between the samples and the
correct output, and then apply Fano’s inequality. Let p = (p1, · · · , pn) and q = (q1, · · · , qn). Recall that B indicates the correct output and that X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r are the samples given to the algorithm. By Fact A.6,

I(B; X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r) = I(B; X1,1Y1,1)+I(B; X1,2, ..., Xn,r, Y1,2, ..., Yn,r|X1,1Y1,1).

When p, q and B are given, each sample (Xi,j or Yi,j) is independent of the other samples, and thus I(X1,1Y1,1; X1,2, ..., Xn,r, Y1,2, ..., Yn,r|B) = 0. By Fact A.7, we then have

I(B; X1,2, ..., Xn,r, Y1,2, ..., Yn,r|X1,1Y1,1) ≤ I(B; X1,2, ..., Xn,r, Y1,2, ..., Yn,r)

and therefore

I(B; X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r) ≤ I(B; X1,1Y1,1) + I(B; X1,2, ..., Xn,r, Y1,2, ..., Yn,r).

Repeating this, we get

nr

I(B; X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r) ≤

I(B; Xi,jYi,j).

i=1 j=1

By Fact A.9, we have

I(B; Xi,jYi,j)

= Pr[B = 0] · D(Xi,jYi,j|B = 0 Xi,jYi,j) + Pr[B = 1] · D(Xi,jYi,j|B = 1 Xi,jYi,j)

= (pi(1 − qi) + qi(1 − pi))

1−H

pi(1 − qi) pi(1 − qi) + qi(1 − pi)

= I(pi, qi).

10

It follows that

nr

n

I(B; X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r) ≤

I(B; Xi,jYi,j) = r · I(pi, qi) = rI(p, q).

i=1 j=1

i=1

For any algorithm, let pe be its error probability on Domination(n, p, q, r). By Fano’s inequality, we have that

H(pe) ≥ H(B|X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r) = H(B) − I(B; X1,1, X1,2, ..., Xn,r, Y1,1, ..., Yn,r) = 1 − rI(p, q) ≥ 0.95.

Since H(pe) ≥ 0.95, we ﬁnd that pe ≥ 1/4, as desired.

In the following section, we will concern ourselves with instances C = (n, p, q) that satisfy δ ≤ pi, qi ≤ 1 − δ for some constant δ for all i. For such instances, we can approximate I(p, q) by the ℓ2 distance between p and q.

Lemma 4.3. For some 0 < δ ≤ 12, let δ ≤ p, q ≤ 1 − δ. Then

1 (p − q)2 ≤ I(p, q) ≤ 1 (p − q)2.

4 ln 2

δ ln 2

Proof.

Let x = p(1−q) and y

= q(1−p).

Then

I(p,

q)

=

(x + y )(1 − H (

x x+y

))

and

p−q

= x−y.

We need to show that

(x + y) 1 − H x x+y

≤ 1 (x − y)2. δ ln 2

By Fact A.10,

1 z2 ≤ 1 − H 1 + z = D 1 + z 1 ≤ 4 z2,

ln 2

2

2

2 ln 2

and therefore

1 (x − y)2 ≤ (x + y) 1 − H 4 ln 2 (x + y)

x x+y

≤ 1 (x − y)2 . ln 2 (x + y)

Since

x + y = p(1 − q) + q(1 − p) ≥ 2 p(1 − p)q(1 − q) ≥ 2δ(1 − δ) ≥ δ,

this implies the desired upper bound. The lower bound also holds since,

x + y = p(1 − q) + q(1 − p) ≤ p2 + (1 − p)2 · q2 + (1 − q)2 ≤ δ2 + (1 − δ)2 ≤ 1.

Corollary 4.4. Let C = (n, p, q) be an instance of Domination satisfying δ ≤ pi, qi ≤ 1−δ

for all i ∈ [n]. Then

δ rmin(C) ≥ 0.05 ln(2) · p − q 2 .
2

Proof. By Lemma 4.3, I(p, q) ≤

p−q

2 2

/(δ

ln

2).

The

result

then

follows from Lemma 4.2.

11

5 Domination in the well-behaved regime

We now proceed to the problem of designing algorithms for Domination which are compet-

itive on all instances. As a warmup, we begin by considering only instances C = (n, p, q) of

Domination satisfying δ ≤ pi, qi ≤ 1 − δ for all i ∈ [n] where 0 < δ ≤ 1/2 is some ﬁxed con-

stant. This regime of instances captures much of the interesting behavior of Domination,

but with the added beneﬁt that the mutual information between the samples and B behaves

nicely in this regime: in particular I(p, q) = Θ( p − q 22) (see Lemma 4.3). By Corol-

lary 4.4, we have rmin

≥ Ω(

1 p−q

2 ).

This fact will make it easier to design algorithms for

2

Domination which are competitive in this regime.

In Section 5.1, we give two simple algorithms (counting algorithm and max algorithm) which can solve Domination given O˜(n/ p − q 22) samples which gives them a competitive ratio of O˜(n). We will then show that this is tight, i.e. their competitive ratio is Θ˜ (n) in

Lemma 5.3 and Lemma 5.5. While the sample complexities of these two algorithms are not

optimal, they have the nice property that whenever one performs badly, the other performs

well. In to give

Section 5.2, two diﬀerent

we show how to co algorithms which

mbine can so

the lve

counting algo Domination

rithm using

and t only

hO˜e(m√anx/

alg p

or −

ithm q 2)

samples

i.e.

they

have

a

competitive

ratio

of

O˜(√n).

According

to

Theorem

8.2,

this

is

2
the

best we can do up to polylogarithmic factors.

5.1 Counting algorithm and max algorithm

We now consider two simple algorithms for Domination(n, p, q), which we call the counting

algorithm (Algorithm 1) and the max algorithm (Algorithm 2) denoted by Acount and Amax

respectively. We show that both algorithms require O˜( n 2 ) samples to solve Domination

p−q 2

(Lemmas 5.1 and 5.2).

By

Corollary

4.4,

we

have

rmin

≥

Ω(

1 p−q

2 ),

leading

to

a

O˜(n)

2

competitive ratio for these algorithms. We show in Lemma 5.3 and Lemma 5.5 that this is

tight up to polylogarithmic factors i.e. their competitive ratio is Θ˜ (n).

Algorithm 1 The counting algorithm Acount for Domination(n, p, q, r)

1: for i = 1 to n do 2: Si = rj=1(Xi,j − Yi,j)

3: end for

4: Z =

n i=1

Si

5: If Z > 0, output B = 0. If Z < 0, output B = 1. If Z = 0, output B = 0 with

probability 1/2 and output B = 1 with probability 1/2.

Both the counting algorithm and the max algorithm begin by computing (for each coor-
dinate i) the diﬀerences between the number of ones in the Xi,j samples and Yi,j samples; i.e., we compute the values Si = rj=1(Xi,j − Yi,j). The counting algorithm Acount decides whether to output B = 0 or B = 1 based on the sign of i Si, whereas the max algorithm

12

Algorithm 2 The max algorithm Amax for Domination(n, p, q, r)
1: for i = 1 to n do 2: Si = rj=1(Xi,j − Yi,j) 3: end for 4: i′ = arg max |Si| 5: Z = Si′ 6: If Z > 0, output B = 0. If Z < 0, output B = 1. If Z = 0, output B = 0 with
probability 1/2 and output B = 1 with probability 1/2.

decides its output based on the sign of the Si with the largest absolute value. See Algorithms 1 and 2 for detailed pseudocode for both Acount and Amax.
We begin by proving upper bounds for the sample complexities of both Acount and Amax. In particular, both Acount and Amax need at most O˜(n) times as many samples as the best possible algorithm for any instance in this regime.
Lemma 5.1. Let C = (n, p, q) be an instance of Domination. Then
2n ln(α−1) rmin(C, Acount, 1 − α) ≤ p − q 2 .
1
If C further satisﬁes δ ≤ pi, qi ≤ 1 − δ for all i for some constant δ > 0, then

rmin(C, Acomb) ≤ O(n)rmin(C).

Proof. Let pe be the probability that B = 0 and Acount outputs B = 1 when provided with

r=

2n ln(α−1)
2

samples.

By symmetry pe

is equal to the probability that we are in the case

p−q 1

B = 1 and Acount outputs B = 0 when provided with r samples. It therefore suﬃces to show

that pe is at most α. When B = 0,

E[Z] = E

n
Si
i=1

n
= r (pi − qi).
i=1

By the Chernoﬀ bound,

nr pe ≤ Pr[Z ≤ 0] ≤ exp − 2 ·

ni=1(pi − qi) 2 n

≤ α.

The second part of the lemma follows from Corollary 4.4, along with the observation that

p−q

2 1

≥

p − q 22.

Lemma 5.2. Let C = (n, p, q) be an instance of Domination. Then

8 ln(2nα−1) rmin(C, Amax, 1 − α) ≤ p − q 2
∞

13

If C further satisﬁes δ ≤ pi, qi ≤ 1 − δ for all i for some constant δ, then

rmin(C, Acomb) ≤ O(n log n)rmin(C).

Proof. Assume without loss of generality that B = 0, and let ε = p1 − q1 = p − q ∞. Let E

be

the

event

that

Amax

makes

an

error

and

outputs

B

=

1

when

given

r

=

8 ln 2nα−1 ε2

samples.

We can upper bound the probability of error as

Pr[E] ≤ Pr[E|S1 > rε/2] + Pr[S1 ≤ rε/2].

We will bound each term separately. Since E[S1] = r(p1 −q1) = rε, by Hoeﬀding’s inequality, Pr[S1 ≤ rε/2] ≤ exp(−rε2/8) ≤ α . 2
Similarly, by Hoeﬀding’s inequality and the union bound, Pr[E|S1 > rε/2] ≤ Pr[∃i : Si < −rε/2] ≤ n exp(−rε2/8) ≤ α . 2

It follows that Pr[E] ≤ α. The second part of the lemma follows from Corollary 4.4, along

with the observation that

p−q

2 2

≤

n

p−q

2∞.

We now show that the upper bounds we proved above are essentially tight. In particular, we demonstrate instances where both Acount and Amax need Ω˜ (n) times as many samples as the best possible algorithms for those instances. Interestingly, on the instance where Acount suﬀers, Amax performs well, and vice versa. This fact will prove useful in the next section.

Lemma 5.3. For each ε < 110 and each suﬃciently large n, there exists an instance C = (n, p, q) of Domination such that the following two statements are true:

1.

rmin(C, Amax, 1 − n2 ) ≤

16 ln ε2

n

.

2.

rmin(C, Acount) ≥

n 128ε2

.

Proof. Let k be an arbitrary integer between 1 and n − 1. Let p, q be any vectors satisfying the following constraints:

1.

For

all

i ∈ [n],

1 4

< pi, qi

<

43 .

2. If i ∈ {k, k + 1}, pi = qi .

3. If i ∈ {k, k + 1}, qi = pi − ε .

Note that

p − q ∞ = ε.

Therefore, by Lemma 5.2, rmin(C, Amax, 1 − n2 ) ≤

16 ln ε2

n

,

thus

proving the ﬁrst part of the lemma.

Now assume that r ≤ n/128ε2. We will show that with this many samples, Acount

solves instance C with probability at most 3/4, thus implying the second part of the lemma.

Without loss of generality, assume that B = 0. Deﬁne the following random variables Ui,j:

14

1. Ui,j = Xi,j − Yi,j for i = 1, ..., k − 1, k + 2, ..., n and j = 1, .., r.

2. Ui,j = Xi,j − Yi,j − ε. i = k, k + 1 and j = 1, ..., r. It is straightforward to check that for all i = 1, ..., n, j = 1, .., r, E[Ui,j] = 0, E[Ui2,j] ≥ 1/4 and E[|Ui,j|3] ≤ 1. Let Φ be the cdf of the standard normal distribution.

Pr[Acount outputs B = 1 (incorrectly)]

nr

nr

= Pr[

(Xi,j − Yi,j) < 0] = Pr[

Ui,j < −2rε]

i=1 j=1

i=1 j=1

 ≥ Φ −2rε ·



1

n i=1

r j=1

E[|Ui,j

|3

]

n

r

− E[U 2 ] (

n i=1

r j=1

E[Ui2,j

])−3/2

i=1 j=1

i,j

(By Berry-Esseen theorem (Lemma A.11))

≥ Φ − 8rε2 − √8 ≥ Φ(−1/4) − √8 ≥ 1/4.

n

nr

nr

It is not hard to observe that in certain cases, the counting algorithm of [SW15] for Top-K reduces to the algorithm Acount for Domination. It follows that there also exists an Ω(n) multiplicative gap between the sample complexity of their counting algorithm and the sample complexity of the best algorithm on some instances.

Corollary 5.4. Let A′ be the Top-K algorithm of [SW15], and let S = (n, k, P) be a Top-K

instance, with P as described in Section 3. Then, for suﬃciently large n and ε < 1/10,

rmin(S,

A′)

≥

Ω(

n ε2

).

Proof. Let i = π−1(k) and j = π−1(k + 1). The algorithm A′ correctly places i in the set of

the top k rows exactly when Acount correctly outputs that row i dominates row j. On the

other hand, any two consecutive rows of P satisfy the constraints in the proof of Lemma 5.3.

It

follows

that

rmin(S,

A′)

≥

Ω(

n ε2

)

.

We will now show that Amax has a competitive ratio of Ω˜ (n).

Lemma 5.5. For each suﬃciently large n, there exists an instance C = (n, p, q) of Domination such that the following two statements are true:

1. rmin(C, Acount, 1 − n1 ) ≤ 2n3 ln n.

2.

rmin(C,

Amax,

4 5

)

≥

n4 214 ln

n

.

15

Proof.

Consider

the

instance

C

= (n, p, q)

where

pi

=

1 2

+ε

and

qi

=

21 ,

with

ε

=

1 n2

.

Since

p−q

1=

n1 ,

by

Lemma

5.1,

rmin(C, Amax, 1 −

1 n

)

≤

2n3

ln

n.

Now assume r =

n4 214 ln

n

.

We will now show that Amax solves Domination(n, p, q, r)

with probability at most 4/5. Without loss of generality, assume that B = 0. Deﬁne

random variables Si = rj=1(Xi,j − Yi,j). Note that S1, · · · , Sn are i.i.d random variables

with E[Si] = rε and Var[Si] = r( 12 − ε2). Our algorithm Amax outputs B = 1 whenever infi Si + supi Si < 0. Let λ > 0 be a parameter whose value we will choose later. Note that:

Pr[inf Si + sup Si < 0] ≥ Pr[inf Si < −λ, sup Si < λ]

i

i

i

i

≥ Pr[sup Si < λ] − Pr[inf Si ≥ −λ, sup Si < λ]

i

i

i

n

n

= Pr[Si < λ]n − Pr[−λ ≤ Si < λ]n

i=1

i=1

= Pr[S1 < λ]n − Pr[−λ ≤ S1 < λ]n

= Pr[S1 < λ]n − (Pr[S1 < λ] − Pr[S1 < −λ])n

We will now apply the Berry-Esseen Theorem (Lemma A.11) with Zj = (X1,j − Y1,j) to

approximate the CDF of S1. We have µ = E[S1] = rε, σ2 = Var[S1] = r( 21 − ε2) ≥ 4r . and

γ=

r j=1

E[|Zj

− ε|3]

≤

8r.

Therefore

for

all

t

∈

R,

t−µ Pr[S1 < t] − Φ σ

√ γ 64 215 ln n 1 ≤ σ3 ≤ √r = n2 ≤ n3/2

when n is large enough. Let us choose λ = µ + σΦ−1(1 − lnn2 ) and let a = λ−σµ , b = λ+σµ . Therefore Φ(a) = 1 − lnn2 . When n is large enough, a > 10. By Fact A.12,

√1 exp(−a2/2) 1 ≥ ln 2 = 1 − Φ(a) ≥ √1 exp(−a2/2) 1 .

2π

an

2π

2a

16

√ From the left hand side of the above inequality, we can conclude that a ≤ 2 ln n. Also,

Φ(−b) = 1 − Φ(b) = ln 2 − (Φ(b) − Φ(a)) n

= ln 2 − √1

b
exp −t2/2 dt

n

2π a

≥ ln 2 − √1 (b − a) exp(−a2/2)

n

2π

≥ ln 2 − 2a(ln 2)(b − a)

n

n

≥ ln 2 − 4aµ

n nσ √

≥ ln 2 − 16ε r ln n

n

n

(Since

√1 2π

exp(−a2/2) 21a

≤

lnn2 )

(µ = rε, σ2 ≥ r , a ≤ 2√ln n) 4

≥ ln 2 − 16 1 1

n

n2 n

≥ ln 2 − 1 n 8n

n4 ln n 214 ln n

Now we can bound the probability of error as follows:

Pr[inf Si + sup Si < 0] ≥ Pr[S1 < λ]n − (Pr[S1 < λ] − Pr[S1 < −λ])n

i

i

λ−µ

1n

λ−µ

−λ − µ

1n

≥ Φ σ − n3/2 − Φ σ − Φ

σ

+ 2 · n3/2

1n

2n

= Φ(a) − n3/2 − Φ(a) − Φ(−b) + n3/2

ln 2 1 n

2 ln 2 1 2 n

≥ 1 − n − n3/2 − 1 − n + 8n + n3/2

≥ exp(− ln 2) − exp(−2 ln 2 + 1/8) − 0.01 (when n is large enough)

> 1. 5

5.2 O˜(√n)-competitive algorithms
We will now demonstrate two algorithms for Domination that use at most O˜(√n) times more samples than the best possible algorithm for each instance. According to Theorem 8.2, this is the best we can do up to polylogarithmic factors.
Note that the counting algorithm Acount tends to work well when the max algorithm Amax fails, and vice versa (e.g., Lemmas 5.3 and 5.5). Therefore, intuitively, combining both algorithms in some way should lead to better performance.
17

Both of the algorithms we present in this section share this intuition. We begin (in Lemma 5.6) by demonstrating a very general method for combining any two algorithms for Domination. Apply√ing this to Acount and Amax, we obtain an algorithm Acomb that satisﬁes rmin(C, Acomb) ≤ O( n log n) · rmin(C) (Corollary 5.7) for instances C in this regime. We then show an alternate algorithm with slightly better performance than Acomb, √which we call the sum of cubes algorithm Acube. This algorithm satisﬁes rmin(C, Acube) ≤ O( n) · rmin(C) for instances C in this regime (Theorem 5.10).

5.2.1 Combining counting and max
We ﬁrst show how to combine any two algorithms for Domination to get an algorithm that always does at least as well as the better of the two algorithms. Call an algorithm A for Domination stable if it always outputs the correct answer with probability at least 1/2 (i.e. it always does at least as well as a random guess). Note that Acount and Amax are both stable. We have the following lemma.

Lemma 5.6. Let A1 and A2 be two stable algorithms for Domination. Then there exists an algorithm Acomb such that for all instances C of Domination,

rmin(C, Acomb, 1 − α) ≤ 32 ln(α−1) · min (rmin(C, A1), rmin(C, A2))

Proof. See Algorithm 3 for a description of Acomb. Assume without loss of generality that B = 0, and let r = 32 log(nα−1) min (rmin(C, A1), rmin(C, A2)). We will show that Acomb
outputs B = 0 correctly with probability at least 1 − α. Let r′ = 32 rln n ; note that either r′ ≥ rmin(C, A1) or rmin(C, A2). Assume ﬁrst that
r′ ≥ rmin(C, A1). Then, A1 will output B = 0 in each of its 16 ln α−1 groups with probability at least 43. On the other hand, since it is stable, A2 will output B = 0 in each of its groups with probability at least 12. Therefore

E Z1 + Z2 ≤ 1 + 1 ≤ 3 .

2

84 8

Since Z1+2 Z2 is the average of 32 ln α−1 random variables, by Hoeﬀding’s inequality, the

probability that Z1+2 Z2 ≥ 12 is at most exp −2(32 ln α−1)( 18 )2 ≤ α.

Similarly, if r′ ≥ rmin(C, A2), the probability that

Z1+Z2 2

≥

1 2

is also at most α.

This

concludes the proof.

Algorithm 3 Combining two algorithms A1 and A2 for Domination(n, p, q, r)
1: Divide the samples into 32 ln α−1 groups. 2: Run A1 on each of the ﬁrst 16 ln α−1 groups and let Z1 be the average of the outputs. 3: Run A2 on each of the last 16 ln α−1 groups and let Z2 be the average of the outputs. 4: If Z1+2 Z2 ≤ 12 output B = 0, else output B = 1.

18

Corollary 5.7. Let Acomb be the algorithm we obtain by combining Acount and Amax in the manner of Lemma 5.6. Then for any instance C = (n, p, q) of Domination,
√ n log n
rmin(C, Acomb) ≤ O p − q 2 . 2
If C further satisﬁes δ ≤ pi, qi ≤ 1 − δ for all i for some constant δ, then
rmin(C, Acomb) ≤ O( n log n)rmin(C).
Proof. This follows from Lemmas 5.1, 5.2, 5.6, and the following observation:

n

log n

min p − q 2 , p − q 2

1

∞

n

log n

≤

p−q 2 · p−q 2

√

1

∞

n log n

≤ p−q 2.

2

The last inequality follows from the fact that for any vector x,

x

2 2

≤

x 1·

x ∞. The

second part of the corollary then follows directly from Corollary 4.4.

5.2.2 The sum of cubes algorithm
We now give a diﬀerent algorithm for Domination which we call the sum of cubes algorithm, Acube. If we let Si = j(Xi − Yi), then intuitively, whereas Acount decides its output based on the signed ℓ1 norm of the Si and whereas Amax decides its output based on the signed ℓ∞ norm of the Si, Acube decides its output based on the signed ℓ3 norm of the Si. See Algorithm 4 for a detailed description of the algorithm.

Algorithm 4 Sum of cubes algorithm Acube for Domination(n, p, q, r)

1: Ti,j = 1 with probability 1 + (Xi,j−Yi,j) and Ti,j = −1 with probability 1 − (Xi,j−Yi,j)

2: Si =

r j=1

Ti,j

2

2

2

2

3: Z =

n i=1

Si3

4: If Z ≥ 0, output B = 0. If Z < 0, output B = 1.

To analyze the performance of Acube, we begin by analyzing statistical properties of the random variable S.

Lemma 5.8. Let S =

r j=1

Xj

where

X1, · · ·

, Xr

are

i.i.d

{−1, 1}-valued

random

variables

with mean ε ≥ 0 and r ≥ 8. Let Z = S3. Then

E[Z] ≥ 2r2ε + 1 r3ε3 2
Var[Z] ≤ 15r3 + 36r4ε2 + 9r5ε4.

19

Proof. By applying the multinomial theorem and using the fact that Xi2 = 1 for each i, we can write multilinear expressions for S3 and S6. We can now use linearity of expectation and the independence among the Xi’s to compute the mean and variance exactly.
E[Z] = E[S3] = (−2r + 3r2)ε + (2r − 3r2 + r3)ε3 ≥ 2r2ε + 1r3ε3 2
Var[Z] = E[S6] − E[S3]2 = (16r − 30r2 + 15r3) + (−136r + 282r2 − 183r3 + 36r4)ε2+ (240r − 522r2 + 381r3 − 108r4 + 9r5)ε4 + (−120r + 270r2 − 213r3 + 72r4 − 9r5)ε6
≤ 15r3 + 36r4ε2 + 9r5ε4

Lemma 5.9. Let Si =

r j=1

Xi,j

where

for

each

i

∈

[n],

Xi,1, · · · , Xi,r

are

i.i.d

{−1, 1}-

valued

random

variables

with
n

mean

εi,

along

with

the√conditnion

that

either

all

εi

≥

0

or

all εi ≤ 0. Let Z = i=1 Si3. If r ≥ 8 and r ≥ η n/( i=1 ε2i ) for some η ≥ 1 then,

E[Z]2 ≥ 3η6 Var[Z].

Proof. Without loss of generality, we can assume that εi ≥ 0 for all i ∈ [n]. By Lemma 5.8,

E[Z]2 ≥ 4r4( εi)2 + 1 r6( ε3)2 + 2r5( εi)( ε3)

(1)

4

i

i

i

i

i

Var[Z] ≤ 15nr3 + 36r4 ε2i + 9r5 ε4i .

(2)

i

i

We will show that each term in the Equation 2 is dominated by some term in Equation 1.

nr3 = r5 n ≤ 1 r5( ε2)2 ≤ 1 r5( εi)( ε3)

r2 η2

i

η2

i

i

i

i

r4( ε2) ≤ √1 r5( ε2)2 ≤ √1 r5( εi)( ε3)

i ηn

i

ηn

i

i

i

i

i

(Cauchy-Schwarz inequality)

r5( ε4) ≤ r6 √1 ( ε2)( ε4) ≤ r6 √1

i

ηn

i

i

ηn

i

i

i

√ n·(

ε4)1/2 (

ε4)

i

i

i

i

(Cauchy-Schwarz inequality)

= r6 ( ε4)3/2 ≤ r6 ( ε3)2 (monotonicity of ℓp norms)

η

i

η

i

i

i

Adding the above inequalities, we get Var[Z] ≤ 3η6 E[Z]2.

Theorem 5.10. If C = (n, p, q) is any instance of Domination, then

rmin(C, Acube) ≤ max

144√n p−q 2,8 .
2

If C satisﬁes δ ≤ pi, qi ≤ 1 − δ for all i for some constant δ, then

√ rmin(C, Acube) ≤ O( n)rmin(C).

20

Proof. Assume without loss of generality that B = 0. We have Si =

r j=1

Ti,j

and

Z

=

n i=1

Si3.

Note that for each i, the Ti,j are i.i.d.

{−1, 1} random variables with mean

√

E[Ti,j] = pi − qi. Applying Lemma 5.9 with η = 144, if r ≥ max 1p4−4 qn2 , 8 we have that

2

E[Z]2 ≥ 13068 Var[Z] = Var3[Z]. Since the algorithm makes an error (i.e. outputs B = 1) when Z < 0, we can use Chebyshev’s inequality to bound the probability that Z < 0.

Pr[Z < 0] ≤ Pr[|Z − E[Z]| ≥ E[Z]] ≤ Var[Z] ≤ 1 . E[Z]2 4

The second part of the theorem then follows directly from Corollary 4.4.

6 Domination in the general regime

In this section, we consider Domination in the general regime. Unlike in the previous section, it is no longer true that I(Xi,jYi,j; B) = I(pi, qi) = Θ((pi − qi)2). In particular, when pi and qi are both very small, I(pi, qi) can be much bigger than (pi − qi)2; as a result, the algoIrnitShemctsiodnes6ig.1n,ewdeinprtehseenptreavnioeuws aslegcotrioitnhmcanwhfaicilhuins dO˜e(r√thne·sremcinir)c-ucommstpaentciteisv.e. According to Theorem 8.2, this is the best we can do up to polylogarithmic factors. In Section 6.2, we then demonstrate that the general regime is indeed harder than the restricted regime in Section 5. In particular, we give instances where the algorithms presented in the previous
section fail; we show that the competitive ratio of these algorithms is unbounded (even for ﬁxed n).

6.1 An O˜(√n)-competitive algorithm

√

Here we give an (Theorem 6.5).

algorithm that only needs By Lemma 4.2, this is

O( only

nO˜lo(√g(nn))/tIi(mpe, sq)a)ssammapnlyesstaomspollvese

Domination as the opti-

mal algorithm needs. Intuitively, the algorithm works as follows: if for some coordinate i,

Xi,1Yi,1...Xi,r, Yi,r conveys enough information about B, we will only use samples from coordi-

nate i to determine B. Otherwise, the information about B must be well-spread throughout

all the coordinates, and a majority vote will work.

We begin by bounding the probability we can determine the answer from a single ﬁxed

coordinate.

Lemma 6.1 (Sanov’s theorem). Let P(Σ) denote the space of all probability distributions on some ﬁnite set Σ. Let R ∈ P(Σ) and let Z1, · · · , Zk be i.i.d random variables with distribution R. For every x ∈ Σk, we can deﬁne an empirical probability distribution Pˆx on Σ as
∀σ ∈ Σ Pˆx(σ) = |{i ∈ [k] : xi = σ}|. k
Let C be a closed convex subset of P(Σ) such that for some P ∈ C, D(P ||R) < ∞. Then

Pr Pˆ(Z1,··· ,Zk) ∈ C ≤ exp (−k(ln 2)D(Q∗||R))

21

where Q∗ = argminQ∈C D(Q||R) is unique. In the case when D(Q||R) = ∞ for all Q ∈ C, Pr Pˆ(Z1,··· ,Zk) ∈ C = 0.

Proof. See exercise 2.7 and 3.20 in [CK11].

Lemma 6.2. Let 0 ≤ q < p ≤ 1 and let X1, · · · , Xk be i.i.d B(p) and Y1, · · · , Yk be i.i.d B(q). Then

k

1

Pr i=1 (Xi − Yi) ≤ 0 ≤ exp −2(ln 2)k log √pq + (1 − p)(1 − q) .

Proof. We will use Sanov’s theorem (Lemma 6.1). Let Σ = {0, 1}2. Consider the set of distributions on Σ,

P(Σ) = {(p00, p01, p10, p11) : 0 ≤ p00, p01, p10, p11 ≤ 1, p00 + p01 + p10 + p11 = 1},

and deﬁne C ⊂ P(Σ) as C = {(p00, p01, p10, p11) : p01 ≥ p10}. Clearly C is a closed convex set. Deﬁne R = ((1 − p)(1 − q), (1 − p)q, p(1 − q), pq) ∈ P(Σ); note that this is exactly the distribution of (Xi, Yi) for each i ∈ [k]. Since p > q, R ∈/ C. Observe that ri=1(Xi − Yi) ≤ 0 iﬀ the empirical distribution generated by (X1, Y1), · · · , (Xk, Yk), Pˆ((X1,Y1),··· ,(Xk,Yk)) belongs to C. We can assume that there is some Q ∈ C such that D(Q||R) < ∞, otherwise the
lemma is trivially true. Therefore by Lemma 6.1,

r
Pr (Xi − Yi) ≤ 0 ≤ exp (−k(ln 2)D(Q∗||R))
i=1
where Q∗ = argminQ∈C D(Q||R) is unique. In addition, Q∗ should lie on the boundary of C i.e. Q∗ should satisfy p01 = p10. So
D(Q∗||R) = min D((1 − x − 2y, y, y, x)||R). 0≤x,y≤1, x+2y≤1

Let f (x, y) = (ln 2)D((1 − x − 2y, y, y, x||R). Since D(Q||R) is convex as a function of Q, f (x, y) is convex as well. We will show that there is always a point in the region {0 ≤ x, y ≤ 1, x + 2y ≤ 1} where the gradient of f (x, y) is zero. Since f is convex, this must be the minimizer of f . Note that

∂f (x, y) = −1 − ln(1 − x − 2y) + ln((1 − p)(1 − q)) + 1 + ln x − ln(pq) = 0 ∂x
∂f (x, y) = −2 − 2 ln(1 − x − 2y) + ln((1 − p)(1 − q)) + 2 + 2 ln y − ln(pq) = 0. ∂y
Solving the above equations for x, y we get

x= √ pq +

pq 2 , (1 − p)(1 − q)

pq(1 − p)(1 − q)

y= √

2.

pq + (1 − p)(1 − q)

22

It is easy to check that 0 ≤ x, y ≤ 1 and x + 2y ≤ 1. Substituting the values of x, y, we ﬁnd that D(Q∗||R) = −2 log √pq + (1 − p)(1 − q) .

Lemma 6.3.

1

1

2 log √pq + (1 − p)(1 − q) ≥ 2 I(p, q).

Proof. We can assume 0 < p, q < 1, otherwise the required inequality follows from the fact that − ln(1 − t) ≥ t for 0 ≤ t < 1. For example, when p = 0, the LHS simpliﬁes to − log(1 − q) and the RHS to q/2, and the inequality is satisﬁed. The other cases are similar. Hence, from now on, assume that 0 < p, q < 1. Let x = p(1 − q) and y = q(1 − p). Thus I(p, q) = (x + y)(1 − H(x/x + y)). We can also the write the LHS of the inequality as:

−2 log √pq +

(1 − p)(1 − q)

= − log pq + (1 − p)(1 − q) + 2 pq(1 − p)(1 − q)

= − log (1 − x − y + 2√xy)

= − log

1

−

√ (x

−

√y

)2

≥

√ (x

−

√y)2/(ln

2)

= (x + y − 2√xy)/(ln 2).

(− log(1 − t) ≥ t/(ln 2))

Now we need to show that

(x + y − 2√xy) 1

x

ln 2 ≥ 2 (x + y) 1 − H x + y .

We can scale x, y such that x + y = 1, so let x = 12 + z and y = 21 − z. Therefore it is enough

to show that

√

ln 2

1

1 − 1 − 4z2 ≥ 2 1 − H 2 + z .

√

We have 1 − 1 − 4z2 ≥ 2z2 and by Fact A.10,

1 − H 1 + z = D 1 + z 1 ≤ 4 z2.

2

2

2 ln 2

Combining these two, we have the required inequality.

Lemma 6.4. In Domination(n, p, q, r), for any i ∈ [n], if r > 6/I(pi, qi), then

Pr sign

r
(Xi,j − Yi,j)
j=1

= (−1)B

> 5/6.

23

Proof. Assume we are in the B = 0 case, the other case is similar. Fix an i ∈ [n]. By Lemma 6.2,

r
Pr j=1 (Xi,j − Yi,j) ≤ 0 ≤ exp −r(ln 2) log √piqi + ≤ exp (−r(ln 2)Ii/2) = 2−rIi/2 < 1/8.

1 (1 − pi)(1 − qi)
(By Lemma 6.3)

We now introduce what we call the general coupling algorithm Acoup for Domination. A detailed description of the algorithm can be found in Algorithm 5; more brieﬂy the algorithm works as follows:

1. Split the r samples for each of the n coordinates into ℓ = 18 log(2nα−1) equally-sized segments where α is the error parameter. For each coordinate i and segment j, set Si,j = 1 if more samples from X equal 1 than samples from Y , and −1 otherwise. This can be thought of as running a miniature version of the counting algorithm on each segment; Si,j = 1 is evidence that B = 0, and Si,j = −1 is evidence that B = −1.

2. Let i′ be the coordinate i which maximizes

ℓ j=1

Si,j

(i.e.

the coordinate that is

“most consistently” either 1 or −1). If

ℓ j=1

Si′,j

≥ ℓ/3 (i.e.

at least 2ℓ/3 of the

segments for this coordinate agree on the value of B), output B according to the sign

of

ℓ j=1

Si′,j

.

3. Otherwise, for each segment, take the majority of the votes from each of the n coordi-

nates; that is, for each 1 ≤ j ≤ ℓ, set Tj = sign(

n i=1

Si,j

).

Then

take

another

majority

over the segments, by setting Z2 = sign(

ℓ j=1

Tj

).

Finally, if Z2 > 0 output B = 0;

otherwise, output B = 1.

Theorem 6.5. If C = (n, p, q) is any instance of Domination, then

2592√n ln(2nα−1)

rmin(C, Acoup, 1 − α) ≤

I(p, q)

and thus

√ rmin(C, Acoup) ≤ O( n log n) · rmin(C).
Proof. Let Ii = I(pi, qi), r = 2592√n log(2nα−1)/I(p, q) and ℓ = 18 ln(2nα−1). two cases to consider:

There are

24

Algorithm 5 General coupling algorithm for Domination(n, p, q, r)

1: ℓ = 18 log(2nα−1).

2: for i = 1 to n do

3: for j = 1 to ℓ do

4:

Si,j = sign(

jr/ℓ t=(j−1)∗(r/ℓ)+1

Xi,t

−

Yi,t)

5:

If Si,j = 0, let Si,j = 1 with probability 1/2 and let Si,j = −1 with probability 1/2.

6: end for

7: end for

8: i′ = arg maxi |

ℓ j=1

Si,j

|

9: Z1 =

ℓ j=1

Si′,j

10: if |Z1| ≥ ℓ/3 then

11: If Z1 > 0 output B = 0, else output B = 1.

12: else

13: for j = 1 to l do

14:

Tj = sign(

n i=1

Si,j

).

15: If Tj = 0, let Tj = 1 with probability 1/2 and let Tj = −1 with probability 1/2.

16: end for

17:

Z2 = sign(

ℓ j=1

Tj

).

18: If Z2 = 0, let Z2 = 1 with probability 1/2 and let Z2 = −1 with probability 1/2.

19: If Z2 > 0 output B = 0, else output B = 1.

20: end if

1. Case 1: There exists an i′ such that 24√nIi′ ≥ nk=1 Ik.

√

r≥

24·6
n

n I

≥

6 I

.

By symmetry, we can assume that B = 0. In this case, we have that ℓ

k=1 k

i′

By Lemma 6.4, for each j = 1, . . . , ℓ, Pr[Si′,j = 1] ≥ 5/6. Therefore we have

l

E

Si′,j ≥ ℓ · (5/6 − 1/6) = 2ℓ/3.

j=1

Since Si′,1, ..., Si′,l are independent when B is given, by the Chernoﬀ bound, we have that
Pr l Si′,j ≥ ℓ/3 ≥ 1 − exp(−ℓ · (1/3)2 · (1/2)) ≥ 1 − 2αn . j=1

For i = i′, since pi ≥ qi, we still have Pr[Si,j = 1] ≥ 1/2. By a similar argument, we
get Pr l Si,j ≥ −ℓ/3 ≥ 1 − exp(−ℓ · (1/3)2 · (1/2)) ≥ 1 − 2αn .
j=1

Let W be the event that

ℓ j=1

Si′,j

≥

ℓ/3

and

for

i

=

i′,

l j=1

Si,j

≥ −ℓ/3.

By the

union

bound,

we

have

that

Pr[W ]

≥

1−n·

α 2n

=

1−

α2 .

Moreover,

when

W

happens,

25

we know that Z1 ≥ ℓ/3 and Acoup outputs B = 0. Therefore, in Case 1, the probability that Acoup outputs B correctly is at least 1 − α2 . 2. Case 2: For all i ∈ {1, . . . , n}, 24√nIi < nk=1 Ik.
Similarly as in Case 1, since Pr[Si,j = (−1)B] ≥ 1/2, the probability that |Z1| ≥ ℓ/3 and our algorithm outputs wrongly is at most α2 . For the rest of Case 2, assume |Z1| < ℓ/3.

Now ﬁx a coordinate i. Our plan is to ﬁrst lower bound the amount of information

samples from coordinate i have about B by using Lemma 6.4 and the subadditivity of

information.

Let

s

=

r/ℓ,

and

let

s′

=

s

·

⌈

6 sI

⌉.

Imagine

that

we

have

s′

new

samples,

Ui,1, Vi,1, ..., Ui,s′, Vi,s′,

where

each

(Ui,j, Vi,j)

i
(j

=

1, . . . , s′)

is

generated

independently

according to the same distribution as (Xi,1, Yi,1). Since s′ ≥ 6/Ii, by Lemma 6.4, we

have that s′

Pr sign

(Ui,j − Vi,j) = (−1)B > 5/6.

j=1

Write (UiVi)[a,b] as shorthand for the sequence ((Ui,a, Vi,a), . . . (Ui,b, Vi,b)), and deﬁne (XiYi)[a,b] analogously. By Fano’s inequality, we have that

I (UiVi)[1,s′]; B

= H(B) − H(B|(UiVi)[1,s′]) ≥ H( 12 ) − H(1 − 56 ) = 1 − H( 61 ) ≥ 1/3.

Since I((UiVi)[1,s]; (UiVi)[s+1,s′]|B) = 0 (our new samples are independent given B), we have

I((UiVi)[1,s′]; B) = I((UiVi)[1,s]; B|(UiVi)[s+1,s′]) + I((UiVi)[s+1,s′]; B)

≤ I((UiVi)[1,s]; B) + I((UiVi)[s+1,s′]; B)

(by Fact A.7)

Repeating this procedure, we get

⌈ s6Ii ⌉

I((UiVi)[1,s′]; B) ≤

I ((UiVi)[(u−1)s+1,us]; B).

u=1

Since we know that for any u = 1, ..., ⌈ s6Ii ⌉,

I((UiVi)[(u−1)s+1,us]; B) = I((XiYi)[1,s]; B),

we get

I((XiYi)[1,s]; B) ≥ I((UiVi)[1,s′]; B) · 1 ≥ sIi . ⌈ s6I ⌉ 6 · 6
i

The last inequality is true because s6Ii = 24nk√=n1IIik ≥ 1.

26

After we lower bound I((XiYi)[1,s]; B), we are going to show that we can output B correctly with reasonable probability based on samples only from coordinate i.

sIi ≤ I((XiYi)[1,s]; B) 6·6

=

Pr[(XiYi)[1,s] = x] · D(B|(XiYi)[1,s] = x B)

x

≤

Pr[(XiYi)[1,s] = x] · 2(Pr[B = 0|(XiYi)[1,s] = x] − 1/2)2 +

x
2(Pr[B = 1|(XiYi)[1,s] = x] − 1/2)2

(by Fact A.10)

=

Pr[(XiYi)[1,s] = x] · (Pr[B = 0|(XiYi)[1,s] = x] − Pr[B = 1|(XiYi)[1,s] = x])2

x

≤

Pr[(XiYi)[1,s] = x] · Pr[B = 0|(XiYi)[1,s] = x] − Pr[B = 1|(XiYi)[1,s] = x] .

x

When sj=1(Xi,j − Yi,j) > 0, it is easy to check that

Pr[B = 0|(XiYi)[1,s]] > Pr[B = 1|(XiYi)[1,s]].

Therefore,

Pr[Si,1 = (−1)B] =

Pr[(XiYi)[1,s] = x] ·

x

max Pr[B = 0|(XiYi)[1,s] = x], Pr[B = 1|(XiYi)[1,s] = x]

= 1 + 1 · Pr[(XiYi)[1,s] = x] · 22x

Pr[B = 0|(XiYi)[1,s] = x] − Pr[B = 1|(XiYi)[1,s] = x]

≥ 1 + sIi

2 12√· 6

≥ 1 + nIi .

2

n k=1

Ik

Similarly, we can show for all i = 1, ..., n, j = 1, ..., l,

Pr[Si,j = (−1)B] ≥ 1 + 2

√

nIi
n

.

k=1 Ik

Now without loss of generality assume that B = 0. We have that

n

n

E

Si,j ≥

i=1

i=1

√

√

1 + nIi − 1 + nIi

2

n k=1

Ik

2

n k=1

Ik

√ = 2 n.

27

Therefore, by the Chernoﬀ bound, Pr[Tj = 1] ≥ 1 − e−(1/n)·(2√n)2·(1/2) > 3/4.

By the Chernoﬀ bound again,

Pr[Z2 > 0] ≥ 1 − e−ℓ·(1/2)2·(1/2) ≥ 1 − α . 2n

Since we initially fail with probability at most α2 , by the union bound, in Case 2 we

fail

with

probability

at

most

α 2

+

α 2n

<

α.

This

concludes

the

proof.

6.2 Acount and Amax have unbounded competitive ratios
In this section, we show that the competitive ratio of Acount and Amax is unbounded. The result in Lemma 6.6 can be easily generalized to show that the counting algorithm of [SW15] for Top-K also has unbounded competitive ratio.

Lemma 6.6. For each suﬃciently large n and for any ε > 0, there exists an instance C = (n, p, q) of Domination such that the following two statements are true:

√

1.

rmin(C, Acoup, 1 − n2 ) ≤

5184

n log n ε

2.

rmin(C, Acount) ≥

n 16ε

2

.

Proof. Deﬁne p, q to be

1. p1 = ε, q1 = 0.

2. pi = qi = 1/2 for i = 2, ..., n.

Note

that

I(p1, q1)

=

ε, and √

I(p2, q2)

=

···

=

I(pn, qn)

=

0.

Therefore,

by

Theorem

6.5,

Acoup

succeeds

given

r

=

5184

n log n ε

samples

with

probability

at

least

1 − 2/n.

Now assume that r ≤ n/(16ε2) ≤ (n − 1)/(8ε2). We will now show that Acount solves

Domination(n, p, q, r) with probability at most 3/4. Without loss of generality assume

that B = 0. Deﬁne the random variables Ui,j as follows:

1. U1,j = X1,j − Y1,j − ε. j = 1, ..., r.

2. Ui,j = Xi,j − Yi,j for i = 2, ..., n, j = 1, .., r.

It is straightforward to check that for all i = 1, . . . , n and j = 1, . . . , r, E[Ui,j] = 0 and E[|Ui,j|3] ≤ 1/2. For all i = 2, . . . , n and j = 1, . . . , r, we further have that E[Ui2,j] = 1/2.

28

Let Φ be the cdf of the standard normal distribution.

Pr[Acount outputs B = 1 (incorrectly)]

nr

nr

= Pr[

(Xi,j − Yi,j) < 0] = Pr[

Ui,j < −r · ε]

i=1 j=1

i=1 j=1

 ≥ Φ −r · ε ·



1

n i=1

r j=1

E[|Ui,j

|3]

n

r

− E[U 2 ] (

n i=1

r j=1

E[Ui2,j

])−3/2

i=1 j=1

i,j

(By Berry-Esseen theorem (Lemma A.11))

≥ Φ −r · ε/ r(n − 1)/2 −

2n2 ≥ Φ(−1/4) − (n − 1)3r

2n2 ≥ 1/4. (n − 1)3r

Lemma 6.7. For each suﬃciently large n and any 0 < ε < 1/n3, there exists an instance C = (n, p, q) of Domination such that the following two statements are true.

√

1.

rmin(C, Acoup, 1 − n2 ) ≤

518400 ε

n ln n .

2.

rmin(C, Amax, 190 ) ≥

1 ε2214 ln n

Proof. Deﬁne p, q as:

1. p1 = ε/100, q1 = 0.

2. pi = 1/2 + ε, qi = 1/2 i = 2, ..., n.

Note that I(p1, q1) = ε/100 and I(p2, q2) = · · · I(pn, qn) = (1 − H(1/2 + ε))/2. By Fact

A.10, I(p2, q2) ≤ ln4(2) · (ε)2 ≤ ε/(100n). Thus ε/100 ≤

n i=1

I(pi,

qi)

≤

ε/50.

Therefore,

√

by

Theorem

6.5,

given

at

least

518400 ε

n ln n

samples,

Acoup

succeeds

with

probability

at

least

1 − 2/n.

Now

ﬁx

r

=

ε2

1 214

ln

n

.

We

will

now

show

that

Amax

solves

Domination(n, p, q, r)

with

probability at most 9/10. Without loss of generality assume B = 0. Deﬁne random variable

Si = rj=1(Xi,j − Yi,j). S1 is always non-negative. S2, · · · , Sn are i.i.d random variables with

E[Si] = rε and Var[Si] = r( 21 − ε2). Algorithm 2 outputs B = 1 when infi Si + supi Si < 0. Let λ > 0 be some parameter which we will choose later.

29

Pr[inf Si + sup Si < 0] ≥ Pr[inf Si < −λ, sup Si < λ]

i

i

i

i

≥ Pr[sup Si < λ] − Pr[inf Si ≥ −λ, sup Si < λ]

i

i

i

n

n

= Pr[Si < λ]n − Pr[−λ ≤ Si < λ]n

i=1

i=1

= Pr[S1 < λ] Pr[S2 < λ]n−1 − Pr[−λ ≤ S2 < λ]n−1

= Pr[S1 < λ] Pr[S2 < λ]n−1 − (Pr[S2 < λ] − Pr[S2 < −λ])n−1

We will now apply Berry-Esseen Theorem (Lemma A.11) with Zj = (X2,j − Y2,j) for

j = 1, · · · , r, to approximate the CDF of S2. We have µ = E[S2] = rε, σ2 = Var[S2] =

r( 21 − ε2) ≥ 4r . and γ =

r j=1

E[|Zj

− ε|3]

≤

8r.

Therefore

for

all

t

∈

R,

t−µ Pr[S2 < t] − Φ σ

≤ γ ≤ √64 ≤ 64

σ3

r n3/2

when n is large enough. Let us choose λ = µ + σΦ−1(1 − nln−21 ) and let a = λ−σµ , b = λ+σµ . Therefore Φ(a) = 1 − nln−21 . When n is large enough, a > 10. By Fact A.12,

√1 exp(−a2/2) 1 ≥ ln 2 = 1 − Φ(a) ≥ √1 exp(−a2/2) 1 .

2π

a n−1

2π

2a

From the left hand side of the above inequality, we can conclude that a ≤ 2 ln(n − 1). Also,

Φ(−b) = 1 − Φ(b) = ln 2 − (Φ(a) − Φ(b)) n

= ln 2 − √1

b
exp −t2/2 dt

n − 1 2π a

≥ ln 2 − √1 (b − a) exp(−a2/2) n − 1 2π

≥ ln 2 − 2a(ln 2)(b − a)

n−1

n−1

≥ ln 2 − 4aµ n − 1 (n − 1)σ

≥ ln 2 − 16ε r ln(n − 1)

n−1

n−1

(µ = rε, σ2 ≥ r4 , a ≤ 2

≥ ln 2 − 16 ε n−1 n−1
≥ ln 2 − 1 n − 1 8(n − 1)

1 ln(n − 1) ε2214 ln n

ln(n − 1))

30

By Chernoﬀ bound, we have

Pr[S1

<

λ]

≥

Pr[S1

≤

µ]

=

1

−

e−r·D(ε

ε/100)

≥

1

−

e−

2.5 ε

·

ε

≥

1/2.

Now we can bound the probability of error as follows:

Pr[inf Si + sup Si < 0]

i

i

≥ Pr[S1 < λ] Pr[S2 < λ]n−1 − (Pr[S2 < λ] − Pr[S2 < −λ])n−1

1

λ−µ

64 n−1

λ−µ

−λ − µ

64 n−1

≥ 2 Φ σ − n3/2

− Φ σ −Φ σ

+ 2 · n3/2

1

64 n−1

128 n−1

= 2 Φ(a) − n3/2

− Φ(a) − Φ(−b) + n3/2

1

ln 2 64 n−1

2 ln 2

1

128 n−1

≥ 2 1 − n − 1 − n3/2

− 1 − n − 1 + 8(n − 1) + n3/2

≥ 1 (exp(− ln 2) − exp(−2 ln 2 + 1/8) − 0.01) 2
> 1. 10

(when n is large enough)

7 Reducing top-K to domination
In this section, we will ﬁnally reduce Top-K to Domination, thus proving Theorem 3.1. First, we will give an algorithm for Top-K problem that uses Acoup for Domination as a subroutine. We begin by reducing Top-K to the following graph theoretic problem.
Lemma 7.1. Let G = ([n], E) be a directed complete graph on vertices {1, 2, · · · , n} i.e. for every distinct i, j ∈ [n], either (i, j) ∈ E or (j, i) ∈ E but not both. Suppose there is a subset S ⊂ [n] of size k such that (i, j) ∈ E for every i ∈ S and j ∈/ S. Then there is a randomized algorithm which runs in expected running time O(n) and ﬁnds the set S given oracle access to the edges of G. Moreover there is some absolute constant C > 0 such that for every λ ≥ 1, the probability that the algorithm runs in more than Cλn time is bounded by exp(−λ).
Proof. Pick v ∈ [n], uniformly at random. Let din(v) and dout(v) be the indegree and outdegree of vertex v ∈ [n]. Clearly din(v) + dout(v) = n − 1. Also v ∈ S iﬀ din(v) < k. We can thus easily test if v ∈ S by querying the n − 1 edges, {(i, v) : i ∈ [n] \ {v}}. Depending on whether v ∈ S, we now have two cases:
• Case 1: v ∈ S
For every i such that (i, v) ∈ E, we can conclude that i ∈ S. We can therefore remove these vertices and iterate. We have reduced the problem to a graph on n − 1 − din(v) = dout(v) vertices.
31

• Case 2: v ∈/ S
For every i such that (v, i) ∈ E, we can conclude that i ∈/ S. We can therefore remove these vertices and iterate. We have reduced the problem to a graph on n−1−dout(v) = din(v) vertices.

Let n′ be the number of vertices that remain after the above random process. Note that

Ev[n′] = Pr[v ∈ S] · E[dout(v)|v ∈ S] + Pr[v ∈/ S] · E[din(v)|v ∈/ S]

= k n−k+ k−1 + n−k k+ n−k−1

n

2

n

2

= n − 1 + k(n − k) ≤ 3n.

2

2n

4

By Markov’s inequality, Pr[n′ ≥ 4n/5] ≤ 1165 . We will repeatedly choose v at random until we ﬁnd a v such that n′ < 45n . Once we ﬁnd such a v, we can remove at least n/5 vertices from the graph and iterate the same procedure for the remaining graph. Let T0 denote the random variable equal to the number of times we sample v. We have that Pr[T0 ≥ t] ≤ ( 1165 )t and therefore ∞
E[T0] = Pr[T0 ≥ t] ≤ 15.
t=1
Similarly let Ti represent the number of times we must sample v in iteration i of this process; by the same logic, E[Ti] ≤ 15 for all i. If we let the random variable X denote the number of edge queries the algorithm makes, then since the graph shrinks by a factor of 4/5 at each iteration,

4 X = T0 · n + T1 · 5 n + T2 · E[X] ≤ 15 · 1 + 4 + 4 2 + . . .
55

4 2n+··· 5 · n ≤ 75n.

This completes the proof that E[X] = O(n), as required. We can similarly analyze the tail probability of X; note that:

λC Pr[X > Cλn] ≤ Pr ∃i : Ti > 9

10 i 9

since

Ti

≤

Cλ 9

190 i for every i implies that

Cλn ∞ 4 i 10

X≤ 9

59

i=0

i Cλn ∞ =9
i=0

8 i ≤ Cλn. 9

32

By the union bound, Cλ
Pr ∃i : Ti > 9

10 i

∞

Cλ

9 ≤ Pr Ti > 9

i=0

10 i 9

∞

Cλ 16

≤ exp − 9 ln 15

i=0

10 i 9

≤ exp(−λ).

(for suﬃciently large C)

The following lemma shows that when p ≥ q, I(p, q) is an increasing function of p and a decreasing function of q.

Lemma 7.2. Let 0 ≤ q′ ≤ q ≤ p ≤ p′ ≤ 1, then I(p′, q′) ≥ I(p, q).

Proof. We have:

∂I(p, q) = (1 − q) log ∂p
∂I(p, q) = (1 − p) log ∂q

2p(1 − q) p(1 − q) + (1 − p)q
2(1 − p)q p(1 − q) + (1 − p)q

− q log − p log

2(1 − p)q p(1 − q) + (1 − p)q
2p(1 − q) p(1 − q) + (1 − p)q

When p ≥ q,

log 2p(1 − q) ≥ 0, log 2(1 − p)q ≤ 0.

p(1 − q) + (1 − p)q

p(1 − q) + (1 − p)q

Thus

∂I(p,q) ∂p

≥

0

and

∂I(p,q) ∂q

≤

0

when

p

≥

q.

Thus increasing p or decreasing q cannot

decrease I(p, q) when p ≥ q.

We are now ready to give an algorithm for Top-K.

Theorem 7.3. There exists an algorithm A for Top-K such that for any α > 0 and any instance S = (n, k, P), A runs in time O(n2r log(1/α)) and satisﬁes
7776√n log(2nα−1) rmin(S, A, 1 − α) ≤ I(Pk, Pk+1)
where Pk, Pk+1 are the k and k + 1 rows of P.
Proof. Let Pi denote the ith row of P, and let ∆ = I(Pk, Pk+1). Recall that A is given as input the three-dimensional array of samples Zi,j,l, where for each i, j ∈ [n] and 1 ≤ l ≤ r, Zi,j,l is the result of the lth noisy comparison between item i and item j (sampled from B(Pπ−1(i),π−1(j))). We will deﬁne a complete directed graph G = ([n], E) as follows. For every 1 ≤ i < j ≤ n, run Acoup with input Xh,l = Zi,h,l and Yh,l = Zj,h,l; if Acoup returns

33

B = 0, then direct the edge from i towards j, and otherwise, direct the edge from j towards

i.

Let T = {π(1), π(2), . . . , π(k)} be the set of labels of the top k items. We claim that if

i

∈

T

and

j

∈

T,

then

with

probability

at

least

1−

α n2

,

the

edge

is

directed

from

i

towards

j. To see this, note that in the corresponding input to Acoup, X is drawn from Pπ−1(i) and

Y is drawn from Pπ−1(j). If i ∈ T and j ∈ T , then π−1(i) ≤ k < π−1(j). In particular,

Pπ−1(i) dominates Pπ−1(j), and moreover by Lemma 7.2, I(Pπ−1(i), Pπ−1(j)) ≥ ∆. It follows

from

Theorem

6.5

that

Acoup

outputs

B

=

0

on

this

input

with

probability

at

least

1

−

α 2n2

,

since in general,

2592√n log(4n3α−1)

rmin(C,

Acoup,

1

−

α 2n2

)

≤

I(p, q)

7776√n log(2nα−1)

≤ I(p, q) .

By the union bound, the probability that all of these comparisons are correct is at least 1− α2 . Therefore, by the tail bounds in Lemma 7.1, we can ﬁnd the subset T in O(n log(1/α)) oracle calls to Acoup with probability at least 1 − α2 . The probability of failure is at most α2 + α2 = α. Each call to Algorithm 5 takes O(nr) time, so the overall time of the algorithm is O(n2r log(1/α)).

To prove that this algorithm is competitive, we will conclude by proving a lower bound on rmin(S) (again, by reduction to the appropriate lower bound for Domination).

Lemma 7.4. Let S = (n, k, P) be an instance of Top-K. Then rmin(S) ≥ I(Pk0,P.1k+1) .

Proof. We will proceed by contradiction. Suppose there exists an algorithm A which satisﬁes

rmin(S, A) ≤ I(Pk0,.P01k+1) . We will show how to convert this into an algorithm A′ which solves

the

instance

C

=

(n, Pk, Pk+1)

of

Domination

with

probability

at

least

3 4

when

given

at

least 2r = 0.05/I(Pk, Pk+1) samples, thus contradicting Lemma 4.2.

The algorithm A′ is described in Algorithm 6; essentially, A′ embeds the inputs X and

Y to the Domination instance as rows/columns k and k + 1 respectively of the Top-K

instance. It is easy to check that the Zi,j,l for i, j ∈ [n],l ∈ [r] generated in A′ are distributed

according to the same distribution as the corresponding elements in the instance S of Top-K.

Therefore A will output the top k items correctly with probability at least 3/4. In addition,

if B = 0 the item labeled k will be in the top k items and if B = 1 the item labeled k will

not be in the top k items. Therefore, A′ succeeds to solve this instance of Domination with

probability at least 3/4, leading to our desired contradiction.

We are now ready to prove our main upper bound result.

34

Algorithm 6 Algorithm A′ for the lower bound reduction
1: Get input Xi,l, Yi,l for i ∈ [n] and l ∈ [2r] from Domination(n, Pk, Pk+1, 2r). 2: Generate a random permutation π on n elements s.t. π({k, k + 1}) = {k, k + 1}. 3: for i ∈ [n], j ∈ [n], l ∈ [r] do 4: If i = k, set Zi,j,l = Xj,l. 5: If i = k + 1, set Zi,j,l = Yj,l. 6: If i ∈ {k, k + 1}, j = k, set Zi,j,l = Xi,l+r. 7: If i ∈ {k, k + 1}, j = k + 1, set Zi,j,l = Yi,l+r. 8: If i ∈ {k, k + 1}, j ∈ {k, k + 1}, sample Zi,j,l from B(Pπ−1(i),π−1(j)). 9: end for
10: Run A on samples Zi,j,l, i, j ∈ [n], l ∈ [r]. 11: If A said k is amongst the top k items, output B = 0. Otherwise output B = 1.

Corollary 7.5. There is an algorithm A for Top-K such that A runs in time O(n2r) and on every instance S of Top-K on n items,
√ rmin(S, A) ≤ O( n log n)rmin(S).

Proof. Let S =Top-K(n, k, P, ·) be an instance of Top-K. By Lemma 7.4,

0.1 rmin(S) ≥ I(Pk, Pk+1) .

If

A

is

the

algorithm

in

Theorem

7.3

with

α

=

1 4

then

A

runs

in

time

O(n2r)

and

√n log n rmin(S, A) ≤ O I(Pk, Pk+1) .

Combining these two inequalities, we obtain our result.

8 Hardness of domination and top-K

In the using

aptremvioosutsO˜se(c√tino)ntwime edsemmoonrestsraamtepdleasn

algorithm that solves Top-K on any than the optimal algorithm for that

distribution distribution

(see Corollary 7.5). In algorithm, there exists

this section, we show this is tight up to some distribution where that algorithm

lroegqauriirtehsmΩ˜ic(√fanc)totrims; efsormaonrye

samples than the optimal algorithm for that distribution. Speciﬁcally, we show the following

lower bound.

Theorem 8.1. For any algorithm A, there exists an instance S of Top-K of size n such

√

that rmin(S, A) ≥ Ω

n log n

rmin(S).

35

As in the previous sections, instead of proving this lower bound directly, we will ﬁrst prove a lower bound for the domination problem, which we will then embed in a Top-K instance.

Theorem 8.2. For any algorithm A, there exists an instance C of Domination of size n

√

such that rmin(C, A) ≥ Ω

n log n

rmin (C ).

8.1 A hard distribution for domination
To prove Theorem 8.2, we will show that there exists a distribution over instances of the domination problem such that, while each instance in the support of this distribution can be solved by some algorithm with a small number of samples, any algorithm requires a large number of samples given an instance randomly sampled from this distribution.
Let C be a distribution over instances C of the domination problem of size n. We extend rmin to distributions by deﬁning rmin(C, A, p) as the minimum number of samples algorithm A needs to successfully solve Domination with probability at least p over instances randomly sampled from C, and let rmin(C, A) = rmin(C, A, 3/4). The following lemma relates the distributional sample complexity to the single instance sample complexity.
Lemma 8.3. For any p > 1/2, algorithm A and any distribution C over instances of the domination problem, there exists a C in the support of C such that rmin(C, A, p) ≥ rmin(C, A, p).
Proof. Let ε(C, A, r) be the probability that algorithm A errs given r samples from C. By the deﬁnition of rmin(C, A, p), we have that
Pr[C] · ε(C, A, rmin(C, A, p)) = 1 − p
C C∈supp C
It follows that there exists some C∗ ∈ supp C such that
ε(C∗, A, rmin(C, A, p)) ≥ 1 − p Since ε(C∗, A, r) is decreasing in r, this implies that rmin(C∗, A, p) ≥ rmin(C, A, p), as desired.

We will ﬁnd it useful to work with distributions that are only mostly supported on easy instances. The following lemma lets us do that. Lemma 8.4. Let C be a distribution over instances of the domination problem, and let E be an event with Pr[E] = 1 − δ. Then for any algorithm A and any 1 − δ > p > 12, rmin(C|E, A, p + δ) ≥ rmin(C, A, p). Proof. By the deﬁnition of rmin(C, A, p), we have that
Pr[C] · ε(C, A, rmin(C, A, p)) = 1 − p
C C∈supp C
36

Rewrite this as

Pr[E]·

Pr[C]·ε(C, A, rmin(C, A, p))+Pr[E]·

Pr[C]·ε(C, A, rmin(C, A, p)) = 1−p

C∈supp C C|E

C|E C∈supp C

Since C∈supp C PrC|E[C] = 1 and Pr[E] = δ, it follows that

Pr[C] · ε(C, A, rmin(C, A, p)) ≥ 1 − p − δ
C|E C∈supp C
from which it follows that rmin(C|E, A, p + δ) ≥ rmin(C, A, p).

We can now deﬁne the hard distribution for the domination problem. Deﬁne γ = 1001√n .

Let SP be a random subset of [n] where each i ∈ [n] is independently chosen to belong to

SP with probability γ. Likewise, deﬁne SQ the same way (independently of SP ). Finally,

ﬁx n constants Ri all in the range [ 41, 34] (for now, it is okay to consider only the case where

Ri

=

1 2

for

all

i;

to

extend

this

lower

bound

to

the

top-k

problem,

we

will

need

to

choose

diﬀerent values of Ri). Then the hard distribution Chard is the distribution over instances

C(SP , SQ) = (n, p, q) of Domination where

pi = Ri(1 + ε) Ri

if i ∈ SP if i ∈ SP

and

qi = Ri(1 − ε) Ri

if i ∈ SQ if i ∈ SQ

We claim that the majority of the instances in the support of Chard have an algorithm that requires few samples. Intuitively, if SP and SQ are ﬁxed, then the best algorithm for that speciﬁc instance can restrict attention only to the indices in SP and SQ. In particular, if SP is large enough (some constant times its expected size), then simply throwing away all indices not in SP and counting which row has more heads is an eﬃcient algorithm for recovering the dominant set.

Theorem 8.5. Fix any SP and SQ such that |SP | ≥ 110 nγ. Then rmin(C(SP , SQ), p) = O log(ε12−√pn)−1 for all p < 1.

Proof. It suﬃces to demonstrate an algorithm A such that rmin(C(SP , SQ), A, p) = O

log(1−√p)−1 ε2 n

.

Any algorithm A receives two sets X, Y , each of r samples from n coins. Write X =

(X1, X2, . . . , Xn), where each Xi = (Xi,1, Xi,2, . . . Xi,r) is the collection of r samples from coin

i (likewise, write Y = (Y1, Y2, . . . , Yn), and Yi = (Yi,1, Yi,2, . . . Yi,r)). Consider the following

algorithm: A computes the value

37

r

T=

(Xi,j − Yi,j)

i∈SP j=1

and outputs that B = 0 if T ≥ 0 and outputs B = 1 otherwise.

For

each

i, j,

let

Ai,j

= Xi,j − Yi,j.

If

B

= 0,

then

Ai,j

∈ [−1, 1],

E[Ai,j] ≥ εRi

≥

ε 4

and

all the Ai,j are independent. It follows from Hoeﬀding’s inequality that in this case,

Pr[T < 0] = Pr[T − E[T ] < −E[T ]] ≤ exp −2E[T ]2 4|SP |r = exp − |SP |rε2 32 ≤ exp −γnε2r 320 √nε2r = exp − 32000

Therefore, choosing r = 32000√lnn(ε12−p)−1 = O log(√1n−εp2)−1 guarantees Pr[T < 0] ≤ 1 − p. Similarly, the probability that T ≥ 0 if B = 1 is also at most 1 − p for this r. The conclusion follows.
By a simple Chernoﬀ bound, we also know that the event that SP has size at least 110 nγ occurs with high probability.
√
Lemma 8.6. Pr |SP | ≥ 110 nγ ≥ 1 − e− n/400.
In the following subsection, we will prove that for all A, rmin(Chard, A) is large. More precisely, we will prove the following theorem.

Theorem 8.7. For all algorithms A, rmin(Chard, A, 32 ) = Ω

1 ε2 log n

.

Given that this theorem is true, we can complete the proof of Theorem 8.2.

Proof of Theorem 8.2. By Theorem 8.7, for any algorithm A, rmin(Chard, A, 32 ) = Ω

1 ε2 log n

.

Let E be the event that |SP | ≥ 110 nγ. By Lemma 8.6, if n ≥ (400 ln 1112 )2, Pr[E] ≥ 112 . It then follows from Lemma 8.4 that

rmin(Chard|E, A) = rmin(Chard|E, A, 3/4) ≥ rmin(Chard, A, 2/3) ≥Ω 1 . ε2 log n
38

It then follows by Lemma 8.3 that there is a speciﬁc instance C = C(SP , SQ) with |SP |

at least 110 γn such that rmin(C, A) ≥ Ω

1 ε2 log n

. On the other hand, by Theorem 8.5, for

this C, rmin(C) ≤ O ε2√1 n . It follows that for any algorithm A, there exists an instance C

√

such that rmin(C, A) ≥ Ω

n log n

rmin(C), as desired.

8.2 Proof of hardness

In this subsection, we prove Theorem 8.7; namely, we will show that any algorithm needs at

least Ω

1 ε2 log n

samples to succeed on Chard with constant probability. Our main approach

will be to bound the mutual information between the samples provided to the algorithm and

the correct output (recall that B is the hidden bit that determines whether the samples in

X are drawn from p or from q).

Lemma 8.8. If I(XY ; B) < 0.05, then there is no algorithm that can succeed at identifying B with probability at least 23 .
Proof. Fix an algorithm A, and let pe be the probability that it errs at computing B. By Fano’s inequality, we have that

H(pe) ≥ H(B|XY ) = H(B) − I(XY ; B) = 1 − I(XY ; B) > 0.95

Since H( 13) ≤ 0.95, it follows that A must err with probability at least 1/3.

Via the chain rule, we can decompose I(XY ; B) into the sum of many smaller mutual informations.

Lemma 8.9. I(XY ; B) ≤

n i=1

(I

(Xi;

B

)

+

I

(Yi;

B

))

Proof. Write X<i to represent the concatenation X1X2 . . . Xi−1. By the chain rule, we have that

n
I(XY ; B) = I(XiYi; B|X<iY <i)
i=1
We claim that I(XiYi; X<iY <i|B) = 0. To see this, note that given B, each coin in Xi is sampled from some B(p) distribution, where p only depends on whether i ∈ SP or i ∈ SQ. Since each i is chosen to belong to SP and SQ independently with probability γ, this implies

39

Xi (and similarly Yi) are independent from X<i and Y <i given B. By Fact A.7, this implies that I(XiYi; B|X<iY <i) ≤ I(XiYi; B), and therefore that
n
I(XY ; B) ≤ I(XiYi; B).
i=1
Likewise, we can write I(XiYi; B) = I(Xi; B) + I(Yi; B|Xi). Since I(Xi; Yi|B) = 0 (since SP and SQ are chosen independently), again by Fact A.7 it follows that I(Yi; B|Xi) ≤ I(Yi; B) and therefore that
n
I(XY ; B) ≤ (I(Xi; B) + I(Yi; B)) .
i=1

Lemma

8.10.

If

n

≥

400

and

r

=

1 100ε2

ln

n

,

then

for

all

i,

I(B; Xi)

=

I(B; Yi)

≤

1010n .

Proof. By symmetry, I(B; Xi) = I(B; Yi). We will show that I(B; Xi) ≤ 1010n . Let Zi = j Xi,j. Note that Zi is a suﬃcient statistic for B, and therefore I(B; Xi) =
I(B; Zi). By Fact A.9,

I(B; Zi) = EZi[D(B|Zi B)]

r

=

Pr[Zi = z] · D(Pr[B = 0|Zi = z] 21 ).

z=0

We next divide the range of z into two cases.

1. Case 1: |z − rRi| ≤ 11rε ln n. In this case, we will bound the size of D(Pr[B = 0|Zi = z] 21 ). Note that

Pr[B = 0|Zi = z] − 1 = Pr[Zi = z|B = 0] · Pr[B = 0] − 1

2

Pr[Zi = z]

2

=

Pr[Zi = z|B = 0]

−1

Pr[Zi = z|B = 0] + Pr[Zi = z|B = 1] 2

= |Pr[Zi = z|B = 0] − Pr[Zi = z|B = 1]|

(3)

2(Pr[Zi = z|B = 0] + Pr[Zi = z|B = 1])

Now, note that

Pr[Zi = z|B = 0] = (1 − γ) r Rz(1 − Ri)r−z + γ r (Ri(1 + ε))z(1 − Ri(1 + ε))r−z

zi

z

Pr[Zi = z|B = 1] = (1 − γ) r Rz(1 − Ri)r−z + γ r (Ri(1 − ε))z(1 − Ri(1 − ε))r−z

zi

z

40

We can therefore lower bound the denominator of (3) via

2(Pr[Zi = z|B = 0] + Pr[Zi = z|B = 1]) ≥ 4(1 − γ) r Rz(1 − Ri)r−z zi
≥ 2 r Rz(1 − Ri)r−z zi
Likewise, we can write the numerator of (3) as

where

|Pr[Zi = z|B = 0] − Pr[Zi = z|B = 1]| = γ r Rz(1 − Ri)r−zM zi

M= =

(1 + ε)z (1 + ε)z

1 − Ri(1 + ε)

r−z
− (1 − ε)z

1 − Ri

1 − Ri(1 − ε) r−z 1 − Ri

1−

Ri

r−z
ε − (1 − ε)z

1 − Ri

1 + Ri ε r−z . 1 − Ri

To bound M, note that (applying the inequality 1 + x ≤ ex)

(1 + ε)z 1 − Ri ε r−z ≤ exp εz − ε Ri (r − z)

1 − Ri

1 − Ri

= exp ε z − rRi 1 − Ri

≤ exp(4ε(z − rRi))

≤ exp(44rε2 ln n)

= e0.44

<2

r−z
Similarly, (1 − ε)z 1 + 1−RRi i ε ≤ 2. It follows that M ≤ 2, and therefore that

1 Pr[B = 0|Zi = z] − 2

= |Pr[Zi = z|B = 0] − Pr[Zi = z|B = 1]| 2(Pr[Zi = z|B = 0] + Pr[Zi = z|B = 1])

≤

γ

r z

Riz(1 − Ri)r−zM

2

r z

Riz(1 − Ri)r−z

= γM 2
≤γ

41

By Fact A.10, this implies that
D(Pr[B = 0|Zi = z] 1 ) ≤ 4γ2 . 2 ln 2
2. Case 2: |z − rRi| > 11rε ln n. Let Z+ be the sum of r i.i.d. B (Ri(1 + ε)) random variables. Note that since Z is the sum of r B(p) random variables for some p ≤ Ri(1 + ε), Pr[Z+ ≥ x] ≥ Pr[Z ≥ x] for all x. Therefore, by Hoeﬀding’s inequality, we have that

Pr [Z − rRi ≥ 11rε ln n] ≤ Pr Z+ − rRi ≥ 11rε ln n ≤ Pr Z+ − rRi(1 + ε) ≥ rε(11 ln n − Ri) ≤ Pr Z+ − E[Z+] ≥ 10rε ln n
≤ exp −2(10rε ln n)2 r
= exp(−2 ln n) = n−2

Likewise, we can show that

Pr [Z − rRi ≤ −11rε ln n] ≤ n−2 so

Pr [|Z − rRi| ≥ 11rε ln n] ≤ 2n−2

Combining these two cases, we have that (for n ≥ 400)

r

I(B; Zi) =

Pr[Zi = z] · D(Pr[B = 0|Zi = z] 12 )

z=0

≤

Pr[Zi = z] · 1 +

Pr[Zi = z] · O(γ2)

| z −r/2|>11rε ln n

| z −r/2|≤11rε ln n

≤ 2n−2 + 4γ2 ln 2
≤ 1. 100n

We can now complete the proof of Theorem 8.7. 42

Proof

of

Theorem

8.7.

Combining

Lemmas

8.9

and

8.10,

we

have

that

if

r

=

1 100ε2

ln

n

,

then

(for n ≥ 400) I(XY ; B) ≤ 2nI(Xi; B) ≤ 0.02. Therefore by Lemma 8.8, there exists no

algorithm A that, given this number of samples, correctly identiﬁes B (and thus solves the

domination problem) with probability at least 2/3. It follows that

as desired.

rmin(Chard, A, 2 ) ≥

1

=Ω

1

3 100ε2 ln n

ε2 log n

8.3 Proving hardness for Top-K

We will now show how to use our hard distribution of instances of Domination to generate

a hard distribution of instances of Top-K. Our goal will be to embed our Domination

instance as rows k and k + 1 of our SST matrix; hence, intuitively, deciding which of the two

rows (k or k + 1) belongs to the top k is as hard as solving the domination problem.

Unfortunately, the SST condition imposes additional structure that prevents us from di-

rectly embedding any instance of the domination problem. However, for appropriate choices

of the constants Ri, all instances in the support of Chard give rise to valid SST matrices.

Speciﬁcally, we construct the following distribution Shard over Top-K instances S of

size n + 2. Consider the distribution Chard over Domination instances of size n, where for

1

≤

i

≤

n,

Ri

=

1 4

+

i 8n

,

and

ε

=

1 100n2

.

Now,

consider

the

following

map

f

from

Domination

instances C = (p, q) to Top-K instances S = f (C) = (n + 2, k, P): we choose k = n + 1 (so

that the problem becomes equivalent to identifying row n + 2) and deﬁne the matrix P as

follows:



pj

if i = n + 1 and j ≤ n





qj

if i = n + 2 and j ≤ n



Pij = 1 − pi if j = n + 1 and i ≤ n

1 − qi   1
2

if j = n + 2 and i ≤ n otherwise

In general, for arbitrary p and q, this matrix may not be an SST matrix. Note however

that for this choice of Ri and ε, it is always the case that Ri(1 + ε) ≤ Ri+1(1 − ε), so for all i (regardless of sample C), pi < pi+1. In addition, all the Ri belong to [1/4, 3/8], so for all i, pi and qi are less than 1/2. From these two observations, it easily follows that if C belongs to the support of Chard, P is an SST matrix, and f (C) is a valid instance of the top-k problem. We will write Shard = f (Chard) to denote the distribution of instances of top-k f (C) where C is sampled from Chard. Likewise, for any event E (e.g. the event that |SP | ≥ 110 nγ) , we write Shard|E to denote the distribution f (Chard|E).
We will begin by showing that, if there exists a sample eﬃcient algorithm for some

Domination instance C in the support of Chard, there exists a similarly eﬃcient algorithm for the corresponding Top-K instance S = f (C).

43

Lemma 8.11. If C ∈ supp Chard and S = f (C), then rmin(S) ≤ max(rmin(C, 54 ), 1000n2(1 + ln n)).

Proof. Let A be an algorithm that successfully solves the Domination instance C with

probability at least

4 5

using

rmin(C, 54 )

samples.

We will show how to use A to construct

an algorithm A′ that solves the Top-K instance S with probability at least 3/4 using r =

max(rmin(C, 54 ), 1000n2(1 + ln n)) samples.

For each i, j, write Zi,j =

r ℓ=1

Zi,j,ℓ.

Our

algorithm

A′

operates

as

follows.

1. We begin by ﬁnding the two rows with the smallest row sums j Zi,j. Let these two rows have indices c and d. We claim that, with high probability, π−1({c, d}) = {n + 1, n + 2}.
To see this, note that for all i ∈ π({n + 1, n + 2}), Pi,j ≥ 12 , so E j Zi,j ≥ n2 + 1 r. Thus, for any ﬁxed i ∈ π({n + 1, n + 2}), it follows from Hoeﬀding’s inequality that

7

nr

Pr Zi,j ≤ 16n + 1 r ≤ exp − 128

j

so by the union bound, the probability that there exists an i ∈ π−1({n + 1, n + 2}) such that j Zi,j ≤ 176 n + 1 r is at most n exp − 1n2r8 .
On the other hand, if i ∈ π({n+1, n+2}) then Pi,j ≤ 83 (1+ε) unless j ∈ π({n+1, n+2}), where Pi,j = 21 ; it follows that in this case, E j Zi,j ≤ 38n (1 + ε) + 1 r. Similarly, applying Hoeﬀding’s inequality in this case, we ﬁnd that for any ﬁxed i ∈ π−1({n + 1, n + 2}),

7

nr

nr

Pr Zi,j ≥ 16n + 1 r ≤ exp −128(1 + ε)2 ≤ 1.5 exp −128

j

and thus the probability that there exists some i ∈ π−1({n + 1, n + 2}), such that j Zi,j ≥ 176 n + 1 r is at most 3 exp − 1n2r8 . It follows that, altogether, the probabil-
ity that π−1({c, d}) = {n+1, n+2} is at most (n+3) exp − 1n2r8 . Since r ≥ 1000n2 ln n, this is at most 4 exp(−1000/128) < 0.01.

2. We next sort the values Zc,j for j ∈ [n + 2] \ {c, d} and obtain indices j1, j2, . . . , jn so that Zc,j1 ≤ Zc,j2 ≤ · · · ≤ Zc,jn. We claim that, with high probability, for all a, π−1(ja) = a.

For each i, let Ui be the interval

Ri(1

−

ε)

−

201n ,

Ri(1

+

ε)

+

1 20n

.

Note that, by our

choice of Ri and ε, all the intervals Ui are disjoint, with Ui less than Ui+1 for all i. We

will show that with high probability, 1r Zc,π(i) ∈ Ui for all i, thus implying the previous claim.

44

Note that Zc,π(i) is the sum of r B(p) random variables, where p is either (1 + ε)Ri, Ri, or (1 − ε)Ri. By Hoeﬀding’s inequality, it follows that

1 Pr Zc,π(i) ≥ r Ri(1 + ε) + 20n Likewise,

≤ exp −2 (r/20n)2 r
= exp − r 200n2

Pr Zc,π(i) ≤ r Thus, for any ﬁxed i,

1 Ri(1 − ε) − 20n

≤ exp − r 200n2

Zc,π(i)

r

Pr r ∈ Ui ≤ 2 exp − 200n2

and by the union bound, the probability this fails for some i is at most 2n exp −200rn2 . Since r ≥ 1000n2(1 + ln n), exp − 200rn2 ≤ (ne)−5, so this probability is at most 2e−5 < 0.02.

3. Finally, we give algorithm A as input Xi,ℓ = Zc,ji,ℓ and Yi,ℓ = Zd,ji,ℓ. Note that (conditioned on the above two claims holding), this input is distributed equivalently to input from the Domination instance C. In particular, if π−1(c) = n + 1 and π−1(d) = n + 2, then each Xi,ℓ is distributed according to B(pi) and each Yi,ℓ is distributed according to B(qi), and if π−1(c) = n + 2 and π−1(d) = n + 1, then each Xi,ℓ is distributed according to B(qi) and each Yi,ℓ is distributed according to B(pi). Thus, if A returns B = 0, we return [n + 2] \ {d} as the top n + 1 indices, and if A
returns B = 1, we return [n + 2] \ {c} as the top n + 1 indices.
The probability that A fails given that steps 1 and 2 succeed is at most 0.2, and the probability that either of the two steps fail to succeed is at most 0.01 + 0.02 = 0.03. Since 0.2 + 0.03 < 14 , A′ succeeds with probability at least 34 , as desired.

Corollary 8.12. Let E be the event that |SP | ≥ 110 nγ. If C ∈ supp (Chard|E) and S = f (C), then rmin(S) ≤ O(n3.5).

Proof.

Recall

that

by

Theorem

8.5,

for

any

C

∈

supp (Chard|E),

rmin(C,

4 5

)

≤

O

√n1ε2

=

O(n3.5).

By

Lemma

8.11,

rmin (S )

≤

max(rmin(C,

4 5

),

1000n2(1

+

ln

n))

≤

O(n3.5).

We next show that solving Top-K over the distribution Shard|E is at least as hard as solving Domination over the distribution Chard|E.

45

Lemma 8.13. For any algorithm A that solves Top-K, there exists an algorithm A′ that

solves

domination

such

that

rmin(Shard, A, p)

≥

1 2

rmin

(Char

d

,

A′

,

p).

Proof. We will show more generally that for any distribution C of Domination instances,

if

S

=

f (C)

is

a

valid

distribution

of

Top-K

instances,

then

rmin(S, A, p)

≥

1 2

r

mi

n

(

C

,

A′

,

p

)

.

We will construct A′ by embedding the domination instance inside a top-k instance in

much the same way that the function f does, and then using A to solve the top-k instance.

We receive as input two sets of samples Xi,ℓ and Yi,ℓ (where 1 ≤ i, j ≤ n and 1 ≤ ℓ ≤ r) from

some Domination instance C drawn from C. We then generate a random permutation

π of [n + 2]. We use our input and this permutation to generate a matrix Zi,j,ℓ (where 1 ≤ i, j ≤ n + 2 and 1 ≤ ℓ ≤ 2r ) of samples to input to A as follows.
For 1 ≤ i, j ≤ n, set each Zπ(i),π(j),ℓ to be a random B( 21 ) random variable. Similarly, for n + 1 ≤ i, j ≤ n + 2, set each Zπ(i),π(j),ℓ to be a random B( 12 ) random variable. Now, for all 1 ≤ j ≤ n, set Zπ(n+1),π(j),ℓ = Xj,ℓ and set Zπ(n+2),π(j),ℓ = Yj,ℓ. Similarly, for all 1 ≤ i ≤ n,

set Zπ(i),π(n+1),ℓ = 1 − Xi,ℓ+r/2 and set Zπ(i),π(n+2),ℓ = 1 − Yi,ℓ+r/2. Finally, set k = n + 1 and

ask A to solve the Top-K instance deﬁned by k and Zi,j,ℓ. If A returns that π(n + 1) is in

the top n + 1 indices, return B = 0, and otherwise return B = 1.

From our construction, if the r samples of X and Y are distributed according to a

Domination instance C, then the r/2 samples of Z are distributed according to the Top-K

instance S = f (C). Since A succeeds with probability p on distribution S with rmin(S, A, p)

samples, A′ therefore succeeds with probability p on distribution C with 2rmin(S, A, p) sam-

ples,

thus

implying

that

rmin(S, A, p)

≥

1 2

r

mi

n

(

C

,

A′

,

p

)

.

Corollary 8.14. For all algorithms A that solve Top-K, rmin(Shard, A, 32 ) = Ω

n4 log n

.

Proof.

Theorem

8.7

tells

us

that

for

all

algorithms

A′

that

solve

Domination,

rmin(Chard,

A,

2 3

)

=

Ω

1 ε2 log n

=Ω

long4n . Combining this with Lemma 8.13, we obtain the desired result.

We can now prove Theorem 8.1 in much the same fashion as Theorem 8.2.

Proof of Theorem 8.1. By Corollary 8.14, rmin(Shard, A, 23 ) = Ω

n4 log n

. Let E be the event

that |SP | ≥ 110 nγ (in the original Domination instance C). By Lemma 8.6, if n ≥ (400 ln 1112 )2, Pr[E] ≥ 112 , and it follows from Lemma 8.4 that

3 rmin(Shard|E, A) = rmin(Shard|E, A, 4 )
2 ≥ rmin(Shard, A, 3 ) ≥ Ω n4
log n

It therefore follows from 8.3 that there is a speciﬁc instance S in the support of Shard|E

such that rmin(S, A) ≥ Ω

n4 log n

. However, by Corollary 8.12, rmin(S) ≤ O(n3.5). It follows

46

that for any algorithm A, there exists an instance S of Top-K such that rmin(S, A) ≥

√

Ω

n log n

rmin(S), as desired.

References
[ACN08] N Ailon, M. Charikar, and A. Newman. Aggregating inconsistent information: ranking and clustering. Journal of the ACM, 55(5):23:1–23:27, 2008.
[Ail11] N. Ailon. Active learning ranking from pairwise preferences with almost optimal query complexity. In Advances in Neural Information Processing Systems, 2011.
[BM08] M. Braverman and E. Mossel. Noisy sorting without resampling. In Proc. ACMSIAM symposium on Discrete algorithms, 2008.
[BM15] Mark Braverman and Jieming Mao. Simulating noisy channel interaction. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science. ACM, 2015.
[BMW16] Mark Braverman, Jieming Mao, and Matthew S. Weinberg. Parallel algorithms for se- lect and partition with noisy comparisons. In 48th Annual Symposium on the Theory of Computing, STOC, 2016.
[BT52] R. Bradley and M. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.
[BW97] T. P. Ballinger and N. T. Wilcox. Decisions, error and heterogeneity. The Economic Journal, 107(443):1090–1105, 1997.
[BWV13] Sebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identiﬁcations in multi-armed bandits. In Proceedings of the International Conference on Machine Learning (ICML), 2013.
[CK11] Imre Csiszar and J´anos Ko¨rner. Information theory: coding theorems for discrete memoryless systems. Cambridge University Press, 2011.
[CS15] Y. Chen and C. Suh. Spectral mle: Top-k rank aggregation from pairwise comparisons. In International Conference on Machine Learning, 2015.
[DKNS01] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the web. In Proceedings of the Tenth International World Wide Web Conference, 2001.
[DM59] D. Davidson and J. Marschak. Experimental tests of a stochastic decision theory. Measurement: Deﬁnitions and theories, pages 233–269, 1959.

47

[Eri13] B. Eriksson. Learning to top-k search using pairwise comparisons. In Conference on Artiﬁcial Intelligence and Statistics, 2013.
[Fis73] P. C. Fishburn. Binary choice probabilities: on the varieties of stochastic transitivity. Journal of Mathematical psychology, 10(4):327–352, 1973.
[JKSO13] Minje Jang, Sunghyun Kim, Changho Suh, and Sewoong Oh. Top-k ranking from pairwise comparisons: When spectral ranking is optimal. arXiv preprint arXiv:1603.04153, 2013.
[JMN+14] Kevin Jamieson, Matthew Malloy, Robert Nowak, , and S´ebastien Bubeck. lil’ ucb : An optimal exploration algorithm for multi-armed bandits. In Proceedings of Conference on Learning Theory, 2014.
[JN11] K. Jamieson and R. Nowak. Active ranking using pairwise comparisons. In Advances in Neural Information Processing Systems, 2011.
[KMS07] C. Kenyon-Mathieu and W. Schudy. How to rank with few errors. In Symposium on Theory of computing (STOC), 2007.
[LB11] Tyler Lu and Craig Boutilier. Learning mallows models with pairwise preferences. In ICML, 2011.
[Luc59] R Duncan Luce. Individual choice behavior: A theoretical analysis. New York: Wiley, 1959.
[ML65] D. H. McLaughlin and R. D. Luce. Stochastic transitivity and cancellation of preferences between bitter-sweet solutions. Psychonomic Science, 2(1–12):89–90, 1965.
[NOS12] S. Negahban, S. Oh, and D. Sha. Rank centrality: Ranking from pair-wise comparisons. arXiv preprint arXiv:1209.1688, 2012.
[RA14] A. Rajkumar and S. Agarwal. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In International Conference on Machine Learning, 2014.
[SBGW15] N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. arXiv preprint arXiv:1510.05610, 2015.
[STZ16] Changho Suh, Vincent Tan, and Renbo Zhao. Adversarial top-k ranking. arXiv preprint arXiv:1602.04567, 2016.
[SW15] N. B. Shah and M. Wainwright. Simple, robust and optimal ranking from pairwise comparisons. arXiv preprint arXiv:1512.08949, 2015.
48

[Thu27] L. L. Thurstone. A law of comparative judgement. Psychological Reviews, 34(4):273, 1927.
[Tve72] A. Tversky. Elimination by aspects: A theory of choice. Psychological review, 79(4):281–299, 1972.
[WMJ13] F. Wauthier, M.Jordan, and N. Jojic. Eﬃcient ranking from pairwise comparisons. In International Conference on Machine Learning, 2013.
[ZCL14] Yuan Zhou, Xi Chen, and Jian Li. Optimal pac multiple arm identiﬁcation with applications to crowdsourcing. In Proceedings of International Conference on Machine Learning, 2014.
A Probability and Information Theory Preliminaries
We brieﬂy review some standard facts and deﬁnitions from information theory we will use throughout this paper. For a more detailed introduction, we refer the reader to [CK11].
Throughout this paper, we use log to refer to the base 2 logarithm and use ln to refer to the natural logarithm.
Deﬁnition A.1. The entropy of a random variable X, denoted by H(X), is deﬁned as H(X) = x Pr[X = x] log(1/ Pr[X = x]).
If X is drawn from Bernoulli distributions B(p), we use H(p) = −(p log p+(1−p)(log(1− p)) to denote H(X).
Deﬁnition A.2. The conditional entropy of random variable X conditioned on random variable Y is deﬁned as H(X|Y ) = Ey[H(X|Y = y)].
Fact A.3. H(XY ) = H(X) + H(Y |X).
Deﬁnition A.4. The mutual information between two random variables X and Y is deﬁned as I(X; Y ) = H(X) − H(X|Y ) = H(Y ) − H(Y |X).
Deﬁnition A.5. The conditional mutual information between X and Y given Z is deﬁned as I(X; Y |Z) = H(X|Z) − H(X|Y Z) = H(Y |Z) − H(Y |XZ).
Fact A.6. Let X1, X2, Y, Z be random variables, we have I(X1X2; Y |Z) = I(X1; Y |Z) + I(X2; Y |X1Z).
Fact A.7. Let X, Y, Z, W be random variables. If I(Y ; W |X, Z) = 0, then I(X; Y |Z) ≥ I(X; Y |ZW ).
Deﬁnition A.8. The Kullback-Leibler divergence between two random variables X and Y is deﬁned as D(X Y ) = x Pr[X = x] log(Pr[X = x]/ Pr[Y = x]).
49

If X and Y are drawn from Bernoulli distribution Bp and Bq, we write D(p q) as an abbreviation for D(X Y ).

Fact A.9. Let X, Y, Z be random variables, we have I(X; Y |Z) = Ex,z[D((Y |X = x, Z = z) (Y |Z = z))].

Fact A.10. Let X, Y be random variables,

| Pr[X = x] − Pr[Y = x]|2 ≤ ln(2) · D(X Y ) ≤ | Pr[X = x] − Pr[Y = x]|2 .

x 2 max{Pr[X = x], Pr[Y = x]}

x Pr[Y = x]

Proof. A proof of Fact A.10 can be found in [BM15].

We will also need the following quantitative version of the central limit theorem.

Lemma A.11 (Berry-Esseen Theorem). Let Z1, · · · , Zk be independent random variables

and let S =

k i=1

Zi.

Let µ = E[S] =

k i=1

E[Zi

],

σ2

=

Var[S]

=

k i=1

Var[Zi

]

and

γ

=

k i=1

E[|Zi

− E[Zi]|3].

Let

Φ

be

the

CDF

of

standard

Gaussian.

Then

for

all

t

∈

R,

Pr [S < t] − Φ t − µ σ

≤ γ. σ3

Finally, we will need the following estimates on the tails of the Gaussian distribution.

Lemma A.12. Let Φ(t) be the CDF of standard Gaussian distribution then for t > 0,

√1 exp(−t2/2) 2π

1− 1 t t3

≤ 1 − Φ(t) ≤ √1 exp(−t2/2) 1.

2π

t

Proof.

1 − Φ(t) = √1 2π
= √1 2π
= √1 2π
= √1 2π

∞

exp(−x2/2)dx

t
∞ 1 · x exp(−x2/2)dx tx

exp(−t2/2) − ∞ 1 exp(−x2/2)dx

t

t x2

(integration by parts)

exp(−t2/2) − exp(−t2/2) + ∞ 3 exp(−x2/2)dx .

t

t3

t x4

(integration by parts again)

From the last two expressions, we get the required upper and lower bounds.

50

