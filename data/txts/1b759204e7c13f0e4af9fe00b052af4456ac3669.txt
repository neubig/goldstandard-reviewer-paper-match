An Analysis of Frame-skipping in Reinforcement Learning

Shivaram Kalyanakrishnan
IIT Bombay shivaram@cse.iitb.ac.in

Siddharth Aravindan *
National University of Singapore siddharth.aravindan@comp.nus.edu.sg

Vishwajeet Bagdawat *
Panasonic Corporation vishwajeet.singh@jp.panasonic.com

Varun Bhatt *
University of Alberta vbhatt@ualberta.ca

Harshith Goka *
Samsung Research h9399.goka@samsung.com

Archit Gupta *
Rubrik archit.gupta@rubrik.com

arXiv:2102.03718v1 [cs.LG] 7 Feb 2021

Kalpesh Krishna *
UMass Amherst kalpesh@cs.umass.edu
ABSTRACT
In the practice of sequential decision making, agents are often designed to sense state at regular intervals of 𝑑 time steps, 𝑑 > 1, ignoring state information in between sensing steps. While it is clear that this practice can reduce sensing and compute costs, recent results indicate a further benefit. On many Atari console games, reinforcement learning (RL) algorithms deliver substantially better policies when run with 𝑑 > 1—in fact with 𝑑 even as high as 180. In this paper, we investigate the role of the parameter 𝑑 in RL; 𝑑 is called the “frame-skip” parameter, since states in the Atari domain are images. For evaluating a fixed policy, we observe that under standard conditions, frame-skipping does not affect asymptotic consistency. Depending on other parameters, it can possibly even benefit learning. To use 𝑑 > 1 in the control setting, one must first specify which 𝑑-step open-loop action sequences can be executed in between sensing steps. We focus on “action- repetition”, the common restriction of this choice to 𝑑-length sequences of the same action. We define a task-dependent quantity called the “price of inertia”, in terms of which we upper-bound the loss incurred by action-repetition. We show that this loss may be offset by the gain brought to learning by a smaller task horizon. Our analysis is supported by experiments on different tasks and learning algorithms.
KEYWORDS
Reinforcement Learning, TD Learning, Frame-skipping .
1 INTRODUCTION
Sequential decision making tasks are most commonly formulated as Markov Decision Problems [1]. An MDP models a world with state transitions that depend on the action an agent may choose. Transitions also yield rewards. Every MDP is guaranteed to have an optimal policy: a state-to-action mapping that maximises expected long-term reward [2]. Yet, on a given task, it might not be necessary to sense state at each time step in order to optimise performance. For example, even if the hardware allows a car to sense state and select actions every millisecond, it might suffice on typical roads to do so once every ten milliseconds. The reduction in reaction time
* Equal contribution

Vihari Piratla *
IIT Bombay viharipiratla@gmail.com
by so doing might have a negligible effect on performance, and be justified by the substantial savings in sensing and computation.
Recent empirical studies bring to light a less obvious benefit from reducing the frequency of sensing: sheer improvements in performance when behaviour is learned [3–5]. On the popular Atari console games benchmark [6] for reinforcement learning (RL), reduced sensing takes the form of “frame-skipping”, since agents in this domain sense image frames and respond with actions. In the original implementation, sensing is limited to every 4-th frame, with the intent of lightening the computational load [7]. However, subsequent research has shown that higher performance levels can be reached by skipping up to 180 frames in some games [3].
We continue to use the term “frame-skipping” generically across all sequential decision making tasks, denoting by parameter 𝑑 ≥ 1 the number of time steps between sensing steps (so 𝑑 = 1 means no frame-skipping). For using 𝑑 > 1, observe that it is necessary to specify an entire sequence of actions, to execute in an open-loop fashion, in between sensed frames. The most common strategy for so doing is “action-repetition”, whereby the same atomic action is repeated 𝑑 times. Action-repetition has been the default strategy for implementing frame-skipping on the Atari console games, both when 𝑑 is treated as a hyperparameter [3, 7] and when it is adapted on-line, during the agent’s lifetime [4, 5, 8].
In this paper, we analyse the role of frame-skipping and actionrepetition in RL—in short, examining why they work. We begin by surveying topics in sequential decision making that share connections with frame-skipping and action-repetition (Section 2). Thereafter we provide formal problem definitions in Section 3. In Section 4, we take up the problem of prediction: estimating the value function of a fixed policy. We show that prediction with frameskipping continues to give consistent estimates when used with linear function approximation. Additionally, 𝑑 serves as a handle to simultaneously tune the amount of bootstrapping and the task horizon. In Section 5, we investigate the control setting, wherein behaviour is adapted based on experience. First we define a taskspecific quantity called the “price of inertia”, in terms of which we bound the loss incurred by action-repetition. Thereafter we show that frame-skipping might still be beneficial in aggregate because it reduces the effective task horizon. In Section 6, we augment our analysis with empirical findings on different tasks and learning algorithms. Among our results is a successful demonstration of

learning defensive play in soccer, a hitherto less-explored side of the game [9]. We conclude with a summary in Section 7.
2 LITERATURE SURVEY
Frame-skipping may be viewed as an instance of (partial) openloop control, under which a predetermined sequence of (possibly different) actions is executed without heed to intermediate states. Aiming to minimise sensing, Hansen et al. [10] propose a framework for incorporating variable-length open-loop action sequences in regular (closed-loop) control. The primary challenge in general open-loop control is that the number of action sequences of some given length 𝑑 is exponential in 𝑑. Consequently, the main focus in the area is on strategies to prune corresponding data structures [10– 12]. Since action repetition restricts itself to a set of actions with size linear in 𝑑, it allows for 𝑑 itself to be set much higher in practice [3].
To the best of our knowledge, the earliest treatment of actionrepetition in the form we consider here is by Buckland and Lawrence [13]. While designing agents to negotiate a race track, these authors note that successful controllers need only change actions at “transition points” such as curves, while repeating the same action for long stretches. They propose an algorithmic framework that only keeps transition points in memory, thereby achieving savings. In spite of limitations such as the assumption of a discrete state space, their work provides conceptual guidance for later work. For example, Buckland [14, see Section 3.1.4] informally discusses “inertia” as a property determining a task’s suitability to action repetition—and we formalise the very same concept in Section 5.
Investigations into the effect of action repetition on learning begin with the work of McGovern et al. [15], who identify two qualitative benefits: improved exploration (also affirmed by Randløv [16]) and the shorter resulting task horizon. While these inferences are drawn from experiments on small, discrete, tasks, they find support in a recent line of experiments on the Atari environment, in which neural networks are used as the underlying representation [4]. In the original implementation of the DQN algorithm on Atari console games, actions are repeated 4 times, mainly to reduce computational load [7]. However, subsequent research has shown that higher performance levels can be reached by persisting actions for longer—up to 180 frames in some games [3]. More recently, Lakshminarayanan et al. [5] propose a policy gradient approach to optimise 𝑑 (fixed to be either 4 or 20) on-line. Their work sets up the FiGAR (Fine Grained Action Repetition) algorithm [8], which optimises over a wider range of 𝑑 and achieves significant improvements in many Atari games. It is all this empirical evidence in favour of action repetition that motivates our quest for a theoretical understanding.
The idea that “similar” states will have a common optimal action also forms the basis for state aggregation, a process under which such states are grouped together [17]. In practice, state aggregation usually requires domain knowledge, which restricts it to low-dimensional settings [18, 19]. Generic, theoretically-grounded state-aggregation methods are even less practicable, and often validated on tasks with only a handful of states [20]. In contrast, actionrepetition applies the principle that states that occur close in time are likely to be similar. Naturally this rule of thumb is an approximation, but one that remains applicable in higher-dimensional settings (the HFO defense task in Section 6 has 16 features).

Even on tasks that favour action-repetition, it could be beneficial to explicitly reason about the “intention” of an action, such as to reach a particular state. This type of temporal abstraction is formalised as an option [21], which is a closed-loop policy with initial and terminating constraints. As numerous experiments show, action-repetition performs well on a variety of tasks in spite of being open-loop in between decisions. We expect options to be more effective when the task at hand requires explicit skill discovery [22].
In this paper, we take the frame-skip parameter 𝑑 as discrete, fixed, and known to the agent. Thus frame-skipping differs from Semi-Markov Decision Problems [23], in which the duration of actions can be continuous, random, and unknown. In the specific context of temporal difference learning, frame-skipping may both be interpreted as a technique to control bootstrapping [24][see Section 6.2] and one to reduce the task horizon [25].

3 PROBLEM DEFINITION
We begin with background on MDPs, and thereafter formalise the prediction and control problems with frame-skipping.

3.1 Background: MDPs

A Markov Decision Problem (MDP) 𝑀 = (𝑆, 𝐴, 𝑅,𝑇 , 𝛾) comprises a set of states 𝑆 and a set of actions 𝐴. Taking action 𝑎 ∈ 𝐴 from state 𝑠 ∈ 𝑆 yields a numeric reward with expected value 𝑅(𝑠, 𝑎), which is bounded in [−𝑅max, 𝑅max] for some 𝑅max > 0. 𝑅 is the reward func-

tion of 𝑀. The transition function 𝑇 specifies a probability distribu-
tion over 𝑆: for each 𝑠 ′ ∈ 𝑆, 𝑇 (𝑠, 𝑎, 𝑠 ′) is the probability of reaching 𝑠 ′ by taking action 𝑎 from 𝑠. An agent is assumed to interact with 𝑀

over time, starting at some state. At each time step the agent must

decide which action to take. The action yields a next state drawn

stochastically according to 𝑇 and a reward according to 𝑅, resulting

in a state-action-reward sequence 𝑠0, 𝑎0, 𝑟0, 𝑠1, 𝑎1, 𝑟1, 𝑠2, . . . . The nat-

ural objective of the agent is to maximise some notion of expected

long

term

reward,

which

we

take

here

to

be

E

[𝑟

0

+

𝛾

𝑟

1

+

𝛾

2
𝑟

2

+

.

.

.

]

,

where 𝛾 ∈ [0, 1] is a discount factor. We assume 𝛾 < 1 unless the

task encoded by 𝑀 is episodic: that is, all policies eventually reach

a terminal state with probability 1.

A policy 𝜋 : 𝑆 × 𝐴 → [0, 1], specifies for each 𝑠 ∈ 𝑆, a probability

𝜋 (𝑠, 𝑎) of taking action 𝑎 ∈ 𝐴 (hence 𝑎∈𝐴 𝜋 (𝑠, 𝑎) = 1). If an agent

takes actions according to such a policy 𝜋 (by our definition, 𝜋 is

Markovian and stationary), the expected long-term reward accrued

starting at state 𝑠 ∈ 𝑆 is denoted 𝑉 𝜋 (𝑠); 𝑉 𝜋 is the value function

of 𝜋. Let Π be the set of all policies. It is a well-known result that

for

every

MDP

𝑀,

there

is

an

optimal

policy

★
𝜋

∈

Π

such

that

for

all 𝑠 ∈ 𝑆 and 𝜋 ∈ Π, 𝑉 𝜋★ (𝑠) ≥ 𝑉 𝜋 (𝑠) [2] (indeed there is always a

deterministic policy that satisfies optimality).

In the reinforcement learning (RL) setting, an agent interacts

with an MDP by sensing state and receiving rewards, in turn specifying actions to influence its future course. In the prediction setting, the agent follows a fixed policy 𝜋, and is asked to estimate the value function 𝑉 𝜋 . Hence, for prediction, it suffices to view the agent as

interacting with a Markov Reward Process (MRP) (an MDP with decisions fixed by 𝜋). In the control setting, the agent is tasked with improving its performance over time based on the feedback

received. On finite MDPs, exact prediction and optimal control can

both be achieved in the limit of infinite experience [26, 27].

3.2 Frame-skipping
In this paper, we consider generalisations of both prediction and control in which a frame-skip parameter 𝑑 ≥ 1 is provided as input in addition to MDP 𝑀. With frame-skipping, the agent is only
allowed to sense every 𝑑-th state: that is, if the agent has sensed state 𝑠𝑡 at time step 𝑡 ≥ 0, it is oblivious to states 𝑠𝑡+1, 𝑠𝑡+2, . . . , 𝑠𝑡+𝑑−1, and next only observes 𝑠𝑡+𝑑 . We assume, however, that the discounted sum of the rewards accrued in between (or the 𝑑-step return), is available to the agent at time step 𝑡 +𝑑. Indeed in many applications (see, for example, Section 6), this return 𝐺𝑡+𝑑 , defined below, can
𝑡
be obtained without explicit sensing of intermediate states.

𝐺𝑡𝑡+𝑑 d=ef 𝑟𝑡 + 𝛾𝑟𝑡+1 + · · · + 𝛾𝑑−1𝑟𝑡+𝑑−1.

In the problems we formalise below, taking 𝑑 = 1 gives the

versions with no frame-skipping.

Prediction problem. In the case of prediction, we assume that a

fixed policy 𝜋 is independently being executed on 𝑀: that is, for 𝑡 ≥ 0, 𝑎𝑡 ∼ 𝜋 (𝑠𝑡 , ·). However, since the agent’s sensing is limited

to every 𝑑-th transition, its interaction with the resulting MRP

𝑑

2𝑑

3𝑑

becomes

a

sequence

of

the

form

𝑠0,

𝐺
0

,

𝑠𝑑 ,

𝐺
𝑑

, 𝑠2𝑑, 𝐺 , . . . . The
2𝑑

agent must estimate 𝑉 𝜋 based on this sequence.

Control problem. In the control setting, the agent is itself in

charge of action selection. However, due to the constraint on sens-

ing, the agent cannot select actions based on state at all time steps.

Rather, at each time step 𝑡 that state is sensed, the agent can specify

a

𝑑 -length

action

sequence

𝑏

∈

𝑑
𝐴,

which

will

be

executed

open-

loop for 𝑑 steps (until the next sensing step 𝑡 + 𝑑). Hence, the agent-

𝑑

2𝑑

environment interaction takes the form 𝑠0, 𝑏0, 𝐺0 , 𝑠𝑑, 𝑏𝑑, 𝐺𝑑 , 𝑠2𝑑, . . . ,

where for 𝑖 ≥ 0, 𝑏𝑑𝑖 is a 𝑑-length action sequence. The agent’s aim

is still to maximise its long-term reward, but observe that for 𝑑 > 1,

it might not be possible to match 𝜋★, which is fully closed-loop.

In the next section, we analyse the prediction setting with frameskipping; in Section 5 we consider the control setting.

4 PREDICTION WITH FRAME-SKIPPING
In this section, we drop the reference to MDP 𝑀 and policy 𝜋, only assuming that together they fix an MRP 𝑃 = (𝑆, 𝑅,𝑇 , 𝛾). For 𝑠, 𝑠 ′ ∈ 𝑆, 𝑅(𝑠) is the reward obtained on exiting 𝑠 and 𝑇 (𝑠, 𝑠 ′) the probability of reaching 𝑠 ′. For the convergence of any learning algorithm to the value function 𝑉 : 𝑆 → R of 𝑃, it is necessary that 𝑃 be irreducible, ensuring that each state will be visited infinitely often in the limit. If using frame-skip 𝑑 > 1, we must also assume that 𝑃 is aperiodic—otherwise some state might only be visited in between sensing steps, thus precluding convergence to its value. We
proceed with the assumption that 𝑃 is irreducible and aperiodic—in other words, ergodic. Let 𝜇 : 𝑆 → (0, 1), subject to 𝑠 ∈𝑆 𝜇 (𝑠) = 1, be the stationary distribution on 𝑆 induced by 𝑃.

4.1 Consistency of Frame-skipping

If using frame-skipping with parameter 𝑑 ≥ 1, it is immediate that

the agent’s interaction may be viewed as a regular one (with no

frame-skipping)

with

induced

MRP

𝑃𝑑

=

(𝑆, 𝑅 ,𝑇 , 𝛾𝑑 ),
𝑑𝑑

in

which,

if we treat reward functions as |𝑆 |-length vectors and transition

functions as |𝑆 | × |𝑆 | matrices,

𝑅

=

𝑅

+ 𝛾𝑇 𝑅

+ 𝛾 2𝑇 2𝑅

+

·

·

·

+

𝛾

𝑑

−1
𝑇

𝑑

−1𝑅,

and 𝑇

= 𝑇𝑑.

𝑑

𝑑

Since 𝑃 is ergodic, it follows that 𝑃𝑑 is ergodic. Thus, any standard prediction algorithm (for example, TD(𝜆) [24, see Chapter

12]) can be applied on 𝑃 with frame-skip 𝑑—equivalent to being

applied on 𝑃𝑑 with no frame-skip—to converge to its value func-

tion 𝑉
𝑑

:

𝑆

→

R.

It

is

easy

to

see

that

𝑉
𝑑

=

𝑉 . Surprisingly, it

also emerges that the stationary distribution on 𝑆 induced by 𝑃𝑑 — denote it 𝜇𝑑 : 𝑆 → (0, 1), where 𝑠 ∈𝑆 𝜇𝑑 (𝑠) = 1—is identical to 𝜇,

the stationary distribution induced by 𝑃. The following proposition

formally establishes the consistency of frame-skipping.

Proposition 1.

For 𝑑

≥

1

,

𝑉
𝑑

=𝑉

and 𝜇𝑑

= 𝜇.

Proof. For the first part, we have that for 𝑠 ∈ 𝑆,

∞

∞ 𝑑−1

𝑉
𝑑

(𝑠 )

=

∑︁ 𝛾𝑑𝑖 E[𝐺𝑑𝑖+𝑑

|𝑠0

=

𝑠]

=

∑︁

∑︁ 𝛾𝑑𝑖+𝑗 E[

𝑟𝑑𝑖+𝑗

|𝑠0

=

𝑠]

𝑑𝑖

𝑖 =0

𝑖=0 𝑗=0

∞
∑︁ = E[𝛾𝑡 𝑟𝑡 |𝑠0 = 𝑠] = 𝑉 (𝑠).

𝑡 =0

For the second part, observe that since 𝜇 is the stationary distribu-
tion induced by 𝑃, it satisfies 𝑇 𝜇 = 𝜇. With frame-skip 𝑑, we have 𝑇 𝜇 = 𝑇 𝑑 𝜇 = 𝑇 𝑑−1 (𝑇 𝜇) = 𝑇 𝑑−1𝜇 = · · · = 𝑇 𝜇 = 𝜇, establishing that
𝑑
𝜇𝑑 = 𝜇 (its uniqueness following from the ergodicity of 𝑃𝑑 ). □

Preserving the stationary distribution is especially relevant for prediction with approximate architectures, as we see next.

4.2 Frame-skipping with a Linear Architecture

As a concrete illustration, we consider the effect of frame-skip 𝑑

in Linear TD(𝜆) [24, see Chapter 12], the well-known family of

on-line prediction algorithms. We denote our generalised version

of the algorithm TD (𝜆), where 𝑑
𝑑

≥

1 is the given frame-skip

parameter and 𝜆 ∈ [0, 1] controls bootstrapping. With a linear

architecture,

𝑉
𝑑

(𝑠

)

is

approximated

by 𝑤

· 𝜙 (𝑠),

where

for 𝑠

∈

𝑆,

𝜙 (𝑠) is a 𝑘-length vector of features. The 𝑘-length coefficient vector

𝑤 is updated based on experience, keeping a 𝑘-length eligibility

trace vector for backing up rewards to previously-visited states.
Starting with 𝑒0 = 0 and arbitrary 𝑤0, an update is made as follows for each 𝑖 ≥ 0, based on the tuple (𝑠𝑑𝑖, 𝐺𝑑𝑖+𝑑, 𝑠𝑑𝑖+𝑑 ):
𝑑𝑖

𝛿𝑖 ← 𝐺𝑑𝑖+𝑑 + 𝛾𝑑𝑤𝑖 · 𝜙 (𝑠𝑑𝑖+𝑑 ) − 𝑤𝑖 · 𝜙 (𝑠𝑑𝑖 );
𝑑𝑖

𝑤𝑖+1 ← 𝑤𝑖 + 𝛼𝛿𝑖𝑒𝑖 ;

𝑒𝑖+1

←

𝑑
𝛾 𝜆𝑒𝑖

+

𝜙 (𝑠𝑑𝑖 ),

where 𝛼 > 0 is the learning rate. Observe that with full bootstrap-

ping

(𝜆

=

0),

each

update

by

TD
𝑑

(𝜆)

is

identical

to

a

multi-step

(here

𝑑-step) backup [24, see Chapter 7] on 𝑃. The primary difference,

however, is that regular multi-step (and 𝜆-) backups are performed

at

every

time

step.

By

contrast,

TD
𝑑

(𝜆)

makes

an

update

only

once

every 𝑑 steps, hence reducing sensing (as well as the computational

cost of updating 𝑤) by a factor of 𝑑.

With linear function approximation, the best result one can hope

to achieve is convergence to

∑︁ 𝑤opt = argmin 𝐸 (𝑤), where 𝐸 (𝑤) = 𝜇 (𝑠){𝑉 (𝑠) − 𝑤 · 𝜙 (𝑠)}2.

𝑤 ∈R𝑘

𝑠 ∈𝑆

It

is

also

well-known

that

linear 𝑇 𝐷 (𝜆)

converges

to

some

𝑤

∈

𝑘
R

such that 𝐸 (𝑤) ≤ 1−𝛾𝜆 𝐸 (𝑤 ) [28]. Note that TD (𝜆) on 𝑃 is

1−𝛾

opt

𝑑

the same as TD(𝜆) on 𝑃𝑑 . Hence, from Proposition 1, we conclude

that TD (𝜆) on 𝑃
𝑑

converges to some 𝑤

∈

𝑘
R

such that 𝐸 (𝑤)

≤

11−−𝛾𝛾𝑑𝑑𝜆 𝐸 (𝑤𝑜𝑝𝑡 ). The significance of this result is that the rate of

sensing can be made arbitrarily small (by increasing 𝑑), and yet

convergence to 𝑤𝑜𝑝𝑡 achieved (by taking 𝜆 = 1). The result might

appear intriguing, since for fixed 𝜆 < 1, a tighter bound is obtained by increasing 𝑑 (making fewer updates). Nonetheless, note that the bound is on the convergent limit; the best results after any finite number of steps are likely to be obtained for some finite value of 𝑑.

The bias-variance analysis of multi-step returns [29] applies as is

to 𝑑: small values imply more bootstrapping and bias, large values

imply higher variance.

To demonstrate the effect of 𝑑 in practice, we construct a rela-

tively simple MRP—described in Figure 1—in which linear TD (𝜆)
𝑑
has to learn only a single parameter 𝑤. Figure 1c shows the prediction errors after 100,000 steps (thus ⌊100, 000/𝑑⌋ learning updates).

When 𝛼 and 𝜆 are fixed, observe that the error for smaller values

of 𝜆 is minimised at 𝑑 > 1, suggesting that 𝑑 can be a useful param-

eter to tune in practice. However, the lowest errors can always be

obtained by taking 𝜆 sufficiently close to 1 and suitably lowering 𝛼,

with no need to tune 𝑑. We obtain similar results by generalising

“True online TD(𝜆)” [30]; its near-identical plot is omitted.

5 CONTROL WITH ACTION-REPETITION
In this section, we analyse frame-skipping in the control setting, wherein the agent is in charge of action selection. If sensing is restricted to every 𝑑-th step, recall from Section 3 that the agent must

choose

a

𝑑 -length

sequence

of

actions

𝑏

∈

𝑑
𝐴

at

every

sensing

step.

The most common approach [3, 4] is to perform action-repetition:
that is, to restrict this choice to sequences of the same action. This way the agent continues to have |𝐴| action sequences to consider (rather than |𝐴|𝑑 ). It is also possible to consider 𝑑 as a parameter for

the agent to itself learn, possibly as a function of state [5, 31]. We

report some results from this approach in Section 6, but proceed

with our analysis by taking 𝑑 to be a fixed input parameter. Thus,

the agent must pick an action sequence 𝑏 ∈ {𝑎𝑑, 𝑎 ∈ 𝐴}.

It is not hard to see that interacting with input MDP 𝑀 = (𝑆, 𝐴, 𝑅,𝑇 , 𝛾) by repeating actions 𝑑 times is equivalent to interact-

ing

with

an

induced

MDP

𝑀
𝑑

=

(𝑆, 𝐴 , 𝑅 ,𝑇 , 𝛾𝑑 )
𝑑 𝑑𝑑

without

action-

repetition [10]. Here 𝐴 = {𝑎𝑑, 𝑎 ∈ 𝐴}. For 𝑠, 𝑠 ′ ∈ 𝑆, 𝑎 ∈ 𝐴, (1) let
𝑑

𝑎
𝑅

denote

𝑅 ( ·,

𝑎)

as

an

|𝑆 |-length

vector—thus

𝑎
𝑅

(𝑠 )

=

𝑅 (𝑠,

𝑎)—and

(2) let 𝑇 𝑎 denote 𝑇 (·, 𝑎, ·) as an |𝑆 | × |𝑆 | matrix—thus 𝑇 𝑎 (𝑠, 𝑠 ′) =

𝑇 (𝑠, 𝑎, 𝑠 ′). Then 𝑅

(𝑠,

𝑎)

=

𝑎
𝑅

(𝑠 )

and

𝑇

(𝑠,

𝑎,

𝑠 ′)

=

𝑎
𝑇

(𝑠,

𝑠 ′),

where

𝑑

𝑑

𝑑

𝑑

𝑑 −1

∑︁

𝑎
𝑅

=

(𝛾𝑇 𝑎) 𝑗 𝑅𝑎,

𝑎
and 𝑇

=

(𝑇 𝑎)𝑑 .

𝑑

𝑑

𝑗 =0

5.1 Price of inertia

The risk of using 𝑑 > 1 in the control setting is that in some tasks,

a single unwarranted repetition of action could be catastrophic. On

the other hand, in tasks with gradual changes of state, the agent

must be able to recover. To quantify the amenability of task 𝑀

to action repetition, we define a term called its “price of inertia”,

denoted Δ𝑀 . For 𝑑

≥

1, 𝑠

∈

𝑆, 𝑎

∈

𝐴,

let

★
𝑄

(𝑠,

𝑎𝑑 )

denote the

𝑀

expected long-term reward of repeating action 𝑎 from state 𝑠 for

𝑑 time steps, and thereafter acting optimally on 𝑀. The price of

—V*m-V*d—

2000∆ = 1

3.6

99

1500

1000 3.4
500

. . .

0 0 2 4 6 8 10 12 14 16

3.2

E(w100,000/d)

α=10-7, λ=0.5

α=10-7, λ=0

α=10-7, λ=0.99

d

−240 delta=-1

1

−280

3 α=10-7, λ=0.9
2.8

Reward

−320 −360

2.6

E(wopt)

α=2x10-8, λ=0.999

0 −400

0 2 4 6 8 10 12 14 16 d

1

5

10

15

20

d

(a)

(b)

(c)

Figure 1: (a) An MRP with a set of states 𝑆 = {0, 1, . . . , 99}. From state 𝑖 ∈ 𝑆, there is a transition with probability 0.9−0.8𝑖/99 to state (𝑖 +1) mod 99; otherwise the agent stays in state 𝑖. All rewards are 0, except for a 1-reward when starting from state 99. The MRP

uses a discount factor of 0.99. (b) The lower panel shows 𝜇; the upper panel shows 𝑉 as well as its best linear approximation.

The linear architecture has a single parameter 𝑤: for state 𝑖 ∈ 𝑆, 𝑉 (𝑖) is approximated as 𝑤𝑖. (c) Value estimation error as a

function

of

𝑑

after

100,

000

steps

of

TD (𝜆),
𝑑

with

𝑤0

picked

uniformly

at

random

from

[−1,

1].

Each

plot

specifies

the

values

of

𝛼

and

𝜆;

the

optimal

error

is

also

shown.

Since

TD (𝜆)
𝑑

makes

only

( 1/𝑑 )

the

number

of

updates

of

TD(𝜆),

we

compensate

by

running it with learning rate 𝛼𝑑. Results are averages from 1000 random runs; standard errors are negligible.

inertia quantifies the cost of a single repetition:

Δ𝑀 =def

max

(𝑄★

(𝑠,

𝑎)

−

★
𝑄

(𝑠,

𝑎2)).

𝑠 ∈𝑆,𝑎 ∈𝐴 𝑀

𝑀

Δ𝑀 is a local, horizon-independent property, which we expect to be

small in many families of tasks. As a concrete illustration, consider

the family of deterministic MDPs that have “reversible” actions. A calculation in Appendix A 1 shows that Δ𝑀 for any such MDP 𝑀 is at most 4𝑅max—which is a horizon-independent upper bound.
To further aid our understanding of the price of inertia Δ𝑀 , we devise a “pitted grid world” task, shown in Figure 2a. This task allows for us to control Δ𝑀 and examine its effect on performance as 𝑑 is varied. The task is a typical grid world with cells, walls,

and a goal state to be reached. Episodes begin from a start state

chosen uniformly at random from a designated set. The agent can

select “up”, “down”, “left”, and “right” as actions. A selected action

is retained with probability 0.85, otherwise switched to one picked

uniformly at random, and thereafter implemented to have the corresponding effect. There is a reward of −1 at each time step, except

when reaching special “pit” states, which incur a larger penalty. It
is precisely by controlling the pit penalty that we control Δ𝑀 . The task is undiscounted. Figure 2a shows optimal policies for 𝑑 = 1 and 𝑑 = 3 (that is, on 𝑀3); observe that they differ especially in the vicinity of pits (which are harder to avoid with 𝑑 = 3).

5.2 Value deficit of action-repetition

Naturally, the constraint of having to repeat actions 𝑑 > 1 times
may limit the maximum possible long-term value attainable. We upper-bound the resulting deficit as a function of Δ𝑀 and 𝑑. For MDP 𝑀, note that 𝑉 ★ is the optimal value function.2
𝑀
Lemma 2. For 𝑑 ≥ 1, ∥𝑉𝑀★ − 𝑉𝑀★𝑑 ∥∞ ≤ Δ𝑀 (1−1𝛾−)𝛾(𝑑1−−1𝛾𝑑 ) .

Proof. For 𝑚 ≥ 2 and 𝑛 ≥ 1, define the terms 𝐺𝑚 =def

and 𝐻𝑚,𝑛 =def

𝑛−1 𝑚𝑖
𝑖=0 𝛾 . First we prove

𝑚−2 𝑖
𝑖=0 𝛾

𝑄𝑀★ (𝑠, 𝑎𝑑 ) ≥ 𝑄𝑀★ (𝑠, 𝑎) − Δ𝑀𝐺𝑑 (1)

for 𝑠 ∈ 𝑆, 𝑎 ∈ 𝐴, 𝑑 ≥ 2. The result is trivial for 𝑑 = 2. Assuming it is true for 𝑑 ≤ 𝑚, we get

★
𝑄

(𝑠, 𝑎𝑚+1)

=

𝑅(𝑠, 𝑎)

+𝛾

∑︁

𝑇

(𝑠, 𝑎, 𝑠 ′)𝑄★

(𝑠 ′, 𝑎𝑚)

𝑀

𝑀

𝑠′ ∈𝑆

≥ 𝑅(𝑠, 𝑎) + 𝛾 ∑︁ 𝑇 (𝑠, 𝑎, 𝑠 ′){𝑄𝑀★ (𝑠 ′, 𝑎) − Δ𝑀𝐺𝑚 }

𝑠′ ∈𝑆

=

★
𝑄

(𝑠,

𝑎2)

−

𝛾

Δ𝑀 𝐺𝑚

𝑀

≥ 𝑄𝑀★ (𝑠, 𝑎) − Δ𝑀 − 𝛾 Δ𝑀𝐺𝑚 = 𝑄𝑀★ (𝑠, 𝑎) − Δ𝑀𝐺𝑚+1.

In effect, (1) bounds the loss from persisting action 𝑎 for 𝑑 steps,

which we incorporate in the long-term loss from action-repetition.

To do so, we consider a policy 𝜋 : 𝑆 → {𝑎𝑑, 𝑎 ∈ 𝐴} that takes the

same atomic actions as 𝜋★ , but persists them for 𝑑 steps. In other
𝑀

words,

for

𝑠

∈

𝑆,

𝜋 (𝑠)

=

𝑑
𝑎

⇐⇒

★
𝜋

(𝑠 )

=

𝑎.

For

𝑗

≥

1,

let 𝑈 𝑗 (𝑠)

𝑀

denote the expected long-term reward accrued from state 𝑠 ∈ 𝑆 by

1
Appendices are provided in the supplementary material.
2
In our forthcoming analysis, we treat value and action value functions as vectors, with ∥ · ∥∞ denoting the max norm.

||V*M - V*Md||∞

(a)

2000 1500 1000

∆M=2.13 ∆M=10.12 ∆M=55.25

500

0 0 2 4 6 8 10 12 14 16

-300

d

-320

-340

-360

-380

-400 0 2 4 6 8 10 12 14 16

d

(b)

(c)

Episodic Reward

Figure 2: (a) Left: the “Pitted Grid World” task (specified in text), using Δ𝑀 = 10.12. Right: Optimal policies when constrained to use frame-skip 𝑑 = 1 (black/dark arrows) and 𝑑 = 3 (golden/light) arrows (only one type is shown for states

in which both policies give the same action). (b) The up-

per

panel

shows

||𝑉 ★

−

★
𝑉

||∞, as a function of d for Δ𝑀

∈

𝑀 𝑀𝑑

{2.13, 10.12, 55.26}. The lower panel shows the episodic re-

ward obtained after 6000 episodes of Q-learning with aliased

states, again as a function of 𝑑. (c) Full learning curves for Q-learning (for Δ𝑀 = 2.13), showing policy performance (evaluated separately for 50 episodes) at regular intervals. In

this and subsequent experimental plots, error bars show one

standard error, based on 5 or more independent runs.

taking the first 𝑗 decisions based on 𝜋 (that is, applying 𝜋 for 𝑗𝑑

time steps), and then acting optimally (with no action-repetition,

according

to

★
𝜋

).

We

prove

by

induction,

for

𝑠

∈

𝑆:

𝑀

𝑈 𝑗 (𝑠) ≥ 𝑉 ★ (𝑠) − Δ𝑀𝐺𝑑 𝐻𝑑,𝑗 .

(2)

𝑀

For base case, we apply (1) and get

𝑈1 (𝑠) = 𝑄𝑀★ (𝑠, (𝜋𝑀★ (𝑠))𝑑 ) ≥ 𝑄𝑀★ (𝑠, 𝜋𝑀★ (𝑠)) − Δ𝑀𝐺𝑑 = 𝑉 ★ (𝑠) − Δ𝑀𝐺𝑑 𝐻𝑑,1.
𝑀

Assuming the result true for 𝑗, and again using (1), we establish it for 𝑗 + 1.

𝑈 𝑗+1 (𝑠)

=𝑅

(𝑠,

★
𝜋

(𝑠 ) )

+

𝑑
𝛾

∑︁ 𝑇

(𝑠, 𝜋★ (𝑠), 𝑠 ′)𝑈

(𝑠 ′)

𝑑𝑀

𝑑𝑀

𝑗

𝑠′ ∈𝑆

≥𝑅

(𝑠,

★
𝜋

(𝑠 ) )

+

𝑑
𝛾

∑︁ 𝑇

(𝑠,

★
𝜋

(𝑠 ),

𝑠

′) {𝑉

★

(𝑠

′)

−

Δ

𝐺𝐻

}

𝑑𝑀

𝑑𝑀

𝑀

𝑀 𝑑 𝑑,𝑗

𝑠′ ∈𝑆

= 𝑄𝑀★ (𝑠, (𝜋𝑀★ (𝑠))𝑑 ) − 𝛾𝑑 Δ𝑀𝐺𝑑 𝐻𝑑,𝑗

≥ 𝑄𝑀★ (𝑠, 𝜋𝑀★ (𝑠)) − Δ𝑀𝐺𝑑 − 𝛾𝑑 Δ𝑀𝐺𝑑 𝐻𝑑,𝑗 = 𝑉 ★ (𝑠) − Δ𝑀𝐺𝑑 𝐻𝑑,𝑗+1.
𝑀

Observe that lim𝑗→∞ 𝑈 𝑗 (𝑠)

=

𝜋
𝑉

(𝑠): the value of 𝑠 when 𝜋

is

𝑀𝑑

executed in 𝑀

. The result follows by using 𝑉 𝜋

(𝑠 )

≤

★
𝑉

(𝑠), and

𝑑

𝑀

𝑀

𝑑

𝑑

substituting for 𝐺𝑑 and 𝐻𝑑,∞.

□

The upper bound in the lemma can be generalised to action value
functions, and also shown to be tight. Proofs of the following results
are given in appendices B and C.
Corollary 3. For 𝑑 ≥ 1, ∥𝑄𝑀★ − 𝑄𝑀★𝑑 ∥∞ ≤ Δ𝑀 (1−1𝛾−)𝛾(𝑑1−−1𝛾𝑑 ) . Proposition 4. For every Δ > 0, 𝑑 ≥ 2 , and 𝛾 ∈ [0, 1), there exists an MDP 𝑀 with Δ𝑀 = Δ and discount factor 𝛾 such that ∥𝑉𝑀★ − 𝑉𝑀★𝑑 ∥∞ = ∥𝑄𝑀★ − 𝑄𝑀★𝑑 ∥∞ = Δ𝑀 (1−1𝛾−)𝛾(𝑑1−−1𝛾𝑑 ) .
The matching lower bound in Proposition 4 arises from a carefully-
designed MDP; in practice we expect to encounter tasks 𝑀 for which the upper bound on ∥𝑉 ★ − 𝑉 ★ ∥∞ is loose. Although our analysis
𝑀 𝑀𝑑
is for infinite discounted reward, we expect Δ𝑀 to play a similar role on undiscounted episodic tasks such as the pitted grid world.
Figure 2b shows computed values of the performance drop from
action-repetition, which monotonically increases with 𝑑 for every Δ𝑀 value. Even so, the analysis to follow shows that using 𝑑 > 1 might yet be the most effective if behaviour is learned.

5.3 Analysis of control with action-repetition
We now proceed to our main result: that the deficit induced by 𝑑 can be offset by the benefit it brings in the form of a shorter task horizon. Since standard control algorithms such as Q-learning and Sarsa may not even converge with function approximation, we sidestep the actual process used to update weights. All we assume is that (1) the learning process produces as its output 𝑄ˆ, an approximate action value function, and (2) as is the common practice, the recommended policy 𝜋ˆ is greedy with respect to 𝑄ˆ: that is, for 𝑠 ∈ 𝑆, 𝜋ˆ (𝑠) = argmax𝑎∈𝐴 𝑄ˆ (𝑠, 𝑎). We show that on an MDP 𝑀 for which Δ𝑀 is small, it could in aggregate be beneficial to execute 𝜋ˆ with frame-skip 𝑑 > 1; for clarity let us denote the resulting policy 𝜋ˆ : 𝑆 → {𝑎𝑑, 𝑎 ∈ 𝐴}. The result holds regardless of whether 𝑄ˆ was
𝑑
itself learned with or without frame-skipping, although in practice, we invariably find it more effective to use the same frame-skip parameter 𝑑 for both learning and evaluation.
Singh and Yee [32] provide a collection of upper bounds on the performance loss from acting greedily with respect to an approximate value function or action value function. The lemma below is not explicitly derived in their analysis; we furnish an independent proof in Appendix D.

Lemma 5. For MDP 𝑀 = (𝑆, 𝐴,𝑇 , 𝑅, 𝛾), let 𝑄ˆ : 𝑆 × 𝐴 → R be an

𝜖

-approximation

of

★
𝑄

.

In

other

words,

∥𝑄★

− 𝑄ˆ ∥∞

≤

𝜖. Let 𝜋ˆ

be

𝑀

𝑀

greedy with respect to 𝑄ˆ. We have: ∥𝑉𝑀★ − 𝑉𝑀𝜋ˆ ∥∞ ≤ 12−𝜖𝛾𝛾 .

The implication of the lemma is that the performance loss due to

a

prediction

error

scales

as

𝜃

(

𝛾 1−𝛾

).

Informally,

1 1−𝛾

may be viewed

as the effective task horizon. Now observe that if a policy is im-

𝑑
plemented with frame-skip 𝑑 > 1, the loss only scales as 𝜃 ( 𝛾 ),
1−𝛾𝑑

which can be substantially smaller. However, the performance loss

defined in Lemma 5 is with respect to optimal values in the under-

lying

MDP,

which

is

𝑀
𝑑

(rather

than

𝑀)

when

action-repetition

is performed with 𝑑 > 1. Fortunately, we already have an upper

bound on ∥𝑉 ★ − 𝑉 ★ ∥∞ from Lemma 2, which we can add to the
𝑀 𝑀𝑑
one from Lemma 5 to meaningfully compare 𝜋ˆ : 𝑆 → {𝑎𝑑, 𝑎 ∈ 𝐴}
𝑑
with 𝜋★ . Doing so, we obtain our main result.
𝑀

Theorem 6. Fix MDP 𝑀 = (𝑆, 𝐴, 𝑅,𝑇 , 𝛾), and 𝑑 ≥ 1. Assume that

a learning algorithm returns action-value function 𝑄ˆ : 𝑆 × 𝐴 → R.

Let

𝜋ˆ
𝑑

:

𝑆

→

{𝑎𝑑, 𝑎

∈

𝐴}

be

greedy

with

respect

to 𝑄ˆ.

There

exist

constants 𝐶1 (𝛾, 𝑑) and 𝐶2 (𝑀, 𝑄ˆ) such that

𝑑

∥𝑉 ★ − 𝑉 𝜋ˆ𝑑 ∥∞ ≤ Δ 𝐶1 (𝛾, 𝑑) + 𝛾 𝐶2 (𝑀, 𝑄ˆ),

𝑀𝑀

𝑀

1 − 𝛾𝑑

with the dependencies of 𝐶1 and 𝐶2 shown explicitly in parentheses.

Proof. By the triangle inequality,

∥𝑉 ★ − 𝑉 𝜋ˆ𝑑 ∥∞ ≤ ∥𝑉 ★ − 𝑉 ★ ∥∞ + ∥𝑉 ★ − 𝑉 𝜋ˆ𝑑 ∥∞.

𝑀𝑀

𝑀 𝑀𝑑

𝑀𝑑 𝑀

Lemma 2 upper-bounds the first RHS term by Δ𝑀𝐶3 (𝛾, 𝑑), where 𝐶3 (𝛾, 𝑑) = (1−1𝛾−)𝛾(𝑑1−−1𝛾𝑑 ) . Observe that the second RHS term may be

𝑑
written as ∥𝑉𝑀★𝑑 − 𝑉𝑀𝜋ˆ𝑑 ∥∞, which Lemma 5 upper-bounds by 12−𝜖𝛾𝛾𝑑 ,

where

𝑄ˆ

is

an

𝜖 -approximation

of

★
𝑄

. In turn, 𝜖 can be replaced

𝑀𝑑

by ∥𝑄★ − 𝑄ˆ ∥∞, which is itself upper-bounded using the triangle
𝑀𝑑

inequality by ∥𝑄★

−

★
𝑄

∥∞

+

∥𝑄★

− 𝑄ˆ ∥∞.

Corollary

3

upper-

𝑀𝑑

𝑀

𝑀

bounds ∥𝑄★ − 𝑄★ ∥∞ by Δ𝑀𝐶3 (𝛾, 𝑑). As for ∥𝑄★ − 𝑄ˆ ∥∞, observe

𝑀 𝑀𝑑

𝑀

that it only depends on 𝑀 and 𝑄ˆ. In aggregate, we have

∥𝑉 ★ − 𝑉 𝜋ˆ𝑑 ∥∞ ≤ ∥𝑉 ★ − 𝑉 ★ ∥∞ + ∥𝑉 ★ − 𝑉 𝜋ˆ𝑑 ∥∞

𝑀𝑀

𝑀 𝑀𝑑

𝑀𝑑 𝑀

2

Δ𝑀𝐶3 (𝛾, 𝑑) + ∥𝑄★

− 𝑄ˆ ∥∞

𝑑
𝛾

𝑀

≤ Δ𝑀𝐶3 (𝛾, 𝑑) +

1 − 𝛾𝑑

𝑑

𝛾

= Δ𝑀𝐶1 (𝛾, 𝑑) +

𝐶2 (𝑀, 𝑄ˆ)

1 − 𝛾𝑑

for appropriately defined 𝐶1 (𝛾, 𝑑) and 𝐶2 (𝑀, 𝑄ˆ).

□

While the first term in the bound increases with 𝑑, the second term decreases on account of the shortening horizon. The overall bound is likely to be minimised by intermediate values of 𝑑 especially when the price of inertia (Δ𝑀 ) is small and the approximation error (∥𝑄★ − 𝑄ˆ ∥∞) large. We observe exactly this trend in
𝑀
the pitted grid world environment when we have an agent learn using Q-learning (with 0.05-greedy exploration and a geometricallyannealed learning rate). As a crude form of function approximation, we constrain (randomly chosen) pairs of neighbouring states to

share the same Q-values. Observe from figures 2b (lower panel) and 2c that indeed the best results are achieved when 𝑑 > 1.

6 EMPIRICAL EVALUATION
The pitted grid world was an ideal task to validate our theoretical results, since it allowed us to control the price of inertia and to benchmark learned behaviour against optimal values. In this section, we evaluate action-repetition on more realistic tasks, wherein the evaluation is completely empirical. Our experiments test methodological variations and demonstrate the need for action-repetition for learning in a new, challenging task.

6.1 Acrobot

We begin with Acrobot, the classic control task consisting of two

links and two joints (shown in Figure 3a). The goal is to move the

tip of the lower link above a given height, in the shortest time pos-

sible. Three actions are available at each step: leftward, rightward,

and zero torque. Our experiments use the OpenAI Gym [33] im-

plementation of Acrobot, which takes 5 actions per second. States are represented as a tuple of six features: cos(𝜃1), sin(𝜃1), cos(𝜃2), sin(𝜃2), 𝜃1, and 𝜃2, where 𝜃1 and 𝜃2 are the link angles. The start

state in every episode is set up around the stable point: 𝜃1, 𝜃1, 𝜃2, and 𝜃2 are sampled uniformly at random from [−0.1, 0.1]. A re-

ward of -1 is given each time step, and 0 at termination. Although

Acrobot is episodic and undiscounted, we expect that as with the

pitted grid world, the essence of Theorem 6 will still apply. Note

that with control at 5 Hz, Acrobot episodes can last up to 500 steps

when actions are selected uniformly at random.

We execute Sarsa (𝜆), a straightforward generalisation of TD (𝜆)

𝑑

𝑑

to the control setting, using 1-dimensional tile coding [24, see Sec-

tion 12.7]. Tuning other parameters to optimise results for 𝑑 = 1,

we set 𝜆 = 0.9, 𝛼 = 0.1, and an initial exploration rate 𝜖 = 0.1,

decayed by a factor of 0.999 after each episode. Figure 3b shows

learning curves for different 𝑑 values. At 8,000 episodes, the best

results

are

for 𝑑

=

3;

in

fact

Sarsa (𝜆)
𝑑

with 𝑑

up

to

5

dominates

Sarsa(𝜆). It appears that Acrobot does not need control at 5 Hz;

action-repetition shortens the task horizon and enhances learning.

Frame-skipping versus reducing discount factor. If the key

contribution of 𝑑 to successful learning is the reduction in horizon

from

1/(1

−

𝛾)

to

1/(1

−

𝑑
𝛾

),

a

natural

idea

is

to

artificially

reduce

the task’s discount factor 𝛾, even without action-repetition. Indeed

this approach has been found effective in conjunction with approxi-

mate value iteration [25] and model-learning [34]. Figure 3c shows

the values of policies learned by Sarsa (𝜆) after 8, 000 episodes of
𝑑
training, when the discount factor 𝛾 (originally 1) is reduced. Other

parameters are as before. As expected, some values of 𝛾 < 1 do improve learning. Setting 𝛾 = 0.99 helps the agent finish the task

in 95.6 steps: an improvement of 11.8 steps over regular Sarsa(𝜆). However, the configuration of 𝛾 = 1, 𝑑 = 5 performs even better—

implying that on this task, 𝑑 is more effective to tune than 𝛾. Al-

though decreasing 𝛾 and increasing 𝑑 both have the effect of shrink-

ing the horizon, the former has the consequence of revising the

very definition of long-term reward. As apparent from Proposi-

tion 1, 𝑑 entails no such change. That tuning these parameters in conjunction yields the best results (at 𝑑 = 3, 𝛾 = 0.999) prompts

future work to investigate their interaction. Interestingly, we find no benefit from using 𝛾 < 1 on the pitted grid world task.
Action-repetition in policy gradient methods. Noting that some of the recent successes of frame-skipping are on policy gradient methods [8], we run reinforce [35] on Acrobot using actionrepetition. Our controller computes an output for each action as a linear combination of the inputs, thereafter applying soft-max action-selection. The 21 resulting weights 𝑤 (including biases) are updated by gradient ascent to optimise the episodic reward 𝐽 (𝑤), using the Adam optimiser with initial learning rate 0.01. We set 𝛾 to 0.99. Figure 3d shows that yet again, performance is optimised at 𝑑 = 3. Note that our implementation of reinforce performs baseline subtraction, which reduces the variance of the gradient estimate and improves results for 𝑑 = 1. Even so, an empirical

(a)

(b)

𝛾

𝑑 =1

𝑑 =2

𝑑 =3

𝑑 =5

0.9 −123.1(0.9) −346.6(1.8) −310.5(1.7) −270.4(2.4)

0.98 −104.8(0.5) −79.7(1.8) −92.9(6.2) −104.6(11.5)

0.99 −94.6(3.6) −75.5(0.7) −74.1(0.7) −82.3(1.2)

0.999 −95.8(3.8) −76(0.8) −72.7(0.8) −81.4(0.8)

1 −106.4(3) −76(0.7) −74.5(1.2) −80(0.2)

(c)

(d)

(e)

Figure 3: (a) Screenshot of Acrobot. (b) Learning curves for Sarsa (𝜆) with different frame-skip values 𝑑. (c) Episodic re-
𝑑
ward (and one standard error) obtained by Sarsa (𝜆) after
𝑑
8, 000 episodes of training with different 𝑑 and 𝛾 combinations. (d) Learning curves for reinforce with different 𝑑 values. (e) For different 𝑑, an empirical estimate of the aggregate variance of ∇𝑤 𝐽 (𝑤) for 𝑤 found after 5, 000 episodes of training. For each seed, the policy found after 5,000 episodes of reinforce is frozen and run for 100 transitions, each giving a sample gradient. The y axis shows the trace of the resulting covariance matrix.

Fraction Captured

0.30 0.25 0.20 0.15 0.10
0

10000 Nu2m0b0e0r0of E3p0i0s0o0des 40000 50000

Sarsa Sarsa (d = 16) Sarsa (d = 32) Sarsa (d = 48) FiGAR-Sarsa Random (d=128) Random (d=1) Helios

(a)

(b)

(c)

Figure 4: (a) Screenshot of 2v2 HFO. (b) Learning curves; “Random” and helios are static policies. (c) The learning curve for a “meta” learner that uses the EXP3.1 algorithm to switch between different values of 𝑑 (a single value is used within each episode). The inset shows the number of episodes (averaged from 10 independent runs) that each value of 𝑑 is invoked.

plot of the variance (Figure 3e) shows that it falls further as 𝑑 is increased, with a relatively steep drop around 𝑑 = 3. As yet, we do

not have an analytical explanation of this behaviour. Although

known upper bounds on the variance of policy gradients [36]

have a quadratic dependence on the task horizon, which is de-

creased by 𝑑 from 1/(1 − 𝛾) to 1/(1 − 𝛾𝑑 ), they are also quadratic

in the maximum reward, which is increased by 𝑑 from 𝑅max to

𝑅max (1

+

𝛾

+

2
𝛾

+

·

·

·

+

𝛾 𝑑 −1 )

=

𝑅max (1

−

𝛾𝑑 )/(1

−

𝛾)

.

We

leave

it

to future work to explain the empirical observation of a significant

reduction of the policy gradient variance with 𝑑 on Acrobot.

6.2 Action-repetition in new, complex domain
Before wrapping up, we share our experience of implementing action-repetition in a new, relatively complex domain. We expect practitioners to confront similar design choices in other tasks, too.
The Half Field Offense (HFO) environment [9] models a game of soccer in which an offense team aims to score against a defense team, when playing on one half of a soccer field (Figure 4a). While previous investigations in this domain have predominantly focused on learning successful offense behaviour, we address the task of learning defense. Our rationale is that successful defense must anyway have extended sequences of actions such as approaching the ball and marking a player. Note that in 2 versus 2 (2v2) HFO, the average number of decisions made in an episode is roughly 8 for offense, and 100 for defense. We implement four high-level defense actions: mark_player, reduce_angle_to_goal, go_to_ball, and defend_goal. The continuous state space is represented by 16 features such as distances and positions [9]. Episodes yield a single reward at the end: 1 for no goal and 0 for goal. No discounting is used. As before, we run Sarsa (𝜆) with 1-dimensional tile coding.
𝑑
In the 2v2 scenario, we train one defense agent, while using builtin agents for the goalkeeper and offense. Consistent with earlier studies [4, 15], we observe that action-repetition assists in exploration. With 𝑑 = 1, random action-selection succeeds on only 10% of the episodes; the success rate increases with 𝑑, reaching 15% for 𝑑 = 128. Figure 4b shows learning curves: points are shown after every 5,000 training episodes, obtained by evaluating learned policies

for 2,000 episodes. All algorithms use 𝛼 = 0.1, 𝜖 = 0.01, 𝜆 = 0.8 (optimised for Sarsa1(𝜆) at 50,000 episodes). Action-repetition shows a dramatic effect on Sarsa, which only registers a modest improvement over random behaviour with 𝑑 = 1, but with 𝑑 = 32, even outperforms a defender from the helios team [37] that won the RoboCup competition in 2010 and 2012.

Optimising 𝑑. A natural question arising from our observations is

whether we can tune 𝑑 “on-line”, based on the agent’s experience.

We obtain mixed results from investigating this possibility. In one

approach, we augment the atomic set of actions with extended

sequences; in another we impose a penalty on the agent every

time it switches actions. Neither of these approaches yields any

appreciable benefit. The one technique that does show a rising

learning curve, included in Figure 4b, is FiGAR-Sarsa, under which we associate both action and 𝑑 (picked from {1, 2, 4, 8, 16, 32, 64})

with state, and update each 𝑄-value independently. However, at

50,000 episodes of training, this method still trails Sarsa (𝜆) with
𝑑
(static) 𝑑 = 32 by a significant margin.

Observe that the methods described above all allow the agent to adapt 𝑑 within each learning episode. On the other hand, the reported successes of tuning 𝑑 on Atari games [5, 8] are based on

policy gradient methods, in which a fixed policy is executed in each

episode (and updated between episodes). In line with this approach,

we design an outer loop that treats each value of 𝑑 (from a finite set)

as

an

arm

of

a

multi-armed

bandit.

A

full

episode,

with

Sarsa
𝑑

(𝜆)

updates using the corresponding, fixed frame-skip 𝑑 is played out on

every pull. The state of each arm is saved between its pulls (but no

data is shared between arms). Since we cannot make the standard

“stochastic” assumption here, we use the EXP3.1 algorithm [38],

which maximises expected payoff in the adversarial setting. Under

EXP3.1, arms are sampled according to a probability distribution,

which gets updated whenever an arm is sampled. Figure 4c shows

a learning curve corresponding to this meta-algorithm (based on a moving average of 500 episodes); we set 𝜆 = 0.9375 for the best

overall results. It is apparent from the curve and affirmed by the inset that Exp3.1 is quick to identify 𝑑 = 31 as the best among the

given choices (𝑑 = 81 and 𝑑 = 151 are also picked many times due to their quick convergence, even if to suboptimal performance).
7 CONCLUSION
In this paper, we analyse frame-skipping a, simple approach that has recently shown much promise in applications of RL, and is especially relevant as technology continues to drive up frame rates and clock speeds. In the prediction setting, we establish that frameskipping retains the consistency of prediction. In the control setting, we provide both theoretical and empirical justification for actionrepetition, which applies the principle that tasks anyway having gradual changes of state can benefit from a shortening of the horizon. Indeed action-repetition allows TD learning to succeed on the defense variant of HFO, a hitherto less-studied aspect of the game. Although we are able to automatically tune the frame-skip parameter 𝑑 using an outer loop, it would be interesting to examine how the same can be achieved within each episode.

REFERENCES

[1] Martin L. Puterman. Markov Decision Processes. Wiley, 1994.

[2]

Richard

Bellman.

Dynamic

Programming.

Princeton

University

Press,

𝑠𝑡
1

edition,

1957.

[3] Alex Braylan, Mark Hollenbeck, Elliot Meyerson, and Risto Miikkulainen. Frame skip is a powerful parameter for learning to play Atari. In Proc. 2015 AAAI Workshop on Learning for General Competency in Video Games, pages 10–11. AAAI Press, 2015.

[4] Ishan P. Durugkar, Clemens Rosenbaum, Stefan Dernbach, and Sridhar Mahade-

van. Deep reinforcement learning with macro-actions. Preprint available at

https://arxiv.org/pdf/1606.04615.pdf, 2016.

[5] Aravind S. Lakshminarayanan, Sahil Sharma, and Balaraman Ravindran. Dynamic action repetition for deep reinforcement learning. In Proc. AAAI 2017, pages 2133–2139. AAAI Press, 2017.

[6] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res., 47:253–279, 2013.
[7] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,

Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg

Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen

King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529–533, 2015.

[8] Sahil Sharma, Aravind S. Lakshminarayanan, and Balaraman Ravindran. Learning to repeat: Fine grained action repetition for deep reinforcement learning. In Proc. ICLR 2017. OpenReview.net, 2017.
[9] Matthew Hausknecht, Prannoy Mupparaju, Sandeep Subramanian, Shivaram

Kalyanakrishnan, and Peter Stone. Half field offense: An environment for multiagent learning and ad hoc teamwork. In Adaptive and Learning Agents (ALA) Workshop at AAMAS 2016, 2016. Proceedings available at http://ala2016.csc.liv.ac.uk/ALA2016_Proceedings.pdf.

[10] Eric A. Hansen, Andrew G. Barto, and Shlomo Zilberstein. Reinforcement learning
for mixed open-loop and closed-loop control. In Advances in Neural Information Processing Systems 9. MIT Press, 1997. [11] Ming Tan. Cost-sensitive reinforcement learning for adaptive classification and
control. In Proc. AAAI 1991, pages 774–780. AAAI Press, 1991. [12] Andrew Kachites McCallum. Reinforcement Learning with Selective Perception
and Hidden State. PhD thesis, Department of Computer Science, U. Rochester, 1996.

[13] Kenneth M. Buckland and Peter D. Lawrence. Transition point dynamic programming. In Advances in Neural Information Processing Systems 6, pages 639–646. Morgan Kaufmann, 1994.
[14] Kenneth M. Buckland. Optimal Control of Dynamic Systems through the Reinforcement Learning of Transition Points. PhD thesis, Department of Electrical Engineering, U. British Columbia, April 1994.

[15] Amy McGovern, Richard S. Sutton, and Andrew H. Fagg. Roles of macro-
actions in accelerating reinforcement learning. In Proc. 1997 Grace Hopper Celebration of Women in Computing, pages 13–18, 1997. Preprint available at http://www.mcgovern-fagg.org/amy/pubs/mcgovern_ghc97.pdf.
[16] Jette Randløv. Learning macro-actions in reinforcement learning. In Advances in Neural Information Processing Systems 11, pages 1045–1051. MIT Press, 1999.

[17] Lihong Li, Thomas J. Walsh, and Michael L. Littman. Towards a unified theory of state abstraction for MDPs. In Proc. ISAIM 2006, 2006. Available at http://anytime.cs.umass.edu/aimath06/proceedings/P21.pdf.
[18] Valdinei Freire Da Silva and Anna Helena Reali Costa. Compulsory Flow Q-
Learning: an RL algorithm for robot navigation based on partial-policy and macro-states. Journal of the Brazilian Computer Society, 15(3):65–75, 2009. [19] Richard Dazeley, Peter Vamplew, and Adam Bignold. Coarse Q-Learning: Ad-
dressing the convergence problem when quantizing continuous state variables.
[20] David Abel, Dilip Arumugam, Lucas Lehnert, and Michael L. Littman. State abstractions for lifelong reinforcement learning. In Proc. ICML 2018, pages 10–19. PMLR, 2018.
[21] Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semiMDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181–211, 1999.
[22] George Konidaris. Constructing abstraction hierarchies using a skill-symbol loop. In Proc. IJCAI 2016, pages 1648–1654. IJCAI/AAAI Press, 2016.
[23] Steven J. Bradtke and Michael O. Duff. Reinforcement learning methods for continuous-time Markov decision problems. In Advances in Neural Information Processing Systems 7, pages 393–400. MIT Press, 1995.
[24] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2nd edition, 2018.
[25] Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a lower discount factor. In Advances in Neural Information Processing Systems 21, pages 1265–1272. Curran Associates, 2009.
[26] Christopher J. C. H. Watkins and Peter Dayan. Q-Learning. Machine learning, 8(3-4):279–292, 1992.
[27] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems.
Technical Report CUED/F-INFENG/TR 166, Cambridge University Engineering
Department, 1994.
[28] John N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.
[29] Michael Kearns and Satinder Singh. Bias-variance error bounds for temporal difference updates. In Proc. COLT 2000, pages 142–147. Morgan Kaufmann, 2000.
[30] Harm van Seijen, Ashique Rupam Mahmood, Patrick M. Pilarski, Marlos C. Machado, and Richard S. Sutton. True online temporal-difference learning. J. Mach. Learn. Res., 17:145:1–145:40, 2016.
[31] Aravind S Lakshminarayanan, Sahil Sharma, and Balaraman Ravindran. Dynamic frame skip deep q network. arXiv preprint arXiv:1605.05365, 2016.
[32] Satinder P. Singh and Richard C. Yee. An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16(3):227–233, 1994.
[33] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-
man, Jie Tang, and Wojciech Zaremba. OpenAI Gym. 2016. Available at
https://arxiv.org/pdf/1606.01540.pdf.
[34] Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning horizon on model accuracy. In Proc. AAMAS 2015, pages 1181–1189. IFAAMAS, 2015.
[35] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992.
[36] Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of policy gradient estimation. In Advances in Neural Information Processing Systems 24, pages 262–270. Curran Associates, 2011.
[37] Hidehisa Akiyama and Tomoharu Nakashima. HELIOS2012: RoboCup 2012 soccer simulation 2D league champion. In RoboCup 2012: Robot Soccer World Cup XVI, pages 13–19. Springer, 2013.
[38] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The Nonstochastic Multiarmed Bandit Problem. SIAM journal on computing, 32(1):48– 77, 2002.

A PRICE OF INERTIA FOR DETERMINISTIC
MDPS WITH REVERSIBLE TRANSITIONS
Consider a deterministic MDP 𝑀 in which transitions can be “reversed”: in other words, for 𝑠, 𝑠 ′ ∈ 𝑆, 𝑎 ∈ 𝐴, if taking 𝑎 from 𝑠 leads to 𝑠 ′, then there exists an action 𝑎𝑐 such that taking 𝑎𝑐 from 𝑠 ′ leads to 𝑠. Now suppose action 𝑎 carries the agent from 𝑠 to 𝑠 ′, and thereafter from 𝑠 ′ to 𝑠 ′′. We have:

★
𝑄

(𝑠,

𝑎)

−

★
𝑄

(𝑠,

𝑎2)

𝑀

𝑀

=

𝛾𝑉

★

(𝑠

′)

−

★
𝛾𝑄

(𝑠

′,

𝑎)

𝑀

𝑀

= 𝛾𝑉 ★ (𝑠 ′) − 𝛾 {𝑅(𝑠 ′, 𝑎, ) + 𝛾𝑉 ★ (𝑠 ′′)}

𝑀

𝑀

≤

𝛾𝑉

★

(𝑠

′)

−

𝛾

{𝑅

(𝑠

′,

𝑎)

+

★
𝛾𝑄

(𝑠

′′,

𝑎c)}

𝑀

𝑀

=

𝛾𝑉

★

(𝑠

′)

−

𝛾

{𝑅

(𝑠

′,

𝑎)

+

𝛾

(𝑅

(𝑠

′′,

𝑐
𝑎

)

+

𝛾𝑉

★

(𝑠

′))}

𝑀

𝑀

=

𝛾𝑉

★

(𝑠

′)

(1

−

𝛾

2)

−

𝛾𝑅

(𝑠

′,

𝑎)

−

𝛾

2
𝑅

(𝑠

′′,

𝑐
𝑎

)

𝑀

≤ 𝛾 𝑅max (1 − 𝛾2) + 𝛾𝑅 + 𝛾 2𝑅

1−𝛾

max

max

= 2𝛾 (1 + 𝛾)𝑅max.

Since 𝛾 ≤ 1, it follows that Δ𝑀 (𝑠, 𝑎) = max𝑠 ∈𝑆,𝑎∈𝐴 (𝑄★ (𝑠, 𝑎) −
𝑀

★
𝑄

(𝑠,

𝑎2))

is

at

most

4𝑅max.

𝑀

B PROOF OF PROPOSITION 3
The following bound holds for all 𝑠 ∈ 𝑆, 𝑎 ∈ 𝐴. The first “≥” step follows from Lemma 2 and the second such step is based on an application of (1).

★
𝑄

(𝑠, 𝑎) = 𝑅

∑︁

(𝑠,

𝑎)

+

𝑑
𝛾

𝑇

(𝑠, 𝑎, 𝑠 ′)𝑉 ★

(𝑠 ′)

𝑀

𝑑

𝑑

𝑀

𝑑

𝑑

𝑠′ ∈𝑆

≥

𝑅𝑑 (𝑠, 𝑎)

+ 𝛾𝑑

∑︁ 𝑇

(𝑠, 𝑎, 𝑠 ′)𝑉 ★ (𝑠 ′)

𝑑

− 𝛾𝑑 Δ𝑀𝐺𝑑 𝐻𝑑,∞

𝑀

𝑠′ ∈𝑆

= 𝑄𝑀★ (𝑠, 𝑎𝑑 ) − 𝛾𝑑 Δ𝑀𝐺𝑑 𝐻𝑑,∞

≥ 𝑄𝑀★ (𝑠, 𝑎) − Δ𝑀𝐺𝑑 − 𝛾𝑑 Δ𝑀𝐺𝑑 𝐻𝑑,∞

1 − 𝛾𝑑−1

=

★
𝑄

(𝑠,

𝑎)

−

Δ𝑀

.

𝑀

(1 − 𝛾)(1 − 𝛾𝑑)

C PROOF OF PROPOSITION 4
The figure below shows an MDP 𝑀 with states 1, 2, . . . , 𝑑, and actions stay (dashed) and move (solid). All transitions are deterministic, and shown by arrows labeled with rewards. The positive reward 𝑥 is set to Δ/𝛾.

It

can

be

verified

that

Δ𝑀

=

★
𝑄

(1, move)

− 𝑄★

(1, move2)

=

Δ,

𝑀

𝑀

and also that

∥𝑉 ★ − 𝑉 ★ ∥∞ = 𝑉 ★ (1) − 𝑉 ★ (1)

𝑀 𝑀𝑑

𝑀

𝑀𝑑

=

★
𝑄

(1,

1,

move)

−

★
𝑄

(1, move)

𝑀

𝑀𝑑

= ∥𝑄𝑀★ − 𝑄𝑀★𝑑 ∥∞

1 − 𝛾𝑑−1

= Δ𝑀

,

(1 − 𝛾)(1 − 𝛾𝑑)

which matches the upper bound in Lemma 2.

D PROOF OF LEMMA 5
We furnish the relatively simple proof below, while noting that
many similar results (upper-bounds on the loss from greedy action
selection) are provided by Singh and Yee [32]. For 𝑠 ∈ 𝑆 and 𝑗 ≥ 0, let 𝑈 𝑗 (𝑠) denote the expected long-term
discounted reward obtained by starting at 𝑠, following 𝜋ˆ for 𝑗 steps, and thereafter following an optimal policy 𝜋★. Our induction hypothesis is that 𝑈 𝑗 (𝑠) ≥ 𝑉𝑀★ (𝑠) − 2𝜖 𝑘𝑗 =1 𝛾𝑘 . As base case, it is clear that 𝑈0 (𝑠) = 𝑉 ★ (𝑠). Assuming the induction hypothesis to
𝑀
be true for 𝑗, we prove it for 𝑗 + 1. We use the fact that 𝑄ˆ is an 𝜖-approximation of 𝑄★ , and also that 𝜋ˆ is greedy with respect to
𝑀
𝑄ˆ. For 𝑠 ∈ 𝑆,

𝑈 𝑗+1 (𝑠) = 𝑅(𝑠, 𝜋ˆ (𝑠)) + 𝛾 ∑︁𝑇 (𝑠, 𝜋ˆ (𝑠), 𝑠 ′)𝑈 𝑗 (𝑠 ′)
𝑠′

𝑗

≥

𝑅

(𝑠,

𝜋ˆ

(𝑠 ) )

+

𝛾

∑︁ 𝑇

(𝑠,

𝜋ˆ

(𝑠 ),

𝑠

′)𝑉

★

(𝑠

′)

−

2𝜖

∑︁

𝛾 𝑘 +1

𝑀

𝑠′

𝑘 =1

𝑗

≥

𝑅

(𝑠,

𝜋ˆ

(𝑠 ) )

+

𝛾

∑︁ 𝑇

(𝑠,

𝜋ˆ

(𝑠 ),

𝑠

′)𝑄★

(𝑠

′,

𝜋ˆ

(𝑠

′))

−

2𝜖

∑︁

𝛾 𝑘 +1

𝑀

𝑠′

𝑘 =1

𝑗

≥

𝑅 (𝑠,

𝜋ˆ

(𝑠 ) )

+

𝛾

∑︁ 𝑇

(𝑠,

𝜋ˆ

(𝑠 ),

𝑠

′)𝑄ˆ

(𝑠

′,

𝜋ˆ

(𝑠

′))

−

𝛾𝜖

−

2𝜖

∑︁

𝛾 𝑘 +1

𝑠′

𝑘 =1

𝑗

≥

𝑅 (𝑠,

𝜋 ★ (𝑠 ) )

+

𝛾

∑︁ 𝑇

(𝑠,

𝜋 ★ (𝑠 ),

𝑠

′)𝑄ˆ

(𝑠

′,

𝜋 ★ (𝑠

′))

−

𝛾𝜖

−

2𝜖

∑︁

𝛾 𝑘 +1

𝑠′

𝑘 =1

𝑗

≥

𝑅 (𝑠,

𝜋 ★ (𝑠 ) )

+

𝛾

∑︁ 𝑇

(𝑠,

𝜋 ★ (𝑠 ),

𝑠

′)𝑄★

(𝑠

′,

𝜋 ★ (𝑠

′))

−

2𝛾𝜖

−

2𝜖

∑︁

𝛾 𝑘 +1

𝑀

𝑠′

𝑘 =1

𝑗 +1

∑︁

= 𝑉 ★ (𝑠) − 2𝜖

𝑘
𝛾.

𝑀

𝑘 =1

Since lim𝑗→∞ 𝑈 𝑗 (𝑠) = 𝑉𝑀𝜋ˆ (𝑠), we have 𝑉𝑀𝜋ˆ (𝑠) ≥ 𝑉𝑀★ (𝑠) − 12−𝜖𝛾𝛾 .

0

𝑥

𝑥

𝑥

1

2

3

...

𝑑

𝑥

0

0

0

0

