ECRM: Efﬁcient Fault Tolerance for Recommendation Model Training via Erasure Coding

Kaige Liu∗† Facebook
kaigeliu98@gmail.com

Jack Kosaian† Carnegie Mellon University
jkosaian@cs.cmu.edu

K. V. Rashmi Carnegie Mellon University
rvinayak@cs.cmu.edu

arXiv:2104.01981v1 [cs.LG] 5 Apr 2021

Abstract
Deep-learning-based recommendation models (DLRMs) are widely deployed to serve personalized content to users. DLRMs are large in size due to their use of large embedding tables, and are trained by distributing the model across the memory of tens or hundreds of servers. Server failures are common in such large distributed systems and must be mitigated to enable training to progress. Checkpointing is the primary approach used for fault tolerance in these systems, but incurs signiﬁcant training-time overhead both during normal operation and when recovering from failures. As these overheads increase with DLRM size, checkpointing is slated to become an even larger overhead for future DLRMs, which are expected to grow in size. This calls for rethinking fault tolerance in DLRM training.
We present ECRM, a DLRM training system that achieves efﬁcient fault tolerance using erasure coding. ECRM chooses which DLRM parameters to encode, correctly and efﬁciently updates parities, and enables training to proceed without any pauses, while maintaining consistency of the recovered parameters. We implement ECRM atop XDL, an open-source, industrial-scale DLRM training system. Compared to checkpointing, ECRM reduces training-time overhead for large DLRMs by up to 88%, recovers from failures up to 10.3× faster, and allows training to proceed during recovery. These results show the promise of erasure coding in imparting efﬁcient fault tolerance to training current and future DLRMs.
1 Introduction
Deep-learning-based recommendation models (DLRMs) are key tools in serving personalized content to users at Internet scale [7, 28]. As the value generated by DLRMs often relies on the ability to reﬂect recent data, production DLRMs are frequently retrained [2]. Reducing DLRM training time is thus critical to maintaining an accurate and up-to-date model.
∗Work done while at Carnegie Mellon University †Equal contribution

DLRMs consist of embedding tables and neural networks. Embedding tables map sparse categorical features (e.g., properties of a client) to a learned dense representation. Embedding tables resemble lookup tables in which millions or billions [13, 17] of sparse features each map to a small dense vector representation of tens to hundreds of ﬂoats. We refer to a single dense embedding vector corresponding to a single sparse feature as an “embedding table entry,” or “entry” for short. A small neural network processes dense vectors resulting from embedding table “lookups” to produce a ﬁnal prediction (e.g., whether a client like a video).
Embedding tables are typically large, ranging from hundreds of gigabytes to terabytes in size [17]. Such large models are trained in a distributed fashion across tens/hundreds of nodes [5, 17], as depicted (at a much smaller scale) in Figure 1a. Embedding tables and neural network parameters are sharded across a set of servers and kept in memory for fast access. Workers operate in a data-parallel fashion to perform neural network training by accessing model parameters from servers, send gradients to servers to update parameters via an optimizer (e.g., Adam).
Since model parameters are stored in memory, any server failure requires training to restart from scratch. Given that DLRM training is resource and time intensive and that failures are common in large-scale settings, it is imperative for DLRM training to be fault tolerant [24]. In this work, we focus on imparting fault tolerance against server failures. Handling server failures is critical, as failure of a single server results in loss of fraction of embedding table entries. In contrast additional fault tolerance is not needed for handling worker failures, as each worker contains a replica of the DLRM’s neural network, which can simply be recovered from another worker.
Checkpointing is the main approach used for fault tolerance in DLRM training [24]. This involves periodically pausing training and writing the current parameters and optimizer state to stable storage, such as a distributed ﬁle system. If a failure occurs, the entire system resets to the most recent checkpoint and restarts training from that point. While sim-

1

Server 0
Shard 0 e0
update Optimizer
∇0

Server 1
Shard 1 e1
update Optimizer
∇1

Server 2
Shard 2 e2
Optimizer

Server 0
Shard 0 e0
update Optimizer
∇0

Server 1
Shard 1 e1
update Optimizer
∇1

Server 2
Shard 2 e2
Optimizer
∇0

Server 3
“Parity Shard” p = e0 + e1 + e2
updates Optimizer
∇1

Worker 0

Worker 1

Worker 0

Worker 1

(a) Example of the distributed setup used to train DLRMs.

(b) Naive erasure-coded DLRM with k = 3 and r = 1.

Figure 1: Example of (a) normal and (b) erasure-coded DLRM training with embedding table entries e0, e1, and e2, and gradients ∇0 and ∇1.

ple, checkpointing frequently pauses training to save DLRM state and has to redo work after failure. Thus, checkpointing has been shown to add signiﬁcant overhead to training production DLRMs [24]. Even more concerning, this overhead increases with DLRM size. Given the trend of increasing model size [40], checkpointing is slated to incur even larger overhead for future DLRMs.
An alternative to checkpointing that does not require stalls or lengthy recovery is to replicate DLRM parameters on separate servers. However, replication requires at least 2× as much memory as a checkpointing-based system, which is impractical given the large sizes of DLRMs. Another alternative is to reduce the overhead of checkpointing by taking approximate checkpoints [6, 12, 24, 31]. However, this can result in accuracy loss in training, which makes debugging production systems harder due to uncertainty in the accuracy of the model recovered from the checkpoint. Furthermore, even small drops in accuracy have been noted to result in a signiﬁcant reduction in the business value generated by DLRMs [12, 44], making potential accuracy loss induced by an approach to fault tolerance undesirable.
An ideal approach to fault-tolerant DLRM training would (1) operate with low training-time overhead, with (2) low memory overhead, while (3) not introducing potential accuracy loss (and the associated uncertainty). Designing such a fault tolerance approach is the goal of this paper.
Erasure codes are coding-theoretic tools for adding proactive redundancy (like replication) but with signiﬁcantly less memory overhead, which have been widely employed in storage and communication systems (e.g., [30, 34]). Like replication and traditional checkpointing, erasure coding would not alter the accuracy of training. Due to their low memory overhead, erasure codes offer potential for efﬁcient fault tolerance in DLRM training. As shown in Figure 1b, an erasure-coded DLRM training system would construct “parity parameters” by encoding k embedding table entries from separate servers. In this example, a parity p is formed from parameters e0, e1, and e2 via the encoding function p = e0 + e1 + e2, and placed on a separate server. If a server fails, lost parameters are recovered by reading the k available parameters and performing the erasure code’s decoding process (e.g., e1 = p − e0 − e2).

While erasure codes appear promising for imparting fault tolerance to DLRM training, this vision presents a number of challenges: (1) Parities must be kept up-to-date with DLRM parameters to ensure correct recovery. This requires additional communication and computation, which can reduce the throughput of training. (2) As will be shown in §3.3.1, correctly updating parities when using optimizers that store internal state (e.g., Adagrad, Adam) is challenging without incurring large memory overhead. (3) An erasure code’s recovery process can be resource intensive [33, 35]. This can potentially lead to long recovery times during which training is stalled.
We present ECRM,1 an erasure-coded DLRM training system that overcomes these challenges through a careful system design adapting simple erasure codes and ideas from storage systems to DLRM training. ECRM reduces training-time overhead and circumvents the difﬁculty of maintaining correctness with stateful optimizers (Challenges 1 and 2) by delegating the responsibility for updating parities to servers, rather than workers, via an approach we call “difference propagation.” ECRM recovers quickly from failure (Challenge 3) by enabling training to continue during the erasure code’s recovery process. The net result of ECRM’s design is a DLRM training system that recovers quickly from failures with low trainingtime and memory overhead, without requiring pauses during normal operation or recovery, and without any changing the accuracy guarantees of the underlying training system.
We implement ECRM atop XDL, an open-source, industrial-scale DLRM training system developed by Alibaba [17]. We evaluate ECRM in training variants of the Criteo DLRM in MLPerf [25] across 20 nodes. ECRM recovers from failures signiﬁcantly faster than checkpointing and with lower training-time overhead. ECRM’s beneﬁts improve for larger DLRMs, showing promise for current and future DLRMs. For example, ECRM reduces training-time overhead for a large DLRM by up to 88% compared to checkpointing (from 33.4% to 4%). Furthermore, ECRM recovers from failure up to 10.3× faster than the average case for checkpointing, and enables training to continue during recovery with only a 6–12% drop in throughput, while checkpointing pauses
1ECRM: Erasure-Coded Recommendation Model

2

training during recovery. ECRM’s beneﬁts come at the cost of additional memory requirements and load on the training cluster. However, ECRM keeps memory overhead to only a fractional amount and balances additional load evenly among servers. These results show the promise of erasure coding to enable efﬁcient fault tolerance in DLRM training.
2 Challenges in fault-tolerant DLRM training
We next describe DLRM training systems, the insufﬁciency of current approaches to fault-tolerant DLRM training, and opportunity to impart more efﬁcient fault tolerance in DLRM training.
2.1 DLRM training systems
As described in §1, DLRMs are large in size due to embedding tables that span hundreds of gigabytes to terabytes in size, and DLRM training is typically distributed across a set of servers and workers (see Figure 1a). Model parameters are sharded across server memory. Workers read embedding table entries, perform a forward and backward pass over a neural network to generate gradients (for both neural network parameters and embedding table entries), and send gradients back to the servers hosting the entries. An optimizer (e.g., Adam) on each server uses gradients received from workers to update parameters. Each training sample typically accesses only a few embedding table entries, but all neural network parameters. Thus, embedding table entries are updated sparsely, while neural network parameters are updated on every training sample. Finally, many systems use asynchronous training when training DLRMs (e.g., Facebook and Alibaba [5, 17]). We thus focus on asynchronous training systems in this work, but describe in §4.4 how the techniques we propose could apply to synchronous training as well.
Many popular optimizers use per-parameter state in updating parameters (e.g., Adam [9], Adagrad [10], momentum SGD). We refer to such optimizers as “stateful optimizers.” For example, Adagrad tracks the sum of squared gradients for each parameter over time and uses this when updating the parameter. Per-parameter optimizer state is kept in memory alongside model parameters on servers and is updated when the corresponding parameter is updated. As per-parameter state grows with DLRM size, such optimizer state for embedding tables can consume a large amount of memory.
2.2 Checkpointing and its downsides
Given the large number of nodes on which DLRMs are trained, failures are common [24]. Due to the time it takes to train such models, it is critical that DLRM training be made fault tolerant.
We focus on imparting fault tolerance against server failures. Handling server failures is critical, as failure of a single

Write checkpoint Read checkpoint

Ckpt every 30 min. Ckpt every 60 min.

Time (minutes) Increase in training time (%)

10
5
0 44 88 176
Embedding table size per server (GB)

30 20 10
0 44 88 176
Embedding table size per server (GB)

(a) Checkpoint write/read time (b) Training-time overhead
Figure 2: Time required to read and write checkpoints and overhead of checkpointing on training time with varying embedding table size.

server results in loss of fraction of embedding table entries. In contrast additional fault tolerance is not needed for handling worker failures, as each worker contains a replica of the DLRM’s neural network, which can simply be recovered from another worker.
Checkpointing is the primary approach used for fault tolerance in DLRM training [24]. Under checkpointing, training is periodically paused and DLRM parameters and optimizer state are writen to stable storage (e.g., a distributed ﬁle system). Upon failure, the most recent checkpoint is read from stable storage, and the entire system restarts training from this checkpoint, redoing any training iterations that occurred between the most recent checkpoint and the failure.
Recently, Facebook reported that overheads from checkpointing account for, on average, 12% of DLRM training time, and that these overheads add up to over 1000 machine-years of computation [24].
We next describe two primary time penalties that make up the overhead of checkpointing on training time of DLRMs, and illustrate their impact with varying size of the embedding table and the checkpointing interval.
1. Time penalty during normal operation. Writing checkpoints to stable storage is a slow process given the large sizes of embedding tables and optimizer state, and training is paused during this time so that the saved model is consistent. Intuitively, the overhead of checkpointing on normal operation increases the more frequently checkpoints are taken and the longer it takes to write a checkpoint (and thus the larger the DLRM).
To illustrate this overhead, we evaluate checkpointing DLRMs in XDL. Training is performed on a cluster of 15 workers and 5 servers, with checkpoints periodically written to an HDFS cluster. We train the DLRM used for the Criteo Terabyte dataset in MLPerf, which requires 220 GB of memory for embedding tables (44 GB per server). We additionally evaluate with increased DLRM size by increasing the size of embedding tables. The full setup used for this evaluation is described in §4.1.
Figure 2a shows that the time overhead for writing checkpoints is signiﬁcant (on the order of minutes) and increases

3

with increasing embedding table size per server. Figure 2b shows the overhead of checkpointing on training in the absence of failures with two checkpointing periods: 30 and 60 minutes. We measure the time it takes for a each setup to reach the same number of iterations that a system with no fault tolerance (and thus no overhead) reaches in four hours. As expected, training time increases both with increased DLRM size and with decreased time between checkpoints.
2. Time penalty during recovery. Upon failure, a system using checkpointing must roll back the DLRM to the state of the most recent checkpoint by reading it from stable storage, and redo all training iterations that occurred between this checkpoint and the failure. New training iterations are paused during this time. The time needed to read checkpoints from storage can be signiﬁcant [24] and grows with DLRM size. The expected time to redo iterations grows with the time between checkpoints: if checkpoints are written every T time units, this time will be 0 at best (failing just after writing a checkpoint), T at worst (failing just before writing a checkpoint), and T2 on average.
Takeaway. Checkpointing suffers a fundamental tradeoff between training-time overhead in normal operation and when recovering from failure. Increasing the time between checkpoints reduces the fraction of time paused when saving checkpoints, but increases the expected work to be redone in recovery. Furthermore, these overheads increase with model size. Given the trend of increasing model size [40] checkpointing is slated to become an even larger overhead in training future DLRMs. This calls for alternate approaches to fault tolerance in DLRM training.
2.3 Reducing overhead via approximation?
Several recent approaches aim to reduce checkpointing overhead by taking approximate checkpoints or via approximate recovery [6,12,24,31]. However, in the event of a failure, such techniques roll back an approximation of the true DLRM, which can potentially alter convergence and ﬁnal accuracy. Given the signiﬁcant business value generated by DLRMs, prior works [12, 44] have noted that even small drops in DLRM accuracy must be avoided. Furthermore, our personal conversations with multiple practitioners working on largescale DLRM training indicate that this potential accuracy drop introduces a source of uncertainty that makes debugging production systems difﬁcult, and thus is less desirable. Hence, ideally, one would like to reduce the overhead of checkpointing without compromising accuracy.
Another approach to reducing the overhead of checkpointing is to asynchronously write checkpoints while training progresses by writing updates to stable storage as they are generated [4]. This is feasible only if writing to stable storage can keep pace with the rate at which gradients are generated. As DLRM training systems have many workers generating gradients asynchronously, gradients are generated at a high

rate that stable storage cannot keep pace with. In fact, if storage could keep pace, then embedding tables could be kept in stable storage, rather than in memory. Thus, asynchronous checkpointing is not a viable alternative for fault-tolerant DLRM training.
2.4 Fault tolerance via in-memory redundancy?
An alternative to checkpointing is to provision extra memory in the system to redundantly store DLRM parameters and optimizer state in memory in a fault-tolerant manner.
Replication. The simplest and most common approach to redundancy is replication, in which a system proactively provisions redundant servers that contain copies of the DLRM parameters that are kept up-to-date throughout training and that can immediately take over for failed servers. Replicated DLRM training would use twice as much memory to store copies of each DLRM parameter on two servers. Gradients for a given parameter are sent to and applied on both servers holding copies. The system seamlessly continues training if a single server fails by accessing the replica, avoiding the need of checkpointing-based approaches to redo work after a failure. Similar to traditional checkpointing-based approaches, replicated DLRM training would preserve the accuracy guarantees of the underlying training system. However, a replicated DLRM training system requires at least twice as much memory as a non-replicated one. Given the large sizes of embedding tables and optimizer state, this memory overhead is impractical.
Erasure codes. Erasure codes are coding-theoretic tools that enable redundancy with low overhead. They have been used for imparting resilience against unavailability in storage and communication systems with signiﬁcantly less overhead than replication [30, 34, 37]. An erasure code encodes k data units to generate r redundant “parity units” such that any k out of the total (k + r) data and parity units sufﬁce for a decoder to recover the original k data units. Erasure codes operate with overhead of k+k r , which is less than that of replication by setting r < k. These properties have led to wide adoption of erasure codes in storage, communication, and caching systems [30, 32, 34, 37, 39].
For example, consider an erasure-coded storage system in which data units x1, x2, and x3 are stored on three separate disks, and that the system must tolerate one disk failure. An erasure code with parameters k = 3 and r = 1 would do so by encoding a parity unit as p = x1 + x2 + x3 and storing this parity unit on a fourth disk. Suppose the disk holding x2 fails. The system recovers x2 using the erasure code’s subtraction decoder: x2 = p − x1 − x3. This setup can recover from any one of the four disks failing by using only one extra disk, while replication would require three extra disks to impart the same level of fault tolerance.
Figure 1b shows an example of how erasure codes might

4

potentially be used in DLRM training to reduce the memory overhead of in-memory redundancy in DLRM training. However, there are several challenges in using erasure codes for DLRM training, which we discuss and address in the remainder of the paper.
Takeaway. An ideal approach to fault-tolerant DLRM training would have (1) low-latency recovery, (2) low memory overhead, (3) no potential for accuracy loss. Erasure codes offer promising potential for achieving these goals. However, there are several challenges in using erasure codes for DLRM training. We describe these challenges in detail and how they can be overcome in the next section.
3 ECRM: erasure-coded DLRM training
We propose ECRM, a system that imparts efﬁcient fault tolerance to DLRM training via careful design adapting simple erasure codes and ideas from storage systems. ECRM makes the same accuracy guarantees as the underlying training system, unlike the approximate approaches described in §2.3.
Using erasure codes in DLRM training raises unique challenges compared to the traditional use of erasure codes in storage and communication systems. We ﬁrst provide a highlevel overview of ECRM and then introduce these challenges and how ECRM overcomes them.
3.1 Overview of ECRM
Figure 3 shows the high-level operation of ECRM. ECRM encodes DLRM parameters using an erasure code and distributes parities throughout the cluster before training begins. Groups of k parameters from separate servers are encoded to produce r parities that are placed in memory on separate servers. ECRM thus uses k+k r -times as much memory as the original system. We describe in §3.2 which DLRM parameters ECRM encodes and how parities are placed in the cluster. As DLRM parameters are updated during training, ECRM updates the corresponding parities. When a server fails, ECRM uses the erasure code’s decoder to reconstruct lost parameters.
While the use of erasure codes in DLRM training is enticing, it presents many challenges and design decisions: (1) Which parameters should be encoded and where should parities be placed (§3.2)? (2) How can parities be updated correctly and efﬁciently (§3.3)? (3) How can ECRM avoid pausing training during recovery (§3.4)? (4) How can ECRM guarantee the consistency of the DLRM recovered after failure (§3.5)? We next describe how ECRM addresses these challenges.
3.2 Encoding and placing parity parameters
We next describe which parameters of a DLRM are encoded in ECRM and where parities are placed.

Server 0
p0 = e0 + e1 + e2 e3 e6 e9
Optimizer

Server 1
e0 p1 = e3 + e4 + e5
e7 e10
update
Optimizer

e10

∇10

Server 2
e1 e4 p2 = e6 + e7 + e8 e11 entry diff optimizer state diff
Optimizer

Server 3
e2 e5 e8 p3 = e9 + e10 + e11
Optimizer

Worker 0

Worker 1

Figure 3: Example of ECRM with k = 3, r = 1.

Which parameters should be encoded? Fault tolerance is primarily needed in DLRM training to recover failed servers, which hold DLRM parameters and optimizer state. If a server fails, the portion of the DLRM hosted on that server is lost, and training cannot proceed. In contrast, systems with architectures as described in §2.1 are naturally tolerant of worker failures, as the system can continue training with fewer workers while replacements are provisioned (albeit, at lower training throughput).
As each worker pulls all neural network parameters from servers when training, the neural network in the DLRM is naturally replicated on workers. If a server fails, the neural network parameters it held can be recovered from a worker.2
In contrast, embedding tables and optimizer state are not naturally replicated. Embedding tables and optimizer state are sharded across servers, and each worker reads only a few entries each training iteration. Thus, lost embedding table entries and optimizer state cannot be recovered from workers. Furthermore, replicating embedding tables and optimizer state is impractical, given their large size.
Thus, ECRM encodes only embedding tables and optimizer state, while neural network parameters need not be encoded.
Where should parities be placed? Recall from §2.1 that embedding tables and optimizer state are sharded across servers. ECRM encodes groups of k embedding table entries from different shards to produce a “parity entry,” and places the parity entry on a separate server. Optimizer state is similarly encoded to form “parity optimizer state,” which is placed on the same server hosting the corresponding parity entry.
Parities in ECRM are updated whenever any of the k corresponding embedding table entries are updated. Hence, parities are updated more frequently than the original entries, and must be placed carefully within the cluster so as not to introduce load imbalance among servers. ECRM uses rotating parity placement to distribute parities among servers, resulting in an equal number of parities per server. An example of this is shown in Figure 3 with k = 3: each server is chosen to host a parity in a rotating fashion, and the entries used to
2While asynchronous training does not guarantee that all workers have up-to-date neural network parameters, the neural network recovered from a worker is equivalent to one observable under asynchronous training.

5

encode the parity are hosted on the 3 other servers. This approach is inspired by parity placement in RAID-5 hard-disk systems [30].
Encoder and decoder. We focus on using erasure codes with parameter r = 1 (i.e., one parity per k entries, and recovering from a single failure). We focus on this setting because it represents the most common failure scenario experienced by a cluster in datacenters [33]. We describe in §4.4 how ECRM can be easily adapted to handle concurrent failures with r > 1. Within this setting of r = 1, ECRM uses the simple summation encoder shown in Figure 3, and the corresponding subtraction decoder. For example, with k = 3, embedding table entries e0, e1, and e2 are encoded to form parity p = e0 + e1 + e2. If the server holding e1 fails, e1 will be recovered as e1 = p − e0 − e2.

3.3 Correctly and efﬁciently updating parities
We next describe challenges in correctly and efﬁciently updating parities, and how ECRM overcomes them.

3.3.1 Challenges in keeping up-to-date parities

Maintaining correctness with stateful optimizers. The naive approach to erasure-coded DLRM training shown in Figure 1b suffers a fundamental challenge in correctly updating parity entries when using a stateful optimizer.
Consider the example in Figure 1b when using the Adagrad optimizer. Let ei,t denote the value of embedding table entry ei after t updates, and ∇i,t denote the gradient for ei,t . The update performed by Adagrad for e0,t with gradient ∇0,t is:

e0,t+1 = e0,t −

α ∇0,t
G0,t + ε

where α is a constant learning rate, G0,t = ∇20,0 + ∇20,1 + . . . + ∇20,t is the sum of squares of the previous gradients for parameter e0, and ε is a small constant. G0,t , which we call e0’s “accumulator,” is an example of per-parameter optimizer state.
As described in §3.1, ECRM maintains one “parity optimizer parameter” for every k original optimizer parameters. In the example above, using the encoder described in §3.1, a “parity accumulator” would be Gp = G0 + G1 + G2. This is easily kept up-to-date by adding to the parity accumulator the squared gradients for updates to each of the k original embedding table entries. However, using such a parity accumulator to update the corresponding parity entry based on ∇0,t would result in an incorrect parity entry, as G0,t = Gp,t .
The issue illustrated in the example above arises for any stateful optimizer, such as Adagrad, Adam, and momentum SGD. Given the popularity of such optimizers, ECRM must employ some means of maintaining correct parities when using stateful optimizers. This issue could be overcome by keeping replicas the k original optimizer parameters on the

server hosting the parity. However, as described in §3.1, optimizer state is large and grows with embedding tables, so such replication is impractical.
Maintaining low overhead. Even if the issue above were not present, the approach to erasure-coded DLRM training shown in Figure 1b can have high training-time overhead. Under this approach, updating parities requires gradients for an entry to be sent both to the server hosting the entry and to the server hosting the parity, and that the optimizer be applied on both servers. This results in network and compute overhead for workers. Given that workers are typically the bottleneck in DLRM training [17], ECRM must minimize this overhead.
Fundamental limitation underlying the challenges. The challenges described above stem from sending gradients directly to the servers hosting parities, an approach we call “gradient propagation.” Under gradient propagation, workers must send duplicate gradients, resulting in CPU and network overhead on workers. Servers holding parity entries receive only the gradient for the original embedding table entry and must correctly update the parity entry and optimizer state. As described above, performing these updates correctly given only gradients and parity optimizer state is challenging.
3.3.2 Difference propagation
To overcome these challenges, ECRM introduces difference propagation. As shown in Figure 3, under difference propagation, workers send gradients only to the servers holding embedding table entries for that gradient. After applying the optimizer to embedding table entries and updating optimizer state, the server asynchronously sends the differences in entry and optimizer state to the server holding the corresponding parity entry. The receiving server adds these differences to the parity entry and optimizer state.
Difference propagation has two key beneﬁts over gradient propagation. (1) By sending differences to servers, rather than gradients, difference propagation updates parity entries correctly when using stateful optimizers. (2) Difference propagation adds no overhead to workers, which are often the bottleneck in DLRM training [17].
While difference propagation does introduce network and CPU overhead on servers for sending and applying differences, §4 will show that it signiﬁcantly outperforms gradient propagation. As an additional note, the differences sent by difference propagation can be dense (e.g., due to momentum used by the optimizer), and thus less amenable to sparsitybased compression than gradients. However, DLRM training clusters typically have high-bandwidth networks (e.g., 100 Gbps [5]), for which compression has been shown to provide little beneﬁt [43].

6

3.4 Pause-free recovery from failure
We next describe how ECRM recovers from failure without requiring training to pause.
Challenges in erasure-coded recovery. Due to the property of the erasure codes described in §2.4 that any k out of the (k + 1) original and parity units sufﬁce to recover the original k units, ECRM can continue training even if a single server fails. For example, a worker in ECRM could read entry e1 in Figure 3 even if Server 2 fails by reading e0, e2, and p, and decoding e1 = p − e0 − e2. Such read operations that require decoding are referred to as “degraded reads” in erasure-coded storage systems.
Despite the ability to perform degraded reads, ECRM must still fully recover failed servers to remain tolerant of future failures. However, prior work on erasure-coded storage has shown that full recovery can be time-intensive [33, 35]. Full recovery in ECRM requires decoding all embedding table entries and optimizer state held by the failed server. This consumes signiﬁcant network bandwidth in transferring available entries for decoding, and server CPU in performing decoding. Given the large sizes embedding table entries and optimizer state, completing full recovery before resuming training can signiﬁcantly pause training.
Training during recovery in ECRM. Rather than solely performing degraded reads after a failure or pausing until full recovery is complete, ECRM enables training to continue while full recovery takes place. Upon failure, ECRM begins full recovery of lost embedding table entries and optimizer state. In the meantime, the system continues to perform new training iterations, with workers performing degraded reads to access entries from the failed server.
ECRM must avoid updating an embedding table entry in parallel with its use for recovery. If the recovery process reads the new value of the entry, but the old value of the parity entry (e.g., because the update has not yet reached the parity), then the recovered entry will be incorrect. ECRM uses granular locking to avoid such race conditions. The recovery process “locks” a fraction of the lost entries that it will decode. While this lock is held, all updates to entries that will be used in recovery for the locked chunk are buffered in memory on servers. Workers reading an updated, but locked entry do so by reading from the buffer. When a lock is released, all buffered updates are applied to the embedding tables, and the next chunk is locked. The number of entries covered by each lock introduces a tradeoff between time overhead in switching locks and server memory overhead for buffering that can be navigated based on the requirements of a given system.
3.5 Consistency of recovered DLRM
ECRM provides the same guarantees regarding the consistency of a recovered DLRM as the general asynchronous system it builds atop.

Consistency of each parameter. ECRM ensures that each embedding table entry and optimizer parameter is recovered to the value corresponding to its most recent update that was applied both to the original entry and the parity. One case that requires care: if recovery is triggered after an update has been applied to an embedding table entry but before it is applied to the corresponding parity entry, the decoded entry will be incorrect. ECRM avoids this scenario by ensuring that all in-ﬂight updates are completed before recovery begins.
Consistency across parameters. ECRM ensures that the recovered parameters, taken as a whole, represent a DLRM that could have been achieved by asynchronous training. ECRM cannot guarantee that the recovered DLRM represents an exact state observed in training. However, we show in §A of the appendix that, when such scenarios occur, the recovered DLRM is equivalent to one that could have occurred during asynchronous training. Thus, ECRM does not introduce additional inconsistency atop general asynchronous training.
3.6 Tradeoffs in ECRM
ECRM encodes k embedding table entries into a single parity entry (r = 1) (similarly for optimizer state). Parameter k results in the following tradeoffs in ECRM, some of which differ from those in the traditional use of erasure codes:
Increasing k decreases memory overhead and fault tolerance. As ECRM encodes one parity entry for every k embedding table entries (and similarly for optimizer state), ECRM requires less memory for storing parities with increased k. However, since the erasure codes employed by ECRM can recover from any one out of (k + 1) failures, increasing k decreases the fraction of failed servers ECRM can tolerate.
Increasing k does not change load during normal operation. As each embedding table entry in ECRM is encoded to produce a single parity entry, each update applied to an entry is also be applied to one parity. Thus, the total increase in load due to ECRM is 2×, regardless of the value of k. In addition to this constant load increase, we show in §4.3 that ECRM balances this load evenly with various values of k.
Increasing k increases the time to fully recover. Recovery in ECRM requires reading k available entries from separate servers and decoding. Thus, the network trafﬁc and computation used during recovery increases with k, which increases the time to fully recover a failed server. However, as described in §3.4, ECRM allows training to continue during this time.
4 Evaluation
We next evaluate the performance of ECRM. The highlights of the evaluation are as follows:

7

• ECRM recovers from failure up to 10.3× faster than the average recovery time for checkpointing.
• ECRM enables training to proceed during recovery with only a 6%–12% drop in throughput, whereas checkpointing requires training to completely pause.
• ECRM reduces training-time overhead by up to 88% compared to checkpointing (from 33.4% to 4%) on large DLRMs.
• While ECRM introduces additional load for updating parities, the impact of this increased load on training throughput is alleviated by improved cluster load balance.
• ECRM operates with low training-time overhead in a variety of cluster conﬁgurations and recovers quickly from failure with varying lock granularity (see §3.4).
4.1 Evaluation setup
We implement ECRM in C++ on XDL, an open-source DLRM training system from Alibaba [17].
Dataset. We evaluate with the Criteo Terabyte dataset [1]. We randomly draw one day of samples from the dataset by picking each sample with probability 214 in one pass through the dataset, and use this subset in evaluation to reduce storage requirements. This random sampling results in a sampled dataset that mimics the full dataset.
Models. We use the DLRM for the Criteo dataset from MLPerf [28], which has 13 embedding tables, for a total of nearly 200M entries each with 128 dense features. We use SGD with momentum as the optimizer, which add one ﬂoating point value of optimizer state per parameter. Any other optimizer can also be used. The total size of the embedding tables and optimizer state is 220 GB. The DLRM uses a sevenlayer multilayer perceptron with 128–1024 features per layer as a neural network [3].
We evaluate DLRMs of different size by varying embedding table size in two ways: (1) Increasing the number of entries (i.e., sparse dimension). This increases the memory required per server and the amount of data that must be checkpointed/coded and recovered. (2) Increasing the size of each entry (i.e., dense dimension). This increases the memory required per server, the amount of data that must be checkpointed/coded, the network bandwidth in transferring entries/gradients, the work performed by neural networks, and the work done by servers in updating entries. We consider three variants of the DLRM: (1) Criteo-Original, the original Criteo DLRM described above, (2) Criteo-2S, which has 2× the embedding table entries (i.e., 2× sparse dimension), and (3) Criteo-2S-2D, which has 2× the number of entries and with each entry being 2× as large (i.e., 2× sparse and dense dimensions). For Criteo-2S-2D, the input layer of the neural network is modiﬁed to accommodate the larger entry size. These variants have size 220, 440, and 880 GB, respectively. The two larger DLRMs reﬂect performance for future

Throughput (samples / sec)

Ckpt-30 best ECRM (k = 2)
·105 4
2
0 10
·109 1.5
1 0.5
0 10

Ckpt-30 average ECRM (k = 4)

Ckpt-30 worst

20

30

40

50

60

20

30

40

50

60

Time (minutes)

Sample Number

Figure 4: Training throughput (top) and progress (bottom) when recovering from failure at 10 minutes.

DLRMs, which are expected to grow in size [40]
Coding parameters and baselines. We evaluate ECRM with k of 2, 4, and 10, which have 50%, 25%, and 10% memory overhead, respectively. We evaluate with k = 10 in a limited set of experiments due to the cost of the larger cluster needed. We use one lock during recovery by default (see §3.4), but also evaluate other locking granularities.
We compare ECRM to taking checkpoints to HDFS every 30 minutes (Ckpt-30) and every 60 minutes (Ckpt-60). Checkpointing to HDFS is representative of production DLRM training environments, which leverage HDFS-like distributed ﬁle systems [5]. Furthermore, the checkpointing baselines we use have competitive performance: we ﬁnd that checkpointing via HDFS is only 7%–27% slower than a (purposely unrealistic) baseline of writing directly to a local SSD. In addition, for the Criteo-Original DLRM, which is representative of current DLRMs, the checkpoint-writing overhead we report in §4.3 is similar to that reported in production training jobs by Facebook [24]. We compare ECRM only to approaches to fault tolerance that, like ECRM, maintain the same accuracy guarantees as the underlying training system.
Cluster setup. We evaluate on AWS with 5 servers of type r5n.8xlarge, each with 32 vCPUs, 256 GB of memory, and 25 Gbps network bandwidth (r5n.12xlarge is used for Criteo2S-2D due to memory requirements). We use 15 workers of type p3.2xlarge, each with a V100 GPU, 8 vCPUs, and 10 Gbps of network bandwidth. This ratio of worker to server nodes is inspired by XDL [17]. Workers use batch size of 2048. We consider a varying number of workers and servers with limited CPU and network resources in §4.3. For checkpointing, we use 15 additional nodes of type i3en.xlarge as HDFS nodes, each equipped with NVMe SSDs and 25 Gbps of network bandwidth. All instances we consider use AWS ENA networking.

8

(samples/sec)

No FT

ECRM (k = 4)

Ckpt-30

ECRM (k = 2)

·105 Ckpt-60 best Ckpt-30 best ECRM (k = 4)

Ckpt-60 average Ckpt-30 average ECRM (k = 2)

Ckpt-60 worst Ckpt-30 worst

No FT

ECRM (k = 4)

Ckpt-30

ECRM (k = 2)

·105

Throughput (samples/sec)

Recovery time (minutes)

60

4

4 40 20 0

Criteo-Original

Criteo-2S

Criteo-2S-2D

220 GB

440 GB

880 GB

Figure 5: Time to fully recover a failed server.

2

0

50

100

150

200

Time (minutes)

2 Ckpt-60 Ckpt-30

ECRM (k = 4) ECRM (k = 2)

Figure 7: Throughput of training Criteo-2S-2D

No FT

ECRM (k = 4)

Increase in training time (%)

30

Ckpt-30

ECRM (k = 2)

·109

20

6

Sample Number

10 4

00

220

440

880

2

50 100 DLRM size (GB)

0

Figure 6: Training-time overhead.

150 50

100

150

200

Time (minutes) Metrics. For performance during recovery, we measure

Time (minutes) Figure 8: Progress of training Criteo-2S-2D

200

the time to fully recover a failed server and training through-

put during recovery (samples/second). For performance during normal operation, we measure training throughput and training-time overhead, which is the percent increase in the time to train a certain number of samples.

faster with k = 2). More importantly, unlike checkpointing, ECRM enables training to continue during recovery with high throughput.
Effect of parameter k and DLRM size. Figure 5 illus-

4.2 Performance during recovery

trates the discussion from §3.6 that it takes longer for ECRM to fully recover with higher value of parameter k. However,

We ﬁrst evaluate the performance of ECRM and checkpointing in recovering from failure. As recovery time for checkpointing depends on when failure occurs (see §2.2), we show the best-, average-, and worst-case for checkpointing. The recovery performance of ECRM and checkpointing is best compared in Figure 4, which plots the throughput and training progress of ECRM and Ckpt-30 on Criteo-2S-2D after a single server failure at time 10 minutes. As the recovery performance of Ckpt-60 is even worse than Ckpt-30, we omit it from the plots for clarity. ECRM fully recovers faster than the average case for Ckpt-30, and, critically, maintains throughput within 6%–12% of that during normal operation during recovery. In contrast, Ckpt-30 cannot perform new training iterations during recovery. As shown in the bottom plot of Figure 4, which plots the time taken to reach a number of training samples, ECRM’s high throughput during recovery

ECRM maintains high throughput during recovery for each value of k (see Figure 4).
Figure 5 also shows that the time to fully recover increases with DLRM size for both ECRM and checkpointing, as expected (see §2.2 and §3.6). ECRM’s recovery time increases more quickly with DLRM size than checkpointing due to the k-fold increase in data read and compute performed by a single server in ECRM when decoding. However, this does not signiﬁcantly affect training in ECRM because ECRM can continue training during recovery with high throughput.
Effect of lock granularity. Section 3.4 described ECRM’s approach of using granular locks to prevent race conditions during recovery. The number of embedding table entries covered by each lock (i.e., the lock granularity) leads to a tradeoff between memory overhead in recovery for buffering updates and time overhead for switching locks.

enables it to progress in training faster than even the best case

To evaluate this tradeoff, we compare the full recovery time

for Ckpt-30.

of ECRM with k = 4 when using a single lock throughout

Figure 5 shows the time it takes for ECRM, Ckpt-30, and recovery, and that when using 10 locks during recovery. Us-

Ckpt-60 to recover a failed server. ECRM with k = 4 re- ing 10 locks increases recovery time by 23.3%, 10%, and

covers 1.9–6.8× faster and 1.1–3.5× faster than the average 9.4% for Criteo-Original, Criteo-2S, and Criteo-2S-2D, re-

case for Ckpt-60 and Ckpt-30, respectively (and up to 10.3× spectively. It is important to note that, even when employing

9

locks with ﬁner granularity, and thus having longer overall recovery time, ECRM can continue to provide high training throughput during recovery, whereas checkpointing requires training to pause during recovery. Switching locks during recovery in ECRM involves (1) momentarily pausing training to synchronize workers and servers, and (2) copying updated embedding table entries from buffers to the original embedding table entries. The time overhead incurred from synchronization is constant regardless of DLRM size, whereas buffer copying overhead will grow with DLRM size. Thus, with larger DLRM sizes, synchronization overhead is better amortized, reducing the overall overhead of lock switching for larger DLRMs.
4.3 Performance during normal operation
We now compare the performance of ECRM and checkpointing during normal operation.
Figure 6 shows the training-time overhead of ECRM and checkpointing as compared to a system with no fault tolerance (and thus no overhead) in a four-hour training run. ECRM reduces training-time overhead during normal operation by 71.3%–88% and 41.3%–71.6% compared to Ckpt-30 and Ckpt-60, respectively. While the training-time overhead of checkpointing decreases with decreased checkpointing frequency, §4.2 showed that this came the expense of recovery performance. It is important to note that ECRM’s beneﬁt over checkpointing grows with DLRM size. For example, on the 880 GB Criteo-2S-2D, Ckpt-30 has training-time overhead of 33.4%, while ECRM has overheads of 4.2% and 4% with k of 4 and 2, respectively. This illustrates the promise of ECRM for future DLRMs, which are expected to grow in size.
Figure 7 plots the throughput of ECRM and Ckpt-30 compared to training with no fault tolerance (No FT) on Criteo-2S2D. As shown in the inset, ECRM has slightly lower throughput compared to No FT, while Ckpt-30 causes throughput to ﬂuctuate from that equal to No FT, to zero when writing a checkpoint. The effects of this ﬂuctuation are shown in Figure 8: Ckpt-30 progresses slower than ECRM.
Effect of parameter k. As described in §3.6, ECRM has constant network bandwidth and CPU overhead during normal operation regardless of the value of parameter k. This is illustrated in Figures 6, 7, and 8, in which ECRM has nearly equal performance with k = 2 and k = 4.
We also measure the overhead of ECRM with k = 10 on a cluster twice the size as that in §4.1 (to accommodate the higher value of k) and on a version of Criteo-Original scaled up to have the same number of entries per server as in the original cluster. In this setting, ECRM has training-time overhead of 0.5%. This smaller overhead stems not from the increase in k, but from the decreased load on each server due to the increased number of servers. Nevertheless, this experiment shows that ECRM can support high values of k.
Effect of ECRM on load imbalance. We next evaluate

the effect of ECRM’s approach to parity placement on cluster load imbalance. We measure load on each server by counting the number of updates that occur on each server when training Criteo-Original.
When training without erasure coding, the most-heavily loaded server performs 2.28× more updates than the leastheavily loaded server. In contrast, in ECRM with k = 2 and k = 4, this difference in load is 1.64× and 1.58×, respectively. This indicates that the increased load introduced by ECRM leads to improved load balance. Under ECRM, parities corresponding to the embedding table entries of a given server are distributed among all other servers. Thus, the same amount of load that an individual server experiences for non-parity updates will also be distributed among the other servers to update parities. While all servers will experience increased load, the most-loaded server in the absence of erasure coding is likely to experience the smallest increase in load due to the addition of erasure coding because all other servers for whom it hosts parities have lower load. A similar argument holds for the least-loaded server experiencing the largest increase in load. Hence, the expected difference in load between the most- and least-loaded servers will decrease. Thus, while ECRM doubles the total number of updates in the system, its impact is alleviated by improved load balancing provided by its approach to parity placement.
Effect of reduced server resources. As ECRM introduces CPU and network bandwidth overhead on servers during training, it is expected that ECRM will have higher training-time overhead when server CPU and network resources are limited. We evaluate ECRM in these settings by artiﬁcially limiting these resources when training Criteo-Original.
To evaluate ECRM with limited server CPU resources, we replace the r5n.8xlarge server instances described in §4.1 with x1e.2xlarge instances, which have the same amount of memory, but 4× less CPU cores. ECRM’s training-time overhead with k = 4 is 11.1% when using these instances, higher than that on the more-capable servers (2.6%).
To evaluate ECRM with limited server network bandwidth, we replace the r5n.8xlarge instances (which have 25 Gbps) described in §4.1 with r5.8xlarge instances (which have 10 Gbps). ECRM’s training-time overhead with k = 4 is 6.5% on these bandwidth-limited instances, higher than that on the more-capable servers (2.6%).
Even on these resource-limited servers, ECRM still beneﬁts from signiﬁcantly improved performance during recovery compared to checkpointing and has training-time overhead comparable to Ckpt-30 and slightly higher than Ckpt-60.
Effect of number of workers. We additionally performed experiments with a different number of workers in the system. We vary the number of workers from 5 to 25 corresponding to 1× to 5× the number of servers, and measure the average training throughput. Increasing the number of workers in the system increases the rate at which gradients are generated, and thus the load on servers in the system.

10

No FT

Ckpt-30

ECRM (k = 4)

4,000

Throughput (batches/sec)

3,000

2,000

1,000

0

0

5

10

15

20

25

Number of workers

Figure 9: Average training throughput with varying number of workers during normal operation

Figure 9 shows the throughput of ECRM, Ckpt-30, and training without any fault tolerance (No FT) on CriteoOriginal. Compared to No FT, ECRM’s overhead increases with the number of workers from 1.4% with 5 workers to 2.6% for 15 workers, and ﬁnally to 7.5% for 25 workers. This trend is expected: as the number of workers increases, gradients are sent to servers at a higher rate, which increases the load on each server. These more heavily-loaded servers are more heavily affected by the increase in load due to updating parities in ECRM than less-heavily loaded servers. Figure 9 also shows the training throughput of Ckpt-30 has a constant overhead of 9.0%. The results show that even in settings with higher worker to server ratio, ECRM maintains lower overhead than checkpointing during normal operation..
Beneﬁt of difference propagation. One motivation for ECRM’s approach of difference propagation is to efﬁciently update parities (see §3.3.2). To illustrate this, we compare ECRM to the naive alternative, gradient propagation on Criteo-Original. With k = 4, gradient propagation has trainingtime overhead of 9.0%, while ECRM has only 2.6% overhead, showing the beneﬁt of difference propagation.
4.4 Discussion
Handling concurrent failures. Recall from §3 that ECRM leverages erasure codes parameterized with r = 1, that is, which can recover from a single server failure. This choice was informed by prior studies of cluster failures, which showed that single-node failures are the most common failure scenarios among groups of nodes across which data is encoded [33].
If higher fault tolerance is desired, ECRM can be easily adapted to tolerate additional faults by using erasure codes with parameter r > 1. An alternative to this that still leverages r = 1 is to partition the overall cluster used in training into smaller groups of servers over which erasure coding with r = 1 is performed, such that more than a single failure within each group is unlikely. Finally, ECRM could also be adapted to take checkpoints at a much lower frequency than normal checkpointing schemes as a second layer of defense against concurrent failures. This ﬁnal approach bears similarity to

multi-level checkpointing [27]. Synchronous training. As described in §2.1, many organi-
zations deploying widely used recommendation systems use asynchronous training [5,17]. We have built ECRM atop XDL, an asynchronous training framework from Alibaba. However, ECRM can also support synchronous training. Synchronous training adds a barrier after a certain number of training iterations in which workers communicate gradients with one another and servers, combine these gradients, and perform a single update to each modiﬁed parameter. In such a synchronous framework, ECRM would require that parity entries also be updated during this barrier so that they are kept consistent with training updates. As this setting is not the focus of our work, we leave a full study and evaluation of ECRM in synchronous settings to future work.
Checkpointing for model deployment. In addition to checkpointing for fault tolerance, a system may also take checkpoints of a DLRM for later deployment. These checkpoints are taken infrequently (e.g., at the end of training), and thus add little overhead. ECRM does not preclude the use of checkpointing for this purpose.
5 Related Work
DLRM systems. System support for recommendation model training and inference has recently received signiﬁcant attention. Solutions tailored toward understanding and improving DLRM inference range from workload and system analysis [16, 23], model-system codesign [13, 15], and specialized hardware support [18, 38].
More recently, work has emerged for improving the performance of training DLRMs. For example, recent works from organizations that train large-scale DLRMs have described systems designed for training DLRMs [5,17,19,28,44]. Other works have focused on model-system codesign, such as reducing the sizes of embedding tables through compression and precision-reduction techniques [14, 40, 41].
ECRM differs from these works by its focus on fault tolerance for DLRM training and its novel use of erasure codes therein.
Checkpointing. Computer systems have long used checkpointing for fault tolerance [8, 20, 27]. Recent works optimize checkpointing in neural network training [26, 29], but do not focus on DLRM training. In contrast, ECRM leverages the unique characteristics of DLRM training to use erasure codes for efﬁcient fault tolerance. Other works have developed approximation-based checkpointing techniques to reduce the overhead of checkpointing in iterative machine learning training algorithms [6, 31]. Unlike these approaches, ECRM does not change the accuracy guarantees of the underlying training system.
Most closely related to ECRM are the works of Maeng et al. [24] and Eisenman et al. [12], which propose techniques for reducing the overhead of checkpointing in DLRM training.

11

Maeng et al. [24] propose to use partial recovery to reduce the overhead of rolling back a DLRM during recovery; when a failure occurs, only the failed node rolls back to its most recent checkpoint. Eisenman et al. [12] propose a combination of incremental checkpointing and reducing the numerical precision of checkpointed parameters. Both of these works leverage techniques that can potentially reduce the accuracy of DLRM training upon recovering from failure. While both works empirically demonstrate small accuracy drops, they cannot provide the same accuracy guarantees as the underlying DLRM training system. ECRM differs from these works in two regards: (1) ECRM maintains the same accuracy guarantees as the underlying DLRM training system on top of which it is built. This reduces uncertainty about whether the fault tolerance approach being used will deliver a model with high enough accuracy needed for deployment, and reduces data scientist effort needed in debugging model performance when there are multiple sources of potential inaccuracy present. (2) ECRM leverages erasure coding to reduce the overhead of fault tolerance.
Erasure-coded systems. Erasure codes have been widely used in storage systems, communication systems, and caching systems for various purposes, such as fault/loss tolerance, load balancing, and alleviation of slowdowns [30,32,34,39]. To the best of our knowledge, ECRM is the ﬁrst approach of leveraging erasure codes for fault tolerance within DLRM training systems. Of the more traditional uses of erasure codes described above, ECRM bears greatest similarity erasure-coded storage systems. As described in §3, some of the techniques employed by ECRM (e.g., rotating parity placement) are inspired by erasure-coded storage systems and are adapted to the unique properties of DLRM training systems.
Erasure coding in machine learning systems. Recent work has applied coding-theoretic ideas to machine learning systems. These works primarily focus on alleviating the effects of transient slowdowns in neural network inference systems [21] and in training certain classes of machine learning models (e.g., [11, 22, 36, 42]). In contrast, ECRM imparts fault tolerance to DLRM training, which differs signiﬁcantly in model architecture and system design from the settings considered in these works.
6 Conclusion
ECRM is a new approach to fault-tolerant DLRM training that employs erasure coding to overcome the downsides of checkpointing. ECRM encodes the large embedding tables and optimizer state in DLRMs, maintains up-to-date parities with low overhead, and enables training to continue during recovery, while maintaining the same accuracy guarantees as the underlying training system. Compared to checkpointing, ECRM reduces training-time overhead in the absence of failures by up to 88%, recovers from failures up to 10.3× faster, and allows training to proceed without pauses both during

normal operation or recovery. While ECRM’s beneﬁts come at the cost of additional memory requirements and load on the servers, the impact of these is alleviated by the fact that memory overhead is only fractional and that load gets evenly distributed. ECRM shows the potential of erasure coding as a superior alternative to checkpointing for fault tolerance in training current and future DLRMs.
References
[1] Criteo labs: Download terabyte click logs https://labs.criteo.com/2013/12/ download-terabyte-click-logs/. Last accessed 04 February 2021.
[2] Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems. https://tinyurl.com/yy82pd2l. Last accessed 04 February 2021.
[3] MLPerf Inference Github Repository. https:// github.com/mlperf/inference. Last accessed 04 February 2021.
[4] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A System for Large-Scale Machine Learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016.
[5] Bilge Acun, Matthew Murphy, Xiaodong Wang, Jade Nie, Carole-Jean Wu, and Kim Hazelwood. Understanding Training Efﬁciency of Deep Learning Recommendation Models at Scale. In 2021 IEEE International Symposium on High Performance Computer Architecture (HPCA 21), 2021.
[6] Yu Chen, Zhenming Liu, Bin Ren, and Xin Jin. On Efﬁcient Constructions of Checkpoints. In Proceedings of the International Conference on Machine Learning (ICML 20), 2020.
[7] Paul Covington, Jay Adams, and Emre Sargin. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems, 2016.
[8] John T Daly. A Higher Order Estimate of the Optimum Checkpoint Interval for Restart Dumps. Future Generation Computer Systems, 22(3):303–312, 2006.

12

[9] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR 15), 2015.
[10] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12(7), 2011.
[11] Sanghamitra Dutta, Ziqian Bai, Haewon Jeong, Tze Meng Low, and Pulkit Grover. A Uniﬁed Coded Deep Neural Network Training Strategy Based on Generalized Polydot Codes for Matrix Multiplication. In Proceedings of the 2018 IEEE International Symposium on Information Theory (ISIT 18), 2018.
[12] Assaf Eisenman, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Murali Annavaram, Krishnakumar Nair, and Misha Smelyanskiy. Check-N-Run: A Checkpointing System for Training Recommendation Models. arXiv preprint arXiv:2010.08679, 2020.
[13] Assaf Eisenman, Maxim Naumov, Darryl Gardner, Misha Smelyanskiy, Sergey Pupyrev, Kim Hazelwood, Asaf Cidon, and Sachin Katti. Bandana: Using NonVolatile Memory for Storing Deep Learning Models. In The Second Conference on Systems and Machine Learning (SysML 19), 2019.
[14] Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. Mixed Dimension Embeddings with Application to Memory-Efﬁcient Recommendation Systems. arXiv preprint arXiv:1909.11810, 2019.
[15] Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S Lee, David Brooks, and Carole-Jean Wu. DeepRecSys: A System for Optimizing End-to-End At-Scale Neural Recommendation Inference. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA 20), 2020.
[16] Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, et al. The Architectural Implications of Facebook’s DNNbased Personalized Recommendation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA 20), 2020.
[17] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui Huang, Xinyang Guo, Dongyue Wang, Yue Song, et al. XDL: An Industrial Deep Learning Framework for High-Dimensional Sparse Data. In Proceedings of the 1st International Workshop on Deep

Learning Practice for High-Dimensional Sparse Data, 2019.
[18] Wenqi Jiang, Zhenhao He, Shuai Zhang, Thomas B Preußer, Kai Zeng, Liang Feng, Jiansong Zhang, Tongxuan Liu, Yong Li, Jingren Zhou, et al. MicroRec: Accelerating Deep Recommendation Systems to Microseconds by Hardware and Data Structure Solutions. In The Fourth Conference on Systems and Machine Learning (MLSys 21), 2021.
[19] Dhiraj Kalamkar, Evangelos Georganas, Sudarshan Srinivasan, Jianping Chen, Mikhail Shiryaev, and Alexander Heinecke. Optimizing Deep Learning Recommender Systems’ Training On CPU Cluster Architectures. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC 20), 2020.
[20] Richard Koo and Sam Toueg. Checkpointing and Rollback-Recovery for Distributed Systems. IEEE Transactions on software Engineering, (1):23–31, 1987.
[21] Jack Kosaian, K. V. Rashmi, and Shivaram Venkataraman. Parity Models: Erasure-Coded Resilience for Prediction Serving Systems. In Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP 19), 2019.
[22] Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, and Kannan Ramchandran. Speeding Up Distributed Machine Learning Using Codes. IEEE Transactions on Information Theory, July 2018.
[23] Michael Lui, Yavuz Yetim, Özgür Özkan, Zhuoran Zhao, Shin-Yeh Tsai, Carole-Jean Wu, and Mark Hempstead. Understanding Capacity-Driven Scale-Out Neural Recommendation Inference. 2020.
[24] Kiwan Maeng, Shivam Bharuka, Isabel Gao, Mark C Jeffrey, Vikram Saraph, Bor-Yiing Su, Caroline Trippel, Jiyan Yang, Mike Rabbat, Brandon Lucia, et al. CPR: Understanding and Improving Failure Tolerant Training for Deep Learning Recommendation with Partial Recovery. In The Fourth Conference on Systems and Machine Learning (MLSys 21), 2021.
[25] Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. MLPerf Training Benchmark. 2020.
[26] Jayashree Mohan, Amar Phanishayee, and Vijay Chidambaram. CheckFreq: Frequent, Fine-Grained DNN Checkpointing. In 19th USENIX Conference on File and Storage Technologies (FAST 21), 2021.

13

[27] Adam Moody, Greg Bronevetsky, Kathryn Mohror, and Bronis R De Supinski. Design, Modeling, and Evaluation of a Scalable Multi-level Checkpointing System. In Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC 10), 2010.
[28] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep Learning Recommendation Model for Personalization and Recommendation Systems. arXiv preprint arXiv:1906.00091, 2019.
[29] Bogdan Nicolae, Jiali Li, Justin M Wozniak, George Bosilca, Matthieu Dorier, and Franck Cappello. Deepfreeze: Towards Scalable Asynchronous Checkpointing of Deep Learning Models. In 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID), 2020.
[30] David A. Patterson, Garth Gibson, and Randy H. Katz. A Case for Redundant Arrays of Inexpensive Disks (RAID). In Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 88), 1988.
[31] Aurick Qiao, Bryon Aragam, Bingjing Zhang, and Eric Xing. Fault Tolerance in Iterative-Convergent Machine Learning. In International Conference on Machine Learning, pages 5220–5230, 2019.
[32] K. V. Rashmi, Mosharaf Chowdhury, Jack Kosaian, Ion Stoica, and Kannan Ramchandran. EC-Cache: LoadBalanced, Low-Latency Cluster Caching with Online Erasure Coding. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016.
[33] K. V. Rashmi, Nihar B Shah, Dikang Gu, Hairong Kuang, Dhruba Borthakur, and Kannan Ramchandran. A Hitchhiker’s Guide to Fast and Efﬁcient Data Reconstruction in Erasure-Coded Data Centers. In Proceedings of the 2014 ACM SIGCOMM Conference (SIGCOMM 14), 2014.
[34] Luigi Rizzo. Effective Erasure Codes for Reliable Computer Communication Protocols. ACM SIGCOMM Computer Communication Review, 27(2):24–36, 1997.
[35] Mahesh Sathiamoorthy, Megasthenis Asteris, Dimitris Papailiopoulos, Alexandros G Dimakis, Ramkumar Vadali, Scott Chen, and Dhruba Borthakur. XORing Elephants: Novel Erasure Codes for Big Data. Proceedings of the VLDB Endowment, 6(5), 2013.

[36] Rashish Tandon, Qi Lei, Alexandros G Dimakis, and Nikos Karampatziakis. Gradient Coding: Avoiding Stragglers in Distributed Learning. In International Conference on Machine Learning (ICML 17), 2017.
[37] Hakim Weatherspoon and John D Kubiatowicz. Erasure Coding vs. Replication: A Quantitative Comparison. In International Workshop on Peer-to-Peer Systems (IPTPS 2002), 2002.
[38] Mark Wilkening, Udit Gupta, Samuel Hsia, Caroline Trippel, Carole-Jean Wu, David Brooks, and Gu-Yeon Wei. RecSSD: Near Data Processing for Solid State Drive Based Recommendation Inference. In Proceedings of the Twenty-Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 21), 2021.
[39] Shiqin Yan, Huaicheng Li, Mingzhe Hao, Michael Hao Tong, Swaminathan Sundararaman, Andrew A Chien, and Haryadi S Gunawi. Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs. In 15th USENIX Conference on File and Storage Technologies (FAST 17), 2017.
[40] Jie Amy Yang, Jianyu Huang, Jongsoo Park, Ping Tak Peter Tang, and Andrew Tulloch. MixedPrecision Embedding Using a Cache. arXiv preprint arXiv:2010.11305, 2020.
[41] Chunxing Yin, Bilge Acun, Xing Liu, and Carole-Jean Wu. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models. In The Fourth Conference on Systems and Machine Learning (MLSys 21), 2021.
[42] Qian Yu, Netanel Raviv, Jinhyun So, and A Salman Avestimehr. Lagrange Coded Computing: Optimal Design for Resiliency, Security and Privacy. In Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 19), 2019.
[43] Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman Arora, and Xin Jin. Is network the bottleneck of distributed training? In Proceedings of the Workshop on Network Meets AI & ML, 2020.
[44] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li. AIBox: CTR Prediction Model Training on a Single Node. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM 2019), 2019.
Appendix A Cross-parameter consistency of ECRM
As described in §3.5, ECRM guarantees that a recovered DLRM represents one that could have been reached by asyn-

14

Table 1: Order of events illustrating consistency of ECRM atop asynchronous training.
Time Prev. State New State Event

0

xt , yt

xt+1, yt

Entry x is updated from xt to xt+1 on Server 0. Entry and optimizer difference is asynchronously propagated to the server holding the parity for x.

1

xt+1, yt

xt+1, yt

Entry yt is read from Server 1.

2

xt+1, yt

xt+1, yt+1

Entry y is updated from yt to yt+1 on Server 1. Entry and optimizer difference is asynchronously propagated to the server holding the parity for y.

3

xt+1, yt+1 xt+1, yt+1 The parity for entry y is updated

to reﬂect the change in y.

4

xt+1, yt+1 xt+1, yt+1 Server 0 fails, having not yet

sent the difference for x.

5

xt+1, yt+1 xt , yt+1

Recovery process decodes x.

chronous training, but does not guarantee that the recovered DLRM represents a state that was truly experienced during recovery. We will next illustrate this by example and show how the guarantee above results in ECRM providing the same consistency semantics as asynchronous training.
Consider the timeline of events in Table 1 for training a DLRM with embedding table entries x and y. We consider the state of the DLRM to the combined state of each of these parameters.
As illustrated in the timeline in Table 1, due to the asynchrony of difference propagation, the recovery process results in a DLRM state {xt , yt+1} that was never experienced during training: in training, x was in state t + 1 before yt was even read.
Though the DLRM state recovered by ECRM in the timeline above was never truly experienced during training, it is a DLRM state that could have just as easily been experienced during asynchronous training. Under asynchronous training, it would be just as valid for the event at time 0 to have been performed after the event at time 2, which would have resulted in the DLRM state being {xt , yt+1}. Thus, the state recovered by ECRM is still valid from the lens of asynchronous training.

15

