arXiv:2108.09932v1 [cs.LG] 23 Aug 2021

Federated Learning Meets Fairness and Diﬀerential Privacy
Manisha Padala, Sankarshan Damle and Sujit Gujar
Machine Learning Lab, International Institute of Information Technology (IIIT), Hyderabad
{manisha.padala,sankarshan.damle}@research.iiit.ac.in sujit.gujar@iiit.ac.in
Abstract. Deep learning’s unprecedented success raises several ethical concerns ranging from biased predictions to data privacy. Researchers tackle these issues by introducing fairness metrics, or federated learning, or diﬀerential privacy. A ﬁrst, this work presents an ethical federated learning model, incorporating all three measures simultaneously. Experiments on the Adult, Bank and Dutch datasets highlight the resulting “empirical interplay” between accuracy, fairness, and privacy.
Keywords: Federated Learning · Fairness · Diﬀerential Privacy.
1 Introduction
Deep Learning’s success is made possible in part due to the availability of big datasets – distributed across several owners. To resolve this, researchers propose Federated Learning (FL), which enables parallel training of a uniﬁed model [18]. Such a requirement naturally arises for mobile phones, network sensors, and other IoT applications. In FL, these respective owners are referred to as ‘clients’ (henceforth agents). The agents individually train a model on their private data. A ‘central server,’ referred henceforth as an aggregator, receives the individual models and computes a single overall model through diﬀerent heuristics for achieving high performance on any test data [28].
The data available with each agent is often imbalanced or biased. Machine learning models may further amplify the bias present. More concretely, when trained only for achieving high accuracy, the model predictions become highly biased towards certain demographic groups like gender, age, or race [4,5,7]. Such groups are known as the sensitive attributes. Post the impossibility result on achieving a perfectly unbiased model [7], researchers propose several approaches which focus on minimizing the bias while maintaining high accuracy [6,17,2,23].
Invariably all these approaches require the knowledge of the sensitive attribute. These attributes often comprise the most critical information. The law regulations at various places prohibit using such attributes to develop ML models. E.g., the EU General Data Protection Regulation prevents the collection of sensitive user attributes [26]. Thus, it is imperative to address discrimination while preserving the leakage of sensitive attributes from the data samples.

2

Padala, Damle and Gujar

Observing that the aggregator in FL has no direct access to private data or sensitive attributes, prima facie preserves privacy. However, there exist several attacks that highlight the information leak in an FL setting [20]. To plug this information leak, researchers either use cryptographic solutions based mainly on complex Partial Homomorphic Encryption (PHE) or use Diﬀerential Privacy (DP). Private FL solutions using PHE (e,g., [19,30,31,12]) suﬀer from computational ineﬃciency and post-processing attacks. Thus, in this work we focus on the strong privacy guarantees provided by a diﬀerentially private solution [24,25,22,29].
FPFL Framework. A ﬁrst, we incorporate both fairness and privacy guarantees for an FL setting through our novel framework FPFL: Fair and Private Federated Learning. Our primary goal is to simultaneously preserve the privacy of the training data and the sensitive attribute while ensuring fairness. With FPFL, we achieve this goal by ingeniously decoupling the training process in two phases. In Phase 1, each agent trains a model, on its private dataset, for fair predictions. Then in Phase 2, the agents train a diﬀerentially private model to mimic the fair predictions from the previous model. At last, each agent communicates the private model from Phase 2 to the aggregator.
Fairness and Privacy Notions. The deliberate phase-wise training ensures an overall fair and accurate model which does not encode any information related to the sensitive attribute. Our framework is general and absorbs any fairness or DP metrics. It also allows any fairness guaranteeing technique in Phase 1. In this paper we demonstrate FPFL’s eﬃcacy w.r.t. the state-of-the-art technique and the following notions.
1. Fairness: We consider demographic parity (DemP) and equalized odds (EO). DemP states that a model’s predictions are independent of a sensitive attribute of the dataset [10]. EO states that the false positive rates and false negative rates of a model are equal across diﬀerent groups or independent of the sensitive attribute [14].
2. Privacy: We quantify the privacy guarantees within the notion of ( , δ)-local diﬀerential privacy [11]. The notion of local-DP is a natural ﬁt for FL. In our setting, the aggregator acts as an adversary with access to each agent’s model. We show that with local-DP, the privacy of each agent’s training data and sensitive attribute is protected from the aggregator.

Empirical interplay between accuracy, fairness, and privacy. The authors in [3] were one of the ﬁrst to show that ensuring privacy may come at a cost to fairness. While the trade-oﬀ between fairness and privacy in ML is under-explored, Table 1 presents the existing literature. Apart from those, the authors in [16] consider accuracy parity which is weaker than EO in a non-private FL setting.
Contributions. In summary, we propose our novel FPFL framework (Fig. 2). We prove that FPFL provides the local-DP guarantee for both the training data and the sensitive attribute(s) (Proposition 1). Our experiments on the Adult, Bank, and Dutch datasets show the empirical trade-oﬀ between fairness, privacy, and accuracy of an FL model (Section 4).

Federated Learning Meets Fairness and Diﬀerential Privacy

3

Fairness

Privacy

Paper

FL

Demographic Parity Equalized Odds Training Data Sensitive Attribute

[8,21]











[26]











[9]











[22,27,29]











FPFL











Table 1. Comparing Existing Literature with FPFL

2 Preliminaries

We consider a binary classiﬁcation problem with X as our (d-dimensional) instance space, X ∈ Rd; and our output space as Y ∈ {0, 1}. We consider a single sensitive attribute A associated with each individual instance. Such an attribute may represent sensitive information like age, gender or caste. Each a ∈ A represents a particular category of the sensitive attribute like male or female.

Federated Learning Model. Federated Learning (FL) decentralizes the classical

machine learning training process. FL comprises two type of actors: (i) a set

of agents A = {1, . . . , m} where each agent i owns a private dataset Xi12; and

(ii) an Aggregator. Each agent provides its model, trained on its dataset, to the

aggregator. The aggregator’s job then is to derive an overall model, which is then

communicated back to the agents. This back-and-forth process continues until a

model with suﬃcient accuracy is derived.

At the start of an FL training, the aggregator communicates an initial, often

random, set of model parameters to the agents. Let us refer to the initial param-

eters as θ0. At each timestep t each agent updates their individual parameters

denoted by θ(i,t), using their private datasets. The agents then communicate the

updated parameters to the aggregator, who derives an overall model through dif-

ferent heuristics [28]. In this paper, we focus on the weighted sum heuristics, i.e.,

the overall model parameters take the form θt =

j∈A

|Xj | X

·

θ(j,t).

To

distinguish

the ﬁnal overall model, we refer to it as θ∗, calculated at a timestep T .

Fairness Metrics. We consider the following two fairness constraints. Deﬁnition 1 (Demographic Parity (DemP)). A classiﬁer h satisﬁes Demographic Parity under a distribution over (X , A, Y) if its predictions h(X ) is independent of the sensitive attribute A. That is, ∀a ∈ A and p ∈ {0, 1},
Pr[h(X ) = p|A = a] = Pr[h(X ) = p]
Given that p ∈ {0, 1}, we have ∀a
E[h(X )|A = a] = E[h(X )]. 1 Let |Xi| denote the cardinality of Xi with X = i |Xi|. 2 We use the sub-script “i” when referring to a particular agent i and drop it when
not referring to any particular agent.

4

Padala, Damle and Gujar

Deﬁnition 2 (Equalized Odds (EO)). A classiﬁer h satisﬁes Equalized Odds under a distribution over (X , A, Y) if its predictions h(X ) are independent of the sensitive attribute A given the label Y. That is, ∀a ∈ A, p ∈ {0, 1} and y ∈ Y
Pr[h(X ) = p|A = a, Y = y] = Pr[h(X ) = p|Y = y]
Given that p ∈ {0, 1}, we can say ∀a, y
E[h(X )|A = a, Y = y] = E[h(X )|Y = y].

Local Diﬀerential Privacy (LDP). We now deﬁne LDP in the context of our FL model. We remark that LDP does not require deﬁning adjacency.

Deﬁnition 3 (Local Diﬀerential Privacy (LDP) [11]). For an input set X and the set of noisy outputs Y, a randomized algorithm M : X → Y is said to be ( , δ)-LDP if ∀x, x ∈ X and ∀y ∈ Y the following holds,

Pr[M(x) = y] ≤ exp( ) Pr[M(x ) = y] + δ.

(1)

LDP provides a statistical guarantee against an inference which the adversary can make based on the output of M. This guarantee is upper-bounded by , referred to as the privacy budget. is a metric of privacy loss deﬁned as,

Ly

M(x) = y

= ln

.

(2)

M(x)||M(x )

M(x ) = y

The privacy budget, , controls the trade-oﬀ between quality (or, in our case, the accuracy) of the output vis-a-vis the privacy guarantee. That is, there is no “free-dinner” – lower the budget, better the privacy but at the cost of quality. The “δ” parameter in (1) allows for the violation of the upper-bound , but with a small probability.
Diﬀerentially private ML solutions focus on preserving an individual’s privacy within a dataset. Such privacy may be compromised during the training process or based on the predictions of the trained model [13]. The most famous of such an approach is the DP-SGD algorithm, introduced in [1]. In DP-SGD, the authors sanitize the gradients provided by the Stochastic Gradient Descent (SGD) algorithm with Gaussian noise (N (0, σ2)). This step aims at controlling the impact of the training data in the training process.

Adversary Model. Towards designing a private FL system, it suﬃces to provide DP guarantees for any possible information leak to the aggregator. The postprocessing properties of DP further preserves the DP guarantee for the training data and the sensitive attribute from any other party.
We consider the “black-box” model for our adversary, i.e., the aggregator has access to the trained model and can interact with it via inputs and outputs. With this, it can perform model-inversion attacks [13], among others.

Federated Learning Meets Fairness and Diﬀerential Privacy

5

FPFL Framework
1. Initialization 2. Local Training Process. Each
agent i ∈ A, invokes Algorithm 1 3. Local Training Process. Each
agent i ∈ A, invokes Algorithm 2 4. Local training process ends 5. Model Aggregation. Aggregator
computes and then broadcasts an overall model 6. Agents re-initialize their local models with the overall model received 7. Repeat steps 3-6 till a suﬃcient overall accuracy is reached (Eq. 7)

Fig. 1. FPFL Model.

Fig. 2. FPFL Framework

3 FPFL: Fair and Private Federated Learning

In FPFL (Figure 1), we consider a classiﬁcation problem. Each agent i deploys two multi-layer neural networks (NNs) to learn the model parameters in each phase. The training comprises of two phases: (i) In Phase 1, each agent privately trains a model on its private dataset to learn a highly fair and accurate model; and (ii) In Phase 2, each agent trains a second model to mimic the ﬁrst, with DP guarantees. This process is akin to knowledge distillation [15]. In FPFL, only the model trained in Phase 2 is broadcasted to the aggregator.
To enhance readability and to remain consistent with FL notations, we denote the model parameters learned for Phase 1 with φ and Phase 2 with θ. Likewise, we represent the total number of training steps in Phase 1 with T1, and for Phase 2, we use T2.
Phase 1: Fair-SGD. In this phase, we train the network to maximize accuracy while achieving the best possible fairness on each agent’s private dataset. We adapt the Lagrangian Multiplier method [23] to achieve a fair and accurate model. We denote the model for agent i as hφi with parameters φi. Brieﬂy, the method trains a network with a uniﬁed loss that has two components. The ﬁrst component of the loss maximizes accuracy, i.e., the cross-entropy loss,
lCE(hφi , X , Y) = E [−yi log(hφi (x)) − (1 − y) log(1 − hφi (x))]
(x,y)∼(X ,Y)

The second component of the loss is a speciﬁc fairness measure. For achieving DemP (Deﬁnition 1), the loss function is given by,

lDemP (hφi , X , A) = |E[hφi (x)|A = a] − E[hφi (x)]|

(3)

6

Padala, Damle and Gujar

Algorithm 1 Fair-SGD for an Agent i
Input: Training dataset Xi = {x1, . . . , xn}, Loss function L1(·) as deﬁned in (5). Hyperparameters: learning rate η, η , batch size B, sampling probability q = B/|Xi|. Output: φ(i,T1) Initialization: φ(i,0) ← randomly, λm i ax ← max value for t ∈ [T1] do
Take a random sample Bt with probability q ∀k ∈ Bt: gt(xk) ← ∇φ(i,t) L1(·); g t(xk) ← ∇λ(i,t) L1(·) φ(i,t+1) ← φ(i,t) − ηtgt(xk); λ(i,t+1) ← λ(i,t) + η gt(xk) end for

For achieving EO (Deﬁnition 2), the corresponding loss function is,

lEO(hφi , X , A, Y) = |E[hφi (x)|A = a, y] − E[hφi (x)|y]|

(4)

Hence, the overall loss from the Lagrangian method is,

L1(hφi , X , A, Y ) = lCE + λlk, k ∈ {DemP, EO}

(5)

In the equation above, λ ∈ R+ is the Lagrangian multiplier. The overall opti-

mization is as follows: min max L1(·). Thus, each agent trains the Fair-SGD

φ

λ

model hφi to obtain the best accuracy w.r.t. a given fairness metric. We present

it formally in Algorithm 1.

Phase 2: DP-SGD. In this phase, the agents train a model that is communicated with the aggregator. This model denoted by hθi is trained by each agent i to learn the predictions of its own Fair-SGD model (hφi ) from Phase 1. The loss
function is given by,

L2(hθi , hφi ) = E [−hφi (x) log(hθi (x)) − (1 − hφi (x)) log(1 − hφi (x))] (6)
x∼X
Equation 6 is the cross-entropy loss between predictions from DP-SGD model and the labels given by the predictions from Fair-SGD model. That is, L2(·) ↓ =⇒ θi → φi.
To preserve privacy of training data and sensitive attribute, we use ( , δ)-LDP (Deﬁnition 3). In particular, we deploy DP-SGD. In it, the privacy of the training data is preserved by sanitizing the gradients provided by SGD with Gaussian noise (N (0, σ2)). Given that the learnt model hθi , mimics hφi , the learnt model is reasonably fair and accurate. Algorithm 2 formally presents the training.
FPFL Framework. The θi’s from each agent are communicated to the aggregator for further performance improvement. The aggregator takes a weighted sum of the individual θi’s and broadcasts it to the agents. The agents further train on top of the aggregated model before sending it to the aggregator. This process gets repeated to achieve the following overall objective,

θ∗ = arg min

|Xj | · L2(hθ, hφj ).

(7)

θ j∈[m] X

Federated Learning Meets Fairness and Diﬀerential Privacy

7

Algorithm 2 DP-SGD for an Agent i [1]
Input: Training dataset Xi = {x1, . . . , xn}, Loss function L2(·) as deﬁned in (6). Hyperparameters: learning rate η, standard deviation σ, batch size B, sampling probability q = B/|Xi| and clipping norm C. Output: θ(i,T2) Initialization: θ(i,0) ← randomly for t ∈ [T2] do
Take a random sample Bt with probability q ∀k ∈ Bt: gt(xk) ← ∇θ(i,t) L2(·) g¯t(xk) ← gt(xk)/ max 1, ||gt(Cxk)||2 ; g˜t(xk) ← B1 i g¯t(xk) + N (0, σ2C2I) θ(i,t+1) ← θ(i,t) − ηtg˜t(xk) end for

We now formally couple these processes above to present the FPFL framework with Figure 2. The framework presents itself as a plug-and-play system, i.e., a user can use any other loss function instead of L1, L2, or change the underlying algorithms for fairness and DP, or do any other tweak it so desires.
FPFL: Diﬀerential Privacy Bounds. Observe that the model learned in Phase 1, hφ, requires access to both the training data (X ) and the sensitive attribute (A). Fortunately, this phase is entirely independent of the FL aggregation process. In contrast, the model learned in Phase 2, hθ – trained to mimic the predictions of hφ – is communicated to the aggregator.
Any information leak in FPFL may take place in the following two ways. Firstly, training data may get compromised through hθ. Secondly, mimicking the predictions from hφ may, in turn, leak information about the sensitive attribute. We observe that the DP guarantee for the training data follows from [1, Theorem 1] directly. The following proposition proves that the training process in Phase 2 does not leak any additional information regarding A to the aggregator. Then, Corollary 1 uses the result with [1, Theorem 1] to provide the privacy bounds.
Proposition 1. With the diﬀerentially private FPFL framework (Figure 2), the aggregator with access to the model hθ learns no additional information, over the DP guarantee, regarding the sensitive attribute A.
Corollary 1. For the FPFL framework (Figure 2), ∀i ∈ A there exists constants c1 and c2, with the sampling probability qi = Bi/Xi and the total number of timesteps T in Phase 2, such that for any i < c1qi2T , the framework satisﬁes ( i, δi)-LDP for δi > 0 and for
σi ≥ c2 qi T ln(1/δi) .
i
4 FPFL: Experimental Results
Datasets. We conduct experiments on the following three datasets: Adult [23], Bank [23] and Dutch [32]. The ﬁrst two have ≈ 40k samples, while the Dutch

8

Padala, Damle and Gujar

dataset has ≈ 60k. In the Adult dataset, the task is a binary prediction of whether an individual’s income is above or below USD 50000. The sensitive attribute is gender and is available as either male or female. In the Bank dataset, the task is to predict if an agent has subscribed to the term deposit or not. In this case, we consider age as the sensitive attribute. We consider only two categories for age, where people between the ages 25 to 60 form the majority group, and those under 25 or over 60 form the minority group. In the Dutch dataset, similar to Adult, we consider gender as the sensitive attribute comprising males and females. The task is to predict the occupation. For training an FL model, we split the datasets such that each agent has an equal number of samples. In order to do so, we duplicate the samples in the existing data – especially the minority group – to get exactly 50k samples for the ﬁrst two datasets. Despite this, each agent ends up with an uneven distribution of samples belonging to each attribute maintaining the data heterogeneity. This results in heterogeneous data distribution among the agents. We hold 20% of the data from each dataset as the test set.

Hyperparameters. For each agent, we train two fully connected neural networks having the same architecture. Each network has two hidden layers with (500, 100) neurons and ReLU activation. For DemP, we consider 5 agents in our experiments and split datasets accordingly. To estimate EO, we need suﬃcient samples for both sensitive groups such that each group has enough samples with both the possible outcomes. In the Adult dataset, we ﬁnd only 3% female samples earning above USD 50000. Similarly, in the Bank dataset, the minority group that has subscribed to the term deposit forms only 1% of the entire data. Due to this, in our experiments for EO, we consider only 2 agents.

Training Fair-SGD (Phase 1). We use Algorithm 1, with η = 0.001 and B = 500. The optimizer used is Adam for updating the loss using the Lagrangian multiplier method. For the Adult dataset, we initialize with λ = 10, and for the Bank and Dutch datasets, we initialize with λ = 5. The model is trained for 200 epochs.

Training DP-SGD (Phase 2). We use Algorithm 2, with η = 0.25, B = 500, and the clipping norm C = 1.5. For the optimizer we use the Tensorﬂow-privacy library’s Keras DP-SGD optimizer3. We train the model in this phase for 5 epochs locally before aggregation. This process is repeated 4 times, i.e. T = 20.

Baselines. To compare the resultant interplay between accuracy, fairness, and privacy in FPFL, we create the following two baselines.
B1 In this, the agents train the model in an FL setting only for maximizing accuracy without any fairness constraints in the loss.
B2 To obtain B2, each agent trains the model in an FL setting for both accuracy and fairness using Algorithm 1 with DemP loss (3) or EO loss (4).
3 https://github.com/tensorflow/privacy

Federated Learning Meets Fairness and Diﬀerential Privacy

9

(a) Demographic Parity (DemP)

(b) Equalized Odds (EO)

Fig. 3. Performance for the Adult dataset

(a) Demographic Parity (DemP)

(b) Equalized Odds (EO)

Fig. 4. Performance for the Bank dataset

For both B1 and B2, the ﬁnal model obtained after multiple aggregations is used to report the results. These baselines maximize accuracy and ensure fairness without any privacy guarantee. Essentially, this lack of a privacy guarantee implies that for both baselines, we skip FPFL’s Phase 2.
( , δ)-bounds. We calculate the bounds for an agent from Corollary 1. To remain consistent with the broad DP-ML literature, we vary in the range (0, 10] by appropriately selecting σ (noise multiplier). Observe that → ∞ for B1 and B2 as the sensitivity is unbounded. As standard ∀i ∈ A, we keep δ = 10−4 < 1/|Xi| for DemP and δ = 0.5 × 10−4 < 1/|Xi| for EO.
DemP and EO. When the loss for DemP (3) and EO (4) is exactly zero, the model is perfectly fair. As perfect fairness is impossible, we try to minimize the loss. In our results, to quantify the fairness guarantees, we plot lDemP and lEO on the test set. Lower the values, better is the guarantee. For readability we refer lDemP and lEO as DemP and EO in our results. Demographic Parity: Figures 3(a), 4(a), and 5(a). We consider an FL setting with 5 agents for ensuring DemP. For the Adult dataset, Figure 3(a), we ﬁnd that for B1, we get an accuracy of 87% and a DemP of 0.17. We observe that a model trained with fairness constraints, i.e., for B2, has a reduced accuracy of

10

Padala, Damle and Gujar

(a) Demographic Parity (DemP)

(b) Equalized Odds (EO)

Fig. 5. Performance for the Dutch dataset

85%, but DemP reduces to 0.029. We ﬁnd similar trends in the baselines for the Bank (Figure 4(a)) and the Dutch datasets (Figure 5(a)).
Introducing privacy guarantees with FPFL, we observe a further compromise in either accuracy and fairness as compared to our baselines. In general, with increasing , i.e., increasing privacy loss, there is an improvement in the trade-oﬀ of accuracy and DemP. For = 10, the accuracy and DemP are similar to that in B2. While the drop in accuracy is consistent with decrease in , DemP values do not always follow this trend.
Equalized Odds: Figures 3(b), 4(b), and 5(b). For EO, we consider FL setting with only 2 agents. From Figure 3(b), we ﬁnd in B1 the accuracy is 87% for the Adult dataset with EO as 0.104. With B2, we obtain reduced accuracy of 80%, but EO reduces to 0.008. We ﬁnd similar trends in the baselines for the Bank (Figure 4(b)) and the Dutch datasets (Figure 5(b)).
When we compare the FPFL training, which also guarantees privacy, we observe a trade-oﬀ in fairness and accuracy. We note that ensuring EO, especially in the Bank dataset, is very challenging. Therefore, the trade-oﬀ is not as smooth. With decrease in , the accuracy decreases, but the EO values do not follow any trend. We believe this is due to the lack of distinct samples for each category after splitting the data (despite duplication) for FL.
Future Work. Our goal is to establish the FPFL framework, where a user can customize λ and σ to ensure the desired performance. The framework allows the use of any fairness measure of choice by appropriately modifying the loss in Phase 1. Exploring these on other relevant datasets and exploring other fairness [16,9] and privacy techniques [27] with varying number of clients is left for future work.
5 Conclusion
We provide a framework that learns fair and accurate models while preserving privacy. We refer to our novel framework as FPFL (Figure 2). We showed that decoupling the training process into separated phases for fairness and privacy allowed us to provide a DP guarantee for the training data and sensitive attributes

Federated Learning Meets Fairness and Diﬀerential Privacy

11

and reduce the number of training timesteps. We then applied FPFL on the Adult, Bank and Dutch datasets to highlight the relation between accuracy, fairness, and privacy of an FL model.

References
1. Abadi, M., Chu, A., Goodfellow, I., McMahan, H.B., Mironov, I., Talwar, K., Zhang, L.: Deep learning with diﬀerential privacy. In: Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. pp. 308–318 (2016)
2. Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., Wallach, H.: A reductions approach to fair classiﬁcation. In: Dy, J., Krause, A. (eds.) Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 80, pp. 60–69. PMLR, Stockholmsma¨ssan, Stockholm Sweden (10–15 Jul 2018), http://proceedings.mlr.press/v80/agarwal18a.html
3. Bagdasaryan, E., Poursaeed, O., Shmatikov, V.: Diﬀerential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems 32, 15479–15488 (2019)
4. Barocas, S., Selbst, A.D.: Big data’s disparate impact. Cal. L. Rev. 104, 671 (2016) 5. Berk, R., Heidari, H., Jabbari, S., Kearns, M., Roth, A.: Fairness in criminal
justice risk assessments: The state of the art. Sociological Methods & Research p. 0049124118782533 (2018) 6. Bilal Zafar, M., Valera, I., Gomez Rodriguez, M., Gummadi, K.P.: Fairness Constraints: Mechanisms for Fair Classiﬁcation. ArXiv e-prints (Jul 2015) 7. Chouldechova, A.: Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5 2, 153–163 (2017) 8. Cummings, R., Gupta, V., Kimpara, D., Morgenstern, J.: On the compatibility of privacy and fairness. In: Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization. pp. 309–315 (2019) 9. Du, W., Xu, D., Wu, X., Tong, H.: Fairness-aware agnostic federated learning. In: Proceedings of the 2021 SIAM International Conference on Data Mining (SDM). pp. 181–189. SIAM (2021) 10. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness through awareness. In: Proceedings of the 3rd innovations in theoretical computer science conference. pp. 214–226 (2012) 11. Dwork, C., Roth, A., et al.: The algorithmic foundations of diﬀerential privacy. Foundations and Trends in Theoretical Computer Science 9(3-4), 211–407 (2014) 12. Fang, H., Qian, Q.: Privacy preserving machine learning with homomorphic encryption and federated learning. Future Internet 13(4), 94 (2021) 13. Fredrikson, M., Jha, S., Ristenpart, T.: Model inversion attacks that exploit conﬁdence information and basic countermeasures. In: Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. pp. 1322–1333 (2015) 14. Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning. In: NIPS (2016) 15. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015) 16. Li, T., Sanjabi, M., Beirami, A., Smith, V.: Fair resource allocation in federated learning. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https: //openreview.net/forum?id=ByexElSYDr

12

Padala, Damle and Gujar

17. Madras, D., Creager, E., Pitassi, T., Zemel, R.S.: Learning adversarially fair and transferable representations. In: Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 1015, 2018. pp. 3381–3390 (2018), http://proceedings.mlr.press/v80/madras18a. html
18. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communicationeﬃcient learning of deep networks from decentralized data. In: Artiﬁcial Intelligence and Statistics. pp. 1273–1282. PMLR (2017)
19. Mohassel, P., Zhang, Y.: Secureml: A system for scalable privacy-preserving machine learning. In: 2017 IEEE Symposium on Security and Privacy (SP). pp. 19–38. IEEE (2017)
20. Mothukuri, V., Parizi, R.M., Pouriyeh, S., Huang, Y., Dehghantanha, A., Srivastava, G.: A survey on security and privacy of federated learning. Future Generation Computer Systems 115, 619–640 (2021)
21. Mozannar, H., Ohannessian, M., Srebro, N.: Fair learning with private demographic data. In: International Conference on Machine Learning. pp. 7066–7075. PMLR (2020)
22. Naseri, M., Hayes, J., De Cristofaro, E.: Toward robustness and privacy in federated learning: Experimenting with local and central diﬀerential privacy. arXiv preprint arXiv:2009.03561 (2020)
23. Padala, M., Gujar, S.: Fnnc: Achieving fairness through neural networks. In: Bessiere, C. (ed.) Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI-20. pp. 2277–2283. International Joint Conferences on Artiﬁcial Intelligence Organization (7 2020). https://doi.org/10.24963/ijcai.2020/315, https://doi.org/10.24963/ijcai.2020/315, main track
24. Pathak, M.A., Rane, S., Raj, B.: Multiparty diﬀerential privacy via aggregation of locally trained classiﬁers. In: NIPS. pp. 1876–1884. Citeseer (2010)
25. Shokri, R., Shmatikov, V.: Privacy-preserving deep learning. In: Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. pp. 1310–1321 (2015)
26. Tran, C., Fioretto, F., Van Hentenryck, P.: Diﬀerentially private and fair deep learning: A lagrangian dual approach. arXiv preprint arXiv:2009.12562 (2020)
27. Triastcyn, A., Faltings, B.: Federated learning with bayesian diﬀerential privacy. In: 2019 IEEE International Conference on Big Data (Big Data). pp. 2587–2596. IEEE (2019)
28. Wahab, O.A., Mourad, A., Otrok, H., Taleb, T.: Federated machine learning: Survey, multi-level classiﬁcation, desirable criteria and future directions in communication and networking systems. IEEE Communications Surveys & Tutorials (2021)
29. Wei, K., Li, J., Ding, M., Ma, C., Yang, H.H., Farokhi, F., Jin, S., Quek, T.Q.S., Poor, H.V.: Federated learning with diﬀerential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security 15, 3454–3469 (2020). https://doi.org/10.1109/TIFS.2020.2988575
30. Yang, Q., Liu, Y., Chen, T., Tong, Y.: Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST) 10(2), 1–19 (2019)
31. Zhang, C., Li, S., Xia, J., Wang, W., Yan, F., Liu, Y.: Batchcrypt: Eﬃcient homomorphic encryption for cross-silo federated learning. In: 2020 {USENIX} Annual Technical Conference ({USENIX}{ATC} 20). pp. 493–506 (2020)
32. Zˇliobaite, I., Kamiran, F., Calders, T.: Handling conditional discrimination. In: 2011 IEEE 11th International Conference on Data Mining. pp. 992–1001 (2011). https://doi.org/10.1109/ICDM.2011.72

