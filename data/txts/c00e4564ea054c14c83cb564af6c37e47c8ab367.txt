Centralized active tracking of a Markov chain with unknown dynamics
Mrigank Raman, Ojal Kumar, Arpan Chattopadhyay

arXiv:2010.16095v1 [cs.LG] 30 Oct 2020

Abstract—In this paper, selection of an active sensor subset for tracking a discrete time, ﬁnite state Markov chain having an unknown transition probability matrix (TPM) is considered. A total of N sensors are available for making observations of the Markov chain, out of which a subset of sensors are activated each time in order to perform reliable estimation of the process. The trade-off is between activating more sensors to gather more observations for the remote estimation, and restricting sensor usage in order to save energy and bandwidth consumption. The problem is formulated as a constrained minimization problem, where the objective is the long-run averaged mean-squared error (MSE) in estimation, and the constraint is on sensor activation rate. A Lagrangian relaxation of the problem is solved by an artful blending of two tools: Gibbs sampling for MSE minimization and an on-line version of expectation maximization (EM) to estimate the unknown TPM. Finally, the Lagrange multiplier is updated using slower timescale stochastic approximation in order to satisfy the sensor activation rate constraint. The on-line EM algorithm, though adapted from literature, can estimate vectorvalued parameters even under time-varying dimension of the sensor observations. Numerical results demonstrate approximately 1 dB better error performance than uniform sensor sampling and comparable error performance (within 2 dB bound) against complete sensor observation. This makes the proposed algorithm amenable to practical implementation.
Index terms— Active tracking, sensor selection, stochastic approximation, Gibbs sampling, on-line expectation maximization.
I. INTRODUCTION
Remote estimation of physical processes via sensor observations is an integral part of cyber-physical systems. These estimates are typically fed to some controller in order to control a physical process or system. Typical applications of remote estimation include object tracking, environment monitoring, industrial process monitoring and control, state estimation in smart grid, system identiﬁcation and disaster management. One key challenge is such remote estimation problems is that the sensors are required to perform high-quality sensing, control, communication, and tracking, but they are constrained in terms of energy and bandwidth availability. Hence, it is necessary to activate only the most informative sensor subset at each time, so that a good compromise is achieved between the ﬁdelity of the estimates and energy/bandwidth usage by sensors.
Herein, we consider the problem of designing a lowcomplexity algorithm for dynamically activating an optimal sensor subset, that minimizes the time-averaged MSE under a
Mrigank Raman and Ojal Kumar are with the Mathematics department, and Arpan Chattopadhyay is associated with the EE department and the Bharti School of Telecom Technology and Management, IIT Delhi. Email: {mt1170736, mt1170741 }@maths.iitd.ac.in, arpanc@ee.iitd.ac.in. This work was funded by the PDA and PLN6R budget head, IIT Delhi.

Figure 1. Remote state estimation with active sensing.
sensor activation rate constraint reﬂecting a constraint on the total energy consumed across sensors. The setup is centralized in the sense that sensors directly report their observations to a remote estimator; in distributed tracking, there can be multiple nodes, each individually estimating a process, via information exchanged over a multi-hop mesh network. In this paper, we consider centralized tracking of a Markov chain with unknown TPM, and solve the problem via a combination of Gibbs sampling, stochastic approximation, and on-line EM. We also work out the problem for the known TPM case. While these algorithms are motivated by solid theoretical consideration, numerical results demonstrate promising MSE performance despite reduced complexity.
A. Literature survey
Active sensor subset selection for process tracking may be either centralized or distributed. In centralized tracking, sensors send observations to a remote estimator which estimates the process. In distributed tracking, a number of nodes, each having a number of sensors, constitute a connected, multihop network; each node estimates the global state using its local sensor observation and the information coming from the adjacent nodes.
There have been considerable recent work on centralized active tracking of a process with several applications; see [1] for application on sensor networks, [2] for mobile crowdsensing, [3] for body-area sensing application and [4] for target tracking application. Even when the process does not have time-variation, calculating the estimation error given sensor observations and ﬁnding the best subset of sensors poses some serious technical challenges. To address these challenges, the authors of [1] provided a lower bound on performance, and used a greedy algorithm for subset selection. On the other hand, there have been a number of recent work on centralized active tracking of a time-varying process: [5] for single sensor selection by a centralized controller to track a Markov chain, Markov decision process formulation for sensor subset selection in [6] to track a Markov chain with known TPM, energy aware active sensing [7], existence and structure of active subset selection policy under linear quadratic Gaussian

(LQG) model [8], active sensing for a linear process with unknown Gaussian noise statistics via Thompson sampling [2], etc. The paper [9] considered the model where an observation is shared across multiple sensors. Recently, there have been a series of work for i.i.d. process tracking: [10] using Gibbs sampling based subset selection for an i.i.d. process with known distribution, [11] for learning an unknown parametric distribution of the process via stochastic approximation (see [12]), and an extended version [13] of these two papers. On the other hand, the authors of [14] have proposed an algorithm for distributed tracking a Markov chain with known TPM using tools from stochastic approximation, Gibbs Sampling and Kalman-consensus ﬁlter.
B. Our Contribution
Following points are our contribution in the paper: 1) We provide an online learning algorithm for active
sensing to track a Markov chain with unknown TPM. The unknown TPM is learnt via online EM (see [15]), suitably adapted for variable dimension of observations due to active sensing. Gibbs sampling (see [16]) is used for low-complexity sensor activation, and stochastic approximation is used for meeting the sensor activation constraint. 2) An interesting trick to handle variable dimension of observations was to maintain a global library of unknown parameter estimates, and update only the relevant ones in an asynchronous fashion, depending on sensor activation and observation sequence. This idea was absent in the online EM algorithm [15]. 3) Another interesting feature of the algorithm which we have proposed is running various iterates in multiple timescales.
C. Organisation
We have organised our paper in the following manner. In Section II, we have described system model. The necessary mathematical background is summarised in Section III. In Section IV we have provided active sensing, state estimation and learning algorithms. Numerical results are provided in Section V and ﬁnally we have conclusion in Section VI.

j = 1, 2, · · · , q}. The TPM of {x(t)}t≥0 is denoted by A (transpose of A), which is unknown to the remote estimator.
Note that:

x(t + 1) = Ax(t) + x(t + 1) − Ax(t)

(1)

.
=w(t)

where w(t) is a zero-mean process noise (non-Gaussian). At time t, let b(t) ∈ {0, 1}N denote the activation status
of N sensors; if bk(t) = 1, then the k-th sensor is active, otherwise it is inactive. Let b−k(t) ∈ {0, 1}N−1 be the same as b(t), except that bk(t) is removed. Also, let (b−k(t), 0) be the same as b(t) except that bk(t) = 0, and let us assume similar notation for (b−k(t), 1). An active sensor makes an observation and communicates that observation to the remote
estimator, whereas an inactive sensor does neither of these.
If bk(t) = 1 and if x(t) = ei, then the observation yk(t) from sensor k follows a Gaussian distribution as yk(t) ∼ N (µk,i, Qk,i) ∈ Rnk×1. Mathematically, we can write the observation coming from sensor i at time t as:

 Hkx(t) + vk(t) , bk(t) = 1, x(t) = ei 

yk(t) =

∼N (0,Qk,i)

(2)



∅,

bk(t) = 0

where Hk =. [µk,1 : µk,2 : · · · : µk,q] ∈ Rnk×q is the observation matrix of sensor k, and vk(t) denotes the Gaussian observation noise at sensor k. We assume that vk(t) is independent across sensors and time. We will consider the cases where (µk,i, Qk,i) can be either known or unknown.
The collection of observations {yk(t) : bk(t) = 1, k ∈ N }, arranged as a column vector via vertical concatenation, is called y(t). This y(t) is collected by the remote estimator to estimate xˆ(t) at each time t. The estimate xˆ(t) can be viewed as a belief vector on X .

B. The optimization problem
Let π˜1 and π˜2 denote two generic rules (deterministic or randomized) for sensor activation and process estimation, respectively. In this paper, we seek to solve the following problem:

II. SYSTEM MODEL
Bold capital and bold small letters will represent matrices and vectors respectively, whereas sets will be represented by calligraphic font throughout this paper.
A. Sensing and observation model
We consider a remote estimation setting as in Figure 1. The set of sensors is N = {1, 2, 3, ..., N }. The sensors are used to sense a discrete time process {x(t)}t≥0, which is a timehomogeneous positive recurrent Markov chain with q states. For the sake of mathematical convenience, we denote the j-th state as ej which is the j-th standard basis (column) vector of length q, with 1 at the j-th coordinate and 0 everywhere else. Hence, the state space of {x(t)}t≥0 becomes X := {ej :

such that

1 T −1

min lim sup

E||xˆ(t) − x(t)||2

π˜1,π˜2 T →∞ T t=0

lim sup T1 T −1 E||b(t)||1 ≤ N¯ (3)

T →∞

t=0

By standard Lagrange multiplier theory, problem (3) can be solved by solving the following relaxed problem:

1 T −1

min lim sup

E||(xˆ(t) − x(t)||2 + λ||b(t)||1)(4)

π˜1,π˜2 T →∞ T t=0

under a suitable Lagrange multiplier λ∗ such that the inequality constraint is met with equality.

III. BACKGROUND

In this section, we provide a basic background that will be useful in solving (4).
Gibbs sampling: Let us assume (for the sake of illustration) that x(t) is i.i.d. with known distribution. Then, there exists an optimal b∗ such that, using b(t) = b∗ over the entire time horizon along with MMSE estimation is optimal for (4). Thus, the problem reduces to:

min f (b) + λ||b||1

(5)

b∈{0,1}N .

=h(b)

where f (b) is the MSE under sensor activation vector b,

and h(b) is the cost under this activation vector. In order to avoid searching over 2N possible activation vectors, the

authors of [10] used Gibbs sampling for sensor activation.

Gibbs sampling generates a Markov chain {b(t)}t≥0 whose

stationary distribution is πβ(b) =.

e−βh(b)

with a

b˜∈{0,1}N e−βh(b˜)

parameter β > 0 interpreted as the inverse temperature in

statistical physics. Note that, limβ→∞ πβ(b∗) = 1 if the

unique minimizer for h(·) is b∗ . Hence, for sufﬁciently large

β, Gibbs sampling under steady state selects b∗ with high

probability, and we obtain a near-optimal solution of (4). At

any time t, Gibbs sampling randomly selects sensor k ∈ N

with uniform distribution, and sets bk(t) = 1 with probability e−βh(b−e−kβ,1h)(+b−e−kβ,1h)(b−k,0) and bk(t) = 0 otherwise. Then the kth sensor is activated accordingly, and the activation status

of other sensors remain unchanged. Finally, the constrained

problem (3) was solved by using a stochastic approximation (see [12]) iteration λ(t + 1) = λ(t) + a(t)(||b(t)||1 − N¯ ), to

satisfy the activation constraint.

Expectation maximization: The expectation maximization

(EM) algorithm (see [17]) is used to estimate an unknown

parameter θ from noisy observation y of a random vector

x having a parametric distribution with unknown parameter

θ. It maintains an iterate θ(t) at iteration t. In the E step,

E(log p(x|θ)|y, θ(t)) is computed, and this is maximized over

θ to obtain θ(t + 1) in the M step. It was shown in [17] that,

under certain regularity conditions, θ(t) converges to the set of stationary points such that ∂p∂(θy) = 0. Later, the authors of [15] proposed one online EM algorithm for hidden Markov

model; their model is similar to our process and observation

models, except that they do not consider active sensing and

consider scalar observations of ﬁxed dimension. However, due

to active sensing, our problem allows variable dimension of

observations, which requires some nontrivial modiﬁcation of

the algorithm of [15].

IV. THE GEM ALGORITHM
In this section, we propose an algorithm called GEM (Gibbs Expectation Maximization) to solve (3). Since the algorithm is technically involved, we will ﬁrst describe the major components and concepts related to the algorithm and ﬁnally provide a summarised version of the complete algorithm.

A. Key components of the algorithm 1) Some useful notation:

Symbols

Meaning

b(t) λ(t) Aˆt h(t)(b), f (t)(b) µˆk,i(t), Qˆk,i(t) Λˆ t Ψˆ i(t)
Mˆ ib(t)(t), Ψˆ bi (t)(t)

Activation status of N sensors
Estimate of the cost of a sensor activation
Running Estimate of Transpose of Transition Probability Matrix
Cost and MSE estimates under activation vector b
Running Estimate of mean and covariance of yk(t) where x(t) = ei Running estimate of the observation matrix [H1 : H2 : · · · : HN ] blkdiag(Qˆ1,i(t), Qˆ2,i(t), · · · , QˆN,i(t)) Running estimates of those components of Λˆ t and Ψˆ i(t), that correspond to the active sensors under activation vector b(t)

The proposed algorithm maintains running estimates µˆk,i(t)

and Qˆk,i(t) for µk,i and Qk,i, respectively. Equivalently, it

maintains

an .

estimate

Λˆ t

of

the

matrix

[H1

:

H2

:

···

:

HN ]

where Hk = [µk,1 : µk,2 : · · · : µk,q]. The algorithm also

maintains q block-diagonal matrices {Ψˆ i(t) : 1 ≤ i ≤ q} where Ψˆ i(t) =. blkdiag(Qˆ1,i(t), Qˆ2,i(t), · · · , QˆN,i(t)) (block

diagonal matrix consisting of these N covariance matrix

estimates at time t). For an activation vector b, we also deﬁne another matrix Ψˆ bi (t) =. blkdiag{Qˆk,i(t) : 1 ≤ k ≤ N, bk = 1} which can be extracted from Ψˆ i(t). Similarly, we deﬁne µˆbi (t) as an estimate of of the column vector vertcat(µˆk,i :
1 ≤ k ≤ N, bk = 1) (vertical concatenation of these column

vectors). Clearly, given x(t) = ei and activation vector b(t),

our

algorithm

assumes

that

y(t)

∼

N

(µˆ bi

(

t)

(t

),

Ψˆ

b i

(t

)

(

t))

.

For a given activation vector b we also deﬁne the matrix

Mˆ b(t) =. [µˆb(t) : µˆb(t) : · · · : µˆb(t)]. The algorithm also

1

2

q

maintains an estimate Aˆt for A.

The algorithm also maintains the iterates h(t)(b)∀b ∈

{0, 1}N , f (t)(b)∀b ∈ {0, 1}N , and λ(t) (see Section III),

as estimates of h(b)∀b ∈ {0, 1}N , f (b)∀b ∈ {0, 1}N , and

λ∗, respectively. Estimate of the MSE under activation vector

b can be denoted by f (t)(b)∀b ∈ {0, 1}N at any time t.

The quantities h(t)(b)∀b ∈ {0, 1}N are used as cost in

Gibbs sampling at time t to decide the sensor activation

set. The Lagrange multiplier λ(t) is updated using stochastic

approximation so that the activation constraint in (3) satisﬁes

the equality constrain. t. t We deﬁne νb(t) = τ=1 I{b(τ ) = b} as the number of
times the activation vector b is used up to time t.

2) Step sizes: The algorithm maintains two non-increasing,

positive step size sequences {α(t) : t = 0, 1, 2, · · · } and

{γ(t) : t = 0, 1, 2, · · · } for running multi-timescale stochastic

approximation updates. The step size sequences satisfy the

following properties: (i) ∞ t=0 s(t) = ∞, s ∈ {α, γ}, (ii)

∞ t=0 s2(t)

<

∞, s

∈

{α, γ},

(iii)

limt→∞

γ(t) α(t)

=

0.

The

ﬁrst

two requirements are standard for stochastic approximation.

The third condition is required for timescale separation; the f (t)(·) update uses step-size α(t), and the λ(t) update and

online EM updates will use step size γ(t).

3) Gibbs sampling: At time t, pick a random sensor jt ∈ N

uniformly and independently. For sensor jt, choose bjt (t) =

1 with probability pt =

e−βh(t)(b−jt (t−1),1)

e−βh(t)(b−jt (t−1),1)+e−βh(t)(b−jt (t−1),0)

and choose bjt (t) = 0 with probability (1−pt). For all k = jt,

we choose bk(t) = bk(t − 1). Activate sensors according to

b(t) and obtain the observations y(t).

4) λ(t) update: The Lagrange multiplier is updated as:

λ(t + 1) = [λ(t) + γ(t)(||b(t)||1 − N¯ )]l0

(6)

The iterates are projected onto an interval which is compact namely [0, l] to ensure boundedness, where l > 0 is a
sufﬁciently large number. The intuition behind this approach is that, if ||b(t)||1 > N¯ , then λ(t) (the cost of a sensor activation) is increased, and λ(t) is decreased otherwise.
5) State estimation: We use a Kalman-like state estimator
from [6], designed to track a Markov chain. In this algorithm, xˆt+1|t and xˆt|t denote the estimates of x(t + 1) and x(t) respectively, given {y(0), y(1), · · · , y(t)}; here xˆt|t is basically the ﬁnal estimate xˆ(t) declared by the estimator at time t, and xˆt|t−1 is an intermediate estimate at time t. Additionally, Σt|t and Σt|t−1 will denote covariance matrices estimates of the estimation and prediction errors (xˆt|t − x(t)) and (xˆt|t−1 −x(t)) respectively. Unlike standard Kalman ﬁlter where the observation dimension is ﬁxed, this Kalman-like
estimator has variable observation dimension depending on b(t), and the gain and error covariance matrix updates also take into account b(t) at time t.

State Estimation algorithm

Recursion: For each t ≥ 0, do:

1) xˆt|t−1 = Axˆt−1|t−1,
(Comment: State estimate at time t, keeping observations up to time

(t − 1) in account.)

2) yt|t−1 = Mˆ b(t)(t)xˆt|t−1,

3) Σt|t−1 = diag(xˆt|t−1) − xˆt|t−1xˆt|t−1,
(Comment: Estimate of error covariance matrix at time t, keeping

observations up to time (t − 1) in account.)

4) Ψ˜ t =

q i=1

xˆt|t−1(i)Ψˆ bi (t),

(Comment: Estimate of the observation noise covariance matrix under

activation vector b(t), averaged over the belief xˆt|t−1(.) on the state

of the process.)

5) Gt

=

Σt|t−1Mˆ b(t)(t))

∗

(Mˆ b(t)(t))Σt|t−1((Mˆ b(t)(t)) + Ψ˜ t)−1,

(Comment: Kalman gain update.)

6) Compute xˆt|t = xˆt|t−1 + Gt(y(t) − yt|t−1), and project

it on the probability simplex.

(Comment: State estimate at time t, keeping observations up to time

(t − 1) in account. Projection ensures that the estimate is a valid

probability belief vector on the state space.)
7) Σt|t = diag(xˆt|t) − xˆt|txˆt|t.
(Comment: Estimate of error covariance matrix at time t, keeping

observations up to time t in account.)

6) f (t)(·) update: At time t, MSE estimate f (t)(·) (for sensor subset b(t) only) is updated using the following equation:

f (t+1)(b) = [f (t)(b) + I{b = b(t)}α(νb(t))(Tr(Σt|t) − f (t)(b))]l0 (7)

7) Online EM for parameter estimation: Online EM requires an initial distribution π for the Markov chain. However, unlike [15], here we have vector-valued observations whose
dimension change over time. Also, the unknown parameters µˆbi (t) and Ψˆ bi (t) (known µˆbi (t) and Ψˆ bi (t) can be handled in a straightforward way) will also have different dimensions for different values of t. This requires an asynchronous update of
various components of the unknown parameters, depending on
the currently active sensors and their observations.
Since the online EM algorithm is heavy in notation, we have
made some explanatory comments in between the steps of the
algorithm. See [15] for a detailed understanding.
The algorithm requires the auxiliary functions φˆ, ρˆA, ρˆg, SˆA and Sˆg, and gt(ei, y). We also deﬁne y2(t) = blkdiag{yk(t)yk(t) : 1 ≤ k ≤ N, bk(t) = 1}. In general, we need yd(t) in the algorithm where d ∈ {0, 1, 2}; we deﬁne y0(t) = 1(scalar), andy1(t) =. y(t).

Online EM algorithm

Input: Initial distribution π of {x(t)}t≥0, y(t). Initialization: Initialize Aˆ0 Ψˆ i(0) for all 1 ≤ i ≤ q and Λˆ0
randomly and compute, for all 1 ≤ i, j, k ≤ q and 0 ≤ d ≤ 2,

φˆ0(k) = πq(k)g0(ek, y0) , g0(ek , y0)
k1 =1
ρˆA0 (i, j, k) = 0,

ρˆg (i,
0,d

k)

=

δik y d (0)

Recursion: For t ≥ 1, for all 1 ≤ i, j, k ≤ q and 0 ≤ d ≤ 2,

DO

Approx. Filter Update

q
φˆt−1(ek1 )Aˆt−1(ek1 , ek)gt−1(ek, y(t))

φˆt(ek) =

k1 =1 qq

φˆt−1(ek1 )Aˆt−1(ek1 , ek2 )gt−1(ek2 , y(t))

k1=1 k2=1

where gt(ei, y) =. exp[−(y − µˆbi (t))[Ψbi (t)]−1(y − µˆbi (t))T /2]

(Comment: Here φˆt(·) is interpreted as an estimate of the steady state probability distribution of the Markov chain given all observations up to time t.)
E-Step

1) ρˆAt (i, j, k)

=

γ(t)δjkrˆt(i|j) + (1 −

q

γ(t)) ρˆAt−1(i, j, k1)rˆt(k1|k)

k1 =1

2)

ρˆg (i, k)
t,d

=

γ(t)δikyd(t) + (1 −

q

γ(t))

ρˆg
t−

1,d

(i,

k1

)

rˆt

(k

1

|

k

)

k1 =1

φˆt−1(ei)Aˆt−1(ei, ej )

where rˆt(i|j) = q

.

φˆt−1(ei1 )Aˆt−1(ei1 , ej )

i1 =1

(Comment:

ρˆA t (·,

·,

·)

)

and

ρˆg (·,
t,d

·)

together

constitute

the

expectation

of

a

sufﬁcient statistic for the expected log-likelihood involved in the E step. The

sufﬁcient statistic is updated over time using Bayes’ theorem. Details can be

found in [15].)
M-Step

q
1) SˆtA(i, j) = ρˆAt (i, j, k1)φˆt(ek1 )

k1 =1

2) Aˆt(ei, ej) =

SˆtA(i, j)
q
SˆtA(i, j1)

j1 =1

(Comment: Update for the estimate of the transition probability

matrix.)

q

3)

Sˆg (i) =
t,d

ρˆg
t,d

(

i,

k1

)φˆt

(e

k1

)

k1 =1

b(t) Sˆtg,1(i)

4) µˆi = Sˆg (i)

t,0

(Comment: Update for the estimate of the sensor observation mean

for subset b(t).)

5) Ψˆ b(t) = Sˆtg,2(i) − µˆb(t)(µˆb(t))

i

Sˆg (i) i i

t,0

(Comment: Update for the estimate of the sensor observation covari-

ance for subset b(t).)

6) For every active sensor k such that bk(t) = 1 and for all 1 ≤ i ≤ q, modify the values of µˆk,i(t) in Λˆ t−1 and Qˆk,i(t) in Ψˆ i(t − 1) accordingly.

B. The complete algorithm
1) GEM algorithm: The complete GEM algorithm is out-
lined below. The Complete GEM Algorithm Input: {α(t)}t≥0, {γ(t)}t≥0, π, N¯ , β Initialization: Initialise b(0), λ(0) ≥ 0, Aˆ0, µˆk,i(0) and Qˆk,i(0) for all 1 ≤ k ≤ N, 1 ≤ i ≤ q,
xˆ0|−1 = xˆ0|0 = π. Recursion: For all t ≥ 0, DO:

1) Use Gibbs Sampling described in Section IV-A3 to

attain b(t), activate sensors accordingly, and collect the

corresponding observations y(t).

2) Update λ(t) as in Section IV-A4.

3) Compute xˆ(t) = xˆt|t as in Section IV-A5, by running the Kalman-like algorithm.
4) Calculate f (t)(b(t)) as in Section IV-A6. 5) Calculate h(t+1)(b) = f (t+1)(b) + λ(t + 1)||b||1 6) Compute the estimates Aˆt, µˆk,i(t) and Qˆk,i(t) for all
1 ≤ k ≤ N, 1 ≤ i ≤ q, by using online EM as in

Section IV-A7. 2) Discussion:

• The GEM algorithm runs in multiple timescales (see

[12]). Gibbs sampling corresponds to the fastest

timescale, and the λ(t) update and online EM run in

the slowest timescale. Timescale separation is ensured by

limt→∞

γ(t) α(t)

=

0.

• f (t)(·) update involves asynchronous stochastic approxi-
mation for various sensor subsets. • Projection on [0, l] is done to make sure that all iterates
remain bounded.
3) Computational complexity of GEM for every time t: At each time, Gibbs sampling and λ(t) update require O(1) computations. Updating f (t)(·) requires O(q) computations
for trace calculation. The approximate ﬁlter update step for all states will require O(qN 3 max1≤k≤N {n3k}) computations due to the matrix inversion involved in gt(·, ·) calculation. The ﬁrst step in E step requires O(q4) computations. Com-
putational complexity of other steps of online EM as well as
state estimation is dominated by these two steps and hence
the computational complexity of online EM at each time is O(qN 3 max1≤k≤N {n3k} + q4), where nk is the dimension of yk(t) as deﬁned before. The function h(t)(·) needs to be computed only for three vectors as required by Gibbs sampling, and hence this requires O(1) computations.

V. NUMERICAL RESULTS

We consider number of sensors N = 20, number of states

q = 10, activation constraint N¯ = 5, inverse temperature β =

1

1

10, α(t) = t0.7 , γ(t) = t0.8 and λ(0) = 0.1. The TPM A is

chosen randomly and then the rows are normalized to obtain

a stochastic matrix. The quantities {µk,i, Qk,i}1≤k≤N,1≤i≤q

are also chosen randomly. We have not considered larger value

of N because it is unlikely (even in current cyber-physical

systems) that so many sensors estimate a single physical

process, especially when the observation from each sensor is

a vector.

Under this setting, we compare the performance of the

following six algorithms:

1) GEM-K: Here the observation mean and covariances are known (i.e., K), but TPM is unknown.
2) GEM-UK: Here observation covariances are known, but observation mean and TPM are unknown (i.e., UK).
3) GEM-FO: This is GEM Algorithm with full observation. Here all the sensors are always active. The observation mean and covariances are known, but the TPM is unknown.
4) GEM-U: This is GEM Algorithm with uniform random sampling of sensors: at each time t, a sensor is actiN¯ vated independently with probability . The observaN tion mean and covariances are known, but the TPM is unknown.
5) GEM-FI: This is GEM with full information of the TPM, observation mean and covariances.
6) GEN: Here GEN stands for genie. At time t, the estimator perfectly knows x(t − 1), but no observation is available from sensors. In this case, the MSE will be the limiting variance of x(t) given x(t − 1).

A. Convergence of the algorithms Figure 2 shows the convergence of Aˆt to A for GEM-
K. Similarly, we have noticed that Aˆt converges to A for GEM-FO and GEM-U. However, the TPM estimate does not

0.25

0.2

(t) MSE(dB)

0.7
0.15
0.6

Error in TPM Estimation

0.5
0.1
0.4

0.3
0.05
0.2

0.1

0

0

1

2

3

4

5

t

0

0

0.5

1

1.5

2

t

Figure 2. Variation of the TPM estimation error 1t t for GEM-K.

t τ

=1

||A

−

Aˆτ

||F

with

Figure 4. Variation of λ(t) with t for GEM-K.

2.5

3

104

8

7

Mean number of active sensors

6

5

4

3

2

0

0.5

1

1.5

2

2.5

3

3.5

4

t

104

Figure 3. Variation of 1t

t τ =1

||b(τ )||1

with

t

for

GEM-K.

2

1

0

-1

-2

-3

-4

-5

-6
GEM-Un GEM-Uniform GEM-Fo GEM-K
-7

-8

0

0.5

1

1.5

2

2.5

3

t

104

Figure 5. MSE performance comparison among various algorithms.

converge to the true TPM for GEM-UK; instead, it converges

to some local optimum as guaranteed by the EM algorithm. For

all relevant algorithms, we have noticed that the mean number

of active sensors, calculated as 1t

t τ

=1

||b(τ

)||1

,

converges

to N¯ ; this has been illustrated only for GEM-K algorithm in

Figure 3 and the corresponding λ(t) variation is shown in

Figure 4. We observe that λ(t) converges at a slower rate.

B. Performance comparison
In Figure 5, we have compared the MSE performance of various algorithms. We observe that, the TPM estimate in GEM-K converges to the true TPM, and hence the asymptotic MSE performance of GEM-K and GEM-FI are same. Hence, we do not show the MSE performance of GEM-FI separately. Figure 5 shows that, GEN has the best MSE performance due to perfect knowledge of the previous state, and GEMUK has the worst MSE performance because it converges

to a local optimum. GEM-FO performs better than GEMK because it uses more sensor observation, but it cannot outperform GEN. GEM-U outperforms GEM-UK, since it has knowledge of observation mean and covariances. However, we note in multiple simulations that, on a fair comparison with GEM-K, GEM-U performs worse by approximately 1 dB; this shows the power of Gibbs sampling against uniform sampling.
We have repeated this for 10 different instances, and found that the ordering of MSE performances across algorithms remained unchanged, though the relative performance gaps among the algorithms varied. However, performance gap of GEM-K was observed to be within 2 dB of the MSE of GEMFO, and within several dB from the MSE of GEN. This shows that GEM is very useful for tracking a Markov chain.
VI. CONCLUSIONS
We have provided a low-complexity active sensor selection algorithm for centralized tracking of a Markov chain with

unknown transition probability matrix. The algorithm uses
Gibbs sampling, multi-timescale stochastic approximation,
online EM and Kalman-like state estimation to achieve a
good compromise among computational complexity, ﬁdelity of
estimate, and energy and bandwidth usage in state estimation.
Performance of the algorithm has been validated numerically.
We seek to prove convergence of the proposed algorithm, and
also extend this work for distributed tracking problems in our
future research.
REFERENCES
[1] D. Wang, J. W. Fisher III, and Q. Liu, “Efﬁcient observation selection in probabilistic graphical models using bayesian lower bounds.”
[2] F. Schnitzler, J. Yu, and S. Mannor, “Sensor selection for crowdsensing dynamical systems,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015, pp. 829–837.
[3] W. Hui, C. Hyeok-soo, A. Nazim, D. M. Jamal, and H. J. Won-Ki, “Information-based energy efﬁcient sensor selection in wireless body area networks,” in Communications (ICC), 2011 IEEE International Conference on. IEEE, 2011, pp. 1–6.
[4] F. R. Armaghani, I. Gondal, and J. Kamruzzaman, “Dynamic sensor selection for target tracking in wireless sensor networks,” in Vehicular Technology Conference (VTC Fall), 2011 IEEE. IEEE, 2011, pp. 1–6.
[5] V. Krishnamurthy and D. Djonin, “Structured threshold policies for dynamic sensor scheduling—a partially observed markov decision process approach,” IEEE Transactions on Signal Processing, vol. 55, no. 10, pp. 4938–4957, 2007.
[6] D. Zois, M. Levorato, and U. Mitra, “Active classiﬁcation for pomdps: A kalman-like state estimator,” IEEE Transactions on Signal Processing, vol. 62, no. 23, pp. 6209–6224, 2014.
[7] ——, “Energy-efﬁcient, heterogeneous sensor selection for physical activity detection in wireless body area networks,” IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1581–1594, 2013.
[8] W. Wu and A. Arapostathis, “Optimal sensor querying: General markovian and lqg models with controlled observations,” IEEE Transactions on Automatic Control, vol. 53, no. 6, pp. 1392–1405, 2008.
[9] V. Gupta, T. Chung, B. Hassibi, and R. Murray, “On a stochastic sensor selection algorithm with applications in sensor scheduling and sensor coverage,” Automatica, vol. 42, pp. 251–260, 2006.
[10] A. Chattopadhyay and U. Mitra, “Optimal sensing and data estimation in a large sensor network,” in GLOBECOM 2017-2017 IEEE Global Communications Conference. IEEE, 2017, pp. 1–7.
[11] ——, “Optimal active sensing for process tracking,” in 2018 IEEE International Symposium on Information Theory (ISIT). IEEE, 2018, pp. 551–555.
[12] V. S. Borkar, Stochastic approximation: a dynamical systems viewpoint. Cambridge University Press, 2008.
[13] A. Chattopadhyay and U. Mitra, “Dynamic sensor subset selection for centralized tracking of an iid process,” IEEE Transactions on Signal Processing, 2020.
[14] ——, “Active sensing for markov chain tracking,” in GlobalSIP 2018. IEEE, 11 2018, pp. 1050—1054.
[15] O. Cappé, “Online em algorithm for hidden markov models,” Journal of Computational and Graphical Statistics, vol. 20, no. 3, pp. 728–749, 2011.
[16] P. Brémaud, Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues. Springer Science & Business Media, 2013, vol. 31.
[17] B. Hajek, An Exploration of Random Processes for Engineers. Lecture Notes for ECE 534, 2011.

