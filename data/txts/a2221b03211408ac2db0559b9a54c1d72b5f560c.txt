arXiv:2008.00781v2 [eess.AS] 31 Jan 2021

MusiCoder: A Universal Music-Acoustic Encoder Based on Transformers
Yilun Zhao1,2∗ and Jia Guo2
1 Zhejiang University/University of Illinois at Urbana-Champaign Institute, Zhejiang University, Haining, China zhaoyilun@zju.edu.cn
2 YouKu Cognitive and Intelligent Lab, Alibaba Group, Hangzhou, China {yilun.zyl, gj243069}@alibaba-inc.com
Abstract. Music annotation has always been one of the critical topics in the ﬁeld of Music Information Retrieval (MIR). Traditional models use supervised learning for music annotation tasks. However, as supervised machine learning approaches increase in complexity, the increasing need for more annotated training data can often not be matched with available data. In this paper, a new self-supervised music acoustic representation learning approach named MusiCoder is proposed. Inspired by the success of BERT, MusiCoder builds upon the architecture of self-attention bidirectional transformers. Two pre-training objectives, including Contiguous Frames Masking (CFM) and Contiguous Channels Masking (CCM), are designed to adapt BERT-like masked reconstruction pre-training to continuous acoustic frame domain. The performance of MusiCoder is evaluated in two downstream music annotation tasks. The results show that MusiCoder outperforms the state-of-the-art models in both music genre classiﬁcation and auto-tagging tasks. The eﬀectiveness of MusiCoder indicates a great potential of a new self-supervised learning approach to understand music: ﬁrst apply masked reconstruction tasks to pre-train a transformer-based model with massive unlabeled music acoustic data, and then ﬁnetune the model on speciﬁc downstream tasks with labeled data.
Keywords: Music Information Retrieval · Self-supervised Representation Learning · Masked Reconstruction · Transformer
1 Introduction
The amount of music has been growing rapidly over the past decades. As an eﬀective measure for utilizing massive music data, automatically assigning one music clip a set of relevant tags, providing high-level descriptions about the music clip such as genre, emotion, theme, are of great signiﬁcance in MIR community [5, 39]. Some researchers have applied several supervised learning models [14,
Supported by Alibaba Group, and Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Zhejiang University

2

Y. Zhao, J. Guo

20, 22, 28], which are trained on human-annotated music data. However, the performance of supervised learning method are likely to be limited by the size of labeled dataset, which is expensive and time consuming to collect.
Recently, self-supervised pre-training models [23,24,31,37], especially BERT, dominate Natural Language Processing (NLP) community. BERT proposes a Masked Language Model (MLM) pre-training objective, which can learn a powerful language representation by reconstructing the masked input sequences in pre-training stage. The intuition behind this design is that a model available to recover the missing content should have learned a good contextual representation. In particular, BERT and its variants [25, 38, 40] have reached signiﬁcant improvements on various NLP benchmark tasks [36]. Compared with the text domain whose inputs are discrete word tokens, in acoustics domain, the inputs are usually multi-dimensional feature vectors (e.g., energy in multiple frequency bands) of each frame, which are continuous and smoothly changed over time. Therefore, some particular designs need to be introduced to bridge the gaps between discrete text and contiguous acoustic frames. We are the ﬁrst to apply the idea of masked reconstruction pre-training to the continuous music acoustic domain. In this paper, a new self-supervised pre-training scheme called MusiCoder is proposed, which can learn a powerful acoustic music representations through reconstructing masked acoustic frame sequence in pre-training stage.
Our contributions can be summarized as:
1. We present a new self-supervised pre-training model named MusiCoder. MusiCoder builds upon the structure of multi-layer bidirectional self-attention transformers. Rather than relying on massive human-labeled data, MusiCoder can learn a powerful music representation from unlabeled music acoustic data, which is much easier to collect.
2. The reconstruction procedure of BERT-like model is adapted from classiﬁcation task to regression task. In other word, MusiCoder can reconstruct continuous acoustic frames directly, which avoids an extra transformation from continuous frames to discrete word tokens before pre-training.
3. Two pre-training objectives, including Contiguous Frames Masking (CFM) and Contiguous Channels Masking (CCM), are proposed to pre-train MusiCoder. The ablation study shows that both CFM and CCM objectives can eﬀectively improve the performance of MusiCoder pre-training.
4. The MusiCoder is evaluated on two downstream tasks: GTZAN music genre classiﬁcation and MTG-Jamendo music auto-tagging. And MusiCoder outperforms the SOTA model in both tasks. The success of MusiCoder indicates a great potential of applying transformer-based masked reconstruction pretraining in Music Information Retrieval (MIR) ﬁeld.

2 Related Work
In the past few years, pre-training models and self-supervised representation learning have achieved great success in NLP community. Huge amount of self-

MusiCoder

3

Fig. 1. System overview of the MusiCoder model
supervised pre-training models based on multi-layer self-attention transformers [34], such as BERT [12], GPT [30], XLNet [38], Electra [9] are proposed. Among them, BERT is perhaps the most classic and popular one due to its simplicity and outstanding performance. Speciﬁcally, BERT is designed to reconstruct the masked input sequences in pre-training stage. Through reconstructing the missing content from a given masked sequence, the model can learn a powerful contextual representation.
More recently, the success of BERT in NLP community draws the attention of researchers in acoustic signal processing ﬁeld. Some pioneering works [2, 23, 24, 31, 37] have shown the eﬀectiveness of adapting BERT to Automatic

4

Y. Zhao, J. Guo

Speech Recognition (ASR) research. Speciﬁcally, they design some speciﬁc pretraining objectives to bridge the gaps between discrete text and contiguous acoustic frames. In vq-wav2vec [2], input speech audio is ﬁrst discretized to a K-way quantized embedding space by learning discrete representation from audio samples. However, the quantization process requires massive computing resources and is against the continuous nature of acoustic frames. Some works [7, 23, 24, 31, 37] design a modiﬁed version of BERT to directly utilize continuous speech. In [7,23,24], continuous frame-level masked reconstruction is adapted in BERT-like pre-training stage. In [37], SpecAugment [27] is applied to mask input frames. And [31] learns by reconstructing from shuﬄed acoustic frame orders rather than masked frames.
As for MIR community, representation learning has been popular for many years. Several convolutional neural networks (CNNs) based supervised methods [8, 14, 20, 22, 28] are proposed in music understanding tasks. They usually employ variant depth of convolutional layers on Mel-spectrogram based representations or raw waveform signals of the music to learn eﬀective music representation, and append fully connected layers to predict relevant annotation like music genres, tags. However, training such CNN-based models usually requires massive human-annotated data. And in [6, 17], researchers show that compared with supervised learning methods, using self-supervision on unlabeled data can signiﬁcantly improve the robustness of the model. Recently, the self-attention transformer has shown promising results in symbolic music generation area. For example, Music Transformer [18] and Pop Music Transformer [19] employ relative attention to capture long-term structure from music MIDI data, which can be used as discrete word tokens directly. However, compared with raw music audio, the size of existing MIDI dataset is limited. Moreover, transcription from raw audio to MIDI ﬁles is time-consuming and not accurate. In this paper, we proposed MusiCoder, a universal music-acoustic encoder based on transformers. Speciﬁcally, MusiCoder is ﬁrst pre-trained on massive unlabeled music acoustic data, and then ﬁnetuned on speciﬁc downstream music annotation tasks using labeled data.

3 MusiCoder Model
A universal transformer-based encoder named MusiCoder is presented for music acoustic representation learning. The system overview of proposed MusiCoder is shown in Fig. 1.

3.1 Input representation
For each input frame ti, its vector representation xi is obtained by ﬁrst projecting ti linearly to hidden dimension Hdim, and then added with sinusoidal positional encoding [34] deﬁned as following:
P E(pos,2i) = sin(pos/100002i/Hdim ) (1)
P E(pos,2i+1) = cos(pos/100002i/Hdim )

MusiCoder

5

The positional encoding is used to inject information about the relative position of acoustic frames. The design of positional encoding makes the transformer encoder aware of the music sequence order.

3.2 Transformer Encoder

A multi-layer bidirectional self-attention transformer encoder [34] is used to en-
code the input music acoustic frames. Speciﬁcally, a L-layer transformer is used to encode the input vectors X = {xi}Ni=1 as:

Hl = T ransf ormerl(Hl−1)

(2)

where

l

∈

[1, L],

H0

=

X

and

HL

=

[

hL1

,

...,

h

L N

].

We

use

the

hidden

vector

hLi

as the contextualized representation of the input token ti. The architecture of

transformer encoder is shown in Fig. 1.

3.3 Pre-training Objectives
The main idea of masked reconstruction pre-training is to perturb the inputs by randomly masking tokens with some probability, and reconstruct the masked tokens at the output. In the pre-training process, a reconstruction module, which consists of two layers of feed-forward network with GeLU activation [16] and layer-normalization [1], is appended to predict the masked inputs. The module uses the output of the last MusiCoder encoder layer as its input. Moreover, two new pre-training objectives are presented to help MusiCoder learn acoustic music representation.

Objective 1: Contiguous Frames Masking (CFM). To avoid the model exploiting local smoothness of acoustic frames, rather than only mask one span with ﬁxed number of consecutive frames [24], we mask several spans of consecutive frames dynamically. Given a sequence of input frames X = (x1, x2, ..., xn), we select a subset Y ⊂ X by iteratively sampling contiguous input frames (spans) until the masking budget (e.g., 15% of X) has been spent. At each iteration, the span length is ﬁrst sampled from a geometric distribution ∼ Geo(p). Then the starting point of the masked span is randomly selected. We set p = 0.2, min = 2 and max = 7. The corresponding mean length of span is around 3.87 frames (≈ 179.6ms). In each masked span, the frames are masked according to the following policy: 1) replace all frames with zero in 70% of the case. Since each dimension of input frames are normalized to have zero mean value, setting the masked value to zero is equivalent to setting it to the mean value. 2) replace all frames with a random masking frame in 20% of the case. 3) keep the original frames unchanged in the rest 10% of the case. Since MusiCoder will only receive acoustic frames without masking during inference time, policy 3) allows the model to receive real inputs during pre-training, and resolves the pretrain-ﬁntune inconsistency problem [12].

6

Y. Zhao, J. Guo

Objective 2: Contiguous Channels Masking (CCM). The intuition of channel masking is that a model available to predict the partial loss of channel information should have learned a high-level understanding along the channel axis. For log-mel spectrum and log-CQT features, a block of consecutive channels is randomly masked to zero for all time steps across the input sequence of frames. Speciﬁcally, the number of masked blocks, n, is ﬁrst sampled from {0, 1, . . . , H} uniformly. Then a starting channel index is sampled from {0, 1, . . . , H − n}, where H is the number of total channels.

Pre-training Objective Function.
Loss(x) = 0.5 · x2 if |x| < 1 (3) |x| − 0.5 otherwise
The Huber Loss [15] is used to minimize reconstruction error between masked input features and corresponding encoder output. Huber Loss is a robust L1 loss that is less sensitive to outliers. And in our preliminary experiments, we found that compared with L1 loss used in [24], using Huber loss will make the training process easier to converge.

3.4 MusiCoder Model Setting
We primarily report experimental results on two models: MusiCoderBase and MusiCoderLarge. The model settings are listed in Table 1. The number of Transformer block layers, the size of hidden vectors, the number of self-attention heads are represented as Lnum, Hdim, Anum, respectively.

Table 1. The proposed model settings

Lnum Hdim Anum #parameters

MusiCoderBase 4 768 12 MusiCoderLarge 8 1024 16

29.3M 93.1M

4 Experiment Setup
4.1 Dataset Collection and Preprocess Table 2. Statistics on the datasets used for pre-training and downstream tasks

Task

Datasets

#clips

duration (hours)

Description

Pre-training
Classiﬁcation Auto-tagging

Music4all FMA-large MTG-Jamendo1
GTZAN MTG-Jamendo2

109.2K 106.3K 37.3K
1000
18.4K

908.7 886.4 1346.9
8.3
157.1

– – –
100 clips for each genre
56 mood/theme tags

1,2 For MTG-Jamendo dataset, we removed music clips used in Auto-tagging task when pre-training.

MusiCoder

7

As shown in Table 2, the pre-training data were aggregated from three datasets: Music4all [13], FMA-Large [11] and MTG-Jamendo [4]. Both Music4all and FMA-Large datasets provide 30-seconds audio clips in .mp3 format for each song. And MTG-Jamendo dataset contains 55.7K music tracks, each with a duration of more than 30s. Since the maximum time stamps of MusiCoder is set to 1600, those music tracks exceeding 35s would be cropped into several music clips, the duration of which was randomly picked from 10s to 35s.
GTZAN music genre classiﬁcation [32] and MTG-Jamendo music auto-tagging tasks [4] were used to evaluate the performance of ﬁnetuned MusiCoder. GTZAN consists of 1000 music clips divided into ten diﬀerent genres (blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae & rock). Each genre consists of 100 music clips in .wav format with a duration of 30s. To avoid seeing any test data in downstream tasks, for pre-training data, we ﬁltered out those music clips appearing in downstream tasks.
Audio Preprocess. The acoustic music analysis library, Librosa [26], provides ﬂexible ways to extract features related to timbre, harmony, and rhythm aspect of music. In our work, Librosa was used to extract the following features from a given music clip: Mel-scaled Spectrogram, Constant-Q Transform (CQT), Mel-frequency cepstral coeﬃcients (MFCCs), MFCCs delta and Chromagram, as detailed in Table 4. Each kind of features was extracted at the sampling rate of 44,100Hz, with a Hamming window size of 2048 samples (≈ 46 ms) and a hop size of 1024 samples (≈ 23 ms). The Mel Spectrogram and CQT features were transformed to log amplitude with S = ln(10 · S + ), where S, represents the feature and an extremely small number, respectively. Then Cepstral Mean and Variance Normalization (CMVN) [29, 35] were applied to the extracted features for minimizing distortion caused by noise contamination. Finally these normalized features were concatenated to a 324-dim feature, which was later used as the input of MusiCoder.

Table 3. Acoustic features of music extracted by Librosa

Feature
Chromagram MFCCs
MFCCs delta Mel-scaled Spectrogram Constant-Q Transform

Characteristic
Melody, Harmony Pitch Pitch
Raw Waveform Raw Waveform

Dimension
12 20 20 128 144

4.2 Training Setup
All our experiments were conducted on 5 GTX 2080Ti and can be reproduced on any machine with GPU memory more than 48GBs. In pre-training stage, MusiCoderBase and MusiCoderLarge were trained with a batch size of 64 for 200k and 500k steps, respectively. We applied the Adam optimizer [21] with

8

Y. Zhao, J. Guo

β1 = 0.9, β2 = 0.999 and = 10−6. And the learning rate were varied with warmup schedule [34] according to the formula:
lrate = Hd−im 0.5 · min(step num−0.5, step num · warmup steps−1.5) (4)
where warmup steps was set as 8000. Moreover, library Apex was used to accelerate the training process and save GPU memory.
For downstream tasks, we performed an exhaustive search on the following sets of parameters. The model that performed the best on the validation set was selected. All the other training parameters remained the same as those in pre-training stage:

Table 4. Parameter settings for downstream tasks

Parameter
Batch size Learning Rate
Epoch Dropout Rate

Candidate Value
16, 24, 32 2e-5, 3e-5, 5e-5
2, 3, 4 0.05, 0.1

5 Results

5.1 Music Genre Classiﬁcation

Table 5. Results of GTZAN Music Classiﬁcation task

Models
hand-crafted features + SVM [3] CNN + SVM [8] CNN+MLP based ensemble [14] MusiCoderBase MusiCoderLarge Theoretical Maximum Score [32]

accuracy
87.9% 89.8% 94.2% 94.2% 94.3% 94.5%

Since GTZAN dataset only contains 1000 music clips, the experiments were conducted in a ten-fold cross-validation setup. For each fold, 80, 20 songs of each genre were randomly selected and placed into the training and validation split, respectively. The ten-fold average accuracy score is shown in Table 5. In prevoious work, [3] applied low-level music features and rich statistics to predict music genres. In [8], researchers ﬁrst used a CNN based model, which was trained on music auto-tagging tasks, to extract features. These extracted features were then applied on SVM [33] for genre classiﬁcation. In [14], the authors trained two models: a CNN based model trained on a variety of spectral and rhythmic features, and an MLP network trained on features, which were extracted from a model for music auto-tagging task. Then these two models were

MusiCoder

9

combined in a majority voting ensemble setup. The authors reported the accuracy score as 94.2%. Although some other works reported their accuracy score higher than 94.5%, we set 94.5% as the state-of-the-art accuracy according to the analysis in [32], which demonstrates that the inherent noise (e.g., repetitions, mis-labelings, distortions of the songs) in GTZAN dataset prevents the perfect accuracy score from surpassing 94.5%. In the experiment, MusiCoderBase and MusiCoderLarge achieve accuracy of 94.2% and 94.3%, respectively. The proposed models outperform the state-of-the-art models and achieve accuracy score close to the ideal value.

5.2 Music Auto-Tagging

Table 6. Results of MTG-Jamendo Music Auto-tagging task

Models
VQ-VAE+CNN [20] VGGish [4] CRNN [22] FA-ResNet [22] SampleCNN (reproduced) [28] Shake-FA-ResNet [22]
MusiCoderBase w/o pre-training MusiCoderBase with CCM Ours MusiCoderBase with CFM MusiCoderBase with CFM+CCM MusiCoderLarge with CFM+CCM

ROC-AUC macro
72.07% 72.58% 73.80% 75.75% 76.93% 77.17%
77.03% 81.93% 81.38% 82.57% 83.82%

PR-AUC macro
10.76% 10.77% 11.71% 14.63% 14.92% 14.80%
15.02% 19.49% 19.51% 20.87% 22.01%

For the music auto-tagging task, two sets of performance measurements, ROC-AUC macro and PR-AUC macro, were applied. ROC-AUC can lead to over-optimistic scores when data are unbalanced [10]. Since the music tags given in the MTG-Jamendo dataset are highly unbalanced [4], the PR-AUC metric was also introduced for evaluation. The MusiCoder model was compared with other state-of-the-art models competing in the challenge of MediaEval 2019: Emotion and Theme Recognition in Music Using Jamendo [4]. We used the same train-valid-test data splits as the challenge. The results are shown in Table 6. For VQ-VAE+CNN [20], VGGish [4], CRNN [22], FA-ResNet [22], Shake-FAResNet [22] models, we directly used the evaluation results posted in the competition leaderboard3. For SampleCNN [28], we reproduced the work according to the oﬃcial implementation4. As the results suggest, the proposed MusiCoder model achieves new state-of-the-art results in music auto-tagging task.
3 https://multimediaeval.github.io/2019-Emotion-and-Theme-Recognition-in-Music-Task/ results
4 https://github.com/tae-jun/sample-cnn

10

Y. Zhao, J. Guo

Ablation Study. Ablation study were conducted to better understand the performance of MusiCoder. The results are also shown in Table 6. According to the experiemnt, even without pre-training, MusiCoderBase can still outperform most SOTA models, which indicates the eﬀectiveness of transformer-based architecture. When MusiCoderBase is pre-trained with objective CCM or CFM only, a signiﬁcant improvement over MusiCoderBase without pre-training is observed. And MusiCoderBase with CCM and CFM pre-training objectives combined achieves better results. The improvement indicates the eﬀectiveness of pre-training stage. And it shows that the designed pre-training objectives CCM and CFM are both the key elements that drives pre-trained MusiCoder to learn a powerful music acoustic representation. We also explore the eﬀect of model size on downstream task accuracy. In the experiment, MusiCoderLarge outperforms MusiCoderBase, which reﬂects that increasing the model size of MusiCoder will lead to continual improvements.

6 Conclusion
In this paper, we propose MusiCoder, a universal music-acoustic encoder based on transformers. Rather than relying on massive human labeled data which is expensive and time consuming to collect, MusiCoder can learn a strong music representation from unlabeled music acoustic data. Two new pre-training objectives Contiguous Frames Masking (CFM) and Contiguous Channel Masking (CCM) are designed to improve the pre-training stage in continuous acoustic frame domain. The eﬀectiveness of proposed objectives is evaluated through extensive ablation studies. Moreover, MusiCoder outperforms the state-of-theart model in music genre classiﬁcation on GTZAN dataset and music autotagging on MTG-Jamendo dataset. Our work shows a great potential of adapting transformer-based masked reconstruction pre-training scheme to MIR community. Beyond improving the model, we plan to extend MusiCoder to other music understanding tasks (e.g., music emotion recognition, chord estimation, music segmentation). We believe the future prospects for large scale representation learning from music acoustic data look quite promising.

References
1. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint arXiv:1607.06450 (2016)
2. Baevski, A., Schneider, S., Auli, M.: vq-wav2vec: Self-supervised learning of discrete speech representations. arXiv preprint arXiv:1910.05453 (2019)
3. Baniya, B.K., Lee, J., Li, Z.N.: Audio feature reduction and analysis for automatic music genre classiﬁcation. In: 2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC). pp. 457–462. IEEE (2014)
4. Bogdanov, D., Won, M., Tovstogan, P., Porter, A., Serra, X.: The mtg-jamendo dataset for automatic music tagging. In: Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019). Long Beach, CA, United States (2019), http://hdl.handle.net/10230/42015

MusiCoder

11

5. Bu, J., Tan, S., Chen, C., Wang, C., Wu, H., Zhang, L., He, X.: Music recommendation by uniﬁed hypergraph: combining social media information and music content. In: Proceedings of the 18th ACM international conference on Multimedia. pp. 391–400 (2010)
6. Carmon, Y., Raghunathan, A., Schmidt, L., Duchi, J.C., Liang, P.S.: Unlabeled data improves adversarial robustness. In: Advances in Neural Information Processing Systems. pp. 11192–11203 (2019)
7. Chi, P.H., Chung, P.H., Wu, T.H., Hsieh, C.C., Li, S.W., Lee, H.y.: Audio albert: A lite bert for self-supervised learning of audio representation. arXiv preprint arXiv:2005.08575 (2020)
8. Choi, K., Fazekas, G., Sandler, M., Cho, K.: Transfer learning for music classiﬁcation and regression tasks. arXiv preprint arXiv:1703.09179 (2017)
9. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 (2020)
10. Davis, J., Goadrich, M.: The relationship between precision-recall and roc curves. In: Proceedings of the 23rd international conference on Machine learning. pp. 233– 240 (2006)
11. Deﬀerrard, M., Benzi, K., Vandergheynst, P., Bresson, X.: Fma: A dataset for music analysis. arXiv preprint arXiv:1612.01840 (2016)
12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)
13. Domingues, M., Pegoraro Santana, I., Pinhelli, F., Donini, J., Catharin, L., Mangolin, R., Costa, Y., Feltrim, V.D.: Music4all: A new music database and its applications (07 2020). https://doi.org/10.1109/IWSSIP48289.2020.9145170
14. Ghosal, D., Kolekar, M.H.: Music genre recognition using deep neural networks and transfer learning. In: Interspeech. pp. 2087–2091 (2018)
15. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 1440–1448 (2015)
16. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016)
17. Hendrycks, D., Mazeika, M., Kadavath, S., Song, D.: Using self-supervised learning can improve model robustness and uncertainty. In: Advances in Neural Information Processing Systems. pp. 15663–15674 (2019)
18. Huang, C.Z.A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., Dai, A.M., Hoﬀman, M.D., Dinculescu, M., Eck, D.: Music transformer: Generating music with long-term structure. In: International Conference on Learning Representations (2018)
19. Huang, Y.S., Yang, Y.H.: Pop music transformer: Generating music with rhythm and harmony. arXiv preprint arXiv:2002.00212 (2020)
20. Hung, H.T., Chen, Y.H., Mayerl, M., Zangerle, M.V.E., Yang, Y.H.: Mediaeval 2019 emotion and theme recognition task: A vq-vae based approach
21. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)
22. Koutini, K., Chowdhury, S., Haunschmid, V., Eghbal-zadeh, H., Widmer, G.: Emotion and theme recognition in music with frequency-aware rf-regularized cnns. arXiv preprint arXiv:1911.05833 (2019)
23. Ling, S., Liu, Y., Salazar, J., Kirchhoﬀ, K.: Deep contextualized acoustic representations for semi-supervised speech recognition. In: ICASSP 2020-2020 IEEE

12

Y. Zhao, J. Guo

International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6429–6433. IEEE (2020) 24. Liu, A.T., Yang, S.w., Chi, P.H., Hsu, P.c., Lee, H.y.: Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders. In: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6419–6423. IEEE (2020) 25. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019) 26. McFee, B., Raﬀel, C., Liang, D., Ellis, D.P., McVicar, M., Battenberg, E., Nieto, O.: librosa: Audio and music signal analysis in python. In: Proceedings of the 14th python in science conference. vol. 8, pp. 18–25 (2015) 27. Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D., Le, Q.V.: Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779 (2019) 28. Pons, J., Nieto, O., Prockup, M., Schmidt, E., Ehmann, A., Serra, X.: End-to-end learning for music audio tagging at scale. arXiv preprint arXiv:1711.02520 (2017) 29. Pujol, P., Macho, D., Nadeu, C.: On real-time mean-and-variance normalization of speech recognition features. In: 2006 IEEE international conference on acoustics speech and signal processing proceedings. vol. 1, pp. I–I. IEEE (2006) 30. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf (2018) 31. Song, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., Meng, H.: Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv preprint arXiv:1910.10387 (2019) 32. Sturm, B.L.: The gtzan dataset: Its contents, its faults, their eﬀects on evaluation, and its future use. arXiv preprint arXiv:1306.1461 (2013) 33. Suykens, J.A., Vandewalle, J.: Least squares support vector machine classiﬁers. Neural processing letters 9(3), 293–300 (1999) 34. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998–6008 (2017) 35. Viikki, O., Laurila, K.: Cepstral domain segmental feature vector normalization for noise robust speech recognition. Speech Communication 25(1-3), 133–147 (1998) 36. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Glue: A multitask benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 (2018) 37. Wang, W., Tang, Q., Livescu, K.: Unsupervised pre-training of bidirectional speech encoders via masked reconstruction. In: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6889–6893. IEEE (2020) 38. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet: Generalized autoregressive pretraining for language understanding. In: Advances in neural information processing systems. pp. 5753–5763 (2019) 39. Zhang, K., Zhang, H., Li, S., Yang, C., Sun, L.: The pmemo dataset for music emotion recognition. In: Proceedings of the 2018 acm on international conference on multimedia retrieval. pp. 135–142 (2018) 40. Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q.: Ernie: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129 (2019)

