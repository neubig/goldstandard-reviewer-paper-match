Tolling for Constraint Satisfaction in Markov Decision Process Congestion Games
Sarah H. Q. Li1, Yue Yu1, Daniel Calderone2, Lillian Ratliff2, Behc¸et Ac¸ıkmes¸e1

arXiv:1903.00747v1 [cs.GT] 2 Mar 2019

Abstract— Markov Decision Process (MDP) congestion game is an extension of classic congestion games, where a continuous population of selﬁsh agents each solve a Markov decision processes with congestion: the payoff of a strategy decreases as more population uses it. We draw parallels between key concepts from capacitated congestion games and MDPs. In particular, we show that the population mass constraints in MDP congestion games are equivalent to imposing tolls/incentives on the reward function, which can be utilized by a social planner to achieve auxiliary objectives. We demonstrate such methods on a simulated Seattle ride-share model, where tolls and incentives are enforced for two distinct objectives: to guarantee minimum driver density in downtown Seattle, and to shift the game equilibrium towards a maximum social output.
I. INTRODUCTION
We consider a class of non-cooperative games, Markov decision process congestion games (MDPCG) [1], [2], which combine features of classic nonatomic routing games [3]– [5]—i.e. games where a continuous population of agents each solve a shortest path problem—and stochastic games [6], [7]—i.e. games where each agent solves a Markov decision process (MDP). In MDP congestion games, similar to mean ﬁeld games with congestion effects [8], [9], a continuous population of selﬁsh agents each solve an MDP with congestion effects on its state-action rewards: the payoff of a strategy decreases as more population mass chooses it. An equilibrium concept for MDPCG’s akin to the Wardrop equilibrium [3] for routing games was introduced in [1].
In this paper, we consider modifying MDPCG’s game rewards to enforce artiﬁcial state constraints that may arise from a system level. For example, in a trafﬁc network with selﬁsh users, tolls can be used to lower the trafﬁc in certain neighbourhoods to decrease ambient noise. Drawing on techniques from capacitated routing games [10], [11] and constrained MDPs [12], [13], we derive reward modiﬁcation methods that shifts the game equilibrium mass distribution. Alternatively, constraints may arise in the following scenario: central agent, which we denote by a social planner, may enforce constraints to improve user performance as measured by an alternative objective. Equilibria of MDPCGs have been shown to exhibit similar inefﬁciencies to classic routing games [14], [15]. As in routing games, we show how reward adjustments can minimize the gap between the equilibrium distribution and the socially optimal distribution [16], [17].
*This work was is supported by NSF award CNS-1736582. 1Authors are with the William E. Boeing Department of Aeronautics and Astronautics, University of Washington, Seattle. sarahli@uw.edu yueyu@uw.edu behcet@uw.edu 2Authors are with the Department of Electrical Engineering, University of Washington, Seattle. ratliffl@uw.edu djcal@uw.edu

Since MDPCG models selﬁsh population behaviour under stochastic dynamics, our constraint enforcing methods can be considered as an incentive design framework. One practical application in particular is modifying the equilibrium behaviour of ride-sharing drivers competing in an urban setting. Ride-share has become a signiﬁcant component of urban mobility in the past decade [18]. As data becomes more readily available and computation more automated, drivers will have the option of employing sophisticated strategies to optimize their proﬁts—e.g. as indicated in popular media, there are a number of mechanisms available to support strategic decision-making by ride-sharing drivers [19]–[21]. This provides the need for game theoretic models of ridesharing competition [22]: while rational drivers only seek to optimize their individual proﬁts, ride-sharing companies may choose to incentivize driver behaviours that are motivated by other objectives, such as maintaining driver coverage over large urban areas with varied rider demand as well as increasing overall proﬁts.
The rest of the paper is organized as follows. Section II provides a discussion of related work. In Section III, we introduce the optimization model of MDPCG’s and highlight the relationship between the classical congestion game equilibrium—i.e. Wardrop equilibrium—and Q-value functions from MDP literature. Section IV-A shows how a social planner can shift the game equilibrium through reward adjustments. Section IV-B adopts the Frank-Wolfe numerical method [23] to solve the game equilibrium and provides an online interpretation of Frank-Wolfe in the context of MDPCG. Section V provides an illustrative application of MDPCG, in which agents repeatedly play a ride-share model game in the presence of population constraints as well as improving the social welfare. Section VI concludes and comments on future work.
II. RELATED WORK
Stochastic population games were ﬁrst studied in the literature as anonymous sequential games [24]–[27]. Recent developments in stochastic population games has been in the mean ﬁeld game [8], [28] community. Our work is related to potential mean ﬁeld games [8], [29] in discrete time, discrete state space [30] and mean ﬁeld games on graphs [9], [31].
Our work can also be thought of as a continuous population potential game [32] where the strategies are policies of an MDP or as a modiﬁcation of classic nonatomic routing games [5] where routes have been replaced by policies.
Techniques for cost modiﬁcation to satisfy capacity constraints in nonatomic routing games were developed in [10]. See also [5, Sec. 2.8.2] for a discussion of tolling to enforce

side-constraints and [5, Sec. 2.4] for a discussion of tolling to improve social welfare in routing games.

III. MDP CONGESTION GAMES

We consider a continuous population of selﬁsh agents each

solving a ﬁnite-horizon MDP with horizon of length T , ﬁnite

state space S, and ﬁnite action space A. We use the notation

[T ] = {1, . . . , T } to denote the integer set of length T .

The population mass distribution, y ∈ RT ×|S|×|A|, is

deﬁned for each time step t ∈ [T ], state s ∈ S, and action

a ∈ A. ytsa ∈ R is the population mass in state s taking

action a at time t, and in state s at time t.

a ytsa is the total population mass

Let P ∈ R(T −1)×|S|×|S|×|A| be a stochastic transition ten-

sor. Pts sa ∈ R deﬁnes the probability of ytsa transitioning to

state s in stage t + 1 when action a is chosen. The transition

tensor P is deﬁned such that

Pts sa ≥ 0 ∀ s , s ∈ S, a ∈ A, t ∈ [T ]

and Pts sa = 1 ∀ s ∈ S, t ∈ [T ]
s ∈S a∈A
The population mass distribution obeys the stochastic mass propogation equation

y0sa = ps,
a∈A

∀ s∈S

yt+1,sa =

Ptss ayts a, ∀ t ∈ [T − 1]

a∈A

s ∈S a∈A

where ps is the initial population mass in state s.

The reward of each time-state-action triplet is given by

a

function

rtsa

:

T ×|S|×|A|
R+

→

R.

rtsa(y)

is

the

reward

for taking action a in state s at time t for given population

distribution y. One important case is where rtsa(y) simply

depends on ytsa, i.e. there exists functions tsa : R+ → R

such that

rtsa(y) = tsa(δtTsay)

(2)

where δtsa is an indicator vector for (t, s, a) such that δtTsay = ytsa. We say the game is a congestion game if the rewards have the form of (2) and the functions tsa(ytsa)
satisfy the following assumption.

Assumption 1. tsa(ytsa) is a strictly decreasing continuous function of ytsa for each t, s, a.

Intuitively, the reward of each time-state-action triplet decreases as more members of the population choose that state-action pair at that time. We will use r(y) or (y) to refer to the tensor of all reward functions in each case.
Each member of the population solves an MDP with population dependent rewards rtsa(y). As in the MDP literature, we deﬁne Q-value functions for each (t, s, a) pair as



rtsa(y) + Pts sa

Qtsa =

s

rtsa(y)

max Qt+1,s a
a

t ∈ [T − 1]
t=T (3)

In the game context, Q-value function Qtsa(y) represents the distribution dependent payoff that the population receives when choosing action a at (t, s). The Q-value functions can be used to deﬁne an equilibrium akin to the Wardrop equilibrium of routing games [1].

Deﬁnition 1 (Wardrop Equilibrium [1]). A population distribution over time-state-action triplets, {ytsa}t∈[T ],s∈S,a∈A is an MDP Wardrop equilbrium for the corresponding MDPCG, if for any (s, t), ytsa > 0 implies

Qtsa ≥ Qtsa ∀a = a, a ∈ A

(4)

Intuitively, deﬁnition 1 amounts to the fact that at every state and time, population members only choose actions that are optimal.
When game rewards satisfy assumption 1, MDPCG can be characterized as a potential game.

Deﬁnition 2 (Potential Game [1], [32]). We say that the MDPCG associated with rewards r(y) is a potential game if there exists a continuously differentiable function F such that
∂F ∂ytsa = rtsa(y)

In the speciﬁc case when the rewards have form (2), we

can use the potential function

ytsa

F (y) =

tsa(x) dx

(5)

t∈T s∈S a∈A 0

As shown in [1, Theorem 1.3] given a potential function F (y), the equilibrium to the ﬁnite horizon MDPCG can be found by solving the following optimization problem for an initial population distribution p.

max F (y)
y

s.t.

yt+1,sa =

Ptss ayts a, ∀ t ∈ [T − 1],

a∈A

s ∈S a∈A

y0sa = ps, ∀ s ∈ S,

a∈A
ytsa ≥ 0, ∀ s ∈ S, a ∈ A, t ∈ [T ]

The proof that the optimizer of (6) is a Wardrop equilibrium relies on the fact that the Q-value functions (3) are encoded in the KKT optimality conditions of the problem. The equilibrium condition (4) is then speciﬁcally derived from the complementary slackness condition [1]. When F (y) has form (5) and Assumption 1 is satisﬁed, F (y) is strictly concave, and MDPCG (6) has a unique Wardrop equilibrium.

IV. CONSTRAINED MDPCG
In this section, we analyze the problem of shifting the game equilibrium by augmenting players’ reward functions. In Section IV-A, we show that introducing constraints cause the optimal population distribution to obey Wardrop equilibrium for a new set of Q-value functions. Section IVB outlines the Frank Wolfe numerical method for solving a constrained MDPCG as well as provides a population behavioural interpretation for the numerical method.

A. Planning Perspective: Model and Constraints

The Wardrop equilibrium of an MDP congestion game is given by (6). The planner may use additional constraints to achieve auxiliary global objectives. For example, in a city’s trafﬁc network, certain roads may pass through residential neighbourhoods. A city planner may wish to artiﬁcially limit trafﬁc levels to ensure residents’ wellbeing.
We consider the case where the social planner wants the equilibrium population distribution to satisfy constraints of the form

gi(y) ≥ 0 ∀i ∈ I

(7)

where gi are continuously differentiable concave functions.
The social planner cannot explicitly constrain players’ behaviour, but rather seeks to add incentive functions {ftisa}i∈I to the reward functions (y) in order to shift the equilibrium to be within the constrained set deﬁned by (7). The modiﬁed
rewards have form

r¯tsa(y) = rtsa(y) + ftisa(y)

(8)

i∈I

To determine the incentive functions, the social planner ﬁrst solves the constrained optimization problem

max F (y)
y

s.t.

yt+1,sa =

Ptss ayts a, ∀ t ∈ [T − 1],

a∈A

s ∈S a∈A

y0sa = ps, ∀ s ∈ S

a∈A

ytsa ≥ 0, ∀ s ∈ S, a ∈ A, t ∈ [T ]

gi(y) ≥ 0, ∀ i ∈ I

(9a)

and then computes the incentive functions as

f i (y) = (τ i) ∂gi (y)

(10)

tsa

∂ytsa

where {(τ i) ∈ R+}i∈I are the optimal Lagrange multipliers associated with the additional constraints (7).
The following theorem shows that the Wardrop equilibrium of the MDPCG with modiﬁed rewards in (8) satisﬁes the new constraints in (9a).
Theorem 1. Let the MDPCG (6) with rewards r(y) be a potential game with a strictly concave potential function F (y). If y is a Wardrop equilibrium for a modiﬁed MDPCG with reward functions

r¯ (y) = r (y) + (τ i) ∂gi (y)

(11)

tsa

tsa

∂ytsa

i∈I

then y also solves (9) and thus satisﬁes the additional constraints (7).

Proof. The Lagrangian of (9) is given by

L(y, µ, V, τ ) = F (y) − µtsaytsa + τ igi(y)

tsa

i

T −1

+

Pt,ss ayt,s a − yt+1,sa Vt+1,s (12)

t=1 s as

a

+
s

ps − y1sa V1s
a

and note that by strict concavity

sup inf L(y, µ, V, τ )
y≥0 µ≥0,V,τ ≥0

has unique solution, which we denote by (y , µ , V , τ ). We then note that

F¯(y) = F (y) + (τ i) gi(y)

(13)

i

is a potential function for the MDPCG with modiﬁed rewards (11). Since F (y) is strictly concave, gi(y) is concave, and (τ i) is positive, F¯(y) is strictly concave. The equilibrium
for the MDPCG with modiﬁed rewards can be computed by solving (9) with F¯(y) as the objective.
The Lagrangian for (9) with F¯(y) is given by

L¯(y, µ, V ) = L(y, µ, V, τ )

(14)

Again by strict concavity

sup inf L¯(y, µ, V ) = sup inf L(y, µ, V, τ ) (15)

y≥0 µ≥0,V

y≥0 µ≥0,V

has a unique solution which we denote as (y¯ , µ¯ , V¯ ). It follows that y¯ = y . Thus the equilibrium of the game with modiﬁed rewards, y¯ satisﬁes gi(y¯ ) = gi(y ) ≥ 0 as
desired.

For the social planner, Theorem 1 has the following interpretation: in order to impose constraints of form (7) on a MDPCG, the planner could solve the constrained game (9) for optimal dual variables τ and offer incentives of form (10).

B. Population Perspective: Numerical Method
After the social planner has offered incentives, the population plays the Wardrop equilibrium deﬁned by modiﬁed rewards (8); this equilibrium can be computed using the Frank Wolfe (FW) method [23], given in Algorithm 3, with known optimal variables {τi }.
FW is a numerical method for convex optimization problems with continuously differentiable objectives and compact feasible sets [33], including routing games. One advantage of this learning paradigm is that the population does not need to know the function r(·). Instead, they simply react to the realized rewards of previous game at each iteration. It also provides an interpretation for how a Wardrop equilibrium might be asymptotically reached by agents in MDPCG in an online fashion.
Assume that we have a repeated game play, where players execute a ﬁxed strategy determined at the start of each

game. At the end of each game k, rewards of game k based on ytksa are revealed to all players. FW models the population as having two sub-types: adventurous and conservative. Upon receiving reward information tsa(ytksa), the adventurous population decides to change its strategy while the conservative population does not. To determine its new strategy, the adventurous population uses value iteration on the latest reward information—i.e. Algorithm 1—to compute a new optimal policy. Their resultant density trajectory is then computed using Algorithm 2. The step size at each iteration is equivalent to the fraction of total population who switches strategy. The stopping criteria for the FW algorithm is determined by the Wardrop equilibrium notion—that is, as the population iteratively gets closer to an optimal strategy, the marginal increase in potential decreases to zero.

Algorithm 1 Value Iteration Method
Input: r, P . Output: {πts}t∈[T ], s∈S
for t = T, . . . , 1 do for each s ∈ S do Vts = max Qtsa
a∈A
πts = argmax Qtsa
a∈A
end for
end for

Eqn. (3) Eqn. (3)

Algorithm 2 Retrieving density trajectory from a policy

Input: P , p, π.

Output: {dtsa}t∈[T ],s∈S,a∈A dtsa = 0, ∀ t ∈ [T ], s ∈ S, a ∈ A

for t = 1, . . . , T do

for each s ∈ S do

if t = 1 then

d1sπ1s = ps

else

dts(πts) =

Pt−1,ss adt−1,s a

a∈A s ∈S

end if

end for

end for

In contrast to implementations of FW in routing game literature, Algorithm 3’s descent direction is determined by solving an MDP [34, Section 4.5] as opposed to a shortest path problem from origin to destination [5, Sec.4.1.3]. Algorithm 3 is guaranteed to converge to a Wardrop equilibirum if the predetermined step sizes decrease to zero as a harmonic series [23]—e.g., k+2 1 . FW with predetermined step sizes has been shown to have sub-linear worst case convergence in routing games [33]. On the other hand, replacing ﬁxed step sizes with optimal step sizes found by a line search method leads to a much better convergence rate.
V. NUMERICAL EXAMPLE
In this section, we apply the techniques developed in Section IV to model competition among ride-sharing drivers

Algorithm 3 Frank Wolfe Method with Value Iteration

Input: ¯, P , ps, N , .

Output: {ytsa}t∈[T ],s∈S,a∈A. y0 = 0 ∈ RT ×|S|×|A|

for k = 1, 2, . . . , N do cktsa = ¯tsa(yk), ∀ t ∈ [T ], s ∈ S, a ∈ A πts = VALUEITERATION(ck, P )

dk = RETRIEVEDENSITY(P, ps, πts)

αk

=

2 k+1

yk = (1 − αk)yk−1 + αkdk

Alg. 1 Alg. 2

Stop if

2
cktsa − ckts−a1 ≤

t∈[T ] s∈S a∈A

end for

Fig. 1: State representation of metro Seattle.
in metro Seattle. Using the set up described in Section VA, we demonstrate how a ride-share company takes on the role of social planner and shifts the equilibrium of the driver game in the following two scenarios:
– Ensuring minimum driver density in various neighborhoods (Section V-B). – Improving the social welfare (Section V-C).
A. Ride-sharing Model
Consider a ride-share scenario in metro Seattle, where rational ride-sharing drivers seek to optimize their proﬁts while repeatedly working Friday nights. Assume that the demand for riders is constant during the game and for each game play. The time step is taken to be 15 minutes, i.e. the average time for a ride, after which the driver needs to take a new action.
We model Seattle’s individual neighbourhoods as an abstract set of states, s ∈ S, as shown in Fig 1. Adjacent neighbourhoods are connected by edges. The following states are characterized as residential: ‘Ballard’ (3), ‘Fremont’ (4), ‘Sand Point’ (8), ‘Magnolia’ (9), ‘Green Lake’ (11), ‘Ravenna’ (12). Assume drivers have equal probabilities of starting from any of the residential neighbourhoods.
Because drivers cannot see a rider’s destination until after accepting a ride, the game has MDP dynamics. At each state s, drivers can choose from two actions. ar, wait for a rider in s, or asj , transition to an adjacent state sj. When

choosing ar, we assume the driver will eventually pick up a rider, though it may take more time if there are many drivers
waiting for riders in that neighborhood. Longer wait times decrease the driver’s reward for choosing ar.
On the other hand, there are two possible scenarios when drivers choose asj . The driver either drives to sj and pays the travel costs without receiving a fare, or picks up a rider in si. We allow the second scenario with a small probability to model the possibility of drivers deviating from
their predetermined strategy during game play. The probability of transition for each action at state si are
given in (16). Ni denotes the set of neighbouring states, and |Ni| the number of neighbouring states for state si.


    
P (s, a, si) =
    

|Ni1|+1 , |Ni1|+1 , |0N.1i| , 0.9,
0,

if s ∈ Ni, a = ar if s = si, a = ar if s ∈ Ni, s = sj, a = asj if s ∈ Ni, s = sj, a = asj otherwise

(16)

The reward function for taking each action is given by

tsa(ytsa) = Es mts s − cttrsavs − cwt ait · ytsa
= Pts sa mts s − cttrsavs − cwt ait · ytsa
s
where mts s is the monetary cost for transitioning from state s to s , cttrsavs is the travel cost from state s to s , cwt ait is the coefﬁcient of the cost of waiting for a rider. We compute these various parameters as

mts s = Rate · Dist

cttrsavs = τ Dist

Vel −1 + Fuel
Price

Fuel −1 Dist
Eff

(17a) (17b)

mi

hr/mi

$/gal gal/mi mi

 τ · 

−1

Customer Demand Rate

,

cwtaait =

rides/hr



 tsas ,

if a = ar if a = as

(17c)

where tsas is the congestion effect from drivers who all decide to traverse from s to s , and τ is a time-money tradeoff parameter, computed as
Rate · Dave Time Step
Where the average trip length, Dave, is equivalent to the average distance between neighbouring states. The values independent of speciﬁc transitions are listed in the Tab. I.

Rate $6 /mi

Velocity 8 mph

Fuel Price $2.5/gal

Fuel Eff 20 mi/gal

τ $27 /hr

Dave 1.25 mi

TABLE I: Parameters for the driver reward function.

B. Ensuring Minimum Driver Density
To ensure rider satisfaction, the ride-share company aims for a minimum driver coverage of 10 drivers in ‘Belltown’, s = 7, a neighborhood with highly variable rider demand. To

20 bs7ouunndcovnasltureained

Drivers

s7 constrained

10

s2 unconstrained

s2 constrained

0

toll charged

5

10

15

20

Time

Fig. 2: State density of the optimal trajectory solution to

(9). A constraint of form (18) is placed on ‘Belltown’, for

t ∈ [3, 20]. The imposed constraints also affect optimal

population distribution of other states, as shown by changes

in the population distribution of neighboring state s = 2.

100 10−1

|| · ||2 || · ||2, state 7

||y −y || ||y ||

10−2

10−3 0

1000

2000

Iterations

Fig. 3: Convergence of y to y . Plot shows difference between constrained optimal solution and FW approximation, y − y 2, normalized by y 2.

this end, they solve the optimization problem in (9) where (9a) for t ∈ {3, . . . , T }, s = 7, take on the form

gi(y) = ytsa − 10

(18)

a

The modiﬁed rewards (11) are given by

r¯tsa(y) = tsa(ytsa) + τts

(19)

where each τts is the optimal dual variable corresponding to each new constraint.
The optimal population distribution in ‘Belltown’ (state 7) and an adjacent neighbourhood, ‘Capitol Hill’ (state 2), are shown in Fig. 2. Note that the incentive τts is applied to all actions of state s. Furthermore, if the solution to the unconstrained problem is feasible for the constrained problem, then τts = 0—i.e. no incentive is offered. We simulate drivers’ behaviour with Algorithm 3, as a function of decreasing termination tolerance . In Fig. (3), the result shows that the optimal population distribution from the FW algorithm converge to Wardrop equilibrium as the approximation tolerance decreases.

C. Increasing Social Welfare
In most networks with congestion effects, the population does not achieve the maximum social welfare, which can be

Total objective value

×107 8.0 7.9

social objective unconstrained user objective constrained user objective

Toll Value

500
250 | min(τtsa)|
| max(τtsa)|
0 100 200 300 400 500 600 Number of constraints imposed

200

400

600

Number of constraints imposed

Fig. 4: With population of 3500, the social welfare of the user

selected equilibrium is shown as a function of the number

of imposed constraints; increasing the number of constraints

is equivalent to decreasing tolerance.

achieved by optimizing (6) with objective

J(y) =

ytsartsa(y)

(20)

t∈[T ] s∈S a∈A

In general, a gap exists between J(x ) and J(y ), where y = {ytsa} is the optimal solution to (6), and x = {xtsa} is the optimal solution to (6) with objective (20).
The typical approach to closing the social welfare gap is to impose mass dependent incentives. An alternative method, perhaps under-explored, is to impose constraints. As opposed to congestion dependent taxation methods for improving social welfare [4], [16], constraint generated tolls are congestion independent.
We can compare the two distributions and generate upper/lower bound constraints with an threshold—see Algorithm 4 for constraint selection method. The number of constraints increases with decreasing . Since the objective function in (6) is continuous in ytsa, as approaches zero, the objective will also approach the social optimal. In Fig. 4, we

Algorithm 4 Constraint Generation
Input: x , y . Output: U = {(ui, t, s, a) ∈ R × [T ] × S × A}
L = {(li, t, s, a) ∈ R × [T ] × S × A} for each s ∈ S, a ∈ A, t ∈ [T ] do
if ytsa − xtsa > then (xtsa, t, s, a) → U
else if ytsa − xtsa < then (xtsa, t, s, a) → L
end if
end for

compare the optimal social welfare to the social welfare at Wardrop equilibrium of the unconstrained congestion game, modeled in Section (V-A), as a function of the population size. We use CVXPY [35] to solve the optimization problem.
We utilize Algorithm 4 to generate incentives for the congestion game. Then, we simulate (9) and compare the game output to the social objective in Fig. 4. For a population

Income/loss

0.05 0.00

hdriv J(x )
hplan J(x )
hnet J(x )

−0.05

200

400

600

Number of constraints imposed

Fig. 5: Maximum and minimum toll values are shown in (a) as a function of number of constraints. In (b), the income/loss required to increase social welfare is shown as a function of constraints imposed.

size of 3500, there is a discernible gap between the social and user-selected optimal values. Note that with only 200 (t, s, a) constraints, the gap between the social optimal and the user-selected equilibrium is already less than 5%.
An interesting question to ask is how much of the total
market worth is affected by the incentives. In Fig. 5, we
demonstrate how payouts vary based on the number of constraints imposed. Let (·)− = min{0, ·} and (·)+ = max{0, ·}. The total payout from the drivers to the social planner and the social planner to the drivers are given by

hdriv = ytsa|(τtsa)−|,
tsa

hplan

= ytsa(τtsa)+
tsa

The net revenue the social planner receives from tolls is

hnet = ytsa(τtsa) = hplan − hdriv.
tsa
Fig. 5(b) shows how these quantities change as the total number of constraints is increased.

VI. CONCLUSIONS
We presented a method for adjusting the reward functions of a MDPCG in order to shift the Wardrop equilibrium to satisfy population mass constraints. Applications of this constraint framework have been demonstrated in a ride-share example in which a social planner aims to constrain state densities or to maximize overall social gain without explicitly constraining the population. Future work include developing online methods that updates incentives corresponding to constraints while the game population adjusts its strategy.

REFERENCES
[1] D. Calderone and S. S. Sastry, “Markov decision process routing games,” in Proc. Int. Conf. Cyber-Physical Syst. ACM, 2017, pp. 273–279.
[2] D. Calderone and S. Shankar, “Inﬁnite-horizon average-cost markov decision process routing games,” in Proc. Intell. Transp. Syst. IEEE, 2017, pp. 1–6.

[3] J. G. Wardrop, “Some theoretical aspects of road trafﬁc research,” in Inst. Civil Engineers Proc. London/UK/, 1952.
[4] M. Beckmann, “A continuous model of transportation,” Econometrica, pp. 643–660, 1952.
[5] M. Patriksson, The trafﬁc assignment problem: models and methods. Courier Dover Publications, 2015.
[6] L. S. Shapley, “Stochastic games,” Proc. Nat. Acad. Sci., vol. 39, no. 10, pp. 1095–1100, 1953.
[7] J.-F. Mertens and A. Neyman, “Stochastic games,” Int. J. Game Theory, vol. 10, no. 2, pp. 53–66, 1981.
[8] J.-M. Lasry and P.-L. Lions, “Mean ﬁeld games,” Japan J. Math., vol. 2, no. 1, pp. 229–260, 2007.
[9] O. Gue´ant, “Existence and uniqueness result for mean ﬁeld games with congestion effect on graphs,” Appl. Math. Optim., vol. 72, no. 2, pp. 291–303, 2015.
[10] T. Larsson and M. Patriksson, “An augmented lagrangean dual algorithm for link capacity side constrained trafﬁc assignment problems,” Transport. Res. B, vol. 29, no. 6, pp. 433–455, 1995.
[11] D. Hearn, “Bounding ﬂows in trafﬁc assignment models,” Research Report, pp. 80–4, 1980.
[12] E. Altman, Constrained Markov decision processes. CRC Press, 1999, vol. 7.
[13] M. El Chamie, Y. Yu, B. Acikmese, and M. Ono, “Controlled markov processes with safety state constraints,” IEEE Transa. Autom. Control, 2018.
[14] T. Roughgarden, Selﬁsh routing and the price of anarchy. MIT press Cambridge, 2005, vol. 174.
[15] D. Calderone, “Models of competition for intelligent transportation infrastructure: Parking, ridesharing, and external factors in routing decisions,” Ph.D. dissertation, U.C. Berkeley, ProQuest ID: Calderone berkeley 0028E 17079, 5 2017, an optional note.
[16] A. Pigou, The economics of welfare. Routledge, 2017. [17] R. Cole, Y. Dodis, and T. Roughgarden, “How much can taxes help
selﬁsh routing?” Journal of Computer and System Sciences, vol. 72, no. 3, pp. 444–467, 2006. [18] M. Furuhata, M. Dessouky, F. Ordo´n˜ez, M.-E. Brunet, X. Wang, and S. Koenig, “Ridesharing: The state-of-the-art and future directions,” Transport. Res., vol. 57, pp. 28–46, 2013. [19] J. Knope. Download These 12 Best Apps For Uber Drivers & Lyft Drivers. [Online]. Available: https://therideshareguy.com/ 12-must-have-apps-for-rideshare-drivers [20] M. Katz. This App Let’s Drivers Juggle Competing Uber and Lyft Rides. [Online]. Available: https://www.wired.com/story/ this-app-lets-drivers-juggle-competing-uber-and-lyft-rides/ [21] P. Solman. How Uber drivers game the app and force surge pricing. [Online]. Available: https://www.pbs.org/newshour/economy/ uber-drivers-game-app-force-surge-pricing [22] A. Ahmed, P. Varakantham, and S.-F. Cheng, “Uncertain congestion games with assorted human agent populations,” arXiv preprint arXiv:1210.4848 [cs.GT], 2012. [23] R. M. Freund and P. Grigas, “New analysis and results for the frank– wolfe method,” Math. Program., vol. 155, no. 1-2, pp. 199–230, 2016. [24] B. Jovanovic and R. W. Rosenthal, “Anonymous sequential games,” J. Math. Econ., vol. 17, no. 1, pp. 77–87, 1988. [25] J. Bergin and D. Bernhardt, “Anonymous sequential games with aggregate uncertainty,” J. Math. Econ., vol. 21, no. 6, pp. 543–562, 1992. [26] ——, “Anonymous sequential games: existence and characterization of equilibria,” Econ. Theory, vol. 5, no. 3, pp. 461–489, 1995. [27] P. Wiecek and E. Altman, “Stationary anonymous sequential games with undiscounted rewards,” J. Optim. Theory Appl., vol. 166, no. 2, pp. 686–710, 2015. [28] O. Gue´ant, J.-M. Lasry, and P.-L. Lions, “Mean ﬁeld games and applications,” in Paris-Princeton lectures on mathematical ﬁnance 2010. Springer, 2011, pp. 205–266. [29] O. Gue´ant, “From inﬁnity to one: The reduction of some mean ﬁeld games to a global control problem,” arXiv preprint arXiv:1110.3441 [math.OC], 2011. [30] D. A. Gomes, J. Mohr, and R. R. Souza, “Discrete time, ﬁnite state space mean ﬁeld games,” J. Math. Pures Appl., vol. 93, no. 3, pp. 308–328, 2010. [31] O. Gue´ant, “Mean ﬁeld games on graphs,” in NETCO 2014, 2014. [32] W. H. Sandholm, “Potential Games with Continuous Player Sets,” J. Econ. Theory, vol. 97, no. 1, pp. 81–108, 2001.

[33] W. B. Powell and Y. Shefﬁ, “The convergence of equilibrium algorithms with predetermined step sizes,” Transport. Sci., vol. 16, no. 1, pp. 45–55, 1982.
[34] M. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
[35] S. Diamond and S. Boyd, “Cvxpy: A python-embedded modeling language for convex optimization,” The Journal of Machine Learning Research, vol. 17, no. 1, pp. 2909–2913, 2016.

