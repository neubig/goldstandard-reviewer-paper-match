Group-Aware Threshold Adaptation for Fair Classiﬁcation
Taeuk jang1, Pengyi Shi2, Xiaoqian Wang1 *
1School of Electrical and Computer Engineering, Purdue University, West Lafayette, USA, 47907 2Krannert School of Management, Purdue University, West Lafayette, USA, 47907 {jang141, shi178, joywang}@purdue.edu

arXiv:2111.04271v1 [cs.LG] 8 Nov 2021

Abstract
The fairness in machine learning is getting increasing attention, as its applications in different ﬁelds continue to expand and diversify. To mitigate the discriminated model behaviors between different demographic groups, we introduce a novel post-processing method to optimize over multiple fairness constraints through group-aware threshold adaptation. We propose to learn adaptive classiﬁcation thresholds for each demographic group by optimizing the confusion matrix estimated from the probability distribution of a classiﬁcation model output. As we only need an estimated probability distribution of model output instead of the classiﬁcation model structure, our post-processing model can be applied to a wide range of classiﬁcation models and improve fairness in a model-agnostic manner and ensure privacy. This even allows us to post-process existing fairness methods to further improve the trade-off between accuracy and fairness. Moreover, our model has low computational cost. We provide rigorous theoretical analysis on the convergence of our optimization algorithm and the trade-off between accuracy and fairness of our method. Our method theoretically enables a better upper bound in near optimality than existing method under same condition. Experimental results demonstrate that our method outperforms state-of-the-art methods and obtains the result that is closest to the theoretical accuracy-fairness trade-off boundary.
Introduction
Machine learning is broadening its impact in various ﬁelds including credit analysis, job screening and etc. Consequently, importance of fairness in machine learning is emerging. However, recent models have been found to behave differently between demographic groups in favorable predictions. For example, it has been discovered that COMPAS, the criminal risk assessment software currently used to help pretrial release decisions, has biases between different races (Dressel and Farid 2018). Speciﬁcally, blacks got higher risk scores predicted from the model than whites with similar proﬁles. Therefore, discrimination truly exists and resolving it is critical as its direct and potential impact is growing tremendously.
However, obtaining fairness is not a trivial problem, as the dataset itself will be biased when it is accumulated arti-
*Corresponding author. Copyright © 2022, by the author(s).

ﬁcially. Simply modifying sensitive features (such as race, gender) from the data does not solve the bias, because there is indirect discrimination (Pedreshi, Ruggieri, and Turini 2008) caused by the feature relevance, which means sensitive information can be inferred from other features.
In order to alleviate discrimination from different perspectives, various quantitative measurements of group equity (Hardt et al. (2016), Kleinberg et al. (2016) Chouldechova et al. (2017)) have been proposed. It has been proven that the pursuit of fairness is subject to a trade-off between fairness and accuracy (Liu et al. (2019), Kim et al. (2020)).
Moreover, Pleiss et al. (2017) studied the trade-offs between fairness notions that cannot be satisﬁed at the same time. Therefore, recent works (Feldman et al. (2015), Zhang et al. (2018), Hardt et al. (2016)) usually target at a certain fairness notion. However, these approaches suffer from the lack of ﬂexibility, i.e., target fairness cannot be adjusted by the needs. If the fairness constraints change under some circumstances, traditional fairness models need to be retrained from scratch, which is computationally demanding and sometimes inapplicable due to model settings.
To overcome the limitations, we propose a novel postprocessing method to improve fairness in a model-agnostic manner i.e., we only need the prediction of an unknown model. GSTAR (Group Speciﬁc Threshold Adaptation for faiR classiﬁcation) model learns adaptive classiﬁcation thresholds for each demographic group in classiﬁcation task to improve the trade-off between fairness and accuracy. Given an existing classiﬁcation model, GSTAR approximates the probability distribution of the model output and utilizes confusion matrix to quantify accuracy and fairness w.r.t. the group-aware classiﬁcation thresholds. This allows us to: 1) prevent from burdening additional complexity or deteriorate the stability of the training process of the classiﬁer; 2) integrate different fairness notions into one uniﬁed objective function; 3) easily adapt one pre-trained model to other fairness constraints.
We summarize our contributions of this paper as follows:
1. We propose a novel post-processing method, named GSTAR, which can learn group-aware thresholds to optimize the fairness-accuracy trade-off in classiﬁcation. We empirically show that GSTAR outperforms state-of-theart methods.
2. With GSTAR, we can simultaneously optimize over mul-

tiple fairness constraints with a low computational cost. GSTAR does not require multiple iterations over data, instead, it takes at most one pass of data in training for fast computation.
3. We conduct extensive rigorous theoretical analysis on our method, in terms of convergence analysis and fairnessaccuracy trade-off. We theoretically prove a tighter upper bound of near optimality than existing method.
4. We derive Pareto frontiers of our model for the fairnessaccuracy trade-offs that contextualize the quality of fair classiﬁcation.
Related Works
In order to achieve group fairness, which quantiﬁes the discrimination among different sensitive groups, a diverse notion of fairness has been introduced. Equalized odds (Hardt et al. (2016)) enforce equality of true positive rates and false positive rates between different demographic groups. Pleiss et al. (2017) relaxed equalized odds to satisfy the calibration. Demographic parity or disparate impact (Barocas and Selbst 2016) suggests that a model is unbiased if the model prediction is independent of the protected attribute.
Among different fairness methods, post-processing techniques propose to improve fairness by modifying the output of a given classiﬁer. Hardt et al. (2016) propose to ensure equalized odds by constraining the model output. Kim et al. (2020) utilize confusion matrix and propose leastsquare accuracy-fairness optimization problem. Kamiran et al. (2012) propose to give a favorable outcome to unprivileged and an unfavorable outcome to the privileged group when the conﬁdence of the prediction is in a certain range. However, such static conﬁdence window keeps the same regardless of the demographic group and is determined by grid search, so it is less efﬁcient.
Threshold adjustment (a.k.a. thresholding) was introduced to improve the performance of static thresholds. In the literature, Menon et al. (2018) prove that instancedependent thresholding of the predictive probability function is the optimal classiﬁer in cost-sensitive fairness measures. Also, when considering immediate utility, CorbettDavies et al. (2017) show that optimal algorithm is achieved from group-speciﬁc threshold which is determined by group statistics. However, to the best of our knowledge, the threshold adjustment approach has not been deeply studied that neither encompasses broad group fairness metrics nor describes an explicit method to achieve the threshold.
Trade-off between fairness and accuracy exists when we impose fairness constraint to a model. Recent studies (Chouldechova 2017; Zhao and Gordon 2019) prove that models targeting at such fairness notions conform to an information theoretic lower bound on the joint error across different sensitive groups. Therefore, our work presents a practical upper bound of the best achievable accuracy given the fairness constraints.
Here, our work is the most related to the post-processing methods (Hardt et al. (2016), Kim et al. (2020)). However, ours differ from theirs in several aspects. First, we theoretically prove that GSTAR achieves a better upper bound of

near optimality than Hardt et al. (2016) as we directly operate on ROC curve instead of linear intersections in Hardt et al. (2016). Also, GSTAR corrects the predicted label by the conﬁdence of the prediction from a given model instead of randomly ﬂipping the output to achieve equalized odds, which is more reliable in post-processing. FACT (Kim et al. (2020)) utilizes a single point (static) from the classiﬁer to be post-processed as a reference which does not fully utilize the classiﬁer for the post-processing. In contrast, by approximating the distribution of the continuous predicted logits, GSTAR model enables a larger feasible region than Kim et al. (2020) with a better fairness-accuracy trade-off. We validate the improvement in this trade-off via both theoretical and empirical results. It is notable that these related methods can be considered as a special case of GSTAR.
GSTAR for Fair Classiﬁcation
Motivation
Consider a binary classiﬁcation problem with a binary sensitive feature, such that the sensitive feature A ∈ {0, 1} and label Y ∈ {0, 1}. In general, for a given data X, a binary classiﬁcation model outputs an unnormalized logit h(X) ∈ R with the class label probability R(X) = σ(h(X)) ∈ [0, 1], where σ is an activation function (e.g., sigmoid function). It is not necessary to calculate R in a classiﬁcation model, e.g., support vector machines directly use the positiveness/negativeness of logit h(X) to determine classiﬁcation outcome.
For traditional models, we use a cut-off threshold θh = 0 for h(X) (i.e., θR = σ(0) = 0.5 for R(X)) in classiﬁcation, such that the predicted label is determined by Yˆ = I{h(X) ≥ θh}. In the following context, unless otherwise mentioned, we use θ to refer to the threshold θh on logit h since it is applicable to a wider range of classiﬁcation models, and the corresponding threshold on label probability θR can be easily inferred from the threshold on logit h. Traditional models use the same cut-off threshold θ for different demographic groups. However, since the distribution of logits h in different demographic groups can be different, using the same threshold θ brings biased classiﬁcation.
In Figure 1, we show a real-world example of image classiﬁcation on CelebA dataset with ResNet50 (He et al. 2016) to show that the default setting of classiﬁcation thresholds affects both accuracy and fairness in classiﬁcation. The goal is to predict the image of a person is whether attractive or not, and consider sensitive attribute as gender. This can be generalized to different sensitive attributes, e.g., age or race (Lokhande et al. 2020). We can observe an obvious difference in the distribution of logit h between two gender groups. If we use a uniﬁed classiﬁcation threshold θ1 = θ0 = 0, it naturally brings a difference in the true positive rate and true negative rate between two gender groups, thus it behaves as a biased classiﬁcation. Instead, we observe that the optimal group-speciﬁc threshold obtained from GSTAR (θ1∗ > θ1, and θ0∗ < θ0) can adapt to such discrepancy in distribution between two demographic groups to improve both fairness and accuracy.

Figure 1: Histograms of logit h distribution from logistic re-

gression on CelebA data, where θ is the threshold to assign

predicted label based on h. The top and bottom plot is for

positive samples (Y = 1, attractive), and negative samples

(Y = 0, unattractive). Bars represent the distributions of

logit h of sensitive groups, and curves are estimated proba-

bility density functions of logit h of sensitive groups as in the

legend. θ = 0 (black dashed line) is the default classiﬁcation

thresholds. The default thresholds result in biased prediction

towards the unprivileged group (A = 0) due to the different

logit

h

distributions

in

different

sensitive

groups.

(θ

∗ 0

,

θ1∗

)

(colored dashed line) are group-aware thresholds for each

sensitive group achieved by GSTAR.

Group-Aware Classiﬁcation Thresholds

Given an existing classiﬁcation model and a sensitive attribute a, we can denote true positive rate (TPa), false positive rate (FPa), true negative rate (TNa), and false negative rate (FNa) in the confusion matrix. Most fairness notions can be represented with entries in the confusion matrix. For
instance, Equal Opportunity (EOp) (Hardt et al. (2016)) requires T P0 = T P1, and Demographic Parity (DP) (Barocas and Selbst 2016) requires

T P1n11 + F P1n01 = T P0n10 + F P0n00 ,

N1

N0

where nya denotes the number of samples in the subset {Y = y, A = a}, Na = y nya denotes the number of samples in {Y = y}, and N = y,a nya is the total number of samples.
Consider the group-aware classiﬁcation threshold θ = (θ1, θ0)T, where θa is the classiﬁcation threshold for sensitive group A = a. We can formulate the entries in the
confusion matrix w.r.t. θ as below:

θa

TPa(θa) ≈ 1 −

f1a(x)dx, FNa(θa) ≈ 1 − TPa(θa)

−∞

(1)

θa

FPa(θa) ≈ 1 −

f0a(x)dx, TNa(θa) ≈ 1 − FPa(θa)

−∞

where fya(x) is an estimated probability density function of the distribution of output logit h in the subset {Y = y, A = a}.
Then, we formulate the fairness-constrained classiﬁcation problem with the objective of minimizing classiﬁcation error into a least-squared optimization problem. We denote our objective function as L(θ) which consists of the performance loss Lper(θ) and fairness loss Lfair(θ) that are represented with the entries of the confusion matrix. In other words, our goal is to minimize the objective function L(θ) as below:

L(θ) = Lper(θ) + λLfair(θ),

(2)

where λ is a hyperparameter that determines how much fairness is enforced in the optimization. The performance error Lper(θ) can be written as

Lper(θ) =

n01 FP1(θ1) + n11 FN1(θ1)

N

N

n00

n10

2

+ FP0(θ0) + FN0(θ0) .

N

N

As for Lfair(θ), it can be formulated to any fairness metrics that are expressible with confusion matrix. For instance, when we impose EOp (TP1 = TP0) and predictive equality (PE) (FP1 = FP0) (Chouldechova 2017), we can get the corresponding Lfair(θ) by summing over the least squared form of each constraint. Also, satisfying EOp and PP is
equivalent to satisfying Equalized Odds (EOd) (Hardt, Price, and Srebro 2016), This can be formulated in our Lfair as

LEfaOird(θ) = LEfaOirp(θ) + LPfaPir(θ)

2

2 (3)

= T P1(θ1) − T P0(θ0) + F P1(θ1) − F P0(θ0) .

Note that a lower Lfair value indicates a fairer threshold. When LEfaOirD(θ) = 0, we can interpret as the θ satisﬁes the perfect EOd fairness. Similar to (3), we can enforce multiple fairness constraints by summing over the least squared of each metric with different weight constant λ to each fairness constraints if needed.
Also, it is notable that compared to FACT (Kim et al. (2020)) that enforces fairness through confusion tensor, our formulation of fairness in Lfair(θ) represents a direct notion of fairness metrics and improves the measures that allows us to achieve better performance and Pareto frontiers that is shown in Section and Figure 2. For example, FACT integrates multiple constraints as a weighted sum with the weights being the number of samples in each class. In this expression, the imbalance between the two fairness criteria will grow as the degree of imbalance in the data increases. In contrast, our formulation expresses the constraints as the exact notion of each metric that is not biased by the statistics of the datset and we observe improved Pareto frontier as in Figure 2.

Optimization of GSTAR
Our GSTAR objective in (2) lies in the family of Nonlinear Least Squares Problem (NLSP) (Gratton, Lawless, and Nichols 2007). To optimize objective (2) and ﬁnd the

threshold θ, we adopt the Gaussian-Netwon optimization method (Gratton, Lawless, and Nichols 2007). Here we take EOp constraint as an example to show the alternating optimization steps, then Lfair(θ) can be written as

LEfaOirp(θ) = (TP1(θ1) − TP0(θ0))2 .

(4)

To solve NLSP with the Gauss-Newton method, ﬁrst convert the nonlinear optimization problem to a linear least square problem using Taylor expansion. That is, the parameter values are calculated in an iterative fashion with

θa ≈ θak+1 = θak + ∆a,

(5)

in the k-th iteration number, with the vector of increments ∆ = {∆a} = {θak+1 − θak} (also known as the shift vector).
We rewrite our objective function as a real vector func-
tion r(θ) = r1(θ), r2(θ) = (Lper, λLfair). We linearize each component in the loss function to a ﬁrst-order Taylor
polynomial expansion as

ri(θ) ≈ ri(θk) + ∂ri(θk) ∆a (6) a ∂θa

with

θk

=

(θ

k 0

,

θ1k

).

Plugging

this

linearized

equation

into

the objective function, we get the usual least square problem.

Then, the optimal solution can be obtained as

∆ = −(J T J )−1J T f (θk),

(7)

where J = {Jia} with Jia = { ∂r∂iθ(aθ) } is the Jacobian. Each entry of the jacobian can be expressed with linear combina-
tion of pdf and cdf of fya for i, a, y ∈ {0, 1}. we can ﬁnalize the alternating optimization as

θ0τ = θ0τ−1 + ∆τ0 , θ1τ = θ1τ−1 + ∆τ1 .

(8)

It is notable that in each iteration we derive the optimal update step ∆a, which eliminates the burden of tuning hyperparameter (such as learning rate) in iterative algorithm. See the supplementary for detailed optimization process.
The alternating optimization of GSTAR model is of low computational cost. We take at most one pass of the data for learning the estimated probability density functions fya in (1) (we do not even need to traverse the data if the parameters (such mean and variance in Gaussian distribution) for the estimated probability density functions fya can be provided). The optimization of θ with alternating optimization is efﬁcient since we only need fya. Therefore, we need a constant time for each update. Overall, the time complexity of GSTAR is O(n + T ), where n is the number of samples, and T is the number of iterations in alternating optimization.
Besides, if a uniﬁed threshold is necessary (CorbettDavies et al. 2017), i.e., θ1 = θ0, the optimization algorithm also applies and we only have one scalar variable in (2). When we have a uniﬁed threshold, we do not require sensitive information in the testing phase that we can conform more strict privacy regulations than group-aware thresholding. However, we have to sacriﬁce both fairness and accuracy as the thresholding is less ﬂexible.

Theoretical Analysis
Upper Bounds on FPR/FNR Gap Between Groups We ﬁrst state the assumptions we need to make for Theorem 1 and 2.
Assumption 1. For any given classier h and its induced PDF fya and CDF Fya, we assume the following holds:
• The PDF fya(x) is uniformly bounded, i.e., there is an fˆya(x) = maxx fya(x).
• The inverse CDF Fy−a1(x) is Lipschitz continuous with Lipschitz constant Mya.
• The difference in the CDF between two groups is uniformly bounded, i.e.,

|Fy1(x) − Fy0(x)| ≤ uy, ∀x.

Theorem 1. For any given classiﬁer that satisﬁes Assumption 1 and any given pair of thresholds (θ0, θ1) that satisﬁes the perfect EOp condition, the gap between false-positive rates of the two group is upper bounded by

| 1| = FP0(θ0) − FP1(θ1) ≤ u0 + C1u1, (9)

where C1 = fˆ01M10.
Theorem 2. For any given classiﬁer that satisﬁes Assumption 1 and any given pair of thresholds (θ0, θ1) that satisﬁes the perfect PE condition, the gap between false-negative rates of the two group is upper bounded by

| 2| = FN0(θ0) − FN1(θ1) ≤ u1 + C0u0,

(10)

where C0 = fˆ11M00.
Theorem 1 and 2 characterize the upper bound of false positive/negative rate gap between two groups when the false negative/positive rate gap is 0. At the same time, it captures the upper bound of additional accuracy loss due to the two different thresholds for different groups under a perfect fairness (EOp or EP) condition.

Trade-off between Accuracy and Fairness Now we prove a theorem to characterize the trade-off between accuracy and fairness. Let θa∗ = argminθa Lper(θa), and its perturbed value θ˜a as
|FN1(θ1∗) − FN1(θ˜1)| ≤ γ/2, (11)
|FN0(θ0∗) − FN0(θ˜0)| ≤ γ/2,

for some perturbation coefﬁcient γ. Then for optimal perturbed version θ˜a∗ = argminθ˜ Lper(θ˜a), we state the theo-
a
rem below:

Theorem 3. Under Assumption 1 and condition (11),

Lper(θ1∗) − Lper(θ˜1∗) ≤ Cγ,

where

C = 2L∗

r1 fˆ01M11 n00 2 +r0 2 + N

fˆ M + ˆ1M11

00 10

2

+ n10 N

and ˆ1 = max ˜1 is the maximum of the derivative of ˜1.
Theorem 3 quantiﬁes the decrease in accuracy loss (i.e., the improvement in accuracy) when we allow a gap of true positive rates between two groups (i.e., relaxation from the perfect EOp condition).

Convergence Analysis of GSTAR Our objective function and the optimization solution algorithm belong to the family of Gauss-Newton algorithm. Given the assumptions A1 and A2 below, • A1. There exists θ∗ such that JT (θ∗)r(θ∗) = 0, • A2. The Jacobian at θ∗ has full rank,
we state the following theorem of convergence:
Theorem 4. Assume that the estimated density function f (·) satisfy assumptions A1 and A2. Further, f (·) satisﬁes that
||Q(θk)(J T J )−1(θk)||2 ≤ η
for some constant η ∈ [0, 1) for each iteration k, where Q(θ) denotes the second order terms i ri(θ)∇2ri(θ). Then as long as the initial solution is sufﬁciently close to the true optimal with ||θ0 −θ∗||2 ≤ , the sequence of Gauss-Newton iterates {θk} converges to θ∗.
Near Optimality of GSTAR Following the proof of Theorem 5.6 of Hardt et al. (2016), we provide the following near optimality theorem for our GSTAR model.
Theorem 5. With a bounded loss function and a given estimated density function h(x), let Rˆh ∈ [0, 1] be the induced random variable from the density h(x). Then the equalized odds predictor Yˆh derived from (Rˆh, A) using the method in our paper can achieve near optimality in the following sense:
E[ (Yˆh, Y )] ≤ E[ (Y ∗, Y )] + 2dK (Rˆh, R∗).
Here, Y is the true label, Y ∗ is the optimal equalized odds predictor derived from the Bayes optimal regressor R∗ as given in Hardt et al. (Hardt, Price, and Srebro 2016), and dK (Rˆh, R∗) is the conditional Kolmogorov distance.
Theorem 5 provides that GSTAR has tighter bound of near optimality than Hardt et al. (2016) under the same condition. See the supplementary for the proof of Theorem 1 ∼ 5.
Experiments
In this section, we validate GSTAR model on four wellknown fairness datasets and compare with other state-of-theart methods.
Experimental Setup
We compare with multiple fairness approaches in the experiments. For clear demonstration of results, we use different shapes of marker for each comparing methods in Figure 2 and Figure 4. The comparing methods include: FGP (Tan et al. (2020)), FACT (Kim et al. (2020)), DIR (Feldman et al. (2015)), AdvDeb (Zhang et al. (2018)), CEOPost (Pleiss et al. (2017)), Eq.Odds (Hardt et al. (2016)), LAFTR (Madras et al. (2018)), and Baseline: For CelebA dataset, we use ResNet50 (He et al. 2016) as a reference, and logistic regression for all other datasets.
We choose broadly used fairness metrics in evaluation including: equal opportunity difference (EOp) and equalized odds difference (EOd) (Hardt et al. (2016)); 1disparate impact (1-DIMP) (Barocas et al. (2016)); balanced accuracy difference (BD).

Accuracy

0.825 0.800 0.775 0.750 0.725 0.700 0.675
100

FACT Pareto GSTAR Pareto GSTAR(DP) GSTAR(EOd) GSTAR(DP+EOd) FACT Eq.Odds CEOPost LAFTR Baseline

10−1

10−2

10−3

10−4

EOd (smaller the better)

10−5

10−6

(a) CelebA Dataset

Accuracy

0.86 0.84 0.82 0.80 0.78 0.76
100

FACT Pareto GSTAR Pareto GSTAR(DP) GSTAR(EOd) GSTAR(DP+EOd) FACT AdvDeb Eq.Odds CEOPost LAFTR Baseline FGP

10−1

10−2

10−3

10−4

EOd (smaller the better)

10−5

10−6

(b) Adult Dataset

Accuracy

0.80 0.75 0.70 0.65 0.60 0.55 0.50
100

10−1

10−2

10−3

10−4

EOd (smaller the better)

(c) Compas Dataset

FACT Pareto GSTAR Pareto GSTAR(DP) GSTAR(EOd) GSTAR(DP+EOd) FACT AdvDeb Eq.Odds CEOPost LAFTR Baseline FGP

10−5

10−6

Accuracy

0.8 0.7 0.6 0.5 0.4 0.3
100

10−1

10−2

10−3

10−4

EOd (smaller the better)

(d) German Dataset

FACT Pareto GSTAR Pareto GSTAR(DP) GSTAR(EOd) GSTAR(DP+EOd) FACT AdvDeb Eq.Odds CEOPost LAFTR Baseline FGP

10−5

10−6

Figure 2: Pareto frontiers of equalized odds to show the upper bound of best achievable accuracy under different fairness constraints. Upper right region under the boundary is desired. For three variations of GSTAR with different fairness objectives, GSTAR (stars) is the closest to the Pareto frontier which indicates the best trade-offs.

We evaluate the methods on four fairness datasets: CelebA dataset (Liu et al. 2015), Adult dataset (Kohavi 1996), COMPAS1 dataset, and German dataset (Dua and
1https://github.com/propublica/compas-analysis

Fairness

Performance

Bias
1.60 1.40 1.20 1.00 0.80 0.60 0.40 0.20
Fair Baseline

AdvDeb

Eq.Odds

CEOPost

EOp EOd BD 1-DIMP BA ACC
LAFTR

Good
0.85 0.82 0.80 0.78 0.75 0.72 0.70 0.68
FACT GSTAR(DP) GSTAR(EOdG) STAR(DP+EOd)0B.a65d

Bias
0.60 0.50 0.40 0.30 0.20 0.10
Fair Baseline

AdvDeb

Eq.Odds

(a) CelebA Dataset

EOp

0G.6o8od

EOd BD

0.67

1-DIMP

BA ACC

0.66

0.65

0.64

0.63

0.62

0.61

CEOPost LAFTR

0B.a60d FACT GSTAR(DP) GSTAR(EOdG) STAR(DP+EOd)

(c) Compas Dataset

Performance

Performance

Fairness

Fairness

Bias
0.70 0.60 0.50 0.40 0.30 0.20 0.10
Fair Baseline

AdvDeb

Eq.Odds

CEOPost

EOp EOd BD 1-DIMP BA ACC
LAFTR

0G.8o6od
0.84 0.82 0.80 0.78 0.76 0.74 0.72
FACT GSTAR(DP) GSTAR(EOdG) STAR(DP+EOd)Bad

Bias
0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10
Fair Baseline

AdvDeb

(b) Adult Dataset
EOp EOd BD 1-DIMP BA ACC
Eq.Odds CEOPost LAFTR FACT

0G.8o0od
0.75 0.70 0.65 0.60 0.55
0B.a50d GSTAR(DP) GSTAR(EOdG) STAR(DP+EOd)

(d) German Dataset

Fairness

Performance

Figure 3: Evaluation on fairness and performance metrics. The bar plots indicate fairness measures of each model. The line plots indicate the performance measure of each model. Lower fairness values (left y-axis) and higher performance values (right y-axis) show better fairness and performance respectively. We consider three variations of GSTAR models (DP, EOd, DP+EOd).

Accuracy

0.84

0.82

0.80

0.78

0.76

100

10−1

10−2

10−3

10−4

EOd (smaller the better)

FACT Pareto GSTAR Pareto AdvDeb GSTAR-AdvDeb CEOPost GSTAR-CEOPost DIR GSTAR-DIR Eq.Odds GSTAR-Eq.Odd FGP GSTAR-FGP

10−5

10−6

Accuracy

0.68

0.66

0.64

0.62

0.60

0.58

0.56

0.54

0.52

100

10−1

10−2

10−3

10−4

EOd (smaller the better)

FACT Pareto GSTAR Pareto AdvDeb GSTAR-AdvDeb CEOPost GSTAR-CEOPost DIR GSTAR-DIR Eq.Odds GSTAR-Eq.Odd FGP GSTAR-FGP

10−5

10−6

(a) Adult Dataset

(b) Compas Dataset

Figure 4: Illustration of post-processing (magenta colored points) on existing fairness models (blue colored points). Given the outputs of each model, we efﬁciently improve existing fairness models with optimized group-aware thresholds from GSTAR.

Graff 2019). More details of the comparing methods, evaluation metrics, and datasets are provided in the Supplementary material.
Performance and Fairness-Accuracy Trade-Offs
In this subsection, we look into the performance evaluation of GSTAR comparing with other state-of-the-art methods. We consider Pareto frontier to visualize the trade-offs between fairness and accuracy to demonstrate the measure of performance.
In Figure 2, we plot Pareto frontier, which is the upper bound for the accuracy-fairness trade-offs, desired output

locates at the upper right region under the boundary which corresponds to higher values in accuracy and lower values in fairness discrepancy. With the same fairness constraints are given, we achieve a better frontier than the FACT (Kim et al. (2020)) as we equally weigh on demographic statistics and have a better feasible region. To obtain our results (star points), we ﬁrst estimate the logit distribution from the output of the baseline model, and then we get optimal adaptive thresholds with corresponding fairness metric by updating w.r.t. the objective function in (2). Here we have three combinations of fairness imposed to GSTAR: demographic parity (DP), equalized odds (EOd), and with both constraints

(DP+EOd). By post-processing on a simple baseline, we achieved signiﬁcantly better fairness with small or no sacriﬁce in accuracy. In all datasets, GSATR got competitive or better results than other state-of-the-art methods on both fairness and accuracy.
For example, we got θ∗EOd = (0.640, −0.627)T for the CelebA dataset. This shows that we have a higher threshold for the privileged group and a lower threshold for the unprivileged group. This optimal thresholding from GSTAR allows more samples from the privileged group to be correctly predicted as unattractive that would compensate for the discrimination of the original model. In other words, this improves predictive equality (Chouldechova 2017) with a huge amount from 0.235 to 0.014. Also, true positive rate difference (also known as equality of opportunity (Hardt, Price, and Srebro 2016)) got reduced from 0.282 to 0.018. It is notable that GSTAR only sacriﬁced 2.2% of accuracy to bring the big improvement in fairness.
Since the objective function of our model is independent to data dimensionality, our model is much more efﬁcient especially for high dimensional data. We mostly outperform the computational cost comparing to the other methods. The comparison of computational time and auxiliary experiments on the datasets can be found in the Supplementary material.
Flexibility and Multiple Fairness Constraints
Since each fairness metric has different interests, it has been theoretically proven that they cannot be perfectly satisﬁed all together (Pleiss et al. 2017; Chouldechova 2017; Kleinberg, Mullainathan, and Raghavan 2016). Because of this inherent trade-offs between fairness metrics, most of the recent works focus on a single metric at a time to achieve fairness. However with GSTAR, we have the ﬂexibility to optimize on multiple fairness constraints that can be represented in the confusion matrix format. Moreover, given the estimated distribution fya of a arbitrary classiﬁcation model, we can adjust the optimal θ based on the needs by accommodating different fairness criteria.
Figure 3 demonstrates the result of the methods with fairness metrics and accuracy trade-off evaluations. Overall, the variations of GSTAR achieve the best fairness on each target fairness while preserving the performance. For example in Figure 3(a), GSTAR with EOd constraint has outstanding performance in most fairness metrics with comparable accuracy (80.3%). Comparing with GSTAR (EOd), when we introduce EOd and DP together (DP+EOd), we achieve signiﬁcantly better w.r.t. DP fairness with sacriﬁcing a small amount of accuracy and EOd.
In general, by sacriﬁcing individual fairness performance, we could introduce multiple constraints. Also, we observe that the more fairness constraints are introduced, the more accuracy is sacriﬁced. We empirically found that in some cases (e.g., Figure 3(c)), introducing multiple fairness is complementary to each other that improves both conditions.
Post-Processing on an Existing Fair Model
For a binary classiﬁer that has a single ﬁxed classiﬁcation threshold (0 for out logit, and 0.5 for label probability),

we can provide better trade-off between fairness and accuracy with GSTAR. Given the logit/probability in the modelagnostic manner, we can improve the fairness as illustrated in Figure 4. In most cases, we observe improvement in fairness after GSTAR post-processing. It is also interesting to note that by optimizing the different thresholds for each protected group, we even obtain better performance on both fairness and accuracy, which indicates that the threshold optimization can not only improve fairness but also accuracy.
However, when the distribution of the logits/probability is highly extreme (such as the results of using GSTAR to post-process CEOPost), it is difﬁcult to estimate the distribution and thus causes erroneous optimization in GSTAR. We empirically found that when the dataset is extremely imbalanced such that we do not have enough samples to estimate the logit/probability distribution, or the given classiﬁcation model is too certain to the prediction that samples are concentrated to certain output, this problem arises.
Conclusion and Discussion
In this paper, we propose a group-aware threshold adaptation method (GSTAR) to post-process in model-agnostic manner and optimize over multiple fairness constraints.We directly optimize the classiﬁcation threshold for each demographic group w.r.t. the classiﬁcation error and multiple fairness constraints in a uniﬁed objective function, such that we can practically achieve an optimal trade-off between accuracy and fairness in fair classiﬁcation. Our method is applicable to diverse notions of group fairness as the majority of fairness notions can be expressed as a linear or quadratic equation through confusion matrix. We empirically show that GSTAR is ﬂexible with fairness regularization, efﬁcient with low computational cost. We also notice that the adaptive thresholds beneﬁt accuracy in some cases. GSTAR agrees to protect privacy such as article 17 of EU’s GDPR (Regulation 2016). We only require the estimated distribution of the output from a given model i.e., our post-processing method is oblivious to features. Thus training data is no longer needed and allowed to be discarded after training the model that to be post-processed. Thus, GSTAR can be applied to relaxed scenarios where practitioners cannot access individual-level sensitive information but have estimated distributions of logits for each sensitive group.
Further, we empirically ﬁnd that GSTAR is not applicable to post-process some classiﬁcation models in the following situations: 1) the model does not provide logit/probability as the outcome; 2) The model provides an extreme distribution of the output logit/probability. For example, when the model is too certain about its prediction, it will be difﬁcult to perform probability density estimation. In our future work, we will study possible strategies to solve the above limitations, and extend GSTAR to multi-class, multi-sensitive group problems and improve the fairness-accuracy trade-off in a more general scheme.
References
Barocas, S.; and Selbst, A. D. 2016. Big data’s disparate impact. Calif. L. Rev., 104: 671.

Chouldechova, A. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2): 153–163.
Corbett-Davies, S.; Pierson, E.; Feller, A.; Goel, S.; and Huq, A. 2017. Algorithmic decision making and the cost of fairness. In KDD, 797–806.
Dressel, J.; and Farid, H. 2018. The accuracy, fairness, and limits of predicting recidivism. Sci. Adv, 4(1): eaao5580.
Dua, D.; and Graff, C. 2019. UCI Machine Learning Repository.
Feldman, M.; Friedler, S. A.; Moeller, J.; Scheidegger, C.; and Venkatasubramanian, S. 2015. Certifying and removing disparate impact. In KDD, 259–268.
Gratton, S.; Lawless, A. S.; and Nichols, N. K. 2007. Approximate Gauss–Newton methods for nonlinear least squares problems. SIAM, 18(1): 106–132.
Hardt, M.; Price, E.; and Srebro, N. 2016. Equality of opportunity in supervised learning. In NeurIPS, 3315–3323.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770–778.
Kamiran, F.; Karim, A.; and Zhang, X. 2012. Decision theory for discrimination-aware classiﬁcation. In ICDM, 924– 929. IEEE.
Kim, J. S.; Chen, J.; and Talwalkar, A. 2020. ModelAgnostic Characterization of Fairness Trade-offs. arXiv preprint arXiv:2004.03424.
Kleinberg, J.; Mullainathan, S.; and Raghavan, M. 2016. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807.
Kohavi, R. 1996. Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree hybrid. In KDD, volume 96, 202–207.
Liu, L. T.; Simchowitz, M.; and Hardt, M. 2019. The implicit fairness criterion of unconstrained learning. In ICML, 4051–4060.
Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep learning face attributes in the wild. In ICCV, 3730–3738.
Lokhande, V. S.; Akash, A. K.; Ravi, S. N.; and Singh, V. 2020. FairALM: Augmented Lagrangian Method for Training Fair Models with Little Regret. In ECCV, 365–381. Springer.
Madras, D.; Creager, E.; Pitassi, T.; and Zemel, R. 2018. Learning adversarially fair and transferable representations. arXiv preprint arXiv:1802.06309.
Menon, A. K.; and Williamson, R. C. 2018. The cost of fairness in binary classiﬁcation. In ACM FAccT, 107–118.
Pedreshi, D.; Ruggieri, S.; and Turini, F. 2008. Discrimination-aware data mining. In KDD, 560–568.
Pleiss, G.; Raghavan, M.; Wu, F.; Kleinberg, J.; and Weinberger, K. Q. 2017. On fairness and calibration. In NeurIPS, 5680–5689.
Quadrianto, N.; Sharmanska, V.; and Thomas, O. 2019. Discovering fair representations in the data domain. In CVPR, 8227–8236.

Regulation, G. D. P. 2016. Regulation EU 2016/679 of the European Parliament and of the Council of 27 April 2016. Ofﬁcial Journal of the European Union. Available at: http://ec. europa. eu/justice/dataprotection/reform/ﬁles/regulation oj en. pdf (accessed 20 September 2017).
Tan, Z.; Yeom, S.; Fredrikson, M.; and Talwalkar, A. 2020. Learning fair representations for kernel models. In AISTATS, 155–166.
Zhang, B. H.; Lemoine, B.; and Mitchell, M. 2018. Mitigating unwanted biases with adversarial learning. In AIES, 335–340.
Zhao, H.; and Gordon, G. 2019. Inherent tradeoffs in learning fair representations. In NeurIPS, 15675–15685.

Supplementary Material for “Group-Aware Threshold Adaptation for Fair Classiﬁcation”

6 Optimization Procedure of GSTAR

The threshold θ is optimized with alternating optimization method in GSTAR. Here we take EOp constraint as an example to show the alternating optimization steps, then Lfair(θ) can be written as

LEfaOirp(θ) = (TP1(θ1) − TP0(θ0))2 ,

(12)

and overall objective is to minimize

L(θ) = Lper(θ) + λLEfaOirp(θ).

(13)

The ﬁrst step is to ﬁx θ0 and update θ1. We can approximate
the terms that are related to θ1 (e.g., TP1, FP1, TN1, FN1) in (20) with ﬁrst-order Taylor expansion at θ1τ−1. For example,

TP1(θ1) ≈ TP1(θτ−1) + ∂TP1

(θ1 − θτ−1) (14)

1

∂θ1 θ1=θ1τ−1

1

From (20), we can easily derive that

TP1(θ1τ−1) = 1 −

θ1τ −1
f11(x)dx,

−∞

(15)

∂TP1 = ∂θ1

− f11(θ1τ−1).

Similarly, we can ﬁnd the ﬁrst order Taylor expansion of FP1, FN1, and TN1. Then, the update of θ1 w.r.t. (13) can be approximated with the following minimization problem w.r.t. ∆1

∆τ1

:=

argmin(η1τ

+ α1τ ∆1)2

+ λ(

τ 1

+ β1τ ∆1)2,

(16)

∆1

where ∆1 = θ1 − θ1τ−1 and

α1τ = nN11 f11(θ1τ−1) − nN01 f01(θ1τ−1), β1τ = −f11(θ1τ−1),

η1τ =

θ1τ −1 −∞

n11 f11(x) + n01 (1 − f01(x) dx

N

N

θ0τ−1 n10

n00

(17)

+
−∞

N f10(x) + N (1 − f00(x) dx,

θ1τ −1

θ0τ −1

τ 1

=

f11(x)dx −

f10(x)dx.

∞

∞

Algorithm 1: Optimization Algorithm of GSTAR Model
Input dataset X × A × Y = {(xi, ai, yi)}ni=1, classiﬁcation model h(X), hyperparameter λ.
Output Group-speciﬁc threshold θ = (θ1, θ0). Initialize θ = (θ1, θ0) = (0, 0). 1. Given a classiﬁer H(x), estimate probability density
function fya, y, a ∈ {0, 1} by maximum likelihood estimation.
while not converge do
2. Calculate the optimal step ∆1 as ∆1 = − α1αη211++λλββ121 1 , with α1, β1, η1, 1 values shown in (17); 3. Update the threshold: θ1 ← θ1 + ∆1; 4. Calculate the optimal step ∆0 as ∆0 = − α0αη200++λλββ020 0 with α0, β0, η0, 0 values calculated in a similar way as in (17):
5. Update the threshold:θ0 ← θ0 + ∆0. end while

Taking the derivative of (16) w.r.t. ∆1 and setting it to 0, we can easily obtain the closed-form solution of ∆τ1 as

τ

ατ ητ + λβτ τ

∆1 = − (ατ )2 + λ(βτ )2 .

(18)

The second step is to ﬁx θ1 and update θ0, and this can be achieved in a similar way of updating θ1. Then we can ﬁnalize the alternating optimization as:

θ0τ = θ0τ−1 + ∆τ0 ,

θ1τ = θ1τ−1 + ∆τ1 .

(19)

It is notable that in each iteration we derive the optimal update step ∆a, which eliminates the burden of tuning hyperparameter (such as learning rate) in iterative algorithm. The optimization step is summarized in Algorithm 1. The above algorithm can easily extend to multiple fairness constraints by adding corresponding squared-loss fairness terms to (13).

7 Upper Bounds on False-Positive/Negative Rate Gap Between Groups
7.1 Notations
We start from deﬁning notations. We denote fya(x) for the estimated parametric probability density function (PDF) of the distribution of output logit h in the subset {Y = y, A =

a}. Correspondingly, we denote the corresponding cumulative distribution function (CDF) as

x

Fya(x) =

fya(x)dx.

−∞

We use Fy−a1(x) to denote the inverse of the CDF. Then, following the deﬁnitions given in the main paper,
we have

TPa(θa) = 1 − F1a(θa), FPa(θa) = 1 − F0a(θa),

FNa(θa) = F1a(θa), (20)
TNa(θa) = F0a(θa).

7.2 Characterizing the accuracy loss function under perfect EOp condition
Before stating the theorem, we illustrate the difference between Lper(θ) used in our paper versus loss function one would use in a population-wise classiﬁcation problem (without considering group-aware thresholds). That is, one would only consider the loss function on accuracy

L¯per(θ) = (r1F¯N(θ) + r0F¯P(θ))2 ,

(21)

where only one threshold θ (for both groups) needs to be decided, ry = (ny0 + ny1)/N is the population ratio of data samples with label y, F¯N(θ), F¯P(θ) are the populationwise false-negative and false-positive rate. F¯N(θ), F¯P(θ) are deﬁned in a similar way as in (20) except that we just use the population-wise pdf f¯y(x) in the integral for label y. (21) will be our benchmark to compare with Lper(θ) used in our paper.
We start from considering the case that we achieve perfect EOp condition, that is

TP1(θ1) = TP0(θ0),

(22)

or equivalently

FN1(θ1) = FN0(θ0).

This means that θ0 and θ1 satisﬁes the following condition

F11(θ1) = F10(θ0).

(23)

Equivalently, we have

θ0 = F1−01 F11(θ1) .

(24)

Under any given pair of (θ0, θ1) that satisﬁes (24), recall that the performance error Lper(θ) is deﬁned as

Lper(θ) = n01 FP1(θ1) + n11 FN1(θ1)+

N

N

n00

n10

2

FP0(θ0) + FN0(θ0) . (25)

N

N

From (22), we get

n11 FN1(θ1) + n10 FN0(θ0) =

N

N

=

n11 +n10 N

FN(θ1

)

r1FN(θ1),

where r1 denotes, over the entire population (across different groups), proportion of samples with positive labels. In other words, r1FN(θ1) represents the proportion of data

samples (from both groups) with positive label but falsely classiﬁed as negative out of the entire dataset.
Next, we look at the other two terms:

n01 FP1(θ1) + n00 FP0(θ0).

N

N

This sum can be written as

n01 N

FP1

(θ1

)

+

n00 N

FP0

(θ0

)

=

n01 +n00 N

FP1

(θ1

)

+

n00 N

FP0(θ0) − FP1(θ1)

=

r0FP(θ1)

+

n00 N

FP0(θ0) − FP1(θ1)

.

We denote 1 = FP0(θ0) − FP1(θ1) . Hence,

Lper(θ) = Lper(θ1) =

r1FN(θ1) + r0FP(θ1) + n00

1

2
. (26)

N

Comparing (21) with (26), we can see that, when

FP0(θ0)

>

FP1(θ1), the term

n00 N

1

captures the addi-

tional accuracy loss due to that we have chosen two different

thresholds even though that condition (23) is satisﬁed. Next,

we characterize an upper bound for 1.

7.3 Theorem 1 and its Proof
Proof. Recall that FP1(θ1) = 1 − F01(θ1) and FP0(θ0) = 1 − F00(θ0). Hence,

FP0(θ0) − FP1(θ1)

= F01(θ1) − F00(θ0) ≤ F01(θ1) − F01(θ0)
+ F01(θ0) − F00(θ0) .

To bound , we just need to bound F01(θ1) − F01(θ0) and
F01(θ0) − F00(θ0) . For the second one, we note that from Assumption 1 that

F01(θ0) − F00(θ0) ≤ u0.

For the ﬁrst one, we note that F01(θ1) − F01(θ0) ≤

fˆ01|θ1 − θ0|,

where fˆ01 = maxx f01(x). Next, we bound |θ1 − θ0|. Note that from (24),

|θ1 − θ0| = =

F1−01 F11(θ1) F1−01 F11(θ1)

− θ1 − F1−01 F10(θ1)

≤ M10 F11(θ1) − F10(θ1)| ≤ M10u1.

Theorem 1 provides an upper bound on the difference in the false positive rate between the two groups, for any given pair of (θ0, θ1) such that the false negative rates are the same for the two groups (i.e., satisﬁes the perfect EOp condition). As discussed in Section 7.2, this upper bound also characterize the additional accuracy loss due to that we have groupdependent thresholds compared to the case with only one threshold for both groups.

7.4 Theorem 2 and its Proof

For predictive equality (PE) condition, we prove a similar

result. That is, assuming we achieve perfect PE condition

with

FP1(θ1) = FP0(θ0),

(27)

or equivalently

TN1(θ1) = TN0(θ0).

(28)

This means that θ0 and θ1 satisﬁes the following condition

F01(θ1) = F00(θ0).

(29)

Equivalently, we have

θ0 = F0−01 F01(θ1) .

(30)

Under any given pair of (θ0, θ1) that satisﬁes (30), the performance error Lper(θ) can be written as

Lper(θ) = =

n01 FP1(θ1) + n11 FN1(θ1)

N

N

n00

n10

2

+ FP0(θ0) + FN0(θ0)

N

N

n10 2

r1FN(θ1) + r0FP(θ1) +

2,

N

where

2 = FN0(θ0) − FN1(θ1) .

Similar to Theorem 1, we can provide an upper bound on

2 under Assumption 1.

Proof. The proof is similar to that of Theorem 1. We provide the main steps and omit details that repeat with the proof of Theorem 1. We have
FN0(θ0) − FN1(θ1) = F11(θ1) − F10(θ0)

≤ F11(θ1) − F11(θ0)

+ F11(θ0) − F10(θ0) ≤ fˆ11|θ1 − θ0| + u1 ≤ fˆ11M00u0 + u1.

Theorem 2 provides an upper bound on the difference in the false negative rate between the two groups, for any given pair of (θ0, θ1) such that the false positive rates are the same for the two groups (i.e., satisﬁes the perfect PE condition).
To sum up, Theorem 1 and 2 characterize the upper bound of false positive/negative rate gap between two groups when the false negative/positive rate gap is 0. At the same time, it captures the upper bound of additional accuracy loss due to the two different thresholds for different groups under a perfect fairness (EOp or EP) condition.
8 Characterizing the trade-off between Accuracy and Fairness
In this section, we prove a theorem to characterize the tradeoff between accuracy and fairness. That is, we start from the perfect EOp or PE conditions and perturb the solution by a small amount. We then bound the difference in the accuracy loss by comparing the perturbed solution with the original solution that satisﬁes the perfect fairness conditions.

8.1 Perturbed EOp condition

To start with, let us consider solutions (θ0, θ1) that satisfy the perfect EOp condition (24). Under this condition, the optimization problem becomes one dimensional, that is,
θ1∗ = argmin Lper(θ1),
θ1

where Lper(θ1) =
and

n00

2

r1FN1(θ1) + r0FP1(θ1) +

1(θ1)

N

1(θ1) = FP0(θ0) − FP1(θ1)

= F01(θ1) − F00 F1−01(F11(θ1)) .

From θ1∗, we can get the corresponding θ0∗ = F1−01(F11(θ1∗)). We further denote this optimal accuracy loss

value as

L∗ = Lper(θ1∗).

Now with the optimal solution (θ0∗, θ1∗), we investigate the changes in Lper(θ1∗) when we perturb the perfect EOp con-

dition and allow a small difference. That is, now consider solution (θ˜0, θ˜1) such that

|FN1(θ1∗) − FN1(θ˜1)| ≤ γ/2, (31)
|FN0(θ0∗) − FN0(θ˜0)| ≤ γ/2.

Consequently, the solution (θ˜0, θ˜1) satisfy the following perturbed EOp condition:

TP1(θ˜1) − TP0(θ˜0) = FN1(θ˜1) − FN0(θ˜0) ≤ γ. (32)

Without loss of generality, we assume that (i) the true pos-
itive rate of group 1 is higher than that of group 0, and
(ii) the above inequality is binding (because if not binding,
then we can always choose a smaller γ to make it binding). Thus, we have TP1(θ˜1) − TP0(θ˜0) = γ, or equivalently, FN0(θ˜0) − FN1(θ˜1) = γ. This gives us

θ˜0 = F1−01 F11(θ˜1) + γ) .

(33)

Next, we analyze Lper(θ˜1) by substituting (θ˜0, θ˜1) in (25), which gives us

Lper(θ˜1) =

r1FN1(θ˜1) + r0FP1(θ˜1) + n10 γ + n00 ˜1(θ˜1)

2
,

N

N

where

˜1(θ˜1) = FP0(θ˜0) − FP1(θ˜1)

= F01(θ˜1) − F00 F1−01 F11(θ˜1) + γ) .

We denote the optimal value for this perturbed version as θ˜1∗, and its corresponding loss value as
L˜∗ = Lper(θ˜1∗).

Furthermore, from (31), we have

|FN1(θ1∗) − FN1(θ˜1∗)| = |F11(θ1∗) − F11(θ˜1∗)| ≤ γ/2. (34)

Under Assumption 1, we have

|θ1∗ − θ˜1∗| = F1−11(F11(θ1∗)) − F1−11(F11(θ˜1∗)) ≤ M11 F11(θ1∗) − F11(θ˜1∗)

= M11γ/2.

8.2 Theorem 3 and its proof
We are ready to compare Lper(θ1∗) and Lper(θ˜1∗). The latter loss should be no larger than the former since we relaxed the perfect EOp condition (constraint) in the optimization, i.e., L∗ ≥ L˜∗.

Proof. We have that

Lper(θ1∗) − Lper(θ˜1∗)

≤ 2L∗ r1FN1(θ1∗) + r0FP1(θ1∗) + nN00 1(θ1∗)

− r1FN1(θ˜∗) + r0FP1(θ˜∗) + n10 γ + n00 ˜1(θ˜∗)

1

1N

N

1

≤ 2L∗ r1γ/2 + r0|FP1(θ1∗) − FP1(θ˜1∗)|

+ n00 1(θ∗) − ˜1(θ˜∗) + n10 γ ,

N

1

1

N

where we further have that

|FP1(θ1∗) − FP1(θ˜1∗)| = |F01(θ1∗) − F01(θ˜1∗)| ≤ fˆ01|θ1∗ − θ˜1∗| ≤ fˆ01M11γ/2,

and
1(θ1∗) − ˜1(θ˜1∗) ≤ 1(θ1∗) − ˜1(θ1∗) + ˜1(θ1∗) − ˜1(θ˜1∗)
≤ F00(F1−01(F11(θ1∗))) −F00(F1−01(F11(θ1∗) + γ))
+ˆ1M11γ/2 = (fˆ00M10 + ˆ1M11/2)γ.
Here, ˆ1 = max ˜1 is the maximum of the derivative of ˜1. Combining all the terms in front of γ gives us the desired upper bound.

Theorem 3 quantiﬁes the decrease in accuracy loss (i.e., the improvement in accuracy) when we allow a gap of true positive rates between two groups (i.e., relaxation from the perfect EOp condition).
Repeating the analysis for the perturbed PE condition, we can obtain a similar bound for the changes in the accuracy loss function. We omit the details here in the interest of space.

9 Convergence Analysis of GSTAR

9.1 GSTAR as Nonlinear Least Squares Problem

Our objective function and the optimization solution algo-

rithm belong to the family of Gauss-Newton algorithm to

solve Nonlinear Least Squares Problem (NLSP). To specify,

NLSP is to solve

min ||r(θ)||22,
θ

where the decision variables, θ, is an n-dimensional real vector and the objective function r is an m-dimensional real vector function of θ. Connecting to our setting and using two groups as an example, our decision variables is the twodimensional vector θ = (θ0, θ1) for group 0 and group 1,

and our objective function is the following 2-dimensional real vector function:

r(θ) = r1(θ), r2(θ)

with

r1(θ) = r1(θ0, θ1) = n01 FP1(θ1) + n11 FN1(θ1)

N

N

+ n00 FP0(θ0) + n10 FN0(θ0),

√N

N

r2(θ) = r2(θ0, θ1) = λ (TP1(θ1) − TP0(θ0))

when taking the EOp constraint. The L2 norm ||r(θ)||22 = r1(θ)2 + r2(θ)2 recovers the objective function in Equation (2) in the main paper.
A classic family of algorithms to solve NLSP is the Gauss-Newton Method. The main idea is to convert the nonlinear optimization problem to a linear least square problem using Taylor expansion. That is, the parameter values are calculated in an iterative fashion with

θj ≈ θjk+1 = θjk + ∆j ,

in the k-th iteration number, with the vector of increments ∆ = {∆j} = {θjk+1 − θjk} (also known as the shift vector). We linearize each component in the f function to a ﬁrst-
order Taylor polynomial expansion as

ri(θ) ≈ ri(θk) + ∂ri(θk) ∆j (35) j ∂θj

with

θk

=

(θ

k 0

,

θ1k

).

Plugging

this

linearized

equation

into

the objective function, we get the usual least square problem.

Then, the optimal solution can be obtained as

∆ = −(J T J )−1J T f (θk),

(36)

where J = {Jij} with Jij = { ∂r∂iθ(jθ) } is the Jacobian. Note that in the GSTAR algorithm, we ignore the terms for j = i in the Taylor expansion (35). Thus, in calculating JT J, we
only kept the diagonal terms

∂r1(θ) 2 +
∂θj

∂r2(θ) 2 ∂θj

for j = 0, 1. Plugging in the form of r1 and r2 as speciﬁed above, we achieve the solution provided in (18).

9.2 Convergence Property for Gauss-Newton Algorithm
There is a long history of studying the convergence property of the Gauss-Newton algorithm, e.g., see (Gratton, Lawless, and Nichols 2007). The convergence of the algorithm is generally not guaranteed, e.g., if the initial solution is far from the true optimal or JT J is ill-conditioned. In other words, the convergence of the algorithm heavily depends on the density estimation f (·). We state the following sufﬁcient conditions from (Gratton, Lawless, and Nichols 2007) to guarantee the convergence of the algorithm. The following assumptions are made in order to establish the theory.

• A1. There exists θ∗ such that JT (θ∗)r(θ∗) = 0; • A2. The Jacobian at θ∗ has full rank.
We state Theorem 4 from (Gratton, Lawless, and Nichols 2007) on the sufﬁcient conditions for convergence.
Theorem 4. Assume that the estimated density function f (·) satisfy assumptions A1 and A2. Further, f (·) satisﬁes that
||Q(θk)(J T J )−1(θk)||2 ≤ η
for some constant η ∈ [0, 1) for each iteration k, where Q(θ) denotes the second order terms i ri(θ)∇2ri(θ). Then as long as the initial solution is sufﬁciently close to the true optimal with ||θ0 −θ∗||2 ≤ , the sequence of Gauss-Newton iterates {θk} converges to θ∗.

9.3 Protection against Divergence

It is known that for general function f (·) such as estimates

from a neural network, the above sufﬁcient conditions that

guarantee convergence do not necessarily hold. As a result,

protection against divergence is essential. In our numerical

experiments, we adopt a commonly used, simple protection,

the shift-cutting method. That is, we to reduce the length of

the shift vector ∆ by a fraction η. In other words, the update

becomes

θjk+1 = θjk + η∆j .

10 Near Optimality of GSTAR

Here, we show regarding on how the accuracy of h affects the accuracy of Yˆ . Following the proof of Theorem 5.6 of Hardt et al. (2016), we provide the following near optimality results for our method.
Before we prove the theorem, we ﬁrst state the results from Lemma 5.5 proved in Hardt et al. (2016), which will be used in our proof.

Lemma 5 (Restatement of Lemma 5.5 in Hardt et al.

(2016)). Let R, R ∈ [0, 1] be two random variables in the

same probability space as A and Y . Then, for any point p on

a conditional ROC curve of R, there is a point q on the cor-

responding ROC curve of R achieving the same threshold

such that

√ ||p − q||2 ≤ 2dK (R, R ),

dK (R, R ) = max sup |P r(R > t|A = a, Y = y)

a,y t

(37)

− P r(R > t|A = a, Y = y)|.

Proof. Similar to Hardt et al. (2016), we focus on proving this theorem for equalized odds. The case of equal opportunity is analogous. The optimal classiﬁer Y ∗ corresponds to a point p∗. Under the equalized odds condition, our algorithm essentially ﬁnds the intersection point, q, of the two conditional ROC curves of Rˆh for a = 0 and a = 1. Then directly applying the above lemma, we get that
√ ||p∗ − q||2 ≤ 2dK (R, R ).
The rest follows the same argument as in Theorem 5.6 of Hardt et al. (2016). That is, by assumption√on the loss function, there is a vector v with ||v||2 ≤ 2 such that

E[ (Yˆh, Y )] = v, q and E[ (Y ∗, Y )] = v, p∗ . Applying Cauchy-Schwarz, we get
E[ (Yˆh, Y )] − E[ (Y ∗, Y )] = v, q − p∗ ≤ ||v||2 · ||p∗ − q||2
≤ 2dK (R, R ).
Remark 1. In Hardt et al. (2016), the point q from their algorithm under equalized odds condition is the intersection point between two line segments, not the two ROC curves as in our paper. That is, assume without loss of generality that the ﬁrst coordinate of q1 (for group a = 1) is greater than the ﬁrst coordinate of q0 (for group a = 0) on the ROC curve plane; and that all points p∗, q0, q1 lie above the main diagonal. Then q ∈ L0 ∩ L1 from their algorithm, where L0 is the line segment between q0 and the point (1, 1), and L1 is the line segment between the point (0, 0) and q1. As a result, in proving their Theorem 5.6, they need to show that q from this construction satisﬁes ||p∗ − q||2 ≤ 2dK (R, R ). However, because the point q from our algorithm lies on the ROC curve, we can directly apply the results from the lemma. This difference is further illustrated in ﬁgure 5 below, where the purple pentagram corresponds to q found by our algorithm, and the green cross corresponds to q from their algorithm. The ﬁgure shows the intersection points found from our algorithm versus Hardt et al.
Moreover, the requirement for achieving the near optimality in our method (our Theorem 5) and in Hardt et al. (their Theorem 5.6) is the same. That is, the closeness between the conditional densities is required, not just the conditional probability estimates.
To specify, the closeness requirement in Hardt et al. based on conditional Kolmogorov distance is shown in Equation (37), where R ∈ [0, 1] and R ∈ [0, 1] are real-valued scores, i.e., two regressors. Note that the distance is taking sup over all t ∈ [0, 1], so this condition requires the entire conditional density curves from R and R to be close, not just close at some given threshold t.
Next, the near optimality of Hardt et al. (their Theorem 5.6) shows:
√ E[ (Yˆh, Y )] ≤ E[ (Y ∗, Y )] + 2 2dK (Rˆ, R∗),
where R∗ ∈ [0, 1] is the Bayes optimal regressor and Rˆ ∈ [0, 1] is a regressor whose density is estimated. In fact, the distribution function of their corresponds to the score function σ(h) in our paper, where h is the logit and σ(·) is the softmax function.
Seeing this connection, we stress that the closeness requirement in our result is the same as in Hardt et al., and that the near optimality of our algorithm follows:
E[ (Yˆh, Y )] ≤ E[ (Y ∗, Y )] + 2dK (Rˆ, R∗),
where R∗ ∈ [0, 1] is the Bayes optimal regressor as given in Hardt et al., and Rˆ ∈ [0, 1] comes from our estimated density, i.e., , the distribution of Rˆh comes from by applying softmax function σ(·) on logit h.

1.0 1.0

0.8

0.8

0.6

0.6

TPR

0.4

A=0 (Logistic Regression) 0.4

A=1 (Logistic Regression)

Hardt et al. (A=0)

0.2

Hardt et al. (A=1)

GSTAR (A=0)

0.2

GSTAR (A=1)

GSTAR optimal

0.0

Hardt et al. optimal

0.0 0.2 0.4 FPR 0.6 0.8 1.00.0

Figure 5: Comparison of optimal point of Hardt et al.and GSTAR. Given the ROC curve of each protected group, ours (magenta star) achieves better optimum than that of Hardt et al.(green cross), as ours has higher TPR and lower FPR.

11 Experimental Details
11.1 Comparing Methods
We compared our method with multiple state-of-the-art methods to verify our work. The details about the comparing methods are as below:
• Learning fair representations for kernel models (abbreviated as FGP) (Tan et al. 2020): a pre-processing method to learn representation focusing on kernel-based models. The fair model that satisﬁes certain fairness criterion is obtained by Bayesian learning from fair Gaussian process (FGP) prior.
• Fairness confusion tensor (abbreviated as FACT) (Kim, Chen, and Talwalkar 2020): a post-processing model that minimize the least-squares accuracy-fairness optimality problem based on confusion tensor.
• Adversarial de-biasing (abbreviated as AdvDeb) (Zhang, Lemoine, and Mitchell 2018): an in-processing model that mitigates the conﬂicting gradient directions in utility and fairness objectives by projecting one gradient to another to remove the opposite direction.
• Calibrated equal odds post-processing (abbreviated as CEOPost) (Pleiss et al. 2017): a post-processing method that minimizes the disparity in the predicted probability to the preferred class among different sensitive groups, while maintaining the calibration condition in a relaxed condition.
• Equality of opportunity in supervised learning (abbreviated as Eq.Odds) (Hardt, Price, and Srebro 2016): a post-processing method that learns the threshold to yield the equalized odds/opportunity between different demographic by exploring the intersection of achievable true positive rates and false positive rates.
• Learning adversarially fair and transferable representations (abbreviated as LAFTR) (Madras et al. 2018): a fair representation learning model that adopts fairness metrics as the adversarial objectives and analyze the balance between utility and fairness.

• Baseline: For CelebA dataset, we use ResNet50 (He et al. 2016) as a reference because we input second last layer (2048 features) of ResNet to all methods. For other tabular datasets, logistic regression is used as all other methods except for FGP and LAFTR are based on logistic regression.
11.2 Evaluation Metrics In the experiments, we evaluate the methods on four fairness and two performance measures. Four fairness metrics are as below: • Equal Opportunity (abbreviated as EOp) (Hardt, Price,
and Srebro 2016) : This measures absolute difference of favorable prediction given positive label. |P (Yˆ = 1|Y = 1, A = 1) − P (Yˆ = 1|Y = 1, A = 0)|.
• Equalized Odds (abbreviated as EOd) (Hardt, Price, and Srebro 2016) : This measures the difference between the probability given the true labels. |P (Yˆ = 1|Y = 1, A = 1) − P (Yˆ = 1|Y = 1, A = 0)|+
|P (Yˆ = 1|Y = 0, A = 1) − P (Yˆ = 1|Y = 0, A = 0)|.
• Balanced Accuracy Difference (abbreviated as BD) : This measures difference between balanced accuracy between the groups.
P (Yˆ = 1|Y = 1, A = 1) + P (Yˆ = 0|Y = 0, A = 1)
− P (Yˆ = 1|Y = 1, A = 0) + P (Yˆ = 0|Y = 0, A = 0) .
If BD and EOd has the same value, it indicates that both TPR and TNR are higher in a certain sensitive group. However, if the gap between the two terms is large, we can interpret as the classiﬁer is more biased as a group with higher TPR has lower TNR. This is more unfair as a sample from the privileged group is more likely to be falsely and correctly predicted as positive output. EOp is a partial measure of EOd as it only measures the difference from a favorable class. • Absolute (1 - Disparate Impact) (abbreviated as 1DIMP) (Barocas and Selbst 2016) : This measures ratio of the probability of the favorable prediction given a protected group.
P (Yˆ = 1|A = 1) 1 − P (Yˆ = 1|A = 0) .
We evaluate performance of the methods with two metrics. • Balanced Accuracy (abbreviated as BA) : This measures
average between true positive rate and true negative rate. Compared to the traditional accuracy, this measure effectively shows the whether the classiﬁer is focusing on the performance of a certain class when the dataset is unbalanced.
1 P (Yˆ = 1|Y = 1) + P (Yˆ = 0|Y = 0) . 2 • Accuracy (abbreviated as ACC) : This measures traditional classiﬁcation accuracy of the method.

11.3 Experimental Setup
For experimental setup, all comparing methods apply EOd as the fair constraint if fairness constraint is selectable, thus we compare them via EOd in Figure 2 in the main paper. Both the Pareto frontier from GSTAR and FACT are derived based on EOd constraint for a fair comparison. We follow the setup in Section G.3 of the FACT (Kim, Chen, and Talwalkar 2020) to report their method, which does not require λ.
For GSTAR, we estimate fya and optimize θa from the training data, and report evaluation results (with the θa learned from training data) on the testing data. We use the same λ for multiple fairness constraints for simplicity, but λ can be introduced individually. Our method is optimized with λ in the range of [10−1, 104] with alternating optimization method.
To ﬁnd estimated distribution fya, we consider gamma, Student’s t, and normal distribution as the candidates for the experiments reported in the main paper, and select the one that has the maximum likelihood with the output distribution. Without loss of generality, this can be generalized nonparametric density estimation such as kernel density estimation to ﬁt more complicated distribution. More experiments with complicated distribution estimation is in Section 12.4 in the supplementary.
Figure 2 illustrates Pareto frontiers with 5 points of different λ values in [10−2, 107] with equal logspace. To visualize the plots, we sweep hyperparameters (e.g, weights for each term in the objective function) for comparing methods. Figure 3 takes λ or hyperparameter values from the upper-right point of the Pareto frontiers in Figure 2, which indicates the best trade-off for each method. Figure 3 paper presents the 5 runs with the setup chosen based on the Pareto frontier to show the consistency of the performance of each model.
All experiments are implemented with Pytorch framework on i9-9960X CPU and a Quadro RTX 6000 GPU.
11.4 Dataset Description
We evaluate the methods on four fairness datasets. The goal for all datasets is binary classiﬁcation on binary sensitive feature. The details of the datasets are as below:
• CelebA image dataset2 (Liu et al. 2015): The data consists of 202,599 face images in diverse demographics. The images are annotated with 40 attributes (face shape, skin tone, smiling, etc.). Similar to Quadrianto et al. (Quadrianto, Sharmanska, and Thomas 2019), the goal is to predict whether a person in the image is attractive or not. The feature sex is used as the sensitive feature.
• Adult dataset from the UCI repository (Kohavi 1996) contains 48,842 instances described by 14 features (workclass, age, education, sex, race, etc) with the goal of the income prediction whether a person’s income exceeds 50K USD per year. The feature sex is used as the sensitive feature.
2http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

Model Time Model Time
Model Time Model Time
Model Time Model Time
Model Time Model Time

GSTAR 0.287 DIR 183.20
GSTAR 0.29 DIR 168
GSTAR 0.292 DIR 123.20
GSTAR 0.271 DIR 1.68

CelebA

FGP -
Eq.Odds

FACT 0.067 LAFTR

0.062 107.04(min)

Adult

FGP 51.28

FACT 0.055

Eq.Odds LAFTR 0.037 53.04(min)

Compas

FGP

FACT

43.74 Eq.Odds
0.034

0.035 LAFTR 57.04(min)

German

FGP

FACT

7.08

0.0257

Eq.Odds LAFTR 0.034 56.51(min)

CEOPost 0.077
AdvDeb 303.15
CEOPost 25.61
AdvDeb 102.00
CEOPost 8.3
AdvDeb 15.45
CEOPost 2.64
AdvDeb 2.17

Table 1: Computational time (in seconds) for all comparing fairness methods for each dataset.

• COMPAS3(Correctional Offender Management Proﬁling for Alternative Sanctions) dataset includes 6,167 samples described by 401 features with the target of recidivism prediction with the label showing if each person gets rearrested within two years. The feature race is used as the sensitive feature for this dataset.
• German credit dataset from the UCI repository (Dua and Graff 2019) contains 1,000 samples described by 20 features. The goal is to predict the credit risks. The feature sex is used as the sensitive feature.
All data is split as 70% for training and 30% for testing.
11.5 Computational Cost
In Table 1, we describe the computational time for each method on each dataset. By introducing estimated PDF functions for post-processing, we outperform other methods except Eq.Odds (Hardt, Price, and Srebro 2016) and FACT (Kim, Chen, and Talwalkar 2020). As they both only utilize the entries of the confusion matrix to ﬁnd optimal mixing rate in their methods, they have less computation than ours. However, as we discussed in the main paper, we explore better feasible region than theirs by group-speciﬁc thresholding that results better in both fairness and performance by sacriﬁcing little efﬁciency, yet outperforms most of the other works.
12 Auxiliary Experiments
12.1 GSTAR with single threshold
We conduct experiments on COMPAS dataset to evaluate GSTAR with a single adaptive thresohld. Figure 7 presents
3https://github.com/propublica/compas-analysis

Figure 6: Trend of converged θ values based on the variation of weight λ. Dashed line indicates single threshold version and θa indicates threshold for a group.

0.80

FACT Pareto

GSTAR Pareto

0.75

GSTAR(DP)

GSTAR(EOd)

0.70

GSTAR(DP+EOd)

GSTAR(DP, 1 θ)

GSTAR(EOd, 1 θ)

0.65

GSTAR(DP+EOd, 1 θ)

Baseline

0.60

Accuracy

0.55

0.50 100

10−1

10−2

EOd (smaller the better)

Figure 7: Comparison of single threshold (squares, 1θ) and group-aware threshold method (stars) on Pareto frontier. The result suggests group-aware threshold greatly improve fairness with comparable accuracy.

the trend of fairness-accuracy trade-off of two versions of GSTAR based on λ values. Comparing with the baseline (θ = 0), we observe that even with a single threshold in GSTAR (1θ in the legend), the adaptive threshold helps to improve the fairness with comparable accuracy. However the improvement is not as signiﬁcant as that of the groupwise version because it is impossible to achieve perfect fairness with a single threshold as the intersection of f1a and f0a differs by a. Figure 6 shows the trend of learned θ based on λ values. We see a single threshold version (black) lies between two thresholds of group-aware GSTAR in most cases. This implies that the single threshold converges to some point that gives up some of the fairness.
12.2 Quality of Estimated Distribution
The performance of GSTAR relies on the quality of estimated distribution. For the benchmark datasets, we empirically found that the distribution of logits resembles some parametric distributions. Thus, we estimate the distribution with generally used parametric distributions such as Student’s t-distribution by measuring the negative loglikelihood (NLL) in the training data. Note that GSTAR can be extended to a wide range of other distributions, even nonparametric distributions.

(a) Variation of estimated distribution of f by the noise factor α.
(b) The inﬂuence of the noise factor α and NLL of corresponding estimated distribution to the performance and fairness of GSTAR. Figure 8: Variation of estimated distributions by the noise α and its impact on the performance of GSTAR.
For further analysis, we add new experiments by sweeping the parameters of parametric distribution to see the effect of the estimation quality. In COMPAS dataset, the best estimate (i.e., smallest NLL) of group (y = 0, a = 0) with Student’s t-distribution has parameters of df = 2.235, loc = -0.567, scale = 0.756 based on scipy package. To generate variations as in Figure 8(a) of distributions with varying estimation qualities, we add noise α ∈ [−0.1, 100] to the scale of this distribution.

GSTAR (DP) GSTAR (EOd) GSTAR (DP + EOd)

ACC (train) 0.679 0.714 0.705

DP (train) 0.001 0.071 0.050

EOd (train) 0.089 0.030 0.030

ACC (test) 0.639 0.0643 0.641

DP (test) 0.017 0.032 0.027

EOd (test) 0.018 0.034 0.025

Table 2: Comparison of fairness and performance measure in training and testing set with different fairness constraints.

Figure 9: Distribution of synthetic dataset and its kernel density estimation.
In ﬁgure 8(b), we illustrate the trend of NLL (black), fairness violation (the lower the better), and accuracy (the higher the better) with varying noise (α, x-axis). The color of lines follows the main paper. Dashed lines indicate the quantity of baseline model (θ = 0). From this, we observed that the accuracy is the most sensitive to the change of estimation quality, while fairness is relatively stable.
However, for our experiments, we assume the estimation is reliable and the guarantee on the estimation reliability is beyond our focus of this paper.
12.3 Interpretation of Results on COMPAS
In COMPAS in Figure 3(c), we observe improvements in total fairness violation with multiple fairness constraints employed. We deduce this could happen due to: 1) generalization of the estimated distribution from training data to testing data; 2) difference in the training and testing data distributions. For the training set, we achieve better fairness violation on the model with a single constraint, compared to the multi-constrained version or other single-constrained versions. In the training set of COMPAS data, we have the results as in the Table 2.

12.4 Complicated Distribution Estimation with Kernel Density Estimation
We can generalize our density estimation to non-parametric by kernel density estimation (KDE) method. Given the logit distribution h(X), we build a histogram with B bins. Denote Tb as the mean logit value of b-th bin and wb as normalized weight indicates how many samples belong to b-th bin, where b ∈ {1, · · · , B} and b wb = 1. Then our kernel density estimator of distribution h(X) is

f (x) = wbK(x − Tb),

(38)

b

where K is kernel function and we employ normal distribution with standard deviation as 0.5. As non-parametric density estimation of h(x) can be expressed as linear combination of parametric distributions, we can easily apply the optimization step demonstrated in the section 6.
To validate the KDE method for GSTAR, we generate synthetic data that each logit distribution hya consist of mixture of three gaussian distributions with additional standard normal noise. Speciﬁcally, each distribution is conﬁgured with mean µ, variation σ2, and weight w and number of samples n as in Table 3. For example, we generate the samples from a group (Y = 0, A = 0) by sampling n00 samples x on h00 and add noise N (0, 12) as below:

h00 =

w0(i0) · N (µ(0i0), σ0(i0)2 ),

i∈{0,1,2}

l(k) ∼ h00, (k) ∼ N (0, 12), x(k) := l(k) + (k), k ∈ {1, · · · , n00}
where i is the index of gaussian distributions in Table 3 and k is the index of sampling instance.

µ

σ2

w

n

h00 [-7.0, -2.0, 1.1] [3.0, 1.5, 2.0] [0.3, 0.5, 0.2] 5000

h01 [-4.5, -1.2, 1.2] [1.2, 1.5, 2.0] [0.3, 0.5, 0.2] 10000

h10 [-1.8, 1.5, 6.0] [1.2, 1.3, 2.0] [0.2, 0.5, 0.3] 15000

h11 [-1.1, 2.3, 7.0] [1.2, 1.5, 2.0] [0.2, 0.4, 0.4] 10000

Table 3: Conﬁguration of each synthetic data distribution hya.

Figure 9 illustrates histograms of logit h distributions of synthetic data and their KDE results in colored lines. The top plot is about positive samples i.e., h11 and h10, and the bottom plot is about positive samples i.e., h00 and h01 respectively. We could observe that KDE accurately estimated the density function h that cannot be ﬁtted with parametric distribution.

(a) Lfair = LDfaPir

(b) Lfair = LEfaOird

(c) Lf air = LDf aPir+EOd

Figure 10: Trend of performance and fairness measure by the change of λ values. Color of the lines indicates the measure of performance and fairness as in the legend. Solid lines indicate GSTAR results and dotted lines indicate baseline (θ = (0, 0)) respectively. It is lower the better for fairness and higher the better for accuracy.

(a) Lfair = LDfaPir

(b) Lfair = LEfaOird

(c) Lf air = LDf aPir+EOd

Figure 11: Trend of converged group-aware threshold θ achieved by GSTAR.

Moreover, we conduct experiments to validate GSTAR can achieve the proposed goal. Given 4 probability distributions and number of samples for each group as in Table 3, we divide the dataset into training (70%), validation (15%), and testing (15%) set. We train GSTAR on training set and ﬁnd the best θ by selecting one that has minimum validation loss and report the result on testing set.
In Figure 10 and 11, we quantitatively evaluate GSTAR with KDE method with different λ values on fairness constraint Lfair. In Figure 10, color of the lines are performance and fairness measure as described in the legend. Dotted lines indicate baseline (θ = (0, 0)) and solid lines indicate the measures of GSTAR. Note that GSTAR improve target fairness signiﬁcantly with small lose of accuracy. In DP+EOd constraint, we even achieve almost perfect equal opportunity i.e., Eq. Opp ≈ 0 with high enough λ values.
References
Barocas, S.; and Selbst, A. D. 2016. Big data’s disparate impact. Calif. L. Rev., 104: 671.
Chouldechova, A. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2): 153–163.
Corbett-Davies, S.; Pierson, E.; Feller, A.; Goel, S.; and Huq, A. 2017. Algorithmic decision making and the cost of fairness. In KDD, 797–806.

Dressel, J.; and Farid, H. 2018. The accuracy, fairness, and limits of predicting recidivism. Sci. Adv, 4(1): eaao5580.
Dua, D.; and Graff, C. 2019. UCI Machine Learning Repository.
Feldman, M.; Friedler, S. A.; Moeller, J.; Scheidegger, C.; and Venkatasubramanian, S. 2015. Certifying and removing disparate impact. In KDD, 259–268.
Gratton, S.; Lawless, A. S.; and Nichols, N. K. 2007. Approximate Gauss–Newton methods for nonlinear least squares problems. SIAM, 18(1): 106–132.
Hardt, M.; Price, E.; and Srebro, N. 2016. Equality of opportunity in supervised learning. In NeurIPS, 3315–3323.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770–778.
Kamiran, F.; Karim, A.; and Zhang, X. 2012. Decision theory for discrimination-aware classiﬁcation. In ICDM, 924– 929. IEEE.
Kim, J. S.; Chen, J.; and Talwalkar, A. 2020. ModelAgnostic Characterization of Fairness Trade-offs. arXiv preprint arXiv:2004.03424.
Kleinberg, J.; Mullainathan, S.; and Raghavan, M. 2016. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807.
Kohavi, R. 1996. Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree hybrid. In KDD, volume 96, 202–207.

Liu, L. T.; Simchowitz, M.; and Hardt, M. 2019. The implicit fairness criterion of unconstrained learning. In ICML, 4051–4060.
Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep learning face attributes in the wild. In ICCV, 3730–3738.
Lokhande, V. S.; Akash, A. K.; Ravi, S. N.; and Singh, V. 2020. FairALM: Augmented Lagrangian Method for Training Fair Models with Little Regret. In ECCV, 365–381. Springer.
Madras, D.; Creager, E.; Pitassi, T.; and Zemel, R. 2018. Learning adversarially fair and transferable representations. arXiv preprint arXiv:1802.06309.
Menon, A. K.; and Williamson, R. C. 2018. The cost of fairness in binary classiﬁcation. In ACM FAccT, 107–118.
Pedreshi, D.; Ruggieri, S.; and Turini, F. 2008. Discrimination-aware data mining. In KDD, 560–568.
Pleiss, G.; Raghavan, M.; Wu, F.; Kleinberg, J.; and Weinberger, K. Q. 2017. On fairness and calibration. In NeurIPS, 5680–5689.
Quadrianto, N.; Sharmanska, V.; and Thomas, O. 2019. Discovering fair representations in the data domain. In CVPR, 8227–8236.
Regulation, G. D. P. 2016. Regulation EU 2016/679 of the European Parliament and of the Council of 27 April 2016. Ofﬁcial Journal of the European Union. Available at: http://ec. europa. eu/justice/dataprotection/reform/ﬁles/regulation oj en. pdf (accessed 20 September 2017).
Tan, Z.; Yeom, S.; Fredrikson, M.; and Talwalkar, A. 2020. Learning fair representations for kernel models. In AISTATS, 155–166.
Zhang, B. H.; Lemoine, B.; and Mitchell, M. 2018. Mitigating unwanted biases with adversarial learning. In AIES, 335–340.
Zhao, H.; and Gordon, G. 2019. Inherent tradeoffs in learning fair representations. In NeurIPS, 15675–15685.

