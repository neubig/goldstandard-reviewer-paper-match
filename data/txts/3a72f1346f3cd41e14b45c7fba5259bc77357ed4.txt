Journal of Arti cial Intelligence Research 2 (1995) 541-573

Submitted 9/94; published 5/95

Pac-learning Recursive Logic Programs: Negative Results

William W. Cohen
AT&T Bell Laboratories 600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract
In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is e ciently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of paclearnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of e cient learnability for recursive function-free clauses.
1. Introduction
Inductive logic programming (ILP) (Muggleton, 1992; Muggleton & De Raedt, 1994) is an active area of machine learning research in which the hypotheses of a learning system are expressed in a logic programming language. While many di erent learning problems have been considered in ILP, including some of great practical interest (Muggleton, King, & Sternberg, 1992; King, Muggleton, Lewis, & Sternberg, 1992; Zelle & Mooney, 1994; Cohen, 1994b), a class of problems that is frequently considered is to reconstruct simple list-processing or arithmetic functions from examples. A prototypical problem of this sort might be learning to append two lists. Often, this sort of task is attempted using only randomly-selected positive and negative examples of the target concept.
Based on its similarity to the problems studied in the eld of automatic programming from examples (Summers, 1977; Biermann, 1978), we will (informally) call this class of learning tasks automatic logic programming problems. While a number of experimental systems have been built (Quinlan & Cameron-Jones, 1993; Aha, Lapointe, Ling, & Matwin, 1994), the experimental success in automatic logic programming systems has been limited. One common property of automatic logic programming problems is the presence of recursion. The goal of this paper is to explore by analytic methods the computational limitations on learning recursive programs in Valiant's model of pac-learnability (1984). (In brief, this model requires that an accurate approximation of the target concept be found in polynomial time using a polynomial-sized set of labeled examples, which are chosen stochastically.) While it will surprise nobody that such limitations exist, it is far from obvious from previous

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Cohen
research where these limits lie: there are few provably fast methods for learning recursive logic programs, and even fewer meaningful negative results.
The starting point for this investigation is a series of positive learnability results appearing in a companion paper (Cohen, 1995). These results show that a single constant-depth determinate clause with a constant number of \closed" recursive calls is pac-learnable. They also show that a two-clause constant-depth determinate program consisting of one nonrecursive clause and one recursive clause of the type described above is pac-learnable, if some additional \hints" about the target concept are provided.
In this paper, we analyze a number of generalizations of these learnable languages. We show that that relaxing any of the restrictions leads to di cult learning problems: in particular, learning problems that are either as hard as learning DNF (an open problem in computational learning theory), or as hard as cracking certain presumably secure cryptographic schemes. The main contribution of this paper, therefore, is a delineation of the boundaries of learnability for recursive logic programs.
The paper is organized as follows. In Section 2 we de ne the classes of logic programs and the learnability models that are used in this paper. In Section 3 we present cryptographic hardness results for two classes of constant-depth determinate recursive programs: programs with n linear recursive clauses, and programs with one n-ary recursive clause. We also analyze the learnability of clauses of constant locality, another class of clauses that is paclearnable in the nonrecursive case, and show that even a single linearly recursive local clause is cryptographically hard to learn. We then turn, in Section 4, to the analysis of even more restricted classes of recursive programs. We show that two di erent classes of constant-depth determinate programs are prediction-equivalent to boolean DNF: the class of programs containing a single linear recursive clause and a single nonrecursive clause, and the class of programs containing two linearly recursive clauses. Finally, we summarize the results of this paper and its companion, discuss related work, and conclude.
Although this paper can be read independently of its companion paper we suggest that readers planning to read both papers begin with the companion paper (Cohen, 1995).
2. Background
For completeness, we will now present the technical background needed to state our results; however, aside from Sections 2.2 and 2.3, which introduce polynomial predictability and prediction-preserving reducibilities, respectively, this background closely follows that presented in the companion paper (Cohen, 1995). Readers are encouraged to skip this section if they are already familiar with the material.
2.1 Logic Programs
We will assume that the reader has some familiarity in logic programming (such as can be obtained by reading one of the standard texts (Lloyd, 1987).) Our treatment of logic programs di ers only in that we will usually consider the body of a clause to be an ordered set of literals. We will also consider only logic programs without function symbols|i.e., programs written in Datalog.
The semantics of a Datalog program P will be de ned relative to to a database, DB, which is a set of ground atomic facts. (When convenient, we will also think of DB as a
542

Pac-Learning Recursive Logic Programs: Negative Results
conjunction of ground unit clauses). In particular, we will interpret P and DB as a subset of the set of all extended instances. An extended instance is a pair (f; D) in which the instance fact f is a ground fact, and the description D is a set of ground unit clauses. An extended instance (f; D) is covered by (P; DB) i
DB ^ D ^ P ` f
If extended instances are allowed, then function-free programs can encode many computations that are usually represented with function symbols. For example, a function-free program that tests to see if a list is the append of two other lists can be written as follows:
Program P:
append(Xs,Ys,Ys) null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^ components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).
Database DB:
null(nil). Here the predicate components(A,B,C) means that A is a list with head B and tail C; thus an extended instance equivalent to append( 1,2], 3], 1,2,3]) would have the instance fact f = append(list12; list3; list123) and a description containing these atoms: components(list12,1,list2), components(list2,2,nil), components(list123,1,list23), components(list23,2,list3), components(list3,3,nil) The use of extended instances and function-free programs is closely related to \ attening" (Rouveirol, 1994; De Raedt & Dzeroski, 1994); some experimental learning systems also impose a similar restriction (Quinlan, 1990; Pazzani & Kibler, 1992). Another motivation for using extended instances is technical. Under the (sometimes quite severe) syntactic restrictions considered in this paper, there are often only a polynomial number of possible ground facts|i.e., the Herbrand base is polynomial. Hence if programs were interpreted in the usual model-theoretic way it would be possible to learn a program equivalent to any given target by simply memorizing the appropriate subset of the Herbrand base. However, if programs are interpreted as sets of extended instances, such trivial learning algorithms become impossible; even for extremely restricted program classes there are still an exponential number of extended instances of size n. Further discussion can be found in the companion paper (Cohen, 1995).
Below we will de ne some of the terminology for logic programs that will be used in this paper.
2.1.1 Input/Output Variables
If A B1 ^ : : :^ Br is an (ordered) de nite clause, then the input variables of the literal Bi are those variables which also appear in the clause A B1 ^ : : : ^ Bi 1; all other variables appearing in Bi are called output variables.
543

Cohen
2.1.2 Types of Recursion
A literal in the body of a clause is a recursive literal if it has the same predicate symbol and arity as the head of the clause. If every clause in a program has at most one recursive literal, the program is linear recursive. If every clause in a program has at most k recursive literals, the program is k-ary recursive. If every recursive literal in a program contains no output variables, the program is closed recursive.
2.1.3 Depth
The depth of a variable appearing in a (ordered) clause A B1^: : :^Br is de ned as follows. Variables appearing in the head of a clause have depth zero. Otherwise, let Bi be the rst literal containing the variable V , and let d be the maximal depth of the input variables of Bi; then the depth of V is d+1. The depth of a clause is the maximal depth of any variable
in the clause.
2.1.4 Determinacy
The literal Bi in the clause A B1^: : :^Br is determinate i for every possible substitution that uni es A with some fact e such that DB ` B1 ^ : : : ^ Bi 1
there is at most one maximal substitution so that DB ` Bi . A clause is determinate
if all of its literals are determinate. Informally, determinate clauses are those that can be evaluated without backtracking by a Prolog interpreter.
The term ij-determinate (Muggleton & Feng, 1992) is sometimes used for programs that are depth i, determinate, and contain literals of arity j or less. A number of experimental systems exploit restrictions associated with limited depth and determinacy (Muggleton & Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). The learnability of constant-depth determinate clauses has also received some formal study (Dzeroski, Muggleton, & Russell, 1992; Cohen, 1993a).
2.1.5 Mode Constraints and Declarations
Mode declarations are commonly used in analyzing Prolog code or describing Prolog code; for instance, the mode declaration \components(+; ; )" indicates that the predicate components can be used when its rst argument is an input and its second and third arguments are outputs. Formally, we de ne the mode of a literal L appearing in a clause C to be a string s such that the initial character of s is the predicate symbol of L, and for j > 1 the j-th character of s is a \+" if the (j 1)-th argument of L is an input variable and a \ " if the (j 1)-th argument of L is an output variable. (This de nition assumes that all arguments to the head of a clause are inputs; this is justi ed since we are considering only how clauses behave in classifying extended instances, which are ground.) A mode constraint
is a set of mode strings R = fs1; : : :; skg, and a clause C is said to satisfy a mode constraint R for p if for every literal L in the body of C, the mode of L is in R.
We de ne a declaration to be a tuple (p; a0; R) where p is a predicate symbol, a0 is an integer, and R is a mode constraint. We will say that a clause C satis es a declaration if
544

Pac-Learning Recursive Logic Programs: Negative Results
the head of C has arity a0 and predicate symbol p, and if for every literal L in the body of C the mode of L appears in R.
2.1.6 Determinate Modes
In a typical setting, that facts in the database DB and extended instances are not arbitrary: instead, they are representative of some \real" predicate, which may obey certain restrictions. Let us assume that all database and extended-instance facts will be drawn from some
(possibly in nite) set F. Informally, a mode is determinate if the input positions of the facts in F functionally determine the output positions. Formally, if f = p(t1; : : :; tk) is a fact with predicate symbol p and p is a mode, then de ne inputs(f; p ) to be hti1; : : :; ti i, where i1, : : :, ik are the indices of containing a \+", and de ne outputs(f; p ) to kbe htj1; : : :; tj i, where j1, : : :, jl are the indices of containing a \ ". We de ne a mode string p flor a predicate p to be determinate for F i
fhinputs(f; p ); outputs(f; p )i : f 2 Fg is a function. Any clause that satis es a declaration Dec 2 DetDEC must be determinate.
The set of all declarations containing only modes determinate for F will be denoted DetDECF. Since in this paper the set F will be assumed to be xed, we will generally omit
the subscript.
2.1.7 Bounds on Predicate Arity
We will use the notation a-DB for the set of all databases that contain only facts of arity a or less, and a-DEC for the set of all declarations (p; a0; R) such that every string s 2 R is of length a + 1 or less.
2.1.8 Size Measures
The learning models presented in the following section will require the learner to use resources polynomial in the size of its inputs. Assuming that all predicates are arity a or less for some constant a allows very simple size measures to be used. In this paper, we will measure the size of a database DB by its cardinality; the size of an extended instance (f; D) by the cardinality of D; the size of a declaration (p; a0; R) by the cardinality of R; and the
size of a clause A B1 ^ : : : ^ Br by the number of literals in its body. 2.2 A Model of Learnability
2.2.1 Preliminaries
Let X be a set. We will call X the domain, and call the elements of X instances. De ne a concept C over X to be a representation of some subset of X, and de ne a language Lang to be a set of concepts. In this paper, we will be rather casual about the distinction between a concept and the set it represents; when there is a risk of confusion we will refer to the set represented by a concept C as the extension of C. Two sets C1 and C2 with the same extension are said to be equivalent. De ne an example of C to be a pair (e; b) where b = 1 if e 2 C and b = 0 otherwise. If D is a probability distribution function, a sample of C from
545

Cohen

X drawn according to D is a pair of multisets S+; S drawn from the domain X according to D, S+ containing only positive examples of C, and S containing only negative ones.
Associated with X and Lang are two size complexity measures, for which we will use the following notation:
The size complexity of a concept C 2 Lang is written jCj. The size complexity of an instance e 2 X is written jej. If S is a set, Sn stands for the set of all elements of S of size complexity no greater than n. For instance, Xn = fe 2 X : jej ng and Langn = fC 2 Lang : jCj ng.
We will assume that all size measures are polynomially related to the number of bits needed to represent C or e; this holds, for example, for the size measures for logic programs and databases de ned above.

2.2.2 Polynomial Predictability

We now de ne polynomial predictability as follows. A language Lang is polynomially

predictable i there is an algorithm PacPredict and a polynomial function m(1; 1; ne; nt)

so that for every nt > 0, every ne > 0, every C 2 Langn , every : 0 < < 1, every

:0<

<

1,

and

every

probability

distribution

function

D,

t
PacPredict

has

the

following

behavior:

1. given a sample S+; S of C from Xn drawn according to D and containing at least

m( 1 ;

1;

ne;

nt)

examples,

e
PacPredict

outputs

a

hypothesis

H

such

that

Prob(D(H C) + D(C H) > ) <

where the probability is taken over the possible samples S+ and S and (if PacPredict is a randomized algorithm) over any coin ips made by PacPredict;

2. PacPredict runs in time polynomial in 1, 1, ne, nt, and the number of examples; and

3. The hypothesis H can be evaluated in polynomial time.

The algorithm PacPredict is called a prediction algorithm for Lang, and the function m(1; 1; ne; nt) is called the sample complexity of PacPredict. We will sometimes abbreviate \polynomial predictability" as \predictability".
The rst condition in the de nition merely states that the error rate of the hypothesis must (usually) be low, as measured against the probability distribution D from which the training examples were drawn. The second condition, together with the stipulation that the sample size is polynomial, ensures that the total running time of the learner is polynomial. The nal condition simply requires that the hypothesis be usable in the very weak sense that it can be used to make predictions in polynomial time. Notice that this is a worst case learning model, as the de nition allows an adversarial choice of all the inputs of the learner.

546

Pac-Learning Recursive Logic Programs: Negative Results
2.2.3 Relation to Other Models
The model of polynomial predictability has been well-studied (Pitt & Warmuth, 1990), and is a weaker version of Valiant's (1984) criterion of pac-learnability. A language Lang is pac-learnable i there is an algorithm PacLearn so that
1. PacLearn satis es all the requirements in the de nition of polynomial predictability, and
2. on inputs S+ and S , PacLearn always outputs a hypothesis H 2 Lang.
Thus if a language is pac-learnable it is predictable. In the companion paper (Cohen, 1995), our positive results are all expressed in the model
of identi ability from equivalence queries, which is strictly stronger than pac-learnability; that is, anything that is learnable from equivalence queries is also necessarily pac-learnable.1 Since this paper contains only negative results, we will use the the relatively weak model of predictability. Negative results in this model immediately translate to negative results in the stronger models; if a language is not predictable, it cannot be pac-learnable, nor identi able from equivalence queries.
2.2.4 Background Knowledge in Learning
In a typical ILP system, the setting is slightly di erent, as the user usually provides clues about the target concept in addition to the examples, in the form of a database DB of \background knowledge" and a set of declarations. To account for these additional inputs it is necessary to extend the framework described above to a setting where the learner accepts inputs other than training examples. Following the formalization used in the companion paper (Cohen, 1995), we will adopt the notion of a \language family".
If Lang is a set of clauses, DB is a database and Dec is a declaration, we will de ne
Lang DB; Dec] to be the set of all pairs (C; DB) such that C 2 Lang and C satis es Dec. Semantically, such a pair will denote the set of all extended instances (f; D) covered by (C; DB). Next, if DB is a set of databases and DEC is a set of declarations, then de ne
Lang DB; DEC] = fLang DB; Dec] : DB 2 DB and Dec 2 DECg
This set of languages is called a language family. We will now extend the de nition of predictability queries to language families as follows.
A language family Lang DB; DEC] is polynomially predictable i every language in the set is predictable. A language family Lang DB; DEC] is polynomially predictable i there is a single algorithm Identify(DB; Dec) that predicts every Lang DB; Dec] in the family
given DB and Dec. The usual model of polynomial predictability is worst-case over all choices of the target
concept and the distribution of examples. The notion of polynomial predictability of a language family extends this model in the natural way; the extended model is also worst-
case over all possible choices for database DB 2 DB and Dec 2 DEC. This worst-case
1. An equivalence query is a question of the form \is H equivalent to the target concept?" which is answered with either \yes" or a counterexample. Identi cation by equivalence queries essentially means that the target concept can be exactly identi ed in polynomial time using a polynomial of such queries.
547

Cohen
model may seem unintuitive, since one typically assumes that the database DB is provided by a helpful user, rather than an adversary. However, the worst-case model is reasonable because learning is allowed to take time polynomial in the size of smallest target concept in the set Lang DB; Dec]; this means that if the database given by the user is such that the target concept cannot be encoded succinctly (or at all) learning is allowed to take more time.
Notice that for a language family Lang DB; Dec] to be polynomially predictable, every language in the family must be polynomially predictable. Thus to show that a family is not polynomially predictable it is su cient to construct one language in the family for which learning is hard. The proofs of this paper will all have this form.
2.3 Prediction-Preserving Reducibilities
The principle technical tool used in our negative results in the notion of prediction-preserving reducibility, as introduced by Pitt and Warmuth (1990). Prediction-preserving reducibilities are a method of showing that one language is no harder to predict than another. Formally, let Lang1 be a language over domain X1 and Lang2 be a language over domain X2. We say that predicting Lang1 reduces to predicting Lang2, denoted Lang1 Lang2, if
there is a function fi : X1 ! X2, henceforth called the instance mapping, and a function fc : Lang1 ! Lang2, henceforth called the concept mapping, so that the following all hold:
1. x 2 C if and only if fi(x) 2 fc(C) | i.e., concept membership is preserved by the
mappings; 2. the size complexity of fc(C) is polynomial in the size complexity of C|i.e., the size
of concept representations is preserved within a polynomial factor; 3. fi(x) can be computed in polynomial time. Note that fc need not be computable; also, since fi can be computed in polynomial time, fi(x) must also preserve size within a polynomial factor.
Intuitively, fc(C1) returns a concept C2 2 Lang2 that will \emulate" C1|i.e., make
the same decisions about concept membership|on examples that have been \preprocessed" with the function fi. If predicting Lang1 reduces to predicting Lang2 and a learning algorithm for Lang2 exists, then one possible scheme for learning concepts from Lang1 would be the following. First, convert any examples of the unknown concept C1 from the domain X1 to examples over the domain X2 using the instance mapping fi. If the conditions of the de nition hold, then since C1 is consistent with the original examples, the concept fc(C1) will be consistent with their image under fi; thus running the learning algorithm for Lang2 should produce some hypothesis H that is a good approximation of fc(C1). Of course, it may not be possible to map H back into the original language Lang1, as computing fc 1 may be di cult or impossible. However, H can still be used to predict membership in C1: given an example x from the original domain X1, one can simply predict
x 2 C1 to be true whenever fi(x) 2 H.
Pitt and Warmuth (1988) give a more rigorous argument that this approach leads to a prediction algorithm for Lang1, leading to the following theorem.
548

Pac-Learning Recursive Logic Programs: Negative Results
Theorem 1 (Pitt and Warmuth) Assume Lang1 Lang2. Then if Lang1 is not poly-
nomially predictable, Lang2 is not polynomially predictable.
3. Cryptographic Limitations on Learning Recursive Programs
Theorem 1 allows one to transfer hardness results from one language to another. This is useful because for a number of languages, it is known that prediction is as hard as breaking cryptographic schemes that are widely assumed to be secure. For example, it is known that predicting the class of languages accepted by deterministic nite state automata is \cryptographically hard", as is the class of languages accepted by log-space bounded Turing machines.
In this section we will make use of Theorem 1 and previous cryptographic hardness results to show that certain restricted classes of recursive logic programs are hard to learn.
3.1 Programs With n Linear Recursive Clauses
In a companion paper (Cohen, 1995) we showed that a single linear closed recursive clause was identi able from equivalence queries. In this section we will show that a program with a polynomial number of such clauses is not identi able from equivalence queries, nor even polynomially predictable.
Speci cally, let us extend our notion of a \family of languages" slightly, and let DLog n; s] represent the language of log-space bounded deterministic Turing machines with up to s states accepting inputs of size n or less, with the usual semantics and complexity measure.2 Also let d-DepthLinRecProg denote the family of logic programs containing only depth-d linear closed recursive clauses, but containing any number of such clauses. We have the following result:
Theorem 2 For every n and s, there exists a database DBn;s 2 1-DB and declaration Decn;s 2 1-DetDEC of sizes polynomial in n and s such that
DLog n; s] 1-DepthLinRecProg DB ;n;s Decn;s] Hence for d 1 and a 1, d-DepthLinRecProg DB; a-DetDEC] is not uniformly poly-
nomially predictable under cryptographic assumptions.3
Proof: Recall that a log-space bounded Turing machine (TM) has an input tape of length
n, a work tape of length log2 n which initially contains all zeros, and a nite state control with state set Q. To simplify the proof, we assume without loss of generality that the tape and input alphabets are binary, that there is a single accepting state qf 2 Q, and that the machine will always erase its work tape and position the work tape head at the far left after it decides to accept its input.
At each time step, the machine will read the tape squares under its input tape head and work tape head, and based on these values and its current state q, it will
2. , I.e. a machine represents the set of all inputs that it accepts, and its complexity is the number of states. 3. Speci cally, this language is not uniformly polynomially predictable unless all of the following crypto-
graphic problems can be solved in polynomial time: solving the quadratic residue problem, inverting the RSA encryption function, and factoring Blum integers. This result holds because all of these cryptographic problems can be reduced to learning DLOG Turing machines (Kearns & Valiant, 1989).
549

Cohen
write either a 1 or a 0 on the work tape, shift the input tape head left or right, shift the work tape head left or right, and transition to a new internal state q0 A deterministic machine can thus be speci ed by a transition function
: f0; 1g f0; 1g Q ! f0; 1g fL; Rg fL; Rg Q
Let us de ne the internal con guration of a TM to consist of the string of symbols written on the worktape, the position of the tape heads, and the internal state q of the machine: thus a con guration is an element of the set
CON f0; 1glog2 n f1; : : :; log2 ng f1; : : :; ng Q
A simpli ed speci cation for the machine is the transition function
0 : f0; 1g CON ! CON where the component f0; 1g represents the contents of the input tape at the square below
the input tape head. Notice that for a machine whose worktape size is bounded by log n, the cardinality of
CON is only p = jQjn2 log2 n, a polynomial in n and s = jQj. We will use this fact in our
constructions. The background database DBn;s is as follows. First, for i = 0; : : :; p, an atom of the
form coni(ci) is present. Each constant ci will represent a di erent internal con guration of the Turing machine. We will also arbitrarily select c1 to represent the (unique) accepting con guration, and add to DBn;s the atom accepting(c1). Thus
DBn;s fconi(ci)gpi=1 faccepting(c1)g
Next, we de ne the instance mapping. An instance in the Turing machine's domain is a binary string X = b1 : : :bn; this is mapped by fi to the extended instance (f; D) where
f accepting(c0) D ftruei gbi2X:bi=1 ffalseigbi2X:bi=0 The description atoms have the e ect of de ning the predicate truei to be true i the i-th bit of X is a \1", and the de ning the predicate falsei to be true i the i-th bit of X is \0". The constant c0 will represent the start con guration of the Turing machine, and the predicate accepting(C) will be de ned so that it is true i the Turing machine accepts input X starting from state C. We will let Decn;s = (accepting; 1; R) where R contains the modes coni(+) and coni( ), for i = 1; : : :; p; and truej and falsej for j = 1; : : :; n. Finally, for the concept mapping fc, let us assume some arbitrary one-to-one mapping between the internal con gurations of a Turing machine M and the predicate names
550

Pac-Learning Recursive Logic Programs: Negative Results
con0,: : :,conp 1 such that the start con guration (0log2 n; 1; q0) maps to con0 and the accepting con guration (0log2 n; 1; qf) maps to con1. We will construct the program fc(M) as follows. For each transition 0(1; c) ! c0 in 0, where c and c0 are in CON, construct a clause of the form
accepting(C) conj(C) ^ truei ^ conj0(C1) ^ accepting(C1). where i is the position of the input tape head which is encoded in c, conj = (c), and conj0 = (c0). For each transition 0(0; c) ! (c0) in 0 construct an analogous clause, in
which truei is replaced with falsei. Now, we claim that for this program P, the machine M will accept when started in
con guration ci i DBn;s ^ D ^ P ` accepting(ci)
and hence that this construction preserves concept membership. This is perhaps easiest to see by considering the action of a top-down theorem prover when given the goal accepting(C ): the sequence of subgoals accepting(ci), accepting(ci+1), : : :generated by the theorem-prover precisely parallel the sequence of con gurations ci, : : :entered by the Turing machine.
It is easily veri ed that the size of this program is polynomial in n and s, and that the clauses are linear recursive, determinate, and of depth one, completing the proof.
There are number of ways in which this result can be strengthened. Precisely the same construction used above can be used to reduce the class of nondeterministic log-space bounded Turing machines to the constant-depth determinate linear recursive programs. Further, a slight modi cation to the construction can be used to reduce the class of log-space bounded alternating Turing machines (Chandra, Kozen, & Stockmeyer, 1981) to constantdepth determinate 2-ary recursive programs. The modi cation is to emulate con gurations corresponding to universal states of the Turing machine with clauses of the form accepting(C)
conj(C) ^ truei ^ conj10(C1) ^ accepting(C1) ^ conj20(C2) ^ accepting(C2).
where conj10 and conj20 are the two successors to the universal con guration conj. This is a very strong result, since log-space bounded alternating Turing machines are known to be able to perform every polynomial-time computation.
3.2 Programs With One n-ary Recursive Clause
We will now consider learning a single recursive clause with arbitrary closed recursion. Again, the key result of this section is an observation about expressive power: there is a background database that allows every log-space deterministic Turing machine M to be emulated by a single recursive constant-depth determinate clause. This leads to the following negative predictability result.
551

Cohen
Theorem 3 For every n and s, there exists a database DBn;s 2 3-DB and declaration Decn;s 2 3-DetDEC of sizes polynomial in n and s such that
DLog n; s] 3-DepthRec DB ;n;s Decn;s] Hence for d 3 and a 3, d-DepthRec DBn; a-DetDEC] is not uniformly polynomially
predictable under cryptographic assumptions.
Proof: Consider a DLOG machine M. As in the proof of Theorem 2, we assume without loss of generality that the tape alphabet is f0; 1g, that there is a unique starting con gura-
tion c0, and that there is a unique accepting con guration c1. We will also assume without loss of generality that there is a unique \failing" con guration cfail; and that there is exactly one transition of the form
0(b; cj) ! c0j for every combination of i 2 f1; : : :; ng, b 2 f0; 1g, and cj 2 CON fc1; cfailg. Thus on input X = x1 : : :xn the machine M starts with CONFIG=c0, then executes transitions until it reaches CONFIG=c1 or CONFIG=cfail, at which point X is accepted or rejected (respectively). We will use p for the number of con gurations. (Recall that p is polynomial in n and s.)
To emulate M, we will convert an example X = b1 : : :bn into the extended instance fi(X) = (f; D) where
f accepting(c0) D fbiti(bi)gni=1 Thus the predicate biti(X) binds X to the i-th bit of the TM's input tape. We also will de ne the following predicates in the background database DBn;s. For every possible b 2 f0; 1g and j : 1 j p(n), the predicate statusb;j(B,C,Y) will be de ned so that given bindings for variables B and C, statusb;j(B,C,Y) will fail if C = c ; fail otherwise it will succeed, binding Y to active if B = b and C = cj and binding Y to inactive otherwise. For j : 1 j p(n), the predicate nextj(Y,C) will succeed i Y can be bound to either active or inactive. If Y = , then C will be bound to cj; otherwise, C will be bound to the accepting con guration c1. The database also contains the fact accepting(c1). It is easy to show that the size of this database is polynomial in n and s. The declaration Decn;s is de ned to be (accepting; 1; R) where R includes the modes statusbj(+; +; ), nextj(+; ), and biti( ) for b 2 f0; 1g, j = 1; : : :; p, and i = 1; : : :; n. Now, consider the transition rule 0(b; cj) ! c0j, and the corresponding conjunction TRANSibj biti(Bibj) ^ statusb;j(C,Bibj,Yibj) ^ nextj0(Yibj,C1ibj) ^ accepting(C1ibj)
552

Pac-Learning Recursive Logic Programs: Negative Results

Given DBn;s and D, and assuming that C is bound to some con guration c, this conjunction

will fail if c = c . fail It will succeed if xi 6= b or c 6= cj; in this case Yibj will be bound to

inactive, C1ibj will be bound to c1, and the recursive call succeeds because accepting(c1) is in

DBn;s. Finally, if xi = b and c = cj, TRANSibj will succeed only if the atom accepting(cj0)

is provable; in this case, Yibj will be bound to active and C1ibj will be bound to cj0.

From this it is clear that the clause fc(M) below

accepting(C)

^ TRANSibj

i2f1;:::;ng; b2f0;1g j2f1;:::;pg

will correctly emulate the machine M on examples that have been preprocessed with the function fi described above. Hence this construction preserves concept membership. It is also easily veri ed that the size of this program is polynomial in n and s, and that the clause is determinate and of depth three.

3.3 One k-Local Linear Closed Recursive Clause
So far we have considered only one class of extensions to the positive result given in the companion paper (Cohen, 1995)|namely, relaxing the restrictions imposed on the recursive structure of the target program. Another reasonable question to ask is if linear closed recursive programs can be learned without the restriction of constant-depth determinacy.
In earlier papers (Cohen, 1993a, 1994a, 1993b) we have studied the conditions under which the constant-depth determinacy restriction can be relaxed while still allowing learnability for nonrecursive clauses. It turns out that most generalizations of constant-depth determinate clauses are not predictable, even without recursion. However, the language of nonrecursive clauses of constant locality is a pac-learnable generalization of constant-depth determinate clauses. Below, we will de ne this language, summarize the relevant previous results, and then address the question of the learnability of recursive local clauses.
De ne a variable V appearing in a clause C to be free if it appears in the body of C but not the head of C. Let V1 and V2 be two free variables appearing in a clause. V1 touches V2 if they appear in the same literal, and V1 in uences V2 if it either touches V2, or if it touches some variable V3 that in uences V2. The locale of a free variable V is the set of literals that either contain V , or that contain some free variable in uenced by V . Informally, variable V1 in uences variable V2 if the choice of a binding for V1 can a ect the possible choices of bindings for V2.
The locality of a clause is the size of its largest locale. Let k-LocalNonRec denote the language of nonrecursive clauses with locality k or less. (That is, k-LocalNonRec is the set of logic programs containing a single nonrecursive k-local clause.) The following facts are known (Cohen, 1993b):
For xed k and a, the language family k-LocalNonRec a-DB; a-DEC] is uniformly
pac-learnable.
For every constant d, every constant a, every database DB 2 a-DB, every declaration Dec 2 a-DetDEC, and every clause C 2 d-DepthNonRec DB; Dec], there is an
553

Cohen
equivalent clause C0 in k-LocalNonRec DB; Dec] of size bounded by kjCj, where k is a function only of a and d (and hence is a constant if d and a are also constants.)
Hence
k-LocalNonRec DB; a-DEC]
is a pac-learnable generalization of
d-DepthNonRec DB; a-DetDEC] It is thus plausible to ask if recursive programs of k-local clauses are pac-learnable. Some facts about the learnability of k-local programs follow immediately from previous results.
For example, an immediate consequence of the construction of Theorem 2 is that programs with a polynomial number of linear recursive k-local clauses are not predictable for k 2. Similarly, Theorem 3 shows that a single recursive k-local clause is not predictable for k 4.
It is still reasonable to ask, however, if the positive result for bounded-depth determinate recursive clauses (Cohen, 1995) can be extended to k-ary closed recursive k-local clauses. Unfortunately, we have the following negative result, which shows that even linear closed recursive clauses are not learnable.
Theorem 4 Let Dfa s] denote the language of deterministic nite automata with s states,
and let k-LocalLinRec be the set of linear closed recursive k-local clauses. For any constant s there exists a database DBs 2 3-DB and a declaration Decs 2 3-DEC, both of size polynomial in s, such that
Dfa s] 3-LocalLinRec DBs; Decs] Hence for k 3 and a 3, k-LocalLinRec a-DB; Dec] is not uniformly polynomially
predictable under cryptographic assumptions.
Proof: Following Hopcroft and Ullman (1979) we will represent a DFA M over the alphabet
as a tuple (q0; Q; F; ) where q0 is the initial state, Q is the set of states, F is the set of accepting states, and : Q ! Q is the transition function (which we will sometimes think of as a subset of Q Q). To prove the theorem, we need to construct a database DBs of size polynomial in s such that every s-state DFA can be emulated by a linear recursive k-local clause over DBs.
Rather than directly emulating M, it will be convenient to emulate instead a modi cation of M. Let M^ be a DFA with state set Q^ Q fq( 1); qe; qfg, where q( 1), qe and qf are new states not found in Q. The initial state of M^ is q( 1). The only nal state of M^ is qf. The transition function of M^ is
^ f(q( 1); a; q0); (qe; c; qf)g f(qi; b; qe)g
2qi F
where a, b, and c are new letters not in . Note that M^ is now a DFA over the alphabet fa; b; cg, and, as described, need not be a complete DFA over this alphabet. (That
is, there may be pairs (qi; a) such that ^(qi; a) is unde ned.) However, M^ can be easily
554

Pac-Learning Recursive Logic Programs: Negative Results

M

1

1

? 0- ?

q0

q1

0

M^

1

1

q1

a

?
- q0

0?
- q1
0

b - qe c - qf

M0

a,b,c,0,1

b,c,0,1

?

-

qr

a,b,c,0,1

a,b,c

6 a,b,

a,b,c

0,1

1

1

q1

a

?
- q0

0?
- q1
0

b - qe c - qf

Figure 1: How a DFA is modi ed before emulation with a local clause

555

Cohen

made complete by introducing an additional rejecting state qr, and making every unde ned transition lead to qr. More precisely, let 0 be de ned as

0 ^ f(qi; x; qr) j qi 2 Q^ ^ x 2 fa; b; cg ^ (6 9qj : (qi; x; qj) 2 ^)g

Thus M0 = (q( 1); Q^ fqrg; fqfg; 0) is a \completed" version of M^ , with Q0 = Q^ fqrg. We will use M0 in the construction below; we will also let Q0 = Q^ fqrg and 0 = fa; b; cg.
Examples of M, M^ and M0 are shown in Figure 1. Notice that aside from the arcs into and out of the rejecting state qr, the state diagram of M0 is nearly identical to that of M. The di erences are that in M0 there is a new initial state q( 1) with a single outgoing arc labeled a to the old initial state q0; also every nal state of M has in M0 an outgoing arc labeled b to a new state qe, which in turn has a single outgoing arc labeled c to the nal state qf. It is easy to show that

x 2 L(M) i axbc 2 L(M0)

Now, given a set of states Q0 we de ne a database DB that contains the following predicates:
arcq ; ;q (S,X,T) is true for any S 2 Q0, any T 2 Q0, and any X 2 0, unless S = qi, X =i ,jand T 6= qj. state(S) is true for any S 2 Q0.

accept(c,nil,qe,qf) is true.

As motivation for the arc predicates, observe that in emulating M0 it is clearly useful to be

able to represent the transition function 0. The usefulness of the arc predicates is that any

transition function 0 can be represented using a conjunction of arc literals. In particular,

the conjunction

^ arcq ; ;q (S; X; T )

(qi; ;qj)2 0

i

j

succeeds when 0(S; X) = T, and fails otherwise. Let us now de ne the instance mapping fi as fi(x) = (f; D) where

f = accept(a; xbc; q( 1); q0)

and D is a set of facts that de nes the components relation on the list that corresponds to the string xbc. In other words, if x = 1 : : : n, then D is the set of facts

components( 1 : : : nbc; 1; 2 : : : nbc) components( 2 : : : nbc; 2; 3 : : : nbc) ... components(c,c,nil)

The declaration Decn will be Decn = (accept; 4; R) where R contains the modes components(+; ; ), state( ), and arcq ; ;q (+; +; +) for qi, qj in Q0, and 2 0.
Finally, de ne the concept mapping fic(Mj ) for a machine M to be the clause

556

Pac-Learning Recursive Logic Programs: Negative Results

accepVt((X,Ys),2S,T0 a)rc (S,X,T)

^

qi; ;qj

qi; ;qj

components(Ys,X1,Ys1)

^

state(U)

^

accept(X1,Ys1,T,U).

where 0 is the transition function for the corresponding machine M0 de ned above. It is easy to show this construction is polynomial.
In the clause X is a letter in 0, Ys is a list of such letters, and S and T are both states in Q0. The intent of the construction is that the predicate accept will succeed exactly when (a) the string XYs is accepted by M0 when M0 is started in state S, and (b) the rst action taken by M0 on the string XYs is to go from state S to state T.
Since all of the initial transitions in M0 are from q( 1) to q0 on input a, then if the predicate accept has the claimed behavior, clearly the proposed mapping satis es the requirements of Theorem 1. To complete the proof, therefore, we must now verify that the predicate accept succeeds i XYs is accepted by M0 in state S with an initial transition to T. From the de nition of DFAs the string XYs is accepted by M0 in state S with an initial transition to T i one of the following two conditions holds.

0(S; X) = T, Ys is the empty string and T is a nal state of M0, or;

0(S; X) = T, Ys is a nonempty string (and hence has some head X1 and some tail Ys1) and Ys1 is accepted by M0 in state T, with any initial transition.

The base fact accept(c,nil,qe,qf) succeeds precisely when the rst case holds, since in M0 this transition is the only one to a nal state. In the second case, the conjunction of the arc conditions in the fc(M) clause succeeds exactly when (S; X) = T (as noted above). Further the second conjunction in the clause can be succeeds when Ys is a nonempty string with head X1 and tail Ys1 and X1Ys1 is accepted by M0 in state T with initial transition to any state U, which corresponds exactly to the second case above.
Thus concept membership is preserved by the mapping. This completes the proof.

4. DNF-Hardness Results for Recursive Programs
To summarize previous results for determinate clauses, it was shown that while a single k-ary closed recursive depth-d clause is pac-learnable (Cohen, 1995), a set of n linear closed recursive depth-d clauses is not; further, even a single n-ary closed recursive depth-d clauses is not pac-learnable. There is still a large gap between the positive and negative results, however: in particular, the learnability of recursive programs containing a constant number of k-ary recursive clauses has not yet been established.
In this section we will investigate the learnability of these classes of programs. We will show that programs with either two linear closed recursive clauses or one linear closed recursive clause and one base case are as hard to learn as boolean functions in disjunctive normal form (DNF). The pac-learnability of DNF is a long-standing open problem in computational learning theory; the import of these results, therefore, is that establishing the learnability of these classes will require some substantial advance in computational learning theory.
557

Cohen
4.1 A Linear Recursive Clause Plus a Base Clause
Previous work has established that two-clause constant-depth determinate programs consisting of one linear recursive clause and one nonrecursive clause can be identi ed, given two types of oracles: the standard equivalence-query oracle, and a \basecase oracle' (Cohen, 1995). (The basecase oracle determines if an example is covered by the nonrecursive clause alone.) In this section we will show that in the absence of the basecase oracle, the learning problem is as hard as learning boolean DNF.
In the discussion below, Dnf n; r] denotes the language of r-term boolean functions in disjunctive normal form over n variables.
Theorem 5 Let d-Depth-2-Clause be the set of 2-clause programs consisting of one
clause in d-DepthLinRec and one clause in d-DepthNonRec. Then for any n and any r there exists a database DBn;r 2 2-DB and a declaration Decn;r 2 2-DEC, both of sizes polynomial in n and r, such that
Dnf n; r] 1-Depth-2-Clause DB ;n;r Decn;r] Hence for a 2 and d 1 the language family d-Depth-2-Clause DB; a-DetDEC] is
uniformly polynomially predictable only if DNF is polynomially predictable.
Proof: We will produce a DBn;r 2 DB and Decn;r 2 2-DetDEC such that predicting
DNF can be reduced to predicting 1-Depth-2-Clause DBn;r; Decn;r]. The construction makes use of a trick rst used in Theorem 3 of (Cohen, 1993a), in which a DNF formula is emulated by a conjunction containing a single variable Y which is existentially quanti ed over a restricted range.
We begin with the instance mapping fi. An assignment = b1 : : :bn will be converted to the extended instance (f; D) where
f p(1) D fbiti(bi)gni=1 Next, we de ne the database DBn;r to contain the binary predicates true1, false1, : : :, truer, falser which behave as follows: truei(X,Y) succeeds if X = 1, or if Y 2 f1; : : :; rg fig. falsei(X,Y) succeeds if X = 0, or if Y 2 f1; : : :; rg fig. Further, DBn;r contains facts that de ne the predicate succ(Y,Z) to be true whenever Z = Y + 1, and both Y and Z are numbers between 1 and r. Clearly the size of DBn;r is polynomial in r. Let Decn;r = (p; 1; R) where R contains the modes biti( ), for i = 1; : : :; n; truej(+; +) and falsej(+; +), for j = 1; : : :; r, and succ(+; ). Now let be an r-term DNF formula = _ri=1 ^sj=i 1 lij over the variables v1; : : :; vn. We may assume without loss of generality that contains exactly r terms, since any DNF formula with fewer than r terms can be padded to exactly r terms by adding terms of the
558

Pac-Learning Recursive Logic Programs: Negative Results

Background database:
for i = 1; : : :; r truei(b; y) for all b; y : b = 1 or y 2 f1; : : :; rg but y 6= i falsei(b; y) for all b; y : b = 0 or y 2 f1; : : :; rg but y 6= i
succ(y,z) if z = y + 1 and y 2 f1; : : :; rg and z 2 f1; : : :; rg DNF formula: (v1 ^ v3 ^ v4) _ (v2 ^ v3) _ (v1 ^ v4) Equivalent program:
p(Y) succ(Y,Z)^p(Z). p(Y) bit1(X1) ^ bit2(X2) ^ bit3(X3) ^ bit4(X4) ^
true1(X1,Y) ^ false1(X3,Y) ^ true1(X4,Y) ^ false2(X2,Y) ^ false2(X3,Y)^ true3(X1,Y) ^ false3(X4,Y). Instance mapping: fi(1011) = (p(1); fbit1(1); bit2(0); bit3(1); bit4(1)g)

Figure 2: Reducing DNF to a recursive program

form v1v1. We now de ne the concept mapping fc( ) to be the program CR; CB where CR is the linear recursive depth 1 determinate clause

p(Y ) succ(Y; Z) ^ p(Z)

and CB is the nonrecursive depth 1 determinate clause

p(Y ) ^n bitk(Xk) ^ ^r ^si Bij

k=1

i=1 j=1

where Bij is de ned as follows: Bij

( truei(Xk,Y) if lij = vk falsei(Xk,Y) if lij = vk

An example of the construction is shown in Figure 2; we suggest that the reader refer to this gure at this point. The basic idea behind the construction is that rst, the clause CB will succeed only if the variable Y is bound to i and the i-th term of succeeds (the de nitions of truei and falsei are designed to ensure that this property holds); second, the recursive clause CR is constructed so that the program fc( ) succeeds i CB succeeds with Y bound to one of the values 1; : : :; n.
We will now argue more rigorously for the correctness of the construction. Clearly, fi( ) and fc( ) are of the same size as and respectively. Since DBn;r is also of polynomial size, this reduction is polynomial.
Figure 3 shows the possible proofs that can be constructed with the program fc( ); notice that the program fc( ) succeeds exactly when the clause CB succeeds for some value

559

Cohen

p(1)
A@ A@
B(1) succ(A1A,2@)@p(2)
@A A@
B(2) succ(A2A,3@)@p(3)
A@ A@
B(3) AA @@
:::

B(i) V biti(Xi) ^ V V Bij

p(n-1)
A@ A@
B(n-1) AA @@ succ(n-1,n) p(n)

B(n)

Figure 3: Space of proofs possible with the program fc( )

of Y this

cabseetwVesen

1 and r. Now, if B succeeds with

Y

is true bound

ttohetnhesovmaleueteirmandTiV=s 0

Vsj=i 1 B0

lij must be for every i0

true; in
6= i also

succeeds wijti=h1Y ibjound to i. On the other hand, if is false for aj=in1assiijgnment, then each Ti

fails, and hence for every possible binding of Y generated by repeated use of the recursive

clause CR the base clause CB will also fail. Thus concept membership is preserved by the

mapping.

This concludes the proof.

4.2 Two Linear Recursive Clauses
Recall again that a single linear closed recursive clause is identi able from equivalence queries (Cohen, 1995). A construction similar to that used in Theorem 5 can be used to show that this result cannot be extended to programs with two linear recursive clauses.
Theorem 6 Let d-Depth-2-Clause0 be the set of 2-clause programs consisting of two
clauses in d-DepthLinRec. (Thus we assume that the base case of the recursion is given as background knowledge.) Then for any constants n and r there exists a database DBn;r 2 2-DB and a declaration Decn;r 2 2-DEC, both of sizes polynomial in n, such that
Dnf n; r] 1-Depth-2-Clause0 DB ;n;r Decn;r] Hence for any constants a 2 and d 1 the language family
d-Depth-2-Clause0 DB; a-DetDEC]
560

Pac-Learning Recursive Logic Programs: Negative Results

is uniformly polynomially predictable only if DNF is polynomially predictable.

Proof: As before, the proof makes use of a prediction-preserving reducibility from DNF to
d-Depth-2-Clause0 DB; Dec] for a speci c DB and Dec. Let us assume that is a DNF with r terms, and further assume that r = 2k. (Again, this assumption is made without loss of generality, since the number of terms in can be increased by padding with vacuous terms.) Now consider a complete binary tree of depth k + 1. The k-th level of this tree has exactly r nodes; let us label these nodes 1, : : :, r, and give the other nodes arbitrary labels. Now construct a database DBn;r as in Theorem 5, except for the following changes:

The predicates truei(b,y) and falsei(b,y) also succeed when y is the label of a node at some level below k.

Rather than the predicate succ, the database contains two predicates leftson and rightson that encode the relationship between nodes in the binary tree.

The database includes the facts p(!1), : : :, p(!2r), where !1, : : :, !2r are the leaves of the binary tree. These will be used as the base cases of the recursive program that is to be learned.

Let be the label of the root of the binary tree. We de ne the instance mapping to be

fi(b1 : : :b1) (p( ); fbit1(b1); : : :; bitn(bn)g)

Note that except for the use of rather than 1, this is identical to the instance mapping used in Theorem 5. Also let Decn;r = (p; 1; R) where R contains the modes biti( ), for i = 1; : : :; n; truej(+; +) and falsej(+; +), for j = 1; : : :; r; leftson(+; ); and rightson(+; ).
The concept mapping fc( ) is the pair of clauses R1; R2, where R1 is the clause

p(Y ) ^n bitk(Xk) ^ ^r ^si Bij ^ leftson(Y; Z) ^ p(Z)

k=1

i=1 j=1

and R2 is the clause p(Y )

^n bitk(Xk) ^ ^r ^si Bij ^ rightson(Y; Z) ^ p(Z)

k=1

i=1 j=1

Note that both of these clause are linear recursive, determinate, and have depth 1. Also, the construction is clearly polynomial. It remains to show that membership is preserved.
in FFigiguurere34, Bsh(io)wasbtbhreevsipaatcees tohfepcroonojfusntchtaiotncaVnbbitei(cXoni)st^ruVcVtedBiwj.itNhotthiceeptrhoagtrathme fpcr(og)r;aams
will succeed only if the recursive calls manage to nally recurse to one of the base cases p(!1), : : :, p(!2r), which correspond to the leaves of the binary tree. Both clauses will both succeed on the the rst k 1 levels of the tree. However, to reach the base cases of the recursion at the leaves of the tree, the recursion must pass through the k-th level of the tree; that is, one of the clauses above must succeed on some node y of the binary tree, where y is on the k-th level of the tree, and hence the label of y is a number between 1 and r. The program thus succeeds on fi( ) precisely when there is some number y between 1 and

561

Cohen

p(
"H"H" """ """ ""
B( ) p(L)
`` \Z\ZZ \Z \Z

)
@bb @bb @@bbb @ bb @@ bb
B( ) p(R)
XX Z\\ZZ \Z \\ Z

:::

:::

:::

:::

XXXEEX E E E

B

B

B

B B

:::

B(1) p(LL: : : L) B(1) p(LL: : : R)

:::

B

J

B

J

B

J

B

J

B

J

B(n) p(RR: : : LR) B(n) p(RR: : : R)

p(!1)

p(!2)

p(!2r 1)

p(!2r )

Figure 4: Proofs possible with the program fc( )

r such that the conjunction B(i) succeeds, which (by the argument given in Theorem 5) can happen if and only if is satis ed by the assignment . Thus, the mappings preserve concept membership. This completes the proof.
Notice that the programs fc( ) used in this proof all have the property that the depth of every proof is logarithmic in the size of the instances. This means that the hardness result holds even if one additionally restricts the class of programs to have a logarithmic depth bound.
4.3 Upper Bounds on the Di culty of Learning
The previous sections showed that several highly restricted classes of recursive programs are at least as hard to predict as DNF. In this section we will show that these restricted classes are also no harder to predict than DNF.
We will wish to restrict the depth of a proof constructed by a target program. Thus, let h(n) be any function; we will use Langh(n) for the set of programs in the class Lang such
that all proofs of an extended instance (f; D) have depth bounded by h(jDj).

562

Pac-Learning Recursive Logic Programs: Negative Results
Theorem 7 Let Dnf n; ] be the language of DNF boolean functions (with any number
of terms), and recall that d-Depth-2-Clause is the language of 2-clause programs consisting of one clause in d-DepthLinRec and one clause in d-DepthNonRec, and that d-Depth-2-Clause0 is the language of 2-clause programs consisting of two clauses in d-DepthLinRec.
For all constants d and a, and all databases DB 2 DB and declarations Dec 2 a-DetDEC, there is a polynomial function poly(n) such that
d-Depth-2-Clause DB; Dec] Dnf poly(jDBj); ] d-Depth-2-Clause0h(n) DB; Dec] Dnf poly(jDBj); ] if h(n) is bounded by c log n for some constant c. Hence if either of these language families is uniformly polynomially predictable, then Dnf n; ]
is polynomially predictable.
Proof: The proof relies on several facts established in the companion paper (Cohen, 1995).
For every declaration Dec, there is a clause BOTTOMd(Dec) such that every nonrecursive depth-d determinate clause C is equivalent to some subclause of BOTTOMd. Further, the size of BOTTOM d is polynomial in Dec. This means that the language of subclauses of BOTTOM is a normal form for nonrecursive constant-depth determinate clauses. Every linear closed recursive clause CR that is constant-depth determinate is equivalent to some subclause of BOTTOM plus a recursive literal Lr; further, there are only a polynomial number of possible recursive literals Lr.
For any constants a, a0, and d, any database DB 2 a-DB, any declaration Dec = (p; a0; R), any database DB 2 a-DB, and any program P in d-Depth-2-Clause DB; Dec], the depth of a terminating proof constructing using P is no more than hmax, where hmax is a polynomial in the size of DB and Dec.
At can be assumed without loss of generality that the database DB and all decsriptions D contain an equality predicate, where an equality predicate is simply a predicate equal(X,Y) which is true exactly when X = Y . The idea of the proof is to contruct a prediction-preserving reduction between the two classes of recursive programs listed above to and DNF. We will begin with two lemmas.
Lemma 8 Let Dec 2 a-DetDEC, and let C be a nonrecursive depth-d determinate clause
consistent with Dec. Let SubclauseC denote the language of subclauses of C, and let Monomial u] denote the language of monomials over u variables. Then there is a polynomial poly1 so that for any database DB 2 DB,
SubclauseC DB; Dec] Monomial poly1(jDBj)]
563

Cohen

Proof of lemma: Follows immediately from the construction used in Theorem 1 of
Dzeroski, Muggleton, and Russell (Dzeroski et al., 1992). (The basic idea of the construction is to introduce a propositional variable representing the \success" of each connected chain of literals in C. Any subclause of C can then be represented as a conjunction of these propositions.)

This lemma can be extended as follows.

Lemma 9 Let Dec 2 a-DetDEC, and let S = fC1; : : :; Crg be a set of r nonrecursive depth-
d determinate clauses consistent with Dec, each of length n or less. Let SubclauseS denote the set of all programs of the form P = (D1; : : :; Ds) such that each Di is a subclause of some Cj 2 S.
Then there is a polynomial poly2 so that for any database DB 2 DB,

SubclauseS DB; Dec] Dnf poly2(jDBj; r); ]

Proof of lemma: By Lemma 8, for each Ci 2 S, there is a set of variables Vi of size

polynomial in Sover Vi. Let

j DB j V=

sSuch
ri=1

that Vi.

every clause
Clearly, jV j

in is

SubclauseC can be emulated by a polynomial iin n and r, and every

monomial clause in

i SubclauseC can be also emulated by a monomial over V . Further, every disjunction

of r such clausesi can be represented by a disjunction of such monomials.

Since the Ci's all satisfy a single declaration Dec = (p; a; R), they have heads with the

same principle function and arity; further, we may assume (without loss of generality, since

an equality predicate is assumed) that the variables appearing in the heads of these clauses

are all distinct. Since the Ci's be represented as a disjunction

are D1

also
_:::

nonrecursive, every
_D where for all i,

Dpro2gr(aSm

P 2 SubclauseS can
Subclause ). Hence

every

P

2

SubclauseS

can

be

r
represented

by

an

r-term

i
DNF

i
over

the

set

of

Ci
variables

V

.

Let us now introduce some additional notation. If C and D are clauses, then we will use C u D to denote the result of resolving C and D together, and Ci to denote the result of resolving C with itself i times. Note that C u D is unique if C is linear recursive and C and D have the same predicate in their heads (since there will be only one pair of complementary literals.)
Now, consider some target program

P = (CR; CB) 2 d-Depth-2-Clause DB; Dec]

where CR is the recursive clause and CB is the base. The proof of any extended instance

(f; D) must use clause CR repeatedly h times and then use clause CB to resolve away

the

nal

subgoal.

Hence

the

nonrecursive

clause

Ch R

u

CB

could

also

be

used

to

cover

the

instance (f; D).

Since the depth of any proof for this class of programs is bounded by a number hmax

that is polynomial in jDBj and ne, the nonrecursive program

P 0 = fCRh u CB : 0 h hmax g

564

Pac-Learning Recursive Logic Programs: Negative Results

is equivalent to P on extended instances of size ne or less.

Finally, recall that we can assume that CB is a subclause of BOTTOMd; also, there

is a polynomial-sized set LR = Lr1; : : :; Lr of closed recursive literals such that for some

Lr

2

LR,

the

clause

CR

is

a

subclause

of

p
BOTTOM d

Lr . This means that if we let S

beithe polynomial-sized set

i

S1 = f(BOTTOM d Lri)h u BOTTOM d j 0 h hmax and Lri 2 LRg

then P 0 2 SubclauseS1. Thus by Lemma 9, d-Depth-2-Clause
the proof of the rst statement in the the theorem. To show that

Dnf. This concludes

d-Depth-2-Clause0h(n) DB; Dec] Dnf poly(jDBj; ]

a similar argument applies. Let us again introduce some notation, and de ne MESHh;n(CR1; CR2) as the set of all clauses of the form

C u C u : : : u C Ri;1

Ri;2

0 Ri;h

where for all j, CR = CR1 or CR = CR2, and h0 h(n). Notice that for functions

h(n) c log n the nuijmber of such claijuses is polynomial in n.

Now let p be the predicate appearing in the heads of CR1 and CR2, and let C^ (respectively

D^B) be a a version of C (DB) in which every instance of the predicate p has been replaced

with a new predicate p^. If P is a recursive program P = fCR1; CR2g in d-Depth-2-Clause0

over the database DB, then P ^ DB is equivalent4 to the nonrecursive program P0 ^ D^B,

where

P 0 = fC^ j C 2 MESHh;n (CR1; CR2)g

e

Now recall that there are a polynomial number of recursive literals Lr , and hence a polynomial number of pairs of recursive literals Lr ; Lr . This means that thie set of clauses

i

j

S2 = fC^ j C 2 MESHh;ne(BOTTOM d Lri; BOTTOM d Lrj )g
(Lri ;Lrj )2LR LR

is also polynomial-sized; furthermore, for any program P in the language d-Depth-2-Clause, P0 2 SubclauseS2. The second part of the theorem now follows by application of Lemma 9.

An immediate corollary of this result is that Theorems 6 and 5 can be strengthened as follows.
Corollary 10 For all constants d 1 and a 2, the language family d-Depth-2-Clause DB; a-DetDEC]
is uniformly polynomially predictable if and only if DNF is polynomially predictable.
For all constants d 1 and a 2, the language family d-Depth-2-Clause0 DB; a-DetDEC]
is uniformly polynomially predictable if and only if DNF is polynomially predictable.
4. On extended instances of size ne or less.
565

Cohen
Thus in an important sense these learning problems are equivalent to learning boolean DNF. This does not resolve the questions of the learnability of these languages, but does show that their learnability is a di cult formal problem: the predictability of boolean DNF is a long-standing open problem in computational learning theory.
5. Related Work
The work described in this paper di ers from previous formal work on learning logic programs in simultaneously allowing background knowledge, function-free programs, and recursion. We have also focused exclusively on computational limitations on e cient learnability that are associated with recursion, as we have considered only languages known to be paclearnable in the nonrecursive case. Since the results of this paper are all negative, we have concentrated on the model of polynomial predictability; negative results in this model immediately imply a negative result in the stronger model of pac-learnability, and also imply negative results for all strictly more expressive languages.
Among the most closely related prior results are the negative results we have previously obtained for certain classes of nonrecursive function-free logic programs (Cohen, 1993b). These results are similar in character to the results described here, but apply to nonrecursive languages. Similar cryptographic results have been obtained by Frazier and Page (1993) for certain classes of programs (both recursive and nonrecursive) that contain function symbols but disallow background knowledge.
Some prior negative results have also been obtained on the learnability of other rstorder languages using the proof technique of consistency hardness (Pitt & Valiant, 1988). Haussler (1989) showed that the language of \existential conjunction concepts" is not paclearnable by showing that it can be hard to nd a concept in the language consistent with a given set of examples. Similar results have also been obtained for two restricted languages of Horn clauses (Kietz, 1993); a simple description logic (Cohen & Hirsh, 1994); and for the language of sorted rst-order terms (Page & Frisch, 1992). All of these results, however, are speci c to the model pac-learnability, and none can be easily extended to the polynomial predictability model considered here. The results also do not extend to languages more expressive than these speci c constrained languages. Finally, none of these languages allow recursion.
To our knowledge, there are no other negative learnability results for rst-order languages. A discussion of prior positive learnability results for rst-order languages can be found in the companion paper (Cohen, 1995).
6. Summary
This paper and its companion (Cohen, 1995) have considered a large number of di erent subsets of Datalog. Our aim has been to be not comprehensive, but systematic: in particular, we wished to nd precisely where the boundaries of learnability lie as various syntactic restrictions are imposed and relaxed. Since it is all too easy for a reader to \miss the forest for the trees", we will now brie y summarize the results contained in this paper, together with the positive results of the companion paper (Cohen, 1995).
566

Pac-Learning Recursive Logic Programs: Negative Results

Local Clauses nCR nCR

Constant-Depth Determinate Clauses

nCRjCB nCR; CB

k nCR

n nCR

kCR kCR+ kCRjCB+ kCR; CBDNF k k0CRDNF n kCR

1CR

1CR+

1CRjCB+

1CR

;

C

=DNF
B

2 1CR=DNF

n 1CR

Table 1: A summary of the learnability results Throughout these papers, we have assumed that a polynomial amount of background knowledge exists; that the programs being learned contain no function symbols; and that literals in the body of a clause have small arity. We have also assumed that recursion is closed, meaning that no output variables appear in a recursive clause; however, we believe that this restriction can be relaxed without fundamentally changing the results of the paper. In the companion paper (Cohen, 1995) we showed that a single nonrecursive constantdepth determinate clause was learnable in the strong model of identi cation from equivalence queries. In this learning model, one is given access to an oracle for counterexamples|that is, an oracle that will nd, in unit time, an example on which the current hypothesis is incorrect|and must reconstruct the target program exactly from a polynomial number of these counterexamples. This result implies that a single nonrecursive constant-depth determinate clause is pac-learnable (as the counterexample oracle can be emulated by drawing random examples in the pac setting). The result is not novel (Dzeroski et al., 1992); however the proof given is independent, and is also of independent interest. Notably, it is somewhat more rigorous than earlier proofs, and also proves the result directly, rather than via reduction to a propositional learning problem. The proof also introduces a simple version of the forced simulation technique, variants of which are used in all of the positive results. We then showed that the learning algorithm for nonrecursive clauses can be extended to the case of a single linear recursive constant-depth determinate clause, leading to the result that this restricted class of recursive programs is also identi able from equivalence queries. With a bit more e ort, this algorithm can be further extended to learn a single k-ary recursive constant-depth determinate clause. We also considered extended the learning algorithm to learn recursive programs consisting of more than one constant-depth determinate clauses. The most interesting extension was to simultaneously learn a recursive clause CR and a base clause CB, using equivalence queries and also a \basecase oracle" that indicates which counterexamples should be covered by the base clause CB. In this model, it is possible to simultaneously learn a recursive clause and a nonrecursive base case in all of the situations for which a recursive clause is learned
567

Cohen

Language Family

B R L/R Oracles Notation Learnable

d-DepthNonRec a-DB; a-DetDEC] 1 0

EQ

CB

yes

d-DepthLinRec a-DB; a-DetDEC]

0 1 1 EQ

1CR yes

d-Depth-k-Rec a-DB; a-DetDEC]

0 1 k EQ

kCR

yes

d-Depth-2-Clause a-DB; a-DetDEC] 1 1 1 EQ,BASE 1CRjCB yes

kd-MaxRecLang a-DB; a-DetDEC] 1 1 k EQ,BASE kCRjCB yes

d-Depth-2-Clause a-DB; a-DetDEC] 1 1 1 EQ

1CR; CB =DNF

d-Depth-2-Clause0 a-DB; a-DetDEC] 0 2 1 EQ

2 1CR =DNF

d-DepthLinRecProg a-DB; a-DetDEC] 0 n 1 EQ

n 1CR no

d-DepthRec a-DB; a-DetDEC]

0 1 n EQ

nCR

no

k-LocalLinRec a-DB; a-DEC]

0 1 1 EQ

1CR no

Table 2: Summary by language of the learnability results. Column B indicates the number of base (nonrecursive) clauses allowed in a program; column R indicates the number of recursive clauses; L/R indicates the number of recursive literals allowed in a single recursive clause; EQ indicates an oracle for equivalence queries and BASE indicates a basecase oracle. For all languages except k-LocalLinRec, all clauses must be determinate and of depth d.

alone; for instance, one can learn a k-ary recursive clause to together with its nonrecursive base case. This was our strongest positive result.
These results are summarized in Tables 1 and 2. In Table 1, a program with one rary recursive clause is denoted rCR, a program with one r-ary recursive clause and one nonrecursive basecase is denoted rCR; CB, or rCRjCB if there is a \basecase" oracle, and a program with s di erent r-ary recursive clauses is denoted s rCR. The boxed results are associated with one or more theorems from this paper, or its companion paper, and the unmarked results are corollaries of other results. A \+" after a program class indicates that it is identi able from equivalence queries; thus the positive results described above are summarized by the four \+" entries in the lower left-hand corner of the section of the table concerned with constant-depth determinate clauses.
Table 2 presents the same information in a slightly di erent format, and also relates the notation of Table 1 to the terminology used elsewhere in the paper.
This paper has considered the learnability of the various natural generalizations of the languages shown to be learnable in the companion paper. Consider for the moment single clauses. The companion paper showed that for any xed k a single k-ary recursive constantdepth determinate clause is learnable. Here we showed that all of these restrictions are necessary. In particular, a program of n constant-depth linear recursive clauses is not polynomially predictable; hence the restriction to a single clause is necessary. Also, a single clause with n recursive calls is hard to learn; hence the restriction to k-ary recursion is necessary. We also showed that the restriction to constant-depth determinate clauses is necessary, by considering the learnability of constant locality clauses. Constant locality clauses are the only known generalization of constant-depth determinate clauses that are pac-learnable in the nonrecursive case. However, we showed that if recursion is allowed,
568

Pac-Learning Recursive Logic Programs: Negative Results
then this language is not learnable: even a single linear recursive clause is not polynomially predictable.
Again, these results are summarized in Table 1; a \ " after a program class means that it is not polynomially predictable, under cryptographic assumptions, and hence neither pac-learnable nor identi able from equivalence queries.
The negative results based on cryptographic hardness give an upper bound on the expressiveness of learnable recursive languages, but still leave open the learnability of programs with a constant number of k-ary recursive clauses in the absence of a basecase oracle. In the nal section of this paper, we showed that the following problems are, in the model of polynomial predictability, equivalent to predicting boolean DNF:
predicting two-clause constant-depth determinate recursive programs containing one linear recursive clause and one base case; predicting two-clause recursive constant-depth determinate programs containing two linear recursive clauses, even if the base case is known. We note that these program classes are the very nearly the simplest classes of multi-clause recursive programs that one can imagine, and that the pac-learnability of DNF is a longstanding open problem in computational learning theory. These results suggest, therefore, that pac-learning multi-clause recursive logic programs is di cult; at the very least, they show that nding a provably correct pac-learning algorithm will require substantial advances in computational learning theory. In Table 1, a \= Dnf" (respectively Dnf) means that the corresponding language is prediction-equivalent to DNF (respectively at least as hard as DNF). To further summarize Table 1: with any sort of recursion, only programs containing constant-depth determinate clauses are learnable. The only constant-depth determinate recursive programs that are learnable are those that contain a single k-ary recursive clause (in the standard equivalence query model) or a single k-ary recursive clause plus a base case (if a \basecase oracle" is allowed). All other classes recursive programs are either cryptographically hard, or as hard as boolean DNF.
7. Conclusions
Inductive logic programming is an active area of research, and one broad class of learning problems considered in this area is the class of \automatic logic programming" problems. Prototypical examples of this genre of problems are learning to append two lists, or to multiply two numbers. Most target concepts in automatic logic programming are recursive programs, and often, the training data for the learning system are simply examples of the target concept, together with suitable background knowledge.
The topic of this paper is the pac-learnability of recursive logic programs from random examples and background knowledge; speci cally, we wished to establish the computational limitations inherit in performing this task. We began with some positive results established in a companion paper. These results show that one constant-depth determinate closed k-ary recursive clause is pac-learnable, and that further, a program consisting of one such recursive clause and one constant-depth determinate nonrecursive clause is also pac-learnable given an additional \basecase oracle".
569

Cohen
In this paper we showed that these positive results are not likely to be improved. In particular, we showed that either eliminating the basecase oracle or learning two recursive clauses simultaneously is prediction-equivalent to learning DNF, even in the case of linear recursion. We also showed that the following problems are as hard as breaking (presumably) secure cryptographic codes: pac-learning n linear recursive determinate clauses, pac-learning one n-ary recursive determinate clause, or pac-learning one linear recursive k-local clause.
These results contribute to machine learning in several ways. From the point of view of computational learning theory, several results are technically interesting. One is the prediction-equivalence of several classes of restricted logic programs and boolean DNF; this result, together with others like it (Cohen, 1993b), reinforces the importance of the learnability problem for DNF. This paper also gives a dramatic example of how adding recursion can have widely di ering e ects on learnability: while constant-depth determinate clauses remain pac-learnable when linear recursion is added, constant-locality clauses become cryptographically hard.
Our negative results show that systems which apparently learn a larger class of recursive programs must be taking advantage either of some special properties of the target concepts they learn, or of the distribution of examples that they are provided with. We believe that the most likely opportunity for obtaining further positive formal results in this area is to identify and analyze these special properties. For example, in many examples in which FOIL has learned recursive logic programs, it has made use of \complete example sets"| datasets containing all examples of or below a certain size, rather than sets of randomly selected examples (Quinlan & Cameron-Jones, 1993). It is possible that complete datasets allow a more expressive class of programs to be learned than random datasets; in fact, some progress has been recently made toward formalizing this conjecture (De Raedt & Dzeroski, 1994).
Finally, and most importantly, this paper has established the boundaries of learnability for determinate recursive programs in the pac-learnability model. In many plausible automatic programming contexts it would be highly desirable to have a system that o ered some formal guarantees of correctness. The results of this paper provide upper bounds on what one can hope to achieve with an e cient, formally justi ed system that learns recursive programs from random examples alone.
Acknowledgements
The author wishes to thank three anonymous JAIR reviewers for a number of useful suggestions on the presentation and technical content.
References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication with small training sets. In Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture Notes in Computer Science # 784. 570

Pac-Learning Recursive Logic Programs: Negative Results
Biermann, A. (1978). The inference of regular lisp programs from examples. IEEE Transactions on Systems, Man and Cybernetics, 8(8).
Chandra, A. K., Kozen, D. C., & Stockmeyer, L. J. (1981). Alternation. Journal of the ACM, 28, 114{113.
Cohen, W. W. (1993a). Cryptographic limitations on learning one-clause logic programs. In Proceedings of the Tenth National Conference on Arti cial Intelligence Washington, D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. To appear in Arti cial Intelligence.
Cohen, W. W. (1993c). Rapid prototyping of ILP systems using explicit bias. In Proceedings of the 1993 IJCAI Workshop on Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994a). Pac-learning nondeterminate clauses. In Proceedings of the Eleventh National Conference on Arti cial Intelligence Seattle, WA.
Cohen, W. W. (1994b). Recovering software speci cations with inductive logic programming. In Proceedings of the Eleventh National Conference on Arti cial Intelligence Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: e cient algorithms. Journal of AI Research, 2, 501{539.
Cohen, W. W., & Hirsh, H. (1994). The learnability of description logics with equality constraints. Machine Learning, 17(2/3).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories are PAC-learnable. In Wrobel, S. (Ed.), Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad Honnef/Bonn, Germany.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability of determinate logic programs. In Proceedings of the 1992 Workshop on Computational Learning Theory Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993). Learnability of recursive, non-determinate theories: Some basic results and techniques. In Proceedings of the Third International Workshop on Inductive Logic Programming Bled, Slovenia.
Haussler, D. (1989). Learning conjunctive concepts in structural domains. Machine Learning, 4(1).
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.
Kearns, M., & Valiant, L. (1989). Cryptographic limitations on learning Boolean formulae and nite automata. In 21th Annual Symposium on the Theory of Computing. ACM Press.
571

Cohen
Kietz, J.-U. (1993). Some computational lower bounds for the computational complexity of inductive logic programming. In Proceedings of the 1993 European Conference on Machine Learning Vienna, Austria.
King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design by machine learning: the use of inductive logic programming to model the structureactivity relationships of trimethoprim analogues binding to dihydrofolate reductase. Proceedings of the National Academy of Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge and declarative bias in inductive concept learning. In Jantke, K. P. (Ed.), Analogical and Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures in Arti cial Intelligence Series #642.
Lloyd, J. W. (1987). Foundations of Logic Programming: Second Edition. Springer-Verlag. Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19/20(7), 629{679. Muggleton, S., & Feng, C. (1992). E cient induction of logic programs. In Inductive Logic
Programming. Academic Press. Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5(7), 647{657. Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press. Page, C. D., & Frisch, A. M. (1992). Generalization and learnability: A study of constrained
atoms. In Inductive Logic Programming. Academic Press. Pazzani, M., & Kibler, D. (1992). The utility of knowledge in inductive learning. Machine
Learning, 9(1). Pitt, L., & Warmuth, M. K. (1988). Reductions among prediction problems: On the dif-
culty of predicting automata. In Proceedings of the 3rd Annual IEEE Conference on Structure in Complexity Theory Washington, D.C. Computer Society Press of the IEEE. Pitt, L., & Valiant, L. (1988). Computational limitations on learning from examples. Journal of the ACM, 35(4), 965{984. Pitt, L., & Warmuth, M. (1990). Prediction-preserving reducibility. Journal of Computer and System Sciences, 41, 430{467. Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: A midterm report. In Brazdil, P. B. (Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes in Computer Science # 667. Quinlan, J. R. (1990). Learning logical de nitions from relations. Machine Learning, 5(3).
572

Pac-Learning Recursive Logic Programs: Negative Results
Quinlan, J. R. (1991). Determinate literals in inductive logic programming. In Proceedings of the Eighth International Workshop on Machine Learning Ithaca, New York. Morgan Kaufmann.
Rouveirol, C. (1994). Flattening and saturation: two representation changes for generalization. Machine Learning, 14(2).
Summers, P. D. (1977). A methodology for LISP program construction from examples. Journal of the Association for Computing Machinery, 24(1), 161{175.
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11). Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers from treebanks:
a machine learning approach. In Proceedings of the Twelfth National Conference on Arti cial Intelligence Seattle, Washington. MIT Press.
573

