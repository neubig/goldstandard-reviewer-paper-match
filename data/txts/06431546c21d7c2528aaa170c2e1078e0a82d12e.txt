Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer
Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, Kristina Toutanova Google Research
{iuliaturc,kentonl,jeisenstein,mingweichang,kristout}@google.com

arXiv:2106.16171v1 [cs.CL] 30 Jun 2021

Abstract
Despite their success, large pre-trained multilingual models have not completely alleviated the need for labeled data, which is cumbersome to collect for all target languages. Zero-shot cross-lingual transfer is emerging as a practical solution: pre-trained models later ﬁne-tuned on one transfer language exhibit surprising performance when tested on many target languages. English is the dominant source language for transfer, as reinforced by popular zero-shot benchmarks. However, this default choice has not been systematically vetted. In our study, we compare English against other transfer languages for ﬁne-tuning, on two pretrained multilingual models (mBERT and mT5) and multiple classiﬁcation and question answering tasks. We ﬁnd that other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori. Unexpectedly, this can be true even when the training sets were automatically translated from English. This ﬁnding can have immediate impact on multilingual zero-shot systems, and should inform future benchmark designs.
1 Introduction
Developing language technologies for lowresource languages has become a priority in the natural language processing (NLP) community. However, collecting labeled data for the more than 6,000 languages spoken around the world (Hale et al., 1992) would be a massive undertaking. Cross-lingual transfer has emerged as a practical solution, leveraging labeled data from high-resource languages to improve performance on low-resource ones (Ruder et al., 2019). In particular, zero-shot learning has surged in popularity, as it requires no labeled training data in the target language(s). In zero-shot cross-lingual

1
Pre-training
on pre-training languages

EN: To be or not to be, that is the question. ES: Puedo escribir los versos más tristes. RU: Чем ночь темней, тем ярче звёзды JA: 見ぬが花

2
Fine-tuning
on a single transfer language (typically English)

EN ES RU
What a wonderful day! This is awful.

JA
pos neg

3 ES: ¡Que alegria!

pos

RU: Как жаль!

neg

Zero-shot evaluation

on target languages

JA: 大好きです

pos

Figure 1: Is English the best language for zeroshot cross-lingual transfer? In current literature, English is the dominant transfer language for ﬁne-tuning (step 2). In this study, we investigate whether this is the most effective choice on standard multilingual benchmarks.

transfer, a large pre-trained multilingual model such as mBERT (Devlin et al., 2018), XLM-R (Conneau et al., 2020) or mT5 (Xue et al., 2020) is ﬁne-tuned with labeled training data presented in a single language, called the source or transfer language. Despite the monolingualism of its labeled corpus, the model can exhibit surprisingly good end-task performance for the other languages seen during pre-training (Pires et al., 2019; Wu and Dredze, 2019). This process is illustrated in Figure 1.
Current studies default to English when selecting the transfer language for ﬁne-tuning, even though this particular choice has not been systematically vetted. In this paper, we ask a question that has been overlooked: Is English the best source language for zero-shot cross-lingual transfer?
The standard of using English as the default source langauge was adopted by popular multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which truncated the training sets of the constituent tasks

to English, even when the original tasks offered training data in other languages.1 While more recent benchmarks such as XTREME-R (Ruder et al., 2021) are starting to include cross-lingual training sets, English is still overwhelmingly dominant in this domain. Our standpoint is that adopting English as the de-facto language for transfer without considering alternatives is an opportunity cost, as there might be other languages with better zero-shot transferability. Identifying such languages could boost a system’s zero-shot performance without changes to the training pipeline, by simply collecting labeled data in those languages.
We focus on the scenario in which the complete list of target languages is large and might not even be known a priori, which is common for multilingual systems with an ever-expanding international user base. We also assume a restricted budget for data collection that can acquire labeled examples for ﬁne-tuning in a single language only. This constraint enables us to study transferability of languages in isolation; investigating combinations of source languages is left for future work.
The universality of targeting many languages is particularly challenging. Previous work has addressed the simpler problem of identifying the most effective transfer language(s) for a single target, or a small set of closely related ones. Practitioners either made decisions informed by the phylogenetic language tree (Cotterell and Heigold, 2017) or designed automated rankers that leverage hand-crafted similarity features such as syntactic, morphological, or geographic proximity (Lin et al., 2019). To the best of our knowledge, no previous study investigated general language transferability towards a set of targets that is not predetermined.
Finding a universal answer is intractable: there is an unbounded pool of NLP tasks, and acquiring test data in 6,000+ languages is almost as difﬁcult as acquiring training data. To address these difﬁculties, we ﬁrst design metrics that capture, given a source language, its zero-shot transferability towards a ﬁxed set of test languages for which data is readily available. Then, we conduct extensive experiments with mBERT (Devlin et al., 2018) and mT5 (Xue et al., 2020) on multiple tasks, including classiﬁcation and question answering.
1For instance, XNLI (Conneau et al., 2018) provides machine translations of the English subset, and TyDi QA (Clark et al., 2020) offers data produced by native speakers.

Our experiments reveal two surprising results. First, we ﬁnd that English is not the most universally transferable language in most settings, with the exception of question answering on mT5; German and Russian are often more effective sources. Second, even more unexpectedly, we ﬁnd that zero-shot performance can often be improved by ﬁne-tuning on a transfer set that was originally written in English and machine-translated to one of the better source languages. Making this change is effective even when the source language itself is not in the set of target languages. These ﬁndings are immediately applicable for building highly multilingual systems.
2 Related Work
2.1 Zero-Shot Cross-lingual Transfer
After the publication of multilingual BERT (mBERT) (Devlin et al., 2018), multiple studies observed its zero-shot capabilities: when ﬁnetuned for a speciﬁc task in language x (most commonly English), it performs well on the same task in another language y, without having seen any supervised data in y (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). With few adjustments, zero-shot transfer can succeed even for languages y that were not included in the original pretraining set (Wang et al., 2020a; Ponti et al., 2020).
Cross-lingual transferability is particularly surprising in the absence of explicit cross-lingual alignment during pre-training. While the transfer mechanism is still not fully understood, there are multiple hypotheses for its success. One debated aspect is vocabulary overlap: some studies observed a positive correlation between the number of shared tokens and transfer compatibility of two languages (Wu and Dredze, 2019); in contrast, others concluded from synthetic experiments that lexical overlap has a negligible effect (K et al., 2020). Another presumed catalyst for transfer is jointly training across multiple languages, shown to encourage cross-lingual alignment of contextual representations (Cao et al., 2020). While models such as mBERT clearly produce cross-lingual representations, there is evidence that they also preserve language-speciﬁc information (Wu and Dredze, 2019; Wang et al., 2020b).
2.2 Transfer Language Selection
While popular multilingual zero-shot benchmarks such as XTREME (Hu et al., 2020) and XGLUE

(Liang et al., 2020) provide development and test sets in tens of diverse low-resource languages, their transfer sets are limited to English. Even for tasks such as TyDi QA (Clark et al., 2020) that originally had training data in multiple languages, the authors of the benchmark removed any nonEnglish data from the transfer set. The effect is that most studies mentioned above defaulted their analysis to English as the only transfer language. More recently however, the XTREME-R benchmark (Ruder et al., 2021) introduced two truly cross-lingual retrieval tasks, where both training and evaluation data use a mixture of languages (Roy et al., 2020; Botha et al., 2020).
Previous studies on the effectiveness of source languages focus on a single target language, or a small set of related ones. Lauscher et al. (2020) observe strong correlations between transfer performance and multiple measures of linguistic proximity between the transfer and target language, including syntax and morphology. Lin et al. (2019) automatically identify the most effective transfer languages via a ranker that leverages various distances (geographic, genetic, syntactic, phonological, etc) between a single target language and multiple transfer candidates. In contrast, we seek to ﬁnd languages that transfer to many targets, potentially not even known a priori.
The pursuit of a language that can help others is also reminiscent of pivot-based machine translation (source → pivot + pivot → target), where a high-resource pivot bridges the gap between pairs of languages with insufﬁcient parallel training data (Cheng et al., 2017; Kim et al., 2019; Dabre et al., 2021). English was shown to not always be the best pivot for machine translation (Paul et al., 2013; Dabre et al., 2015), which prompted us to investigate whether that is also the case for zeroshot cross-lingual transfer.

3 Metrics for Language Transferability

In this section, we formally deﬁne metrics for measuring the impact of a particular source language on the cross-lingual ability of a model.
Let the relative zero-shot ability Z of a source language S to transfer to a target language T be:

E(M S, T )

Z(S → T ) = E(M T , T )

(1)

where M L is a pre-trained model M ﬁne-tuned on a corpus containing labeled data in language L,

and E is a standard evaluation metric (e.g. accuracy for classiﬁcation, F1 score for question answering, etc.). Z measures how much of the quality of a model ﬁne-tuned and evaluated on the same target language T can be recovered when training it on a different language S instead. Trivially, Z(L, L) = 1.0. In our tables, we multiply these values by 100 for readability, so that they can be interpreted as percentages.
Given that English is currently the dominant transfer language, we will often express the transferability of a source S towards a target T in terms of its zero-shot advantage over English:

Z(S → T ) − Z(en → T )

(2)

To measure the transferability of a source S to a set of target languages L, we average over relative zero-shot abilities:

1

Z(S → L) =

Z(S → T ) (3)

|L| T ∈L

When S ∈ L, zero-shot ability is a slight misnomer, since it includes a constant term 1/|L| for self-transfer. This term cancels out when computing the overall zero-shot advantage over English:

Z(S → L) − Z(en → L)

(4)

In other words, the metric in Equation 4 is fully zero-shot, since it disregards self-transfer terms.2
For some tasks, the denominator E(M T , T )
is not available; to keep the number of experi-
ments manageable, we did not train all models MT for every task. In such cases, instead of the
relative metric in Equation 1 we will use the unnormalized standard evaluation metric E(M S, T ). Note that the self-transfer term is now E(M S, S),
which ceases to be constant and no longer cancels
out when computing the advantage over English.

4 Datasets

In this section, we list the standard multilingual benchmarks we selected for evaluation. The main desideratum for our datasets is that training data is available in multiple languages. Ideally, all such

2XTREME deﬁnes a cross-lingual transfer gap metric as:

1 |L|

T ∈L E(M S, S)−E(M S, T ), which is an alternative to

Equation 3. If we were to substitute this deﬁnition in Equa-

tion 4, the self-transfer terms would (undesirably) survive.

Also, this metric is oblivious to how difﬁcult it is to solve

the task for a target language T , which we capture via the E(M T , T ) in the denominator of Equation 1.

Model mBERT mT5-Base

XNLI  

PAWS-X  

XQuAD  

Table 1: English was out-performed () by other source languages in 5/6 experimental settings.

training sets would be produced by humans (or veriﬁably high-quality). In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer. We will be mindful of this when drawing conclusions from our experiments.
To ensure that all language-speciﬁc subsets have the same size and informational content, we occasionally depart from the established way of using some of these datasets, as elaborated below.
XNLI The Cross-lingual Natural Language Inference corpus (Conneau et al., 2018) consists of premise/hypothesis pairs that are either entailments, contradictions, or neutral. XNLI extends the English MultiNLI dataset (Williams et al., 2018) to 15 languages, including low-resource ones such as Swahili and Urdu; training sets are machine-translated, while the development and test sets are human-translated.
PAWS-X The Cross-lingual Paraphrase Adversaries from Word Scrambling corpus (Yang et al., 2019) is a binary classiﬁcation task for paraphrase identiﬁcation. Its 6 training sets were machinetranslated from the English PAWS dataset (Zhang et al., 2019). The development and test sets were human-translated.
XQuAD The Cross-lingual Question Answering Dataset (Artetxe et al., 2020) requires answering questions by identifying answer spans in accompanying paragraphs. XQuAD consists of human translations of the development and test sets of the English SQuAD 1.1 corpus (Rajpurkar et al., 2016) into 10 languages. For training, we automatically translated the SQuAD training set using an in-house MT system. This process is lossy because the translated answers need to be located within the translated paragraphs. We applied the fuzzy matching procedure in Hu et al.

(2020), but dropping examples more aggressively (when the edit distance between the closest match and translated answer is >5 instead of >10). When comparing such machine-translated datasets, we ensure equal corpus sizes by taking the intersection of questions whose answers were successfully found in the paragraphs after translation.
TyDi QA The Typologically Diverse Question Answering corpus (Clark et al., 2020) gathers human-generated data in 11 languages, for both training and development (the test set is kept private). Speciﬁcally, we use its Gold Passage subtask, which has the same format as XQuAD. In contrast to the latter, TyDi QA contains different context/question pairs across languages. In our experiments, we hold the informational content constant by always comparing in-house machine translations of the same human-generated subset. This is similar to XQuAD, except that it allows the source language to be different from English.
Notation We attach superscripts to dataset names to indicate whether they are the original version of a corpus (O), human-translated (HT) or machine-translated (MT).
5 Models
In our experiments, we ﬁne-tune two widely used pre-trained multilingual language models. We report trends that are consistent between the two model families, and will be less concerned with fully explaining the corner cases when they exhibit different zero-shot behavior.
mBERT Multilingual BERT (mBERT) (Devlin et al., 2018) is an encoder model that was jointly trained on 104 languages from Wikipedia, with masked language model and next-sentence prediction objectives. As elaborated in section 2, mBERT has been extensively studied in the context of zero-shot learning, with impressive crosslingual transfer capabilities.
mT5 Multilingual T5 (mT5) (Xue et al., 2020) is an encoder-decoder model that was jointly trained on 101 languages from Common Crawl. We use its mT5-Base variant, whose encoder is similar in size to mBERT (mT5-Base has a larger parameter count due to the additional decoder). mT5 was also shown to transfer well to new languages.
The zero-shot cross-lingual strengths of the two models are distributed differently across tasks.

Train Latin–High Resource Latin–Low Res.

Miscellaneous

Averages

Data enO deHT esHT frHT swHT trHT viHT arHT bgHT elHT hiHT ruHT urHT thHT zhHT →LH →LL →M →All

mBERT
enO 100.0 92.2 95.7 93.4 81.2 89.0 92.1 94.8 90.5 91.4 89.1 93.8 94.7 80.5 90.5 95.3 87.4 90.7 91.3
deMT -4.2 +7.8 +0.4 +2.1 -4.5 +2.3 +0.7 +2.5 +2.5 +2.0 +4.4 +1.5 +4.0 +5.8 +3.0 +1.5 -0.5 +3.2 +2.0 esMT -3.6 +2.8 +4.3 +2.3 -1.7 -1.0 +2.5 +1.9 +1.9 +0.1 +3.8 +2.6 +3.4 +4.5 +3.2 +1.4 -0.1 +2.7 +1.8 frMT -3.0 +2.9 +1.4 +6.6 -1.9 -1.8 +1.1 +3.0 +1.5 -1.2 +1.2 +2.2 +3.7 +2.7 +3.5 +2.0 -0.9 +2.1 +1.5
swMT -9.7 -3.9 -8.0 -5.1 +18.8 -5.7 -4.3 -4.2 -4.7 -2.7 -1.1 -5.0 -3.0 +0.1 -5.5 -6.7 +2.9 -3.3 -2.9 trMT -14.2 -2.9 -4.6 -2.5 -1.7 +11.0 -2.9 -0.5 -0.1 -1.7 +4.6 -0.3 +2.6 +2.5 +0.4 -6.1 +2.2 +0.9 -0.7 viMT -8.4 -1.0 -2.0 +0.6 -0.9 -2.7 +7.9 +0.6 +0.7 -0.1 +3.4 +0.0 +1.6 +6.5 +1.5 -2.7 +1.4 +1.8 +0.5
arMT -9.3 -0.5 -2.8 +0.2 -2.0 -1.8 -0.7 +5.2 +1.6 +0.3 +2.7 +0.4 +2.6 +1.5 -0.2 -3.1 -1.5 +1.8 -0.2 bgMT -7.6 +0.8 -2.1 +0.5 -3.4 -1.4 -0.1 +1.5 +9.5 +0.7 +3.5 +1.5 +1.8 +2.2 +2.2 -2.1 -1.6 +2.9 +0.6 elMT -9.4 -1.6 -3.4 -0.9 -1.2 -0.5 -1.8 -0.2 +0.7 +8.6 +3.5 -0.6 +0.4 +5.7 -0.3 -3.8 -1.2 +2.2 -0.1 hiMT -15.5 -3.3 -8.4 -3.6 -4.2 -2.1 -3.4 -2.0 -1.9 -3.5 +10.9 -2.4 +7.5 +2.0 -0.3 -7.7 -3.2 +1.3 -2.0 ruMT -6.2 +2.1 -0.1 +1.8 -4.3 -0.6 +2.0 +1.5 +4.8 +2.1 +3.7 +6.2 +4.3 +4.5 +2.9 -0.6 -1.0 +3.7 +1.6 urMT -24.2 -12.9 -16.7 -13.1 -16.1 -12.4 -14.6 -9.8 -9.9 -11.8 +1.5 -9.8 +5.3 -17.0 -9.4 -16.7 -14.3 -7.6 -11.4 thMT -24.1 -11.3 -13.8 -11.3 -4.8 -12.9 -9.8 -10.6 -9.3 -8.6 -10.0 -11.4 -12.6 +19.5 -9.7 -15.1 -9.2 -6.6 -9.4 zhMT -7.0 -0.9 -2.6 +0.1 -9.0 -0.1 +1.6 +0.5 +0.6 -1.4 +3.1 +0.7 +3.6 -0.2 +9.5 -2.6 -2.5 +2.0 -0.1

mT5
enO 100.0 96.0 98.4 99.1 94.0 92.8 96.2 95.0 96.7 97.0 93.0 96.1 93.8 94.3 92.1 98.4 94.3 94.8 95.6
deMT -1.6 +4.0 +0.6 +1.3 +2.7 +2.6 +1.2 +2.7 +1.5 +1.3 +4.5 +2.5 +4.1 +3.2 +2.5 +1.1 +2.2 +2.8 +2.2 esMT -2.0 +0.4 +1.6 +0.8 +1.8 +1.3 +0.9 +2.4 +0.9 +1.3 +3.0 +1.8 +2.3 +2.8 +1.9 +0.2 +1.3 +2.1 +1.4 frMT -2.7 -0.7 -0.5 +0.9 +0.3 -0.4 -2.0 +1.1 -0.9 -0.9 +0.5 -0.1 -0.4 +0.1 +1.9 -0.8 -0.7 +0.1 -0.3
swMT -4.3 -0.8 -1.8 -1.1 +6.0 +1.4 -1.3 +2.9 -1.1 -0.8 +2.8 -0.8 +1.2 +2.6 +2.9 -2.0 +2.0 +1.2 +0.5 trMT -4.8 +0.1 -1.6 -0.7 +1.2 +7.2 -0.4 +1.4 +0.0 +0.2 +4.7 +0.8 +4.6 +2.4 +2.4 -1.7 +2.7 +2.1 +1.2 viMT -6.6 -1.9 -2.1 -1.1 +2.7 -1.0 +3.8 +2.0 -1.1 -0.9 +2.0 -0.8 +2.4 +3.1 +0.5 -2.9 +1.8 +0.9 +0.1
arMT -3.5 -0.6 -0.7 -0.4 -2.5 +0.4 -1.0 +5.0 -0.0 +0.5 +2.0 +0.5 +2.2 +2.8 +2.6 -1.3 -1.0 +2.0 +0.5 bgMT -1.8 +2.2 +0.7 +1.1 +4.3 +3.6 +1.0 +4.4 +3.3 +2.7 +4.9 +2.6 +4.8 +5.6 +4.8 +0.6 +3.0 +4.1 +3.0 elMT -3.1 +0.6 +0.5 +0.4 +3.7 +1.2 +0.7 +3.2 +1.2 +3.0 +3.9 +1.8 +3.4 +3.6 +2.0 -0.4 +1.8 +2.7 +1.7 hiMT -7.0 -1.0 -3.3 -2.2 +1.3 +2.7 -0.6 +1.6 -1.8 -0.2 +7.0 -0.6 +5.7 +0.9 +0.4 -3.4 +1.1 +1.6 +0.2 ruMT -1.7 +0.8 +1.2 +1.7 +4.0 +2.2 +1.4 +3.0 +2.0 +2.1 +4.8 +3.9 +4.0 +4.7 +4.0 +0.5 +2.5 +3.6 +2.5 urMT -8.5 -2.0 -4.5 -3.5 +1.0 +2.6 -1.4 -1.0 -2.0 -0.9 +4.4 -1.2 +6.2 +0.7 +0.1 -4.6 +0.7 +0.8 -0.7 thMT -3.9 -0.3 -1.6 -0.9 +1.4 +0.4 -0.2 +1.8 -0.2 +0.0 +1.0 +0.1 +1.0 +5.7 +3.0 -1.7 +0.6 +1.5 +0.5 zhMT -2.5 +1.5 -0.2 +0.9 +4.8 +3.9 +1.5 +3.7 +1.3 +1.3 +5.1 +2.0 +4.5 +5.7 +7.9 -0.1 +3.4 +3.9 +2.8

Table 2: XNLI: zero-shot transfer. Values for enO are relative zero-shot abilities (Equation 1). Values for other languages (xxMT) are zero-shot advantages over English (Equation 2). Surprisingly, some machine-translated datasets (such as German
and Russian) are more transferable across the board than the original English set.

Compared to mBERT when transferring from English, mT5-Base achieves +10.0 in XNLI accuracy, +4.5 in PAWS-X accuracy, −2.5 in XQuAD F1 score and −2.5 in TyDi QA F1 score, among others (Xue et al., 2020). Its generative nature (which poses challenges for extractive QA) contrasts with mBERT’s extractive approach.
6 Fine-Tuning Procedure
We measure the zero-shot performance of various languages in isolation, by ﬁne-tuning the pretrained models listed in section 5 on monolingual corpora. We follow the ﬁne-tuning procedures established by previous work.
mBERT Similarly to XTREME, we ﬁne-tune mBERT for a ﬁxed number of epochs, with the following hyperparameters: 5 epochs, learning rate 3e-5, training batch size 128. Note that the development set does not contribute to checkpoint selection in any way.
mT5 Following the authors of mT5, we ﬁnetune it with early stopping. We store checkpoints every 200 steps, for a total of 2000 steps. When

using transfer language x, we select the checkpoint with best performance on x’s development set. Finally, we evaluate its zero-shot quality on all other languages using the test set. The only exception is TyDi QA, which kept its test set private: we ﬁne-tune for 500 steps and select the last checkpoint, then evaluate it on the development set.
7 Analysis and Results
We ﬁnd that English is often out-performed by other source languages on standard multilingual benchmarks. Table 1 summarizes our results.
7.1 Sequence Classiﬁcation
In this set of experiments, we ﬁne-tuned mBERT and mT5-Base separately on all 15 source languages from XNLI (Table 2) and all 7 source languages from PAWS-X (Table 3). For both models, both tasks, and all combinations of source S and target T languages, we computed the relative zeroshot ability Z(S → T ) deﬁned in Equation 1.

Train

Latin Scripts

CJK

Averages

Data enO deHT esHT frHT zhHT jaHT koHT Latin CJK All

mBERT
enO 100.0 97.8 97.4 98.3 93.6 94.8 93.2 98.4 93.9 96.4
deMT -1.7 +2.2 +0.5 +0.1 +2.1 +2.9 +2.7 +0.3 +2.6 +1.3 esMT -1.0 -0.1 +2.6 +1.4 +0.9 +1.2 +1.1 +0.7 +1.1 +0.9 frMT -1.7 -0.6 +1.7 +1.7 +2.0 +2.3 +1.3 +0.3 +1.9 +1.0
zhMT -5.1 -2.6 -2.7 -3.4 +6.4 +5.9 +5.0 -3.5 +5.8 +0.5 jaMT -13.8 -10.2 -10.8 -10.5 +0.3 +5.2 +1.7 -11.3 +2.4 -5.4 koMT -8.4 -4.3 -5.3 -5.6 +2.1 +5.6 +6.8 -5.9 +4.9 -1.3

mT5
enO 100.0 98.8 99.6 99.1 95.3 97.5 93.9 99.4 95.6 97.7
deMT -0.2 +1.2 +0.8 +0.6 +2.6 -0.2 +1.9 +0.6 +1.4 +1.0 esMT -1.7 -1.1 +0.4 -0.2 +0.0 -2.4 -0.1 -0.6 -0.8 -0.7 frMT -1.2 -0.6 +0.9 +0.9 +0.5 -1.4 +1.8 +0.0 +0.3 +0.1
zhMT -1.5 -0.1 +0.6 +0.2 +4.7 +3.6 +4.9 -0.2 +4.4 +1.8 jaMT -1.7 -0.5 -0.1 -0.1 +4.5 +2.5 +5.5 -0.6 +4.2 +1.5 koMT -1.1 -0.3 +0.4 +0.2 +3.3 +3.7 +6.1 -0.2 +4.4 +1.8

Table 3: PAWS-X zero-shot transfer (averaged over 3 runs). Values for enO are relative zero-shot abilities (Equation 1). Values for xxMT are zero-shot advantages over English
(Equation 2). Interestingly, German and French both out-
perform English by a signiﬁcant margin across the board.

English is often not the best source language.
Here we tackle our main research question: Is English the most effective source language for zeroshot cross-lingual transfer? To explore this question, we compute, for each source language, its zero-shot advantages over English, as deﬁned in Equation 2. Results in Table 2 and Table 3 identify multiple source languages that out-perform English across the board. For instance, on XNLI (Table 2), German (deMT) scores an average advantage of +2.0 on mBERT and +2.2 on mT5, while Russian (ruMT) achieves an advantage of +1.6 on mBERT and +2.5 on mT5. Notably, these advantages are consistent across groups of targets; Russian doesn’t only transfer better to related languages such as Bulgarian (bgHT), but also to Latin-scripted languages or other scripts such as Thai (thHT). Similarly, for PAWS-X (Table 3), German scores +1.3 on mBERT and +1.0 on mT5, with consistent gains across target groups.
Most remarkably, all non-English transfer sets were machine-translated from a corpus initially written in English. The fact that the latter is de-ranked by automated translations is counterintuitive, since the conversion process is presumed to be imperfect. This ﬁnding offers a simple yet effective recipe for improving multilingual systems trained via zero-shot transfer: instead of ﬁnetuning on English data, translate it ﬁrst.3
3This recipe only applies to tasks that accommodate for machine translation, that is, the labels remain valid or can be

Train Data entest artest bgtest detest eltest estest frtest hitest rutest swtest thtest trtest urtest vitest zhtest

Average advantage over English

(Equation 2)

HT N/A MT

N/A

HT -0.3 MT

-0.0

HT +0.8 MT

+1.0

HT +1.2 MT

+0.6

HT +0.5 MT

+0.3

HT +1.5 MT

+1.4

HT +0.8 MT

+0.3

HT -0.8 MT

+0.1

HT +0.6 MT

+0.7

HT -8.9 MT

-10.7

HT -5.8 MT

-5.7

HT -1.9 MT

-1.4

HT -0.9 MT

-0.5

HT +0.4 MT

+0.8

HT +0.6 MT

+0.7

Table 4: XNLI models ﬁne-tuned on translations of the English test set (re-purposed for training), produced either by humans (HT) or by an MT system. Transferability is comparable across the two columns, meaning that the inferior performance of English in Table 2 and Table 3 is not explained by fortunate artefacts of MT.

Does machine translation boost transferability? In light of the results above, the following question arises: are the gains over the English baseline due to the other languages being intrinsically better sources, or from a fortunate side-effect of automated translation (e.g. insertion of noise in the transfer set that makes the model less prone to over-ﬁtting)? To tease these apart, we propose a new experiment using the XNLI dataset: we re-purpose the test set (which was human-generated) for training. We train models on both the human-translated (HT) and their in-house machine translations (MT) and measure how the advantage over English differs in the two scenarios. We use the human-generated development set for evaluation. Table 4 shows that English is outperformed by the same set of languages in both cases, without evidence that machine-translated training sets transfer better than human-generated ones.
mBERT vs mT5 Across both tasks, mBERT displays larger gaps than mT5 between the highest and lowest performing languages. For instance, for XNLI, Urdu’s average (dis)advantage over English is −11.4 on mBERT and only −0.7 on mT5 (with successful transfer to Hindi in particular). This might be due to mT5’s more comprehensive
adapted after the text was translated. If the budget permits, the alternative is to collect data from scratch in one of the better source languages.

Model

Source

Other Languages

Averages

enO deHT ruHT arHT elHT esHT hiHT thHT trHT viHT zhHT →Other →All

mBERT mBERT mBERT

enO

82.2 67.1 67.3 54.4 57.6 72.9 52.0 36.3 49.7 66.5 55.4

enO→deMT -4.7 +5.8 +1.5 +1.2 +4.3 -0.7 +0.7 +3.3 -2.5 -1.3 -1.2

enO→ruMT -7.4 -0.4 +5.9 +5.9 +2.0 -4.1 +1.3 +8.2 +4.8 +1.4 +2.7

55.6 60.1 +0.5 +0.6 +2.8 +1.8

mT5-Base enO

84.5 72.4 58.2 64.0 59.1 74.2 60.1 56.8 68.1 70.8 67.6 65.1 66.9

mT5-Base enO→deMT -6.5 +4.0 -3.7 -5.6 -7.0 -1.3 -4.4 +1.2 -2.4 -2.7 -1.5 -3.0 -2.7

mT5-Base enO→ruMT -6.2 -3.2 +18.1 -2.2 -9.5 -3.9 -6.5 +2.2 -4.9 -5.6 -2.7 -4.2 -2.2

mT5-Uniform enO

82.1 66.2 63.0 57.6 62.2 69.2 52.0 49.4 55.6 60.4 64.4

mT5-Uniform enO→deMT -9.8 +5.6 +0.7 -0.9 +1.4 -3.7 +4.3 +6.9 -0.0 -3.9 -0.2

mT5-Uniform enO→ruMT -9.4 -1.6 +8.7 +2.0 +3.0 -0.6 +3.2 +3.8 +1.6 +1.8 -2.0

58.9 62.0 +0.5 +0.0 +1.6 +1.0

Table 5: XQuAD (F1 scores). The pre-trained models were ﬁne-tuned separately on the original English (enO) dataset (for which we show F1 scores) and its machine translations to German (deMT) and Russian (deMT) subsets (for which we show improvements over enO F1). All three datasets contain the same 80,000 questions. On mBERT and mT5-Uniform, English is
out-performed by the other datasets, despite them being machine-translated.

pre-training set, and suggests that the handling of a particular language during pre-training can inﬂuence its later zero-shot ability during ﬁne-tuning.
7.2 Extractive Question Answering (QA)
Next, we investigate whether German and Russian, two of the languages that out-performed English in the experiments above, hold their advantage in question answering tasks. For this purpose, we ﬁne-tuned mBERT and mT5 separately on the English SQuAD corpus (enO) and two in-house translations (deMT and ruMT), and evaluated their transferability to XQuAD. See section 4 for more information on these QA datasets.
QA on mBERT: Russian transfers better
Table 5 shows that, when ﬁne-tuning mBERT, English is signiﬁcantly out-performed by Russian (+2.8 F1→Other) and is on-par with German (+0.5 F1→Other), despite the last two being machine-translated. Zero-shot transfer to Thai (thHT) beneﬁts the most: transferring from Russian brings an additional +8.2 F1 over transferring from English. Interestingly, Hu et al. (2020) showed that mBERT generally suffers from impoverished transfer to Thai across multiple tasks. It is remarkable that such a pervasive issue can be partly mitigated by simply transferring from Russian, even when machine-translated.
QA on mT5: English transfers better
We repeated the XQuAD experiment on mT5 and, in contrast to all previous results, the original English training set enO performs signiﬁcantly better than its German translation deMT (−3.0 F1→Other) and Russian translation ruMT

(−4.2 F1→Other). Interestingly however, zeroshot transfer to Thai (thHT) is still more effective from both German (+1.2) and Russian (+2.2).
This anomaly could be linked to the purely generative nature of mT5, which is less aligned with the task of extractive QA and is known to produce illegal predictions such as accidental translations (Xue et al., 2020, 2021). Generally, the behavior of zero-shot cross-lingual transfer in generative models is under-studied. The presence of a decoder is yet another variable that can inﬂuence cross-lingual transferability, and interact with other aspects of training (model capacity, quality and distribution of pre-training data, language of ﬁne-tuning data, etc.). The reason why ﬁne-tuning mT5 on English leads to better transfer remains an open question. The rest of this section makes observations that can inform future investigations.
XQuAD contains English-centric content.
Even though the test sets in XQuAD were humantranslated (and therefore likely high-quality), Table 7 shows that some answers consist of English entities that were understandably neither translated nor transliterated (e.g. "Lady Gaga"). We roughly quantify this phenomenon by counting, for each language that doesn’t use the Latin script, how many answers are exclusively ASCII and contain at least one letter (to exclude numeric answers like years). Notably, Greek (elHT) is the language with highest proportion of such answers (9.4%), and also the target language that suffers the most when transferring from German (−7.0 F1) or Russian (−9.5 F1) instead of English. This nonnegligible portion of English test answers could be

Other Languages

Averages

Model

Source

enO ruO fiO arO bnO idO koO swO teO →Other →All

mT5-Base fiO→enMT 63.3 39.4 51.6 51.1 20.0 60.1 32.2 62.7 37.5 43.9 46.4

mT5-Base fiO→deMT -8.0 -4.1 +0.0 -5.0 -1.5 -3.3 -1.5 -3.0 -14.2 -4.8 -4.5

mT5-Base fiO→ruMT -13.0 +10.3 -10.9 -9.0 -5.6 -7.6 -6.0 -19.8 -13.1 -10.2 -8.3

mT5-Base fiO

-5.6 -2.0 +8.6 +0.4 -3.0 +1.2 -2.0 -2.5 -6.7 -2.1 -1.3

mT5-Uniform fiO→enMT 62.2 49.1 47.4 58.1 21.7 52.7 25.3 42.8 23.8 37.4 42.6

mT5-Uniform fiO→deMT -8.2 +0.8 +0.7 +2.2 +2.6 -0.6 +2.8 -1.0 -4.4 +0.3 -0.6

mT5-Uniform fiO→ruMT -14.6 -2.4 -2.1 +0.7 +9.0 -0.4 +2.0 -3.2 -2.9 +0.9 -1.5

mT5-Uniform fiO

-10.7 -1.7 +10.5 +1.9 +10.7 +4.1 +9.1 +7.9 +2.4 +6.0 +3.8

Table 6: TyDi QA-GoldP (F1 scores) after ﬁne-tuning mT5-Base and mT5-Uniform on datasets that were machine-translated from the original Finnish subset fiO (6,800 instances). For the English translation enMT, we show F1 scores; for all others, we show improvements over enMT F1. mT5-Uniform (trained on 32B tokens) shows smaller gaps between source languages than
mT5-Base (trained on 1T tokens). See Table 8 for the same analysis when the source dataset is in Arabic.

a reason why the English source scores higher.4
Translated English still transfers well on mT5.
In all the experiments above, the English training set was produced by humans, while all others were machine-translated. Our goal here is to test whether the mT5/QA setting is particularly susceptible to this difference. We level the playing ﬁeld by leveraging the TyDi QA corpus differently from its standard usage. We select the two largest training sets: Arabic (14,000 instances) and Finnish (6,800 instances); in turn, we machine-translate these original sets into English, German, and Russian.
Results in Table 6 and Table 8 show that, surprisingly, the gap between English and the other two transfer languages becomes even more salient. It is possible that these differences stem, at least partly, from uneven translation quality across language pairs. However, the fact that enMT scores +2.1 F1 higher than the original dataset fiO implies that the superior transferability of English compared to deMT and ruMT on TyDi QA is not just due to better fiO→enMT translation quality.
Under-trained mT5 closes or reverses the gap.
Another possibility is that the pre-training strategy is responsible for the discrepancy between English and other sources when ﬁne-tuning mT5 on QA. This hypothesis is supported by the following observation: when mT5 is under-trained with 32B tokens (instead of 1T as the published model)
4A reasonable counter-argument to this hypothesis is that, when using the same XQuAD dataset to ﬁne-tune mBERT, English was outranked–hence the dataset cannot be the culprit. However, implicit in this hypothesis is the fact that the generative nature of mT5 makes it more susceptible to artefacts in the data compared to mBERT.

XQuAD Test Set
arHT elHT hiHT ruHT thHT zhHT

%ASCII Answers
0.4% 9.4% 1.8% 3.4% 3.3% 2.2%

Examples of ASCII-only test answers in elHT
120 m Lady Gaga State Route 99 Toyota Corona Mark II User Datagram Protocol "business as usual" (BAU)

Table 7: Languages with non-Latin scripts have nonnegligible proportions of ASCII-only answers (with at least one letter) in the XQuAD test set. Some of these are Englishcentric entities that cannot be translated.

using a uniform sampling distribution across pretraining languages5, the gap between English and other ﬁne-tuning languages is either closed or reversed (see the mT5-Uniform model in Table 5, Table 6 and Table 8).
8 Conclusion
In this study, we presented empirical evidence that zero-shot cross-lingual transfer from languages other than English can be more effective, especially when the set of target languages is diverse or unknown in advance. Our experiments surface German and Russian as very strong candidates in most settings, even when machine-translated from English. One exception is question answering on mT5; however, when its pre-training strategy is altered, the performance gap between sources is closed or inverted. These ﬁndings provide an immediately applicable recipe for improving zeroshot systems (translate them to German or Russian ﬁrst) and can inform future data collection efforts.
5We also pre-trained mT5-Base on 32B tokens with a sampling distribution proportional to dataset sizes (same sampling as the original paper), but observed severe degradation when transferring to lower-resource languages.

There are multiple future directions for study. Investigating the most effective combinations of transfer languages under a limited data collection budget is a natural next step. Analyzing the relationship between pre-training and the effectiveness of source languages during ﬁne-tuning is another interesting avenue.
Acknowledgements
The authors wish to thank Noah Constant, Jonathan H. Clark and Alexis Conneau for their feedback on this work. We would also like to thank our linguistic consultant Vitaly Nikolaev, and our colleagues David Reitter, Peter Shaw and Henry Tsai for their advice.
References
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Association for Computational Linguistics.
Jan A. Botha, Zifei Shan, and Daniel Gillick. 2020. Entity Linking in 100 Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7833–7845, Online. Association for Computational Linguistics.
Steven Cao, Nikita Kitaev, and Dan Klein. 2020. Multilingual alignment of contextual word representations. In International Conference on Learning Representations.
Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, and Wei Xu. 2017. Joint training for pivot-based neural machine translation. In Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence, IJCAI-17, pages 3974–3980.
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454–470.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics.
Ryan Cotterell and Georg Heigold. 2017. Crosslingual character-level neural morphological tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 748–759, Copenhagen, Denmark. Association for Computational Linguistics.
Raj Dabre, Fabien Cromieres, Sadao Kurohashi, and Pushpak Bhattacharyya. 2015. Leveraging small multilingual corpora for SMT using many pivot languages. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1192–1202, Denver, Colorado. Association for Computational Linguistics.
Raj Dabre, Aizhan Imankulova, Masahiro Kaneko, and Abhisek Chakrabarty. 2021. Simultaneous multi-pivot neural machine translation.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Ken Hale, Michael Krauss, Lucille J. Watahomigie, Akira Y. Yamamoto, Colette Craig, LaVerne Masayesva Jeanne, and Nora C. Eng-

land. 1992. Endangered languages. Language, 68(1):1–42.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating crosslingual generalization. CoRR, abs/2003.11080.
Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth. 2020. Cross-lingual ability of multilingual bert: An empirical study. In International Conference on Learning Representations.
Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. 2019. Pivot-based transfer learning for neural machine translation between non-English languages. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 866–876, Hong Kong, China. Association for Computational Linguistics.
Anne Lauscher, Vinit Ravishankar, Ivan Vulic´, and Goran Glavaš. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483–4499, Online. Association for Computational Linguistics.
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6008–6018, Online. Association for Computational Linguistics.
Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe

Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. 2019. Choosing transfer languages for cross-lingual learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3125–3135, Florence, Italy. Association for Computational Linguistics.
Michael Paul, Andrew Finch, and Eiichrio Sumita. 2013. How to choose the best pivot language for automatic translation of low-resource languages. ACM Transactions on Asian Language Information Processing, 12(4).
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001, Florence, Italy. Association for Computational Linguistics.
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic´, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362–2376, Online. Association for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.
Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, and Yinfei Yang. 2020. LAReQA: Language-agnostic answer retrieval from a multilingual pool. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5919–5930, Online. Association for Computational Linguistics.
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, and Melvin Johnson. 2021. Xtreme-r: Towards more challenging and nuanced multilingual evaluation.

Sebastian Ruder, Ivan Vulic´, and Anders Søgaard. 2019. A survey of cross-lingual word embedding models. J. Artif. Int. Res., 65(1):569–630.
Zihan Wang, Karthikeyan K, Stephen Mayhew, and Dan Roth. 2020a. Extending multilingual BERT to low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2649–2656, Online. Association for Computational Linguistics.
Zirui Wang, Zachary C. Lipton, and Yulia Tsvetkov. 2020b. On negative interference in multilingual models: Findings and a metalearning treatment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4438– 4450, Online. Association for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122. Association for Computational Linguistics.
Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833–844, Hong Kong, China. Association for Computational Linguistics.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2021. Byt5: Towards a token-free future with pre-trained byteto-byte models.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer.
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A Cross-lingual

Adversarial Dataset for Paraphrase Identiﬁcation. In Proc. of EMNLP.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase Adversaries from Word Scrambling. In Proc. of NAACL.

Model
mT5-Base mT5-Base mT5-Base mT5-Base
mT5-Uniform mT5-Uniform mT5-Uniform mT5-Uniform

Source
arO→enMT arO→deMT arO→ruMT arO
arO→enMT arO→deMT arO→ruMT arO

enO
65.2 -8.4 -17.3 -1.0
61.1 -7.9 -12.6 -8.2

ruO
42.1 -7.2 +5.5 +12.3
48.6 +0.8 -1.5 +6.5

arO
52.9 -5.7 -9.9 +30.0
58.8 +1.3 -0.5 +21.7

bnO
15.9 +2.3 -2.8 +13.6
21.3 +3.8 +7.1 +6.8

fiO
53.9 -2.2 -12.0 +10.4
47.0 +0.1 -1.1 +2.4

Other Languages idO koO

63.2 -6.1 -11.4 +6.9

30.5 +0.2 -6.1 +19.4

52.9 26.1 -0.8 +0.9 -0.3 +2.2 +8.5 +2.8

swO
60.3 -0.5 -16.0 +1.8
40.9 -1.5 -2.1 +0.6

teO
35.9 -16.1 -15.9 +5.2
22.9 -5.1 -1.3 +3.8

Averages →Other →All

43.3 -3.7 -10.7 +9.6

46.7 -4.8 -9.5 +11.0

35.2 42.2 -0.4 -0.9 +0.8 -1.1 +4.2 +5.0

Table 8: TyDi QA-GoldP (F1 scores) after ﬁne-tuning mT5-Base and mT5-Uniform on datasets that were machine-translated from the original Arabic subset arO (14,000 instances). For the English translation enMT, we show F1 scores; for all others, we show improvements over enMT F1. mT5-Uniform (trained on 32B tokens) shows smaller gaps between source languages
than mT5-Base (trained on 1T tokens). See Table 6 for the same analysis when the source dataset is in Finnish.

