arXiv:2110.08258v3 [cs.LG] 17 Feb 2022

A Framework for Learning to Request Rich and Contextually Useful Information from Humans
Khanh Nguyen∗ 1 Yonatan Bisk 2 Hal Daum´e III 1,3
1University of Maryland, College Park 2Carnegie Mellon University 3Microsoft Research
Abstract
When deployed, AI agents will encounter problems that are beyond their autonomous problemsolving capabilities. Leveraging human assistance can help agents overcome their inherent limitations and robustly cope with unfamiliar situations. We present a general interactive framework that enables an agent to determine and request contextually useful information from an assistant, and to incorporate rich forms of responses into its decision-making process. We demonstrate the practicality of our framework on a simulated human-assisted navigation problem. Aided with an assistance-requesting policy learned by our method, a navigation agent achieves up to a 7× improvement in success rate on tasks that take place in previously unseen environments, compared to fully autonomous behavior. We show that the agent can take advantage of different types of information depending on the context, and analyze the beneﬁts and challenges of learning the assistance-requesting policy when the assistant can recursively decompose tasks into subtasks.
1 Introduction
Machine learning has largely focused on creating agents that can solve problems on their own. Despite much progress, these autonomous agents struggle to perform novel tasks or operate in new environments (Goodfellow et al., 2014; Jia and Liang, 2017; Eykholt et al., 2018; Qi et al., 2020; Shridhar et al., 2020). However, an agent’s abilities can be extended if it is equipped with humancompatible communication to leverage assistance from humans in its environment (Rosenthal et al., 2010; Tellex et al., 2014; Nguyen et al., 2019; Nguyen and Daum´e III, 2019; Thomason et al., 2020). For example, consider a home-assistant robot assigned a task of ﬁnding a newly bought rice cooker in a house, which it has never heard of and therefore does not how to proceed. The robot can overcome its limitation by asking “where is the rice cooker? ”, expecting an instruction like “in the kitchen, near the sink ”, which it may know how to execute. By making a request for new information, the robot has converted the initially diﬃcult task into a more feasible one.
This paper presents a general, POMDP-based interactive framework that allows agents to effectively leverage human assistance through communication. We identify and provide solutions to two fundamental problems: the speaker problem and the listener problem. The speaker problem concerns teaching an agent to elicit information from humans that can improve its actions. Inspired
∗Corresponding author (kxnguyen@umd.edu).
1

Environment

C

GOAL (tell me about the
goal location)

The cooker is in the kitchen, next
to a sink.

SUB (break the current task into subtasks and give
me the ﬁrst one)

Find the statue in the hallway.

BLiving room
I’m ﬁnding a rice cooker

Hallway
D

A
CUR (tell me about my current location)

You are in the living room, next
to a couch.

subtask
stack
DONE [subtask) (stop doing subtask, resume main task)

C
DONE [main task) (terminate)

A E Kitchen

subtask
B stack D E

Figure 1.1: An illustration of our framework in a navigation task. The agent can only observe part of an environment and is asked to ﬁnd a rice cooker. An assistant communicates with the agent and can provide information about the environment and the task. Initially (A) the agent may request information about the goal, but may not know where it currently is. For example, at location B, due to limited perception, it does not recognize that it is next to a couch in a living room. It can obtain such information from the assistant. If the current task becomes too diﬃcult (like at location C), the agent can ask the assistant for a simpler subtask that helps it make progress toward the goal. When the agent receives a subtask, it pushes the subtask to the top of a goal stack, and when it decides to stop executing a subtask, it pops the subtask from the stack (e.g., at location D). At location E, the agent empties the stack and terminates its execution.

the intention-based theory of human communication (Sperber and Wilson, 1986; Tomasello et al., 2005; Scott-Phillips, 2014), we formulate a framework that equips the learning agent with a set of general types of information that are helpful in solving any POMDP problem. At every time step, our agent can request a description about (i) its current state, (ii) the goal state which it needs to reach, or (iii) a new subgoal state which, if reached, helps it make progress on the current task. The agent learns to decide which type of information to request through reinforcement learning, by learning how much each type of information enhances its performance in a given situation. The agent’s request decisions are thus grounded in its own perception of its (un)certainties about the current situation and are optimized to be maximally contextually useful to it.
Our approach contrasts with methods to train agents to ask questions by mimicking those asked by humans (Labutov et al., 2015; Mostafazadeh et al., 2016; De Vries et al., 2017; Rao and Daum´e III, 2018; Liu et al., 2020). While eﬀective in some situations, this approach has a potential signiﬁcant drawback: questions asked by humans may not always be contextually useful for agents. For example, humans are experts in recognizing objects, thus rarely ask questions like “what objects are next to me?” Meanwhile, such questions may be helpful for robot localization, as robots may have imperfect visual perception in unfamiliar environments. Rather than focusing on naturalness, we aim to endow an agent with the cognitive capability of determining which type of information would be contextually useful to itself.
In addition to being able to ask for useful information, the agent must be able to incorporate the information it receives into its decision-making process: the listener problem. Humans can oﬀer various types of assistance and may express them using diverse media (e.g., language, gesture, image). Our framework implements a ﬂexible communication protocol that is deﬁned by the agent’s task-solving policy: information coming from the human is given as state descriptions that the

2

policy can take as input and map into actions. Hence, the space of information that the human can convey is as rich as the input space of the policy. This design takes advantage of the ﬂexibility of constructing an agent policy: (i) the policy can be further trained to interpret new information from humans, and (ii) it can leverage modern neural network architectures to be able to encode diverse forms of information. This contrasts with existing approaches based on reinforcement learning or imitation learning (Knox and Stone, 2009; Judah et al., 2010; Torrey and Taylor, 2013; Judah et al., 2014), in which feedback to the agent is limited to scalar feedback of rewards (in RL) or actions (in IL).
To evaluate our framework, we simulate a human-assisted navigation problem where an agent can request extra information about the environment. Our agent learns an intention policy to decide at each step whether it wants to request additional information in a given situation, and, if so, which type of information it wants to obtain. On tasks in previously unseen environments, the ability to ask for help improves the agent’s success rate by 7× compared to performing tasks on its own. This human-assisted agent even outperforms an agent that has perfect perception and goal descriptions in unseen environments, thanks to the ability to request subgoals.

2 Preliminaries

We consider an environment deﬁned by a partially observed Markov decision process (POMDP)

E = (S, A, T, c, D, ρ) with state space S, action space A, transition function T : S ×A → ∆(S), cost

function c : S × A → R, description space D, and description function ρ : S → ∆(D), where ∆(Y) denotes the set of probability distributions over Y.1 We refer to this as the execution environment,

where the agent performs tasks—e.g., a navigation environment. A task is a tuple (s1, g1, dg1) where s1 is the start state, g1 is the goal state, and dg1 is a limited
description of g1. Initially, a task (s1, g1, dg1) is sampled from a task distribution T. An agent starts in s1 and is only given the goal description dg1. It has to reach the goal state g1 within H time steps. Let gt and dgt be the goal state and goal description being executed at time step t, respectively. In a standard POMDP, gt = g1 and dgt = dg1 for 1 ≤ t ≤ H. But later, we will enable the agent to set
new goals via communication with humans.

At time t, the agent does not know its state st but only receives a state description dst ∼ ρ(st). Given dst and dgt , the agent makes a decision at ∈ A, transitions to the next state st+1 ∼ T (st, at), and receives a cost ct c(st, at). When the agent takes the adone action to terminate its ex-

ecution, it receives a task error c(st, adone) that indicates how far it is from actually complet-

ing the task. The agent’s goal is to reach g1 with minimum total cost C(τ ) =

H t=1

ct,

where

τ = (s1, ds1, a1, . . . , sH , dsH ) is an execution of the task.

We

denote

by

bst

a

belief

state—a

description

of

the

history

(

ds1

,

a1

,

.

.

.

,

d

s t

)—and

by

B

the

set

of all belief states. The agent maintains an execution policy πˆ : B × D → ∆(A). The learning

objective is to estimate an execution policy that minimizes the expected total cost of performing

tasks:

min E(s1,g1,dg)∼T,τ ∼Pπ(·|s1,dg) [C(τ )]

π

1

1

(2.1)

where Pπ(· | s1, dg1) is the distribution over executions generated by a policy π given start state

1We say “description” in lieu of “observation” to emphasize (i) the description can come in varied modalities (e.g., image or text), and (ii) it can be obtained via perception as well as communication.

3

s1 and goal description dg1. In a standard POMDP, an agent performs tasks on its own, without asking for any external assistance.
3 Leveraging Assistance via Communication
For an agent to accomplish tasks beyond its autonomous capabilities, we introduce a human assistant, who can provide rich information about the environment and the task. We then describe how the agent requests and incorporates information from the assistant. Figure 1.1 illustrates an example communication between the agent and the assistant on an example object-ﬁnding task. Our framework is inspired by the intention-based theory of human communication (Sperber and Wilson, 1986; Tomasello et al., 2005; Scott-Phillips, 2014), which characterizes communication as the expression and recognition of intentions in context. In this section, we draw connections between the theory and elements of POMDP learning, which allows us to derive reinforcement learning algorithms to teach the agent to make intentional requests.
Assistant. We assume an ever-present assistant who knows the agent’s current state st, the goal state gt, and the optimal policy π . Their ﬁrst capability is to provide a description of a state, deﬁned by function ρA : S × D → ∆(D) where ρA(d | s, d) speciﬁes the probability of giving d to describe state s given a current description d, which can be empty. The second capability is to propose a subgoal of a current goal, speciﬁed by a function ωA : S × S → ∆(S), where ωA(g | s, g) indicates the probability of proposing g as a subgoal given a current state s and a goal state g.
Common ground. Common ground represents mutual knowledge between the interlocutors and is a prerequisite for communication (Clark and Brennan, 1991; Stalnaker, 2002). In our context, knowledge of the agent is contained in its execution policy πˆ, while knowledge of the assistant is given by π . When πˆ and π maps to the same action distribution given an input (bs, dg), we say that the input belongs to the common ground of the agent and the assistant. For meaningful communication to take place, we assume that execution policy has been pre-trained to a certain level of performance so that there exists a non-empty subset of inputs on which the outputs of πˆ and π closely match.
3.1 The Listener Problem: Incorporating Rich Information Provided by the Assistant
Upon receiving a request from the agent, the assistant replies with a state description d ∈ D. The generality of our notion of state description (see § 2) means that the assistant can provide information in any medium and format, so long as it is compatible with the input interface of πˆ. This reﬂects that only information interpretable by the agent is useful.
Concretely, the assistant can provide a new current-state description dst+1, which the agent appends to its history to compute belief state bt+1 (e.g., using a recurrent neural network). The assistant can also provide a new goal description dgt+1, in which case the agent simply replaces the current goal description dgt with the new one. Our framework allows the human-agent communication protocol to be augmented in two ways: (i) training πˆ to interpret new state descriptions or (ii) designing its model architecture to incorporate new types of information. In comparison, frameworks that implement a reinforcement learning- or imitation learning-based communication
4

protocol (e.g., Knox and Stone, 2009; Torrey and Taylor, 2013) allow humans to only give advice using low-bandwidth media like rewards or primitive actions.

3.2 The Speaker Problem: Requesting Contextually Useful Information
Asking Questions as a Cognitive Capability. Asking questions is a cognitive process motivated by a person’s self-recognition of their knowledge deﬁcits or mismatches with a common ground (Graesser et al., 1992). The intention-based theory of human communication suggest that a question, like other communicative acts, should convey an information-seeking intention that is grounded in the speaking context. In this case, the speaker’s decision-making policy should also be included in the context and, ultimately, a speaker asks a question to enhance their next decisions.
Approaches that teach an agent to mirror questions asked by humans (e.g., De Vries et al., 2017), do not consider the agent’s policy as part of the generation context. The training questions are selected to be useful for the human speakers, not the learning agent. To address this issue, we endow the agent with the cognitive capability of anticipating how various types of information would aﬀect its future performance. The agent learns this capability using reinforcement learning, via interacting with the assistant and the environment, rather than imitating pre-collected human behaviors.

Information-Seeking Intentions. While humans possess capabilities that help them come up with questions, it is unclear how to model those capabilities. Some approaches Mostafazadeh et al. (e.g., 2016); Rao and Daum´e III (e.g., 2018) rely on pre-composed questions, which are domainspeciﬁc. We instead endow the agent with a set of intentions that correspond to speech acts in embodied dialogue (Thomason et al., 2020). They are suﬃciently general to be relevant to solving general POMDPs, and are connected to the agent’s decision-making process, while being agnostic to its implementation:
(a) Cur: requests a new description of the current state st and receives dst+1 ∼ ρA (· | st, dst ); (b) Goal: requests a new description of the current goal gt and receives dgt+1 ∼ ρA (· | gt, dgt ); (c) Sub: requests a description of a subgoal and receives dgt+1 ∼ ρA (· | gt+1, ∅) where the subgoal
gt+1 ∼ ωA (· | st, gt) and ∅ is an empty description. These intentions can potentially guide construction of more speciﬁc intentions (e.g., asking a speciﬁc feature of description). We leave this problem for future exploration.

Intention Selection. To decide which intention to invoke, the agent learns an intention pol-

icy ψθ, which is itself a function of πˆ. The policy’s action space consists of ﬁve intentions: A¯ = {Cur, Goal, Sub, Do, Done}. The ﬁrst three actions convey the three intentions deﬁned

previously. The remaining two actions traverse the environment:

(d) Do: executes the most-probable action adt o to a new state st+1 ∼ T (st, adt o);

argmaxa∈A πˆ (a | bst , dgt ). The agent transitions

(e) Done: decides that the current goal gt has been reached. If gt is the main goal (gt = g1), the

episode ends. If gt is a subgoal (gt = g1), the agent may choose a new goal to follow.

In our implementation, we feed the hidden features and the output distribution of πˆ as inputs

to ψθ so that the agent can take its execution policy into account when choosing its intentions.

We do not yet formally specify the input space of the interaction policy nor how the next subgoal

is chosen (when Done is taken), as these details depend on how the agent implements its goal

5

memory. The next section introduces an instantiation where the agent uses a stack data structure to manage goals.
4 Learning When and What to Ask
To formalize the problem of learning to select information-seeking intentions, we construct the POMDP environment that the intention policy acts in, referred to as the intention environment (§ 4.1) to distinguish with the environment that the execution policy acts in. Our construction employs a goal stack to manage multiple levels of (sub)goals (§ 4.2). A goal stack stores all the (sub)goals the agent has been assigned but has not yet decided to terminate, which is updated in every step depending on the selected intention action. Finally, we design a cost function (§ 4.4) that trades oﬀ between taking few actions and completing tasks.
4.1 Intention Environment
Given an execution environment E = (S, A, T, c, D, ρ), the intention environment is a POMDP E¯ = (S¯, A¯, T¯ , c¯, D¯, ρ¯): • State space S¯ = S × D × GL, where GL is the set of all goal stacks of at most L elements. Each
state s¯ = (s, ds, G) ∈ S¯ is a tuple of an execution state s, description ds, and a goal stack G. Each element in the goal stack G is a tuple of a goal state g and description dg; • Action space A¯ = {Cur, Goal, Sub, Do, Done}; • State-transition function T¯ = Ts · TG where Ts : S × D × A¯ → ∆(S × D) and TG : GL × A¯ → ∆(GL); • Cost function c¯ : (S × GL) × A¯ → R, deﬁned in § 4.4 to trade oﬀ operation cost and task error; • Description space D¯ = D × GLd where GLd is the set of all goal-description stacks of size L. The agent cannot access the environment’s goal stack G, which contains true goal states, but only observe descriptions in G. We call this partial stack a goal-description stack, denoted by Gd; • Description function ρ¯ : S¯ → D¯, where ρ¯(s¯) = ρ¯(s, ds, G) = (ds, Gd). Unlike in the standard POMDP formulation, this description function is deterministic.
A belief states ¯bt of the intention environment summarizes a history (s¯1, a¯1, · · · , s¯t). We deﬁne the intention policy as ψθ : B¯ → ∆(A), where B¯ is the set of all belief states.
4.2 Goal Stack
The goal stack is a list of tasks that the agent has not declared complete. The initial stack G1 = {(g1, dg1)} contains the main goal and its description. At time t, the agent executes the goal at the top of the current stack, i.e. gt = Gt.top(). Only the Goal, Sub, and Done actions alter the stack. Goal replaces the top goal description of Gt with the new description dgt+1 from the assistant. Sub, which is only available when the stack is not full, pushes a new subtask (gt+1, dgt+1) to Gt. Done pops the top element from Gt. The goal-stack transition function is TG(Gt+1 | Gt, a¯t) = 1{Gt+1 = Gt.update(a¯t)} where 1{.} is an indicator and Gt.update(a) is the stack after taking action a.
6

4.3 State Transition

The transition function Ts is factored into two terms:

Ts(st+1, dst+1 | st, dst , a¯t) = P (st+1 | st, a¯t) · P (dst+1 | st+1, dst , a¯t)

pstate (st+1 )

pdesp (dst+1 )

(4.1)

Only the Do action changes the execution state, with: pstate(st+1) = T st+1 | st, adot , where ado is the action chosen by πˆ and T is the execution environment’s state transition function. The
current-state description is altered when the agent requests a new current-state description (Cur), where pdesp(dst+1) = ρA(dst+1 | st+1, dst ), or moves (Do), where pdesp(dst+1) = ρ(dst+1 | st+1) (recall ρ is the description distribution of the execution environment).

4.4 Cost Function
The intention policy needs to trade-oﬀ between two types of cost: the cost of operation (making information requests and traversing in the environment), and the cost of not completing a task (task error). For example, the agent may lower its task error if it is willing to suﬀer a larger operation cost by making more requests to the assistant.
We employ a simpliﬁed model where all types of cost are non-negative real numbers of the same unit. Making a request of type a is assigned a constant cost γa. The cost of taking the Do action is c(st, adt o), the cost of executing the adt o action in the environment. Calling Done to terminate execution of the main goal g1 incurs a task error c(st, adone). We exclude the task errors of executing subgoals because the intention policy is only evaluated on reaching the main goal. The magnitudes of the costs naturally specify a trade-oﬀ between acting cost and task error. For example, setting the task errors much larger than the other costs indicates that completing tasks is prioritized over taking few actions.

5 Modeling Human-Assisted Navigation
Problem. We apply our framework to modeling a human-assisted navigation (Han) problem, in which a human requests an agent to ﬁnd an object in an indoor environment. Each task request asks the agent to go to a room of type r and ﬁnd an object of type o (e.g., ﬁnd a mug in a kitchen). The agent shares its current view with the human (e.g. via an app). We assume that the human is familiar with the environment and can recognize the agent’s location. Before issuing a task request, the human imagines a goal location (not revealed to the agent). We are interested in evaluating success in goal-ﬁnding, i.e. whether the agent can arrive at the human’s intended goal location. Even though there could be multiple locations that match a request, the agent only succeeds if it arrives exactly at the chosen goal location.
Environment. We construct the execution environments using the house layout graphs provided by the Matterport3D simulator (Anderson et al., 2018). Each graph is generated from a 3D model of a house where each node is a location in the house and each edge connects two nearby unobstructed locations. At any time, the agent is at a node of a graph. Its action space A consists of traversing to any of the nodes that are adjacent to its current node.

7

Scenario. We simulate the scenario where the agent is pre-trained in simulated environments and then deployed in real-world environments. Here, due to the mismatches between the pre-training and deployment conditions, the capability of the agent degrades. It may not reliably recognize objects, the room it is currently in, or the intent of a request. However, the simulated human assistant is available to provide additional information about the environment and the task, which helps the agent relate its current task to one it has learned to fulﬁll during pre-training.
Subgoals. If the agent requests a subgoal, the assistant describes a new goal roughly halfway to its current goal. Speciﬁcally, let pt be the shortest path from the agent’s current state st to the current goal gt, and pt,i be the i-th node on the path (0 ≤ i < |pt|). The subgoal location is chosen as pt,k where k = min( |p|/2 , lmax), where lmax is a pre-deﬁned constant.
State Description. We employ a discrete bag-of-features representation for state descriptions.2 A bag-of-feature description (ds or dg) emulates the information that the agent has extracted from a raw input that it perceives (e.g., an image, a language sentence). Concretely, we assume that when the agent sees a view, it detects the room and nearby objects. Similarly, when it receives a task request or human responses to a request, it identiﬁes information about rooms, objects, and actions in the response. Working with this discrete input representation allows us to easily simulate various types and amounts of information given to the agent.
Speciﬁcally, we model three types of input features: the name of a room, information about an object (name, distance and direction relative to a location), and the description of a navigation action (travel distance and direction). The human can supply these types of information to assist the agent. We simulate two settings of descriptions: dense and sparse. A dense description is a variablelength lists containing the following features: the current room’s name and features of at most 20 objects within ﬁve meters of a viewpoint. Sparse descriptions represent imperfect perception of the agent in real-world environments. A sparse description is derived by ﬁrst constructing a dense description and then removing the features of objects that are not in the top 100 most frequent (out of ∼1K objects). The sparse description of the current location (ds) does not contain the room name, but that of the goal location (dg) does. As each description is a sequence of feature vectors, whose length varies depend on the location, we use a Transfomer model (Vaswani et al., 2017) to be able to encode such inputs into continuous feature vectors. Details about the feature representation and the model architecture are provided in the Appendix.
Experimental Procedure. We conduct our experiments in three phases. In the pre-training phase, the agent learns an execution policy πˆ with access to only dense descriptions. This emulates training in a simulator that supplies rich information to the agent.
In the training phase, the agent is only given sparse descriptions of its current and goal locations. This mimics the degradation of the agent’s perception about the environment and the task when deployed in real-world conditions. In this phase, the agent can request dense descriptions from the human. Upon a request for information about a (goal or current) location, the human gives a dense description with room and object features of that location. However, when the agent chooses the Sub action and is adjacent to the subgoal that the human wants to direct it to, the
2While our representation of state descriptions simpliﬁes the object/room detection problem for the agent, it does not necessarily make the navigation problem easier than with image input, as images may contain information that is not captured by our representation (e.g., object shapes and colors, visualization of paths).
8

Table 5.1: Test success rates and the average number of diﬀerent types of actions taken by the agent (on all task types).

Agent

Success Rate % ↑ Avg. number of actions ↓ Unseen Unseen Unseen Start Object Env. Cur Goal Sub Do

Rule-based intention policy ψθ (when to call Done is decided by the execution policy πˆ)
(ds: current-state description, dg : goal description)

No assistance (always Do until Done) Dense dg (Goal then always Do until Done) Dense ds (always Cur then Do until Done) Dense dg and ds (Goal then always Cur then Do until Done)

43.4 16.4 67.2 56.6 77.9 30.6 97.8 81.7

Random + rules to match with # of actions of learned-RL ψθ

78.8 68.5

3.0 - - 9.7 - 1.0 4.1 12.0 - 9.4 11.0 1.0 12.7 2.0 1.0 1.7

13.1 12.6 12.0 11.0 11.3

learned-RL intention policy ψθ

With pre-trained navigation policy πˆ (ours)

85.8 78.2 19.8 2.1 1.0 1.7 11.1

With uncooperative assistant (change a request randomly to Cur, Goal or Sub) 81.1 71.2 16.1 2.7 2.7 1.5

9.6

With perfect navigation policy on sub-goals (skyline)

94.3 95.1 92.6 0.0 0.0 6.3 7.3

human simply gives a description of the next optimal navigation action to go that location. We use advantage actor-critic (Mnih et al., 2016) to learn an intention policy ψθ that determines which type of information to request. The intention policy is now trained in environment graphs that are previously seen as well as unseen during the pre-training phase.
Finally, in the evaluation phase, the agent is tested on three conditions: seen environment and target object type but starting from a new room (UnseenStr), seen environment but new target object type (UnseenObj), and unseen environment (UnseenEnv). The execution policy πˆ is ﬁxed during the training and evaluation phases. We created 82,104 examples for pre-training, 65,133 for training, and approximately 2,000 for each validation or test set. Additional details about the training procedure and dataset are included in the Appendix.
6 Results and Analyses
Settings. In our main experiments, we set: the cost of taking a Cur, Goal, Sub, or Do action to be 0.01 (we will consider other settings subsequently), the cost of calling Done to terminate the main goal (i.e. task error) equal the (unweighted) length of the shortest-path from the agent’s location to the goal, and the goal stack’s size (L) to be 2.
We compare our learned-RL intention policy with several rule-based baselines. Their descriptions are given in Table 5.1. We additionally construct a strong baseline that ﬁrst takes the Goal action (to clarify the human’s intent) and then selects actions in such a way to match the distribution of actions taken by our RL-trained agent. In particular, this policy is constrained to take at most Xa + y actions of type a, where y ∼ Bernoulli(Xa − Xa ) and Xa is a constant tuned on the validation set so that the policy has the same average count of each action as the learned-RL policy. To prevent early termination, we enforce that the rule-based policy cannot take more Done actions than Sub actions unless its Sub action’s budget is exhausted. When there is no constraint to take an action, the policy chooses a random action in the set of available actions. With this construction, our learned-RL policy can only outperform this policy by being able to determine contextually useful information to request, which is exactly the capability we wish to evaluate. We also construct a skyline where the intention policy is also learned by RL but with an execution policy that always fulﬁll subgoals perfectly.
9

unseen_str unseen_obj unseen_env

Average actions

10

5

0 CUR

DO

GOAL SUB

Action type

(a) Subgoals are requested much more in unseen environments.

q CUR

GOAL

SUB

2.0

Average actions

1.5

q

1.0

q

0.5 q q q
0.0

0.2

0.4

0.6

0.8

1.0

Fraction of episode

(b) Subgoals are requested in the middle, goal information at the beginning.

Figure 6.1: Analyzing the behavior of the learned-RL intention policy (on validation environments).

Average actions

Success rate (%)

q unseen_str

unseen_obj

q qq q q q
75

unseen_env
q q

50

25

0q

−1

−2

−3

log10(cost)

(a) Eﬀect of cost on success.

q CUR

GOAL

SUB

DO

9

6

3q

qq q

q

q

q

q

0q

−1

−2

−3

log10(cost)

(b) Eﬀect of cost on actions.

Figure 6.2: Analyzing the eﬀect of simultaneously varying the cost of the Cur, Goal, Sub, Do actions (on validation environments), thus trading oﬀ success rate versus number of actions taken.

In the problem we construct, diﬀerent types of information are useful in diﬀerent
contexts (Table 5.1). Comparing the rule-based baselines reveals the beneﬁts of making each
type of request. Overall, we observe that information about the current state is only helpful on
tasks in seen environments (UnseenStr and UnseenObj). Information about the goal greatly improves performance of the agent in unseen environments (dense-dg outperforms no-assistance by 6.4% in UnseenEnv), but information about the current state does not. Dense-dg outperforms dense-ds in UnseenObj but underperforms in UnseenStr, showing that goal information is more useful than current-state information in ﬁnding new objects but less useful in ﬁnding seen ones. The two types of information is complementary: dense-dg-and-ds improves success rate versus dense-dg and dense-ds in most evaluation conditions.
We expect subgoal information to be valuable mostly in unseen environments. This is conﬁrmed
by the superior performance of our learned-RL policy, which can request subgoals, over the rule-
based baselines, which do not have this capability, in those environments.

10

Our learned-RL policy learns the advantages of each type of information (Table 5.1 & Figure 6.1a). Aided by our learned-RL policy, the agent observes a substantial ∼2× increase in success rate on UnseenStr, ∼5× on UnseenObj, and ∼7× on UnseenEnv, compared to when performing tasks without assistance. This result indicates that the assistance-requesting skills are increasingly more helpful as the tasks become more unfamiliar. Figure 6.1a shows the request pattern of the policy in each evaluation condition. Our policy learns that it is not useful to make more than one Goal request. It relies increasingly on Cur and Sub requests in more diﬃcult conditions (UnseenObj and UnseenEnv). The policy makes about 3.5× more Sub requests in UnseenEnv than in UnseenObj. Speciﬁcally, with the capability of requesting subgoals, the agent impressively doubles the success rate of the dense-dg-and-ds policy in unseen environments, which always has access to dense descriptions in these environments. Moreover, our policy does not have to bother the human in every time step: only 1/4 of its actions are requests for information.
Our learned-RL policy selects contextually useful requests (Table 5.1, bottom half & Figure 6.1b). The policy is signiﬁcantly more eﬀective than the rule-based baseline that uses the same number of average actions per episode (+7.1% on UnseenEnv). Note that we enforce rules so that this baseline only diﬀers from our learned-RL policy in where they place the Cur and Sub requests. Thus, our policy has gained advantages by making these requests in more appropriate situations. To further showcase the importance of being able to obtain the right type of information, we use RL to train a policy with an uncooperative assistant, who disregards the agent’s request intention and instead replies to an intention randomly selected from {Cur, Goal, Sub}. As expected, performance of the agent drops in all evaluation conditions. This shows that our agent has eﬀectively leveraged the cooperative assistant.
We also observe that the agent adapts its request strategy through the course of an episode. As observed in Figure 6.1b. the Goal action, if taken, is always taken only once and immediately in the ﬁrst step. The number of Cur actions gradually decreases over time. The agent makes most Sub requests in the middle of an episode, after its has attempted but failed to accomplish the main goals. We observe similar patterns on the other two validation sets.
Finally, results obtained by the skyline shows that further improving performance of the execution policy on short-distance goals would eﬀectively enhance the agent’s performance on longdistance goals.
Eﬀects of Varying Action Cost (Figure 6.2). As mentioned, we assign the same cost to each Cur, Goal, Sub, or Do action. Figure 6.2a demonstrates the eﬀects of changing this cost on the success rate of the agent. Setting the cost equal to 0.5 makes it too costly to take any action, inducing a policy that always calls Done in the ﬁrst step and thus fails on all tasks. Overall, the success rate of the agent rises as we reduce the action cost. The increase in success rate is most visible in UnseenEnv and least visible in UnseenStr. Figure 6.2b provides more insights. As the action cost decreases, we observe a growth in the number of Sub and Do actions taken by the intention policy. Meanwhile, the numbers of Cur and Goal actions are mostly static. Since requesting subgoals is more helpful in unseen environments than in seen environments, the increase in the number of Sub actions leads the more visible boost in success rate on UnseenEnv tasks.
Recursively Requesting Subgoals of Subgoals (Table 6.1). In Table 6.1, we test the functionality of our framework with a stack size 3, allowing the agent to request subgoals of subgoals.
11

Table 6.1: Success rates and numbers of actions taken with diﬀerent stack sizes (on validation). Larger stack sizes signiﬁcantly aid success rates in unseen environments, but not in seen environments.

Stack size

Success rate (%) ↑ Average # of actions ↓ Unseen Unseen Unseen Start Obj. Env. Cur Goal Sub Do

1 (no subgoals) 92.2 78.4 12.5 5.1 1.9 0.0 10.7

2

86.9 77.6 21.6 2.1 1.0 1.7 11.2

3

83.2 78.6 33.5 1.3 1.0 5.0 8.2

As expected, success rate on UnseenEnv is boosted signiﬁcantly (+11.9% compared to using a stack of size 2). Success rate on UnseenObj is largely unchanged; we ﬁnd that the agent makes more Sub requests on those tasks (averagely 4.5 requests per episode compared to 1.0 request made when the stack size is 2), but doing so does not further enhance performance. The agent makes less Cur requests, possibly in order to oﬀset the cost of making more Sub requests, as we keep the action cost the same in these experiments. Due to this behavior, success rate on UnseenStr declines with larger stack sizes, as information about the current state is more valuable for these tasks than subgoals. These results show that the critic model overestimates the V values in states where Sub actions are taken, leading to the agent learning to request subgoals more than needed. This suggests that the our critic model is not expressive enough to encode diﬀerent stack conﬁgurations.
7 Related Work
Knowledge Transfer in Reinforcement Learning. Frameworks have been proposed to model knowledge transfer from an expert agent to a novice one (Da Silva and Costa, 2019). Torrey and Taylor (2013) introduce the action-advising framework where a learner strategically requests reference actions from a teacher. Da Silva et al. (2020) investigate uncertainty-based strategies for deciding when to request in this framework. In an agent-to-agent setting, (Da Silva et al., 2017; Zimmer et al., 2014; Omidshaﬁei et al., 2019) focus on learning a teaching policy in addition to an advice-requesting policy. An important assumption in these papers is that the teacher must share a common action space with the learner. Recent frameworks (Kim et al., 2019; Nguyen et al., 2019; Nguyen and Daum´e III, 2019) relax this assumption by allowing the teacher to specify high-level subgoals instead of low-level actions. Our framework can be viewed as a strict extension of these frameworks. It allows the human to specify not only subgoals, but also additional information about the current and goal states. Importantly, the framework enables the agent to convey various speciﬁc intentions rather just calling for generic help. Another line of work employs standard RL communication protocol, where the human transfer knowledge through numerical scores or categorical feedback (Knox and Stone, 2009; Judah et al., 2010; Peng et al., 2016; Griﬃth et al., 2013). Maclin and Shavlik (1996) propose a framework where the human advises the agent using a domain-speciﬁc language, specifying rules that can be incorporated into the agent’s model. In contrast, our framework is agnostic to the implementation of the agent’s policy model. Sumers et al. (2020) extract features from various types of language feedback to construct an RL reward function. We instead focus on deployment-time communication and directly incorporate the feedback as input

12

to the agent’s policy.
Task-Oriented Dialog and Generating Natural Language Questions. Our framework models a task-oriented dialog problem. Many variants of this problem requires the agent to compose speciﬁc questions (De Vries et al., 2017; Das et al., 2017; Thomason et al., 2020). The dominant approach in these problems is to mimic pre-collected human utterances. As discussed previously, naively mirroring human external behavior cannot enable agents to understand the limits of their knowledge. We teach the agent to understand its intrinsic needs through interaction with the human and the environment rather than through imitation of human behaviors. Another related line of work concerns generating natural language explanations of model decisions (Camburu et al., 2018; Hendricks et al., 2016; Rajani et al., 2019).
8 Conclusion
While we demonstrate this framework on a simpliﬁed navigation problem, our framework can theoretically capture richer types of human-agent communication. Hence, an important empirical question is how well our formulation generalizes to richer environments with more complex interactions and state spaces (Shridhar et al., 2020). Enhancing the sample eﬃciency of the learning policy by exploiting the hierarchical policy structure is an exciting future direction. Finally, towards generating more speciﬁc, human-like questions, methods for generating faithful explanations, (Kumar and Talukdar, 2020; Madsen et al., 2021), measuring feature importance (Zeiler and Fergus, 2014; Koh and Liang, 2017; Das and Rad, 2020), or learning the casual structure of black-box policies (Geiger et al., 2021) are relevant to investigate.
References
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1215.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1625–1634, 2018.
Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9982–9991, 2020.
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions
13

for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740–10749, 2020.
Stephanie Rosenthal, Joydeep Biswas, and Manuela M Veloso. An eﬀective personal mobile robot agent through symbiotic human-robot interaction. In AAMAS, volume 10, pages 915–922, 2010.
Stefanie Tellex, Ross Knepper, Adrian Li, Daniela Rus, and Nicholas Roy. Asking for help using inverse semantics. In Proceedings of Robotics: Science and Systems, Berkeley, USA, July 2014. doi: 10.15607/RSS.2014.X.024.
Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. URL https: //arxiv.org/abs/1812.04155.
Khanh Nguyen and Hal Daum´e III. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), November 2019. URL https://arxiv.org/abs/1909.01871.
Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning, pages 394–406. PMLR, 2020.
Dan Sperber and Deirdre Wilson. Relevance: Communication and cognition, volume 142. Citeseer, 1986.
Michael Tomasello, Malinda Carpenter, Josep Call, Tanya Behne, and Henrike Moll. Understanding and sharing intentions: The origins of cultural cognition. Behavioral and brain sciences, 28(5): 675–691, 2005.
Thom Scott-Phillips. Speaking our minds: Why human communication is diﬀerent, and how language evolved to make it special. Macmillan International Higher Education, 2014.
Igor Labutov, Sumit Basu, and Lucy Vanderwende. Deep questions without deep understanding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 889–898, 2015.
Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende. Generating natural questions about an image. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1802– 1813, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/ v1/P16-1170.
Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. Guesswhat?! visual object discovery through multi-modal dialogue. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5503–5512, 2017.
14

Sudha Rao and Hal Daum´e III. Learning to ask good questions: Ranking clariﬁcation questions using neural expected value of perfect information. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2737–2746, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/ P18-1255. URL https://aclanthology.org/P18-1255.
Bang Liu, Haojie Wei, Di Niu, Haolan Chen, and Yancheng He. Asking questions the human way: Scalable question-answer generation from text corpus. In Proceedings of The Web Conference 2020, pages 2032–2043, 2020.
W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The tamer framework. In Proceedings of the ﬁfth international conference on Knowledge capture, pages 9–16, 2009.
Kshitij Judah, Saikat Roy, Alan Fern, and Thomas Dietterich. Reinforcement learning via practice and critique advice. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 24, 2010.
Lisa Torrey and Matthew Taylor. Teaching on a budget: Agents advising agents in reinforcement learning. In Proceedings of the 2013 international conference on Autonomous agents and multiagent systems, pages 1053–1060, 2013.
Kshitij Judah, Alan P Fern, Thomas G Dietterich, and Prasad Tadepalli. Active imitation learning: Formal and practical reductions to iid learning. Journal of Machine Learning Research, 15(120): 4105–4143, 2014.
Herbert H Clark and Susan E Brennan. Grounding in communication. 1991.
Robert Stalnaker. Common ground. Linguistics and philosophy, 25(5/6):701–721, 2002.
Arthur C Graesser, Natalie Person, and John Huber. Mechanisms that generate questions. Questions and information systems, 2:167–187, 1992.
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Su¨nderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674–3683, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928–1937. PMLR, 2016.
Felipe Leno Da Silva and Anna Helena Reali Costa. A survey on transfer learning for multiagent reinforcement learning systems. Journal of Artiﬁcial Intelligence Research, 64:645–703, 2019.
15

Felipe Leno Da Silva, Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. Uncertaintyaware action advising for deep reinforcement learning agents. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 5792–5799, 2020.
Felipe Leno Da Silva, Ruben Glatt, and Anna Helena Reali Costa. Simultaneously learning and advising in multiagent reinforcement learning. In Proceedings of the 16th conference on autonomous agents and multiagent systems, pages 1100–1108, 2017.
Matthieu Zimmer, Paolo Viappiani, and Paul Weng. Teacher-student framework: a reinforcement learning approach. In AAMAS Workshop Autonomous Robots and Multirobot Systems, 2014.
Shayegan Omidshaﬁei, Dong-Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher Amato, Murray Campbell, and Jonathan P How. Learning to teach in cooperative multiagent reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 6128–6136, 2019.
Dong-Ki Kim, Miao Liu, Shayegan Omidshaﬁei, Sebastian Lopez-Cot, Matthew Riemer, Golnaz Habibi, Gerald Tesauro, Sami Mourad, Murray Campbell, and Jonathan P How. Learning hierarchical teaching policies for cooperative agents. arXiv preprint arXiv:1903.03216, 2019.
Bei Peng, James MacGlashan, Robert Loftin, Michael L Littman, David L Roberts, and Matthew E Taylor. A need for speed: Adapting agent action speed to improve task learning from nonexpert humans. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, 2016.
Shane Griﬃth, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. Georgia Institute of Technology, 2013.
Richard Maclin and Jude W Shavlik. Creating advice-taking reinforcement learners. Machine Learning, 22(1):251–281, 1996.
Theodore R Sumers, Mark K Ho, Robert D Hawkins, Karthik Narasimhan, and Thomas L Griﬃths. Learning rewards from linguistic feedback. arXiv preprint arXiv:2009.14715, 2020.
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos´e MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 326–335, 2017.
Oana-Maria Camburu, Tim Rockt¨aschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. In Proceedings of Advances in Neural Information Processing Systems, 2018.
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeﬀ Donahue, Bernt Schiele, and Trevor Darrell. Generating visual explanations. In European conference on computer vision, pages 3–19. Springer, 2016.
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.
16

Sawan Kumar and Partha Talukdar. NILE : Natural language inference with faithful natural language explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8730–8742, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.771. URL https://aclanthology.org/2020.acl-main. 771.
Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. Evaluating the faithfulness of importance measures in nlp by recursively masking allegedly important tokens and retraining. arXiv preprint arXiv:2110.08412, 2021.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818–833. Springer, 2014.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions. In International Conference on Machine Learning, pages 1885–1894. PMLR, 2017.
Arun Das and Paul Rad. Opportunities and challenges in explainable artiﬁcial intelligence (xai): A survey. arXiv preprint arXiv:2006.11371, 2020.
Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks. arXiv preprint arXiv:2112.00826, 2021.
St´ephane Ross, Geoﬀrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278–287, 1999.
Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. A recurrent vision-and-language bert for navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.
17

A Training Procedure

Cost function. The cost function in our framework is given as follows

 c(st, adt o)   γa¯t c¯(st, Gt, a¯t) = c(st, adone)   0

if a¯t = Do if a¯t ∈ {Cur, Goal, Sub}, if a¯t = Done, |Gt| = 1 if a¯t = Done, |Gt| > 1,

(A.1)

Training Algorithms. We pre-train the operation policy πˆ with DAgger (Ross et al., 2011),
minimizing the cross entropy between its action distribution with that of a shortest-path oracle
(which is a one-hot distribution with all probability concentrated on the optimal action).
We use advantage actor-critic (Mnih et al., 2016) to train the interaction policy ψθ. This method simultaneously estimates an actor policy ψθ : B¯ → ∆(A¯) and a critic function Vφ : B¯ → R. Given an execution τ¯ = (s¯1, a¯1, c¯1 · · · , s¯H ), the gradients with respect to the actor and critic are

H
∇θLactor =
t=1 H
∇φLcritic =
t=1

Vφ(¯bvt ) − Ct Vφ(¯bvt ) − Ct

∇θ log ψθ(a¯t | ¯bat ) ∇φVφ(¯bvt )

(A.2) (A.3)

where Ct =

H j=t

cj

,

¯bat

is

a

belief

state

that

summarizes

the

partial

execution

τ¯1:t

for

the

actor,

and ¯bvt is a belief state for the critic.

Cost function. The cost function introduced in § 4.4 is not eﬀective for learning the interaction policy because the task error is given only at the end of an episode. We extend the reward-shaping method proposed by Ng et al. (1999) to goal-conditioned policies, augmenting the original cost function with a shaping function Φ(s, g) with s, g ∈ S. We set Φ(s, g) to be the (unweighted) shortest-path distance from s to g. The cost received by the agent at time step t is c˜t c¯t + Φ(st+1, gt+1) − Φ(st, gt). We assume that the agent transitions to a special terminal state sterm ∈ S and remains there after it terminates execution of the main goal. We set Φ(sterm, None) = 0, where gt = None signals that the episode has ended. Hence, the cumulative cost of an execution under the new cost function is

H

H

H

c˜t = c¯t + Φ(st+1, gt+1) − Φ(st, gt) = c¯t − Φ(s1, g1)

t=1

t=1

t=1

(A.4)

Since Φ(s1, g1) does not depend on the action taken in s1, minimizing the new cumulative cost does not change the optimal policy for the task (s1, g1).

Model Architecture. We adapt the V&L BERT architecture (Hong et al., 2020) for modeling
the operation policy πˆ. Our model has two components: an encoder and a decoder; both are
implemented as Transformer models (Vaswani et al., 2017). The encoder takes as input a description dst or dgt and generates a sequence of hidden vectors. In every step, the decoder takes as input the

18

Table A.1: Dataset statistics.

Split

Number of examples

Pre-training Pre-training validation Training Validation UnseenStr Validation UnseenObj Validation UnseenEnv Test UnseenStr Test UnseenObj Test UnseenEnv

82,104 3,000 65,133 1,901 1,912 1,967 1,653 1,913 1,777

previous hidden vector bst−1, the sequence of vectors representing dst , and the sequence of vectors representing dgt . It then performs self-attention on these vectors to compute the current hidden vector bst and a probability distribution over navigation actions pt.
The interaction policy ψθ (the actor) is an LSTM-based recurrent neural network. The input of this model is the operation policy’s model outputs, bst and pt, and the embedding of the previously taken action a¯t−1. The critic model also has a similar architecture but outputs a real number (the V value) rather than an action distribution. When training the interaction policy, we always ﬁx the parameters of the operation policy. We ﬁnd it necessary to pre-train the critic before training it jointly with the actor.
Representation of State Descriptions. The representation of each object, room, or action is computed as follows. Let f name, f horz, f vert, f dist, and f type are the features of an object f , consisting of its name, horizontal angle, vertical angle, distance, and type (a type is either Object, Room, or Action; in this case, the type is Object). For simplicity, we discretize real-valued features, resulting in 12 horizontal angles (corresponding to π/6 · k, 0 ≤ k < 12), 3 vertical angles (corresponding to π/6·k, −1 ≤ k ≤ 1), and 5 distance values (we round down a real-valued distance to the nearest integer). We then lookup the embedding of each feature from an embedding table and sum all the embeddings into a single vector that represents the corresponding object. For a room, f horz, f vert f dist are zeroes. For an action, f name is either ActionStop for the stop action adone or ActionGo otherwise.
During pre-training, we randomly drop features in dst and dgt so that the operation policy is familiar with making decisions under sparse information. Concretely, we refer to all features of an object, room or action as a feature set. For dst , let M be the number objects in a description. We uniformly randomly keep m feature sets among the M + 1 feature sets of dst (the plus one is the room’s feature set), where m ∼ Uniform(min(5, M + 1), M + 1).
For dst , we have two cases. If g1 is not adjacent or equals to s1, we uniformly randomly alternate between giving a dense and a sparse description. In this case, the sparse description contains the features of the target object and the goal room’s name. Otherwise, with a probability of 1⁄3, we give either (a) a dense description (b) a (sparse) description that contains the target object’s features and the goal room’s name, or (c) a (sparse) description that describes the next ground-truth action.
We pre-train the operation policy on various path lengths (ranging from 1 to 10 graph nodes) so that it learns to accomplish both long-distance main goals and short-distance subgoals.

19

Table A.2: Hyperparameters.

Hyperparameter Name

Value

Environment
Max. subgoal distance (lmax) Max. stack size (L) Max. object distance for dst Max. object distance for dgt Max. number of objects (Mmax) Cost of taking each Cur, Goal, Sub, Do action

3 nodes 2
5 meters 3 meters
20 0.01

Operation policy πˆ Hidden size Number of hidden layers Attention dropout probability Hidden dropout probability Number of attention heads Optimizer Learning rate Batch size Number of training iterations Max. number of time steps (H)

256 2 0.1 0.1 8
Adam 10−4
32 105 15

Interaction policy ψθ Hidden size Number of hidden layers Entropy regularization weight Optimizer Learning rate Batch size Number of critic pre-training iterations Number of training iterations Max. number of time steps (H) Max. number of time steps for executing a subgoal

512 1
0.001 Adam 10−5
32 5 × 103 5 × 104
30 3× shortest distance to the subgoal

Data. Table A.1 summarizes the data splits. From a total of 72 environments provided by the Matterport3D dataset, we use 36 environments for pre-training, 18 as unseen environments for training, 7 for validation UnseenEnv, and 11 for test UnseenEnv. We use a vocabulary of size 1738, which includes object and room names, and special tokens representing the distance and direction values. The length of a navigation path ranges from 5 to 10 graph nodes.
Hyperparameters. See Table A.2.

20

