DISCRETIZATION AND RE-SYNTHESIS: AN ALTERNATIVE METHOD TO SOLVE THE COCKTAIL PARTY PROBLEM
Jing Shi1, Xuankai Chang2, Tomoki Hayashi3,5, Yen-Ju Lu2,4, Shinji Watanabe2, Bo Xu1
1Institute of Automation, Chinese Academy of Sciences (CASIA), 2Carnegie Mellon University, 3Nagoya University, 4Academia Sinica, 5Human Dataware Lab. Co., Ltd.

arXiv:2112.09382v2 [cs.SD] 9 Jan 2022

ABSTRACT
Deep learning based models have signiﬁcantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the ﬁrst time, that the synthesis-based approach can also perform well on this problem, with great ﬂexibility and strong potential. Speciﬁcally, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classiﬁcation. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be resynthesized. Evaluation results based on the WSJ0-2mix and VCTKnoisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difﬁcult to avoid in regressionbased methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.
Index Terms— Speech enhancement, speech separation, speech synthesis, cocktail party problem, deep learning
1. INTRODUCTION
With the popularity of speech-related intelligent devices and related applications, front-end processing has become a popular research topic [1]. Among them, a batch of methods based on end-to-end deep learning have emerged to solve the cocktail party problem [2]–[7]. Compared to conventional approaches like computational auditory scene analysis [8], [9] and non-negative matrix factorization [10], end-to-end models are entirely data-driven, achieving remarkable improvement in speech quality and intelligibility.
Although satisfactory results have been observed on laboratory corpus, the design patterns followed by existing prominent methods (e.g., frequency-domain and time-domain speech separation) will also introduce corresponding problems or risks, making them unable to be used reliably in real-world scenarios. Speciﬁcally, most existing methods usually build regression models to recover the ground-truth speech as much as possible, with the masking-based design and the signal-level loss criterion, e.g., the mean square error (MSE) between the spectrograms or negative scale-invariant signal-to-noise ratio (SI-SNR) between the waveforms. Normally, the essence of regression based speech separation/enhancement methods is to map the mixed/noisy speech into a continuous high-dimensional space so that the speech of different source can be better separated. However,

in a complex auditory scene, the representations of different sources in the mixture speech may be very similar in some segments or even the whole utterance, making it difﬁcult for regression-based methods to exclude the interference from other sources completely. For this reason, in our preliminary experiments and observation from others, the well-trained SOTA time-domain methods suffer in separating similar speakers’ mixed speech and some non-stationary noise.
To tackle these agonizing challenges, one possible solution is to replace the direct signal-level masking over the original input. In this work, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, converting the paradigm of the speech frontend related tasks from regression to classiﬁcation. Without loss of generality, we show the case with two speakers condition of our proposed method in Fig.1. Speciﬁcally, by utilizing the great reconstruction ability of the vector quantized variational autoencoder (VQ-VAE) [11] or HuBERT[12] + HiFi-GAN[13] model, we transfer the target speech of speech separation model into a sequence of discrete symbols. With the estimation of discrete symbol sequence, each target speech could be re-synthesized with optional transferred style.
The experimental evaluation with the WSJ0-2mix [14] and VCTK-noisy [15] corpus in various settings shows that the proposed method could steadily synthesize the separated speech of good listening quality, nearly without any interference, which is difﬁcult to avoid in masking based methods. In addition, with almost no impact on perception and cognition evaluation, the speaker conversion of separated speech could be easily realized through our model.
The core contribution of this work is to articulate an alternative direction, besides the dominating masking-based regression paradigm, with synthesis after discretization for speech enhancement/separation task. We also demonstrate the beneﬁt it gets, along with the difﬁculty it faces with current mainstream evaluation metrics. Our proposed method presents a promising avenue for exploring solutions to the problem. The sound demo for this paper is available at https://shincling.github.io/discreteSeparation/.
2. METHODOLOGY
2.1. Background Methods
2.1.1. Discretization of Speech
Similar to the low-bitrate speech codecs [16]–[19], speech discretization aims at encode the speech input into a discrete sequence, shown in Fig.1(c). Ideally, we want to encode as much information as possible so that the original speech can be restored. In this paper, we use two methods to gets the discrete units from raw speech: VQVAE [11] and HuBERT [12].
VQ-VAE model can convert a sequential input with an arbitrary length into a downsampled sequence of discrete symbols, and precisely reconstruct the input from the downsampled discrete symbol sequence [11] in a uniﬁed structure. HuBERT is a self-supervised

Mixture wave

Speech separation
model

(a) conventional speech separation

Pseudo multi-spk ASR model
Mixture wave
(b) discrete speech separation

Speaker 1
Speaker 2
Discrete sequence 1
Discrete sequence 2

X
Speaker 1

Pre-trained

Z

Discretization

Model

Y Unit y1 Unit y2 Unit y3

Unit yN

Target discrete sequence 1

(c) speech discretization

Discrete vocoder
Shared
Discrete vocoder

Speaker 1 Speaker 2

Fig. 1. Illustration of conventional speech separation model and our proposed discretization-synthesis model.

learning based model trained with masked continuous audio signals. It shows superior performance across multiple tasks such as ASR, spoken language modeling, and speech synthesis [20]. For the HuBERT model, we use the k-means algorithm to cluster the extracted latent representations into the clustering center while the VQ-VAE directly use the nearest centroid in VQ-codebook.

2.1.2. Discrete Vocoder
As illustrated in Fig.1(b), the synthesis of the speech from discrete symbols can be formed as the traditional text-to-speech (TTS) models, which usually produce Mel-spectrogram autoregressively given textual features as input. Next, a vocoder is applied to reconstruct the phase from the Mel-spectrogram. In this study, we propose to use the learned discrete speech units as an input to a vocoder module with no spectrogram estimation. The discrete symbols are extracted with the pre-trained self-supervised models, e.g., VQ-VAE or HuBERT model, and we use MelGAN [21] and HiFi-GAN [13] as architecture for the vocoder module accordingly. It is worth mentioning that, to synthesize the original speech as much as possible with various speakers, the speaker identities information can be utilized as the condition in Vocoder.

2.2. Proposed Discrete Separation

Our proposed method shown in Fig.1(b) can be divided into two modules: discrete sequence prediction and speech re-synthesis1. As-
sume the mixed/noisy signal is M, and the target clean signal to be Xk, where k is the index of the target speaker.

Discrete sequence prediction Before training, as illustrated in

Fig.1(c), we ﬁrst build the tool that can discretize continuous

waveform X into the sequence of the discrete symbols Y =

{y1, ..., yn, ..., yN }, i.e., centroid IDs in the VQ-codebook or Hu-

BERT clustering . As mentioned in Sec.2.1.1, we use either our

own-trained VQ-VAE encoder or the pre-trained HuBERT model to

sever as the discretizer here. This process could be formulated as

follows:

Z = {z1, ..., zn, ..., zN } = F (X),

(1)

yn = Q(zn) = i = argmin zn − ej ,

(2)

j

where F is the feature extraction function (VQ-VAE encoder or HuBERT), Q the discretization function, ej the j-th centroid in codebook or clustering, and N the total length of discrete sequence Y,

1Without loss of generality, we set the output number of sources to 2 in this ﬁgure, which can be single or more speakers.

which is down-sampled from the original length T. To summarize, this here acts as: X −→ Z −→ Y. With the discretizer, in the training phase, the ground-truth speech X of each speaker could be transferred into a target discrete sequence Y.
With these discrete symbols Y for each speaker as the target rather than the original continuous clean speech X, the speech separation/enhancement task could be converted into the task of multi/single-speaker speech recognition, which we use the pseudo ASR module in Fig. 1(b) to conduct. For the module of pseudo ASR, taking the input M from the noisy/mixed speech, this module acts to estimate the posterior probability of the discrete sequence as follows:
N
p(Yˆ |M) = p(Yˆ n|M) where Yˆ = {yˆ1, ..., yˆN }. (3)
n=1
Thanks to the discretizer’s convolutional architecture, the alignment between the target sequence Y and the corresponding mixture/noisy speech M is guaranteed. Based on this, we adopt the pseudo ASR module which performs as the frame-by-frame processing similar to hybrid HMM/DNN ASR framework. To be specifc, for pseudo ASR module, our framework consisting of Encoder, Separator2 and Classiﬁer. On the contrast to the conventional mainstream speech separation framework with Encoder (STFT/Conv Encoder), Separator and Decoder (iSTFT/Conv-Decoder), our proposed framework replaces the Decoder with Classiﬁer. This is because we do not need to reconstruct the signal at this module, but to predict the discrete symbols. In addition, as mentioned before, the speaker information can also be used in Vocoder to reconstruct the speech. Based on that, the speaker prediction is also implemented together within this module by averaging the latent features after the Separator.
Speech re-synthesis After the prediction of discrete symbols (also the optional speaker), the discrete vocoder described in Sec.2.1.2 is used to synthesize the ﬁnal predicted speech. In practice, the vocoder could be highly ﬂexible with different training settings. For instance, if we use the homologous data used in the separation part, the vocoder could reconstruct speech as closely as possible to the original data set. By contrast, the vocoder could also be trained with heterogeneous data given the uniﬁed discrete symbols, e.g., from the HuBERT, resulting in the re-synthetic speech with the transferred style of the heterogeneous data. In our experiments, we implemented both the multi-speaker homologous vocoder and a singlespeaker heterogeneous vocoder to valid the speciality of speakertransfer. Besides this simple setting, more styles or types of reconstruction could be explored further, e.g., accent-transfer, tonetransfer, emotion-transfer and so on, which we defer to future work.
The detailed procedure of our whole proposed framework could been seen in Procedure 1. 2-stage reﬁning As we can imagine and through our practice, even though it is almost indistinguishable from hearing, synthesis-based method gets difﬁculties to accurately recover the original speech at the signal level. To show that our reconstructed speech already contains useful information of the target speech as we hear, here we propose a 2-stage reﬁning strategy. In brief, we use the estimated speech Xˆ as the condition (target speech) to further trained a separation (target-speech extraction) model. With the concatenation of mixture signal M and Xˆ as input, we expect the 2-stage model can reﬁne the estimated speech towards the ground-truth signal X.
3. RELATED WORK
Our method shares some similarities with exemplar-based speech enhancement [22]–[24], which often uses linear combination of the
2To keep it simple, we use the term Separator herein for both speech separation and speech enhancement task.

trained speech/noisy combined dictionaries to reconstruct the raw waveform and then to perform enhancement. To some extent, our discretized sequence can be regarded as an extreme sparse case of linear coefﬁcients, and the Vocoder is much more complex with a GAN-based neural networks rather than the simple summation of the spectrograms or waveforms. Recently, the discrete units based methods have been applied for speech synthesis [25], [26] and speech translation [27]. Although with a similar discretization and synthesis pipeline, our work focuses more on the extraction of speech-like audios given the complex noisy/mixed speech, rather than the reconstruction or speaker conversion of the clean speech. In particular, we ﬁrst came up with the multi-source re-synthesis from one single speech (fully-overlapped speech separation task), which is quite different with above works.
4. EXPERIMENTS
Our experiments include two settings: speech separation on WSJ02mix corpus and speech enhancement on VCTK-noisy corpus. Both subjective evaluation and objective evaluation are conducted to evaluate the proposed methods.
4.1. Implementation details
4.1.1. Pseudo ASR module
Based on the similarity with closely related speech separation task, two types of architectures have been investigated. First, the transformer-based ASR module, similar with the multi-speaker ASR task and the architecture proposed by [28] with FBank base feature extraction and multiple transformer encoder as our Separator. Differently, for the prediction of discrete sequence, thanks to the alignment of speech and discrete units, the transformer-based autoregressive decoder is not used, but we just use the encoded feature to predict the probability distribution at each frame with the common cross-entropy loss function. Second, the conventional module of speech separation could be directly used in our Encoder and Separator models. In our experiments, we adopt the same architecture of the DPRNN model [6] to better compare our proposed method and existing ones. Moreover, for the speech separation task, utterancelevel PIT [2] is used to determine the best permutation of the output streams predicted by the Separator+Classiﬁer .
In the following section, after trying and comparing different models towards different tasks (separation/enhancement), we choose transformer-based model in speech enhancement experiments with single-speaker and Conv-DPRNN based 2-speaker model in speech separation experiments, which performs better in each task. The details about the architecture could be been in Appendix A

4.1.2. Discrete Vocoder As mentioned before, two types of Vocoder setting is tried. In preliminary experiments, VQ-VAE was used with the same architecture proposed by [25], and got reasonable results in our task. Then, inspired by the impressive performance with HuBERT model [20] in various speech tasks and the great speech quality synthetized with HiFi-GAN vocoder, we use the combination of HuBERT&HiFiGAN architecture in the following experiments. To be speciﬁc, we use the HuBERT Large model trained on Libri-Light [29] 60k hour without any downstream ﬁnetuning to extract the features. Then, we trained a kMeans model with 100 clusters, which will be used to

Table 1. Training settings for the discrete vocoder.

Vocoder index

Setting name

Vocoder Architecture

Training Set

Num of Downsample

Speakers

rate

1

VQ-VAE

MelGAN WSJ0 (si tr s) 8k

101

64

2

WSJ0 (si tr s) 8k

101

160

3

VCTK∗ 16k

90

160

4 Hubert + HiFi-GAN HiFi-GAN LJSpeech 8k 1 160

∗We use the subset of VCTK corpus, with 90 speakers in total.

Table 2. The overall objective evaluation in separation performance

with our proposed method and some baseline models in WSJ0-2mix

validation and test set.

Model

Discrete Vocoder

STOI

Separation Performance (cv/tt)

SAR

SDR

SIR

MOSNet

(1) GT wave (2) Baseline: Conv-TasNet[31] (3) Baseline: Conv-DPRNN[6]
(4) GT HuBERT discrete symbols

x
x
x
Multi-spk WSJ0 2 Single-spk LJspeech 4

0.937/0.938

15.45/14.75

14.68/13.78

24.35/23.15

3.129/3.216 2.917/3.019

0.950/0.960 17.41/17.09 16.83/16.46 27.43/26.90 2.953/3.047

0.80/N.A -10.53/N.A -11.19/N.A 9.19/N.A 2.936/N.A

0.62/0.62 -13.68/-13.52 -14.94/-14.79 5.79/5.78 2.982/2.977

(5) Our-base (Conv-DPRNN)
+ 2stage Conv-TasNet
+ 2stage Conv-DPRNN

Multi-spk WSJ0 2 Single-spk LJspeech 4 Multi-spk WSJ0 2 Multi-spk WSJ0 2

0.70/0.67 -11.95/-12.44 -12.94/-13.60 7.81/6.41 2.937/2.976 0.58/0.58 -13.67/-13.53 -15.08/-14.97 5.23/5.16 2.976/2.978 0.944/0.946 16.77/15.97 16.17/15.31 26.80/25.76 3.021/3.119 0.948/0.959 17.59/17.13 16.98/16.48 27.69/27.12 3.009/3.092

Table 3. Separation MOS with different gender combinations for 2-speaker’s mixtures from test set of WSJ0-2mix.

Methods Ground-Truth

Gender Combination with 2 spkrs

F+M

M+M

F+F

4.33 ± 0.13 4.46 ± 0.12 4.27 ± 0.13

Conv-TasNet[31] Conv-DPRNN[6] Our-base (Conv-DPRNN)

3.45 ± 0.17 3.85 ± 0.14 4.12 ± 0.13

3.37 ± 0.17 3.58 ± 0.16 4.22 ± 0.12

3.27 ± 0.18 3.53 ± 0.16 4.10 ± 0.14

generate the discrete symbols for the corresponding data. The details of different settings in Vocoder is shown in Table 1. It is worthy mentioning that although the default pre-trained HuBERT model is only used for the speech with sampling rate of 16kHz, the corresponding extracted discrete symbols Y could be successfully used to train the vocoder with different sampling rate for the output waveform. From the observation of our trained vocoder, the reconstructed speech still gets very impressive listening quality. All the training speakers in Vocoder has no-overlap with the speakers in test for all experiments.
4.2. Evaluation Metrics
Due to the essence of synthesis-based model, the proposed model is hard to compete against the regression-based methods in the metrics of the signal-level similarity, e.g., SDR, SNR, SI-SNR, STOI and so on. This is because these above metrics are all designed to evaluate how accurate the reconstruction of the original sound signal is, while our synthesized/reconstructed speech is more like the perceptual approximation of human hearing. Based on these facts, in the following sections, along with the conventional objective evaluation metrics for speech enhancement/separation, we also conduct the subjective evaluation results. Through the evaluation of both sides, we hope to better analyze the advantages and difﬁculty of the synthesis methods we propose. Besides the STOI, SAR, SDR and SIR, we use an additional open-source MOSNet [30] results to provide some rough simulation of subjective evaluation about the speech quality. More details about the deﬁnition of various metrics used in the following sections could be seen in Appendix C.
4.3. Speech separation results
For speech separation experiments, we use the WSJ0-2mix corpus, sampled at 8 kHz with the max-length mode, by which we want to maintain the semantic integrity of each target speech.
The overall objective evaluation is listed in the Table 2. The oracle and baseline models are listed as: (1) ground-truth reference waveform; (2) the Conv-TasNet [31]; (3) the Conv-DPRNN [6]; (4) the results with ground-truth Hubert discrete symbols with our trained Vocoder.

Table 4. The overlap-ratio of different methods with two thresholds.

The lower the threshold, the more sensitive it is to detect overlaps.

Threshold=0.3

valid set (Threshold = 0.3 / 0.5)

test set (Threshold = 0.3 / 0.5)

Model

Speech length(s)

Overlap length (s)

Ratio(%)

Speech length(s)

Overlap length (s)

Ratio(%)

Conv-TasNet Conv-DPRNN Our-base

7.69

0.369 / 0.233 4.80 / 3.04

7.46

0.4441 / 0.2566

5.95 / 3.44

7.61

0.175 / 0.115 2.30 / 1.51

7.35

0.2405 / 0.1594

3.27 / 2.17

6.75 0.0404 / 0.0102 0.598 / 0.150 6.60 0.04547 / 0.01067 0.689 / 0.162

For our models, we set the base model (5) of our proposed methods with the same architecture of Conv-DPRNN [6] in ASR module, while replacing the Conv-Decoder to classiﬁer as mentioned in the Sec.4.1.1. In all our experiments trained for WSJ02mix, we found the time-domain based separation models, e.g., Conv-TasNet, DPRNN, Dual-path Transformer, perform better than the transformer-based multi-spk ASR model by an obvious margin. Moreover, the DPRNN performs more stable than the others two models. Based on these results, we use the Conv-DPRNN model as our base model in the separation experiments.
Unsurprisingly, for the signal-level separation metrics, our synthesis based models shows quite poor performance. We consider there are several reasons: (1) The speech reconstructed by the vocoder, trained with the GAN-based loss criteria, is not fully designed to reconstruct the original waveform but to make it difﬁcult to distinguish with the distribution of real speech; (2) the Vocoder is performed with the condition of predicted known speaker in 2 and the one external speaker in 4 from the LJSpeech. These outputted speech with this setting actually could be seen as the audio after speaker conversion, which will inevitably change the original signal; (3) The reconstructed speech gets frame-shift after the vocoder. After the 2-stage reﬁning, the better results compared with the same baseline separation model ((2) and (3)) indicate that the reconstructed speech Xˆ from our base model could provide informative clue in signal-level, which is consistent with our hearing’s feeling.
Although the separation metrics from the base model is poor, from the generated samples with either the oracle HuBERT symbols or our predicted HuBERT symbols, we found the speech is quite audible and reasonable. In particular, the re-synthesize speech contains almost no interference, no matter how close the two speakers are in the mixture, which is difﬁcult for the masking-based models to tackle. To better evaluate our discretization-synthesis methods, then we use the subjective evaluation of separation mean opinion score(sMOS), through which we want to show the human judge about the separation degree. In detail, we randomly sampled 20 mixture samples from WSJ0-2mix test set with each gender combinations: 2 females, 2 males and 1 male 1 female. The number of subjects is 23, and that of evaluation samples per each subject is 120 (= 10 samples (from 20 in total) × 3 gender combination × 4 models). Each subject rated the amount of interference of each sample on a 5-point scale: 5 for completely no interference, 4 for almost not interference, 3 for a small amount of interference could be head, 2 for obvious interference could be heard, and 1 for too much interference. We instructed subjects to work in a quiet room and use headphones. We used the WebMUSHRA to build the online tools. The ﬁnal results of sMOS are listed in Table 3. From the separation MOS, we could ﬁnd that our proposed method get obvious advantage. Moreover, our method shows a similar tendency with the Ground-truth results, which indicates the stability across the different gender combination, while baselines are all suffered with the same-gender setting compared with the different gender.
To further evaluate the purity of our generated samples, we use the overlapped speech length ratio calculated by the Pyannote Overlapped detection tool [32]. Lower overlap-ratio means less part of

the predicted speech gets over one speaker, indicating the more clean speech. The results shown in Table 4 on all the samples on testset of WSJ0-2mix is also quite promising with nearly an order of magnitude lower than the baselines. For better demonstration of our proposed system, we recommend the readers to visit more samples via: https://shincling.github.io/discreteSeparation/.
4.4. Speech enhancement results
Next, we carried out the speech enhancement experiment on the VCTK noisy corpus [15], trained with HuBERT discretizer sing the training set (tr 26spk). To predict the discrete symbols on the noisy data, we build our single-speaker pseudo ASR model with a 12-layer Transformer Encoder using similar architecture as in end-to-end ASR models [28]. The HiFi-GAN vocoder used was trained on partial VCTK data. We ﬁrst show the objective evaluation metrics in Table 5. Then we show the noise mean opinion scores (nMOS) collected from 21 testees in Table 6. At last, we show the speech recognition performance of synthesized audios in Table 7. In consistent with separation task, the objective evaluation of our synthesized audio is relatively low. However, the noise MOS are promising, even close to the clean ground truth audios. In terms of intelligibility, We trained an ASR model using the clean VCTK data. The WER of synthesized audio from predicted discrete symbol sequences are better than the original noisy audio after ﬁnetuning ASR model on the generated speech in training set. Please refer to the Appendix C for the deﬁnition of the used metrics.

Table 5. The objective evaluation performance on the cv 2spk & tt 2spk sets of VCTK noisy corpus. Four metrics are used: PESQ, scale of signal distortion (SIG), scale of background noise (BAK) and overall effect using scale of MOS (OVL).

Model

PESQ

SIG

BAK

OVL

Conv-TasNet [31] MetricGAN [33] MTL-MIMIC [34]

1.57 / 2.84 2.79 / 3.13 2.49 / 2.87

2.56 / 2.33 3.42 / 4.02 3.06 / 3.66

2.08 / 2.62 2.81 / 3.16 3.09 / 3.42

2.01 / 2.51 3.07 / 3.57 2.76 / 3.26

Proposed

1.26 / 1.21 2.81 / 2.70 1.71 / 1.62 1.97 / 1.87

Table 6. Noise MOS for vctk noisy corpus.

Model

MOS

Clean GT Noisy GT

4.03 ± 0.13 2.65 ± 0.14

Conv-TasNet[31] MetricGAN[33] MTL-MIMIC[34]

2.49 ± 0.14 3.30 ± 0.14 3.39 ± 0.15

Proposed (VCTK vocoder 3 ) 4.02 ± 0.14

Table 7. Performance of speech recognition using VCTK-ASR model before / after ﬁnetuning on generated speech.

ASR w/o ASR w/ ﬁnetuning ﬁnetuning

Clean

5.8

1.4

Noisy

19.5

14.6

GT-symbols

14.9

8.5

Predicted

22.4

13.0

5. CONCLUSION AND FUTURE WORK
In this work, we have proposed a discretization-resynthesis framework for speech enhancement and speech separation. With the predicted discrete sequence for each target speaker, we convert the existing regression-based speech separation/enhancement paradigm into a classiﬁcation-based one. Evaluation results on the WSJ0-2mix and VCTK-noisy corpus in various settings show that the proposed method can steadily synthesize the separated speech of good listening quality, nearly without any interference, which is difﬁcult to avoid in regression based methods. On the other hand, the big performance gap between the subjective and objective evaluation indicates the disadvantages of our method in the current signal-level evaluation system, which we defer to future work. We believe our proposed method can inspire another feasible avenue for exploring solutions to the cocktail party problem.

6. REFERENCES
[1] R. Haeb-Umbach, S. Watanabe, T. Nakatani, et al., “Speech processing for digital home assistants: Combining signal processing with deep-learning techniques,” IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 111–124, 2019.
[2] M. Kolbaek, D. Yu, Z. H. Tan, and J. Jensen, “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 10, pp. 1901–1913, 2017.
[3] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31–35.
[4] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. ICASSP, 2017, pp. 241–245.
[5] Y. Luo and N. Mesgarani, “Tasnet: Time-domain audio separation network for real-time, single-channel speech separation,” in Proc. ICASSP, 2018, pp. 696–700.
[6] Y. Luo, Z. Chen, and T. Yoshioka, “Dual-path rnn: Efﬁcient long sequence modeling for time-domain single-channel speech separation,” in Proc. ICASSP, 2020, pp. 46–50.
[7] A. S. Subramanian, X. Wang, M. K. Baskar, et al., “Speech enhancement using end-to-end speech recognition objectives,” in Proc. WASPAA, 2019, pp. 234–238.
[8] A. Bregman, Auditory scene analysis: The perceptual organization of sound, 1990.
[9] A. S. Bregman, Auditory scene analysis: the perceptual organization of sound. 1994, pp. 79–80.
[10] M. N. Schmidt and R. K. Olsson, “Single-channel speech separation using sparse non-negative matrix factorization,” in Proc. INTERSPEECH, 2006.
[11] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, “Neural discrete representation learning,” in Proc. NeurIPS, 2017, pp. 6309–6318.
[12] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, et al., “Hubert: Selfsupervised speech representation learning by masked prediction of hidden units,” arXiv preprint arXiv:2106.07447, 2021.
[13] J. Kong, J. Kim, and J. Bae, “Hiﬁ-gan: Generative adversarial networks for efﬁcient and high ﬁdelity speech synthesis,” in Proc. NeurIPS, 2020.
[14] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-channel multi-speaker separation using deep clustering,” in Proc. INTERSPEECH, 2016.
[15] C. Valentini-Botinhao, “Noisy speech database for training speech enhancement algorithms and tts models,” 2017.
[16] B. S. Atal and S. L. Hanauer, “Speech analysis and synthesis by linear prediction of the speech wave,” The journal of the acoustical society of America, vol. 50, no. 2B, pp. 637–655, 1971.
[17] A. McCree, K. Truong, E. B. George, T. P. Barnwell, and V. Viswanathan, “A 2.4 kbit/s melp coder candidate for the new us federal standard,” in Proc. ICASSP, vol. 1, 1996, pp. 200– 203.

[18] W. B. Kleijn, F. S. Lim, A. Luebs, et al., “Wavenet based low rate speech coding,” in Proc. ICASSP, 2018, pp. 676–680.
[19] F. S. Lim, W. B. Kleijn, M. Chinen, and J. Skoglund, “Robust low rate speech coding based on cloned networks and wavenet,” in Proc. ICASSP, 2020, pp. 6769–6773.
[20] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, et al., “Superb: Speech processing universal performance benchmark,” arXiv preprint arXiv:2105.01051, 2021.
[21] K. Kumar, R. Kumar, T. de Boissiere, et al., “MelGAN: Generative adversarial networks for conditional waveform synthesis,” in Proc. NeurIPS, 2019, pp. 14 881–14 892.
[22] D. Baby, J. F. Gemmeke, T. Virtanen, et al., “Exemplar-based speech enhancement for deep neural network based automatic speech recognition,” in Proc. ICASSP, 2015, pp. 4485–4489.
[23] D. Baby, T. Virtanen, J. F. Gemmeke, et al., “Coupled dictionaries for exemplar-based speech enhancement and automatic speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 11, pp. 1788–1799, 2015.
[24] D. Baby, T. Virtanen, T. Barker, et al., “Coupled dictionary training for exemplar-based speech enhancement,” in Proc. ICASSP, 2014, pp. 2883–2887.
[25] T. Hayashi and S. Watanabe, “Discretalk: Text-to-speech as a machine translation problem,” arXiv preprint arXiv:2005.05525, 2020.
[26] A. Polyak, Y. Adi, J. Copet, et al., “Speech resynthesis from discrete disentangled self-supervised representations,” arXiv preprint arXiv:2104.00355, 2021.
[27] A. Lee, P.-J. Chen, C. Wang, et al., “Direct speech-to-speech translation with discrete units,” arXiv preprint arXiv:2107.05604, 2021.
[28] S. Karita, N. Chen, T. Hayashi, et al., “A comparative study on transformer vs rnn in speech applications,” in Proc. ASRU, 2019, pp. 449–456.
[29] J. Kahn, M. Rivie`re, W. Zheng, et al., “Libri-light: A benchmark for asr with limited or no supervision,” in Proc. ICASSP, 2020, pp. 7669–7673.
[30] C.-C. Lo, S.-W. Fu, W.-C. Huang, et al., “Mosnet: Deep learning based objective assessment for voice conversion,” in Proc. Interspeech, 2019.
[31] Y. Luo and N. Mesgarani, “Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 1256–1266, 2019.
[32] H. Bredin, R. Yin, J. M. Coria, et al., “Pyannote. audio: Neural building blocks for speaker diarization,” in Proc. ICASSP, 2020, pp. 7124–7128.
[33] S.-W. Fu, C. Yu, T.-A. Hsieh, et al., “Metricgan+: An improved version of metricgan for speech enhancement,” arXiv preprint arXiv:2104.03538, 2021.
[34] D. Bagchi, P. Plantinga, A. Stiff, and E. Fosler-Lussier, “Spectral feature mapping with mimic loss for robust speech recognition,” in Proc. ICASSP, 2018.

Utterance-level Classifier

Fbank Mixture wave

Ouput Stream 1
Output Stream 2

Frame-level Classifier
Shared
Frame-level Classifier

self-attention encoder block
(a) Transformer-based multi-spk ASR module

Utterance-level Classifier

Predcted Spk 1
Predcted Discrete sequence 1
Predicted Discrete sequence 2
Predcted Spk 2

ConvEncoder

ConvEncoder

Mixture wave Dual-Path RNN block
(b) ConvDPRNN-based multi-spk ASR module

Utterance-level Classifier
Output Stream 1 Frame-level
Classifier
Shared
Frame-level Output Classifier Stream 2
Utterance-level Classifier

Predcted Spk 1
Predcted Discrete sequence 1
Predicted Discrete sequence 2
Predcted Spk 2

Fig. 2. Illustration of two different pseudo-ASR models we used: (a) transformer-based model, (b) ConvDPRNN-based model. Each model could be seen as a speciﬁc implementation of the pseudo-ASR module (with color of blue) in Figure 1(b).

A. THE DETAIL OF PSEUDO-ASR MODULE

B. COMPARISON RESULTS FOR VQ-VAE

As described in Sec 4.1.1, there are two types of ASR model we used in our experiments: (1) transformer-based models and (2) ConvDPRNN based models, which is elaborated in the Figure 2. We address here again that the number of output streams could be set as 1 or a number larger than 2, which should be identical with the number of speakers in given speech input (speech separation of 2 speakers or speech enhancement of 1 speaker).
In detail, for the transformer-based models, we use the similar architecture proposed by [28] with FBank as the base feature extraction and multiple transformer encoder following that to output single or multiple streams. After that, for each output stream, a shared frame-level classiﬁer is adopted to predict the discrete sequence at each frame. We use the transformer-based model for the experiments in speech enhancement in Sec 4.4. At each block of the Transformer encoder, the network has 4 attention heads and each has dimensionalities dattn = 64. The dimensionality of the feedforward layer is dff = 2048. A two-layer CNN with downsampling rate of 2 is used to make the time resolution consistent with the discrete symbols.
Similarly, the conventional speech separation model, e.g. ConvTasNet, Conv-DPRNN, could be directly utilized as the feature extraction and separator part. In speciﬁc, for the results in speech separation experiments in Sec 4.3, we use the 1-D convolution with 1,024 channels, 512 kernel size and the frame-shift stride of 20ms (which is consistent with the HuBERT discretization setting) to extract the base features. The same architecture of Dual-path RNN block proposed by [6] is used as the separator with 6 layers, 256 units and 4 segment size .

As described before, we also implemented proposed discretizationsynthesis framework with the VQ-VAE model with the MelGAN vocoder. The results compared with the baseline models and HuBERT+Hiﬁ-GAN are shown in Table 8. Interestingly, we ﬁnd that the objective metrics with VQ-VAE + Mel-GAN are better than the HuBERT model + HiFiGAN vocoder, which is opposite with our subjective listening. In particular, there is a signiﬁcant gap in the ability of speaker conversion with the VQ-VAE + MelGAN. We infer that, the lower downsampling rate (64 vs 160) with the VQ-VAE setting causes less frame shift in the generated speech Xˆ compared with the ground-truth X, which is quite important for the evaluation of used objective measures.

C. EXPLANATION OF SUBJECTIVE AND OBJECTIVE
METRICS

C.1. Objective measures

For the evaluation of speech separation part mentioned in Sec.4.3 and Tabel 2, we used objective metrics, e.g., SAR, SDR, SIR, STOI, to evaluate the signal-level similarity of the predicted speech compared with the ground-truth clean signal. To be speciﬁc, the sources-toartifacts ratio

||starget + einterf + enoise||2

SAR := 10 log10

||eartif ||2

, (4)

Table 8. The objective evaluation in separation performance with our proposed method and some baseline models in WSJ0-2mix validation

and test set.

Model

Discrete Vocoder

STOI

Separation Performance (cv/tt)

SAR

SDR

SIR

MOSNet

(1) GT wave

x

-

-

-

-

3.129/3.216

(3) Baseline:

x

0.950/0.960 17.41/17.09 16.83/16.46 27.43/26.90 2.953/3.047

Conv-DPRNN[6]

(5) Our-base (Conv-DPRNN)

HuBERT + Hiﬁ-GAN Multi-spk WSJ0 2 VQ-VAE + MelGAN Multi-spk WSJ0 1

0.70/0.67 0.82/0.82

-11.95/-12.44 -12.94/-13.60 7.81/6.41 2.937/2.976

0.16/-0.95

-0.06/-1.23 18.65/17.22 2.873/2.924

the source-to-distortion ratio

||starget||2

SDR := 10 log10 ||einterf + enoise + eartif ||2 ,

(5)

the source-to-interferences ratio

||starget||2

SIR := 10 log10 ||einterf ||2 ,

(6)

we recommend the readers to refer to the original deﬁnition proposed by Emmanuel Vincent et al. In addition, STOI (the short-time objective intelligibility) refers one objective intelligibility measure, which shows high correlation with the intelligibility of both noisy, and TFweighted noisy speech.
For the speech enhancement related measures mentioned in Sec.4.4 and Tabel 5: PESQ , SIG, BAK and OVL, we use the provided tool from the publisher website https://www.crcpress. com/downloads/K14513/K14513_CD_Files.zip to evaluate the results.
• PESQ: Perceptual evaluation of speech quality, using the wide-band version recommended in ITU-T P.862.2 (from –0.5 to 4.5).
• SIG: the signal distortion attending only to the speech signal(from 1 to 5).
• BAK: the intrusiveness of background noise (from 1 to 5).
• OVL: MOS prediction of the overall effect (from 1 to 5).

C.2. Subjective measures
Besides the objective measures to evaluate how much the original signal is recovered. We also use some subjective measures to test the human feeling about the quality of the given speech. In speech separation, we use the estimator (MOSNet in Table 2 provided by [30], which basically is ﬁtted to the human 5-scale mean opinion score (MOS) of the given speech.
For the real human evaluation, we used the separation MOS deﬁned by us in section 4.3. For the noise MOS recently proposed by community, we follow the 5-scale setting that the rater will only attend to the background. The categories of background in this sample are 5-Not Noticeable / 4-Slightly Noticeable / 3-Noticeable But Not Intrusive / 2-Somewhat Intrusive / 1-Very Intrusive.

D. PROCEDURE OF THE FRAMEWORK
The detail procedure of our discretization and re-synthesis could be formulated as follows:

Procedure 1: The formulation of our discretization and re-
synthesis framework.
Input: mixed/noisy speech M, oracle number of speaker K, ground-truth speech set {Xk} ∈ M ;
Pre-trained: discretization model, e.g., HuBERT, to convert Xk into a
sequence of symbols Yk; Vocoder model to recover the Xk from the character-level
sequence Yk;
Training:
train the pseudo-ASR model to predict all K target discrete sequences {Yk} and the estimated speaker for each
sequence with uPIT[2] strategy;
Test:
(1) For the given input M, ﬁrst use the pseudo-ASR model to predict the K discrete sequences {Yˆ k} and the
estimated speaker k for each; (2) Use the vocoder to synthesize the speech stream Xˆ k of
each Yˆ k on the condition of estimated speaker identity; (3) Evaluate the generated speech set {Xˆ k} with the
ground-truth {Xk}

