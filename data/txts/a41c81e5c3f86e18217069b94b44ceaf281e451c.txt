As-You-Go Deployment of a Wireless Network with On-Line Measurements and Backtracking

Arpan Chattopadhyay∗, Marceau Coupechoux†, and Anurag Kumar∗

∗Dept. of ECE, Indian Institute of Science Bangalore 560012, India
arpanc.ju@gmail.com, anurag@ece.iisc.ernet.in

†Telecom ParisTech and CNRS LTCI Dept. Informatique et Re´seaux
23, avenue d’Italie, 75013 Paris, France marceau.coupechoux@telecom-paristech.fr

arXiv:1308.0686v2 [cs.NI] 22 Aug 2013

Abstract—We are motivated by the need, in some applications, for impromptu or as-you-go deployment of wireless sensor networks. A person walks along a line, making link quality measurements with the previous relay at equally spaced locations, and deploys relays at some of these locations, so as to connect a sensor placed on the line with a sink at the start of the line. In this paper, we extend our earlier work on the problem (see [1]) to incorporate two new aspects: (i) inclusion of path outage in the deployment objective, and (ii) permitting the deployment agent to make measurements over several consecutive steps before selecting a placement location among them (which we call backtracking). We consider a light trafﬁc regime, and formulate the problem as a Markov decision process. Placement algorithms are obtained for two cases: (i) the distance to the source is geometrically distributed with known mean, and (ii) the average cost per step case. We motivate the per-step cost function in terms of several known forwarding protocols for sleep-wake cycling wireless sensor networks. We obtain the structures of the optimal policies for the various formulations, and provide some sensitivity results about the policies and the optimal values. We then provide a numerical study of the algorithms, thus providing insights into the advantage of backtracking, and a comparison with simple heuristic placement policies.
I. INTRODUCTION
Wireless interconnection of resource-constrained mobile user devices or wireless sensors to the wireline infrastructure via relay nodes is an important requirement, since a direct one-hop link from the source node to the infrastructure “basestation” may not always be feasible, due to distance or poor channel condition. Such wireless interconnection of sensors with the wireline infrastructure is usually performed by a multi-hop wireless network, the resulting system being commonly called a Wireless Sensor Network (WSN). There are situations in which a WSN needs to be deployed in an “asyou-go” fashion. One such situation is in emergencies, e.g, situational awareness networks deployed by ﬁrst-responders such as ﬁre-ﬁghters or anti-terrorist squads. As-you-go deployment is also of interest when deploying networks over large terrains, such as forest trails, particularly when the network is temporary and needs to be quickly redeployed in a different
The research reported in this paper was supported by a Department of Electronics and Information Technology (DeitY, India) and NSF (USA) funded project on Wireless Sensor Networks for Protecting Wildlife and Humans, by an Indo-French Centre for Promotion of Advance Research (IFCPAR) funded project, and by the Department of Science and Technology (DST, India).

ËÒ ´ÆÓ

Ê Ð Ý 1 Ê Ð Ý 2 Ê Ð Ý 3 ËÓÙÖ ÆÓ

0µ

´ÆÓ 4µ

Fig. 1. A line network with one source, one sink and three relays.

part of the forest (e.g., to monitor a moving phenomenon such as groups of wildlife), or when the deployment needs to be stealthy (e.g., to monitor fugitives). Motivated by these more general problems, we consider the problem of “as-yougo” deployment of relay nodes along a line, between a sink node and a source node (see Figure 1), where the deployment operative1 starts from the sink node, places relay nodes along the line, and places the source node where the line ends.
In [1] we have formulated such a problem as one of optimal sequential relay deployment driven by measurements between a node yet to be deployed and the last relay already deployed. In [1] we worked under the restriction that the deployment agent only moves forward. Such forward-only movement would be a necessity if the deployment needs to be quick. Due to shadowing, the path-loss over a link of a given length is random and a more efﬁcient deployment can be expected if link quality measurements at several locations along the line are compared and an optimal choice made among these. Since, in general, this would require the deployment agent to retrace his steps, we call this approach backtracking. Backtracking would take more time, but might provide a good compromise between deployment speed and deployment efﬁciency, for an application such as the as-you-go deployment of a wireless sensor network over a forest trail (e.g, for wildlife monitoring). When placing a relay at some distance from the previous relay, we can expect a better deployment if we explore several locations in the vicinity, at which the link qualities can be expected to be uncorrelated, and then pick the best among these.
In this paper, we mathematically formulate the problems of as-you-go deployment of relays along a line as optimal sequential decision problems. We introduce various measures of hop-cost with justiﬁcation, and then formulate relay placement problems that minimize (i) the expected total hop cost when the distance L of the source from the sink is geometrically distributed, or (ii) the expected average cost per-step. Our

1In this paper we consider a single person carrying out the deployment and refer to this person as a “deployment operative,” or a “deployment agent.”

channel model accounts for path-loss, shadowing, and fading. The cost of a deployment is evaluated as a linear combination of three components: the sum or the maximum transmit power along the path, the sum outage probability along the path, and the number of relays deployed. We explore deployment with and without backtracking. We formulate each of these problems as a Markov decision process (MDP), obtain the optimal policy structures, illustrate their performance numerically and compare their performance with reasonable heuristics.
A. Related Work
Recent years have seen increasing interest in the research community to explore the impromptu relay placement problem. For example, Howard et al., in [2], provide heuristic algorithms for incremental deployment of sensors in order to cover the deployment area. Souryal et al., in [3], address the problem of impromptu wireless network deployment with experimental study of indoor RF link quality variation; similar approach is taken in [4] also. The authors of [5] describe a breadcrumbs system for aiding ﬁreﬁghting inside buildings. However,these approaches are based purely on heuristics and experimentation; they lack the rigour, both in the formulation and in the deployment strategy, and hence are not convincingly optimal or near optimal in terms of performance. In our work our effort has been to formulate these problems as optimal sequential decision problems in order derive optimal policies whose performance can be compared with simple heuristics, and which could be used to propose other heuristics. Recently, Sinha et al. ([6]) have provided an algorithm based on MDP formulation in order to establish a multi-hop network between a sink and an unknown source location, by placing relay nodes along a random lattice path. Their model uses a deterministic mapping between power and wireless link length, and, hence, does not consider the effect of shadowing that leads to statistical variability of the transmit power required to maintain the link quality over links having the same length. This problem was addressed by Chattopadhyay et al. in [1], where they considered spatial variation in link qualities due to shadowing in the context of as-you-go deployment along a line of unknown random length. The variation of link qualities over space led to the introduction of measurementbased deployment, in which the deployment agent measures the power required to establish a link (with a given quality) to the already placed nodes; the placement algorithm uses this value to decide whether to place a relay at that point.
The work reported in [1], however, has limitations that we address in the present paper.
(i) The framework of [1] requires the link of each hop to have a certain outage probability. In practice, as the deployment agent walks away from the previously placed node, he can reach a point where even the maximum node power does not provide a link of the desired quality to the previous relay, and walking any farther is unlikely to provide a workable link. At this point the deployment is considered to have failed. In our present paper, we do not

bound the outage probability of each hop but make the sum outage over all the hops a part of the optimization objective. (ii) In the framework of [1], the deployment agent can only move forward. In the present paper we introduce “backtracking,” which permits the deployment agent to compare the link qualities over several potential placement locations before deploying the relay at any one of them.

B. Organization

The rest of the paper is organized as follows. The system model and notation has been described in Section II. Asyou-go deployment (without backtracking, for a line having geometrically distributed length) for sum and max power objectives have been described in Section III and Section IV respectively. Section V and Section VI have addressed the problems of as-you-go deployment with backtracking, for a line having geometrically distributed length, for sum and max power objectives respectively. As-you-go deployment with backtracking along a line of inﬁnite length, for average cost per step objective, has been discussed in Section VII. The numerical results have been discussed in Section VIII, followed by the conclusion in Section IX.

II. SYSTEM MODEL AND NOTATION

A. Length of the Line

We consider two different models:
(i) We ﬁrst consider the scenario where the distance L to the source from the sink at the start of the line is a priori unknown, but there is prior information (e.g., the mean L) that leads us to model L as a geometrically distributed number of steps. The step length δ and the mean L, can be used to obtain the parameter of the geometric distribution, i.e., the probability θ that the line ends at the next step. All distances are assumed to be integer multiples of δ. In the problem formulation, we assume δ = 1 for simplicity.2
(ii) Next, we consider the setting where the line has inﬁnite length. This can be useful where the line is long enough, and the end is not known (e.g., a long forest trail). Also, it can be used to deploy a chain of relays over a long line, which can be used by several source-sink pairs, and the source-sink pairs could even be moved from one place to another (if required).

B. Channel Model

In order to model the wireless channel, we consider the usual aspects of path-loss, shadowing, and fading. The received power for a particular link (i.e., a transmitter-receiver pair) of length r is given by:

r −η

Prcv = PT c r0 HW

(1)

2The geometric distribution is the maximum entropy discrete probability mass function with a given mean. Thus, by using the geometric distribution, we are leaving L as uncertain as we can, given the prior knowledge of L.

where PT is the transmit power, c corresponds to the pathloss at the reference distance r0, η is the path-loss exponent, H denotes the fading random variable (e.g., it could be an exponential random variable) and W denotes the shadowing random variable. H accounts for the variation of the received power over time, and it takes independent values from one
coherence time to another. The path-loss between a transmitter
and a receiver at a given distance can have a large spatial
variability around the mean path-loss (averaged over fading),
as the transmitter is moved over different points at the same distance; this is called shadowing.3 Shadowing is usually
modeled as a log-normally distributed, random, multiplicative
path-loss factor; in dB, shadowing is normally distributed with values of standard deviation as large as 8 to 10 dB. Also, shadowing is spatially uncorrelated over distances that depend
on the sizes of the objects in the propagation environment (see
[7]); our measurements in a forest-like region of our Indian
Institute of Science campus gave a decorrelation distance of 6 meters. This is evident from Figure 2, where the variation of the measured correlation, ρ, between shadowing of two links (having one end common and the other ends located
on the same straight line) with the distance between the other two ends, d, has been shown. Log-normality of shadowing was veriﬁed via the Kolmogorov-Smirnov goodness-of-ﬁt test, and hence, the very small values of ρ at d ≥ 6 meters show that we can safely assume independent shadowing at different potential relay locations if the step size δ is 6 meters or above. However, in our formulation, W is assumed to take values from a ﬁnite set W with the probability mass function pW (w) := P(W = w); in our numerical work we have quantized the range of values that log-normal shadowing can
assume.
A link is considered to be in outage if the received signal power drops (due to fading) below Prcv−min (e.g., below −88 dBm). Since practical radios can only be set to transmit at a ﬁnite set of power levels, the transmit power of each node can be chosen from a discrete set, S := {P1, P2, · · · , PM }, where {P1, P2, · · · , PM } is arranged in ascending order. For a link of length r, a transmit power γ ∈ S and any particular realization of shadowing W = w, the outage probability is denoted by Pout(r, γ, w), which is increasing in r and decreasing in γ, w (according to (1)). It is also easy to check that the random variable Pout(r, γ, W ) is stochastically increasing in r for each γ ∈ S, and stochastically decreasing in γ for each r. Note that Pout(r, γ, w) depends on the fading statistics in the

0.8

0.6

ρ

0.4

0.2

0

0

2

4

6

8

10

12

d (in meters)

Fig. 2. Variation of link shadowing correlation ρ as a function of distance d measured in a forest-like environment in our campus; one end (either the transmitter or the receiver) is common to both links and the other ends for both links are kept on the same line, but d distance apart from each other.
w6
w5 w4
ÈÖ Ú ÓÙ× ÆÓ Ë Ô 3 ÄÓ Ø ÓÒ× Å ×ÙÖ Ñ ÒØ ÖÓÑ 3 ÄÓ Ø ÓÒ×

Fig. 3. Backtracking with A = 3 and B = 3; the deployment agent skips the ﬁrst A steps from the previous node and measures the shadowing wA+1, wA+2, · · · , wA+B from next B locations in order to decide where to place the next relay.
environment.4
C. Deployment Process and Some Notation
As the deployment agent walks along the line, at each step or at some subset of steps (each step is assumed to be a potential relay location, thereby discretizing the problem) he measures the link quality from the current location to the previous node (see Figure 3); these measurements are used to decide where to place the next relay node and at what transmit power it should operate. In this paper, we do not consider the possibility of another person following behind, who can learn from the measurements and actions of the ﬁrst person, thereby supplementing the actions of the preceding individual. For deployment with backtracking, we assume that after placing a node, the deployment agent skips the next A locations (i.e., walks forward a distance Aδ, where A ≥ 0) and measures the shadowing w := (wA+1, wA+2, · · · , wA+B)5 to the previous node from locations (A+1), (A+2), · · · , (A+B). Then he places the relay at one of the locations (A + 1), (A + 2), · · · , (A + B) and moves on. This procedure is illustrated

3Consider (1). If we transmit a sufﬁciently large number of packets on a

link over multiple coherence times and record the received signal strength

of all the packets, we can compute P rcv which is the mean received signal

power averaged over fading. But if the realization of shadowing in that link is
−η

w, then P rcv = PT c rr0 w.

wE(H), from which we can easily calculate

4For a link with shadowing realization w, if the transmit power is γ, the
−η
received power of a packet will be Prcv = γc rr0 wH. Outage is

deﬁned to be the event Prcv ≤ Prcv−min. If H is exponentially distributed
−η
with mean 1, then we have, Pout(r, γ, w) = P γc rr0 wH ≤

Prcv−min

Prcv−min

(

r r

)η

= 1 − e−

0

γcw

.

5Underlined symbols denote vectors in this paper.

in Figure 3. For the geometrically distributed length if the line ends within (A + B) steps from the previous node, then the source is placed where the line ends. In this case, after the deployment process is complete (i.e., when the source is placed), we denote the number of deployed relays by N , which is a random number, with the randomness coming from the randomness in the link qualities (due to shadowing) and in the length of the line.
As shown in Figure 1, the sink is called Node 0, the relay closest to the sink is called Node 1, and ﬁnally the source is called Node (N + 1). The link whose transmitter is Node i and receiver is Node j is called link (i, j). A generic link is denoted by e.
We assume that the shadowing at any two different links in the network are independent, i.e., W(e1) is independent of W(e2) for e1 = e2. This can be a reasonable assumption if δ is chosen to be at least the de-correlation distance of shadowing (as discussed in Section II-B).
For comparison, we also consider the case in which backtracking is not allowed. In this case, after placing a relay, the agent skips the next A steps, and sequentially measures shadowing from the locations (A+1), (A+2), · · · , (A+B). As the agent explores the locations (A + 1), (A + 2), · · · , (A + B − 1) and measures the shadowing in those locations, at each step he decides whether to place a relay there, and if the decision is to place a relay, then he also decides at what transmit power the relay will operate. In this process, if he has walked (A + B) steps away from the previous relay, or if he encounters the source location within this distance, then he must place the relay or the source.
The choice of A and B depends on the constraints and requirements for the deployment. A larger value of A will result in faster exploration of the line, since many locations can be skipped. For a ﬁxed A, a larger value of B results in more measurements, and hence we can expect a better performance on an average. However, A and B must be chosen such that the outage probability of a randomly chosen link having length (A+B) steps are within tolerable limits with high probability.6
D. Trafﬁc Model
We consider a trafﬁc model where the trafﬁc is so low that there is only one packet in the network at a time; we call this the “lone packet model.” As a consequence of this assumption, (i) the performance over each link depends only on the pathloss, shadowing and fading over that link, as there are no simultaneous transmissions to cause interference, and (ii) the transmission delay over each link is easily calculated, as there are no simultaneous transmitters to contend with. This permits us to easily write down the communication cost on a path over the deployed relays. Such a trafﬁc model is realistic for sensor networks that carry out low duty cycle measurement of environment variables, or just carry an occasional alarm
6Randomness in outage probability of a randomly chosen link comes from the spatial variation of link quality due to shadowing.

packet. Also, a design with the lone packet model can be the starting point for a design with desired positive trafﬁc.

E. Network Cost

In each case, we evaluate the cost of a deployed network in terms of the sum of certain hop costs. In case all the nodes have wake-on radios, the nodes normally stay in sleep mode, and each sleeping node draws a very small current from the battery (see [8]). When a node has a packet, it sends a wake-up tone to the intended receiver. The receiver wakes up and the sender transmits the packet. The receiver sends an ACK packet in reply. Clearly, the energy spent in transmission and reception of data packets govern the lifetime of a node, given that the ACK size is negligible compared to the packet size. Also, the energy spent in transmission and reception of packets govern the lifetime in certain receiver-centric synchronous duty cycled MAC protocols, under moderate trafﬁc which ensures no contention and substantial amount of energy consumption in data transmission and reception.
Let tp be the duration of a packet, and suppose that the node i uses power Γi during transmission, which can be chosen according to the link quality. Let Pr denote the power that any receiving node uses for a packet reception. The relay node k (1 ≤ k ≤ N ) can deliver E/(Γk +Pr)tp packets before its battery is drained out. The source can deliver E/ΓN+1tp packets, where ΓN+1 is the transmit power used by the source. Writing ΓN+1 = ΓN+1 + Pr, we can write the cost function which appropriately captures the lifetime of the network:

N +1
E( max Γi + ξo Po(ui,ti−1) + ξrN ) (2)
i∈{1,2,··· ,N +1} i=1

where ξr is the cost of placing a relay and ξo is the cost per unit outage probability. Po(ui,ti−1) is the outage probability in the

link (i, i − 1), and is decreasing in the transmit power Γi. The

sum outage probability

N +1 i=1

Po(ui,ti−1)

is

an

indicator

of

the

end-to-end packet dropping rate when the outage probabilities

are small and there is no retransmission for dropped packets.

On the other hand, since the packet arrival rate ζ at

the source is very small, the lifetime of the k-th node is
Tk := ζ(Γk+EPr)tp seconds. Hence, the rate at which we have to replace the batteries in the network is given by
k T1k = k ζ(Γk+EPr)tp . The energy expenditure due to Pr is absorbed into ξr, and we have the following cost function:

N +1

N +1

E( Γi + ξo

Po(ui,ti−1) + ξrN )

(3)

i=1

i=1

For average cost per step objective, the max power cost does not make any sense and we consider only sum-power cost.

III. IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITHOUT BACKTRACKING: SUM
POWER AND SUM OUTAGE OBJECTIVE

A. Problem Formulation

Here we seek to solve the following problem:

N +1

N +1

min Eπ

Γ(i,i−1) + ξo

Po(ui,ti−1) + ξrN

(4)

π∈Π

i=1

i=1

where Π is the set of all placement policies and π is a speciﬁc placement policy.

Let us recall the deployment procedure for no backtracking as described in Section II-C. When the agent is r steps away from the previous node (A+1 ≤ r ≤ A+B), he measures the shadowing w on the link from the current location to the previous node. He uses the knowledge of (r, w) to decide whether to place a node there. We formulate (4) as a Markov Decision Process with state space {A + 1, A + 2, · · · , A + B} × W. At state (r, w), (A + 1) ≤ r ≤ (A + B − 1), w ∈ W, the action is either to place a relay and select some transmit power γ ∈ S, or not to place. When r = A + B, the only feasible action is to place and select a transmit power γ ∈ S. Note that, the problem restarts after placing a relay, because of the memoryless property of the geometric distribution and the independence of shadowing across links; the state of the system at such regeneration points is denoted by 0. When the source is placed, the process terminates. The randomness in the system comes from the geometric distribution of the length of the line and the random shadowing in different links. Note that the cost function in (4) can also be motivated as Lagrangian relaxations of constraints on the expectations of the sum outage and the number of deployed relays, N .

B. Bellman Equation
Let us denote the optimal expected cost-to-go at state (r, w) and at state 0 be J(r, w) and J(0) respectively. Note that here we have an inﬁnite horizon total cost MDP with a ﬁnite state space and ﬁnite action space. The assumption P of Chapter 3 in [9] is satisﬁed here, since the singlestage costs are nonnegative (power, outage and relay costs are all nonnegative). Hence, by the theory developed in [9], we can restrict ourselves to the class of stationary deterministic Markov policies. Any deterministic Markov policy π is a sequence {µk}k≥1 of mappings from the state space to the action space. A deterministic Markov policy is called “stationary” if µk = µ for all k ≥ 1.
By Proposition 3.1.1 of [9], the optimal value function J(·) satisﬁes the Bellman equation which is given by, for all (A + 1) ≤ r ≤ (A + B − 1),

J(r, w) = min min(γ + ξoPout(r, γ, w)) + ξr + J(0),
γ∈S
θEW min(γ + ξoPout(r + 1, γ, W ))
γ∈S

+(1 − θ)EW J(r + 1, W ) ,

J(A + B, w) = min(ξr + γ + ξoPout(A + B, γ, w)) + J(0)
γ∈S

A+1

J(0) =

(1 − θ)k−1θEW min(γ + ξoPout(k, γ, W ))

γ∈S

k=1

+(1 − θ)A+1EW J(A + 1, W )

(5)

The equation for J(r, w) can be understood as follows. If the current state is (r, w), (A + 1) ≤ r ≤ (A + B − 1) and
the line has not ended yet, we can either place a relay and use some γ power in it, or we may not place. If we place, a cost minγ∈S (ξr + γ + ξoPout(r, γ, w)) is incurred at the current step and the cost-to-go from there is J(0) since the decision
process regenerates at the point. If we do not place a relay, the line will end with probability θ in the next step, in which case a cost EW minγ∈S (γ + ξoPout(r + 1, γ, W )) will be incurred. If the line does not end in the next step, the next state will be a random state (r + 1, W ) and a mean cost of EW J(r + 1, W ) will be incurred. At state (A+B, w) the only possible decision is to place a relay; hence the expression follows. At state 0,
the deployment agent starts walking until he encounters the source location or location (A + 1); if the line ends at step k, 1 ≤ k ≤ A + 1 (with probability (1 − θ)k−1θ), a cost of EW minγ∈S (γ + ξoPout(k, γ, W )) is incurred. If the line does not end within (A + 1) steps (this event has probability (1 − θ)A+1), the next state will be a random state (A + 1, W ).

C. Value Iteration
The value iteration for (4) is given by, for all (A + 1) ≤ r ≤ (A + B − 1):

J (k+1)(r, w) = min min(γ + ξoPout(r, γ, w)) + ξr + J (k)(0),
γ∈S
θEW min(γ + ξoPout(r + 1, γ, W ))
γ∈S

+(1 − θ)EW J(k)(r + 1, W ) ,

J (k+1)(A + B, w) = min(γ + ξoPout(A + B, γ, w) + ξr)
γ∈S

+J (k)(0)

A+1

J (k+1)(0) =

(1 − θ)k−1θEW min(γ + ξoPout(k, γ, W ))

γ∈S

k=1

+(1 − θ)A+1EW J (k)(A + 1, W )

(6)

with J (0)(r, w) := 0 for all r, w and J (0)(0) := 0.
Lemma 1: The value iteration (6) provides a nondecreasing sequence of iterates that converges to the optimal value function, i.e., J(k)(r, w) ↑ J(r, w) for all r, w and J(k)(0) ↑ J(0).
Proof: See Appendix A.

D. Policy Structure
Lemma 2: J(r, w) is increasing in r, ξo and ξr, decreasing in w, and jointly concave in ξo and ξr. J(0) is increasing and jointly concave in ξo and ξr.
Proof: See Appendix A. Theorem 1: At state (r, w) (A + 1 ≤ r ≤ A + B − 1), the optimal decision is to place a relay iff minγ∈S (γ + ξoPout(r, γ, w)) ≤ cth(r) where cth(r) is a threshold increasing in r. In this case if the decision is to place a relay, the
optimal power to be selected is given by argminγ∈S γ +
ξoPout(r, γ, w) . At state (A + B, w), the optimal power to
be selected is argminγ∈S γ + ξoPout(A + B, γ, w) .
Proof: See Appendix A. Remark: cth(r) captures the effect of the tradeoff that if we place relays far apart, the cost due to outage increases, but the cost of placing the relays decreases. cth(r) is increasing in r because Pout(r, γ, w) is increasing in r for any γ, w. Note that the threshold cth(r) does not depend on w, due to the fact that shadowing is i.i.d across links.7
E. Computation of the Optimal Policy
Let us write V (r) := EW J (r, W ) = w∈W pW (w)J (r, w), i.e., for all r ∈ {A+1, A+2, · · · , A+ B}, and V (0) := J(0). Also, for each stage k ≥ 0 of the value iteration (6), deﬁne V (k)(r) := EW J (k) (r, W ) and V (k)(0) := J (k)(0). Observe that from the value iteration (6), we obtain for all (A + 1) ≤ r ≤ (A + B − 1):

for all r (by Monotone Convergence Theorem) and V (k)(0) ↑ J (0). Thus, V (k)(r) ↑ V (r) and V (k)(0) ↑ V (0). Hence, by the function iteration (7), we obtain V (0) and V (r) for all r ≥ 1. Then, from (26), we can compute cth(r). Thus, for this iteration, we need not keep track of the cost-to-go values J(k)(r, w) for each state (r, w), at each stage k; we simply need to keep track of V (k)(0) and V (k)(r) for each r.
IV. IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITHOUT BACKTRACKING: MAX
POWER AND SUM OUTAGE OBJECTIVE
A. Problem Formulation
Here we seek to solve the following problem without backtracking, for a line having geometrically distributed length:

min Eπ
π∈Π

N +1

max

Γ(i,i−1) + ξo

Po(ui,ti−1) + ξrN

i∈{1,2,··· ,N +1}

i=1

(11)

We formulate (11) as an MDP with (r, w, γmax) as a typical state, where γmax is the maximum transmit power used by already deployed nodes. At state (r, w, γmax), (A + 1) ≤ r ≤ (A + B − 1), w ∈ W, the action is either to place a relay and

select some transmit power γ ∈ S, or not to place. When r =

A+B, we must place a relay. The state of the system at a point

where a relay has just been placed and the maximum power

used in all previous links is γmax, is denoted by (0; γmax). The state at the sink is (0; γmax) with γmax = 0. Hence, in our current problem formulation, γmax can take values from the set {0} ∪ S. At state (0; γmax), the only possible action

is to move to the next step. When the source is placed, the

process terminates.

V (k+1)(r) =

pW (w) min min γ +

γ∈S

w∈W

ξoPout(r, γ, w) + ξr + V (k)(0),

θEW min(γ + ξoPout(r + 1, γ, W ))
γ∈S

+(1 − θ)V (k)(r + 1) ,

V (k+1)(A + B) =

pW (w) min γ +

γ∈S

w∈W

+ξoPout(A + B, γ, w) + ξr + V (k)(0)

A+1

V (k+1)(0) =

(1 − θ)k−1θEW min γ + ξoPout(k, γ, W )

γ∈S

k=1

+(1 − θ)A+1V (k)(A + 1)

(7)

with V (0)(r) := 0 for all A+1 ≤ r ≤ A+B and V (0)(0) := 0.
Since J (k)(r, w) ↑ J (r, w) for each r, w and J (k)(0) ↑ J(0) as k ↑ ∞, we can argue that V (k)(r) ↑ EW J(r, W )

7Though the length of the line is assumed to be geometrically distributed, similar approach as in this paper can be used to analyze the case where the length of the line is constant and known. The only difference will be that the optimal policy will be nonstationary.

B. Bellman Equation
Unlike problem (4), here the cost of the maximum power over all links is incurred when the source is placed. However, the outage and relay costs are incurred whenever a node is placed.
The optimal value function J(·) satisﬁes the Bellman equation given by (8). This equation can be understood as follows. If the current state is (r, w, γmax), (A + 1) ≤ r ≤ (A + B − 1) and the line has not ended yet, we can either place a relay and use some γ power in it, or we may not place. If we place and use power γ, a cost (ξoPout(r, γ, w) + ξr) is incurred at the current step and the state becomes (0; max{γ, γmax}). If we do not place a relay, the line will end with probability θ in the
next step, in which case a cost EW minγ∈S max{γ, γmax}+
ξoPout(r + 1, γ, W ) will be incurred. If the line does not
end in the next step, the next state will be a random state (r + 1, W, γmax) and a mean cost of EW J (r + 1, W, γmax) will be incurred. At state (A + B, w, γmax) the only possible decision is to place a relay; hence the expression follows. At state (0; γmax), the deployment agent explores at least

J(r, w, γmax) = min min ξoPout(r, γ, w) + ξr + J(0; max{γ, γmax}) , θEW min max{γ, γmax} + ξoPout(r + 1, γ, W )

γ∈S

γ∈S

+(1 − θ)EW J(r + 1, W, γmax) , ∀(A + 1) ≤ r ≤ (A + B − 1)

J(A + B, w, γmax) = min ξoPout(A + B, γ, w) + ξr + J(0; max{γ, γmax})
γ∈S

A+1

J (0; γmax) =

(1 − θ)k−1θEW min max{γ, γmax} + ξoPout(k, γ, W ) + (1 − θ)A+1EW J (A + 1, W, γmax)

(8)

γ∈S

k=1

J (k+1)(r, w, γmax) = min min ξoPout(r, γ, w) + ξr + J (k)(0; max{γ, γmax}) , θEW min max{γ, γmax}

γ∈S

γ∈S

+ξoPout(r + 1, γ, W ) + (1 − θ)EW J(k)(r + 1, W, γmax) , ∀(A + 1) ≤ r ≤ (A + B − 1)

J (k+1)(A + B, w, γmax) = min ξoPout(A + B, γ, w) + ξr + J (k)(0; max{γ, γmax})
γ∈S

A+1

J (k+1)(0; γmax) =

(1 − θ)k−1θEW min max{γ, γmax} + ξoPout(k, γ, W ) + (1 − θ)A+1EW J (k)(A + 1, W, γmax) (9)

γ∈S

k=1

V (k+1)(r, γmax) =

pW (w) min min ξoPout(r, γ, w) + ξr + V (k)(0; max{γ, γmax}) , θEW min max{γ, γmax}

γ∈S

γ∈S

w∈W

+ξoPout(r + 1, γ, W ) + (1 − θ)V (k)(r + 1, γmax) , ∀(A + 1) ≤ r ≤ (A + B − 1)

V (k+1)(A + B, γmax) =

pW (w) min ξoPout(A + B, γ, w) + ξr + V (k)(0; max{γ, γmax})

γ∈S

w∈W

A+1

V (k+1)(0; γmax) =

(1 − θ)k−1θEW min max{γ, γmax} + ξoPout(k, γ, W ) + (1 − θ)A+1V (k)(A + 1, γmax) (10)

γ∈S

k=1

upto the (A + 1)-st step. If the line ends at a distance of k-th step (1 ≤ k ≤ A + 1) (with probability (1 − θ)k−1θ),
a cost EW minγ∈S max{γ, γmax} + ξoPout(k, γ, W ) is
incurred. If the line does not end in (A + 1) steps (with probability (1 − θ)A+1), the next state will be a random state (A + 1, W, γmax).
C. Value Iteration
Starting with J (0)(r, w, γmax) = 0 and J (0)(0; γmax) = 0 for all r, w, γmax, the value iteration for problem (11) is given by (9).
Lemma 3: The value iteration (9) provides a nondecreasing sequence of iterates that converges to the optimal value function, i.e., J (k)(r, w, γmax) ↑ J (r, w, γmax) and J (k)(0; γmax) ↑ J (0; γmax).
Proof: Proof follows along the same line of arguments as in Lemma 1.
D. Policy Structure
Lemma 4: J (r, w, γmax) is increasing in r, γmax, ξo and ξr, decreasing in w, and jointly concave in ξo and ξr. J(0; γmax) is increasing and jointly concave in ξo and ξr, and increasing in γmax.

Proof: See Appendix B. Theorem 2: At state (r, w, γmax) (A + 1 ≤ r ≤ A + B − 1), the optimal decision is to place a re-
lay iff minγ∈S ξoPout(r, γ, w) + J (0; max{γ, γmax}) ≤
cth(r, γmax) where cth(r, γmax) is a threshold function increasing in r and γmax. A relay must be placed at r = A + B. If the decision is to place a relay, then the optimal transmit power for the new relay is given by
argminγ∈S ξoPout(r, γ, w) + ξr + J (0; max{γ, γmax}) .
Proof: See Appendix B.
E. Computation of the Optimal Policy
Let us write V (r, γmax) := EW J (r, W, γmax) = w∈W pW (w)J (r, w, γmax), for all r ∈ {A + 1, A + 2, · · · , A + B} and all γmax ∈ {0} ∪ S, V (0; γmax) := J(0; γmax). Also, for each stage k ≥ 0 of the value iteration (9), deﬁne V (k)(r, γmax) := EW J (k) (r, W, γmax), and V (k)(0; γmax) := J (k)(0; γmax). Observe that from the value iteration (9), we obtain (10). Using similar arguments as in Section III-E, we can conclude that V (k)(·) ↑ V (·) in (10). Then for each r, γ, the value of cth(r, γ) can be computed from the function V (·), using the

Bellman equation (8).
V. IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITH BACKTRACKING: SUM
POWER AND SUM OUTAGE OBJECTIVE
A. Problem Formulation
Consider the deployment procedure as in Section II-C, with the objective (4), under the scenario where the length of the line is geometrically distributed with parameter θ. We formulate this problem as an MDP with state space WB ∪ {z; 0}0≤z≤B−1. The deployment agent starts walking from the previous node location, explores the next (A + B) steps and measures w = (wA+1, wA+2, · · · , wA+B) which belongs to WB. The state (z; 0) means that a relay has already been placed at the current position and the residual length of the line from the current location is (z + L1) where L1 ∼ Geometric(θ). At state w an action (u, γ) is taken, where u ∈ {A + 1, A + 2, · · · , A + B} and γ ∈ S. At state (z; 0) the action is to explore next (A + B) steps, out of which B steps will involve measurements. Note that the link qualities obtained from these new measurements will be independent from the previous measurements, since here new links (transmitter-receiver pairs) are being measured. The state (z; 0) is needed for the following reason: suppose that at some state w the optimal decision is to place the next relay u steps away from the previous relay, where A + 1 ≤ u ≤ A + B. After placing this relay, the residual length of the line becomes (A + B − u + L1) where L1 ∼ Geometric(θ); the problem does not restart after the placement of a relay as it did in Section III. When the line ends, the process terminates.
B. Bellman Equation
Following the same arguments as in Section III-B, we can argue that the optimal expected cost-to-go function J(·) satisﬁes the following Bellman equation:

J(w) =

min

ξr + γ +

u∈{A+1,··· ,A+B},γ∈S

ξoPout(u, γ, wu) + J (A + B − u; 0)

A+B−z

J(z; 0) =

(1 − θ)k−1θEWz+k

k=1

min γ + ξoPout(z + k, γ, Wz+k)
γ∈S

+(1 − θ)A+B−z g(w)J (w)

(12)

w

When the state is w, if the action (u, γ) is taken then a cost of ξr + γ + ξoPout(u, γ, wu) is incurred in the current step and the next state becomes (A + B − u; 0), resulting in an additional cost J(A + B − u; 0). If the state is (z; 0), the source can appear in the (z + k)-th step (1 ≤ k ≤ A + B − z) from the current location with probability (1 − θ)k−1θ

(since the residual length of the line is z plus a geometrically distributed random variable), in which case a mean cost of EWz+k minγ∈S (γ + ξoPout(z + k, γ, Wz+k)) is incurred in the last hop. If the line does not end in next (A + B) steps (which has probability (1 − θ)A+B−z), the next state becomes w with probability g(w) := ΠrA=+AB+1pWr (wr) (since shadowing is i.i.d across links). Note that the optimal expected cost-to-go at the sink node is J(0; 0).
C. Value Iteration
The value iteration for this problem is given by:

J (k+1)(w) =

min

ξr + γ +

u∈{A+1,··· ,A+B},γ∈S

ξoPout(u, γ, wu) + J (k)(A + B − u; 0)

A+B−z

J (k+1)(z; 0) =

(1 − θ)k−1θEWz+k

k=1

min γ + ξoPout(z + k, γ, Wz+k)
γ∈S
+(1 − θ)A+B−z g(w)J (k)(w) (13)
w
with J(0)(·) := 0 for all states. Lemma 5: Each of J (w) and {J (z; 0)}0≤z≤B−1 is increas-
ing and jointly concave in ξr, ξo. Proof: The proof follows from the convergence of value
iterates to the optimal value function, along the same lines as in Lemma 2.
Lemma 6: J(w) is decreasing in each component of w. Proof: Note that for each (u, γ), Pout(u, γ, wu) is de-
creasing in wu. Hence, the result follows from the ﬁrst equation in (12).
Lemma 7: J(z; 0) is increasing in z. Proof: See Appendix C.

D. Policy Structure
Theorem 3: The optimal action at state w is the pair (u, γ) achieving the minimum in (12). The minimum is always achieved since we have ﬁnite action space.

E. Policy Computation
Note that in the k-th iteration of the value iteration obtained from the Bellman equation (12), we need to update J(k)(w) for |W|B possible values of the state w, which could be computationally very much expensive for large values of |W|. Let us deﬁne the sequence {V (k)}k≥0 by V (0) = 0, V (k) = w∈WB g(w)J (k)(w). Now consider the following iteration (with J(0)(z; 0) := 0 for all 0 ≤ z ≤ B − 1) obtained from (13):

V (k+1) =

g(w)

min

w u∈{A+1,··· ,A+B},γ∈S

ξr + γ +

ξoPout(u, γ, wu) + J (k)(A + B − u; 0)

A+B−z

J (k+1)(z; 0) =

(1 − θ)k−1θEWz+k

k=1

min γ + ξoPout(z + k, γ, Wz+k)
γ∈S
+(1 − θ)A+B−zV (k), 0 ≤ z ≤ (B − 1) (14)

By Monotone Convergence Theorem, V (k) ↑ w g(w)J(w). Hence, we can just use the function iteration (14) to compute the optimal value function, from
which the policy can be computed. The advantage of this function iteration is that we need not update J(k)(·) for each
state.

VI. IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITH BACKTRACKING: MAX
POWER AND SUM OUTAGE OBJECTIVE
A. Problem Formulation
In this section, we seek to develop optimal placement policy with backtracking for the problem (11). We formulate this problem as an MDP with state space WB ∪

{z; 0}0≤z≤B−1 × (S ∪ {0}). The state (z; 0; γmax) (γmax ∈
S ∪ {0}) means that a relay has already been placed at the current position, the residual length of the line from the current location is (z + L1) where L1 ∼ Geometric(θ), and the maximum transmit power used so far by the previous nodes is γmax. At state (w; γmax) an action (u, γ) is taken, where u ∈ {A+1, A+2, · · · , A+B} and γ ∈ S. At state (z; 0; γmax) the action is to explore next (A + B) steps.
B. Bellman Equation
The optimal expected cost-to-go function J(·) satisﬁes the following Bellman equation:

J (w; γmax) =

min

ξoPout(u, γ, wu)

u∈{A+1,··· ,A+B},γ∈S

+ξr + J(A + B − u; 0; max{γ, γmax})

A+B−z

J (z; 0; γmax) =

(1 − θ)k−1θEW min
z+k γ∈S k=1

max{γ, γmax}

+ξoPout(z + k, γ, Wz+k)

+(1 − θ)A+B−z g(w)J (w; γmax)

(15)

w

When the state is (w; γmax), if the action (u, γ) is taken then a cost of ξoPout(u, γ, wu) + ξr is incurred in the current step

and the next state becomes (A + B − u; 0; γmax), resulting in an additional cost J(A + B − u; 0; γmax). If the state is (z; 0; γmax), the source can appear in the (z + k)-th step (1 ≤ k ≤ A+B −z) from the current location with probability (1 − θ)k−1θ (since the residual length of the line is z plus a geometrically distributed random variable), resulting in a mean cost of EWz+k minγ∈S (max{γ, γmax} + ξoPout(z + k, γ, Wz+k)), which is a combination of the max power used in the network and the outage probability of the last hop. If the line does not end in next (A + B) steps (which has probability (1 − θ)A+B−z), the next state becomes (w; γmax) with probability g(w) := ΠAr=+AB+1pWr (wr) (since shadowing is i.i.d across links).
C. Value Iteration
The value iteration for this problem is given by:

J (k+1)(w; γmax) =

min

ξoPout(u, γ, wu)

u∈{A+1,··· ,A+B},γ∈S

+ξr + J(k)(A + B − u; 0; max{γ, γmax})

A+B−z

J (k+1)(z; 0; γmax) =

(1 − θ)k−1θEWz+k

k=1

min max{γ, γmax} + ξoPout(z + k, γ, Wz+k)
γ∈S

+(1 − θ)A+B−z g(w)J (k)(w; γmax)

(16)

w

with J(0)(·) := 0 for all states.

Lemma 8: Each

of

J (w; γmax)

and

{J (z; 0; γmax)}0≤z≤B−1 is increasing and jointly concave in

ξr, ξo, and increasing in γmax.

Proof: The proof follows from the convergence of value

iterates to the optimal value function, along the same lines as

in Lemma 4.

Lemma 9: J(w; γmax) is decreasing in each component of w.

Proof: Note that for each (u, γ), Pout(u, γ, wu) is decreasing in wu. Hence, the result follows from (15).

Lemma 10: J(z; 0; γmax) is increasing in z.

Proof: It is easy to show that J(z + 1; 0; γmax) ≥ J(z; 0; γmax), by similar arguments as in the proof of

Lemma 10.

D. Policy Structure
Theorem 4: The optimal action at state (w; γmax) is the pair (u, γ) achieving the minimum in (15). The minimum is always achieved since we have ﬁnite action space.

E. Policy Computation
Note that in the k-th iteration of the value iteration obtained from the Bellman equation (15), we need to update J (k)(w; γmax) for |W|B|(S| + 1) possible values of the state (w; γmax), which could be computationally very much expensive for large values of |W|. Let us deﬁne the sequence of functions {V (k)(γmax)}k≥0 by V (0)(γmax) = 0,

V (k)(γmax) = w∈WB g(w)J (k)(w; γmax). Now consider the following iteration (with J(0)(z; 0; γmax) := 0 for all
0 ≤ z ≤ B − 1) obtained from (16):

V (k+1)(γmax)

= g(w)

min

ξoPout(u, γ, wu)

w u∈{A+1,··· ,A+B},γ∈S

+ξr + J (k)(A + B − u; 0; max{γ, γmax})

A+B−z

J (k+1)(z; 0; γmax) =

(1 − θ)k−1θEWz+k

k=1

min max{γ, γmax} + ξoPout(z + k, γ, Wz+k)
γ∈S

+(1 − θ)A+B−zV (k)(γmax)

(17)

By Monotone Convergence Theorem, V (k)(γmax) ↑ w g(w)J(w; γmax). Hence, we can just use the function iteration (17) to compute the optimal value function, from
which the policy can be computed. The advantage of this function iteration is that we need node update J(k)(·) for each
state.

F. Comparison of the Optimal Expected Costs of Problem (4) and Problem (2)
Theorem 5: Under the same class of policies, the optimal expected cost for Problem (4) is always greater than or equal to that of Problem (11).
Proof: Let Π be a class of policies, and let π ∈ Π be a speciﬁc policy. Consider any realization of L and any realization of shadowing in all potential links; under policy π, relays will be placed at some locations and the relays and the source will use some transmit power levels. But, for any such deployed network the sum power is always greater than or equal to the max power, and hence we can write Jπsum ≥ Jπmax, where Jπsum and Jπmax are the expected costs under policy π of the problems (4) and (11) respectively. Hence, infπ∈Π Jπsum ≥ infπ∈Π Jπmax, which completes the proof.

VII. AVERAGE COST PER STEP: WITH AND WITHOUT BACKTRACKING
Consider the deployment process as described in Section II-C. After making the measurements (wA+1, wA+2, · · · , wA+B), the deployment agent chooses one integer u from the set {A + 1, A + 2, · · · , A + B} and places the relay u steps away from the last relay and also decides at what transmit power γ ∈ S the new relay should operate. The objective is to minimize the long-run expected average cost per step.

A. Problem Formulation
We formulate our problem as a Semi-Markov Decision Process (SMDP) with state space WB and action space

{A + 1, A + 2, · · · , A + B} × S. After placing a relay, the deployment agent measures w := (wA+1, wA+2, · · · , wA+B) which is the state in our SMDP. At state w, if the action (u, γ) is taken (where A + 1 ≤ u ≤ A + B and γ ∈ S), the cost c(w, u, γ) := (γ + ξoPout(u, γ, wu) + ξr) is incurred and the next state becomes w := (wA+1, wA+2, · · · , wA+B) with probability g(w ) := ΠrA=+AB+1pWr (wr) (since shadowing is i.i.d across links). A deterministic Markov policy π is a sequence of mappings {µk}k≥1 from the state space to the action space, and it is called a stationary policy if µk = µ for all k ≥ 1. Let us denote, by the vector-valued random variable W (k), the state at the k-th decision instant, and by µk(W (k)) the action at the k-th decision instant. For a deterministic Markov policy {µk}k≥1, let us deﬁne the functions µ(k1) : WB → {A + 1, A + 2, · · · , A + B} and µ(k2) : WB → S as follows: if µk(w) = (u, γ), then µ(k1)(w) = u and µ(k2)(w) = γ.
Our problem is to minimize the long-run average cost per
step (see equation (5.33) of [9] for deﬁnition) as follows:

n k=1

Eµk

c

W (k), µ(k1)(W (k)), µ(k2)(W (k))

inf lim sup

(18)

π∈Π n→∞

n k=1

Eπ

µ(k1) (W

(k))

where Π denotes the set of all deterministic, Markov policies, π = {µi}i≥1 is a speciﬁc deterministic, Markov policy and c(·, ·, ·) is the cost incurred when we place a relay (as explained earlier in this section). Note that, under any policy, the state evolution process is a positive recurrent Discrete Time Markov Chain (DTMC) (under i.i.d shadowing assumption, W (k) will be i.i.d across k, k ≥ 1). Also, the state and action spaces are ﬁnite. Hence, it is sufﬁcient to work with stationary deterministic policies (see [10]).

Under our current scenario, the average cost per step exists
(in fact, the limit exists) and is same for all states, i.e. for all w ∈ WB. Let us denote the optimal average cost per step by λ∗.

B. Policy Structure
Theorem 6: The optimal action at state w in the problem (18) is given by:

µ∗(w) =

argmin

γ + ξoPout(u, γ, wu) + ξr − λ∗u (19)

u∈{A+1,··· ,A+B},γ∈S

where λ∗ is the optimal average cost per step in (18). Proof: The optimality equation for the SMDP is given by
(see [10], Equation 7.2.2):

v∗(w) =

min

γ + ξoPout(u, γ, wu) + ξr

u∈{A+1,··· ,A+B},γ∈S

−λ∗u +

g(w )v∗(w )

w ∈WB

v∗(w ) = 0 for some w ∈ WB

(20)

where v∗(w) is the optimal differential cost corresponding to state w. v∗(w ) = 0 for some w ∈ WB is required to ensure that the system of equations in (20) has a unique solution. The structure of the optimal policy is obvious from (20), since
w ∈WB g(w )v∗(w ) does not depend on (u, γ). Remark: If we take an action (u, γ), a cost (γ + ξoPout(u, γ, wu) + ξr) will be incurred. On the other hand, if we incur a cost of λ∗ over each one of those u steps, the total cost incurred will be λ∗u. The policy selects the placement point that minimizes the difference between these two costs. Note that due to the choice of the steps at which measurements are made, the shadowing is independent over the steps. This results in each placement point being a regeneration point in the placement process. Theorem 7: The optimal average cost λ∗ is jointly concave and increasing in ξr and ξo.
Proof: See Appendix D.
C. Policy Computation
We adapt a policy iteration from [10] based algorithm to calculate λ∗. The algorithm generates a sequence of stationary policies {µk}k≥1 (note that the notation µk was used for a different purpose in Section VII-A; in this subsection each µk is a stationary, deterministic, Markov policy), such that for any k ≥ 1, µk(w) : WB → {A + 1, A + 2, · · · , A + B} × S maps a state into some action. Deﬁne the sequence {µ(k1), µ(k2)}k≥1 of functions as follows: if µk(w) = (u, γ), then µ(k1)(w) = u and µ(k2)(w) = γ.

Policy Iteration based Algorithm:
Step 0 (Initialization): Start with an initial stationary deterministic policy µ1.
Step 1 (Policy Evaluation): Calculate the average cost λk corresponding to the policy µk, for k ≥ 1. This can be done by applying the Renewal Reward Theorem as follows:

ξr + λk =

w g(w)

µ(k2)(w) + ξoPout(µ(k1)(w), µ(k2)(w), wµ(1)(w)) k
w g(w)µ(k1)(w) (21)

Step 2 (Policy Improvement): Find a new policy µk+1 by

solving the following:

µk+1(w) = argmin γ + Pout(u, γ, wu) + ξr − λku

(22)

(u,γ)

If µk and µk+1 are the same policy (i.e., if λk−1 = λk), then stop and declare µ∗ = µk, λ∗ = λk. Otherwise, go to Step 1.
Remark: It was shown in [10] that this policy iteration will converge in a ﬁnite number of iterations, for ﬁnite state and action spaces as in our current problem. The convergence requires that under any stationary policy, the state evolves as an irreducible Markov chain, which is satisﬁed in our current problem.

Computational Complexity: The state space has cardinality |W|B, and hence O(|W|B) addition operations are required to compute λk from (21). However, careful manipulation leads to a drastic reduction in this computational requirement, as we
will see next.
Note that in (22), if the minimum is achieved by more than one pair of (u, γ), then any one of them can be considered to be the optimal action. Let us use the convention that among all minimizers the pair (u, γ) with minimum u will be considered as the optimal action, and if there are more than one such minimizing pair with same values of u, then the pair with smallest value of γ will be considered. We recall that S = {P1, P2, · · · , PM }. Let us denote, under policy µk+1, the probability that the optimal control is (u, γ) and the shadowing is w at the u-th location, by bk(u, γ, w). Then,

bk(u, γ, w) = Πur=−A1 +1P min (γ + ξoPout(r, γ , Wr)) − λkr
γ ∈S
> γ + ξoPout(u, γ, w) − λku × pW (w) ×ΠAr=+uB+1P min (γ + ξoPout(r, γ , Wr)) − λkr
γ ∈S
≥ γ + ξoPout(u, γ, w) − λku

×I γ = argmin{P1, P2, · · · , PM } :

γ + Pout(u, γ, w)

= min(γ + ξoPout(u, γ , w))

(23)

γ

Now, we can write,

g(w) µ(k2)(w) + ξoPout(µ(k1)(w), µ(k2)(w), wµ(1)(w))

w

k

A+B M

=

bk−1(u, Pj , w) Pj + ξoPout(u, Pj , w)

u=A+1 j=1 w∈W

(24)

and

A+B M

g(w)µ(k1)(w) =

bk−1(u, Pj , w)u

w

u=A+1 j=1 w∈W

A+B M

=

u

bk−1(u, Pj , w) (25)

u=A+1 j=1 w∈W

Now, for each (u, γ, w), bk−1(u, γ, w) (in (23)) can be computed in O(BM |W|) operations. Hence, total number of operations required to compute bk−1(u, γ, w) for all u, γ, w is O(B2M 2|W|2). Now, only O(BM |W|) operations are
required in (24) and (25). Hence, the number of computations required in each iteration is O(B2M 2|W|2).
Note that, the policy improvement step is not explicitly
required in the policy iteration. This is because in the policy evaluation step, λk is sufﬁcient to compute bk(u, γ, w) for all u, γ, w and thereby to compute λk+1. Hence, we need not store the policy in each iteration.

D. No Backtracking
When there is no backtracking (i.e., the deployment agent decides at each step whether to place a relay or not), the state and action spaces are same as discussed in Section III-A. In this section, we are interested in the minimum average cost per step problem, assuming that the line has inﬁnite length. The single-stage cost is the same as in Section III.
Note that the problem (4) can be considered as an inﬁnite horizon discounted cost problem with discount factor (1 − θ). Hence, keeping in mind that we have ﬁnite state and action spaces, we observe that for the discount factor sufﬁciently close to 1, i.e., for θ sufﬁciently close to 0, the optimal policy for problem (4) is optimal for the problem (18) (see [9], Proposition 4.1.7). In particular, the optimal average cost per step with no backtracking, λ , is given by λ = limθ→0 θJθ(0) (see [9], Section 4.1.1), where Jθ(0) is the optimal cost for problem (4) with backtracking with the probability of the line ending in the next step is θ.
Theorem 8: λ ≥ λ∗.
Proof: See Appendix D.

VIII. NUMERICAL WORK

A. Parameter Values

Recall the notation used in Section II. We consider deploy-

ment along a line with step size δ = 6 meters, A = 5,

B = 5 and θ = 0.04 (mean length of the line is 25 steps,

i.e., 150 meters). The set of transmit power levels S is

taken to be {−25, −15, −10, −5, 0} dBm. For the channel

model as in (1), we consider path-loss exponent η = 3.8 and c = 100.00054. Fading is assumed to be Rayleigh;

H ∼ Exponential(1). Shadowing W is assumed to be

log-normal with W

=

10

Y 10

with Y

∼ N (0, σ2) where

σ = 7 dB. The values of the parameters in the channel

model were estimated from data obtained by experiments

(using 2.2 dBi antennas in the transmitter and the receiver)

in a forest-like environment inside our campus. However, for

the purpose of numerical computation we assume that Y can

take values in the interval [−4σ, 4σ] in steps of 0.02. Thus we

have converted the probability density function of Y into the

probability mass function of a discrete-valued random variable,

and the probability of Y being outside the interval [−4σ, 4σ] is negligible (6.3342 × 10−5). This discretization renders the

state space ﬁnite for each problem. We deﬁne outage to be

the event when the received signal power of a packet falls

below Prcv−min = 10−8.8 mW (−88 dBm). For a commercial

implementation of the PHY/MAC of IEEE 802.15.4 (a popular

wireless sensor networking standard), −88 dBm received

power corresponds to a 2% packet loss probability for 140 byte

packets.

B. Geometrically distributed distance L to the source; no backtracking

1) Sum-Power, Sum-Outage Objective; Policy Structure: The variation of cth(r) (for Problem (4)) with the relay cost ξr and the cost of outage ξo has been shown in Figure 4

0.25 ξr=0.001
0.2 ξr=0.1
0.15

cth(r)

0.1

0.05

35

40

45

50

55

r (in meters)

Fig. 4. As-you-go deployment without backtracking; variation of cth(r) with r for ξo = 1 and various values of ξr.
1 ξ =0.1
o
0.8 ξo=1 ξo=10
0.6

cth(r)

0.4

0.2

0

35

40

45

50

55

r (in meters)

Fig. 5. As-you-go deployment without backtracking; variation of cth(r)

with r for ξr = 0.001 and various values of ξo.

ξr

ξo Optimal cost

Optimal cost

for Sum Power for Max Power

0.001 0.1

0.0926

0.0472

0.001 1

0.2646

0.1442

0.001 10

0.8177

0.4532

0.01 0.1

0.1182

0.0757

0.01 1

0.2925

0.1734

0.01 10

0.8457

0.4826

TABLE I GEOMETRICALLY DISTRIBUTED DISTANCE TO THE SOURCE WITH θ = 0.04: COMPARISON OF THE OPTIMAL COST WITHOUT BACKTRACKING
BETWEEN PROBLEMS (4) AND (11), FOR THE PARAMETERS IN SECTION VIII-A, FOR VARIOUS VALUES OF ξr AND ξo .

and Figure 5. For a ﬁxed ξo, cth(r) decreases with ξr; i.e., as the cost of placing a relay increases, we place relays less frequently. On the other hand, for a ﬁxed ξr, cth(r) increases with ξo. This happens because if the cost of outage increases, we cannot tolerate outage and place the relays close to each other.
2) Comparison between the total costs of the sum power and the max power problem: Table I compares the optimal total costs without backtracking of the problems (4) and (11), for various values of ξo and ξr. The ﬁrst problem always has higher cost (Theorem 5), since the sum power in a network is always greater than the max power.
C. Geometrically distributed distance L to source; with and without backtracking
The comparison between the optimal cost of as-you-go deployment with and without backtracking, for Problem (4), for various values of ξr and ξo, and for parameter values as in Section VIII-A, are shown in Table II. It is obvious that backtracking can provide signiﬁcant reduction in the cost compared to no backtracking, due to the fact that in backtracking we choose the best relay location among many

ξr

ξo

Optimal cost

Optimal cost

without backtracking with backtracking

0.001 0.1

0.0926

0.0581

0.001 1

0.2646

0.1502

0.001 10

0.8177

0.4650

0.01 0.1

0.1182

0.0806

0.01 1

0.2925

0.1728

0.01 10

0.8457

0.4878

TABLE II

SUM POWER OBJECTIVE; GEOMETRICALLY DISTRIBUTED DISTANCE TO

THE SOURCE; WITH AND WITHOUT BACKTRACKING; COMPARISON OF THE

OPTIMAL COST FOR VARIOUS VALUES OF ξr AND ξo .

ξr

ξo mean power mean hop Mean outage

per hop

length

probability

(in mW) (in steps)

per link

0.001 0.1

0.0092

7.5965

0.1157

0.001 1

0.0311

7.6260

0.0251

0.001 10

0.0842

7.5445

0.0085

0.01 0.1

0.0097

7.7576

0.1160

0.01 1

0.0312

7.6900

0.0254

0.01 10

0.0844

7.5645

0.0085

0.1 0.01

0.0032

10.0000

0.7856

0.1 0.1

0.0191

9.0787

0.1382

0.1

1

0.0332

8.1944

0.0305

0.1 10

0.0869

7.7556

0.0089

TABLE III

AVERAGE COST PER STEP OBJECTIVE WITH BACKTRACKING: MEAN

POWER PER LINK, MEAN OUTAGE PROBABILITY PER LINK AND THE MEAN

HOP LENGTH UNDER THE OPTIMAL POLICY; VARIOUS VALUES OF ξr , ξo .

ξr

ξo

λ∗

λ

λh

0.001 0.1 0.0029 0.0035 0.0029

0.001 1 0.0075 0.0100 0.0075

0.001 10 0.0226 0.0307 0.0228

0.01 0.1 0.0040 0.0047 0.0041

0.01 1 0.0087 0.0113 0.0087

0.01 10 0.0238 0.0321 0.0239

0.1 0.01 0.0111 0.0111 0.0111

0.1 0.1 0.0146 0.0155 0.0147

0.1

1 0.0200 0.0238 0.0200

0.1 10 0.0355 0.0450 0.0357

TABLE IV AVERAGE COST PER STEP OBJECTIVE: AS-YOU-GO DEPLOYMENT WITH AND WITHOUT BACKTRACKING AND FOR A HEURISTIC; VARIOUS VALUES
OF ξr AND ξo .
(similar arguments as in Theorem 8 works here).

D. Average cost per step; sum power and sum outage; with

and without backtracking λ∗ in Table IV denotes the optimal average cost per step

with backtracking, as discussed in Theorem 6. λ denotes

the optimal average cost per step without backtracking, as

discussed in Section VII-D. λh is the optimal average cost per

step for the following heuristic policy. Recall the notation used

in Section VII. The heuristic policy solves the problem (at state

w) minu∈{A+1,··· ,A+B},γ∈S

γ +ξo Pout (u,γ ,wu )+ξr u

to select the

placement location and the transmit power level to use. Note

that this heuristic, unlike our earlier policies, does not require

any channel model to make the placement decision (e.g., we

need not know explicitly the values of η, σ etc., as we had required earlier to compute λ∗). In this heuristic policy, the

deployment agent, at each u ∈ {A + 1, · · · , A + B}, measures

for each γ ∈ S the outage probability to the previous node

(without using the model to calculate shadowing). Then he

performs

minu,γ

γ +ξo Pout (u,γ ,wu )+ξr u

to make the placement

decision. Thus, the heuristic policy focuses on minimizing the

per-step cost over the new link.

Table III shows the mean power per link, the mean distance between two consecutive nodes, and the mean outage probability per link under the optimal policy with backtracking. Note that for some cases (e.g., ξr = 0.1, ξo = 0.01), the relay is always placed at the 10-th step (step (A + B)) and uses 0.0032 mW (i.e., −25 dBm) power, but this renders the outage probability very high. However, for each of ξr = 0.001, 0.01, 0.1, we have reasonably small outage probability for higher values of the outage cost (ξo = 1, 10). For each ξr, as ξo increases, the outage probability decreases, the mean power per link increases (to reduce the outage probability) and the relays are placed closer and closer to each other.
From Table IV, we ﬁnd that λ∗ is in general substantially smaller than λ , except for some special cases where we always place at (or near) the 10-th step and use −25 dBm transmit power (the optimal policy without backtracking also does the same in such cases). All that it says that by backtracking we can save substantial amount of cost, though it will require some additional walking and measurements. However, we notice that λh is always equal to or very close to λ∗. This shows that this model-free heuristic policy can perform as a very good suboptimal policy.
IX. CONCLUSION
In this paper, we have developed several approaches for as-you-go deployment of wireless relay networks assuming very light trafﬁc, using on-line measurements, and permitting backtracking. Each problem was formulated as an MDP and its optimal policy structure was studied. Numerical results have been provided to illustrate the performance and tradeoffs, and a nice heuristic policy was proposed for the average cost per step problem with backtracking. This work can be extended in several ways: (i) We could design a more robust network by asking for each relay to have multiple neighbours, (ii) It may be noted that even though our design approach assumes the lone packet trafﬁc model, the network thus obtained will be able to carry a certain amount of positive trafﬁc. Can the design process be modiﬁed to increase network capacity? All these aspects are problems that we are currently pursuing.
APPENDIX A IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITHOUT BACKTRACKING: SUM
POWER AND SUM OUTAGE OBJECTIVE
Proof of Lemma 1 Here we have an inﬁnite horizon total cost MDP with ﬁnite state space and ﬁnite action space. The assumption P of Chapter 3 in [9] is satisﬁed since the singlestage cost is nonnegative. Hence, by combining Proposition 3.1.5 and Proposition 3.1.6 of [9], we obtain the result.
Proof of Lemma 2 Note that the function J(0)(·) := 0 satisﬁes all the assertions. Let us assume, as our induction hypothesis, that J(k)(·) satisﬁes all the assertions. Now Pout(r, γ, w) is increasing in r and decreasing in w (by our channel modeling assumptions in Section II-B), and the single stage costs are linear (hence concave) increasing in ξr, ξo.

Then from the value iteration (6), J(k+1)(r, w) is pointwise minimum of functions which are increasing in r, ξo and ξr, decreasing in w, and jointly concave in ξo and ξr. Similarly, J(k+1)(0) is also pointwise minimum of functions which are increasing and jointly concave in ξr and ξo. Hence, the assertions hold for J (k+1)(0). Since J (k)(·) ↑ J (·), the results follow.
Proof of Theorem 1 Consider the Bellman equation (5). We will place a relay at state (r, w) iff the cost of placing a relay, i.e., minγ∈S (γ + ξoPout(r, γ, w)) + ξr + J (0) is less than or equal to the cost of not placing, i.e., θEW minγ∈S (γ + ξoPout(r + 1, γ, W )) + (1 − θ)EW J(r + 1, W ). Hence, it is obvious that we will place a relay at state (r, w) iff minγ∈S (γ + ξoPout(r, γ, w)) ≤ cth(r) where the threshold cth(r) is given by:
cth(r) = θEW min(γ + ξoPout(r + 1, γ, W ))
γ∈S
+(1 − θ)EW J(r + 1, W ) − (ξr + J(0))(26)
By Proposition 3.1.3 of [9], if there exists a stationary policy {µ, µ, · · · } such that for each state, the action chosen by the policy is the action that achieves the minimum in the Bellman equation, then that stationary policy will be an optimal policy, i.e., the minimizer in Bellman equation gives the optimal action. Hence, if the decision is to place a relay at state (r, w), then the power has to be chosen as
argminγ∈S γ + ξoPout(r, γ, w) .
Since Pout(r, γ, w) and J(r, w) is increasing in r for each γ, w, it is easy to see that cth(r) is increasing in r.
APPENDIX B IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITHOUT BACKTRACKING: MAX
POWER AND SUM OUTAGE OBJECTIVE
Proof of Lemma 4 Note that the function J(0)(·) := 0 satisﬁes all the assertions. Let us assume, as our induction hypothesis, that J(k)(·) satisﬁes all the assertions. Now Pout(r, γ, w) is increasing in r and decreasing in w (by our channel modeling assumptions in Section II-B), and the single stage costs are linear (hence concave) increasing in ξr, ξo and also increasing in γmax. Then from the value iteration (9), J (k+1)(r, w, γmax) is pointwise minimum of functions which are increasing in r, γmax, ξo and ξr, decreasing in w, and jointly concave in ξo and ξr. J (k+1)(0; γmax) is the sum of pointwise minimum of functions which are increasing and jointly concave in ξr and ξo. J (k+1)(0; γmax) is the sum of increasing functions of γmax. Hence, the assertions hold for J (k+1)(0; γmax). Since J (k)(·) ↑ J (·), the results follow.
Proof of Theorem 2 Consider the Bellman equation (8). We will place a relay at state (r, w, γmax) iff the
cost of placing a relay cp := minγ∈S ξoPout(r, γ, w) +
ξr + J(0; max{γ, γmax}) , is less than or equal to the

x=z+1

x=z+l+1

x=0

x=z

x = yl;w(0:z+1+l) x = z + l

Fig. 6. A diagram illustrating the idea behind the proof of Lemma 7.

cost of not placing cnp := θEW minγ∈S max{γ, γmax} +

ξoPout(r + 1, γ, W ) + (1 − θ)EW J (r + 1, W, γmax).

This yields the condition that argminγ∈S ξoPout(r, γ, w) +

J (0; max{γ, γmax}) ≤ cnp − ξr := cth(r, γmax). Since
cnp is increasing in r and γmax, cth(r, γmax) increases in r, γmax. Also, the minimizer in Bellman equation gives the optimal action (by the same arguments as in the proof of Theorem 1). Hence, if the decision is to place a relay at state (r, w, γmax), then the power has to be chosen as
argminγ∈S ξoPout(r, γ, w) + ξr + J (0; max{γ, γmax}) .

APPENDIX C IMPROMPTU DEPLOYMENT FOR GEOMETRICALLY DISTRIBUTED LENGTH WITH BACKTRACKING: SUM
POWER AND SUM OUTAGE OBJECTIVE

Proof of Lemma 7 We will show that J(z+1; 0) ≥ J(z; 0).

Consider two instances of the deployment process where

the agent has placed a relay at his current location. The

deployment over the remaining part of the line depends on

two things: (i) the shadowing realizations in all the links

over the rest of the line, (ii) the residual length of the line. Suppose that the deployment is being done along x-axis and that the current location of the deployment agent is x = 0

in both instances. For the ﬁrst instance the residual length of the line is (z + 1) + L1 where L1 ∼ Geometric(θ) and for the second instance the residual length is (z + L2) where L2 ∼ Geometric(θ).

Let us denote the optimal policy for the ﬁrst instance by

µ∗z+1 and that for the second instance by µ∗z. We will prove that

J (z + 1; 0) := Jµ∗ (z + 1; 0) ≥ Jµ∗ (z; 0) ≥ Jµ∗ (z; 0) =:

z+1

z+1

z

J(z; 0), where Jµ(s) is the cost-to-go under policy µ from

the state s.

Note that any pair of potential relay locations of the form {(i, j) : i > j, i ∈ {0, 1, 2, · · · }, j ∈ {0, 1, 2, · · · }, |i − j| ≤ (A + B)} is a possible link. Let us denote the shadowing component of the path-loss over in link (i, j) by wi,j. Let us

denote the collection of the random shadowing of all possible links emanating from and ending at {j, j + 1, · · · , i} (i > j)

by W(j:i), and that of all possible links emanating from the segment {j, j + 1, · · · , i} and ending at {j1, j1 + 1, · · · , i1}
by W(j:i);(j1:i1). Let us also denote the realizations of these collection of random variables by w(j:i) and w(j:i);(j1:i1) respectively. Now, for L1 = l and the realization of the
shadowing w(0:z+1+l) of all possible links, let us denote

the location of the last placed relay in the ﬁrst instance (excluding the source at x = z + 1 + l) under policy µ∗z+1 by x = yl;w(0:z+1+l) , and the cost incurred over the links solely in the locations {j, j + 1, · · · , i} by c(l; w(0:z+1+l); j : i).
The idea behind the proof is as follows. Consider Figure 6
which depicts the two instances of the problem. Consider the case where we ﬁx L1 = l, L2 = l and the shadowing of all possible links between x = 0 and x = z + 1 + l are also
ﬁxed and they are the same for both instances. Note that the
location of the last placed relay (before the source) for the ﬁrst instance under the policy µ∗z+1 is denoted by y . l;w(0:z+1+l) If yl;w(0:z+1+l) = z + l, then the source placed at x = z + l in the second instance will use the same transmit power as used by the relay placed at x = z + l in the ﬁrst instance; in fact, in the region between x = 0 and x = z + l we will
have the same placement locations, power and outage costs
in both instances. But then there will be an extra link in the ﬁrst instance from x = z + l + 1 to x = z + l, and hence
the ﬁrst instance will have more cost. On the other hand, if yl;w(0:z+1+l) < z + l, then in the region between x = 0 and x = yl;w(0:z+1+l) we will have the same power and outage cost and same placement locations in both instances. But the last link in the ﬁrst instance has length (z + 1 + l − yl;w(0:z+1+l) ), and that in the second instance has length (z+l−yl;w(0:z+1+l) ). If we now take expectation of the costs of these two links in
two different cases over the shadowing in all possible links between x = yl;w(0:z+1+l) and x = z + 1 + l, then the link in the ﬁrst instance will have higher expected cost since it is longer. This will happen for every possible values of l and yl;w(0:z+1+l) .
Now we will formally prove this lemma. By total probability
theorem, we can write,

Jµ∗z+1 (z + 1; 0)

∞

=

(1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

× c(l; w(0:z+1+l); 0 : yl;w(0:z+1+l) ) +

c(l; w(0:z+1+l); yl;w(0:z+1+l) : z + 1 + l)

∞

=

(1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

z+l

×

I(yl;w(0:z+1+l) = k)

k=1

c(l; w(0:z+1+l); 0 : k) +

c(l; w(0:z+1+l); k : z + 1 + l)

∞

=

(1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

z+l−1

×

I(yl;w(0:z+1+l) = k)

k=1

c(l; w(0:z+1+l); 0 : k) +

c(l; w(0:z+1+l); k : z + 1 + l)

∞

+ (1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

×I(yl;w(0:z+1+l) = z + l) c(l; w(0:z+1+l); 0 : z + l) +

c(l; w(0:z+1+l); z + l : z + 1 + l)

(27)

On the other hand,

Jµ∗z+1 (z; 0)

∞

=

(1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

z+l−1

×

I(yl;w(0:z+1+l) = k)

k=1

c(l; w(0:z+1+l); 0 : k) +

c(l; w(0:z+1+l); k : z + l)

∞

+ (1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

×I(yl;w(0:z+1+l) = z + l) c(l; w(0:z+1+l); 0 : z + l) (28)

Now, note that, the deployment upto the last relay does
not depend on the shadowing in the links emanating from the locations x > y . l;w(0:z+1+l) Hence, for any realization of the shadowing in all potential links, c(l; w(0:z+1+l); 0 : k) = c(l; w0:z+l; 0 : k) for all k ≤ z + l − 1. Note that I(E) is the indicator of the event E; its vale is equal to 1 if the event E occurs, or 0 otherwise.
Hence, we obtain,

Jµ∗z+1 (z + 1; 0) − Jµ∗z+1 (z; 0)

∞

=

(1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

z+l−1

×

I(yl;w(0:z+1+l) = k)

k=1

c(l; w(0:z+1+l); k : z + 1 + l)

−c(l; w(0:z+1+l); k : z + l)

∞

+ (1 − θ)l−1θ

pW(0:z+1+l) (w(0:z+1+l))

l=1

w(0:z+1+l)

×I(yl;w(0:z+1+l) = z + l)c(l; w(0:z+1+l); z + l : z + 1 + l)

(29)

Now,

pW(0:z+1+l) (w(0:z+1+l))I(yl;w(0:z+1+l) = k)
w(0:z+1+l)
c(l; w(0:z+1+l); k : z + 1 + l) − c(l; w(0:z+1+l); k : z + l)

=

pW(0:z+1+l) (w(0:z+1+l))I(yl;w(0:z+1+l) = k)

w(0:z+1+l)

min(γ + ξoPout(z + 1 + l − k, γ, wz+1+l,z+1+l−k)
γ∈S

− min(γ + ξoPout(z + l − k, γ, wz+l,z+l−k))

(30)

γ∈S

Since Pout(r, γ, w) is increasing in r for each γ, w, we

must have the expression in (30) greater than or equal to

0. Also c(l; w(0:z+1+l); z + l : z + 1 + l) in (29) is always

nonnegative. Hence, Jµ∗ (z+1; 0) ≥ Jµ∗ (z; 0). Now, since

z+1

z+1

Jµ∗ (z; 0) ≥ Jµ∗ (z; 0), the result follows.

z+1

z

APPENDIX D AVERAGE COST PER STEP: WITH AND WITHOUT
BACKTRACKING

Proof of Theorem tion of the functions

7 Recall µ(1) and

the deﬁniµ(2). Now,

ξr + w g(w) µ(2)(w)+ξoPout(µ(1)(w),µ(2)(w),wµ(1)(w))

g (w)µ(1) (w)

is

w

the average cost of a speciﬁc stationary deterministic policy

µ (by the Renewal Reward Theorem, since the placement

process regenerates at each placement point). Hence,

[2] M. Howard, M.J. Mataric´, and S. Sukhat Gaurav. An incremental selfdeployment algorithm for mobile sensor networks. Kluwer Autonomous Robots, 13(2):113–126, 2002.
[3] M.R. Souryal, J. Geissbuehler, L.E. Miller, and N. Moayeri. Real-time deployment of multihop relays for range extension. In Proc. of the International Conference on Mobile Systems, Applications, and Services (MobiSys), pages 85–98. ACM, 2007.
[4] Thorsten Aurisch and Jens To¨lle. Relay Placement for Ad-hoc Networks in Crisis and Emergency Scenarios. In Proc. of the Information Systems and Technology Panel (IST) Symposium. NATO Science and Technology Organization, 2009.
[5] H. Liu, J. Li, Z. Xie, S. Lin, K. Whitehouse, J. A. Stankovic, and D. Siu. Automatic and robust breadcrumb system deployment for indoor ﬁreﬁghter applications. In Proc. of the International Conference on Mobile Systems, Applications, and Services (MobiSys). ACM, 2010.
[6] A. Sinha, A. Chattopadhyay, K.P. Naveen, M. Coupechoux, and A. Kumar. Optimal sequential wireless relay placement on a random lattice path. http://arxiv.org/abs/1207.6318.
[7] Piyush Agrawal and Neal Patwari. Correlated link shadow fading in multi-hop wireless networks. http://arxiv.org/abs/0804.2708.
[8] Matthias Vodel and Wolfram Hardt. Energy-efﬁcient communication in distributed, embedded systems. In Proc. of the 9th International Workshop on Resource Allocation, Cooperation and Competition in Wireless Networks (RAWNET), in conjunction with IEEE WiOpt. IEEE, 2013.
[9] D.P. Bertsekas. Dynamic Programming and Optimal Control, Vol. II. Athena Scientiﬁc, 2007.
[10] H.C. Tijms. A First Course in Stochastic Models. WILEY, 2003.

ξr + λ∗ = inf
µ

w g(w) µ(2)(w) + ξoPout(µ(1)(w), µ(2)(w), wµ(1)(w)) w g(w)µ(1)(w)

For each policy (µ(1), µ(2)), the numerator is linear, increasing in ξr and ξo and the denominator is independent of ξr and ξo. The proof follows immediately since the pointwise inﬁmum of increasing, linear functions of ξr and ξo is increasing and jointly concave in ξr and ξo.
Proof of Theorem 8 Note that for the average cost problem
with no backtracking, there exists an optimal threshold policy
(similar to Theorem 1), since the optimal policy for problem (4) achieves λ average cost per step for θ sufﬁciently close to 0. So, let one such optimal policy be given by the set of thresholds {cth(r)}A+1≤r≤A+B−1.
Now, let us consider the average cost minimization problem
with backtracking. Consider the policy where we ﬁrst measure wA+1, wA+2, · · · , wA+B and decide to place a relay u steps away from the previous relay (where A + 1 ≤ u ≤ A + B − 1) if minγ∈S (γ + ξoPout(r, γ, wr)) > cth(r) for all r ≤ (u − 1) and minγ∈S (γ + ξoPout(u, γ, wu)) ≤ cth(u). We must place if we reach at a distance (A + B) from the previous relay. But
this is a particular policy for the problem where we gather wA+1, wA+2, · · · , wA+B and then decide where to place the relay, and clearly the average cost per step for this policy is λ which cannot be less than the optimal average cost λ∗.

REFERENCES
[1] A. Chattopadhyay, M. Coupechoux, and A. Kumar. Measurement based impromptu deployment of a multi-hop wireless relay network. In Proc. of the 11th Intl. Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt). IEEE, 2013.

