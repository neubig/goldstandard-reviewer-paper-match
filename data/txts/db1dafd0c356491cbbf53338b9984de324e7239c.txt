Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces
Barun Patra∗, Joel Ruben Antony Moniz∗, Sarthak Garg∗, Matthew R. Gormley, Graham Neubig Carnegie Mellon University
{bpatra, jrmoniz, sarthakg, mgormley, gneubig}@cs.cmu.edu

arXiv:1908.06625v1 [cs.CL] 19 Aug 2019

Abstract
Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) — a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness ﬁltering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.∗
1 Introduction
Bilingual lexicon induction (BLI), the task of ﬁnding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), ﬁnds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classiﬁcation (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018).
Most work on BLI uses methods that learn a mapping between two word embedding spaces
∗Equal Contribution ∗Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS.

(Ruder, 2017), which makes it possible to leverage pre-trained embeddings learned on large monolingual corpora. A commonly used method for BLI, which is also empirically effective, involves learning an orthogonal mapping between the two embedding spaces (Mikolov et al. (2013a), Xing et al. (2015), Artetxe et al. (2016), Smith et al. (2017)). However, learning an orthogonal mapping inherently assumes that the embedding spaces for the two languages are isometric (subsequently referred to as the orthogonality assumption). This is a particularly strong assumption that may not necessarily hold true, and consequently we can expect methods relying on this assumption to provide sub-optimal results. In this work, we examine this assumption, identify where it breaks down, and propose a method to alleviate this problem.
We ﬁrst present a theoretically motivated approach based on the Gromov-Hausdroff (GH) distance to check the extent to which the orthogonality assumption holds (§2). We show that the constraint indeed does not hold, particularly for etymologically and typologically distant language pairs. Motivated by the above observation, we then propose a framework for Bilingual Lexicon Induction with Semi-Supervision (BLISS) (§3.2) Besides addressing the limitations of the orthogonality assumption, BLISS also addresses the shortcomings of purely supervised and purely unsupervised methods for BLI (§3.1). Our framework jointly optimizes for supervised embedding alignment, unsupervised distribution matching, and a weak orthogonality constraint in the form of a back-translation loss. Our results show that the different losses work in tandem to learn a better mapping than any one can on its own (§4.2). In particular, we show that two instantiations of the semi-supervised framework, corresponding to different supervised loss objectives, outperform their supervised and unsupervised counterparts on nu-

merous language pairs across two datasets. Our best model outperforms the state-of-the-art on 10 of 16 language pairs on the MUSE datasets.
Our analysis (§4.4) demonstrates that adding supervision to the learning objective, even in the form of a small seed dictionary, substantially improves the stability of the learning procedure. In particular, for cases where either the embedding spaces are far apart according to GH distance or the quality of the original embeddings is poor, our framework converges where the unsupervised baselines fail to. We also show that for the same amount of available supervised data, leveraging unsupervised learning allows us to obtain superior performance over baseline supervised, semisupervised and unsupervised methods.
2 Isometry of Embedding Spaces
Both supervised and unsupervised BLI often rely on the assumption that the word embedding spaces are isometric to each other. Thus, they learn an orthogonal mapping matrix to map one space to another Xing et al. (2015).
This orthogonality assumption might not always hold, particularly for the cases when the language pairs in consideration are etymologically distant — Zhang et al. (2017b) and Søgaard et al. (2018) provide evidence of this by observing a higher Earth Mover’s distance and eigenvector similarity metric respectively between etymologically distant languages. In this work, we propose a novel way of a-priori analyzing the validity of the orthogonality assumption using the Gromov Hausdorff (GH) distance to check how well two language embedding spaces can be aligned under an isometric transformation†.
The Hausdorff distance between two metric spaces is a measure of the worst case or the diametric distance between the spaces. Intuitively, it measures the distance between the nearest neighbours that are the farthest apart. Concretely, given two metric spaces X , and Y with a distance function d(., .), the Hausdorff distance is deﬁned as:
H(X , Y) = max{ sup inf d(x, y),
x∈X y∈Y
(1) sup inf d(x, y) }.
y∈Y x∈X
The Gromov-Hausdorff distance minimizes the Hausdorff distance over all isometric transforms
†Note that since we mean center the embeddings, the orthogonal transforms are equivalent to isometric transforms

between X and Y, thereby providing a quantitative estimate of the isometry of two spaces
H(X , Y) = inf H(f (X ), g(Y)), (2)
f,g
where f, g belong to set of isometric transforms. Computing the Gromov-Hausdorff distance
involves solving hard combinatorial problems, which is intractable in general. Following Chazal et al. (2009), we approximate it by computing the Bottleneck distance between the two metric spaces (the details of which can be found in Appendix (§A.1)). As can be observed from Table 2, the GH distances are higher for etymologically distant language pairs.
3 Semi-supervised Framework
In this section, we motivate and deﬁne our semisupervised framework for BLI. First we describe issues with purely supervised and unsupervised methods, and then lay the framework for tackling them along with orthogonality constraints.
3.1 Drawbacks of Purely Supervised and Unsupervised Methods
Most purely supervised methods for BLI just use words in an aligned bilingual dictionary and do not utilize the rich information present in the topology of the embeddings’ space. Purely unsupervised methods, on the other hand, can suffer from poor performance if the distribution of the embedding spaces of the two languages are very different from each other. Moreover, unsupervised methods can successfully align clusters of words, but miss out on ﬁne grained alignment within the clusters.
We explicitly show the aforementioned problem of purely unsupervised methods with the help of the toy dataset shown in 1a, and 1b. In this dataset, due to the density difference between the two large blue clusters, unsupervised matching is consistently able to align them properly, but has trouble aligning the smaller embedded green and red sub-clusters. The correct transformation of the source space is a clockwise 90◦ rotation followed by reﬂection along the x-axis. Unsupervised matching converges to this correct transformation only half of the time; in rest of the cases, it ignores the alignment of the sub-clusters and converges to a 90◦ counter-clockwise transformation as shown in 1c.
We also ﬁnd evidence of this problem in the real datasets used in our experiments as shown in Ta-

(a) Source distribution

(b) Target distribution

(c) Misaligned source distribution

Figure 1: A toy dataset demonstrating the shortcomings of unsupervised distribution matching. Fig. a) and b) show two different distributions (source and target respectively) over six classes. Classes 1 and 2; classes 3 and 4; classes 5 and 6 were respectively drawn from a uniform distribution over a sphere, rectangle and triangle respectively. Fig. c) shows the misprojected source distribution obtained from unsupervised distribution matching which fails to align with the target distribution of Fig. b).

Source → Target Incorrect Predicted

aunt → тетя бабушка (Grandmother)

uruguay → уругвая аргентины (Argentina)

regiments → полков кавалерийские (Cavalry)

comedian → комик

актёр (Actor)

Table 1: Words for which semi-supervised method predicts correctly, but unsupervised method doesn’t. The unsupervised method is able to guess the general family but fails to pinpoint exact match.

ble 1. It can be seen that the unsupervised method aligns clusters of similar words, but is poor at ﬁne grained alignment. We hypothesize that this problem can be resolved by giving it some supervision in the form of matching anchor points inside these sub-clusters, which correctly aligns them. Analogously, for the task of BLI, generating a small supervised seed lexicon for providing the requisite supervision, is generally feasible for most language pairs, through bilingual speakers, existing dictionary resources, or Wikipedia language links.

3.2 A Semi-supervised Framework

In order to alleviate the problems with the or-

thogonality constraints, the purely unsupervised

and supervised approaches, we propose a semi-

supervised framework, described below.

Let X = {x1 . . . xn} and Y = {y1 . . . ym}, xi, yi ∈ Rd be two sets of word embeddings from

the source and target language respectively and let

S

=

{

(x

s 1

,

y1s

)

.

.

.

(x

s k

,

yks

)

}

denote

the

bilingual

aligned word embeddings.

For learning a linear mapping matrix W that

maps X to Y we leverage unsupervised distribution matching, aligning known word pairs and a data-driven weak orthogonality constraint.
Unsupervised Distribution Matching: Given all word embeddings X and Y, the unsupervised loss LW |D aims to match the distribution of both embedding spaces. In particular, for our formulation, we use an adversarial distribution matching objective, similar to the work of Lample et al. (2018). Speciﬁcally, a mapping matrix W from the source to the target is learned to fool a discriminator D, which is trained to distinguish between the mapped source embeddings W X = {W x1 . . . W xn} and Y. We parameterize our discriminator with an MLP, and alternatively optimize the mapping matrix and the discriminator with the corresponding objectives:

1

LD|W

=

− n

log(1 − D(W xi))

xi∈X

1

− m

log D(xi)

(3)

xi∈Y

1

LW |D

=

− n

log D(W xi)

xi∈X

Aligning Known Word Pairs: Given aligned bilingual word embeddings S, we aim to minimize a similarity function (fs) which maximizes the similarity between the corresponding matched pairs of words. Speciﬁcally, the loss is deﬁned as:

1 LW |S = −

fs(W xsi , yis) (4)

|S |

(xsi ,yis)∈S

Weak Orthogonality Constraint: Given an
embedding space X , we deﬁne a consistency loss
that maximizes a similarity function fa between x and W T W x, x ∈ X . This cyclic consistency loss
LW|O encourages orthogonality of the W matrix based on the joint optimization:

1 LW |O = −

fa(xi, W T W xi) (5)

|X |

xi∈X

The above loss term, used in conjunction with

the supervised and unsupervised losses, allows the

model to adjust the trade-off between orthogo-

nality and accuracy based on the joint optimiza-

tion. This is particularly helpful in the embedding

spaces where the orthogonality constraint is vio-

lated (§4.4). Moreover, this data driven orthogo-

nality constraint is more robust than an enforced

hard constraint (A.3).

The ﬁnal loss function for the mapping matrix

is:

L = LW |D + LW |S + LW |O

(6)

LW |D enables the model to leverage the distributional information available from the two embedding spaces, thereby using all available monolingual data. On the other hand, LW |S allows for the correct alignment of labeled pairs when available in the form of a small seed dictionary. Finally, LW |O encourages orthogonality. One can think of LW |O and LW |S as working against each other when the spaces are not isometric. Jointly optimizing both helps the model to strike a balance between them in a data driven manner, encouraging orthogonality but still allowing for ﬂexible mapping.

3.3 Nearest Neighbor Retrieval
For NN lookup, we use the CSLS distance deﬁned by Lample et al. (2018). Let ΓA(b) be the average cosine similarity between b and it’s k-NN in A. Then CSLS is deﬁned as CSLS(x, y) = 2cos(W x, y) − ΓY (W x) − ΓW X (y).∗.

3.4 Iterative Procrustes Reﬁnement and Hubness Mitigation
A common method of improving BLI is iteratively expanding the dictionary and reﬁning the mapping matrix as a post-processing step (Artetxe et al., 2017; Lample et al., 2018). Given a learnt mapping matrix, Procrustes reﬁnement ﬁrst ﬁnds
∗W X denotes the set {W x : x ∈ X }

the pair of points in the two languages that are very closely matched by the mapping matrix and constructs a bilingual dictionary from these pairs. These pair of points are found by considering the nearest neighbors (NN) of the projected source words in the target space. The mapping matrix is then reﬁned by setting it to be the Procrustes solution of the dictionary obtained. Iterative Procrustes Reﬁnement (also referred as Iterative Dictionary Expansion) applies the above step iteratively.
However, learning an orthogonal linear map in such a way leads to some words (known as hubs) to become nearest neighbors of a majority of other words (Radovanovic´ et al., 2010; Dinu and Baroni, 2014). In order to estimate the hubness of a point, (Radovanovic´ et al., 2010) ﬁrst compute Nx(k), the counts of all points y such that x ∈ k−N N (y), normalized over all k. The skewness of the distribution over Nx(k) is deﬁned as the hubness of the point, with positive skew representing hubs and negative skew representing isolated points. An approximation to this would be Nx(1), i.e the number of points for which x is the nearest neighbor.
We use a simple hubness ﬁltering mechanism to ﬁlter out words in the target domain that are hubs, i.e., words in the target domain which have more than a threshold number of neighbors in the source domain are not considered in the iterative dictionary expansion. Empirically, this leads to a small boost in performance. In our models, we use iterative Procrustes reﬁnement with hubness ﬁltering at each reﬁnement step.
4 Experiments and Results
In this section, we measure the GH distances between embedding spaces of various language pairs, and compute their correlation with several empirical measures of orthogonality. Next, we analyze the performance of the instantiations of our semi-supervised framework for two settings of supervised losses, and show that they outperform their supervised and unsupervised counterparts for a majority of the language pairs. Finally we analyze our performance with varying amounts of supervision and highlight the framework’s training stability over unsupervised methods.
4.1 Empirical Evaluation of GH Distance
To evaluate the lower bound on the GH distance between the two embedding spaces, we select the

GH Λ
MUSE(U) RCSLS GeoMM BLISS(R)
||I − W T W ||2

ru-uk en-fr en-es es-fr en-uk en-ru en-sv en-el en-hi en-ko
0.18 0.17 0.2 0.24 0.34 0.44 0.46 0.47 0.5 0.92 16.4 4.1 5.9 4.1 11.7 14.7 7.3 11.5 7.7 6.6
* 82.3 81.7 85.5 29.1 44.0 53.3 37.9 34.6 5.1 * 83.3 84.1 87.1 38.3 57.9 61.7 47.6 37.3 37.5 * 82.1 81.4 87.8 39.1 51.3 65.0 47.8 39.8 34.6 * 83.9 84.3 87.1 40.7 57.1 65.1 48.5 38.1 39.9
0.03 0.01 0.03 0.02 59.8 54.3 71.6 72.6 106.3 98.46

|Corr| |Corr| (GH) (Λ)

*

*

**

0.87 0.61 0.74 0.52 0.76 0.49 0.73 0.50

0.84 0.75

Table 2: Correlation of GH and Eigenvector similarity with performance of BLI methods. Bold marks best metrics.

5000 most frequent words of the source and target language and compute the Bottleneck distance. These embeddings are mean centered, unit normed and the Euclidean distance is used as the distance metric.
Row 1 of Table 2 summarizes the GH distances obtained for different language pairs. We ﬁnd that etymologically close languages such as en-fr and ru-uk have a very low GH distance and can possibly be aligned well using orthogonal transforms. In contrast, we ﬁnd that etymologically distant language pairs such as en-ru and en-hi cannot be aligned well using orthogonal transforms.
To further corroborate this, similar to Søgaard et al. (2018) , we compute correlations of the GH distance with the accuracies of several methods for BLI. We ﬁnd that the GH distance exhibits a strong negative correlation with these accuracies, implying that as the GH distance increases, it becomes increasingly difﬁcult to align these language pairs. Søgaard et al. (2018) proposed the eigenvector similarity metric between embedding spaces for measuring similarity between the embedding spaces. We compute their metric over top n (100, 500, 1000, 5000 and 10000) embeddings (Column Λ in Table 2 shows correlation for the best setting of n) and show that the GH distance (Column GH) correlates better with the accuracies than eigenvector similarity. Furthermore, we also compute correlations against an empirical measure of the orthogonality of two embedding spaces by computing ||I − W T W ||2, where W is a mapping from one language to the other obtained from an unsupervised method (MUSE(U)). Note that an advantage of this metric is that it can be computed even when the supervised dictionaries are not available (ru-uk in Table 2). We obtain a strong correlation with this metric as well.

4.2 Benchmark Tasks: Setup
Baseline Methods
MUSE (U/S/IR): Lample et al. (2018) proposed two models: MUSE(U) and MUSE(S) for unsupervised and supervised BLI respectively. MUSE(U) uses a GAN based distribution matching followed by iterative Procrustes reﬁnement. MUSE(S) learns an orthogonal map between the embedding spaces by minimizing the Euclidean distance between the supervised translation pairs. Note that for unit normed embedding spaces, this is equivalent to maximizing the cosine similarity between these pairs. MUSE(IR) is the semisupervised extension of MUSE(S), which uses iterative reﬁnement using the CSLS distance starting from the mapping learnt by MUSE(S). We also use our proposed hubness ﬁltering technique during the iterative reﬁnement process (MUSE(HR)) which leads to small performance improvements. We consequently use the hubness ﬁltering technique in all our models.
RCSLS: Joulin et al. (2018) propose optimizing the CSLS distance‡ directly for the supervised matching pairs. This leads to signiﬁcant improvements over MUSE(S) and achieves state of the art results for a majority of the language pairs at the time of writing.
VecMap models: Artetxe et al. (2017) and Artetxe et al. (2018a) proposed two models, VecMap and VecMap++ which were based on Iterative Procrustes reﬁnement starting from a small seed lexicon based on numeral matching.
We also compare against two well known methods GeoMM (Jawanpuria et al., 2019) and Vecmap (U )++ (Artetxe et al., 2018b). These methods learn orthogonal mappings for both source and target spaces to a common embedding space, and
‡Since the CSLS distance requires computing the nearest neighbors over the whole embedding space, this can also be considered a semi-supervised method.

subsequently translate in the common space.
BLISS models
We instantiate two instances of our framework corresponding to the two supervised losses in the baseline methods mentioned above. BLISS(M) optimizes the cosine distance between supervised matching pairs as its supervised loss (LW |S), while BLISS(R) optimizes the CSLS distance between these matching pairs for its LW |S. We use the unsupervised CSLS metric as a stopping criterion during training. This metric, introduced by Lample et al. (2018), computes the average cosine similarity between matched source-target pairs using the CSLS distance for NN retrieval; and the authors showed that this correlates well with ground truth accuracy.
After learning the ﬁnal mapping matrix, the translations of the words in the source language are mapped to the target space and their nearest neighbors according to the CSLS distance are chosen as the translations.
Datasets
We evaluate our models against baselines on two popularly used datasets: the MUSE dataset and the VecMap dataset. The MUSE dataset used by Lample et al. (2018) consists of embeddings trained by Bojanowski et al. (2017) on Wikipedia and bilingual dictionaries generated by internal translation tools used at Facebook. The VecMap dataset introduced by Dinu and Baroni (2014) consists of the CBOW embeddings trained on the WacKy crawling corpora. The bilingual dictionaries were obtained from the Europarl word alignments. We use the standard training and test splits available for both the datasets.
4.3 Benchmark Tasks: Results
In Tables 3 and 4, we group the instantiations of BLISS(M/R) with it’s supervised counterparts. We use † to compare models within a group, and use bold do compare across different groups for a language pair.
As can be seen from Table 3, BLISS(M/R) outperform baseline methods within their groups for 9 of 10 language pairs. Moreover BLISS(R) gives the best accuracy across all baseline methods for 6 out of 10 language pairs.
We observe a similar trend for the VecMap datasets, where BLISS(M/R) outperforms its supervised and unsupervised counterparts (Table 4).

It can be seen that BLISS(M) and BLISS(R) outperform the MUSE baselines (MUSE(U), MUSE(R)) and RCSLS respectively.
We observe that GeoMM and VecMap(U)++ outperform BLISS models on the VecMap datasets. A potential reason for this could be the slight disadvantage that BLISS suffers from because of translating in the target space, as opposed to in the common embedding space. This hypothesis is also supported by the results of Kementchedjhieva et al. (2018).
All the hyperparameters for the experiments can be found in the Appendix (§A.4)
4.4 Beneﬁts of BLISS
Languages with high GH distance: As can be seen from Table 2, BLISS(R) substantially outperforms RCSLS on 6 of 9 language pairs, especially when the GH distance between the pairs is high (en-uk (2.4%), en-sv (3.4%), en-el (0.9%), en-hi(0.8%), en-ko (2.4%)). Results from Table 3 also underscores this point, wherein BLISS(R) performs at least at par with (and often better than) RCSLS on European languages, and performs signiﬁcantly better on en-zh (2.8%) and zhen (0.9%).
Performance with varying amount of supervision: Table 5 shows the performance of BLISS(R) as a function of the number of data points provided for supervision. As can be observed, the model performs reasonably well even for low amounts of supervision and outperforms the unsupervised baseline MUSE(U) and it’s supervised counterpart RCSLS. Moreover, note that the difference is more prominent for the etymologically distant pair en↔zh. In this case the baseline models completely fail to train for 50 points, whereas BLISS(R) performs reasonably well.
Stability of Training: We also observe that providing even a little bit of supervision helps stabilize the training process, when compared to purely unsupervised distribution matching. We measure the stability during training using both the ground truth accuracy and the unsupervised CSLS metric. As can be seen from Figure 2, BLISS(M) is signiﬁcantly more stable than MUSE(U), converging to better accuracy and CSLS values. Furthermore, for en↔zh, Vecmap(U)++ fails to converge, while MUSE is somewhat unstable. However, BLISS does not suffer from this issue.
When the word vectors are not rich enough

Model Type MUSE(U) Unsup

Objective GAN

Translation Space
target

en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en 81.7 83.3 82.3 82.1 74.0 72.2 44.0 59.1 32.5 31.4

MUSE(S) MUSE(IR) MUSE(HR) BLISS(M)

Sup Semi Semi Semi

Cos Cos + IR Cos + IR Cos + GAN

target target target target

81.4
81.9 82.3† 82.3†

82.9
83.5
83.3 84.3†

81.1
82.1
82.5 83.3†

82.4
82.4
83.2 83.9†

73.5
74.3 75.7† 75.7†

72.4
72.7
72.8 73.8†

51.7
51.7
52.8 55.7†

63.7
63.7 64.1†
63.7

42.7† 42.7† 42.7†
41.1

36.7
36.7
36.7 41.4†

RCSLS BLISS(R)

Semi

CSLS

Semi CSLS + GAN

target target

84.1 86.3† 83.3 84.1 79.1† 76.3 57.9† 67.2 45.9 46.4 84.3† 86.2 83.9† 84.7† 79.1† 76.6† 57.1 67.7† 48.7† 47.3†

GeoMM

Sup Classiﬁcation Loss

Vecmap(U)++ Unsup NN Based Dist matching + IR

common common

81.4 85.5 82.1 84.1 74.7 76.7 51.3 67.6 49.1 45.3 82.2 84.5 82.5 83.6 75.2 74.2 48.5 65.1 0.0 0.0

Table 3: Performance comparison of BLISS on the MUSE dataset. Sup, Unsup and Semi refer to supervised, unsupervised and semi-supervised methods. Objective refers to the metric optimized. † marks the best in each category, while bold marks the best performance across all groups for a language pair.

Pairs se#eds MVeacp MVapec++ M(UUS)E M(IURS)E B(LMIS)S RCSLS BL(RIS) S GeoMM

all 39.7 45.3 45.8 45.3 45.9† 45.4 46.2† 48.3

en-it Num. 37.3 -

45.8† 0.7 44.3

0.3 44.6†

1.2

all 40.9 44.1 0.0 47.0 48.3† 47.3 48.1† 48.9

en-de Num. 39.6 -

0.0 39.9 47.2† 1.0 46.5† 2.3

Vec Map(U)++
48.5 48.5
48.1 48.1

Table 4: Performance of different models on the VecMap dataset. † marks the best in each category, while bold marks the best performance across different levels of supervision for a language pair.

# Datapoints

Model

en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en

*

MUSE(U)

81.7 83.3 82.3 82.1 74.0 72.2 44.0 59.1 32.5† 31.4†

*

Vecmap(U)++ 82.2† 84.5† 82.5† 83.6† 75.2† 74.2† 48.5† 65.1† 0.0 0.0

MUSE(IR)

0.3 82.7 0.5 1.6 31.9 72.7† 0.1 0.0 0.3 0.3

50

GeoMM

0.3 1.9 0.3 1.0 0.3 0.3 0.0 0.6 0.0 0.0

RCSLS

0.1 0.4 0.0 0.3 0.1 0.1 0.1 0.1 0.0 0.0

BLISS (R)

82.1† 83.6† 82.8† 83.0† 75.1† 72.7† 39.3† 61.0† 32.6† 32.5†

MUSE(IR)

81.6 83.5† 82.1 82.0 73.1 72.7 40.3 62 34.5 32.2

500

GeoMM

31.9 46.6 34.4 44.7 13.5 14.7 10.6 20.5 3.9 2.9

RCSLS

22.9 44.9 22.4 43.5 9.9 10.2 7.9 19.6 6.6 7.1

BLISS(R)

82.3† 83.4 82.3† 82.9† 74.7† 73.1† 41.6† 63.0† 36.3† 35.1†

5000

MUSE(IR) GeoMM RCSLS BLISS(R)

81.9
79.7 80.9 82.4†

82.8 82.2 82.1
82.7 79.9 83.2 82.9 80.4 82.5 84.9† 82.6† 83.9†

75.2
71.7 72.5 75.7†

72.4
70.6 70.9 72.5†

50.4
49.7 51.3 52.1†

63.7 65.5† 63.8
65.2

39.2 43.7† 42.5
42.5

36.3
40.1 41.9 42.8†

Table 5: Performance with different levels of supervision. † marks the best performance at a given level of supervision, while bold marks the best for a language pair.

(word2vec (Mikolov et al., 2013b) instead of fastText), the unsupervised method can completely fail to train. This can be observed for the case of en-de in Table 4. BLISS(M/R) does not face this problem: adding supervision, even in the form of 50 mapped words for the case of en-de, helps it to achieve reasonable performance.

5 Related Work
Mikolov et al. (2013a) ﬁrst used anchor points to align two embedding spaces, leveraging the fact that these spaces exhibit similar structure across languages. Since then, several approaches have been proposed for learning bilingual dictionaries (Faruqui and Dyer, 2014; Zou et al., 2013; Xing

Accuracy CSLS
Accuracy CSLS
Accuracy CSLS

70 60 50 40 30 20 10 0
0

CSLS: Unsup en-de 5 10 15
Acc: Unsup Epochs

CSLS: Semi
0.175 0.150 0.125 0.100 0.075 0.050 0.025 20 25 Acc: Semi

50 40 30 20 10 0
0

CSLS: Unsup en-ru 5 10 15
Acc: Unsup Epochs

CSLS: Semi
0.125 0.100 0.075 0.050 0.025 0.000 20 25 Acc: Semi

35 30 25 20 15 10 5
0

CSLS: Unsup en-zh 5 10 15
Acc: Unsup Epochs

CSLS: Semi
0.03 0.02 0.01 0.00 0.01 0.02 0.03 0.04 20 25 Acc: Semi

Figure 2: Training Stability of different language pairs (en-de), (en-ru), (en-zh)

et al., 2015). Xing et al. (2015) showed that adding an orthogonal constraint signiﬁcantly improves performance, and admits a closed form solution. This was further corroborated by the work of Smith et al. (2017), who showed that in orthogonality was necessary for self-consistency. Artetxe et al. (2016) showed the equivalence between the different methods, and their subsequent work (Artetxe et al., 2018a) analyzed different techniques proposed in various works (like embedding centering, whitening etc.), and showed that leveraging a combination of different methods showed signiﬁcant performance gains.
However, the validity of this orthogonality assumption has of late come into question: Zhang et al. (2017b) found that the Wasserstein distance between distant language pairs was considerably higher , while Søgaard et al. (2018) explored the orthogonality assumption using eigenvector similarity. We ﬁnd our weak orthogonality constraint (along the lines of Zhang et al. (2017a)) when used in our semi-supervised framework to be more robust to this.
There has also recently been an increasing focus on generating these bilingual mappings without an aligned bilingual dictionary, i.e., in an unsupervised manner. Zhang et al. (2017a) and Lample et al. (2018) both use adversarial training for aligning two monolingual embedding spaces without any seed lexicon, while Zhang et al. (2017b) used a Wasserstein GAN to achieve this adversarial alignment, and use an earth-mover based ﬁnetuning approach; while Grave et al. (2018) formulate this as a joint estimation of an orthogonal matrix and a permutation matrix. However, we show that adding a little supervision, which is usually easy to obtain, improves performance.
Another vein of research (Jawanpuria et al., 2019; Artetxe et al., 2018b; Kementchedjhieva et al., 2018) has been to learn orthogonal map-

pings from both the source and the target embedding spaces into a common embedding space and doing the translations in the common embedding space. Artetxe et al. (2017) and Søgaard et al. (2018) motivate the utility of using both the supervised seed dictionaries and, to some extent, the structure of the monolingual embedding spaces. They use iterative Procrustes reﬁnement starting with a small seed dictionary to learn a mapping; but doing may lead to sub-optimal performance for distant language pairs. However, these methods are close to our methods in spirit, and consequently form the baselines for our experiments.
Another avenue of research has been to try and modify the underlying embedding generation algorithms. Cao et al. (2016) modify the CBOW algorithm (Mikolov et al., 2013b) by augmenting the CBOW loss to match the ﬁrst and second order moments from the source and target latent spaces, thereby ensuring the source and target embedding spaces follow the same distribution. Luong et al. (2015), in their work, use the aligned words to jointly learn the embedding spaces of both the source and target language, by trying to predict the context of a word in the other language, given an alignment. An issue with the proposed method is that it requires the retraining of embeddings, and cannot leverage a rich collection of precomputed vectors (like ones provided by Word2Vec (Mikolov et al., 2013b), Glove (Pennington et al., 2014) and FastText (Bojanowski et al., 2017)).
6 Conclusions
In this work, we analyze the validity of the orthogonality assumption and show that it breaks for distant language pairs. We motivate the task of semisupervised BLI by showing the shortcomings of purely supervised and unsupervised approaches. We ﬁnally propose a semi-supervised framework which combines the advantages of supervised and

unsupervised approaches and uses a joint optimization loss to enforce a weak and ﬂexible orthogonality constraint. We provide two instantiations of our framework, and show that both outperform their supervised and unsupervised counterparts. On analyzing the model errors, we ﬁnd that a large fraction of them arise due to polysemy and antonymy (An interested reader can ﬁnd the details in Appendix (§A.2).
We also ﬁnd that translating in a common embedding space, as opposed to the target embedding space, obtains orthogonal gains for BLI, and plan on investigating this in the semi-supervised setting in future work.
Acknowledgements
We would like to thank Sebastian Ruder and Anders Søgaard for their assistance in helping with the computation the eigenvector similarity metric, Arjun Balgovind for his help in replicating the experiments of GeoMM, and Guillaume Lample for his help in replicating the experiments of MUSE. We would also like to thank Paul Michel and Junjie Hu for their invaluable feedback and discussions that helped shape the paper into its current form. Finally, we would also like to thank the anonymous reviewers for their valuable comments and helpful suggestions.
References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016. Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2289–2294.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462, Vancouver, Canada. Association for Computational Linguistics.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence (AAAI-18).
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018b. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of the 56th Annual Meeting of

the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 789–798.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135–146.
Hailong Cao, Tiejun Zhao, Shu Zhang, and Yao Meng. 2016. A distribution-based model to learn bilingual word embeddings. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1818– 1827.
Fre´de´ric Chazal, David Cohen-Steiner, Leonidas J Guibas, Facundo Me´moli, and Steve Y Oudot. 2009. Gromov-hausdorff stable signatures for shapes using persistence. In Computer Graphics Forum, volume 28, pages 1393–1403. Wiley Online Library.
Georgiana Dinu and Marco Baroni. 2014. Improving zero-shot learning by mitigating the hubness problem. volume abs/1412.6568.
Herbert Edelsbrunner and Dmitriy Morozov. 2013. Persistent homology: theory and practice.
Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–471.
Edouard Grave, Armand Joulin, and Quentin Berthet. 2018. Unsupervised alignment of embeddings with wasserstein procrustes. arXiv preprint arXiv:1805.11222.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL08: HLT, pages 771–779, Columbus, Ohio. Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013. Combining bilingual and comparable corpora for low resource machine translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 262–270, Soﬁa, Bulgaria. Association for Computational Linguistics.
Pratik Jawanpuria, Arjun Balgovind, Anoop Kunchukuttan, and Bamdev Mishra. 2019. Learning multilingual word embeddings in latent metric space: a geometric approach. Transaction of the Association for Computational Linguistics (TACL), 7:107–120.
Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Herve´ Je´gou, and Edouard Grave. 2018. Loss in translation: Learning bilingual word mapping with a retrieval criterion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2979–2984.

Yova Kementchedjhieva, Sebastian Ruder, Ryan Cotterell, and Anders Søgaard. 2018. Generalizing procrustes analysis for better bilingual dictionary induction. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 211–220.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. pages 1459–1474.
Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve´ Je´gou. 2018. Word translation without parallel data. In International Conference on Learning Representations.
Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 151–159.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.
Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 529–535, New Orleans, Louisiana. Association for Computational Linguistics.
Milosˇ Radovanovic´, Alexandros Nanopoulos, and Mirjana Ivanovic´. 2010. Hubs in space: Popular nearest neighbors in high-dimensional data. volume 11, pages 2487–2531.
Sebastian Ruder. 2017. A survey of cross-lingual embedding models. CoRR, abs/1706.04902.
Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Ofﬂine bilingual word vectors, orthogonal transformations and the inverted softmax.
Anders Søgaard, Sebastian Ruder, and Ivan Vulic´. 2018. On the limitations of unsupervised bilingual

dictionary induction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 778–788.
Min Xiao and Yuhong Guo. 2014. Distributed word representation learning for cross-lingual dependency parsing. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 119–129.
Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized word embedding and orthogonal transform for bilingual word translation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1006–1011.
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017a. Adversarial training for unsupervised bilingual lexicon induction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1959–1970.
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017b. Earth mover’s distance minimization for unsupervised bilingual lexicon induction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1934–1945.
Yuan Zhang, David Gaddy, Regina Barzilay, and Tommi Jaakkola. 2016. Ten pairs to tagmultilingual pos tagging via coarse mapping between embeddings. Association for Computational Linguistics.
Will Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398.

A Appendix
A.1 Details on Gromov Hausdorff
We brieﬂy outline the procedure for computing the Bottleneck distance here. An interested reader can ﬁnd further details at Edelsbrunner and Morozov (2013).
Computing the Gromov-Hausdorff distance involves solving hard combinatorial problems, but can be tractably approximated using the Bottleneck distance (Chazal et al., 2009). In order to compute the Bottleneck distance between two metric spaces, we compute the ﬁrst order VietorisRips complex (ﬁrst order for computational efﬁciency) at t for both spaces: a graph containing an edge between two points iff they lie within a Euclidean distance t from each other in the metric space. As t is varied, the Vietoris-Rips complex goes from the individual points (at t = 0) to a single cluster (at t = ∞). As t increases, clusters are formed (birth) and eventually merge together (death). The persistence diagram is a 2D plot of the (tbirth, tdeath) of each cluster, where tbirth and tdeath are the values of t at which the cluster was born and died respectively. Given two persistence diagrams f, g, let γ be a bijective map from the points of f to the points of g. The bottleneck distance (B) is then deﬁned as:
B(f, g) = inf sup ||u − γ(u)||∞ (7)
γ u∈f
Chazal et al. (2009) showed that the GromovHausdorff distance can be lower bounded by the Bottleneck Distance between the Persistence Diagrams of the Vietoris-Rips Filtration of the two spaces.
A.2 Analyzing Model Errors
We characterize the mistakes made by the model, and ﬁnd that most fall into the following 4 categories:
Polysemy on the target side: These are the cases in which the predicted words and the gold translation are synonyms/hypernyms/hyponyms of each other.
Polysemy on the source side: These are the cases in which the gold translations and the predicted words are different senses of the source word.
Antonyms: The distribution of the context of antonyms is often very similar. Unsurprisingly the

word vectors of antonyms are quite similar. This leads to cases where the predicted words and gold labels are antonyms of each other.
Words that occur in common contexts: Words that occur in numerous contexts often have poor word embeddings, since a single embedding can’t capture polysemy. Consequently, multiple such word embeddings that are frequent and have poor representations often get incorrectly translated to each other. Some examples include proper nouns and numbers
We quantitatively estimate the fraction of errors due to these reasons using WordNet synsets. Given 2 synsets, WordNet provides a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1. A score of 1 represents identity i.e. comparing a sense with itself will return 1. We approximate the fraction of target polysemy errors by ﬁnding those cases for which the aforementioned similarity scores between the synsets of the predicted words and the gold translations ≥ 0.1. Similarly we approximate the fraction of source polysemy errors by ﬁnding those cases for which the similarity scores between the synsets of the source word and the predicted word ≥ 0.1. Fig 3 shows these estimations for different language pairs. See Table 6 for examples sampled from each of these error types.
A.3 β orthogonality projection vs. autoencoding loss
Lample et al. (2018) constrained the mapping matrix to be close to the manifold of orthogonal matrices by applying the following projection step after every update.
W ← (1 + β)W − β(W W T )W
In our experiments we found out that the ﬁnal accuracy is highly sensitive to the value of the hyperparameter β (Table 7). Our approach on the other hand uses an autoencoding loss which allows the model to ﬂexibly adjusts the degree of orthogonality in a data driven manner and works consistently well for one choice of the scaling of the autoencoding loss.
A.4 Hyper-Parameters
The following are the hyper parameters used in the experiments. The values separated by / are the different values tried in the parameter search.

Figure 3: Fraction of errors coming from polysemy in the source/target side and antonymy, for the language pairs en-zh, en-it, en-es and en-fr

Type of Error Target Polysemy Target Polysemy Source Polysemy Source Polysemy Antonyms Antonyms Antonyms Common Words Common Words

Source Shadows Quest Worn Bitter Unofﬁcial Mature Afraid Everybody Fourteen

Gold 影子 Quest use´ 苦 Ufﬁciale Mature Paura Jeder Vierzehn

Predicted 阴影 Avventura veˆtement 辛辣
Funzionario Jeune Contento Spaß Dreirzehn

Comments synonyms synonyms Gold: used, Predicted: cloth Gold: bitter (taste), predicted: bitter (feeling) funzionario: ofﬁcial Jeune: young Gold: fear, Predicted: happy Gold: Everybody, Predicted: Fun Numbers translated incorrectly

Table 6: Sampled Errors

Lang Ortho β Auto 1e-2 1e-3 1e-4
en-de 19.9 74.8 67.4 73.7 74.3 en-ru 102.5 40.8 30.7 36.7 46.1 en-zh 171.1 0 23.8 32.1 33.3
Table 7: Unsupervised accuracies for different values of β (MUSE) and our autoencoding loss.
• Number of words per language considered for GAN training: top 75000
• Discriminator Parameters:
– embedding dim: 300 – hidden layers: 2 – hidden dim: 2048, 2048 – dropout prob: 0.1 (Only on the input
layer) – label smoothing: 0.1 – non-linearity: LeakyReLU (α = 0.2)

• Generator Parameters
– Initialization: Identity / Random Orthogonal
– Mean Center: True
• GAN Training Parameters
– batch size: 32 – Optimizer: SGD – Supervised loss optimizer: SGD / Adam – lr: 0.1 (with a schedule of 0.98 decay
per round, and halved if unsupervised CSLS metric does not improve over two rounds). – Hubness Threshold: 20
• fa = cosine

