Optimal Dynamic Sensor Subset Selection for Tracking a Time-Varying Stochastic Process
Arpan Chattopadhyay, Urbashi Mitra

arXiv:1711.10610v1 [cs.SY] 28 Nov 2017

Abstract—Motivated by the Internet-of-things and sensor networks for cyberphysical systems, the problem of dynamic sensor activation for the tracking of a time-varying process is examined. The tradeoff is between energy efﬁciency, which decreases with the number of active sensors, and ﬁdelity, which increases with the number of active sensors. The problem of minimizing the time-averaged mean-squared error over inﬁnite horizon is examined under the constraint of the mean number of active sensors. The proposed methods artfully combine three key ingredients: Gibbs sampling, stochastic approximation for learning, and modiﬁcations to consensus algorithms to create a high performance, energy efﬁcient tracking mechanisms with active sensor selection. The following progression of scenarios are considered: centralized tracking of an i.i.d. process; distributed tracking of an i.i.d. process and ﬁnally distributed tracking of a Markov chain. The challenge of the i.i.d. case is that the process has a distribution parameterized by a known or unknown parameter which must be learned. The key theoretical results prove that the proposed algorithms converge to local optima for the two i.i.d process cases; numerical results suggest that global optimality is in fact achieved. The proposed distributed tracking algorithm for a Markov chain, based on Kalman-consensus ﬁltering and stochastic approximation, is seen to offer an error performance comparable to that of a competetive centralized Kalman ﬁlter.
Index Terms—Wireless sensor networks, active sensing, sensor subset selection, distributed tracking, data estimation, Gibbs sampling, stochastic approximation, Kalman-consensus ﬁlter.
I. INTRODUCTION
Controlling and monitoring physical processes via sensed data are integral parts of internet-of-things (IOT), cyberphysical systems, and defense with applications to industrial process monitoring and control, localization, tracking of mobile objects, environmental monitoring, system identiﬁcation and disaster management. In such applications, sensors are simultaneously resource constrained (power and/or bandwdith) and tasked to achieve high performance sensing, control, communication, and tracking. Wireless sensor networks must further contend with interference and fading. One strategy for balancing resource use with performance is to activate a subset of the total possible number of sensors to limit both computation as well as bandwidth use.
Parts of this paper have been published in previous conferences; see [1], [2].
Arpan Chattopadhyay and Urbashi Mitra are with Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, USA. Email: {achattop,ubli}@usc.edu
This work was funded by the following grants: ONR N00014-15-1-2550, NSF CNS-1213128, NSF CCF-1410009, AFOSR FA9550-12-1-0215, and NSF CPS-1446901.

Herein, we address the fundamental problem of optimal dynamic sensor subset selection for tracking a time-varying stochastic process. We ﬁrst examine the centralized tracking of an iid process with an unknown, parametric distribution which serves as a benchmark for the ﬁrst extension to decentralized tracking of this process. For both proposed algorithms, almost sure convergence to local optima can be proven. Next a distributed algorithm for tracking a Markov process with a known probability transition matrix is developed. All algorithms are numerically validated.
A. Related Literature
Optimal sensor subset selection problems can be broadly classiﬁed into two categories: (i) optimal sensor subset selection for static data with known prior distribution, but unknown realization, and (ii) dynamic sensor subset selection to track a time-varying stochastic process. There have been several recent attempts to solve the ﬁrst problem; see [3], [1]. This problem poses two major challenges: (i) computing the estimation error given the observations from a subset of sensors, and (ii) ﬁnding the optimal sensor subset from exponentially many number of subsets. In [3], a tractable lower bound on performance addressed the ﬁrst challenge and a greedy algorithm addressed the second. In [1] only the second challenge is addressed via Gibbs sampling approach.
Dynamic sensor subset selection for a time-varying stochastic process is considered in e.g. [4], [5], [6], [7], [8], [9]. Markov process tracking with centralized controllers for single ([6]) or multiple sensor selection with energy constraints and sequential decision making ([4], [5]) have been previously studied. The optimal policy and its structural properties for a special case of dynamic sensor selection over an inﬁnite horizon was examined in [7]. Single sensor selection with the broadcast of sensor data is studied in [8]. To our knowledge, the combination of Gibbs sampling (e.g. [10]) and stochastic approximation (e.g. [11]) has not been previously applied to iid process tracking as we do herein. The paper [12], using Thompson sampling, has solved the problem of centralized tracking of a linear Gaussian process (with unknown noise statistics) via active sensing.
Given our consideration of dynamic sensor subset selection for distributed tracking of a Markov process, traditional Kalman ﬁltering is not applicable; however, there is much prior, recent art on the developmemt of consensus-based Kalman ﬁltering [13], [14], [15], [16], [17] which we further adapt to work with our Gibbs sampling/stochastic approxi-

mation approach for the iid case. This distributed tracking problem does not appear to be heavily studied. In [8] perfect information sharing between sensors is assumed, which is impractical and [18] assumes that the estimation is done in a centralized fashion, while sensors make decentralized decisions about whether to sense or communicate to the fusion center. In [18], the sparsity of the Markov process is also exploited.
B. Our Contribution
In this paper, we make the following contributions:
1) A centralized tracking and learning algorithm for an iid process with an unknown, but parametric distribution is developed. In particular, Gibbs sampling minimizes computational complexity with stochastic approximation to achieve the mean number of activated sensors constraint. Furthermore, simultaneous perturbation stochastic approximation (SPSA) is employed for parameter estimation obviating the need for the expectation-maximization algorithm. A challenge we overcome in the analysis, is handling updates at different time scales. As a precursor to the algorithm for unknown parameters, algorithms have been developed for a simpler version of the problem when the parameter of the distribution is known.
2) The centralized algorithm, which serves as a benchmark, is adapted to the distributed case by exploiting partial consensus. Our partial consensus is novel in that SPSA is again employed to learn the optimal consensus gains adaptively.
3) A trick for ensuring that all sensors employ similar sampling strategies is to have each sensor use the same seed in a random number generator.
4) For both the centralized and distributed algorithms, we can prove almost sure convergence to local optima. Furthermore, we an prove that the resources needed for communication and learning can be made arbitrarily small, by exploiting properties of the multi-scale updates.
5) Our ﬁnal generalization to is to develop an algorithm for sensor subset selection for the decentralized tracking of a Markov chain with known probability transition matrix. We adapt methods for Kalman consensus ﬁltering to our framework with Gibbs sampling and stochastic approximation.
6) Numerical results show that the decentralized scheme for a Markov chain performs close to that of the centralized scheme. Numerical results also show a tradeoff between performance and message exchange for learning. Furthermore, the numerical results show that global (not local) optima are achieved in tracking iid process.
C. Organization
The rest of the paper is organized as follows. The system model is described in Section II. Centralized tracking of an iid process with known distribution is described in Section III. Section IV deals with centralized tracking of an

iid process having a parametric distribution with unknown parameters. Distributed tracking of iid process is discussed in Section V. Distributed tracking of a Markov chain is described in Section VI. Numerical results are presented in Section VII, followed by the conclusion in Section VIII.
II. SYSTEM MODEL
A. Network, data and sensing model
We consider a connected single or multi-hop wireless sensor network. The sensor nodes are denoted by the set N = {1, 2, · · · , N }. It might also be possible that the connectivity in the network is maintained via a few relay nodes, but we ignore this possibility for the ease of analysis. There can be a fusion center (connected to all sensors via single hop wireless links) responsible for all control or estimation operations in the network, or, alternatively, the sensors can operate autonomously in a multihop mesh network.
The physical process under measurement is denoted by {X(t)}t≥0, where t is a discrete time index and X(t) ∈ Rq×1. We consider two models for the evolution of {X(t)}t≥0:
• IID model: {X(t)}t≥0 is an i.i.d. process with a parametric distribution pθ0 (X), where the unknown parameter θ0 needs to be be learnt via the measurements. θ0 lies inside the interior of a compact subset Θ ⊂ Rd.
• Markov model: {X(t)}t≥0 is a ﬁnite-state ergodic Markov chain with known transition probability matrix.
At time t, if a sensor k is used to sense the process, then the observation at sensor k is provided by a r-dimensional column vector zk(t) = HkX(t) + vk(t) where Hk is an observation matrix of appropriate dimension, and vk(t) is a Gaussian vector; the observation noise vk(t) is assumed to be independent across k and i.i.d. across t.
Let B(t) ∈ {0, 1}1×N be a vector where Bk(t) is the indicator that the sensor k is activated at time t; Bk(t) = 1 if sensor k is active at time t, or else Bk(t) = 0. The decision to activate any sensor for sensing and communicating the observation is taken either by the fusion center or by the sensor itself in the absence of a fusion center. We denote by B := {0, 1}N the set of all possible conﬁgurations in the network, and by B a generic conﬁguration. Clearly, B(t) ∈ B. Each conﬁguration represents a set of activated sensors. B−j ∈ {0, 1}N−1 is used to represent the conﬁguration B with its j-th entry removed.
The observation made by sensor k at time t is Zk(t) = Bk(t)zk(t). We deﬁne Z(t) := {Zk(t) : 1 ≤ k ≤ N }}.
B. Centralized estimation problem
The estimate of X(t) at the fusion center (connected to all sensors via direct wireless links) is denoted by Xˆ (t). We denote by Hp(t) the information available at time t at the fusion center about the history of observations, activations and estimates up to time (t − 1), before the fusion center determines B(t). On the other hand, we deﬁne Hc(t) := {B(t); Z(t); θ(t)} where θ(t) is the current estimate of θ0; Hc(t) denotes the information used by the fusion center at

time t to estimate Xˆ (t). For i.i.d. time varying process, Hc(t) is sufﬁcient to estimate X(t) and obtain Xˆ (t), and Hc(t) will be available only after deciding the activation vector B(t) and collecting all observations. In order to optimally decide B(t), the fusion center needs knowledge about the performance of all conﬁgurations in the past. Hence, Hp(t) and Hc(t) have two different information structures. However, we will see that, our Gibbs sampling algorithm determines B(t) by using only a sufﬁcient statistic calculated iteratively in each slot.
The information structure Hc(t) used to track a Markov chain will be different, which we will see in Section VI.
We deﬁne a policy µ = {(µ1, µ2} as a pair of mappings, where µ1(Hp(t)) = B(t) and µ2(Hc(t)) = Xˆ (t). Our ﬁrst goal is to solve the following centralized problem of minimizing the time-average mean squared error (MSE) subject to a constraint on the mean number of sensors active per unit time:
µ∗ = arg min lim sup 1 t Eµ||X(τ ) − Xˆ (τ )||2 µ t→∞ t τ =1 s.t. lim sup 1 t Eµ||B(τ )||1 ≤ N¯ (P1) t→∞ t τ =1
C. Distributed estimation problem

In the absence of a fusion center in a multi-hop network, the estimate of X(t) at sensor k is denoted by Xˆ (k)(t). We denote by Hp(k)(t) the information available at time t at sensor k
about the history before the sensor determines Bk(t), and by Hc(k)(t) the information available at sensor k at time t just before Xˆ (k)(t) is estimated.
We deﬁne a policy µ = {(µ(1k), µ(2k))}1≤k≤N , where µ(1k)(Hp(k)(t)) = Bk(t) and µ(2k)(Hc(k)(t)) = Xˆ (k)(t).
We seek to solve the following distributed problem:

µ∗ =

arg min lim sup 1 t 1 N Eµ||X(τ ) − Xˆ (k)(τ )||2 µ t→∞ t τ =1 N k=1

s.t. lim sup 1 t Eµ||B(τ )||1 ≤ N¯ (P2) t→∞ t τ =1

III. CENTRALIZED TRACKING OF IID PROCES: KNOWN θ0

In this section, we provide an algorithm for solving the centralized problem (P1) when X(t) ∼ pθ0 (x) i.i.d. This is done by relaxing (P1) by a Lagrange multiplier. Though our ﬁnal goal is to track an i.i.d. process with unknown θ0, we discuss algorithms for known θ0 in this section, as a precursor to the algorithms developed in subsequent sections for unknown θ0 and also tracking a Markov chain. Also, extension to distributed tracking will be discussed in Section V for unknown θ0, and hence will be omitted in this section.

A. The relaxed version of the constrained problem
We relax (P1) by using a Lagrance multiplier λ and obtain the following unconstrained problem:

1t

µ∗ = arg min lim sup

Eµ

µ t→∞ t τ =1

||X(τ )−Xˆ (τ )||2+λ||B(τ )||1 (P3)

The multiplier λ ≥ 0 can be viewed as the cost incurred for

activating a sensor at any time instant. Now, since (P3) is an

unconstrained problem and X(t) is i.i.d. across t, there exists

one optimizer B∗ ∈ B (not necessarily unique) for the problem

(P3); if the conﬁguration B∗ is chosen at each t, the minimum

cost of (P3) can be achieved by the law of large numbers.

Hence, for known θ0, the problem (P3) can be equivalently

written as:

arg min Eµ ,B||X(τ ) − Xˆ (τ )||2 +λ||B||1 (P4)
B∈B 2 :=f (B)

:=h(B)

The following result tells us how to choose the optimal λ∗ to

solve (P1).

Theorem 1: Consider problem (P1) and its relaxed version (P4). If there exists a Lagrange multiplier λ∗ ≥ 0 and a B∗ ∈ B, such that an optimal conﬁguration for (P4) under λ = λ∗ is B∗, and the constraint in (P1) is satisﬁed with equality under the pair (B∗, λ∗), then B∗ is an optimal

conﬁguration for (P1).

In case there exist multiple conﬁgurations

B

∗ 1

,

B2∗

,

·

·

·

, Bm∗ ,

a

multiplier

λ∗

≥

0,

and

a

probability

mass function (p1, p2, · · · , pm) such that (i) each of

B1∗, B2∗, · · · , Bm∗ is optimal for problem (P4) under λ∗,

and (ii)

m i=1

pi||Bi∗||1

=

N¯ ,

then

an

optimal

solution

for

(P1) is to choose one conﬁguration from B1∗, B2∗, · · · , Bm∗

with probability mass function (p1, p2, · · · , pm).

Proof: See Appendix A.

Remark 1: Theorem 1 allows us to obtain a solution for (P1) from the solution of by choosing an appropriate λ∗; this

will be elaborated upon in Section III-D.

B. Basics of Gibbs sampling for known pθ0 (·)

Finding the optimal solution of (P4) requires us to search over 2N possible conﬁgurations and to compute MMSE for
each of these conﬁgurations. Hence, we propose Gibbs sampling based algorithms to avoid this O(2N ) computation.
Let us deﬁne the probability distribution πβ(·) over B as (with β > 0):

e−βh(B)

e−βh(B)

πβ(B) := B ∈B e−βh(B ) := Zβ . (1)

Following the terminology in statistical physics, we call β the inverse temperature, and Zβ the partition function. h(B) is viewed as the energy under conﬁguration B. Now, limβ↑∞ B∈arg minA∈B h(A) πβ (B) = 1. Hence, if a con-

ﬁguration B(t) is selected at each time t with probability

distribution πβ(·) for sufﬁciently large β > 0, then B(t) will

belong to the set of minimizers of (P3) with high probability. However, computing Zβ requires 2N addition operations;

hence, we use a sequential subset selection algorithm based

on Gibbs sampling (see [10, Chapter 7]) in order to avoid

explicit computation of Zβ while picking X(t) ∼ pθ0 (·).

Algorithm 1: Start with an initial conﬁguration B(0). At

time t, pick a random sensor jt uniformly from the set of

all sensors. Choose Bjt (t) = 1 with probability p(t) :=

and choose B (t) = 0 e−βh(B−jt (t−1),1)

e−βh(B−jt (t−1),1)+e−βh(B−jt (t−1),0)

jt

with probability (1 − p(t)). For k = jt, choose Bk(t) =

Bk(t − 1). Activate the sensors according to B(t).

Theorem 2: Under Algorithm 1, {B(t)}t≥0 is a reversible, ergodic, time-homogeneous Markov chain with stationary distribution πβ(·).
Proof: Follows from the theory in [10, Chapter 7]).
Remark 2: Theorem 2 tells us that if the fusion center runs Algorithm 1 and reaches the steady state distribution of the Markov chain {B(t)}t≥0, then the conﬁguration chosen by the algorithm will have distribution πβ(·). For very large β > 0, if one runs {B(t)}t≥0 for a sufﬁciently long, ﬁnite time T , then the terminal state BT will belong to arg minB∈B h(B) with high probability. Also, by the ergodicity of {B(t)}t≥0, the time-average occurence rates of all conﬁgurations match the distribution πβ(·) almost surely.

C. The exact solution
Algorithm 1 is operated with a ﬁxed β; but the optimal soultion of the unconstrained problem (P3) can only be obtained with β ↑ ∞; this is done by updating β at a slower time-scale than the iterates of Algorithm 1.
Algorithm 2: This algorithm is same as Algorithm 1 except that at time t, we use β(t) := β(0) log(1 + t) to compute the update probabilities, where β(0) > 0, β(0)N ∆ < 1, and ∆ := maxB∈B,A∈B |h(B) − h(A)|.

Theorem 3: Under Algorithm 2, the Markov chain {B(t)}t≥0 is strongly ergodic, and the limiting probability distribution satisﬁes limt→∞ A∈arg minC∈B h(C) P(B(t) = A) = 1.
Proof: See Appendix C. We have used the notion of weak and strong ergodicity of time-inhomogeneous Markov chains from [10, Chapter 6, Section 8]), which is provided in Appendix B. The proof is similar to the proof of one theorem in [19], but is given here for completeness.
Remark 3: Theorem 3 shows that we can solve (P3) exactly if we run Algorithm 2 for inﬁnite time, in contrast with Algorithm 1 which provides an approximate solution.
Remark 4: For i.i.d. time varying {X(t)}t≥0 with known joint distribution, we can either: (i) ﬁnd the optimal conﬁguration B∗ using Algorithm 2 and use B∗ for ever, or (ii) run Algorithm 2 at the same timescale as t, and use the

running conﬁguration B(t) for sensor activation; both schemes will minimize the cost in (P3). By the strong ergodicity of {B(t)}t≥0, optimal cost will be achieved for (P3) under Algorithm 2.
1) Convergence rate of Algorithm 1: Let π(t) denote the probability distribution of B(t) under Algorithm 1. Let us consider the transition probability matrix P of the Markov chain {Y (l)}l≥0 with Y (l) = B(lN ), under Algorithm 1. Let us recall the deﬁnition of the Dobrushin’s ergodic coefﬁcient δ(P ) from [10, Chapter 6, Section 7] for the matrix P ; using a method similar to that of the proof of Theorem 3, we can show that δ(P ) ≤ (1 − e−NβNN∆ ). Then, by [10, Chapter 6, Theorem 7.2], we can say that under Algorithm 1, we have
l
dV (π(lN), πβ ) ≤ dV (π(0), πβ ) 1 − e−NβNN∆ . We can prove
similar bounds for any t = lN + k, where 0 ≤ k ≤ N − 1.
Unfortunately, we are not aware of such a closed-form bound for Algorithm 2.
Remark 5: Clearly, under Algorithm 1, the convergence rate decreases as β increases. Hence, there is a trade-off between convergence rate and accuracy of the solution in this case. Also, the rate of convergence decreases with N . For Algorithm 2, the convergence rate is expected to decrease with time.
D. Gibbs sampling and stochastic approximation based approach to solve the constrained problem
In Section III-B and Section III-C, we presented Gibbs sampling based algorithms for (P3). Now we provide an algorithm that updates λ with time in order to meet the constraint in (P1) with equality, and thereby solves (P1) (via Theorem 1).
Lemma 1: For the unconstrained problem (P4), the optimal mean number of active sensors, Eµ2 ||B∗||1, decreases with λ. Similarly, the optimal error, Eµ2 f (B∗), increases with λ.
Proof: See Appendix D.
Remark 6: The optimal mean number of active sensors, Eµ2 ||B∗||1, for the unconstrained problem (P4) is a decreasing staircase function of λ, where each point of discontinuity is associated with a change in the optimizer B∗(λ).
Lemma 1 provides an intuition about how to update λ in Algorithm 1 or in Algorithm 2 in order to solve (P1). We seek to provide one algorithm which updates λ(t) at each time instant, based on the number of active sensors in the previous time instant. In order to maintain the necessary timescale difference between the {B(t)}t≥0 process and the λ(t) update process, we use stochastic approximation ([11]) based update rules for λ(t). But the above remark tells us that the optimal solution of the constrained problem (P1) requires us to randomize between two values of λ in case the optimal λ∗ as in Theorem 1 belongs to the set of such discontinuities. However, this randomization will require us to update a randomization probability at another timescale; having stochastic approximations running in multiple timescales leads to slow

convergence. Hence, instead of using a varying β(t), we use a ﬁxed, but large β and update λ(t) in an iterative fashion using

stochastic approximation. Our proposed Algorithm 3 updates λ(t) iteratively in order to solve (P1).

Algorithm 3: Choose any initial B(0) ∈ {0, 1}N and

λ(0) ≥ 0. At each discrete time instant t = 0, 1, 2, · · · ,

pick a random sensor jt ∈ N independently and uni-

formly. For sensor jt, choose Bjt (t) = 1 with probability

p :=

e−βhλ(t)(B−jt (t−1),1) e−βhλ(t)(B−jt (t−1),1)+e−βhλ(t)(B−jt (t−1),0)

and choose

Bjt (t) = 0 with probability (1−p). For k = jt, we choose

Bk(t) = Bk(t − 1).

After this operation, before the (t + 1) decision instant,

update λ(t) at each node as follows.

λ(t + 1) = [λ(t) + a(t)(||B(t − 1)||1 − N¯ )]cb
The stepsize {a(t)}t≥1 constitutes a positive sequence such that ∞ t=1 a(t) = ∞ and ∞ t=1 a2(t) = ∞. The nonnegative projection boundaries b and c for the λ(t) iterates are such that λ∗ ∈ (b, c) where λ∗ is deﬁned in Assumption 1.

The update of λ(t) in Algorithm 3 is inspired by the following result which is crucial in the convergence proof.
Lemma 2: Under Algorithm 1, Eµ2 ||B(t)||1 is a Lipschitz continuous and decreasing function of λ.
Proof: See Appendix E.
Discussion of Algorithm 3:
• If ||B(t−1)||1 is more than N¯ , then λ(t) is increased with the hope that this will reduce the number of active sensors in subsequent iterations, as suggested by Lemma 2.
• The B(t) and λ(t) processes run on two different timescales; B(t) runs in the faster timescale whereas λ(t) runs in a slower timescale. This can be understood from the fact that the stepsize in the λ(t) update process decreases with time t. Here the faster timescale iterate will view the slower timescale iterate as quasi-static, while the slower timescale iterate will view the faster timescale as almost equilibriated. This is reminiscent of two-timescale stochastic approximation (see [11, Chapter 6]).
We make the following feasibility assumption for (P1), under the chosen β > 0.
Assumption 1: There exists λ∗ ≥ 0 such that the constraint in (P1) under λ∗ and Algorithm 1 is met with equality.
Remark 7: By Lemma 2, E||B||1 continuously decreases in λ. Hence, if N¯ is feasible, then such a λ∗ must exist by the intermediate value theorem.
Let us deﬁne: hλ(t)(B) := f (B) + λ(t)||B||1. Let πβ|λ∗ (·) denote πβ(·) under λ = λ∗.
Theorem 4: Under Algorithm 3 and Assumption 1, we have λ(t) → λ∗ almost surely, and the limiting distribution of {B(t)}t≥0 is πβ|λ∗ (·).
Proof: See Appendix F.
This theorem says that Algorithm 3 produces a conﬁguration

from the distribution πβ|λ∗ (·) under steady state.

E. A hard constraint on the number of activated sensors

Let us consider the following modiﬁed constrained problem:

min f (B) s.t. ||B||1 ≤ N¯
B∈B

(MCP)

It is easy to see that (MCP) can be easily solved using similar Gibbs sampling algorithms as in Section III, where the Gibbs sampling algorithm runs only on the set of conﬁgurations which activate N¯ number of sensors. Thus, as a by-product, we have also proposed a methodology for the problem in [3], though our framework is more general than [3].
Remark 8: The constraint in (P1) is weaker than (MCP).
Remark 9: If we choose β very large, then the number of sensors activated by GIBBSLEARNING will have very small variance. This allows us to solve (MCP) with high probability.

IV. CENTRALIZED TRACKING OF IID PROCESS: UNKNOWN θ0
In Section III, we described algorithms for centralized tracking of an i.i.d. process where pθ0 (·) is known. In this section, we will deal with the centralized tracking of an i.i.d. process {X(t)}t≥0 where X(t) ∼ pθ0 (·) with θ0 unknown; in this case, θ0 has to be learnt over time through observations, which creates many nontrivial issues that need to be addressed before using Gibbs sampling for sensor subset selection.

A. The proposed algorithm for unknown θ0

Since θ0 is unknown, its estimate θ(t) has to be updated over time using the sensor observations. On the other hand, to solve
the constrained problem (P1), we need to update λ(t) over time so as to attain the optimal λ∗ of Theorem 1 iteratively. Further,
f (B) (MSE under conﬁguration B) in (P4) is unknown since θ0 is unknown, and f (t)(B) has to be learnt over time using the sensor observations. Hence, we combine the Gibbs sampling algorithm with update schemes for f (t)(B), λ(t) and θ(t)

using stochastic approximation (see [11]).

The algorithm also requires a sufﬁciently large positive number A0 and a large integer T as input.

Let J (t) denote the indicator that time t is an integer multiple of T . Deﬁne ν(t) := tτ=0 J (τ ).
We ﬁrst describe some key features and steps of the algo-

rithm and then provide a brief summary of the algorithm.

1) Step size: For the stochastic approximation updates, the

algorithm uses step sizes which are nonnegative sequences

{a(t)}t≥0, {b(t)}t≥0, {c(t)}t≥0, {d(t)}t≥0 such that:

(i)

∞ t=0

a(t)

=

∞ t=0

b(t)

=

∞ t=0

c(t)

=

∞ t=0

a2(t)

<

∞,

∞ t=0

b2(t)

<

(iii) limt→∞ d(t) = 0, (iv)

limt→∞ b(t) = limt→∞ c(

t T

) = 0.

a(t)

b(t)

∞,

∞ t=0

c2(t)

∞ t=0 dc22((tt)) <

∞, (ii) < ∞, ∞, (v)

2) Gibbs sampling step: The algorithm also maintains

a running estimate h(t)(B) of h(B) for all B ∈ B. At

time t, it selects a random sensor jt ∈ N with uniformly

and independently, and sets Bjt (t) = 1 with probability

p(t) :=

e−βh(t)(B−jt (t−1),1)

and Bj (t) = 0

e−βh(t)(B−jt (t−1),1)+e−βh(t)(B−jt (t−1),0)

t

with probability (1−p(t)) (similar to Algorithm 1). For k = jt,

it sets Bk(t) = Bk(t−1). This operation can even be repeated

multiple times. The sensors are activated according to B(t),

and the observations ZB(t)(t) := {zk(t) : Bk(t) = 1} are collected. Then the algorithm declares Xˆ (t) = µ2(Hc(t)).

3) Occasional reading of all the sensors: If J (t) = 1, the fusion center reads all sensors and obtains Z(t). This is required primarily because we seek to update θ(t) iteratively and reach a local maximum of the function g(θ) = EX(t)∼pθ0 (·),B(t)=[1,1,··· ,1] log p(Z(t)|θ).
4) θ(t) update when J (t) = 1: Since we seek to reach a local maximum of g(θ) = EX(t)∼pθ0 (·),B(t)=[1,1,··· ,1] log p(Z(t)|θ), a gradient ascent scheme needs to be used. The gradient of g(θ) along any coordinate can be computed by perturbing θ in two opposite

directions along that coordinate and evaluating the difference of g(·) at those two perturbed values. However, if θ0 is high-dimensional, then estimating this gradient along each

coordinate is computationally intensive. Moreover, evaluating g(θ) for any θ requires us to compute an expectation, which

might also be expensive. Hence, we perform a noisy gradient estimation for g(θ) by simultaneous perturbation stochastic

approximation (SPSA) as in [20]. Our algorithm generates ∆(t) ∈ {1, −1}d uniformly over all sequences, and perturbs
the current estimate θ(t) by a random vector d(ν(t))∆(t) in

two opposite directions, and estimates each component of the gradient from the difference log p(Z(t)|θ(t) + d(ν(t))∆(t)) − log p(Z(t)|θ(t) − d(ν(t))∆(t)); this estimate is noisy because (i) Z(t) and ∆(t) are random, and (ii) d(ν(t)) > 0.

The k-th component of θ(t) is updated as follows:

θk(t + 1) =

log p(Z(t)|θ(t) + d(ν(t))∆(t))

θk(t) + c(ν(t))J (t)

2d(ν (t))∆k (t)

log p(Z(t)|θ(t) − d(ν(t))∆(t))

−

(2)

2d(ν (t))∆k (t)

Θ

The iterates are projected onto the compact set Θ to ensure boundedness. The diminishing sequence {d(t)}t≥0 ensures that the gradient estimate becomes more accurate with time.

5) λ(t) update: λ(t) is updated as follows:

λ(t + 1) = [λ(t) + b(t)(||B(t)||1 − N¯ )]A0 0 .

(3)

The intuition here is that, if ||B(t)||1 > N¯ , the sensor activation cost λ(t) needs to be increased to prohibit activating large
number of sensors in future; this is motivated by Lemma 2. The goal is to converge to λ∗ as deﬁned in Theorem 1.
6) f (t)(B) update: Since pθ0 (X) is not known initially, the true value of f (B) is not known; hence, the algorithm updates an estimate f (t)(B) using the sensor observations. If J (t) = 1, the fusion center obtains Z(t) by reading all

sensors. The goal is to obtain an estimate YB(t) of the MSE under a conﬁguration B, by using these observations, and update f (t)(B) using YB(t). However, since θ0 is unknown and only θ(t) is available, as an alternative to the MSE
under conﬁguration B, the fusion center uses the trace of the
conditional covariance matrix of X(t) given ZB(t), assuming that X(t) ∼ pθ(t)(·). Hence, we deﬁne a random variable YB(t) := EX(t)∼p(·|θ(t),ZB(t))(||X(t) − XˆB(t)||2|ZB(t), θ(t)) for all B ∈ B, where XˆB(t) is the MMSE estimate declared by µ2 under conﬁguration B, and ZB(t) is the observation made by active sensors determined by B. Clearly, YB(t) is a random variable with the randomness coming from two sources: (i)
randomness of θ(t), and (ii) randomness of ZB(t) which has a distribution p(ZB(t)|θ0) since the original X(t) process that yields ZB(t) has a distribution pθ0 (·). Computation of YB(t) is simple for Gaussian X(t) and the MMSE estimator, since
closed form expressions are available to compute YB(t).
Using YB(t), the following update is made for all B ∈ B:

f (t+1)(B) = [f (t)(B) + J (t)a(ν(t))(YB (t) − f (t)(B))]A0 0

(4)

The iterates are projected onto [0, A0] to ensure boundedness. The goal here is that, if θ(t) → θ∗, then f (t)(B)
will converge to EZB(t)∼p(·|θ0)EX(t)∼p(·|θ∗,ZB(t))(||X(t) − XˆB(t)||2|ZB(t), θ∗), which is equal to f (B) under θ∗ = θ0.
We will later argue that this occasional O(2N ) computation
for all B ∈ B can be avoided, but convergence will be slow.

7) The algorithm: A summary of all the steps of our scheme is provided in Algorithm 4. We will show in Theorem 5 that this algorithm almost surely converges to the set of locally optimum solutions for (P1).

Algorithm 4: Initialize all iterates arbitrarily. For any time t = 0, 1, 2, · · · :

1. Perform the Gibbs sampling step as in Section IV-A2, obtain the observations ZB(t)(t), and estimate Xˆ (t). Up-
date λ(t) according to (3). 2. If J (t) = 1, compute h(t)(B) = f (t)(B) + λ(t)||B||1
for all B ∈ B. Read all sensors and obtain Z(t). Update f (t)(B) for all B ∈ B using (4) and θ(t) using (2).

8) Multiple timescales in Algorithm 4: Algorithm 4 has

multiple iterations running in multiple timescales (see [11,

Chapter 6]). The {B(t)}t≥0 process runs ar the fastest

timescale, whereas the {θ(t)}t≥0 update scheme runs at the

slowest timescale. The basic idea is that a faster timescale

iterate views a slower timescale iterate as quasi-static, whereas

a slower timescale iterate views a faster timescale iterate as

almost

equilibriated.

For

example,

since

limt→∞

c(t) a(t)

=

0,

the θ(t) iterates will vary very slowly compared to f (t)(B)

iterates; as a result, f (t)(B) iterates will view quasi-static θ(t).

B. Complexity of Algorithm 4
1) Sampling and communication complexity: Since all sensors are activated when J (t) = 1, the mean number of additional active sensors per unit time is O( NT ); these observations need to be communicated to the fusion center. O( NT ) can be

made arbitrarily small by choosing T large enough.

2) Computational complexity: The computation of YB(t) in (4) for all B ∈ B requires O(2N ) computations whenever
J (t) = 1. However, if one chooses large T (e.g., O(4N )),

then this additional computation per unit time will be small.

However, if one wants to avoid that computation also, then one can simply compute YB(t)(t) and update f (t)(B(t)) instead of doing it for all conﬁgurations B ∈ B. However, the stepsize

sequence a(ν(t)) cannot be used; instead, a stepsize a(νB(t))

has to be used when B(t) = B and f (t)(B) is updated using

(4), where νB(t) :=

t τ =0

J (τ )I(B(τ )

=

B)

is

the

number

of times conﬁguration B was chosen till time t. In this case,

the convergence result (Theorem 5) on Algorithm 4 will still

hold; however, the proof will require a technical condition

lim inft→∞

νB (t) t

>

0

almost

surely

for

all

B

∈

B,

which

will

be satisﬁed by the Gibbs sampler using ﬁnite β and bounded

h(t)(B). However, we discuss only (4) update in this paper for

the sake of simplicity in the convergence proof, since technical

details of asynchrounous stochastic approximation required in

the variant is not the main theme of this paper.

When J (t) = 1, one can avoid computation of h(t+1)(B)

for all B ∈ B in Step 2 of Algorithm 4. Instead, the fusion
center can update only h(t)(B(t)), h(t)(B−jt (t − 1), 1) and h(t)(B−jt (t − 1), 0) at time t, since only these iterates are
required in the Gibbs sampling.

C. Convergence of Algorithm 4
1) List of assumptions:
Assumption 2: The distribution pθ(·) and the mapping µ2(·; ·; θ) (or µ(2k) for distributed case) as deﬁned before are Lipschitz continuous in θ ∈ Θ.
Assumption 3: µ2 is known to the fusion center (centralized case), and {µ(2k)}k≥1 are known to all sensors (distributed).
Remark 10: Assumption 3 allows us to focus only on the sensor subset selection problem rather than the problem of estimating the process given the sensor observations. For optimal MMSE estimators, µ2(Hc(t)) = E(X(t)|Hc(t)). Computation of µ2(·) will depend on the exact functional form of pθ(X), and it can be done by using Bayes’ theorem.
Assumption 4: Let us consider YB(t) with θ(t) = θ ﬁxed in Algorithm 4. Suppose that, one uses Algorithm 1 to solve (P3) for a given λ but with the MSE ||X(t) − Xˆ (t)||2 replaced by YB(t)(t) in the objective function of (P3), and then ﬁnds the λ∗(θ) as in Theorem 1 to meet the constraint N¯ . We assume that, for the given β and N¯ , and for each θ ∈ Θ, there exists λ∗(θ) ∈ [0, A0) such that, the optimal Lagrange multiplier to relax this new unconstrained problem is λ∗(θ) (Theorem 1). Also, λ∗(θ) is Lipschitz continuous in θ ∈ Θ.
Remark 11: Assumption 4 makes sure that the λ(t) iteration (3) converges, and the constraint is met with equality. Let us deﬁne the function Γ¯θ(φ) := limδ↓0 [θ+δφδ]Θ−θ .
Assumption 5: Consider the function g(θ) = EX(t)∼pθ0 (·),B(t)=[1,1,··· ,1] log p(Z(t)|θ); this is the expected conditional log-likelihood function of Z(t) conditioned on θ,

given that X(t) ∼ pθ0 (·) and B(t) = [1, 1, · · · , 1]. We assume that the ordinary differential equation θ˙(t) = Γ¯θ(t)(∇g(θ(t))) has a globally asymptotically stable solution θ∗ in the interior

of Θ. Also, ∇g(θ) is Lipschitz continuous in θ.

Remark 12: One can show that the θ(t) iteration (2) asymptotically tracks the ordinary differential equation θ˙(τ ) = ∇g(θ(τ )) inside the interior of Θ. In fact, Γ¯θ(τ)(∇g(θ(τ )) =
∇g(θ(τ )) when θ(τ ) lies inside the interior of Θ. The condition on θ˙(τ ) = Γ¯θ(τ)(∇g(θ(τ ))) is required to make sure that
the iteration does not converge to some unwanted point on the

boundary of Θ due to the forced projection. The assumption on θ∗ makes sure that the θ(t) iteration converges to θ∗.

2) The main result: Now we present convergence result for

Algorithm 4. This result tells us that the iterates of Algorithm 4

almost surely converge to the set of local optima for (P1).

Theorem 5: Under Assumptions 2, 3, 4, 5 and Algorithm 4, we have limt→∞ θ(t) = θ∗ almost surely. Correspondingly, λ(t) → λ∗(θ∗) almost surely. Also, f (t)(B) → EZB(t)∼p(·|θ0)EX(t)∼p(·|θ∗,ZB(t))(||X (t) − XˆB(t)||2|ZB(t), θ∗) =: fθ∗ (B) almost surely for all B ∈
B. The B(t) process reaches the steady-state distribution

πβ,fθ∗ ,λ∗(θ∗),θ∗ (·) which can be obtained by replacing h(B) in (1) by fθ∗ (B) + λ∗||B||1.

Proof: See Appendix G.

Remark 13: If θ(t) → θ∗, but the constraint in (P1) is satisﬁed with λ = 0 and policy µ2(·; ·; θ∗), then λ(t) → 0, i.e., λ∗(θ∗) = 0, and the constraint becomes redundant. If θ∗ = θ0, then the algorithm reaches the global optimum.

Remark 14: If all sensors are not read when J (t) =

1, then one has to update θ(t) based on the observations

ZB(t)(t) collected from the sensors determined by B(t).

In that case, θ(t) will converge to a local maximum θ1

of limt→∞ Eπβ,f ,λ∗(θ),θ log p(ZB(t)(t)|θ), which will be dif-

ferent

from

θ∗

θ
of

Theorem

5

in

general.

However,

in

the

numerical example in Section VII, we observe that θ1 = θ∗.

V. DISTRIBUTED TRACKING OF THE I.I.D. PROCESS
We next seek to solve the constrained problem (P2). This problem brings in additional challenges compared to (P1): (i) each sensor k has access only to its local measurement, i.e., zk(t), if Bk(t) = 1, (ii) sharing measurements across the network will consume a large amount of energy and bandwidth, and (iii) ideally, the iterates λ(t) and θ(t) should be known at all sensors. To resolve these issues, we propose an algorithm that combines Algorithm 4 with consensus among sensor nodes (see [16]). However, our approach is different from traditional consensus schemes in the following aspects: (i) traditional consensus schemes run many steps of a consensus iteration, thus requiring many rounds of message exchange among neighbouring nodes, and (ii) traditional consensus schemes do not care about the correctness of the data at any particular sensor node. In contrast, our proposed algorithm allows each sensor to broadcast its local information only once to its neighbours in a time slot. Also, since many of the sensors may use outdated estimates, we propose an on-line learning

scheme based on stochastic approximation in order to optimize the coefﬁcients of the linear combination used in consensus.

A. The proposed algorithm

Note that, if the Gibbs sampling step of Algorithm 4 is run

at all sensors to make their individual activation decisions,

but all sensors are supplied with the same initial seed for

randomization, then all sensors will sample the same B(t) at

each time t. We will exploit this fact in the next algorithm.

However, depending on the current conﬁguration B(t) = B,

each node uses a linear combination of its own estimate

and the estimates made by its neighbours. Let us denote the initial estimate made by node k at time t by X¯ (k)(t);

this estimation is done at node k based on Hc(t). The actual estimate Xˆ (t) := [Xˆ (1)(t), Xˆ (2)(t), · · · , Xˆ (N)(t)] is obtained from X¯ (t) := [X¯ (1)(t), X¯ (2)(t), · · · , X¯ (N)(t)] by Xˆ (t) = KB(t()t)X¯ (t); this method is motivated by the Kalman consensus ﬁlter proposed in [15]. Here KB(t) ∈ RN×N is the weight matrix to be used at time t under conﬁguration B;

this matrix has (i, j)-th entry zero if nodes i and j are not

connected in the wireless network, it can induce a consensus such as in [16]. The matrices KB(t) for all B ∈ B are updated for all t when J (t) = 1, and are broadcast to the sensors. As

with the θ(t) update in (2), we use SPSA to ﬁnd optimal KB in order to minimize the error.

First we describe some special steps of the algorithm. Deﬁne

ν(t) =

tτ=1 J (τ ), and νB(t) =

t τ =1

J (τ )I(B(τ )

=

B)

(the number of times conﬁguration B is sampled till time t).

1) f (t)(B) update when J (t) = 1: If J (t) = 1, all sensors

are read to obtain Z(t); this Z(t) is either supplied to all

sensors, or sent to some speciﬁc node which does centralized

computation (this is done only when J (t) = 1) and broadcasts

the results to all sensors. For each B ∈ B, all estimates {XˆB(k)(t)}1≤k≤N are computed using ZB(t), where XˆB(k)(t)

denotes the estimate at node k at time t if B(t) = B. Then, the

quantity YK(t) := EX(t)∼p(·|θ(t),Z (t),K(t))( N1 Nk=1 ||X(t) −

B

B

B

XˆB(k)(t)||2|θ(t), ZB(t), KB(t)) is computed, and the following

update is done for all B ∈ B:

f (t+1)(B) = f (t)(B) + J (t)a(ν(t)) Y (t) − f (t)(B)

(5)

KB

We will see in Section V-B1 that this O(2N ) computation can
be avoided without sacriﬁcing convergence.
2) KB(t) update when J (t) = 1: The algorithm requires another condition ∞ t=0 db22((tt)) < ∞ (apart from those in Section IV-A1) to ensure convergence of the KB(t) iterates which are updated via the following SPSA algorithm whenever
J (t) = 1 (i.e., t is an integer multiple of T ). A random matrix Γt ∈ RN×N is generated such that
Γt(i, j) = 0 if i and j are not neighbours, otherwise Γt(i, j) ∈ {−1, 1} independently with equal probability. Now,

the following updates are done for all links (i, j):

KB(t+1)(i, j) = KB(t)(i, j) − b(νB (t))J (t)

I(B(t) = B)

Y (t)

Y (t)

KB +d(νB (t))Γt − KB −d(νB (t))Γt

2d(νB (t))Γt(i, j) 2d(νB (t))Γt(i, j)

A0
(6)
−A0

(6) is a gradient descent scheme, with the goal to converge to some KB∗ so that limt→∞ E(YK(t) (t)) (if it exists) is
B
minimized. ∇E(YK(t) (t)) is estimated by SPSA. B 3) Outline of the proposed algorithm: The entire scheme
is described in Algorithm 5.

Algorithm 5: The same seed is supplied to all sensors for
Gibbs sampling. All iterates are initialized arbitrarily.
For any time t = 0, 1, 2, · · · , do at each sensor k:
1. Run Gibbs sampling as in Section IV-A2, once or
multiple times. All sensors will have the same B(t); they
will be activated according to B(t) and make observations. Compute X¯ (k)(t) = µ(2k)(Hc(t)) and Xˆ (t) = KB(t()t)X¯ (t) locally. Update λ(t) using (3).
2. If J (t) = 1, read all sensors and obtain Z(t). For each B ∈ B, compute all estimates {XˆB(k)(t)}1≤k≤N using ZB(t), where XˆB(k)(t) denotes the estimate at node k at time t if B(t) = B. Update f (t)(B) using (5), and KB(t) using (6) for all B ∈ B.
3. Do the same θ(t) update as in (2). 4. Broadcast KB(t(+t)1), f (t)(B) and θ(t + 1) if J (t) = 1.

Remark 15: Algorithm 5 is similar to Algorithm 4 except that (i) consensus is used for deciding the estimates, and (ii) an additional SPSA algorithm has been used to optimize the consensus gains. However, this scheme does not achieve perfect consensus, and is optimal only when one round message exchange among neighbouring nodes is allowed per slot.
Remark 16: Since KB(t) iteration does not depend on λ(t), we can run (6) at the same timescale as λ(t) iteration.

B. Performance of Algorithm 5
1) Complexity and distributed nature: The mean number of additional sensors activated per slotis O( NT ), which can be made small by taking T large enough. The same argument applies to the computation of YKB(t) . Moreover, one can decide only to compute Y (t) (t) and update f (t)(B(t)) when
KB(t)
J (t) = 1 as discussed in Section IV-B2; the algorithm will converge slowly but O(2N ) computation will be avoided.
Gibbs sampling is run at all nodes; they will yield the same B(t) since all sensors have the same seed. All sensors will have the same λ(t), and can update h(t)(B(t)) for each t. The consensus gains and f (t)(B) updates need to be sent to all sensors if J (t) = 1; however, a bounded delay in broadcast does not affect convergence. Since nodes use only local consensus and periodic broadcast, and Gibbs sampling step is distributed, Algorithm 5 is distributed.
2) Convergence of Algorithm 5:
Assumption 6: For given λ ∈ [0, A0], θ ∈ Θ, for all B ∈

B, the function E(YKB ) of KB (Section V-A1) is Lipschitz continuous in KB. The set of ordinary differential equations K˙ B(τ ) = −∇E(YKB(τ)(τ )) (vectorized) for any ﬁxed θ ∈ Θ has a globally asymptotically stable equilibrium KB∗ (θ) which is Lipschitz continuous in θ.

Now we present the main result related to Algorithm 5,

which shows that the iterates of Algorithm 5 almost surely

converge to a set of locally optimal solutions of (P2).

Theorem 6: Under Assumptions 2-6 and Algorithm 5,

we have limt→∞ θ(t) Correspondingly, λ(t)

= θ∗ almost surely.

→

λ∗(θ∗) almost

surely. As a result, for all B ∈ B, we have

KB(t) → KB∗ (θ∗) almost surely. Also, f (t)(B) →

1 N

N k=1

EZB

E (t)∼p(·|θ0) µ(k)(·;·;θ∗),K∗

(θ∗ ),θ∗

(||X

(τ

)

−

2

B

Xˆ (k)(τ )||2|ZB(t)) := fθ∗ (B) almost surely for all B ∈ B.

The B(t) process reaches the steady-state distribution

πβ,λ∗(θ∗),K∗ (θ∗),θ∗ (·) which can be obtained by replacing

h(B)

in

B
(1) by

fθ∗ (B)

+

λ∗||B||1.

Proof: The proof is similar to that of Theorem 5. If θ∗ = θ0, then global optimum for (P2) is reached.

VI. DISTRIBUTED TRACKING OF A MARKOV CHAIN
In this section, we seek to track a Markov chain {X(t)}t≥0 with transition probability matrix AT and ﬁnite state space X . In order to have a meaningful problem, we enumerate each state and denote the i-th state by ei which is an |X |-length 0 − 1 column vector with 1 only at the i-th location. Thus, the state space becomes X = {ei : 1 ≤ i ≤ |X |}. We also consider a measurement process where, given X(t) = ei and a conﬁguration B(t) = B of active sensors, any sensor k with Bk = 1 makes an observation zk(t) ∼ N (mk,i, Σk,i) where the mean mk,i and covariance matrix Σk,i depend on the state ei. Under this model, a centralized, ﬁnite horizon version of the dynamic sensor subset selection problem has been solved in [4], where it is shown that a sufﬁcient statistic for decision making is a belief vector on the state space conditioned on the history. The authors of [4] formulated a partially observable Markov decision process (POMDP) with this belief vector working as a proxy to the (hidden) state, and also proposed a Kalman-like estimator to make on-line update to the belief vector using new observations made by chosen sensors. Hence, in this section, we skip the centralized problem (P1) and directly solve the distributed problem (P2).
The centralized problem in [4] itself leads to intractability in the sequential subset selection problem; the POMDP formulation in [4] does not provide any structural result on the optimal policy. Hence, for solving the distributed problem (P2), we restrict ourselves to the class of myopic policies which seek to minimize the cost at the current time instant.
The estimation scheme yields Xˆ (k)(t) at each node k; Xˆ (k)(t) is the belief vector on the state space, and is generated by the use of a Kalman-consensus ﬁlter (KCF) as in [15]. Consensus is required since all nodes do not have access to the complete observation set; consensus requires each sensor

to combine the estimates made by its neighbouring sensors. Kalman ﬁltering operation is required since the dynamical system can be expressed as a linear stochastic system (for which Kalman ﬁlter is the best linear MMSE estimator):

X(t + 1) = AX(t) + (X(t + 1) − AX(t))

:=w(t)
zk(t + 1) = [mk,1, mk,2, · · · , mk,|X |] X(t) +

vk (t)

:=Hk
Zk(t) = Bk(t)zk(t)

∼N (mk,i,Σk,i) if X(t)=ei

Since A is known, the conditional covariance matrix Q(i) of w(t) given X(t) = ei is known to all sensors.

A. Kalman consensus ﬁltering

The KCF that we use is adapted from [15] with the
additional consideration of B(t). Here each sensor k maintains an estimate X¯ (k)(t) at time t before making any observation. Once the observations are made, sensor k computes Xˆ (k)(t)
by using KCF. The estimates evolve as follows:

Xˆ (k)(t) = [X¯ (t) + Kk(t)Bk(t)(zk(t) − HkX¯ (k)(t))

+Ck

(X¯ (j)(t) − X¯ (k)(t))]D

j∈nbr(k)

X¯ (k)(t + 1) = AXˆ (k)(t)

(7)

Here Ck and Kk(t) are called consensus gain and Kalman gain matrices for sensor k, and nbr(k) is the set of neighbours

of sensor k. Projection on the probability simple D is done to ensure that Xˆ (k)(t) is a valid probability belief vector.

In [15, Theorem 1], the Kalman gains Kk(t) at all nodes

are optimized for given consensus gains Ck, so that the MSE

1 N

N k=1

E||Xˆ (k)(t) − X(t)||2

at

the

current

time

step

is

min-

imized; but the computational and communication complexity

per node for its implementation grows rapidly with N . Hence,

[15, Section IV] also provides an alternative suboptimal KCF

algorithm to compute Kk(t) at each sensor k, which has low complexity and is easily implementable. Hence, we adapt this

suboptimal algorithm from [15, Section IV] to our problem.

The KCF gain update scheme from [15, Section IV] main-

tains two matrices Mk(t) and Pk(t) for each k, which are
viewed as proxies for the covariance matrices of the two errors Xˆ (k)(t) − X(t) and X¯ (k)(t) − X(t). It also requires

system noise covariance matrix; since system noise w(t) is

dependent on X(t) whose exact value is unknown to the sensors, we use Qk(t) = Ni=1 X¯ (k)(t + 1)(i)Q(i) at node k

as an estimate of the covariance matrix of w(t + 1). Similarly,

Rk(t + 1) = Bk(t)

|X | i=1

X¯ (k)(t

+

1)(i)Σk,i

is

used

as

an

alternative to the covariance matrix vk(t + 1). The KCF ﬁlter

also maintains an abstract iterate Fk(t). The overall KCF gain

update equations from [15, Section IV] are as follows:

Kk(t) = Bk(t)Pk(t)HkT (Rk(t) + HkPk(t)HkT )−1

|X |
Rk(t + 1) = Bk(t) X¯ (k)(t + 1)(i)Σk,i

i=1

Qk(t) =

N
X¯ (k)(t + 1)(i)Q(i)

i=1

Fk(t) = I − Kk(t)HkBk(t)

Mk(t) = Fk(t)Pk(t)FkT (t) + Kk(t)Rk(t + 1)KkT (t)

Pk(t + 1) = AMk(t)AT + Qk(t)

(8)

B. The proposed algorithm

The sensor subset selection is done via Gibbs sampling

run at all nodes supplied with the same seed; all nodes

generate the same conﬁguration B(t) at each time t. The quantity f (t)(B) is updated via stochastic approximation to

converge to the MSE under conﬁguration B; but since the

MSE under B cannot be computed directly, sensor k uses

1 N

N k=1

T

r(Mk

(t

−

T ))

of

a

past

slot

(t

−

T)

to

update

f (t)(B). λ(t) is varied at a slower timescale.

The proposed scheme is provided in Algorithm 6.

Algorithm 6: Input: A0, T > 0 stepsize sequences {a(t)}t≥0 and {b(t)}t≥0 as in Section IV, consensus gain matrices Ck for all k ∈ {1, 2, · · · , N }, the same seed for randomization to all sensors, initial covariance matrix

P (0) of X(0). All iterates are initialized arbitrarily. Deﬁne

νB(t) :=

t τ =0

I(B(τ

)

=

B)J

(t).

For any time t = 0, 1, 2, · · · , do at each sensor k:

1. Select B(t) at each sensor by running the Gibbs

sampling step as in Section IV-A2. Activate sensors au-

tonomously according to the common B(t) selected by

them, and make observations accordingly.

2. At sensor k, perform state estimation with (7) and gain

update with (8). Update λ(t) at all sensors using (3). Compute h(t+1)(B) = f (t+1)(B) + λ(t)||B||1 either for all B ∈ B, or for B(t) if computation is a bottleneck.

3. If J (t) = 1, update:

f (t+1)(B) = [f (t)(B) + J (t)I(B(t − T ) = B)a(νB (t − T ))

1N

(t)

A0

( N

T r(Mk(t − T )) − f (B))]0

k=1

4. If J (t) = 1, broadcast T r(Mk(t)) to all sensors, so that step 3 can be performed at time (t + T ).
Remark 17: Algorithm 6 is suboptimal for (P2) because: (i) it greedily chooses B(t) via Gibbs sampling without caring about the future cost, and (ii) KCF update is suboptimal. But it has low complexity, and it performs well (see Section VII).

C. Complexity of Algorithm 6
At each time t, a sensor k needs to obtain X¯ (j)(t) from all its neighbours j ∈ nbr(k) for consensus. Also, {T r(Mk(t))}1≤k≤N needs to be broadcast to all nodes when J (t) = 1 so that f (t+T )(B) update can be done at each node; but the per slot communication for this broadcast can be made

small enough by making T arbitrarily large but ﬁnite, and the broadcast can even be done over T slots to avoid network congestion at any particular slot. Interestingly, Mk(t) and Pk(t) can be updated using local iterates, and do not need any communication. Computing Mk(t) and Pk(t) involves simple matrix operations which have polynomial complexity.

VII. NUMERICAL RESULTS

Now we demonstrate the performance of Algorithm 4

(centralized) and Algorithm 6 (distributed). We consider the

following parameter values: N = 5, N¯ = 2, a(t) = t01.6 ,

b(t)

=

2 t0.8

,

c(t)

=

1t ,

d(t)

=

0.2 t0.1

,

T

=

20,

λ(0)

=

0.1,

β = 150. Gibbs sampling is run 10 times per slot.

1) Performance of Algorithm 4: For illustration purpose, we assume that X(t) ∼ N (θ0, (1 − θ0)2) scalar, and zk(t) = X(t) + wk(t), where θ0 = 0.5 and wk(t) is zero mean i.i.d. Gaussian noise independent across k. Standard deviation of

wk(t) is chosen uniformly and independently from the interval [0, 0.5], for each k ∈ {1, 2, · · · , N }. Initial estimate θ(0) =

0.2, Θ = [0, 0.8].

We consider three possible algorithms: (i) Algorithm 4 in

its basic form, which we call GIBBS, (ii) a variation of

Algorithm 4 called LOWCOMPLEXGIBBS where all sensors

are not read when J (t) = 1, and the relatively expensive
f (t)(B(t)) and θ(t) updates are done every T slots, and (iii) an algorithm GREEDY where N¯ sensors are picked arbitrarily

and used for ever with the wrong estimate θ(0) = 0.2 without

any update. The MSE per slot, mean number of active sensors

per slot, λ(t) and θ(t) are plotted against t in Figure 1.

MSE of all these three algorithms are much smaller than V ar(X(t)) = (1 − θ0)2 (this is MMSE without any obser-
vation). We notice that GIBBS and LOWCOMPLEXGIBBS

signiﬁcantly outperform GREEDY in terms of time-average

MSE; this shows the power of Gibbs sampling and learning

θ(t) over time. We have plotted only one sample path since

GIBBS and LOWCOMPLEXGIBBS converge almost surely

to the global optimum. We also observe that GIBBS con-

verges faster than LOWCOMPLEXGIBBS, since it uses more

computational and communication resources. We observe that

1 t

t τ

=1

||B

(τ

)||1

→

N

and

θ(t)

→

θ0

almost

surely

for

both GIBBS and LOWCOMPLEXGIBBS (veriﬁed by sim-

ulating multiple sample paths). It is interesting to note that θ∗ = θ1 = θ0 in this numerical example (recall Theorem 5 and
Remark 14), i.e., both GIBBS and LOWCOMPLEXGIBBS

converge to the true parameter. Convergence rate will vary

with stepsize and other parameters.

2) Performance of Algorithm 6: We consider |X | = 4

states, and assume that the sensors form a line topology. Transition probability matrix AT is chosen randomly, and

Ck = 0.1 is set in Algorithm 6 (which we call GIBBSKCF here) for all k. The values mk,i are chosen uniformly from [0, 1], and Σk,i = 0.05 ∗ (1 + |k − i|) (scalar values in this case) are set for all 1 ≤ i ≤ |X |, 1 ≤ k ≤ N .

We compared the MSE performance of three algorithms:

(i) GIBBSKCF, i.e., Algorithm 6, (ii) CENTRALKALMAN,

MSE

0.15

GIBBS

GREEDY

0.1

LOWCOMPLEXGIBBS

0.05

0

0

2000

4000

6000

8000

10000

t

(t)

0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

GIBBS

1

2

3

t

4

5

104

active sensors per slot

Mean number of

2.5

0.7

0.6

0.5 2
0.4

(t)

0.3

GIBBS

GIBBS

1.5

LOWCOMPLEXGIBBS

0.2

LOWCOMPLEXGIBBS

0.1

1

0

2000

4000

6000

8000

10000

t

0

0

1

2

3

t

Fig. 1. Performance of Algorithm 4 for centralized tracking of the iid process.

4

5

104

where a centralized Kalman ﬁlter tracks X(t) using obser-

vations only from two arbitrary sensors, and (iii) PERFECT-

BLIND, where at each time t, the state X(t − 1) is known

perfectly to all sensors, but no observation is allowed to compute Xˆ (k)(t). The MSE of PERFECTBLIND will be

limt→∞ E(V ar(X(t)|X(t − 1))).

In Figure 2, we plot the time-average MSE for all three

algorithms along one sample path, and 1t

t τ

=1

||B

(τ

)||1

for

GIBBSKCF. We observe that, 1t

t τ =1

||B(τ )||1

converges

to

N¯ . For the given sample path, GIBBSKCF provides better

MSE than PERFECTBLIND, but the MSE was seen to be

slightly worse along some other sample paths. We also observe

that the MSE of GIBBSKCF is slightly worse than CEN-

TRALKALMAN for the given instance, but they are very close

in many other problem instances (veriﬁed numerically); this

establishes the efﬁcacy of GIBBSKCF and the power of Gibbs

sampling based sensor subset selection, despite using only

one round of consensus per slot. Basically, dynamic subset

selection compensates for the performance loss due to lack of

a fusion center.

VIII. CONCLUSIONS
We have proposed low-complexity centralized and distributed learning algorithms for dynamic sensor subset selection for tracking i.i.d. time-varying as well as Markovian processes. We ﬁrst provided algorithms based on Gibbs sampling and stochastic approximation for i.i.d. time-varying data with unknown parametric distribution, and proved almost sure convergence. Next, we provided an algorthm based on Kalman consensus ﬁltering, Gibbs sampling and stochastic approximation for distributed tracking of a Markov chain. Numerical results demonstrate the efﬁcacy of the algorithms against simple algorithms without learning.

REFERENCES

[1] A. Chattopadhyay and U. Mitra. Optimal sensing and data esti-

mation in a large sensor network. Technical Report, available in

https://arxiv.org/abs/1707.08074, a shorter version has been accepted

in IEEE Globecom 2017, 2017.

[2] A. Chattopadhyay and U. Mitra.

Optimal dynamic

sensor subset selection for tracking a time-varying

stochastic process.

submitted to IEEE INFOCOM 2018,

https://drive.google.com/open?id=0B458nrWAgTUEbjd4R3Z5NS1wSlk.

[3] D. Wang, J. Fisher III, and Q. Liu. Efﬁcient observation selection

in probabilistic graphical models using bayesian lower bounds. In

MSE

0.9

0.8

0.7

0.6

0.5

0.4

0.3

PERFECTBLIND

0.2

CENTRALKALMAN

0.1

GIBBSKCF

0

0

2000

4000

6000

8000

10000

t

2.1

2

1.9

1.8

GIBBSKCF

1.7

1.6

[13] S. Das and J.M.F. Moura. Consensus+innovations distributed kalman ﬁlter with optimized gains. IEEE Transactions on Signal Processing, 65(2):467–481, 2017.
[14] S. Kar and J.M.F. Moura. Gossip and distributed kalman ﬁltering: Weak consensus under weak detectability. IEEE Transactions on Signal Processing, 59(4):1766–1784, 2011.
[15] R. Olfati-Saber. Kalman-consensus ﬁlter : Optimality, stability, and performance. In Conference on Decision and Control, pages 7036–7042. IEEE, 2009.
[16] L. Xiao, S. Boyd, and S. Lall. A scheme for robust distributed sensor fusion based on average consensus. In International Symposium on Information Processing in Sensor Networks (IPSN), pages 63–70. IEEE, 2005.
[17] V. Tzoumas, A. Jadbabaie, and G.J. Pappas. Near-optimal sensor scheduling for batch state estimation: Complexity, algorithms, and limits. In Conference on Decision and Control, pages 2695–2702. IEEE, 2016.
[18] N. Michelusi and U. Mitra. Cross-layer estimation and control for cognitive radio: Exploiting sparse network dynamics. IEEE Transactions on Cognitive Communications and Networking, 1(1):128–145, 2015.
[19] A. Chattopadhyay and B. Baszczyszyn. Gibbsian on-line distributed content caching strategy for cellular networks. https:// arxiv.org/ abs/ 1610.02318, 2016.
[20] J.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332–341, 1992.
[21] A. Chattopadhyay, M. Coupechoux, and A. Kumar. Sequential decision algorithms for measurement-based impromptu deployment of a wireless relay network along a line. IEEE/ACM Transactions on Networking, longer version available in http:// arxiv.org/ abs/ 1502.06878, 24(5):2954–2968, 2016.

Mean number of active sensors per slot

1.5

1.4

0

2000

4000

6000

8000

10000

Fig. 2. Performance of Algorithm 6 for distributed tracking of a Markov chain.

Proceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 755–764. ACM, 2016. [4] D.S. Zois, M. Levorato, and U. Mitra. Active classiﬁcation for pomdps: A kalman-like state estimator. IEEE Transactions on Signal Processing, 62(23):6209–6224, 2014. [5] D.S. Zois, M. Levorato, and U. Mitra. Energy-efﬁcient, heterogeneous sensor selection for physical activity detection in wireless body area networks. IEEE Transactions on Signal Processing, 61(7):1581–1594, 2013. [6] V. Krishnamurthy and D.V. Djonin. Structured threshold policies for dynamic sensor schedulinga partially observed markov decision process approach. IEEE Transactions on Signal Processing, 55(10):4938–4957, 2007. [7] W. Wu and A. Arapostathis. Optimal sensor querying: General markovian and lqg models with controlled observations. IEEE Transactions on Automatic Control, 53(6):1392–1405, 2008. [8] V. Gupta, T.H. Chung, B. Hassibi, and R.M. Murray. On a stochastic sensor selection algorithm with applications in sensor scheduling and sensor coverage. Automatica, 42:251–260, 2006. [9] A. Bertrand and M. Moonen. Efﬁcient sensor subset selection and link failure response for linear mmse signal estimation in wireless sensor networks. In European Signal Processing Conference (EUSIPCO), pages 1092–1096. EURASIP, 2010. [10] P. Bremaud. Markov Chains, Gibbs Fields, Monte Carlo Simulation, and Queues. Springer, 1999. [11] Vivek S. Borkar. Stochastic approximation: a dynamical systems viewpoint. Cambridge University Press, 2008. [12] F. Schnitzler, J.Y. Yu, and S. Mannor. Sensor selection for crowdsensing dynamical systems. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 829–837, 2015.

works,

cyber-physical

Arpan Chattopadhyay obtained his B.E. in Electronics and Telecommunication Engineering from Jadavpur University, Kolkata, India in the year 2008, and M.E. and Ph.D in Telecommunication Engineering from Indian Institute of Science, Bangalore, India in the year 2010 and 2015, respectively. He is currently working in the Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles as a postdoctoral researcher. Previously he worked as a postdoc in INRIA/ENS Paris. His research interests include wireless netsystems, machine learning and control.

Urbashi Mitra is a Deans Professor in the De-

partments of Electrical Engineering and Computer

Science, University of Southern California, Los

PLACE PHOTO HERE

Angeles, CA, USA. Previous appointments include Bellcore and the Ohio State University. Her honors include: U.S.-U.K. Fulbright, Leverhulme Trust Visiting Professorship, Editor-in-Chief IEEE

TRANSACTIONS ON MOLECULAR, BIOLOGI-

CAL AND MULTISCALE COMMUNICATIONS,

IEEE Communications Society Distinguished Lec-

turer, U.S. NAE Galbreth Lectureship, Okawa Foun-

dation Award, and an NSF CAREER Award. Her research is in wireless

communications.

APPENDIX A PROOF OF THEOREM 1
We will prove only the ﬁrst part of the theorem where there exists one B∗. The second part of the theorem can be proved similarly.
Let us denote the optimizer for (P1) by B, which is possibly different from B∗. Then, by the deﬁnition of B∗, we have f (B∗) + λ∗||B∗||1 ≤ f (B) + λ∗||B||1. But ||B||1 ≤ K (since B is a feasible solution to the constrained problem) and ||B∗||1 = K (by assumption). Hence, f (B∗) ≤ f (B). This completes the proof.

APPENDIX B WEAK AND STRONG ERGODICITY
Consider a discrete-time Markov chain (possibly not timehomogeneous) {B(t)}t≥0 with transition probability matrix (t.p.m.) P (m; n) between t = m and t = n. We denote by D the collection of all possible probasbility distributions on the state space. Let dV (·, ·) denote the total variation distance between two distributions in D. Then {B(t)}t≥0 is called weakly ergodic if, for all m ≥ 0, we have limn↑∞ supµ,ν∈D dV (µP (m; n), νP (m; n)) = 0.
The Markov chain {B(t)}t≥0 is called strongly ergodic if there exists π ∈ D such that, limn↑∞ supµ∈D dV (µT P (m; n), π) = 0 for all m ≥ 0.

APPENDIX C PROOF OF THEOREM 3

We will ﬁrst show that the Markov chain {B(t)}t≥0 in weakly ergodic.

Let us deﬁne ∆ := maxB∈B,A∈B |h(B) − h(A)|.

Consider the transition probability matrix (t.p.m.) Pl for
the inhomogeneous Markov chain {Y (l)}l≥0 (where Y (l) :=
B(lN )). The Dobrushin’s ergodic coefﬁcient δ(Pl) is given
by (see [10, Chapter 6, Section 7] for deﬁnition) δ(Pl) =
1 − infB ,B ∈B B∈B min{Pl(B , B), Pl(B , B)}. A sufﬁcient condition for the Markov chain {B(t)}t≥0 to be weakly ergodic is ∞ l=1(1 − δ(Pl)) = ∞ (by [10, Chapter 6, Theorem 8.2]).

Now, with positive probability, activation states for all nodes

are updated over a period of N slots. Hence, Pl(B , B) > 0 for
all B , B ∈ B. Also, once a node jt for t = lN + k is chosen
in MODIFIEDGIBBS algorithm, the sampling probability for any activation state in a slot is greater than e−β(lN2 +k)∆ . Hence, for independent sampling over N slots, we have, for all pairs

B , B:

N −1 e−β(lN +k)∆

Pl(B , B) >

>0 2N

k=0

Hence,
= ≥ ≥ = ≥ =

∞

(1 − δ(Pl))

l=0 ∞

inf

min{Pl(B , B), Pl(B , B)}

l=0 B ,B ∈B B∈B

∞

N −1

2N

l=0

k=0

e−β(0) log(1+lN +k)×∆ 2N

∞ N −1 e−β(0) log(1+lN +N )×∆

l=0 k=0
1∞

N 1

NN

(1 + lN )β(0)N∆

l=1

1

∞

1

N N+1

(1 + i)β(0)N∆

i=N +1

∞

(9)

Here the ﬁrst inequality uses the fact that the cardinality of B is 2N . The second inequality follows from replacing k by

N in the numerator. The third inequality follows from lower-

bounding

1 (1+lN )β(0)N∆

by

1 N

ilN=l+NN −1 (1+i)β1(0)N∆ . The last

equality follows from the fact that

∞1 i=1 ia

diverges

for

0<

a < 1.

Hence, the Markov chain {B(t)}t≥0 is weakly ergodic.

In order to prove strong ergodicity of {B(t)}t≥0, we

invoke [10, Chapter 6, Theorem 8.3]. We denote the t.p.m.

of {B(t)}t≥0 at a speciﬁc time t = T0 by Q(T0), which is

a given speciﬁc matrix. If {B(t)}t≥0 evolves up to inﬁnite

time with ﬁxed t.p.m. Q(T0), then it will reach the stationary

distribution πβ (B) = e−βT0 h(B) . Hence, we can claim that

T0

ZβT0

Condition 8.9 of [10, Chapter 6, Theorem 8.3] is satisﬁed.

Next, we check Condition 8.10 of [10, Chapter 6, Theo-

rem 8.3]. For any B ∈ arg minB ∈B h(B ), we can argue
that πβT0 (B) increases with T0 for sufﬁciently large T0; this can be veriﬁed by considering the derivative of πβ(B) w.r.t.

β. For B ∈/ arg minB ∈B h(B ), the probability πβT0 (B) decreases with T0 for large T0. Now, using the fact that

any monotone, bounded sequence converges, we can write ∞ T0=0 B∈B |πβT0+1 (B) − πβT0 (B)| < ∞.
Hence, by [10, Chapter 6, Theorem 8.3], the Markov chain

{B(t)}t≥0 is strongly ergodic. It is straightforward to verify

the claim regarding the limiting distribution.

APPENDIX D PROOF OF LEMMA 1
Let λ1 > λ2 > 0, and the corresponding optimal error and mean number of active sensors under these multiplier values be (f1, n1) and (f2, n2), respectively. Then, by deﬁnition, f1+ λ1n1 ≤ f2 + λ1n2 and f2 + λ2n2 ≤ f1 + λ2n1. Adding these two inequalities, we obtain λ1n1 + λ2n2 ≤ λ1n2 + λ2n1, i.e., (λ1 − λ2)n1 ≤ (λ1 − λ2)n2. Since λ1 > λ2, we obtain n1 ≤ n2. This completes the ﬁrst part of the proof. The second

part of the proof follows using similar arguments.

1) Convergence in the fastest timescale: Let us denote the

probability distribution of B(t) under Algorithm 4 by π(t) (a

APPENDIX E PROOF OF LEMMA 2

Let us denote Eµ2 ||B||1 =: g(λ) =

. B∈B ||B||1e−βh(B)
Zβ

It is straightforward to see that Eµ2 ||B||1 is continuously differentiable in λ. Let us denote Zβ by Z for simplicity. The

derivative of g(λ) w.r.t. λ is given by:

column vector indexed by the coﬁgurations from B), and the
corresponding transition probability matrix (TPM) by A(t); i.e., (π(t+1))T = (π(t))T A(t) = (1 − 1) × (π(t))T + 1 × (π(t))T A(t). This form is similar to a standard stochastic
approximation scheme as in [11, Chapter 2] except that the step size sequence for π(t) iteration is a constant sequence. Also, if f (t)(B), λ(t) and θ(t) are constant with time t, then

g (λ)

A(t) = A will also be constant with time t, and the stationary

−Zβ B∈B ||B||21e−β(f(B)+λ||B||1) − B∈B ||B||1e−β(f(B)+λ||B||1) ddZλdistribution for the TPM A will exist and will be Lipschitz

=

Z2

continuous in all (constant) slower timescale iterates. Hence,

Now, it is straightforward to verify that ddZλ = −βZg(λ). by using similar argument as in [11, Chapter 6, Lemma 1],

Hence,

one can show the following for all B ∈ B:

g (λ) −Zβ B∈B ||B||21e−β(f (B)+λ||B||1) + B∈B ||B||1e−β(f (B)+λ||B||1) βZg(λ) =
Z2

lim |π(t)(B) − πβ,f(t),λ(t),θ(t)(B)| = 0 a.s. (11)
t→∞

Now, g (λ) ≤ 0 is equivalent to

2) Convergence of iteration (4): Note that, (4) depends on θ(t) and not on B(t) and λ(t); the iteration (4) depends on

g(λ) ≤

B∈B ||B||21e−β(f (B)+λ||B||1) B∈B ||B||1e−β(f (B)+λ||B||1)

θ(t) through the estimation function µ2(·; ·; ·). Now, f (t)(B) is updated at a faster timescale compared to θ(t). Let us consider the iterations (4) and (2); they constitute a two-

Noting that E||B||1 =: g(λ) and dividing the numerator and timescale stochastic approximation.

denominator of R.H.S. by Z, the condition is reduced to E||B||1 ≤ EE||||BB||||121 , which is true since E||B||21 ≥ (E||B||1)2.

Note that, for a given θ, the iteration (4) remains bounded inside a compact set independent of θ; hence, using [11,

Hence, E||B||1 is decreasing in λ for any β > 0. Also, it Chapter 2, Theorem 2] with additional modiﬁcation as sugis easy to verify that |g (λ)| ≤ (β + 1)N 2. Hence, g(λ) is gested in [11, Chapter 5, Section 5.4] for projected stochastic

Lipschitz continuous in λ.

approximation, we can claim that limt→∞ f (t)(B) → fθ(B)

almost surely for all B ∈ B, if θ(t) is kept ﬁxed at a value

APPENDIX F PROOF OF THEOREM 4
Let the distribution of B(t) under Algorithm 3 be π(t)(·). Since limt→∞ a(t) = 0, it follows that limt→∞ dV (π(t−1), πβ|λ(t−1)) = 0 (where dV (·, ·) is the total variation distance), and limt→∞(Eπ(t−1) ||B||1 − Eπβ|λ(t−1)||B||1) := limt→∞ e(t) = 0. Now, we can rewrite the λ(t) update equation as follows:

θ. Also, since µ2(·; ·; θ) is Lipschitz continuous in θ, we can

claim that fθ(B) is Lipschitz continuous in θ for all B ∈ B.

We

also

have

limt→∞

c(ν(t)) a(ν(t))

=

0.

Hence, by using an analysis similar to that in [21, Ap-

pendix E, Section C.2] (which uses [11, Chapter 6, Lemma 1]),

one can claim that:

lim |f (t)(B) − fθ(t)(B)| = 0 a.s. ∀B ∈ B (12)
t→∞

λ(t + 1) = [λ(t) + a(t)(Eπβ|λ(t−1)||B||1 − N¯ + Mt + et)]cb (10) This proves the desired convergence of the iteration (4).

Here Mt := ||B(t − 1)||1 − Eπ(t−1) ||B(t − 1)||1 is a Martingale difference noise sequence, and limt→∞ et = 0. It is easy to see that the derivative of Eπβ|λ||B||1 w.r.t. λ is bouned for λ ∈ [b, c]; hence, Eπβ|λ||B||1 is a Lipschitz continuous function of λ. It is also easy to see that the sequence {Mt}t≥0 is bounded. Hence, by the theory presented in [11, Chapter 2] and [11, Chapter 5, Section 5.4], λ(t) converges to the unique zero of Eπβ|λ||B||1 − N¯ almost surely. Hence, λ(t) → λ∗ almost surely. Since limt→∞ dV (π(t−1), πβ|λ(t−1)) = 0 and πβ|λ is continuous in λ, the limiting distribution of B(t) becomes πβ|λ∗ .
APPENDIX G PROOF OF THEOREM 5
The proof involves several steps, and the brief outline of these steps are provided one by one.

3) Convergence of λ(t) iteration: The λ(t) iteration will view θ(t) as quasi-static and B(t), f (t)(·) iterations as equi-
libriated.
Let us assume that θ(t) is kept ﬁxed at θ. Then, by (11) and (12), we can work with πβ,fθ,λ(t),θ in this timescale. Under this situation, (3) asymptotically tracks the iteration λ(t + 1) = [λ(t) + b(t)( B∈B πβ,fθ,λ(t),θ(B)||B||1 − N¯ + Mt)]A0 where {Mt}t≥0 is a Martingale differenece sequence. Now, πβ,fθ,λ(t),θ(B) is Lipschitz continuous in θ and λ(t) (using Assumption 2, Assumption 4 and a little algebra on the expression (1)). If A0 is large enough, then, by the theory of [11, Chapter 2, Theorem 2] and [11, Chapter 5, Section 5.4], one can claim that λ(t) → λ∗(θ) almost surely, and λ∗(θ) is Lipschitz continuous in θ (by Assumption 4).
Hence, by using similar analysis as in [21, Appendix E, Section C.2] (which uses [11, Chapter 6, Lemma 1]), we can

say that, under iteration (3):

lim |λ(t) − λ∗(θ(t))| = 0 a.s.

(13)

t→∞

4) Convergence of the θ(t) iteration: Note that, (2) is the
slowest timescale iteration and hence it will view all other
there iterations (at three different timescales) as equilibriated.
However, this iteration is not affected by other iterations.
Hence, this iteration is an example of simultaneous perturbation stochastic approximation as in [20] (since θ∗ lies inside
the interior of Θ), but with a projection operation applied on
the iterates. Hence, by combining [20, Proposition 1] and the
discussion in [11, Chapter 5, Section 5.4], we can say that limt→∞ θ(t) = θ∗ almost surely.
5) Completing the proof: We have seen that limt→∞ θ(t) = θ∗ almost surely. Hence, by (13), limt→∞ λ(t) = λ∗(θ∗) almost surely. By (12), limt→∞ f (t)(B) = fθ∗ (B) almost surely for all B ∈ B. Then, by (11), limt→∞ π(t)(B) = πβ,fθ∗ ,λ∗(θ∗),θ∗ (B) almost surely. Hence, Theorem 5 is proved.

