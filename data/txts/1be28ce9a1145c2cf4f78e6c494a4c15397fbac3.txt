arXiv:2010.01450v2 [cs.LG] 6 May 2021

SumGNN: Multi-typed Drug Interaction Prediction via Efﬁcient Knowledge Graph Summarization
Yue Yu*1, Kexin Huang*2, Chao Zhang1, Lucas M. Glass3, Jimeng Sun4, and Cao Xiao3
1College of Computing, Georgia Institute of Technology, Atlanta, GA 2Health Data Science, Harvard T.H. Chan School of Public Health, Boston, MA
3Analytic Center of Excellence, IQVIA, Cambridge, MA 4Department of Computer Science, University of Illinois at Urbana-Champaign,
Urbana, IL
May 10, 2021
Abstract
Motivation: Thanks to the increasing availability of drug-drug interactions (DDI) datasets and large biomedical knowledge graphs (KGs), accurate detection of adverse DDI using machine learning models becomes possible. However, it remains largely an open problem how to effectively utilize large and noisy biomedical KG for DDI detection. Due to its sheer size and amount of noise in KGs, it is often less beneﬁcial to directly integrate KGs with other smaller but higher quality data (e.g., experimental data). Most of existing approaches ignore KGs altogether. Some tries to directly integrate KGs with other data via graph neural networks with limited success. Furthermore most previous works focus on binary DDI prediction whereas the multi-typed DDI pharmacological effect prediction is more meaningful but harder task. Results: To ﬁll the gaps, we propose a new method SumGNN: knowledge summarization graph neural network, which is enabled by a subgraph extraction module that can efﬁciently anchor on relevant subgraphs from a KG, a self-attention based subgraph summarization scheme to generate reasoning path within the subgraph, and a multi-channel knowledge and data integration module that utilizes massive external biomedical knowledge for signiﬁcantly improved multi-typed DDI predictions. SumGNN outperforms the best baseline by up to 5.54%, and performance gain is particularly signiﬁcant in low data relation types. In addition, SumGNN provides interpretable prediction via the generated reasoning paths for each prediction. Availability: The code and data are available at https://github.com/yueyu1030/SumGNN. Contact: cao.xiao@iqvia.com Supplementary information: Supplementary data are available at Bioinformatics online
*Equal Contribution
1

1 Introduction
Adverse drug-drug interactions (DDI) are modiﬁcations of the effect of a drug when administered with another drug, which is a common and dangerous scenario for patients with complicated conditions. Undetected adverse DDIs have become serious health threats and caused nearly 74, 000 emergency room visits and 195, 000 hospitalizations each year in the United States alone (Percha and Altman, 2013). To mitigate these risks and costs, accurate prediction of DDIs becomes a clinically important task. Two types of data are being utilized for developing DDI detection models: Manually curated DDI networks and large biomedical knowledge graphs. Curated DDI networks: Researchers have curated DDI networks based on experimental datasets and literature such as TWOSIDES (Tatonetti et al., 2012), MINER (Zitnik et al., 2018b) and DrugBank (Wishart et al., 2018; Ryu et al., 2018). These curated data are of higher quality but expensive to create and usually smaller in size. Knowledge Graph: Over the years, large knowledge graph (KG) such as (Rotmensch et al., 2017), Hetionet (Himmelstein and Baranzini, 2015) and DRKG (Ioannidis et al., 2020) have been constructed from literature mining and database integration. However, these KGs are large and noisy: out of their tens of thousands of nodes with millions of edges, only a small subgraph is relevant to a prediction target. Deep Learning: Graph neural networks (GNN) have achieved great performance by casting DDI prediction as a link prediction problem on DDI graphs (Gysi et al., 2020; Zitnik et al., 2018a; Huang et al., 2020a). However, existing deep learning models are often trained only based on the DDI dataset at hand, ignoring the large biomedical knowledge graph (Ioannidis et al., 2020; Himmelstein and Baranzini, 2015) which can beneﬁt the DDI predictions since DDI is driven by complicated biomedical mechanism. Some recent works (Karim et al., 2019; Lin et al., 2020) tried to integrate knowledge graph into the DDI prediction via direct integration of standard KG and GNN methods. But DDI prediction presents unique modeling difﬁculties since the input KG is large and noisy while the pertinent information for a drug pair is local. Moreover, most existing works also only make binary classiﬁcation - predicting the presence of DDIs, despite that predicting the particular DDI type is a more meaningful task. Our Approach. In this work, we propose a new method SumGNN that efﬁciently uses KG to aid drug interaction prediction. SumGNN enjoys improved predictive performance, efﬁciency, inductiveness and interpretability. SumGNN provides the following technical contributions:
1. Local subgraph for identifying useful information. We use local subgraph in the KG around drug pairs to extract useful information, instead of the entire KG. The subgraph formulation allows noise reduction by anchoring on relevant information and is highly scalable since the message passing receptive ﬁeld is signiﬁcantly decreased.
2. Subgraph summarization scheme for generating reasoning path. We then propose a summarization scheme to generate mechanism pathway for drug interactions. We develop a layer-independent self-attention mechanism to generate signal intensity score for each edge in the subgraph and prune out a KG subgraph pathways that have high scores. As this pruned subgraph is sparse, it provides insights on the biological processes that drive drug interactions.
3. Multi-channel data and knowledge integration for improved multi-typed DDI predictions. We propose to use multi-channel neural encoding to aggregate diverse set of data sources, ranging from the summarized subgraph embedding to chemical structures. It enables utilization of massive external biomedical knowledge for signiﬁcantly improved multi-typed DDI predictions. In addition, the neural encoding takes different subgraph in
2

each propagation, forming an inductive bias that promotes generalizability in low-resource DDI types.
We conduct extensive experiments to show SumGNN improves DDI prediction signiﬁcantly. It has up to 5.54% increase over the best baseline in F1 while the inference time is greatly reduced. Moreover, SumGNN excels at low-resource settings whereas previous works do not. SumGNN is also able to provide reasonable clues about the underlying mechanism of the drug interactions.
2 Related Works
External knowledge graph integration. Recently, several efforts have attempted to leverage the KG for downstream tasks such as recommendation (Wang et al., 2019a,b; Sun et al., 2018), information extraction (Wang et al., 2018; Liang et al., 2020) and drug interaction prediction (Celebi et al., 2019; Karim et al., 2019; Lin et al., 2020). For drug interaction prediction, Takeda et al. (2017) integrate the pharmacokinetic (PK) or pharmacodynamic (PD) side-effect when predicting drug interaction and Li et al. (2015) develop a Bayesian network to combine molecular similarity and drug side-effect similarity to predict the drug effect. However, these methods only consider side effect as the external knowledge, which may not be comprehensive enough in our task. With the emergence of the biomedical knowledge graphs (Himmelstein and Baranzini, 2015; Rotmensch et al., 2017; Ioannidis et al., 2020), more types of entities and relations have been applied to this task, as Wang (2017); Burkhardt et al. (2019); Celebi et al. (2019); Karim et al. (2019); Dai et al. (2020) project each entity and relation to a dense vector with knowledge graph embedding techniques (Bordes et al., 2013; Su et al., 2020; Trouillon et al., 2016) and then feed them to neural networks for prediction. However, they do not directly harness the neighborhood information for target entities during inference, thus the external knowledge information are not sufﬁciently exploited. To tackle the above drawback, (Lin et al., 2020) adopts graph convolutional networks with neighborhood sampling to explicitly model the neighborhood relations with higher inference speed. However, as each neighboring entity could play a crucial role in the drug interaction mechanism, random sampling could potentially dropout these important factors and hinders the prediction performance. In contrast, SumGNN provides a learnable way to extract useful information in the neighborhood. In addition, the previous works all focus on binary DDI prediction whereas SumGNN evaluates on multi-type relation network.
Subgraph Graph Neural Network. Graph neural networks have been proposed for modeling the relation between nodes (Kipf and Welling, 2017; Velicˇkovic´ et al., 2018; Schlichtkrull et al., 2018; Yu et al., 2020c; Srinivasa et al., 2020) and have been successfully applied to various domains (Shi et al., 2020; Yu et al., 2020b; Shang et al., 2019; Yu et al., 2020a). Subgraph structure contains rich information for many graph learning tasks (Teru and Hamilton, 2020; Velicˇkovic´ et al., 2019; Huang and Zitnik, 2020). For instance, Ego-CNN applies local ego network to identify structures for graph classiﬁcation (Tzeng and Wu, 2019). Alsentzer et al. (2020) formulate a multi-channel way to subgraph classiﬁcation. Cluster-GCN (Chiang et al., 2019) and GraphSAINT (Zeng et al., 2020) use subgraphs to improve GNN scalability. More relevant to us, Zhang and Chen (2018) apply local subgraph for link prediction and GraIL (Teru and Hamilton, 2020) extend this idea into KG completion task via utilizing multi-relational information. In contrast, SumGNN is driven by the domain DDI prediction problem and is the ﬁrst to design a graph summarization module on subgraphs to obtain tractable pathway. It also integrates a new multi-channel neural encoding mechanism. We show these modules signiﬁcantly improve predictive performance over these related works in Section 4.2.
3

u< l a t e x i t s h a 1 _ b a s e 6 4 = " s e 8 y c s t Y h 4 Z 5 R z S H N Q / a 0 9 e G r D Q = " > A A A B 7 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U x c C F 7 C 1 7 s G F v 7 7 I 7 Z 0 I u / A Q b C 2 2 M r f / H z n / j A l c o + J L d v L w 3 k 5 l 5 Q S K F Q d f 9 d g o r q 2 v r G 8 X N 0 t b 2 z u 5 e e f / g 3 s S p Z r z F Y h n r d k A N l 0 L x F g q U v J 1 o T q N A 8 o d g d D X 1 H 5 6 4 N i J W d z h O u B / R g R K h Y B S t d F t N q 7 1 y x a 2 5 M 5 B l 4 u W k A j m a v f J X t x + z N O I K m a T G d D w 3 Q T + j G g W T f F L q p o Y n l I 3 o g H c s V T T i x s 9 m q 0 7 I i V X 6 J I y 1 f Q r J T P 3 d k d H I m H E U 2 M q I 4 t A s e l P x P 6 + T Y n j h Z 0 I l K X L F 5 o P C V B K M y f R u 0 h e a M 5 R j S y j T w u 5 K 2 J B q y t C m U 7 I h e I s n L 5 P H e s 0 7 q 9 m v f l O v N C 7 z R I p w B M d w C h 6 c Q w O u o Q k t Y D C A Z 3 i F N 0 c 6 L 8 6 7 8 z E v L T h 5 z y H 8 g f P 5 A 1 k 2 j f Q = < / l a t e x i t >

?< l a t e x i t s h a 1 _ b a s e 6 4 = " u d 1 P I n 1 w B 0 H 0 L Z V C L 8 i s l 3 / f I h k = " > A A A B 7 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t J N o Y 4 l R B A M X s r c M s G F v 7 7 K 7 Z 0 I u / A Q b C 2 2 M r f / H z n / j A l c o + J L d v L w 3 k 5 l 5 Q S y 4 N q 7 7 7 e R W V t f W N / K b h a 3 t n d 2 9 4 v 7 B g 4 4 S x b D B I h G p V k A 1 C i 6 x Y b g R 2 I o V 0 j A Q 2 A x G 1 1 O / + Y R K 8 0 j e m 3 G M f k g H k v c 5 o 8 Z K d + X L c r d Y c i v u D G S Z e B k p Q Y Z 6 t / j V 6 U U s C V E a J q j W b c + N j Z 9 S Z T g T O C l 0 E o 0 x Z S M 6 w L a l k o a o / X S 2 6 o S c W K V H + p G y T x o y U 3 9 3 p D T U e h w G t j K k Z q g X v a n 4 n 9 d O T P / C T 7 m M E 4 O S z Q f 1 E 0 F M R K Z 3 k x 5 X y I w Y W 0 K Z 4 n Z X w o Z U U W Z s O g U b g r d 4 8 j J 5 r F a 8 s 4 r 9 q r f V U u 0 q S y Q P R 3 A M p + D B O d T g B u r Q A A Y D e I Z X e H O E 8 + K 8 O x / z 0 p y T 9 R z C H z i f P w a G j b 4 = < / l a t e x i t >

v< l a t e x i t s h a 1 _ b a s e 6 4 = " y U q B n o M 2 N g 7 i X 3 B N c U s A k K t x O b Y = " > A A A B 7 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U w c C F 7 C 1 7 s G F v 7 7 I 7 R 0 I I P 8 H G Q h t j 6 / + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e k E h h 0 H W / n d z a + s b m V n 6 7 s L O 7 t 3 9 Q P D x 6 N H G q G W + w W M a 6 F V D D p V C 8 g Q I l b y W a 0 y i Q v B k M b 2 Z + c 8 S 1 E b F 6 w H H C / Y j 2 l Q g F o 2 i l + / K o 3 C 2 W 3 I o 7 B 1 k l X k Z K k K H e L X 5 1 e j F L I 6 6 Q S W p M 2 3 M T 9 C d U o 2 C S T w u d 1 P C E s i H t 8 7 a l i k b c + J P 5 q l N y Z p U e C W N t n 0 I y V 3 9 3 T G h k z D g K b G V E c W C W v Z n 4 n 9 d O M b z y J 0 I l K X L F F o P C V B K M y e x u 0 h O a M 5 R j S y j T w u 5 K 2 I B q y t C m U 7 A h e M s n r 5 K n a s W 7 q N i v e l c t 1 a 6 z R P J w A q d w D h 5 c Q g 1 u o Q 4 N Y N C H Z 3 i F N 0 c 6 L 8 6 7 8 7 E o z T l Z z z H 8 g f P 5 A 1 q + j f U = < / l a t e x i t >

G< l a t e x i t s h a 1 _ b a s e 6 4 = " y G L V L h e X X 4 z r Q 4 R L W v a 2 7 V 9 5 F g Y = " > A A A B / 3 i c b V C 9 T s M w G H T K X y l / g Y 4 s F i 0 S U 5 W U A c Y K K g F b k S g t a q P I c Z 3 W q u N E t o M U R e F V W B h g Q a y 8 B h t v g 9 N m g J a T b J 3 u v k 8 + n x c x K p V l f R u l l d W 1 9 Y 3 y Z m V r e 2 d 3 z 9 w / u J d h L D D p 4 p C F o u 8 h S R j l p K u o Y q Q f C Y I C j 5 G e N 7 3 M / d 4 j E Z K G / E 4 l E X E C N O b U p x g p L b l m t T 4 M k J p g x N K r z E 3 b 7 Z u s 7 p o 1 q 2 H N A J e J X Z A a K N B x z a / h K M R x Q L j C D E k 5 s K 1 I O S k S i m J G s s o w l i R C e I r G Z K A p R w G R T j o L n 8 F j r Y y g H w p 9 u I I z 9 f d G i g I p k 8 D T k 3 l S u e j l 4 n / e I F b + u Z N S H s W K c D x / y I 8 Z V C H M m 4 A j K g h W L N E E Y U F 1 V o g n S C C s d F 8 V X Y K 9 + O V l 8 t B s 2 K c N f T V v m 7 X W R d F I G R y C I 3 A C b H A G W u A a d E A X Y J C A Z / A K 3 o w n 4 8 V 4 N z 7 m o y W j 2 K m C P z A + f w D H v 5 T t < / l a t e x i t > D

D

I

Melatonin Thiamine

Different Biomedical Entities

Subgraph

Subgraph Pruning

Subgraph Extraction
Summarization

G< l a t e x i t s h a 1 _ b a s e 6 4 = " B 7 2 E D l r A U 4 d g M 3 / a C U I 0 M 1 T s o 1 w = " > A A A B / n i c b V C 7 T s M w F L 3 h W c o r l J H F o k V i q p I y w F j B U C S W I l F a 1 E a R 4 z q t V e c h 2 0 F U U X 6 F h Q E W x M p 3 s P E 3 O G 0 G a D m S r a N z 7 p W P j x d z J p V l f R s r q 2 v r G 5 u l r f L 2 z u 7 e v n l Q u Z d R I g j t k I h H o u d h S T k L a U c x x W k v F h Q H H q d d b 3 K V + 9 1 H K i S L w j s 1 j a k T 4 F H I f E a w 0 p J r V m q D A K s x w T x t Z W 5 6 0 8 p q r l m 1 6 t Y M a J n Y B a l C g b Z r f g 2 G E U k C G i r C s Z R 9 2 4 q V k 2 K h G O E 0 K w 8 S S W N M J n h E + 5 q G O K D S S W f Z M 3 S i l S H y I 6 F P q N B M / b 2 R 4 k D K a e D p y T y o X P R y 8 T + v n y j / w k l Z G C e K h m T + k J 9 w p C K U F 4 G G T F C i + F Q T T A T T W R E Z Y 4 G J 0 n W V d Q n 2 4 p e X y U O j b p / V 9 d W 4 b V S b l 0 U j J T i C Y z g F G 8 6 h C d f Q h g 4 Q e I J n e I U 3 I z N e j H f j Y z 6 6 Y h Q 7 h / A H x u c P O + y U p A = = < / l a t e x i t > K

G

Drug Pairs of Interest Subgraph

G< l a t e x i t s h a 1 _ b a s e 6 4 = " q 2 T u F w J P X d u 0 l 6 6 Q 3 W H q j z d J C P 8 = " > A A A B / 3 i c b V C 7 T s M w F L 0 p r 1 J e g Y 4 s F i 0 S U 5 W U A c Y K B h i L o L S o j S L H d V u r z k O 2 g x R F 4 V d Y G G B B r P w G G 3 + D 0 2 a A l i P Z O j r n X v n 4 e B F n U l n W t 1 F a W V 1 b 3 y h v V r a 2 d 3 b 3 z P 2 D e x n G g t A O C X k o e h 6 W l L O A d h R T n P Y i Q b H v c d r 1 p p e 5 3 3 2 k Q r I w u F N J R B 0 f j w M 2 Y g Q r L b l m t T 7 w s Z o Q z N O r z E 1 v Y y + r u 2 b N a l g z o G V i F 6 Q G B d q u + T U Y h i T 2 a a A I x 1 L 2 b S t S T o q F Y o T T r D K I J Y 0 w m e I x 7 W s a Y J 9 K J 5 2 F z 9 C x V o Z o F A p 9 A o V m 6 u + N F P t S J r 6 n J / O k c t H L x f + 8 f q x G 5 0 7 K g i h W N C D z h 0 Y x R y p E e R N o y A Q l i i e a Y C K Y z o r I B A t M l O 6 r o k u w F 7 + 8 T B 6 a D f u 0 o a / m T b P W u i g a K c M h H M E J 2 H A G L b i G N n S A Q A L P 8 A p v x p P x Y r w b H / P R k l H s V O E P j M 8 f U L 6 V R g = = < / l a t e x i t > S

ub

Subgraph Neural Encoding

h< l a t e x i t s h a 1 _ b a s e 6 4 = " j M B K h 3 K j u 0 Q f J C Q 2 + V Q W l x + x 6 5 Q = " > A A A B / H i c b V C 9 T s M w G P x S / k r 5 C z C y W L R I T F V S B h g r W B i L R C m o j S L H d V q r j h P Z T q U q y p u w M M C C W H k Q N t 4 G p 8 0 A L S f Z O t 1 9 n 3 y + I O F M a c f 5 t i p r 6 x u b W 9 X t 2 s 7 u 3 v 6 B f X j 0 o O J U E t o l M Y / l Y 4 A V 5 U z Q r m a a 0 8 d E U h w F n P a C y U 3 h 9 6 Z U K h a L e z 1 L q B f h k W A h I 1 g b y b f t x i D C e h y E 2 T j 3 s z R v + H b d a T p z o F X i l q Q O J T q + / T U Y x i S N q N C E Y 6 X 6 r p N o L 8 N S M 8 J p X h u k i i a Y T P C I 9 g 0 V O K L K y + b J c 3 R m l C E K Y 2 m O 0 G i u / t 7 I c K T U L A r M Z B F T L X u F + J / X T 3 V 4 5 W V M J K m m g i w e C l O O d I y K G t C Q S U o 0 n x m C i W Q m K y J j L D H R p q y a K c F d / v I q e W o 1 3 Y u m u V p 3 r X r 7 u m y k C i d w C u f g w i W 0 4 R Y 6 0 A U C U 3 i G V 3 i z M u v F e r c + F q M V q 9 w 5 h j + w P n 8 A T X a U L A = = < / l a t e x i t > u

h< l a t e x i t s h a 1 _ b a s e 6 4 = " 2 G J z i 2 E P g m r 2 k J v W E B r t O l S H s / w = " > A A A B / H i c b V C 9 T s M w G P x S / k r 5 C z C y W L R I T F V S B h g r W B i L R C m o j S L H d V q r j h P Z T q U q y p u w M M C C W H k Q N t 4 G p 8 0 A L S f Z O t 1 9 n 3 y + I O F M a c f 5 t i p r 6 x u b W 9 X t 2 s 7 u 3 v 6 B f X j 0 o O J U E t o l M Y / l Y 4 A V 5 U z Q r m a a 0 8 d E U h w F n P a C y U 3 h 9 6 Z U K h a L e z 1 L q B f h k W A h I 1 g b y b f t x i D C e h y E 2 T j 3 s 2 n e 8 O 2 6 0 3 T m Q K v E L U k d S n R 8 + 2 s w j E k a U a E J x 0 r 1 X S f R X o a l Z o T T v D Z I F U 0 w m e A R 7 R s q c E S V l 8 2 T 5 + j M K E M U x t I c o d F c / b 2 R 4 U i p W R S Y y S K m W v Y K 8 T + v n + r w y s u Y S F J N B V k 8 F K Y c 6 R g V N a A h k 5 R o P j M E E 8 l M V k T G W G K i T V k 1 U 4 K 7 / O V V 8 t R q u h d N c 7 X u W v X 2 d d l I F U 7 g F M 7 B h U t o w y 1 0 o A s E p v A M r / B m Z d a L 9 W 5 9 L E Y r V r l z D H 9 g f f 4 A T v + U L Q = = < / l a t e x i t > v

DDI Type 18: Increase of cardiotoxic
activity.

h
< l a t e x i t s h a 1 _ b a s e 6 4 = " B J U w f X L y F + P X V / k n A K / L 8 w i J b / A = " > A A A C D H i c b V C 7 T s M w F L 0 p r 1 J e A U Y W i x a J q U r K A G M F A 4 x F U F r U R p X j O q 1 V 5 y H b Q a q i r C z 8 C g s D L I i V D 2 D j b 3 D S D N B y J F v H 5 9 w r 3 3 v c i D O p L O v b K C 0 t r 6 y u l d c r G 5 t b 2 z v m 7 t 6 d D G N B a J u E P B R d F 0 v K W U D b i i l O u 5 G g 2 H c 5 7 b i T i 8 z v P F A h W R j c q m l E H R + P A u Y x g p W W B i a q 9 X 2 s x q 6 X j N N B k n O C e X K p H z e x m 6 a 1 g V m 1 6 l Y O t E j s g l S h Q G t g f v W H I Y l 9 G i j C s Z Q 9 2 4 q U k 2 C h G O E 0 r f R j S S N M J n h E e 5 o G 2 K f S S f J N U n S k l S H y Q q F P o F C u / u 5 I s C / l 1 H d 1 Z T a q n P c y 8 T + v F y v v z E l Y E M W K B m T 2 k R d z p E K U x Y K G T F C i + F Q T T A T T s y I y x g I T p c O r 6 B D s + Z U X y X 2 j b p / U 9 d W 4 b l S b 5 0 U i Z T i A Q z g G G 0 6 h C V f Q g j Y Q e I R n e I U 3 4 8 l 4 M d 6 N j 1 l p y S h 6 9 u E P j M 8 f J w G b T Q = = < / l a t e x i t >

G

S

ub

Pruned Edges

Signal Edges

Figure 1: SumGNN illustration.
3 Method
We present SumGNN in this section1. We summarize problem settings in Section 3.1 and describe our method in detail in Section 3.2. Our method can be decomposed into three modules. First, we extract the local subgraph in the KG around drug pairs to obtain useful information. Then, we propose a summarization scheme to generate a mechanism pathway for drug interactions. After that, we describe a multi-channel neural encoding layer to predict the pharmacological effect.
3.1 Problem Settings
Deﬁnition 1 (Drug Interaction Graph). Given drugs D and pharmacological effects RD, the drug interaction graph GDDI is deﬁned as a set of triplets GDDI = {(u, r, v) | u ∈ D, r ∈ RD, v ∈ D)}, where each triplet (u, r, v) represents that drug u and drug v have pharmacological effect r.
Deﬁnition 2 (External Biomedical Knowledge Graph). Given a set of various biomedical entities E and the biomedical relation among the entities R, the external biomedical knowledge graph GKG is deﬁned as GKG = {(h, r, t) | h, t ∈ E, r ∈ R} with each item (h, r, t) describes a biomedical relation r between entity h and entity t. Note that we aggregate the drug entities in GDDI to GKG, i.e., RD ∈ R, and D ∈ E.
Problem 1 (Multi-relational DDI Prediction). The Drug-drug interaction (DDI) prediction is to output the pharmacological effect given the a pair of drugs. Mathematically, it is to learn a mapping F : D × D → RD from a drug pair (u, v) ∈ (D × D) to the pharmacological effect r ∈ RD.
3.2 The SumGNN Method
SumGNN is composed of three modules: subgraph anchoring, knowledge summarization, and multi-channel neural encoding. For a given drug pair, we anchor to a subgraph of potential biomedical entities that are close to the pairs in the KG. Then, we propose a new graph neural network that has a summarization scheme to provide a condense pathway to reason about the drug interaction mechanism. Given this pathway graph, we use multi-channel neural encoding, to
1The SumGNN code is available at https://github.com/yueyu1030/SumGNN.

4

integrate diverse sources of available information to generate a sufﬁcient drug pair representation. At last, a decoding classiﬁer is followed to predict the interaction outcome. We initialize all entity embedding using KG method TransE (Bordes et al., 2013), where a entity is denoted as h(u0).
(A) The Local Subgraph Extraction Module
The biomedical KG describes the complicated mechanism of human biology. Modulation in several nodes (drug-pairs) in the KG can perturb the connected nodes (e.g. disease, cellular component, and etc.) which creates a ripple effect that eventually result in various physiological outcomes (Himmelstein and Baranzini, 2015). The effect is diffused as distance between the drug pairs and the biomedical entities increases. Thus, to understand the drug interactions, we focus on local subgraphs in the KG around the drug pairs. Speciﬁcally, for drug pairs u and v, we ﬁrst extract the k-hop neighboring nodes for both u and v, Nk(u) = {s | d(s, u) ≤ k} and Nk(v) = {s | d(s, v) ≤ k}, where d(·, ·) stands for the distance between two nodes on GKG. Then, we obtain the enclosing subgraph based on the intersection of these nodes, GSub = {(u, r, v) | u, v ∈ Nk(u) ∩ Nk(v), r ∈ R}.
Motivated by Zhang and Chen (2018) which highlights the importance of node relative position to the central node u, v in the subgraph, we augment the initial node embedding in the subgraph by concatenating a position vector. For each node i in the subgraph GSub, we ﬁrst compute the shortest path length d(i, u) and d(i, v) between i and the center drug pairs nodes u, v. After that, we convert it into a position vector pi = [one-hot(d(i, u)) ⊕ one-hot(d(i, v))]. Then, we update the node i representation as h(i0) = [h(i0), pi].

(B) The Knowledge Summarization Module

To provide biological insights in addition to the predictive outcome, we design a knowledge summarization module to summarize the subgraph information into a graph-based pathway for potential drug interactions. Note that the pathway is not a linear line, but a sparse subgraph since drug interactions are usually due to complicated interplays among many types of biomedical entities. The summarization requires that we need to retain edges that contain most useful signals for the drug interactions while removing paths that are not important. To achieve this, we adopt a layer-independent, relation-aware self-attention module to assign a weight for every edge in GSub. These weights are generated based on the input featurization h(0) and represent the interaction signal intensities for edge pruning.
Speciﬁcally, we denote the interaction signal intensity score for the edge connecting any biomedical entity i and j as αi,j. Inspired by the relation-aware transformer architecture (Shaw et al., 2018), we use self-attention mechanism, which takes account into all neighbor nodes in the subgraph to generate the attention weight. This attention mechanism is ideal for us because it generates the signal intensity score after examining all biomedical entities in the subgraph around the drug-pairs. Here, αu,v is calculated as

h(j0)WJ (h(i0)WI + rij )T

αi,j = Threshold φ

√

,γ ,

(1)

dk

where the WJ and WI are the self-attention key weights that contain represent√ation for each node in the subgraph, rij encodes the relationship between the two entities i and j, dk is the size of feature vector h(0) for normalization, γ is the signal threshold and φ(x) = eexx−+ee−−xx is the tanh

5

function for non-linear transformation. Intuitively, this function ﬁrst computes the dot product between h(i0) and WI to get an attention score between node i and every neighbor node in the subgraph. Then we sum it up with the relation embedding, followed up the same procedure to calculate attention score between node j and every other node through dot product between h(j0) and WJ . By taking the dot product with every other nodes for both i, j, the ﬁnal score considers all the subgraph information. Then, after non-linear transformation, we calculates the signal intensity score ranging from −1 to 1 for this edge. At last, we apply a threshold function to screen out edges that are below an intensity score threshold γ by setting them with weight 0 since they are not important for the interaction prediction and setting them 0 would prune these edges from message passing process in the graph neural network. This step is applied to every edge in the subgraph.
Note that existing graph attention approaches (Velicˇkovic´ et al., 2018; Cai and Lam, 2020; Shaw et al., 2018) generate attention weights for every edge in every layer. However, this way can provide potentially contradicting signals across layers for the same edge, precluding the generation of interpretable pathways. In the contrast, SumGNN adopts a layer-independent attention mechanism, which only depends on the ﬁrst layer embedding to prune edges. It provides an unequivocal pathway for model explainability. As many biological networks are constructed through text mining where many edges are potentially false positives, this pruning mechanism also allows noise reduction.

(C) The Multi-Channel Integration Module
To obtain a powerful representation for drug interaction prediction, we integrate a diverse set of information sources.

Channel 1: Summarized Knowledge Using the knowledge summarization approach we desribed
above, we identify a summarized subgraph that is important to input drug pairs. We want to gen-
erate the latent representation that leverages this subgraph for the input drug pairs. We integrate it using the following message-passing scheme. For each node v, we compute a relation-aware message weighted by the signal intensity score blv at layer l using the attention score as

b(vl) =

αu(l,)v h(ul−1)Wr(l) ,

(2)

u∈Nv

where Nv denotes the neighbors of node v in subgraph GSub, Wr(l) is the weight matrix to transform hidden representation for node u, v’s relation r in layer l. To avoid overﬁtting, we use basis decomposition (Schlichtkrull et al., 2018) to decompose Wr(l) into the linear combination of a small number of basis matrices {Vb}b∈B as

B

Wr(l) = a(rlb)Vb(l).

(3)

b=1

Then, we propagate the message b(vl) to the updated representation h(vl) of node v via

h(vl) = ReLU Ws(el)lfh(vl−1) + b(vl) ,

(4)

where Wself is the weight matrix to transform the node embedding itself.

6

Channel 2: Subgraph Features To obtain the embeddings for subgraphs (denoted as hGSub), we take the average of all node embeddings in GSub at layer l projected by a linear layer as
h(GlS)ub = Mean WSubh(il) . (5)
Channel 3: Drug Fingerprint Molecular information such as chemical ﬁngerprints have shown to be powerful predictor of drug interactions (Huang et al., 2020b). Thus, in addition to the network representation, we obtain the Morgan ﬁngerprint fv (Rogers and Hahn, 2010), which is a predictive descriptor of drugs, for each drug v. Note that it is infeasible to use this feature as the input node feature in the KG since KG consists of various types of nodes other than drugs (e.g. Side Effect, Disease and genes) and they cannot be represented by Morgan Fingerprints, which lead to inconsistent node features for GNN propagation.

Layer-wise Channels Aggregation To assemble various representation generated via each layer,
we adopt the layer-aggregation mechanism (Xu et al., 2018). We concatenate node/subgraph embeddings in every layer, i.e., hv = [h(v1), h(v2), · · · , h(vL)] and hGSub = [h(G1S)ub , h(G2S)ub , · · · , h(GLS)ub ] where is L is the layer size. To integrate chemical ﬁngerprints, we update the layer-aggregated embedding by concatenation of chemical representation: hv = [hv ⊕ fv].
At last, we combine the various channels together to obtain the input drug-pairs representation hu,v = [hu, hv, hGSub]. To predict the relation, we obtain a prediction probability vector pu,v where each value in the vector corresponds to a the likelihood of a relation. pu,v is computed via feeding the drug pair representation to a decoder parameterized by Wpred:

pu,v = Wpredhu,v.

(6)

3.2.1 Training and Inference

During training, for multi-class classiﬁcation task, we adopt the the cross entropy loss CE for each edge (u, r, v) as
R

CE(u, r, v) = − log(yˆr) · yr,

(7)

r=1

where yˆr = softmax(pru,v) =

exp(pru,v) and yr is the binary indicator if class r is the correct

R i=1

exp(piu,v

)

label for u and v. For multi-label classiﬁcation task, given the edge (u, r, v), we adopt the binary

cross entropy loss BCE as

BCE(u, r, v) = − log yˆr − Ew∼Pw(v) log (1 − yru,w) ,

(8)

where (u, r, w) is the sampled negative edge for relation r. This is achieved by replacing node
v to node w that is sampled randomly according to a distribution Pw(v) ∝ dw(v)3/4 (Mikolov et al., 2013). Then yˆr = sigmoid(pru,v), yruw = sigmoid(pru,w) is the prediction score for two edges. Considering all edges, the ﬁnal loss L in SumGNN is

L=

(u, r, v),

(9)

(u,r,v)∈E

where is either Eq. (7) or Eq. (8) depending on the task type. During training, we learn the model parameter by minimizing the total loss L using stochastic gradient optimizers such as Adam (Kingma and Ba, 2014).

7

During inference, an unseen node pair u, v’s subgraph in the KG is extracted and fed into the same pipeline to calculate the relation vector. For multi-class task, we use the highest probability relation as the predicted relation and for multi-label task, we collect all scores from both positive and negative counterparts for all relations.
4 Experiments
4.1 Experiment Setup
Datasets (1) DrugBank dataset (Wishart et al., 2018) contains 1,709 drugs (nodes) and 136,351 drug pairs (edges), which are associated with 86 types of pharmacological relations between drugs, such as increase of cardiotoxic activity, decrease of serum concentration and etc. Each drug pair can contain one or two relations. As more than 99.8% of edges have only one edge type (Ryu et al., 2018), we ﬁltered the edge with more than one type in our study. (2) TWOSIDES (Tatonetti et al., 2012) dataset contains 645 drugs (nodes) and 46,221 drug-drug pairs (edges) with 200 different drug side effect types as labels. For each edge, it may be associated with multiple labels. Following (Zitnik et al., 2018a; Dai et al., 2020), we keep 200 commonly-occurring DDI types ranging from Top-600 to Top-800 to ensure every DDI type has at least 900 drug combinations. (3) For external knowledge base, we use HetioNet (Himmelstein and Baranzini, 2015), which is a large heterogeneous knowledge graph merged from 29 public databases. To ensure no information leakage, we remove all the overlapping DDI edges between HetioNet and the dataset. In the end, we obtain 33,765 nodes out of 11 types (e.g., gene, disease, pathway, molecular function and etc.) with 1,690,693 edges from 23 relation types.
Baselines We compare our models with several baselines2.
• MLP (Rogers and Hahn, 2010) uses a two-layer MLP on Morgan ﬁngerprint to directly predict drug interactions.
• Deepwalk (Perozzi et al., 2014) ﬁrst learns the embeddings for drugs in the network via random walk. Then, it predict the relation for drug pairs via a linear layer over embeddings.
• LINE (Tang et al., 2015) use a one-layer feedforward neural network to learn the embeddings for drugs. Then, it stack a linear layer over embeddings to predict the relation.
• Node2vec (Grover and Leskovec, 2016) ﬁrst learns the embeddings for drugs in the network. Similar to Deepwalk, it predict the relation via a linear layer over embeddings.
• Decagon (Zitnik et al., 2018a) adopts multi-relational graph convolutional network (Schlichtkrull et al., 2018) on the DDI network for drug interaction prediction.
• GAT (Velicˇkovic´ et al., 2018) uses attention networks to aggregate neighborhood information in DDI network.
• SkipGNN (Huang et al., 2020a) predicts drug interactions by aggregating information from both direct interactions and second-order interactions via two GNNs.
• PRD (Wang, 2017) ﬁrst use KG embeddings for drugs in the KG, then pass through a linear layer for drug interaction prediction.
• KG-DDI (Karim et al., 2019) ﬁrst extracts KG embeddings for drugs in the KG, then adopts a Conv-LSTM model using the embeddings for drug interaction prediction.
2Further details on baseline methods, implementation and parameters are in the supplementary.
8

• GraIL (Teru and Hamilton, 2020) is for inductive relation prediction on KGs, which uses local subgraph.

• KGNN (Lin et al., 2020) samples and aggregates neighborhoods for each node from their local receptives via GNN and with external knowledge graph, which achieves the state-ofthe-art result on binary DDI prediction problem.

Metrics The task on the DrugBank dataset is a multi-class classiﬁcation, thus we consider the following metrics:
• F1 Score: average F1 score over different classes as F1 Score = N1 Nk=1 2PPkk+·RRkk , where N is the # of classes and Pk, Rk is the precision and recall for k-th class. Since it gives equal weights for each classes, they are more sensitive to the results for classes where samples are fewer.
• Accuracy: Accuracy over all samples Accuracy = |Yk|Y∩kYˆ|k| , Yk is the predicted labels at k and Yˆk are the ground-truth labels.
• Cohen’s Kappa (Cohen, 1960) measures the inter-annotator agreement as κ = (po−pe)/(1− pe), where po is the observed agreement (identical to accuracy), pe is the probability of randomly seeing each class.

The task on the TWOSIDES dataset is a multi-label prediction. We follow (Zitnik et al., 2018a) and consider the following measure. For each side effect type, we calculate the performance individually and use the average performance over all side effects as the ﬁnal result.

• ROC-AUC is the average area under the receiver operating characteristics curve as ROC-AUC =

n k=1

TPk

∆

FPk

,

where

k

is

k-th

true-positive

and

false-positive

operating

point

(TPk

,

FPk

).

• PR-AUC is the average area under precision-recall curve PR-AUC =

n k=1

Preck

∆

Reck

where k is k-th precision/recall operating point (Preck, Reck).

• AP@50 is the average precision at 50, where AP@k = |Yk|Y∩kYˆ|k| , Yk is the predicted labels at k and Yˆk are the ground-truth labels.

Evaluation Strategy. For both datasets, we split it into 7:1:2 as train, development and test set. For the DrugBank dataset, since the label distribution is highly imbalanced, we ensure train/dev/test set contain samples from all classes. For the TWOSIDES dataset, we use the same method in (Zitnik et al., 2018a) to generate negative counterparts for each positive edge by sampling the complement set of positive examples. For every experiment, we conduct ﬁve independent runs and select the best performing model based on the loss value on the validation set.

4.2 SumGNN achieves superior predictive performance

We report the performance of our model and all baselines in table 1. From the result, we ﬁnd that SumGNN achieves the best performance in DDI prediction on two datasets, accurately predicting the correct DDI pharmacological effect consistently. Particularly, SumGNN has 27.19%, 5.47%, 4.65% absolute increase over the best baseline without KG on three metrics respectively on DrugBank dataset and 2.84%, 2.45%, 4.50% increase on TWOSIDES dataset. Also, SumGNN achieves 5.54%, 1.77%, 1.08% and 1.97%, 2.25%. 2.54% absolute increase over the state-ofthe-art baselines with KG on two datasets. These results clearly veriﬁes the efﬁcacy of our method.
9

Table 1: SumGNN achieves the best predictive performance compared to state-of-the-art baselines in

DDI prediction. Average and standard deviation of ﬁve runs are reported. For these metrics, higher

values always indicate better performance.

Dataset

Dataset 1: DrugBank

Dataset 2: TWOSIDES

Classiﬁcation Task

Multi-class

Multi-label

Methods

F1 Score Accuracy Cohen’s Kappa ROC-AUC PR-AUC AP@50

MLP (Rogers and Hahn, 2010) Deepwalk (Perozzi et al., 2014) LINE (Tang et al., 2015) Node2Vec (Grover and Leskovec, 2016) Decagon (Zitnik et al., 2018a) GAT (Velicˇkovic´ et al., 2018) SkipGNN (Huang et al., 2020a)

61.10±0.38 24.77±0.40 30.26±0.45 24.92±0.32 57.35±0.26 33.49±0.36 59.66±0.26

82.14±0.33 68.50±0.38 76.57±0.49 71.09±0.40 87.19±0.28 77.18±0.15 85.83±0.18

80.50±0.18 58.44±0.23 70.91±0.53 63.79±0.37 86.07±0.08 74.20±0.23 84.20±0.16

82.60±0.26 88.27±0.09 91.20±0.34 90.66±0.13 91.72±0.04 91.18±0.14 92.04±0.08

81.23±0.14 85.42±0.14 90.02±0.28 88.87±0.23 90.60±0.12 89.86±0.05 90.90±0.10

73.45±0.28 81.09±0.16 84.07±0.19 83.00±0.30 82.06±0.45 82.80±0.17 84.25±0.25

PRD (Wang, 2017) KG-DDI (Karim et al., 2019) GraIL (Teru and Hamilton, 2020) KGNN (Lin et al., 2020)

40.73±0.44 36.39±0.32 81.31±0.30 73.99±0.11

81.52±0.34 82.48±0.12 89.89±0.24 90.89±0.20

78.80±0.36 78.89±0.27 88.07±0.20 89.64±0.24

88.75±0.23 90.75±0.07 92.89±0.09 92.84±0.07

85.26±0.33 88.16±0.12 91.10±0.19 90.78±0.20

79.86±0.26 83.48±0.05 86.21±0.05 86.05±0.12

SumGNN (Ours)

86.85±0.44 92.66±0.14

90.72±0.13

94.86±0.21 93.35±0.14 88.75±0.22

SumGNN-KG SumGNN-Sum (w/o Summarization) SumGNN-SF (w/o Subgraph Features) SumGNN-CF (w/o Chemical Features) SumGNN-LIA (w/o Layer Independent Attention)

78.35±0.51 83.20±0.34 84.47±0.22 83.57±0.36 86.54±0.22

89.05±0.36 90.83±0.19 91.88±0.21 91.31±0.17 92.44±0.30

87.28±0.08 90.14±0.10 90.26±0.19 90.07±0.11 90.34±0.10

92.62±0.13 94.09±0.16 93.94±0.11 94.35±0.11 93.92±0.31

90.80±0.40 92.55±0.24 92.45±0.22 92.86±0.20 92.33±0.19

85.75±0.10 87.65±0.24 87.69±0.08 88.10±0.07 86.15±0.13

4.3 SumGNN excels at low-data imbalanced relations predictions
We observe that the improvement of SumGNN is much more signiﬁcant on DrugBank than TWOSIDES dataset. We take a closer look at this problem and ﬁnd that the major difference of these two datasets is that the data distribution of DrugBank is more imbalanced – more than 30% of the relation types occurs less than 50 times in the training set while more than 10% of the relation types occurs more than 1000 times, as shown in Fig. 2(a).
To examine the model’s prediction performance on the size of training data associated with each relation, we ﬁrst split the relations types into 5 groups with various amounts of training data and then plot the average F1 score of these bins in Figure 2. By comparing the performance of SumGNN against the strongest baseline on Accuracy (KGNN) and the variant of no KG (Decagon), we ﬁnd that SumGNN can effectively boost the performance when the samples are extremely scarce. When the size of the training samples of the relation is less than 10, both decagon and KGNN cannot give any correct predictions, while our model can still achieve 57.14% F1 score (i.e., choose the correct relation out of the 86 relations). One potential reason is that SumGNN feeds different subgraphs in every GNN propagation, which forms a much-needed inductive bias over unseen subgraphs, such as the ones in the low-data relations. This is in sharp contrast to previous approaches such as KGNN. It also justiﬁes that SumGNN’s knowledge summarization via subgraphs is more effective to harness the external knowledge. In addition, we ﬁnd SumGNN also brings at least 38.19% improvement on F1 score for relations occuring less than 50 times. This is an important ﬁnding since a high overall accuracy does not mean good performance across all relations. The low-data relations are the hardest to predict correctly and we show that SumGNN can be used for these low-data relations whereas other previous models cannot.

10

Number of Training Samples Prediction F1 Score

4×104 3×104

0.8 0.6

Decagon KGNN SumGNN
0.571

0.806 0.653

0.909 0.805
0.623

0.922 0.873
0.780

0.920 0.930 0.886

2×104
104 103
0

20

40

60

80

Classes

0.4
0.271
0.2

0.0 0.000 0.000

<10

10-50 50-100 100-1000 >1000

(8.13%) (24.42%) (18.60%) (33.72%) (15.12%)

The Number of Training Samples

(a) The number of training samples wrt. different (b) The average F1 score vs. different size of training

classes in DrugBank dataset.

samples.

Figure 2: The dataset statistics and the average F1 score for different relations with various number of training samples on DrugBank dataset. Here, in (a), the x-axis indicates the index of classes (there are 86 classes in total) and the y-axis indicates the number of training samples in the corresponding class. in (b), the x-axis indicate the range for number of training samples in this group as well as the proportion of classes in this group to all classes and the y-axis stands for the average prediction F1 score for the classes in the group.

4.4 External knowledge improves prediction
We ﬁnd that the ﬁrst seven method cannot obtain satisfactory result compared to SumGNN. MLP, Deepwalk, LINE and Node2vec only use shallow embedding layers to learn the features for drugs while do not use graph neural networks for modeling the drug interactions, which have limited expression power. Moreover, although GAT considers the attention on different edges, it fails to consider the multi-relation information, as its performance is also not good. By comparing the latter four methods with the former ﬁve methods, we ﬁnd the use of KG signiﬁcantly improves DDI performance, highlighting the necessity of combining external knowledge usage with graph neural networks, as it can provide complementary information for DDI task.

4.5 Knowledge summarization module is the best to capture external knowledge
Comparing SumGNN with KGNN and KG-DDI, we show that our model can consistently outperform them on both datasets (more than 4% on average for DrugBank and 2% on average for TWOSIDES), which indicates that simply adopting KG embeddings as well as neighborhood sampling are insufﬁcient to fully harness the KG information for DDI prediction. SumGNN provides the best approach to leverage the external KG and also corroborates with our motivation that the use of subgraph reduces noise and irrelevant information. Although GraIL also extracts subgraphs for downstream tasks, it does not use any knowledge summarization techniques, which potentially further eliminating the irrelevant information in the local subgraph. In addition, GraIL merely considers the position information while neglecting the multi-channel features during information propagation.

11

Orthostatic hypotension

Hydroflumethiazide

Paroxetine

MIF Gene

Aplastic anaemia

PRR7 Gene

Figure 3: SumGNN generates a short reasoning path to provide clues for understanding drug interactions. The shade of color indicates the strength of attention weight. Low-weight edges in the extracted subgraph are pruned by SumGNN and SumGNN focus on a sparse set of signal edges and nodes.
4.6 Ablation Study
To study the usage of KG, we remove the knowledge graph (SumGNN-KG) and perform prediction on the DDI graph. We see SumGNN has 8.5% absolute increase in Macro F1 on DrugBank, highlighting the usefulness of KG. To evaluate the knowledge summarization module, we remove the summarization component (SumGNN-Sum) and use the raw local subgraph to predict the outcome. We see SumGNN has 3.65% increase for DrugBank and 2.24% increase for TWOSIDES on Macro F1, suggesting that the summarization further condenses the relevant information and elevate the performance.
To study the effect of multi-channel neural encoding, we compare the result of SumGNN with several variants that remove speciﬁc channel information (i.e. subgraph features and chemical features), and we ﬁnd that these channel information all contribute to the overall performance. Particularly, after removing the subgraph embedding (SumGNN-SF), the Macro F1 drops by 2.38% on DrugBank and 0.92% on TWOSIDES respectively. When removing chemical ﬁngerprint (SumGNN-CF), the performance also degrades 3.28% on Macro F1 for DrugBank, corroborating with the indispensable roles of each of the channel. We also see that by replacing the layer independent attention (SumGNN-LIA) with the previous layer-dependent ones, the performance drops. This suggests that our attention mechanism not only provides interpretability but also increases predictive performance.
4.7 Case Study
The usefulness of this model lies in twofolds. First, given the high predictive performance, it can identify novel drug-drug interactions that are ﬂagged high by the model while not in the dataset. Second, using the external knowledge summarization module, we are able to discover signal edges, which provide biological pathways to hint at the potential mechanism of DDIs. We provide a case study of a novel drug pair predicted by the model, Paroxetine and Hydroﬂumethiazide. Paroxetine is an antidepressant, and Hydroﬂumethiazide is used to treat hypertension and edema. SumGNN assigns highest probability for the DDI type ”increase of the central nervous system

12

Score Score

k=1

k=3

95

k=2

k=4

90

85

80

75 F1 Score Accuracy
Metrics

Kappa

(a) Result of DrugBank

k=1

k=3

95

k=2

k=4

90

85

80

75 ROC-AUC PR-AUC
Metrics

AP@50

(b) Result of TWOSIDES

Figure 4: Model performance given different parameter k.

Table 2: The running time for one epoch of SumGNN for two datasets. Note that w/o Subgraph is a variant that directly aggregates the information for all neighbors on KG.

Dataset

Size of Hop k 1 2 3 4 w/o Subgraph

DrugBank 132 s 141 s 178 s 205 s TWOSIDES 62 s 75 s 91 s 102 s

1279 s 471 s

depressant activities”. We then visualize the generated pathway from SumGNN’s summarization scheme in Fig 3. We see that the model signiﬁcantly reduces irrelevant nodes and edges in the subgraph of the KG and focuses on a sparse set of nodes to make prediction. We examine the nodes that have high signals connection to the target pairs and ﬁnd literature evidence support. Particularly, the model assigns high weights to two side effects nodes, orthostatic hypotension and aplastic anaemia. Orthostatic hypotension refers to a sudden drop in blood pressure when standing up, and aplastic anemia is a condition in which the body stops producing enough new blood cells. Notably, orthostatic hypotension is closely related to multiple system atrophy, a central nervous system problem (Jones et al., 2015). As both drugs incur risk in the side effects, the complication on the central nervous systems could be aggravated when these drugs are taken together, supporting our model prediction. This case study illustrates how to use SumGNN for potential DDI prediction.
4.8 Parameter studies
We study the effect of key parameters. When evaluating one parameter, we ﬁx other parameters to their default values. • Effect of the hop of the subgraph k: Figure 4 shows the result of SumGNN with varying k. From the result, we ﬁnd that for DrugBank dataset, the performance ﬁrst increase when k is small. However, when k increases from 3 to 4, we observe slight performance drops on F1 score. These indicate that the larger subgraph can bring more useful information while when k is too large, it may also bring some noise and hurt the performance. For TWOSIDES dataset, we ﬁnd the result is more stable with different k, indicating even 1-hop subgraph provides adequate information for DDI task.
Moreover, to show how subgraph formulation drives efﬁciency, we compare the training time between SumGNN with varying subgraph size and SumGNN with the entire KG to propagate (See Table 2). We ﬁnd SumGNN saves 80% of training time via subgraph anchoring, which demonstrates the efﬁciency of our approach.
13

Score Score

k=1

k=3

95

k=2

k=4

90

85

80

75 F1 Score Accuracy
Metrics

Kappa

(a) Result of DrugBank

d=8

95

d = 16

90

85

80

75 ROC-AUC PR-AUC
Metrics

d = 32 d = 64
AP@50

(b) Result of TWOSIDES

Figure 5: Model performance given different parameter d.

• Effect of the dimension of embeddings d: Figure 5 exhibits the inﬂuence of embedding dimension d. The result indicates that when d is small, increasing d can boost the performance. But when d becomes large, the gain is marginal.

Score Score

95
90
85
80 0

F1 Score

Accuracy

95

Cohen's Kappa

90

85

ROC-AUC

PR-AUC

AP@50

80

0.2

0.4

0.6

0.8

γ

0

0.2

0.4

0.6

0.8

γ

(a) Result of DrugBank (b) Result of TWOSIDES

Figure 6: Model performance given different parameter γ.

• Effect of the threshold for summarization γ: Figure 6 shows the result of SumGNN with different threshold γ, which demonstrates that when γ is small, the performance is rather stable as ﬁltering edges with low score has little effect on the ﬁnal prediction. This also means SumGNN is able to achieve similar predictive performance while removing many irrelevant edges. However, when γ is large (γ > 0.4 for DrugBank and γ > 0.6 for TWOSIDES), it is clear that the performance drops more. In such cases, the summarized graph is more sparse and we might ﬁlter out potential useful edges. To sum up, there is a trade-off between the explainability and performance.

5 Conclusion
In this paper, we propose a new method SumGNN: knowledge summarization graph neural network for multi-typed DDI predictions, which is mainly enabled by a local subgraph module that can efﬁciently anchor on relevant subgraphs from a KG, a self-attention based subgraph summarization scheme that can generate reasoning path within the subgraph, and a multi-channel knowledge and data integration module that utilizes massive external biomedical knowledge for signiﬁcantly improved multi-typed DDI predictions. Experiments on real-world datasets demonstrated the strong performance of SumGNN.
In addition, computational approaches depend heavily on the training data. If the number of training data associated with one speciﬁc drug interaction type is low, it is difﬁcult to predict accurately. In contrast to other works, SumGNN is able to generate good performance in lowresource settings. SumGNN is also a general framework and can be adapted to predict any other

14

interactions such as drug-disease interaction. The ability to low-resource learning could also mean to excel at ﬁnding drugs for rare diseases.
References
Emily Alsentzer, Samuel G Finlayson, Michelle M Li, and Marinka Zitnik. 2020. Subgraph Neural Networks. NeurIPS (2020).
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NeurIPS.
Hannah A Burkhardt, Devika Subramanian, Justin Mower, and Trevor Cohen. 2019. Predicting adverse drug-drug interactions with neural embedding of semantic predications. bioRxiv (2019), 752022.
Deng Cai and Wai Lam. 2020. Graph Transformer for Graph-to-Sequence Learning. In AAAI.
Remzi Celebi, Huseyin Uyar, Erkan Yasar, Ozgur Gumus, Oguz Dikenelli, and Michel Dumontier. 2019. Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings. BMC Bioinformatics (2019).
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. Cluster-GCN: An efﬁcient algorithm for training deep and large graph convolutional networks. In KDD.
Jacob Cohen. 1960. A coefﬁcient of agreement for nominal scales. Educational and Psychological Measurement (1960).
Yuanfei Dai, Chenhao Guo, Wenzhong Guo, and Carsten Eickhoff. 2020. Wasserstein Adversarial Autoencoders for Knowledge Graph Embedding based Drug-Drug Interaction Prediction. arXiv:2004.07341 (2020).
Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD.
Deisy Morselli Gysi, ´Italo Do Valle, Marinka Zitnik, Asher Ameli, Xiao Gan, Onur Varol, Helia Sanchez, Rebecca Marlene Baron, Dina Ghiassian, Joseph Loscalzo, et al. 2020. Network medicine framework for identifying drug repurposing opportunities for covid-19. arXiv:2004.07229 (2020).
Daniel S Himmelstein and Sergio E Baranzini. 2015. Heterogeneous network edge prediction: a data integration approach to prioritize disease-associated genes. PLoS Computional Biology (2015).
Kexin Huang, Cao Xiao, Lucas Glass, Marinka Zitnik, and Jimeng Sun. 2020a. SkipGNN: Predicting Molecular Interactions with Skip-Graph Networks. arXiv:2004.14949 (2020).
Kexin Huang, Cao Xiao, Trong Hoang, Lucas Glass, and Jimeng Sun. 2020b. CASTER: Predicting Drug Interactions with Chemical Substructure Representation. In AAAI.
15

Kexin Huang and Marinka Zitnik. 2020. Graph Meta Learning via Local Subgraphs. NeurIPS (2020).
Vassilis N. Ioannidis, Xiang Song, Saurav Manchanda, Mufei Li, Xiaoqin Pan, Da Zheng, Xia Ning, Xiangxiang Zeng, and George Karypis. 2020. DRKG - Drug Repurposing Knowledge Graph for Covid-19. https://github.com/gnn4dr/DRKG/.
Pearl K Jones, Brett H Shaw, and Satish R Raj. 2015. Orthostatic hypotension: managing a difﬁcult problem. Expert Review of Cardiovascular Therapy (2015).
Md Rezaul Karim, Michael Cochez, Joao Bosco Jares, Mamtaz Uddin, Oya Beyan, and Stefan Decker. 2019. Drug-drug interaction prediction based on knowledge graph embeddings and convolutional-LSTM network. In ACM-BCB.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv:1412.6980 (2014).
Thomas N Kipf and Max Welling. 2017. Semi-supervised classiﬁcation with graph convolutional networks. ICLR (2017).
Peng Li, Chao Huang, Yingxue Fu, Jinan Wang, Ziyin Wu, Jinlong Ru, Chunli Zheng, Zihu Guo, Xuetong Chen, Wei Zhou, et al. 2015. Large-scale exploration and analysis of drug combinations. Bioinformatics 31, 12 (2015).
Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao Zhang. 2020. BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision. In KDD.
Xuan Lin, Zhe Quan, Zhi-Jie Wang, Tengfei Ma, and Xiangxiang Zeng. 2020. KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction. In IJCAI.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NeurIPS.
B. Percha and R. B. Altman. 2013. Informatics confronts drug–drug interactions. Trends in Pharmacological Sciences 34, 3 (2013), 178–184.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD.
David Rogers and Mathew Hahn. 2010. Extended-connectivity ﬁngerprints. Journal of Chemical Information and Modeling (2010).
Maya Rotmensch, Yoni Halpern, Abdulhakim Tlimat, Steven Horng, and David Sontag. 2017. Learning a health knowledge graph from electronic medical records. Scientiﬁc Reports (2017).
Jae Yong Ryu, Hyun Uk Kim, and Sang Yup Lee. 2018. Deep learning improves prediction of drug–drug and drug–food interactions. PNAS (2018).
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In ESWC.
16

Junyuan Shang, Tengfei Ma, Cao Xiao, and Jimeng Sun. 2019. Pre-training of graph augmented transformers for medication recommendation. In IJCAI.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In NAACL.
Hongzhi Shi, Quanming Yao, Qi Guo, Yaguang Li, Lingyu Zhang, Jieping Ye, Yong Li, and Yan Liu. 2020. Predicting Origin-Destination Flow via Multi-Perspective Graph Convolutional Network. In ICDE.
Rakshith S Srinivasa, Cao Xiao, Lucas Glass, Justin Romberg, and Jimeng Sun. 2020. Fast Graph Attention Networks Using Effective Resistance Based Graph Sparsiﬁcation. arXiv:2006.08796 (2020).
Chang Su, Jie Tong, Yongjun Zhu, Peng Cui, and Fei Wang. 2020. Network embedding in biomedical data science. Brieﬁngs in Bioinformatics (2020).
Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon, Long-Kai Huang, and Chi Xu. 2018. Recurrent knowledge graph embedding for effective recommendation. RecSys (2018).
Takako Takeda, Ming Hao, Tiejun Cheng, Stephen H Bryant, and Yanli Wang. 2017. Predicting drug–drug interactions through drug structural similarities and interaction networks incorporating pharmacokinetics and pharmacodynamics knowledge. Journal of cheminformatics 9, 1 (2017).
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW.
Nicholas P Tatonetti, P Ye Patrick, Roxana Daneshjou, and Russ B Altman. 2012. Data-driven prediction of drug effects and interactions. Science Translational Medicine (2012).
Komal Teru and William Hamilton. 2020. Inductive Relation Prediction on Knowledge Graphs. In ICML.
The´o Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In ICML.
Ruo-Chun Tzeng and Shan-Hung Wu. 2019. Distributed, Egocentric Representations of Graphs for Detecting Critical Structures. In ICML.
Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio`, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.
Petar Velicˇkovic´, William Fedus, William L. Hamilton, Pietro Lio`, Yoshua Bengio, and R Devon Hjelm. 2019. Deep Graph Infomax. In ICLR. https://openreview.net/forum?id=rklz9iAcKQ
Guanying Wang, Wen Zhang, Ruoxu Wang, Yalin Zhou, Xi Chen, Wei Zhang, Hai Zhu, and Huajun Chen. 2018. Label-free distant supervision for relation extraction via knowledge graph embedding. In EMNLP.
17

Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and Zhongyuan Wang. 2019b. Knowledge-aware graph neural networks with label smoothness regularization for recommender systems. In KDD.
Meng Wang. 2017. Predicting rich drug-drug interactions via biomedical knowledge graphs and text jointly embedding. arXiv preprint arXiv:1712.08875 (2017).
Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019a. KGAT: Knowledge graph attention network for recommendation. In KDD.
David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al. 2018. DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic acids research 46, D1 (2018), D1074–D1082.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation Learning on Graphs with Jumping Knowledge Networks. In ICML.
Donghan Yu, Yiming Yang, Ruohong Zhang, and Yuexin Wu. 2020c. Generalized Multi-Relational Graph Convolution Network. arXiv:2006.07331 (2020).
Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang. 2020a. STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths. In KDD.
Yue Yu, Tong Xia, Huandong Wang, Jie Feng, and Yong Li. 2020b. Semantic-Aware Spatio-Temporal App Usage Representation via Graph Convolutional Network. IMWUT (Sept. 2020).
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning Method. In ICLR.
Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. In NeurIPS.
Marinka Zitnik, Monica Agrawal, and Jure Leskovec. 2018a. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics (2018).
Marinka Zitnik, Rok Sosicˇ, Sagar Maheshwari, and Jure Leskovec. 2018b. BioSNAP Datasets: Stanford Biomedical Network Dataset Collection. http://snap.stanford.edu/biodata.
18

A Implementation Details
A.1 SumGNN Parameter Setup
We use the following hyperparameter set for SumGNN after random search on validation set. We use 1, 024-bits Morgan ﬁngerprint for drug featurization. We set the subgraph to be 2-hops neighbors (i.e. k = 2). In the subgraph summarization module, we use weight matrix of size d = 32 for W1 and W2. The hidden dimension hkv is set to be d = 32. The relation matrix r is set to be 32. The edge pruning threshold is set to be γ = 0. The input hidden representation of each node is d = 32. The number of basis B in Eq. (3) is set to 8 as the performance do not change much when set from 4 to 16 and suffer from over-ﬁtting with B > 16. We study the effect of key parameter d, γ and k in our experiment part (Section 4).

A.2 Training Details
Training Parameters. For both our method and baselines, the training parameters are set as follows unless speciﬁed. We train the model for 50 epochs with batch size 256. Our model is optimized with ADAM optimizer (Kingma and Ba, 2014) of learning rate 5 × 10−3 with gradient clipping set to 10 under L2 norm. We set the L2 weight decay to 1 × 10−5, the layer of GNN to 2 and set the dropout rate to 0.3 for each Layer in GNN. Model Implementation and Computing Infrastructure. All methods are implemented in PyTorch3 and the graph neural network modules are build on Deep Graph Library (DGL)4. The System we use is Ubuntu 18.04.3 LTS with Python 3.6, Pytorch 1.2 and DGL 0.4.3. Our code is run in a Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz CPU and a GeForce GTX TITAN X GPU.

A.3 The Range for Tuning Hyper-parameters
We use grid search to determine hyper-parameters and list the search space of key hyper-parameters as follows.

Table 3: The range for tuning hyper-parameters. The bold numbers are the default settings.

Parameters

Range

Learning Rate Weight Decay
Dropout Layers of GNN
d k B

[5 × 10−4, 1 × 10−3, 5 × 10−3, 1 × 10−2] [1 × 10−6, 1 × 10−5, 1 × 10−4, 1 × 10−3]
[0.3, 0.4, 0.5] [1, 2, 3]
[8, 16, 32, 64] [1, 2, 3, 4]
[4, 8, 12, 16, 24, 32]

3https://pytorch.org/ 4https://www.dgl.ai/

19

A.4 Baseline Setup
For the baselines, the settings are described as follows: • MLP: We implement MLP with Pytorch with the Morgan ﬁngerprint. We use a two-layer MLP and set the hidden dimension to 100 with dropout 0.3. • Node2vec: We follow the ofﬁcially released implementation from authors5 and set the embedding dimension to 64. • Decagon: We use DGL to implement the model. Following (Zitnik et al., 2018a), we set the number of GNN layers to 2 set the hidden dimension to 64 and 32 for two layers with a dropout rate of 0.1 and a minibatch size of 512. • GAT: We use DGL to implement the model and set the hidden dimension to 64 with 4 attention heads, as we ﬁnd that improving the number of heads will hurt the performance. We set the activation function to LeakyReLU with α = 0.2. • Others: We follow the ofﬁcially released implementa- tion from the authors listed as follows: – SkipGNN: https://github.com/kexinhuang12345/SkipGNN. – KG-DDI: the neural model is based on code in https://github.com/rezacsedu/ Drug-Drug-Interaction-Prediction, and the KG embeddings are trained via OpenKE toolbox https://github.com/thunlp/OpenKE. – GraIL: https://github.com/kkteru/grail. – KGNN: https://github.com/xzenglab/KGNN.
5https://github.com/aditya-grover/node2vec
20

