Journal of Machine Learning Research 1 (2000) 1-48

Submitted 4/00; Published 10/00

arXiv:1508.06235v4 [stat.ML] 31 Oct 2015

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

Daniel Khashabi Department of Computer Science University of Illinois, Urbana-Champaign Urbana, IL 61801 USA
John Wieting Department of Computer Science University of Illinois, Urbana-Champaign Urbana, IL 61801 USA
Jeﬀrey Yufei Liu Google 1600 Amphitheatre Parkway Mountain View CA, 94043
Feng Liang Department of Statistics University of Illinois, Urbana-Champaign Urbana, IL 61801 USA

khashab2@illinois.edu wieting2@illinois.edu
liu105@illinois.edu liangf@illinois.edu

Editor: ?

Abstract

In this paper, we propose a model-based clustering method (TVClust) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data. Our method is based on a nonparametric Bayesian hierarchical model that combines a probabilistic model for the data instances with one for the side-information. An eﬃcient Gibbs sampling algorithm is proposed for posterior inference. Using the small-variance asymptotics of our probabilistic model, we derive a new deterministic clustering algorithm (RDP-means). It can be viewed as an extension of K-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be speciﬁed a priori. We compare our work with many constrained clustering algorithms from the literature on a variety of data sets and conditions such as using noisy side information and erroneous k values. The results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature.
Keywords: Constrained Clustering, Model-based methods, Two-view clustering, Asymptotics, Non-parametric models.

. Authors have contributed equally to this work. c 2000 Daniel Khashabi, John Wieting, Jeﬀrey Yufei Liu, Feng Liang.

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang
1. Introduction
We consider the problem of clustering with side information, focusing on the type of side information represented as pairwise cluster constraints between any two data instances. For example, when clustering genomics data, we could have prior knowledge on whether two proteins should be grouped together or not; when clustering pixels in an image, we would naturally impose spatial smoothness in the sense that nearby pixels are more likely to be clustered together.
Side information has been shown to provide substantial improvement on clustering. For example, Jin et al. (2013) showed that combining additional tags with image visual features oﬀered substantial beneﬁts to information retrieval and Khoreva et al. (2014) showed that learning and combining additional knowledge (must-link constraints) oﬀers substantial beneﬁts to image segmentation.
Despite the advantages of including side information, how to best incorporate it remains unresolved. Often the side-information in real applications can be noisy, as it is usually based on heuristic and inexact domain knowledge, and should not be treated as the ground truth which further complicates the problem.
In this paper, we approach incorporating side information from a new perspective. We model the observed data instances and the side information (or constraint) as two sources of data that are independently generated by a latent clustering structure - hence we call our probabilistic model TVClust (Two-View Clustering). Speciﬁcally, TVClust combines the mixture of Dirichlet Processes of the data instances and the random graph of constraints. We derive a Gibbs sampler for TVClust (Section 3). Furthermore, inspired by Jiang et al. (2012), we scale the variance of the aforementioned probabilistic model to derive a deterministic model. This can be seen as a generalization of K-means to a nonparametric number of clusters that also uses side instance-level information (Section 4). Since it is based on the DP-means algorithm (Jiang et al., 2012), and it uses relational side information we call our ﬁnal algorithm Relational DP-means (RDP-means). Lastly, experiments and results are presented (Section 5) in which we investigate the behavior of our algorithm in diﬀerent settings and compare to existing work in the literature.
2. Related Work
There has been a plethora of work that aims to enhance the performance of clustering via side information, either in deterministic or probabilistic settings. We refer the interested reader to existing comprehensive literature reviews of this subarea such as Basu et al. (2008).
K-means with side information: Some of the earliest eﬀorts to incorporate instancelevel constraints for clustering were proposed by Wagstaﬀ & Cardie (2000) and Wagstaﬀ et al. (2001). In these papers, both must-link and cannot-link constraints were considered in a modiﬁed K-means algorithm. A limitation of their work is that the side information must be treated as the ground-truth and is incorporated into the models as hard constraints.
Other algorithms similar in nature to K-means have been proposed as well that incorporate soft constraints. These include MPCK-means Bilenko et al. (2004), Constrained Vector Quantization Error (CVQE) Pelleg & Baras (2007) and its variant Linear Con-
2

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm
strained Quantization Error (LCVQE) Pelleg & Baras (2007).1 Unlike these approaches, our algorithm is derived from using small variance asymptotics on our probabilistic model and therefore is derived in a more principled fashion. Moreover, our deterministic model doesn’t require as input the goal number of clusters, as it determines this from the data.
Probablistic clustering with side information: Motivated by enforcing smoothness for image segmentation, Orbanz & Buhmann (2008) proposed combining a Markov Random Field (MRF) prior with a nonparametric Bayesian clustering model. One issue with their approach is that by its nature, MRF can only handle must-links but not cannot links. In contrast, our model, which is also based on a nonparametric Bayesian clustering model, can handle both types of constraints.
Spectral clustering with side information: Following the long tail of works on spectral clustering techniques (e.g. Ng et al. (2002); Shi & Malik (2000)), they’re some works using these techniques with side information, mostly diﬀering by how the Laplacian matrix is constructed or by various relaxations of the objective functions. These works include Constrained Spectral Clustering (CSR) Wang & Davidson (2010) and Constrained 1-Spectral Clustering (C1-SC) Rangapuram & Hein (2012).
Supervised clustering : There has been considerable interest in supervised clustering, where there is a labeling for all instances Finley & Joachims (2005); Zhu et al. (2011) and the goal is to create uniform clusters with all instances of a particular class. In our work, we aim to use side cues to improve the quality of clustering, making full labeling unnecessary as we can also make use of partial and/or noisy labels.
Non-parametric K-means: There has been recent work that bridges the gap between probabilistic clustering algorithms and deterministic algorithms. The work by Kulis & Jordan (2011) and Jiang et al. (2012) show that by properly scaling the distributions of the components, one can derive an algorithm that is very similar to K-means but without requiring knowledge of the number of clusters, k. Instead, it requires another parameter λ, but DP-means is much less sensitive to this parameter than K-means is to k. We use a similar technique to derive our proposed algorithm, RDP-means.
3. A Nonparametric Bayesian Model
In this section, we introduce our probabilistic model based on multi-view learning (Blum & Mitchell, 1998). In multi-view learning, the datas consists of multiple views (independent sources of information). In our approach we consider the following two views:
1. A set of observations {xi ∈ Rp}ni=1.
2. The side information, between pairs of points, indicating how likely or unlikely two points are to appear in the same cluster. The side information is represented by a symmetric n×n matrix E: if a priori xi and xj are believed to belong to the same cluster, then Eij = 1. If they are believed to be in diﬀerent clusters, Eij = 0. Otherwise, if there is no side information about the pair (i, j), we denote it with Eij = NULL. For future reference, denote the set of side information as C = {(i, j) : Eij = NULL}.
1. These models are further studied in Covoes et al. (2013).
3

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

We refer to our data, x1:n and E, as two diﬀerent views of the underlying clustering structure. It is worth noting that either view is suﬃcient for clustering with existing algorithms. Given only the data instances xi’s, it is the familiar clustering task where many methods such as K-means, model-based clustering (Fraley & Raftery, 2002) and DPM can be applied. Given the side information E, many graph-based clustering algorithms, such as normalized graph-cut (Shi & Malik, 2000) and spectral clustering (Ng et al., 2002) can be applied.
Our approach tries to aggregate information from the two views through a Bayesian framework and reach a consensus about the cluster structure. Given the latent clustering structure, data from the two views is modeled independently by two generative models: x1:n is modeled by a Dirichlet Process Mixture (DPM) model (Antoniak, 1974; Ferguson, 1973) and E is modeled by a random graph (Erd¨os & R´enyi, 1959).
Aggregating the two views of x1:n and E is particularly useful when neither view can be fully trusted. While previous work such as constrained K-means or constrained EM assume and rely on constraint exactness, TVClust uses E in a “soft” manner and is more robust to errors. We can call Eij = 1 a may link and Eij = 0 a may-not link, in contrast with the aforementioned must-link and cannot-link, to emphasize that our model tolerates noise in the side information.

3.1 Model for Data Instances
We use the Mixture of Dirichlet Processes as the underlying clustering model for the data instances {xi}ni=1. Let θi denote the model parameter associated with observation xi, which is modeled as an iid sample from a random distribution G. A Dirichlet Process DP(α, G0) is used as the prior for G:

θ1, . . . , θn|G i∼id G, G ∼ DP(α, G0).

(1)

Denote the collection (θ1, . . . , θi−1, θi+1, . . . , θn) by θ\i. With prior speciﬁcation (1), the distribution of θi given θ\i (after integrating out G) can be found following the BalckwellMacQueen urn scheme (Blackwell & MacQueen, 1973):

K
p(θi|θ\i) ∝ n−i,kδθk∗ (θi) + αG0(θi), (2)
k=1

where we assume there are K unique values among θ\i, denoted by θ1∗, . . . , θK∗ , δθk∗(·) is the Kronecker delta function, and n−i,k is the number of instances accumulated in cluster k

excluding instance i. From (2) we can see a natural clustering eﬀect in the sense that with

a

positive

probability,

θi

will

take

an

existing

value

from

θ

∗ 1

,

.

.

.

,

θ

∗ K

,

i.e.

it

will

join

one

of

the K clusters. This eﬀect can be interpreted using the Chinese Restaurant Process (CRP)

metaphor Aldous (1983), where assigning θi to a cluster is analogous to a new customer

choosing a table in a Chinese restaurant. The customer can join an already occupied table

or start a new one.

Given θi, we use a parametric family p(xi|θi) to model the instance xi. In this paper,

we focus on exponential families:

p(x|θ) = exp ( T (x), θ − ψ(θ) − h(x)) ,

(3)

4

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

where ψ(θ) = log exp ( T (x), θ − h(x)) dx is the log-partition function (cumulant generating function) and T (x) is the vector of suﬃcient statistics, given input point x. To simplify the exposition, we assume that x is the augmented vector of suﬃcient statistics given an input point, and simplify (3) by removing T (·):

p(x|θ) = exp ( x, θ − ψ(θ) − h(x)) .

(4)

It is easy to show that for this formulation,

Ep [x] = ∇θψ(θ),

(5)

Covp [x] = ∇2θψ(θ).

(6)

For convenience, we choose the base measure G0, in DP(α, G0) from the conjugate family, which takes the following form:

dG0(θ|τ , η) = exp ( θ, τ − ηψ(θ) − m(τ , η)) ,

(7)

where τ and η are parameters of the prior distribution. Given these deﬁnitions of the likelihood and conjugate prior, the posterior distribution over θ is an exponential family distribution of the same form as the prior distribution, but with scaled parameters τ + x and η + 1.
Exponential families contain many popular distributions used in practice. For example, Gaussian families are often used to model real valued points in Rp, which correspond to T (x) = [x, xT x]T , and θ = (µ, Σ), where µ is the mean vector, and Σ is the covariance matrix. The base measure often chosen for Gaussian families is its conjugate prior, the Normal-Inverse-Wishart distribution. Another popular parametric family, the multinomial distribution, is often used to model word counts in text mining or histograms in image segmentation. This distribution corresponds to T (x) = x and the base measure is often chosen to be a Dirichlet distribution, its conjugate prior.

3.2 Model for Side Information

Given θ1:n = (θ1, . . . , θn), we can summarize the clustering structure by a matrix Hn×n where Hij = δθi(θj). Note that H should not be confused with E. E represents the side information and can be viewed as a random realization based on the true clustering structure

H. We want to infer H based on E and x1:n. We model E using the following generative process: with probability p an existing edge

of H is preserved in E, and with probability q a false edge (of H) is added to E, i.e. for

any (i, j) ∈ C:


p(Eij = 0|Hij = 1) = p,   p(Eij = 1|Hij = 0) = 1 − p,

p(Eij = 0|Hij = 0) = q,   p(Eij = 1|Hij = 0) = 1 − q.

or more concisely,

p(Eij |Hij , p, q) = pEijHij (1 − p)(1−Eij)Hij q(1−Eij)(1−Hij)(1 − q)Eij(1−Hij).

(8)

5

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

G0

α

DP

G

ψi,j

θi

Φi,j

xi
i = 1...n

Eij
i, j = 1...n

Figure 1: Graphical representation of TVClust. The data generating process for the data instances is on the left and the process for the side information is on the right.

The values p and q represent the credibility of the values in the matrix E, while the values 1 − p and 1 − q are error probabilities. One may be able to set the value for (p, q) based on expert knowledge or learn them from the data in a fully Bayesian approach by adding another layer of priors over p and q,
p ∼ Beta(αp, βp), q ∼ Beta(αq, βq).

3.3 Posterior Inference via Gibbs Sampling

A graphical representation of our model TVClust is shown in Figure 1. parameters θ1, . . . , θn, the full data likelihood is

n

n

p(x1:n, E|θ1:n) = p(xi|θi)

p(Eij|θ1:n, p, q),

i=1

1≤i<j≤n

Based on the

where p(Eij = NULL|θ1:n, p, q) = 1, i.e. no side information is provided for pair (i, j). A Gibbs sampling scheme can be derived for our TVClust model, which is in spirit
similar to other Gibbs samplers for DPM (see the comprehensive review at Neal (2000)). The Gibbs sampler involves iteratively sampling from the full conditional distribution of each unknown parameter given other parameters and the data. The key step is the sampling of p(θi|θ\i, xi, E, p, q). Using the independence between variables (see1 the graphical model in Figure 1), we have

p(θi|θ\i, x1:n, E, p, q) ∝ p(xi|θi)p(Ei|θi, θ\i, p, q)p(θi|θ\i),

(9)

where we use Ei = {Eij : i = j} to denote the set of side information related to data instance i. Following the Blackwell-MacQueeen urn presentation of the prior (2), we have

K
p(θi|θ\i, x1:n, E, p, q) ∝ n−i,kp(xi|θk∗)p(Ei|θk∗, θ\i, p, q)δθk∗ (θi)
k=1

+ αp(xi|θi)p(Ei|θi, θ\i, p, q)G0(θi).

(10)

6

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

The full conditional of θi given others is a mixture of a discrete distribution with point

masses

located

at

θ

∗ 1

,

.

.

.

,

θ

∗ K

and

a

continuous

component.

Sampling from the discrete

component only involves evaluation of the likelihood function of xi and Ei, which can

be easily computed. Now we focus on sampling θi from the continuous component. First

observe that when θi is sampled from this continuous component, we have Hij = δθi(θj) = 0

for all j = i, therefore:

p(Ei|θi, θ\i) = qEij (1 − q)(1−Eij),

j=i

which does not depend on the actual value of θi. Also note that

p(xi|θi)G0(θi) = pG0 (θi|xi)pG0 (xi),

where the subscript G0 is used to emphasize that the posterior and the marginal distributions, pG0(θi|xi) and pG0(xi), are calculated with respect to prior G0. Then we can rewrite the sampling distribution for the continuous component as

αp(xi|θi)p(Ei|θi, θ\i, p, q)G0(θi) ∝ αpG0 (xi) qEij (1 − q)(1−Eij) pG0 (θi|xi).
j=i

Finally we can simplify the sampling distribution (10) as

p(θi|θ\i, xi, E, p, q)
K
∝ n−i,kp(xi|θk∗)δθk∗ (θi)
k=1

p fki 1−q

1−p q

sik
+ αpG0 (xi)pG0 (θi|xi),

(11)

where

fki

=

#{j

:

θj

=

θ

∗ k

,

Eij

=

1},

(12)

sik

=

#{j

:

θj

=

θ

∗ k

,

Eij

=

0}.

Using the analogy of the Chinese Restaurant interpretation of DPM (Aldous, 1983), we
can interpret the sampling distribution of θi in the following way. Let instance i be a friend of instance j, if Eij = 1. Similarly two instances are strangers, if Eij = 0. So fik is the number of friends of instance i at table k, and ski is the number of strangers for i at table k.
As mentioned before, the values p and q represent the credibility of side information.
For a reasonable conﬁdence over constraints usually p > 1 − q. Then by (11), the chance
of a person assigned to a table not only increases with the popularity of the table (i.e. the table size n−i,k) like in the original DPM, but also increases with their friend count fki and decreases with their stranger count sik.
Instead of sequentially updating the point-speciﬁc parameters (θ1, . . . , θn), one can sequentially update an equivalent parameter set: the set of cluster-speciﬁc parameters (θi∗, . . . , θK∗ ) and the cluster assignment indicators (z1, . . . , zn), where zi ∈ {1, . . . , K} indicates the cluster assignment for instance i, i.e., θi = θz∗i. By our derivation at (11), we can update zi’s sequentially as

 p(zi = k) p(zi = knew)

∝ n−i,kp(xi|θ∗) p fki
k 1−q
∝ α p(xi|θk∗)dG0.

1−q p sik ,

(13)

7

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

The

cluster

parameters

(θ

∗ i

,

.

.

.

,

θ

∗ K

),

given

the

partition

z1:n

and

the

data

x1:n,

can

be

updated similarly as they were in Algorithm 2 of Neal (2000).

4. RDP-means: A Deterministic Algorithm
In this section, we apply the scaling trick as described in Jiang et al. (2012) to transform the Gibbs sampler to a deterministic algorithm, which we refer to as RDP-mean.

4.1 Reparameterization of the exponential family using Bregman divergence
We bring in the notion of Bregman divergence and its connection to the exponential family. Our starting point is the formal deﬁnition of the Bregman divergence.
Deﬁnition 1 ((Bregman, 1967)) Deﬁne a strictly convex function φ : S → R, such that the domain S ⊆ Rp is a convex set, and φ is diﬀerentiable on ri(S), the relative interior of S, where its gradient ∇φ exists. Given two points x, y ∈ Rp, the Bregman divergence Dφ(x, y) : S × ri(S) → [0, +∞) is deﬁned as:
Dφ(x, y) = φ(x) − φ(y) − x − y, ∇φ(y) .
The Bregman divergence is a general class of distance measures. For instance, with a squared function φ, Bregman divergence is equivalent to Euclidean distance (See Table 1 in Banerjee et al. (2005) for other cases).
Forster & Warmuth (2002) showed that there exists a bijection between exponential families and Bregman divergences. Given this connection, Banerjee et al. (2005) derived a K-means type algorithm for ﬁtting a probabilistic mixture model (with ﬁxed number of components) using Bregman divergence, rather than the Euclidean distance.

Deﬁnition 2 (Legendre Conjugate) For a function ψ(.) deﬁned over Rp, deﬁne its convex conjugate ψ∗(.) as, ψ∗(µ) = supθ∈dom(ψ) { µ, θ − ψ(θ)} . In addition, if the function ψ(θ) is closed and convex, (ψ∗)∗ = ψ.

It can be shown that the log-partition function of the exponential families of distributions is a closed convex function (see Lemma 1 of Banerjee et al. (2005)). Therefore there is a bijection between the conjugate parameter µ of the Legendre conjugate ψ∗(·), and the parameter of the exponential family, θ, in the log-partition function deﬁned for the exponential family at (3). With this bijection, we can rewrite the likelihood (4) using the Bregman divergence and the Legendere conjugate:

p(x|θ) = p(x|µ) = exp (−Dψ∗(x, µ)) fψ∗(x),

(14)

where fψ∗(x) = exp (ψ∗(x) − h(x)). The left side of (14) is written as p(x|θ) = p(x|µ) to stress that conditioning on θ is equivalent to conditioning on µ, since there is a bijection between them and the right side of (14) is essentially the same as (4). A nice intuition about this reparameterization is that now the likelihood of any data point x is related to how far it is from the cluster components parameters µ, where the distance is measured using the Bregman divergence Dψ∗(x, µ).

8

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

Similarly we can rewrite the prior (7) in terms of the Bregman divergence and the Legendere conjugate:

τ

p(θ|τ , η) = p(µ|τ , η) = exp −ηDψ∗( η , µ) gψ∗(τ , η),

(15)

where gψ∗(τ , η) = exp (ηψ(θ) − m(τ, η)) .

4.2 Scaling the Distributions

Lemma 3 (Jiang et al. (2012)) Given the exponential family distribution (4), deﬁne another probability distribution with parameter θ˜, and log-partition function ψ˜(.), where θ˜ = γθ, and ψ˜(θ˜) = γψ(θ˜/γ), then:
1. The scaled probability distribution p˜(.) deﬁned with parameter vector θ˜, and log-partition function ψ˜(.), is a proper probability distribution and belongs to the exponential family.

2. The mean and variance of the probability distribution p˜(.) are:

Ep˜(x) = Ep(x),

1 Covp˜(x) = γ Covp(x).

3. The Legendre conjugate of ψ˜(.)2 is: ψ˜∗(θ˜) = γψ∗(θ˜).

The implication of Lemma 3 is that the covariance Covp(x) scales with 1/γ, which is close to zero when γ is large, but the mean Ep(x) remains the same. Thus we can obtain a deterministic algorithm when γ goes to inﬁnity.
With the scaling trick, the scaled prior and scaled likelihood can be written as:

p˜(x|θ, γ) = p˜(x|µ, γ) = exp (−γDψ∗(x, µ)) fγψ∗(x) p˜(θ|τ , η, γ) = p˜(µ˜ |τ , η, γ) = exp −ηDψ∗( τη , µ) gγψ∗(τ /γ, η/γ) (16)

4.3 Asymptotics of TVclust
Using the scaling distributions (16) we can write the Gibbs update (13) in the following form:



p fki 1 − p sik

p(zi = k)

∝ n−i,k exp (−γDψ∗ (xi, µk)) 1 − q

q

(17)

p(zi = knew) ∝ α p˜(xi|θ)p˜(θ|τ , η)dθ

Following Jiang et al. (2012), we can approximate the integral I = p˜(x|θ)p˜(θ|τ , η)dθ using the Laplace approximation (Tierney & Kadane, 1986):

p˜(x|τ , η, γ) ≈gγψ∗(τ /γ, η/γ) exp −γφ(x) − ηφ(τ /η) − (γ + η)φ( γx + τ ) γdCov γx + τ .

γ+η

γ+τ

2. ψ˜(.) ∗ the conjugate of ψ˜(.), is denoted with ψ˜∗(.) for simplicity.

9

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

We can write the resulting expression as a product of a function of the parameters and a function of the input observations:

p˜(x|τ , η, γ) ≈ κ(τ , η, γ) × ν(x; τ , η, γ)

The concentration parameter of the DPM, α in (8), is usually tuned by user. To get the desired result, we choose it to be:
α = κ(τ , η, γ)−1 exp(λγ),

where λ is a new parameter introduced for the model. In other words, the eﬀect of the other parameters (α, τ , η) is now transferred to λ. Then the 2nd line of (17) becomes

p(zi = knew) = 1 ν(xi; τ , η, γ) exp (−γλ) ,

(18)

Z n+α−1

such that ν(xi; τ , η, γ) becomes a positive constant when γ goes to inﬁnity. Applying a similar trick to the 1st line of (17), we have

p fki 1−q

1−p q

sik
= exp
= exp

fki ln

p 1−q

− sik ln

γ fki .ξ1 − sik.ξ2

q 1−p

where we introduced new variables ξ1 = ln

p 1−q

and ξ2 = ln

q 1−p

, which represents the

conﬁdence on having a link, and not having a link, respectively. Then the 1st line of (8)

becomes:

p(zi = k) = Z1 n +n−αi,−k 1 exp −γ Dψ∗(xi, µk) − fki .ξ1 + sik.ξ2 . (19)

Combining (18) and (19), we can rewrite the Gibbs updates (13) as follows:

p(zi = k) p(zi = knew)

∝ n−i,k exp −γ Dψ∗ (xi, µk) − fki .ξ1 + sik.ξ2 ∝ ν(xi; τ , η, γ) exp (−γλ) .

When γ goes to inﬁnity, the Gibbs sampler degenerates into a deterministic algorithm, where in each iteration, the assignment of xi is determined by comparing the K + 1 values below:

Dψ∗ (xi, µ1) − f1iξ1 + si1ξ2, . . . , Dψ∗ (xi, µK ) − fKi .ξ1 + siK ξ2, λ ;

If the k-th value (where k = 1, . . . , K) is the smallest, then assign xi to the k-th cluster. If λ is the smallest, form a new cluster.

10

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

4.4 Sampling the cluster parameters
Given the cluster assignments {zi}ni=1, the cluster centers are independent of the side information. In other words, the posterior distribution over the cluster assignments can be written in the following form:

p(µk|x1:n, z1:n, τ , η, γ, ξ) ∝

p˜(xi|µk, γ) × p˜(µk|τ , η, γ)

i:zi=k

∝ exp − (γnk + η) Dψ∗

i:zi=k γxi + τ , µ

γnk + η

k

in which nk = # {i : zi = k}. When γ → ∞,







1

p(µk|x1:n, z1:n, τ , η, γ, ξ) ∝ exp − (γnk + η) Dψ∗  nk

xi, µk .

i:zi=k

The maximum is attained when the arguments of the Bregman divergence are the same,

i.e. 1

µk = nk

xi.

i:zi=k

So cluster parameters are just updated by the corresponding cluster means. This completes the algorithm for RDP-mean which is shown in Algorithm 1.

4.5 Eﬀect of changing ξ1 and ξ2
Taking ξ1, ξ2 → 0, RDP-means will behave like DP-means, i.e. no side information is considered. Taking ξ1, ξ2 → +∞ puts all the weight on the side information and no weight on the point observations. In other words, it generates a set of clusters according to just the constraints in E. In a similar way, we can put more weight on may links compared to may-not links by choosing ξ1 > ξ2 and vice versa.
As we will show, there is an objective function which corresponds to our algorithm. The objective function has many local minimum and the algorithm minimizes it in a greedy fashion. Experimentally we have observed that if we initialize ξ1 = ξ2 = ξ with a very small value ξ0 and increase it each iteration, incrementally tightening the constraints, it gives a desirable result.

4.6 Objective Function
Theorem 4 The constrained clustering RDP-means (Algorithm 1) iteratively minimizes the following objective function.

K
min
{Ik}K k=1 k=1 i∈Ik

Dφ(xi, µk) − ξ1fki + ξ2sik

+ λK

(20)

where I1, . . . , IK denote a partition of the n data instances.

11

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

Algorithm 1: Relational DP-means algorithm

Input: The data points D = {xi}, Relational matrix E, The parameter of the Bregman divergence ψ∗, the parameters λ, ξ0, and its rate of increase at each iteration ξrate. Result: The assignment variables z = [z1, z2, . . . , zn] and the component parameters. Initialization: ξ ← ξ0, and all points are assigned to one single cluster.
while not converged do

for xi ∈ D do for µk ∈ C do Find the values of fki and sik for xi from the matrix E, and using the current z as deﬁned in (20). ;
dist(xi, µk) ← Dψ∗ (xi, µk) − ξ1fki + ξ2sik ; end
[dmin, imin] ← {dist(xi, µ1), . . . , dist(xi, µK )} ; // dmin is the minimum distance and imin is the index of the minimum distance. if dmin < λ then
zi ← imin ; else
// Add a new cluster:
C ← {C ∪ xi} K ←K+1 end

end

for µk ∈ C do // given the current assignment of points, ﬁnd the set of points assigned to cluster k, Dk:

if |Dj | > 0 then

µK ←

xi∈Dj xi |Dj |

else

// Remove the cluster and apply the changes to the related variables ;

end

end ξ ← ξ × ξrate end

Proof In the proof we follow a similar argument as in Kulis & Jordan (2011). For simplicity, let us assume ξ1 = ξ2 = ξ and call the value Dφ(xi, µk) − ξ(fki − sik) the augmented distance. For a ﬁxed number of clusters, each point gets assigned to the cluster that has
the smaller augmented distance, thus decreasing the value of the objective function. When the augmented distance value of an element Dφ(xi, µk) − ξ(fki − sik) is more than λ, we remove the point from its existing cluster, add a new cluster centered at the data point
and increase the value of K by one. This increases the objective by λ (overall decrease in
the objective function). For a ﬁxed assignment of the points to the clusters, ﬁnding the
cluster centers by averaging the assigned points minimizes the objective function. Thus,
the objective function is decreasing after each iteration.

4.7 Spectral Interpretation
Following the spectral relaxation framework for the K-means objective function introduced by Zha et al. (2001) and Kulis & Jordan (2011), we can apply the same reformulation to our framework, given the objective function (20). Consider the following optimization problem:

max tr Y K − λI + ξ1E+ − ξ2E− Y ,

(21)

{Y |Y Y =In}

12

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

where Y = Z(Z Z)−1/2 ∈ Rp×k is the normalized point-component assignment matrix, and K is the kernel matrix which is deﬁned as:
K = A A ∈ Rp×p, A = [x1, . . . , xn] ∈ Rp×n
E+ = 1{E > 0} and E− = 1{E < 0} are side information matrices for may and may-not links, respectively (where 1{.} is applied elementwise). In particular if ξ1 = ξ2 = ξ Equation 21 becomes:

max tr Y (K − λI + ξE) Y .
{Y |Y Y =In}

Theorem 5 The objective function in (21) is equivalent the objective function in (20).

Proof In Kulis & Jordan (2011) (Lemma 5.1) it has been proved that max{Y |Y Y =In} tr Y (K − λI) Y

is equivalent minimizing the objective function of DP-means. For simplicity, we prove the

case for ξ1 = ξ2 = ξ, although the general case can also be proved in a very similar fashion.

In our objective function we have the additional term max{Y |Y Y =In} tr Y we will prove to be ξ Kk=1 i∈Ik fki − sik :

(ξE) Y

which

tr Y (ξE)Y

= ξtr Y EY

K
=ξ

E(i, j)

k=1 i∈Ik j∈Ik

K
=ξ

  1 {E(i, j) = 1} −

 1 {E(i, j) = −1}

k=1 i∈Ik j∈Ik

j∈Ik

K

=ξ

fki − sik .

k=1 i∈Ik

Given the objective function in Equation 21, we can use Theorem 5.2 in Kulis & Jordan (2011) and design a spectral algorithm for solving our problem, simply by ﬁnding eigenvectors of K + ξ1E+ − ξ2E− that have an eigenvalue larger than λ. By this interpretation one can easily see that if ξ1 = ξ2 = 0, the objective function is equivalent to the DP-means objective and when ξ1 and ξ2 are large, the clustering only makes use of the side information.
5. Experiments
In this section we report experiments on simulated data, a variety of UCI datasets and an Image Net dataset.3 For evaluation, we report the F -measure (F) exactly as deﬁned in Section 4.1 of Bilenko et al. (2004), adjusted Rand index (AdjRnd) and normalized mutual
3. The code and data for our experiments and implementation is available at https://goo.gl/i6yoPb.
13

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

True labels K−means RDP−means LCVQE

10

10

10

10

MPCKMeans TVClust

10

10

0

0

0

0

0

0

−10

−10

−10

−10

−10

−10

−10 0 10 −10 0 10 −10 0 10 −10 0 10 −10 0 10 −10 0 10

True labels K−means RDP−means LCVQE MPCKMeans TVClust

5

5

5

5

5

5

0

0

0

0

0

0

−5

−5

−5 0 5 −5 0 5

True labels K−means

10

10

0

0

−10

−10

−10 0 10
True labels 6 4 2 0 −2 −4
0 5 10
True labels 20

−10 0 10
K−means 6 4 2 0 −2 −4
0 5 10
K−means 20

−5

−5

−5 0 5 −5 0 5

RDP−means LCVQE 10

0

0

−10

−10

−20 −10 0 10

−10 0 10

RDP−means LCVQE

6

6

4

4

2

2

0

0

−2

−2

−4

−4

0 5 10 0 5 10

RDP−means LCVQE

20

20

−5

−5

−5 0 5 −5 0 5

MPCKMeans TVClust

10

10

0

0

−10

−10

−10 0 10

−10 0 10

MPCKMeans TVClust

6

6

4

4

2

2

0

0

−2

−2

−4

−4

0 5 10 0 5 10

MPCKMeans TVClust

20

20

0

0

0

0

0

0

−20

−20

−20

−20

−20

−20

−20 0 20 −20 0 20 −20 0 20 −20 0 20 −20 0 20 −20 0 20

True labels K−means RDP−means LCVQE MPCKMeans TVClust

20

20

20

20

20

20

0

0

0

0

0

0

−20

−20

−20

−20

−20

−20

−20 0 20

−20 0 20

−20 0 20

−20 0 20

−20 0 20

−20 0 20

Figure 2: Comparison of cluster quality on example of our synthetic data.

information (NMI). For RDP-Means, in all experiments, we terminate the algorithm when the cluster assignments did not change after 20 iterations, and we initialize ξ0 = 0.001 and ξrate = 2. For DP-means and RDP-means we calculate λ based on the k-th furthest ﬁrst method explained in Kulis & Jordan (2011). Although we use the actual k in calculating λ, in practice, λ is less sensitive to initialization (See Figure 3).
We compare with all constrained or semi-supervised clustering techniques from the literature that we could ﬁnd online and from personal communications.4 In our experiments, we do not include results from methods where we observed unstable behavior such as numerical instabilities. The parameters for all algorithms were set to the default settings of the authors’ implementation. 5.
We experiment on three tasks. The ﬁrst experiment evaluates the algorithms on a set of two-dimensional simulated data that showcases diﬃcult clustering problems in order to gain some visual intuition of how the algorithms perform. The second experiment evaluates on a collection of 5 datasets from the UCI repository, commonly used for evaluation of clustering tasks: iris, wine, ecoli, glass, and balance. We also study the eﬀect of varying some of the key parameters in these experiments. The third task illustrates the eﬀectiveness of using side information in the image clustering task of Jiang et al. (2012).
One important variable is the percentage of side information r, which is the number of ±1 elements in the E matrix (Section 3.1) normalized by its size. We experimented
4. See http://goo.gl/tSSH95 for a list of methods with links to their implementations. 5. The code for LCVQE is kindly provided by the authors of Covoes et al. (2013) via personal communi-
cation.
14

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

Method \ Dataset
K-means DP-means TVClust(variational) RDP-means MPCKMeans LCVQE Method \ Dataset
K-means DP-means TVClust(variational) RDP-means MPCKMeans LCVQE

F 0.81 0.74 0.91 0.86 0.53 0.73
F 0.57 0.53 0.42 0.82 0.46 0.64

iris AdjRnd
0.71 0.57 0.85 0.80 0.29 0.58
glass AdjRnd
0.47 0.29 0.22 0.76 0.30 0.55

NMI 0.74 0.69 0.90 0.80 0.30 0.60
NMI 0.71 0.45 0.42 0.73 0.36 0.66

F 0.59 0.63 0.56 0.81 0.54 0.57
F 0.47 0.31 0.94 0.94 0.70 0.62

wine AdjRnd
0.36 0.37 0.45 0.73 0.30 0.35
balance AdjRnd
0.14 0.12 0.92
0.92 0.26 0.38

NMI 0.42 0.44 0.53 0.72 0.30 0.39
NMI 0.12 0.21 0.91 0.88 0.28 0.37

ecoli F AdjRnd NMI 0.61 0.50 0.63 0.71 0.61 0.64 0.85 0.79 0.76 0.90 0.86 0.82 0.57 0.33 0.33 0.61 0.52 0.61
averaged over datasets F AdjRnd NMI 0.61 0.44 0.52 0.58 0.39 0.48 0.74 0.64 0.70 0.87 0.81 0.79 0.56 0.30 0.31 0.64 0.48 0.53

Table 1: Results over each UCI dataset averaging over p and r parameters. RDP-means has the best performance overall but not in some particular cases as shown in Tables 2 and 3.

with varying r, adding noise with probability 1 − p to the constraint matrix, and deviating initialization from the true k value.
5.1 Simulated Data
We evaluate on 6 diﬀerent patterns using p = 1 (i.e. no noise) with a sampling rate of r = 0.01. The results for all patterns are shown in Figure 2. Each algorithm was tested 5 times, and we took the strongest result from these 5 runs to display.
5.2 UCI Datasets
We evaluate on ﬁve datasets6 experimenting with diﬀerent settings. In the ﬁrst setting, we vary the percentage of constraints sampled, r, which we choose from {0.01, 0.03, 0.05}. Secondly, we add noise, letting the parameter p take on values in {1.0, 0.95, 0.90, 0.80}. Lastly, we investigate the sensitivity of the algorithms to deviations from the true number of clusters, k, where we choose the deviation from the set {±3, ±2, ±1, 0}. For each dataset and set of hyper-parameters in this section, we average the results of ﬁve trials to produce the ﬁnal result.
Average over all parameters: We present the performance of the algorithms, per dataset, averaged over diﬀerent values of the parameters p and r. The results are summarized in Table 1 and show that overall, RDP-means has the best performance.
Average all parameters varying amount of noise: To analyze how adding noise to the constraints aﬀects performance, we vary the values of p for each algorithm on each dataset. As mentioned previously, the probability of choosing noisy constraints is proportional to 1 − p. The higher the value of p, the less noise in the constraints. The results as
6. The data is directly downloaded from http://archive.ics.uci.edu/ml/.
15

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

Method \ Param.
K-means DP-means TVClust (variational) RDP-means MPCKMeans
LCVQE

F 0.61 0.58 0.77 0.93 0.94 0.83

p=1
AdjRnd 0.44 0.39 0.69 0.90 0.91 0.76

NMI 0.53 0.49 0.74 0.89 0.90 0.79

F 0.60 0.59 0.75 0.92 0.46 0.64

p = 0.95
AdjRnd 0.43 0.39 0.66 0.89 0.14 0.48

NMI 0.52 0.48 0.72 0.87 0.17 0.53

F 0.61 0.59 0.73 0.87 0.44 0.58

p = 0.9
AdjRnd 0.43 0.40 0.63 0.82 0.10 0.39

NMI 0.53 0.49 0.69 0.79 0.12 0.45

F 0.61 0.58 0.68 0.75 0.41 0.50

p = 0.8
AdjRnd 0.43 0.38 0.56 0.65 0.04 0.27

NMI 0.52 0.48 0.64 0.62 0.07 0.35

Table 2: This table illustrates how the algorithms perform under diﬀerent levels of noise. We average the results over each UCI dataset and values of r.

a function of noise are summarized in Table 2. We average over diﬀerent constraint sizes r ∈ {0.01, 0.03, 0.05} and diﬀerent datasets.
First note that the results for K-means and DP-means are the same across diﬀerent noise rates,7 since these algorithms do not make use of constraints. Another observation is that, MPCKMeans has the best performance for p = 1, although for p = 0.95 its performance drops signiﬁcantly. Therefore, this method is a good option when the side information is relatively pure. Other methods, including RDP-means, TVClust and LCVQE, have drops as well when increasing the noise level, although the drops for TVClust and RDP-means are smaller.
Average all parameters varying amount of side information: To better understand the eﬀect of side-information, we unroll the results of Table 2 and show the performance as a function of r. The results are shown in Table 3.
Unsurprisingly, adding constraints (increasing r) increases the performance of those algorithms that make use of them. Interestingly, for p = 0.8 and r = 0.01, the best algorithms that do make use of constraints have similar performance to K-means and DPmeans (which do not use constraints). This suggests there could be space for improvement for handling noisy constraints.
Eﬀect of deviation from true number of clusters: Most of the algorithms we analyze are dependent on the true number of clusters, which is usually unknown in practice. Here, we investigate the sensitivity of those algorithms to perturbations in the true value of k. The DP-means algorithm of Jiang et al. (2012) is said to be less sensitive to the choice of k, since its parameter has weaker dependence on the choice of k. Similarly, since RDP-means is derived from DP-means, it is expected that it too would be relatively robust to deviations from the actual k.
For all algorithms and for each dataset, we set p = 1 and r = 0.03 and vary the number of clusters to k − deviation where deviation ∈ {±3, ±2, ±1, 0}.8 The results are shown in Figure 3. The x-axis shows the value of deviation and the y axis shows the value of F -measure.
Notice the performance of DP-means is clearly stable for diﬀerent choices of k, which supports the claim made in Jiang et al. (2012). Similarly RDP-means and TVClust show very stable results. MPCKmeans generally works well unless k is underestimated.
7. Small variations in the results of K-means is possible due to random initialization of each run 8. For some datasets where k = 3, we dropped the value deviation = 3. Also the implementation of
LCVEQ that we used needs at least 2 clusters to work. We are not aware of a more general available implementation for this algorithm.
16

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

r = 0.01

r = 0.03

Method \ Param.
K-means DP-means TVClust(variational) RDP-means MPCKMeans
LCVQE
K-means DP-means TVClust(variational) RDP-means MPCKMeans
LCVQE
K-means DP-means TVClust(variational) RDP-means MPCKMeans
LCVQE

F 0.62 0.58 0.72 0.84 0.83 0.73
F 0.60 0.59 0.78 0.98 0.99 0.86
F 0.62 0.58 0.81 0.96 1.00 0.88

p=1 AdjRnd
0.45 0.39 0.62 0.77 0.76 0.62
AdjRnd 0.43 0.39 0.70 0.98 0.99 0.81
AdjRnd 0.45 0.39 0.74 0.96 1.00 0.84

NMI 0.53 0.48 0.69 0.76 0.73 0.66
NMI 0.52 0.49 0.75 0.96 0.97 0.85
NMI 0.54 0.49 0.79 0.95 0.99 0.86

F 0.62 0.59 0.69 0.79 0.52 0.69
F 0.60 0.58 0.77 0.98 0.43 0.62
F 0.59 0.59 0.79 0.99 0.44 0.60

p = 0.95 AdjRnd
0.45 0.39 0.57 0.71 0.23 0.56
AdjRnd 0.42 0.39 0.69 0.97 0.07 0.47
AdjRnd 0.42 0.39 0.72 0.99 0.12 0.42

NMI 0.53 0.48 0.65 0.68 0.29 0.59
NMI 0.52 0.49 0.74 0.94 0.10 0.52
NMI 0.52 0.48 0.77 0.98 0.13 0.47

F 0.61 0.58 0.66 0.69 0.49 0.62
F 0.61 0.59 0.76 0.93 0.41 0.57
F 0.60 0.60 0.78 0.98 0.41 0.55

p = 0.9 AdjRnd
0.44 0.38 0.52 0.58 0.16 0.46
AdjRnd 0.44 0.39 0.68 0.90 0.04 0.38
AdjRnd 0.43 0.42 0.70 0.97 0.08 0.34

NMI 0.53 0.48 0.60 0.57 0.21 0.49
NMI 0.53 0.48 0.73 0.86 0.06 0.45
NMI 0.52 0.50 0.75 0.94 0.09 0.41

F 0.60 0.58 0.58 0.56 0.44 0.52
F 0.61 0.58 0.70 0.77 0.40 0.48
F 0.60 0.58 0.74 0.91 0.38 0.50

p = 0.8 AdjRnd
0.43 0.38 0.43 0.39 0.07 0.32
AdjRnd 0.45 0.38 0.59 0.69 0.02 0.25
AdjRnd 0.42 0.38 0.66 0.87 0.03 0.25

NMI 0.52 0.48 0.53 0.41 0.12 0.36
NMI 0.53 0.47 0.67 0.63 0.04 0.33
NMI 0.51 0.48 0.72 0.82 0.05 0.34

r = 0.05

Table 3: This table is an expanded version of Table 2 and shows how the algorithms perform under diﬀerent levels of noise and for each constraint sampling rate, averaged over each UCI dataset.

5.3 ImageNet Clustering
We repeat the same experiment from Jiang et al. (2012), where 100 images from 10 diﬀerent categories of the ImageNet data were sampled.9 Each image was processed via standard visual-bag-of-words where SIFT was applied to images patches and the resulting SIFT vectors were mapped into 1000 visual works. The SIFT feature counts were then used as features for that image, and since these features are discrete counts, they were modeled as if coming from a multinomial distribution. Thus we used the corresponding divergence measure, i.e. KL-divergence (as opposed to Euclidean distance in the Gaussian case) as the distance metric in the clustering.
We use Laplace smoothing,10 with a smoothing parameter of 0.3 to remove the illconditioning (division by zero inside the KL divergence). We also include the clustering results using a Gaussian model, to show the importance of choosing the appropriate distribution. The result of the evaluation are in Table 4. Clearly RDP-means has the best result as it makes use of the side information.
We also investigate the behavior of RDP-means as a function of the percentage of pairs sampled. The result is depicted in the Figure 4. In the case when the rate is close to zero, the model is equivalent to DP-means. The ﬁgure shows that, as we add more constraints the performance of the model consistently increases. When we sample only 6% of the pairs, we are able to almost fully reconstruct the true clustering without any loss of information.
9. The set of images from each clusters, and the extracted SIFT features are available at http://image-net. org.
10. See http://en.wikipedia.org/wiki/Additive_smoothing.
17

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang

Ecoli 1

Balance 1

0.8

0.8

F−measure F−measure

0.6

0.6

0.4

K−means

0.4

K−means

RDP−means

RDP−means

DP−means

DP−means

0.2

MPCKMeans

0.2

MPCKMeans

LCVQE

LCVQE

TVClust (Variational)

TVClust(variational)

0

0

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

deviation

deviation

Glass

Wine

1

1

0.8

0.8

F−measure F−measure

0.6

0.6

0.4

K−means

0.4

K−means

RDP−means

RDP−means

DP−means

DP−means

0.2

MPCKMeans

0.2

MPCKMeans

LCVQE

LCVQE

TVClust (Variational)

TVClust(variational)

0

0

−3

−2

−1

0

1

2

3

−3 −2.5 −2 −1.5 −1 −0.5 0 0.5 1 1.5 2

deviation

deviation

Iris

1

0.8

F−measure

0.6

K−means

0.4

RDP−means

DP−means

MPCKMeans 0.2 LCVQE

TVClust

0

−3

−2

−1

0

1

2

deviation

Figure 3: Comparison of clustering quality on each UCI datasets with deviations from the actual number of clusters. The x-axis shows deviation, where the number of clusters declared to each algorithm is k − deviation. The y-axis shows the F -measure evaluation of each clustering result.

F -measure

DP-Means

Gaussian Multinomial

0.18

0.22

K-Means

Gaussian Multinomial

0.18

0.25

RDP-Means

Gaussian Multinomial

0.20

0.44

Table 4: Results of clustering on ImageNet dataset. For each measure, the result is averaged over 10 runs.

Acknowledgments
The authors would like to thank Ke Jiang for providing the the data used in the UCI image clustering. We also thank Daphne Tsatsoulis, Eric Horn, Shyam Upadhyay, Adam Volrath and Stephen Mayhew for helpful comments on the draft.
References
Aldous, David. Random walks on ﬁnite groups and rapidly mixing markov chains. In S´eminaire de Probabilit´es XVII 1981/82, pp. 243–297. Springer, 1983.
18

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm

Performance measure

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4 NMI

0.3

F1

0.2

0.1

0

0.02

0.04

0.06

0.08

0.1

Rate for the pairs of constraints added to the constrained matrix E

Figure 4: Eﬀect of adding side information to the performance of RDP-means. As more information it added, performance improves.

Antoniak, Charles E. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics, pp. 1152–1174, 1974.
Banerjee, Arindam, Merugu, Srujana, Dhillon, Inderjit S, and Ghosh, Joydeep. Clustering with bregman divergences. The Journal of Machine Learning Research, 6:1705–1749, 2005.
Basu, Sugato, Davidson, Ian, and Wagstaﬀ, Kiri. Constrained clustering: Advances in algorithms, theory, and applications. CRC Press, 2008.
Bilenko, Mikhail, Basu, Sugato, and Mooney, Raymond J. Integrating constraints and metric learning in semi-supervised clustering. In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 11. ACM, 2004.
Blackwell, D. and MacQueen, J. Ferguson distributions via Polya urn schemes. The Annals of Statistics, 1:353–355, 1973.
Blum, Avrim and Mitchell, Tom. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pp. 92–100, New York, NY, USA, 1998. ACM. ISBN 1-58113-057-0. doi: 10.1145/279943.279962. URL http://doi.acm.org/10.1145/279943.279962.
Bregman, Lev M. The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming. USSR computational mathematics and mathematical physics, 7(3):200–217, 1967.
Covoes, Thiago F, Hruschka, Eduardo R, and Ghosh, Joydeep. A study of k-means-based algorithms for constrained clustering. Intelligent Data Analysis, 17(3):485–505, 2013.
Erd¨os, P. and R´enyi, A. On random graphs, I. Publicationes Mathematicae (Debrecen), 6: 290–297, 1959. URL http://www.renyi.hu/~{}p_erdos/Erdos.html#1959-11.
19

Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang
Ferguson, Thomas S. A bayesian analysis of some nonparametric problems. The annals of statistics, pp. 209–230, 1973.
Finley, Thomas and Joachims, Thorsten. Supervised clustering with support vector machines. In Proceedings of the 22nd international conference on Machine learning, pp. 217–224. ACM, 2005.
Forster, Ju¨rgen and Warmuth, Manfred K. Relative expected instantaneous loss bounds. Journal of Computer and System Sciences, 64(1):76–102, 2002.
Fraley, Chris and Raftery, Adrian E. Model-based clustering, discriminant analysis, and density estimation. Journal of the American Statistical Association, 97(458):611–631, 2002.
Jiang, Ke, Kulis, Brian, and Jordan, Michael. Small-variance asymptotics for exponential family dirichlet process mixture models. In Advances in Neural Information Processing Systems 25, pp. 3167–3175, 2012.
Jin, Xin, Luo, Jiebo, Yu, Jie, Wang, Gang, Joshi, Dhiraj, and Han, Jiawei. Reinforced similarity integration in image-rich information networks. Knowledge and Data Engineering, IEEE Transactions on, 25(2):448–460, 2013.
Khoreva, Anna, Galasso, Fabio, Hein, Matthias, and Schiele, Bernt. Learning must-link constraints for video segmentation based on spectral clustering. In Pattern Recognition, pp. 701–712. Springer, 2014.
Kulis, Brian and Jordan, Michael I. Revisiting k-means: New algorithms via bayesian nonparametrics. arXiv preprint arXiv:1111.0352, 2011.
Neal, Radford M. Markov chain sampling methods for dirichlet process mixture models. Journal of computational and graphical statistics, 9(2):249–265, 2000.
Ng, Andrew Y, Jordan, Michael I, Weiss, Yair, et al. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849–856, 2002.
Orbanz, Peter and Buhmann, Joachim M. Nonparametric bayesian image segmentation. International Journal of Computer Vision, 77(1-3):25–45, 2008.
Pelleg, Dan and Baras, Dorit. K-means with large and noisy constraint sets. In Machine Learning: ECML 2007, pp. 674–682. Springer, 2007.
Rangapuram, Syama S and Hein, Matthias. Constrained 1-spectral clustering. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 1143–1151, 2012.
Shi, Jianbo and Malik, Jitendra. Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888–905, 2000.
Tierney, Luke and Kadane, Joseph B. Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association, 81(393):82–86, 1986.
20

Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm Wagstaﬀ, Kiri and Cardie, Claire. Clustering with instance-level constraints. In
AAAI/IAAI, pp. 1097, 2000. Wagstaﬀ, Kiri, Cardie, Claire, Rogers, Seth, Schr¨odl, Stefan, et al. Constrained k-means
clustering with background knowledge. In ICML, volume 1, pp. 577–584, 2001. Wang, Xiang and Davidson, Ian. Flexible constrained spectral clustering. In Proceedings
of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 563–572. ACM, 2010. Zha, Hongyuan, He, Xiaofeng, Ding, Chris, Gu, Ming, and Simon, Horst D. Spectral relaxation for k-means clustering. In Advances in neural information processing systems, pp. 1057–1064, 2001. Zhu, Jun, Chen, Ning, and Xing, Eric P. Inﬁnite svm: a dirichlet process mixture of largemargin kernel machines. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 617–624, 2011.
21

