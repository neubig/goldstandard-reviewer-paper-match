©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
CONDITIONAL DIFFUSION PROBABILISTIC MODEL FOR SPEECH ENHANCEMENT
Yen-Ju Lu1,3, Zhong-Qiu Wang1, Shinji Watanabe1, Alexander Richard2, Cheng Yu3, and Yu Tsao3
1Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2Reality Labs Research, Pittsburgh PA, USA
3Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan

arXiv:2202.05256v1 [eess.AS] 10 Feb 2022

ABSTRACT
Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More speciﬁcally, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.
Index Terms— speech enhancement, diffusion probabilistic model, generative model, deep learning
1. INTRODUCTION
Speech enhancement, a key element for immersive audio experiences in telecommunication as well as a crucial front-end processor for robust speech recognition [1, 2], assistive hearing [3], and robust speaker recognition [4, 5], is a challenging and still unsolved problem in audio processing. Riding on the advance of deep learning, considerable progress has been made in the past decade [6, 7]. Deep learning based approaches can be roughly divided into two categories, based on the criteria used to estimate the transformation function from noisy-reverberant speech to clean speech. The ﬁrst category trains discriminative models to minimize the difference between enhanced and clean speech, where the difference can be a point-wise Lp-norm distance [8], or can be computed based on a perceptual metric [9, 10]. The second category considers the distribution of the clean speech signals to form the objective function. Well-known examples along this direction include generative adversarial networks (GANs) [11, 12], Bayesian wavenet [13], variational autoencoders [14], and ﬂow-based models [15]. While the best performing approaches typically fall into the ﬁrst category [16, 6], they usually introduce unpleasant speech distortion and phonetic inaccuracies to the enhanced speech [17, 18, 19]. Generative approaches that aim to match the distribution of speech signals rather than regressive approaches optimizing a point-wise loss hold the promise to produce more natural sounding speech, although they are currently lagging behind regressive approaches and require more research to unfold their potential.

This work investigates diffusion probabilistic models [20], a class of generative models that have shown outstanding performance in image generation [21, 22] and audio synthesis [23, 24, 25], for speech enhancement. Diffusion probabilistic models convert clean input data to an isotropic Gaussian distribution in a step-by-step diffusion process and, in a reverse process, gradually restore the clean input by predicting and removing the noise introduced in each step of the diffusion process. These models, in their vanilla formulation, assume isotropic Gaussian noise in each step of the diffusion process as well as the reverse process. However, in realistic conditions, the noise characteristics are usually non-Gaussian, which violates the model assumption when directly combining the noisy speech signal in the sampling process. We address this problem by formulating a generalized conditional diffusion probabilistic model that incorporates the observed noisy data into the model. We derive the corresponding conditional diffusion and reverse processes as well as the evidence lower bound (ELBO) optimization criterion [21], and show that the resulting model is a generalization of the original diffusion probabilistic model. In our experiment, we will demonstrate that our formulation can not only improve over the vanilla diffusion probabilistic model, but also outperform other generative models.

2. DIFFUSION PROBABILISTIC MODEL
A T -step diffusion model [21] consists of two processes: the diffusion process with steps t ∈ {0, 1, · · · , T } and the reverse process t ∈ {T, T − 1, · · · , 0}. We start with a brief summary of the vanilla diffusion probabilistic model, i.e., we revisit the original diffusionand reverse process.

2.1. Diffusion Process

Given the clean speech data x0, the diffusion process qdata(x0) of
the ﬁrst diffusion step (t = 0) is deﬁned as the data distribution x0 on RL, where L is the signal length in samples. For the the t-th diffusion step, we have a step-dependent variable xt ∈ RL with the
same signal length L. The diffusion process from data x0 to the
variable xT can be formulated based on a ﬁxed Markov chain:

T

q(x1, · · · , xT |x0) = q(xt|xt−1),

(1)

t=1

√

with a Gaussian model q(xt|xt−1) = N (xt; 1 − βtxt−1, βtI),

where βt is a small positive constant. In other words, in each step

a Gaussian noise is added to the previous sample xt−1. According

to the pre-deﬁned schedule β1, · · · , βT , the overall process gradu-

ally converts clean x0 to a latent variable with an isotropic Gaussian

distribution of platent(xT ) = N (0, I).

By substituting the Gaussian model of q(xt|xt−1) into Eq. (1)

and by marginalizing x1, . . . , xt−1, the sampling distribution of xt

Diffusion Process !(#!|#!"#, &)

Clean Speech
$!
#&

Mixed
… … Speech (1 − %!)'" + %!)

…

…

Noisy Speech
"

loss (" − $!) + !
⟺

Combined Noise

Gaussian Noise !

Gaussian Noise !

#! Reverse Process

#% ($(#!"#|#!, &)

Fig. 1. Diffusion process (solid arrows) and reverse process (dashed arrows) of the proposed conditional diffusion probabilistic model.

can be derived as the following distribution conditioned on x0:

√

q(xt|x0) = N (xt; α¯tx0, (1 − α¯t)I),

(2)

where αt = 1 − βt and α¯t =

t s=1

αs.

2.2. Reverse Process

The reverse process converts the latent variable xT ∼ N (0, I) to x0, also based on a Markov chain similar to Eq. (1):

T

pθ(x0, · · · , xT −1|xT ) = pθ(xt−1|xt),

(3)

t=1

where pθ(·) is the distribution of the reverse process with learnable parameters θ. Unlike the diffusion process, the following marginal

likelihood is intractable:

pθ(x0) = pθ(x0, · · · , xT −1|xT ) · platent(xT )dx1:T . (4)

Therefore, we use the ELBO to form an approximated objective function for model training. In [21], it is reported that minimizing the following equation leads to higher generation quality:

T
c + κtEx0,

− θ(√α¯tx0 + √1 − α¯t , t) 22, (5)

t=1

with constants c and κt. Here θ is the model trained to estimate the

Gaussian noise in xt. After optimizing Eq. (5), the corresponding

reverse process equation becomes:

pθ(xt−1|xt) = N (xt−1; µθ(xt, t), β˜tI),

(6)

where the mean µθ(xt, t) is:

1

βt

µθ(xt, t) = √ (xt − √

θ(xt, t)).

(7)

αt

1 − α¯t

The µθ(xt, t) predicts the mean of xt−1 distribution by removing
the estimated Gaussian noise θ(xt, t) in the xt, and the variance is ﬁxed to a constant β˜t = 1−1−α¯αt¯−t 1 βt.

3. CONDITIONAL DIFFUSION PROBABILISTIC MODEL
The original diffusion process in Section 2.1 starts from the clean data qdata(x0) and adds Gaussian noise into the speech signal. In the proposed conditional diffusion probabilistic model, we incorporate the noisy data y into the diffusion process, as shown in Fig. 1.

3.1. Conditional Diffusion Processes
In the conditional diffusion process, we use an interpolation parameter mt to combine the clean data x0 and the noisy data y, the summation of x0 and the real noise n, on the solid arrows in Fig. 1. Instead of starting from the Markov chain Gaussian model q(xt|xt−1) in the

original diffusion process, we ﬁrst deﬁne the following conditional diffusion process q(xt|x0, y): √ √
qcdiff(xt|x0, y) = N (xt; (1 − mt) α¯tx0 + mt α¯ty, δtI), (8)

where δt is the variance. Unlike the original diffusion process q(xt|x0) in Eq. (2), we assume that the Gaussian mean in Eq. (8) is represented as a linear interpolation between the clean data x0 and the noisy data y with the interpolation ratio mt. mt starts from m0 = 0 and is gradually increased to mT ≈ 1, turning the mean of xt from the clean speech x0 to noisy speech y as in Fig. 1.

Given the interpolation formulation in Eq. (8), we can derive

qcdiff(xt|x0) = qcdiff(xt|x0, y)py(y|x0)dy by marginalizing y in

the multiplication of qcdiff(xt|x0, y) and py(y|x0) with the special

case where n ∼ N (0, I). Then, qcdiff(xt|x0) becomes equivalent to

the original diffusion process q(xt|x0) in Eq. (2) when

δt = (1 − α¯t) − m2t α¯t.

(9)

This analytical result indicates that our model is a generalization of

the original diffusion probabilistic model. In our previous study [25],

we investigated directly utilizing noisy signal in the reverse process;

the idea is found to work well empirically, but there lacks a theoret-

ical justiﬁcation. In Sec. 3.2, we will propose a conditional reverse

process that is theoretically sound. To further research the effect of

incorporating noisy signal in the diffusion model, in Sec. 3.3, we will
set δt according to Eq. (9) so that the conditional diffusion process becomes a generalized version of the original diffusion process1.

3.2. Conditional Reverse Processes

In the conditional reverse process, we start from xT , noisy speech

signal y with variance δT , according to Eq. (8) with mT = 1:

√

pcdiff(xT |y) = N (xT , α¯T y, δT I).

(10)

Based on the Markov chain, similar to Eq. (6), the conditional re-

verse process on the dashed arrows in Figure 1 aims to predict xt−1

based on xt and y:

pcdiff(xt−1|xt, y) = N (xt−1; µθ(xt, y, t), δ˜tI),

(11)

where the µθ(xt, y, t) is the estimated mean of the conditional reverse process. The concrete form of the variance δ˜t is introduced later. In contrast to the vanilla diffusion model, we further condition the diffusion model on y. Therefore, similar to Eq. (7), the mean µθ(xt, y, t) in each reverse step is a linear combination of xt, y, and estimated noise with weights cxt, cyt and c t,

µθ(xt, y, t) = cxtxt + cyty − c t θ(xt, y, t),

(12)

where the θ(xt, y, t) is the model to estimate the Gaussian and nonGaussian noise combination. The coefﬁcients cxt, cyt, and c t can be derived from the ELBO optimization criterion, see Section 3.3.

3.3. Coefﬁcient Estimation by Optimizing ELBO
By modifying the derivations in [21], we obtain the ELBO condition for the conditional diffusion process to optimize the likelihood:
ELBO = −Eq DKL(qcdiff(xT |x0, y)||platent(xT |y))
T
+ DKL(qcdiff(xt−1|xt, x0, y)||pθ(xt−1|xt, y)) (13)
t=2
− log pθ(x0|x1, y) .
1It is difﬁcult to derive qcdiff(xt|xt−1) for satisfying the original diffusion process if we ﬁrst deﬁne qcdiff(xt|xt−1, y), because the distribution of y depends on x0 as y = x0 + n.

Algorithm 1 Training

for i = 1, 2, · · · , Niter do

Sample (x0, y)∼qdata, ∼N (0, I), and

t∼Uniform({1, ·√· · , T }) √

√

xt = ((1 − mt) α¯tx0 + mt α¯ty) + δt

Take gradient step√on

√

∇θ

√1

(mt

α¯t(y − x0) +

δt ) −

θ(xt, y, t)

2 2

1−α¯t

according to Eq. (21)

end for

Algorithm 2 Sampling √
Sample xT ∼N (xT , α¯T y, δT I), for t = T, T − 1, · · · , 1 do
Compute cxt, cyt and c t using Eqs. (18), (19), and (20) Sample xt−1 ∼ pcdiff(xt−1|xt, y) = N (xt−1; cxtxt + cyty − c t θ(xt, y, t), δ˜tI } end for
return x0

To optimize Eq. (13), we ﬁrst need the distribution qcdiff(xt|xt−1, y). Generally, the diffusion process deﬁne qcdiff(xt|xt−1, y) ﬁrst and derive qcdiff(xt|x0, y) by marginalizing x0, · · · , xt−1 . Instead, we ﬁrst design the interpolation form in Eq. (8) as mentioned in Sec. 3.1.
Therefore, we compare the coefﬁcients of the marginalized result and Eq. (8) to compute the coefﬁcients of qcdiff(xt|xt−1, y) as:
1 − mt √ qcdiff(xt|xt−1, y) = N xt; 1 − mt−1 αtxt−1
+ mt − 1 − mt mt−1 √α¯ty, δt|t−1I , (14) 1 − mt−1

where the δt|t−1 is also calculated by δt to satisfy Eq. (9) as:

1 − mt 2 δt|t−1 = δt − 1 − mt−1 αtδt−1. (15)

Then, by combining Eqs. (8) and (14), qcdiff(xt−1|xt, x0, y) can

be derived through Bayes’ theorem and the Markov chain property:

1 − mt δt−1 √ qcdiff(xt−1|xt, x0, y) = N xt−1; 1 − mt−1 δt αtxt (16)

+ (1 − mt−1) δt|t−1 √α¯t−1x0

δt √

+ mt−1δt − mt(1 − mt) αtδt−1 α¯t−1 y, δ˜tI ,

1 − mt−1

δt

where δ˜t, the variance term of qcdiff(xt−1|xt, x0, y), is

δ˜t

=

δt|t−1

∗ δt .

(17)

δt−1

To optimize the KL divergence term in Eq. (13), δ˜t is also used as the

variance of pcdiff(xt−1|xt, y) in Eq. (11) to match qcdiff(xt−1|xt, x0, y),

and the coefﬁcients cxt, cyt, c t in Eq. (12) are then be derived as:

1 − mt δt−1 √

δt|t−1 1

cxt = 1 − mt−1 δt

αt + (1 − mt−1) δt

√, α

(18)

√

t

cyt = (mt−1δt − mt(1 − mt) αtδt−1) α¯t−1 , (19)

1 − mt−1

δt

√

δt|t−1 1 − α¯t

c t = (1 − mt−1)

√.

(20)

δt

αt

Now, given the explicit form of all distributions in Eq. (13), the

ELBO to be optimized simpliﬁes to

T
c + κtEx0, ,y
t=1

√

√

mt α¯t

δt

(√

(y − x0)+ √

)− θ(xt, y, t)

2 2

1 − α¯t

1 − α¯t

(21)

with constants c and κt, and the is the Gaussian noise in xt. Be-

cause we have the interpolation form of xt with the coefﬁcient mt

in Eq. (8), the optimization target in Eq. (21) keeps the simple form

in training. Comparing to Eq. (5), the θ(xt, y, t) in the conditional

diffusion model estimates both Gaussian noise and non-Gaussian

noise y − x0 in xt. Therefore, the proportion of y − x0 and coef-

ﬁcients is the same as y and the standard deviation in Eq. (8).

3.4. CDiffuSE Training and Sampling Algorithm
In the conditional reverse process, according to Eq. (11) and (21), θ(xt, y, t) computes the combined noise, which is then deducted from the combination of xt and y to obtain cleaned data xt−1. Finally, iterative application of this process over all T steps yields the clean signal x0. The overall diffusion and reverse process of the conditional diffusion probabilistic models are described in Algorithms 1 and 2. When the interpolation weight mt of the real noise is set to 0, the optimization target in Eq. (21) and the reverse process in (11) becomes (5) and (6) as in the original diffusion probabilistic models.
In our previous study [25], a supportive reverse process was proposed as a less theoretically rigorous implementation to carry out the reverse process from the noisy speech (rather than isotropic Gaussian noise in the original reverse process) without changing the diffusion process. In our proposed CDiffuSE model, we remove the assumption that the real noise in y follows the Gaussian distribution and avoid the mismatch issue between the diffusion and reverse process.
4. EXPERIMENTS
In this section, we evaluate the performance of our approach against other generative speech enhancement models and we show generalization capabilities under conditions where state of the art approaches such as Demucs [16] collapse. The samples of the CDiffuSE-enhanced signals can be found online2.
4.1. Experimental Setup
Dataset: we evaluate the CDiffuSE model on the VoiceBankDEMAND dataset [26]. The dataset consists of 30 speakers from the VoiceBank corpus [27], which is further divided into a training set and a testing set with 28 and 2 speakers, respectively. The training utterances are artiﬁcially contaminated with eight real-recorded noise samples from the DEMAND database [28] and two artiﬁcially generated noise samples (babble and speech shaped) at 0, 5, 10, and 15 dB SNR levels, amounting to 11,572 utterances. The testing utterances are mixed with different noise samples at 2.5, 7.5, 12.5, and 17.5 dB SNR levels, amounting to 824 utterances in total. We consider perceptual evaluation of speech quality (PESQ) [29], prediction of the signal distortion (CSIG), prediction of the background intrusiveness (CBAK), and prediction of the overall speech quality (COVL) [30] as the evaluation metrics. Higher scores mean better performance for all the metrics. Model Architecture and Training: we implement CDiffuSE based on the same model architecture and the same pre-training strategy with clean Mel-ﬁlterbank conditioner as that of DiffuSE reported in [25]. We investigate two systems, namely Base and Large CDiffuSE, which respectively take 50 and 200 diffusion steps. The linearly spaced training noise schedule is reduced to βt ∈ [1 × 10−4, 0.035]
2https://github.com/neillu23/CDiffuSE

for Base CDiffuSE, and to βt ∈ [1 × 10−4, 0.0095] for Large CDiffuSE. The int√erpolation parameter mt in Section 3.1 is set to mt =
(1 − α¯t)/ α¯t which satisﬁes the m0 = 0 and mt ≈ 1 requirement. We train both Base and Large CDiffuSE models for 300,000 iterations, based on an early stopping scheme. The batch size is set to 16 for Base CDiffuSE and to 15 for Large CDiffuSE. The fast sampling scheme [23] is used in the reverse processes with the inference schedule γt = [0.0001, 0.001, 0.01, 0.05, 0.2, 0.35] for both Base CDiffuSE and Large CDiffuSE. The proposed CDiffuSE model performs enhancement in the time domain. After the reverse process is completed, the enhanced waveform further combine the original noisy signal with the ratio 0.2 to recover the high frequency speech in the ﬁnal enhanced waveform, as suggested in [16, 31].
4.2. Evaluation results
4.2.1. Results on VoiceBank-DEMAND
In Table 1, we report the results of CDiffuSE and DiffuSE using the supportive reverse process from [25]. As expected, the large models for DiffuSE and CDiffuSE both outperform the smaller base models. Moreover, CDiffuSE shows improved performance over the

Table 1. Results of DiffuSE and CDiffuSE on VoiceBank.

Method

PESQ CSIG CBAK COVL

Unprocessed

1.97 3.35 2.44 2.63

DiffuSE (Base) [25] CDiffuSE (Base)

2.41 3.61 2.81 2.99 2.44 3.66 2.83 3.03

DiffuSE (Large) [25] CDiffuSE (Large)

2.43 3.63 2.81 3.01 2.52 3.72 2.91 3.10

Table 2. Performance comparison of CDiffuSE and time-domain generative models on VoiceBank.

Method

PESQ CSIG CBAK COVL

Unprocessed SEGAN [11] DSEGAN [32] SE-Flow [15]

1.97 3.35 2.44 2.63 2.16 3.48 2.94 2.80 2.39 3.46 3.11 2.90 2.28 3.70 3.03 2.97

CDiffuSE (Base) CDiffuSE (Large)

2.44 3.66 2.83 3.03 2.52 3.72 2.91 3.10

Table 3. Comparison of CDiffuSE and discriminative models. (a) Trained and tested on VoiceBank (matched condition).

Method

PESQ CSIG CBAK COVL

Unprocessed

1.97 3.35 2.44 2.63

WaveCRN [33] Demucs [16] Conv-TasNet [34]

2.63 3.95 3.06 3.29 2.65 3.99 3.33 3.32 2.84 2.33 2.62 2.51

CDiffuSE (Large)

2.52 3.72 2.91 3.10

(b) Trained on VoiceBank, tested on CHiME-4 (mismatched condition).

Method

PESQ CSIG CBAK COVL

Unprocessed

1.27 2.61 1.93 1.88

WaveCRN [33] Demucs [16] Conv-TasNet [34]

1.43 2.53 2.03 1.91 1.38 2.50 2.08 1.88 1.63 1.70 1.82 1.54

CDiffuSE (Large)

1.66 2.98 2.19 2.27

diffusion probabilistic model baseline DiffuSE. Note that the key to success here is that CDiffuSE has had direct access to the noisy data while learning the reverse diffusion process, allowing it to actively compensate for the noise characteristics in the input signals. Being able to leverage noise from the input signal, our approach improves on all the metrics, conﬁrming that the theoretically sound CDiffuSE leads to improved results in practice. We additionally compare CDiffuSE to other time-domain generative models, namely SEGAN [11], SE-Flow [15], and improved deep SEGAN (DSEGAN) [32]. CDiffuSE outperforms its competitors on all metrics - with the exception of CBAK - and achieves a particularly signiﬁcant improvement in PESQ, see Table 2.
4.2.2. Results on CHiME-4
Generative models typically aim to ﬁt the distribution of the training samples instead of optimizing a point-wise Lp-loss. This property has made them state of the art in applications like text-to-speech and vocoding [35, 36] and also makes them more robust against domain shifts in the input data.
In this section, we investigate this property of our proposed CDiffuSE. We compare the generalization abilities of our approach to other, Lp-loss based approaches and demonstrate that our approach is particularly resistant towards shifts in noise characteristics of the speech data. The models in this section are trained on VoiceBankDEMAND and evaluated on the simulated test data of CHiME4 [37]. The CHiME-4 simulated test data is created based on real-recorded noises from four real-world environments (including street, pedestrian areas, cafeteria and bus) based on four speakers. We use the signals from the ﬁfth microphone for evaluation.
As mentioned previously and as Table 3(a) shows, generative speech enhancement models are still lagging behind the performance of their regressive counterparts. A model from the latter category trained on VoiceBank and evaluated on the VoiceBank test set performs far better than most generative methods. Particularly, Demucs [16] and Conv-TasNet [34] outperform our CDiffuSE, which was the strongest generative model in Table 2.
Given a domain shift in test data, however, regression based approaches such as Demucs, Conv-TasNet, and WaveCRN suffer from a signiﬁcant drop in performance, see Table 3(b). Different signal characteristics between the VoiceBank training data and the CHiME4 test set sufﬁce to let the evaluation scores fall drastically, in some cases even below the scores of unprocessed data. Our proposed CDiffuSE, on the contrary, proves to be much more resilient against such shifts in signal characteristics. While the scores on the CHiME4 test set are lower than the VoiceBank scores, CDiffuSE degrades to a much smaller degree than its regressive competitors, leaving it with the best scores on the CHiME-4 test data and demonstrating its high robustness to variation in noise characteristics.
5. CONCLUSION
We proposed CDiffuSE, a conditional diffusion probabilistic model that can explore noise characteristics from the noisy input signal explicitly and thereby adapts better to non-Gaussian noise statistics in real-world speech enhancement problems. We showed that our model is a strict generalization of the original diffusion probabilistic model and achieves state of the art results compared to other generative speech enhancement approaches. In contrast to non-generative approaches, our method exposes great generalization capabilities to speech data with noise characteristics not observed in the training data. We were able to show that CDiffuSE maintains strong performance when regression-based approaches such as Demucs and Conv-TasNet collapse.

6. REFERENCES
[1] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach, “An overview of noise-robust automatic speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745–777, 2014.
[2] R. Haeb-Umbach, J. Heymann, L. Drude, S. Watanabe, M. Delcroix, and T. Nakatani, “Far-ﬁeld automatic speech recognition,” Proceedings of the IEEE, 2020.
[3] E. W. Healy, J. L. Vasko, and D. Wang, “The optimal threshold for removing noise from speech is similar across normal and impaired hearing—a time-frequency masking study,” The Journal of the Acoustical Society of America, vol. 145, no. 6, pp. EL581–EL586, 2019.
[4] J. H.L. Hansen and T. Hasan, “Speaker recognition by machines and humans: A tutorial review,” IEEE Signal Processing Magazine, vol. 32, no. 6, pp. 74–99, 2015.
[5] D. Michelsanti and Z.-H. Tan, “Conditional generative adversarial networks for speech enhancement and noise-robust speaker veriﬁcation,” arXiv preprint arXiv:1709.01703, 2017.
[6] D. Wang and J. Chen, “Supervised speech separation based on deep learning: An overview,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702–1726, 2018.
[7] X. Lu, Y. Tsao, S. Matsuda, and C. Hori, “Speech enhancement based on deep denoising autoencoder,” in Proc. Interspeech 2013.
[8] S.-W. Fu, T.-W. Wang, Y. Tsao, X. Lu, and H. Kawai, “Endto-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 9, pp. 1570–1584, 2018.
[9] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, “DNN-based source enhancement to increase objective sound quality assessment score,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1780– 1792, 2018.
[10] S.-W. Fu, C.-F. Liao, Y. Tsao, and S.-D. Lin, “MetricGAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement,” in Proc. ICML 2019.
[11] S. Pascual, A. Bonafonte, and J. Serra, “SEGAN: Speech enhancement generative adversarial network,” arXiv preprint arXiv:1703.09452, 2017.
[12] M. H. Soni, N. Shah, and H. A. Patil, “Time-frequency masking-based speech enhancement using generative adversarial network,” in Proc. ICASSP 2018.
[13] K. Qian, Y. Zhang, S. Chang, X. Yang, D. Floreˆncio, and M. Hasegawa-Johnson, “Speech enhancement using bayesian Wavenet,” in Proc. Interspeech 2017.
[14] S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud, “A recurrent variational autoencoder for speech enhancement,” in Proc. ICASSP 2020.
[15] M. Strauss and B. Edler, “A ﬂow-based neural network for time domain speech enhancement,” in Proc. ICASSP 2021.
[16] A. Defossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement in the waveform domain,” arXiv preprint arXiv:2006.12847, 2020.
[17] P. Wang, K. Tan et al., “Bridging the gap between monaural speech enhancement and recognition with distortionindependent acoustic modeling,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 39–48, 2019.
[18] D. Bagchi, P. Plantinga, A. Stiff, and E. Fosler-Lussier, “Spectral feature mapping with mimic loss for robust speech recognition,” in Proc. ICASSP 2018.

[19] T. Gao, J. Du, L.-R. Dai, and C.-H. Lee, “Snr-based progressive learning of deep neural network for speech enhancement.” in Proc. Interspeech 2016.
[20] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep unsupervised learning using nonequilibrium thermodynamics,” in Proc. ICML 2015.
[21] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” arXiv preprint arXiv:2006.11239, 2020.
[22] A. Nichol and P. Dhariwal, “Improved denoising diffusion probabilistic models,” arXiv preprint arXiv:2102.09672, 2021.
[23] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Diffwave: A versatile diffusion model for audio synthesis,” arXiv preprint arXiv:2009.09761, 2020.
[24] S. Liu, Y. Cao, D. Su, and H. Meng, “Diffsvc: A diffusion probabilistic model for singing voice conversion,” arXiv preprint arXiv:2105.13871, 2021.
[25] Y.-J. Lu, Y. Tsao, and S. Watanabe, “A study on speech enhancement based on diffusion probabilistic model,” arXiv preprint arXiv:2107.11876, 2021.
[26] C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi, “Investigating RNN-based speech enhancement methods for noise-robust text-to-speech.” in SSW, 2016, pp. 146–152.
[27] C. Veaux, J. Yamagishi, and S. King, “The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,” in Proc. CASLRE 2013.
[28] J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,” in Proceedings of Meetings on Acoustics, vol. 19, no. 1, 2013, p. 035081.
[29] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,” vol. 2, 2001, pp. 749–752.
[30] Y. Hu and P. C. Loizou, “Evaluation of objective quality measures for speech enhancement,” IEEE Transactions on audio, speech, and language processing, vol. 16, no. 1, pp. 229–238, 2007.
[31] M. Abd El-Fattah, M. I. Dessouky, S. Diab, and F. Abd ElSamie, “Speech enhancement using an adaptive wiener ﬁltering approach,” Progress In Electromagnetics Research M, vol. 4, pp. 167–184, 2008.
[32] H. Phan, I. V. McLoughlin, L. Pham, O. Y. Che´n, P. Koch, M. De Vos, and A. Mertins, “Improving gans for speech enhancement,” IEEE Signal Processing Letters, vol. 27, pp. 1700–1704, 2020.
[33] T.-A. Hsieh, H.-M. Wang, X. Lu, and Y. Tsao, “Wavecrn: An efﬁcient convolutional recurrent neural network for end-to-end speech enhancement,” IEEE Signal Processing Letters, vol. 27, pp. 2149–2153, 2020.
[34] Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 1256–1266, 2019.
[35] A. V. D. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “Wavenet: A generative model for raw audio,” arXiv preprint arXiv:1609.03499, 2016.
[36] J. Kong, J. Kim, and J. Bae, “Hiﬁ-gan: Generative adversarial networks for efﬁcient and high ﬁdelity speech synthesis,” arXiv preprint arXiv:2010.05646, 2020.
[37] E. Vincent, S. Watanabe, A. A. Nugraha, J. Barker, and R. Marxer, “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” Computer Speech & Language, vol. 46, pp. 535–557, 2017.

