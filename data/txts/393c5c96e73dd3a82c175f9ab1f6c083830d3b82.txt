Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals

arXiv:1711.04837v2 [stat.ML] 26 Apr 2018

John Alberg Euclidean Technologies john.alberg@euclidean.com

Zachary C. Lipton Amazon AI
Carnegie Mellon University zlipton@cmu.edu

Abstract
On a periodic basis, publicly traded companies are required to report fundamentals: ﬁnancial data such as revenue, operating income, debt, among others. These data points provide some insight into the ﬁnancial health of a company. Academic research has identiﬁed some factors, i.e. computed features of the reported data, that are known through retrospective analysis to outperform the market average. Two popular factors are the book value normalized by market capitalization (bookto-market) and the operating income normalized by the enterprise value (EBIT/EV). In this paper, we ﬁrst show through simulation that if we could (clairvoyantly) select stocks using factors calculated on future fundamentals (via oracle), then our portfolios would far outperform a standard factor approach. Motivated by this analysis, we train deep neural networks to forecast future fundamentals based on a trailing 5-years window. Quantitative analysis demonstrates a signiﬁcant improvement in MSE over a naive strategy. Moreover, in retrospective analysis using an industry-grade stock portfolio simulator (backtester), we show an improvement in compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor model.

1 Introduction
Public stock markets provide a venue for buying and selling shares, which represent fractional ownership of individual companies. Prices ﬂuctuate frequently, but the myriad drivers of price movements occur on multiple time scales. In the short run, price movements might reﬂect the dynamics of order execution, and the behavior of high frequency traders. On the scale of days, price ﬂuctuation might be driven by the news cycle. Individual stocks may rise or fall on rumors or reports of sales numbers, product launches, etc. In the long run,we expect a company’s market value to reﬂect its ﬁnancial performance, as captured in fundamental data, i.e., reported ﬁnancial information such as income, revenue, assets, dividends, and debt. In other words, shares reﬂect ownership in a company thus share prices should ultimately move towards the company’s intrinsic value, the cumulative discounted cash ﬂows associated with that ownership. One popular strategy called value investing is predicated on the idea that long-run prices reﬂect this intrinsic value and that the best features for predicting long-term intrinsic value are the currently available fundamental data.
In a typical quantitative (systematic) investing strategy, we sort the set of available stocks according to some factor and construct investment portfolios comprised of those stocks which score highest. Many quantitative investors engineer value factors by taking fundamental data in a ratio to stock’s price, such as EBIT/EV or book-to-market. Stocks with high value factor ratios are called value stocks and those with low ratios are called growth stocks. Academic researchers have demonstrated empirically that portfolios of stocks which overweight value stocks have signiﬁcantly outperformed portfolios that overweight growth stocks over the long run [12, 7].
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

In this paper, we propose an investment strategy that constructs portfolios of stocks today based on predicted future fundamentals. Recall that value factors should identify companies that are inexpensively priced with respect to current company fundamentals such as earnings or book-value. We suggest that the long-term success of an investment should depend on the how well-priced the stock currently is with respect to its future fundamentals. We run simulations with a clairvoyant model that can access future ﬁnancial reports (by oracle). In Figure 1, we demonstrate that for the 2000-2016 time period, a clairvoyant model applying the EBIT/EV factor with 12-month clairvoyant fundamentals, if possible, would achieve a 44% compound annualized return.

Simulated Annualized Return

80%

Book / Market
70%
EBIT / EV
60%
Net-Income / EV

50%

Sales / EV

40%

30%

20%

10%

0% 0

3

6

9

12

24

36

Months of Clairvoyance

Figure 1: Annualized return for various factor models for different degrees of clairvoyance.

Motivated by the performance of factors applied to clairvoyant future data, we propose to predict future fundamental data based on trailing time series of 5 years of fundamental data. We denote these algorithms as Lookahead Factor Models (LFMs). Both multilayer perceptrons (MLPs) and recurrent neural networks (RNNs) can make informative predictions, achieving out-of-sample MSE of .47, vs .53 for linear regression and .62 for a naive predictor. Simulations demonstrate that investing with LFMs based on the predicted factors yields a compound annualized return (CAR) of 17.1%, vs 14.4% for a normal factor model and a Sharpe ratio .68 vs .55.

Related Work Deep neural networks models have proven powerful for tasks as diverse as language translations [14, 1], video captioning [11, 16], video recognition [6, 15], and time series modeling [9, 10, 3]. A number of recent papers consider deep learning approaches to predicting stock market performance. [2] evaluates MLPs for stock market prediction. [5] uses recursive tensor nets to extract events from CNN news reports and uses convolutional neural nets to predict future performance from a sequence of extracted events. Several preprinted drafts consider deep learning for stock market prediction [4, 17, 8] however, in all cases, the empirical studies are limited to few stocks and short time periods.

2 Deep Learning for Forecasting Fundamentals
Data In this research, we consider all stocks that were publicly traded on the NYSE, NASDAQ or AMEX exchanges for at least 12 consecutive months between between January, 1970 and September, 2017. From this list, we exclude non-US-based companies, ﬁnancial sector companies, and any company with an inﬂation-adjusted market capitalization value below 100 million dollars. The ﬁnal list contains 11, 815 stocks. Our features consist of reported ﬁnancial information as archived by the Compustat North America and Compustat Snapshot databases. Because reported information arrive intermittently throughout a ﬁnancial period, we discretize the raw data to a monthly time step. Because we are interested in long-term predictions and to smooth out seasonality in the data, at every month, we feed in inputs with a 1-year lag between time frames and predict the fundamentals 12 months into the future.
For each stock and at each time step t, we consider a total of 20 input features. We engineer 16 features from the fundamentals as inputs to our models. Income statement features are cumulative trailing twelve months, denoted TTM, and balance sheet features are most recent quarter, denoted MRQ. First we consider These items include revenue (TTM); cost of goods sold (TTM); selling, general & and admin expense (TTM); earnings before interest and taxes or EBIT (TTM); net income (TTM); cash and cash equivalents (MRQ); receivables (MRQ); inventories (MRQ); other current assets (MRQ); property plant and equipment (MRQ); other assets (MRQ); debt in current liabilities (MRQ); accounts payable (MRQ); taxes payable (MRQ); other current liabilities (MRQ); total liabilities (MRQ). For all features, we deal with missing values by ﬁlling forward previously observed values, following the methods of [9]. Additionally we incorporate 4 momentum features, which

2

indicate the price movement of the stock over the previous 1, 3, 6, and 9 months respectively. So that our model picks up on relative changes and doesn’t focus overly on trends in speciﬁc time periods, we use the percentile among all stocks as a feature (vs absolute numbers).
Preprocessing Each of the fundamental features exhibits a wide dynamic range over the universe of considered stocks. For example, Apple’s 52-week revenue as of September 2016 was $215 billion (USD). By contrast, National Presto, which manufactures pressure cookers, had a revenue $340 million. Intuitively, these statistics are more meaningful when scaled by some measure of a company’s size. In preprocessing, we scale all fundamental features in given time series by the market capitalization in the last input time-step of the series. We scale all time steps by the same value so that the neural network can assess the relative change in fundamental values between time steps. While other notions of size are used, such as enterprise value and book equity, we choose to avoid these measure because they can, although rarely, take negative values. We then further scale the features so that they each individually have zero mean and unit standard deviation.
Modeling In our experiments, we divide the timeline in to an in-sample and out-of-sample period. Then, even within the in-sample period, we need to partition some of the data as a validation set. In forecasting problems, we face distinct challenges in guarding against overﬁtting. First, we’re concerned with the traditional form of overﬁtting. Within the in-sample period, we do not want to over-ﬁt to the ﬁnite observed training sample. To protect against and quantify this form of overﬁtting, we randomly hold out a validation set consisting of 30% of all stocks. On this in-sample validation set, we determine all hyperparameters, such as learning rate, model architecture, objective function weighting. We also use the in-sample validation set to determine early stopping criteria. When training, we record the validation set accuracy after each training epoch, saving the model for each best score achieved. When 25 epochs have passed without improving on the best validation set performance, we halt training and selecting the model with the best validation performance. In addition to generalizing well to the in-sample holdout set, we evaluate whether the model can predict the future out-of-sample stock performance. Since this research is focused on long-term investing, we chose large in-sample and out-of-sample periods of the years 1970-1999 and 2000-2016, respectively.
In previous experiments, we tried predicting relative returns directly with RNNs and while the RNN outperformed other approaches on the in-sample period, it failed to meaningfully out-perform a linear model (See results in Table 2a). Given only returns data as targets, RNN’s easily overﬁt the training data while failing to improve performance on in-sample validation. One key beneﬁt of our approach is that by doing multi-task learning, predicting all 16 future fundamentals, we provide the model with considerable training signal and may thus be less susceptible to overﬁtting.
The price movement of stocks is extremely noisy [13] and so, suspecting that the relationships among fundamental data may have a larger signal to noise ratio than the relationship between fundamentals and price, we set up the problem thusly: For MLPs, at each month t, given features for 5 months spaced 1 year apart (t − 48, t − 36, t − 24, t − 12), predict the fundamental data at time t + 12. For RNNs, the setup is identical but with the small modiﬁcation that for each input in the sequence, we predict the corresponding 12 month lookahead data.
We evaluated two classes of deep neural networks: MLPs and RNNs. For each of these, we tune hyperparameters on the in-sample period. We then evaluated the resulting model on the out-of-sample period. For both MLPs and RNNs, we consider architectures evaluated with 1, 2, and 4 layers with 64, 128, 256, 512 or 1024 nodes. We also evaluate the use of dropout both on the inputs and between hidden layers. For MLPs we use ReLU activations and apply batch normalization between layers. For RNNs we test both GRU and LSTM cells with layer normalization. We also searched over various optimizers (SGD, AdaGrad, AdaDelta), settling on AdaDelta. We also applied L2-norm clipping on RNNs to prevent exploding gradients. Our optimization objective is to minimize square loss.
To account for the fact that we care more about our prediction of EBIT over the other fundamental values, we up-weight it in the loss (introducing a hyperparameter α1). For RNNs, because we care primarily about the accuracy of the prediction at the ﬁnal time step (of 5), we upweight the loss at the ﬁnal time step by hyperparameter α2 (as in [9]). Some results from our hyperparameter search on in-sample data are displayed in Table 1. These hyperparameters resulted in MSE on in-sample validation data of 0.6141 for and 0.6109 for the MLP and RNN, respectively.
.
3

Strategy
S&P 500 Market Avg. Price-LSTM QFM LFM-Linear LFM-MLP LFM-LSTM

MSE
n/a n/a n/a 0.62 0.53 0.47 0.47

CAR
4.5% 7.7% 11.3% 14.4% 15.9% 17.1% 16.7%

Sharpe Ratio
0.19 0.29 0.60 0.55 0.63 0.68 0.67

(a) Out-of-sample performance for the 2000-2016 time period. All factor models use EBIT/EV. QFM uses current EBIT while our proposed LFMs use predicted EBIT. Price-LSTM is trained to predict relative return.

Mean Squared Error

2

MLP Mean

1.8

Naive Mean

1.6

MLP Predictor

1.4 Naive Predictor

1.2

1

0.8

0.6

0.4

0.2

0 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
Date

(b) MSE over out-of-sample period for MLP (orange) and naive predictor (black).

Figure 2: Quantitative results

Evaluation As a ﬁrst step in evaluating the forecast produced by the neural networks, we compare the MSE of the predicted fundamental on out-of-sample data with a naive prediction where predicted fundamentals at time t is assumed to be the same as the fundamentals at t − 12. To compare the practical utility of traditional factor models vs lookahead factor models we employ an industry grade investment simulator. The simulator evaluates hypothetical stock portfolios constructed on out-of-sample data. Simulated investment returns reﬂect how an investor might have performed had they invested in the past according to given strategy.

Hyperparameter
Hidden Units Hidden Layers Input Dropout Keep Prob. Hidden Dropout Keep Prob. Recurrent Dropout Keep Prob. Max Gradient Norm α1 α2

MLP
1024 2 1.0 0.5 n/a 1.0
0.75 n/a

RNN
64 2 1.0 1.0 0.7 1.0 0.5 0.7

Table 1: Final hyperparameters for MLP and RNN

The simulation results reﬂect assets-under-management at the start of each month that, when adjusted by the S&P 500 Index Price to January 2010, are equal to $100 million. We construct portfolios by ranking all stocks according to the factor EBIT/EV in each month and investing equal amounts of capital into the top 50 stocks holding each stock for one-year. When a stock falls out of the top 50 after one year, it is sold with proceeds reinvested in another highly ranked stock that is not currently in the simulated portfolio. We limit the number of shares of a security bought or sold in a month to no more than 10% of the monthly volume for a security. Simulated prices for stock purchases and sales are based on the volume-weighted daily closing price of the security during the ﬁrst 10 trading days of each month. If a stock paid a dividend during the period it was held, the dividend was credited to the simulated fund in proportion to the shares held. Transaction costs are factored in as $0.01 per share, plus an additional slippage factor that increases as a square of the simulation’s volume participation in a security. Speciﬁcally, if participating at the maximum 10% of monthly volume, the simulation buys at 1% more than the average market price and sells at 1% less than the average market price. Slippage accounts for transaction friction, such as bid/ask spreads, that exists in real life trading.

Our results demonstrate a clear advantage for the lookahead factor model. In nearly all months, however turbulent the market, neural networks outperform the naive predictor (that fundamentals remains unchanged) (Figure 2b). Simulated portfolios lookahead factor strategies with MLP and RNN perform similarly, both beating traditional factor models (Table 2a).

3 Discussion

In this paper we demonstrate a new approach for automated stock market prediction based on time series analysis. Rather than predicting price directly, predict future fundamental data from a trailing window of values. Retrospective analysis with an oracle motivates the approach, demonstrating the superiority of LFM over standard factor approaches. In future work we will thoroughly investigate the relative advantages of LFMs vs directly predicting price. We also plan to investigate the effects of the sampling window, input length, and lookahead distance.

4

References
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473, 2014.
[2] Bilberto Batres-Estrada. Deep learning for multivariate ﬁnancial time series. 2015. [3] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent
neural networks for multivariate time series with missing values. arXiv:1606.01865, 2016. [4] Kai Chen, Yi Zhou, and Fangyan Dai. A lstm-based method for stock returns prediction: A
case study of china stock market. In Big Data (Big Data), 2015 IEEE International Conference on. IEEE, 2015. [5] Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. Deep learning for event-driven stock prediction. [6] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015. [7] Eugene F. Fama and Kenneth R. French. The cross-section of expected stock returns. Journal of Finance 47, 427-465., 1992. [8] Hengjian Jia. Investigation into the effectiveness of long short term memory networks for stock price prediction. arXiv:1603.07893, 2016. [9] Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. Learning to diagnose with lstm recurrent neural networks. ICLR, 2016. [10] Zachary C Lipton, David C Kale, and Randall Wetzel. Directly modeling missing data in sequences with rnns: Improved classiﬁcation of clinical time series. Machine Learning for Healthcare (MLHC), 2016. [11] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR, 2015. [12] Eero Pätäri and Timo Leivo. A closer look at the value premium. Journal of Economic Surveys, Vol. 31, Issue 1, pp. 79-168, 2017, 2017. [13] Robert J Shiller. Do stock prices move too much to be justiﬁed by subsequent changes in dividends?, 1980. [14] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, 2014. [15] Subarna Tripathi, Zachary C Lipton, Serge Belongie, and Truong Nguyen. Context matters: Reﬁning object detection in video with recurrent neural networks. BMVC, 2016. [16] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015. [17] Barack Wamkaya Wanjawa and Lawrence Muchemi. Ann model to predict stock prices at stock exchange markets. arXiv:1502.06434, 2014.
5

