TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation
Pengcheng Yin, Graham Neubig Language Technologies Institute
Carnegie Mellon University {pcyin,gneubig}@cs.cmu.edu

arXiv:1810.02720v1 [cs.CL] 5 Oct 2018

Abstract
We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). TRANX uses a transition system based on the abstract syntax description language for the target MR, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target MR to constrain the output space and model the information ﬂow, and (2) it is highly generalizable, and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.1
1 Introduction
Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs). The target MRs can be deﬁned according to a wide variety of formalisms. This include linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as λcalculus (Zettlemoyer and Collins, 2005) or the abstract meaning representations (Banarescu et al., 2013). Alternatively, for more task-driven approaches to semantic parsing, it is common for meaning representations to represent executable programs such as SQL queries (Zhong et al., 2017), robotic commands (Artzi and Zettlemoyer, 2013), smart phone instructions (Quirk et al., 2015), and even general-purpose programming languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java (Ling et al., 2016).
1Available at https://github.com/pcyin/tranX. An earilier version is used in Yin et al. (2018).

Because of these varying formalisms for MRs, the design of semantic parsers, particularly neural network-based ones has generally focused on a small subset of tasks — in order to ensure the syntactic well-formedness of generated MRs, a parser is usually speciﬁcally designed to reﬂect the domain-dependent grammar of MRs in the structure of the model (Zhong et al., 2017; Xu et al., 2017). To alleviate this issue, there have been recent efforts in neural semantic parsing with general-purpose grammar models (Xiao et al., 2016; Dong and Lapata, 2018). Yin and Neubig (2017) put forward a neural sequence-to-sequence model that generates tree-structured MRs using a series of tree-construction actions, guided by the task-speciﬁc context free grammar provided to the model a priori. Rabinovich et al. (2017) propose the abstract syntax networks (ASNs), where domain-speciﬁc MRs are represented by abstract syntax trees (ASTs, Fig. 2 Left) speciﬁed under the abstract syntax description language (ASDL) framework (Wang et al., 1997). An ASN employs a modular architecture, generating an AST using speciﬁcally designed neural networks for each construct in the ASDL grammar.
Inspired by this existing research, we have developed TRANX, a TRANsition-based abstract syntaX parser for semantic parsing and code generation. TRANX is designed with the following principles in mind:
• Generalization ability TRANX employs ASTs as a general-purpose intermediate meaning representation, and the task-dependent grammar is provided to the system as external knowledge to guide the parsing process, therefore decoupling the semantic parsing procedure with speciﬁcities of grammars.
• Extensibility TRANX uses a simple transition system to parse NL utterances into tree-

ASDL Grammar stmt ↦ Expr(expr value) expr ↦ Call(expr func, expr* args,
keyword* keywords) | Attribute(expr value,
identifier attr)
| Name(identifier id) | Str(string s)
Input Utterance
pandas read top 100 lines in file.csv

Transition System
(actions to construct AST)
s1 ApplyConstr(Expr)

s2 ApplyConstr(Call)

s3 ApplyConstr(Attr.)

s4 GenToken(sorted)

s5

. . .

Abstract Syntax Tree (general-purpose intermediate MR)

Expr Call

Attr.

Name Keyword

. . .

pandas

file.csv

read_csv

Figure 1: Workﬂow of TRANX

Meaning Representation (to domain-specific MR)

AST_to_MR(

)

pandas.read_csv(
file.csv, nrows=100)

structured ASTs. The transition system is designed to be easy to extend, requiring minimal engineering to adapt to tasks that need to handle extra domain-speciﬁc information.
• Effectiveness We test TRANX on four semantic parsing (ATIS, GEO) and code generation (DJANGO, WIKISQL) tasks, and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance, out-performing existing neural networkbased approaches on three of the four datasets (GEO, ATIS, DJANGO).
2 Methodology
Given an NL utterance, TRANX parses the utterance into a formal meaning representation, typically represented as λ-calculus logical forms, domain-speciﬁc, or general-purpose programming languages (e.g., Python). In the following description we use Python code generation as a running example, where a programmer’s natural language intents are mapped to Python source code. Fig. 1 depicts the workﬂow of TRANX. We will present more use cases of TRANX in § 3.
The core of TRANX is a transition system. Given an input NL utterance x, TRANX employs the transition system to map the utterance x into an AST z using a series of treeconstruction actions (§ 2.2). TRANX employs ASTs as the intermediate meaning representation to abstract over domain-speciﬁc structure of MRs. This parsing process is guided by the userdeﬁned, domain-speciﬁc grammar speciﬁed under the ASDL formalism (§ 2.1). Given the generated AST z, the parser calls the user-deﬁned function, AST to MR(·), to convert the intermediate AST into a domain-speciﬁc meaning representation y, completing the parsing process. TRANX uses a probabilistic model p(z|x), parameterized by a neural network, to score each hypothesis AST (§ 2.3).

2.1 Modeling ASTs using ASDL Grammar
TRANX uses ASTs as the general-purpose, intermediate semantic representation for MRs. ASTs are commonly used to represent programming languages, and can also be used to represent other tree-structured MRs (e.g., λ-calculus). The ASDL framework is a grammatical formalism to deﬁne ASTs. See Fig. 1 for an excerpt of the Python ASDL grammar. TRANX provides APIs to read such a grammar from human-readable text ﬁles.
An ASDL grammar has two basic constructs: types and constructors. A composite type is deﬁned by the set of constructors under that type. For example, the stmt and expr composite types in Fig. 1 refer to Python statements and expressions, repectively, each deﬁned by a series of constructors. A constructor speciﬁes a language construct of a particular type using its ﬁelds. For instance, the Call constructor under the composite type expr denotes function call expressions, and has three ﬁelds: func, args and keywords. Each ﬁeld in a constructor is also strongly typed, which speciﬁes the type of value the ﬁeld can hold. A ﬁeld with a composite type can be instantiated by constructors of the same type. For example, the func ﬁeld above can hold a constructor of type expr. There are also ﬁelds with primitive types, which store values. For example, the id ﬁeld of Name constructor has a primitive type identifier, and is used to store identiﬁer names. And the ﬁeld s in the Str (string) constructor hold string literals. Finally, each ﬁeld has a cardinality (single, optional ? and sequential ∗), denoting the number of values the ﬁeld holds.
An AST is then composed of multiple constructors, where each node on the tree corresponds to a typed ﬁeld in a constructor (except for the root node, which denotes the root constructor). Depending on the cardinality of the ﬁeld, a node can hold one or multiple constructors as its values. For instance, the func ﬁeld with single car-

f2 func t3 Attribute

t1 Expr f1 value
t2 Call
f3 args

f4 keywords

f5 value

f6 attr

t4 Name t6 read_csv
id f7 t8
t5 pandas

t7 Str f8 s
file.csv

t9 keyword

f9 args

f10 value

t10 nrows

t11 t12

Num f11 n
100

t nft Action t1 root Expr(expr value) t2 f1 Call(expr func, expr* args,
keyword* keywords)
t3 f2 Attribute(expr value, identifier attr) t4 f5 Name(identifier id) t5 f7 GENTOKEN[pandas] t6 f6 GENTOKEN[read csv] t7 f3 Str(string s) t8 f8 GENTOKEN[ﬁle.csv] t9 f8 GENTOKEN[</f>] t10 f3 REDUCE (close the frontier ﬁeld f3) t11 f4 keyword(identifier arg, expr value) t12 f9 GENTOKEN[nrows] t13 f10 Num(object n) t14 f11 GENTOKEN[1000] t15 f4 REDUCE (close the frontier ﬁeld f4)

Figure 2: Left The ASDL AST for the target Python code in Fig. 1. Field names are labeled on upper arcs, and indexed as fi. Purple squares denote ﬁelds with sequential cardinality. Grey nodes denote primitive identiﬁer ﬁelds. Fields are labeled with time steps at which they are generated. Right The action sequence used to construct the AST. Each action is labeled with its frontier ﬁeld nft . APPLYCONSTR actions are represented by their constructors.

dinality in the ASDL grammar in Fig. 1 is instantiated with one Name constructor, while the args ﬁeld with sequential cardinality have multiple child constructors.
2.2 Transition System
Inspired by Yin and Neubig (2017) (hereafter YN17), we develop a transition system that decomposes the generation procedure of an AST into a sequence of tree-constructing actions. We now explain the transition system using our running example. Fig. 2 Right lists the sequence of actions used to construct the example AST. In high level, the generation process starts from an initial derivation AST with a single root node, and proceeds according to a top-down, left-to-right order traversal of the AST. At each time step, one of the following three types of actions is evoked to expand the opening frontier ﬁeld nft of the derivation:
APPLYCONSTR[c] actions apply a constructor c to the opening composite frontier ﬁeld which has the same type as c, populating the opening node using the ﬁelds in c. If the frontier ﬁeld has sequential cardinality, the action appends the constructor to the list of constructors held by the ﬁeld.
REDUCE actions mark the completion of the generation of child values for a ﬁeld with optional (?) or multiple (∗) cardinalities.
GENTOKEN[v] actions populate a (empty) primitive frontier ﬁeld with a token v. For example, the ﬁeld f7 on Fig. 2 has type identifier, and is instantiated using a single GENTOKEN action. For ﬁelds of string type, like f8, whose value could consists of multiple tokens (only one shown here), it can be ﬁlled using a sequence of GENTOKEN actions, with a special

GENTOKEN[</f>] action to terminate the generation of token values.
The generation completes once there is no frontier ﬁeld on the derivation. TRANX then calls the user speciﬁed function AST to MR(·) to convert the generated intermediate AST z into the target domain-speciﬁc MR y. TRANX provides various helper functions to ease the process of writing conversion functions. For example, our example conversion function to transform ASTs into Python source code contains only 32 lines of code. TRANX also ships with several built-in conversion functions to handle MRs commonly used in semantic parsing and code generation, like λcalculus logical forms and SQL queries.
2.3 Computing Action Probabilities p(z|x) Given the transition system, the probability of an z is decomposed into the probabilities of the sequence of actions used to generate z
p(z|x) = p(at|a<t, x),
t
Following YN17, we parameterize the transitionbased parser p(z|x) using a neural encoderdecoder network with augmented recurrent connections to reﬂect the topology of ASTs.
Encoder The encoder is a standard bidirectional Long Short-term Memory (LSTM) network, which encodes the input utterance x of n tokens, {xi}ni=1 into vectorial representations {h}ni=1. Decoder The decoder is also an LSTM network, with its hidden state st at each time temp given by
st = fLSTM([at−1 : ˜st−1 : pt], st−1),
where fLSTM is the LSTM transition function, and [:] denotes vector concatenation. at−1 is the em-

expr = Variable(var variable) | Entity(ent entity) | Number(num number) | Apply(pred predicate, expr∗ arguments) | Argmax(var variable, expr domain, expr body) | Argmin(var variable, expr domain, expr body) | Count(var variable, expr body) | Exists(var variable, expr body) | Lambda(var variable, var type type, expr body) | Max(var variable, expr body) | Min(var variable, expr body) | Sum(var variable, expr domain, expr body) | The(var variable, expr body) | Not(expr argument) | And(expr∗ arguments) | Or(expr∗ arguments) | Compare(cmp op op, expr left, expr right)
cmp op = Equal | LessThan | GreaterThan

stmt = Select(agg op? agg, idx column idx, cond expr∗ conditions)
cond expr = Condition(cmp op op, idx column idx, string value)
agg op = Max | Min | Count | Sum | Avg cmp op = Equal | GreaterThan | LessThan | Other
Figure 4: The ASDL grammar for WIKISQL
The binary probability p(gen|·) and p(copy|·) is given by softmax(W˜st). The probability of generating v from a closed-set vocabulary, p(v|gen, ·) is deﬁned similarly as Eq. (1). The copy probability of copying the i-th word in x is deﬁned using a pointer network (Vinyals et al., 2015)
p(xi|copy, a<t, x) = softmax(hi W˜st).

Figure 3: The λ-calculus ASDL grammar for GEO and ATIS, deﬁned in Rabinovich et al. (2017)
bedding of the previous action. We maintain an embedding vector for each action. ˜st is the attentional vector deﬁned as in Luong et al. (2015)
˜st = tanh(Wc[ct : st]).
where ct is the context vector retrieved from input encodings {hi}ni=1 using attention.
Parent Feeding pt is a vector that encodes the information of the parent frontier ﬁeld nft on the derivation, which is a concatenation of two vectors: the embedding of the frontier ﬁeld nft, and spt, the decoder’s state at which the constructor of nft is generated by the APPLYCONSTR action. Parent feeding reﬂects the topology of treestructured ASTs, and gives better performance on generating complex MRs like Python code (§ 3).
Action Probabilities The probability of an APPLYCONSTR[c] action with embedding ac is2
p(at = APPLYCONSTR[c]|a<t, x) = softmax(ac W˜st) (1)
For GENTOKEN actions, we employ a hybrid approach of generation and copying, allowing for out-of-vocabulary variable names and literals (e.g., “ﬁle.csv” in Fig. 1) in x to be directly copied to the derivation. Speciﬁcally, the action probability is deﬁned to be the marginal probability
p(at = GENTOKEN[v]|a<t, x) = p(gen|at, x)p(v|gen, at, x)+ p(copy|at, x)p(v|copy, at, x)
2REDUCE is treated as a special APPLYCONSTR action.

3 Experiments
3.1 Datasets
To demonstrate the generalization and extensibility of TRANX, we deploy our parser on four semantic parsing and code generation tasks.
3.1.1 Semantic Parsing We evaluate on GEO and ATIS datasets. GEO is a collection of 880 U.S. geographical questions (e.g., “Which states border Texas?”), and ATIS is a set of 5,410 inquiries of ﬂight information (e.g., “Show me ﬂights from Dallas to Baltimore”). The MRs in the two datasets are deﬁned in λ-calculus logical forms (e.g., “lambda x (and (state x) (next to x texas))” and “lambda x (and (flight x dallas) (to x baltimore))”). We use the pre-processed datasets released by Dong and Lapata (2016). We use the ASDL grammar deﬁned in Rabinovich et al. (2017), as listed in Fig. 3.
3.1.2 Code Generation We evaluate TRANX on both general-purpose (Python, DJANGO) and domain-speciﬁc (SQL, WIKISQL) code generation tasks. The DJANGO dataset (Oda et al., 2015) consists of 18,805 lines of Python source code extracted from the Django Web framework, with each line paired with an NL description. Code in this dataset covers various real-world use cases of Python, like string manipulation, I/O operation, exception handling, etc.
WIKISQL (Zhong et al., 2017) is a code generation task for domain-speciﬁc languages (i.e., SQL). It consists of 80,654 examples of NL questions (e.g., “What position did Calvin Mccarty play?”) and annotated SQL queries (e.g., “SELECT Position FROM Table WHERE

Methods ZH15 (Zhao and Huang, 2015) ZC07 (Zettlemoyer and Collins, 2007) WKZ14 (Wang et al., 2014) Neural Network-based Models SEQ2TREE (Dong and Lapata, 2016) ASN (Rabinovich et al., 2017)
+ supervised attention TRANX (w/o parent feeding) TRANX (w/ parent feeding)

GEO 88.9 89.0 90.4
87.1 85.7 87.1 88.2 87.7

ATIS 84.2 84.6 91.3
84.6 85.3 85.9 86.2 86.2

Table 1: Semantic parsing accuracies on GEO and ATIS

Methods Phrasal Statistical MT (Ling et al., 2016) SEQ2TREE (Dong and Lapata, 2016) NMT (Neubig, 2015) LPN (Ling et al., 2016) YN17 (Yin and Neubig, 2017) TRANX (w/o parent feeding) TRANX (w parent feeding)

ACC. 31.5 39.4 45.1 62.3 71.6 72.7 73.7

Table 2: Code generation accuracies on DJANGO

Player = Calvin Mccarty”). Different from other datasets, each example also has a table extracted from Wikipedia, and the SQL query is executed against the table to get an answer.
Extending TRANX for WIKISQL In order to achieve strong results, existing parsers, like most models in Tab. 3, use speciﬁcally designed architectures to reﬂect the syntactic structure of SQL queries. We show that the transition system used by TRANX can be easily extended for WIKISQL with minimal engineering, while registering strong performance. First, we use deﬁne a simple ASDL grammar following the syntax of SQL (Fig. 4). We then augment the transition system with a special GENTOKEN action, SELCOLUMN[k]. A SELCOLUMN[k] action is used to populate a primitive column idx ﬁeld in Select and Condition constructors in the grammar by selecting the k-th column in the table. To compute the probability of SELCOLUMN[k] actions, we use a pointer network over column encodings, where the column encodings are given by a bidirectional LSTM network over column names in an input table. This can be simply implemented by overriding the base Parser class in TRANX and modifying the functions that compute action probabilities.
3.2 Results In this section we discuss our experimental results. All results are averaged over three runs with different random seeds.
Semantic Parsing Tab. 1 lists the results for semantic parsing tasks. We test TRANX with

Methods
Seq2Seq (Zhong et al., 2017) SEQ2TREE (Dong and Lapata, 2016) Seq2SQL (Zhong et al., 2017) SQLNet (Xu et al., 2017) PT-MAML (Huang et al., 2018) TypeSQL (Yu et al., 2018)
TRANX w/ parent feeding w/o parent feeding
PointSQL (Wang et al., 2017)† TypeSQL+TC (Yu et al., 2018)† STAMP (Sun et al., 2018)† STAMP+RL (Sun et al., 2018)†
TRANX w par. feed. + answer pruning† w/o par. feed. + answer pruning†

ACCEM 23.4 23.4 48.3 – 62.8 –
62.6 62.9 61.5
– 60.7 61.0
68.4 68.6

ACCEX 35.9 35.9 59.4 68.0 68.0 73.5
71.6 71.7 66.8 82.6 74.4 74.6
78.6 78.6

Table 3: Exact match (EM) and execution (EX) accuracies on WIKISQL. †Methods that use the contents of input tables.

two conﬁgurations, with or without parent feeding (§ 2.3). Our system outperforms existing neural network-based approaches. This demonstrates the effectiveness of TRANX in closed-domain semantic parsing. Interestingly, we found the model without parent feeding achieves slightly better accuracy on GEO, probably because that its relative simple grammar does not require extra handling of parent information.
Code Generation Tab. 2 lists the results on DJANGO. TRANX achieves state-of-the-art results on DJANGO. We also ﬁnd parent feeding yields +1 point gain in accuracy, suggesting the importance of modeling parental connections in ASTs with complex domain grammars (e.g., Python).
Tab. 3 shows the results on WIKISQL. We ﬁrst discuss our standard model which only uses information of column names and do not use the contents of input tables during inference, as listed in the top two blocks in Tab. 3. We ﬁnd TRANX, although just with simple extensions to adapt to this dataset, achieves impressive results and outperforms many task-speciﬁc methods. This demonstrates that TRANX is easy to extend to incorporate task-speciﬁc information, while maintaining its effectiveness. We also extend TRANX with a very simple answer pruning strategy, where we execute the candidate SQL queries in the beam against the input table, and prune those that yield empty execution results. Results are listed in the bottom two-blocks in Tab. 3, where we compare with systems that also use the contents of tables. Surprisingly, this (frustratingly) simple extension yields signiﬁcant improvements, outperforming many task-speciﬁc models that use speciﬁcally de-

signed, heavily-engineered neural networks to incorporate information of table contents.
4 Conclusion
We present TRANX, a transition-based abstract syntax parser. TRANX is generalizable, extensible and effective, achieving strong results on semantic parsing and code generation tasks.
Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 1815287. PY would like to thank Junxian He and Li Dong for helpful discussions.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transaction of ACL.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of LAW-ID@ACL.
Li Dong and Mirella Lapata. 2016. Language to logical form with neural attention. In Proceedings of ACL.
Li Dong and Mirella Lapata. 2018. Coarse-to-ﬁne decoding for neural semantic parsing. In Proceedings of ACL.
Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen tau Yih, and Xiaodong He. 2018. Natural language to structured query generation via meta-learning. In Proceedings of NAACL-HLT.
Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Toma´s Kocisky´, Fumin Wang, and Andrew Senior. 2016. Latent predictor networks for code generation. In Proceedings of ACL.
Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attentionbased neural machine translation. In Proceedings of EMNLP.
Graham Neubig. 2015. lamtram: A toolkit for language and translation modeling using neural networks. http://www.github.com/neubig/lamtram.
Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation (T). In Proceedings of ASE.
Chris Quirk, Raymond J. Mooney, and Michel Galley. 2015. Language to code: Learning semantic parsers for if-this-then-that recipes. In Proceedings of ACL.

Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract syntax networks for code generation and semantic parsing. In Proceedings of ACL.
Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and Ming Zhou. 2018. Semantic parsing with syntax- and table-aware SQL generation. CoRR, abs/1804.08338.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proceedings of NIPS.
Adrienne Wang, Tom Kwiatkowski, and Luke Zettlemoyer. 2014. Morpho-syntactic lexical generalization for CCG semantic parsing. In Proceedings of EMNLP.
Chenglong Wang, Marc Brockschmidt, and Rishabh Singh. 2017. Pointing out SQL queries from text. Technical report.
Daniel C. Wang, Andrew W. Appel, Jeffrey L. Korn, and Christopher S. Serra. 1997. The Zephyr abstract syntax description language. In Proceedings of DSL.
Chunyang Xiao, Marc Dymetman, and Claire Gardent. 2016. Sequence-based structured prediction for semantic parsing. In Proceedings of ACL.
Xiaojun Xu, Chang Liu, and Dawn Song. 2017. SQLNet: Generating structured queries from natural language without reinforcement learning. arXiv preprint arXiv:1711.04436.
Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. In Proceedings of ACL.
Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. 2018. StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing. In Proceedings of ACL.
Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir R. Radev. 2018. TypeSQL: Knowledgebased type-aware neural text-to-sql generation.
Luke Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form structured classiﬁcation with probabilistic categorial grammars. In Proceedings of UAI.
Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of EMNLP-CoNLL.
Kai Zhao and Liang Huang. 2015. Type-driven incremental semantic parsing with polymorphism. In Proceedings of NAACL-HLT.
Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103.

