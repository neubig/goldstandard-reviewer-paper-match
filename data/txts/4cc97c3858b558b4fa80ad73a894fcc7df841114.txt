FACT: A Diagnostic for Group Fairness Trade-offs

arXiv:2004.03424v3 [cs.LG] 7 Jul 2020

Joon Sik Kim 1 2 Jiahao Chen 3 Ameet Talwalkar 1 4

Abstract
Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conﬂict with one another, often with a necessary cost in loss of model’s predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness–confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classiﬁers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness.
1. Introduction
As machine learning continues to be more widely used for applications with societal impact such as credit decisioning, predictive policing, and employment applicant screening, practitioners face regulatory, ethical, and legal challenges to prove whether or not their models are fair (Crawford et al., 2019). To provide quantitative tests of model fairness, the practitioners further need to choose between multiple deﬁnitions of fairness that exist in the machine learning literature (Calders et al., 2009; Zˇ liobaite˙, 2015; Narayanan, 2018). Among them is a class of deﬁnitions called group
1Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA 2Work paritally done during an internship at JP Morgan 3JP Morgan AI Research, New York, USA 4Determined AI, San Francisco, USA. Correspondence to: Joon Sik Kim <joonkim@cmu.edu>.
Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).

fairness, which measures how a group of individuals with certain protected attributes are treated differently from other groups. This notion is widely studied as a concept of disparate impact in the legal context, and one speciﬁc instance of this notion was enforced as a law for fair employment process back in 1978 (Biddle, 2006). From a technical point of view however, several notions of group fairness have been shown to conﬂict with one another (Kleinberg et al., 2017; Chouldechova, 2017), sometimes with a necessary cost in loss of accuracy (Liu et al., 2019). Such considerations complicate the practical development and assessment of machine learning models designed to satisfy group fairness, as the conditions under which these trade-offs must necessarily occur can be too abstract to understand. Previous works on these trade-offs have been presented in ad hoc and deﬁnition-speciﬁc manner, which further calls for a more general perspective addressing the trade-offs in practice.
As an example, suppose an engineer is responsible for training a loan prediction model from a large user dataset, subject to mandatory group fairness requirements shaped by regulatory concerns. She has many choices for how to train this fair model, with fairness enforced before (Kamiran et al., 2010; Zemel et al., 2013; Madras et al., 2018; Samadi et al., 2018; Song et al., 2019; Tan et al., 2020), during (Zafar et al., 2015; 2017), or after (Dwork et al., 2012; Feldman et al., 2015; Hardt et al., 2016) training. However, she must resort to trial and error to determine which of these myriad approaches, if any, will produce a compliant model with sufﬁcient performance1 to satisfy business needs. It may even turn out that despite her best efforts, the fairness constraints set by the regulators are actually impossible to satisfy to begin with, due to limitations intrinsic to the prediction task and data at hand. If there was a tool to understand the potential trade-offs exhibited by the model, even before training, it would be easier for multiple parties to effectively reconcile the conﬂicting components in designing fair classiﬁers.
Motivated by such practical considerations, we propose the FACT (FAirness-Confusion Tensor) diagnostic for exploring the trade-offs involving group fairness: the diagnostic provides a general framework under which the practitioners can understand both fairness–fairness trade-offs and fairness–
1In this work, performance refers to classical metrics derived from the confusion matrix, e.g., accuracy, precision and fairness notions are not part of it.

FACT: A Diagnostic for Group Fairness Trade-offs

performance trade-offs. At the core of our diagnostic lies the fairness–confusion tensor, which is the confusion matrix divided along an additional axis for protected attributes. The FACT diagnostic ﬁrst expresses the majority of group fairness notions as linear/quadratic functions of the elements of this tensor. The simplicity of these functions makes it easy for them to be naturally integrated into a class of optimization problems over the elements of the tensor (not over the model parameters), which we call performance– fairness optimality problem (PFOP). It essentially considers the geometry of valid fairness–confusion tensors that satisfy a speciﬁed set of performance and/or fairness conditions.
By noting that many settings involve only linear notions of fairness, in this work we focus on least-squares accuracy– fairness optimality problem (LAFOP) and model-speciﬁc least-squares accuracy–fairness optimality problem (MSLAFOP), which are speciﬁc instantiations of PFOP, each representative of model-agnostic and model-speciﬁc scenarios. In particular, for the model-agnostic case, the diagnostic allows for a comparative analysis of the relative difﬁculty of learning a classiﬁer under additional group fairness constraints imposed. This difﬁculty is interpreted with respect to the Bayes error, which is the inherent difﬁculty of the fairness-unconstrained learning problem, hence a natural reference point.
Our contributions are:
1. to demonstrate how fairness–confusion tensor characterizes the majority of group fairness deﬁnitions in the literature as linear or quadratic functions, whose simplicity can be leveraged to formulate optimization problems suited for trade-off analysis,
2. to formulate the FACT diagnostic as a PFOP, LAFOP, and MS-LAFOP over the fairness–confusion tensor, enabling both model-agnostic and model-speciﬁc analysis of fairness trade-offs,
3. to provide a general understanding of group fairness incompatibility, which simpliﬁes the existing results in the literature and extends them to new types,
4. to demonstrate the use of the FACT diagnostic on synthetic and real datasets, e.g. how it can be used for diagnosis of relative inﬂuence of the fairness notions on performance and other fairness conditions, and how it can be used as a post-processing method for designing fair classiﬁers.
2. Related Work
Fairness–confusion tensor is not a completely new notion – several work has implicitly mentioned it, mostly disregarding it as a simple computational tool that eases the

computation on an implementation level (Bellamy et al., 2018; Celis et al., 2019). It is also a natural object considered in several post-processing methods in fairness (Hardt et al., 2016; Pleiss et al., 2017), a group of algorithms that ﬁne-tune a trained model to mitigate the unfairness while keeping the performance change minimal. Here we take a closer look at the fairness–confusion tensor itself and study how this object naturally brings together several notions of group fairness, simplifying and generalizing the analysis of inherent trade-offs within.
Quantitative deﬁnitions of group fairness exist in many different variations (Narayanan, 2018; Kleinberg et al., 2017; Chouldechova, 2017; Dwork et al., 2012; Hardt et al., 2016; Calders & Verwer, 2010; Berk et al., 2018) but few work exists to categorize these notions with a broader perspective encompassing the trade-off schemes. Verma & Rubin (2018) categorized the existing group fairness deﬁnitions based on entries and rates derived from the fairness– confusion tensor but did not explore any trade-offs and incompatibilities within. Our work extends this effort and provides a versatile geometric formalism to study the tradeoffs.
Fairness–performance trade-offs have been studied in many speciﬁc cases (Calders et al., 2009; Zˇ liobaite˙, 2015; Kamiran et al., 2010; Feldman et al., 2015; Menon & Williamson, 2018; Liu et al., 2019; Zhao & Gordon, 2019), for limited deﬁnitions of fairness, performance, and models. To our knowledge, these trade-offs have not been studied in the general way we present below. Zafar et al. (2015; 2017) presented an optimization-based analysis of the trade-offs, albeit over the parameter space of a particular model.
Fairness–fairness trade-offs describe the incompatibility of multiple notions of group fairness (Kleinberg et al., 2017; Chouldechova, 2017; Pleiss et al., 2017; Berk et al., 2018) without some strong assumptions about the data and the model. Previous incompatibility results have been presented mostly in ad hoc and deﬁnition-speciﬁc manner, which our diagnostic addresses with a more general perspective for understanding incompatibilities. We show a general incompatibility result involving Calibration fairness condition, which naturally implies the result in Kleinberg et al. (2017) along with many other new ones. To the best of our knowledge, our work is the ﬁrst to provide a systematic approach to diagnose both fairness–fairness and fairness–performance trade-offs together for group fairness under the same formalism.
3. The Fairness–confusion Tensor
Our key insight is that the elements of the fairness– confusion tensor encode all the information needed to study many notions of performance and group fairness. The

FACT: A Diagnostic for Group Fairness Trade-offs

Name of fairness

Deﬁnition and linear system

Terms in fairness–confusion tensor

Demographic parity (DP) Equality of opportunity (EOp)(Hardt et al., 2016) Predictive equality (PE)(Chouldechova, 2017) Equalized odds (EOd)(Hardt et al., 2016) Equal false negative rate (EFNR) 2 Calibration within groups (CG)(Kleinberg et al., 2017)
Positive class balance (PCB)(Kleinberg et al., 2017) Negative class balance (NCB)(Kleinberg et al., 2017) Relaxed Equalized Odds (REod)(Pleiss et al., 2017)

Pr(yˆ = 1|a = 1) = Pr(yˆ = 1|a = 0)

ADP = N1 N0 0 N0 0 −N1 0 −N1 0 Pr(yˆ = 1|y = 1, a = 1) = Pr(yˆ = 1|y = 1, a = 0)

AEOP = N1 M0 0 0 0 −M1 0 0 0 Pr(yˆ = 1|y = 0, a = 1) = Pr(yˆ = 1|y = 0, a = 0)

APE = N1 0 0 N0 − M0 0 0 0 −N1 + M1 0 EOp ∧ PE

Pr(yˆ = 0|y = 1, a = 1) = Pr(yˆ = 0|y = 1, a = 0)

AEFNR = N1 0 M0 0 0 0 −M1 0 0 Pr(y = 1|Pθ(x) = s, a = 1) = Pr(y = 1|Pθ(x) = s, a = 0) = s

1 − v1 0 −v1 0 0

0 0 0

ACG =  00 1 −0 v0 00 −0v0 1 −0 v1 00 −0v1 00 

0

0 0 0 0 1 − v0 0 −v0

E(Pθ|y = 1, a = 1) = E(Pθ|y = 1, a = 0) APCB = mina(Ma) Mv11 Mv01 0 0 − Mv10 − Mv00 0 0 E(Pθ|y = 0, a = 1) = E(Pθ|y = 0, a = 0) ANCB = mina(Na − Ma) 0 0 N1v−1M1 N1v−0M1 0 0 α0F P R0 + β0F N R0 = α1F P R1 + β1F N R1

− v1 N0 −M0

− v0 N0 −M0

AREOD = 0 Mβ11 N1α−1M1 0 0 Mβ00 N0α−0M0 0 /N

∧

∧

∧

∧

Predictive parity (PP)(Chouldechova, 2017) Equal false omission rate (EFOR) 1 Conditional accuracy equality (CA)(Berk et al., 2018)

Pr(y = 1|yˆ = 1, a = 1) = Pr(y = 1|yˆ = 1, a = 0)
21 zT BPPz = (T P1F P0 − T P0F P1)/N 2 Pr(y = 1|yˆ = 0, a = 1) = Pr(y = 1|yˆ = 0, a = 0)
12 zT BEFORz = (T N1F N0 − T N0F N1)/N 2 PP ∧ EFOR

(

)(2)

(

)(2)

(

)(2) ∧ (

)(2)

1 To our knowledge, EFOR has not been described in literature in isolation, but is used in the deﬁnition of conditional accuracy equality (CA)(Berk et al., 2018). 2 Deﬁned implicitly in (Chouldechova, 2017).

Table 1. Some common group fairness deﬁnitions and corresponding abbreviations used throughout the paper in terms of linear functions φ(z) = Az or quadratic functions φ(z) = 12 zT Bz that appear in the performance–fairness optimality problem (5). There are two groups separated by the horizontal line: those that are speciﬁed by linear functions (above), or quadratic functions (below). The graphical notation
is described in Section 3. Pθ is the probability produced by a model (parameterized by θ) of yˆ = 1. The fairness functions φ are uniquely
deﬁned only up to a normalization factor and overall sign.

fairness–confusion tensor is simply the stack of confusion matrices for each protected attribute a, as shown in Table 2. We focus on the simplest case, with one binary protected attribute a ∈ {0, 1}, and a binary classiﬁer yˆ ∈ {0, 1} for a binary prediction label y ∈ {0, 1}.2

a=1 yˆ = 1 yˆ = 0

y=1 TP1 FN1

y=0 FP1 TN1

a=0 yˆ = 1 yˆ = 0

y=1 TP0 FN0

y=0 FP0 TN0

Table 2. The fairness–confusion tensor, showing the two planes corresponding to the confusion matrix for each of the favored (a = 1) and disfavored groups (a = 0).

Let us denote the elements of the fairness–confusion tensor as T Pa, F Pa, F Na, T Na, each element with subscripts indicating a, N be the number of data points, Na = T Pa + F Na + F Pa + T Na be the number of data points in each group a ∈ {0, 1}, and Ma = T Pa + F Na be the number of positive-class instances (y = 1) for each group. Assume N , Na and Ma are known constants. Unraveling the fairness–confusion tensor into an 8-dimensional vector, we write it as
z = (T P1, F N1, F P1, T N1, T P0, F N0, F P0, T N0)T /N,
2The arguments generalize to multiple and non-binary protected attributes with high-dimensional tensors.

normalized and constrained to lie on K = {z ≥ 0 : Aconstz = bconst, z 1 = 1}, where Aconst and bconst encode marginal sum constraints of the dataset (e.g., T Pa + F Na = Ma) in matrix notations:
1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 Aconst = 0 0 0 0 1 1 1 1 ,
00001100 bconst = (N1, M1, N0, M0)T /N.
We show below that some typical notions of group fairness can be reformulated as simple functions of z, namely as a form of φ(z) = 0.
Demographic parity (DP) states that each protected group should receive positive prediction at an equal rate: Pr(yˆ = 1|a = 1) = Pr(yˆ = 1|a = 0), which is equivalent to (T P1 + F P1)/N1 = (T P0 + F P0)/N0, or also the linear system φ(z) = ADPz = 0, where
ADP = N0 0 N0 0 −N1 0 −N1 0 /N. (1)
The choice of normalization, 1/N , ensures that the matrix coefﬁcients are in [0, 1]. We will refer to these matrices A that encode information about the fairness conditions as fairness matrices.

FACT: A Diagnostic for Group Fairness Trade-offs

Predictive parity (PP) (Chouldechova, 2017) states that the
likelihood of being in the positive class given the positive prediction is the same for each group: Pr(y = 1|yˆ = 1, a = 1) = Pr(y = 1|yˆ = 1, a = 0), which is equivalent to T PT1+PF1 P1 = T PT0+PF0 P0 ⇐⇒ TT PP10 = FF PP10 . Unlike for DP, the marginal sum constraints do not relate T Pa and F Pa, so this notion of fairness is not linear in the fairness–confusion tensor. PP actually can be expressed using a quadratic form:

φ(z) = 1 zT BPPz = 0, 2

 0 0 0 0 0 0 −1 0

 0 0 0 0 0 0 0 0

 0 0 0 0 1 0 0 0





BPP =  00 00 01 00 00 00 00 00 .

 

0

00000

0

0

−1 0 0 0 0 0 0 0

0 00000 0 0

(2)

Calibration within groups (CG) (Kleinberg et al., 2017), when specialized to binary classiﬁers and binary protected classes, can be written as the system of equations F Na = v0(F Na + T Na); T Pa = v1(T Pa + F Pa), where the vis are scores satisfying 0 ≤ v0 < v1 ≤ 1 and have no implicit dependence on any entries of the fairness–confusion tensor. We can rewrite this this condition explicitly as the matrix equation φ(z) = ACGz = 0 with a fairness matrix

1 − v1 0 −v1 0 0

0 0 0

ACG =  00 1 −0 v0 00 −0v0 1 −0 v1 00 −0v1 00  .

0

0 0 0 0 1 − v0 0 −v0

(3)

Equalized odds (EOd) (Hardt et al., 2016) states that truepositive rates and false-positive rates are the same for both groups, which can be expressed as a linear system φ(z) = AEODz = 0 with a fairness matrix

1 AEOD = N

M0 0 0 0 −M1 0

0

0

0 0 N0 − M0 0 0 0 −N1 + M1 0

(4)

where each row respectively corresponds to conditions for

Equality of Opportunity (EOp) (Hardt et al., 2016) and

Predictive Equality (PE) (Chouldechova, 2017). Likewise,

vertically stacking multiple fairness matrices results in a

fairness matrix corresponding to the conjunction of different

fairness notions.

In Table 1 we generalize this formulation to a wide ma-

jority of group fairness deﬁnitions in the literature, along

with their abbreviations used throughout the paper. We ﬁnd

that most of the deﬁnitions take either linear or quadratic

form with respect to z. We further introduce a graphical

notation to help visualize which components of the fairness–

confusion tensor participate in the fairness deﬁnition. Depict

the fairness–confusion tensor as

, with the left matrix

for the favored class (a = 1) and the right matrix for the

disfavored class (a = 0). Since each component of z cor-

responds to some element of the fairness–confusion tensor,

we shade each component that appears in the equation. Blue

shading denotes the favored class, while red shading denotes

the disfavored class. We further distinguish two kinds of

dependencies. Components that have a nonzero coefﬁcient

in the matrix are shaded fully. However, the values of these

coefﬁcients themselves can depend on other components,

albeit implicitly, and we shade these implicit components in

a lighter shade. Putting this all together, we can represent

DP in (1) graphically as

, EOd as

∧

, PP

as (

)(2) , with the superscript denoting the quadratic

order of the term. As shown in the third column of Table 1,

all group fairness notions can be effectively described in

this notation.

4. Optimization over the Fairness–confusion Tensor

The fairness–confusion tensor z allows for a succinct linear and quadratic characterization of group fairness deﬁnitions in the literature. We naturally consider the following family of optimization problems over z ∈ K, where the objective function is constructed so that the solution reﬂects trade-offs between fairness and performance.
Deﬁnition 1. Let f (i) : K → [0, 1] be performance metrics (indexed by i) with best performance 0 and worst performance 1, φ(j)(z) be fairness functions (indexed by j) with µi, λj be real constants with µ0 = 1. Then, the performance–fairness optimality problem (PFOP) is a class of optimization problem of form:

arg min µif (i)(z) + λj φ(j)(z)

(5)

z∈K i≥0

j≥0

PFOP is a general optimization problem containing two groups of terms; the ﬁrst quantifying performance loss; the second quantifying unfairness. The restriction z ∈ K is necessary to ensure that z is a valid fairness–confusion tensor that obeys the requisite marginal sums. In our discussion below, it will be convenient to consider solutions with explicit bounds on their optimality.
Deﬁnition 2. Let ≥ 0 and δ ≥ 0. Then, a ( , δ)solution to the PFOP is a z that satisﬁes (5) such that
j λj φ(j)(z) ≤ and i µif (i)(z) ≤ δ.
The parameters and δ represent the sum total of deviation from perfect fairness and perfect predictive performance respectively. Unless otherwise stated, the rest of the paper is dedicated to analyzing one of the simplest instantiations of PFOP, deﬁned below.
Deﬁnition 3. The least-squares accuracy–fairness optimality problem (LAFOP) is a PFOP with accuracy (or classiﬁcation error rate) as the performance function f (0), and

FACT: A Diagnostic for Group Fairness Trade-offs

K ≥ 1 fairness constraints in the form of a fairness matrix A (each row indexed by j), with

φ(j)(z) = (Aj,∗z)2, j = 0, ..., K − 1 f (0)(z) = (c · z)2,
(6) c = (0, 1, 1, 0, 0, 1, 1, 0)T , λ = λ0 = ... = λK−1.

In other words, LAFOP is the problem

arg min (c · z)2 + λ Az 22,

(7)

z∈K

where c · z encodes the usual notion of classiﬁcation error, and A encodes K linear fairness functions stacked together as the regularizer. A single hyperparameter λ speciﬁes the relative importance of satisfying the fairness constraints while optimizing classiﬁcation performance, with λ = 0 considering only performance and disabling all fairness constraints, and λ = ∞ imposing fairness constraints without regard to accuracy.
LAFOP is a convex optimization problem which is simple to analyze. Despite its simplicity, LAFOP encompasses many situations involving linear notions of fairness, allowing us to reason about multiple fairness constraints as well as fairness– accuracy trade-offs under versatile scenarios.

4.1. Reduction to a post-processing method for fair classiﬁcation
PFOP and LAFOP do not assume anything about the model, therefore are designed to be model-agnostic. In this section we highlight the versatility of LAFOP by showing that adding a model-speciﬁc constraint on LAFOP reduces it to a post-processing algorithm for fair classiﬁcation.
Post-processing method, in particular for EOd as introduced in Hardt et al. (2016), solves the following optimization problem for Y˜ , which is a post-processed, supposedly fair, classiﬁer, given Yˆ , a vanilla classiﬁer:
min El(Y˜ , Y ) such that γ0(Y˜ ) = γ1(Y˜ )
Y˜
and γ0(Y˜ ) ∈ P0(Yˆ ), γ1(Y˜ ) ∈ P1(Yˆ ) (8)
where γa(Y˜ ) represents EOd constraints for Y˜ as a tuple of (F P Ra, T P Ra), and Pa(Yˆ ) is a modelspeciﬁc set of feasible γa values, deﬁned as Pa(Yˆ ) = convhull{(0, 0), γa(Yˆ ), γa(1 − Yˆ ), (1, 1)}. All the components of (8) can be rewritten in terms of zˆ and z˜, the fairness–confusion tensors corresponding to the classiﬁers Yˆ and Y˜ respectively. This yields a LAFOP over z˜ with additional model-speciﬁc constraints derived from zˆ on the solution space. More formally, we have the following optimization problem for post-processing:

Deﬁnition 4. Given a classiﬁer to be post-processed and its corresponding fairness–confusion tensor zˆ, the modelspeciﬁc LAFOP (MS-LAFOP) for EOd is the variant of LAFOP with model-speciﬁc constraints on the solution space as the following:

arg min (c · z˜)2 + λ AEODz˜ 22, where

(9)

z˜∈Kˆ

where

Kˆ = z˜ ≥ 0 : Aconstz˜ = bconst, z˜ 1 = 1, βa(z˜) ∈ convhull {(0, 0), βa(zˆ), βa(1 − zˆ), (1, 1)} ∀a

with βa expressing (F P Ra, T P Ra) tuples computed from the corresponding fairness–confusion tensor of group a.

From the solution of MS-LAFOP, it is possible to compute mixing rates for post-processing the given classiﬁer. We note that MS-LAFOP can be extended to other group fairness notions as long as the model-speciﬁc constraints are accordingly set up for them. For more details, refer to Appendix G.3.

5. Incompatible Group Fairness Deﬁnitions
In this section, we show how LAFOP yields a more general view of understanding group fairness incompatibility results. As λ → ∞, for linear fairness functions φ(i)(z) = A(i)z, LAFOP becomes equivalent to solving the following linear system of equations:

 A(0)   0 

 ..   .. 

 .  z =  .  , z ≥ 0,

(10)





A(K−1)  0 

Aconst

bconst

Notice the compatibility of fairness conditions encoded by these K fairness matrices A(i) is equivalent to having inﬁnitely many solutions to the above linear system. We formally deﬁne (in)compatibility of fairness notions below based on this observation.
Deﬁnition 5. Let Φ = {φ(i)}Ki=−01 be a set of linear fairness functions, encoded in a fairness matrix A (of which each row corresponds to φ(i)), and let ρ be the number of solutions for the system in (10). If ρ = 0, then Φ is said to be incompatible. Otherwise, Φ is compatible. When Φ is incompatible, some additional set of constraints on the dataset or the model is required for it to be compatible.

This means that in general, incompatibility results among the group fairness deﬁnitions can be proven simply by asking if and when solutions exist to their corresponding linear system of form (10).

FACT: A Diagnostic for Group Fairness Trade-offs
Sets of fairness deﬁnitions {CG, PP, DP, and any of EOp, PE, PCB, NCB, EFOR} {CG, DP, and any of EOp, PE, PCB, NCB, EFOR} {CG,EOp}, {CG,PCB}, {CG,EOp,PCB},{CG,EFOR,EOp}, {CG,EFOR,PCB},{CG,EFOR,EOp,PCB} {CG,PE}, {CG,NCB}, {CG,EOp,NCB}, {CG,EFOR,PE}, {CG,EFOR,NCB}, {CG,EFOR,EOp,NCB} {CG,EOd}(Pleiss et al., 2017), {CG, PCB, NCB} (Kleinberg et al., 2017),{CG,EOd,PCB,NCB}, {CG,EFOR,EOd}, {CG,EFOR,PCB,NCB},{CG,EFOR,EOd,PCB,NCB}

Necessary conditions
M0 = M1 and N0 = N1 EBR only v0 = 0 or EBR v1 = 1 or EBR
(v0 = 0 and v1 = 1) or EBR

Table 3. Some sets of fairness deﬁnitions containing Calibration(CG), which are incompatible in the sense of Deﬁnition 5 (left-column), together with their necessary conditions to be compatible (right column). EBR is the equal base rate condition, M0/N0 = M1/N1. For other abbreviations, refer to Table 1. These are all special cases of Theorem 1, while not exhaustive.

5.1. The incompatibility involving CG

We introduce a general incompatibility result involving CG that leads to many other new results as well as the one from Kleinberg et al. (2017).
Theorem 1. Let B = 2 be the number of bins in the deﬁnition of calibration within groups fairness (CG) (Kleinberg et al., 2017), and v0, v1 be the scores, with 0 ≤ v0 < v1 ≤ 1, and K > 1 with φ(0)(z) = ACGz. Then, the corresponding (10) has the only solution

 v1(M1 − N1v0) 

 v0(−M1 + N1v1) 





 (1 − v1)(M1 − N1v0) 

1





(1 − v0)(−M1 + N1v1)

z0 =



N (v1 − v0) 

v1(M0 − N0v0)

 , (11) 





 v0(−M0 + N0v1) 





 (1 − v1)(M0 − N0v0) 

(1 − v0)(−M0 + N0v1)

and only when

0 ≤ v0 ≤ min Ma ≤ max Ma ≤ v1 ≤ 1. (12)

a Na

a Na

Otherwise, no solution exists.

Theorem 1 yields other extended results regarding the incompatibility of CG and other notions of fairness. As one canonical instance, simply substituting z0 in (11) to the linear system of the form in (10) with PCB and NCB fairness matrices yields the following corollary, which is equivalent to the result presented in Kleinberg et al. (2017) (proof is in Appendix B).
Corollary 1 (Re-derivation of (Kleinberg et al., 2017)). Consider a classiﬁer that satisﬁes CG, PCB and NCB fairness simultaneously. Then, at least one of the following statements is true:
1. the data have equal base rates for each class a, i.e. M0/N0 = M1/N1, or
2. the classiﬁer has perfect prediction, i.e. v0 = 0 and v1 = 1.

Similar approach can be applied to derive incompatibilities of CG with other linear and quadratic notions of fairness as below (proofs in Appendix C, Appendix D). Corollary 2. (Linear notion of fairness: DP) Consider a classiﬁer that satisﬁes CG and DP fairness simultaneously. Then, the data have equal base rates for each group a. Corollary 3. (Quadratic notion of fairness: PP) Consider a classiﬁer that satisﬁes CG and PP fairness simultaneously. Then, at least one of the following is true:
1. v0 = (M1 − M0)/(N1 − N0). 2. v1 = 1.
From Theorem 1 and its corollaries, we curate the extended incompatibility results involving CG in Table 3 along with conditions for compatibility. To our knowledge, all cases other than the bottom row of the table are new.
5.2. The incompatibility of {PE, EFNR, PP}
Using the same logic as the previous section, we re-derive an incompatibility result in Chouldechova (2017) and provide more precise necessary conditions for compatibility. For details of the proof, refer to Appendix E. Theorem 2 (Restatement of Chouldechova (2017)). Consider a classiﬁer that satisﬁes {PE, EFNR, PP}. Then, at least one of these statements must be true:
1. The classiﬁer has no true positives. 2. The classiﬁer has no false positives. 3. Each protected class has the same base rate.
Theorem 2 systematically shows that equal false positive rates, equal false negative rates, and predictive parity are compatible only under speciﬁc data/model-dependent circumstances, that were otherwise not clear in the original statements in Chouldechova (2017).
6. Experiments
In this section we show how the FACT diagnostic can practically show the relative impact of several notions of fairness

FACT: A Diagnostic for Group Fairness Trade-offs

on accuracy on synthetic and real datasets3. First we introduce FACT Pareto frontiers which characterize a model’s achievable accuracy for a given set of fairness conditions, as a tool for understanding the trade-offs and contextualizing some recent works in fair classiﬁcation (Section 6.2). We then explore a model-agnostic assessment of multiple fairness conditions via LAFOP (Section 6.3, Appendix G.2), as well as a model-speciﬁc assessment of post-processing methods in fair classiﬁcation via MS-LAFOP (Section 6.4, Appendix G.3).

6.1. Datasets

We study a synthetic dataset similar to that in Zafar et al. (2015), consisting of two-dimensional features along with a single binary protected attribute that is either sampled from an independent Bernoulli distribution (“unbiased” variant, denoted S(U)), or sampled dependent on the features (“biased” variant, denoted S(B)). The synthetic dataset consists of two-dimensional data x = (x0, x1) that follow the Gaussian distributions

x|y = 1 ∼N

2 , 51 2 15

(13) x|y = 0 ∼N −2 , 10 1 .
−2 1 3

For the S(U) dataset, the protected attribute value is independent of x and y, and is instead distributed according to the Bernoulli distribution a ∼ B 12 . This notion of fairness was described in (Calders et al., 2009).
For the S(B) dataset, the protected attribute value is assigned as a|x = sgn(x0), which corresponds to a situation when some features (but not all) encode a protected attribute.
We also study the UCI Adult dataset (Dua & Graff, 2017), a census dataset used for income classiﬁcation tasks where we consider sex as the protected attribute of interest.
6.2. FACT Pareto frontiers
With LAFOP and MS-LAFOP, one can naturally consider a FACT Pareto frontier of accuracy and fairness by plotting ( , δ) values of the ( , δ)-solutions. In this section, we want to highlight the use of this frontier in the context of several published results in the literature as well as its implications.
The FACT Pareto frontier can be computed both in modelagnostic (MA) and model-speciﬁc (MS) scenarios by solving LAFOP and MS-LAFOP respectively, and Figure 1 shows such example on the Adult dataset for EOd fairness. We also consider three fair classiﬁcation models: FGP (Tan et al., 2020), Op. (Zafar et al., 2015), and Eq.Odd. (Hardt et al., 2016), individually representing three different ap-
3Code available: github.com/wnstlr/FACT

Accuracy (1-δ)

1.00

0.95

0.90

0.85

0.80

0.75

100

10−1

10−2

10−3

10−4

10−5

10−6

Fairness gap of [EOd] (smaller the better)

MA FACT Pareto frontier MS FACT Pareto Frontier FGP (Tan et al. 2020) Eq.Odd. (Hardt et al. 2016) Op. (Zafar et al. 2015) Bayes clf LogisticRegression SVM RandomForest ConstantPred(-)

Figure 1. Model-agnostic (MA) and model-speciﬁc (MS) FACT Pareto frontiers of equalized odds on the Adult dataset. Three fair models (FGP, Eq.Odd., Op.) are shown in context by varying the strength of the fairness condition imposed, along with some baseline models (LR, SVM, RF, ConstantPrediction). The MA frontier should be interpreted relative to the Bayes error because it is oblivious to it — δ = 0 means that the upper bound of the accuracy is the accuracy of the Bayes classiﬁer, not 1. The MS frontier on the other hand provides realistic more bounds.
proaches one can take in training fair models (imposing fairness before, during, or after training). Some baseline models (logistic regression, SVM, random forest) are also plotted for reference, and a perfectly fair classiﬁer (ConstantPredict: predicting all instances to be negative) on the bottom right corner is considered as an edge-case.
It is important to note that the MA FACT Pareto frontier should be interpreted as characterizing the model’s achievable accuracy relative to the Bayes error (i.e., the degree to which the added fairness constraints adversely impact the Bayes error), which in this case is empirically estimated at around 0.12 from a wide range of ML models that have been tested on the Adult datset (Chakrabarty & Biswas, 2018). This relatively less realizable bound calls for a modelspeciﬁc counterpart, the MS FACT Pareto frontier, which limits the frontier to be derived from a given pre-trained classiﬁer. As shown in Figure 1, it indeed provides a more reasonable frontier for the models considered.
Placing different types of classiﬁers on the frontier, it is easy to visually grasp strengths and weaknesses of each models. FGP seems to outperform all other models in terms of the trade-off, while Op and EqOdd suffer more from early accuracy drops. The frontier further informs that for any model trained, only for fairness gaps below 10−2 will the accuracy start to suffer. Such understanding of the trade-offs will be helpful in anticipating practical limitations of models to be trained, as well as in comparing multiple models to determine which is better-suited for different situations.
In the rest of the following sections and ﬁgures, for the model-agnostic analysis, δ should be interpreted in reference to the Bayes error, i.e δ = 0 means that the upper bound of the best-achievable accuracy is the accuracy of the Bayes

FACT: A Diagnostic for Group Fairness Trade-offs

{PCB, CB}, {PE, NCB}
{PCB, DP}
{EOd, DP},   {EOd, DP, PCB},   {EOd, DP, CB, PE},   {EOd, DP, CB, PE, EOp}
{PCB, NCB, CG}
{CG, CB, EOp, DP}

Figure 2. Model-agnostic FACT Pareto frontier for different groups of fairness notions (colored and grouped according to their convergence value as → 0) for three datasets (Section 6.1). The bottom two groups of fairness notions are incompatible (black, red), hence the halted trajectories before reaching smaller values of . Similar convergence behaviors within the fairness groups in blue reﬂect the dominance of {EOd, DP} – any additional fairness notions added on top of these have no impact on the convergence value. Best viewed in color.

classiﬁer, not 1.

6.3. Model-agnostic scenario with multiple fairness conditions

We are now interested in how a group of fairness conditions

simultaneously affect accuracy. This can be assessed by

looking at the shape of the MA FACT Pareto frontier of

LAFOP with multiple fairness constraints, particularly δ

values of ( , δ)-solutions when is varied to be zero (or

very close to it) on multiple fairness notions. Figure 2

shows this in two different ways: (i) ( ,δ)-solutions ob-

tained when fairness conditions are imposed as hard in-

equality constraints instead of as regularizers, i.e. solving

arg minz∈K (c · z)2 s.t.

Az

2 2

≤

(solid line), and (ii)

( ,δ)-solutions obtained from the LAFOP (7) while vary-

ing λs (crosses). Different groups of fairness notions are

colored according to their convergence behaviors.

Similar trajectories and convergence of the curves allow us to identify fairness notions that come “for free” given some others, in terms of additional accuracy drops. In other words, the Pareto frontiers are effective at demonstrating the relative strength of the fairness notions within a group. For instance, under {EOd, DP} (third group, blue) the best attainable accuracy drops by over 60 percent for S(U) and S(B), but we also observe that adding CB, PE, and/or PCB on top of them causes no additional accuracy drop – {EOd, DP} essentially determines δ for the entire group of fairness notions in blue.

The MA FACT Pareto frontiers for multiple fairness conditions also show not only the existing incompatibility of the fairness notions, but also how much relaxation is required for them to be approximately compatible. The halted trajectories before hitting much smaller for the bottom two groups in black and red clearly verify this. Because the S(U) dataset has a smaller base rate gap between the groups compared to the Adult or the S(B) dataset by design, the incompatibility in S(U) becomes only visible at a much

smaller value.
Taking a more macroscopic perspective, the MA FACT Pareto frontiers also show which dataset allows overall better trade-off scheme compared to the others. Because the S(U) dataset was designed to be less biased compared to the S(B) dataset, it exhibits signiﬁcantly smaller drop in overall accuracy, particularly for the green group involving DP. The way S(U) was designed aligns with this observation, as the sensitive attributes were randomly sampled independently from the features. However, EOd and DP together (in blue) drives down the accuracy just like the biased counterpart, which demonstrates how conservative EOd fairness is for these datasets.
More observations and experiments are presented in Appendix G.2. It is possible to further extend these analyses to an arbitrary number of fairness constraints imposed on LAFOP, as well as to other performance metrics like precision or recall as seem ﬁt.
6.4. Model-speciﬁc scenario with post-processing methods
While the MA FACT Pareto frontier shows a broader tradeoff landscape for any classiﬁers, model-speciﬁc analysis using MS-LAFOP in (9) can be helpful in practice with more reasonable MS Pareto frontiers. Also after solving the MS-LAFOP, its solution can be used to compute the mixing rates for post-processing any given classiﬁer just like done in Hardt et al. (2016). For more details, refer to Appendix G.3.
Figure 3 shows the MS FACT Pareto frontier of EOd computed from MS-LAFOP for the Adult dataset (it is a zoomedin version of the MA FACT Pareto frontier in Figure 1). We also plot two types of post-processed classiﬁers: EOdsolutions using the algorithm in Hardt et al. (2016) (circles), and FACT-solutions using MS-LAFOP (stars). EOd solutions undergo steeper trade-off while the FACT-solutions

FACT: A Diagnostic for Group Fairness Trade-offs

Accuracy (1 - δ)

0.84 0.82 0.80 0.78 0.76
100

MS FACT Frontier (EOd) before EOd-solution FACT-solution

10−1

10−2

10−3

Fairness Gap (ε)

10−4

10−5

Figure 3. Model-speciﬁc FACT Pareto frontier of EOd on Adult dataset. Compared to the model-agnostic frontier, it yields a more realizable bounds on the trade-off between fairness and accuracy. Post-processed solutions for the given classiﬁers (crosses) using the algorithm in (Hardt et al., 2016) (circles, EOd-solution) and FACT (stars, FACT-solution) are also shown. The FACT-solutions suffer signiﬁcantly less from the trade-off, yielding competitive accuracy to the original classiﬁers while achieving smaller fairness gaps compared to the EOd-solutions.

are able to ﬁnd a better conﬁguration with smaller fairness gaps, retaining a competitive accuracy level to the original classiﬁer (cross).
7. Conclusions
The FACT diagnostic facilitates systematic reasoning about different kinds of trade-offs involving arbitrarily many notions of performance and group fairness notions, which all can be expressed as functions of the fairness–confusion tensor. In our formalism, the majority of group fairness deﬁnitions in the literature are in fact linear or quadratic thus are easy to be imposed as constraints to the PFOP. The FACT diagnostic further beneﬁts from elementary linear algebra and convex optimization to provide a uniﬁed perspective of viewing fairness–fairness trade-offs and fairness–performance trade-offs. We have also empirically demonstrated the practical use of the FACT diagnostic in several scenarios. Many of the presented results require only linear fairness functions and accuracy, as in the LAFOP/MS-LAFOP setting. Nevertheless, it is easy to extend this to quadratic fairness functions with more varied performance metrics depending on different use cases. We also brieﬂy introduce a small theoretical result regarding fairness–accuracy trade-offs using the FACT diagnostic in Appendix F, which deserves further analysis.

IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, a JP Morgan A.I. Research Faculty Award, and a Carnegie Bosch Institute Research Award. JSK acknowledges support from Kwanjeong Educational Fellowship. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of DARPA, the National Science Foundation, or any other funding agency.
This paper was prepared for information purposes by the Artiﬁcial Intelligence Research group of JPMorgan Chase & Co and its afﬁliates (“JP Morgan”), and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, ﬁnancial instrument, ﬁnancial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful. 2020 JPMorgan Chase & Co. All rights reserved.
References
Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., et al. Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943, 2018.
Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 2018. doi: 10.1177/0049124118782533.
Biddle, D. Adverse impact and test validation: A practitioner’s guide to valid and defensible employment testing. Gower Publishing, Ltd., 2006.
Calders, T. and Verwer, S. Three naive bayes approaches for discrimination-free classiﬁcation. Data Mining and Knowledge Discovery, 21(2), September 2010. doi: 10. 1007/s10618-010-0190-x.

Acknowledgements
We thank Valerie Chen, Jeremy Cohen, Amanda Coston, Mikhail Khodak, Jeffrey Li, Liam Li, Gregory Plumb, Nick Roberts, and Samuel Yeom for helpful feedback and discussions. This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants

Calders, T., Kamiran, F., and Pechenizkiy, M. Building classiﬁers with independency constraints. In 2009 IEEE International Conference on Data Mining Workshops, 2009. doi: 10.1109/ICDMW.2009.83.
Celis, L. E., Huang, L., Keswani, V., and Vishnoi, N. K. Classiﬁcation with fairness constraints: A meta-algorithm

FACT: A Diagnostic for Group Fairness Trade-offs

with provable guarantees. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.
Chakrabarty, N. and Biswas, S. A statistical approach to adult census income level prediction. In 2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN). IEEE, 2018.
Chouldechova, A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2), 2017.
Crawford, K., Dobbe, R., Dryer, T., Fried, G., Green, B., Kaziunas, E., Kak, A., Mathur, V., McElroy, E., Sa´nchez, A. N., Raji, D., Rankin, J. L., Richardson, R., Schultz, J., West, S. M., and Whittaker, M. Ai now 2019 report, 2019. URL https://ainowinstitute. org/AI_Now_2019_Report.html.
Dantzig, G. B. Linear programming and extensions. Technical Report R-366-PR, RAND Corporation, Santa Monica, California, 1963. URL https://www.rand.org/content/dam/ rand/pubs/reports/2007/R366part1.pdf.
Dua, D. and Graff, C. UCI machine learning repository. University of California, Irvine, School of Information and Computer Sciences, 2017. URL http: //archive.ics.uci.edu/ml.
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, 2012. doi: 10.1145/2090236.2090255.
Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., and Venkatasubramanian, S. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, 2015. doi: 10.1145/2783258.2783311.
Hardt, M., Price, E., and Srebro, N. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, 2016.
Jones, E., Oliphant, T., Peterson, P., et al. SciPy: Open source scientiﬁc tools for Python, 2001. URL http: //www.scipy.org/.
Kamiran, F., Calders, T., and Pechenizkiy, M. Discrimination aware decision tree learning. In 2010 IEEE International Conference on Data Mining, 2010. doi: 10.1109/ICDM.2010.50.

Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent trade-offs in the fair determination of risk scores. In Proceedings of the 8th Innovations in Theoretical Computer Science Conference, 2017. doi: 10.4230/LIPIcs.ITCS. 2017.43.
Kraft, D. A software package for sequential quadratic programming. Technical Report DFVLR-FB 88-28, Institut fr Dynamik der Flugsysteme, Deutsche Forschungs- und Versuchsanstalt fr Luft- und Raumfahrt (DFVLR), 1988.
Kraft, D. Algorithm 733: TOMPFortran modules for optimal control calculations. ACM Transactions on Mathematical Software, 20(3), 1994. doi: 10.1145/192115. 192124.
Liu, L. T., Simchowitz, M., and Hardt, M. The implicit fairness criterion of unconstrained learning. In Proceedings of the 36th International Conference on Machine Learning, 2019.
Madras, D., Creager, E., Pitassi, T., and Zemel, R. Learning adversarially fair and transferable representations. In Proceedings of the 35th International Conference on Machine Learning, 2018.
Menon, A. K. and Williamson, R. C. The cost of fairness in binary classiﬁcation. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, 2018.
Narayanan, A. Translation tutorial: 21 fairness deﬁnitions and their politics. In Proceedings of the Conference on Fairness, Accountability and Transparency, New York, USA, 2018.
Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K. Q. On fairness and calibration. In Advances in Neural Information Processing Systems, 2017.
Samadi, S., Tantipongpipat, U., Morgenstern, J. H., Singh, M., and Vempala, S. The price of fair PCA: One extra dimension. In Advances in Neural Information Processing Systems, 2018.
Song, J., Kalluri, P., Grover, A., Zhao, S., and Ermon, S. Learning controllable fair representations. In Proceedings of Machine Learning Research, 2019.
Tan, Z., Yeom, S., Fredrikson, M., and Talwalkar, A. Learning fair representations for kernel models. In Proceedings of The 23rd International Conference on Artiﬁcial Intelligence and Statistics, 2020.
Verma, S. and Rubin, J. Fairness deﬁnitions explained. In Proceedings of the International Workshop on Software Fairness, 2018. doi: 10.1145/3194770.3194776.

FACT: A Diagnostic for Group Fairness Trade-offs
Zˇ liobaite˙, I. On the relation between accuracy and fairness in binary classiﬁcation. In Proceedings of the Second Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2015.
Zafar, M. B., Valera, I., Rodriguez, M. G., and Gummadi, K. P. Fairness constraints: Mechanisms for fair classiﬁcation. In Proceedings of the Second Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2015.
Zafar, M. B., Valera, I., Gomez Rodriguez, M., and Gummadi, K. P. Fairness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, 2017. doi: 10.1145/3038912.3052660.
Zemel, R., Wu, Y., Swersky, K., Pitassi, T., and Dwork, C. Learning fair representations. In Proceedings of the 30th International Conference on Machine Learning, 2013.
Zhao, H. and Gordon, G. J. Inherent tradeoffs in learning fair representations. In Advances in Neural Information Processing Systems, 2019.

FACT: A Diagnostic for Group Fairness Trade-offs
A. Proof of Theorem 1
A useful strategy is to solve (10) for a set of solutions, then ask if any of these solutions satisﬁes an additional fairness constraint φ(K)(z) = 0. This proof, as well as many of the ones below, illustrate this strategy in practice.
Proof. First, set K = 1 and A(0) = ACG in (10). Since v0 = v1, the matrix A is full rank and therefore admits the solution (11). Considering z0 ≥ 0 yields immediately the condition (12). Next, set K > 1. Then either z0 is a solution (which is the case when all other fairness notions are linear and linearly dependent on ACG ), or otherwise no solution exists to both (10) and φ(1)(z) = · · · = φ(K−1)(z) = 0 simultaneously.
Aconst

This theorem states that Φ ={CG} is incompatible when v0 = v1, since it is a singleton set of incompatible fairness.

The condition v0 = v1 is necessary in Theorem 1, which is reasonable to assume as we would expect the positive class

to have a higher score than the negative class in the deﬁnition of CG. We can prove the necessity of this condition by

contradiction. In the degenerate case v0 = v1 = v, Φ ={CG} is a set of compatible fairness notions. It turns out that (10)

with K = 1 is only on rank 6. Denoting i as the ith row of the matrix, we have two linear dependencies, 5 + 6 +v 1 = 2

and 7 + 8 + v 3 = 4 . There is no longer a unique solution to the (10); instead, we have a two-parameter family of

solutions,

 v(N1(1 − v) − α) 



vα







(1 − v)(N1(1 − v) − α)

1

 

(1 − v)α

 

z(α, β) =



,

N (1 − v)  v(N0(1 − v) − β) 

(14)







vβ







(1 − v)(N0(1 − v) − β)

(1 − v)β

0 ≤ α ≤ (1 − v)N1, 0 ≤ β ≤ (1 − v)N0.

Furthermore, this family of solutions satisﬁes Aconstz0 = bconst if and only if v = M0/N0 = M1/N1, i.e. the base rates are equal and furthermore the score for both bins is equal to the base rate.

B. Proof of Corollary 1

Proof. Consider the product

APCB ANCB

z0 = M1N0 − M0N1 N

v0 v1
(1−M v00)(M1−1 v1) .
(M0 −N0 )(M1 −N1 )

(15)

This product equals the zero vector (and hence satisﬁes both PCB and NCB) if and only if either of the conditions of the Corollary hold. (The last solution, v0 = 1 and v1 = 0, is inadmissible since v0 < v1 by assumption.)

C. Proof of Corollary 2

Proof. The result follows from solving

M1N0 − M0N1

ADPz0 = N 2(v1 − v0) = 0.

(16)

D. Proof of Corollary 3

Proof. The result follows from solving

φPP(z0) = v1(1 − v1) (M1 − N1v0)2 − (M0 − N0v0)2 = 0

(17)

FACT: A Diagnostic for Group Fairness Trade-offs
which is true if and only if either condition in the Corollary is true. (The last case, v1 = 0, is inadmissible by assumption.)
In addition, here is a situation of fairness “for free”, in the sense that one notion of fairness automatically implies another. Corollary 4. Consider a classiﬁer that satisﬁes CG fairness. Then, the classiﬁer also satisﬁes EFOR fairness. In other words, {CG, EFOR} is incompatible.
Proof. φEFOR(z0) = 0 vanishes identically.

E. Proof of Theorem 2
Proof. Finding the solution to φPP(z) = φEFPR(z) = φEFNR(z) = 0 and also the linear system Aconstz = bconst yields the three conditions of the Theorem.

F. CG–accuracy trade-offs

In the paper, we have only considered the case when λ = ∞ in the LAFOP: we only consider when the fairness criteria are satisﬁed exactly yielding several fairness–fairness trade-off results without heed to the accuracy of the classiﬁers. Nonetheless, recall that LAFOP allows us to express both fairness–accuracy and fairness–fairness trade-offs by introducing an accuracy objective along with a fairness regularizer. In this section, we show how the LAFOP can be used to theoretically analyze a simple fairness–accuracy trade-off. We present a small result that is relevant to the CG–accuracy trade-off considered in (Liu et al., 2019).

Theorem 3. Let α = (M0 + M1)/N be the base rate. Consider a classiﬁer that satisﬁes CG with 0 ≤ v0 < v1 ≤ 1. Then,

perfect accuracy is attained if and only if

√

v0(1 − 2v1) = α ≤ 1 , v0 − 1 ≤ 1 − 8α . (18)

1 − v1 + v0

8

4

4

Proof. The case of necessity (⇒) follows immediately from solving c · z0 = 0, where z0 is deﬁned in Theorem 1. The inequality conditions follow immediately from the constraint 0 ≤ v0 < v1 ≤ 1. The case of sufﬁciency (⇐) follows immediately from Theorem 1 and substituting the equality condition.

The condition of this theorem relates the scores v0 and v1 to the base rate of the data, thus providing simple, explicit data dependencies that are necessary and sufﬁcient.

G. Experiment Details
G.1. Optimization
For solving the optimization problems, we used solvers in the scipy package for Python (Jones et al., 2001). For linear fairness constraints, we used the simplex algorithm (Dantzig, 1963), and for other constrained optimization forms, we used sequential least-squares programming (SLSQP) solver (Kraft, 1988; 1994).

G.2. Model-agnostic multi-way fairness–accuracy trade-offs

We have only considered situations where zero or one parameter is sufﬁcient to simultaneously specify the fairness strength

for every fairness function, i.e. λ = λ0 = · · · = λK−1. In this section, we generalize this and allow each regularization

parameter to vary freely. It is then natural to consider the multilinear least-squares accuracy–fairness optimality problem

(MLAFOP): arg minz∈K (c · z)2 +

K −1 i=0

λi

A(i)z

22, where the regularization parameters λi now take different values

across each of the K fairness constraints. This allows for a general inspection of the individual effect of fairness constraints

in a group.

For instance, a three-way trade-off among EOd, DP, and accuracy can be visualized as a contour plot, similar to the one shown in Figure 4. And for general (K + 1)-way trade-offs involving K fairness constraints and accuracy, we visualize two-dimensional slices along the K + 1-dimensional surface. For example, consider a four-way trade-off between a group of three fairness deﬁnitions (DP, EOd, PCB) and accuracy. Figure 2 already showed that imposing PCB given (DP, EOd)

FACT: A Diagnostic for Group Fairness Trade-offs
Figure 4. Fairness–fairness–accuracy trade-off analysis using contour plot of accuracy with varying regularization strengths of Demographic Parity (DP) and Equalized Odds (EOd) for the unbiased synthetic dataset (left), biased synthetic dataset (middle), and Adult dataset (right). The contours show how the regularization strength of each fairness individually inﬂuence the accuracy (1 − δ) given the other (accuracy of 1.0 being the accuracy of the Bayes classiﬁer). For the unbiased synthetic data, the accuracy change along the vertical axis (DP) is practically nonexistent given EOd, while along the horizontal axis (EOd) the change is drastic. Other datasets demonstrate more complex relationships.
Figure 5. The four-way trade-off between accuracy, PCB, EOd, and DP in the biased synthetic dataset (Section 6.1). Shown here is the (1−δ) value as a function of some regularization strength λφ for some fairness function φ, while holding all other λφ s constant (accuracy of 1.0 being the accuracy of the Bayes classiﬁer). The value next to each colored line in the legend represents constant values for the ﬁxed λφ s. Sweeping through PCB while keeping DP and EOd ﬁxed (left) does not change the accuracy, whereas the other plots show multiple levels of variations. For EOd (right), the accuracy levels converge quickly to the limiting value of 0.392 as shown in Figure 2, suggesting that the accuracy is more sensitive to changes in EOd constraint strength compared to the others.
does not affect δ, which implies that PCB is the weakest in terms of its inﬂuence on δ. To get more information, for the S(B) dataset, we show in Figure 5 three cases of varying one λ for one fairness constraint while keeping the other λ values ﬁxed in MLAFOP. Sweeping through PCB condition (left) does not affect 1 − δ at ﬁxed EOd and DP levels, conﬁrming the observation from Figure 2. Sweeping through DP conditions while keeping PCB and EOd strengths ﬁxed (middle) results in a slight drop, but not big enough to make all levels to converge to values reported in Figure 2 (0.392). Sweeping through EOd while keeping PCB and DP strengths ﬁxed (right) on the other hand results in signiﬁcant changes for all levels and convergence to the value 0.392, suggesting EOd is stronger than DP in terms of its inﬂuence on changing δ. This notion of relative inﬂuence of fairness deserves further investigation, to see if these preliminary results are robust across other slices and datasets. Nonetheless, such analysis demonstrates a clear picture of how different notions of fairness interact with one another when they are to be imposed together.

FACT: A Diagnostic for Group Fairness Trade-offs
G.3. Connection to the post-processing methods for fair classiﬁcation We can explicitly rewrite the constraints in (8) using zˆ and z˜, which respectively correspond to the fairness–confusion tensor of the given pre-trained classiﬁer Yˆ and the derived fair classiﬁer Y˜ :

γ0(Y˜ ) = γ1(Y˜ ) ⇐⇒ AEODz˜ = 0

γ0(Y˜ ) ∈ P0(Yˆ ) ⇐⇒ z˜7 , z˜5 ∈ z˜7 + z˜8 z˜5 + z˜6

convhull (0, 0), zˆ7 , zˆ5 , zˆ8 , zˆ6 , (1, 1) (19)

zˆ7 + zˆ8 zˆ5 + zˆ6

zˆ7 + zˆ8 zˆ5 + zˆ6

γ1(Y˜ ) ∈ P1(Yˆ ) ⇐⇒ z˜3 , z˜1 ∈ z˜3 + z˜4 z˜1 + z˜2

convhull (0, 0), zˆ3 , zˆ1 , zˆ4 , zˆ2 , (1, 1) (20)

zˆ3 + zˆ4 zˆ1 + zˆ2

zˆ3 + zˆ4 zˆ1 + zˆ2

where the subscript i of the fairness–confusion tensor corresponds to the i-th element in their vector representation as in Section 3. By setting the objective function to be the classiﬁcation error, imposing EOd fairness constraint and the modeldependent feasibility constraints in (19) and (20), MS-LFAOP is the same optimization problem as the post-processing methods, now over the space of the fairness–confusion tensors. The FACT Pareto frontier obtained by solving MS-LAFOP therefore can assess the trade-off exhibited by any classiﬁer post-processed in such ways.
In practice, the post-processing method solves (8) by parameterizing Y˜ with two variables for each group a = 0, 1: Pr(Y˜ = 1|Yˆ = 1, A = a), Pr(Y˜ = 1|Yˆ = 0, A = a). (Hardt et al., 2016). These values are called the mixing rates, as they indicate the probability of labels that should be ﬂipped or kept for each group when post-processing the given classiﬁer Yˆ . The algorithm then randomly selects the instances for each group to ﬂip according to these mixing rates. These mixing rates can also be written in terms of the fairness–confusion tensor z˜ and zˆ, by using the fact that

Pr(Y˜ = y˜|Y = y, A = a) = Pr(Y˜ = y˜|Yˆ = 1, A = a)Pr(Yˆ = 1|Y = y, A = a)+ Pr(Y˜ = y˜|Yˆ = 0, A = a)Pr(Yˆ = 0|Y = y, A = a),

and that Pr(Y˜ = y˜|Y = y, A = a), Pr(Yˆ = yˆ|Y = y, A = a) terms are essentially what z˜ and zˆ encode. Therefore, by using z˜ obtained from the MS-LAFOP above, we can compute the mixing rates to post-process the given classiﬁer.

