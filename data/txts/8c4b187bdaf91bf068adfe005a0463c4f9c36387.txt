Beyond the Pixel-Wise Loss for Topology-Aware Delineation
Agata Mosinska1∗ Pablo Ma´rquez-Neila12 Mateusz Kozin´ski1 Pascal Fua1 1Computer Vision Laboratory, E´ cole Polytechnique Fe´de´rale de Lausanne (EPFL)
2ARTORG Center for Biomedical Engineering Research, University of Bern
{agata.mosinska, pablo.marquezneila, mateusz.kozinski, pascal.fua}@epfl.ch

arXiv:1712.02190v1 [cs.CV] 6 Dec 2017

Abstract
Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on ﬁnding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary crossentropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reﬂect the topological impact of mistakes in the ﬁnal prediction. We propose a new loss term that is aware of the higher-order topological features of linear structures. We also introduce a reﬁnement pipeline that iteratively applies the same model over the previous delineation to reﬁne the predictions at each step while keeping the number of parameters and the complexity of the model constant.
When combined with the standard pixel-wise loss, both our new loss term and our iterative reﬁnement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classiﬁer trained with the binary cross-entropy alone. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images.
1. Introduction
Automated delineation of curvilinear structures, such as those in Fig. 1(a, b), has been investigated since the inception of the ﬁeld of Computer Vision in the 1960s and 1970s. Nevertheless, despite decades of sustained effort, full automation remains elusive when the image data is noisy and the structures are complex. As in many other ﬁelds, the advent of Machine Learning techniques in general, and Deep Learning in particular, has produced substantial advances, in large part because learning features from the data makes them more robust to appearance variations [4, 15, 23, 28].
However, all new methods focus on ﬁnding either better
∗This work was supported in part by SNF

(a)

(b)

(c)

(d)

Figure 1: Linear structures. (a) Detected roads in an aerial image. (b) Detected cell membranes in an electron microscopy (EM) image. (c) Segmentation obtained after detecting neuronal membranes using [18] (d) Segmentation obtained after detecting membranes using our method. Our approach closes small gaps, which prevents much bigger topology mistakes.

features to feed a classiﬁer or more powerful deep architectures while still using a pixel-wise loss such as binary cross-entropy for training purposes. Such loss is entirely local and does not account for the very speciﬁc and sometimes complex topology of curvilinear structures penalizing all mistakes equally regardless of their inﬂuence on geometry. As a shown in Fig. 1(c,d) this is a major problem because small localized pixel-wise mistakes can result in large topological changes.
In this paper, we show that supplementing the usual

1

pixel-wise loss by a topology loss that promotes results with appropriate topological characteristics makes a big difference and yields a substantial performance increase without having to change the network architecture. In practice, we exploit the feature maps computed by a pretrained VGG19 [22] to obtain high-level descriptions that are sensitive to linear structures. We use them to compare the topological properties of the ground truth and the network predictions and estimate our topology loss.
In addition to this, we introduce an iterative reﬁnement pipeline inspired by recent detection and segmentation methods [16, 21]. We show that, unlike in these earlier methods, we can share the architecture and the parameters along all the reﬁnement steps instead of instantiating a new network each time. This keeps the number of parameters constant irrespective of the number of iterations, which is important when only a relatively small amount of training data is available, as is often the case in biomedical and other specialized applications. In our experiments, we will show that our topology loss, together with the iterative reﬁnement approach, can boost the accuracy of a deep network by up to 30 percent points as compared to the same network trained in the same way but using a standard pixel-wise loss and without iterative reﬁnement.
Our main contribution is therefore a demonstration that properly accounting for topology in the loss used to train the network is an important step in boosting performance.
2. Related Work
2.1. Detecting Linear Structures
Delineation algorithms can rely either on hand-crafted or on learned features. Optimally Oriented Flux (OOF) [10] and Multi-Dimensional Oriented Flux (MDOF) [26], its extension to irregular structures, are successful examples of the former. Their great strength is that they do not require training data but at the cost of struggling with very irregular structures at different scales along with the great variability of appearances and artifacts.
In such challenging situations, learning-based methods have an edge and several approaches have been proposed over the years. For example, the approach of [30] combines Haar wavelets with boosted trees, while that of [6] performs SVM binary classiﬁcation on spectral features. In [23], the classiﬁer is replaced by a regressor that predicts the distance to the closest centerline, which enables estimating the width of the structures.
In more recent work, Decision Trees and SVMs have been replaced by Deep Networks. For the purpose of road delineation, this was ﬁrst done in [14], directly using image patches as input to a fully connected neural net. While the patch provided some context around the linear structures, it was still relatively small due to memory limita-

tions. With the advent of Convolutional Neural Networks (CNNs), it became possible to use larger receptive ﬁelds. In [4], CNNs were used to extract features that could then be matched against words in a learned dictionary. The ﬁnal prediction was made based on the votes from nearest neighbors in the feature space. A fully-connected network was replaced by a CNN in [13] for road detection. In [12] a differentiable Intersection-over-Union loss was introduced to obtain a road segmentation, which is then used to extract graph of the road network.
In the biomedical ﬁeld, the VGG network [22] pretrained on real images has been ﬁne-tuned and augmented by specialized layers to extract blood vessels [11]. Similarly the U-Net [18], has been shown to give excellent results for biomedical image segmentation and is currently among the methods that yield the best results for neuron boundaries detection in the ISBI’12 challenge [1]. It is made of an encoder, that is, a contracting path that captures context, followed by an expanding decoder, which localizes objects. It comprises a series of convolutional layers with an either increasing or decreasing number of channels, interleaved with pooling operations for the encoding layers and up-convolutions for the decoding layers. Skip connections between corresponding layers in analysis and synthesis paths provide high-resolution features to the latter.
While effective, all these approaches rely on a standard cross entropy loss for training purposes. Since they operate on individual pixels as though they were independent of each other, they ignore higher-level statistics while scoring the output. We will see in Section 4 that this is detrimental even when using an architecture designed to produce a structured output, such as the U-Net.
Of course, topological knowledge can be imposed in the output of these linear structure detectors. For example, in [28], this is done by introducing a CRF formulation whose priors are computed on higher-order cliques of connected superpixels likely to be part of road-like structures. Unfortunately, due to the huge number of potential cliques, it requires sampling and hand-designed features. Another approach to model higher-level statistics is to represent linear structures as a sequence of short linear segments, which can be accomplished using a Marked Point Process [24] [2]. However, the inference involves Reversible Jump Markov Chain Monte Carlo, which is computationally expensive and relies on a very complex objective function. More recently, it has been shown that the delineation problem could be formulated in terms of ﬁnding an optimal subgraph in a graph of potential linear structures by solving an Integer Program [27] . However, this requires a complex pipeline whose ﬁrst step is ﬁnding points on the centerline of candidate linear structures.

2.2. Recursive Reﬁnement
Recursive reﬁnement of a segmentation has been extensively investigated. It is usually implemented as a procedure of iterative predictions [14, 17, 25], sometimes at different resolutions [20]. Such methods use the prediction from a previous iteration (and sometimes the image itself) as the input to a classiﬁer that produces the next prediction. This enables the classiﬁer to better consider the context surrounding a pixel when trying to assign a label to it and has been successfully used for delineation purposes [23].
In more recent works, the preferred approach to reﬁnement with Deep Learning is to stack several deep modules and train them in an end-to-end fashion. For example, the pose estimation network of [16] is made of eight consecutive hourglass modules and supervision is applied on the output of each one during training, which takes several days. In [21] a similar idea is used to detect neuronal membranes in electron microscopy images, but due to memory size constraints the network is limited to 3 modules. In other words, even though such end-to-end solutions are convenient, the growing number of network parameters they require can become an issue when time, memory, and available amounts of training data are limited. This problem is tackled in [8] by using a single network that moves its attention ﬁeld within the volume to be segmented. It predicts the output for the current ﬁeld of view and ﬁlls in the prediction map. Both the image and the updated prediction map are used as the input to the network in the next iteration. The network weights are then updated after each new prediction and the output prediction is thresholded before serving as an input for the next iteration in order to avoid saturating the network. The method also requires pre-computed seeds to initialize the prediction map at the ﬁrst iteration. As will be discussed in Section 3.3, our approach to reﬁnement also uses a single network and recursively reﬁnes its output. However, in training, we use a loss function that is a weighted sum of losses computed after each processing step. This enables us to accumulate the gradients and requires neither seeds for initialization nor processing the intermediate output.
3. Method
We use the fully convolutional U-Net [18] as our trainable model, as it is currently among the best and most widely used architectures for delineation and segmentation in both natural and biomedical images. The U-Net is usually trained to predict the probability of each pixel of being a linear structure using a standard pixel-wise loss. As we have already pointed out, this loss relies on local measures and does not account for the overall geometry of curvilinear structures, which is what we want to remedy.
In the remainder of this section, we ﬁrst describe our

topology-aware loss function, and we then introduce our iterated procedure to recursively reﬁne our predictions.

3.1. Notation
In the following discussion, let x ∈ RH·W be the W ×H input image, and let y ∈ {0, 1}H·W be the corresponding ground-truth labeling, with 1 indicating pixels in the curvilinear structure and 0 indicating background pixels.
Let f be our U-Net parameterized by weights w. The output of the network is an image yˆ = f (x, w) ∈ [0, 1]H·W .1 Every element of yˆ is interpreted as the probability of pixel i having label 1: yˆi ≡ p(Yi = 1 | x, w), where Yi is a random Bernoulli variable Yi ∼ Ber(yˆi).
3.2. Topology-aware loss

In ordinary image segmentation problems, the loss function used to train the network is usually the standard pixelwise binary cross-entropy (BCE):

Lbce(x, y, w) = − [(1 − yi) · log(1 − fi(x, w))

i

+yi · log fi(x, w)] .

(1)

Even though the U-Net computes a structured output and considers large neighborhoods, this loss function treats every pixel independently. It does not capture the characteristics of the topology, such as the number of connected components or number of holes. This is especially important in the delineation of thin structures: as we have seen in Fig. 1(c, d), the misclassiﬁcation of a few pixels might have a low cost in terms of the pixel-wise BCE loss, but have a large impact in the topology of the predicted results.
Therefore, we aim to introduce a penalty term in our loss function to account for this higher-order information. Instead of relying on a hand-designed metric, which is difﬁcult to model and hard to generalize to different image modalities, we leverage the knowledge that a pretrained network contains about the structures of real-world images. In particular, we use the feature maps at several layers of a VGG19 network [22] pretrained on the ImageNet dataset [19] as a description of the higher-level features of the delineations. Our new penalty term tries to minimize the differences between the VGG19 descriptors of the groundtruth images and the corresponding predicted delineations:

N Mn

Ltop(x, y, w) =

lnm(y) − lnm(f (x, w))

2 2

,

(2)

n=1 m=1

where lnm denotes the m-th feature map in the n-th layer of the pretrained VGG19 network, N is the number of convo-

lutional layers considered and Mn is the number of channels in the n-th feature map. Ltop can be understood as

1For simplicity and without loss of generality, we assume that x and yˆ have the same size. This is not the case in practice, and usually yˆ corresponds to the predictions of a cropped area of x (see [18] for details).

ground truth (a)

Ltop = 0.2279 (b)

Ltop = 0.7795 (c)

Ltop = 0.2858 (d)

Ltop = 0.9977 (e)

Figure 2: The effect of mistakes on topology loss. (a) Ground truth (b)-(e) we ﬂip 240 pixels in each prediction, so that Lbce is the same for all of them, but as we see Ltop penalizes more the cases with more small mistakes, which considerably change the structure of the prediction.

(a)

(b)

(c)

Figure 3: Examples of activations in VGG layers. (a)

Ground-truth and corresponding prediction with errors. (b) Re-

sponses of a VGG19 channel specialized in elongated structures.

(c) Responses of a VGG19 channel specialized in small connected

components. Ltop strongly encourages responses in the former

and penalizes responses in the latter.

a measurement of the difference between the higher-level visual features of the linear structures in the ground-truth and those in predicted image. These higher-level features include concepts such as connectivity or holes that are ignored by the simpler pixel-wise BCE loss. Fig. 2 shows examples where the pixel-wise loss is too weak to penalize properly a variety of errors that occur in the predicted delineations, while our loss Ltop correctly measures the topological importance of the errors in all cases: it penalizes more the mistakes that considerably change the structure of the image and those that do not resemble linear structures.
The reason behind the good performance of the VGG19 in this task can be seen in Fig. 3. Certain channels of the VGG19 layers are activated by the type of elongated structures we are interested in, while others respond strongly to small connected components. Thus, minimizing Ltop strongly penalizes generating small false positives, which do not exist in the ground-truth, and promotes the generation of elongated structures. On the other hand, the shape

of the predictions is ignored by Lbce. An interesting direction for future research would be
learning Ltop instead of deﬁning it with a pretrained model. However, this bears some resemblance to Generative Adversarial Networks (GAN) [5], whose training is known to be prone to problems such as mode collapse and instability. They are also known to be hard to train for segmentation purposes [7].
In the end, we minimize

L(x, y, w) = Lbce(x, y, w) + µLtop(x, y, w) (3)

with respect to w. µ is a scalar weighing the relative inﬂuence of both terms. We set it so that the order of magnitude of both terms is comparable. Fig. 4(a) illustrates the proposed approach.

3.3. Iterative reﬁnement

The topology loss term of Eq. 2 improves the quality of

the predictions. However, as we will see in Section 4, some

mistakes still remain. They typically show up in the form

of small gaps in lines that should be uninterrupted. We it-

eratively reﬁne the predictions to eliminate such problems.

At each iteration, the network takes both the input image

and the prediction of the previous iteration to successively

provide better predictions. In earlier works that advocate a similarly iterative approach [16, 21], a different module f k

is trained for each iteration k, thus increasing the number

of parameters of the model and making training more de-

manding in terms of the amount of required labeled data.

An interesting property of this iterative approach is that the

correct delineation y should be the ﬁxed point of each module f k, that is, feeding the correct delineation should return

the input

y = f k(x ⊕ y),

(4)

where ⊕ denotes channel concatenation and we omitted the weights of f k for simplicity. Assuming that every mod-

(a)

(b)

Figure 4: Network architecture. (a) We use the U-Net for delineation purposes. During training, both its output and the ground-truth image serve as input to a pretrained VGG network. The loss Ltop is computed from the VGG responses. The loss Lbce is computed pixelwise between the prediction and the ground-truth. (b) Our model iteratively applies the same U-Net f to produce progressive reﬁnements of the predicted delineation. The ﬁnal loss is a weighted sum of partial losses Lk computed at the end of each step.

ule f k is Lipschitz-continuous on y,2 we know that the ﬁxed-point iteration

f k(x ⊕ f k(x ⊕ f k(. . .)))

(5)

converges to y. We leverage this ﬁxed-point property to remove the necessity of training a different module at each iteration. Instead, we use the same single network f at each step of the reﬁnement pipeline, as depicted in Fig. 4(b). This makes our model much simpler and less demanding of labeled data for training. Also, it helps the network to learn a contraction map that successively improves the estimations. Our predictive model can therefore be expressed as
yˆk+1 = f (x ⊕ yˆk, w), k = 0, . . . , K − 1 , (6)

where K is the total number of iterations and yˆK the ﬁnal prediction. We initialize the model with an empty prediction yˆ0 = 0.
Instead of minimizing only the loss for the ﬁnal network output, we minimize a weighted sum of partial losses. The k-th partial model, with k ≤ K, is the model obtained from iterating Eq. 6 k times. The k-th partial loss Lk is the loss from Eq. 3 evaluated for the k-th partial model. Using this notation, we deﬁne our reﬁnement loss as a weighted sum of the partial losses

1K

Lref (x, y, w) =

k Lk(x, y, w) ,

(7)

Z

k=1

with the normalization factor Z = Kk=1 k = 12 K(K + 1). We weigh more the losses associated with the ﬁnal itera-
tions to boost the accuracy of the ﬁnal result. However, ac-
counting for the earlier losses enables the network to learn

2Lipschitz continuity is a direct consequence of the assumption that every f k will always improve the prediction of the previous iteration.

from all the mistakes it can make along the way and increases numerical stability. It also avoids having to preprocess the predictions before re-injecting them into the computation, as in [8].
In practice, we ﬁrst train a single module network, that is, for K = 1. We then increment K, retrain, and iterate. We limit K to 3 during training and testing as the results do not change signiﬁcantly for larger K values. We will show that this successfully ﬁlls in small gaps while removing background noise.
4. Results
Data. We evaluate our approach against state-of-the-art methods [13, 18, 20, 23] on three datasets featuring very different kinds of linear structures:
1. Cracks: Images of cracks in asphalt [31]. It consists of 104 training and 20 test images. As can be seen in Fig. 5, the multiple shadows and cluttered background makes their detection a challenging task. Potential applications include quality inspection and material characterization.
2. Roads: The Massachusetts Roads Dataset [13] is one of the largest publicly available collections of aerial road images, containing both urban and rural neighbourhoods, with many different kinds of roads ranging from small paths to highways. The set is split into 1108 training and 49 test images, 2 of which are shown in Fig. 6.
3. EM: We detect neuronal boundaries in Electron Microscopy images from the ISBI’12 challenge [1] (Fig. 7). There are 30 training images, with ground truth annotations, and 30 test images for which the ground-truth is withheld by the organizers. Following [20], we split the training set into 15 training and 15 test images. We report our results on this split.

Training protocol. Since the U-Net cannot handle very large images, we work with patches of 450 × 450 pixels for training. We perform data augmentation mirroring and rotating the training images by 90◦, 180◦ and 270◦. Additionally, in the EM dataset, we also apply elastic deformations as suggested in [18] to compensate for the small amount of training data. We use batch normalization for faster convergence and use current batch statistics also at the test time as suggested in [3]. We chose Adam [9] with a learning rate of 10−4 as our optimization method.
Pixel-wise metrics. Our algorithm outputs a probabilty map, which lends itself to evaluation in terms of precisionand recall-based metrics, such as the F1 score [20] and the precision-recall break-even point [13]. They are well suited for benchmarking binary segmentations, but their local character is a drawback in the presence of thin structures. Shifting a prediction even by a small distance in a direction perpendicular to the structure yields zero precision and recall, while still reasonably representing the data. We therefore evaluate the results in terms of correctness, completeness, and quality [29]. They are measures designed speciﬁcally for linear structures. They measure the similarity between predicted skeletons and ground truth-ones. They are more sensitive to alignments of centerlines than to precise locations or small width changes of the underlying structures. Potential shifts in centerline positions are handled by relaxing the notion of a true positive from being a precise coincidence of points to not exceeding a distance threshold. Correctness corresponds to relaxed precision, completeness to relaxed recall, and quality to intersectionover-union. We give precise deﬁnitions in appendix. In our experiments we use a threshold of 2 pixels for roads and cracks, and 1 for the neuronal membranes.
Topology-based metrics. The pixel-wise metrics are oblivious of topological differences between the predicted and ground-truth networks. A more topology-oriented set of measures was proposed in [28]. It involves ﬁnding the shortest path between two randomly picked connected points in the predicted network and the equivalent path in the ground-truth network. If no equivalent ground truth path exists, the former is classiﬁed as infeasible. It is classiﬁed as too-long/-short if the length of the paths differ by more than 10%, and as correct otherwise. In practice, we sample 200 paths per image, which is enough for the proportion of correct, infeasible, and too-long/-short paths to stabilize.
The organizers of the EM challenge use a performance metric called foreground-restricted random score, oriented at evaluating the preservation of separation between different cells. It measures the probability that two pixels belonging to the same cell in reality also do so in the predicted output. As shown in Fig. 1, this kind of metric is far more sensitive to topological than to pixel-wise perturbations.

VGG layers Quality Number of iterations Quality

None layer 1 layer 2 layer 3 layers 1,2,3

0.4050 0.6408 0.6427 0.6974 0.7151

OURS-NoRef OURS 1 iteration OURS 2 iterations OURS 3 iterations OURS 4 iterations

0.5580 0.5621 0.5709 0.5722 0.5727

Table 1: Testing different conﬁgurations. (Left) Quality scores for OURS-NoRef method when using different VGG layers to compute the topology loss of Eq. 2 on the Cracks dataset. (Right) Quality scores for OURS method on the EM dataset as a function of the number of reﬁnement iterations. OURS-NoRef included for

comparison.

Baselines and variants of the proposed method. We compare the results of our method to the following baselines:
• CrackTree [31] a crack detection method based on segmentation and subsequent graph construction
• MNIH [13], a neural network for road segmentation in 64 × 64 image patches,
• U-Net [18], pixel labeling using the U-Net architecture with BCE loss,
• CHM-LDNN [20], a multi-resolution recursive approach to delineating neuronal boundaries,
• Reg-AC [23], a regression-based approach to ﬁnding centerlines and reﬁning the results using autocontext.
We reproduce the results for MNIH, U-Net and Reg-AC, and report the results published in the original work for CHM-LDNN. We also perform an ablation study to isolate the individual contribution of the two main components of our approach. To this end, we compare two variants of it.
• OURS-NoRef, our approach with the topological loss of Eq. 3 but no reﬁnement steps, that is, K = 1. To extract global features we use the channels from the VGG layers conv1 2, conv2 2 and conv3 4, and set µ to 0.1 in Eq. 3.
• OURS, our complete, iterative method including the topological term and K = 3 reﬁnement steps. It is trained using the reﬁnement loss of Eq. 7 as explained in Section 3.3.
4.1. Quantitative Results
We start by identifying the best-performing conﬁguration for our method. As can be seen in Table 1(left), using all three ﬁrst layers of the VGG network to compute the topology loss yields the best results on the Cracks dataset. It also did so on the other two datasets. Similarly, we evaluated the impact of the number of improvement iterations on the resulting performance on the EM dataset, which we present in Table 1(right). The performance stabilizes after the third iteration. We therefore used three reﬁnement iterations in all further experiments. Note that in Table 1(right)

Figure 5: Cracks. From left to right: image, Reg-AC, U-Net, OURS-NoRef and OURS prediction, ground-truth.

Figure 6: Roads. From left to right: image, MNIH, U-Net, OURS-NoRef and OURS prediction, ground-truth.

Method MNIH [13] U-Net [18] OURS-NoRef OURS

P/R 0.6822 0.7460 0.7610 0.7782

Method CHM-LDNN [20] U-Net [18] OURS-NoRef OURS

F1 0.8072 0.7952 0.8140 0.8230

Table 2: Experimental results on the Roads and EM datasets. (Left) Precision-recall break-even point (P/R) for the Roads dataset. Note the results are expressed in terms of the standard precision and recall, as opposed to the relaxed measures reported in [13]. (Right) F1 scores for the EM dataset.

the ﬁrst iteration of OURS yields a result that is already better than OURS-NoRef. This shows that iterative training not only makes it possible to reﬁne the results by iterating at test time, but also yields a better standalone classiﬁer.
We report results of our comparative experiments for the three datasets in Tables 2, 3, and 4. Even without reﬁnement, our topological loss outperforms all the baselines. Reﬁnement boosts the performance yet further. The differences are greater when using the metrics speciﬁcally designed to gauge the quality of linear structures in Table 3 and even more when using the topology-based metrics in Table 4. This conﬁrms the hypothesis that our contributions improve the quality of the predictions mainly in its topological aspect. The improvement in per-pixel measures, presented in Table 2 suggests that the improved topology is

Dataset Method

Correct. Complet. Quality

Cracks

CrackTree [31] Reg-AC [23] U-Net [18] OURS-NoRef OURS

0.7900 0.1070 0.4114 0.7955 0.8844

0.9200 0.9283 0.8936 0.9208 0.9513

0.7392 0.1061 0.3924 0.7446 0.8461

Roads

Reg-AC [23] MNIH [13] U-Net [18] OURS-NoRef OURS

0.2537 0.5314 0.6227 0.6782 0.7743

0.3478 0.7517 0.7506 0.7986 0.8057

0.1719 0.4521 0.5152 0.5719 0.6524

Reg-AC [23]

0.7110 0.6647 0.5233

U-Net [18]

0.6911 0.7128 0.5406

EM

OURS-NoRef 0.7096 0.7231 0.5580

OURS

0.7227 0.7358 0.5722

Table 3: Correctness, completeness and quality scores for extracted centerlines.

correlated with better localisation of the predictions.
Finally, we submitted our results to the ISBI challenge server for the EM task. We received a foreground-restricted random score of 0.981. This puts us in ﬁrst place among algorithms relying on a single classiﬁer without additional processing. In second place is the recent method of [21], which achieves the slightly lower score of 0.978 even though it relies on a signiﬁcantly more complex base classiﬁer.

Figure 7: EM. From left to right: image, Reg-AC, U-Net, OURS-NoRef and OURS prediction, ground-truth.

Figure 8: Iterative Reﬁnement. (a) Prediction after 1, 2 and 3 reﬁnement iterations. The right-most image is the ground-truth. The red boxes highlight parts of the image where reﬁnement is closing gaps.

Dataset Cracks Roads EM

Method
Reg-AC [23] U-Net [18] OURS-NoRef OURS
Reg-AC [23] MNIH [13] U-Net [18] OURS-NoRef OURS
Reg-AC [23] U-Net [18] OURS-NoRef OURS

Correct
39.7 68.4 90.8 94.3
16.2 45.5 56.3 63.4 69.1
36.1 51.5 63.2 67.0

Infeasible
56.8 27.4 6.1 3.1
72.1 49.73 38.0 32.3 24.2
38.2 16.0 16.8 15.5

2Long 2Short
3.5 4.2 3.1 2.6
11.7 4.77 5.7 4.3
6.7
25.7 32.5 20.0 17.5

Table 4: The percentage of correct, infeasible and too-long/tooshort paths sampled from predictions and ground truth.

4.2. Qualitative Results

Figs. 5, 6, and 7 depict typical results on the three datasets. Note that adding our topology loss term and iteratively reﬁning the delineations makes our predictions more

structured and consistently eliminates false positives in the background, without losing the curvilinear structures of interest as shown in Fig. 8. For example, in the aerial images of Fig. 6, line-like structures such as roofs and rivers are ﬁltered out because they are not part of the training data, while the roads are not only preserved but also enhanced by closing small gaps. In the case of neuronal membranes, the additional topology term eliminates false positives corresponding to cell-like structures such as mitochondria.
5. Conclusion
We have introduced a new loss term that accounts for topology of curvilinear structures by exploiting their higherlevel features. We have further improved it by introducing a recursive reﬁnement stage that does not increase the number of parameters to be learned. Our approach is generic and can be used for detection of many types of linear structures including roads and cracks in natural images and neuronal membranes in micrograms. We have relied on the U-Net to demonstrate it but it could be used in conjunction with any other network architecture. In future work, we will explore the use of adversarial networks to adapt our measure

of topological similarity and learn more discriminative features.
References
[1] I. Arganda-Carreras, S. Turaga, D. Berger, D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber, D. Laptev, S. Dwivedi, J. Buhmann, T. Liu, M. Seyedhosseini, T. Tasdizen, L. Kamentsky, R. Burget, V. Uher, X. Tan, C. Sun, T. Pham, E. Bas, M. Uzunbas, A. Cardona, J. Schindelin, and S. Seung. Crowdsourcing the creation of image segmentation algorithms for connectomics. Frontiers in Neuroanatomy, page 142, 2015. 2, 5
[2] D. Chai, W. Forstner, and F. Lafarge. Recovering Line-networks in Images by Junction-Point Processes. In Conference on Computer Vision and Pattern Recognition, 2013. 2
[3] O¨ . C¸ ic¸ek, A. Abdulkadir., S. Lienkamp, T. T. Brox, and O. Ronneberger. 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. arXiv Preprint, June 2016. 6
[4] Y. Ganin and V. S. Lempitsky. N4-ﬁelds: Neural network nearest neighbor ﬁelds for image transforms. In ACCV, 2014. 1, 2
[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672– 2680, 2014. 4
[6] X. Huang and L. Zhang. Road Centreline Extraction from High-Resolution Imagery Based on Multiscale Structural Features and Support Vector Machines. International Journal of Remote Sensing, 30:1977–1987, 2009. 2
[7] S. Isola, J. Zhu, T. Zhou, and A. Efros. Image-toImage Translation with Conditional Adversarial Networks. In Conference on Computer Vision and Pattern Recognition, 2017. 4
[8] M. Januszewski, J. Maitin-Shepard, P. P. Li, J. Kornfeld, W. Denk, and V. Jain. Flood-Filling Networks. arXiv Preprint, 2016. 3, 5
[9] D. Kingma and J. Ba. Adam: A Method for Stochastic Optimisation. In International Conference for Learning Representations, 2015. 6
[10] M. Law and A. Chung. Three Dimensional Curvilinear Structure Detection Using Optimally Oriented Flux. In European Conference on Computer Vision, 2008. 2
[11] K. Maninis, J. Pont-Tuset, P. Arbela´ez, and L. V. Gool. Deep Retinal Image Understanding. In Conference on

Medical Image Computing and Computer Assisted Intervention, 2016. 2
[12] G. Mattyusand, W. L. R., and Urtasun. DeepRoadMapper: Extracting Road Topology From Aerial Images. In International Conference on Computer Vision, 2017. 2
[13] V. Mnih. Machine Learning for Aerial Image Labeling. PhD thesis, University of Toronto, 2013. 2, 5, 6, 7, 8
[14] V. Mnih and G. Hinton. Learning to Detect Roads in High-Resolution Aerial Images. In European Conference on Computer Vision, 2010. 2, 3
[15] V. Mnih and G. Hinton. Learning to Label Aerial Images from Noisy Data. In International Conference on Machine Learning, 2012. 1
[16] A. Newell, K. Yang, and J. Deng. Stacked Hourglass Networks for Human Pose Estimation. In European Conference on Computer Vision, 2016. 2, 3, 4
[17] P. Pinheiro and R. Collobert. Recurrent Neural Networks for Scenel Labelling. In International Conference on Machine Learning, 2014. 3
[18] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Conference on Medical Image Computing and Computer Assisted Intervention, 2015. 1, 2, 3, 5, 6, 7, 8
[19] O. Russakovsky, J. Deng, H. Su, J. Krause, S.Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and L. Fei-Fei. Imagenet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211–252, 2015. 3
[20] M. Seyedhosseini, M. Sajjadi, and T. Tasdizen. Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks. In International Conference on Computer Vision, 2013. 3, 5, 6, 7
[21] W. Shen, B. Wang, Y. Jiang, Y. Wang, and A. L. Yuille. Multi-stage Multi-recursive-input Fully Convolutional Networks for Neuronal Boundary Detection. 2017. 2, 3, 4, 7
[22] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In International Conference for Learning Representations, 2015. 2, 3
[23] A. Sironi, E. Turetken, V. Lepetit, and P. Fua. Multiscale Centerline Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(7):1327– 1341, 2016. 1, 2, 3, 5, 6, 7, 8

[24] R. Stoica, X. Descombes, and J. Zerubia. A Gibbs Point Process for Road Extraction from Remotely Sensed Images. International Journal of Computer Vision, 57(2):121–136, 2004. 2
[25] Z. Tu and X. Bai. Auto-Context and Its Applications to High-Level Vision Tasks and 3D Brain Image Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009. 3
[26] E. Turetken, C. Becker, P. Glowacki, F. Benmansour, and P. Fua. Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using MultiDirectional Oriented Flux. In International Conference on Computer Vision, December 2013. 2
[27] E. Turetken, F. Benmansour, B. Andres, P. Glowacki, H. Pﬁster, and P. Fua. Reconstructing Curvilinear Networks Using Path Classiﬁers and Integer Programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(12):2515–2530, 2016. 2
[28] J. Wegner, J. Montoya-Zegarra, and K. Schindler. A Higher-Order CRF Model for Road Network Extraction. In Conference on Computer Vision and Pattern Recognition, 2013. 1, 2, 6
[29] C. Wiedemann, C. Heipke, H. Mayer, and O. Jamet. Empirical Evaluation Of Automatically Extracted Road Axes. In Empirical Evaluation Techniques in Computer Vision, pages 172–187, 1998. 6
[30] S. K. Zhou, C. Tietjen, G. Soza, A. Wimmer, C. Lu, Z. Puskas, D. Liu, and D. Wu. A Learning Based Deformable Template Matching Method for Automatic Rib Centerline Extraction and Labeling in CT Images. In Conference on Computer Vision and Pattern Recognition, 2012. 2
[31] Q. Zou, Y. Cao, Q. Li, Q. Mao, and S. Wang. CrackTree: Automatic crack detection from pavement images. Pattern Recognition Letters, 33(3):227–238, 2012. 5, 6, 7

