Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured Corpora

Lidong Bing

Bhuwan Dhingra Kathryn Mazaitis Jong Hyuk Park School of Computer Science
Carnegie Mellon University, Pittsburgh, PA 15213 {lbing, bdhingra, krivard, jp1, wcohen}@cs.cmu.edu

William W. Cohen

arXiv:1606.03398v2 [cs.CL] 11 Aug 2016

Abstract
We propose a framework to improve performance of distantly-supervised relation extraction, by jointly learning to solve two related tasks: concept-instance extraction and relation extraction. We combine this with a novel use of document structure: in some small, well-structured corpora, sections can be identiﬁed that correspond to relation arguments, and distantlylabeled examples from such sections tend to have good precision. Using these as seeds we extract additional relation examples by applying label propagation on a graph composed of noisy examples extracted from a large unstructured testing corpus. Combined with the soft constraint that concept examples should have the same type as the second argument of the relation, we get signiﬁcant improvements over several state-of-the-art approaches to distantly-supervised relation extraction.
1 Introduction
In distantly-supervised information extraction (IE), a knowledge base (KB) of relation or concept instances is used to train an IE system. For example, a set of facts like sideEffect(meloxicam, stomachBleeding), interactsWith(meloxicam, ibuprofen), etc are matched against a corpus, and the matching sentences are then used to generate training data consisting of labeled relation mentions. Distant supervision is less expensive to obtain than directly supervised labels, but produces noisy training data whenever matching errors occur. Hence distant supervision is often coupled with learning methods that allow for this sort of noise, e.g., by introducing latent variables for each entity mention (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et

al., 2012); by carefully selecting the entity mentions from contexts likely to include speciﬁc KB facts (Wu and Weld, 2010); or by careful ﬁltering of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012).
Another recently-introduced approach to reducing the noise in distant supervision is to combine distant labeling with label propagation (LP) (Bing et al., 2015; Bing et al., 2016). Label propagation is a family of graph-based semi-supervised learning (SSL) methods in which instances that are “nearby” in the graph are encouraged to have similar labels. Depending on the LP method used, agreement with seed labels can be imposed as a hard constraint (Zhu et al., 2003) or a soft constraint (Lin and Cohen, 2010; Talukdar and Cohen, 2014). When seed-label agreement is a soft constraint, then LP can be viewed as a way of smoothing the seed labels, so that labels for groups of “similar” instances (i.e., instances nearby in the graph) are upweighted if they agree, and downweighted if they disagree.
In combining distant supervision with LP, one must build a graph that connects instances that are likely to have the same label. Previously, systems have constructed graphs which connect mentions appearing in the same coordinate-list structure— e.g., the underlined noun phrases in “Get medical help if you experience chest pain, weakness, or shortness of breath” (Bing et al., 2015). This approach was shown to improve performance in recognizing instances of certain medical noun-phrase (NP) categories, such as drug names and disease names. An extension of this approach (Bing et al., 2016) learned to classify NP pairs as relations, using a more complex graph structure.
This paper presents three new contributions extending this line of work. First, we combine the concept-instance extraction and relationextraction tasks, in the process greatly simplifying

Figure 1: A structured document in WebMD describing the drug meloxicam. All documents in this corpora have the same seven sections.
the relation-extraction LP step. The combination of the tasks is simple but effective. In (Bing et al., 2016), relation extraction was performed on an “entity centric” corpus, where each document is primarily concerned with a particular “title entity”, and the ﬁrst argument of each relation is always the title entity: hence relation extraction can be viewed as classiﬁcation, where an entity mention is labeled with its slot ﬁlling role, i.e., its relation to the title entity. The intuition behind combining concept extraction and relation extraction is that relation arguments are often constrained to be of a particular type; for example, the sideEffect of a drug is necessarily of the type symptom.
The second contribution is a novel use of document structure; in particular, we exploit the fact that in some small, well-structured corpora, sections can be identiﬁed that correspond fairly accurately to relation arguments. Figure 1 shows a document from such a structured corpus (discussed below) which contains sections labeled “Side Effects”. If “nausea” is distantly labeled as a sideEffect of meloxicam in this well-structured document, it is very likely to be a correct mention for the sideEffect relation. Used naively, extending a corpus with a small well-structured one needs not to lead to improvements, but when combined with LP, we show a consistent and sometimes substantial improvement in performance. We thus illustrate a novel and effective way to make use of a small well-structured corpus, a commonly available resource that is intermediate in structure between a KB and an ordinary text corpus.
The third contribution is experimental. We perform extensive experiments comparing this approach to state-of-the-art distant labeling methods based on latent variables, and show substantial improvements in two domains: the relative improvements under F1 measure are from 72% to 110% on one domain, and 22% to 30% on a second domain.
Below we present our method, in outline and

Small structured corpus DIEL pipeline

Distant labeling

mentions mentions

Freebase seeds

Target corpus DIEL pipeline
Relation classifier

Distant labeling

mentions

mentions
Build graph Bipartite graph
MRW SVM Distilled relation
examples

Figure 2: Architecture of DIEJOB.
then in detail; present experimental results; discuss related work; and ﬁnally conclude.

2 DIEJOB: Distant IE by JOint Bootstrapping
2.1 Overview
DIEJOB, our system for distantly-supervised relation extraction, is shown in Figure 2. We consider a common case, in which most information is found in relatively unstructured free text, but some smaller corpora exist that are wellstructured. DIEJOB thus assumes at least two corpora exist for the domain of interest: a large target corpus and a smaller structured corpus. Further, it assumes that every document in these two corpora is associated with a particular entity, called title entity or subject entity. Many widely-used corpora have this structure, including Wikipedia and the authoritative consumer-oriented websites we use, DailyMed and WebMD.
From each corpus, DIEJOB produces two types of mention sets: relation mention set R and concept mention set C. For the example of Figure 1, R contains a sideEffect relation mention for “stomach upset” from the ﬁrst sentence, and C may contain mentions of the Symptom concept, like “stomach upset” and “nausea” from the same sentence. The tail argument values (such as “nausea” in sideEffect(meloxicam, nausea)) of a relation are often from a particular unary concept. This is especially true in the biomedical domain, where for example, sideEffect takes instances of Symptom as the value range of its second argument. Naively, those concept mentions in C could serve as a source to generate relation examples, but not all concept mentions are relation mentions: e.g., the Symptom mentions of “confusion” and “mood changes” from “Symptoms of overdose may include: confusion, mood changes ...” are not mentions of the sideEffect

relation (or any other relation we currently extract). For the structured corpus, the relation and concept mention sets are referred to as Rs and Cs, and for the target corpus as Rt and Ct. Some special treatments (discussed in Section 2.3) are done while preparing Rs and Cs.
After producing Rs, Rt, Cs and Ct, DIEJOB builds a bipartite graph, following prior work (Lin, 2012), in which the nodes are either mentions in the four sets, or features of these mentions, with edges between a mention and its features. To distill a cleaner set of relation training examples, DIEJOB performs LP on the bipartite graph. Only the mentions from Rs are used as seed relation examples in this LP stage (because they are more accurate, see Section 2.3).
Finally the distilled relation examples are used to train an ordinary SVM classiﬁer over their extracted features. DIEJOB thus ﬁnally learns to classify an unseen mention by the relation which holds between the mention and its corresponding title entity based on features of the mention—a convenient architecture to use for large-scale extraction.
Below we will describe the components of DIEJOB and the experiments in more detail.
2.2 Relations and Corpora
Even large curated KBs are often incomplete and the situation is worse in the medical domain where the coverage of large KBs like Freebase is fairly limited. We focus on extracting instances of eight relations, deﬁned in Freebase, about drugs and diseases. The drug relations are usedToTreat, conditionsThisMayPrevent, and sideEffect, and the concept types of their second arguments are DiseaseOrMedicalCondition, DiseaseOrMedicalCondition, and Symptom. The disease relations are hasTreatment, hasSymptom, riskFactor, hasCause, and preventionFactor, with corresponding concept types as MedicalTreatment, Symptom, RiskFactor, DiseaseCause, and ConditionPreventionFactor.
We are primarily concerned with extraction from large, authoritative sources. Our target drug corpus, called DailyMed, is downloaded from dailymed.nlm.nih.gov and contains 28,590 XML documents. Our target disease corpus, called WikiDisease, is extracted from a Wikipedia dump of May 2015 and contains 8,596 disease

articles. The structured drug corpus1, called WebMD, contains 2,096 pages collected from www.webmd.com. Each page has the same sections, such as Uses and Side Effects, corresponding to usedToTreat/conditionsThisMayPrevent andsideEffect relations, respectively. The structured disease corpus, called MayoClinic, contains 1,117 pages collected from www.mayoclinic.org. Each page also has regular sections, such as Symptoms, Causes, Risk Factors, Treatments/Drugs, and Prevention, corresponding to hasSymptom, hasCause, riskFactor, hasTreatment, and preventionFactor, respectively. These corpora are all entity centric, i.e., each pages discusses a single entity.
We use GDep (Sagae and Tsujii, 2007), a dependency parser trained on GENIA Treebank, to parse the corpora, followed by a simple POS-tag based chunker to extract NPs. We also extract a list (e.g. “stomach upset, nausea, and dizziness”) for each coordinating conjunction that modiﬁes a noun. For each NP mention, we extract features (described below) from its sentence; and for each coordinate list, we extract the similar features and the NP chunks included in it. A mention not inside a list is regarded as a singleton list that contains only one item.
2.3 Mention Preparation
Relation mention sets, i.e. Rs and Rt, are prepared with distant supervision. The extracted NP mentions are distantly labeled using relation seed triples from Freebase (e.g. sideEffect(meloxican,nausea)). Speciﬁcally, we require that the title entity matches the ﬁrst argument value of the relation, and the NP mention matches the second argument value. To improve the quality of Rs, we also require that the section from which the mention was taken is relevant to the relation. E.g., a mention labeled with the sideEffect relation must appear in a section entitled Side Effects. Such constraint limits the number of mentions in Rs. In the next section, we will show how to extend this small but accurate example set to a larger training set of examples, with reasonable quality.
1It is not difﬁcult to ﬁnd such structured pages in different domains, such as scientist (http://famouschemists.org/, having “Famous For”, “Awards”, and “Discoveries” sections) and movie (http://www.imdb.com/chart/top, having “Awards”, “Plot Summary”, etc.)

The concept mentions are designed to have high recall with respect to possible argument values for a relation. For each relation r, we generate a set of concept mentions which lie in the range of r’s second argument. Following the DIEL system (Bing et al., 2015), we extract concept instances from Freebase as seeds, and extend the seed set using LP in each corpus. The reached coordinate-term lists and singleton lists (NPs) are collected as concept mentions. Thus, we get two concept mention sets: Cs from the structured corpus, and Ct from the target corpus. Note that some mentions in Cs may come from unrelated sections; for instance, Cs for the Symptom concept may contain mentions from the Overdose section, which cannot be examples of the sideEffect relation. Therefore, we ﬁlter out the mentions in Cs that are not from the appropriate section for this concept.
We emphasize that the section-speciﬁc processing is only done on the structured corpus, i.e. for Cs and Rs. Our target corpora have thousands of section titles, most of which are not related in any way to the relations being extracted. Thus the target relation mentions (Rt) and target concept mentions (Ct) are collected without considering section information.
2.4 Relation Label Propagation
With the relation mentions and the concept mentions lying in the range of the corresponding relation, we are able to distill a cleaner set of training relation examples to learn extractors. Rs contains more conﬁdent relation examples because of constraints by document structure, but it is limited in size. In contrast, the number of Rt mentions is larger, but they are noisier. In general, the degree to which Rt mentions will be useful may be domain- and corpus-speciﬁc. Cs and Ct are generated with respect to the type of the mentions, but not their relationship with the title entity: e.g., a mention in Ct corresponding to the NP “dizziness” would not be associated with the triple sideEffect(meloxican,dizziness); and indeed, dizziness might be a condition treated by, not caused by, the title entity “meloxican”. Therefore, Ct itself cannot be directly used as relation examples, however, it can serve as a resource to distill relation examples. In our experiments, Rs mentions are always used as seed relation examples in LP, but we build bipartite propagation graphs with different combinations of the four sets of mentions and study their performance.

s_100012_8 s_100065_11 s_100177_13 s_100159_13

pf=diz bw=trouble bw=patient vmod_use bw=occur l2=to_treat pf=nau sf=sea

Figure 3: A bipartite graph of relation label propagation, with unique IDs on the left and features on the right (refer to Section 2.5).

In total, we have 7 bipartite graphs, each with a different set of mentions from the following combinations: Rs ∪ Cs ∪ Rt ∪ Ct, Rs ∪ Cs ∪ Rt, Rs ∪ Cs ∪ Ct, Rs ∪ Cs, Rs ∪ Rt ∪ Ct, Rs ∪ Rt, or Rs ∪ Ct. In a bipartite graph, one set of nodes are mentions, and the other set of nodes are features of mentions. An edge is added between each feature and each mention containing that feature. The edges are TFIDF-weighted (treating the features as words and the mentions as documents). Figure 3 shows such a bipartite graph (edge weights are omitted), which has four mentions on the left-hand side, and eight features on the right-hand side.
We use an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010), which is a graph-based SSL method related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006)). MRW can be viewed simply as computing a personalized PageRank vector for each class, each of which is computed using a personalization vector that is initially uniform over the seeds, and ﬁnally assigning to each node the class associated with its highest-scoring vector. MRW’s ﬁnal scores depend on the centrality of nodes, as well as their proximity to seeds. The MRW implementation we use is based on ProPPR (Wang et al., 2013).

2.5 Classiﬁer Learning
Given the ranked mention lists of these relation labels from the above LP, we pick the top N to train binary classiﬁers, which can then be used to classify the entity mentions (singleton lists) and coordinate lists in a new document. We use the same feature generator for both mentions and lists. Shallow features include: tokens in the NPs, and character preﬁxes/sufﬁxes of these tokens; BOW from the sentence containing the NP; and tokens and bigrams from a window around the NPs. From

dependency parsing, we ﬁnd the verb which is closest ancestor of the head of current NP, all modiﬁers of this verb, and the path to this verb. For lists, the dependency features are computed relative to the head of the list.
We use SVMs (Chang and Lin, 2001) and discard singleton features, as well as the most frequent 5% of features (as a stop-wording variant). Speciﬁcally, binary classiﬁers are trained with examples of one relation as the positives, and examples of the other classes as negatives. We also add N general negative examples, randomly picked from those that are not distantly labeled by any relation. A linear kernel and default values for all other parameters are used 2. A threshold 0.5 is used to cut positive and negative predictions. If a new list or mention is not classiﬁed as positive by any classiﬁer, it is predicted as “other”.
3 Experiments
3.1 Evaluation Dataset
Our evaluation dataset contains 20 manually labeled pages, 10 pages each from the disease corpus WikiDisease and the drug corpus DailyMed. This data was originally generated in (Bing et al., 2016). The annotated text fragments are manually chunked NPs which are the second argument values of any of the eight relations considered here, with the title drug or disease entity of the corresponding document as the relation subject. The evaluation data contains 436 triples for the disease domain and 320 triples for the drug domain. A system’s task then is to extract all correct values of the second argument of a given relation from a test document.
3.2 Experimental Comparisons
The ﬁrst three baselines are distant supervision (DS) systems. They classify each testing NP mention into one of the interested relation types or “other”, using naive matching to the Freebase seed triples as distant supervision. Each sentence in the corpus is processed with the same preprocessing pipeline to detect NPs. Then, these NPs are labeled with the Freebase seed triples. The features are deﬁned and extracted in the same way as we did for DIEJOB, and binary classiﬁers are trained with the same method. The ﬁrst DS baseline, named DS Struct, only uses the section-ﬁltered examples from a structured corpus, i.e. Rs, as
2https://www.csie.ntu.edu.tw/ cjlin/libsvm/

training data. The second DS baseline, named DS Target, only uses labeled examples from the target corpus, i.e. Rt. While the third DS baseline, named DS Both, uses examples from both target corpus and structured corpus.
We also compare against two latent variable learners. The ﬁrst is MultiR (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR. The second one is MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classiﬁer to aggregate the mention level predictions into an entity pair prediction. We used the publicly available code from the authors3 for our experiments. Since these methods do not distinguish between structured and unstructured corpora, we used the union of these corpora in our experiments, and the feature set used in the bipartite graph. We found that the performance of these methods varies signiﬁcantly with the number of negative examples used during training, and hence we tuned these and other parameters4 directly on the evaluation data, and report their best performance. Another distant-supervision baseline we compare to is the Mintz++ model from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classiﬁers, and allowing multiple labels per entity pair.
We also compare with DIEBOLDS (Bing et al., 2016), which uses LP on a graph containing entity mention pairs. The graph used by DIEBOLDS is more complex than the mention-feature graph used here, in DIEJOB. One set of vertices correspond to (title-entity, mention-entity) pairs. The other set of vertices are identiﬁers for coordinate lists: a mention pair is connected with the lists from any document describing the subject, and containing the mention. Additional edges are also introduced based on document structure and BOW context features. DIEBOLDS performs label propagation from the mention pairs distantly labeled with Freebase relation triples.
3.3 Experimental Settings
We extracted triples of the eight relations from Freebase as distant labeling seeds. Speciﬁcally, if the subject of a triple matches with a drug or
3http://aiweb.cs.washington.edu/ai/raphaelh/mr/ and http://nlp.stanford.edu/software/mimlre.shtml
4Parameters include the number of epochs (for both MultiR and MIML-RE) and the number of training folds for MIML-RE.

Table 1: Extraction results on the labeled pages.

DS Struct DS Target DS Both DIEBOLDS MultiR* Mintz++* MIML-RE* DIEJOB Target DIEJOB Both DIEJOB Target* DIEJOB Both*

P
0.300 0.228 0.233 0.143 0.198 0.192 0.211 0.231 0.317 0.235 0.317

Disease R
0.300 0.335 0.353 0.372 0.333 0.353 0.360 0.337 0.333 0.339 0.333

F1
0.300 0.271 0.281 0.209 0.249 0.249 0.266 0.275 0.324 0.277 0.324

P
0.232 0.170 0.154 0.050 0.156 0.177 0.167 0.299 0.327 0.289 0.282

Drug R
0.072 0.188 0.175 0.435 0.138 0.178 0.160 0.300 0.288 0.425 0.422

F1
0.110 0.178 0.164 0.090 0.146 0.178 0.163 0.300 0.306 0.344 0.338

disease name in a corpus and its object value also appears in that document, it is extracted. For the disease domain, we get 2022, 2453, 905, 753, and 164 triples for hasTreatment, hasSymptom, riskFactor, hasCause, and preventionFactor, respectively. For the drug domain, we get 3112, 315, and 265 triples for usedToTreat, conditionsThisMayPrevent, and sideEffect, respectively.
We have two strategies to pick the top N lists for classiﬁer learning. One strategy picks the top N directly, without distinguishing if they come from the structured corpus or the target corpus. It is referred to as DIEJOB Both. The other strategy picks the top N examples only from the target corpus, and it is referred to as DIEJOB Target. Here our concern is the difference between the feature distributions of the two corpora.
We evaluate the performance of different systems from an IR perspective: a title entity (i.e., document name) and a relation together act as a query, and the extracted NPs as retrieval results.

3.4 Results on Labeled Pages
The results for precision, recall and F1 measure are given in Table 1. The results for DIEBOLDS are from (Bing et al., 2016). The systems with “*” are directly tuned on the evaluation data and should be considered as upper bounds on true performance. DIEJOB Target and DIEJOB Both are tuned with a tuning dataset (details in Section 3.5). (Note that for the disease domain, DIEJOB Both and DIEJOB Both* get the same results, because they use the same parameters, although they are tuned with different data.)
DIEJOB Both outperforms all the other systems. Compared with MultiR, Mintz++, and MIML-RE, the relative improvements under the F1 measure are 22% to 30% in the disease domain, and 72% to 110% in the drug domain. The

precision values of DIEJOB Both are much higher than previous work. For recall, DIEBOLDS and DIEJOB Both’s performance are comparable to the latent-variable systems on the disease domain and much better on the drug domain. One reason may be that our method predicts one label for a coordinate-term list (lists are common in the drug domain), which implicitly coordinates the labels of list items, while MultiR, Mintz++, and MIMLRE break a list into individual items which are predicted separately.
The precision values of DIEBOLDS are much lower than DIEJOB, especially for the drug domain. Unlike DIEJOB, DIEBOLDS builds an LP graph containing all singleton and coordinate lists of noun phrases in the corpus, which introduces many irrelevant examples. DIEBOLDS achieves the highest recall values, but in practice, it is also likely to predict a testing mention as belonging to one of the eight relations, but not “other”.
On these tasks, the simple DS baselines’ performance is competitive with MIML-RE and the other complex models. One exception is DS Struct on the drug domain, where the recall is only 0.072. This is perhaps because the total number of examples in Rs for the three drug relations is only 485, which is too small to get good recall. Interestingly, the precision of DS Struct is better than DS Target and DS Both for both domains, presumably, because of the high quality of the examples in Rs.
For the disease domain, DIEJOB Both performs better than DIEJOB Target, no matter how they are tuned (i.e. on tuning or evaluation data). This shows that the mentions from Rs and Cs of MayoClinic corpus provide good training examples. For the drug domain, DIEJOB Both and DIEJOB Target achieve similar results. This may be because DIEJOB Both is more sensitive to the difference in feature distributions of structured and target corpora, since it uses examples from the structured corpus to learn classiﬁers as well. Among the four corpora we use, WebMD, MayoClinic, and WikiDisease are written to be readable by a large audience, while DailyMed articles are more difﬁcult in terms of readability: hence the difference between the structured and unstructured corpora is larger in the drug domain.
Precision-recall curves are given in Figure 4. For the drug domain, DIEJOB’s precision is consistently better, at the same recall level, than any of the other methods. For the disease domain, our

1

1

MultiR*

MultiR*

0.8 Mintz++* 0.8 Mintz++*

MIML−RE*

MIML−RE*

DIEJOB_Both

DIEJOB_Both

0.6

0.6

Precision Precision

0.4

0.4

0.2

0.2

0

0

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0

Recall

(a) Disease domain.

0.05

0.1

0.15

0.2

0.25

0.3

0.35

Recall

(b) Drug domain.

Figure 4: Precision-recall curves.

system’s precision is generally better after the recall level 0.05.
3.5 Tuning and Variant Comparison
Here we examine the performance of different variants, and the effect of the parameter N . The performance of all graph variants on a tuning dataset (containing 10 labeled pages) is given in Figure 5. Combined with the strategies for picking top N (i.e. DIEJOB Target and DIEJOB Both), there are 13 variants: shown in Figures 5a and 5b for disease; Figures 5c and 5d for drug. Note that DIEJOB Target does not have the variant RsCs, because RsCs does not contain any examples from the target corpus.
For the disease domain, the same variant under DIEJOB Both and DIEJOB Target performs similarly, and on average, DIEJOB Both is slightly better than DIEJOB Target. For the drug domain, on average, DIEJOB Target is better than DIEJOB Both. One explanation is that the two corpora in disease domain are similar in the aspect of feature distribution, so in general, mixing the examples from them are beneﬁcial. However, the effect of such a mixture is negative for drug domain, whose structured and target corpora are dissimilar.
In Table 1, the reported results of the tuned DIEJOB Both and DIEJOB Target for the disease domain are from the variants RsCs and RsCsRt respectively, while for drug domain, both are from RsRt. One explanation could be: (1) if the structured corpus is similar to the target corpus, it is better to use DIEJOB Both, and including examples of the structured corpus (e.g., RsCs and RsCsRt, both have Cs used) generally performs well with a larger N value; (2) if the structured and target corpora are dissimilar, DIEJOB Target is better and RsRt has an advantage over other variants where the main focus is distilling good training examples from Rt and a smaller number

of top N examples is preferred.
4 Related Work
To overcome the noise in distantly-labeled examples, (Riedel et al., 2010) introduced an “at least one” heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation. MultiR (Hoffmann et al., 2011) and MIMLRE (Surdeanu et al., 2012) extend this approach to support multiple relations expressed by different sentences in a bag. Unlike these approaches, DIEJOB improves the quality of training data with a bootstrapping step before feeding the noisy examples into a learner, by using the conﬁdent examples from a structured corpus as seeds. The beneﬁt of this step is two-fold. First, it distills the distantly-labeled examples by propagating labels from good seed examples, and downweights the noisy ones. Second, the propagation will walk to more relation examples in the concept mention set that cannot be distantly labeled with triples from knowledge bases.
Document structure was previously explored by (Bing et al., 2016), which used the structure to enrich an LP graph by adding coupling edges between mentions in the same section of particular documents. In this work, we explore the semantic association between section titles and relation arguments. Furthermore, we perform a joint bootstrapping on relation and type mentions to collect training examples with better quality. Technically, the propagation graphs used are different: DIEJOB’s graph has carefully produced mention nodes (from those four sets) and their feature nodes, while DIEBOLDS’s graph has triple nodes (i.e., subject-NP pairs) and all singleton and coordinate lists of noun phrases of the corpora. Accordingly, their propagation seeds are different: DIEJOB uses conﬁdent examples as seeds (labeled from particular sections of a structured

F1 value

R sCs Rt Ct

R sCs Rt

Rs Cs Ct

0.4

0.33 0.2

0.26 0.1

0.19

0

200

400

800

1200

0.12

0.05

200

400

800

1200

2000 3000

5000

8000

Top N

(a) Variants of DIEJOB Both for disease domain.
0.5

0.42

0.34

0.26

0.18

0.1 200 400 800 1200 2000 3000 5000 8000 15000 Top N
(c) Variants of DIEJOB Both for drug domain.

Rs Rt Ct 0.35

RsRt

Rs Ct

Rs Cs

0.31

F1 value

0.27

0.23 2000 Top N 0.19

3000

5000

8000

0.15

200

400

800

1200

2000 3000

5000

8000

Top N

(b) Variants of DIEJOB Target for disease domain.
0.5

0.4

F1 value

0.3

0.2

0.1

0 200 400 800 1200 2000 3000 5000 8000 15000
Top N
(d) Variants of DIEJOB Target for drug domain.

F1 value

Figure 5: Performance of DIEJOB variants and effect of the parameter N.

corpus) to propagate labels to more examples via feature similarity, while DIEBOLDS directly uses Freebase triples as seeds and propagates labels through edges built from coordinate lists and sections.

6 Acknowledgments
This work was funded by grants from Baidu USA and Google, and the research grant IIS-1250956 from NSF.

In the classic bootstrap learning scheme (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007), a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances. Then in an iterative fashion, new instances are used to extract more patterns. DIEJOB departs from earlier bootstrapping methods in combining label propagation with a standard classiﬁcation learner, and it can improve the quality of distant examples and collect new examples simultaneously.
5 Conclusions
We proposed the DIEJOB framework to generate good examples for distantly-supervised IE. It exploits the document structure of a small wellstructured corpus to collect seed relation examples, and it also collects concept mentions that could be the second argument values of relations. DIEJOB then conducts label propagation to ﬁnd mentions that can be conﬁdently used as training examples to train classiﬁers for labeling new entity mentions. The experimental results show that this approach consistently and signiﬁcantly outperforms state-of-the-art approaches.

References
[Agichtein and Gravano2000] Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries.
[Bing et al.2015] Lidong Bing, Sneha Chaudhari, Richard C Wang, and William W. Cohen. 2015. Improving distant supervision for information extraction using label propagation through lists. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 524–529, Lisbon, Portugal, September. Association for Computational Linguistics.
[Bing et al.2016] Lidong Bing, Mingyang Ling, Richard Wang, and William W. Cohen. 2016. Distant ie by bootstrapping using lists and document structure. CoRR, abs/1601.00620.
[Bunescu and Mooney2007] Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic.
[Chang and Lin2001] Chih-Chung Chang and ChihJen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

[Haveliwala et al.2003] Taher Haveliwala, Sepandar Kamvar, Ar Kamvar, and Glen Jeh. 2003. An analytical comparison of approaches to personalizing pagerank. Technical report, Stanford University.
[Hoffmann et al.2011] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541–550. Association for Computational Linguistics.
[Lin and Cohen2010] Frank Lin and William W. Cohen. 2010. Semi-supervised classiﬁcation of network data using very few labels. In Nasrullah Memon and Reda Alhajj, editors, ASONAM, pages 192–199. IEEE Computer Society.
[Lin2012] Frank Lin. 2012. Scalable methods for graph-based unsupervised and semi-supervised learning. Ph.D. thesis, Carnegie Mellon University.
[Mintz et al.2009] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.
[Movshovitz-Attias and Cohen2012] Dana Movshovitz-Attias and William W. Cohen. 2012. Bootstrapping biomedical ontologies for scientiﬁc text using nell. In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing, BioNLP ’12, pages 11–19, Stroudsburg, PA, USA. Association for Computational Linguistics.
[Riedel et al.2010] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148–163. Springer.
[Riloff and Jones1999] Ellen Riloff and Rosie Jones. 1999. Learning Dictionaries for Information Extraction by Multi-level Boot-strapping. In Proceedings of the Sixteenth National Conference on Artiﬁcial Intelligence, pages 1044–1049.

[Sagae and Tsujii2007] K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Proceedings of the CoNLL 2007 Shared Task in the Joint Conferences on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL’07 shared task), pages 1044–1050.
[Surdeanu et al.2012] Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 455– 465, Stroudsburg, PA, USA. Association for Computational Linguistics.
[Talukdar and Cohen2014] Partha Pratim Talukdar and William W. Cohen. 2014. Scaling graph-based semi supervised learning to large number of labels using count-min sketch. In Proceedings of the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland, April 22-25, 2014, pages 940–947.
[Tong et al.2006] Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk with restart and its applications. In ICDM, pages 613–622. IEEE Computer Society.
[Wang et al.2013] William Yang Wang, Kathryn Mazaitis, and William W Cohen. 2013. Programming with personalized pagerank: a locally groundable ﬁrst-order probabilistic logic. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 2129–2138. ACM.
[Wu and Weld2010] Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118– 127. Association for Computational Linguistics.
[Zhu et al.2003] X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In Proceedings of ICML-03, the 20th International Conference on Machine Learning.

