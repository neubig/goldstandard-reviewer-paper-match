Adaptive Gradient Methods Converge Faster with Over-Parameterization (but you should do a line-search)

arXiv:2006.06835v3 [cs.LG] 18 Feb 2021

Sharan Vaswani 1 Issam Laradji 2 Frederik Kunstner 3 Si Yi Meng 4 Mark Schmidt 3 Simon Lacoste-Julien 5

Abstract
Adaptive gradient methods are typically used for training over-parameterized models. To better understand their behaviour, we study a simplistic setting – smooth, convex losses with models overparameterized enough to interpolate the data. In this setting, we prove that AMSGrad with constant step-size and momentum converges to the minimizer at a faster O(1/T ) rate. When interpolation is only approximately satisﬁed, constant step-size AMSGrad converges to a neighbourhood of the solution at the same rate, while AdaGrad is robust to the violation of interpolation. However, even for simple convex problems satisfying interpolation, the empirical performance of both methods heavily depends on the step-size and requires tuning, questioning their adaptivity. We alleviate this problem by automatically determining the step-size using stochastic line-search or Polyak step-sizes. With these techniques, we prove that both AdaGrad and AMSGrad retain their convergence guarantees, without needing to know problem-dependent constants. Empirically, we demonstrate that these techniques improve the convergence and generalization of adaptive gradient methods across tasks, from binary classiﬁcation with kernel mappings to multi-class classiﬁcation with deep networks.
1 Introduction
Adaptive gradient methods such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), Adam (Kingma & Ba, 2015), and AMSGrad (Reddi et al., 2018) are popular optimizers for training deep neural networks (Goodfellow et al., 2016). These methods scale well and exhibit good performance across problems, making them the default choice for many
1University of Alberta 2Mila, McGill University 3University of British Columbia 4Cornell University 5 Mila, Université de Montréal. Correspondence to: Sharan Vaswani <vaswani.sharan@gmail.com>.

machine learning applications. Theoretically, these methods are usually studied in the non-smooth, online convex optimization setting (Duchi et al., 2011; Reddi et al., 2018) with recent extensions to the strongly-convex (Mukkamala & Hein, 2017; Wang et al., 2020; Xie et al., 2020) and nonconvex settings (Li & Orabona, 2019; Ward et al., 2019; Zhou et al., 2018; Chen et al., 2019; Wu et al., 2019; Défossez et al., 2020; Staib et al., 2019). Further, an onlineto-batch reduction gives guarantees similar to stochastic gradient descent (SGD) in the ofﬂine setting (Cesa-Bianchi et al., 2004; Hazan & Kale, 2014; Levy et al., 2018).
While constant step-size AdaGrad has been shown to be “universal” as it converges with any step-size in the stochastic smooth and non-smooth settings (Levy et al., 2018), its empirical performance is often disappointing when training deep models (Kingma & Ba, 2015). Improving the empirical performance was indeed the main motivation behind Adam and other methods (Tieleman & Hinton, 2012; Zeiler, 2012) that followed AdaGrad. However, there are several discrepancies between the theory and application of these methods. Although the theory advocates for using decreasing stepsizes for Adam, AMSGrad and its variants (Kingma & Ba, 2015; Reddi et al., 2018), a constant step-size is typically used in practice. Similarly, the standard analyses of these methods requires a decreasing momentum parameter (Reddi et al., 2018), which is also ﬁxed in practice. Consequently, there are no theoretical results corroborating the adaptivity to the step-size for the Adam variants used in practice.
Furthermore, adaptive gradient methods are typically used to train highly expressive, over-parameterized models (Zhang et al., 2017; Liang & Rakhlin, 2020) capable of interpolating the data. However, the standard theoretical analyses do not take advantage of these additional structural properties. A recent line of work (Schmidt & Le Roux, 2013; Jain et al., 2018; Ma et al., 2018; Cevher & Vu˜, 2019; Vaswani et al., 2019a;b; Wu et al., 2019; Liu & Belkin, 2020; Loizou et al., 2020) focuses on the convergence of SGD in this interpolation setting. For a ﬁnite-sum of loss functions, interpolation implies that all the functions in the sum are minimized at the same solution. Under this additional assumption, these works show that constant step-size SGD converges at a faster rate for both convex and non-convex smooth functions.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

1.1 Background and Contributions
As a ﬁrst step to reconcile the theory and practice of adaptive gradient methods, we focus on their convergence in a simplistic setting - minimizing smooth, convex loss functions using models capable of interpolating the data. We study two practical methods - constant step-size AdaGrad, and AMSGrad with a constant step-size and constant momentum. In particular, we make the contributions below.
Constant step-size AdaGrad. For smooth, convex functions, Levy et al. (2018) prove that constant step-size AdaGrad adapts to the smoothness and gradient noise, resulting in an O(1/T + ζ/√T ) convergence rate, where T is the number of iterations and ζ2 is a global bound on the variance in the stochastic gradients. This convergence rate matches that of SGD under the same setting (Moulines & Bach, 2011).
Contribution. In Section 3.1, we show that constant stepsize AdaGrad also adapts to the degree of interpolation and prove an O(1/T + σ/√T ) rate, where σ is the extent to which interpolation is violated. Similarly to the result of Levy et al. (2018), this holds for any bounded constant step-size.
AMSGrad with constant step-size and momentum. Unlike AdaGrad, the preconditioner for AMSGrad does not have nice structural properties (Défossez et al., 2020), making it difﬁcult to prove strong guarantees. To analyze the convergence of AMSGrad, we make the simplifying assumption that its corresponding preconditioner remains bounded.
Contribution. With this additional assumption, we show that AMSGrad with a constant step-size and momentum converges to the minimizer at a O(1/T ) rate under interpolation (Section 3.2). Unlike AdaGrad, this result requires a speciﬁc range of step-sizes that depend on the smoothness of the problem. In general, constant step-size AMSGrad converges to a neighbourhood of the solution, attaining an O(1/T + σ2) rate, matching the rate of constant step-size SGD (Schmidt & Le Roux, 2013; Vaswani et al., 2019a). For over-parameterized models, σ2 ≈ 0 and this result provides some justiﬁcation for the faster (O(1/T ) vs. O(1/√T )) convergence of the AMSGrad variants used in practice.
Sensitivity of AdaGrad and AMSGrad. Although AdaGrad converges at the same asymptotic rate for any step-size, it is unclear how the choice of step-size affects its practical performance. On the other hand, our theoretical results for convex minimization indicate that AMSGrad is sensitive to the step-size, converging only for a speciﬁc range that depends on typically unknown problem-dependent constants.
Contribution. In Section 4, we empirically demonstrate this sensitivity to the step-size for both methods. In particular, we show that even for a convex problem satisfying interpolation, such as logistic regression on linearly separable data, the choice of step-size has a big impact on the performance of

both AdaGrad and AMSGrad, questioning their adaptivity.
AdaGrad and AMSGrad with an adaptive step-size. To improve the robustness to the step-size, we use recent techniques (Vaswani et al., 2019a; Loizou et al., 2020) that automatically determine the step-size for each iteration of SGD. These works use stochastic variants of the classical Armijo line-search (Armijo, 1966) and Polyak step-size (Polyak, 1963) and prove their convergence under interpolation.
Contribution. In Section 5, we modify these techniques for their use with adaptive gradient methods. In particular, we show that a variant of stochastic line-search (SLS) can be used to set the step-size in each iteration of AdaGrad. SLS enables AdaGrad to adapt to the smoothness of the underlying function, while retaining its favourable convergence properties (Section 5.1). Under the same bounded preconditioner assumption, we prove that AMSGrad used with a variant of SLS or stochastic Polyak step-size (SPS) can match the convergence rate of its constant step-size counterpart, but without requiring the knowledge of problem-dependent constants (Section 5.2). For the logistic regression example considered in Section 4, we observe that using SLS/SPS improves the convergence of both AdaGrad and AMSGrad, matching or out-performing the best constant step-size.
Large-scale experiments. To demonstrate the empirical advantage of SLS/SPS beyond convex minimization, we consider both convex and non-convex tasks, ranging from binary classiﬁcation with a kernel mapping to multi-class classiﬁcation with standard deep network architectures (Section 6). We benchmark the performance of AdaGrad and AMSGrad equipped with SLS, and compare against tuned Adam and recently proposed variants (Luo et al., 2019; Liu et al., 2020). To disentangle the effects of the step-size and the adaptive preconditioner, we compare against SGD using SLS (Vaswani et al., 2019b) and SPS (Loizou et al., 2020). We ﬁnd that both the step-size and the adaptive preconditioner contribute to good performance, with the SLS variants of AdaGrad and AMSGrad consistently outperforming other methods. Furthermore, for the experiments with deep neural networks, these variants generalize better than SGD, demonstrating the effect of the step-size in the generalization performance (Nar & Sastry, 2018).
2 Problem formulation
Adaptive methods are still poorly understood, and stateof-the-art analyses (Levy et al., 2018; Reddi et al., 2018; Alacaoglu et al., 2020) do not show an improvement over stochastic gradient descent in the worst-case. The objective of our theoretical analysis is to better understand the interplay between over-parameterization, step-sizes and momentum. To this end, we make the simplifying assumptions described in this section.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Table 1. Adaptive preconditioners (analyzed methods are bolded), with G0 = 0 and β1, β2 ∈ [0, 1). In practice, a small I is added to ensure Ak 0. *: We use the PyTorch implementation in experiments which includes bias correction.

Optimizer Gk

(∇k := ∇fik (wk))

Ak

β

AdaGrad Gk−1 + diag(∇k∇k )

G1k/2 0

RMSProp β2Gk−1 + (1 − β2) diag(∇k∇k )

G1k/2

0

Adam

(β2Gk−1 + (1 − β2) diag(∇k∇k ))/(1 − β2k) G1k/2

β1

AMSGrad* (β2Gk−1 + (1 − β2) diag(∇k∇k ))/(1 − β2k) max{Ak−1, G1k/2} β1

We consider the unconstrained minimization of an objec-

tive f : Rd → R with a ﬁnite-sum structure, f (w) =

1 n

n i=1

fi

(w).

In

supervised

learning,

n

represents

the

number of training examples, and fi is the loss of training

example i. Although we focus on the ﬁnite-sum setting, our

results can be generalized to the online optimization setting.

We assume f and each fi are differentiable, convex, and lower-bounded by f ∗ and fi∗, respectively. Furthermore, we assume that each function fi in the ﬁnite-sum is Li-smooth,

implying that f is Lmax-smooth with Lmax = maxi Li. We

include formal deﬁnitions of these properties in Appendix A.

We also assume that the iterates remain bounded in a ball of radius D around a global minimizer, wk − w∗ ≤ D for all wk (Ahn et al., 2020). We remark that the bounded iterates assumption simpliﬁes the analysis but is not essential. Indeed, similarly to (Reddi et al., 2018; Duchi et al., 2011; Levy et al., 2018) we can consider constrained minimization over a compact, feasible set F with a bounded diameter D, and our theoretical results can be easily extended to include an explicit projection step. Without an explicit projection step, we believe it is possible to prove that the iterates do remain bounded with high-probability (Mertikopoulos et al., 2020). This would complicate the analysis without changing the conclusions, and we thus leave this for future work.

The interpolation assumption means that the gradient of each fi in the ﬁnite-sum converges to zero at an optimum. If the overall objective f is minimized at w∗, ∇f (w∗) = 0, then for all fi we have ∇fi(w∗) = 0. The interpolation condition can be exactly satisﬁed for numerous machine
learning models such as linear classiﬁcation on a separable
dataset, non-parametric kernel regression without regulariza-
tion (Belkin et al., 2019; Liang & Rakhlin, 2020) and over-
parameterized deep neural networks (Zhang et al., 2017).
Since exact interpolation is a relatively strong assumption,
we consider a weaker version (Loizou et al., 2020) of it.
Speciﬁcally, we measure the extent to which interpolation is
violated by the disagreement between the minimum overall function value f ∗ and the minimum value of each individual functions fi∗, σ2 := Ei[f ∗ − fi∗] ∈ [0, ∞). Interpolation is said to be exactly satisﬁed if σ2 = 0. Since σ2 only depends on the minimum function values, the minimizer w∗ of f need not be unique for σ2 to be uniquely deﬁned.

We ﬁrst consider the update for a generic adaptive gradient method at iteration k. For a preconditioner matrix Ak and a constant momentum parameter β ∈ [0, 1), the update is

wk+1 = wk − ηk A− k 1mk

mk = βmk−1 + (1 − β)∇fik (wk).

(1)

Here, ∇fik (wk) is the stochastic gradient of a randomly chosen function fik , and ηk is the step-size. Adaptive gradient methods typically differ in how their preconditioners are constructed and whether or not they include the momentum term βmk−1 (see Table 1 for a list of common methods). Both RMSProp and Adam maintain an exponential moving average of past stochastic gradients, but as Reddi et al. (2018) pointed out, unlike AdaGrad, the corresponding preconditioners do not guarantee that Ak+1 Ak and the resulting per-dimension step-sizes do not go to zero. This can lead to large ﬂuctuations in the effective step-size and prevent these methods from converging. To mitigate this problem, they proposed AMSGrad, which ensures Ak+1 Ak and the convergence of iterates to a stationary point. Consequently, our theoretical results focus on AdaGrad and AMSGrad, though we considered Adam in our experiments.

Although our theory holds for both the full matrix and diagonal variants of these methods, we use only the latter in experiments for scalability. The diagonal variants perform a per-dimension scaling of the gradient and avoid computing full matrix inverses, so their per-iteration cost is the same as SGD, although with an additional O(d) memory. Unlike AdaGrad, the AMSGrad preconditioner does not possess nice structural properties. To prove convergence results for AMSGrad, we assume that the preconditioners are wellbehaved in the sense that their eigenvalues are bounded in an interval [amin, amax]. This is a common assumption in the analysis of adaptive methods, and a small diagonal matrix ( Id) is typically added to the preconditioners to ensure they remain positive deﬁnite. For diagonal preconditioners, this boundedness property is easy to verify, and it is also inexpensive to maintain the desired range by an explicit projection. Furthermore, observe that even though the stochastic gradients can become zero due to over-parameterization, both the max operation and diagonal matrix used in constructing the AMSGrad preconditioner ensures its positive deﬁniteness.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

3 Convergence with a constant step-size
In this section, we analyze the convergence of constant stepsize AdaGrad (Section 3.1) and AMSGrad with a constant step-size and momentum parameter (Section 3.2).

3.1 Constant step-size AdaGrad

For smooth, convex objectives, Levy et al. (2018) showed that AdaGrad converges at a rate O(1/T + ζ/√T ), where ζ2 = supw Ei[ ∇f (w) − ∇fi(w) 2] is a uniform bound
on the variance of the stochastic gradients. We show that

constant step-size AdaGrad achieves the O(1/T ) rate when interpolation is exactly satisﬁed (σ2 = 0) and a slower

convergence to the minimizer if interpolation is violated.

The proofs for this section are in Appendix C.

Theorem 1 (Constant step-size AdaGrad). Assuming (i)

convexity, (ii) Lmax-smoothness of each fi, and (iii)

bounded iterates, AdaGrad with a constant step-size η and

uniform

averaging

w¯T

=

1 T

T k=1

wk

,

converges

at

a

rate

√

E[f (w¯T ) − f ∗] ≤

α +

ασ √,

T

T

where α = 12 Dη2 + 2η 2dLmax.

Theorem 1 shows that AdaGrad is robust to the violation of interpolation and converges to the minimizer at the desired rate for any reasonable step-size. In the over-parameterized setting, σ2 can be much smaller than ζ2 (Zhang & Zhou, 2019), implying a faster convergence compared to the result of Levy et al. (2018). In particular, when interpolation is satisﬁed, σ2 = 0 while ζ2 can still be large.

In the online convex optimization framework, for smooth functions, a similar proof technique can be used to show that AdaGrad incurs only O(1) regr√et when interpolation is exactly satisﬁed and retains its O( T )-regret guarantee in the general setting (Theorem 5 in Appendix C.2). A similar ﬁrstorder regret bound was proven independently by Orabona (2019), for a scalar version of AdaGrad. Our theorem generalizes their result to use a matrix preconditioner.

3.2 Constant step-size AMSGrad
As explained earlier, in order to analyze the convergence of AMSGrad, we assume that the effect of the preconditioning is bounded, meaning that the eigenvalues of Ak lie in the [amin, amax] range. This is a common assumption in the analysis of preconditioned gradient and second-order methods (Yu et al., 2010; Berahas et al., 2016; Moritz et al., 2016; Bollapragada et al., 2018; Meng et al., 2020). We consider AMSGrad without bias correction, as its effect is minimal after the ﬁrst few iterations. The proofs for this section are in Appendix D and Appendix E.
The original analysis of AMSGrad (Reddi et al., 2018) uses a

decreasing step-size and a decreasing momentum parameter.

Assuming a bounded diameter of the feasible set, bounded

maximum eigenvalue of the positive-deﬁnite preconditioner and bounded gradients, it shows an O(D2amax/√T ) conver-

gence for AMSGrad in the non-smooth, convex setting. Un-

der the same assumptions, Alacaoglu et al. (2020) showed

that this analysis is loose and that AMSGrad does not re-

quire a decreasing momentum parameter (but still requires a decreasing step-size) to obtain the O(1/√T ) rate. However,

in practice, AMSGrad is typically used with both a constant

step-size and momentum parameter. Next, we analyze the

convergence for this commonly-used variant of AMSGrad.

Theorem 2. Under the same assumptions as Theorem 1,

and assuming (iv) non-decreasing preconditioners (v)

bounded eigenvalues in the [amin, amax] interval, where

κ = / , amax amin AMSGrad with β ∈ [0, 1), constant step-size η = 11−+ββ 2aLmmianx and uniform averaging converges at a rate,

∗

1 + β 2 2LmaxD2dκ 2

E[f (w¯T ) − f ] ≤ 1 − β

+σ . T

In contrast to (Reddi et al., 2018; Alacaoglu et al., 2020), the above theorem considers the smooth setting and does not require the bounded gradient assumption. However, our rate has an additional dependence on amin, the minimum eigenvalue of the preconditioner. Unless we take advantage of some structure in the AMSGrad preconditioner, we believe that such a dependence on amin is unavoidable in the constant step-size setting. For the common diagonal variant of AMSGrad, we can project the preconditioner entries onto a reasonable [amin, amax] interval, and ensure κ is small.

When σ = 0, we obtain a faster O(1/T ) convergence to the minimizer. When interpolation is only approximately satisﬁed, unlike AdaGrad which converges to the minimizer at a slower rate, AMSGrad converges to a neighbourhood whose size depends on σ2. A similar distinction between the convergence of constant step-size Adam (or AMSGrad) vs. AdaGrad has also been discussed in the non-convex setting (Défossez et al., 2020). We observe that the “noise” σ2 is not ampliﬁed because of the non-decreasing momentum (or step-size), in contrast to the stochastic accelerated gradient method (Devolder et al., 2014; Vaswani et al., 2019a).

Since AMSGrad is typically used for optimizing large,

over-parameterized models, the violation of interpolation is

small and σ2 ≈ 0. Another reason that explains the prac-

tical effectiveness of AMSGrad is the use of large batch-

sizes that result in a smaller σ2. To understand the effect

of the batch-size, note that if we use a batch-size of b,

σb2

:=

EB;|B|=b

[fB

(w∗)

−

fB

(w∗ B

)]

where

wB∗

is

the

mini-

mizer of a batch B of training examples. By convexity, σb2 ≤ E[ w∗ − w∗ ∇fB(w∗) ]. If we assume that the distance
B

w∗ − w∗ B

is bounded, σb2 ∝ E ∇fB(w∗) . Since the ex-

amples in each batch are sampled with replacement, using

Train loss (log) Train loss (log)

101 100 10 1 10 2 0

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Margin:0.01

101

100

10 1

10 2

50 E1p0oc0h 150 200 0
(a) AdaGrad

Margin:0.05
Adagrad Default Adagrad Adagrad + Lipschitz LS Adagrad + Armijo LS 50 E1p0oc0h 150 200

101 100 10 1 10 2
0

Margin:0.01

Margin:0.05
101

10 1

10 3

10 5

Amsgrad

10 7

Default Amsgrad Amsgrad + SLS

50 E1p0oc0h 150 200

0

50 E1p0oc0h 150 200

(b) AMSGrad

Figure 1. Logistic regression on a linearly-separable synthetic dataset to show the impact of the step-size on performance. We compare AdaGrad and AMSGrad (ﬁxed momentum) with varying step-sizes, including the default in PyTorch, against SLS. AdaGrad and AMSGrad are sensitive to the choice of step-size, while the SLS variants of both methods match or out-perform the best constant step-size.

the

bounds

in

(Lohr,

2009),

σb2

∝

n−b nb

∇fi(w∗) , showing

that σb2 shrinks as the batch-size becomes larger, becoming

zero for the full-batch variant. With over-parameterization

and large batch-sizes, σb2 is small enough for constant stepsize AMSGrad to be useful for machine learning tasks that

do not require exact convergence to the solution.

Finally, observe that the constant step-size required for the above result depends on Lmax, which is typically unknown and difﬁcult to estimate. In the next section, we empirically demonstrate the sensitivity of AMSGrad to its step-size.

4 Sensitivity of AdaGrad and AMSGrad
In this section, we empirically demonstrate that the commonly-used constant step-size variants of AdaGrad and AMSGrad are sensitive to their choice of step-size, even for convex problems satisfying interpolation. We use their PyTorch implementations (Paszke et al., 2019) on a binary classiﬁcation task with (unregularized) logistic regression. Following the protocol of Meng et al. (2020), we generate a linearly-separable dataset with n = 103 examples, ensuring interpolation is satisﬁed, and d = 20 features with varying margins. For AdaGrad and AMSGrad with a batch-size of 100, we show the training loss for a grid of step-sizes in the [103, 10−3] range along with their default step-size in PyTorch. In Fig. 1, we observe a large variance across step-sizes and poor performance of the default step-size. The best performing variant of AdaGrad and AMSGrad has a step-size of order 102, which is very different from the default step-size used in practice. The above experiment demonstrates the inability of these methods to adapt to the properties of the function, questioning their robustness. In the next section, we design techniques to enable both AdaGrad and AMSGrad to adapt to the objective’s smoothness.

5 Convergence with adaptive step-sizes
In this section, we modify the stochastic line-search (SLS) (Vaswani et al., 2019b) and stochastic Polyak’s step-

size (SPS) (Loizou et al., 2020) techniques to automatically determine the step-size in each iteration of AdaGrad (Section 5.1) and AMSGrad (Section 5.2).
5.1 Adaptive step-size AdaGrad
From Fig. 1, we have seen that the performance of AdaGrad depends heavily on choosing the correct step-size. To alleviate this problem, we use a conservative Lipschitz line-search that sets the step-size on the ﬂy, improving the empirical performance of AdaGrad (Section 6), while retaining its favourable convergence guarantees.
Lipschitz line-search: At each iteration k, this line-search selects a step-size ηk ≤ ηmax that satisﬁes the property
fik (wk − ηk∇fik (wk)) ≤ fik (wk) − c ηk ∇fik (wk) 2 . (2)
Here, ηmax is the upper-bound on the step-size. The resulting step-size is used in the standard AdaGrad update (Eq. 1). To ﬁnd an acceptable step, our results use a backtracking line-search, described in Appendix F. For simplicity, the theoretical results assume access to the largest step-size that satisﬁes the above condition.1 Here, c is a hyper-parameter determined theoretically and set to 1/2 in our experiments.
We refer to it as the Lipschitz line-search as it is only used to estimate the local Lipschitz constant. Unlike the classical Armijo line-search for preconditioned gradient descent (Armijo, 1966), the line-search in Eq. (2) is in the gradient direction, even though the update is in the preconditioned direction. Intuitively, the Lipschitz line-search enables AdaGrad to take larger steps at iterates where the underlying function is smoother.
By choosing ηmax = ηk−1 for iteration k, we obtain a conservative variant of the Lipschitz line-search. The conservative Lipschitz line-search imposes a non-increasing
1The difference between the exact and backtracking line-search is minimal, and the bounds are only changed by a constant depending on the backtracking parameter.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

constraint on the step-sizes, which is essential for conver-

gence to the minimizer when interpolation is violated.2

The resulting step-size is guaranteed to be in the range

[2(1−c)/Lmax, ηk−1] (Vaswani et al., 2019b) and allows us

to prove the following theorem.

Theorem 3. Under the same assumptions as Theorem 1,

AdaGrad with a conservative Lipschitz line-search with

c = 1/2, and uniform averaging converges at a rate

√

E[f (w¯T ) − f ∗] ≤

α +

ασ √,

T

T

where α = 21 D2 max η10 , Lmax + 2 η0 2dLmax.

Here, η0 is the step-size used to initialize the line-search in the ﬁrst iteration of AdaGrad and is set to a large value in practice. The Lipschitz line-search attains the same rate as constant step-size AdaGrad but automatically chooses a stepsize in each iteration, and allows AdaGrad to adapt to the function’s local smoothness properties. This enables it perform better on some problems, even though its worst-case performance is the same as the constant step-size variant.

5.2 Adaptive step-size AMSGrad

For AMSGrad, we consider variants of both the stochastic line-search (Vaswani et al., 2019b) and the stochastic Polyak step-sizes (Loizou et al., 2020; Berrada et al., 2020).

Armijo stochastic line-search: We design a stochastic vari-

ant of the Armijo line-search (Armijo, 1966) to determine

the step-size. Unlike the Lipschitz line-search whose sole

purpose is to estimate the smoothness constant, the stochas-

tic Armijo line-search (Armijo SLS) selects a suitable step-

size in the preconditioned stochastic gradient direction. In

particular, it returns the largest step-size ηk satisfying the

following conditions at iteration k, ηk ≤ ηmax and

fik (wk

−

η

k

A

− k

1

∇

f

i

k

(w

k

))

≤ fik (wk) − cηk ∇fik (wk) 2A−1 . (3)

k

Similarly to Eq. (2), the step-size is upper-bounded by ηmax

(typically chosen to be a large value). Unlike the Lipschitz

line-search, the stochastic Armijo line-search guarantees

descent on the current function fik and that ηk lies in the [2amin (1−c)/Lmax, ηmax] range.

Armijo stochastic Polyak step-size: We ﬁrst deﬁne the

stochastic Polyak step-size (SPS) and Armijo SPS, its modi-

ﬁcation for the adaptive case.

SPS: ηk = min Armijo SPS: ηk = min

fi (wk) − fi∗

k

k , ηmax ,

c ∇fik (wk) 2

fi (wk) − fi∗

k

k , ηmax .

c

∇fik (wk)

2 A−1

k

2If interpolation is exactly satisﬁed, we can obtain an O(1/T ) convergence without the conservative step-sizes (Appendix C.3).

Here, fi∗k is the minimum value for the function fik . The SPS variants require knowledge of fi∗ for each function in the ﬁnite-sum. This value is difﬁcult to obtain for gen-
eral functions but is readily available in the interpolation
setting for many machine learning applications. For exam-
ple, common loss functions are lower-bounded by zero, and
the interpolation setting ensures that these lower-bounds are tight. Consequently, using SPS with fi∗ = 0 has been shown to yield good performance for over-parameterized
problems (Loizou et al., 2020; Berrada et al., 2020). The ad-
vantage of SPS over a line-search is that it does not require
a backtracking procedure to set the step-size.

For AMSGrad, we propose to use a conservative variant of Armijo SPS that sets ηmax = ηk−1 at iteration k ensuring that ηk ≤ ηk−1. This is because using a potentially increasing step-size sequence along with momentum can make the optimization unstable and result in divergence. Using this step-size, we prove the following result. Theorem 4. Under the same assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded eigenvalues in the [amin, amax] interval with κ = / , amax amin AMSGrad with β ∈ [0, 1), conservative Armijo SPS with c = 1+β/1−β and uniform averaging converges at a rate,

E[f (w¯T ) − f ∗] ≤

1 + β 2 2LmaxD2dκ + σ2.

1−β

T

The above result matches the convergence rate in Theorem 2 but does not require knowledge of the smoothness constant Lmax or the minimum eigenvalue of the preconditioner amin; the step-size adapts to both these quantities. A similar convergence rate can be obtained with a conservative variant of Armijo SLS (Appendix E.2), although our proof technique only allow for a restricted range of β.

5.3 SGD with stochastic heavy-ball momentum
When Ak = Id, the AMSGrad update is equivalent to the update for SGD with heavy-ball momentum (Sebbouh et al., 2020). By setting Ak = Id in Theorem 2, we recover an O(1/T + σ2) rate for constant step-size SGD with heavyball momentum, matching the result for smooth, convex functions in Sebbouh et al. (2020). Furthermore, setting Ak = Id in Theorem 4 and using SPS to set the step-size helps match this rate without requiring the knowledge of the Lipschitz constant. To the best of our knowledge, this is the ﬁrst result for SGD with an adaptive step-size and stochastic heavy-ball momentum. This result also provides theoretical justiﬁcation for the heuristic used for incorporating heavyball momentum for SLS of Vaswani et al. (2019b).

5.4 Alternative stochastic heavy-ball momentum
For a general preconditioner, the AMSGrad update in Eq. (1) is not equivalent to heavy-ball momentum. The standard

Train loss (log)

Adaptive Gradient Methods Converge Faster with Over-Parameterization

ijcnn

mushrooms

rcv1

10 1

10 1

10 3

10 1

10 5

10 2

10 7

10 2

10 9

10 3 0 20 40 60 80 100

0 20 40 60 80 100

Epoch

Epoch

0 20 40Epoch60 80 100

Adabound

Radam

Adam

SLS

Adagrad + SLS

Amsgrad + SLS

Amsgrad + SLS + HB

Figure 2. Comparison on convex objectives: binary classiﬁcation on LIBSVM datasets using RBF kernel mappings, selected to (approximately) satisfy interpolation. All optimizers use default step-sizes. Adam and its variants have poor performance due to the default step-size, while the stochastic line-search (SLS) adresses this issue and improves performance.

stochastic heavy-ball update (Loizou & Richtárik, 2017) is:

wk+1

=

wk

−

αk

A

−1 k

∇

fi

k

(wk

)

+

γ

(wk

−

wk−1)

.

(4)

where αk is the step-size and γ ∈ [0, 1) is the constant momentum parameter. Unlike this update, AMSGrad also preconditions the momentum direction (wk − wk−1) (refer to Appendix E.1 for a relation between the two updates). If we consider the zero-momentum variant of adaptive gradient methods as preconditioned gradient descent, Eq. (4) is a more natural way to incorporate momentum. We explore this alternate method and prove the same O(1/T + σ2) convergence rate for constant step-size, conservative Armijo SPS and Armijo SLS techniques in Appendix E.3. We compare the two forms of heavy-ball momentum in Section 6.

6 Experimental evaluation
Synthetic experiments We verify the effectiveness of the proposed line-search variants on the logistic regression example considered in Fig. 1. For AdaGrad, we compare against the proposed Lipschitz line-search and Armijo SLS variants. As suggested by the theory, for each of these variants, we set the value of c = 1/2. For AMSGrad, we compare against the variant employing the Armijo SLS with c = 1/2.3 and use the default (in PyTorch) momentum parameter of β = 0.9. In Fig. 1, we observe that the line-search variants have good performance across margins, often better than the best-performing constant step-size. In Appendix G, we use synthetic deep matrix factorization to systematically study the effect of over-parameterization for (Rolinek & Martius, 2018; Vaswani et al., 2019b). Our results indicate that over-parameterization improves the convergence of all methods, but a line-search is essential to fully exploit it.
Experiments with real data: For experiments with real data, we use a batch-size of 128 and compare against Adam and its improved variants; RAdam (Liu et al., 2020) and
3This corresponds to the largest allowable step-size in Theorem 8 without momentum. Unfortunately, the values of c suggested by the momentum analysis (Theorem 4) are too conservative.

AdaBound (Luo et al., 2019). To see the effect of preconditioning, we compare against SGD with SLS (Vaswani et al., 2019a) and SPS (Loizou et al., 2020). We ﬁnd that SGD with SLS is more stable and has consistently better test performance than SPS. Hence, we only show results for SLS. Similar to Vaswani et al. (2019a), we observed that tuned constant step-size SGD is consistently outperformed by SGD with SLS, and do not show the corresponding plots.
For the proposed methods, we consider the combinations with theoretical guarantees in the convex setting, speciﬁcally AdaGrad and AMSGrad with the Armijo SLS. For AdaGrad, we only show Armijo SLS since it consistently outperforms the Lipschitz line-search. For all variants with Armijo SLS, we use c = 1/2 for all convex experiments (suggested by Theorem 8 and Vaswani et al. (2019b)). Since we do not have a theoretical analysis for non-convex problems, we follow the protocol in (Vaswani et al., 2019b) and set c = 0.1 for all the non-convex experiments. Throughout, we set β = 0.9 for AMSGrad. We also compare to the AMSGrad variant with heavy-ball momentum (with γ = 0.25 found by grid-search). We refer to Appendix F for a detailed discussion about the practical considerations and pseudocode for SLS and SPS.
Binary classiﬁcation using RBF kernels: We ﬁrst consider convex minimization for a binary classiﬁcation task using RBF kernels without regularization. The kernel bandwidths are chosen by cross-validation following the protocol in (Vaswani et al., 2019b). This setup ensures interpolation is (approximately) satisﬁed in a convex setting. Following the protocol in (Vaswani et al., 2019b; Loizou et al., 2020), we experiment with standard datasets from LIBSVM (Chang & Lin, 2011): mushrooms, rcv1 and ijcnn and use the default parameters for all the optimizers. Figure 2 shows the training performance for the different methods using the logistic loss. We observe the (i) superior convergence of the optimizers using SLS including AdaGrad and AMSGrad with both types of momentum. (ii) Adam and its variants have poor performance, completely stalling for the mushrooms dataset. (iii) The AdaGrad and AMSGrad variants

Train loss (log)

Adaptive Gradient Methods Converge Faster with Over-Parameterization

CIFAR10 - ResNet34

CIFAR100 - DenseNet121

CIFAR100 - ResNet34 101 Tiny ImageNet - ResNet18

100

100

100

100

10 1

10 1

10 1

10 1

10 2

10 2

10 2

10 3

10 2

10 3

10 3

10 4 0 50 100 150 200 Epoch

50 100 150 200 10 3 0 50 100 150 200 0 50 100 150 200

Epoch

Epoch

Epoch

CIFAR10 - ResNet34 0.76 CIFAR100 - DenseNet121 0.76 CIFAR100 - ResNet34 0.40 Tiny ImageNet - ResNet18

0.94

0.74

0.74

0.39

0.92

0.72

0.72

0.38

0.90

0.70

0.88

0.68

0.66 0.860 50 100 150 200
Epoch

0.70
0.68
0.66 50 E1p0o0ch 150 200 0

0.37 0.36 0.35 50 100 150 200 0.34 50 Epoch

100Epoch 150 200

Amsgrad + SLS

Amsgrad + SLS + HB

Adabound

Radam

Adam

SLS

Adagrad + SLS

Validation accuracy

Figure 3. Comparing optimizers for multi-class classiﬁcation with deep networks. Training loss (top) and validation accuracy (bottom) for CIFAR-10, CIFAR-100 and Tiny ImageNet. AdaGrad and AMSGrad equipped with the Armijo SLS not only converge faster than Adam and Radam but also have considerably better test performance.

have better convergence than SGD with SLS demonstrating the positive effects of their preconditioning.
Multi-class classiﬁcation using deep neural networks: Following the protocol in (Luo et al., 2019; Vaswani et al., 2019b; Loizou et al., 2020), we consider training standard neural network architectures for multi-class classiﬁcation on CIFAR-10, CIFAR-100 and variants of the ImageNet datasets. We show a subset of results for CIFAR-10, CIFAR100 and Tiny ImageNet and defer the rest to Appendix G. We compare against tuned Adam with its step-size found by a grid-search for each experiment.
From Fig. 3 we observe that, (i) in terms of generalization, AdaGrad and AMSGrad with Armijo SLS have consistently the best performance, while SGD with SLS is often competitive. (ii) the AdaGrad and AMSGrad variants not only converge faster than Adam and Radam but also have considerably better test performance. AdaBound has comparable convergence in training loss, but does not generalize as well. (iii) AMSGrad momentum is consistently better than the heavy-ball (HB) variant. Moreover, we observed that HB momentum was quite sensitive to the setting of γ, whereas AMSGrad is robust to β. In Appendix G, we include ablation results for AMSGrad with Armijo SLS without momentum, and conclude that momentum does indeed improve the performance. We also plot the wall-clock time for the SLS variants and verify that the performance gains justify the increase in wall-clock time. We show the variation of the

step-size across epochs and observe a warm-up phase where the step-size increases followed by a constant or decreasing step-size (Goyal et al., 2017). In Appendix G, we show that similar trends hold for different datasets and models.
Our results indicate that simply setting the correct step-size on the ﬂy can lead to substantial empirical gains, often more than those obtained by designing a different preconditioner. Furthermore, we see that with an appropriate step-size adaptation, adaptive gradient methods can generalize better than SGD. By disentangling the effect of the step-size from the preconditioner, we observe that AdaGrad has good empirical performance, contradicting common knowledge (Kingma & Ba, 2015). Moreover, our techniques are orthogonal to designing better preconditioners and can be used with other adaptive gradient or even second-order methods.
7 Discussion
When training over-parameterized models in the interpolation setting, we showed that for smooth, convex functions, constant step-size variants of both AdaGrad and AMSGrad are guaranteed to converge to the minimizer at O(1/T ) rates. We proposed to use stochastic line-search techniques to help these methods adapt to the function’s local smoothness, alleviating the need to tune their step-size and resulting in consistent empirical improvements across tasks. Although adaptive gradient methods outperform SGD in practice, their convergence rates are worse than constant step-size SGD and we hope to address this discrepancy in the future.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

8 Acknowledgments
We would like to thank Reza Babanezhad, Aaron Mishkin, Nicolas Loizou and Nicolas Le Roux for helpful discussions. This research was partially supported by the Canada CIFAR AI Chair Program, a Google Focused Research award, an IVADO postdoctoral scholarship, and by the NSERC Discovery Grants RGPIN-2017-06936 and 2015-06068. Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning in Machines & Brains program.
References
Ahn, K., Yun, C., and Sra, S. SGD with shufﬂing: optimal rates without component convexity and large epoch requirements. Advances in Neural Information Processing Systems, NeurIPS, 33, 2020.
Alacaoglu, A., Malitsky, Y., Mertikopoulos, P., and Cevher, V. A new regret analysis for Adam-type algorithms. In 37th International Conference on Machine Learning, ICML, volume 119, pp. 202–210. PMLR, 2020.
Armijo, L. Minimization of functions having lipschitz continuous ﬁrst partial derivatives. Paciﬁc Journal of mathematics, 16(1):1–3, 1966.
Belkin, M., Rakhlin, A., and Tsybakov, A. B. Does data interpolation contradict statistical optimality? In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2019.
Berahas, A. S., Nocedal, J., and Takácˇ, M. A multi-batch L-BFGS method for machine learning. Advances in Neural Information Processing Systems, NeurIPS, pp. 1063– 1071, 2016.
Berrada, L., Zisserman, A., and Kumar, M. P. Training neural networks for and by interpolation. In International Conference on Machine Learning, ICML, volume 119, pp. 799–809. PMLR, 2020.
Bollapragada, R., Nocedal, J., Mudigere, D., Shi, H.-J., and Tang, P. T. P. A progressive batching l-bfgs method for machine learning. In International Conference on Machine Learning, pp. 620–629. PMLR, 2018.
Cesa-Bianchi, N., Conconi, A., and Gentile, C. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050–2057, 2004.
Cevher, V. and Vu˜, B. C. On the linear convergence of the stochastic gradient method with constant step-size. Optimization Letters, 13(5):1177–1187, 2019.

Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):1–27, 2011. Software available at http://www.csie.ntu.edu. tw/~cjlin/libsvm.
Chen, X., Liu, S., Sun, R., and Hong, M. On the convergence of a class of Adam-type algorithms for non-convex optimization. In 7th International Conference on Learning Representations, ICLR, 2019.
Défossez, A., Bottou, L., Bach, F., and Usunier, N. On the convergence of Adam and AdaGrad. arXiv:2003.02395, 2020.
Devolder, O., Glineur, F., and Nesterov, Y. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146(1):37–75, 2014.
Duchi, J. C., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121– 2159, 2011.
Ghadimi, E., Feyzmahdavian, H. R., and Johansson, M. Global convergence of the heavy-ball method for convex optimization. In 2015 European Control Conference (ECC), pp. 310–315. IEEE, 2015.
Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. Adaptive computation and machine learning. MIT press, 2016.
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv:1706.02677, 2017.
Hazan, E. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157–325, 2016.
Hazan, E. and Kale, S. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1):2489–2512, 2014.
Jain, P., Kakade, S. M., Kidambi, R., Netrapalli, P., and Sidford, A. Accelerating stochastic gradient descent for least squares regression. In Conference On Learning Theory, COLT, 2018.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR, 2015.
Levy, K. Y., Yurtsever, A., and Cevher, V. Online adaptive methods, universality and acceleration. In Advances in Neural Information Processing Systems, NeurIPS, 2018.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Li, X. and Orabona, F. On the convergence of stochastic gradient descent with adaptive stepsizes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2019.
Liang, T. and Rakhlin, A. Just interpolate: Kernel “ridgeless” regression can generalize. Annals of Statistics, 48(3): 1329–1347, 06 2020.
Liu, C. and Belkin, M. Accelerating SGD with momentum for over-parameterized learning. In 8th International Conference on Learning Representations, ICLR, 2020.
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. In 8th International Conference on Learning Representations, ICLR, 2020.
Lohr, S. L. Sampling: design and analysis. Nelson Education, 2009.
Loizou, N. and Richtárik, P. Linearly convergent stochastic heavy ball method for minimizing generalization error. NeurIPS Workshop on Optimization for Machine Learning, arXiv:1710.10737, 2017.
Loizou, N., Vaswani, S., Laradji, I., and Lacoste-Julien, S. Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence. arXiv:2002.10542, 2020.
Loshchilov, I. and Hutter, F. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR, 2017.
Luo, L., Xiong, Y., Liu, Y., and Sun, X. Adaptive gradient methods with dynamic bound of learning rate. In 7th International Conference on Learning Representations, ICLR, 2019.
Ma, S., Bassily, R., and Belkin, M. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th International Conference on Machine Learning, ICML, 2018.
Meng, S. Y., Vaswani, S., Laradji, I., Schmidt, M., and Lacoste-Julien, S. Fast and furious convergence: Stochastic second order methods under interpolation. In The 23nd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2020.
Mertikopoulos, P., Hallak, N., Kavis, A., and Cevher, V. On the almost sure convergence of stochastic gradient descent in non-convex problems. Advances in Neural Information Processing Systems, NeurIPS, 33, 2020.

Moritz, P., Nishihara, R., and Jordan, M. A linearlyconvergent stochastic l-bfgs algorithm. In Artiﬁcial Intelligence and Statistics, pp. 249–258, 2016.
Moulines, E. and Bach, F. R. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, NeurIPS, 2011.
Mukkamala, M. C. and Hein, M. Variants of RMSProp and AdaGrad with logarithmic regret bounds. In Proceedings of the 34th International Conference on Machine Learning, ICML, 2017.
Nar, K. and Sastry, S. Step size matters in deep learning. In Advances in Neural Information Processing Systems, NeurIPS, 2018.
Orabona, F. A modern introduction to online learning. arXiv:1912.13213, 2019.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, NeurIPS, 2019.
Polyak, B. T. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 3(4):643–653, 1963.
Reddi, S. J., Kale, S., and Kumar, S. On the convergence of Adam and beyond. In 6th International Conference on Learning Representations, ICLR, 2018.
Rolinek, M. and Martius, G. L4: practical loss-based stepsize adaptation for deep learning. In Advances in Neural Information Processing Systems, NeurIPS, 2018.
Schmidt, M. and Le Roux, N. Fast convergence of stochastic gradient descent under a strong growth condition. arXiv:1308.6370, 2013.
Sebbouh, O., Gower, R. M., and Defazio, A. On the convergence of the stochastic heavy ball method. arXiv:2006.07867, 2020.
Staib, M., Reddi, S. J., Kale, S., Kumar, S., and Sra, S. Escaping saddle points with adaptive gradient methods. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.
Tieleman, T. and Hinton, G. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

Adaptive Gradient Methods Converge Faster with Over-Parameterization
Vaswani, S., Bach, F., and Schmidt, M. Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2019a.
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., and Lacoste-Julien, S. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Advances in Neural Information Processing Systems, NeurIPS, 2019b.
Wang, G., Lu, S., Cheng, Q., Tu, W., and Zhang, L. SAdam: A variant of Adam for strongly convex functions. In 8th International Conference on Learning Representations, ICLR, 2020.
Ward, R., Wu, X., and Bottou, L. AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.
Wu, X., Du, S. S., and Ward, R. Global convergence of adaptive gradient methods for an over-parameterized neural network. arXiv:1902.07111, 2019.
Xie, Y., Wu, X., and Ward, R. Linear convergence of adaptive stochastic gradient descent. In The 23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, volume 108, pp. 1475–1485. PMLR, 2020.
Yu, J., Vishwanathan, S., Günter, S., and Schraudolph, N. N. A quasi-Newton approach to nonsmooth convex optimization problems in machine learning. The Journal of Machine Learning Research, 11:1145–1200, 2010.
Zeiler, M. D. ADADELTA: an adaptive learning rate method. arXiv:1212.5701, 2012.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR, 2017.
Zhang, L. and Zhou, Z. Stochastic approximation of smooth and strongly convex functions: Beyond the O(1/T ) convergence rate. In Conference on Learning Theory, COLT, 2019.
Zhou, D., Tang, Y., Yang, Z., Cao, Y., and Gu, Q. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv:1808.05671, 2018.

Adaptive Gradient Methods Converge Faster with Over-Parameterization
Supplementary material

Organization of the Appendix
A Setup and assumptions
B Line-search and Polyak step-sizes
C Proofs for AdaGrad
Step-size Constant Conservative Lipschitz LS Non-conservative LS (with interpolation)

Rate
O(1/T + σ/√T ) O(1/T + σ/√T ) O(1/T )

D Proofs for AMSGrad and non-decreasing preconditioners without momentum

Constant Armijo LS

O(1/T + σ2) O(1/T + σ2)

E AMSGrad with momentum
Constant Conservative Armijo LS Conservative Armijo SPS
Proofs for AMSGrad with heavy ball momentum
Constant Conservative Armijo LS Conservative Armijo SPS

O(1/T + σ2) O(1/T + σ2) O(1/T + σ2)
O(1/T + σ2) O(1/T + σ2) O(1/T + σ2)

F Experimental details G Additional experimental results

Reference Theorem 1 Theorem 3 Theorem 6
Theorem 7 Theorem 8
Theorem 2 Theorem 10 Theorem 4
Theorem 11 Theorem 13 Theorem 12

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Concept
Iteration counter, maximum Iterates, minimum Step-size Function value, minimum Stoch. function value, minimum

Table 2. Summary of notation

Symbol

Concept

k, T wk, w∗
ηk f (w), f ∗ fi(w), fi∗

General preconditioner Preconditioner bounds Maximum smoothness Dimensionality Diameter bound Variance

Symbol
Ak [amin, amax] Lmax d D σ2 = Ei[fi(w∗) − fi∗]

A Setup and assumptions

We restate the main notation in Table 2. We now restate the main assumptions required for our theoretical results

We assume our objective f : Rd → R has a ﬁnite-sum structure, 1n
f (w) = n fi(w), (5)
i=1
and analyze the following update, with ik selected uniformly at random,

wk+1 = wk − ηk A− k 1mk ; mk = βmk−1 + (1 − β)∇fik (wk)

(Update rule)

where ηk is either a pre-speciﬁed constant or selected on the ﬂy. We consider AdaGrad and AMSGrad and use the fact that the preconditioners are non-decreasing i.e. Ak Ak−1. For AdaGrad, β = 0. For AMSGrad, we further assume that the preconditioners remain bounded with eigenvalues in the range [amin, amax],

aminI Ak amaxI.

(Bounded preconditioner)

For all algorithms, we assume that the iterates do not diverge and remain in a ball of radius D, as is standard in the literature

on online learning (Duchi et al., 2011; Levy et al., 2018) and adaptive gradient methods (Reddi et al., 2018),

wk − w∗ ≤ D.

(Bounded iterates)

Our main assumptions are that each individual function fi is convex, differentiable, has a ﬁnite minimum fi∗, and is Li-smooth, meaning that for all v and w,

fi(v) ≥ fi(w) − ∇fi(w), w − v ,

(Individual Convexity)

fi(v) ≤ fi(w) + ∇fi(w), v − w + Li v − w 2 , 2

(Individual Smoothness)

which also implies that f is convex and Lmax-smooth, where Lmax is the maximum smoothness constant of the individual functions. A consequence of smoothness is the following bound on the norm of the gradient stochastic gradients,

∇fi(w) 2 ≤ 2Lmax(fi(w) − fi∗).

To characterize interpolation, we deﬁne the expected difference between the minimum of f , f (w∗), and the minimum of the individual functions fi∗,

σ2 = E[fi(w∗) − fi∗] < ∞.
i

(Noise)

When interpolation is exactly satisﬁed, every data point can be ﬁt exactly, such that fi∗ = 0 and f (w∗) = 0, we have σ2 = 0.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

B Line-search and Polyak step-sizes

We now give the main guarantees on the step-sizes returned by the line-search. In practice, we use a backtracking line-search to ﬁnd a step-size that satisﬁes the constraints, described in Algorithm 1 (Appendix F). For simplicity of presentation, here we assume the line-search returns the largest step-size that satisﬁes the constraints.

When interpolation is not exactly satisﬁed, the procedures need to be equipped with an additional safety mechanism; either by capping the maximum step-size by some ηmax or by ensuring non-increasing step-sizes, ηk ≤ ηk−1. In this case, ηmax ensures that a bad iteration of the line-search procedure does not result in divergence. When interpolation is satisﬁed, those conditions can be dropped (e.g., setting ηmax → ∞) and the rate does not depend on ηmax. The line-searches depend on a parameter c ∈ (0, 1) that controls how much decrease is necessary to accept a step (larger c means more decrease is demanded).

Assuming the Lipschitz and Armijo line-searches select the largest η such that

fi(w − η∇fi(w)) ≤ fi(w) − cη ∇fi(w) 2 ,

η ≤ ηmax,

fi(w − ηA−1∇fi(w)) ≤ fi(w) − cη ∇fi(w) 2A−1 ,

η ≤ ηmax,

the following lemma holds.

(Lipschitz line-search) (Armijo line-search)

Lemma 1 (Line-search). If fi is Li-smooth, the Lipschitz and Armijo lines-searches ensure

η ∇fi(w) 2 ≤ 1c (fi(w) − fi∗), and

2 (1 − c) min ηmax, Li

η ∇fi(w) 2

≤

1 (fi(w)

−

f ∗),

and

min ηmax, 2 λmin(A) (1 − c)

A−1 c

i

Li

≤ η ≤ ηmax, ≤ η ≤ ηmax.

We do not include the backtracking line-search parameters in the analysis for simplicity, as the same bounds hold, up to some constant. With a backtracking line-search, we start with a large enough candidate step-size and multiply it by some constant γ < 1 until the Lipschitz or Armijo line-search condition is satisﬁed. If η was a proposal step-size that did not satisfy the constraint, but γη does, the maximum step-size η that satisﬁes the constraint must be in the range γη ≤ η < η .

Proof of Lemma 1. Recall that if fi is Li-smooth, then for an arbitrary direction d,

fi(w − d) ≤ fi(w) − ∇fi(w), d + Li d 2 . 2

For the Lipschitz line-search, d = η∇fi(w). The smoothness and the line-search condition are then

Smoothness:

fi(w − η∇fi(w)) − fi(w) ≤ L2i η2 − η ∇fi(w) 2 ,

Line-search:

fi(w − η∇fi(w)) − fi(w) ≤ −cη ∇fi(w) 2 .

As illustrated in Fig. 4, the line-search condition is looser than smoothness if
L2i η2 − η ∇fi(w) 2 ≤ −cη ∇fi(w) 2 .
The inequality is satisﬁed for any η ∈ [a, b], where a, b are values of η that satisfy the equation with equality, a = 0, b = 2(1−c)/Li, and the line-search condition holds for η ≤ 2(1−c)/Li.

•fi(w)

Smoothness:

fi(w)

+

(

Li 2

η2

−

η)

∇fi(w)

2

Line search: fi(w) − cη ∇fi(w) 2

η=0

η = 2(1L−i c)

Figure 4. Sketch of the line-search inequalities.

As the line-search selects the largest feasible step-size, η ≥ 2(1−c)/Li. If the step-size is capped at ηmax, we have η ≥ min{ηmax, 2(1−c)/Li}, and the proof for the Lipschitz line-search is complete. The proof for the Armijo line-search is

Adaptive Gradient Methods Converge Faster with Over-Parameterization

identical except for the smoothness property, which is modiﬁed to use the · A-norm for the direction d = ηA−1∇fi(w);

fi(w − ηA−1∇fi(w)) ≤ fi(w) − η ∇fi(w), A−1∇fi(w) + Li η2 A−1∇fi(w) 2 , 2

≤ fi(w) − η ∇fi(w) 2 + Li η2 ∇fi(w) 2 ,

A−1 2λmin(A)

A−1

= fi(w)+

Li η2 − η

2λmin(A)

∇fi(w) 2A−1 ,

where the second inequality comes from A−1∇fi(w) 2 ≤ λmin1(A) ∇fi(w) 2A−1 .

Similarly, the stochastic Polyak step-sizes (SPS) for fi at w are deﬁned as

SPS:

η = min

fi(w) − fi∗ 2 , ηmax ,

c ∇fi(w)

Armijo SPS:

η = min

fi(w) − fi∗ 2 , ηmax ,

c ∇fi(w) A−1

where the parameter c > 0 controls the scaling of the step (larger c means smaller steps).

Lemma 2 (SPS guarantees). If fi is Li-smooth, SPS and Armijo SPS ensure that

SPS:

η

∇fi(w)

2

≤

1 c

(fi

(w)

−

fi∗

),

min ηmax, 2c1Li ≤ η ≤ ηmax,

Armijo SPS:

η

∇fi(w)

2 A−1

≤

1 c

(fi

(w)

−

fi∗

),

min ηmax, λm2icnL(iA) ≤ η ≤ ηmax

Proof of Lemma 2. The ﬁrst guarantee follows directly from the deﬁnition of the step-size. For SPS,

η ∇fi(w) 2 = min

fi(w) − fi∗ , ηmax c ∇fi(w) 2

∇fi(w) 2 ,

= min

fi(w) − fi∗ , ηmax ∇fi(w) 2 c

1 ≤ c (fi(w) − fi ).

The same inequalities hold for Armijo SPS with

∇fi(w)

2 A−1

.

To

lower-bound

the

step-size,

we

use

the

Li-smoothness

of

fi, which implies fi(w) − fi∗ ≥ 2L1 i ∇fi(w) 2. For SPS,

fi(w) − fi∗

1 2L

∇fi(w) 2

1

c ∇fi(w) 2 ≥ c i ∇fi(w) 2 = 2cLi .

For Armijo SPS, we additionally use ∇fi(w) 2A−1 ≤ λmin1(A) ∇fi(w) 2,

fi(w) − fi∗

1 2L

∇fi(w) 2

λmin(A)

2≥

i
1

2=

c ∇fi(w) A−1 c λ (A) ∇fi(w)

min

. 2cLi

Adaptive Gradient Methods Converge Faster with Over-Parameterization

C Proofs for AdaGrad

We now move to the proof of the convergence of AdaGrad in the smooth setting with a constant step-size (Theorem 1) and the conservative Lipschitz line-search (Theorem 3). We ﬁrst give a rate for an arbitrary step-size ηk in the range [ηmin, ηmax], and derive the rates of Theorems 1 and 3 by specializing the range to a constant step-size or line-search.

Proposition 1 (AdaGrad with non-increasing step-sizes). Assuming (i) convexity and (ii) Lmax-smoothness of each

fi, and (iii) bounded iterates, AdaGrad with non-increasing (ηk ≤ ηk−1), bounded step-sizes (ηk ∈ [ηmin, ηmax]), and

uniform

averaging

w¯T

=

1 T

Tk=1wk, converges at a rate

√

E[f (w¯T ) − f ∗] ≤

α +

ασ √,

1 where α =

T

T

2

D2

2

ηmin + 2ηmax dLmax.

We ﬁrst use the above result to prove Theorems 1 and 3. The proof of Theorem 1 is immediate by plugging η = ηmin = ηmax in Proposition 1. We recall its statement;

Theorem 1 (Constant step-size AdaGrad). Assuming (i) convexity, (ii) Lmax-smoothness of each fi, and (iii)

bounded

iterates,

AdaGrad

with

a

constant

step-size

η

and

uniform

averaging

w¯T

=

1 T

T k=1

wk ,

converges

at a rate

√

E[f (w¯T ) − f ∗] ≤

α +

ασ √,

T

T

where α = 21 Dη2 + 2η 2dLmax.

For Theorem 3, we use the properties of the conservative Lipschitz line-search. We recall its statement;

Theorem 3. Under the same assumptions as Theorem 1, AdaGrad with a conservative Lipschitz line-search

with c = 1/2, and uniform averaging converges at a rate

√

E[f (w¯T ) − f ∗] ≤

α +

ασ √,

T

T

where α = 12 D2 max η10 , Lmax + 2 η0 2dLmax.

Proof of Theorem 3. Using Lemma 1, there is a step-size ηk that satisﬁes the Lipschitz line-search with ηk ≥ 2 (1−c)/Lmax. Setting c = 1/2 and using a maximum step-size ηmax, we have

1

1

1

min ηmax, Lmax ≤ ηk ≤ ηmax, =⇒ ηmin = max ηmax , Lmax .

Before going into the proof of Proposition 1, we recall some standard lemmas from the adaptive gradient literature (Theorem 7 & Lemma 10 in (Duchi et al., 2011), Lemma 5.15 & 5.16 in (Hazan, 2016)), and a useful quadratic inequality (Levy et al., 2018, Part of Theorem 4.2)). We include proofs in Appendix C.1 for completeness.
Lemma 3. If the preconditioners are non-decreasing (Ak Ak−1), the step-sizes are non-increasing (ηk ≤ ηk−1), and the iterates stay within a ball of radius D of the minima,
Tk=1 wk − w∗ 2η1k Ak− ηk1−1 Ak−1 ≤ DηT2 Tr(AT ).

Lemma 4. For AdaGrad, Ak =

k

1/2

i=1 ∇fik (wk)∇fik (wk)

and satisﬁes,

T k=1

∇fik (wk)

2 A−1

≤ 2Tr(AT ),

k

Tr(AT ) ≤

d

T k=1

∇fik (wk) 2.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Lemma 5. If x2 ≤ a(x + b) for a ≥ 0 and b ≥ 0,

1 x≤
2

a2 + 4ab + a

√ ≤ a + ab.

We now prove Proposition 1.

Proof of Proposition 1. We ﬁrst give an overview of the main steps. Using the deﬁnition of the update rule, along with Lemmas 3 and 4, we will show that

2

T k=1

∇fik (wk), wk

− w∗

≤

D2 η

+ 2ηmax

Tr(AT ).

(6)

min

Using the deﬁnition of AT , individual smoothness and convexity, we then show that for a constant a,

T k=1

E[f

(wk )

−

f

∗]

≤

a

E

T k=1

fik

(wk

)

−

fik

(w∗)

+ T σ2

,

(7)

Using the quadratic inequality (Lemma 5), averaging and using Jensen’s inequality ﬁnishes the proof.

To derive Eq. (6), we start with the Update rule, measuring distances to w∗ in the · Ak norm,

wk+1 − w∗ 2Ak = wk − w∗ 2Ak − 2ηk ∇fik (wk), wk − w∗ + ηk2 ∇fik (wk) 2A− k 1 .

Dividing by ηk, reorganizing the equation and summing across iterations yields

T
2 ∇fik (wk), wk − w∗
k=1

T
≤
k=1

w − w∗ 2

k

Ak − Ak−1

ηk ηk−1

T
+ ηk
k=1

∇fik (wk) 2A−1 , k

T
≤
k=1

wk − w∗

2 Ak − Ak−1
ηk ηk−1

T
+ ηmax
k=1

∇fik (wk) 2A−1 . k

We use the Lemmas 3, 4 to bound the RHS by the trace of the last preconditioner,

D2 ≤ ηT Tr(AT ) + 2ηmaxTr(AT ),
D2 ≤ ηmin + 2ηmax Tr(AT ).

(Lemmas 3 and 4) (ηk ≥ ηmin)

To derive Eq. (7), we bound the trace of AT using Lemma 4 and Individual Smoothness,

√ Tr(AT ) ≤ d

T k=1

∇fik (wk) 2,

(Lemma 4, Trace bound)

√ ≤ 2dLmax

Tk=1 fik (wk) − fi∗k .

(Individual Smoothness)

√ ≤ 2dLmax

Tk=1 fik (wk) − fik (w∗) + fik (w∗) − fi∗k

(±fik (w∗))

2

√

Combining the above inequalities with δik = fik (w∗) − fi∗k and a = 21 ( ηD + 2ηmax) 2dLmax,

min

T k=1

∇fik (wk), wk

− w∗

≤a

Using Individual Convexity and taking expectations,

T k=1

fik

(wk

)

−

fik

(w∗)

+

δik

.

T k=1

E[f

(wk )

−

f

∗]

≤

a

E

T k=1

fik

(wk

)

−

fik

(w∗)

+

δik

,

≤a E

T k=1

fik

(wk

)

−

fik

(w∗)

+

δik

.

Letting σ2 := Ei[δi] = Ei[fi(w∗) − fi∗] and taking the square on both sides yields

T

2

T

E[f (wk) − f ∗] ≤ a2 E fik (wk) − fik (w∗) + T σ2 .

k=1

k=1

(Jensen’s inequality)

Adaptive Gradient Methods Converge Faster with Over-Parameterization

The quadratic bound (Lemma 5) x2 ≤ α(x + β) implies x ≤ α + √αβ, with

T
x = E[f (wk) − f ∗],
k=1

gives

the

ﬁrst

bound

below.

Averaging

w¯T

=

1 T

T
E[f (wk) − f ∗] ≤ α + αβ
k=1

1 α=
2

D2 1 + 2ηmax 2dLmax, ηmin

β = T σ2,

Tk=1wk and using Jensen’s inequality give the result;

√

=⇒

E[f (w¯T ) − f ∗] ≤

α +

ασ √.

T

T

Adaptive Gradient Methods Converge Faster with Over-Parameterization
C.1 Proofs of adaptive gradient lemmas For completeness, we give proofs for the lemmas used in the previous section. We restate them here;
Lemma 3. If the preconditioners are non-decreasing (Ak Ak−1), the step-sizes are non-increasing (ηk ≤ ηk−1), and the iterates stay within a ball of radius D of the minima,
Tk=1 wk − w∗ 2η1k Ak− ηk1−1 Ak−1 ≤ DηT2 Tr(AT ).

Proof of Lemma 3. Under the assumptions that Ak is non-decreasing and ηk is non-increasing, η1k Ak − ηk1−1 Ak−1 we can use the Bounded iterates assumption to bound

T k=1

w − w ≤ ∗ 2

k

Ak − Ak−1

ηk ηk−1

λ − T

Ak

k=1 max ηk

Ak−1 ηk−1

wk − w∗ 2

≤ D2 Tk=1 λmax Aηkk − Aηkk−−11 .

We then upper-bound λmax by the trace and use the linearity of the trace to telescope the sum,

≤ D2

Tr − T
k=1

Ak

Ak−1

ηk

ηk−1

= D2

Tk=1 Tr Aηkk − Tr Aηkk−−11 ,

= D2 Tr AηTT − Tr Aη00 ≤ D2 η1T Tr(AT ).

0, so

Lemma 4. For AdaGrad, Ak =

k

1/2

i=1 ∇fik (wk)∇fik (wk)

and satisﬁes,

T k=1

∇fik (wk)

2 A−1

≤ 2Tr(AT ),

k

Tr(AT ) ≤

d

T k=1

∇fik (wk) 2.

Proof of Lemma 4. For ease of notation, let ∇k := ∇fik (wk). By induction, starting with T = 1,

∇fi1 (w1)

2 A−1

= ∇1 A−1 1∇1 = Tr

∇1 A−1 1∇1

= Tr A−1 1∇1∇1

,

1

= Tr A−1 1A21 = Tr(A1).

(Cyclic property of trace) (A1 = (∇1∇1 )1/2)

Suppose that it holds for T − 1,

T −1 k=1

∇k

2 A−1

≤

2Tr(AT −1).

We

will

show

that

it

also

holds

for

T.

Using

the

deﬁnition

k

of the preconditioner and the cyclic property of the trace,

T k=1

∇fik (wk)

2 A−1

≤ 2Tr(AT −1) +

∇T

2 A−1

k

T

(Induction hypothesis)

= 2Tr (A2T − ∇T ∇T )1/2

+ Tr

A

− T

1

∇

T

∇

T

(AdaGrad update)

We then use the fact that for any X Y 0, we have (Duchi et al., 2011, Lemma 8)

2Tr (X − Y )1/2 + Tr X−1/2Y ≤ 2Tr X1/2 .

As X = A2T Y = ∇T ∇T 0, we can use the above inequality and the induction holds for T .

For the trace bound, recall that AT = G1T/2 where GT =

T i=1

∇fik

(wk

)∇fik

(wk

)

. We use Jensen’s inequality,

Tr(AT ) = Tr G1T/2 =

d j=1

λj(GT ) = d

1 d

d j=1

λj(GT ) ,

≤ d d1 dj=1 λj (GT ) = √d Tr(GT ).

To ﬁnish the proof, we use the deﬁnition of GT and the linearity of the trace to get

Tr(GT ) = Tr

T ∇∇ =
k=1 k k

T Tr(∇ ∇ ) =

k=1

kk

T ∇ 2.
k=1 k

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Lemma 5. If x2 ≤ a(x + b) for a ≥ 0 and b ≥ 0,

1 x≤
2

a2 + 4ab + a

√ ≤ a + ab.

Proof of Lemma 5. The starting point is the quadratic inequality x2 − ax − ab ≤ 0. √Letting r1√≤ r2 b√e the roots of the quadratic, the inequality holds if x ∈ [r1, r2]. The upper bound is then given by using a + b ≤ a + b
a + √a2 + 4ab a + √a2 + √4ab √ r2 = 2 ≤ 2 = a + ab.

C.2 Regret bound for AdaGrad under interpolation

In the online convex optimization framework, we consider a sequence of functions fk|Tk=1, chosen potentially adversarially by the environment. The aim of the learner is to output a series of strategies wk|Tk=1 before seeing the function fk. After choosing wk, the learner suffers the loss fk(wk) and observes the corresponding gradient vector ∇fk(wk). They suffer an
instantaneous regret rk = fk(wk) − fk(w) compared to a ﬁxed strategy w. The aim is to bound the cumulative regret,

T
R(T ) = [fk(wk) − fk(w∗)]

k=1

where w∗ = arg min

T k=1

fk (w)

is

the

best

strategy

if

we

had

access

to

the√entire

sequence

of

functions

in

hindsight.

Assuming the functions are convex but non-smooth, AdaGrad obtains an O(1/ T ) regret bound (Duchi et al., 2011). For

online convex optimization, the interpolation assumption implies that the learner model is powerful enough to ﬁt the entire

sequence of functions. For large over-parameterized models like neural networks, where the number of parameters is of the

order of millions, this is a reasonable assumption for large T .

We ﬁrst recall the update of AdaGrad, at iteration k, the learner decides to play the strategy wk, suffers loss fk(wk) and uses the gradient feedback ∇fk(wk) to update their strategy as

wk+1 = wk − ηA−k 1∇fk(wk), where Ak =

k

1/2

i=1 ∇fk(wk)∇fk(wk)

.

Now we show that for smooth, convex functions under the interpolation assumption, AdaGrad with a constant step-size can result in constant regret.

Theorem 5. For a sequence of Lmax-smooth, convex functions fk, assuming the iterates remain bounded s.t. for all k, wk − w∗ ≤ D, AdaGrad with a constant step-size η achieves the following regret bound,

R(T ) ≤ 1 D2 1 + 2η 2dLmax + 2η
where σ2 is an upper-bound on fk(w∗) − fk∗.

11

2

√

2 D2 η + 2η dLmaxσ2 T

√ Observe that σ2 is the degree to which interpolation is violated, and if σ2 = 0, R(T ) = O( T ) matching the regret of (Duchi et al., 2011). However, when interpolation is exactly satisﬁed, σ2 = 0, and R(T ) = O(1).

Proof of Theorem 5. The proof follows that of Proposition 1 which is inspired from (Levy et al., 2018). For convenience, we repeat the basic steps. Measuring distances to w∗ in the · Ak norm,
wk+1 − w∗ 2Ak = wk − w∗ 2Ak − 2η ∇fk(wk), wk − w∗ + η2 ∇fk(wk) 2A− k 1 .
Dividing by 2η, reorganizing the equation and summing across iterations yields

T
∇fk(wk), wk − w∗
k=1

T
≤
k=1

w − w∗ 2

k

Ak − Ak−1

2η

2η

ηT +
2 k=1

∇fk(wk) 2A−1 . k

Adaptive Gradient Methods Converge Faster with Over-Parameterization

By convexity of fk, ∇fk(wk), wk − w∗ ≥ fk(wk) − fk(w∗). Using the deﬁnition of regret,

T
R(T ) ≤
k=1

wk − w∗

2

Ak − Ak−1

2η

2η

ηT +
2 k=1

∇fk(wk) 2A−1 . k

We use the Lemmas 3, 4 to bound the RHS by the trace of the last preconditioner,

D2 R(T ) ≤ 2η + η Tr(AT ).

We now bound the trace of AT using Lemma 4 and Individual Smoothness,

√ Tr(AT ) ≤ d

T k=1

∇fk(wk) 2,

√ ≤ 2dLmax

T k=1

fk

(wk

)

−

fk∗,

√ ≤ 2dLmax

T k=1

fk

(wk

)

−

fk

(w∗

)

+

fk

(w∗

)

−

fk∗

,

(Lemma 4, Trace bound) (Individual Smoothness)
(±fk (w∗ ))

≤ 2dLmax R(T ) + σ2T .

Plugging this back into the regret bound,

D2

R(T ) ≤

+η

2η

2dLmax[

2

√

Squaring both sides and denoting a = D2η + η 2dLmax,

R(T ) + σ2T ].

(Since fk(w∗) − fk∗ ≤ σ2)

[R(T )]2 ≤ a2[R(T ) + σ2T ]. Using the quadratic bound (Lemma 5) x2 ≤ α(x + β) implies x ≤ α + √αβ, with

x = R(T ),

α = 1 D2 1 + 2η 2dLmax, 2η

yields the bound,

β = σ2T,

R(T ) ≤ α +

αβ = 1 D2 1 + 2η 2dLmax + 2η

11

2

2 D2 η + 2η dLmaxσ2T .

Adaptive Gradient Methods Converge Faster with Over-Parameterization

C.3 With interpolation, without conservative line-searches

In this section, we show that the conservative constraint ηk+1 ≤ ηk is not necessary if interpolation is satisﬁed. We give the proof for the Armijo line-search, that has better empirical performance, but a worse theoretical dependence on the problem’s constants. For the theorem below, amin is lower-bounded by in practice. A similar proof also works for the Lipschitz line-search.

Theorem 6 (AdaGrad with Armijo line-search under interpolation). Under the same assumptions of Proposition 1, but without non-increasing step-sizes, if interpolation is satisﬁed, AdaGrad with the Armijo line-search and uniform averaging converges at the rate,

∗

D2 + 2ηm2 ax 2dLmax

1 Lmax 2

E[f (w¯T ) − f ] ≤

2T

max

,

.

ηmax amin

where amin = mink{λmin(Ak)}.

Proof of Theorem 6. Following the proof of Proposition 1,

T
2 ηk ∇fik (wk), wk − w∗
k=1

T
=
k=1

wk − w∗ 2Ak −

wk+1 − w∗ 2Ak + ηk2

∇fik (wk) 2A−1 . k

On the left-hand side, we use individual convexity and interpolation, which implies fik (w∗) = minw fik (w) and we can bound ηk by ηmin, giving

ηk ∇fik (wk), wk − w∗ ≥ ηk (fik (wk) − fik (w∗)) ≥ ηmin(fik (wk) − fik (w∗)).

≥0

On the right-hand side, we can apply the AdaGrad lemmas (Lemma 4)

T
wk − w∗ 2Ak − wk+1 − w∗ 2Ak + ηm2 ax ∇fik (wk)

k=1

≤ D2Tr(AT ) + 2ηm2 axTr(AT ),

√ ≤ D2 + 2ηm2 ax d

T k=1

∇fik (wk) 2,

√ ≤ D2 + 2ηm2 ax 2dLmax

T k=1

fik

(wk

)

−

fik

(w∗).

, 2
A− k 1

(By Lemmas 3 and 4) (By the trace bound of Lemma 4) (By Individual Smoothness and interpolation)

√ Deﬁning a = 2η1 D2 + 2ηm2 ax 2dLmax and combining the previous inequalities yields
min

T
(fik (wk) − fik (w∗)) ≤ a
k=1

T k=1

fik

(wk

)

−

fik

(w∗).

Taking expectations and applying Jensen’s inequality yields

T k=1

E[f

(wk )

−

f

(w∗)]

≤

a

T k=1

E

[f

(wk

)

−

f

(w∗

)].

Squaring both sides, dividing by

T k=1

E[f (wk)

−

f (w∗)],

followed

by

dividing

by

T

and

applying

Jensen’s

inequality,

∗

a2

E[f (w¯T ) − f (w )] ≤ T =

D2 + 2ηm2 ax 2dLmax

2η2 T

.

min

Using the Armijo line-search guarantee (Lemma 1) with c = 1/2 and a maximum step-size ηmax,

ηmin = min ηmax, amin , Lmax

where amin = mink{λmin(Ak)}, giving the rate

∗

D2 + 2ηm2 ax 2dLmax

1 Lmax 2

E[f (w¯T ) − f (w )] ≤

2T

max

,

.

ηmax amin

Adaptive Gradient Methods Converge Faster with Over-Parameterization

D Proofs for AMSGrad and non-decreasing preconditioners without momentum
We now give the proofs for AMSGrad and general bounded, non-decreasing preconditioners in the smooth setting, using a constant step-size (Theorem 7) and the Armijo line-search (Theorem 8). As in Appendix C, we prove a general proposition and specialize it for each of the theorems;

Proposition 2. In addition to assumptions of Theorem 1, assume that (iv) the preconditioners are non-decreasing and

have (v) bounded eigenvalues in the [amin, amax] range. If the step-sizes are constrained to lie in the range [ηmin, ηmax]

and satisfy

ηk

∇fik (wk)

2 A−1

≤ M (fi (wk) − fi∗ ),

k

k

for some M < 2,

(8)

k

using

uniform

averaging

w¯T

=

1 T

T k=1

wk

leads

to

the

rate

∗ 1 D2damax

2 ηmax

2

E[f (w¯T ) − f

]≤

T

+ (2 − M )ηmin

−1 2 − M ηmin

σ.

Theorem 7. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with no momentum, constant step-size η = 2aLmmianx and uniform averaging converges at a rate,

∗ 2D2d amax Lmax 2

E[f (w¯T ) − f ] ≤

amin T

+σ .

Proof of Theorem 7. Using Bounded preconditioner and Individual Smoothness, we have that

2

1

2 2Lmax

∗

∇fik (wk) A− k 1 ≤ amin ∇fik (wk) ≤ amin (fik (wk) − fik).

A constant step-size ηmax = ηmin = 2aLmmianx satisﬁes the step-size assumption (Eq. 8) with M = 1 and

1 D2damax + T (2 − M )ηmin

2 ηmax − 1 σ2 = 1 2LmaxD2damax + σ2.

2 − M ηmin

T

amin

Theorem 8. Under the same assumptions as Theorem 1, AMSGrad with zero momentum, Armijo line-search with c = 3/4, a step-size upper bound ηmax and uniform averaging converges at a rate,

∗

3D2d · amax

2

E[f (w¯T ) − f ] ≤

2T + 3ηmaxσ

× max 1 , 2Lmax . ηmax amin

Proof of Theorem 8. For the Armijo line-search, Lemma 1 guarantees that

η

∇fi

(wk )

2
−1

≤

1 (fi

(wk) − f ∗ ),

and

min ηmax, 2 λmin(Ak) (1 − c)

k

Ak

ck

ik

Lmax

≤ η ≤ ηmax.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Selecting c = 3/4 gives M = 4/3 and ηmin = min ηmax, 2aLmmianx , so

1 D2damax + 2 ηmax − 1 σ2 T (2 − M )ηmin 2 − M ηmin

= 1 D2damax + 2 ηmax − 1 σ2, T (2 − 4/3)ηmin 2 − 4/3 ηmin

= 1 3D2damax + 3ηmax − 1 σ2,

T 2ηmin

ηmin

≤ 3D2damax max 2T

1 , 2Lmax ηmax amin

+ 3ηmaxσ2 max

1 , 2Lmax . ηmax amin

Theorem 9. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded

eigenvalues in the [amin, amax] interval, AMSGrad with no momentum, Armijo SPS with c = 3/4 and uniform averaging converges at a rate,

∗

3D2d · amax

2

1 3Lmax

E[f (w¯T ) − f ] ≤

2T + 3ηmaxσ max ηmax , 2amin .

Proof of Theorem 4. For Armijo SPS, Lemma 2 guarantees that

ηk

∇fi

(wk )

2
−1

≤

1 (fi

(wk) − f ∗ ),

and

min ηmax, amin

k

Ak

ck

ik

2c Lmax

Selecting c = 3/4 gives M = 4/3 and ηmin = min ηmax, 32Lam mainx , so

1 D2damax + 2 ηmax − 1 σ2 T (2 − M )ηmin 2 − M ηmin

= 1 D2damax + 2 ηmax − 1 σ2, T (2 − 4/3)ηmin 2 − 4/3 ηmin

= 1 3D2damax + 3ηmax − 1 σ2,

T 2ηmin

ηmin

≤ 3D2damax max 2T

1 , 3Lmax ηmax 2amin

+ 3ηmaxσ2 max

1 , 3Lmax ηmax 2amin

≤ η ≤ ηmax. .

Before diving into the proof of Proposition 2, we prove the following lemma to handle terms of the form ηk(fik (wk) − fik (w∗)). If ηk depends on the function sampled at the current iteration, fik , as in the case of line-search, we cannot take expectations as the terms are not independent. Lemma 6 bounds ηk(fik (wk) − fik (w∗)) in terms of the range [ηmin, ηmax];
Lemma 6. If 0 ≤ ηmin ≤ η ≤ ηmax and the minimum value of fi is fi∗, then
η(fi(w) − fi(w∗)) ≥ ηmin(fi(w) − fi(w∗)) − (ηmax − ηmin)(fi(w∗) − fi∗).

Proof of Lemma 6. By adding and subtracting fi∗, the minimum value of fi, we get a non-negative and a non-positive term multiplied by η. We can use the bounds η ≥ ηmin and η ≤ ηmax separately;
η[fi(w) − fi(w∗)] = η[fi(w) − fi∗ + fi∗ − fi(w∗)],

≥0

≤0

≥ ηmin[fi(w) − fi∗] + ηmax[fi∗ − fi(w∗)].

Adding and subtracting ηminfi(w∗) ﬁnishes the proof,

= ηmin[fi(w) − fi(w∗) + fi(w∗) − fi∗] + ηmax[fi∗ − fi(w∗)], = ηmin[fi(w) − fi(w∗)] + (ηmax − ηmin)[fi∗ − fi(w∗)].

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Proof of Proposition 2. We start with the Update rule, measuring distances to w∗ in the · Ak norm,

wk+1 − w∗ 2Ak = wk − w∗ 2Ak − 2ηk ∇fik (wk), wk − w∗ + ηk2 ∇fik (wk) 2A− k 1 (9)

To bound the RHS, we use the assumption on the step-sizes (Eq. (8)) and Individual Convexity,

− 2ηk ∇fik (wk), wk − w∗ + ηk2 ∇fik (wk) 2A−1 , k

≤ −2ηk ∇fi (wk), wk − w∗ + M ηk(fi (wk) − fi∗ ),

k

k

k

(Step-size assumption, Eq. (8))

≤ −2ηk[fi (wk) − fi (w∗)] + M ηk(fi (wk) − fi∗ ),

k

k

k

k

(Individual Convexity)

≤ −2ηk[fi (wk) − fi (w∗)] + M ηk(fi (wk) − fi (w∗) + fi (w∗) − fi∗ ),

k

k

k

k

k

k

(±fik (w∗))

≤ −(2 − M )ηk[fi (wk) − fi (w∗)] + M ηmax(fi (w∗) − fi∗ ).

k

k

k

k

(ηk ≤ ηmax)

Plugging the inequality back into Eq. (9) and reorganizing the terms yields

(2 − M )ηk[fik (wk) − fik (w∗)] ≤ wk − w∗ 2Ak − wk+1 − w∗ 2Ak (10)

+ M ηmax(fi (w∗) − fi∗ )

k

k

Using Lemma 6, we have that

(2 − M )ηk[fik (wk) − fik (w∗)] ≥ (2 − M )ηmin(fik (wk) − fik (w∗))

− (2 − M )(ηmax − ηmin)(fi (w∗) − fi∗ ).

k

k

Using this inequality in Eq. (10), we have that

(2 − M )ηmin(fi (wk) − fi (w∗)) − (2 − M )(ηmax − ηmin)(fi (w∗) − fi∗ )

k

k

k

k

≤ wk − w∗ 2Ak − wk+1 − w∗ 2Ak + M ηmax(fik (w∗) − fi∗k),

Moving the terms depending on fik (w∗) − fi∗k to the RHS,

(2 − M )ηmin(fik (wk) − fik (w∗)) ≤ wk − w∗ 2Ak − wk+1 − w∗ 2Ak

+ (2ηmax − (2 − M )ηmin)(fi (w∗) − fi∗ ).

k

k

Taking expectations and summing across iterations yields

T

T

(2 − M )ηmin E[fik (wk) − fik (w∗)] ≤ E

wk − w∗ 2Ak − wk+1 − w∗ 2Ak

k=1

k=1

+(2ηmax − (2 − M )ηmin)T σ2.

Using Lemma 3 to telescope the distances and using the Bounded preconditioner,

T

T

wk − w∗ 2Ak − wk+1 − w∗ 2Ak ≤

wk − w∗ 2Ak−Ak−1 ≤ D2 Tr(AT ) ≤ D2 d amax,

k=1

k=1

which guarantees that

T
(2 − M )ηmin E[f (wk) − f (w∗)] ≤D2damax + (2ηmax − (2 − M )ηmin)T σ2.
k=1

Dividing by T (2 − M )ηmin and using Jensen’s inequality ﬁnishes the proof, giving the rate for the averaged iterate,

∗

1 D2damax

E[f (w¯T ) − f (w )] ≤ T (2 − M )ηmin +

2 ηmax − 1 σ2. 2 − M ηmin

Adaptive Gradient Methods Converge Faster with Over-Parameterization
E AMSGrad with momentum
We ﬁrst show the relation between the AMSGrad momentum and heavy ball momentum and then present the proofs with AMSGrad momentum in E.2 and heavy ball momentum in E.3.

E.1 Relation between the AMSGrad update and preconditioned SGD with heavy-ball momentum

Recall that the AMSGrad update is given as:

wk+1 = wk − ηk A− k 1mk ; mk = βmk−1 + (1 − β)∇fik (wk)

Simplifying,

wk+1 = wk − ηk A− k 1(βmk−1 + (1 − β)∇fik (wk))

wk+1

=

wk

−

ηk (1

−

β)

A

− k

1

∇

f

ik

(wk

)

−

ηk β

A− k 1mk−1

From the update at iteration k − 1,

=⇒

wk

=

wk−1

−

ηk−1

A m −1 k−1

k−1

1 −mk−1 = ηk−1 Ak−1 (wk − wk−1)

From the above relations,

wk+1 = wk − ηk(1 − β) A−1∇fi (wk) + β ηk A−1A (wk − wk−1)

k

k

ηk−1 k k−1

which is of the same form as

wk+1

=

wk

−

ηk

A−1 k

+

γ(wk

−

wk−1),

the update with heavy ball momentum. The two updates are equivalent up to constants except for the key difference that for

AMSGrad,

the

momentum

vector

(wk

−

wk−1)

is

further

preconditioned

by

A A . −1

k

k−1

Adaptive Gradient Methods Converge Faster with Over-Parameterization

E.2 Proofs for AMSGrad with momentum

We now give the proofs for AMSGrad having the update.

wk+1 = wk − ηk A− k 1mk ; mk = βmk−1 + (1 − β)∇fik (wk)

We analyze it in the smooth setting using a constant step-size (Theorem 2), conservative Armijo SPS (Theorem 4) and conservative Armijo SLS (Theorem 10). As before, we abstract the common elements to a general proposition and specialize it for each of the theorems.

Proposition 3. In addition to assumptions of Theorem 1, assume that (iv) the preconditioners are non-decreasing and have (v) bounded eigenvalues in the [amin, amax] range. If the step-sizes are lower-bounded and non-increasing, ηmin ≤ ηk ≤ ηk−1 and satisfy

2

∗

1−β

ηk ∇fik (wk) A−1 ≤ M (fik (wk) − fik), k

for some M < 2

,

1+β

(11)

using

uniform

averaging

w¯T

=

1 T

T k=1

wk

leads

to

the

rate

∗ 1+β

1 + β −1 D2damax

2

E[f (w¯T ) − f ] ≤ 1 − β

2−

M

1−β

+ Mσ . ηminT

We ﬁrst show how the convergence rate of each step-size method can be derived from Proposition 3.

Theorem 2. Under the same assumptions as Theorem 1, and assuming (iv) non-decreasing preconditioners

(v) bounded eigenvalues in the [amin, amax] interval, where κ = / , amax amin AMSGrad with β ∈ [0, 1), constant step-size η = 11−+ββ 2aLmmianx and uniform averaging converges at a rate,

∗

1 + β 2 2LmaxD2dκ 2

E[f (w¯T ) − f ] ≤ 1 − β

+σ . T

Proof of Theorem 2. Using Bounded preconditioner and Individual Smoothness, we have that

2

1

2

2Lmax

∗

η

∇fik (wk)

A− k 1

≤η amin

∇fik (wk)

≤ η amin (fik (wk) − fik).

Using a constant step-size η = 11−+ββ 2aLmmianx satisﬁes the requirement of Proposition 3 (Eq. (11)) with constant M = 11−+ββ . The convergence is then,

∗ 1+β

1 + β −1 D2damax

2

E[f (w¯T ) − f (w )] ≤ 1 − β

2−

M

1−β

+ Mσ , ηminT

1+β =
1−β

D2damax 1 − β 2

1−β

amin

T

+

σ

1+β

,

1+β 2Lmax

= 1 + β 2 2LmaxD2dκ + σ2,

1−β

T

with κ = amax/amin.

Theorem 4. Under the same assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded eigenvalues in the [amin, amax] interval with κ = / , amax amin AMSGrad with β ∈ [0, 1), conservative Armijo SPS with c = 1+β/1−β and uniform averaging converges at a rate,

E[f (w¯T ) − f ∗] ≤

1 + β 2 2LmaxD2dκ + σ2.

1−β

T

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Proof of Theorem 4. For Armijo SPS, Lemma 2 guarantees that

ηk

∇fi

(wk )

2
−1

≤

1 (fi

(wk) − f ∗ ),

and

amin ≤ ηk.

k

Ak

ck

ik

2c Lmax

Setting c = 11−+ββ ensures that M = 1/c satisﬁes the requirement of Proposition 3 and ηmin ≥ 11−+ββ 2aLmmianx . Plugging in these values into Proposition 3 completes the proof.

Theorem 10. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded

eigenvalues in the [amin, amax] interval, AMSGrad with momentum with parameter β ∈ [0, 1/5), conservative Armijo SLS with c = 23 11−+ββ and uniform averaging converges at a rate,

∗

1 + β LmaxD2dκ

2

E[f (w¯T ) − f

]≤3 1 − 5β

T

+ 3σ

Proof of Theorem 10. For Armijo SLS, Lemma 1 guarantees that

ηk

∇fi

(wk )

2
−1

≤

1 (fi

(wk) − f ∗ ),

and

2(1 − c) amin ≤ ηk.

k

Ak

ck

ik

Lmax

The line-search parameter c is restricted to [0, 1] and relates to the the requirement parameter M of Proposition 3 (Eq. (11)) through M = 1/c. The combined requirements on M are then that 1 < M < 2 11−+ββ , which is only feasible if β < 13 . To leave room to satisfy the constraints, let β < 51 .

Setting 1c = M = 32 11−+ββ satisﬁes the constraints and requirement for Proposition 3, and

∗ 1+β

1 + β −1 D2damax

2

E[f (w¯T ) − f (w )] ≤ 1 − β

2−

M

1−β

+ Mσ , ηminT

1+β

3 −1

=

2−

Lmax D2damax + 3 1 − β σ2 ,

1−β

2

2(1 − c) amin T

21+β

= 1 + β Lmax D2dκ + 3σ2 = 3 1 + β LmaxD2dκ + 3σ2.

1 − β (1 − c) T

1 − 5β T

where the last step substituted 1/(1 − c),

2 1 + β 3(1 − β) − 2(1 + β) 1 1 − 5β

1−c=1−

=

=

.

31−β

3(1 − β)

3 1−β

Before diving into the proof of Proposition 3, we prove the following lemma, Lemma 7. For any set of vectors a, b, c, d, if a = b + c, then, a − d 2 = b − d 2 − a − b 2 + 2 c, a − d

Proof. Since c = a − b,

a − d 2 = b + c − d 2 = b − d 2 + 2 c, b − d + c 2
= b − d 2 + 2 a − b, b − d + a − b 2 = b − d 2 + 2 a − b, b − a + a − d + a − b 2 = b − d 2 + 2 a − b, b − a + 2 a − b, a − d + a − b 2 = b − d 2 − 2 a − b 2 + 2 a − b, a − d + a − b 2 = b − d 2 − a − b 2 + 2 c, a − d

We now move to the proof of the main proposition. Our proof follows the structure of Reddi et al. (2018); Alacaoglu et al. (2020).

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Proof of Proposition 3. To reduce clutter, let Pk = Ak/ηk. Using the update, we have the expansion

wk+1 − w∗ = wk − Pk−1mk − w∗,

=

wk

−

(1

−

β

)P

− k

1

∇

f

i

k

(w

k

)

−

βPk−1mk−1

− w∗,

Measuring distances in the · P -norm, such that x 2P = x, Pkx ,

k

k

wk+1 − w∗ 2Pk = wk − w∗ 2Pk − 2(1 − β) wk − w∗, ∇fik (wk) ,

− 2β wk − w∗, mk−1 + mk 2P −1 . k

We separate the distance to w∗ from the momentum in the second inner product using the update and Lemma 7 with a = c = Pk1−/21(wk − w∗), b = 0, d = Pk1−/21(wk−1 − w∗).

−2 mk−1, wk − w∗ = −2 Pk−1(wk−1 − wk), wk − w∗ ,

= wk − wk−1 2Pk−1 + wk − w∗ 2Pk−1 − wk−1 − w∗ 2Pk−1 = mk−1 2Pk−−11 + wk − w∗ 2Pk−1 − wk−1 − w∗ 2Pk−1 , ≤ mk−1 2Pk−−11 + wk − w∗ 2Pk − wk−1 − w∗ 2Pk−1 ,

where the last inequality uses the fact that ηk ≤ ηk−1 and Ak Ak−1, which implies Pk wk − w∗ 2Pk . Plugging this inequality in and grouping terms yields

Pk−1, and

, wk − w∗

≤ 2
Pk−1

2(1 − β) wk − w∗, ∇fik (wk) ≤ wk − w∗ 2Pk − wk+1 − w∗ 2Pk

+ β wk − w∗ 2Pk − wk−1 − w∗ 2Pk−1

+β

mk−1

2 P −1

+

mk

2 P −1

k−1

k

By convexity, the inner product on the left-hand-side is bounded by wk − w∗, ∇fik (wk) ≥ fik (wk) − fik (w∗). The ﬁrst two lines of the right-hand-side will telescope if we sum all iterations, so we only need to treat the norms of the momentum

terms. We introduce a free parameter δ ≥ 0, that is only used for the analysis, and expand

β

mk−1

2 P −1

+

mk 2P −1 = β

mk−1

2 P −1

+ (1 + δ)

mk

2P −1 − δ

mk

2P −1 .

k−1

k

k−1

k

k

To bound mk 2P −1 , we expand it by its update and use Young’s inequality to get k

mk 2P −1 =

βmk−1 + (1 − β)∇fik (wk)

2 P −1

k

k

≤ (1 + )β2 mk−1 2P −1 + (1 + 1/ )(1 − β)2 ∇fik (wk) 2P −1 ,

k

k

where > 0 is also a free parameter, introduced to control the tradeoff of the bound. Plugging this bound in the momentum

terms, we get

β

mk−1

2 P −1

+

mk 2P −1 ≤ β

mk−1

2 P −1

+ (1 +

)(1 + δ)β2

mk−1 2P −1 − δ

mk

2P −1 ,

k−1

k

k−1

k

k

+ (1 + 1/ )(1 + δ)(1 − β)2 ∇fik (wk) 2P −1 . k

As Pk−1

Pk−−11, we have that

mk−1 2P −1 ≤

mk−1

2 P −1

which implies

k

k−1

≤ β + (1 + )(1 + δ)β2

mk−1

2 P −1

−δ

mk

2 P −1

k−1

k

+ (1 + 1/ )(1 + δ)(1 − β)2 ∇fik (wk) 2P −1 . k

To get a telescoping sum, we set δ to be equal to β + (1 + )(1 + δ)β2, which is satisﬁed if δ = β1−+((11++ ))ββ22 , and δ > 0 is satisﬁed if β < 1/√1+ . We now plug back the inequality

β

mk−1

2 P −1

+

mk 2P −1 ≤ δ

mk−1

2 P −1

−

mk

2 P −1

k−1

k

k−1

k

+ (1 + 1/ )(1 + δ)(1 − β)2 ∇fik (wk) 2P −1 , k

Adaptive Gradient Methods Converge Faster with Over-Parameterization

in the previous expression to get 2(1 − β) (fik (wk) − fik (w∗)) ≤ wk − w∗ 2Pk − wk+1 − w∗ 2Pk + β wk − w∗ 2Pk − wk−1 − w∗

2 Pk−1

+δ

m − m 2
k−1 Pk−−11

2 k Pk−1

+ (1 + 1/ )(1 + δ)(1 − β)2 ∇fik (wk) 2P −1 . k

All terms now telescope, except the gradient norm which we bound using the step size assumption,

∇fik (wk)

2 P −1

= ηk

∇fik (wk)

2 A−1

≤ M (fi (wk) − fi∗ ),

k

k

k

k

= M (fi (wk) − fi (w∗)) + M (fi (w∗) − fi∗ ).

k

k

k

k

This gives the expression α (fik (wk) − fik (w∗)) ≤ wk − w∗ 2Pk − wk+1 − w∗ 2Pk + β wk − w∗ 2Pk − wk−1 − w∗

2 Pk−1

+δ

m − m 2
k−1 Pk−−11

2 k Pk−1

+ (1 + 1/ )(1 + δ)(1 − β)2M (fi (w∗) − fi∗ ),

k

k

with α = 2(1 − β) − (1 + 1/ )(1 + δ)(1 − β)2M . Summing all iterations, the individual terms are bounded by the Bounded iterates and Lemma 3;

T
wk − w∗ 2Pk − wk+1 − w∗ 2Pk
k=1

≤ D2Tr(PT )

D2 ≤ ηmin Tr(AT )

T
β
k=1

wk − w∗ 2Pk −

wk−1 − w∗ 2Pk−1

≤ β wT − w∗ 2PT

D2 ≤ β ηmin Tr(AT )

T
δ
k=1

m − 2
k−1 Pk−−11

m2
k Pk−1

≤ δ m0 2P0

= 0.

Using the boundedness of the preconditioners gives Tr(AT ) ≤ damax and the total bound

T

∗

(1 + β)D2damax

T 2

∗

∗

α (fik (wk) − fik (w )) ≤

ηmin

+ (1 + 1/ )(1 + δ)(1 − β) M (fik (w ) − fik).

k=1

k=1

Taking expectations,

T

∗ (1 + β)D2damax

22

α E[f (wk) − f (w )] ≤

ηmin

+ (1 + 1/ )(1 + δ)(1 − β) M σ T.

k=1

It remains to expand α and simplify the constants. We had deﬁned

where

2

β + (1 + )β2

α = 2(1 − β) − (1 + 1/ )(1 + δ)(1 − β) M > 0,

and

δ = 1 − (1 + )β2 > 0,

√

> 0 is a free parameter. This puts the requirement on β that β < 1/ 1 + . To simplify the bounds, we set

β = 1/(1 + ), = 1/β − 1, which gives the substitutions

1 1+ =
β Plugging those into the rate gives

11 1+ =
1−β

β δ=2
1−β

1+β

1+δ =

.

1−β

T

∗ (1 + β)D2damax

2

α E[f (wk) − f (w )] ≤

ηmin

+ (1 + β)M σ T,

k=1

Adaptive Gradient Methods Converge Faster with Over-Parameterization

while plugging them into α gives

α = 2(1 − β) − (1 + 1/ )(1 + δ)(1 − β)2M,

1+β

1−β

= (1 − β) 2 −

M , which is positive if M < 2

.

1−β

1+β

Dividing by αT , using Jensen’s inequality and averaging ﬁnishes the proof, with the rate

T

1+β

[f (wk) − f (w∗)] ≤

E

1−β

k=1

1+β

2−

M

1−β

−1 D2damax + M σ2 . ηminT

Adaptive Gradient Methods Converge Faster with Over-Parameterization

E.3 Proofs for AMSGrad with heavy ball momentum

We now give the proofs for AMSGrad with heavy ball momentum with the update.

wk+1

=

wk

−

ηk

A

−1 k

∇

fi

k

(

wk

)

+

γ

(wk

−

wk−1)

We analyze it in the smooth setting using a constant step-size (Theorem 11), a conservative Armijo SPS (Theorem 12) and conservative Armijo SLS (Theorem 13). As before, we abstract the common elements to a general proposition and specialize it for each of the theorems.

Proposition 4. In addition to assumptions of Theorem 1, assume that (iv) the preconditioners are non-decreasing

and have (v) bounded eigenvalues in the [amin, amax] range. If the step-sizes are lower-bounded and non-increasing,

ηmin ≤ ηk ≤ ηk−1 and satisfy

ηk

∇fik (wk)

2 A−1

≤ M (fi (wk) − fi∗ ),

k

k

for some M < 2 − 2γ,

(12)

k

AMSGrad

with

heavy

ball

momentum

with

parameter

γ

<

1

and

uniform

averaging

w¯T

=

1 T

T k=1

wk

leads

to

the

rate

E[f (w¯T ) − f ∗] ≤

1

1

2 − 2γ − M T

2(1 + γ2)D2amaxd + 2γ[f (w0) − f (w∗)] ηmin

+ Mσ2 .

We ﬁrst show how the convergence rate of each step-size method can be derived from Proposition 4.

Theorem 11. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded

eigenvalues in the [amin, amax] range, AMSGrad with heavy ball momentum with parameter γ ∈ [0, 1), constant step-size η = 2am3iLnm(1ax−γ) and uniform averaging converges at a rate

∗ 1 9 1 + γ2

2

3γ

∗

2

E[f (w¯T ) − f ] ≤ T 2 (1 − γ)2 Lmax D κd + (1 − γ) [f (w0) − f (w )] + 2σ .

Proof of Theorem 11. Using Bounded preconditioner and Individual Smoothness, we have that

2

1

2

2Lmax

∗

η

∇fik (wk)

A− k 1

≤η amin

∇fik (wk)

≤ η amin (fik (wk) − fik).

A constant step-size η = 2amin (1−γ)/3Lmax means the requirement for Proposition 4 is satisﬁed with M = 43 (1 − γ). Plugging (2 − 2γ − M ) = 23 (1 − γ) in Proposition 4 ﬁnishes the proof.

Theorem 12. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with heavy ball momentum with parameter γ ∈ [0, 1), conservative Armijo SPS with c = 3/4(1−γ) and uniform averaging converges at a rate,

∗ 1 9 1 + γ2

2

3γ

∗

2

E[f (w¯T ) − f ] ≤ T 2 (1 − γ)2 LmaxD κd + (1 − γ) [f (w0) − f (w )] + 2σ .

Proof of Theorem 12. For Armijo SPS, Lemma 2 guarantees that

ηk

∇fi

(wk )

2
−1

≤

1 (fi

(wk) − f ∗ ),

and

amin ≤ ηk.

k

Ak

ck

ik

2c Lmax

Selecting c = 3/4(1−γ) gives M = 4/3(1 − γ) ≤ 2(1 − γ) and the requirement of Proposition 4 are satisﬁed. The minimum
step-size is then ηmin = 2caLmminax = 2am3iLnm(1ax−γ) , so ηmin and M are the same as in the constant step-size case (Theorem 11) and the same rate applies.

Theorem 13. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing preconditioners (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with heavy ball momentum with parameter γ ∈ [0, 1/4), conservative Armijo SLS with c = 3/4(1−γ) and uniform averaging converges at a rate,

∗ 1 1 + γ2

2

3γ

∗

2

E[f (w¯T ) − f ] ≤ T 6 1 − 4γ LmaxD κd + (1 − γ) [f (w0) − f (w )] + 2σ .

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Proof of Theorem 13. Selecting c = 3/4(1−γ) is feasible if γ < 1/4 as c < 1. The Armijo SLS (Lemma 1) then guarantees that

ηk

∇fi

(wk )

2
−1

≤

1 (fi

(wk) − f ∗ ),

and

2(1 − c) amin ≤ η,

k

Ak

ck

ik

Lmax

which satisﬁes the requirements of Proposition 4 with M = 43 (1 − γ). Plugging M in the rate yields

∗

1 1 + γ2 D2amaxd

3γ

∗

2

E[f (w¯T ) − f (w )] ≤ T

6 1−γ

ηmin

+ (1 − γ) [f (w0) − f (w )] + 2σ ,

With c = 13−/4γ , ηmin ≥ 2(1L−mc)aaxmin = 2Lammaixn 41(−1−4γγ) . Plugging it into the above bound yields

∗

1 1 + γ2

2

3γ

∗

2

E[f (w¯T ) − f (w )] ≤ T 6 1 − 4γ LmaxD κd + (1 − γ) [f (w0) − f (w )] + 2σ .

We now move to the proof of the main proposition. Our proof follows the structure of Ghadimi et al. (2015); Sebbouh et al. (2020).

Proof of Proposition 4. Recall the update for AMSGrad with heavy-ball momentum,

wk+1

=

wk

−

η

k

A

− k

1

∇

fi

k

(

wk

)

+

γ(wk

−

wk−1).

(13)

The proof idea is to analyze the distance from w∗ to wk and a momentum term,

δk 2 = wk + mk − w∗ 2Ak ,

where

mk

=

γ 1−γ

(wk

− wk−1),

(14)

by considering the momentum update (Eq. 13) as a preconditioned step on the joint iterates (wk + mk),

wk+1

+

mk+1

=

wk

+

mk

−

ηk 1−γ

A−k 1

∇fik

(wk

).

(15)

Let us verify Eq. (15). First, expressing wk+1 + mk+1 as a weighted difference of wk+1 and wk,

wk+1

+ mk+1

=

wk+1

+

γ 1−γ

(wk

+1

− wk)

=

1 1−γ

wk

+1

−

γ 1−γ

wk

.

Expanding wk+1 in terms of the update rule then gives

=

1 1−γ

(wk

− ηkA−k 1∇fik (wk) + γ(wk

− wk−1)) −

γ 1−γ

wk

,

=

1 1−γ

(wk

− ηkA−k 1∇fik (wk) − γwk−1),

=

1 1−γ

wk

−

γ 1−γ

wk

−1

−

ηk 1−γ

A−k 1

∇fik

(wk

),

which

can

then

be

re-written

as

wk

+

mk

−

ηk 1−γ

A

−1 k

∇

fi

k

(wk

).

The

analysis

of

the

method

then

follows

similar

steps

as

the

analysis without momentum. Using Eq. (15), we have the recurrence

δk+1 2

= wk+1 + mk+1 − w∗ 2

=

2
wk + mk − ηk A−1∇fi (wk) − w∗

,

Ak

Ak

1−γ k

k

Ak

2

2ηk

η2

(16)
2

=

δk Ak − 1 − γ ∇fik (wk), wk + mk − w∗ + (1 −kγ)2

∇fik (wk) A−1 . k

To bound the inner-product, we use Individual Convexity to relate it to the optimality gap,

∇fik (wk), wk + mk − w∗

= ∇fi (wk), wk − w∗ + γ ∇fi (wk), wk − wk−1 ,

k

1−γ k

≥ fi (wk) − fi (w∗) + γ [fi (wk) − fi (wk−1)],

k

k

1−γ k

k

= 1 [fi (wk) − fi (w∗)] − γ [fi (wk−1) − fi (w∗)].

1−γ k

k

1−γ k

k

To bound the gradient norm, we use the step-size assumption that

ηk ∇fik (wk) 2A− k 1 ≤ M [fik (wk) − fi∗k ] = M [fik (wk) − fik (w∗)] + M [fik (w∗) − fi∗k ].

For simplicity of notation, let us deﬁne the shortcuts

hk(w) = fik (w) − fik (w∗),

σk2 = fik (w∗) − fi∗k .

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Plugging those two inequalities in the recursion of Eq. (16) gives δk+1 2Ak ≤ δk 2Ak − (1 −ηkγ)2 (2 − M )hk(wk) + (12−ηkγγ)2 hk(wk−1) + (1M−ηγk)2 σk2.

We can now divide by ηk/(1−γ)2 and reorganize the inequality as (1 − γ)2
(2 − M )hk(wk) − 2γhk(wk−1) ≤ ηk Taking the average over all iterations, the inequality yields

δk 2Ak − δk+1 2Ak + M σk2.

1T

1 T (1 − γ)2

T (2 − M )hk(wk) − 2γhk(wk−1) ≤ T ηk

k=1

k=1

δk 2Ak − δk+1 2Ak + M σk2.

To bound the right-hand side, under the assumption that the iterates are bounded by wk − w∗ ≤ D, we use Young’s inequality to get a bound on δk 2;

δk 22 = wk + mk − w∗ 22 =

2

1 1−γ

(wk

−

w∗)

−

γ 1−γ

(wk

−1

−

w∗)

2

2 ≤ (1 − γ)2

∗2

2

∗2

2(1 + γ2) 2

2

wk − w 2 + γ wk−1 − w 2 ≤ (1 − γ)2 D = ∆ .

Given the upper bound δk 2 ≤ ∆, a reorganization of the sum lets us apply Lemma 3 to get

T1 k=1 ηk

δk 2Ak − δk+1 2Ak = =

T k=1
T k=1

δk 2η1k Ak − δk 2η1k Ak −

T k=1
T +1 k=2

δ2
k+1 η1k Ak
δ2
k ηk1−1 Ak−1

≤ δ − δ + δ T
k=1

2 k η1k Ak

T k=1

2 k ηk1−1 Ak−1

2 1 η10 A0

= δ ≤ , T
k=1

2 k η1k Ak − ηk1−1 Ak−1

∆2 amax d ηmin

where the last step uses the convention A0 = 0 and Lemma 3 on δk instead of wk − w∗. Plugging this inequality in, we get

the simpler bound on the right-hand-side

1T

2(1 + γ2)D2amaxd

2

T (2 − M )hk(wk) − 2γhk(wk−1) ≤

T ηmin

+ M σk.

k=1

Now that the step-size is bounded deterministically, we can take the expectation on both sides to get

1T T E (2 − M )h(wk) − 2γh(wk−1)
k=1

≤ 2(1 + γ2)D2amaxd + M σ2, T ηmin

where h(w) = f (w) − f ∗ and σ2 = E fik (w∗) − fi∗k . To simplify the left-hand-side, we change the weights on the optimality gaps to get a telescoping sum,

Tk=1(2 − M )h(wk) − 2γh(wk−1) = Tk=1(2 − 2γ − M )h(wk) + 2γh(wk) − 2γh(wk−1),

= (2 − 2γ − M )

T k=1

h(wk

)

+ 2γ(h(wT ) − h(w0)),

≥(2 − 2γ − M )

T k=1

h(wk

)

− 2γh(w0).

The last inequality uses h(wT ) ≥ 0. Moving the initial optimality gap to the right-hand-side, we get

1

T

1

T (2 − 2γ − M ) E h(wk) ≤ T

k=1

2(1 + γ2)D2amaxd + 2γh(w0)
ηmin

+ Mσ2.

Assuming 2 − 2γ − M > 0 and dividing, we get

1T

1

1

T E h(wk) ≤ 2 − 2γ − M T

k=1

2(1 + γ2)D2amaxd + 2γh(w0)
ηmin

Using Jensen’s inequality and averaging the iterates ﬁnishes the proof.

+ Mσ2 .

Adaptive Gradient Methods Converge Faster with Over-Parameterization
F Experimental details
Our proposed adaptive gradient methods with SLS and SPS step-sizes are presented in Algorithms 1 and 3. We now make a few additional remarks on the practical use of these methods.

Algorithm 1 Adaptive methods with SLS(f , precond, β, conservative, mode, w0, ηmax, b, c ∈ (0, 1), γ < 1)

1: for k = 0, . . . , T − 1 do

2: ik ← sample mini-batch of size b

3: Ak ← precond(k)

Form the preconditioner.

4: if mode == Lipschitz then

5:

pk ← ∇fik (wk)

6: else if mode == Armijo then

7:

pk

←

A

− k

1

∇

fi

k

(

wk

)

8: end if

9: if conservative then

10:

if k == 0 then

11:

ηk ← ηmax

12:

else

13:

ηk ← ηk−1

14:

end if

15: else

16:

ηk ← ηmax

17: end if

18: while fik (wk − ηk · pk) > fik (wk) − c ηk ∇fik (wk), pk do

19:

ηk ← γ ηk

Line-search loop

20: end while

21: mk ← βmk−1 + (1 − β)∇fik (wk) 22: wk+1 ← wk − ηkA−k 1mk 23: end for

24: return wT

Algorithm 2 reset(η, ηmax, k, b, n, γ, opt) 1: if k = 0 then 2: return ηmax 3: else if opt= 0 then 4: η ← η 5: else if opt= 1 then 6: η ← η · γb/n 7: else if opt= 2 then 8: η ← ηmax 9: end if 10: return η
As suggested by Vaswani et al. (2019b), the standard backtracking search can sometimes result in step-sizes that are too small while taking bigger steps can yield faster convergence. To this end, we adopted their strategies to reset the initial step-size at every iteration (Algorithm 2). In particular, using reset option 0 corresponds to starting every backtracking line search from the step-size used in the previous iteration. Since the backtracking never increases the step-size, this option enables the “conservative step-size“ constraint for the Lipschitz line-search to be automatically satisﬁed. For the Armijo line-search, we use the heuristic from (Vaswani et al., 2019b) corresponding to reset option 1. This option begins every backtracking with a slightly larger (by a factor of γb/n, γ = 2 throughout our experiments) step-size compared to the step-size at the previous iteration, and works well consistently across our experiments. Although we do not have theoretical guarantees for Armijo SLS with general preconditioners such as Adam, our experimental results indicate that this is in fact a

Adaptive Gradient Methods Converge Faster with Over-Parameterization

promising combination that also performs well in practice.

Algorithm 3 Adaptive methods with SPS(f , [fi∗]ni=1, precond, β,conservative, mode, w0, ηmax, b, c)

1: for k = 0, . . . , T − 1 do

2: ik ← sample mini-batch of size b

3: Ak ← precond(k)

Form the preconditioner

4: if mode == Lipschitz then

5:

pk ← ∇fik (wk)

6: else if mode == Armijo then

7:

pk

←

A

− k

1

∇

fi

k

(

wk

)

8: end if

9: if conservative then

10:

if k == 0 then

11:

ηB ← ηmax

12:

else

13:

ηB ← ηk−1

14:

end if

15: else

16:

ηB ← ηmax

17: end if 18: ηk ← min c f∇ikf(iwkk(w)−kf),i∗kp)k , ηB 19: mk ← βmk−1 + (1 − β)∇fik (wk) 20: wk+1 ← wk − ηkA−k 1mk 21: end for

22: return wT

On the other hand, rather than being too conservative, the step-sizes produced by SPS between successive iterations can vary wildly such that convergence becomes unstable. Loizou et al. (2020) suggested to use a smoothing procedure that limits the growth of the SPS from the previous iteration to the current. We use this strategy in our experiments with τ = 2b/n and show that both SPS and Armijo SPS work well. For the convex experiments, for both SLS and SPS, we set c = 0.5 as is suggested by the theory. For the non-convex experiments, we observe that all values of c ∈ [0.1, 0.5] result in reasonably good performance, but use the values suggested in (Vaswani et al., 2019b; Loizou et al., 2020), i.e. c = 0.1 for all adaptive methods using SLS and c = 0.2 for methods using SPS.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

G Additional experimental results

This section presents additional experimental results showing the effect of the step-size for adaptive gradient methods using a synthetic dataset (Fig. 5). We show the wall-clock times for the optimization methods (Fig. 6). We show the variation in the step-size for the SLS methods when training deep networks for both the CIFAR in Fig. 7 and ImageNet (Fig. 8) datasets. We evaluate these methods on easy non-convex objectives - classiﬁcation on MNIST (Fig. 9) and deep matrix factorization to examine the effect of over-parameterization on the performance of the optimization methods (Fig. 10). Finally in Fig. 11, we quantify the gains of incorporating momentum in AMSGrad by comparing against the performance AMSGrad without momentum.

Train loss (log)

101 100 10 1 10 2 0

Margin:0.01
50 E1p0oc0h 150

101 100 10 1 10 2
200 0 Adagrad

Margin:0.05

100

10 2

10 4

10 6

50 E1p0oc0h Default Adagrad

10 8 150 200 0
Adagrad + Lipschitz LS

Margin:0.1
10 1
10 3
10 5
10 7 50 E1p0oc0h 150 200 0
Adagrad + Armijo LS

Margin:0.5
50 E1p0oc0h 150 200

(a) AdaGrad

Train loss (log)

101 100 10 1 10 2
0

Margin:0.01
50 E1p0oc0h 150

101 10 1 10 3 10 5 10 7 200 0

Margin:0.05
50 E1p0oc0h 150 Amsgrad

Margin:0.1

100

10 1

10 2

10 4

10 3

10 6

10 8

10 5

200 0 50 E1p0oc0h 150 200 0

Default Amsgrad

Amsgrad + SLS

Margin:0.5
50 E1p0oc0h 150 200

(b) AMSGrad

Figure 5. Effect of step-size on the performance of adaptive gradient methods for binary classiﬁcation on a linearly separable synthetic dataset with different margins. We observe that the large variance for the adaptive gradient methods, and the variants with SLS have consistently good performance across margins and optimizers.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Average training time/epoch

Average training time/epoch

113.C3I9F7AR10 - ResNet34

100

80

77.038 75.912

60

77.012 62.962 63.203 91.689

40

20

0

Methods

(a)

300

289.624

CIFAR100 - DenseNet121

220500 203.612 221.197

150

100

114.146 98.339 87.487 108.076

50

0

Methods

(b)

CIFAR100 - ResNet34 100

80

78.195 75.188 81.050

103.426

60

40

40.329 31.368 26.686

20

0

Methods

(c)

175

174.929

Tiny ImageNet - ResNet18

150

154.330

125

126.805

115.225 111.041

100

100.100

75

82.629

50

25

0

Methods

(d)

Amsgrad + SLS Amsgrad + SLS + HB Adagrad + SLS Adabound Radam Adam SLS Amsgrad + SLS Amsgrad + SLS + HB Adagrad + SLS Adabound Radam Adam SLS Amsgrad + SLS Amsgrad + SLS + HB Adagrad + SLS Adabound Radam Adam SLS Amsgrad + SLS Amsgrad + SLS + HB Adabound Radam Adam SLS Adagrad + SLS

Average training time/epoch

Average training time/epoch

Figure 6. Runtime (in seconds/epoch) for optimization methods for multi-class classiﬁcation using the deep network models in Fig. 3. Although the runtime/epoch is larger for the SLS/SPS variants, they require fewer epochs to reach the maximum test accuracy (Figure 3). This justiﬁes the moderate increase in wall-clock time.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Train loss (log)

Train loss (log)

100 10 1 10 2 10 3
0
100 10 1 10 2 10 3
0
100 10 1 10 2 10 3 0
100 10 1 10 2 10 3

CIFAR10-ResNet34

CIFAR10-ResNet34 0.94

Validation accuracy

0.92

0.90

0.88

50 100 150 200 0.860 Epoch

50 E1p0o0ch 150

Adagrad + SLS

Adabound

Radam

Adam

SLS

Step size (log)

CIFAR10-ResNet34
100 10 2 10 4 10 6 10 8 10 10 10 12

200

0

50 E1p0o0ch 150 200

Amsgrad + SLS

Amsgrad + SLS + HB

(a) CIFAR-10 ResNet

CIFAR10-DenseNet121

CIFAR10-DenseNet121 0.94

Validation accuracy

0.92

0.90

0.88

50 100 150 200 0.860 Epoch

50 E1p0o0ch 150

Adagrad + SLS

Adabound

Radam

Adam

SLS

CIFAR10-DenseNet121 100

10 3

Step size (log)

10 6

10 9

10 12

200

0

50 E1p0o0ch 150 200

Amsgrad + SLS

Amsgrad + SLS + HB

(b) CIFAR-10 DenseNet

CIFAR100-ResNet34

50 E1p0o0ch Adagrad + SLS

150 200 Adabound

Validation accuracy

0.76 0.74 0.72 0.70 0.68 0.66
0
Radam

CIFAR100-ResNet34

50 E1p0o0ch 150

Adam

SLS

CIFAR100-ResNet34 101

Step size (log)

10 1

10 3

10 5

200

0

50 E1p0o0ch 150 200

Amsgrad + SLS

Amsgrad + SLS + HB

(c) CIFAR-100 ResNet

CIFAR100-DenseNet121

50 E1p0o0ch Adagrad + SLS

150 200 Adabound

Validation accuracy

0.76 CIFAR100-DenseNet121

0.74

0.72

0.70

0.68

0.66

50 E1p0o0ch 150

Radam

Adam

SLS

102

CIFAR100-DenseNet121

100

Step size (log)

10 2

10 4

10 6

10 8

200

50 E1p0o0ch 150 200

Amsgrad + SLS

Amsgrad + SLS + HB

(d) CIFAR-100 DenseNet

Train loss (log)

Train loss (log)

Figure 7. Comparing optimization methods on image classiﬁcation tasks using ResNet and DenseNet models on the CIFAR-10/100 datasets. For the SLS/SPS variants, refer to the experimental details in Appendix F. For Adam, we did a grid-search and use the best step-size. We use the default hyper-parameters for the other baselines. We observe the consistently good performance of AdaGrad and AMSGrad with Armijo SLS. We also show the variation in the step-size and observe a cyclic pattern (Loshchilov & Hutter, 2017) - an initial warmup in the learning rate followed by a decrease or saturation to a small step-size (Goyal et al., 2017).

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Train loss (log)

Train loss (log)

Imagewoof-ResNet18 100

0.700

Imagewoof-ResNet18

0.675

Validation accuracy

10 1

0.650

10 2

0.625

10 3

0.600

0.575

10 4

0.550

10 5

0.525

10 6 20 40 60 80 100 0.500 20 40 60 80

Epoch

Epoch

Adabound

Radam

Adam

SLS

Amsgrad + SLS

(a) Imagewoof

Imagenette-ResNet18 100

0.84

Imagenette-ResNet18

0.83

Validation accuracy

10 1

0.82

10 2

0.81

10 3

0.80

0.79

10 4

0.78

10 5

0.77

10 6 0 20 40 60 80 100 0.760 20 40 60 80

Epoch

Epoch

Adabound

Radam

Adam

SLS

Amsgrad + SLS

(b) ImageNette

100 10 1 10 2 10 3
50

Tiny ImageNet-ResNet18

75 100 Ep1oc2h5 150 175

Adabound

Radam

Validation accuracy

0.40 0.39 0.38 0.37 0.36 0.35 200 0.340
Adam

Tiny ImageNet-ResNet18

50 SLS

E1p0o0ch 150 Amsgrad + SLS

(c) Tiny Imagenet

Imagewoof-ResNet18 101

Step size (log)

10 1

10 3

10 5

100

20

Adagrad + SLS

40 Epoch60 80 100 Amsgrad + SLS + HB

Imagenette-ResNet18

100

Step size (log)

10 2

10 4

10 6

10 8

100

0 20

Adagrad + SLS

40Epoch60 80 100 Amsgrad + SLS + HB

Tiny ImageNet-ResNet18 101

Step size (log)

10 1

10 3

10 5

200

0

50 E1p0o0ch 150 200

Adagrad + SLS

Amsgrad + SLS + HB

Train loss (log)

Figure 8. Comparing optimization methods on image classiﬁcation tasks using variants of ImageNet. We use the same settings as the CIFAR datasets and observe that AdaGrad and AMSGrad with Armijo SLS is consistently better.

Train loss (log) Validation accuracy
Step size (log)

MNIST
100 10 1 10 2 10 3 10 4

MNIST
0.984 0.982 0.980 0.978

100 MNIST
10 1 10 2 10 3 10 4 10 5

0 20 40 60 80 100 0.9760 20 40 60 80 100

Epoch

Epoch

0 20 40Epoch60 80 100

Adam

Adabound

Radam

SLS

Adagrad + SLS

Amsgrad + SLS

Amsgrad + SLS + HB

Figure 9. Comparing optimization methods on MNIST.

Adaptive Gradient Methods Converge Faster with Over-Parameterization

Train loss (log)

10 1 10 4 10 7 10 10 10 13
0

True model

6 × 10 1 4 × 10 1 3 × 10 1 2 × 10 1

Rank 1

25 Ep5o0ch 75 100

Adam

Adabound

0 25 Ep5o0ch 75

Radam

SLS

Rank 10
10 1

10 4

10 7

10 1

10 10

10 13

100

0 25

Adagrad + SLS

10 2

Ep5o0ch 75 100

0

Amsgrad + SLS

Rank 4
25 Ep5o0ch 75 100 Amsgrad + SLS + HB

Figure 10. Comparison of optimization methods for deep matrix factorization. Methods use the same hyper-parameter settings as above and we examine the effects of over-parameterization on the problem: minW1,W2 Ex∼N(0,I) W2W1x − Ax 2 (Vaswani et al., 2019b; Rolinek & Martius, 2018). We choose A ∈ R10×6 with condition number κ(A) = 1010 and control the over-parameterization via the rank k (equal to 1,4, 10) of W1 ∈ Rk×6 and W2 ∈ R10×k. We also compare against the true model. In each case, we use a ﬁxed dataset
of 1000 samples. We observe that as the over-parameterization increases, the performance of all methods improves, with the methods
equipped with SLS performing the best.

Train loss (log)

CIFAR10 - ResNet34

CIFAR100 - DenseNet121

CIFAR100 - ResNet34 101 Tiny ImageNet - ResNet18

100

10 1

10 2

10 3

10 4 0 0.94

50 E1p0o0ch 150 CIFAR10 - ResNet34

100

100

10 1

10 1

10 2 10 3 200 0.76

10 2
50 100 150 200 10 3 0 Epoch
CIFAR100 - DenseNet121 0.76

50 E1p0o0ch 150 CIFAR100 - ResNet34

0.74

0.74

100
10 1
10 2
10 3 200 0 50 E1p0o0ch 150 200
0.40 Tiny ImageNet - ResNet18 0.39

0.92

0.72

0.72

0.38

0.90

0.70

0.88

0.68

0.66 0.860 50 100 150 200
Epoch

0.70
0.68
0.66 50 E1p0o0ch 150 200 0

0.37 0.36 0.35 50 100 150 200 0.34 50 Epoch

100Epoch 150 200

Amsgrad + SLS

Adagrad + SLS + mom

Amsgrad + SLS + HB

Adam

SLS

Amsgrad + SLS (beta = 0)

Adagrad + SLS

Validation accuracy

Figure 11. Ablation study comparing variants of the basic optimizers for multi-class classiﬁcation with deep networks. Training loss (top) and validation accuracy (bottom) for CIFAR-10, CIFAR-100 and Tiny ImageNet. We consider the AdaGrad with AMSGrad-like momentum and do not ﬁnd improvements in performance. We also benchmark the performance of AMSGrad without momentum, and observe that incorporating AMSGrad momentum does improve the performance, whereas heavy-ball momentum has a minor, sometimes detrimental effect. We use SLS and Adam as benchmarks to study the effects of incorporating preconditioning vs step-size adaptation.

