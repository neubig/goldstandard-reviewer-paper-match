arXiv:2106.09082v2 [math.OC] 19 Feb 2022

Zeroth-Order Methods for Convex-Concave Minmax Problems: Applications to Decision-Dependent Risk
Minimization
Chinmay Maheshwari1∗, Chih-Yuan Chiu1∗, Eric Mazumdar1, S. Shankar Sastry1, Lillian J. Ratliﬀ2
1Electrical Engineering and Computer Sciences, University of California, Berkeley 2Electrical and Computer Engineering, University of Washington, Seattle
Abstract Min-max optimization is emerging as a key framework for analyzing problems of robustness to strategically and adversarially generated data. We propose a random reshuﬄing-based gradient free Optimistic Gradient Descent-Ascent algorithm for solving convex-concave min-max problems with ﬁnite sum structure. We prove that the algorithm enjoys the same convergence rate as that of zeroth-order algorithms for convex minimization problems. We further specialize the algorithm to solve distributionally robust, decision-dependent learning problems, where gradient information is not readily available. Through illustrative simulations, we observe that our proposed approach learns models that are simultaneously robust against adversarial distribution shifts and strategic decisions from the data sources, and outperforms existing methods from the strategic classiﬁcation literature.
1 Introduction
The deployment of learning algorithms in real-world scenarios necessitates versatile and robust algorithms that operate eﬃciently under mild information structures. Min-max optimization has been used as a tool ensure robustness in variety of domains e.g. robust optimization [BTEGN09], robust control [HÅBB13], to name a few. Recently, min-max optimization has emerged as a promising framework for framing problems of algorithmic robustness against adversaries [GPAM+14, SKL17, MMS+17], strategically generated data [DRS+18, BHK20], and distributional shifts in dynamic environments [YLMJ21].
Despite this, recent works in machine learning and robust optimization on designing and analyzing stochastic algorithms for min-max optimization problems have largely operated on a number of assumptions that preclude their application to a broad range of real-world problems e.g., access to ﬁrst-order oracles that provide exact gradients [YKH20, NSH+19, JNJ20] or restrictive structural assumptions such as strong convexity [LLC+20, WBMR20, SBDG21]. Moreover, the developed theory is often not well-aligned with the practical implementation of these algorithms in real-world machine learning applications. For example, [BSG20] propose zeroth-order methods for convex-concave problems but the proposed algorithm may not be suitable for machine learning applications where the objective function is a sum of large numbers of component functions (depending on the size of dataset). Indeed, in order to compute the gradient estimate at any iteration Beznosikov et al requires perturbing all the functions which might not be suitable/possible for many applications. Furthermore, stochastic gradient methods are often used with random reshuﬄing (without replacement) in practice, yet their theoretical performance is usually characterized under the assumption of uniform sampling with replacement [Bot09, JNN19].
∗Equal contribution
1

In this work, we do away with these assumptions and formulate a gradient-free (zerothorder), random reshuﬄing-based algorithm with non-asymptotic convergence guarantees under mild structural assumptions on the underlying min-max objective. Our convergence guarantees are established by balancing the bias and variance of the zeroth-order gradient estimator [BLM18], using coupling-based arguments to analyze the correlations between iterates due to the random reshuﬄing procedure [JNN19], and exploiting the recent connections between the Optimistic Gradient Descent Ascent (OGDA) and Proximal Gradient algorithms [MOP20b].
One of the primary problem areas in which such an algorithm becomes necessary is in learning from strategically generated or decision-dependent data, a classical problem in operations research (see, e.g., [HBT18] and references therein). This problem has garnered a lot of attention of late in the machine learning community under the name “performative prediction” [PZMDH20, MPZ21, BHK20] due to the growing recognition that learning algorithms are increasingly dealing with data from strategic agents. In such problems, assuming access to the response map of strategic agents is often too restrictive, and the introduction of agent’s strategic responses into a convex loss function can often result in non-convex objectives.
As an example of such a decision-dependent problem, consider a scenario in which a ride-sharing platform seeks to devise an adaptive pricing strategy which is responsive to changes in supply and demand. The platform observes the current supply and demand in the environment and adjusts the price to increase the supply of drivers (and potentially decrease the demand) as needed. Drivers, however, have the ability to adjust their availability, and can strategically create dips in supply to trigger price increases. Such gaming has been observed in real ride-share markets (see, e.g., [Ham19, You19]) and results in negative externalities like higher prices for passengers. Importantly, in this situation, the platform does not observe precisely the decision making process of the drivers, only their strategically generated availability, and must learn to optimize through these agents’ responses. This lack of precise knowledge regarding the data generation process, and the reactive nature of the data, motivate the use of game theoretic abstractions for the decision problem, as well as algorithms for ﬁnding solutions in the absence of full information.
Previous work analyzing this problem studies this phenomenon through the lens of risk minimization in which the data distribution is decision-dependent, and seeks out settings in which the decision maker can optimize the decision-dependent risk [MPZ21]. These works, however, do not account for model misspeciﬁcation in their analysis. In particular, if the data generation model is incorrect, the performance of the optimal solution returned by their training methods may potentially degrade rapidly, something we explore in our experiments.
We show that the decision-dependent learning (performative prediction) problem can be robustiﬁed by taking a distributional robustness perspective on the original problem. Moreover, we show that, under mild assumptions, the distributionally robust decisiondependent learning problem can be transformed to a min-max problem and hence our zeroth-order random reshuﬄing algorithm can be applied. The gradient-free nature of our algorithm is important for applications where data is generated by strategic users that one must query; in these scenarios, the decision-maker is unlikely to access the best response map (data generation mechanism) of the strategic users, and hence will lack access to precise gradients.

Contributions. In this paper, we analyze the class of convex-concave min-max problems given by

min max L(x, y),

(1)

x∈X y∈Y

2

where X ⊂ Rdx, Y ⊂ Rdy , and L : Rdx × Rdy → R has the ﬁnite-sum structure given by

L := n1

n i=1

Li,

where

L1, . . . , Ln

:

Rdx × Rdy

→

R

denote

n

individual

loss

functions.

This

formulation is ubiquitous in machine learning applications, where the overall loss objective

is often the average of the loss function evaluated over each data point in a dataset.

The contributions of this paper can be summarized as follows.

I) We propose an eﬃcient zeroth-order random reshuﬄing-based OGDA algorithm for a convex-concave min-max optimization problem, without assuming any other structure on the curvature of the min-max loss (e.g., strong convexity or strong concavity). We provide (to our knowledge) the ﬁrst non-asymptotic analysis of OGDA algorithm with random reshuﬄing and zeroth-order gradient information.

II) As an important application, we formulate the Wasserstein distributionally robust learning with decision-dependent data problem as a constrained ﬁnite-dimensional, smooth convex-concave min-max problem of the form (1). In particular, we consider the setting of learning from strategically generated data, where the goal is to ﬁt a generalized linear model, and where an ambiguity set is used to capture model misspeciﬁcation regarding the data generation process. This setting encapsulates a distributionally robust version of the recently introduced problem of strategic classiﬁcation [HMPW16]. We show that this problem, under mild assumptions on data generation model and the ambiguity set, can be transformed into a convexconcave min-max problem to which our algorithm applies.

III) We complement the theoretical contributions of this paper by presenting illustrative numerical examples.

2 Related Work
Our work draws upon the existing literature on zeroth-order methods for min-max optimization problems, decision-dependent learning (performative prediction), and distributionally robust optimization.
Zero-Order Methods for Min-Max Optimization. Zeroth-order methods provide a computationally eﬃcient method for applications in which ﬁrst-order or higher-order information is inaccessible or impractical to compute, e.g., when generating adversarial examples to test the robustness of black-box machine learning models [LLC+20, LCK+20, CZS+17, IEAL18, TTC+19, LLW+19, AH17]. Recently, Liu et al. and others [LLC+20, GJZ18, WBMR20] provided the ﬁrst non-asymptotic convergence bounds for zeroth-order algorithms, based on analysis methods for gradient-free methods in convex optimization [NS17]. However, these works assume that the min-max objective is either strongly concave in the maximizing variable [LLC+20, WBMR20] or strongly convex [GJLJ17] in the minimizing variable, an assumption that fails to hold in many applications [DRS+18, YLMJ21]. In contrast, the zeroth-order algorithm presented in this work provides nonasymptotic guarantees under the less restrictive assumption that the objective function is convex-concave. In particular, we present the ﬁrst (to our knowledge) zeroth-order variant of the Optimistic Gradient Ascent-Descent (OGDA) algorithm [MOP20b, MOP20a].
In single-variable optimization problems, ﬁrst-order stochastic gradient descent algorithms are empirically observed to converge faster when random reshuﬄing (RR, or sampling without replacement) is deployed, compared to sampling with replacement [RR11, Bot09]. Although considerably more diﬃcult to analyze theoretically, gradient-based RR methods have recently been shown to enjoy faster convergence when the underlying objective function is convex [Sha16, JNN19, MKR21, HS19, SS19, GOP21, RGP20]. Recently, these theoretical results have been extended to ﬁrst-order methods for convex-concave min-max optimization problems [YLMJ21]; in this paper, we further extend these results to the case

3

of zero-order algorithms. Speciﬁcally, we present the ﬁrst (to our knowledge) non-asymptotic convergence rates for zeroth-order random reshuﬄing min-max optimization algorithms.
Distributionally Robust Optimization. Distributionally Robust Optimization (DRO) seeks to ﬁnd solutions to optimization problems (e.g., supervised learning tasks) robust against changes in the data distribution between training and test time [MMS+17, YLMJ21]. These distributional diﬀerences may arise due to imbalanced data, sample selection bias, or adversarial perturbations or deletions [CSSL09, MMS+17], and are often modeled as min-max optimization problems, in which the classiﬁer and an adversarial noise component are respectively modeled as the minimizer and maximizer of a common min-max loss objective [Bag05, BBC10, GMT14, GYdH15, RM19]. In particular, the noise is assumed to generate the worst possible loss corresponding to a bounded training data distribution shift, with the bound given by either the f -divergence or Wasserstein distance. [YLMJ21, BTHW+13, ND16, HNSS18, SAEK15]. While this work considers adversarial noise in generated data, largely in a worst-case context, it has yet to capture strategically generated data wherein a data source generates data via a best response mapping.
Strategic Classiﬁcation and Performative Prediction. Strategic classiﬁcation [HMPW16, DRS+18, KTS+19, SBKK20] and performative prediction [PZMDH20, MDPZH20, MPZ21, DX20] concern supervised learning problems in which the training data distribution shifts in response to the deployed classiﬁer or predictor more generally. This setting naturally arises in machine learning applications in which the selection of the deployed classiﬁer either directly changes the training data (e.g., decisions based on credit scores, such as loan approvals, themselves change credit scores), or prompts the data source to artiﬁcially alter their attributes (e.g. withdrawals during bank runs spur worried clients to make more withdrawals) [PZMDH20, MDPZH20, MPZ21]. Here, the learner accesses only perturbed features representing the strategic agents’ best responses to a deployed classiﬁer, and not the true underlying features [DRS+18]. This is a recently introduced formulation to machine learning; the results in this body of literature (to our knowledge) have not introduced the concept of robustness to model misspeciﬁcation or the data generation process, in the same manner as we capture in this work.

3 Preliminaries

Recall that in this paper, we consider the class of convex-concave min-max problems given by:

min max L(x, y),

(2)

x∈X y∈Y

where

X

⊂

Rdx ,

Y

⊂

Rdy ,

and

L

:=

1 n

n i=1

Li,

where

L1, . . . , Ln

:

Rdx

×

Rdy

→

R

denote

n individual loss functions. For convenience, we denote d := dx + dy.

Assumption 3.1. The following statements hold: (i) The sets X ⊂ Rdx and Y ⊂ Rdy are convex and compact.
(ii) The functions L1, · · · , Ln : Rd → R are convex in x ∈ Rdx for each y ∈ Rdy , concave in y ∈ Rdy for each x ∈ Rdx, and G-Lipschitz and -smooth in (x, y) ∈ Rd (which implies that L : Rd → R, by deﬁnition, also possesses the same properties).

For ease of exposition, we denote u := (x, y), ML := supu∈X ×Y |L(u)|, D := supu,u ∈X ×Y u− u 2, and deﬁne the operators F, Fi : Rd → Rd, for each i ∈ [n], by:

F (u) := ∇xL(u) ,

Fi(u) := ∇xLi(u) , ∀ i ∈ [n].

(3)

−∇y L(u)

−∇y Li (u)

4

Observe that under Assumption 3.1, ML, D < ∞, and F and each Fi are monotone1. Finally, we deﬁne the gap function ∆ : Rd → [0, ∞) associated with the loss L by

∆(x, y) := L(x, y ) − L(x , y) ≥ 0,

(4)

where u := (x , y ) ∈ X × Y denotes any min-max saddle point of the overall loss L(x, y), and (x, y) ∈ X × Y denotes any feasible point. This gap function allows us to measure the convergence rate of our proposed algorithm. To this end, we deﬁne the -optimal saddle-point of (2) as follows.

Deﬁnition 3.1 ( -optimal saddle point solution). A feasible point (x, y) ∈ X × Y is said to be an -optimal saddle-point solution of (2) if

∆(x, y) = L(x, y ) − L(x , y) ≤ ,

4 Algorithms and Performance
In this section we introduce a gradient-free version of the well-studied Optimistic Gradient Descent Ascent (OGDA) algorithm, and give ﬁnite time rates showing that it can eﬃciently ﬁnd the saddle point in constrained convex-concave problems.

4.1 Zero-Order Gradient Estimates

In our zero-order, random-reshuﬄing based variant of the OGDA algorithms, we use the one-shot randomized gradient estimator in [Spa97, FKM05, GJZ18, LLC+20]. In particular, given the current iterate u ∈ Rd and a query radius ε > 0, we sample a vector v uniformly from unit sphere Sd−1 (i.e. v ∼ Unif(Sd−1)), and deﬁne the zeroth-order estimator Fˆ(u; ε, v) ∈ Rd of the min-max loss L(u) to be:

Fˆ(u; ε, v)

:=

d L(u

+

εv)v

ε

Properties of this zeroth-order estimator, derived in [BLM18], are reproduced as Proposition A.4 in Appendix A.1.

4.2 Optimistic Gradient Descent Ascent with Random Reshuﬄing (OGDARR)
In this subsection, we formulate our main algorithm, Optimistic Gradient Descent Ascent with Random Reshuﬄing (OGDA-RR). In each epoch t ∈ {0, 1, · · · , T − 1}, the algorithm generates a uniformly random permutation σt := (σ1t , · · · , σnt ) of [n] := {1, · · · , n} independently of any other randomness, and ﬁxes a query radius t > 0 and search direction vit ∈ Rd. (Note: query radii only depends on epoch indices t, and not on sample indices). For each index i ∈ [n], we compute the OGDA-RR update as follows:

uti+1 = ProjX ×Y uti − ηtFˆσt (uti; εt, vit) − ηtFˆσt (uti; εt, vit) + ηtFˆσt (uti−1; εt, vit−1) , (5)

i

i−1

i−1

After repeating this process for T epochs, the algorithm returns the step-size-weighted average of the iterates, u˜T := n· Tt1=−01 ηt Tt=−01 ni=1 ηtuti. Roughly, the weighting described in Theorem 4.1 below optimally balances the bias and variance of the zero-order gradient estimator in Section 4.1.
1A function F : Rd → Rd is called monotone if F (x) − F (y), x − y ≥ 0 for all x, y ∈ Rd.

5

Algorithm 1: OGDA-RR Algorithm

1

Input:

stepsizes

ηt, εt,

data

points

{

(xi

,

yi

)}

n i=1

∼ D, u(00),

time

horizon

duration

T;

2 for t = 0, 1, · · · , T − 1 do

3 σt = (σ1t , · · · , σnt ) ← a random permutation of set [n];

4 for i = 0, . . . , n − 1 do

5

Sample vit ∼ Unif(Sd−1)

6

uti+1 ← (5)

7 end

8

u0(t+1) ← utn

9

u−(t+1 1) ← utn−1

10 end

11 Output: u˜T := n· Tt1=−01 ηt Tt=−01 ni=1 ηtuti.

Theorem 4.1. Let L(u) denote the objective function in the constrained min-max opti-
mization problem given by (1), and let u = (x , y ) ∈ X × Y denote any saddle point of
L(u). Fix > 0. Suppose Assumption 3.1 holds, and the number of epochs T , step sizes sequence {ηt}Tt=−01, and query radii sequence {εt}Tt=−01 satisfy:

ηt := η0 · (t + 1)−3/4+χ, εt := ε0 · (t + 1)−1/4,

∀ t ∈ {0, 1, · · · , T − 1}, ∀ t ∈ {0, 1, · · · , T − 1},

13

5

0 0 0 0 η0 η0

T > ε4

D + · C · max 16n 4n

ε , η , η ε , ε0 , (ε0)2

1 1+
χ

4 1−4χ
,

for some initial step size η0 ∈ 0, 21 , initial query radius ε0 > 0, parameter χ ∈ (0, 1/4), and constant:

C := max

3ndD

, 18ndDG

, 54ndG2

+ 18ndD

ML

,

90ndGM

L

,

36ndM

2 L

,

6dGn2 + 14Gn2 + 4nG, 6dMLn, 3dDG , 3dD ML > 0.

Then the iterates {uti} generated by the OGDA-RR Algorithm (Alg. 1) satisfy:

E ∆(u˜T ) < .

Remark. Note that our OGDA-RR algorithm is more computationally eﬃcient than Alg. 2 in [YLMJ21], even if one replaces the gradient estimates with true gradient values. This is because Alg. 2 in [YLMJ21] requires M ∼ O(log(n)) inner loop iterations to approximate a proximal point update. Here, we avoid this restriction by exploiting the recent perspective that the OGDA update is a perturbed proximal point update [MOP20a, MOP20b]. For more details, see Appendix A.2 for the proof of Theorem 4.1.

5 Applications to Decision-Dependent DRO
In this section we discuss a novel convex-concave min-max reformulation of a class of decisiondependent distributional robust risk minimization problems, which reﬂects the need for learning classiﬁers that are simultaneously robust to strategic data sources and adversarial model-speciﬁcation. In particular, we present a distributionally robust formulation of strategic classiﬁcation [DRS+18] with generalized linear loss, a semi-inﬁnite optimization problem that can be reformulated to a ﬁnite-dimensional convex-concave min-max problem.
Strategic classiﬁcation is an emerging paradigm in machine learning which attempts to “close the loop"— i.e., account for data (user) reaction at training time—while designing

6

classiﬁers to be deployed in strategic environments in the real world, where deploying naïve classiﬁer (designed ignoring the distribution shift) can be catastrophic. Modeling the exact behavior of such strategic interactions is very complex, since the decision-maker (learner) does not have access to the strategic users’ preferences and hence lacks access to their best response function. To overcome this diﬃculty, we use a natural model for these strategic behaviors that has been exploited in Dong et. al.(2018), and then impose robustness conditions (in the form of an ambiguity set on the decision-dependent data distribution) to capture model misspeciﬁcation. To facilitate the discussion, we provide a primer on decision-dependent DRO in the next subsection.

5.1 Primer on decision-dependent distributionally robust optimization
Consider a generalized linear problem, where the goal is to estimate the parameter θ ∈ Θ, which is assumed to be a compact set, by solving the following convex optimization program:

inf ED [φ ( x¯, θ ) − y¯ x¯, θ ]
θ∈Θ
where φ : R → R is a smooth convex function and the tuple (x¯, y¯) ∈ Rd × {−1, +1} is sampled from an unknown distribution D, often approximated by the empirical distribution of a set of observed data. The generalized linear model encompasses a wide range of machine learning formulations [MN19].
A distributionally robust generalized linear problem, on the other hand, minimizes the worst case expectation over an uncertainty set P in the space of probability measures. This setup can be envisioned as a game between a learning algorithm and an adversary. Based on parameters chosen by the learning algorithm, the adversary then picks a probability measure from the uncertainty set which maximizes the risk for that choice of parameter:

inf sup EP [φ ( x¯, θ ) − y¯ x¯, θ ] ,
θ P∈P
where (x¯, y¯) ∼ P ∈ P. Typically P is chosen as a Wasserstein ball around the empirical distribution D˜n of a set of n observed data points, {(x˜i, y˜i) ∈ Rd × {−1, 1}}ni=1, sampled independently from the data distribution D. Then, for any δ > 0 the uncertainty set P is given by Bδ(D˜n) = {P : W(P, D˜n) ≤ δ}.
A critique of the above problem formulation is that the underlying data distribution D is considered ﬁxed, while in many strategic settings underlying data distribution will depend on the classiﬁer parameter θ. Decision-dependent supervised learning aims to tackle such distribution shifts. When specialized to the generalized linear model, the problem formulation becomes:
inf ED(θ) [φ ( x¯, θ ) − y¯ x¯, θ ] ,
θ
where (x¯, y¯) ∼ D(θ). In this work, we take a step forward and work with the distributionally robust decision-dependent generalized linear model, deﬁned as:

inf sup EP [φ ( x¯, θ ) − y¯ x¯, θ ] ,

(6)

θ P∈P(θ)

where (x¯, y¯) ∼ P ∈ P(θ) and P(θ) = Bδ(D˜n(θ)). Here, the dependence of P on the choice of classiﬁer θ is captured by its inclusion in P(θ) = Bδ(D˜n(θ)). To describe decision-dependent distribution shifts D˜n(θ), we restrict our focus to the setting of strategic classiﬁcation. The
following subsection formalizes our setting.

5.2 Model for strategic response
Below, we denote the data points sampled from true distribution by (x˜i, y˜i) ∼ D where D is a unknown, underlying distribution. For ease of presentation, we associate each data

7

point index i with an agent. For each agent i ∈ [n], let ui(x; θ, x˜i, y˜i) ∈ R denote its utility function that a strategic agent seeks to maximize. In other words, when a classiﬁer parametrized by θ ∈ Rd is deployed, the agent i ∈ [n] responds by reporting bi(θ, x˜i, y˜i), deﬁned as:
bi(θ, x˜i, y˜i) ∈ arg max ui(x; θ, x˜i, y˜i).
x
Note that we allow diﬀerent agent to have diﬀerent utility function. We now impose the following assumptions on the utility functions; these are crucial for
ensuring guaranteed convergence of our proposed algorithms.

Assumption

5.1.

For

each

agent

i

∈

[n],

deﬁne

ui(x; θ, x˜i, y˜i)

:=

1−y˜i 2

x, θ

− gi(x − x˜i),

where gi : Rd → R satisﬁes:

(i) gi(x) > 0 for all x = 0; (ii) gi is convex on Rd; (iii) gi is positive homogeneous2 of degree p > 1; (iv) Its convex conjugate gi∗(θ) := supx∈Rd x, θ − gi(x) is Gi-Lipschitz and G¯i-smooth on
Θ.

As is pointed out in Dong et. al. (2018), a large class of functions g(·) satisfy the
requirements posited in Assumption 5.1. For example, for any arbitrary norm and any p > 1 the function g(x) = p1 x p is a candidate. Note that these assumptions are not very restrictive and capture a large variety of practical scenarios [DRS+18]. A natural
consequence of the above modeling paradigm is that bi(θ, x˜i, +1) = x˜i. To wit, the agents act strategically only if their true label is −1. This is a reasonable setting for many real world applications [DRS+18]. We now present a technical lemma which will be helpful in
subsequent presentation.

Lemma 5.2 (Dong et. al. (2018)). Under Assumption 5.1, for each agent i ∈ [n], the

set of best responses arg maxx ui(x; θ, x˜i, y˜i) is ﬁnite and bounded. The function θ →

bi(θ, x˜i, y˜i), θ

is convex. To wit, for any i ∈ [n]:

bi(θ, x˜i, y˜i), θ

=

x˜i, θ

+

1−y˜i 2

q

gi∗

(θ

)

where p1 + 1q = 1

Against the preceding backdrop, we now present the convex-concave min-max reformulation of the Wasserstein Distributionally Robust Strategic Classiﬁcation (WDRSC) problem.

5.3 Reformulation of the WDRSC Problem
The WDRSC problem formulation contains two main components—the strategic component that accounts for a distribution shift D(θ) in response to the choice of classiﬁer θ, and the adversarial component that accounts for the uncertainty set P(θ). As per the modeling assumptions described in Section 5.2, we have (x˜i, y˜i) ∼ D and (bi(θ, x˜i, y˜i), y˜i) ∼ D(θ) for all i ∈ [n]. We now impose certain restrictions on the adversarial component that would enable us to reformulate the WDRSC problem as a convex-concave min-max optimization problem. Crudely speaking, we allow adversarial modiﬁcations on features for all data points, but adversarial modiﬁcations on labels only when the true label is +1.
For the distributionally robust strategic classiﬁcation problem, we consider a speciﬁc form of uncertainty set P(θ) that allows us to reformulate the inﬁnite-dimensional optimization problem as a ﬁnite-dimensional convex-concave min-max problem. As described above, in our formulation, the features of a given data point i can be perturbed strategically if y˜i = −1, but not if y˜i = +1. On top of the strategic perturbations we also consider the
2A function f : Rd → R is positive homogenous of degree r if for any scalar α > 0 and x ∈ Rd we have f (αx) = αrf (x)

8

adversarial perturbations to the data points. Speciﬁcally, we also assume that the adversary can perturb both the features and label of a data point i if y˜i = 1, but can only perturb the features and not the label if y˜i = −1. A rigorous exposition of this restriction is deferred to Appendix B.1. Under these assumptions, we now present a convex-concave min-max reformulation of the WDRSC problem.
Theorem 5.3. Let the strategic behavior of the agents be governed in accordance with Assumption 5.1. Suppose φ is convex and β-smooth. In addition, suppose R x → φ(x) + x ∈ R is non-decreasing. Then the WDRSC problem (6) can be reformulated into the following convex-concave min-max problem:

min max α(δ − κ) + 1 1 + y˜i (φ ( bi(θ), θ )) + γi ( bi(θ), θ − ακ) (7)

(θ,α) γ∈Rn

n

2

i

+ 1 1 − y˜i (φ( bi(θ), θ ) + bi(θ), θ ) ni 2
s.t. θ ≤ α/(β + 1), γ ∞ ≤ 1

where for any i ∈ [n], we have concisely written bi(θ, x˜i, y˜i) as bi(θ).

The proof of Theorem 5.3 is presented in Appendix B.2.
Remark. The non-decreasing assumption on the map R x → φ(x) + x ∈ R is not overly restrictive; in fact, it is satisﬁed by the logistic regression model in supervised learning (see Appendix C).
Remark. Note that we can convert the smooth convex-concave minmax problem (7) into a non-smooth convex minimization problem by explictly taking maximization over γ. But we refrain from doing as it has been observed [YLMJ21] that solving the smooth minimax optimization problem is faster than solving the non-smooth problem. In fact, we have presented an experimental study in Appendix C which corroborates this observation.
Throughout the rest of this paper, we denote the min-max objective in (7) by L(α, θ, γ).

6 Empirical Results
In this section we deploy zeroth-order OGDA algorithm with random reshuﬄing to solve the convex concave reformulation of WDRSC as presented in (7). We point out that in order to solve (7), the zeroth-order method should only be applied to estimate the gradient with respect to θ. This is because the gradient with respect to other variables, namely (α, γ), can be exactly computed. Speciﬁcally, to compute derivative with respect to θ the designer must know the best response function which is often not available and it can only be queried.
We now present some illustrations of the empirical performance of our proposed algorithm, as well as empirical justiﬁcation for solving the WDRSC problem over existing prior approaches to strategic classiﬁcation.
6.1 Experimental Setup
Our ﬁrst set of empirical results uses synthetic data to illustrate the eﬀectiveness of our algorithms. The datasets used in this section are constructed as follows: the ground truth classiﬁer θ and features x˜i are sampled as θ ∼ N (0, Id) and x˜i ∼ i.i.d. N (0, Id), for each i ∈ [n], while the ground truth labels y˜i are given by y˜i = sign( x˜i, θ + zi) for each i ∈ [n], where zi ∼ i.i.d. N (0, 0.1 · Id). We use n ∈ {500, 1000} with d = 10. The ﬁrst ﬁve of the

9

d = 10 features are chosen to be strategic. In all experiments, we take κ = 0.5 and δ = 0.4. Each strategic agent i ∈ [n] has a utility function given by:

ui(x; θ, x˜i, y˜i, ζi) = 1 − y˜i x, θ − 1 x − x˜i 2,

(8)

2

2ζi

where ζi denote the perturbation “power" of agent i. For simplicity, we assume all agents are homogeneous, in the sense that ζi = ζ > 0 for all i ∈ [n]; in practice, one need not impose this assumption. Given this utility function, the best response of agents takes the form:

bi(θ, x˜i, y˜i; ζ) = x˜i if y˜i = +1, (9) x˜i + ζθ if y˜i = −1

where, in our simulations, we ﬁx ζ = 0.05. We reemphasize that our algorithm does not use the value of ζ in any of its computations. For purposes of illustration, we focus on the performance of the following algorithms:

(A-I) Zeroth-order optimistic-GDA with random reshuﬄing (see Algorithm 1), (A-II) Zeroth-order optimistic-GDA without random reshuﬄing (see Appendix C), (A-III) Zeroth-order stochastic-GDA with random reshuﬄing (see Appendix C), (A-IV) Zeroth-order stochastic-GDA without random reshuﬄing (see Appendix C).

and we evaluate the proposed algorithms and model formulation on two criteria:

(i) Suboptimality: To measure suboptimality, we use the gap function ∆(α, θ, γ) = L(α, θ, γ ) − L(α , θ , γ) (Def. 4) where (α , θ , γ ) is a solution of the min-max reformulation (7) of the WDRSC problem. If the objective L(·) is convex-concave, ∆(·) is non-negative, and equals zero at (and only at) saddle points.

(ii) Accuracy: Given a data set {(x˜i, y˜i)}i∈[n], the accuracy of a classiﬁer θ is measured as n1 i∈[n] y˜i bi(θ, x˜i, y˜i; ζ), θ . Under this criterion we compare the accuracy under diﬀerent perturbations for diﬀerent classiﬁers θ;

To compute suboptimality, we ﬁrst compute a true min-max saddle point (α , θ , γ ) via a ﬁrst order gradient based algorithm (namely, GDA). All experiments were run using Python 3.7 on a standard MacBook Pro laptop (2.6 GHz Intel Core i7 and 16 GB of RAM).

6.2 Results
Simulation results presented in Figure (1a)-(1b) show that our proposed algorithm (i.e. (A-I)) outperforms algorithms without reshuﬄing (i.e. (A-II) and (A-IV)). However, its performance resembles that of zeroth-order stochastic-GDA with random reshuﬄing. More experimental studies need to be conducted to more conclusively determine whether (A-I) outperforms (A-III), or vice versa. In fact , there has been no theoretical investigations even for the ﬁrst order stochastic-GDA algorithm with random reshuﬄing; this is an interesting future direction to explore.
In Figure 1, we also compare the robustness of the classiﬁer obtained by using Algorithm (A-I) with that obtained from prior work on solving probems of strategic classiﬁcation trained with ζ = 0.05 (referred as LogReg SC in Figure 1). As expected, due to the formulation, the performance of the classiﬁer obtained via (A-I) degrades gracefully even when subject to large perturbations, while the performance of existing approaches to strategic classiﬁcation degrades rapidly. Further numerical results on synthetically generated and real world datasets are given in Appendix C.

10

Suboptimality

Synthetic Dataset with 500 datapoints
Z-OGDA w RR Z-OGDA w/o RR
10 1 ZZ--SSGGDDAA ww/oRRRR

10 2

10 3 0

200 400 600 800 1000
Epochs

Accuracy

Synthetic Dataset with 500 datapoints

0.95

LogReg SC Z-OGDA w RR

0.90

0.85

0.80

0.75

0.70

0.65

0.60 0.0 0.2 0.4 0.6 0.8 1.0
Perturbation

(a) n = 500

Synthetic Dataset with 1000 datapoints Synthetic Dataset with 1000 datapoints

Z-OGDA w RR

10 1

Z-OGDA w/o RR 0.9
Z-SGDA w/o RR

Z-SGDA w RR

0.8

10 2

0.7

LogReg SC Z-OGDA w RR

Accuracy

10 3 0

0.6

0.5

200 400 600 800 1000 Epochs

0.0 0.5 1.0 1.5 2.0 2.5 3.0 Perturbation

(b) n = 1000

Suboptimality

Figure 1: Experimental results for a synthetic dataset with n = 500 and n = 1000. (Left panes of (1a), (1b))) Suboptimality iterates generated by the four algorithms (A-I), (A-II), (A-III), (A-IV), respectively denoted as Z-OGDA w RR, Z-OGDA w/o RR, Z-SGDA w RR, Z-SGDA w/o RR. (Right panes of (1a), (1b))) Comparison between decay in accuracy of strategic classiﬁcation with logistic regression (trained with ζ = 0.05) and Alg. (A-I) with change in perturbation.

11

7 Conclusion
This paper presents the ﬁrst (to our knowledge) non-asymptotic convergence rates for a gradient-free stochastic min-max optimization algorithm with random reshuﬄing. Our theoretical results, established for smooth convex-concave min-max objectives, do not require any additional, restrictive structural assumptions to hold. As a concrete application, we reformulate a distributionally robust strategic classiﬁcation problem as a convex-concave min-max optimization problem that can be iteratively solved using our method. Empirical results on synthetic and real datasets demonstrate the eﬃciency and eﬀectiveness of our algorithm, as well as its robustness against adversarial distributional shifts and strategic behavior of the data sources. Immediate directions for future work include establishing convergence results for the random-reshuﬄing based Stochastic Gradient Descent Ascent (SGDA-RR) algorithm, as well as performing more extensive experimental studies to better understand the empirical performance of our algorithm.

Acknowledgements
Research supported by NSF under grant DMS 2013985 “THEORINet: Transferable, Hierarchical, Expressive, Optimal, Robust and Interpretable Networks”. Chinmay Maheshwari, Chih-Yuan Chiu and S. Shankar Sastry were also supported in part by U.S. Oﬃce of Naval Research MURI grant N00014-16-1- 2710.

References

[AH17]

Charles Audet and Warren Hare. Derivative-free and blackbox optimization. Springer, 01 2017.

[Bag05]

J. A. Bagnell. Robust supervised learning. In AAAI, volume 2, pages 714–719, 2005.

[BBC10]

Dimitris Bertsimas, David Brown, and Constantine Caramanis. Theory and applications of robust optimization. SIAM Review, 53, 10 2010.

[BHK20]

Gavin Brown, Shlomi Hod, and Iden Kalemaj. Performative prediction in a stateful world. arXiv preprint arXiv:2011.03885, 2020.

[BLM18]

Mario Bravo, David Leslie, and Panayotis Mertikopoulos. Bandit learning in concave N-person games. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, page 5666–5676, Red Hook, NY, USA, 2018.

[Bot09]

L. Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proceedings of the Symposium on Learning and Data Science, Paris, 2009.

[BSG20]

Aleksandr Beznosikov, Abdurakhmon Sadiev, and Alexander Gasnikov. Gradient-free methods with inexact oracle for convex-concave stochastic saddle-point problem. ArXiv, 2020.

[BTEGN09] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton university press, 2009.
[BTHW+13] A. Ben-Tal, D. D. Hertog, A. D. Waegenaere, B. Melenberg, and G. Rennen. Robust solutions of optimization problems aﬀected by uncertain probabilities. Manag. Sci., 59:341–357, 2013.

12

[CSSL09]

Joaquin Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009.

[CZS+17]

P. Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth-Order Optimization-based Black-box Attacks to Deep Neural Networks Without Training Substitute Models. Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, 2017.

[DRS+18]

Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classiﬁcation from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, EC ’18, page 55–70, New York, NY, USA, 2018. Association for Computing Machinery.

[DX20]

Dmitriy Drusvyatskiy and Lin Xiao. Stochastic optimization with decisiondependent distributions. arXiv, 2020.

[FKM05]

Abraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: Gradient descent without gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’05, page 385–394, USA, 2005. Society for Industrial and Applied Mathematics.

[GJLJ17]

Gauthier Gidel, Tony Jebara, and Simon Lacoste-Julien. Frank-wolfe algorithms for saddle point problems. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 362–371, Fort Lauderdale, FL, USA, 20–22 Apr 2017. PMLR.

[GJZ18]

Xiang Gao, B. Jiang, and S. Zhang. On the information-adaptive variants of the ADMM: An iteration complexity perspective. Journal of Scientiﬁc Computing, 76:327–363, 2018.

[GMT14]

Virginie Gabrel, Cécile Murat, and Aurélie Thiele. Recent advances in robust optimization: An overview. European Journal of Operational Research, 235(3):471–483, 2014.

[GOP21]

Mert Gürbüzbalaban, A. Ozdaglar, and P. Parrilo. Why random reshuﬄing beats stochastic gradient descent. Math. Program., 186:49–84, 2021.

[GPAM+14]

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27, 2014.

[GYdH15]

Bram L. Gorissen, Ihsan Yanikoglu, and Dick den Hertog. A practical guide to robust optimization. Omega, 53(C):124–137, 2015.

[HÅBB13]

Martin Hast, Karl Johan Åström, Bo Bernhardsson, and Stephen Boyd. Pid design by convex-concave optimization. In 2013 European Control Conference (ECC), pages 4460–4465. IEEE, 2013.

[Ham19]

Isobel Hamilton. Uber drivers are reportedly colluding to trigger surge prices because they say the company is not paying them enough. Business Insider, June 14, 2019.

13

[HBT18]

Lars Hellemo, Paul I Barton, and Asgeir Tomasgard. Decision-dependent probabilities in stochastic programs with recourse. Computational Management Science, 15(3):369–395, 2018.

[HMPW16]

Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classiﬁcation. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, ITCS ’16, page 111–122, New York, NY, USA, 2016. Association for Computing Machinery.

[HNSS18]

Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classiﬁers? In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2029–2037. PMLR, 10–15 Jul 2018.

[HS19]

Jeﬀ Z. HaoChen and S. Sra. Random shuﬄing beats SGD after ﬁnite epochs. In ICML, 2019.

[IEAL18]

Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2137–2146. PMLR, 10–15 Jul 2018.

[JNJ20]

Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? In ICML, 2020.

[JNN19]

Prateek Jain, Dheeraj M. Nagaraj, and Praneeth Netrapalli. SGD without replacement: sharper rates for general smooth convex functions. In ICML, 2019.

[KTS+19]

M. Khajehnejad, Behzad Tabibian, B. Schölkopf, A. Singla, and M. GomezRodriguez. Optimal Decision Making Under Strategic Behavior. ArXiv, abs/1905.09239, 2019.

[LCK+20]

Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):43–54, 2020.

[Lee13]

John M. Lee. Introduction to Smooth Manifolds. Springer Science+Business Media New York, 2013.

[LLC+20]

Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah AlDujaili, Mingyi Hong, and Una-May O’Reilly. Min-max optimization without gradients: Convergence and applications to adversarial ML. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6282–6293. PMLR, 13–18 Jul 2020.

[LLW+19]

Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. NATTACK: learning the distributions of adversarial examples for an improved black-box attack on deep neural networks. In arXiv, 2019.

[MDPZH20] Celestine Mendler-Dünner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimization for performative prediction. In Advances in Neural Information Processing Systems, volume 33, pages 4929–4939, 2020.

14

[MKR21]

Konstantin Mishchenko, Ahmed Khaled, and Peter Richtárik. Random reshuﬄing: Simple analysis with vast improvements, 2021.

[MMS+17]

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ArXiv, 06 2017.

[MN19]

Peter McCullagh and John A Nelder. Generalized Linear Models. Routledge, 2019.

[MOP20a]

Aryan Mokhtari, A. Ozdaglar, and S. Pattathil. A Uniﬁed Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach. In AISTATS, 2020.

[MOP20b]

Aryan Mokhtari, A. Ozdaglar, and S. Pattathil. Convergence Rate of O(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM J. Optim., 30:3230–3251, 2020.

[MPZ21]

John Miller, Juan Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the performative risk. arXiv preprint arXiv:2102.08570, 2021.

[ND16]

Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29, 2016.

[Nes14]

Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing Company, Incorporated, 1 edition, 2014.

[NS17]

Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Found. Comput. Math., 17(2):527–566, April 2017.

[NSH+19]

Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, J. Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative ﬁrst order methods. In NeurIPS, 2019.

[PZMDH20] Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dünner, and Moritz Hardt. Performative prediction. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7599–7609. PMLR, 13–18 Jul 2020.

[RGP20]

Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD without replacement. arXiv e-prints, page arXiv:2002.10400, February 2020.

[RM19]

Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. In SIAM, 08 2019.

[RR11]

Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5, 04 2011.

[Rud76]

Walter Rudin. Principles Of Mathematical Analysis. McGraw-Hill, Inc., 1976.

[RW21]

Benjamin Recht and Stephen Wright. Optimization for Data Analysis. Cambridge University Press, 1 edition, 2021.

[SAEK15]

Soroosh Shaﬁeezadeh-Abadeh, Peyman Mohajerin Esfahani, and D. Kuhn. Distributionally robust logistic regression. In NeurIPS, 2015.

15

[SBDG21]

Abdurakhmon Sadiev, Aleksandr Beznosikov, Pavel Dvurechensky, and Alexander Gasnikov. Zeroth-Order Algorithms for Smooth Saddle-Point Problems. ArXiv, 2021.

[SBKK20]

Pier Giuseppe Sessa, Ilija Bogunovic, M. Kamgarpour, and A. Krause. Learning to play sequential games versus unknown opponents. ArXiv, abs/2007.05271, 2020.

[Sha16]

O. Shamir. Without-Replacement Sampling for Stochastic Gradient Methods. In NIPS, 2016.

[SKL17]

Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certiﬁed defenses for data poisoning attacks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 3520–3532, Red Hook, NY, USA, 2017.

[Spa97]

J. Spall. A one-measurement form of simultaneous perturbation stochastic approximation. Autom., 33:109–112, 1997.

[SS19]

Itay Safran and O. Shamir. How good is SGD with random shuﬄing? ArXiv, abs/1908.00045, 2019.

[TTC+19]

Chun-Chen Tu, Pai-Shun Ting, P. Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. AutoZOOM: autoencoder-based zeroth order optimization method for attacking black-box neural networks. In AAAI, 2019.

[WBMR20]

Z. Wang, K. Balasubramanian, Shiqian Ma, and Meisam Razaviyayn. Zerothorder algorithms for nonconvex minimax problems with improved complexities. ArXiv, abs/2001.07819, 2020.

[YKH20]

Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. In Advances in Neural Information Processing Systems, volume 33, pages 1153–1165, 2020.

[YLMJ21]

Yaodong Yu, Tianyi Lin, Eric V. Mazumdar, and Michael I. Jordan. Fast distributionally robust learning with variance reduced min-max optimization. ArXiv, abs/2104.13326, 2021.

[You19]

Soo Youn. Uber, Lyft drivers coordinate to manipulate surge pricing at Virginia airport over pay concerns: Report. ABC News, May 18, 2019.

16

A Results for the Proof of Theorem 4.1

A.1 Lemmas for Theorem 4.1

First, we list some fundamental facts regarding projections onto convex, compact subsets of an Euclidean space. Below, for any ﬁxed convex, compact subset Ω ⊂ Rd, we denote the projection operator onto Ω by ProjΩ(x) := argminz∈Ω x − z 2 for each x ∈ Rd. Note that ProjΩ(x) is well-deﬁned (i.e., exists and is unique) for each x ∈ Rd, if Ω ⊂ Rd were convex and compact.
We begin by summarizing some fundamental properties of the projection operator ProjΩ(·).
Proposition A.1. Let Ω ⊂ Rd be compact and convex, and ﬁx x, y ∈ Rd arbitrarily. Then:

ProjΩ(x) − ProjΩ(y)

2≤
2

ProjΩ(x) − ProjΩ(y)

ProjΩ(x) − ProjΩ(y) 2 ≤ x − y 2.

(x − y),

Proof. From [Nes14], Lemma 3.1.4 (see also [RW21], Lemma 7.4), we have:

ProjΩ(x) − ProjΩ(y) ProjΩ(y) − ProjΩ(x)

x − ProjΩ(x) ≥ 0, y − ProjΩ(y) ≥ 0.

Adding the two expressions and rearranging terms, we obtain:

ProjΩ(x) − ProjΩ(y) (x − y) − (ProjΩ(x) − ProjΩ(y)) ≥ 0,

⇒

ProjΩ(x) − ProjΩ(y)

2 2

≤

ProjΩ(x) − ProjΩ(y)

(x − y),

as given in the ﬁrst claim. The Cauchy Schwarz inequality then implies:

ProjΩ(x) − ProjΩ(y)

2 2

≤

ProjΩ(x) − ProjΩ(y)

(x − y)

≤ ProjΩ(x) − ProjΩ(y) 2 · x − y 2.

If ProjΩ(x) = ProjΩ(y), then the second claim becomes 0 ≤ x − y 2, which is clearly true. Otherwise, dividing both sides above by ProjΩ(x) − ProjΩ(y) 2 gives the second claim.
Lemma A.2. Let Ω ⊂ Rd be a compact, convex subset of Rd, and consider the update zk+1 = ProjΩ(zk − ηF (zk+1) + γk), where zk, zk+1, γk ∈ Rd. Then, for each z ∈ Ω:

F (zk+1), zk+1 − z

1 ≤

zk − z 2 − 1

zk+1 − z 2 − 1

zk+1 − zk 2 + 1 γk, zk+1 − z .

2η

2η

2η

η

Proof. Note that:

zk+1 − z 2 = = = =

zk+1 − zk + zk − z 2 zk+1 − zk 2 + zk − z zk+1 − zk 2 + zk − z zk − z 2 − zk+1 − zk

2+2 2+2 2+2

zk+1 − zk, zk − z zk+1 − zk, zk − zk+1 + zk+1 − z zk+1 − zk, zk+1 − z

By deﬁnition of zk+1, and optimality conditions for the projection operator:

zk+1 − z, zk+1 − zk + ηF (zk+1) − γk ≤ 0, ⇒ zk+1 − zk, zk+1 − z ≤ γk, zk+1 − z − η · F (zk+1), zk+1 − z .

17

Substituting back, we obtain:

zk+1 − z 2 = zk − z 2 − zk+1 − zk 2 + 2 zk+1 − zk, zk+1 − z ≤ zk − z 2 − zk+1 − zk 2 + 2 γk, zk+1 − z − 2η · F (zk+1), zk+1 − z .

Rearranging and dividing by η gives the claim in the lemma.

Next, we state the properties of the mean and variance of the zeroth-order gradient
estimator deﬁned in Section ?? ([BLM18], Lemma C.1). Below, we deﬁne the R-smoothed loss function LR : Rd → R by LR(u) := Ev∼Unif(Bd)[L(u + Rv)], where Sd−1 denotes the (d − 1)-dimensional unit sphere in Rd, Bd denotes the d-dimensional unit open ball in Rd, and Unif(·) denotes the continuous uniform distribution over a set. Similarly, we deﬁne LRi : Rd → R by LRi (u) := Ev∼Unif(Bd)[Li(u + Rv)], for each i ∈ [n] := {1, · · · , n}. We further deﬁne R · Sd−1 := {Rv : v ∈ Sd−1} and R · Bd := {Rv : v ∈ Bd}. Finally, we use
vold(·) to denote the volume of a set in d dimensions.
Proposition A.3. Let Fˆ(u; R, v) = Rd · L(u + Rv)v and F (u) = ∇L(u). Then the following holds:

Ev∼Unif(Sd−1) Fˆ(u; R, v) = ∇LR(u),

(10)

∇LR(u) − F (u) 2 ≤ R,

(11)

Fˆ(u; R, v) 2 ≤ dG + dML ,

(12)

R

Fˆ(u; R, v) − F (u) ≤ min (d + 1)G + dML , R + 2dG + 2dML . (13)

R

R

Proof. First, to establish (10), observe that since LR(u) = Ev∼Unif(Bd)[L(u + Rv)] and

Fˆ(u; R, v)

=

d R

·

L(u

+

Rv)v

for

each

u

∈

Rd,

R

>

0,

and

v

∈

S d−1 :

∇LR(u) = ∇Ev∼Unif(Bd) L(u + Rv)

= ∇Ev∼Unif(R·Bd) L(u + v)

Ev∼Unif(Sd−1) Fˆ(u; R, v)

1 = vold(R · Bd) · ∇

L(u + v) dv
R·Bd

1

v

= vold(R · Bd) ·

L(u + v) ·
R·S d−1

dv, v2

d = R · Ev∼Unif(Sd−1) L(u + Rv)v

d

v

= R · Ev∼Unif(R·Sd−1) L(u + v) · v 2

d

1

v

= R · vold−1(R · Sd−1) ·

L(u + v) ·
R·S d−1

dv, v2

(14)

where (14) follows because Stokes’ Theorem (see, e.g., Lee, Theorem 16.11 [Lee13]) implies that:

v

∇

L(u + v) dv =

L(u + v) ·

dv.

R·Bd

R·S d−1

v2

The equality (10) now follows by observing that the surface-area-to-volume ratio of R · Bd is d/R.
Next, to establish (11), we note that:

∇LR(u) − F (u) 2 = ∇Ev∼Unif(Bd) LR(u) − L(u) 2

18

1

= vold(Bd) · ∇

L(u + Rv) − L(u) dv
Bd

2

1

≤ vold(Bd) ·

F (u + Rv) − F (u) dv
Bd

(15)

2

1 ≤ vold(Bd) · Bd F (u + Rv) − F (u) 2 dv

1 ≤ vold(Bd) · Bd R · v 2 dv

≤ R,

where (15) follows by diﬀerentiating under the integral sign (see, e.g., Rudin, Theorem 9.42 [Rud76]), and the remaining inequalities follow from the fact that F is -Lipschitz.
Next, we establish (12) by using the triangle inequality and the ML-boundedness of L(·) on X × Y, and the G-Lipschitzness of L(·):

|Fˆ(u; R, v)| =

d |L(u + Rv)| ·

v

2

R

d ≤ · |L(u)| + |L(u + Rv) − L(u)| · 1
R

d ≤ R · (ML + RG).

We can then use (12) to establish (13) by observing that:

|Fˆ(u; R, v) − F (u)| ≤ |Fˆ(u; R, v)| + |F (u)| ≤ (d + 1)G + dML . R
and, from (12):

|Fˆ(u; R, v) − F (u)| ≤ Fˆ(u; R, v) − Ev[Fˆ(u; R, v)|u] + Ev[Fˆ(u; R, v)|u] − F (u) ≤ Fˆ(u; R, v) − Ev[Fˆ(u; R, v)|u] + ∇LR(u) − F (u) ≤ 2 dG + dML + R
R

This concludes the proof.

Below, we present technical lemmas that allow us to analyze the convergence rate of the correlated iterates {uti} in our random reshuﬄing-based OGDA Algorithm (Alg. 1).
Let σ0, · · · , σt−1 denote the permutations drawn from epoch 0 to epoch t − 1, and let {uti(σt)}1≤i≤n and {uti(σ˜t)}1≤i≤n denote the iterates obtained at epoch t, when the permutations σt and σ˜t are used for the epoch t, respectively. Moreover, let Di,t denote the distribution of {uti(σt)}1≤i≤n under σt, and for 1 ≤ r ≤ n let Di(,rt) denote the distribution of {uti(σt)}1≤i≤n with σt conditioned on the event {σit−1 = r}.
We use the p-Wasserstein distance between probability distributions on Rd, deﬁned below, to characterize the distance between Di,t and Di(,rt). This is used in the couplingbased techniques employed to establish non-asymptotic convergence results for our random
reshuﬄing algorithm. Note the diﬀerence between the p-Wasserstein distance for probability distributions on Rd, and the Wasserstein distance on Z := Rd × {+1, −1} associated with a metric c : Z × Z → [0, ∞), deﬁned in Appendix B.2 (Deﬁnition B.1).
Deﬁnition A.1 (p-Wasserstein distance between distributions on Rd). Let µ, ν be probability distributions over Rd with ﬁnite p-th moments, for some p ≥ 1, and let Π(µ, ν)

19

denote the set of all couplings (joint distributions) between µ and ν. The p-Wasserstein distance between µ and ν, denoted Wp(µ, ν), is deﬁned by:

1/p

Wp(µ, ν) =

inf

Eπ X − X p .

(X,X )∼π∈Π(µ,ν)

The following proposition characterizes the 1-Wasserstein distance as a measure of the gap between Lipschitz functions of random variables.
Proposition A.4 (Kantorovich Duality). If µ, ν are probability distributions over Rd with ﬁnite second moments, then:

W1(µ, ν) = sup EX∼µ[g(X)] − EY ∼ν[g(Y )],
g∈Lip(1)

where Lip(1) := {g : Rd → R : g is 1-Lipschitz}.

Using [YLMJ21, Lemma C.2], we now bound the diﬀerence between the unbiased gap

E[∆(uti)]

and

the

biased

gap

E[L

σ

t

(x

t i+1

,

y

) − Lσt (x

, yit+1)]

using

the

Wasserstein

metric.

i

i

Lemma A.5. Let u := (x , y ) ∈ Rdx × Rdy = Rd denote a saddle point of the min-max

optimization

problem

(2).

Then,

for

each

t

∈

[T ]

and

i

∈

[n],

the

iterates

{uti}

=

{(

x

t i

,

yit

)

}

of the OGDA-RR algorithm satisfy:

E[∆(uti+1)] − E

Lσ

t

(x

t i+1

,

y

) − Lσt (x

, yit+1)

i

i

Gn

r

≤ n

W2 Di+1,t, Di+1,t

r=1

Proof. Since σt and σ˜t are independently generated permutations of [n], the iterates {uti}1≤i≤n = {uti(σt)}1≤i≤n and {uti(σ˜t)}1≤i≤n are i.i.d. Thus, we have:

E[∆(uti+1)] = E

L

σ

t

(

x

t i+1

(σ˜

t

)

,

y

) − Lσt (x

, yit+1(σ˜t))

,

i

i

and thus:

E[∆(uti+1)] − E

L

σ

t

(

x

t i+1

,

y

) − Lσt (x

, yit+1)

i

i

= E Lσt (xti+1(σ˜t), y ) − Lσt (x , yit+1(σ˜t)) − E Lσt (xti+1, y ) − Lσt (x , yit+1)

i

i

i

i

1n

t

t

t

t

= n

E Lr(xi+1(σ˜ ), y ) − Lr(x , yi+1(σ˜ ))

(16)

r=1

1n

t

t

t

− n

E Lr(xi+1, y ) − Lr(x , yi+1) σi = r

r=1

1n

t

t

t

t

t

t

t

≤ n

E Lr(xi+1(σ˜ ), y ) − Lr(x , yi+1(σ˜ )) − E Lr(xi+1, y ) − Lr(x , yi+1) σi = r

r=1

1n

≤

sup

n r=1 g∈Lip(G)

E

g

(

x

t i+1

(σ˜

t

)

,

yit+1

(σ˜

t

))

−E

g

(x

t i+1

,

yit+1

)|

σit

=

r

(17)

1n

(r)

≤ n

G · W1(Di+1,t, Di+1,t)

r=1

(18)

1n

(r)

≤ n

G · W2(Di+1,t, Di+1,t),

r=1

(19)

where (16) follows by properties of the conditional expectation on {σit = r} and the fact that σt and σ˜t are independent, (17) follows from the fact that L is Lipschitz, (18) follows from Proposition A.4, and (19) follows from the fact that W1(µ, ν) ≤ W2(µ, ν) for any two probability distributions µ, ν.

20

The next lemma bounds the diﬀerence in the iterates {uti(σt)} and {uti(σ˜t)} (assuming, as before, that σ0, · · · , σt−1 were ﬁxed and identical for both sequences.)

Lemma A.6. Denote, with a slight abuse of notation, uti := uti(σt) and u˜ti := uti(σ˜t). Then:

uti+1 − u˜ti+1 2 ≤

n

t

t

t

ηt

6nd + 14n + 2 · 1{σi = σ˜i } G · η + 6ndML · Rt .

i=1

Proof. Our proof strategy is to bound the diﬀerences between zeroth-order and ﬁrst-order OGDA updates, and between the OGDA and proximal point updates. To this end, we deﬁne:

uti+1 = ProjX ×Y u˜ti+1 = ProjX ×Y vit+1 = ProjX ×Y v˜it+1 = ProjX ×Y wit+1 = ProjX ×Y w˜it+1 = ProjX ×Y

uti − ηtFˆσt (uti; Rt, vit) − ηtFˆσt (uti; Rt, vit) + ηtFˆσt (uti−1; Rt, vit−1) ,

i

i−1

i−1

u˜ti − ηtFˆσ˜t (u˜ti; Rt, vit) − ηtFˆσ˜t (u˜ti; Rt, vit) + ηtFˆσ˜t (u˜ti−1; Rt, vit−1) ,

i

i−1

i−1

uti − ηtFσt (uti) − ηtFσt (uti) + ηtFσt (uti−1) ,

i

i−1

i−1

u˜ti − ηtFσ˜t (u˜ti) − ηtFσ˜t (u˜ti) + ηtFσ˜t (u˜ti−1) ,

i

i−1

i−1

uti − ηtFσt (wit+1) , i

u˜ti − ηtFσ˜t (w˜it+1) . i

By the triangle inequality:

uti+1 − u˜ti+1 2 ≤

uti+1 − vit+1 2 + vit+1 − wit+1 2 + wit+1 − w˜it+1 2 + w˜it+1 − v˜it+1 2 + v˜it+1 − u˜ti+1 2.

(20)

Observe that bounding the fourth term is equivalent to bounding the second term, and bounding the ﬁfth term is equivalent to bounding the ﬁrst term.
To bound the ﬁrst term on the right hand side, we use Proposition A.3 to conclude that:

uti+1 − vit+1

2 ≤ ηt · Fˆσt (uti; Rt, vit) − Fσt (uti) + ηt · Fˆσt (uti; Rt, vit) − Fσt (uti)

i

i

i−1

i−1

+ ηt · Fˆσt (uti−1; Rt, vit−1) − Fσt (uti−1)

i−1

i−1

t

ηt

≤ 3(d + 1)Gη + 3dML · Rt

(21)

For the second term, we use the G-Lipschitzness of Lr, for each r ∈ [n] to conclude that:

vit+1 − wit+1 2 ≤ ηt · |Fσt (uti)| + ηt · |Fσt (uti)| + ηt · |Fσt (uti−1)| + ηt · |Fσt (wit+1)|

i

i−1

i−1

i

≤ 4G · ηt.

(22)

For the third term, we observe that if σit = σ˜it, then:

wit+1 − w˜it+1 2 ≤ ≤

uti − u˜ti 2 + ηt · Fσt (wit+1) − Fσ˜t (w˜it+1) 2

i

i

uti − u˜ti 2 + 2G · ηt.

(23)

On the other hand, if σit = σ˜it, then:

wit+1 = ProjX ×Y w˜it+1 = ProjX ×Y

uti − ηtFσt (wit+1) , i
u˜ti − ηtFσt (w˜it+1) , i

so we have:

wit+1 − w˜it+1

2 2

21

≤ (wit+1 − w˜it+1) (uti − η · Fσt (wit+1)) − (u˜ti − η · Fσt (w˜it+1))

i

i

= (wit+1 − w˜it+1) (uti − u˜ti) − η(wit+1 − w˜it+1) Fσt (wit+1)) − Fσt (w˜it+1))

i

i

≤ (wit+1 − w˜it+1) (uti − u˜ti)

≤ wit+1 − w˜it+1 2 · uti − u˜ti 2,

(24)
(25) (26)

so wit+1 − w˜it+1 2 ≤ uti − u˜ti 2. Here, (24) follows from the deﬁnitions of wit+1 and w˜it+1,

as well as Proposition A.1, while (25) holds because the monotonicity of Fi, for each i ∈ [n],

implies that (wit+1 − w˜it+1) Fσt(wit+1) − Fσt(w˜it+1) ≥ 0. Putting together (21), (22), (23),

i

i

(26), we have:

uti+1 − u˜ti+1 2 ≤

t

t

t

ηt

ui − u˜i 2 + (6d + 14)G · η + 6dML · Rt

+ 2G · 1{σit = σ˜it} · ηt,

where the indicator 1(A) returns 1 if the given event A occurs, and 0 otherwise. Since ut0 = u˜t0, we can iteratively apply the above inequality to obtain that, for any and
epoch t and i ∈ [n]:

t

t

t

ηt

n

t

t

ui+1 − u˜i+1 2 ≤ (6d + 14)nG · η + 6ndML · Rt + 2ηtG · 1{σi = σ˜i },

i=1

Remark. In the theorems and lemmas below, we will be concerned with the case where σt and σ˜t have the following speciﬁc relationship. Let Rn denote the set of all random permutations over the set [n]. For each l, m ∈ [n], let Sl,m : Rn → Rn denote the map that swaps, for each input permutation σ, the l-th and m-th entries. For each r, i ∈ [n], deﬁne
the map ωr,i : Rn → Rn as follows:

ωr,i(σ) =

σ, Si−1,j (σ),

if σi−1 = r, . if σj = r and j = i − 1.

Intuitively, ωr,i performs a single swap such that the (i − 1)-th position of the permutation is r. Clearly, if σt is a random permutation (i.e., selected from a uniform distribution over
Rn), then ωr,i(σs) has the same distribution as σt|(σit−1 = r). Based on this construction, we have ui(σt) ∼ Di,t and ui(ωr,i(σt)) ∼ Di(,rt). This gives a coupling between Ds,t and Ds(r,t). Since σt and σ˜t diﬀer by at most two entries, by iteratively applying Lemma A.6, we have:

t

t

t

ηt

ui+1 − u˜i+1 2 ≤ n (6d + 14)G · η + 6dML · Rt

+ 4G · ηt

t

ηt

= (6nd + 14n + 4)G · η + 6ndML · Rt ,

as claimed.

Lemma

A.7.

If

ηt

≤

1/(2

)

for

each

t

∈

{0, 1, · · ·

,T

− 1},

the

iterates

{uti}

=

{

(x

t i

,

yit

)

}

of the OGDA-RR algorithm satisfy, for each u ∈ X × Y:

2ηt · E Fσt (uti+1), uti+1 − u i

≤ E uti − u 22 − E uti+1 − u 22 − 21 E uti+1 − uti 22 + 21 E

+ 2ηt · E Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

uti − uti−1

2 2

22

− 2ηt · E Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 6C1 · η R + (η ) R + (η ) + Rt + (Rt)2 ,

where C1 := d2 max

6G

D, 18G2 + 6ML

D

,

30M

L

G,

12M

2 L

is a constant independent of

the sequences {ηt} and {Rt}.

Proof. The iterates of the OGDA-RR algorithm are given by:

uti+1 = ProjX ×Y uti − ηtFˆσt (uti; Rt, vit) − ηtFˆσt (uti; Rt, vit)

i

i−1

− ηtFˆσt (uti−1; Rt, vit−1) i−1

= ProjX ×Y uti − ηtFσt (uti+1) + ηt γit + Eit,1 + Eit,2 + Eit,3 , i

where we have deﬁned:

γit := Fσt (uti+1) − Fσt (uti) − Fσt (uti) + Fσt (uti−1),

i

i

i−1

i−1

Eit,1 := Fσt (uti) − Fˆσt (uti; Rt, vit),

i

i

Eit,2 := Fσt (uti) − Fˆσt (uti; Rt, vit),

i−1

i−1

Eit,3 := Fσt (uti−1) − Fˆσt (uti−1; Rt, vit−1).

i−1

i−1

First, by applying Lemma A.2 we have:

(27)

2ηt · E Fσt (uti+1), uti+1 − u i

≤E

uti − u

2 2

−E

uti+1 − u

2 2

−E

uti+1 − uti

2 2

+ 2ηt · E γit, uti+1 − u

3
+ 2ηt · E Eit,k, uti+1 − u .

k=1

(28)

Below, we proceed to bound the inner product terms on the right-hand-side of (28). First, we bound γit, uti+1 − u :

γit, uti+1 − u = Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− Fσt (uti) − Fσt (uti−1), uti+1 − u

i−1

i−1

= Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

− Fσt (uti) − Fσt (uti−1), uti+1 − uti

i−1

i−1

≤ Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

+ 12 · uti − uti−1 22 + 12 · uti+1 − uti 22.

(29)

Note that the ﬁnal inequality follows by applying Young’s inequality, and noting that F is -Lipschitz. Next, we bound Eit,1, uti+1 − u :

E Eit,1, uti+1 − u

23

= E Fσt (uti) − Fˆσt (uti, Rt, vit), uti+1 − u

i

i

= E Fσt (uti) − ∇LRσtt (uti), uti+1 − u

i

i

+ E E Fˆσt (uti; Rt, vit|uti − Fˆσt (uti, Rt, vit), uti+1 − u

i

i

= E Fσt (uti) − ∇LRσtt (uti), uti+1 − u

i

i

+ E Ev Fˆσt (uti; Rt, v|uti − Fˆσt (uti, Rt, vit), uti − u

i

i

+ E Ev Fˆσt (uti; Rt, v|uti − Fˆσt (uti, Rt, vit), uti+1 − uti ,

i

i

(30)

where the ﬁrst equality above follows by applying Proposition A.3, (10), and we have used the shorthand Ev := Ev∼Unif(Sd−1). (Recall that LR(u) := Ev∼Unif(Sd−1) L(u + Rv) ) Next, we upper bound each of the three quantities in (30). First, by Proposition A.3, (11), we
have:

E Fσt (uti) − ∇LRσtt (uti), uti+1 − u

i

i

≤ E Fσt (uti) − ∇LRσtt (uti) 2 · uti+1 − u 2

i

i

≤ D · Rt,

(31)

with C1 > 0 as given in Lemma A.7. Meanwhile, the law of iterated expectations can be used to bound the second quantity:

E Ev Fˆσt (uti; Rt, v)|uti − Fˆσt (uti, Rt, vit), uti − u

i

i

= E Ev Fˆσt (uti, Rt, vit), uti − u uti − E Fˆσt (uti, Rt, vit), uti − u

i

i

= 0,

(32)

and we can upper-bound the third quantity as shown below. By using the compactness of X × Y and the continuity of L, we have:

E Ev Fˆσt (uti; Rt, v)|uti − Fˆσt (uti, Rt, vit), uti+1 − uti

i

i

≤ Ev Fˆσit (uti; Rt, v)|uti 2 + Fˆσit (uti, Rt, vit) · uti+1 − uti 2

d ≤ 2 · Rt ·

sup
u∈X ×Y

|L(uti + Rtv)| ·

v∼Unif(S d−1 )

uti+1 − uti 2,

≤ 2 · Rdt · (ML + RtG) · uti+1 − uti 2,

and using (31) and the bound for each Fˆσt 2 given in (33), we have: i

uti+1 − uti 2

≤ ηt · Fˆσt (uti; Rt, vit) + Fˆσt (uti; Rt, vit) − Fˆσt (uti−1; Rt, vit−1)

i

i−1

i−1

≤ ηt · Fσt (uti) + Fσt (uti) − Fσt (uti−1) 2

i

i−1

i−1

+ ηtd · Fˆσt (uti; Rt, vit) − Fσt (uti) 2

i

i

+ ηtd · Fˆσt (uti; Rt, vit) − Fσt (uti) 2

i−1

i−1

+ ηtd · Fˆσt (uti−1; Rt, vit−1) − Fσt (uti−1) 2

i−1

i−1

≤ 3Gηt + 3ηtd · 2(ML + GRt) · 1 + D · Rt . Rt

(33) (34)

24

Substituting (34) back into (33), we have:

E Ev Fˆσt (uti; Rt, v)|uti − Fˆσt (uti, Rt, vit), uti+1 − uti

i

i

2

t

t

2t 2

2t

1

2t 2 1 2

≤ d D6η G · R + 6d η (3G + ML D) + 30d η MLG · Rt + 12d η ML · Rt

t t t ηt

ηt

≤ C1 · η R + η + Rt + (Rt)2 ,

(35)

where C1 := d2 · max

6G

D, 18G2

+ 6ML

D

,

30

M

L

G,

12M

2 L

is a constant independent

of the sequences {ηt} and {Rt}. The quantities E Eit,2, uti+1 − u and E Eit,3, uti+1 − u

can be similarly bounded. Substituting (31), (32), (35) back into (30), and substituting

(30) and (29) into (28), we ﬁnd that:

2ηt · E Fσt (uti+1), uti+1 − u i

=E

uti − u

2 2

−E

uti+1 − u

2 2

+ 2ηt · E γit, uti+1 − u

≤E

uti − u

2 2

−E

uti+1 − u

2 2

−E

uti+1 − uti

2 2

3
+ 2ηt · E

Eit,k, uti+1 − u

k=1

−E

uti+1 − uti

2 2

+ 2ηt · E Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− 2ηt · E Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

+ ηt · E

uti − uti−1

2 2

+ ηt

·E

uti+1 − uti

2 2

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 6C1 · η R + (η ) R + (η ) + Rt + (Rt)2 ,

In particular, since by assumption ηt ≤ 1/(2 ) for each t ∈ {0, 1, · · · , T − 1}, then:

2ηt · E Fσt (uti+1), uti+1 − u i

≤ E uti − u 22 − E uti+1 − u 22 − 21 E uti+1 − uti 22 + 21 E

+ 2ηt · E Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− 2ηt · E Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 6C1 · η R + (η ) R + (η ) + Rt + (Rt)2 ,

uti − uti−1

2 2

Finally, to bound the step size terms above, we require the following lemma, which follows from standard calculus arguments.

Lemma A.8.

T
t−β ≥

1 T 1−β,

1−β

t=1

T t−(1+β) ≤ 1 + 1, β
t=1

∀ β < 1, ∀ β > 0.

25

A.2 Proof of Theorem 4.1
Proof. (Proof of Theorem 4.1) By applying Lemma A.7 (note that ηt ≤ η0 ≤ 21 , for each t ∈ {0, 1, · · · , T − 1}) and using convex-concave nature of Lr (refer Proposition 1 in [MOP20b]), for each r ∈ {1, · · · n}, we have:

2ηt · E Lσt (xti+1, y ) − Lσt (x , yit+1)

i

i

≤ 2ηt · E Fσt (uti+1), uti+1 − u i

≤ E uti − u 22 − E uti+1 − u 22 − 21 E

uti+1 − uti 22 + 12 E

+ 2ηt · E Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− 2ηt · E Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 6C1 · η R + (η ) R + (η ) + Rt + (Rt)2 .

uti − uti−1

2 2

(36)

Meanwhile, Lemma A.5, Proposition A.4 (Kantorovich Duality), and Lemma A.6 imply that:

E[∆(uti+1)] − E

Lσ

t

(x

t i+1

,

y

) − Lσt (x

, yit+1)

i

i

Gn ≤
n r=1

E uti+1(σt) − uti+1(σ˜t) 22

Gn

r

≤ n

W2 Di+1,t, Di+1,t

r=1

t

ηt

≤ G · (6nd + 14n + 4)G · η + 6ndML · Rt .

Substituting back into (36), we have:

2ηt · E ∆(uti)

≤ 2ηt · E Lσt (xti+1, y ) − Lσt (x , yit+1)

i

i

t2

(ηt)2

+ G · (12nd + 28n + 8)G · (η ) + 12ndML · Rt

≤ E uti − u 22 − E uti+1 − u 22 − 21 E uti+1 − uti 22 + 12 E uti − uti−1 22

+ 2ηt · E Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

− 2ηt · E Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 6C1 · η R + (η ) R + (η ) + Rt + (Rt)2

t2

(ηt)2

+ G · (12nd + 28n + 8)G · (η ) + 12ndML · Rt .

(37)

We can now sum the above telescoping terms across the t-th epoch, as shown below:

n
2 · ηt · E ∆(uti)

i=1

≤ E ut1 − u 22 − E ut1+1 − u 22 + 21 E ut1 − ut0 22 − 21 E

+ 2ηt · E Fσt (ut1) − Fσt (ut0), ut1 − u

0

0

ut1+1 − ut0+1

2 2

26

− 2ηt · E + 6nC1 ·

Fσt+1 (ut1+1) − Fσt+1 (ut0+1), ut1+1 − u

0

0

tt

t2 t

t 2 (ηt)2 (ηt)2

η R + (η ) R + (η ) + Rt + (Rt)2

t2

(ηt)2

+ nG · (12nd + 28n + 8)G · (η ) + 12ndML · Rt .

Meanwhile, we have for each t = 0, 1, · · · , T − 1, i ∈ [n]:

E Fσt (uti+1) − Fσt (uti), uti+1 − u

i

i

≤ E Fσt (uti+1) − Fσt (uti) · uti+1 − u

i

i

= · E uti+1 − uti · D

≤ D · E − ηtFˆσt (uti; Rt, vit) − ηtFˆσt (uti; Rt, vit) + ηtFˆσt (uti−1; Rt, vit−1)

i

i−1

i−1

≤ 3 D · ηt · dG + dML Rt

t

ηt

= 3 DdG · η + 3 DdML · Rt ,

where the ﬁnal inequality follows from Proposition A.3, (12). We can upper bound

E Fσt (uti) − Fσt (uti−1), uti − u

i−1

i−1

have:

in a similar fashion. Substituting back into (37), we

n

2 · ηt · E ∆(uti)

i=1

≤ E ut1 − u 22 − E ut1+1 − u 22 + 12 E ut1 − ut0 22 − 21 E

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 6nC1 · η R + (η ) R + (η ) + Rt + (Rt)2

ut1+1 − ut0+1

2 2

t2

(ηt)2

+ nG · (12nd + 28n + 8)G · (η ) + 12ndML · Rt

t2

(ηt)2

+ 6 DdG · (η ) + 6 DdML · Rt

≤ E ut1 − u 22 − E ut1+1 − u 22 + 21 E ut1 − ut0 22 − 21 E ut1+1 − ut0+1 22

(38)

tt

t2 t

t 2 (ηt)2 (ηt)2

+ 2C · η R + (η ) R + (η ) + Rt + (Rt)2 ,

where C := max{3nC1, (6nd + 14n + 4)nG, 6ndML, 3 DdG, 3 DdML}. Finally, summing the above telescoping terms over i ∈ [n] and t ∈ {0, 1, · · · , T − 1},
and removing non-positive terms, we obtain:

T −1 t=0

n i=1

ηt

·E

∆(uti)

T −1 t=0

n i=1

ηt

≤ 2·

1
T −1 t=0

n i=1

ηt

u00 − u 2 − E uTn −1 − u 2 + 21 u01 − u00 2 − 12 E uTn −1 − uTn−−11 2

+C ·

1

T −1

T −1

n

· ηt

t=0 i=1

t=0

tt

t2 t

t 2 (ηt)2 (ηt)2

η R + (η ) R + (η ) + Rt + (Rt)2

27

1

3D

1

T −1 tt

t2 t

t 2 (ηt)2 (ηt)2

≤ T −1 ηt · 4n + C · n T −1 ηt ·

η R + (η ) R + (η ) + Rt + (Rt)2 , (39)

t=0

t=0

t=0

By deﬁnition, ηt = η0 · (t + 1)−3/4−χ and Rt = R0 · (t + 1)−1/4, so by Lemma A.8, we have:

T −1

T

ηt = η0 · t−3/4−χ ≥ 4η0 · T 1/4−χ,

t=0

t=1

T −1 ηtRt = η0R0 · T t−(1+χ) ≤ η0R0 · 1 + 1 ,

χ

t=0

t=1

T −1

T

(ηt)2 = (η0)2 · t−3/2−2χ ≤ (η0)2 ·

t=0

t=1

1 1 + 1 + 2χ
2

≤ 3 · (η0)2,

T −1

T

(ηt)2Rt = (η0)2R0 · t−7/4−2χ ≤ (η0)2R0 ·

t=0

t=1

1 1 + 3 + 2χ
4

≤ 7 · (η0)2 0, 4

T −1 (ηt)2

(η0)2

T −5/4−2χ

(η0)2

Rt = R0 · t

≤ R0 ·

t=0

t=1

1 1 + 1 + 2χ
4

(η0)2 ≤5· 0 ,

T −1 (ηt)2

(η0)2

T −1−2χ

(η0)2

1

(Rt)2 = (R0)2 · t

≤ (R0)2 ·

1+ 2χ

.

t=0

t=1

Substituting back into (39) and using the convexity of the gap function ∆(·), we have:

E ∆(uT )

T −1 t=0

n i=1

ηt

·E

∆(uti)

≤

T −1 n ηt

t=0 i=1

1

3

1

T −1 tt

t2 t

t 2 (ηt)2 (ηt)2

≤

T −1

ηt

·

D 4n

+

C

·

T −1 ηt ·

η R + (η ) R + (η ) + Rt + (Rt)2

t=0

t=0

t=0

3

47

0 0 0 0 η0 η0

≤

D + · C max 16n 4n

R , η , η R , R0 , (R0)2

1 1+
χ

T −1/4+χ

≤ R.

where the ﬁnal inequality follows by deﬁnition of T .

28

B Wasserstein Distributionally Robust Strategic Classiﬁcation

B.1 Model of Adversary
In this subsection, we formally deﬁne our model for the adversary, and the uncertainty set of distributions for the resulting strategically and adversarially perturbed data. For better exposition, in this section we summarize the various distributions used in the main article in Table 1 below.
Table 1: Table of notations

Notation
D D(θ) D˜n(θ)
P Piθ

Explanation
Unknown underlying distribution Unknown underlying distribution strategically perturbed by θ
Empirical distribution of strategically perturbed data An element of uncertainty set P(θ)
Conditional distribution of adversarially generated data given ith data point

The WDRSL problem formulation contains two main components—the strategic compo-
nent that accounts for the distribution shift D(θ) in response to the choice of classiﬁer θ, and
the adversarial component, which accounts for the uncertainty set P(θ). As per the modeling
assumptions put forth in Section 5.2, we have (x˜i, y˜i) ∼ D and (bi(θ, x˜i, y˜i), y˜i) ∼ D(θ) for all i ∈ [n]. For the sake of brevity, we shall use bi(θ) in place of bi(θ, x˜i, y˜i) for all i ∈ [n].
As per the standard formulation of distributionally robust optimization, we restrict P(θ) to be a Wasserstein neighborhood of D˜n(θ) (the empirical distribution of strategic responses {(bi(θ), y˜i)}ni=1), i.e., we set P(θ) ⊂ Bδ(D˜n(θ)) for some δ > 0. However, to ensure that the min-max problem reformulated from the WDRSC problem is convex-concave, we further
require the adversary to modify the label of an data point i in the empirical distribution only
when the true label y˜i is +1, although they are still always allowed to modify the feature bi(θ). As a consequence, this imposes some restrictions on the conditional distribution Piθ of (dx, y), as generated by the adversary, given a data point i in the empirical distribution.
In particular:

Piθ(dx, +1|bi(θ), −1) = 0, ∀ i ∈ [n].

By deﬁnition of conditional distributions, we obtain that any distribution P can be expressed as the average of the conditional distribution Piθ. That is,
1n i P(dx, y) = n Pθ(dx, y|bi(θ), y˜i).
i=1

Below, we formally state the restriction described above.
Assumption B.1. We assume that P ∈ Bδ(D˜n(θ)) and Piθ(dx, +1|bi(θ), −1) = 0 for all i ∈ [n]. As a direct result, the uncertainty set P(θ) is characterized as:

P(θ) = Bδ(D˜n(θ)) ∩

1n i

i

n Pθ(dx, y|bi(θ), y˜i) Pθ(dx, +1|bi(θ), −1) = 0, ∀ i ∈ [n]

i=1

. (40)

In the following subsection, we reformulate the WDRSC problem with a generalized linear model and with the uncertainty set deﬁned in (40).

29

B.2 Proof of Theorem 5.3
The proof takes inspirations from [SAEK15, Theorem 1]. First, we deﬁne the Wasserstein distance between distributions on Z with cost function c; note that this is diﬀerent from the p-Wasserstein distance between probability distributions on Rd deﬁned in Appendix A.1.
Deﬁnition B.1. (Wasserstein distance between distributions on Z with cost Function c) Let µ, ν be probability distributions over Z := Rd × {+1, −1} with ﬁnite second moments, and let Π(µ, ν) denote the set of all couplings (joint distributions) between µ and ν. Given a metric c : Z × Z → [0, ∞) on Z, we deﬁne:

Wc(µ, ν) =

inf

Eπ c(Z, Z ) .

(Z,Z )∼π∈Π(µ,ν)

In Theorem 6 and in our proof below, we use the cost function c(z, z ) :=

x−x

2 2

+

κ · |y − y |, with a ﬁxed constant κ > 0, for each z := (x, y) ∈ Z and z := (x , y ) ∈ Z.

Proof. (Proof of Theorem 5.3) Fix a θ ∈ Θ. Note that bi(θ, x˜i, +1) = x˜i. For any (x, y) ∈ Rd × {−1, 1}, let ((x, y), θ) := φ( x, θ ) − y x, θ . We ﬁrst analyze the inner
supremum term, i.e.

sup EP[φ( x, θ ) − y x, θ ]
P∈P (θ)

= sup (z, θ)P(z)dz
P∈P(θ) Z

  sup = πθ∈Π(P,D˜n(θ))  s.t.

Z (z, θ)πθ(dz, Z) Z×Z z − z˜ πθ(dz, dz˜) ≤ δ

Here, Π(P, D˜n(θ)) denotes the set of all joint distributions that couple P ∈ P(θ) and D˜n(θ). Since the marginal distribution D˜n(θ) of z˜ is discrete, such couplings πθ are completely determined by the conditional distribution Piθ of z given z˜i = (x˜i(θ), y˜i) for each
i ∈ {1, . . . , n}. That is:

1 πθ(dz, dz˜) =

ϑ(b (θ),y˜ )(dz˜)Piθ(dz)

n

i

i

i∈[n]

where for any (x, y) ∈ Z, ϑ(x,y) is a Dirac delta distribution with its support at point (x, y).
We introduce some notations. Let I+1 = {i ∈ [n] : y˜i = +1} and I−1 = {i ∈ [n] : y˜i = −1}. Let’s introduce two distributions µiθ and νθi such that

Pi = µiθ if i ∈ I+1

θ

νθi if i ∈ I−1

Due to the constraint (40), we have νθi (dx, +1) = 0 at every x. This implies:





1 πθ(dz, dz˜) = n 

ϑ(bi(θ),1)(dz˜)µiθ(dz) +

ϑ(bi(θ),−1)(dz˜)νθi (dz)

i∈I+1

i∈I−1

With a slight abuse of notation, we denote µiθ,+1(dx) = µiθ(dx, +1), µiθ,−1(dx) = µiθ(dx, −1) and νθi (dx) = νθi (dx, −1). The optimization problem of concern then simpliﬁes to:

1 sup
n

((x, +1), θ)µiθ,+1(dx) + n1
Rd

((x,

−1),

θ

)µ

i θ

,

−1

(dx

)

Rd

µiθ,±1 ,νθi

i∈I+1

i∈I+1

30

1 +

((x, −1), θ)νi (dx)

n

d

θ

i∈I−1 R

1 s.t.
n

(x, +1) − (bi(θ), y˜i) µiθ,+1(dx)
d

i:y˜i=+1 R

1 +

(x, −1) − (bi(θ), y˜i) µi (dx)

n

d

θ,−1

i:y˜i=+1 R

µiθ,+1(dx) + µiθ,−1(dx) = 1, ∀ i ∈ I+1

Rd

Rd

νθi (dx) = 1, ∀ i ∈ I−1
Rd

First, we rewrite the inequality constraint above as follows. Recall that:

2κ nd

µi

1 (dx) +

θ,−1

nd

x − bi(θ) µiθ,+1(dx)

R i∈I+1

R i∈I+1

1 +
nd

x − bi(θ) µi

1 (dx) +

θ,−1

nd

x − bi(θ) νθi (dx) ≤ δ.

R i∈I+1

R i∈I−1

Hence,

1 sup
n

((x, +1), θ)µiθ,+1(dx) + n1
Rd

((x,

−1),

θ

)

µ

i θ

,

−1

(dx

)

Rd

µiθ,±1 ,νθi

i∈I+1

i∈I+1

1 +

((x, −1), θ)νi (dx)

n

d

θ

y˜i=−1 R

2κ s.t.
n

µiθ,−1(dx) + n1
Rd i∈I+1

Rd i∈I+1

x − bi(θ) µiθ,+1(dx)

1 +
nd

x − bi(θ) µi

1 (dx) +

θ,−1

nd

x − bi(θ) νθi (dx) ≤ δ

R i∈I+1

R i∈I−1

µiθ,+1(dx) + µiθ,−1(dx) = 1, ∀ i ∈ I+1

Rd

Rd

νθi (dx) = 1, ∀ i ∈ I−1
Rd

Now, we can use duality to reformulate the inﬁnite-dimensional optimization problem into a ﬁnite-dimensional problem:

sup EP[φ( x, θ ) − y x, θ ]
P∈P (θ)

 inf α,si   s.t. 
=
      

αδ + n1

i∈I+1

si

+

1 n

i∈I−1 ti

supx ((x, +1), θ) − α · 1+2y˜i x − bi(θ)

supx ((x, −1), θ) − α · 1+2y˜i x − bi(θ)

supx ((x, −1), θ) − α · 1−2y˜i x − bi(θ)

α≥0

≤ si ∀ i ∈ I+1 − ακ(1 + y˜i) ≤ si ≤ ti ∀ i ∈ I−1

∀ i ∈ I+1

which is equivalent to:

sup EP[φ( x, θ ) − y x, θ ]
P∈P (θ)

31

 inf α,si   s.t. 
=
      

αδ + n1

i∈I+1

si

+

1 n

supx ((x, +1), θ) − α

supx ((x, −1), θ) − α

supx ((x, −1), θ) − α

α≥0

i∈I−1 ti x − bi(θ) x − bi(θ) x − bi(θ)

≤ si ∀ i ∈ I+1 − 2ακ ≤ si ∀ i ∈ I+1 ≤ ti ∀ i ∈ I−1

We now invoke [YLMJ21, Lemma A.1], which claims that for any y˜ ∈ {+1, −1} and x˜ ∈ Rd:

((x˜, y˜), θ)

sup ((x, y˜), θ) − α x − x˜ =

x

−∞

if θ ≤ α/(L + 1) otherwise.

We now have:

sup EP[φ( x, θ ) − y x, θ ]
P∈P (θ)


inf α,si  s.t.    
=
         

αδ + n1

i∈I+1

si

+

1 n

i∈I−1 ti

((bi(θ), +1), θ) ≤ si ∀ i ∈ I+1

((bi(θ), −1), θ) − 2ακ ≤ si ∀ i ∈ I+1

((bi(θ), −1), θ) ≤ ti ∀ i ∈ I−1

α≥0

θ ≤ α/(L + 1)

In the above presented optimization problem we can conclude that:

ti = φ( bi(θ), θ ) + bi(θ), θ si = max{ ((bi(θ), +1), θ), ((bi(θ), −1), θ) − 2ακ}
To further simplify the si expression, note that:

∀i ∈ I−1 ∀i ∈ I+1.

si = max{φ( bi(θ), θ ) − bi(θ), θ , φ( bi(θ), θ ) + bi(θ), θ = φ( bi(θ), θ ) − bi(θ), θ + max{0, 2 bi(θ), θ − 2ακ} = φ( bi(θ), θ ) − ακ + max γi ( bi(θ), θ − ακ) ,
γi:|γi|≤1

− 2ακ}

so the overall objective can be written as:

sup EP[φ( x, θ ) − y x, θ ]
P∈P (θ)

 infα maxγ: γ ∞≤1  =
 s.t.

α(δ − κ) + n1

i

1+y˜i 2

(φ(

bi(θ), θ

) + γi (

bi(θ), θ

+ n1

i

1−y˜i 2

(φ(

bi(θ), θ

)+

bi(θ), θ )

θ ≤ α/(L + 1)

− ακ))

We claim that the minimax objective above is convex is θ. There are mainly two cases to analyze:
1. Case I (i ∈ I+1): We have bi(θ) = x˜i as per the strategic classiﬁcation model. Therefore bi(θ), θ is a linear function. For every γ, α, we claim that the mapping θ → φ( bi(θ), θ ) + γi( bi(θ), θ − ακ) is convex. Indeed, the assumption that φ is convex and the observation that bi(θ), θ is aﬃne in θ ensures the convexity.

2. Case II (i ∈ I−1): We know from Lemma 5.2 that bi(θ), θ is convex in θ. Moreover, the convexity of φ and the assumption that z → φ(z) + z is non-decreasing ensures that φ( bi(θ), θ ) + bi(θ), θ is convex for every i.
This concludes the proof.

32

C Details on experimental study
Code used to reproduce the results in the main paper is available at https://drive.google. com/drive/folders/1spuB3R6vEU2AqaXxAxeeXo9z5QMVdtdl?usp=sharing

C.1 Algorithms
In our experiments, we compare the OGDA-RR algorithm (Alg. 1) with three other zerothorder algorithms—Optimistic Gradient Descent Ascent with Sampling with Replacement (OGDA-WR), Stochastic Gradient Descent Ascent with Random Reshuﬄing (SGDA-RR), and Stochastic Gradient Descent Ascent with Sampling with Replacement (SGDA-WR)— characterized by the update equations (42), (43), (44), respectively. For convenience, we have reproduced (5), the update equation for the OGDA-RR algorithm (Algorithm 1), as (41) below:

uti+1 = ProjX ×Y
uti+1 = ProjX ×Y
uti+1 = ProjX ×Y uti+1 = ProjX ×Y

uti − ηtFˆσt (uti; Rt, vit) − ηtFˆσt (uti; Rt, vit) + ηtFˆσt (uti−1; Rt, vit−1) ,

i

i−1

i−1

(41)

uti − ηtFˆjt (uti; Rt, vit) − ηtFˆjt (uti; Rt, vit) + ηtFˆjt (uti−1; Rt, vit−1) ,

i

i−1

i−1

(42)

uti − ηtFˆσt (uti; Rt, vit) , i

(43)

uti − ηtFˆjt (uti; Rt, vit) , i

(44)

where the indices σit and jit are as deﬁned in Algorithms 2, 3, and 4.

Algorithm 2: OGDA-WR Algorithm

1

Input:

stepsizes

ηt, Rt,

data

points

{(

x

i

,

yi

)}

n i=1

∼ D, u(00),

time

horizon

duration

T;

2 for t = 0, 1, · · · , T − 1 do

3 for i = 0, . . . , n − 1 do

4

Sample jit ∼ Unif({1, · · · , n})

5

Sample vit ∼ Unif(Sd−1)

6

uti+1 ← (42)

7 end

8

u0(t+1) ← utn

9

u−(t+1 1) ← utn−1

10 end

11 Output: u˜T := n· Tt1=−01 ηt Tt=−01 ni=1 ηtuti.

C.2 Additional Experimental Results
In this section, we present more experimental ﬁndings, on both synthetic and real-world datasets, that reinforces the utility of the proposed algorithm. In all experimental results throughout this subsection, we take δ = 0.4, κ = 0.5 and ζ = 0.05.
C.2.1 Experimental Study On Synthetic Datasets Figure 2 compares the performance of (A-I)-(A-IV) on a synthetic dataset (whose generating process is the same as that described in Section 6), with 4000 training points and 800 test points. Our proposed algorithm performs better empirically compared to most of
33

Algorithm 3: SGDA-RR Algorithm

1

Input:

stepsizes

ηt, Rt,

data

points

{(

x

i

,

yi

)}

n i=1

∼ D, u(00),

time

horizon

duration

T;

2 for t = 0, 1, · · · , T − 1 do

3 for i = 0, . . . , n − 1 do

4

Sample jit ∼ Unif({1, · · · , n})

5

Sample vit ∼ Unif(Sd−1)

6

uti+1 ← (43)

7 end

8

u0(t+1) ← utn

9

u−(t+1 1) ← utn−1

10 end

11 Output: u˜T := n· Tt1=−01 ηt Tt=−01 ni=1 ηtuti.

Algorithm 4: SGDA-WR Algorithm

1

Input:

stepsizes

ηt, Rt,

data

points

{(

x

i

,

yi

)}

n i=1

∼ D, u(00),

time

horizon

duration

T;

2 for t = 0, 1, · · · , T − 1 do

3 σt = (σ1t , · · · , σnt ) ← a random permutation of set [n];

4 for i = 0, . . . , n − 1 do

5

Sample vit ∼ Unif(Sd−1)

6

uti+1 ← (44)

7 end

8

u0(t+1) ← utn

9

u−(t+1 1) ← utn−1

10 end

11 Output: u˜T := n· Tt1=−01 ηt Tt=−01 ni=1 ηtuti.

34

Suboptimality Accuracy

Synthetic Dataset with 4000 datapoints

100

Z-OGDA w RR

Z-OGDA w/o RR

10 1

Z-SGDA w/o RR Z-SGDA w RR

10 2

10 3

10 4 0

50 100 150 200 250 300 350 400
Epochs

Synthetic Dataset with 4000 datapoints

LogReg SC

0.9

Z-OGDA w RR

0.8

0.7

0.6

0.5 0.0 0.2 0.4 0.6 0.8 1.0
Perturbation

Figure 2: Experimental results for a synthetic dataset with n = 4000. (Left pane)) Suboptimality iterates generated by the four algorithms (A-I), (A-II), (A-III), (A-IV), respectively denoted as Z-OGDA w RR, Z-OGDA w/o RR, Z-SGDA w RR, Z-SGDA w/o RR. (Right pane ) Comparison between decay in accuracy of strategic classiﬁcation with logistic regression (trained with ζ = 0.05) and Alg. (A-I) with changes in perturbation.

its counterparts. Moreover, the proposed classiﬁer, (A-I), is signiﬁcantly more robust than a classiﬁer obtained without considering adversarial perturbations. Note, however, that we cannot make any conclusive claims yet, because of the inherent randomness in these algorithms. Indeed, even if we ﬁx the initialization, then there are two sources of randomness—the construction of the zeroth-order gradient estimator, and the sampling process that generates the data points.
To illustrate the variability in these algorithms’ performance, we run each algorithm repeatedly on a data set with 500 synthetically generated data points, using the same initialization, and present conﬁdence interval plots with ±2 standard deviations for the resulting performance (Figure 3). On average, our proposed algorithm (A-I) outperforms the other algorithms (A-II)-(A-IV). It is also interesting to point out that the performance of algorithms with random reshuﬄing is generally higher, and ﬂuctuate less, compared to the performance of algorithms without random reshuﬄing.
We now illustrate the performance of our algorithm on two real-world data sets—the “GiveMeSomeCredit” dataset 3, and the “Porto Bank” data set4.
C.2.2 Experimental Study on Credit Dataset
In modern times, banks use machine learning to determine whether or not to ﬁnance a customer. This process can be encoded into a classiﬁcation framework, by using features such as age, debt ratio, monthly income to classify a customer as either likely or unlikely to default. However, those algorithms generally do not account for strategic or adversarial behavior on the part of the agents.
To illustrate the eﬀect of our algorithm on datasets of practical signiﬁcance, we deploy our algorithms on the “GiveMeSomeCredit”(GMSC) dataset, while assuming that the underlying features are subject to strategic or adversarial perturbations. We use a subset of the dataset of size 2000 with balanced labels. In Figure 4, we compare the empirical performance of our algorithm (A-I) with that of (A-II)-(A-IV). The left pane shows that (A-I) performs well, and the right pane illustrates that our classiﬁer is signiﬁcantly more robust to adversarial perturbations in data, compared to the strategic classiﬁcation-based
3This dataset can be found at https://www.kaggle.com/c/GiveMeSomeCredit 4This dataset can be found at https://archive.ics.uci.edu/ml/datasets/bank+marketing

35

100

Synthetic Dataset with 500 datapoints

Z-OGDA w RR

Z-OGDA w/o RR

10 1

SGDA w/o RR

SGDA w RR

10 2

Suboptimality

10 3

10 4

10 5 0

100 200 300 400 500 Epochs

Figure 3: Experimental results for a synthetic dataset with n = 500. Suboptimality iterates generated by the four algorithms (A-I), (A-II), (A-III), and (A-IV) are respectively denoted as Z-OGDA w RR, Z-OGDA w/o RR, Z-SGDA w RR, and Z-SGDA w/o RR.

logistic regression algorithm developed recently in the literature [DRS+18].
C.2.3 Experimental Study on Porto-Bank Dataset
Next, we present empirical results obtained by applying our algorithm to the “Porto-Bank” dataset, which describes marketing campaigns of term deposits at Portuguese ﬁnancial institutions. The classiﬁcation task in this scenario aims to predict whether a customer with given features (eg. age, job, marital status etc.) would enroll for term deposits.
In Figure 5, we present the performance of our proposed algorithm (A-I) on the PortoBank dataset. For ease of illustration, we consider a subset of the dataset with 2000 training data points, 800 test data points, and balanced labels. In Figure 5, we compare the empirical performance of our algorithm (A-I) with that of (A-II)-(A-IV). The left pane shows that (A-I) performs well, while the right pane illustrates that our classiﬁer is signiﬁcantly more robust to adversarial perturbations in data, compared to the strategic classiﬁcation-based logistic regression developed recently in the literature [DRS+18].
C.2.4 Eﬀect of n, d on sample complexity
In this part, we demonstrate the empirical results that corroborates the theoretical dependence of sample complexity on n, d. For this purpose, we use synthetic dataset which is generated as per the method described in Section 6.1. Here we work in the setting where n ∈ {500, 1000, 1500, 2000} and d ∈ {10, 15, 20, 25}. We ﬁx the suboptimality to = 0.1 and compute the number of samples required in each of the settings of n and d so that the iterates reach the −suboptimality. We present the results in Figure 6.
C.3 Logistic regression as a Generalized linear model
The goal in logistic regression is to maximize the log-likelihood of the conditional probability of y (the label ) given x (the feature). In this model, it is assumed that:
1 P (Y = 1|x, θ) =
1 + exp(− x, θ )

36

Suboptimality Accuracy

GMSC Dataset with 2000 datapoints

Z-OGDA w RR

Z-OGDA w/o RR

100

Z-SGDA w/o RR Z-SGDA w RR

10 1

GMSC Dataset with 2000 datapoints

1.0

LogReg SC

Z-OGDA w RR

0.8

0.6

0.4

0.2

0 100 200 300 400 500
Epochs

0.0 0.5 1.0 1.5 2.0 2.5 3.0
Perturbation

Figure 4: Experimental results for a balanced GiveMeSomeCredit dataset with n = 2000. (Left pane) Suboptimality iterates generated by the four algorithms (A-I), (A-II), (A-III), (A-IV), respectively denoted as Z-OGDA w RR, Z-OGDA w/o RR, Z-SGDA w RR, Z-SGDA w/o RR. (Right pane) Comparison between decay in accuracy of strategic classiﬁcation with logistic regression (originally trained with ζ = 0.05) and Alg. (A-I) with changes in perturbation.

Suboptimality Accuracy

PortoBank Dataset with 2000 datapoints

100

Z-OGDA w RR

Z-OGDA w/o RR

Z-SGDA w/o RR

10 1 Z-SGDA w RR

10 2

0 100 200 300 400 500
Epochs

PortoBank Dataset with 2000 datapoints

0.80

LogReg SC

Z-OGDA w RR

0.75

0.70

0.65

0.60

0.55

0.50

0.0 0.5 1.0 1.5 2.0 2.5 3.0
Perturbation

Figure 5: Experimental results for a balanced PortoBank dataset with n = 2000. (Left pane) Suboptimality iterates generated by the four algorithms (A-I), (A-II), (A-III), (A-IV), respectively denoted as Z-OGDA w RR, Z-OGDA w/o RR, Z-SGDA w RR, Z-SGDA w/o RR. (Right pane) Comparison between decay in accuracy of strategic classiﬁcation with logistic regression (originally trained with ζ = 0.05) and Alg. (A-I) with change in perturbation.

37

Figure 6: Experimental results presenting the number of samples required to reach −suboptimality, with = 0.1, for our algorithm (A-I) on synthetic dataset with varying values of n ∈ {500, 1000, 1500, 2000} and d ∈ {10, 15, 20, 25}.

This implies that:

exp(− x, θ ) P (Y = −1|x, θ) =
1 + exp(− x, θ )

Given a data point (x, y) the logistic loss is log-likelihood of observing y given x. For any θ and y ∈ {−1, 1}:

1+y

1−y

P (Y = y|x; θ) = (P (Y = 1|x, θ)) 2 (P (Y = −1|x, θ)) 2

Now, the log-likelihood is given by:

L(x, y; θ) = log(P (Y = y|x; θ))

1+y

1

1−y

exp(− x, θ )

=

log

+

log

2

1 + exp(− x, θ )

2

1 + exp(− x, θ )

1−y

1+y 1−y

1

=−

x, θ +

+

log

2

2

2

1 + exp(− x, θ )

1−y

=−

x, θ − log(1 + exp(− x, θ ))

2

y

1

= x, θ − x, θ + x, θ − log(1 + exp( x, θ ))

2

2

y

1

= x, θ + x, θ − log(1 + exp( x, θ ))

2

2

The goal is to maximize the log likelihood, which is equivalent to minimizing the negative log likelihood. Thus the logistic regression minimizes the following loss:
L˜(x, y; θ) = −L(x, y; θ) = − y x, θ + φ( x, θ ) 2

where φ(β) = log(1 + exp(β)) − β2 . If y = 1, then the above loss becomes:

log(1 + exp( x, θ )) − x, θ = log(1 + exp(− x, θ ))

Otherwise, if y = −1, then the above loss becomes log(1 + exp( x, θ )). Thus, the above loss is equivalent to log(1 + exp(−y x, θ )).

38

