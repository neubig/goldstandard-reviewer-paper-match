Deep Neural Networks for Estimation and Inference∗

Max H. Farrell

Tengyuan Liang

Sanjog Misra

University of Chicago, Booth School of Business September 19, 2019

arXiv:1809.09953v3 [econ.EM] 18 Sep 2019

Abstract We study deep neural networks and their use in semiparametric inference. We establish novel rates of convergence for deep feedforward neural nets. Our new rates are suﬃciently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after ﬁrststep estimation with deep learning, a result also new to the literature. Our estimation rates and semiparametric inference results handle the current standard architecture: fully connected feedforward neural networks (multi-layer perceptrons), with the now-common rectiﬁed linear unit activation function and a depth explicitly diverging with the sample size. We discuss other architectures as well, including ﬁxed-width, very deep networks. We establish nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, such as treatment eﬀects, expected welfare, and decomposition eﬀects. Inference in many other semiparametric contexts can be readily obtained. We demonstrate the eﬀectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing.
Keywords: Deep Learning, Neural Networks, Rectiﬁed Linear Unit, Nonasymptotic Bounds, Convergence Rates, Semiparametric Inference, Treatment Eﬀects, Program Evaluation, Treatment Targeting.
1 Introduction
Statistical machine learning methods are being rapidly integrated into the social and medical sci-
ences. Economics is no exception, and there has been a recent surge of research that applies and
explores machine learning methods in the context of econometric modeling, particularly in “big
data” settings. Furthermore, theoretical properties of these methods are the subject of intense re-
cent study. This has netted several breakthroughs both theoretically, such as robust, valid inference
following machine learning, and in novel applications and conclusions. Our goal in the present work
∗We thank Milica Popovic for outstanding research assistance. Liang gratefully acknowledges support from the George C. Tiao Fellowship. Misra gratefully acknowledges support from the Neubauer Family Foundation. We are thank Guido Imbens, the handling co-editor, and two anonymous reviewers, as well as Alex Belloni, Xiaohong Chen, Denis Chetverikov, Chris Hansen, Whitney Newey, and Andres Santos, for thoughtful comments, suggestions, and discussions that substantially improved the paper.

is to study a particular statistical machine learning technique which is widely popular in industrial applications, but less frequently used in academic work and largely ignored in recent theoretical developments on inference: deep neural networks. To our knowledge we provide the ﬁrst inference results using deep learning methods.
Neural networks are estimation methods that model the relationship between inputs and outputs using layers of connected computational units (neurons), patterned after the biological neural networks of brains. These computational units sit between the inputs and output and allow datadriven learning of the appropriate model, in addition to learning the parameters of that model. Put into terms more familiar in nonparametric econometrics: neural networks can be thought of as a (complex) type of sieve estimation where the basis functions are ﬂexibly learned from the data. Neural networks are perhaps not as familiar to economists as other methods, and indeed, were out of favor in the machine learning community for several years, returning to prominence only very recently in the form of deep learning. Deep neural nets contain many hidden layers of neurons between the input and output layers, and have been found to exhibit superior performance across a variety of contexts. Our work aims to bring wider attention to these methods and to take the ﬁrst step toward ﬁlling the gaps in the theoretical understanding of inference using deep neural networks. Our results can be used in many economic contexts, including selection models, games, consumer surplus, and dynamic discrete choice.
Before the recent surge in attention, neural networks had taken a back seat to other methods (such as kernel methods or forests) largely because of their modest empirical performance and challenging optimization. However, the availability of scalable computing and stochastic optimization techniques (LeCun et al., 1998; Kingma and Ba, 2014) and the change from smooth sigmoid-type activation functions to rectiﬁed linear units (ReLU), x → max(x, 0) (Nair and Hinton, 2010), have seemingly overcome optimization hurdles and now this form of deep learning matches or sets the state of the art in many prediction contexts (Krizhevsky et al., 2012; He et al., 2016). Our theoretical results speak directly to this modern implementation of deep learning: we explicitly model the depth of the network as diverging with the sample size and focus on the ReLU activation function.
Further back in history, before falling out of favor, neural networks were widely studied and applied, particularly in the 1990s. In that time, shallow neural networks with smooth activation functions were shown to have many good theoretical properties. Intuitively, neural networks are
1

a form of sieve estimation, wherein basis functions of the original variables are used to approximate unknown nonparametric objects. What sets neural nets apart is that the basis functions are themselves learned from the data by optimizing over many ﬂexible combinations of simple functions. It has been known for some time that such networks yield universal approximations (Hornik et al., 1989). Comprehensive theoretical treatments are given by White (1992) and Anthony and Bartlett (1999). Of particular relevance in this strand of theoretical work is Chen and White (1999), where it was shown that single-layer, sigmoid-based networks could attain suﬃciently fast rates for semiparametric inference (see Chen (2007) for more references).
We explicitly depart from the extant literature by focusing on the modern setting of deep neural networks with the rectiﬁed linear (ReLU) activation function. We provide nonasymptotic bounds for nonparametric estimation using deep neural networks, immediately implying convergence rates. The bounds and convergence rates appear to be new to the literature and are one of the main theoretical contributions of the paper. We provide results for a general class of smooth loss functions for nonparametric regression style problems, covering as special cases generalized linear models and other empirically useful contexts. In our application to causal inference we specialize our results to linear and logistic regression as concrete illustrations. Our proof strategy employs a localization analysis that uses scale-insensitive measures of complexity, allowing us to consider richer classes of neural networks. This is in contrast to analyses which restrict the networks to have bounded parameters for each unit (discussed more below) and to the application of scale sensitive measures such as metric entropy (used by Chen and White, 1999, for example). These approaches would not deliver our sharp bounds and fast rates. Recent developments in approximation theory and complexity for deep ReLU networks are important building blocks for our results.
Our second main result establishes valid inference on ﬁnite-dimensional parameters following ﬁrst-step estimation using deep learning. We focus on causal inference for concreteness and wide applicability, as well as to allow direct comparison to the literature. Program evaluation with observational data is one of the most common and important inference problems, and has often been used as a test case for theoretical study of inference following machine learning (e.g., Belloni et al., 2014; Farrell, 2015; Belloni et al., 2017; Athey et al., 2018). Causal inference as a whole is a vast literature; see Imbens and Rubin (2015) for a broad review and Abadie and Cattaneo (2018) for a recent review of program evaluation methods, and further references in both. Deep neural
2

networks have been argued (experimentally) to outperform the previous state-of-the-art in causal inference (Westreich et al., 2010; Johansson et al., 2016; Shalit et al., 2017; Hartford et al., 2017). To the best of our knowledge, ours are among the ﬁrst theoretical results that explicitly deliver inference using deep neural networks.
We give speciﬁc results for average treatment eﬀects, counterfactual expected utility/proﬁts from treatment targeting strategies, and decomposition eﬀects. Our results allow planners (e.g., ﬁrms or medical providers) to compare diﬀerent strategies, either predetermined or estimated using auxiliary data, and recognizing that targeting can be costly, decide which strategy to implement. An interesting, and potentially useful, point we make in this context is that the selection on observables framework yields identiﬁcation of counterfactual average outcomes without additional structural assumptions, so that, e.g., expected proﬁt from a counterfactual treatment rule can be evaluated.
The usefulness of our deep learning results is of course not limited to causal inference. In particular, our results yield inference on essentially any estimand that admits a locally robust estimator (Chernozhukov et al., 2018c) that depends only on target functions within our class of loss function (under appropriate regularity conditions). Our aim is not to innovate at the semiparametric step, for example by seeking weaker conditions on the ﬁrst stage, but rather, we aim to utilize such results. Prior work has veriﬁed the high-level conditions for other ﬁrst-stage estimators, such as traditional kernels or series/sieves, lasso methods, sigmoid-based shallow neural networks, and others (under suitable assumptions for each method). Our work contributes directly to this area of research by showing that deep nets are a valid and useful ﬁrst-step estimator, in particular, attaining a rate of o(n−1/4) under appropriate smoothness conditions. Finally, we do not rely on sample splitting or cross ﬁtting. In particular, we use localization explicitly to directly verify conditions required for valid inference, which may be a novel application of this proof method that is useful in future semiparametric inference problems.
We numerically illustrate our results, and more generally the utility of deep learning, with a detailed simulation study and an empirical study of a direct mail marketing campaign. Our data come from a large US consumer products retailer and consists around to three hundred thousand consumers with one hundred ﬁfty covariates. Hitsch and Misra (2018) recently used this data to study various estimators, both traditional and modern, of heterogeneous treatment eﬀects. We refer the reader to that paper for a more complete description of the data as well as results using other
3

estimators (see also Hansen et al. (2017)). We study the eﬀect of catalog mailings on consumer purchases, and moreover, compare diﬀerent targeting strategies (i.e. to which consumers catalogs should be mailed). The cost of sending out a single catalog can be close to one dollar, and with millions being set out, carefully assessing the targeting strategy is crucial. Our results suggest that deep nets are at least as good as (and sometimes better than) the best methods in Hitsch and Misra (2018).
The remainder of the paper proceeds as follows. Next, we brieﬂy review the related theoretical literature. Section 2 introduces deep ReLU networks and states our main theoretical results: nonasymptotic bounds and convergence rates for general nonparametric regression-type loss functions. The semiparametric inference problem is set up in Section 3 and asymptotic results are presented in Section 4. The empirical application is presented in Section 5. Results of a simulation study are reported in Section 6. Section 7 concludes. All proofs are given in the appendix.
1.1 Related Theoretical Literature
Our paper contributes to several rapidly growing literatures, and we can not hope to do justice to each here. We give only those citations of particular relevance; more references can be found within these works. First, there has been much recent study of the statistical properties of the machine learning tools as an end in itself. Many studies have focused on the lasso and its variants (Bickel et al., 2009; Belloni et al., 2011, 2012; Farrell, 2015) and tree/forest based methods (Wager and Athey, 2018), though earlier work studied shallow (typically with a single hidden layer) neural networks with smooth activation functions (White, 1989, 1992; Chen and White, 1999). We ﬁll the gap in this literature by studying deep neural networks with the non-smooth ReLU activation.
A second, intertwined strand of literature focuses on inference following the use of machine learning methods, often with a focus on average causal eﬀects. Initial theoretical results were concerned with obtaining valid inference on a coeﬃcient in a high-dimensional regression, following model selection or regularization, with particular focus on the lasso (Belloni et al., 2012; Javanmard and Montanari, 2014; van de Geer et al., 2014). Intuitively, this is a semiparametric problem, where the coeﬃcient of interest is estimable at the parametric rate and the remaining coeﬃcients are collectively a nonparametric nuisance parameter estimated using machine learning methods. Building on this intuition, many have studied the semiparametric stage directly, such as obtaining
4

novel, weaker conditions easing the application of machine learning methods (Belloni et al., 2014; Farrell, 2015; Chernozhukov et al., 2018c; Belloni et al., 2018, and references therein). Conceptually related to this strand are targeted maximum likelihood (van der Laan and Rose, 2001) and the higher-order inﬂuence functions (Robins et al., 2008, 2017). Our work builds on this work, employing conditions therein, and in particular, verifying them for deep ReLU nets.
Finally, our convergence rates build on, and contribute to, the recent theoretical machine learning literature on deep neural networks. Because of the renaissance in deep learning, a considerable amount of study has been done in recent years. Of particular relevance to us are Yarotsky (2017, 2018) and Bartlett et al. (2017); a recent textbook treatment, containing numerous other references, is given by Goodfellow et al. (2016).
2 Deep Neural Networks
In this section we will give our main theoretical results: nonasymptotic bounds and associated convergence rates for deep neural network estimation. The utility of these results for secondstep semiparametric causal inference (the downstream task), for which our rates are suﬃciently rapid, is demonstrated in Section 4. We view our results as an initial step in establishing both the estimation and inference theory for modern deep learning, i.e. neural networks built using the multi-layer perceptron architecture (described below) and the nonsmooth ReLU activation function. This combination is crucial: it has demonstrated state of the art performance empirically and can be feasibly optimized. This is in contrast with sigmoid-based networks, either shallow (for which theory exists, but may not match empirical performance) or deep (which are not feasible to optimize), and with shallow ReLU networks, which are not known to approximate broad classes functions.
As neural networks are perhaps less familiar to economists and other social scientists, we ﬁrst brieﬂy review the construction of deep ReLU nets. Our main focus will be on the fully connected feedfoward neural network, frequently referred to as a multi-layer perceptron, as this is the most commonly implemented network architecture and we want our results to inform empirical practice. However, our results are more general, accommodating other architectures provided they are able to yield a universal approximation (in the appropriate function class), and so we review neural nets
5

more generally and give concrete examples. Our goal is to estimate an unknown, assumed-smooth function f∗(x), that relates the covariates
X ∈ Rd to an outcome Y as the minimizer of the expectation of the per-observation loss function. Collecting these random variables into the vector Z = (Y, X ) ∈ Rd+1, with z = (y, x ) denoting a realization, we write
f∗ = arg min E [ (f, Z)] .
We allow for any loss function that is Lipschitz in f and obeys a curvature condition around f∗. Speciﬁcally, for constants c1, c2, and C that are bounded and bounded away from zero, we assume that (f, z) obeys

| (f, z) − (g, z)| ≤ C |f (x) − g(x)|, c1E (f − f∗)2 ≤ E[ (f, Z)] − E[ (f∗, Z)] ≤ c2E (f − f∗)2 .

(2.1)

Our results will be stated for a general loss obeying these two conditions.1 We give a uniﬁed localization analysis of all such problems. This family of loss function covers many interesting problems. Two leading examples, used in our application to causal inference, are least squares and logistic regression, corresponding to the outcome and propensity score models respectively. For least squares, the target function and loss are

f∗(x) := E[Y |X = x] and

(f, z)

=

1 (y

−

f (x))2,

2

(2.2)

respectively, while for logistic regression these are

f∗(x) := log E[Y |X = x] and 1 − E[Y |X = x]

(f, z) = −yf (x) + log 1 + ef(x) .

(2.3)

Lemma 8 veriﬁes, with explicit constants, that (2.1) holds for these two. Losses obeying (2.1) extend beyond these cases to other generalized linear models, such as count models, and can even cover multinomial logistic regression (multiclass classiﬁcation), as shown in Lemma 9.
1We thank an anonymous referee for suggesting this approach.

6

Figure 1: Illustration of a feedforward neural network with W = 18, L = 2, U = 5, and input dimension d = 2. The input units are shown in blue at left, the output in red at right, and the hidden units in grey between them.
2.1 Neural Network Constructions
For any loss, we estimate the target function using a deep ReLU network. We will give a brief outline of their construction here, paying closer attention to the details germane to our theory; complete introductions, and further references, are given by Anthony and Bartlett (1999) and Goodfellow et al. (2016).
The crucial choice is the speciﬁc network architecture, or class. In general we will call this FDNN. From a theoretical point of view, diﬀerent classes have diﬀerent complexity and diﬀerent approximating power. We give results for several concrete examples below. We will focus on feedforward neural networks. An example of a feedforward network is shown in Figure 1. The network consists of d input units, corresponding to the covariates X ∈ Rd, one output unit for the outcome Y . Between these are U hidden units, or computational nodes or neurons. These are connected by a directed acyclic graph specifying the architecture. The key graphical feature of a feedforward network is that hidden units are grouped in a sequence of L layers, the depth of the network, where a node is in layer l = 1, 2, . . . , L, if it has a predecessor in layer l − 1 and no predecessor in any layer l ≥ l. The width of the network at a given layer, denoted Hl, is the number of units in that layer. The network is completed with the choice of an activation function σ : R → R applied to the output of each node as described below. In this paper, we focus on the popular ReLU activation function σ(x) = max(x, 0), though our results can be extended (at notational cost) to cover piecewise linear activation functions (see also Remark 3).
An important and widely used subclass is the one that is fully connected between consecutive layers but has no other connections and each layer has number of hidden units that are of the same order of magnitude. This architecture is often referred to as a Multi-Layer Perceptron (MLP) and
7

Figure 2: Illustration of multi-layer perceptron FMLP with H = 3, L = 2 (U = 6, W = 25), and input dimension d = 2.
we denote this class as FMLP. See Figure 2, cf. Figure 1. We will assume that all the width of all layers share a common asymptotic order H, implying that for this class U LH.
We will allow for generic feedforward networks in our results, but we present special results for the MLP case, as it is widely used in empirical practice. As we will see below, the architecture, through its complexity, and more importantly, approximation power, plays a crucial role in the ﬁnal convergence rate. In particular, we ﬁnd only a suboptimal rate for the MLP case, but our upper bound is still suﬃcient for semiparametric inference. As a note on exposition, while our main results are in fact nonasymptotic bounds that hold with high probability, for simplicity we will refer to them as “rates” in most discussion.
To build intuition on the computation, and compare to other nonparametric methods, let us focus on least squares for the moment, i.e. Equation (2.2), with a continuous outcome using a multilayer perceptron with constant width H. Each hidden unit u receives an input in the form of a linear combination x˜ w +b, and then returns σ(x˜ w +b), where the vector x˜ collects the output of all the units with a directed edge into u (i.e., from prior layers), w is a vector of weights, and b is a constant term. (The constant term is often referred to as the “bias” in the deep learning literature, but given the loaded meaning of this term in inference, we will largely avoid referring to b as a bias.) The ﬁnal layer’s output is simply x˜ w + b in the least squares case. The collection, over all nodes, of w and b, constitutes the parameters θ which are optimized in the ﬁnal estimation. We denote W as the total number of parameters of the network. For the MLP, W = (d+1)H+(L−1)(H2+H)+H+1. In general, W , U , L, and H, may change with n, but we suppress this in the notation.
Optimization proceeds layer-by-layer using (variants of) stochastic gradient descent, with gradients of the parameters calculated by back-propagation (implementing the chain rule) induced by the network structure. To see this, let x˜h,l denote the scalar output of a node u = (h, l), for
8

h = 1, . . . H, l = 1, . . . L, and let x˜l = (x˜1,l, . . . , x˜H,l) for layer l ≤ L. Each node thus computes x˜h,l = σ(x˜l−1wh,l−1 + bh,l−1) and the ﬁnal output is yˆ = x˜LwL + bL. Once we recall that the network begins with the original observation x, we can view x˜L = x˜L(x), and thus the ﬁnal output may be seen as a basis function approximation (albeit a complex and random one) written as fˆMLP(x) = x˜L(x) wL + bL, which is reminiscent of a traditional series (linear sieve) estimator. If all layers save the last were ﬁxed, we could simply optimize using least squares directly: (wL, bL) = arg minw,b yi − x˜Lw − b 2n.
The crucial distinction is that the basis functions x˜L(·) are learned from the data. The “basis” is x˜L = (x˜1,L, . . . , x˜H,L) , where each x˜h,L = σ(x˜L−1wh,L−1 + bh,L−1). Therefore, “before” we can solve the least squares problem above, we would have to estimate (wh,L−1, bh,L−1), h = 1, . . . , H, anticipating the ﬁnal estimation. These in turn depend on the prior layer, and so forth back to the original inputs X. Measuring the gradient of the loss with respect to each layer of parameters uses the chain rule recursively, and is implemented by back-propagation. This is simply a sketch of course; for further introduction, see Hastie et al. (2009) and Goodfellow et al. (2016).
To further clarify the use of deep nets, it is useful to make explicit analogies to more classical nonparametric techniques, leveraging the form fˆMLP(x) = x˜L(x) wL + bL. For a traditional series estimator, say smoothing splines, the two choices for the practitioner are the spline basis (the shape and the degree) and the number of terms (knots), commonly referred to as the smoothing and tuning parameters, respectively. In kernel regression, these would respectively be the shape of the kernel (and degree of local polynomial) and the bandwidth(s). For neural networks, the same phenomena are present: the architecture as a whole (the graph structure and activation function) are the smoothing parameters while the width and depth play the role of tuning parameters for a set architecture.
The architecture plays a crucial role in that it determines the approximation power of the network, and it is worth noting that because of the relative complexity of neural networks, such approximations, and comparisons across architectures, are not simple. It is comparatively obvious that quartic splines are more ﬂexible than cubic splines (for the same number of knots) as is a higher degree local polynomial (for the same bandwidth). At a glance, it may not be clear what function class a given network architecture (width, depth, graph structure, and activation function) can approximate. As we will show below, the MLP architecture is not yet known to yield an optimal
9

approximation (for a given width and depth) and therefore we are only able to prove a bound with slower than optimal rate. As a ﬁnal note, computational considerations are important for deep nets in a way that is not true conventionally; see Remarks 1, 2, and 3.
Just as for classical nonparametrics, for a ﬁxed architecture, it is the tuning parameter choices that determine the rate of convergence (for a ﬁxed smoothness of the underlying function). The recent wave of theoretical study of deep learning is still in its infancy. As such, there is no understanding yet of optimal architecture(s) or tuning parameters. Choices of both are quite diﬃcult, and only preliminary research has been done (see e.g., Daniely, 2017; Telgarsky, 2016; Safran and Shamir, 2016; Mhaskar and Poggio, 2016a; Raghu et al., 2017, and references therein). Further exploration of these ideas is beyond the current scope. It is interesting to note that in some cases, a good approximation can be obtained even with a ﬁxed width H, provided the network is deep enough, a very particular way of enriching the “sieve space” FDNN; see Corollary 2.
In sum, for a user-chosen architecture FDNN, encompassing the choices σ(·), U , L, W , and the graph structure, the ﬁnal estimate is computed using observed samples zi = (yi, xi) , i = 1, 2, . . . , n, of Z, by solving

fDNN :=

n
arg min
fθ∈FDNN i=1 fθ ∞≤2M

(f, zi) .

(2.4)

Recall that θ collects, over all nodes, the weights and constants w and b. When (2.4) is restricted to the MLP class we denote the resulting estimator fMLP. The choice of M may be arbitrarily large, and is part of the deﬁnition of the class FDNN. This is neither a tuning parameter nor regularization in the usual sense: it is not assumed to vary with n, and beyond being ﬁnite and bounding f∗ ∞ (see Assumption 1), no properties of M are required. This is simply a formalization of the requirement that the optimizer is not allowed to diverge on the function level in the l∞ sense– the weakest form of constraint. It is important to note that while typically regularization will alter the approximation power of the class, that is not the case with the choice of M as we will assume that the true function f∗(x) is bounded, as is standard in nonparametric analysis. With some extra notational burden, one can make the dependence of the bound on M explicit, though we omit this for clarity as it is not related to statistical issues.

10

Remark 1. In applications it is common to apply some form of regularization to the optimization of (2.4). However, in theory, the role of explicit regularization is unclear and may be unnecessary, as stochastic gradient descent presents good, if not better, solutions empirically (see Section 6 and Zhang et al., 2016). Regularization may improve empirical performance in low signal-to-noise ratio problems. A detailed investigation is beyond the scope of the current work, though we do investigate this numerically in Sections 5 and 6. There are many alternative regularization methods, including L1 and L2 (weight decay) penalties, drop out, and others.
2.2 Bounds and Convergence Rates for Multi-Layer Perceptrons
We can now state our main theoretical results: bounds and convergence rates for deep ReLU networks. All proofs appear in the Appendix. We study neural networks from a nonparametric point of view (e.g., White, 1989, 1992; Schmidt-Hieber, 2017; Liang, 2018; Bauer and Kohler, 2017, in speciﬁc scenarios). Chen and Shen (1998) and Chen and White (1999) share our goal, fast convergence rates for use in semiparametric inference, but focus on shallow, sigmoid-based networks compared to our deep, ReLU-based networks, though they consider dependent data which we do not. Our theoretical approach is quite diﬀerent. In particular, Chen and White (1999) obtain suﬃciently fast rates by following the approach of Barron (1993) in using Maurey’s method (Pisier, 1981) for approximation, but applying the reﬁnement of Makovoz (1996). Our analysis of deep nets instead employs localization methods (Koltchinskii and Panchenko, 2000; Bartlett et al., 2005; Koltchinskii, 2006, 2011; Liang et al., 2015), along with the recent approximation work of Yarotsky (2017, 2018) and complexity results of Bartlett et al. (2017).
The regularity conditions we require are collected in the following.
Assumption 1. Assume that zi = (yi, xi) , 1 ≤ i ≤ n are i.i.d. copies of Z = (Y, X) ∈ Y ×[−1, 1]d, where X is continuously distributed. For an absolute constant M > 0, assume f∗ ∞ ≤ M and Y ⊂ [−M, M ].
This assumption is fairly standard in nonparametrics. The only restriction worth mentioning is that the outcome is bounded. In many cases this holds by default (such as logistic regression, where Y = {0, 1}) or count models (where Y = {0, 1, . . . , M }, with M limited by real-world constraints). For continuous outcomes, such as least squares regression, our restriction is not
11

substantially more limiting than the usual assumption of a model such as Y = f∗(X) + ε, where X is compact-supported, f∗ is bounded, and the stochastic error ε possesses many moments. Indeed, in many applications such a structure is only coherent with bounded outcomes, such as the common practice of including lagged outcomes as predictors. Next, the assumption of continuously distributed covariates is quite standard. From a theoretical point of view, covariates taking on only a few values can be conditioned on and then averaged over, and these will, as usual, not enter into the dimensionality which curses the rates. Discrete covariates taking on many values may be more realistically thought of as continuous, and it may be more accurate to allow these to slow the convergence rates. Our focus on L2(X) convergence allows for these essentially automatically. Finally, from a practical point of view, deep networks handle discrete covariates seamlessly and have demonstrated excellent empirical performance, which is in contrast to other more classical nonparametric techniques that may require manual adaptation.
Proceeding now to our results, we begin with the most important network architecture, the multi-layer perceptron. This is the most widely used network architecture in practice and an important contribution of our work is to cover this directly, along with ReLU activation. MLPs are now known to approximate smooth functions well, leading to our next assumption: that the target function f∗ lies in a Sobolev ball with certain smoothness. Discussion of Sobolev spaces, and comparisons to H¨older and Besov spaces, can be found in Gine and Nickl (2016).
Assumption 2. Assume f∗ lies in the Sobolev ball Wβ,∞([−1, 1]d), with smoothness β ∈ N+,

f∗(x) ∈ Wβ,∞([−1, 1]d) := f : max ess sup |Dαf (x)| ≤ 1 ,
α,|α|≤β x∈[−1,1]d
where α = (α1, . . . , αd), |α| = α1 + . . . + αd and Dαf is the weak derivative.

Under Assumptions 1 and 2 we obtain the following result, which, to the best of our knowledge, is new to the literature. In some sense, this is our main result for deep learning, as it deals with the most common architecture. We apply this in Sections 4 and 5 for semiparametric inference.

Theorem 1 (Multi-Layer Perceptron). Suppose Assumptions 1 and 2 hold. Let fMLP be the deep

MLP-ReLU network estimator deﬁned by (2.4), restricted to FMLP, for a loss function obeying

(2.1), with width H

d
n 2(β+d)

log2

n

and

depth

L

log n. Then with probability at least 1 −

12

d
exp(−n β+d

log8

n),

for

n

large

enough,

(a) fMLP − f∗ 2L2(x) ≤ C · n− β+β d log8 n + log nlog n and (b) En (fMLP − f∗)2 ≤ C · n− β+β d log8 n + log log n ,
n

for a constant C > 0 independent of n, which may depend on d, M , and other ﬁxed constants.

Several aspects of this result warrant discussion. We build on the recent results of Bartlett et al. (2017), who ﬁnd nearly-tight bounds on the Vapnik-Chervonenkis (VC) and Pseudo-dimension of deep nets. One contribution of our proof is to use a scale sensitive localization theory with scale insensitive measures, such as VC- or Pseudo-dimension, for deep neural networks for general smooth loss functions. For the special case of least squares regression, Koltchinskii (2011) uses a similar approach, and a similar result to our Theorem 1(a) can be derived for this case using his Theorem 5.2 and Example 3 (p. 85f).
This approach has two tangible beneﬁts. First, we do not restrict the class of network architectures to have bounded weights for each unit (scale insensitive), in accordance to standard practice (Zhang et al., 2016) and in contrast to the classic sieve analysis with scale sensitive measure such as metric entropy. Moreover, this allows for a richer set of approximating possibilities, in particular allowing more ﬂexibility in seeking architectures with speciﬁc properties, as we explore in the next subsection. Second, from a technical point of view, we are able to attain a faster rate on the second term of the bound, order n−1 in the sample size, instead of the n−1/2 that would result from a direct application of uniform deviation bounds. This upper bound informs the trade oﬀs between width and depth, and the approximation power, and may point toward optimal architectures for statistical inference.
This result gives a nonasymptotic bound that holds with high probability. As mentioned above, we will generally refer to our results simply as “rates” when this causes no confusion. This result relies on choosing H appropriately given the smoothness β of Assumption 2. Of course, the true smoothness is unknown and thus in practice the “β” appearing in H, and consequently in the convergence rates, need not match that of Assumption 2. In general, the rate will depend on the smaller of the two. Most commonly it is assumed that the user-chosen β is ﬁxed and that the truth is smoother; witness the ubiquity of cubic splines and local linear regression. Rather than spell

13

out these consequences directly, we will tacitly assume the true smoothness is not less than the β appearing in H (here and below). Adaptive approaches, as in classical nonparametrics, may also be possible with deep nets, but are beyond the scope of this study.
Even with these choices of H and L, the bound of Theorem 1 is not optimal (for ﬁxed β, in the sense of Stone (1982)). We rely on the explicit approximating constructions of Yarotsky (2017), and it is possible that in the future improved approximation properties of MLPs will be found, allowing for a sharpening of the results of Theorem 1 immediately, i.e. without change to our theoretical argument. At present, it is not clear if this rate can be improved, but it is suﬃciently fast for valid inference.
2.3 Other Network Architectures
Theorem 1 covers only one speciﬁc architecture, albeit the most important one at present. However, given that this ﬁeld is rapidly evolving, it is important to consider other possible architectures which may be beneﬁcial in some cases. To this end, we will state a more generic result and then two speciﬁc examples: one to obtain a faster rate of convergence and one for ﬁxed-width networks. All of these results are, at present, more of theoretical interest than practical value, as they are either agnostic about the network (thus infeasible) or rely on more limiting assumptions.
In order to be agnostic about the speciﬁc architecture of the network we need to be ﬂexible in the approximation power of the class. To this end, we will replace Assumption 2 with the following generic assumption, rather more of a deﬁnition, regarding the approximation power of the network.
Assumption 3. Let f∗ lie in a class F . For the feedforward network class FDNN, used in (2.4), let the approximation error DNN be
DNN := sup inf f − f∗ ∞ .
f∗∈F f ∈FDNN f ∞≤2M
It may be possible to require only an approximation in the L2(X) norm, but this assumption matches the current approximation theory literature and is more comparable with other work in nonparametrics, and thus we maintain the uniform deﬁnition.
Under this condition we obtain the following generic result.
14

Theorem 2 (General Feedforward Architecture). Suppose Assumptions 1 and 3 hold. Let fDNN be the deep ReLU network estimator deﬁned by (2.4), for a loss function obeying (2.1). Then with probability at least 1 − e−γ, for n large enough,

(a)

fDNN − f∗

2 L

(x) ≤ C

W L log W log n + log log n + γ + 2DNN

and

2

n

n

(b) En (fDNN − f∗)2 ≤ C W L log W log n + log log n + γ + 2DNN ,

n

n

for a constant C > 0 independent of n, which may depend on d, M , and other ﬁxed constants.

This is a more general than Theorem 1, covering the general deep ReLU network problem deﬁned in (2.4), general feedforward architectures, and the general class of losses deﬁned by (2.1). The same comments as were made following Theorem 1 apply here as well: the same localization argument is used with the same beneﬁts. We explicitly use this in the next two corollaries, where we exploit the allowed ﬂexibility in controlling DNN by stating results for particular architectures. The bound here is not directly applicable without specifying the network structure, which will determine both the variance portion (through W , L, and U ) and the approximation error. With these set, the bound becomes operational upon choosing γ, which can be optimized as desired, and this will immediately then yield a convergence rate.
Turning to special cases, we ﬁrst show that the optimal rate of Stone (1982) can be attained, up to log factors. However, this relies on a rather artiﬁcial network structure, designated to approximate functions in a Sobolev space well, but without concern for practical implementation. Thus, while the following rate improves upon Theorem 1, we view this result as mainly of theoretical interest: establishing that (certain) deep ReLU networks are able to attain the optimal rate.

Corollary 1 (Optimal Rate). Suppose Assumptions 1 and 2 hold. Let fOPT solve (2.4) using the
d
(deep and wide) network of Yarotsky (2017, Theorem 1), with W U n 2β+d log n and depth L log n, the following hold with probability at least 1 − e−γ, for n large enough,
(a) fOPT − f∗ 2L2(x) ≤ C · n− 2β2+β d log4 n + log lognn + γ and
(b) En (fOPT − f∗)2 ≤ C · n− 2β2+β d log4 n + log log n + γ , n
for a constant C > 0 independent of n, which may depend on d, M , and other ﬁxed constants.

15

Next, we turn to very deep networks that are very narrow, which have attracted substantial recent interest. Theorem 1 and Corollary 1 dealt with networks where the depth and the width grow with sample size. This matches the most common empirical practice, and is what we use in Sections 5 and 6. However, it is possible to allow for networks of ﬁxed width, provided the depth is suﬃciently large. The next result is perhaps the largest departure from the classical study of neural networks: earlier work considered networks with diverging width but ﬁxed depth (often a single layer), while the reverse is true here. The activation function is of course qualitatively diﬀerent as well, being piecewise linear instead of smooth. Using recent results (Mhaskar and Poggio, 2016b; Hanin, 2017; Yarotsky, 2018) we can establish the following rate for very deep, ﬁxed-width MLPs.
Corollary 2 (Fixed Width Networks). Let the conditions of Theorem 1 hold, with β ≥ 1 in
d
Assumption 2. Let fFW solve (2.4) for an MLP with ﬁxed width H = 2d+10 and depth L n 2(2+d) . Then with probability at least 1 − e−γ, for n large enough,
(a) fFW − f∗ 2L2(x) ≤ C · n− 2+2d log2 n + log lognn + γ and (b) En (fFW − f∗)2 ≤ C · n− 2+2d log2 n + log log n + γ ,
n for a constant C > 0 independent of n, which may depend on d, M , and other ﬁxed constants.
This result is again mainly of theoretical interest. The class is only able to approximate well functions with β = 1 (cf. the choice of L) which limits the potential applications of the result because, in practice, d will be large enough to render this rate, unlike those above, too slow for use in later inference procedures. In particular, if d ≥ 3, the suﬃcient conditions of Theorem 3 fail.
Finally, as mentioned following Theorem 1, our theory here will immediately yield a faster rate upon discovery of improved approximation power of this class of networks. In other words, for example, if a proof became available that ﬁxed-width, very deep networks can approximate β-smooth functions (as in Assumption 2), then Corollary 2 will trivially be improvable to match the rate of Theorem 1. Similarly, if the MLP architecture can be shown to share the approximation power with that of Corollary 1, then Theorem 1 will itself deliver the optimal rate. Our proofs will not require adjustment.
Remark 2. Although there has been a great deal of work in easing implementation (optimization and tuning) of deep nets, it still may be a challenge in some settings, particularly when using
16

non-standard architectures. See also Remark 1. Given the renewed interest in deep networks, this is an area of study already (Hartford et al., 2017; Polson and Rockova, 2018) and we expect this to continue and that implementations will rapidly evolve. This is perhaps another reason that Theorem 1 is, at the present time, the most practically useful, but that (as just discussed) Theorem 2 will be increasingly useful in the future.
Remark 3. Our results can be extended easily to include piecewise linear activation functions beyond ReLU. Intuitively, being itself piecewise linear, appropriate combinations of a ﬁxed number of ReLU functions can equal a piecewise linear function (with a ﬁxed number of knots) and therefore the complexity and approximation power can be easily adjusted to this case. See Bartlett et al. (2017).
In principle, similar rates of convergence could be attained for other activation functions, given results on their approximation error. However, it is not clear what practical value would be oﬀered due to computational issues (in which the activation choice plays a crucial role). Indeed, the recent switch to ReLU stems not from their greater approximation power, but from the fact that optimizing a deep net with sigmoid-type activation is unstable or impossible in practice. Thus, while it is certainly possible that we could complement the single-layer results with rates for sigmoid-based deep networks, these results would have no consequences for real-world practice.
From a purely practical point of view, several variations of the ReLU activation function have been proposed recently (including the so-called Leaky ReLU, Randomized ReLU, (Scaled) Exponential Linear Units, and so forth) and have been found in some experiments to improve optimization properties. It is not clear what theoretical properties these activation functions have or if the computational beneﬁts persist more generically, though this area is rapidly evolving. We conjecture that our results could be extended to include these activation functions.
3 Parameters of Interest
We will use the results above, in particular Theorem 1, coupled with results in the semiparametric literature, to deliver valid asymptotic inference for causal eﬀects. The novelty of our results is not in this semiparametric stage per se, but rather in delivering valid inference after relying on deep
17

learning for the ﬁrst step estimation. In this section we deﬁne the parameters of interest, while asymptotic inference is discussed next.
We will focus, for concreteness, on causal parameters that are of interest across diﬀerent disciplines: average treatment eﬀects, expected utility (or proﬁts) under diﬀerent targeting policies, average eﬀects on (non-)treated subpopulations, and decomposition eﬀects. Our focus on causal inference with observational data is due to the popularity of these estimands both in applications and in theoretical work, thus allowing our results to be put to immediate use and easily compared to prior literature. The average treatment eﬀect in particular is often used as a benchmark parameter for studying inference following machine learning (see references in the Introduction). However, armed with our results for deep neural networks we can cover a great deal more (some discussion is in Section 3.4).
The estimation of average causal eﬀects is a well-studied problem, and we will give only a brief overview here. Recent reviews and further references are given by Belloni et al. (2017); Athey et al. (2017); Abadie and Cattaneo (2018). We consider the standard setup for program evaluation with observational data: we observe a sample of n units, each exposed to a binary treatment, and for each unit we observe a vector of pre-treatment covariates, X ∈ Rd, treatment status T ∈ {0, 1}, and a scalar post-treatment outcome Y . The observed outcome obeys Y = T Y (1) + (1 − T )Y (0), where Y (t) is the (potential) outcome under treatment status t ∈ {0, 1}. The “fundamental problem” is that only Y (0) or Y (1) is observed for each unit, never both.
The crucial identiﬁcation assumptions, which pertain to all the parameters we consider, are selection on observables, also known as ignorability, unconfoundedness, missingness at random, or conditional independence, and overlap, or common support. Let p(x) = P[T = 1|X = x] denote the propensity score and µt(x) = E[Y (t)|X = x], t ∈ {0, 1} denote the two outcome regression functions. We then assume the following throughout, beyond which, we will mostly need only regularity conditions for inference.
Assumption 4. For t ∈ {0, 1} and almost surely X, E[Y (t)|T, X = x] = E[Y (t)|X = x] and p¯ ≤ p(x) ≤ 1 − p¯ for some p¯ > 0.
It will be useful to divide our discussion between parameters that are fully marginal averages, such as the average treatment eﬀect, and those which are for speciﬁc subpopulations. Here, “sub-
18

populations” refer to the treated or nontreated groups, with corresponding parameters such as the treatment eﬀect for the treated. Any parameter, in either case, can be studied for a suitable subpopulation deﬁned by the covariates X, such as a speciﬁc demographic group. Though causal eﬀects as a whole share some structure, there are slight conceptual and notational diﬀerences. In particular, the form of the eﬃcient inﬂuence function and doubly robust estimator is diﬀerent for the two sets, but common within.

3.1 Full-Population Average Eﬀect Parameters

Here we are interested in averages over the entire population. The prototypical parameter of interest

is the average treatment eﬀect:

τ = E[Y (1) − Y (0)].

(3.1)

In the context of our empirical example, the treatment is being mailed a catalog and the outcome is dollars spent (results for the binary purchase decision are available on request). The average treatment eﬀect, also referred to as “lift” in digital contexts, corresponds to the expected gain in revenue from an average individual receiving the catalog compared to the same person not receiving the catalog.
A closely related parameter of interest is the average realized outcome, which in general may be interpreted as the expected utility or welfare from a counterfactual treatment policy. In the context of our empirical application this is expected proﬁts; in a medical context it would be the total health outcome. The question of interest here is whether a change in the treatment policy would be beneﬁcial in terms of increasing outcomes, and this is judged using observational data. Intuitively, the average treatment eﬀect is the expected gain from treating the “next” person, relative to if they had not been exposed. That is, it is the expected change in the outcome. Expected utility/proﬁt, on the other hand, is concerned with the total outcome, not the diﬀerence in outcomes. In the context of our empirical application, we are interested in total sales rather than the change in sales. Our discussion is grounded in this language for easy comparison.
The parameter depends on a counterfactual/hypothetical treatment targeting strategy, which is often itself the object of evaluation. This is simply a rule that assigns a given set of characteristics (e.g. a consumer proﬁle), determined by the covariates X, to treatment status: that

19

is, a known function (which may include randomization but is not estimated from the sample) s(x) : supp{X} → {0, 1}. Note well that this is not necessarily the observed treatment: s(xi) = ti. The policy maker may wish to evaluate the gain from targeting only a certain subset of customers, a price discrimination strategy, or comparisons of diﬀerent such policies. Our assumptions, while standard, deliver identiﬁcation of such counterfactuals at no cost.
The parameter of interest is expected utility, or proﬁt, from a ﬁxed policy, given by

π(s) = E s(X)Y (1) + (1 − s(X)) Y (0) ,

(3.2)

where we make explicit the dependence on the policy s(·). Compare to Equation (3.1) and recall that the observed outcome obeys Y = T Y (1) + (1 − T )Y (0). Whereas τ is the gain in assigning the next person to treatment and is given by the diﬀerence in potential outcomes, π(s) is the expected outcome that would be observed for the next person if the treatment rule were s(x).
A natural question is whether a candidate targeting strategy, say s (x), is superior to baseline or status quo policy, s0(x). This amounts to testing the hypothesis H0 : π(s ) ≥ π(s0). To evaluate this, we can study the diﬀerence in expected proﬁts, which amounts to

π(s , s0) = π(s ) − π(s0) = E (s (X) − s0(X))Y (1) + s0(X) − s (X) Y (0) .

(3.3)

Assumption 4 provides identiﬁcation for π(s) and π(s , s0), arguing analogously as for τ . Moreover, notice that π(s , s0) = E[(s (X) − s0(X))(Y (1) − Y (0))] = E[(s (X) − s0(X))τ (X)], where τ (x) = E[Y (1) − Y (0) | X = x] is the conditional average treatment eﬀect. The latter form makes clear that only those diﬀerently treated, of course, impact the evaluation of s compared to s0. The strategy s will be superior if, on average, it targets those with a higher individual treatment eﬀect. Estimating the optimal treatment policy from the data is discussed brieﬂy in Section 3.3.
The common structure of these parameters is that they all involve full-population averages of the potential outcomes, possibly scaled by a known function. For these parameters, the inﬂuence function is known from Hahn (1998), and estimators based on the inﬂuence function are doubly robust, as they remain consistent if either the regression functions or the propensity score are correctly speciﬁed (Robins et al., 1994, 1995). With a slight abuse of terminology (since we are

20

omitting the centering), the inﬂuence function for a single average potential outcome, t ∈ {0, 1}, is given by, for z = (y, t, x ) ,

ψt(z) = 1{T = t}(y − µt(x)) + µt(x).
P[T = t | X = x]

(3.4)

Our estimation of τ , π(s), and π(s , s0) will utilize sample averages of this function, with unknown objects replaced by estimators. Our use of inﬂuence functions here follows the recent literature in econometrics showing that the double robustness implies valid inference under weaker conditions on the ﬁrst step nonparametric estimates (Farrell, 2015; Chernozhukov et al., 2018a).

3.2 Subpopulation Eﬀect Parameters
The second type of causal eﬀects of interest are based on potential outcomes averaged over only a speciﬁc treatment group. A single such average, for t, t ∈ {0, 1}, is denoted by

ρt,t = E[Y (t) | T = t ].

(3.5)

Many interesting parameters are linear combinations of these for diﬀerent t and t . We focus on two for concreteness. (We could also consider averages restricted by targeting-type functions, as in expected utility/proﬁt, but for brevity we omit this.) The most well-studied of these parameters is the treatment eﬀect on the treated, given by

τ1,0 = E[Y (1) − Y (0) | T = 1] = ρ1,1 − ρ0,1.

(3.6)

To appreciate the breadth of this framework, and the applicability of our causal inference results, we also consider a decomposition parameter, a semiparametric analogue of Oaxaca-Blinder (Kitagawa, 1955; Oaxaca, 1973; Blinder, 1973). In this context, the “treatment” variable T is typically not a treatment assignment per se, but rather an exogenous covariate such as a demographic indicator, perhaps most commonly a male/female indicator. See Fortin et al. (2011) for a complete discussion and further references. The parameter of interest in this case is the decomposition of ∆ = E[Y (1) | T = 1] − E[Y (0) | T = 0], into the diﬀerence in the covariate distributions and the diﬀerence in expected outcomes. These can be written as functions of diﬀerent ρt,t . For example,
21

∆X = E[Y (1)|T = 1] − E[Y (1)|T = 0] = E[µ1(X)|T = 1] − E[µ1(X)|T = 0] = ρ1,1 − ρ1,0. We are in general interested in

∆ = ∆X + ∆µ,

∆X = ρ1,1 − ρ1,0, and ∆µ = ρ1,0 − ρ0,0.

(3.7)

Just as in the case of full-population averages, the inﬂuence function is known and leads to a doubly robust estimator. For a single ρt,t , the (uncentered) inﬂuence function is (cf. (3.4)):

ψt,t (z) = P[T = t | X = x] 1{T = t}(y − µt(x)) + 1{T = t }µt(x) .

P[T = t ]

P[T = t | X = x]

P[T = t ]

(3.8)

Estimation and inference requires, as above, estimation of the propensity scores and regression functions, depending on the exact choices of t and t , and here we also require the marginal probability of treatments.

3.3 Optimal Policies
Moving beyond a ﬁxed parameter, our results on deep neural networks can be used to address optimal targeting. In the notation of Section 3.1, this amounts to ﬁnding a policy, say s (x), that maximizes a given measure of utility stemming from treatment, generally the expected gain relative to a baseline policy. In Section 3.1 we considered the utility (or proﬁt) diﬀerence between two given strategies, a candidate s (x) and a baseline s0(x). Instead of inference on π(s , s0), we can use the data to ﬁnd the s (x) which maximizes the gain relative to the baseline. This problem has been widely studied in econometrics and statistics; for detailed discussion and numerous references see Manski (2004), Hirano and Porter (2009), Kitagawa and Tetenov (2018), and Athey and Wager (2018). In particular, the latter noticed using the locally robust framework allows policy optimization under nearly the same conditions as inference and proved fast convergence rates of the estimated policy in terms of regret.
More formally, we want to ﬁnd the optimal choice s (x) in some policy/action space S. The policy space, and thus its complexity, is user determined. Simple examples include simple decision trees or univariate-based strategies; more can be found in the references above. Recall that π(s , s0) = E[Y (s )] − E[Y (s0)] = E[(s (X) − s0(X))τ (X)], where τ (x) = E[Y (1) − Y (0) | X = x]

22

is the conditional average treatment eﬀect. Given a space S, we wish to ﬁnd the policy s (x) ∈ S which solves maxs ∈S π(s , s0). The main result of Athey and Wager (2018) is that replacing π with the doubly-robust πˆ of Equation (4.3), and minimizing the empirical analogue of regret, one obtains an estimator sˆ(x) of the optimal policy that obeys the regret bound π(s , s0) − π(sˆ, s0) = OP ( VC(S)/n) (a formal statement would be notationally burdensome). The complexity of the user-chosen policy space enters the bound through its VC dimension. Simple, interpretable policy classes often have bounded or slowly-growing dimension, implying rapid convergence.
3.4 Other Estimands
There are of course many other contexts where ﬁrst-step deep learning is useful. Only trivial extensions to the above would be required for other causal eﬀects, such as multi-valued treatments (reviewed by Cattaneo, 2010) and others with doubly-robust estimators (Sloczynski and Wooldridge, 2018). Further, under selection on observables, treatment eﬀects, missing data, measurement error, and data combination are equivalent, and thus all our results apply immediately to those contexts. For reviews of these and Assumption 4 more broadly, see Chen et al. (2004); Tsiatis (2006); Heckman and Vytlacil (2007); Imbens and Wooldridge (2009).
Moving beyond causal eﬀects, any estimand with a locally/doubly robust estimator depending only on target functions falling into our class of losses can be covered using the results of Section 2. For example, estimands requiring distribution estimation require further study; see Liang (2018) for recent results via Generative Adversarial Networks (GANs). More precisely, along with regularity conditions, our theory can be used to verify the conditions of Chernozhukov et al. (2018c), who treat more general semiparametric estimands using local robustness, sometimes relying on sample splitting or cross ﬁtting. Further in this vein, our results on deep neural networks can be used to address optimal targeting, i.e., ﬁnding the policy, say s (x), that maximizes a given measure of utility, by applying the results of Athey and Wager (2018), who noticed that using the locally robust framework allows policy optimization.
More broadly, the learning of features using deep neural networks is becoming increasingly popular and our results speak to this context directly. To illustrate, consider the simple example of a linear model where some predictors are features learned from independent data. Here, the object of interest is the ﬁxed-dimension coeﬃcient vector λ, which we assume can be partitioned
23

as λ = (λ1, λ2) according to the model Y = f (X) λ1 + W λ2 + ε. The features f (X), often a “score” of some type, are generally learned from auxiliary (and independent) data. For a recent example, see Liu et al. (2017). In such cases, inference on λ can proceed directly, as long as care is taken to interpret the results. See Section 4.2.

4 Asymptotic Inference

We now turn to asymptotic inference for the causal parameters discussed above. We ﬁrst deﬁne the

estimators, which are based on sample averages of the (uncentered) inﬂuence functions (3.4) and

(3.8). We then give a generic result for single averages which can then be combined for inference on

a given parameter of interest. Below we discuss inference under randomized treatment and using

sample splitting.

Throughout, we assume we have a sample {zi = (yi, ti, xi) }ni=1 from Z = (Y, T, X ) . We then

form

ψˆ (z ) = 1{ti = t}(yi − µˆt(xi)) + µˆ (x ),

ti

Pˆ[T = t | X = xi]

ti

(4.1)

where Pˆ[T = t | X = xi] = pˆ(xi) for t = 1 and 1 − pˆ(xi) for t = 0, and similarly

ψˆ (z ) = Pˆ[T = t | X = xi] 1{ti = t}(yi − µˆt(xi)) + 1{ti = t }µˆt(xi) ,

t,t i

Pˆ[T = t ]

Pˆ[T = t | X = xi]

Pˆ[T = t ]

(4.2)

where Pˆ[T = t ] is simply the sample frequency En[1{ti = t }].
For the ﬁrst stage estimates appearing in (4.1) and (4.2) we use our results on deep nets, and Theorem 1 in particular. Speciﬁcally, the estimated propensity score, pˆ(x), is the estimate that results from solving (2.4), with the MLP architecture, for the logistic loss (2.3) with T as the outcome. Similarly, for each status t ∈ {0, 1}, we can let µˆt(x) be the deep-MLP estimate of f∗(x) = E[Y |T = t, X = x], solving (2.4) for least squares loss, (2.2), with outcome Y , using only observations with ti = t. However, it is worth noting that the theoretically-equivalent joint estimation of Equation (5.1) performs much better, as the two groups may share features. To state the results, let βp and βµ be the smoothness parameters of Assumption 2 for the propensity score and outcome models, respectively.
We then obtain inference using the following results, essentially taken from Farrell (2015).

24

Similar results are given by Belloni et al. (2017) and Chernozhukov et al. (2018a). All of these provide high-level conditions for valid inference, and none verify these for deep nets as we do here.
Theorem 3. Suppose that {zi = (yi, ti, xi) }ni=1 are i.i.d. obeying Assumption 4 and the conditions Theorem 1 hold with βp ∧ βµ > d. Further assume that, for t ∈ {0, 1}, E[(s(X)ψt(Z))2|X] is bounded away from zero and, for some δ > 0, E[(s(X)ψt(Z))4+δ|X] is bounded. Then the deep MLP-ReLU network estimators deﬁned above obey the following, for t ∈ {0, 1},
(a) En[(pˆ(xi) − p(xi))2] = oP (1) and En (µˆt(xi) − µt(xi))2 = oP (1),
(b) En[(µˆt(xi) − µt(xi))2]1/2En[(pˆ(xi) − p(xi))2]1/2 = oP (n−1/2), and
(c) En[(µˆt(xi) − µt(xi))(1 − 1{ti = t}/P[T = t|X = xi])] = oP (n−1/2),

and therefore, if pˆ(xi) is bounded inside (0, 1), for a given s(x) and t ∈ {0, 1}, we have

√ nE

s(x )ψˆ (z ) − s(x )ψ (z ) = o (1)

and

En[(s(xi)ψˆt(zi))2] = o (1),

n

i ti

i ti

P

En[(s(xi)ψt(zi))2] P

as well as,

√ nE

ψˆ

(z ) − ψ

(z ) = o (1)

and

En[ψˆt,t (zi)2] = o (1).

n t,t i

t,t i

P

En[ψt,t (zi)2] P

This result, our main inference contribution, shows exactly how deep learning delivers valid

asymptotic inference for our parameters of interest. Theorem 1 (a generic result using Theorem 2

could be stated) proves that the nonparametric estimates converge suﬃciently fast, as formalized

by conditions (a), (b), and (c), enabling feasible eﬃcient semiparametric inference. In general, these

are implied by, but may be weaker than, the requirement of that the ﬁrst step estimates converge faster than n−1/4, which our results yield for deep ReLU nets. The ﬁrst is a mild consistency

requirement. The second requires a rate, but on the product of the two estimates, which can be

satisﬁed under weaker conditions. Finally, the third condition is the strongest. Intuitively, this

condition arises from a “leave-in” type remainder, and as such, it can be weakened using sample

splitting Chernozhukov et al. (2018a); Newey and Robins (2018). We opt to maintain (c) exactly

because deep nets are not amenable to either simple leave-one-out forms (as are, e.g., classical

kernel regression) or to sample splitting, being a data hungry method the gain in theoretically

weaker rate requirements may not be worth the price paid in constants in ﬁnite samples. Instead,

25

we employ our localization analysis, as was used to obtain the results of Section 2, to verify (c) directly (see Lemma 10); this appears to be a novel application of localization, and this approach may be useful in future applications of second-step inference using machine learning methods.
From this result we immediately obtain inference for all the causal parameters discussed above. For the full-population averages, for example, we would form

τˆ = En ψˆ1(zi) − ψˆ0(zi) , πˆ(s) = En s(xi)ψˆ1(zi) + (1 − s(xi))ψˆ0(zi) , πˆ(s , s0) = En [s (xi) − s0(xi)]ψˆ1(zi) − [s (xi) − s0(xi)]ψˆ0(zi) .

(4.3)

The estimator τˆ is exactly the doubly/locally robust estimator of the average treatment eﬀect that is standard in the literature. The estimators for proﬁts can be thought of as the doubly robust version of the constructs described in Hitsch and Misra (2018). Furthermore, to add a per-unit cost of treatment/targeting c and a margin m, simply replace ψ1 with mψ1 − c and ψ0 with mψ0. Similarly, τˆ1,0, ∆ˆ X , and ∆ˆ µ would be linear combinations of diﬀerent ρˆt,t = En[ψˆt,t (zi)].
It is immediate from Theorem 3 that all such estimators are asymptotically Normal. The asymptotic variance can be estimated by simply replacing the sample ﬁrst moments of (4.3) with second moments. That is, looking at πˆ(s) to ﬁx ideas,

√nΣˆ −1/2 (πˆ(s) − π(s)) →d N (0, 1), with Σˆ = En s(xi)ψˆ1(zi) + (1 − s(xi))ψˆ0(zi) 2 − πˆ(s)2.

The others are similar. Further, Theorem 3 can be generalized straightforwardly to yield uniformly valid inference, following the approach of Romano (2004), exactly as in Belloni et al. (2014) or Farrell (2015).
Finally, we note that our focus with Theorem 3 is showcasing the practical utility of deep learning. Our use of local/double robustness here is toward the aim of attaining feasible inference without requiring more detailed assumptions on the machine learning step. This comes at the expense of, for example, stronger-than-minimal smoothness assumptions. That is, the requirement that βp ∧ βµ > d is not minimal, and moreover, neither is the weaker condition βp ∧ βµ > d/2 that would be required after applying Corollary 1 instead of Theorem 1. Obtaining a Gaussian limit, and

26

possibly semiparametric eﬃciency, under minimal conditions has been studied by many, dating at least to Bickel and Ritov (1988); see Robins et al. (2009) for recent results and references on optimal estimation and minimal conditions. For causal inference, Chen et al. (2008) and Athey et al. (2018) obtain semiparametric eﬃciency under strictly weaker conditions than ours on p(x) (the former under minimal smoothness on µt(x) and the latter under a sparsity in a high-dimensional linear model). Further, as above, cross-ﬁtting (Newey and Robins, 2018) paired with local robustness may yield weaker smoothness conditions by providing underﬁtting” robustness (i.e. weakening biasrelated tuning parameter assumptions). On the other hand, weaker variance-related assumptions, or “overﬁtting” robust inference procedures, (Cattaneo and Jansson, 2018; Cattaneo et al., 2018), may also be possible following deep learning, but are less automatic at present. Finally, other methods designed for causal inference under relaxed assumptions may be useful here, such as the recently developed extensions to doubly robust estimation (Tan, 2018) and inverse weighting (Ma and Wang, 2018): pursuing these in the context of deep learning is left to future work.
4.1 Inference Under Randomization
Our analysis thus far has focused on observational data, but it is worth spelling out results for randomized experiments. This is particularly important in the Internet age, where experimentation is common, vast amounts of data are available, and eﬀects are often small in magnitude (Taddy et al., 2015). Indeed, our empirical illustration, detailed in the next section, stems from an experiment with 300,000 units and hundreds of covariates. When treatment is randomized, inference can be done directly using the mean outcomes in the treatment and control groups, such as the diﬀerence for the average treatment eﬀect or the corresponding weighted sum for proﬁt. However, pre-treatment covariates can be used to increase eﬃciency (Hahn, 2004).
We will focus on the simple situation of a purely randomized binary treatment, but our results can be extended naturally to other randomization schemes. We formalize this with the following.
Assumption 5 (Randomized Treatment). T is independent of Y (0), Y (1), and X, and is distributed Bernoulli with parameter p∗, such that p¯ ≤ p∗ ≤ 1 − p¯ for some p¯ > 0.
Under this assumption, the obvious simpliﬁcation is that the propensity score need not be estimated using the covariates, but can be replaced with the (still nonparametric) sample frequency:
27

pˆ(xi) ≡ pˆ = En[ti]. This is plugged into Equation (4.3) and estimation and inference proceeds as above. Only rate conditions on the regression functions µˆt(x) are needed. Further, conditions (a) and (b) of Theorem 3 collapse, as pˆ is root-n consistent, leaving only condition (c) to be veriﬁed. Again, cross-ﬁtting can be used in theory to remove this condition and thus weaken the requirement that βµ > d, but we maintain this for simplicity. We collect this into the following result, which is a trivial corollary of Theorem 3.
Corollary 3. Let the conditions of Theorem 3 hold with Assumption 5 in place of Assumption 4 and only βµ > d. Then deep MLP-ReLU network estimators obey
(a ) En (µˆt(xi) − µt(xi))2 = oP (1) and
(c ) En[(µˆt(xi) − µt(xi))(1 − 1{ti = t}/p∗)] = oP (n−1/2)
and the conclusions of Theorem 3 hold.
4.2 Sample Splitting
Sample splitting may be used to obtain valid inference in cases, unlike those above, where the parameter of interest itself is learned from the data. For the causal estimands above, the regression functions and propensity score must be estimated, but these are nuisance functions. This is not true in the inference after policy or feature learning (Sections 3.3 and 3.4). For policy learning, our results can be used to verify the high-level conditions of Athey and Wager (2018), though they require the additional condition of uniform consistency of the ﬁrst stage estimators, and for machine learning estimators this is not clearly innocuous. However, this gives only point estimation.
Sample splitting is used in the obvious way: the ﬁrst subsample, or more generally, independent auxiliary data, is used to learn the features or optimal policy, and then Theorem 3 is applied in the second subsample, conditional on the results of the ﬁrst. For policy learning this delivers valid inference on π(sˆ) or π(sˆ, s0), while for the simple example of feature learning in a linear model we obtain inference on the parameters deﬁned by the “model” Y = f (X) λ1 + W λ2 + ε, where f (X) is estimated from auxiliary data. Care must be taken in interpreting the results. The results of the ﬁrst-subsample estimation are eﬀectively conditioned upon in the inference stage, redeﬁning the target parameter to be in terms of the learned object. In many contexts this may be suﬃcient
28

(Chernozhukov et al., 2018b), but further assumptions will generally be needed to assume that the ﬁrst subsample has recovered the true population object. To ﬁx ideas, consider policy learning: inference on π(sˆ, s0), conditional on the map sˆ(x) learned in the ﬁrst subsample, is immediate and requires no additional assumptions, but inference on π(s , s0) is not obvious without further conditions.
5 Empirical Application
To illustrate our results, Theorems 1 and 3 in particular, we study, from a marketing point of view, a randomized experiment from a large US retailer of consumer products. The outcome of interest is consumer spending and the treatment is a catalog mailing. The ﬁrm sells directly to the customer (as opposed to via retailers) using a variety of channels such as the web and mail. The data consists of nearly three hundred thousand (292,657) consumers chosen at random from the retailer’s database. Of these, 2/3 were randomly chosen to receive a catalog, and in addition to treatment status, we observe roughly one hundred ﬁfty covariates, including demographics, past purchase behaviors, interactions with the ﬁrm, and other relevant information. For more on the data and a complete discussion of the decision making issues, we refer the reader to Hitsch and Misra (2018) (we use the 2015 sample). That paper studied various estimators, both traditional and modern, of average and heterogeneous causal eﬀects. Importantly, they did not consider neural networks. Our results show that deep nets are at least as good as (and sometimes better than) the best methods in Hitsch and Misra (2018).
In terms of motivation, a key element of a ﬁrm’s toolkit is the design and implementation of targeted marketing instruments. These instruments, aiming to induce demand, often contain advertising and informational content about the ﬁrms oﬀerings. The targeting aspect thus boils down to the selection of which particular customers should be sent the material. This is a particularly important decision since the costs of creation and dissemination of the material can accumulate rapidly, particularly over a large customer base. For a typical retailer engaging in direct marketing the costs of sending out a catalog can be close to a dollar per targeted customer. With millions of catalogs being sent out, the cost of a typical campaign is quite high.
Given these expenses, an important problem for ﬁrms is ascertaining the causal eﬀects of such
29

targeted mailing, and then using these eﬀects to evaluate potential targeting strategies. At a high level, this approach is very similar to modern personalized medicine where treatments have to be targeted. In these contexts, both the treatment and the targeting can be costly, and thus careful assessment of π(s) (interpreted as welfare) is crucial for decision making.
The outcome of interest for the ﬁrm is customer spending. This is the total amount of money that a given customer spends on purchases of the ﬁrm’s products, within a speciﬁed time window. For the experiment in question the ﬁrm used a window of three months, and aggregated sales from all available purchase channels including phone, mail, and the web. In our data 6.2% of customers made a purchase. Overall mean spending is $7.31; average spending conditional on buying is $117.7, with a standard deviation of $132.44. The idea then is to examine the incremental eﬀect that the catalog had on this spending metric. Table 1 presents summary statistics for the outcome and treatment. Figure 3 displays the complete density of spending conditional on a purchase, which is quite skewed.

Frequency 0 500 1000 1500 2000 2500 3000

0

200

400

600

800

1000

Spend conditional on purchase

Figure 3: Spend Conditional on Purchase

30

Table 1: Summary Statistics

Purchase Spend
Spend Conditional on Purchase Treatment
Purchase | Treatment=1 Purchase | Treatment=0
Spend | Treatment=1 Spend | Treatment=0

Mean 0.062 7.311 117.730 0.669 0.069 0.047 8.158 5.597

SD 0.24 43.55 132.44 0.47 0.25 0.21 44.71 41.04

N 292657 292657 18174 292657 195821
96836 195821 96836

5.1 Implementation Details

We estimated deep neural nets under a variety of architecture choices. In what follows we present eight examples and focus on one particular architecture to compute various statistics and tests to illustrate the use of the theory developed above. All computation was done using TensorFlowTM.
For treatment eﬀect and proﬁt estimation we follow Equations (4.1) and (4.3). Because treatment is randomized, we apply Corollary 3, and thus, only require estimates of the regression functions µt(x) = E[Y (t)|X = x], t ∈ {0, 1}. An important implementation detail, from a computation point of view (recall Remark 2) is that we will estimate µ0(x) and τ (x) (and thereby µ1(x)) jointly (results from separate estimation are available). To be precise, recalling Equations (2.2) and (2.4), we solve





µˆ0(x)

n1

2



 := arg min

yi − µ˜0(xi) − τ˜(xi)ti





τˆ(x) = µˆ1(x) − µˆ0(x)

µ˜0,τ˜ i=1 2

(5.1)

where the minimization is over the relevant network architecture. Recall that, in the context of our empirical example yi is the customer’s spending, xi are her characteristics, and ti indicates receipt of a catalog. In this format, µ0(xi) reﬂects base spending and τ (x) = µ1(xi) − µ0(xi) is the conditional average treatment eﬀect of the catalog mailing. In our application, this joint estimation outperforms separately estimating each µt(x) on the respective samples (though these two approaches are equivalent theoretically).
The details of the eight deep net architectures are presented in Table 2. See Section 2.1 for an

31

introduction to the terminology and network construction. Most yielded similar results, both in terms of ﬁt and ﬁnal estimates. A key measure of ﬁt reported in the ﬁnal column of the table is the portion of τˆ(xi) that were negative. As argued by Hitsch and Misra (2018), it is implausible under standard marketing or economic theory that receipt of a catalog causes lower purchasing. On this metric of ﬁt, deep nets perform as well as, and sometimes better than, the best methods found by Hitsch and Misra (2018): Causal KNN with Treatment Eﬀect Projections (detailed therein) or Causal Forests (Wager and Athey, 2018). Figure 4 shows the distribution of τˆ(xi) across customers for each of the eight architectures. While there are diﬀerences in the shapes of the densities, the mean and variance estimates are nonetheless quite similar.
Table 2: Deep Network Architectures

Architecture 1 2 3 4 5 6 7 8

Learning Rate 0.0003 0.0003 0.0001 0.0009 0.0003 0.0003 0.0003
0.00005

Widths [H1, H2, ...]
[60] [100] [30, 20] [30, 10] [30, 30] [30, 30] [100, 30, 20] [80, 30, 20]

Dropout [H1, H2, ...]
[0.5] [0.5] [0.5, 0] [0.3, 0.1] [0, 0] [0.5, 0] [0.5, 0.5, 0] [0.5, 0.5, 0]

Total Parameters
8702 14502 4952 4622 5282 5282 17992 14532

Validation Loss
1405.62 1406.48 1408.22 1408.56 1403.57 1408.57 1408.62 1413.70

Training Loss
1748.91 1751.87 1751.20 1751.62 1738.59 1755.28 1751.52 1756.93

Pn[τˆ(xi) < 0] 0.0014 0.0251 0.0072 0.0138 0.0226 0.0066 0.0103 0.0002

Notes: All networks use the ReLU activation function. The width of each layer is shown, e.g. Architecture 3 consists of two layers, with 30 and 20 hidden units respectively. The ﬁnal column shows the portion of estimated individual treatment eﬀects below zero.

5.2 Results
We present now results for treatment eﬀects, utility/proﬁts, and targeting policy evaluations. Table 3 shows the estimates of the average treatment eﬀect from the eight network architectures along with their respective 95% conﬁdence intervals. These results are constructed following Section 4, using Equations (4.1) and (4.3) in particular, and valid by Corollary 3. Because this is an experiment, we can compare to the standard unadjusted diﬀerence in means, which yields an average treatment eﬀect of 2.561.
Turning to expected proﬁts, we estimate π(s) = E s(X)(mY (1) − c) + (1 − s(X)) mY (0) ,
32

1.0

0.8

0.6

Density

0.4

0.2

0.0

−5

0

5

10

15

20

Conditional Average Treatment Effect

Figure 4: Conditional Average Treatment Eﬀects Across Architectures

Table 3: Average Treatment Eﬀect Estimates and 95% Conﬁdence Intervals

Architecture 1 2 3 4 5 6 7 8

Average Treatment Eﬀect (τˆ) 2.606 2.577 2.547 2.488 2.459 2.430 2.400 2.371

95% Conﬁdence Interval
[2.273 , 2.932] [2.252 , 2.901] [2.223 , 2.872] [2.160 , 2.817] [2.127 , 2.791] [2.093 , 2.767] [2.057 , 2.744] [2.021 , 2.721]

adding a proﬁt margin m and a mailing cost c to (3.2) (our NDA with the ﬁrm forbids revealing m and c). We consider three diﬀerent counterfactual policies s(x): (i) never treat, s(x) ≡ 0; (ii) a blanket treatment, s(x) ≡ 1; (iii) a loyalty policy, s(xi) = 1 only for those who had purchased in the prior calendar year. Results are shown in Table 4. It is clear that proﬁts from the three policies are ordered as π(never) < π(blanket) < π(loyalty).
For both the average eﬀects of Table 3 and the counterfactuals of Table 4 there is broad agree-

33

Table 4: Counterfactual Proﬁts from Three Targeting Strategies

Architecture 1 2 3 4 5 6 7 8

Never Treat

πˆ(s)

95% CI

2.016 [1.923 , 2.110]

2.022 [1.929 , 2.114]

2.027 [1.934 , 2.120]

2.037 [1.944 , 2.130]

2.043 [1.950 , 2.136]

2.048 [1.954 , 2.142]

2.053 [1.959 , 2.148]

2.059 [1.963 , 2.154]

Blanket Treatment

πˆ(s)

95% CI

2.234 [2.162 , 2.306]

2.229 [2.157 , 2.301]

2.224 [2.152 , 2.296]

2.213 [2.140 , 2.286]

2.208 [2.135 , 2.281]

2.202 [2.128 , 2.277]

2.197 [2.122 , 2.272]

2.192 [2.116 , 2.268]

Loyalty Policy

πˆ(s)

95% CI

2.367 [2.292 , 2.443]

2.363 [2.288 , 2.438]

2.358 [2.283 , 2.434]

2.350 [2.274 , 2.425]

2.345 [2.269 , 2.422]

2.341 [2.263 , 2.418]

2.336 [2.258 , 2.414]

2.332 [2.253 , 2.411]

ment among the eight architectures both numerically and substantially. This may be due to the fact that the data is experimental, so that the propensity score is constant. In true observational data this may not be the case. We explore this issue in our Monte Carlo analysis below.

5.2.1 Placebo Experiment
We conducted a set of placebo tests to examine whether the deep neural networks we use can truly recover causal eﬀects. In particular, we take only the untreated customers in the data and randomly assign half to treated status.2 We then ran the eight architectures of Table 2, as in the true data. The conditional average treatment eﬀects across the architectures are plotted in Figure 5. We see that the “true” zero average eﬀect is recovered precisely and with the expected distribution. The average treatment eﬀect across all models is estimated to be around -0.024, compared to 2.56 in the original data. Exercises with diﬀerent proportions of (placebo) treated customers revealed similar results.

5.2.2 Optimal Targeting
To explore further, we focus on architecture #3 and study subpopulation treatment targeting strategies following the ideas of Section 3.3. (The other architectures yield similar results, so we omit them.) Architecture #3 has depth L = 2 with widths H1 = 30 and H2 = 20. The learning rate was set at 0.0001 and the speciﬁcation had a total of 4,952 parameters. For this architecture, recalling Remark 1, we added dropout for the second layer with a ﬁxed probability
2We thank Guido Imbens suggesting this analysis.

34

15

10

Density

5

0

−20

−10

0

10

20

Conditional Average Treatment Effect

Figure 5: Placebo Test

35

Profit Difference −0.05 0.00 0.05 0.10 0.15 0.20 0.25

0

200

400

600

800

1000

1200

Spend

Figure 6: Expected Proﬁts from Threshold Targeting Based on Prior Year Spend

of 1/2. Using this architecture, we compare the blanket strategy (so s0(x) = 1) to targeting customers with spend of at least y¯ dollars in the prior calendar year (prior spending is one of the
covariates), in $50 increments to $1200. The policy class is therefore S = {s(x) = 1(prior spend >
y¯), y¯ = 0, 50, 100, . . . , 1150, 1200}. Figure 6 presents the results. The black dots show the diﬀerence πˆ(spend > y¯) − πˆ(blanket) and the shaded region gives a pointwise 95% conﬁdence band (to ease
presentation, sample splitting is not used). We see that there is a signiﬁcant diﬀerence between various choices of y¯. Initially, targeting customers with higher spend yields higher proﬁts, as would be expected, but this eﬀect diminishes beyond a certain y¯, roughly $500, as fewer and fewer are
targeted. The optimal policy estimate is sˆ(x) = 1(prior spend > 400). In general, simpler policy
classes may yield better decisions, but it is certainly possible to expand our search to diﬀerent S by considering further covariates and/or transformations.

36

6 Monte Carlo Analysis
We conducted a set of Monte Carlo experiments to evaluate our theoretical results. We study inference on the average treatment eﬀect, τ of (3.1), under diﬀerent data generating processes (DGPs). In each DGP we take n = 10, 000 i.i.d. samples and use 1,000 replications. For either d = 20 or 100, X includes a constant term and d independent uniform random variables, U(0, 1). Treatment assignment is Bernoulli with probability p(x), where p(x) is the propensity score. We consider both (i) randomized treatments with p(x) = 0.5 and (ii) observational data with p(x) = (1 + exp(−αpx))−1, where αp,1 = 0.09 and the remainder are drawn once as U (−0.55, 0.55), and then ﬁxed for the replications. For d = 100, we maintain αp 0 = 20 for the simplicity. These generate propensities with an approximate range of approximately (0.30, 0.75) and mean roughly 0.5.
Given covariates and treatment assignment, the outcomes are generated according to
yi = µ0(xi) + τ (xi)ti + εi, µ0(x) = αµx + βµϕ(x), τ (xi) = ατ x + βτ ϕ(x),
where εi ∼ N (0, 1) and ϕ(x) are second-degree polynomials including pairwise interactions. For µ0(x) and τ (x) we consider two cases, linear and nonlinear models. In both cases the intercepts are αµ,1 = 0.09 and ατ,1 = −0.05 and slopes are drawn (once) as αµ,k ∼ N (0.3, 0.7) and ατ,k ∼ U (0.1, 0.22), k = 2, . . . , d + 1. The linear models set βµ = βτ = 0 while the nonlinear models take βµ,k ∼ N (0.01, 0.3) and βτ,k ∼ U (−0.05, 0.06). Altogether, this yields eight designs: d = 20 or 100, p(x) constant or not, and outcome models linear or nonlinear.
For each design, we consider a variety of network architectures, all ReLU-based MLPs. These architectures are variants of the ones used in the empirical application (which were customized for the application). All networks vary in their depth and width, as spelled out in Table 5.
Tables 6 and 7 show the results for all eight DGPs. Table 6 shows randomized treatment while Table 7 shows results mimicking observational data. Overall, the results reported show excellent performance of deep learning based semiparametric inference. The bias is minimal and the coverage is quite accurate, while the interval length is under control. Notice that the most architectures yield similar results with no architecture dominating the others. Further, the coverage and interval
37

Table 5: Monte Carlo Architectures Explored

Architecture 1 2 3 4 5 6 7 8 9

Structure {20, 15, 5} {60, 30, 20} {80, 80, 80} {20, 15, 10, 5} {60, 30, 20, 10} {80, 80, 80, 80} {20, 15, 15, 10, 10, 5} {60, 30, 20, 20, 10, 5} {80, 80, 80, 80, 80, 80}

length are fairly similar with the more complex architecture not exhibiting any systematic patterns of length inﬂation.
None of the architectures we presented earlier used regularization. In typical empirical applications, including our own, researchers adopt architectures that employ dropout, a common method of regularization; see Remark 1. Our own preliminary exploration of dropout and other forms of regularization found expected departures form nonregularized models. In most, but not all, cases the coverage remained accurate, but with increased bias and interval length compared to Table 6 and 7. The results preach caution when applying regularization in applications.

7 Conclusion
The utility of deep learning in social science applications is still a subject of interest and debate. While there is an acknowledgment of its predictive power, there has been limited adoption of deep learning in social sciences such as economics. Some part of the reluctance to adopting these methods stems from the lack of theory facilitating use and interpretation. We have shown, both theoretically as well as empirically, that these methods can oﬀer excellent performance.
In this paper, we have given a formal proof that inference can be valid after using deep learning methods for ﬁrst-step estimation. To the best of our knowledge, ours is the ﬁrst inference result using deep nets. Our results thus contribute directly to the recent explosion in both theoretical and applied research using machine learning methods in economics, and to the recent adoption of deep learning in empirical settings. We obtained novel bounds for deep neural networks, speaking directly to the modern (and empirically successful) practice of using fully-connected feedfoward
38

Table 6: Simulations Results - Constant Propensity Score

Model Linear
Nonlinear

Architecture
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9

20 Covariates

Bias

IL Coverage

0.00027 0.079 0.947

-0.00032 0.079 0.951

-0.00025 0.079 0.955

-0.00068 0.079 0.949

0.00008 0.079 0.945

0.00007 0.079 0.955

0.00128 0.079 0.952

0.00108 0.079 0.949

0.00021 0.078 0.948

0.00087 0.081 0.946

0.00015 0.079 0.954

-0.00072 0.079 0.940

0.00101 0.080 0.945

0.00027 0.079 0.935

-0.00025 0.079 0.929

-0.00052 0.080 0.947

0.00077 0.079 0.938

-0.00013 0.079 0.940

100 Covariates

Bias

IL Coverage

0.00067 0.080 0.946

0.00012 0.080 0.958

-0.00167 0.080 0.939

0.00038 0.080 0.949

-0.00219 0.080 0.929

-0.00010 0.080 0.946

-0.00041 0.080 0.944

-0.00088 0.080 0.941

-0.00080 0.081 0.953

-0.00067 0.163 0.940

0.00093 0.153 0.927

0.00245 0.148 0.926

-0.00087 0.165 0.956

-0.00190 0.154 0.923

-0.00117 0.146 0.902

0.00091 0.165 0.941

0.00201 0.153 0.927

0.00049 0.154 0.936

networks. Our results allow for diﬀerent network architectures, including ﬁxed width, very deep networks. Our results cover general nonparametric regression-type loss functions, covering most nonparametric practice. We used our bounds to deliver fast convergence rates allowing for secondstage inference on a ﬁnite-dimensional parameter of interest.
There are practical implications of the theory presented in this paper. We focused on semiparametric causal eﬀects as a concrete illustration, but deep learning is a potentially valuable tool in many diverse economic settings. Our results allow researchers to embed deep learning into standard econometric models such as linear regressions, generalized linear models, and other forms of limited dependent variables models (e.g. censored regression). Our theory can also be used as a starting point for constructing deep learning implementations of two-step estimators in the context of selection models, dynamic discrete choice, and the estimation of games.
39

Table 7: Simulations Results - Non-constant Propensity Score

Model Linear
Linear

Architecture
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9

20 Covariates

Bias

IL Coverage

-0.00202 0.080 0.948

0.00011 0.079 0.946

-0.00130 0.079 0.964

-0.00106 0.079 0.945

-0.00083 0.079 0.951

-0.00068 0.079 0.955

-0.00119 0.079 0.953

-0.00056 0.079 0.952

-0.00096 0.079 0.948

-0.00076 0.081 0.946

-0.00122 0.080 0.939

-0.00074 0.080 0.926

-0.00171 0.081 0.940

-0.00135 0.080 0.952

-0.00075 0.080 0.950

-0.00153 0.081 0.928

0.00082 0.080 0.953

-0.00127 0.080 0.931

100 Covariates

Bias

IL Coverage

0.0009 0.081 0.955

0.0007 0.081 0.945

-0.0001 0.081 0.937

0.0002 0.081 0.933

-0.0004 0.081 0.944

0.0001 0.081 0.924

-0.0001 0.081 0.942

-0.0008 0.081 0.939

-0.0007 0.081 0.952

-0.00279 0.164 0.937

0.00020 0.155 0.941

-0.00080 0.148 0.914

-0.00184 0.166 0.938

-0.00103 0.154 0.912

-0.00174 0.147 0.905

-0.00377 0.165 0.929

0.00031 0.154 0.919

-0.00094 0.156 0.917

To be clear, we see our paper as a ﬁrst step in the exploration of deep learning as a tool for economic applications. There are a number of opportunities, questions, and challenges that remain. For example, factor models in ﬁnance might beneﬁt from the use of auto-encoders and recurrent neural nets may have applications in time series. For some estimands, it may be crucial to estimate the density as well, and this problem can be challenging in high dimensions. Deep nets, in the form of GANs are a promising tool for distribution estimation. There are also interesting questions remaining as to an optimal network architecture, and if this can be itself learned from the data, as well as computational and optimization guidance. Research into these further applications and structures is underway.

40

8 References
Abadie, A. and M. D. Cattaneo (2018): “Econometric Methods for Program Evaluation,” Annual Review of Economics, 10, 465–503.
Anthony, M. and P. L. Bartlett (1999): Neural Network Learning: Theoretical Foundations, Campbridge University Press.
Athey, S., G. Imbens, T. Pham, and S. Wager (2017): “Estimating average treatment effects: Supplementary analyses and remaining challenges,” American Economic Review: Papers & Proceeding, 107, 278–81.
Athey, S., G. W. Imbens, and S. Wager (2018): “Approximate residual balancing: debiased inference of average treatment eﬀects in high dimensions,” Journal of the Royal Statistical Society, Series B, 80, 597–623.
Athey, S. and S. Wager (2018): “Eﬃcient Policy Learning,” arXiv preprint arXiv:1702.02896.
Barron, A. R. (1993): “Universal approximation bounds for superpositions of a sigmoidal function,” IEEE Transactions on Information theory, 39, 930–945.
Bartlett, P. L., O. Bousquet, and S. Mendelson (2005): “Local rademacher complexities,” The Annals of Statistics, 33, 1497–1537.
Bartlett, P. L., N. Harvey, C. Liaw, and A. Mehrabian (2017): “Nearly-tight VCdimension bounds for piecewise linear neural networks,” in Proceedings of the 22nd Annual Conference on Learning Theory (COLT 2017).
Bauer, B. and M. Kohler (2017): “On Deep Learning as a remedy for the curse of dimensionality in nonparametric regression,” Tech. rep., Technical report.
Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012): “Sparse models and methods for optimal instruments with an application to eminent domain,” Econometrica, 80, 2369–2429.
Belloni, A., V. Chernozhukov, D. Chetverikov, C. Hansen, and K. Kato (2018): “HighDimensional Econometrics and Generalized GMM,” arXiv preprint arXiv:1806.01888.
Belloni, A., V. Chernozhukov, I. Ferna´ndez-Val, and C. Hansen (2017): “Program Evaluation and Causal Inference With High-Dimensional Data,” Econometrica, 85, 233–298.
Belloni, A., V. Chernozhukov, and C. Hansen (2014): “Inference on Treatment Eﬀects after Selection Amongst High-Dimensional Controls,” Review of Economic Studies, 81, 608–650.
Belloni, A., V. Chernozhukov, and L. Wang (2011): “Square-root lasso: pivotal recovery of sparse signals via conic programming,” Biometrika, 98, 791–806.
Bickel, P. J. and Y. Ritov (1988): “Estimating Integrated Squared Density Derivatives: Sharp Best Order of Convergence Estimates,” Sankhy¯a, 50, 381–393.
Bickel, P. J., Y. Ritov, and A. B. Tsybakov (2009): “Simultaneous Analysis of LASSO and Dantzig Selector,” The Annals of Statistics, 37, 1705–1732.
41

Blinder, A. (1973): “Wage Discrimination: Reduced Form and Structural Estimates,” Journal of Human Resources, 8, 436–455.
Cattaneo, M. D. (2010): “Eﬃcient Semiparametric Estimation of Multi-valued Treatment Eﬀects under Ignorability,” Journal of Econometrics, 155, 138–154.
Cattaneo, M. D. and M. Jansson (2018): “Kernel-Based Semiparametric Estimators: Small Bandwidth Asymptotics and Bootstrap Consistency,” Econometrica, 86, 955–995.
Cattaneo, M. D., M. Jansson, and X. Ma (2018): “Two-step Estimation and Inference with Possibly Many Included Covariates,” arXiv:1807.10100, Review of Economic Studies, forthcoming.
Chen, X. (2007): “Large Sample Sieve Estimation of Semi-Nonparametric Models,” in Handbook of Econometrics, ed. by J. Heckman and E. Leamer, Elsevier, vol. 6B of Handbook of Econometrics, chap. 76.
Chen, X., H. Hong, and A. Tarozzi (2004): “Semiparametric Eﬃciency in GMM Models of Nonclassical Measurament Errors, Missing Data and Treatment Eﬀects,” Cowles Foundation Discussion Paper No. 1644.
——— (2008): “Semiparametric Eﬃciency in GMM Models With Auxiliary Data,” The Annals of Statistics, 36, 808–843.
Chen, X. and X. Shen (1998): “Sieve extremum estimates for weakly dependent data,” Econometrica, 66, 289–314.
Chen, X. and H. White (1999): “Improved rates and asymptotic normality for nonparametric neural network estimators,” IEEE Transactions on Information Theory, 45, 682–691.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins (2018a): “Double/debiased machine learning for treatment and structural parameters,” The Econometrics Journal, 21, C1–C68.
Chernozhukov, V., M. Demirer, E. Duflo, and I. Fernandez-Val (2018b): “Generic Machine Learning Inference on Heterogenous Treatment Eﬀects in Randomized Experiments,” arXiv preprint arXiv:1712.04802.
Chernozhukov, V., J. C. Escanciano, H. Ichimura, W. K. Newey, and J. M. Robins (2018c): “Locally Robust Semiparametric Estimation,” arXiv:1608.00033.
Daniely, A. (2017): “Depth separation for neural networks,” arXiv preprint arXiv:1702.08489.
Farrell, M. H. (2015): “Robust Inference on Average Treatment Eﬀects with Possibly More Covariates than Observations,” arXiv:1309.4686, Journal of Econometrics, 189, 1–23.
Fortin, N., T. Lemieux, and S. Firpo (2011): “Decomposition Methods in Economics,” vol. 4 of Handbook of Labor Economics, 1–102.
Gine, E. and R. Nickl (2016): Mathematical Foundations of Inﬁnite-Dimensional Models, Cambridge.
Goodfellow, I., Y. Bengio, and A. Courville (2016): Deep learning, Cambridge: MIT Press.
42

Hahn, J. (1998): “On the Role of the Propensity Score in Eﬃcient Semiparametric Estimation of Average Treatment Eﬀects,” Econometrica, 66, 315–331.
——— (2004): “Functional restriction and eﬃciency in causal inference,” Review of Economics and Statistics, 84, 73–76.
Hanin, B. (2017): “Universal function approximation by deep neural nets with bounded width and relu activations,” arXiv preprint arXiv:1708.02691.
Hansen, C., D. Kozbur, and S. Misra (2017): “Targeted Undersmoothing,” arXiv:1706.07328.
Hartford, J., G. Lewis, K. Leyton-Brown, and M. Taddy (2017): “Deep iv: A ﬂexible approach for counterfactual prediction,” in International Conference on Machine Learning, 1414– 1423.
Hastie, T., R. Tibshirani, and J. Friedman (2009): The elements of statistical learning, Springer Series in Statistics, New York: Springer-Verlag.
He, K., X. Zhang, S. Ren, and J. Sun (2016): “Identity mappings in deep residual networks,” in European conference on computer vision, Springer, 630–645.
Heckman, J. and E. J. Vytlacil (2007): “Econometric Evaluation of Social Programs, Part I,” in Handbook of Econometrics, vol. VIB, ed. by J. Heckman and E. Leamer, Elsevier Science B.V., 4780–4874.
Hirano, K. and J. Porter (2009): “Asymptotics for statistical treatment rules,” Econometrica, 77, 1683–1701.
Hitsch, G. J. and S. Misra (2018): “Heterogeneous Treatment Eﬀects and Optimal Targeting Policy Evaluation,” SSRN preprint 3111957.
Hornik, K., M. Stinchcombe, and H. White (1989): “Multilayer feedforward networks are universal approximators,” Neural networks, 2, 359–366.
Imbens, G. W. and D. B. Rubin (2015): Causal Inference in Statistics, Social, and Biomedical Sciences, Cambridge University Press.
Imbens, G. W. and J. M. Wooldridge (2009): “Recent Developments in the Econometrics of Program Evaluation,” Journal of Economic Literature, 47, 5–86.
Javanmard, A. and A. Montanari (2014): “Conﬁdence intervals and hypothesis testing for high-dimensional regression,” The Journal of Machine Learning Research, 15, 2869–2909.
Johansson, F., U. Shalit, and D. Sontag (2016): “Learning representations for counterfactual inference,” in International Conference on Machine Learning, 3020–3029.
Kingma, D. P. and J. Ba (2014): “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980.
Kitagawa, E. M. (1955): “Components of a Diﬀerence Between Two Rates,” Journal of the American Statistical Association, 50, 1168–1194.
Kitagawa, T. and A. Tetenov (2018): “Who should be treated? empirical welfare maximization methods for treatment choice,” Econometrica, 86, 591–616.
43

Koltchinskii, V. (2006): “Local Rademacher complexities and oracle inequalities in risk minimization,” The Annals of Statistics, 34, 2593–2656.
——— (2011): Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems, Springer-Verlag.
Koltchinskii, V. and D. Panchenko (2000): “Rademacher processes and bounding the risk of function learning,” in High dimensional probability II, Springer, 443–457.
Krizhevsky, A., I. Sutskever, and G. E. Hinton (2012): “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances in neural information processing systems, 1097– 1105.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner (1998): “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, 86, 2278–2324.
Liang, T. (2018): “On How Well Generative Adversarial Networks Learn Densities: Nonparametric and Parametric Results,” arXiv:1811.03179.
Liang, T., A. Rakhlin, and K. Sridharan (2015): “Learning with square loss: Localization through oﬀset Rademacher complexity,” in Conference on Learning Theory, 1260–1285.
Liu, X., D. Lee, and K. Srinivasan (2017): “Large scale cross category analysis of consumer review content on sales conversion leveraging deep learning,” working paper, NYU Stern.
Ma, X. and J. Wang (2018): “Robust Inference Using Inverse Probability Weighting,” arXiv preprint arXiv:1810.11397.
Makovoz, Y. (1996): “Random approximants and neural networks,” Journal of Approximation Theory, 85, 98–109.
Manski, C. F. (2004): “Statistical treatment rules for heterogeneous populations,” Econometrica, 72, 1221–1246.
Mendelson, S. (2003): “A few notes on statistical learning theory,” in Advanced lectures on machine learning, Springer, 1–40.
——— (2014): “Learning without concentration,” in Conference on Learning Theory, 25–39.
Mhaskar, H. and T. Poggio (2016a): “Deep vs. shallow networks: An approximation theory perspective,” arXiv preprint arXiv:1608.03287.
Mhaskar, H. N. and T. Poggio (2016b): “Deep vs. shallow networks: An approximation theory perspective,” Analysis and Applications, 14, 829–848.
Nair, V. and G. E. Hinton (2010): “Rectiﬁed linear units improve restricted boltzmann machines,” in Proceedings of the 27th international conference on machine learning (ICML-10), 807–814.
Newey, W. K. and J. M. Robins (2018): “Cross-ﬁtting and fast remainder rates for semiparametric estimation,” arXiv preprint arXiv:1801.09138.
Oaxaca, R. (1973): “Male-Female Wage Diﬀerentials in Urban Labor Markets,” International Economic Review, 14, 693–709.
44

Pisier, G. (1981): “Remarques sur un r´esultat non publi´e de B. Maurey,” in S´eminaire Analyse fonctionnelle (dit” Maurey-Schwartz”), 1–12.
Polson, N. and V. Rockova (2018): “Posterior Concentration for Sparse Deep Learning,” arXiv preprint arXiv:1803.09138.
Raghu, M., B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein (2017): “On the Expressive Power of Deep Neural Networks,” in Proceedings of the 34th International Conference on Machine Learning, ed. by D. Precup and Y. W. Teh, International Convention Centre, Sydney, Australia: PMLR, vol. 70 of Proceedings of Machine Learning Research, 2847–2854.
Robins, J., L. Li, R. Mukherjee, E. Tchetgen, and A. van der Vaart (2017): “Minimax Estimation of a Functional on a Structured High-Dimensional Model,” The Annals of Statistics, 45, 1951–1987.
Robins, J., L. Li, E. Tchetgen, and A. van der Vaart (2008): “Higher order inﬂuence functions and minimax estimation of nonlinear functionals,” in Probability and Statistics: Essays in Honor of David A. Freedman, ed. by D. Nolan and T. Speed, Beachwood, Ohio, USA: Institute of Mathematical Statistics, vol. 2.
Robins, J., E. T. Tchetgen, L. Li, and A. van der Vaart (2009): “Semiparametric Minimax Rates,” Electronic Journal of Statistics, 3, 1305–1321.
Robins, J. M., A. Rotnitzky, and L. Zhao (1994): “Estimation of Regression Coeﬃcients When Some Regressors Are Not Always Observed,” Journal of the American Statistical Association, 89, 846–866.
——— (1995): “Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data,” Journal of the American Statistical Association, 90, 846–866.
Romano, J. P. (2004): “On non-parametric testing, the uniform behaviour of the t-test, and related problems,” Scandinavian Journal of Statistics, 31, 567–584.
Safran, I. and O. Shamir (2016): “Depth separation in relu networks for approximating smooth non-linear functions,” arXiv preprint arXiv:1610.09887.
Schmidt-Hieber, J. (2017): “Nonparametric regression using deep neural networks with ReLU activation function,” arXiv preprint arXiv:1708.06633.
Shalit, U., F. D. Johansson, and D. Sontag (2017): “Estimating individual treatment eﬀect: generalization bounds and algorithms,” arXiv preprint arXiv:1606.03976.
Sloczynski, T. and J. M. Wooldridge (2018): “A General Double Robustness Result for Estimating Average Treatment Eﬀects,” Econometric Theory, 34, 112–133.
Stone, C. J. (1982): “Optimal global rates of convergence for nonparametric regression,” The annals of statistics, 1040–1053.
Taddy, M., M. Gardner, L. Chen, and D. Draper (2015): “A nonparametric Bayesian analysis of heterogeneous treatment eﬀects in digital experimentation,” Arxiv preprint arXiv:1412.8563.
Tan, Z. (2018): “Model-assisted inference for treatment eﬀects using regularized calibrated estimation with high-dimensional data,” arXiv preprint arXiv:1801.09817.
45

Telgarsky, M. (2016): “Beneﬁts of depth in neural networks,” arXiv preprint arXiv:1602.04485.
Tsiatis, A. A. (2006): Semiparametric Theory and Missing Data, New York: Springer.
van de Geer, S., P. Buhlmann, Y. Ritov, and R. Dezeure (2014): “On Asymptotically Optimal Conﬁdence Regions and Tests for High-Dimensional Models,” The Annals of Statistics, 42, 1166–1202.
van der Laan, M. and S. Rose (2001): Targeted Learning: Causal Inference for Observational and Experimental Data, Springer-Verlag.
Wager, S. and S. Athey (2018): “Estimation and Inference of Heterogeneous Treatment Eﬀects using Random Forests,” Journal of the American Statistical Association, forthcoming.
Westreich, D., J. Lessler, and M. J. Funk (2010): “Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classiﬁers as alternatives to logistic regression,” Journal of clinical epidemiology, 63, 826–833.
White, H. (1989): “Learning in artiﬁcial neural networks: A statistical perspective,” Neural computation, 1, 425–464.
——— (1992): Artiﬁcial neural networks: approximation and learning theory, Blackwell Publishers, Inc.
Yarotsky, D. (2017): “Error bounds for approximations with deep ReLU networks,” Neural Networks, 94, 103–114.
——— (2018): “Optimal approximation of continuous functions by very deep ReLU networks,” arXiv preprint arXiv:1802.03620.
Zhang, C., S. Bengio, M. Hardt, B. Recht, and O. Vinyals (2016): “Understanding deep learning requires rethinking generalization,” arXiv preprint arXiv:1611.03530.
A Proofs
In this section we provide a proof of Theorems 1 and 2, our main theoretical results for deep ReLU networks, and their corollaries. The proof proceeds in several steps. We ﬁrst give the main breakdown and bound the bias (approximation error) term. We then turn our attention to the empirical process term, to which we apply our localization. Much of the proof uses a generic architecture, and thus pertains to both results. We will specialize the architecture to the multilayer perceptron only when needed later on. Other special cases and related results are covered in Section A.4. Supporting Lemmas are stated in Section B.
The statements of Theorems 1 and 2 assume that n is large enough. Precisely, we require n > (2eM )2 ∨ Pdim(FDNN). For notational simplicity we will denote fDNN := fˆ, see (2.4), and DNN := n, see Assumption 3. As we are simultaneously consider Theorems 1 and 2, the generic notation DNN will be used throughout.
46

A.1 Main Decomposition and Bias Term
Referring to Assumption 3, deﬁne the best approximation realized by the deep ReLU network class FDNN as
fn := arg min f − f∗ ∞.
f ∈FDNN f ∞≤2M
By deﬁnition, n := DNN := fn − f∗ ∞. Recalling the optimality of the estimator in (2.4), we know, as both fn and fˆ are in FDNN, that
−En[ (fˆ, z)] + En[ (fn, z)] ≥ 0.

This result does not hold for f∗ in place of fn, because f∗ ∈ FDNN. Using the above display and the curvature of Equation (2.1) (which does not hold with fn in place of f∗ therein), we obtain

c1 fˆ − f∗ 2L2(x) ≤ E[ (fˆ, z)] − E[ (f∗, z)] ≤ E[ (fˆ, z)] − E[ (f∗, z)] − En[ (fˆ, z)] + En[ (fn, z)] = E (fˆ, z) − (f∗, z) − En (fˆ, z) − (f∗, z) + En [ (fn, z) − (f∗, z)]

= (E − En) (fˆ, z) − (f∗, z) + En [ (fn, z) − (f∗, z)] .

(A.1)

Equation (A.1) is the main decomposition that begins the proof. The decomposition must be done this way because of the above notes regarding f∗ and fn. The ﬁrst term is the empirical process term that will be treated in the subsequent subsection. For the second term in (A.1), the bias term or approximation error, we apply Bernstein’s inequality to ﬁnd that, with probability at least 1 − e−γ˜,

En [ (fn, z) − (f∗, z)] ≤ E [ (fn, z) − (f∗, z)] +

2C2 fn − f∗ 2∞γ˜ + 21C M γ˜

n

3n

≤ c2E fn − f∗ 2 +

2C2 fn − f∗ 2∞γ˜ + 7C M γ˜

n

n

≤ c2

2 n

+

n

2C2γ˜ 7C M γ˜

+

,

n

n

(A.2)

using the Lipschitz and curvature of the loss function deﬁned in Equation (2.1) and E fn − f∗ 2∞, along with the deﬁnition of 2n.

fn − f∗ 2 ≤

Once the empirical process term is controlled (in Section A.2), the two bounds will be brought

back together to compute the ﬁnal result, see Section A.3.

47

A.2 Localization Analysis
We now turn to bounding the ﬁrst term in (A.1) (the empirical processes term) using a localized analysis that derives bounds based on scale insensitive complexity measure. The ideas of our localization are rooted in Koltchinskii and Panchenko (2000) and Bartlett et al. (2005), and related to Koltchinskii (2011). Localization analysis extending to the unbounded f case has been developed in Mendelson (2014); Liang et al. (2015). This proof section proceeds in several steps.
A key quantity is the Rademacher complexity of the function class at hand. Given i.i.d. Rademacher draws, ηi = ±1 with equal probability independent of the data, the random variable RnF , for a function class F , is deﬁned as
1n RnF := fsu∈Fp n ηif (xi).
i=1
Intuitively, RnF measures how ﬂexible the function class is for predicting random signs. Taking the expectation of RnF conditioned on the data we obtain the empirical Rademacher complexity, denoted Eη[RnF ]. When the expectation is taken over both the data and the draws ηi, ERnF , we get the Rademacher complexity.

A.2.1 Step I: Quadratic Process

The ﬁrst step is to show that, with high probability, the empirical L2 norm of the error (f −

f∗) is at most twice the population L2 norm bound for the same error, for certain functions f

outside a certain critical radius. This will be an ingredient to be used later on. Denote f n :=

1 n

n i=1

f

(xi

)2

1/2

to

be

the

empirical

L2

norm.

To

do

so,

we

study

the

quadratic

process

f − f∗ 2n − f − f∗ 2L2(x) = En(f − f∗)2 − E(f − f∗)2.

We will apply the symmetrization of Lemma 5 to g = (f − f∗)2 restricted to a radius f − f∗ L2(x) ≤ r. This function g has variance bounded as

V[g] ≤ E[g2] ≤ E((f − f∗)4) ≤ 9M 2r2.

Writing g = (f + f∗)(f − f∗), we see that by Assumption 1, |g| ≤ 3M |f − f∗| ≤ 9M 2, where the ﬁrst inequality veriﬁes that g has a Lipschitz constant of 3M (when viewed as a function of its argument f ), and second that g itself is bounded. We therefore apply Lemma 5, to obtain, with probability at least 1 − exp(−γ˜), that for any f ∈ F with f − f∗ L2(x) ≤ r,

En(f − f∗)2 − E(f − f∗)2

≤ 3ERn{g = (f − f∗)2 : f ∈ F , f − f∗ L2(x) ≤ r} + 3M r

2γ˜ 36M 2 γ˜

+

n

3n

≤ 18M ERn{f − f∗ : f ∈ F , f − f∗ L2(x) ≤ r} + 3M r

2γ˜ 12M 2γ˜

+

,

n

n

(A.3)

48

where the second inequality applies Lemma 2 to the Lipschitz functions {g} (as a function of the real values f (x)) and iterated expectations.
Suppose the radius r satisﬁes

r2 ≥ 18M ERn{f − f∗ : f ∈ F , f − f∗ L2(x) ≤ r}

(A.4)

and

6√6M 2γ˜

r2 ≥

.

n

Then we conclude from from (A.3) that

(A.5)

En(f − f∗)2 ≤ r2 + r2 + 3M r

2γ˜ + 12M 2γ˜ ≤ (2r)2

n

n

(A.6)

where the ﬁrst inequality uses (A.4) and the second line uses (A.5). This means that for r above the “critical radius” (see Step III), the empirical L2-norm is at most twice the population one with probability at least 1 − exp(−γ˜).

A.2.2 Step II: One Step Improvement
In this step we will show that given a bound on fˆ− f∗ L2(x) we can use this bound as information to obtain a tighter bound, if the initial bound is loose as made precise at the end of this step. This tightening will then be pursued to its limit in Step III, which leads to the ﬁnal rate obtained in Step IV. Step I will be used herein.
Suppose we know that for some r0, fˆ − f∗ L2(x) ≤ r0. We may always start with r0 = 3M given Assumption 1 and (2.4). Apply Lemma 5 with G := {g = (f, z) − (f∗, z) : f ∈ FDNN, f − f∗ L2(x) ≤ r0}, we ﬁnd that, with probability at least 1 − 2e−γ˜, the empirical process term of (A.1) is bounded as

(E − En) (fˆ, z) − (f∗, z) ≤ 6EηRnG +

2C2r02γ˜ + 23 · 3M C γ˜ ,

n

3n

(A.7)

where the middle term is due to the following variance calculation (recall Equation (2.1))

V[g] ≤ E[g2] = E[| (f, z) − (f∗, z)|2] ≤ C2E(f − f∗)2 ≤ C2r02

Here the fact that Lemma 5 is variance dependent, and that the variance depends on the radius r0, is important. It is this property which enables a sharpening of the rate with step-by-step reductions in the variance bound, as in Section A.2.4.
For the empirical Rademacher complexity term, the ﬁrst term of (A.7), Lemma 2, Step I, and

49

Lemma 3, yield

EηRnG = EηRn{g : g = (f, z) − (f∗, z), f ∈ FDNN, f − f∗ ≤ r0}

≤ 2C EηRn{f − f∗ : f ∈ FDNN, f − f∗ ≤ r0}

≤ 2C EηRn{f − f∗ : f ∈ FDNN, f − f∗ n ≤ 2r0}

12 2r0

≤ 2C inf 4α + √

0<α<2r0

nα

log N (δ, FDNN, · n)dδ

12 2r0

≤ 2C inf 4α + √

log N (δ, FDNN|x1,...,xn , ∞)dδ ,

0<α<2r0

nα

with probability 1 − exp(−γ˜) (when applying Step I). Recall Lemma 4, one can further upper bound the entropy integral when n > Pdim(FDNN),

12 2r0

inf 4α + √

0<α<2r0

nα

log N (δ, FDNN|x1,...,xn , ∞)dδ

≤ inf
0<α<2r0

12 2r0 4α + √
nα

2eM n Pdim(FDNN) log δ · Pdim(FDNN) dδ

≤ 32r0 Pdim(FDNN) log 2eM + 3 log n

n

r0 2

with a particular choice of α = 2r0 Pdim(FDNN)/n < 2r0. Therefore, whenever r0 ≥ 1/n and n ≥ (2eM )2,
EηRnG ≤ 128C r0 Pdim(FDNN) log n. n
Applying this bound to (A.7), we have

(E − En)

(fˆ, z) − (f∗, z) ≤ Kr0

Pdim(FDNN) log n + r0 n

2C2γ˜ 23M C γ˜

+

n

n

(A.8)

where K = 6 × 128C . Going back now to the main decomposition, plug (A.8) and (A.2) into (A.1), and we overall
have found that, with probability at least 1 − 4 exp(−γ˜), the following holds:

c1 fˆ − f∗ 2L2(x) ≤ Kr0 Pdim(FDNN) log n + r0
n



2C2γ˜ 23M C γ˜

2

+ n

n

+ c2 n + n



2C2γ˜ 7C M γ˜

+



n

n

 ≤ r0 · K

Pdim(FDNN) log n + n



2C 2 γ˜

2

n  + c2 n + n

2C 2 γ˜

γ˜

+ 30M C

n

n

50


√ ≤ r0 · K C

W L log W log n +
n



2C 2 γ˜

2

n  + c2 n + n

2C 2 γ˜

γ˜

+ 30M C .

n

n

(A.9)

The last line applies Lemma 6. Therefore, whenever n r0 and W L lnog W log n r0, the knowledge that fˆ − f∗ L2(x) ≤ r0 implies that (with high probability) fˆ − f∗ L2(x) ≤ r1, for r1 r0. One can recursively improve the bound r to a ﬁxed point/radius r∗, which describes the
fundamental diﬃculty of the problem. This is done in the course of the next two steps.

A.2.3 Step III: Critical Radius We now use the tightening of Step II to obtain the critical radius for this problem that is then used as an input in the ﬁnal rate derivation of Step IV. Formally, deﬁne the critical radius r∗ to be the largest ﬁxed point
r∗ = inf r > 0 : 18M ERn{f − f∗ : f ∈ F , f − f∗ L2(x) ≤ s} < s2, ∀s ≥ r .
By construction this obeys (A.4), and thus so does 2r∗. Denote the event E (depending on the data) to be

E = f − f∗ n ≤ 4r∗, for all f ∈ F and f − f∗ L2(x) ≤ 2r∗
and 1E to be the indicator that event E holds. We know from (A.6) that P(1E = 1) ≥ 1 − n−1,
√ provided r∗ ≥ 18M log n/n to satisfy (A.5).
We can now give an upper bound for the the critical radius r∗. Using the logic of Step II to bound the empirical Rademacher complexity, and then applying Lemma 6, we ﬁnd that

r∗2 ≤ 18M ERn f − f∗ : f ∈ F , f − f∗ L2(x) ≤ r∗

≤ 18M ERn f − f∗ : f ∈ F , f − f∗ L2(x) ≤ 2r∗
≤ 18M E EηRn{f − f∗ : f ∈ F , f − f∗ n ≤ 4r∗}1E + 3M (1 − 1E)

√ ≤ 36M K C · r∗
√ ≤ 72M K C · r∗

W L log W log n + 36M 2 1

n

n

W L log W log n,
n

√ with the last line relying on the above restriction that r∗ ≥ 18M log n/n. Dividing through by

r∗ yields the ﬁnal bound:

√ W L log W

r∗ ≤ 72M K C

log n. n

(A.10)

51

A.2.4 Step IV: Localization
We are now able to derive the ﬁnal rate using a localization argument. This applies the results of Step I and Step II repeatedly. Divide the space FDNN into shells of increasing radius by intersecting it with the L2 balls

B(f∗, r¯), B(f∗, 2r¯)\B(f∗, r¯), . . . B(f∗, 2lr¯)\B(f∗, 2l−1r¯)

(A.11)

where l ≥ 1 is chosen to be the largest integer no greater than log2 √ 2M . We will proceed to
(log n)/n
ﬁnd a bound on r¯ which determines the ﬁnal rate results.
Suppose r¯ > r∗. Then for each shell, Step I and the union bound imply that with probability at least 1 − 2l exp(−γ˜),

f − f∗ L2(x) ≤ 2jr¯ ⇒ f − f∗ n ≤ 2j+1r¯.

(A.12)

Further, suppose that for some j ≤ l

fˆ ∈ B(f∗, 2jr¯)\B(f∗, 2j−1r¯).

(A.13)

Then applying the one step improvement argument in Step II (again the variance dependence captured in Lemma 5 is crucial, here reﬂected in the variance within each shell), Equation (A.9) yields that with probability at least 1 − 4 exp(−γ˜),



fˆ − f 2 ≤ 1 2jr¯ · K√C

∗ L2(x) c1





W L log W log n +
n



2C 2 t

2

n  + c2 n + n



2C 2 γ˜

γ˜ 

+ 30M C

n n

≤ 22j−2r¯2,

if the following two conditions hold:

 1√
K C c1

W L log W log n +
n



2C 2 γ˜

1j

 ≤ 2 r¯

n

2



1 c2 2 + n

 c1

n



2C 2 γ˜

γ˜

1 2j 2

+ 26M C  ≤ 2 r¯ .

n

n4

It is easy to see that these two hold for all j if we choose

 8√ r¯ = K C c1

W L log W log n +
n

 2C 2 γ˜
+ n

2(c2 ∨ 1) n + c1

 120M C γ˜
c1 n  + r∗.

(A.14)

Therefore with probability at least 1 − 6l exp(−γ˜), we can perform shell-by-shell argument

52

combining the results in Step I and Step II:

fˆ − f∗ L2(x) ≤ 2lr¯ and fˆ − f∗ n ≤ 2l+1r¯ implies fˆ − f∗ L2(x) ≤ 2l−1r¯ and fˆ − f∗ n ≤ 2lr¯ ...... implies fˆ − f∗ L2(x) ≤ 20r¯ and fˆ − f∗ n ≤ 21r¯.

The “and” part of each line follows from Step I and the implication uses the above argument following Step II. Therefore in the end, we conclude with probability at least 1 − 6l exp(−γ˜),

fˆ − f∗ L2(x) ≤ r¯ , fˆ − f∗ n ≤ 2r¯ .

(A.15) (A.16)

Therefore choose γ = − log(6l) + γ˜, we know from (A.14), and the upper bound on r∗ in (A.10)

 8√ r¯ ≤ K C c1

W L log W log n +
n

 2C2(log log n + γ)
 n

 +  2(c2 ∨ 1) n +
c2

 120M C log log n + γ
c1 n  + r∗

W L log W

log log n + γ

≤ C n log n + n + n ,

(A.17)

with some constant C > 0 that does not depend on n. This completes the proof of Theorem 2.

A.3 Final Steps for the MLP case
For the multi-layer perceptron, W ≤ C · H2L, and plugging this into the bound (A.17), we obtain

H2L2 log(H2L)

log log n + γ

C n log n + n + n

To optimize this upper bound on r¯, we need to specify the trade-oﬀs in n and H and L. To do so, we utilize the MLP-speciﬁc approximation rate of Lemma 7 and the embedding of Lemma 1. Lemma 1 implies that, for any n, one can embed the approximation class FDNN given by Lemma 7 into a standard MLP architecture FMLP, where speciﬁcally

H

=

H(

n)

≤

W(

n)L(

n)

≤

C2

−d
n β (log(1/

n) + 1)2,

L = L( n) ≤ C · (log(1/ n) + 1).

53

For standard MLP architecture FMLP,

H2L2 log(H2L) ≤ C˜ ·

− 2d
n β (log(1/

n) + 1)7.

Thus we can optimize the upper bound

 r¯ ≤ C 


− 2d
β (log(1/ n) + 1)7

n

log n +

n

 log log n + γ
n + n

by choosing

n

=

n−

β 2(β+d)

,

H

d
·n 2(β+d)

log2

n,

L

· log n. This gives

r¯ ≤ C n− 2(ββ+d) log4 n + log log n + t . n

Hence putting everything together, with probability at least 1 − exp(−γ),
E(fˆ − f∗)2 ≤ r¯2 ≤ C n− β+β d log8 n + log log n + γ , n
En(fˆ − f∗)2 ≤ (2r¯)2 ≤ 4C n− β+β d log8 n + log log n + γ . n
This completes the proof of Theorem 1.

A.4 Proof of Corollaries 1 and 2

For Corollary 1, we want to optimize

W Lnlog U log n + log lognn + γ + 2DNN.

Yarotsky (2017, Theorem 1) shows that for the approximation error DNN to obey DNN ≤ , it

suﬃces to choose W, U ∝

−

d β

(log

(1/

)

+

1)

and

L

∝

(log(1/

)

+

1),

given

the

speciﬁc

architecture

described therein. Therefore, we attain n−β/(2β+d) by setting W, U nd/(2β+d) and L log n,

yielding the desired result.

For Corollary 2, we need to optimize

H2L2 log(HL)

log log n + γ 2

n

log n +

n

+ MLP.

Yarotsky (2018, Theorem 1) shows that for the approximation error MLP to obey MLP ≤ , it

suﬃces to choose H ∝ 2d + 10 and L ∝

−

d 2

,

given

the

speciﬁc

architecture

described

therein.

Thus, for n−1/(2+d) we take L n−d/(4+2d), and the result follows.

54

B Supporting Lemmas
First, we show that one can embed a feedforward network into the multi-layer perceptron architecture by adding auxiliary hidden nodes. This idea is due to Yarotsky (2018). Lemma 1 (Embedding). For any function f ∈ FDNN, there is a g ∈ FMLP, with H ≤ W L + U , such that g = f .
Figure 7: Illustration of how to embed a feedforward network into a multi-layer perceptron, with auxiliary hidden nodes (shown in yellow). Proof. The idea is illustrated in Figure 7. For the edges in the directed graph of f ∈ FDNN that connect nodes not in adjacent layers (shown in yellow in Figure 7), one can insert auxiliary hidden units in order to simply “pass forward” the information. The number of such auxiliary “passforward units” is at most the number of oﬀending edges times the depth L (i.e. for each edge, at most L auxiliary nodes are required), and this is bounded by W L. Therefore the width of the MLP network that subsumes the original is upper bounded by W L + U while still maintaining the required embedding that for any fθ ∈ FDNN, there is a gθ ∈ FMLP such that gθ = fθ. In order to match modern practice we only need to show that auxiliary units can be implemented with ReLU activation. This can be done by setting the constant (“bias”) term b of each auxiliary unit large enough to ensure σ(x˜ w + b) = x˜ w + b, and then subtracting the same b in the last receiving unit along the path.
Next, we give two properties of the Rademacher complexity that we require (see Mendelson, 2003). Lemma 2 (Contraction). Let φ : R → R be a Lipschitz function |φ(f1) − φ(f2)| ≤ L|f1 − f2|, then
EηRn{φ ◦ f : f ∈ F } ≤ 2LEηRnF .
55

Lemma 3 (Dudley’s Chaining). Let N (δ, F, · n) denote the metric entropy for class F (with covering radius δ and metric · n), then

12 r

EηRn{f : f ∈ F , f n ≤ r} ≤ inf 4α + √

log N (δ, F , · n)dδ .

0<α<r

nα

Furthermore, because f n ≤ maxi |f (xi)|, and therefore N (δ, F , · n) ≤ N (δ, F |x1,...,xn, ∞) and so the upper bound in the conclusions also holds with N (δ, F |x1,...,xn, ∞).

The next two results, Theorems 12.2 and 14.1 in Anthony and Bartlett (1999), show that the metric entropy may be bounded in terms of the pseudo-dimension and that the latter is bounded by the Vapnik-Chervonenkis (VC) dimension.

Lemma 4. Assume for all f ∈ F, f ∞ ≤ M . Denote the pseudo-dimension of F as Pdim(F), then for n ≥ Pdim(F), we have for any δ,

N (δ, F |x1,...,xn , ∞) ≤

2eM · n Pdim(F) .
δ · Pdim(F)

The following symmetrization lemma bounds the empirical processes term using Rademacher complexity, and is thus a crucial piece of our localization. This is a standard result based on Talagrand’s concentration, but here special care is taken with the dependence on the variance.

Lemma 5 (Symmetrization, Theorem 2.1 in Bartlett et al. (2005)). For any g ∈ G, assume that |g| ≤ G and V[g] ≤ V . Then for every γ > 0, with probability at least 1 − e−γ

sup {Eg − Eng} ≤ 3ERnG +
g∈G

2V γ 4G γ

+

,

n 3n

and with probability at least 1 − 2e−t

sup {Eg − Eng} ≤ 6EηRnG +
g∈G

2V γ 23G γ

+

.

n

3n

The same result holds for supg∈G {Eng − Eg}.
When bounding the complexity of FDNN, we use the following result. Bartlett et al. (2017) also verify these bounds for the VC-dimension.

Lemma 6 (Theorem 6 in Bartlett et al. (2017), ReLU case). Consider a ReLU network architecture F = FDNN(W, L, U ), then the pseudo-dimension is sandwiched as

c · W L log(W/L) ≤ Pdim(F) ≤ C · W L log W,

with some universal constants c, C > 0.

56

For multi-layer perceptrons we use the following approximation result, Theorem 1 of Yarotsky (2017).

Lemma 7. There exists a network class FDNN, with ReLU activation, such that for any > 0:
(a) FDNN approximates the W β,∞([−1, 1]d) in the sense for any f∗ ∈ W β,∞([−1, 1]d), there exists a fn( ) := fn ∈ FDNN such that fn − f∗ ∞ ≤ ,

(b)

and FDNN has L( ) ≤ C · (log(1/ ) + 1) and W ( ), U ( ) ≤ C ·

−

d β

(log

(1/

)

+

1).

Here C only depends on d and β.

For completeness, we verify the requirements on the loss functions, Equation (2.1), for several examples. We ﬁrst treat least squares and logistic losses, in slightly more detail, as these are used in our subsequent inference results and empirical application.

Lemma 8. Both the least squares (2.2) and logistic (2.3) loss functions obey the requirements of
Equation (2.1). For least squares, c1 = c2 = 1/2 and C = M . For logistic regression, c1 = (2(exp(M ) + exp(−M ) + 2))−1, c2 = 1/8 and C = 1.

Proof. The Lipschitz conditions are trivial. For least squares, using iterated expectations

2E (f, Z) − 2E (f∗, Z) = E −2Y f + f 2 + 2Y f∗ − f∗2 = E −2f∗f (x) + f 2 + 2(f∗)2 − f∗2 = E (f − f∗)2 .

For logistic regression,

E[ (f, Z)] − E[ (f∗, Z)] = E − exp(f∗) (f − f∗) + log 1 + exp(f ) .

1 + exp(f∗)

1 + exp(f∗)

Deﬁne ha(b) = − 1+exepx(pa()a) (b − a) + log 11++eexxpp((ab)) , then

ha(b) = ha(a) + ha(a)(b − a) + 12 ha (ξa + (1 − ξ)b) (b − a)2

and

ha(b)

=

1 exp(b)+exp(−b)+2

≤

14 .

The

lower

bound

holds

as

|ξf∗

+ (1 − ξ)f |

≤

M.

Beyond least squares and logistic regression, we give three further examples, discussed in the general language of generalized linear models. Note that in the ﬁnal example we move beyond a simple scalar outcome.

Lemma 9. For a convex function g(·) : R → R, consider the generalized linear loss function (f, z) = − y, f (x) + g(f (x)). The curvature and the Lipschitz conditions in (2.1) will hold given speciﬁc g(·). In each case, the loss function corresponds to the negative log likelihood function.

57

(a) Poisson: g(t) = exp(t), with f∗(x) = log E[y|X = x]. (b) Gamma: g(t) = − log t, with f∗(x) = −(E[y|X = x])−1. (c) Multinomial Logistic, K + 1 classes: g(t) = log(1 + k∈K exp(t[k])), with
exp(f∗[k](x))/(1 + exp(f∗[k ](x))) = E[y[k]|X = x].
k ∈K
Here v[k] denotes the k-th coordinate of a vector v.
Proof. Denote ∇g, Hessian[g] to be the gradient and Hessian of the convex function g. By the convexity of g, the optimal f∗ satisﬁes E[∂ (f∗, Z)/∂f |X = x] = 0, which implies

∇g(f∗) = E[Y |X = x].

If 2c0 Hessian[g(f )] 2c1 for all f of interest, then the curvature condition in (2.1) holds, because

E[ (f, Z)] − E[ (f∗, Z)] = E[− ∇g(f∗), f − f∗ + g(f ) − g(f∗)]

=

1 E

f

−

f∗, Hessian[g(f˜)]f

−

f∗

2

≥ c0E f − f∗ 2,

and the parallel argument for ≤ c1E f −f∗ 2. The Lipschitz condition is equivalent to ∇g(f ) ≤ C for all f of interest, with bounded Y .
For our three examples in particular, we have the following.

(a) For Poisson regression:

∇c(f ) = | exp(f )| ≤ exp(M ),

Hessian[c(f )] = exp(f ) ∈ [exp(−M ), exp(M )].

(b) For Gamma regression, bounding −Y above and below is equivalent to 1/M ≤ f ≤ M and therefore:
∇c(f ) = |1/f | ≤ M, Hessian[c(f )] = 1/f 2 ∈ [1/M 2, M 2].

(c) For multinomial logistic regression, with general K, we know

∇c(f ) ≤ 1 Hessian[c(f )] = diag{z} − zz

where z[k] := exp(f [k]) . 1 + k f [k ]

58

One can easily verify that the eigenvalues are bounded in the following sense, for bounded f ,

1

exp(M )

(1 + K

exp(M ))2

≤

λ(Hessian[c(f )])

≤

1 + (K

. − 1) exp(−M ) + exp(M )

This completes the proof.

Our last result is to verify condition (c) of Theorem 3. We do so using our localization, which may be of future interest in second-step inference with machine learning methods.

Lemma 10. Let the conditions of Theorem 3 hold. Then

En (µˆt(xi) − µt(xi)) 1 − 1{ti = t}
P[T = t|X = xi]

= oP

n− β+β d log8 n + log log n n

= oP n−1/2 .

Proof. Without loss of generality we can take p¯ < 1/2. The only estimated function here is µt(x), which plays the role of f∗ here. For function(als) L(·) of the form
L(f ) := (f (xi) − f∗(xi)) 1 − 1{ti = t} ,
P[T = t|X = xi]

it is true that

E[L(f )] = E (f (X) − f∗(X)) 1 − E[1{ti = t}|xi] = 0
P[T = t|X = xi]

and

V[L(f )] ≤ (1/p¯ − 1)2 E (f (X) − f∗(X))2 ≤ (1/p¯ − 1)2 r¯2 |L(f )| ≤ (1/p¯ − 1) 2M.

For r¯ deﬁned in (A.14),
18M ERn{f − f∗ : f ∈ F , f − f∗ L2(x) ≤ r¯} ≤ r¯2 ERn{L(f ) : f ∈ F , f − f∗ L2(x) ≤ r¯} ≤ 2 (1/p¯ − 1) ERn{f − f∗ : f ∈ F , f − f∗ L2(x) ≤ r¯}
where the ﬁrst line is due to r¯ > r∗, and second line uses Lemma 2. Then by the localization analysis and Lemma 5, for all f ∈ F , f − f∗ L2(x) ≤ r¯, L(f ) obeys

2

4 (1/p¯ − 1)2 t 8 (1/p¯ − 1) 3M t

2

En[L(f )] = En[L(f )] − E[L(f )] ≤ 6Cr¯ + r¯

+

≤ 4Cr¯

n

3

n

≤ C · n− β+β d log8 n + log log n , n

sup

En[L(f )] ≤ C · n− β+β d log8 n + log log n .

f ∈F , f −f∗ L2(x)≤r¯ n

59

With

probability

at

least

1

−

d
exp(−n β+d

log8

n),

fˆMLP

lies

in

this

set

of

functions,

and

therefore

En[L(fˆMLP)] = En (fn,H,L(x) − f∗(x)) 1 − 1(T = t) P (T = t|x = x)

≤ C · n− β+β d log8 n + log log n , n

as claimed.

60

