Making Human-Like Trade-offs in Constrained Environments by Learning from Demonstrations
Arie Glazier1 Andrea Loreggia2 Nicholas Mattei1 Taher Rahgooy3 Francesca Rossi3 K. Brent Venable3,5
1 Tulane University, New Orleans, LA, USA 2 European University Institute, Italy
3 University of West Florida, Pensacola, FL, USA 4 IBM Research, Yorktown Heights, NY, USA
5 Institute for Human and Machine Cognition, Pensacola, FL, USA adglazier@gmail.com, andrea.loreggia@gmail.com, nsmattei@tulane.edu trahgooy@students.uwf.edu, Francesca.Rossi2@ibm.com, bvenable@ihmc.org

arXiv:2109.11018v1 [cs.AI] 22 Sep 2021

Abstract
Many real-life scenarios require humans to make difﬁcult trade-offs: do we always follow all the trafﬁc rules or do we violate the speed limit in an emergency? These scenarios force us to evaluate the trade-off between collective norms and our own personal objectives. To create effective AI-human teams, we must equip AI agents with a model of how humans make trade-offs in complex, constrained environments. These agents will be able to mirror human behavior or to draw human attention to situations where decision making could be improved. To this end, we propose a novel inverse reinforcement learning (IRL) method for learning implicit hard and soft constraints from demonstrations, enabling agents to quickly adapt to new settings. In addition, learning soft constraints over states, actions, and state features allows agents to transfer this knowledge to new domains that share similar aspects. We then use the constraint learning method to implement a novel system architecture that leverages a cognitive model of human decision making, multi-alternative decision ﬁeld theory (MDFT), to orchestrate competing objectives. We evaluate the resulting agent on trajectory length, number of violated constraints, and total reward, demonstrating that our agent architecture is both general and achieves strong performance. Thus we are able to capture and replicate human-like trade-offs from demonstrations in environments when constraints are not explicit.
1 Introduction
Implicit and explicit constraints are present in many decision making scenarios, and they force us to make difﬁcult decisions: do we always satisfy all constraints, or do we violate some of them in exceptional circumstances? Many techniques can be used to combine constraints and goals so that the agent rationally minimizes constraint violations while achieving the given goal (Noothigattu et al. 2019). However, it is well known that humans are not rational. When we need to make a decision in a constrained environment, we often reason by employing heuristics and approximations which are subject to bias and noise (Busemeyer and Diederich 2002; Booch et al. 2021). This means that opti-

mal techniques may not be suitable if the aim is to design autonomous artiﬁcial agents that act like humans, or decision support systems that simulate human behavior to anticipate it and possibly alert humans by making them aware of their reasoning and inference deﬁciencies.
Moreover, these constraints are often not explicitly given, but need to be inferred from observations of how other agents act in the constrained world. Learning constraints from demonstrations is an important topic in the domains of inverse reinforcement learning (Scobee and Sastry 2020; Abbeel and Ng 2004), which is used to implement AI safety goals including value alignment (Russell, Dewey, and Tegmark 2015) and to circumvent reward hacking (Amodei et al. 2016; Ray, Achiam, and Amodei 2019). Recent work has focused on building ethically bounded agents (Svegliato, Nashed, and Zilberstein 2021; Rossi and Mattei 2019) that comply with ethical or moral theories of action. Following the work of Scobee and Sastry (2020), we propose an architecture that, given access to a model of the environment and to demonstrations of constrained behavior, is able to learn constraints associated with states, actions, or state features. Our method, MESC-IRL, performs comparably with the state of the art and is more general, as it can handle both hard and soft constraints in both deterministic and non-deterministic environments. It is also decomposable into features of the environment, supporting the transfer of learned constraints between environments.
Once the constraints are learned, we turn our attention to making human like trade-offs. Enabling agents to make trade-offs like humans allows them to mirror human behavior or to draw human attention to situations where decision making could be improved (Booch et al. 2021). Additionally, making these trade-offs explicit enables decision support tools that are able to mirror the goals of human decision makers (Balakrishnan et al. 2018, 2019). We propose a novel orchestration technique leveraging Multi-Alternative Decision Field Theory (MDFT) (Busemeyer and Diederich 2002), a decision making framework which is based on a psychological theory of how humans make decisions that is

10 0 10 20 30 40 50 s0

Green

Blue

sG

No color

Figure 1: Grid with constraints of varying costs over actions (top right), state occupancy (grid, left) and state features (bottom right). Note that πN∗ for MN is the red trajectory, obtained by an agent that does not know the constraints while πC∗∗ for MC∗ is the blue trajectory.
able to capture deviations from rationality observed in humans, making trade-offs between competing objectives in a more human-like way. We compare this MDFT-based orchestrator with other methods both theoretically and empirically, showing that our architecture is theoretically more expressive and obtains better empirical performance across a range of metrics when acting in constrained environments. The goal here is to use a cognitive model to capture the sometimes irrational decisions made by humans. Building machines that act more like humans is a step to create effective human-machine teams or decision support systems.
2 Preliminaries and Related Work
We begin this section by providing the preliminary notions on the context of our work, that is, constrained Markov Decision Processes and Reinforcement Learning (Sutton and Barto 2018). We then review fundamental concepts and methods on Inverse Reinforcement Learning (Ng and Russell 2000; Abbeel and Ng 2004) and background on Constrained Markov Decision Processes (Altman 1999) including related work on learning constraints (Ziebart et al. 2008; Malik et al. 2021; Scobee and Sastry 2020), which we will leverage to develop our novel method for learning soft constraints (Rossi, Van Beek, and Walsh 2006) from demonstrations (Chou, Berenson, and Ozay 2018). We conclude this section presenting a short review of the Multi-Alternative Decision Field Theory (Busemeyer and Diederich 2002), the cognitive model of decision making which will be at the core of our novel approach to orchestrating competing objectives.
2.1 Markov Decision Processes and Reinforcement Learning
A ﬁnite-horizon Markov Decision Process (MDP) M is a model for sequential decision making over a number of time steps t ∈ T deﬁned by a tuple (S, A, P, D0, φ, γ, R) (Sutton and Barto 2018). S is a ﬁnite set of discrete states; {As} ⊆ A is a set of actions available at state s; P : S × A × S → [0, 1] is a model of the environment given

as transition probabilities where P (st+1|st, at) is the proba-

bility of transitioning to state st+1 from state st after taking

action at ∈ {Ast } at time t. D0 : S → [0, 1] is a distribution over start states; φ : S × A × S → Rk is a mapping from the

transitions to a k-dimensional space of features; γ ∈ [0, 1) is

a discount factor; and R : S × A × S → R is a scalar reward

received by the agent for being in one state and transitioning

to another state at time t, written as R(st, at, st+1).

An agent acts within the environment deﬁned by the MDP,

generating a sequence of actions called a trajectory of length

t. Let τ = ((s1, a1, s2), ..., (st−1, at−1, st)) ∈ (S ×A×S)t.

We evaluate the quality of a particular trajectory in terms

of the amount of reward accrued over the trajectory, subject

to discounting. Formally, R(τ ) =

t i=1

γ

i

R(si

,

ai

,

si+1

).

A policy, π : S → P(A) is a map of probability dis-

tribution to actions for every state such that π(s, a) is the

probability of taking action a in state s. We can also write

the probability of a trajectory τ under a policy as π(τ ).

The feature vector associated with trajectory τ is deﬁned

as the summation over all transition feature vectors in τ ,

φ(τ ) = (st,at,st+1)∈τ φ(st, at, st+1) The goal within an MDP is to ﬁnd a policy π∗ that max-

imizes the expected reward, J(π) = Eτ∼π[R(τ )] (Malik

et al. 2021). In the MDP literature, classical tabular meth-

ods are used to ﬁnd π∗ including value iteration (VI). Such

method ﬁnds an optimal policy by estimating the expected

reward for taking an action a in a given state s, i.e., the Q-

value of pair (s, a), written q(s, a). (Sutton and Barto 2018).

2.2 Constrained MDPs and Inverse Reinforcement Learning
We are interested in learning constraints from demonstrations. Our goal is to create agents that are able to be trained to follow constraints that are not explicitly prohibited in the MDP, but should be avoided (Rossi and Mattei 2019). (Scobee and Sastry 2020) discusses the importance of such constraints: an MDP M may encode everything necessary about driving a car, e.g. the dynamics of steering and movements, but often one wants to add additional general constraints such as avoid obstacles on the way to the goal. These constraints are often non-Markovian and engineering a reward function that encodes these constraints may be a difﬁculty or impossible task (Vazquez-Chanlatte et al. 2018).
One approach for learning constraints from demonstrations is to use techniques from inverse reinforcement learning (IRL): given a set of demonstrated trajectories D of an agent in an environment M with an unknown reward function M \ R, IRL provides a set of techniques for learning a reward function Rˆ that explains the agent’s demonstrated behavior (Abbeel and Ng 2004; Ng and Russell 2000). However, this technique has many drawbacks: often there are many reward functions that lead to the same behavior (Scobee and Sastry 2020), the reward functions may not be interpretable (Vazquez-Chanlatte et al. 2018), and there may be issues such as reward hacking – wherein the agent learns to behave in ways that create reward but are not intended by the designer – an important topic in the ﬁeld of AI safety (Amodei et al. 2016; Ray, Achiam, and Amodei

2

2019) and value alignment (Rossi and Mattei 2019; Russell,

Dewey, and Tegmark 2015).

We follow the framework of Altman (1999) and Malik

et al. (2021) and deﬁne a Constrained MDP MC which is a

nominal MDP MN with an additional cost function C : S ×

A × S → R and a budget α ≥ 0. We can then deﬁne the cost

of a trajectory to be c(τ ) =

t i=1

c(si, ai, si+1).

Setting

α = 0 is enforcing hard constraints, i.e., we must never

trigger constrained transitions. In this work, unlike the work

of both Scobee and Sastry (2020) and Malik et al. (2021), we

are interested in learning soft constraints (Rossi, Van Beek,

and Walsh 2006). Under a soft constraints paradigm, each

constraint comes with a real-valued penalty/cost and the goal

is to minimize the sum of penalties incurred by the agent.

Following Scobee and Sastry (2020), the task of con-

straint inference in IRL is deﬁned as follows. Given a nomi-

nal MDP MN and a set of demonstrations D in ground-truth constrained world MC∗ , we wish to ﬁnd the most likely

set of constraints C that could modify MN to explain the

demonstrations. We are concerned with three types of con-

straints:

Action Constraints. We may not want an agent to ever perform some (set of) action ai.
Occupancy Constraints. We may not want an agent to occupy a (set of) states si.
Feature Constraints. Given a feature mapping of transitions φ, we may not want an agent to perform an (set of) action in presence of speciﬁc state features.

Without loss of generality, we add the state and actions to the features. Hence, action and occupancy become speciﬁc cases of feature constraints. Note that the set of constraints is deﬁned as a cost function over the set of transitions C : S × A × S → R. In Scobee and Sastry (2020), this deﬁnition is limited to a set of state-actions S × A as they are assuming a deterministic setting and hence are able to deﬁne MC by substituting A = {As} with AC = {ACs } in MN . Finally, both Scobee and Sastry (2020) and Malik et al. (2021) propose a greedy approach to infer a set of constraint C that explains the demonstrations D on MC∗ . In both Scobee and Sastry (2020) and Malik et al. (2021) the domain is restricted to deterministic MDPs, which we strictly generalize in this work; additionally Scobee and Sastry (2020), like our model, only works with discrete actions, while Malik et al. (2021) works for both discrete and continuous action sets. We also generalize to the non-deterministic setting; we additionally generalize to the setting of soft constraints, hence our task is to learn the cost function C.
To test our methods, we use the same grid world setup as Scobee and Sastry (2020). Within our grid world example, shown in Figure 1, we ha√ve an action penalty of −4 for the cardinal directions, −4 × 2 for taking the diagonal actions, and reaching the goal state has a reward of 10. In Figure 1 we set the constraint costs to various values but in all our experiments we ﬁx the constraint costs on the generated grids for states, actions, and features to be −50. Throughout we assume a non-deterministic world with a 10% chance of action failure, resulting in a random action.

2.3 Multi-Alternative Decision Field Theory
Multi-alternative Decision Field Theory (MDFT) is a dynamic cognitive approach that models human decision making based on psychological principles (Busemeyer and Diederich 2002; Roe, Busemeyer, and Townsend 2001). MDFT models preferential choice as an iterative cumulative process in which at each time instant the decision maker attends to a speciﬁc attribute to derive comparisons among options and update their preferences accordingly. Ultimately the accumulation of those preferences informs the decision maker’s choice. In MDFT an agent is confronted with multiple options and equipped with an initial personal evaluation for them along different criteria, called attributes. For example, a student who needs to choose a main course among those offered by the cafeteria will have in mind an initial evaluation of the options in terms of how tasty and healthy they look. More formally, an MDFT model is composed of the following (Roe, Busemeyer, and Townsend 2001):
Personal Evaluation: Given set of options O = {o1, . . . , ok} and set of attributes A = {A1, . . . , AJ }, the subjective value of option oi on attribute Aj is denoted by mij and stored in matrix M. In our example, let us assume that the cafeteria options are Salad (S), Burrito (B) and Vegetable pasta (V). Matrix M, containing the student’s preferences, could be deﬁned as shown in Figure 2 (left), where rows correspond to the options (S, B, V ) and the columns to the attributes T aste and Health.
Figure 2: Evaluation (M), Contrast (C), and Feedback (S) matrix.
Attention Weights: Attention weights are used to express the attention allocated to each attribute at a particular time t during the deliberation. We denote them by vector W(t) where Wj(t) represents the attention to attribute j at time t. We adopt the common simplifying assumption that, at each point in time, the decision maker attends to only one attribute (Roe, Busemeyer, and Townsend 2001). Thus, Wj(t) ∈ {0, 1} and j Wj(t) = 1, ∀t, j. In our example, we have two attributes, so at any point in time t we will have W(t) = [1, 0], or W(t) = [0, 1], representing that the student is attending to, respectively, T aste or Health. The attention weights change across time according to a stationary stochastic process with probability distribution w, where wj is the probability of attending to attribute Aj. In our example, deﬁning w1 = 0.55 and w2 = 0.45 would mean that at each point in time, the student will be attending T aste with probability 0.55 and Health with probability 0.45. In other words, T aste matters slightly more than Health.
Contrast Matrix: Contrast matrix C is used to compute the advantage (or disadvantage) of an option with respect to the other options. In the MDFT literature (Busemeyer and Townsend 1993; Roe, Busemeyer, and Townsend 2001; Hotaling, Busemeyer, and Li 2010), C is deﬁned by contrasting the initial evaluation of one alternative against the average of the evaluations of the others, as shown for the case with three options in Figure 2 (center).

3

At any moment in time, each alternative in the choice set is associated with a valence value. The valence for option oi at time t, denoted vi(t), represents its momentary advantage (or disadvantage) when compared with other options on some attribute under consideration. The valence vector for k options o1, . . . , ok at time t, denoted by column vector V(t) = [v1(t), . . . , vk(t)]T , is formed by V(t) = C × M × W(t). In our example, the valence vector at any time point in which W(t) = [1, 0], is V(t) = [1 − 7/2, 5 − 3/2, 2 − 6/2]T .
In MDFT, preferences for each option are accumulated across the iterations of the deliberation process until a decision is made. This is done by using Feedback Matrix S, which deﬁnes how the accumulated preferences affect the preferences computed at the next iteration. This interaction depends on how similar the options are in terms of their initial evaluation expressed in M. Intuitively, the new preference of an option is affected positively and strongly by the preference it had accumulated so far, while it is inhibited by the preference of similar options. This lateral inhibition decreases as the dissimilarity between options increases. Figure 2 (right) shows S for our example following the MDFT method in (Hotaling, Busemeyer, and Li 2010).
At any moment in time, the preference of each alternative is calculated by P(t+1) = S×P(t)+V(t+1) where S×P(t) is the contribution of the past preferences and V(t + 1) is the valence computed at that iteration. Starting with P(0) = 0, preferences are then accumulated for either a ﬁxed number of iterations (and the option with the highest preference is selected) or until the preference of an option reaches a given threshold. In the ﬁrst case, MDFT models decision making with a speciﬁed deliberation time, while, in the latter, it models cases where deliberation time is unspeciﬁed and choice is dictated by the accumulated preference magnitude. In general, different runs of the same MDFT model may return different choices due to the attention weights’ distribution. In this way MDFT induces choice distributions over set of options and is capable of capturing well know behavioral effects such as the compromise, similarity, and attraction effects that have been observed in humans and that violate rationality principles (Busemeyer and Townsend 1993).
3 Learning Soft Constraints From Demonstrations
We now describe our method for learning a set soft constraints from a set of demonstrations D and a nominal MDP MN . This is the ﬁrst step in our goal of providing a ﬂexible and expressive method for orchestrating trade-offs. The method described here generalizes the work of both Scobee and Sastry (2020) and Malik et al. (2021) to the setting of non-deterministic MDPs and soft constraints.
3.1 MESC-IRL: Max Entropy Inverse Soft-Constraint Reinforcement Learning
Following Ziebart et al. (2008), our goal is to optimize a function that linearly maps the features of each transition to the reward associated with that transition, R(st, at, st+1) = ωφ(st, at, st+1), where ω is the reward weight vector. Ziebart et al. (2008) propose a maximum entropy model for

ﬁnding a unique solution (ω) for this problem. Based on this model, the probability of ﬁnite-length trajectory τ being executed by an agent traversing an MDP M is exponentially proportional to the reward earned by that trajectory and can be approximated by:

P (τ |ω) ≈ eωT φ(τ)
Z(ω)

(st,at,st+1)∈τ P (st+1|st, at).

The optimal solution is obtained by ﬁnding the maximum likelihood of the demonstrations D using this probability distribution: ω∗ = argmax τ∈D log P (τ |ω).
ω
We extend the problem deﬁned in Scobee and Sas-
try (2020) to that of learning a set of soft constraints which
best explain a set of observed demonstrations. This allows
us to move from the notion of a constraint forbidding an
action or a state to that of a soft constraint imposing a
penalty proportional to the gravity of its violation. In other words, given access to MN and a set of demonstrations D in ground-truth constrained MDP MC∗ we want to ﬁnd the costs C. More formally, we deﬁne the residual reward function RR : S × A × S → R+ as a mapping from the transitions to the penalties. We can now formally deﬁne our softconstrained MDP MC as follows:
Deﬁnition 1 Given MN = S, A, P, µ, φ, RN we deﬁne soft-constrained MDP MC = S, A, P, µ, φ, RC where RC = RN − RR.
Thus, the goal of our task is to ﬁnd a residual reward function RR that maximizes the likelihood of the demonstrations D given the nominal MDP MN .
Our solution is based on adapting Maximum Causal
Entropy Inverse Reinforcement learning (Ziebart et al.
2008; Ziebart, Bagnell, and Dey 2010) to soft-constrained
MDPs. Following the setting of Ziebart et al. (2008) we can write the reward function RN (resp. RC) of MN (resp. MC) as a linear combination of the transitions: RN (st, at, st+1) = ωN φ(st, at, st+1) and RC(st, at, st+1) = ωCφ(st, at, st+1). As, both reward functions RN and RC are linear, RR should be linear as well RR = ωRφ(st, at, st+1). From this formulation of RR we can infer that the reward vectors follow ωC = ωN − ωR.
At this point we can use Max Entropy IRL for learning a reward function compatible with the trajectories in D. The
gradient for maximizing the likelihood in this setting is de-
ﬁned as in Ziebart et al. (2008):

(1) ∇ωC L(D) = ED[φ(τ )] − (st,at,st+1) Dst,at,st+1 φ(st, at, st+1)
Where Dst,at,st+1 is the expected feature frequencies for transition (st, at, st+1) using the current ωC weights. Given that the reward vectors follow ωC = ωN − ωR, we can write ∇ωC = −∇ωR . Finally, by substituting this in Eq. 1 we obtain the gradient of likelihood of the constrained trajectories with respect to ωR:

(2) ∇ωR L(D) = (st,at,st+1) Dst,at,st+1 φ(st, at, st+1) − ED[φ(τ )]

As we estimate the residual rewards with respect to the nominal rewards, these rewards are automatically scaled to be compatible with the nominal rewards.

4

False Positive Rate KL-Divergence

0.5 0.4 0.3 0.2 0.1 0.0
0

Scobee(dkl = 0.1) MESC-IRL( 0.4) MESC-IRL( 0.5) MESC-IRL( 0.6) MESC-IRL( 0.7)

20

40

60

80

100

Number of Demonstrations

10 8 6 4 2 0
0

Scobee(dkl = 0.1) MESC-IRL( 0.4) MESC-IRL( 0.5) MESC-IRL( 0.6) MESC-IRL( 0.7)

20

40

60

80

100

Number of Demonstrations

Figure 3: Comparision of MESC-IRL to the best performing method of Scobee and Sastry (2020) at recovering hard constraints in a deterministic setting according to false positive rate (left) and KL-Divergence from the demonstrations D (right) as we vary the number of demonstrations. Each point is the mean of 10 independent draws.

3.2 Generalizing From Penalties to Probabilities

The estimated penalties from the previous section can effec-

tively guide an agent to navigate the environment optimally

as well as provide estimates of the cost of the constraints

scaled to the value of the original reward signal. However,

there may be instances, such as when comparing with hard

constraints, where we desire probabilities that a particular

action is constrained. Having probabilities allows us to com-

pare constraints across environments with possibly different

scales, allows us to use this information to guide our poli-

cies, and allows us to evaluate the conﬁdence we have in a

particular constraint. In this section we describe a method to

transition from penalties to probabilities, as well as a gener-

alized method to extract these probabilities based on a sub-

set of the features of the environment, which can facilitate

transfer learning between domains.

Intuitively, a transition where the residual reward, i.e., the

penalty, is signiﬁcantly larger than zero is more likely to be a

constraint. We estimate the signiﬁcance of a penalty by scal-

ing it to the standard deviation of the mean learned reward.

Therefore, we assume that a transition penalty is a random

variable, denoted by C ∼ logistic(σpooled, σpooled), follow-

ing a logistic distribution with standard deviation σpooled,

where σpooled =

(σ

2 N

+σ

2 C

)/2

and

σN

and

σC

are

the

stan-

dard deviations of the rewards in the nominal and learned

constrained worlds, respectively. Informally, when penalties

are close to zero, we want their probabilities to be small. To

do this we set the mean of the distribution to be µ = σpooled.

We now want to reason about a random variable ζ that

indicates our belief that the transition (st, at, st+1) is for-

bidden. Hence using the above probability distribution we

can deﬁne the probability of constraint given a transition as:

ζ ≡ P C ≤ RR(st, at, st+1) = sigmoid RR(st,atσ,psot+ol1e)d−σpooled

In our formulation, the residual rewards only depend on the features associated with them. Hence, we can use this fact to reason about constraints over only a subset of features f , e.g., only color or state position. Let φf ⊆ φ be the subset of features we are concerned with. In our grids we represent φ with a vector of length 92. The ﬁrst 81 elements represent the states, the next 8 represent the actions, and the last 3

represent the colors. So if we are interested in only learning about constraints over the colors, φf will be a vector equal to the last three elements of φ that is φcolor ≡ φ90,91,92.
Let φf and ωfR be the feature function and residual feature weight vector for f . We can now deﬁne the probability of a feature value to be constrained as:
ζf ≡ P (C ≤ ωfRφf ) = sigmoid ωf φsft−dpsotodlpeodoled .
3.3 Experimental Evaluation of MESC-IRL
In this section we empirically validate our method for soft constraint learning against both the method of Scobee and Sastry (2020) for learning hard constraints in deterministic settings as well as on learning soft constraints in nondeterministic settings. Figure 3 shows the performance of MESC-IRL compared to the method proposed by Scobee and Sastry (2020) on the same metrics from their paper: false positives, i.e., predicting a constraint when one does not exist, and KL-Divergence from the demonstrations set D. For this test we use the same single grid, hard constraints, and a deterministic setting to allow for a direct comparison. We generate 10 independent sets of 100 demonstrations and report the mean. In order to decide if the values returned by MESC-IRL represent a hard constraint, we threshold the value of ζ at various levels and plot the comparison to the best result from Scobee and Sastry (2020). MESC-IRL with ζ ≥ 0.6 performs better than existing methods when the number of demonstrations is low, about the same when there are more demonstrations, and is able to also work for soft constraints and non-deterministic settings.
In order to evaluate MESC-IRL on soft constraints we need to adapt the notion of false positives and false negatives. Let a false positive f p be:

fp =

x∈C|c(x)=0∧(ζC (x)−ζC∗ (x)>χ)

Num. Constraints

.

Where ζC(x) and ζC∗ (x) are the predicted and true probability of transition x being constrained as described in Section 3.2, and χ is a value in [0,1]. Intuitively, we count a constraint as a false positive whenever there is no constraint in MC∗ and the predicted probability exceeds the true proba-

5

bility by more than the threshold χ. We can adapt the notion of false negatives, f n in the same way by taking c(x) = 0.
Figure 4 shows the results of our tests on recovering soft constraints in non-deterministic settings with random grids, results for deterministic settings can be found in the Appendix. For these tests we choose a start and a goal state randomly at least 8 moves apart, set 6 states for blue, 6 for green randomly, and select 6 randomly constrained states; all penalties are set to −50. Again we take 10 sets of 100 demonstrations. We see a strong decrease in both false positives and false negatives as the number of demonstrations grows. We see that in general, and even more so when the optimal threshold χ = 0.2 is selected, our method almost never adds constraints that are not present in the ground truth and rarely underestimates the probability of existing ones, even for relatively small demonstration sets. Likewise our method is able to generate trajectories very close to D, showing that we are able to recover both constraints and behavior even with soft constraints in non-deterministic setting. Hence MESC-IRL is able to work across a variety of settings and accurately capture demonstrated constraints.
4 Orchestrating Goals and Constraints
Often humans are confronted with decisions that require making trade-offs between collective norms and personal objectives (Rossi and Mattei 2019; Noothigattu et al. 2019). In this section we investigate different ways to model this orchestration in constrained grid environments. We consider different methods for combining policies πn for the nominal MN and πc for the learned constrained MC. For every state action pair (s, a) we consider vectors sqn(s, ai) and sqc(s, ai) with i ∈ {1, . . . , k} where sqn(s, ai) (resp. sqc(s, ai)) represents the probability of choosing action ai in state s according to policy πn (resp. πc). They are obtained by taking the softmax of the Q-values for each policy:
Greedy (πG): Let πG(s) = a, where each a is the one with highest Q-value, a=argmax max{qc(s, a), qn(s, a)}.
a∈As
Weighted Average (πW A): Given weight vector (wn, wc) with wn,wc ∈ [0, 1] and wc + wn = 1, action a = πW A(s) is chosen according to probability distribution pW A(ai) = wnsqn(s, ai) + wcsqc(s, ai).
MDFT (πMDF T ): Action a = πMDF T (s) is chosen via an MDFT model where: M is a k × 2 matrix where rows (i.e., options) correspond to actions and columns (i.e., attributes) correspond to MN and MC. The i-th element of the respective world column is sqn(s, ai) (resp., sqc(s, ai)), i.e., we are using the probability of choosing an action as a proxy of its preference. The weight vector (wn, wc) is deﬁned as for πW A, and serves as probability distribution w deﬁning how attention shifts between attributes during deliberation. Matrices C and S are deﬁned in the standard way as described in Section 2.3. When reaching state s, an MDFT deliberation process is launched to decide which action should be chosen. At each step the focus is shifted to MN or MC according to probability distribution (wn, wc), and the preferences

of the actions according to the selected attribute are accumulated as per Section 2.3.
Informally, Greedy is a deterministic approach that takes the most promising action, WA allows the agent to prioritize the pursuit of the goal state and satisfying constraints via a new policy obtained by considering the weighted average of the nominal and constrained distributions, and the MDFT-based orchestrator uses MDFT to chose at each step an action as suggested by the MDFT machinery.
4.1 Comparison of Orchestration Methods
We ﬁrst compare theoretically the expressive power of the three orchestrators. We focus on a single state s and consider how the policies compare in terms of being able to model a given distribution over the actions available in s. We start by considering the Greedy orchestrator that is deterministic and will pick a ﬁxed action a in state s. Both WA and MDFT can model the Greedy policy by shifting all the weight to the environment where the maximum value is obtained and zeroing all preferences except for that of action a. A formal description is provided in the Appendix. This observation, along with the fact that MDFT and WA are non-determistic, allows us to conclude that Greedy is strictly less expressive than the other two orchestrators.
Turning to the comparison between MDFT and WA, we can prove the following statement.
Theorem 1 Given any state s, there exist choice probability distributions over the actions available in s that can be modeled by MDFT but not by WA.
We use an instance of the well known compromise effect (Busemeyer and Diederich 2002) according to which a compromising alternative tends to be chosen more often by humans than options with complementary preferences with respect to the attributes. Consider the case of state s with three actions a1, a2 and a3. Let us assume that, for example, sqn(s, a1) = 1/6, sqn(s, a2) = 1/3 sqn(s, a3) = 1/2 and sqc(s, a1) = 1/2, sqn(s, a2) = 1/3 sqn(s, a3) = 1/6. According to the compromise effect humans will tend to choose a2 more often than a1 and a3. Such a choice distribution over the actions can be modeled by an MDFT deﬁned over option set {a1, a2, a3}, with two attributes and weights wn = 0.55 and wc = 0.45 (Busemeyer and Diederich 2002). However, if we now consider WA, we can see that there is no way to deﬁne weights (wn, wc) such that the corresponding weighted average probability satisﬁes wnsqn(s, a2) + wcsqc(s, a2) > max{wnsqn(s, a1) + wcsqc(s, a1), wnsqn(s, a3) + wcsqc(s, a3)}. Thus, this distribution over actions cannot be modeled by the WA.
On the other hand, if we consider MDFTs in general, i.e. without the restriction of having two attributes, we can model any distribution. Intuitively this is achieved by deﬁning an MDFT model over k actions and with k attributes where the weight of the i-th attribute corresponds to the probability of the i-th action. Matrix M is set to the identity matrix and deliberation is halted after one iteration; see the Appendix for details.

6

Soft False Negative Rate Soft False Positive Rate
JS-Divergence

0.096

0.094

0.092

0.090

0.088

0.086

= 0.01

0.084

= 0.05 = 0.1

0.082

= 0.2

0

20

40

60

80

100

Number of Demonstrations

0.10 0.08 0.06 0.04 0.02 0.00
0

= 0.01 = 0.05 = 0.1 = 0.2

20

40

60

80

100

Number of Demonstrations

0.45 0.40 0.35 0.30 0.25 0.20 0.15
0

MESC-IRL no constraint prediction

20

40

60

80

100

Number of Demonstrations

Figure 4: Performance of MESC-IRL on recovering soft constraints in non-deterministic settings according to false negatives (left), false positives (center), and JS-Divergence to D (right). We see that across all these settings we are able to accurately recover constraints and generate behavior similar to the D even with few demonstrations.

Figure 5: Comparison of the three orchestrators (Greedy, WA, and MDFT) on metrics including average path length (left), normalized penalty (lower is better, center), and average number of violated constraints (lower is better, right). We see that across all the metrics the MDFT orchestration is able to generate shorter paths that incur less penalty and violate fewer constraints.

Theorem 2 Given a s and the set As of actions available in s, consider a probability distribution p deﬁned over As. We can deﬁne an MDFT model where the set of options corresponds to As and the induced choice probability distribution coincides with p.
As a consequence, MDFT is general enough to express the probability distributions induced over the actions by WA. Whether this is true also in the case of MDFT with only two attributes, as used in πMDF T , remains an open theoretical question. However, we can see this experimentally: in Rahgooy and Venable (2019) the authors propose an RNNbased approach that starts from samples of a choice distribution and recovers parameters of an MDFT model, minimizing the divergence between the original and MDFT-induced choice distributions. We adapt their code1 and generate 100 instances of WA distributions starting from random sqn and sqc distributions and (wn, wc) weights. For each of these instances we generate 100 samples (i.e, chosen actions). We ﬁx the sqn and sqc values as parameters for the M matrix and learn the attention weight distribution w using 300 learning iterations. We use the learned MDFT model to generate a choice distribution over the actions with a stopping criteria of 25 deliberations steps. The observed average JS divergence between the original WA distributions and the ones induced by learned MDFT is 0.024 with standard error
1Available at https://github.com/Rahgooy/MDFT

0.0013; showing experimentally we can learn weights for an MDFT model to replicate any choice distribution of WA.
4.2 Experimentally Evaluating Orchestrators
We compare the orchestrators empirically with the goal of testing if the combination of MESC-IRL with the orchestration techniques can be leveraged to create agents that tradeoff between conﬂicting objectives like humans.
We start by generating 100 different non-deterministic nominal worlds, MN , as described in Sections 2 and 3. We learn, via VI on MC∗ the optimal policy in the (ground truth) constrained world, denoted πC∗∗ and, similarly to Scobee and Sastry (2020), we use it to generate sets of 200 demonstrations, D. We then pass D to MESC-IRL, and the learned constraints are added to MN yielding the learned constrained MPD, MC. We use VI on MN and MCto obtain πn and πc, and we consider different ways to prioritize them by sweeping the weight values (wn, wc) from (0, 1) to (1, 0) in steps of 0.1. Note that at (1, 0) (resp. (0, 1)) WA is equivalent to πn (resp. πc), and that in both cases MDFT becomes deterministic, picking the action with highest Q-value.
For all our results we generated 200 trajectories for each step and method (including πn and πc, denoted as Nominal and Constrained in Fig. 5), and for each of the 100 random worlds. We ﬁrst perform the Kolmogorov-Smirnov test to see if the trajectories generated by WA and MDFT induce the same distribution (H0). We reject H0 at every weight

7

Figure 6: Average JS Divergence between policies generated with πc and orchestrators MDFT and WA.
step with p value ≤ 0.01, thus the two techniques induce statistically signiﬁcantly different choice distributions.
Figure 5 (left) shows the average length of trajectories produced by the orchestrators normalized so that 1.0 is the shortest path between the start and goal state; (center) we scale the penalty by the average penalty for trajectories in MC, lower is better; (right) we show the average number of violated constraints. Across all these metrics, the MDFT agent is performing better than WA by always reaching the goal in a smaller number of steps no matter the conﬁguration of the orchestrator. We can also see that the MDFT agent violates fewer constraints and accumulates lower penalties.
Finally, in Figure 6 we show the JS Divergence between the trajectories generated by πC∗∗ and the trajectories generated by MDFT and WA, as the weight vector varies. For both agents, the divergence is small on the left and grows moving to the right, as constraints become less important. This is not surprising, since the reference trajectories are generated using πC∗∗ . Furthermore, we note that the MDFT advantage is more signiﬁcant when wc is larger, that is when constraints matter more. An explanation for this is that a large value wc results in more MDFT deliberation steps to be focused (exclusively) on preferences relative to the constrained world. In WA, the averaging of the values underlying the policies, although weighted, is not able to maintain the importance of the constraints.
5 Conclusions and Future Work
We proposed a novel and general constraint learning method combined with a unique agent architecture aimed at learning constraints from demonstrations and exhibiting human-like trade-offs in environments with competing objectives. Our theoretical and experimental results show that the MDFTbased method exhibits superior expressive power and performance both in terms of the quality of the produced trajectories as well as capability of capturing initial demonstrations. Another important features of this cognitive approach is its effectiveness in capturing behavioral traits of humans decision making, a key factor for real life applications involving human-generated demonstrations and decisions. Our MESC-IRL approach is a general approach for learning soft constraints over actions, states, and features in non-deterministic decision making environments.
We plan to run experiments with human decision makers

and employ also learning-based orchestrators Noothigattu et al. (2019). We are also working on a novel multi-agent architecture with several orchestrators acting as either reactive or deliberate agents, e.g., the system 1 / system 2 model of Kahneman (2011) with a meta-cognitive agent to arbitrate, with the goal of further advancing the performance and generality of decision making agents.
References
Abbeel, P.; and Ng, A. Y. 2004. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning (ICML).
Altman, E. 1999. Constrained Markov Decision Processes, volume 7. CRC Press.
Amodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schulman, J.; and Mane´, D. 2016. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565.
Balakrishnan, A.; Bouneffouf, D.; Mattei, N.; and Rossi, F. 2018. Using Contextual Bandits with Behavioral Constraints for Constrained Online Movie Recommendation. In Proc. of the 27th Intl. Joint Conference on AI (IJCAI).
Balakrishnan, A.; Bouneffouf, D.; Mattei, N.; and Rossi, F. 2019. Incorporating Behavioral Constraints in Online AI Systems. In Proc. of the 33rd AAAI Conference on Artiﬁcial Intelligence (AAAI).
Booch, G.; Fabiano, F.; Horesh, L.; Kate, K.; Lenchner, J.; Linck, N.; Loreggia, A.; Murugesan, K.; Mattei, N.; Rossi, F.; and Srivastava, B. 2021. Thinking Fast and Slow in AI. In Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, 15042–15046. AAAI Press.
Busemeyer, J. R.; and Diederich, A. 2002. Survey of decision ﬁeld theory. Mathematical Social Sciences, 43(3): 345– 370.
Busemeyer, J. R.; and Townsend, J. T. 1993. Decision ﬁeld theory: a dynamic-cognitive approach to decision making in an uncertain environment. Psychological review, 100(3): 432.
Chou, G.; Berenson, D.; and Ozay, N. 2018. Learning constraints from demonstrations. arXiv preprint arXiv:1812.07084.
Hotaling, J. M.; Busemeyer, J. R.; and Li, J. 2010. Theoretical developments in decision ﬁeld theory: Comment on Tsetsos, Usher, and Chater (2010). Psychological Review, 117(4).
Kahneman, D. 2011. Thinking, Fast and Slow. Macmillan.
Malik, S.; Anwar, U.; Aghasi, A.; and Ahmed, A. 2021. Inverse Constrained Reinforcement Learning. In Meila, M.; and Zhang, T., eds., Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, 7390–7399. PMLR.
Ng, A. Y.; and Russell, S. J. 2000. Algorithms for Inverse Reinforcement Learning. In Proceedings of the Seventeenth

8

International Conference on Machine Learning, ICML ’00, 663–670. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2.
Noothigattu, R.; Bouneffouf, D.; Mattei, N.; Chandra, R.; Madan, P.; Varshney, K. R.; Campbell, M.; Singh, M.; and Rossi, F. 2019. Teaching AI agents ethical values using reinforcement learning and policy orchestration. IBM J. Res. Dev., 63(4/5): 2:1–2:9.
Rahgooy, T.; and Venable, K. B. 2019. Learning Preferences in a Cognitive Decision Model. In Zeng, A.; Pan, D.; Hao, T.; Zhang, D.; Shi, Y.; and Song, X., eds., Human Brain and Artiﬁcial Intelligence, 181–194. Singapore: Springer Singapore. ISBN 978-981-15-1398-5.
Ray, A.; Achiam, J.; and Amodei, D. 2019. Benchmarking safe exploration in deep reinforcement learning. arXiv preprint arXiv:1910.01708, 7.
Roe, R. M.; Busemeyer, J. R.; and Townsend, J. T. 2001. Multialternative decision ﬁeld theory: A dynamic connectionst model of decision making. Psychological review, 108(2): 370.
Rossi, F.; and Mattei, N. 2019. Building Ethically Bounded AI. In Proc. of the 33rd AAAI Conference on Artiﬁcial Intelligence (AAAI).
Rossi, F.; Van Beek, P.; and Walsh, T. 2006. Handbook of Constraint Programming. Elsevier.
Russell, S.; Dewey, D.; and Tegmark, M. 2015. Research priorities for robust and beneﬁcial artiﬁcial intelligence. AI Magazine, 36(4): 105–114.
Scobee, D. R. R.; and Sastry, S. S. 2020. Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Sutton, R. S.; and Barto, A. G. 2018. Reinforcement Learning: An Introduction, 2nd Edition. Cambridge, MA, USA: A Bradford Book.
Svegliato, J.; Nashed, S. B.; and Zilberstein, S. 2021. Ethically compliant sequential decision making. In Proceedings of the 35th AAAI International Conference on Artiﬁcial Intelligence (AAAI).
Vazquez-Chanlatte, M.; Jha, S.; Tiwari, A.; Ho, M. K.; and Seshia, S. A. 2018. Learning Task Speciﬁcations from Demonstrations. In Bengio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montre´al, Canada, 5372–5382.
Ziebart, B. D.; Bagnell, J. A.; and Dey, A. K. 2010. Modeling Interaction via the Principle of Maximum Causal Entropy. In Fu¨rnkranz, J.; and Joachims, T., eds., Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, 1255–1262. Omnipress.
Ziebart, B. D.; Maas, A. L.; Bagnell, J. A.; and Dey, A. K. 2008. Maximum Entropy Inverse Reinforcement Learning. In Fox, D.; and Gomes, C. P., eds., Proceedings of the

Twenty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, 1433– 1438. AAAI Press.

9

Additional Material for Making Human-Like Trade-offs in Constrained Environments by Learning from Demonstrations
A Additional Graphs For Learning Constraints
Additional graphs and results for comparision with the methods of Scobee and Sastry (2020) and MESC-IRL. Figure 7 shows the stepping results for MESC-IRL.

False Positive Rate

0.5 0.4 0.3 0.2 0.1 0.0
0

MESC-IRL( 0.4) MESC-IRL( 0.5) MESC-IRL( 0.6) MESC-IRL( 0.7)

20

40

60

80

100

Number of Demonstrations

KL-Divergence

10 8 6 4 2 0
0

MESC-IRL( 0.4) MESC-IRL( 0.5) MESC-IRL( 0.6) MESC-IRL( 0.7)

20

40

60

80

100

Number of Demonstrations

Figure 7: Performance of MESC-IRL for various settings of ζ at recovering hard constraints in a deterministic setting according to false positive rate (left) and KL-Divergence from the demonstrations D (right) as we vary the number of demonstrations. Each point is the mean of 10 independent draws.

Figure 8 shows the performance of our best cutoff with the best method from Scobee and Sastry (2020).

False Positive Rate

0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00
0

Scobee(dkl = 0.1) MESC-IRL( 0.6)

20

40

60

80

100

Number of Demonstrations

KL-Divergence

5

Scobee(dkl = 0.1)

MESC-IRL( 0.6)

4

3

2

1

0

0

20

40

60

80

100

Number of Demonstrations

Figure 8: Comparision of the best performing version of MESC-IRL to the best performing method of Scobee and Sastry (2020) at recovering hard constraints in a deterministic setting according to false positive rate (left) and KL-Divergence from the demonstrations D (right) as we vary the number of demonstrations. Each point is the mean of 10 independent draws.

10

Soft False Negative Rate

Figure 9 shows the performance of MESC-IRL on recovering soft constraints in the deterministic setting.

0.0975 0.0950 0.0925 0.0900 0.0875 0.0850 0.0825 0.0800
0

= 0.01 = 0.05 = 0.1 = 0.2

20

40

60

80

100

Number of Demonstrations

Soft False Positive Rate

0.12 0.10 0.08 0.06 0.04 0.02 0.00
0

= 0.01 = 0.05 = 0.1 = 0.2

20

40

60

80

100

Number of Demonstrations

JS-Divergence

0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05
0

MESC-IRL no constraint prediction

20

40

60

80

100

Number of Demonstrations

Figure 9: Performance of MESC-IRL on recovering soft constraints in deterministic settings according to false negatives (left), false positives (center), and JS-Divergence to D (right). We see that across all these settings we are able to accurately recover constraints and generate behavior similar to the D even with few demonstrations.

Figure 10 shows the performance of MESC-IRL on recovering soft constraints in the non-deterministic setting.

Soft False Negative Rate

0.096

0.094

0.092

0.090

0.088

0.086

= 0.01

0.084

= 0.05 = 0.1

0.082

= 0.2

0

20

40

60

80

100

Number of Demonstrations

Soft False Positive Rate

0.10 0.08 0.06 0.04 0.02 0.00
0

= 0.01 = 0.05 = 0.1 = 0.2

20

40

60

80

100

Number of Demonstrations

JS-Divergence

0.45 0.40 0.35 0.30 0.25 0.20 0.15
0

MESC-IRL no constraint prediction

20

40

60

80

100

Number of Demonstrations

Figure 10: Performance of MESC-IRL on recovering soft constraints in deterministic settings according to false negatives (left), false positives (center), and JS-Divergence to D (right). We see that across all these settings we are able to accurately recover constraints and generate behavior similar to the D even with few demonstrations.

11

B Proof Details for Comparison of Orchestration Methods
We provide the full proofs for the theoretical comparison of the orchestrators. Theorem 3 Consider state s. Any choice probability distribution over the actions available in s that can be modeled by the Greedy approach can be modeled via the MDFT or WA approaches.
Proof. We can model the (degenerate) probability distribution induced by Greedy via an MDFT with as many options as the actions available in s, two attributes with weights set to any random pair of values, and preferences in the M matrix all equally to 0 except for those in the row associated with a which are set to 1. Matrices C and S can be deﬁned in the standard way described in Section 2.3 and deliberation can be halted after one deliberation step. In fact, when deliberation is launched, an attribute will be selected. Regardless of which one is selected, action a will be chosen given that it is the only one with non-zero preference.
Similarly, we can model the Greedy distribution using a weighted average where wn = wc = 1/2, and sqn(s, a) = sqc(s, a) = 1 and sqn(s, a ) = sqc(s, a ) = 0, ∀a = a. Theorem 4 Given state s and the set As of actions available in s, consider a probability distribution p deﬁned over As. We can deﬁne an MDFT model where the set of options corresponds to As and the induced choice probability distribution coincides with p. Prrof. Consider the MDFT model deﬁned as follows: • Matrix M is the k × k identity matrix; • Weight vectors W are deﬁned as in Section 2.3 and select a single attribute at each iteration. Probability distribution over
attributes w is deﬁned in a way such that the probability of selecting the j-th attribute, is p(aj). • Matrices C and S are deﬁned in the standard way as described in Section 2.3. • The deliberation time for the model is ﬁxed at one iteration. It is easy to see that running the model induces a choice probability over the actions which corresponds to p. In fact, in every run, which consists of a single iteration, an attribute Ah will be sampled according to probability p. Given how M is deﬁned and the fact that the initial value of the accumulated preference P(0) = 0, action ah will be chosen. Thus, the probability of action ah being selected, given the MDFT model, coincides with p(ah).
12

