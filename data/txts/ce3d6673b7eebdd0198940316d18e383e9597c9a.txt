Proceedings of Machine Learning Research vol 125:1–52, 2020

33rd Annual Conference on Learning Theory

arXiv:2003.01922v2 [cs.LG] 15 Oct 2020

Taking a hint: How to leverage loss predictors in contextual bandits?

Chen-Yu Wei University of Southern California
Haipeng Luo University of Southern California
Alekh Agarwal Microsoft Research, Redmond

CHENYU.WEI@USC.EDU HAIPENGL@USC.EDU
ALEKHA@MICROSOFT.COM

Editors: Jacob Abernethy and Shivani Agarwal

Abstract

We initiate the study of learning in contextual bandits with the help of loss√predictors. The main question we address is whether one can improve over the minimax regret O( T ) for learning

over T rounds, when the total error of the predicted losses relative to the realized losses, denoted

as E ≤ T , is relatively small. We provide a complete answer to this question, with upper and lower

bounds for various settings: adversarial and stochastic environments, known and unknown E, and

single an√d mu√ltiple predictors. We show several surprising results, such as 1) the optimal √regret is

O(min{

T,

ET

1 4

})

when

E

is

known,

in

contrast

to

the

standard

and

better

bound

O(

E) for

non-contextual problems (such as√multi-armed bandits); 2) the same bound cannot be achieved if E

is unknown, but as a remedy, O(

ET

1 3

)

is

achievable;

3)

with

M

predictors,

a

linear

dependence

on M is necessary, even though logarithmic dependence is possible for non-contextual problems.

We also develop several novel algorithmic techniques to achieve matching upper bounds, in-

cluding 1) a key action remapping technique for optimal regret with known E, 2) computationally

efﬁcient implementation of Catoni’s robust mean estimator via an ERM oracle in the stochastic

setting with optimal regret, 3) an underestimator for E via estimating the histogram with bins of

exponentially increasing size for the stochastic setting with unknown E, and 4) a self-referential

scheme for learning with multiple predictors, all of which might be of independent interest.

1. Introduction

Online learning with the help of loss predictors has been widely studied over the past decade. In

these problems, before making a decision at each round t, the learner is given some prediction mt of

the true gradient or loss vector ℓt. The goal is to ensure regret that is much smaller than the worst-

case bound as lon√g as these predictions are indicative of the loss vectors. For example, for most

problems with Θ( T ) minimax r√egret for learning over T rounds, it has been shown that a more

adaptive regret bound of order O( E) is possible, where E =

T t=1

ℓt − mt

2 ∞

is

the

total

error

of the predictions, which is at most O(T ) but could be much smaller if a good predictor is available.

Such a bound is achievable for problems with full information feedback (Rakhlin and Sridharan,

2013a; Steinhardt and Liang, 2014), as well as partial information feedback such as multi-armed

bandits (Wei and Luo, 2018) and linear bandits (Rakhlin and Sridharan, 2013a).

In contextual bandits (Auer et al., 2002; Langford and Zhang, 2008), a generalization of multi-

armed bandits that has been proven to be useful for applications such as personalized recommen-

dation systems in practice, loss predictors are also commonly used to construct doubly-robust es-

© 2020 C.-Y. Wei, H. Luo & A. Agarwal.

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Table 1:

Summary of main results. T is the total number of rounds. For single predictor, E ≤ T is the total error of predictions. For multiple predictors, E∗ is the total error of the best predictor and M is the number of predictors. Dependence on other param√eters is omitted. Note that for the case with known E or E∗, one can achieve the minimum of O( T ) and the stated upper bound by simply comparing the two bounds and choosing between the minimax algorithm and our algorithms.

Lower bound √ for E, E∗, M = O( T )
Upper bound in the adversarial setting
Upper bound in the i.i.d. setting with oracle-efﬁcient algorithms

Single predictor with known E
√1 Ω( ET 4 ) [Theorem 1]
√1 O( ET 4 ) [Theorem 4]
√1 O( ET 4 ) [Theorem 8]

Single predictor with unknown E √1 O( ET 4 ) is impossible
[Theorem 2] √1
O( ET 3 ) [Theorem 5]
√1 O( ET 3 ) [Theorem 9]

Multiple predictors with known E∗ Ω(√E∗T 41 + M )
[Theorem 3] O(√M E ∗T 14 ) [Theorem 10]

O(M

2 3

(E

∗

T

)

1 3

)

[Theorem 11]

timators, both for off-policy evaluation (Dud´ık et al., 2014) and online exploration (Agarwal et al., 2014). The potentially lower variance of these doubly-robust estimators has been used to motivate this line of work, and resulting improvements are well established for policy evaluation in both ﬁnite sample (Dud´ık et al., 2014) and asymptotic settings (Robins and Rotnitzky, 1995). In the online exploration setting, however, the exten√t of beneﬁts from a good loss predictor and potential rate improvements beyond the worst case O( T ) bound have not been studied at all, despite all the works mentioned above for the simpler non-contextual settings.
In this work, we take the ﬁrst attempt in addressing this question and provide a rather complete answer on upper and lower bounds for various setups: adversarial and stochastic environments, known and unknown E, and single and multiple predictors. The main message is that good predictors indeed help reduce regret for contextual bandits, but not to the same extent as the non-contextual settings. Speciﬁcally, our contributions are (see also Table 1 for a summary):

• (Section 3) In the adversarial setting where contexts, losses, and predictions ar√e all d1ec√ided by an adversary, we show that, somewhat surprisingly, the regret is at least Ω(min{ ET 4 , T }), and

we also provide an algorithm with a matching regret upper bound when E is known. When E is

unknown,

we

show

that

it

is

impossible

to

achieve√the

same
1

bound,

and

as

a

remedy,

we

provide

an

adaptive√version of our algorithm with regret O( ET 3 ), which is always sublinear and is better

than O(

T√)

as

long

as

E

=

o(T

1 3

).

Note

that

these

results

are

in

sharp

contrast

with

the

typical

bou√nd O( E), for non-contextual problems. For multi-armed bandits, even with unknown E,

O( E) is achievable (Wei and Luo, 2018), indicating that the difﬁculty indeed comes from the

contexts, and not just the bandit feedback.

• (Section 4) In the stochastic setting where contexts, losses, and predictions are jointly i.i.d. samples from a ﬁxed and unknown distribution, we show the exact same lower and upper bounds with known or unknown E, but importantly our algorithms are efﬁcient assuming access to some ERM oracle. This a typical computational model for studying efﬁcient contextual bandits algorithms, and avoiding running time that is polynomial in the number of polices (Langford and Zhang,

2

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

2008; Agarwal et al., 2014; Syrgkanis et al., 2016b). So√mewhat surprisingly, we ﬁnd that an adaptation of ǫ-greedy exploration is optimal when E = O( T ).

• (Section 5) Finally, we extend√our results to the setting where M predictors are available and

the goal is to improve the O( T ) regret as long as the total error E∗ of the best pr√edictor is

rela√tively small. For simpli√city we assume known E∗. We show a lower bo√und Ω(min{

E ∗√T

1 4

+

M,

T }) when M = O(

T ),

as

well

as

an

upper

bound

of

O(min{(√ M E∗T

1 4

+

M,

T })

for

the

adversarial

setting

and

an

upper

bound

of

O(min{M

2 3

(E

∗

T

)

1 3

,

T }) for the stochastic

setting with an oracle-efﬁcient algorithm. This is also in contrast with the non-contextual settings

where the dependence on M is logarithmic, even with bandit feedback (Rakhlin and Sridharan,

2013a).

Throughout, we focus on ﬁnite action and policy sets in this work to cleanly illustrate the key ideas. Extensions to inﬁnite actions and policies are interesting avenues for future work.
Techniques. Our algorithms require several novel techniques, brieﬂy summarized below:

• Most importantly, all our algorithms rely on an action remapping technique, which restricts the algorithm’s attention to only a subset of actions at each round. This subset consists of actions with predicted loss not larger than that of a baseline action (such as the action with the smallest predicted loss) by a certain amount, and the algorithms pretend that all actions outside this set are just the baseline action. For the adversarial setting with multiple predictors, we also need to apply a self-referential scheme to ﬁnd the baseline and construct this subset, an idea similar to sleeping experts (Freund et al., 1997). We prove that this action remapping technique reduces both the exploration overhead and the variance of estimators.
• Our algorithms for the stochastic setting require using robust mean estimators. In particular, we use the Catoni’s estimator (Catoni, 2012) and show that it can be implemented efﬁciently using the ERM oracle, which might be of independent interest and useful for developing oracle-efﬁcient algorithms for other problems.
• When E is unknown, we construct a novel underestimator of E by estimating the histogram of the distribution of ℓt − mt with bins of exponentially increasing size in the stochastic setting.

Related work. Similar to prior work such as (Rakhlin and Sridharan, 2013a) (for non-contextual

problems), we consider generic loss predictions given by any predictors as inputs of the algorithm.

A series of works focus on choosing speciﬁc predictions based on observed data and deriving

data-dependent bounds in terms of the variation of the environment (Hazan and Kale, 2010, 2011;

Chiang et al., 2012, 2013; Steinhardt and Liang, 2014; Wei and Luo, 2018; Bubeck et al., 2019),

which are themselves useful for applications such as faster convergence to equilibria for game play-

ing (Rakhlin and Sridharan, 2013b; Syrgkanis et al., 2015; Wei and Luo, 2018). Whether similar

applications can be derived based on our results is an int√eresting future direction. EXP4 is the classic algorithm with optimal regret O( T ) for the adversarial setting (Auer et al.,

2002), albeit with running time linear in the number of policies. For the stochastic setting, the sim-

ple

ǫ-greedy

algorithm

is

oracle-efﬁcient

but

with

suboptimal

regret

O(T

2 3

)

(Langford

and

Zhang,

2008). Later, Agarwal et al. (2014) proposed an oracle-efﬁcient and optimal algorithm ILOVETO-

CONBANDITS. All these algorithms are building blocks for our methods.

Developing adaptive regret bounds for contextual bandits is relatively under-explored. The

only existing work on contextual learning that considers a similar setting with loss predictors

3

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

is (Syrgkanis et al., 2016a, Section 6), but they only consider the easier full-information feedback. On a different direction, Allen-Zhu et al. (2018) derived the ﬁrst small-loss bound for contextual bandits.
Our idea of using robust estimators is inspired by (Krishnamurthy et al., 2019), which studies contextual bandits with continuous actions and uses median-of-means, a standard robust estimator, for a different purpose. It is unclear whether median-of-means can be implemented efﬁciently via an ERM oracle. Instead, we turn to Catoni’s estimator (Catoni, 2012), which provides a similar concentration guarantee and can be implemented efﬁciently as we show.

2. Problem Description and Lower Bounds

Contextual bandits is a generalization of the classic multi-armed bandit problem, where before choosing one of the K actions at each round, the learner observes a context from some arbitrary context space X . In addition to the context, we consider a variant where a loss predictor is also available. Speciﬁcally, for each round t = 1, . . . , T , the environment chooses a context xt ∈ X , a loss vector ℓt ∈ [0, 1]K , and a loss predictor mt ∈ [0, 1]K ; the learner then receives xt and mt; ﬁnally, the learner chooses an action at ∈ [K] and observes its loss ℓt(at).
We consider both the adversarial setting and the stochastic setting. In the former, the sequence (xt, ℓt, mt)1:T can be arbitrary and even depend on the learner’s strategy. For simplicity we assume it is decided ahead of time before the game starts (also known as the oblivious setting). In the latter, each triple (xt, mt, ℓt) is drawn independently from a ﬁxed and unknown distribution D.
As in the standard contextual bandits setup, the learner has access to some ﬁxed policy class Π ⊆ [K]X , assumed to be ﬁnite (for simplicity) with cardinality N , and her goal is to minimize the
(pseudo) regret against the best ﬁxed policy:

Reg(A) maxπ∈Π E

T t=1

ℓt(at)

−

T t=1

ℓt

(π(xt

))

,

where the expectation is with respect to the randomness of the learner, denoted as the algorithm

A which chooses a1, . . . , aT , and also that of the environment in the stochastic case. When it is clear from the context, we omit the dependence on A√and simply denote the regret by Reg. It is well-known that the optimal worst-case regret is O( dT ) where we deﬁne d K ln N .1 The

key question we address in this work is whether one could improve upon this worst-case bound

when the predictor is accurate. More speciﬁcally, we denote the total loss of the predictor by

E

T t=1

ℓt − mt

2 ∞

for

the

adversarial

setting

and

E

T E(x,ℓ,m)∼D

ℓ−m

2 ∞

for the

stochastic setting, and we ask the following question:

√ (Q1) Can we improve the regret over O( dT ) if E = o(T )?

Note that for the special case of multi-armed bandits where Π consists of K constant mappings tha√t always pick one of the K actions (that is, co√ntexts are ignored), Wei and Luo (2018) show that O( dE) is achievable, an improvement over O( dT ) as long as E = o(T ). A natural guess would be that the same holds true for contextual bandits. However, somewhat surprisingly, in the following theorem we show that this is not the case (proofs for all lower bounds are deferred to Appendix B).
1. Throughout the paper, we do not make an effort to optimize the dependence on K and ln N . For example, we often relax K2 ln N by d2 for ease of presentation. For most discussions, we also ignore the dependence on d and only focus on the dependence on T and E .

4

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Theorem 1 For any algorithm and an√y value V ∈ [0, T ], there exists a (sto√chastic or ad√versarial)

environment with E ≤ V and N = Θ( KT ) such that Reg(A) = Ω min

V

(K

T

)

1 4

,

KT

.2

√ This theorem gives a negative answer to (Q1) when E = Ω( T ). Even when E = O(1), the

theorem

shows

that

the

best

one

can

achieve

is

O(T

1 4

),

a

sharp

contrast

with

the

non-contextual

case. N√ote that we require N ≥ K due to Wei and Luo (2018), but perhaps the condition of N = Θ( KT ) can be further weakened. In Sections 3 and 4, we develop algorithms with matching

upper bounds for adversarial and stochastic environments respectively, thus completely an√swering (Q1) and conﬁrming that in the worst case, loss predictors are helpful if and only if E = o( T ).

Robustness when E is unknown. One shortcoming of our algorithms with matching upper bounds

is that they require knowing the value of E, which is clearly undesirable in practice. Put differently,

for each possible value of E, we need a different setting of algorithm parameters to achieve the

optimal bound. Therefore, the next general question we ask is:

√

√

(Q2) Is there an algorithm with regret o( T ) simultaneously for all environments with E = o( T )?

One standard method in online learning to deal with unknown parameters is the so-called dou-

bling trick, which is applicable even for some partial-information settings (Hazan and Kale, 2011;

Wei and Luo, 2018). However, we show yet another surprising result that the answer to (Q2) is no.

√

Theorem 2 If an algorithm A achieves Reg(√A) = o( T ) for all environments with E = 0,√then

there exists another environment with E = o( T ) a√nd N = Ω(√T ) for which Reg(A) = ω( T ).

Thus, no algorithm can achieve Reg(A) = O min

E

(dT

)

1 4

,

dT

simultaneously for all E.

√

The theorem asserts that no algorithm c√an improve over O( T ) when good predictors are avail-

able while simultaneously maintaining O( T ) worst-case rob√ustness. As a remedy, nevertheless,

we develop adaptive version√s of our algorithms with regret O(

ET

1 3

)

for

all

environments

simul-

taneously. This bound is o(

T)

whenever

E

=

o(T

1 3

)

and

at

the

same

time

provides

a

robustness

guarantee

of

O(T

5 6

).

As a comparison,

a bound

of

order√O(E T

1 4

),

achievable

by naively

setting

the parameters of our algorithms independent of E, is o(

T)

only

when

E

=

o(T

1 4

),

and

more

importantly could be linear when E is large and thus provides no robustness guarantee at all.

Learning with multiple predictors. Having a complete understanding of the single predictor

case, we further consider a more general setup where instead of receiving one predictor mt, the

learner receives M predictors m1t , . . . , mM t ∈ [0, 1]K at the beginning of each round. In the

adversarial setting, these are decided ahead of time by an adversary, and we denote by E∗

mini∈[M ]

T t=1

ℓt −mit

2∞, the total error of the best predictor. On the other hand, for the stochastic

setting, each tuple (xt, ℓt, m1t , . . . , mM t ) is an i.i.d. sample of a ﬁxed distribution D, and we denote

by E∗

T mini∈[M ] E(x,ℓ,m1:M )∼D

ℓ − mi

2 ∞

, the expected total error of the best predictor.

The goal of the learner is to improve over the worst-case bound as long as one of the predictors

is reasonably accurate. Speciﬁcally, we ask the following (assuming known E∗ for simplicity).

(Q3) Can we improve the regret over O(√dT ) for E∗ = o(√T ) and reasonably small M ?

2. Note that while seemingly a lower bound for the stochastic environments should imply the same for the adversarial environments, there is a subtle technical difference due to the slightly different deﬁnitions of E in these two cases. We provide proofs for both environments.

5

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

√ For many online learning problems (even those with partial information), achieving O( E + ln M )

is possible (Rakhlin and Sridharan, 2013a). We already know that a worse dependence on T is nec-

essary for contextual bandits, and it turns out that, a worse dependence on M is also unavoidable.

√

√

Theorem 3 For any algorithm A and any M ≤ T and V ∗ ≤ T , there ex√ists an environment

(which can be stochastic or adversarial) with E∗ ≤ V ∗ such that Reg(A) = Ω(

V

∗

(K

T

)

1 4

+

M ).

Compared to the sin√gle predictor case, the lower bound has an extra term linear in M . It shows that when M = Ω( T ), there is no hope to improve the worst-case regret even if there is a per-

fect predictor such that E∗ = 0, again a√sharp contrast with the non-contextual case. In Section 5,

we provide an algorithm with regret O(

ME∗T

1 4

)

for

the

adversarial

setting,

and

another

oracle-

efﬁcient

algorithm

with

regret

O(M

2 3

(E

∗T

)

1 3

)

for

the

stochastic

setting,

answering

(√Q3)

positively

to some extent. (Note that these bounds are larger than the lower bound when M ≤ T .)

Other notations. We use O(·) to hide the dependence on ln T , and Ω(·) to hide the dependence on 1/ ln T ; for an integer n, [n] represents {1, . . . , n}; for a random variable Z, V[Z] denotes its variance; ∆Π and ∆K are the sets of all distributions over the polices and the actions respectively.

3. Algorithms for Adversarial Environments
In this section, we describe our algorithm for the adversarial setting with one predictor. Similar to existing works on online learning with loss predictors, our algorithm is based on the optimistic Online Mirror Descent (OMD) framework (Rakhlin and Sridharan, 2013a). In particular, with the entropy regularizer, the optimistic OMD update maintains a sequence of distributions

Q′1, . . . , Q′T ∈ ∆Π, such that Q′t+1(π) ∝ Q′t(π) exp − ηℓt(π(xt)) ,

where η > 0 is the learning rate and ℓt is some estimator for ℓt. Upon seeing a context xt and a predictor mt at time t, the algorithm computes Qt ∈ ∆Π such that Qt(π) ∝ Q′t(π) exp − ηmt(π(xt)) , and samples a policy according to Qt and follows its suggestion to choose an action at. Suppose
pt ∈ ∆K is the distribution of at, then the standard variance-reduced loss estimator is

ℓt(a) = (ℓt(a) − mt(a))½[at = a] + mt(a).

(1)

pt(a)

When mt(a) = 0 for all t and a, this is exactly the EXP4 algorithm (Auer et al., 2002). While optimistic OMD with entropy regularizer has been used for problems with full-information
feedback (Steinhardt and Liang, 2014; Syrgkanis et al., 2015), it in fact cannot be directly applied to the bandit setting since typical analysis requires ℓt(a) − mt(a) to be lower bounded by −1/η, which does not hold if ℓt(at) ≤ mt(at) and pt(at) is too small. Intuitively this is also the hard case because the predictor over-predicts the loss of a good action and prevents the algorithm from
realizing it due to the bandit feedback. A naive approach of enforcing u√niform exploration so that pt(at) ≥ η contributes ηT K regret already, which eventually leads to Ω( T ) regret. Indeed, to get around this issue for multi-armed bandits, Wei and Luo (2018) uses a different regularizer called
log-barrier, but this does not work for contextual bandits either since it inevitably introduces polynomial dependence on the number of policies N for the regret.

6

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Algorithm 1 EXP4.OAR: Optimistic EXP4 with Action Remapping
Parameter: learning rate η > 0, threshold σ > 0, exploration probability µ ∈ [0, 1]. Initialize: Q′1(π) = 1/N for all π ∈ Π. for t = 1, . . . , T do 1 Receive xt and mt. Deﬁne a∗t = argmina∈[K] mt(a),

At = {a ∈ [K] : mt(a) ≤ mt(a∗t ) + σ}, and φt(a) = a, if a ∈ At,

(3)

a∗t , otherwise.

2 Calculate Qt ∈ ∆Π: Qt(π) ∝ Q′t(π) exp (−ηmt (φt(π(xt)))).
3 Calculate pt ∈ ∆K : pt(a) = (1 − µ) π:φt(π(xt))=a Qt(π) + |Aµt| ½[a ∈ At].
4 Sample at ∼ pt and receive ℓt(at).
5 Construct estimator: ℓt(a) = ℓt(ap)−t(ma)t(a) ½[at = a] + mt(a) for all a ∈ At.
6 Calculate Q′t+1 ∈ ∆Π: Q′t+1(π) ∝ Q′t(π) exp −ηℓt (φt(π(xt))) .

Our solutions. Our ﬁrst key observation is that, despite the range of the loss estimators, Optimistic Exp4 in fact always guarantees the following (cf. Lemma 14): for any π∗ ∈ Π,

T Qt(π)ℓt(π(xt)) − T ℓt(π∗(xt)) ≤ lnηN + 2η T (ℓt(at) − mt(at))2. (2)

t=1 π∈Π

t=1

t=1

Readers familiar with the EXP4 analysis would ﬁnd that pt(at) is missing in the last term compared

to the standard analysis when ℓt(a) − mt(a) ≥ −1/η holds. To see why Eq. (2) is useful, ﬁrst take expectation (over at) on both sides so the last term is bounded by 2ηK t mℓtin−amptt(a2∞) . Then

consider enforcing uniform exploration so that pt(a) ≥ µ/K holds for some µ ∈ [0, 1]. Since this

contributes µT extra regret, using Eq. (2) we have Reg = O( lnηN + ηKµ2E + µT ), which, with the

op√timal

tuning

of

η

and

µ√, already

gives

a

nontrivial

bound

Reg

=

O√((E

T

)

1 3

)!

This

bound

is

also

o(

T ) whenever E = o(

T ), but is worse than the bound Reg = O(

E

T

1 4

)

we

are

aiming

for.

To further improve the algorithm, we introduce a novel action remapping technique. Speciﬁ-

cally, let a∗t = argmina∈[K] mt(a) be the action with smallest predicted loss and let At (Equation 3) be the set of actions with predicted loss not larger than that of a∗t by σ, for some threshold σ ≥ 0. Then, we rename the actions according to a mapping φt : [K] → At such that φt(a) = a for a ∈ At and φt(a) = a∗t for a ∈/ At. In other words, we pretend that every action outside At was just a∗t .

We call our algorithm EXP4.OAR and show its pseudocode in Algorithm 1.

To see why this action remapping is useful, ﬁrst consider the regret compared to

T t=1

ℓt

(φt

(π∗

(xt

)))

due to exploration. Note that we only explore actions in At and all actions in this set have predicted

loss σ-clos√e to each other. Therefore, exploration leads to regret µT σ + 2µ t ℓt − mt ∞ ≤ µT σ + 2µ ET , instead of µT compared to the naive approach. On the other hand, the bias due to
remapping ℓt(φt(π∗(xt))) − ℓt(π∗(xt)) is either zero if π∗(xt) ∈ At or at most 2 ℓt − mt ∞ − σ otherwise (by adding and subtracting mt(a∗t ) and mt(π∗(xt))). Using the AM-GM inequality and summing over t gives E/σ. Combining everything we prove the following theorem.

7

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Theorem 4

EXP4.OAR

(Algorithm

1)

ensures

Reg

≤

ln N

+ 2ηK2E

√ +µT σ+2µ ET

+E.

Picking

η

µ√

1√

σ

µ = min √dT , 1 , η =

µ ln N K2E

,

and

σ

=

E µT

gives Reg = O

dE (T ) 4 + d E .

See Appendix C.1 for the√complete proof. This theorem indicates that whenever th√e predictor is good enough with E = o( T ), our algorithm improves over EXP4 and achieves o( T ) regret. Note tha√t this bound requires setting the parameters in terms of the qua√ntity E, and in the case when E = Ω( T ), one can simply switch to EXP4 and achieve regret O( dT ). Therefore, our result indeed matches the lower bound stated in Theorem 1 (except for a slightly worse dependence on d). Adaptive version with unknown E. Next, we discuss the case when E is unknown. Recall that there is no hope to maintain the same bound of Theorem 4 in this case, as indicated by Theorem 2.
Standard doubling trick does not work due to the large magnitude of loss estimators (more speciﬁ-
cally, the last round before each restart causes some technical problems), even though it works for
non-contextual problems with bandit feedback (Hazan and Kale, 2011; Wei and Luo, 2018). In light of Eq. (2), our solution is to use a time-varying learning rate ηt that is roughly of order s≤t(ℓs(as) − ms(as))2 −1/2 to minimize the right hand side of Eq. (2) for each time. While
standard analysis requires using the same learning rate in Line 2 and Line 6, due to technical issues we are unable to do so while achieving the desired regret bound. Instead, we use ηt−1 in Line 2 and ηt in Line 6, and carefully bound the bias introduced by this learning rate mismatch. More details are provided in Appendix C.2. Our algorithm (Algorithm 4 in Appendix C.2) is completely adaptive, requiring no prior information about E. The following theorem gives its regret guarantee.

Theorem 5 EXP4.OVAR (Algorithm 4 in Appendix C.2) ensures Reg = O d

ically, setting µ = min

1,

(d/T

)

2 3

gives Reg = O

√ dE

+

√E (d2 T

) 31

.

E µ

+

µT

. Specif-

This shows that our algorithm is robust and always ensures sublinear regret, since in the worst case

Reg

=

O(T 5/6)

(when

E

=

T ).

Also,

our

algorithm

improves

over

EXP4

whenever

E

=

o(T

1 3

).

4. Algorithms for Stochastic Environments

In this section, we consider learning in a stochastic environment with one predictor. Recall that

a stochastic environment is parameterized by an unknown distribution D such that each triple

(xt, ℓt, mt) is an i.i.d. sample from D and the total prediction error is E = T E(x,ℓ,m)∼D

ℓ−m

2 ∞

.

Clearly, this is a special case of the adversarial environment, and our goal is to derive the same re-

sults but with oracle-efﬁcient algorithms.

Speciﬁcally, an ERM oracle is a procedure that takes any set S of context-loss pairs (x, c) ∈
X × RK as inputs and outputs a policy ERM(S) ∈ argminπ∈Π (x,c)∈S c(π(x)). An algorithm is oracle-efﬁcient if its total running time and the number of oracle calls are both polynomial in T and

d, excluding the running time of the oracle itself. Oracle-efﬁciency has been proven to be impossible

for adversarial environments (Hazan and Koren, 2016), but achievable for stochastic environments.

The simplest oracle-efﬁcient algorithm is ǫ-greedy (Langford and Zhang, 2008), with suboptimal

regret

O(T

2 3

).

However, somewhat surprisingly√, we are able to build our algorithm on top of ǫ-

greedy and achieve optimal results when E = o( T ).

We ﬁrst review the ǫ-greedy algorithm and point out the difﬁculties of improving its regret with

loss predictors. In each round t, the algorithm with probability µ samples an action at uniformly at

8

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

random, and with probability 1 − µ follows the empirically best policy πt = ERM {xs, ℓs}s<t by choosing at = πt(xt), where ℓs is the standard importance-weighted estimator for round s.
By standard concentration arguments (Freedman inequality), it holds with high probability that the difference between the average estimated loss and the expected loss of any policy π is bounded as

1 t

t s=1

ℓs(π(xs))

−

E(x,ℓ,m)∼D

[ℓ(π(x))]

≤O

1 t

(ln N )

t s=1

Vs

ℓs(π(xs))

+

d µt

,

where Vs[ℓs(π(xs))] is the conditional variance (given everything before round s) and is at most

K/µ. By the optimality of πt, it is then clear that the total regret of following the empirically best

policy is O t d/µt + d/µt = O dT/µ + d/µ . Further taking the uniform exploration into

account shows that the regret of ǫ-greedy has three components: the variance term O( dT/µ), the

lower-order

term O(d/µ), and the exploration

term O(µT ).

Picking the optimal µ gives O

T

2 3

regret. To improve the bound, we improve each of these three terms as described below.

Improving variance/exploration terms via action remapping. One natural idea to improve the variance term is to deploy the same variance-reduced (also known as doubly-robust) estimator ℓt (Eq. (1)) as in the adversarial case. However, the law of total variance implies:

Vt[ℓt(π(xt))] = Ext,mt,ℓt [Vat [ℓt(π(xt))|xt, mt, ℓt]] + Vxt,mt,ℓt [Eat [ℓt(π(xt))|xt, mt, ℓt]],

where

one

can

verify

that

the

ﬁrst

term

is

at

most

KE µT

,

but

the

second

term

is√just

Vxt,mt,ℓt [ℓt(π(xt))]

and is not related to E. Simply bounding the second term by 1 leads to Ω( T ) regret already.

We propose to address this issue by ﬁrst shifting the variance-reduced estimator by mt(a∗t ), where a∗t = argmina∈[K] mt(a) is again the action with the smallest predicted loss. In other words,

we use a new biased estimator: ℓt(a) = ℓt(a) − mt(a∗t ) = ℓt(ap)−t(ma)t(a) ½[at = a] + mt(a) − mt(a∗t ).

Moreover, we apply the same action remapping technique using the mapping φt : [K] → At as in

the adversarial case (Eq. (3)). To see why this is useful, note that the variance term now becomes

Vt ℓt(φt(π(xt))) ≤ Ext,mt,ℓt,at ℓt(φt(π(xt))) − mt(a∗t ) 2

≤ 2Ext,mt,ℓt,at
+ 2Ext,mt ≤ 2µK Ext,mt,ℓt

(ℓt(φt(π(xt))) − mt(φt(π(xt))))2 ½ [at = φt(π(xt))]
p2t (φt(π(xt))) (mt(φt(π(xt))) − mt(a∗t ))2 (using (a + b)2 ≤ 2a2 + 2b2) (ℓt(φt(π(xt))) − mt(φt(π(xt))))2 + 2σ2 ≤ 2KE + 2σ2, (4)
µT

which improves over the variance term Vt[ℓt(π(xt))] if σ is small. Also note that with action remapping, we only explore actions in At, and thus by√the exact same arguments as in the adversarial case, the exploration term also becomes µT σ + 2µ ET , again better than the naive approach as
long as σ is small. Therefore, remapping improves both the variance and the exploration term.
It remains to analyze the bias from both the shifted estimator and the remapping. The former in fact does not introduce any bias for the regret since the shift mt(a∗t ) is the same for all actions. The latter introduces total bias O(E/σ), again by the same analysis as in the adversarial case. With

9

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Algorithm 2 ǫ-Greedy with Action Remapping (and Catoni’s estimator)

Parameters: threshold σ > 0, exploration probability µ ∈ [0, 1].

for t = 1, . . . , T do Receivext and mt. Deﬁne a∗t , At and φt as in Eq. (3). = argminπ∈Π ERM {xs, ℓs ◦ φs}s<t ,

(Option I, termed ǫ-GREEDY.AR)

Find

πt

≈ 

argminπ∈Π

Catoniα

ℓs(φs(π(xs))) s<t

 using Algorithm 3 with α =

(

2 σ2

ln(T t+K

N) E /µ)

.

(Option II, termed ǫ-GREEDY.ARC)

Calculate pt ∈ ∆K: pt(a) = (1 − µ)½[a = φt(πt(xt))] + |Aµt| ½[a ∈ At].

Sample at ∼ pt and receive ℓt(at).

Construct estimator: ℓt(a) = ℓt(ap)−t(ma)t(a) ½[at = a] + mt(a) − mt(a∗t ) for all a ∈ At.

Algorithm 3 Finding the Policy with the Smallest Catoni’s Mean

Input: context xs, loss estimator ℓs, remapping function φs, for s = 1, . . . , t − 1, and parameter α.

ln(1 + y + y2/2), if y ≥ 0, Deﬁne: ψ(y) = − ln(1 − y + y2/2), else.

Initialize:

zright

=

K µ

+

1,

zleft

=

−zright.

while zright − zleft ≥ 1/T do Let zmid = (zleft + zright)/2.

Construct cs ∈ RK for all s < t such that cs(a) = ψ α ℓs(φs(a)) − zmid .

Invoke oracle π = ERM ({xs, cs}s<t). if s<t cs(π(xs)) ≥ 0 then zleft = zmid, else zright = zmid.

Construct cs ∈ RK for all s < t such that cs(a) = ψ α ℓs(φs(a)) − zright . Return πt = ERM ({xs, cs}s<t).

these

modiﬁcations,

we

achieve

O((E

T

)

1 3

)

regret

already

(even

with

the

presence

of

the

lower-order

term). This is summarized in the following theorem (see Appendix D.1 for the proof).

√

Theorem 6 ǫ-GREEDY.AR (Algorithm 2 Option I) ensures Reg = O

dE µ

+σ

dT

+

d µ

+

µT σ

+

√ µ ET

+

E

.

For E

√ ≤ T , picking µ = min

d2

1
3,1

and σ =

E2

1
3 gives Reg =

1 √σ

ET

dT

O (dET ) 3 + dE + d .

Rem√ovin1g the lower-order term via Catoni’s estimator. To further improve the regret bound to O( ET 4 ), we need to improve the lower-order term as well. Fortunately, it turns out that this lowerorder term can be completely removed using robust mean estimators for heavy-tailed distributions, such as median of means, trimmed-mean, and Catoni’s estimator (see the survey (Lugosi and Mendelson, 2019)). In particular, we use Catoni’s estimator, as we show that it can be implemented efﬁciently via the ERM oracle.
More speciﬁcally, instead of following the policy with the smallest average estimated loss, we follow the policy with the smallest Catoni’s mean: argminπ∈Π Catoniα ℓs(φs(π(xs))) s<t

10

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

where Catoniα(y1, . . . , yn) is the root of the function f (z) =

n j=1

ψ(α(yj

−

z))

for

some

increas-

ing function ψ (deﬁned in Algorithm 3) and coefﬁcient α > 0. Generalizing the proof of Theorem

5 in Lugosi and Mendelson (2019) for i.i.d. random variables to a martingale sequence, we obtain a

concentration result without the lower-order term (see Lemma 13 in Appendix A). Furthermore, we

prove that a close approximation of this policy can be found efﬁciently via a binary search invoking

O(ln(T K/µ)) calls of the ERM oracle, detailed in Algorithm 3.

Lemma 7 Algorithm 3 invokes the ERM oracle at most O(ln(T K/µ)) times and returns a policy

πt such that: Catoniα

ℓs(φs(πt(xs)))

≤ minπ∈Π Catoniα

s<t

ℓs(φs(π(xs))) s<t + T1 .

The proof is based on the monotonicity of ψ (Appendix D.1). We remark that this result might

be of independent interest and useful for developing oracle-efﬁcient algorithms for other problems.

Combining the two key techniques above, we improve all the three terms and prove the follow-

ing theorem (see Algorithm 2 for the pseudocode and Appendix D.1 for the complete proof).

Theorem 8 ǫ-GREEDY.ARC (Algorithm 2 Option II) ensures Reg = O

dE

+

√ σ dT

+

µT σ

+

µ

√ µ ET

+

E

. Picking µ = min

d,1

√

−1

√

1√

and σ = E (dT ) 4 gives Reg = O E (dT ) 4 + dE .

σ

T

√ Similarly, this requires setting σ in terms of E, and wh√en E = Ω( T ), one could switch to the optimal algorithm (Agarwal et al., 2014) and achieve O( T ) regret. Therefore, our bound again

matches the lower bound in Theorem 1. In fact, it also enjoys a better dependence on d compared

to the adversarial case (Theorem 4).

4.1. Adaptive version with unknown E

When E is unknown, the same bound is not achievable (Theorem 2) and we relax our√goal to achieve a bound that is robust and always sublinear, and at the same time improves over O( T ) when E is
relatively small. We achieve this goal with a different approach compared to the adversarial case, by exploiting the stochasticity of the environment so we can directly estimate E in the early rounds. Speciﬁcally, we spend the ﬁrst B rounds for pure exploration (i.e., pick at uniformly at random) to collect a set of data {at, ℓt(at), mt(at)}t≤B. Then we design a novel underestimator E deﬁned as

E =T

⌈log2 T ⌉ i=0

αi

−

30 log T B

2−2i,

where

αi

=

1 B

+

B t=1

½

|ℓt(at) − mt(at)| ∈

2−i−1, 2−i

,

and [·]+ = max{·, 0}. For the rest of the game, we simply run Algorithm 2 with Option I, σ =

E

(dT

)−

1 3

,

and

µ

=

min

d2/3T −1/3, 1

. Note that here we use the simpler Option I, as it can

be veriﬁed that even without the lower-order term the regret would still be the same in this case

(moreover, Option II requires setting α in terms of E).

The idea behind this estimator is as follows. First, αi is clearly an unbiased estimator of

αi

=

1 K

K a=1

Pr

|ℓt(a) − mt(a)| ∈ (2−i−1, 2−i]

, and is thus basically estimating the histogram

of the distribution of ℓt − mt with bins of exponentially increasing size. Therefore, i αi2−2i is an

approximation

of

1 K

E

ℓt − mt

2 2

. In the deﬁnition of E, we subtract a deviation term 30 log T /B

from αi to make sure that E is an underestimator. It turns out that both the idea of underestimating

and that of estimating the histogram are critical for the analysis, allowing us to prove the following

guarantee (see Appendix D.2 for the complete pseudocode and proof). Note that this is the same

bound as in the adversarial case (Theorem 5), ignoring the dependence on d.

11

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Theorem 9 ǫ-GREEDY.VAR (Algorithm 6 in Appendix D.2) guarantees Reg = O B + K

K

2

√

E

(dT

)

1 3

.

Setting

B

=

T

1 3

gives

Reg

=

O

(K

2

√

E

(dT

)

1 3

).

ET B

+

5. Algorithms for Multiple Predictors

Finally, we extend our setting and consider learning with multiple predictors. That is, the learner

receives

M

predictors

m

1 t

,

.

.

.

,

m

M t

before

choosing

at

at

each

round.

Recall

that

in

the

adversarial

setting, the total error of the best predictor is measured by E∗ = mini∈[M]

T t=1

ℓt − mit

2 ∞

,

while

in the stochastic setting, it is measur√ed by E∗ = T mini∈[M] E(x,ℓ,m1:M )∼D

ℓ − mi

2 ∞

.

Our goal is to improve over O( T ) regret whenever E∗ and M are relatively small, assuming

E∗ is known for simplicity. In both cases, we deploy a natural idea: maintain an active set of pre-

dictors Pt ⊆ [M ] (starting from P1 = [M ]), and eliminate a predictor from this set whenever its observed total error exceeds E∗. We deﬁne mt(a) = mini∈Pt mit(a) to be the smallest predicted loss

for action a among the active predictors, and follow similar ideas of the single predictor case with

mt serving the role of the single predictor, which can be seen as a form of optimism. In addition

to this basic idea, however, extra new techniques are required for the two settings as described below.

Adversarial Environments. The only extra difference compared to Algorithm 1 is in the construc-

tion of At, the set of actions with predicted loss not larger than that of a baseline by σ. In Algorithm 1, the baseline is simply the action with the smallest predicted loss a∗t = argmina mt(a). However, with multiple predictors, we propose to (essentially) use at, the action to be chosen by

the algorithm, as the baseline. Before explaining why this is a good idea, we ﬁrst point out that this

can indeed be efﬁciently implemented, even though the scheme appears self-referential as at itself

depends on At. Indeed, this resembles the idea of sleeping experts (Freund et al., 1997), if we treat

actions outside At as asleep experts. For implementation details, see Algorithm 5 in Appendix C.3.

The ideas of the analysis are as follows. Using at as the baseline gives that the exploration

overhead and the bias introduced by remapping are both in terms of

T t=1

(ℓt

(at

)

−

mt

(at

))2

,

which

is of order O(M E∗) because each predictor can contribute at most E∗ + 1 before being eliminated

(Lemmas 17, 18 and 19). Second, note that we only need to refer to Eq. (2) (instead of the standard

analysis) when ℓt(at) ≤ mt(at), in which case we have (ℓt(at) − mt(at))2/pt(at) ≤ (ℓt(at) − mit∗(at))2/pt(at) by the deﬁnition of mt (i∗ is the best predictor). This allows us to relate the expectation of this term to E∗ as well. Put together, we prove the following theorem.

Theorem 10 With the optimal para√meters and M ′ =√max{M, K}, EXP4.MOAR (Algorithm 5

in Appendix C.3) ensures Reg = O

M

′

E

∗

(dT

)

1 4

+

dM ′E∗ + d .

Stochastic Environments. There are two extra differences compared to Algorithm 2 in this case. First, at the beginning of each round, we check if all predictors are consistent to some extent. If not, that is, if there exist two predictors who disagree with each other by a large amount on some action, then we simply choose this action deterministically, since this guarantees to reveal which predictor makes a large error for this round. Second, in the case when all predictors are consistent, instead of doing ǫ-greedy as in Algorithm 2 (which we already show is optimal for single predictor), we ﬁnd that we need to resort to the minimax optimal algorithm ILOVETOCONBANDITS (Agarwal et al., 2014) to better control the variance of the estimator. We develop a version

12

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

of it with action remapping and Catoni’s estimators. See Algorithm 7 for details. Combining everything, we prove:

Theorem 11

ILTCB.MARC (Algorithm 7 in Appendix D.3) guarantees Reg = O

M

2 3

d

2 5

(E

∗T

)

1 3

.

As a ﬁnal remark, we remind the reader of the lower bound Ω(√E∗T 41 +M ) for M ≤ √T given by Theorem 3. Our upper bound for the adversarial case has matching dependence on E∗ and T , but
not M , while our bound for the stochastic case is even looser. Closing the gap and generalizing the results to unknown E∗ are two main future directions.

Acknowledgments
The authors would like to thank Akshay Krishnamurthy and Chicheng Zhang for introducing the idea of robust mean estimator. Part of this work was done when CYW was an intern at Microsoft Research. HL and CYW are supported by NSF Awards IIS-1755781 and IIS-1943607.

References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.
Zeyuan Allen-Zhu, Se´bastien Bubeck, and Yuanzhi Li. Make the minority great again: First-order regret bound for contextual bandits. In Proceedings of the 35th International Conference on Machine Learning, 2018.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77, 2002.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 19–26, 2011.
Se´bastien Bubeck, Yuanzhi Li, Haipeng Luo, and Chen-Yu Wei. Improved path-length regret bounds for bandits. In Proceedings of the 32nd Conference On Learning Theory, 2019.
Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. In Annales de l’IHP Probabilite´s et statistiques, volume 48, pages 1148–1185, 2012.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efﬁcient, optimal and parameter-free. In Conference on Learning Theory, pages 696–726, 2019.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, 2012.

13

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Chao-Kai Chiang, Chia-Jung Lee, and Chi-Jen Lu. Beating bandits in gradually evolving worlds. In Conference on Learning Theory, pages 210–227, 2013.

Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In International Conference on Machine Learning, pages 1405–1411, 2015.

Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efﬁcient optimal learning for contextual bandits. In Proceedings of the TwentySeventh Conference on Uncertainty in Artiﬁcial Intelligence, pages 169–178, 2011.

Miroslav Dud´ık, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485–511, 2014.

Yoav Freund, Robert E Schapire, Yoram Singer, and Manfred K Warmuth. Using and combining predictors that specialize. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing, pages 334–343, 1997.

Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine learning, 80(2):165–188, 2010.

Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning Research, 12(Apr):1287–1311, 2011.

Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 128– 141, 2016.

Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual bandits with continuous actions: Smoothing, zooming, and adapting. In Conference on Learning Theory, 2019.

John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems 21, 2008.

Ga´bor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. Foundations of Computational Mathematics, 19(5):1145–1190, 2019.

Haipeng Luo.

Lecture notes 21 of introduction to online learning.

https://haipeng-luo.net/courses/CSCI699/lecture21.pdf, 2017.

Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference on Learning Theory, pages 993–1019, 2013a.

Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In Advances in Neural Information Processing Systems 26, 2013b.

James M Robins and Andrea Rotnitzky. Semiparametric efﬁciency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122–129, 1995.

Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In Proceedings of the 31st International Conference on Machine Learning, 2014.

14

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regularized learning in games. In Advances in Neural Information Processing Systems 28, 2015.
Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efﬁcient algorithms for adversarial contextual learning. In Proceedings of the 33rd International Conference on Machine Learning, 2016a.
Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Processing Systems 29, 2016b.
Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Proceedings of the 31st Conference On Learning Theory, 2018.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic environments. In Advances in neural information processing systems, pages 3972–3980, 2016.
Kai Zheng, Haipeng Luo, Ilias Diakonikolas, and Liwei Wang. Equipping experts/bandits with long-term memory. In Advances in Neural Information Processing Systems, pages 5927–5937, 2019.

Appendix A. Concentration Inequalities

Lemma 12 (Freedman’s inequality, cf. Theorem 1 of (Beygelzimer et al., 2011)) Let F0 ⊂ · · · ⊂

Fn be a ﬁltration, and X1, . . . , Xn be real random variables such that Xi is Fi-measurable,

E[Xi|Fi−1] = 0, |Xi| ≤ b, and

n i=1

E[Xi2|Fi−1]

≤

V

for some ﬁxed b ≥ 0 and V

≥ 0.

Then for

any δ ∈ (0, 1), we have with probability at least 1 − δ,

n
Xi ≤ 2
i=1

Vn log(1/δ) + b log(1/δ).

Lemma 13 (Concentration inequality for Catoni’s estimator) Let F0 ⊂ · · · ⊂ Fn be a ﬁltra-

tion, and X1, . . . , Xn be real random variables such that Xi is Fi-measurable, E[Xi|Fi−1] = µi

for some ﬁxed µi, and

n i=1

E[(Xi

−

µi)2|Fi−1]

≤

V

for some ﬁxed V .

Denote µ

1 n

n i=1

µi

and let µn,α be the Catoni’s robust mean estimator of X1, . . . , Xn with a ﬁxed parameter α > 0,

that is, µn,α is the unique root of the function

n
f (z) = ψ(α(Xi − z))
i=1

where

ln(1 + y + y2/2), if y ≥ 0, ψ(y) = − ln(1 − y + y2/2), else.

Then for any δ ∈ (0, 1), as long as n is large enough such that n ≥ α2(V + 2 log(1/δ), we have with probability at least 1 − 2δ,

|µn,α − µ| ≤ α(V +

ni=1(µi − µ)2) + 2 log(1/δ) .

n

αn

ni=1(µi − µ)2) +

15

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

In particular, if µ1 = · · · = µn = µ, we have3

|µn,α − µ| ≤ αV + 2 log(1/δ) .

n

αn

Proof The proof generalizes that of (Lugosi and Mendelson, 2019, Theorem 5) for i.i.d. random
variables, following similar ideas used in (Beygelzimer et al., 2011, Theorem 1). First, one can verify that ψ(y) ≤ ln(1 + y + y2/2) for all y ∈ R. Therefore, for any ﬁxed z ∈ R and any i, we have

Ei [exp (ψ(α(Xi − z)))]

≤ Ei 1 + α(Xi − z) + α2(Xi − z)2 2

α2Ei (Xi − µi)2 + α2(µi − z)2

= 1 + α(µi − z) +

2

α2Ei (Xi − µi)2 + α2(µi − z)2

≤ exp α(µi − z) +

2

.

(Ei[·] E[· | Fi−1]) (1 + y ≤ ey)

Deﬁne random variables Z0 = 1, and for i ≥ 1,

α2Ei (Xi − µi)2 + α2(µi − z)2

Zi = Zi−1 exp (ψ(α(Xi − z))) exp − α(µi − z) +

2

.

Then the last calculation shows Ei[Zi] ≤ Zi−1. Therefore, taking expectation over all random variables X1, . . . , Xn, we have
E[Zn] ≤ E[Zn−1] ≤ · · · ≤ E[Z0] = 1.

Further deﬁne

g(z) nα(µ − z) + 12 α2 n (µi − z)2 + 12 α2V + log 1δ
i=1

and note that f (z) ≥ g(z) implies

n ψ(α(Xi − z)) ≥ nα(µ − z) + 21 α2 n (µi − z)2 + 12 α2 n Ei

i=1

i=1

i=1

(Xi − µi)2 + log 1 δ
(by the condition of V )

n
=
i=1

α2(µi − z)2 + α2Ei (Xi − µi)2

α(µi − z) +

2

+ log 1 , δ

which further implies Zn ≥ 1/δ. By Markov’s inequality, we then have Pr[f (z) ≥ g(z)] ≤ Pr[Zn ≥ 1/δ] ≤ Pr[Zn ≥ E[Zn]/δ] ≤ δ. Note further that we can rewrite g(z) as

g(z) = nα(µ − z) + 1 α2(nz2 − 2nµz + n µ2) + 1 α2V + log 1

2

i2

δ

i=1

3. In all our applications of this lemma, we have µ1 = · · · = µn = µ.

16

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

= nα(µ − z) + 1 α2(n(z − µ)2 − nµ2 + n µ2) + 1 α2V + log 1

2

i2

δ

i=1

= nα(µ − z) + 1 nα2(z − µ)2 + 1 α2

n
µ2 − nµ2

+ 1 α2V + log

1

2

2

i

2

δ

i=1

Now we pick z to be the smaller root z0 of the quadratic function g(z), that is,

z0 = µ + 1 1 − α

1 − α2(V +

ni=1(µi − µ)2) − 2 log 1

n

n

δ

(which exists due to the condition on n). By the monotonicity of f and the fact f (µn,α) = 0 we then have
Pr [µn,α ≥ z0] = Pr [f (z0) ≥ 0] = Pr [f (z0) ≥ g(z0)] ≤ δ.
In other words, with probability at least 1 − δ, we have

µn,α − µ ≤ 1 1 − α

1 − α2(V +

ni=1(µi − µ)2) − 2 log 1

n

n

δ

≤ 1 α2(V + ni=1(µi − µ)2) + 2 log 1

α

n

n

δ

= α(V +

ni=1(µi − µ)2) + 2 log(1/δ) .

n

αn

√ (1 − 1 − x ≤ x for x ∈ [0, 1])

Finally, via a symmetric argument one can show that µ − µn,α ≤ α(V +

ni=1(µi−µ)2) + 2 log(1/δ)

n

αn

holds with probability at least 1 − δ as well. Applying a union bound then ﬁnishes the proof.

Appendix B. Proofs for Lower Bounds
In this section, we provide proofs for all the lower bounds discussed in Section 2. The techniques we use are reminiscent of those in several previous works on bandit problems that prove lower bounds for adaptive regret (Daniely et al., 2015, Theorem 3), switching regret (Wei et al., 2016, Theorem 4.1), and regret bounds in terms of the sparsity of the losses (Zheng et al., 2019, Theorem 6). While their constructions are for adversarial environments, ours are for the i.i.d. case (which is stronger for lower bounds). To make the proofs concise, we assume that numbers such as T/K are integers without rounding them.

Proof for Theorem 1. We ﬁrst prove that for any algorithm, any K ≥ 2, any T ≥ 8 × 104, and any

value V ∈ [0, T ], there exi√sts a stochast√ic environment with E ≤ V and N = (K − 1) T/K + 1

such that Reg = Ω min

V

(K

T

)

1 4

,

KT

. The con√struction is as follows. There are

T/K

possible

context-predictor-loss

tuples

{(x

(i

)

,

m

(i

)

,

ℓ

(i

)

)}i

T/K =1

,

and

in

each

round,

(xt, mt, ℓt)

is

uniformly randomly drawn from this set. The policy set Π contains (K − 1) T/K + 1 policies such

17

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

that: there is a policy π(0) that always chooses action 1 given any context; other policies are indexed by (i, k) ∈ [ T/K] × {2, . . . , K} such that

π(i,k)(x) = k if x = x(i), 1 otherwise.

Now

ﬁrst

consider
√

an

environment

with

m(i)

=

ℓ(i)

=

( 12 ,

1 2

+

σ, . . . ,

1 2

+

σ)

for

all

i,

where

σ = min

21 ,

V 2(KT )1/4

. Note that E = 0 ≤ V . Under this environment and the given algorithm, if

for all (i, k) ∈ [ T/K] × {2, . . . , K}, the expected total number of times where (xt, at) = (x(i), k)

is larger than 21 , then the algorithm’s regret against π(0) is

√



T

T T/K K

E

ℓt(at) − ℓt(π(0)(xt)) = E 

½[(xt, at) = (x(i), k)] (ℓt(k) − ℓt(1))

t=1

t=1 i=1 k=2

√



T T/K K

= E 

½[(xt, at) = (x(i), k)]σ

t=1 i=1 k=2

≥ T × (K − 1) × 1 × σ ≥ 1 √KT σ.

K

2

4

On the other hand, if there exists a pair (i∗, k∗) ∈ [ T/K] × {2, . . . , K} such that

E then by Markov’s inequality,

T
½[(xt, at) = (x(i∗), k∗)]
t=1

≤ 1, 2

T

T

Pr

½[(xt, at) = (x(i∗), k∗)] = 0 = Pr

½[(xt, at) = (x(i∗), k∗)] < 1

t=1

t=1

= 1 − Pr

T
½[(xt, at) = (x(i∗), k∗)] ≥ 1
t=1

≥ 1. 2

That is, with probability at least 21 , the learner never chooses action k∗ when she sees context x(i∗). In this case, consider another environment where all m(i) and ℓ(i) remain the same except that ℓ(i∗)

is

changed

to

( 12 ,

1 2

+

σ, . . .

,

1 2

−

σ, . . .

,

1 2

+

σ),

where

1 2

−

σ

appears

in

the

k∗√-th

coordinate.

Note

that in this new environment we again have E = T E(x,ℓ,m)

ℓ−m

2 ∞

=

T K × 4σ2 ≤ V .

Moreover,

with

probability

at

least

1 2

the

learner

never

realizes

the

change

of

the

environment

and

behaves exactly the same, since the only way to distinguish the two environments is to pick k∗ under

context x(i∗).

It remains to calculate the regret of the learner under this new environment. First, by Freedman’s

inequality

(Lemma

12),

we

have

with

probability

at

least

1

−

1 T

,

T

∗

√

√

√ KT

½[xt = x(i )] ≥ KT − 2 KT log T − log T ≥

(5)

t=1 3

18

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

where the last step uses the condition K ≥ 2 and T ≥ 8 × 104. Deﬁne events

T

T

√

E1 =

½[(xt, at) = (x(i∗), k∗)] = 0 , E2 =

½[xt = x(i∗)] ≥ KT ,

t=1 t=1 3

and use E′, Pr′ to denote the expectation and probability under the new environment. Now we lower bound the regret against π(i∗,k∗) in this environment as

√



T T/K

E′ 

½[xt = x(i)] ℓt(at) − ℓt(π(i∗,k∗)(x(i))) 

t=1 i=1

≥ Pr′[E1 ∩ E2] × E′

T
½[xt = x(i∗)] ℓt(at) − ℓt(π(i∗,k∗)(x(i∗)))

t=1

= Pr′[E1 ∩ E2] × E′

T
½[xt = x(i∗)]σ E1, E2

√ t=1 √

≥ 1 − 1 × KTσ ≥ KTσ.

2T

3

12

E1, E2

To summarize, in at least one of these two environments, the learner’s regret is

√

√

1√

Ω( KT σ) = Ω min V (KT ) 4 , KT ,

ﬁnishing the lower bound proof for stochastic environments. For adversarial environments, the only

change is to let each tuple (x(i), m(i), ℓ(i)) appear for exactly T /K times, so that E ≤ V still holds

in these two constructions under the slightly different deﬁnition for E (which is

T t=1

ℓt − mt 2∞).

It is clear that the same lower bound holds.

Proof for Theorem 2. The idea of the pr√oof is similar to that of Theorem 1. Assume there is an algorithm that guarantees for some R = o( KT ),

T

T

E

ℓt(at) − ℓt(π∗(xt)) ≤ R,

t=1

t=1

√

whenever E = 0. B√elow we show that there is an environment with E = o( KT ) where the

algorithm suffers ω( KT ) regret.

√

The construction is as follows. First, let C be a universal constant such that R + K ≤ CKT .

Further deﬁne ρ =

√R+K CKT

≤ 1, σ √

=

12 ρ 52 , and L0

=

ρ−

3 5

.

There are

64CT/K context-predictor-

loss tuples {(x(i), m(i), ℓ(i))}i=614CT/K, and in each round, (xt, mt, ℓt) is uniformly randomly drawn

from this set. The policy set contains N = Θ(T ) policies such that: there is a policy π(0) that

always chooses action 1 given any contexts; other policies are indexed by (i, j, k) ∈ [ 64CT/K] ×

[ 64CT/K] × {2, . . . , K} with i ≤ j such that

π(i,j,k)(x) = k, if x ∈ {x(i), x(i+1), . . . , x(j)}. 1, else.

19

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

We

ﬁrst

consider

the

algorithm’s

behavior

under

the

environment

with

m(i)

=

ℓ(i)

=

( 12 ,

1 2

+

σ, . . .

,

1 2

+

σ)

for

all

i.

In

this

environment,

since

E

=

0,

the

algorithm

must

guarantee

that

E T ½[at = 1] ≤ R .

(6)

t=1 σ

This is because every time at = 1, the learner incurs regret σ against π(0). Next we prove the following fact: there exists i, j ∈ [ 64CT/K] and k ∈ {2, . . . , K} such that j − i + 1 = L0 and

E

T t=1

j s=i

½[xt

=

x(s),

at

=

k]

≤ 12 . We prove it by contradiction. Assume that for all (i, j)

with j = i − 1 + L0 and all k ∈ {2, . . . , K}, E

T t=1

j s=i

½[xt

=

x(s),

at

=

k]

≥ 21 . Then we

have

√



T

T

64CT/K K

E ½[at = 1] = E 

½[xt = x(s), at = k]

t=1

t=1 s=1 k=2

≥ 64CT/K × 1 × (K − 1)

√ L0

2

≥ 64CKT ≥ 2R = R ,

4L0

ρL0 σ

which leads to a contradiction (here, we also use the fact 1

≤

L0

=

ρ−

3 5

≤

ρ−1

≤

64C T K

).

Therefore, we have shown that there exist (i∗, j∗) with j∗ − i∗ + 1 = L0 and k∗ ∈ {2, . . . , K} such

that E

T t=1

j∗ s=i∗

½[xt

=

x(s),

at

=

k∗]

≤ 12 . By Markov’s inequality, we thus have









T
Pr 

j∗

T

½[xt = x(s), at = k∗] = 0 = Pr 

j∗ ½[xt = x(s), at = k∗] < 1 ≥ 12 .

t=1 s=i∗

t=1 s=i∗

Now we consider another environment, which is the same as the one above, except that for all

s

=

i∗, i∗

+

1, . . .

, j∗,

the

k∗-th

coordinate

of

ℓ(s)

is

changed

from

1 2

+

σ

to

1 2

−

σ.

Note

that

in

this

environment,

E =O

√ L0 KT σ2

√

1

√

= O( KT ρ 5 ) = o( KT ),

where we use the fact ρ = o(1). Moreover, with probability at least 1/2, the algorithm never realizes

the change and behaves exactly the same, since the only way to distinguish the two environments is to pick k∗ under one of the contexts x(i∗), x(i∗+1), . . . , x(j∗).

It remains to calculate the regret of the learner under this environment. Deﬁne events





 T j∗



E1 = 

½[(xt, at) = (x(s), k∗)] = 0 ,

t=1 s=i∗

and





T E2 = 

j∗ ½[xt = x(s)] ≥ (j∗ − i∗ + 31)

KT/64C  .

t=1 s=i∗

20

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Note that in expectation, each context appears √ T =
64CT/K

inequality

(Lemma

12),

with

probability

at

least

1

−

1 T

,

KT/64C times. By Freedman’s

T j∗
½[xt = x(s)] ≥ (j∗ − i∗ + 1)
t=1 s=i∗

KT − 2 64C

(j∗ − i∗ + 1)

KT log T − log T 64C

(j∗ − i∗ + 1) ≥3

KT
64C .

when K ≥ 2 and T > 6 × 106C. That is, Pr[E2] ≥ 1 − 1/T . Therefore, the expected regret against π(i∗,j∗,k∗) in this new environment is lower bounded by


T
E


j∗
½[xt = x(s)] ℓt(at) − ℓt(π(i∗,j∗,k∗)(x(s))) 

t=1 s=i∗

(j∗ − i∗ + 1)

≥ Pr[E1 ∩ E2] ×

3

KT/64C σ

≥ 1 − 1 × KT/64C σL0 = Ω

2T

3

KT

ρ−

1 5

C

√ = ω( KT ).

This ﬁnishes the proof.

√

1

Pro√of for Theorem 3. When V ∗(KT ) 4 ≥ M , we only n√eed to prove a lower b√ound of

Ω(

V

∗

(K

T

)

1 4

),

which

is

shown

by

Theorem

1

already.

When

V

∗(K

T

)

1 4

≤

M

≤

T , we

construct an stochastic environment below with E∗ = 0, N = M , and K = 2, where the regret of

the algorithm is Ω(M ).

The construction is as follows (and is again similar to those in the proofs of Theorems 1 and 2). There are M − 1 different context-predictor-loss tuples {x(i), m(0,i), . . . , m(M−1,i), ℓ(i)}M i=−1 1, and in every round, (xt, m0t , . . . , mM t −1, ℓt) is uniformly randomly sampled from this set. The policy set contains N = M policies π(0), . . . , π(M−1) such that: π(0) always chooses action 1 given any contexts; for i ∈ [M − 1], π(i)(x) = 2 if x = x(i), and otherwise π(i)(x) = 1.
Now consider an environment where ℓ(i) = m(0,i) = ( 12 , 1) for all i ∈ [M − 1], m(j,i) = ( 21 , 1) for all i, j ∈ [M − 1] with i = j, and m(i,i) = ( 12 , 0) for all i ∈ [M − 1]. Clearly, the predictor m0 is a perfect predictor in this environment and thus E∗ = 0.

In this environment, if the expected number of times the learner chooses action 2 is larger than

M2−1 ,

then

she

already

suffers

an

expected

regret

of

M −1 2

×

1 2

compared

to

policy

π (0) ,

which

always chooses action 1. On the other hand, if the expected number of times the learner chooses

action 2 is smaller than M2−1 , then there exists an i∗ ∈ [M − 1] such that the expected number

of times the learner chooses action 2 upon seeing x(i∗) is less than 21 . By Markov’s inequality,

T t=1

½[(xt, at)

=

(x(i∗), 2)]

=

0

holds

with

probability

at

least

12 .

That

is,

with

probability

at

least

1 2

,

the

learner

never

picks

action

2

when

the

context

is

x(i∗ ) .

Now consider a different environment where the only difference is that the ℓ(i∗)(2) is changed

from 1 to 0. With probability at least 12 , the learner does not realizes the change and behaves

exactly the same. The expected regret compared to policy π(i∗) is thus Ω

T M −1

×

1 2

in this new

21

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

environment. Moreover, notice that in this new environment, E∗ = 0 still holds because mi∗ now

becomes the perfect predictor.

To sum up, we have shown that when there are M > 1 predictors, even if E∗ = 0, the learner

has to suffer Ω

min

M

−

1,

T M −1

= Ω(M ) regret.

Appendix C. Omitted Details for Adversarial Environments
In this section, we provide omitted details for the adversarial case, including the proof of Theorem 4 on the guarantee of Algorithm 1 for the case with single predictor and known E (Section C.1), the adaptive version of Algorithm 1 and its analysis when E is unknown (Section C.2), and the algorithm and analysis for multiple predictors (Section C.3).

C.1. Proof of Theorem 4

We ﬁrst prove a lemma showing a somewhat non-conventional analysis of the optimistic EXP4 update. We denote the KL divergence of two distributions Q and P by D(Q, P ) = π∈Π Q(π) ln PQ((ππ)) .

Lemma 14 For any η > 0, Mt, Lt ∈ RN , and distribution Q′t ∈ ∆Π, deﬁne two distributions Qt, Q′t+1 ∈ ∆Π such that

Qt(π) ∝ Q′t(π) exp (−ηMt(π)) ,

Q′t+1(π) ∝ Q′t(π) exp (−ηLt(π)) .

(7)

Then there exists ξt ∈ ∆Π such that for any Q∗ ∈ ∆Π, we have

Qt − Q∗, Lt

≤

D

(Q

∗

,

Q

′ t

)

−

D

(Q

∗

,

Q

′ t+

1

)

+

2η

ξt(π) (Lt(π) − Mt(π))2 .

(8)

η π∈Π

Moreover, if Lt(π) − Mt(π) ≥ − η1 holds for all π, then we have for any Q∗ ∈ ∆Π,

Qt − Q∗, Lt

≤

D

(Q

∗

,

Q

′ t

)

−

D

(Q

∗

,

Q

′ t+

1

)

+

η

Qt(π) (Lt(π) − Mt(π))2 .

(9)

η π∈Π

Proof First, we rewrite the updates in the standard optimistic online mirror descent framework: Qt = argminQ∈∆Π Ft(Q) and Q′t+1 = argminQ∈∆Π Ft′(Q) where

Ft(Q) = η Q, Mt + D(Q, Q′t),

Ft′(Q) = η Q, Lt + D(Q, Q′t).

(10)

Applying Lemma 6 of (Wei and Luo, 2018) shows

Qt − Q∗, Lt

≤

D

(Q

∗

,

Q

′ t

)

−

D

(Q

∗

,

Q

′ t+

1

)

+

Qt − Q′

, Lt − Mt − 1 D(Q′

, Qt).

η

t+1

η

t+1

Next, we prove Eq. (8). By Taylor expansion, there exists some convex combination of Qt and Qt+1, denoted by ξt, such that
Ft′(Qt) − Ft′(Q′t+1) = ∇Ft′(Q′t+1)(Qt − Q′t+1) + 12 (Qt − Q′t+1)⊤∇2Ft′(ξt)(Qt − Q′t+1)

22

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

= ∇F ′(Q′ )(Qt − Q′ ) + 1 (Qt(π) − Q′t+1(π))2

t t+1

t+1 2
π∈Π

ξt(π)

≥ 1 (Qt(π) − Q′t+1(π))2 ,

2 π∈Π

ξt(π)

where the last step is due to the optimality of Q′t+1. On the other hand, we also have

Ft′(Qt) − Ft′(Q′t+1) = Ft(Qt) − Ft(Q′t+1) + η Qt − Q′t+1, Lt − Mt ≤ η Qt − Q′t+1, Lt − Mt

(by optimality of Qt)

(Qt(π) − Q′t+1(π))2 1/2

1/2 2

≤ η ξt(π)

ξt(π)(Lt(π) − Mt(π))

.

π∈Π

π∈Π

(Cauchy-Schwarz inequality)

Combining the two inequalities shows

Qt − Q′t+1, Lt − Mt ≤ 2η ξt(π)(Lt(π) − Mt(π))2,
π∈Π

which proves Eq. (8) (since D(Q′t+1, Qt) is non-negative).

To

proves

Eq.

(9),

note

that

Q′t+1(π)

=

1 Z

Q

t

(π

)

ex

p

(−

η

(L

t

(π

)

−

Mt(π)))

where

Z = Qt(π) exp(−η(Lt(π) − Mt(π)))
π∈Π

is the normalization factor. Direct calculation shows

Qt − Q′t+1, Lt − Mt − η1 D(Q′t+1, Qt)

= Qt − Q′t+1, Lt − Mt − η1 Q′t+1(π) ln Q′t+1(π) + η1 Q′t+1(π) ln Qt(π)

π∈Π

π∈Π

π∈Π

=

Qt, Lt − Mt + 1 ln Z

π∈Π η

≤

Qt, Lt − Mt + 1 ln Qt(π) 1 − η(Lt(π) − Mt(π)) + η2(Lt(π) − Mt(π))2

π∈Π

η π∈Π

(by e−z ≤ 1 − z + z2 for z ≥ −1 and the condition η(Lt(π) − Mt(π)) ≥ −1)

=

Qt, Lt − Mt + 1 ln

π∈Π η

1−η

≤ η Qt(π) (Lt(π) − Mt(π))2 .

π∈Π

Qt, Lt − Mt

+ η2 Qt(π)(Lt(π) − Mt(π))2
π∈Π
(by ln(1 + z) ≤ z)

This ﬁnishes the proof.

Proof [of Theorem 4] We directly apply Lemma 14 with Mt(π) = mt(φt(π(xt))) and Lt(π) = ℓt(φt(π(xt))) and use Eq. (8) with Q∗ concentrating on the best policy π∗. Summing over t gives

T

T

Qt(π)ℓt(φt(π(xt))) − ℓt(φt(π∗(xt)))

t=1 π∈Π

t=1

23

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤

D

(Q

∗

,

Q

′ 1

)

+

2η

T

η

2
ξt(π) ℓt(a) − mt(a) .

t=1 a∈At π:φt(π(xt))=a

≤ ln N + 2η T η t=1 a∈At

2
ℓt(a) − mt(a)

ln N

T

= η + 2η

t=1

2
ℓt(at) − mt(at) ,

where in the last step we use the fact that ℓt(a) − mt(a) is non-zero only if a = at. Note that this basically proves Eq. (2) (with remapping). The rest of the proof follows the analysis sketch in Section 3. First, we plug in the deﬁnition of ℓt and continue to bound the last expression by

ln N

T (ℓt(at) − mt(at))2 ln N 2ηK T

η + 2η

pt(at)2

≤ η +µ

t=1

t=1

ℓt − mt

2
∞,

pt(at)

where the last step uses the fact pt(at) ≥ µ/|At| ≥ µ/K. Taking expectation on both sides leads to

E T Qt(π)ℓt(φt(π(xt))) − T ℓt(φt(π∗(xt))) ≤ lnηN + 2ηKµ 2E . (11)

t=1 π∈Π

t=1

Next, consider the expected loss of the algorithm at time t:





pt(a)ℓt(a) = (1 − µ) 

Qt(π) ℓt(a) + µ

ℓt(a)

a∈At

a∈At π:φt(π(xt))=a

|At| a∈At

= (1 − µ) Qt(π)ℓt(φt(π(xt))) + µ

ℓt(a).

π∈Π |At| a∈At

Combining with Eq. (11) shows

E

T

ℓ (a )

T
≤ (1 − µ)

ℓ (φ (π∗(x ))) + ln N + 2ηK2E +

T

µ

tt

tt

t

η

µ

|At|

ℓt(a)

t=1

t=1

t=1

a∈At

T

∗

ln N 2ηK2E T µ

= ℓt(φt(π (xt))) + η + µ + |At|

(ℓt(a) − ℓt(φt(π∗(xt)))) ,

t=1

t=1

a∈At

where the last term can be further bounded as (by the deﬁnition of At):

ℓt(a) − ℓt(φt(π∗(xt)) = ℓt(a) − mt(a) + mt(a) − mt(φt(π∗(xt)) + mt(φt(π∗(xt)) − ℓt(φt(π∗(xt))
≤ 2 ℓt − mt ∞ + σ.

This shows

T

E

ℓt(at)

t=1

≤ T ℓt(φt(π∗(xt))) + lnηN + 2ηKµ 2E + µT σ + 2µ T

t=1

t=1

ℓt − mt ∞

24

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤ T ℓt(φt(π∗(xt))) + lnηN + 2ηKµ 2E + µT σ + 2µ√ET .
t=1
(Cauchy-Schwarz inequality)

It remains to bound the bias due to remapping: when π∗(xt) = φt(π∗(xt)) we have φt(π∗(xt)) = a∗t , mt(a∗t ) ≤ mt(π∗(xt)) − σ, and

ℓt(φt(π∗(xt))) − ℓt(π∗(xt))

= ℓt(a∗t ) − mt(a∗t ) + mt(a∗t ) − mt(π∗(xt)) + mt(π∗(xt)) − ℓt(π∗(xt)),

≤ 2 ℓt − mt ∞ − σ ≤

ℓt − mt

2
∞,

(12)

σ

where the last step is by the AM-GM inequality. When π∗(xt) = φt(π∗(xt)), the above holds trivially. Summing over t we have thus shown

Reg ≤ ln N + 2ηK2E + µT σ + 2µ√ET + E ,

η

µ

σ

ﬁnishing the proof.

C.2. Adaptive Version of Algorithm 1

The adaptive version of Algorithm 1 is shown in Algorithm 4. We observe that when E is unknown,

choosing actions only from At is problematic, because in the case when the predictors are highly inaccurate (that is, large E), the environment can be such that the good actions are always outside

At but the learner can never realize that. Based on this intuition, we remove the action remapping

component in this case, implemented by simply setting σ = 1.

For the exploration parameter µ, note that its optimal choice is independent of E already in the

known E case (see Theorem 4), which turns out to be also the case here (albeit with a different

value).

Also note that standard Optimistic Online Mirror Descent analysis requires using the same learn-

ing rate in Lines 2 and 6 (see (Wei and Luo, 2018) for example). However, using ηt in both places is

invalid since at and ℓt(at) are unknown when executing Line 2, while using ηt−1 in both places also

leads to some technical issue due to the large magnitude of loss estimators. Instead, we use ηt−1

in Line 2 and ηt in Line 6, and carefully bound the bias introduced by this learning rate mismatch.

Analyzing this learning rate mismatch is the key of our analysis, as we will show later.

A minor but also necessary difference with Algorithm 1 is that we also enforce Qt and Q′t to be

in the clipped simplex ∆Π =

Q ∈ ∆Π : Q(π) ≥

1 NT

,

∀π ∈ Π

, by writing the updates of Qt and

Q′t in the Optimistic Online Mirror Descent form over ∆Π.

Proof [of Theorem 5] Deﬁne m′t = ηtη−t 1 mt. Note that the update in Line 2 and Line 6 in Algorithm 4

is the same as Eq. (10) with η = ηt, Mt(π) = m′t(π(xt)), and Lt(π) = ℓt(π(xt)), except that the

constraint set becomes ∆Π. By the exact same arguments as the proof of Lemma 14, we conclude

that Eq. (8) holds for any Q∗ ∈ ∆Π. In particular, we pick Q∗ =

1

−

1 T

eπ∗

+

1 NT

1

∈

∆Π,

where

25

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Algorithm 4 EXP4.OVAR: Optimistic EXP4 with Variance-adaptivity and Action Remapping

Parameter: exploration probability µ ∈ [0, 1].

Deﬁne: ∆Π =

Q

∈

∆Π

:

Q(π)

≥

1 NT

,

∀π

∈

Π

and D(Q, P ) =

Initialize:

Q′1(π)

=

1 N

for

all

π

∈

Π

and

η0

=

for t = 1, . . . , T do

log(N T ).

π∈Π Q(π) ln QP ((ππ)) .

1 Receive xt and mt.

2 Calculate

Qt = argmin ηt−1 Q(π)mt(π(xt)) + D(Q, Q′t) .

Q∈∆Π

π∈Π

3

Calculate pt ∈ ∆K:

pt(a) = (1 − µ)

π:π(xt)=a

Qt(π)

+

µ K

.

4 Sample at ∼ pt and receive ℓt(at).

5 Construct estimator: ℓt(a) = ℓt(ap)−t(ma)t(a) ½[at = a] + mt(a) for all a ∈ [K].

6 Calculate

Q′t+1 = argmin ηt Q(π)ℓt(π(xt)) + D(Q, Q′t)

Q∈∆Π

π∈Π

7 where

t

(ℓt(at) − mt(at))2

−

1 2

ηt = log(N T ) 1 +

pt(at)2

.

(13)

s=1

eπ∗

is

the

distribution

that

concentrates

on

π∗

and

1 N

1

is

the

uniform

distribution

over

Π.

With

this

Q∗, summing Eq. (8) over t, we get

T
Qt(π)ℓt(π(xt)) −
t=1 π∈Π

1− 1 T

T ℓt(π∗(xt)) − N1T T

ℓ(π(xt))

t=1

t=1 π∈Π

T
≤

D(Q∗, Q′t) − D(Q∗, Q′t+1) + 2 T η

ξ (π)

ℓ (π(x )) − m′ (π(x ))

2
.

(14)

t=1

ηt

t

t

t=1 π∈Π

t

t

t

t

The ﬁrst term on the right-hand side of Eq. (14) is equal to

D(Q∗, Q′1) + T D(Q∗, Q′ ) 1 − 1 − D(Q∗, Q′T +1) .

(15)

η1 t=2 t ηt ηt−1 ηT

26

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Note that for all Q ∈ ∆Π, we have D(Q∗, Q) =

π∈Π Q∗(π) log

Q∗(π) Q(π)

≤

log(N T ).

Since

1 ηt

≥

1 ηt−1

,

we

can

thus

upper

bound

Eq. (15) by

π∈Π Q∗(π) log

1 1/(N T )

=

log(N T ) T

+ log(N T )

η1

t=2

1− 1 ηt ηt−1

= log(N T ) . ηT

We continue to show that the second term on the right-hand side of Eq. (14) is in fact also of order O logη(NT T ) . First, by direct calculation we have

T
ηt

ξt(π) ℓt(π(xt)) − m′t(π(xt)) 2

t=1 π∈Π

T
= ηt ξt(π)
t=1 π∈Π

(ℓt(at) − mt(at))½[πt(xt) = at]

ηt−1

2

pt(at)

+ mt(π(xt)) − ηt mt(π(xt))

T
≤ 2ηt
t=1

(ℓt(at) − mt(at))2 pt(at)2

T

ηt−1 2

+ 2ηt 1 − ηt .

t=1

To deal with the ﬁrst term in the last expression, we deﬁne bt = (ℓt(atp)t−(amt)t2(at))2 so that

T
ηt
t=1

(ℓt(at) − mt(at))2 pt(at)2

T

=

(log

N

T

)

1 2

t=1


=

O

(log

N

T

)

1 2

bt ≤ (log N T ) 12 Ts=1 bt √ dx

1+

t s=1

bt

0

1+x

1

T
1+ b

2
=O

log(N T )

.

t

ηT

t=1

To deal with the second term, simply note that

T
ηt
t=1

1 − ηt−1 ηt

2 T1

=

(η − η

)2

≤

(log

N

T

)

1 2

T
(η

− η ) ≤ log(N T ) .

ηt t t−1

ηT

t−1 t

ηT

t=1

t=1

Combining everything above, we conclude that the right-hand side of Eq. (14) is upper bounded by



1

O

log(N T )

= O

T
log(N T ) + log(N T )

(ℓt(at) − mt(at))2

2


ηT t=1 pt(at)2



1

= O  log(N T ) + K log(N T ) T (ℓt(at) − mt(at))2

2
,

µ t=1

pt(at)

whose expectation is upper bounded by (using Jensen’s inequality)



1

O  log(N T ) + K log(N T ) E T (ℓt(at) − mt(at))2

2


µ

t=1

pt(at)

27

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

K2 log(N T )E

1 2

E

= O log(N T ) + µ

=O d µ .

Now we lower bound the expectation of the left-hand side of Eq. (14):

T

1

E

Qt(π)ℓt(π(xt)) − 1 − T

t=1 π∈Π

T ℓt(π∗(xt)) − N1T T

ℓ(π(xt))

t=1

t=1 π∈Π

T
≥E

T
Qt(π)ℓt(π(xt)) − ℓt(π∗(xt)) − 1

t=1 π∈Π 

t=1





TK

= E

pt(a) + µ

Qt(π) − Kµ  ℓt(a) − T ℓt(π∗(xt)) − 1

t=1 a=1

π:π(xt)=a

t=1

TK

T

≥E

pt(a)ℓt(a) − ℓt(π∗(xt)) − 1 − µT

t=1 a=1

t=1

T

T

= E ℓt(at) − ℓt(π∗(xt)) − 1 − µT.

t=1

t=1

Combining the bounds for both sides of Eq. (14) ﬁnishes the proof.

C.3. Algorithms and Analysis for Multiple Predictors

The pseudocode of our algorithm for multiple predictors is in Algorithm 5. As discussed in Section 5, there are several extra ingredients compared to Algorithm 1 in this case. First, we maintain an active set Pt of predictors that are still plausibly the best predictor:

T
i∗ = argmin
i∈[M ] t=1

ℓt − mit 2∞.

Speciﬁcally, the variable V i maintains the remaining error “budget” for each predictor (starting from E∗), and is decreased by (ℓt(at) − mit(at))2 at the end of each round. Then Pt is simply the set of predictors with a non-negative error budget. Second, for each action a, we let

mt(a) = min mit(a),
i∈Pt

to be the smallest prediction among all active predictors, and treat mt as if it was the only prediction
similar to the single predictor case, which can be seen as a form of optimism.
Finally and perhaps most importantly, we construct the set At using a different baseline. Essentially, the baseline is at, the action to be chosen by the algorithm, which is of course not available before constructing At. However, instead of using mt(at) as the baseline in the deﬁnition of At, we use the expected prediction pt, mt , and instead of explicitly remapping an action a ∈/ At to be at, we change the values of ℓt(a) and mt(a) to pt, ℓt and pt, mt respectively for these actions. While this is still a self-referential scheme since the construction of pt depends on At,

28

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Algorithm 5 EXP4.MOAR: Optimistic EXP4 with Action Remapping for Multiple predictors

Parameters: learning rate η > 0, threshold σ > 0, exploration probability µ ∈ [0, 12 ], best error E∗.

Initialize:

Q′1(π)

=

1 N

for

all

π

∈

Π,

budget

Vi

=

E∗

for

all

i

∈

[M ],

active

set

P1

=

[M ].

for t = 1, . . . , T do

Receive xt and mit for all i ∈ [M ]. Let mt(a) = mini∈Pt mit(a) for all a ∈ [K].

Step 1. Jointly decide the awake action set At and the action distribution pt. Let b1, b2, . . . , bK be a permutation of [K] such that

mt(b1) ≤ mt(b2) ≤ . . . ≤ mt(bK ).

for j = 1, 2, . . . , K do Set A = {b1, . . . , bj}. Calculate p ∈ ∆K:
 (1 − µ) p(a) = 0,

π:π(xt)=a Q′t(π) exp π:π(xt)∈A Q′t(π) exp

−ηmt (π(xt )) −ηmt (π(xt ))

+ |Aµ| ,

for a ∈ A, for a ∈/ A.

if j = K or mt(bj) ≤ p, mt + σ ≤ mt(bj+1) then At = A, pt = p, break.

Step 2. Choose an action and construct loss estimators. Sample at ∼ pt and receive ℓt(at). Construct estimator:

ℓt(a) =

(ℓt(a)−mptt((aa)))½[at=a] + mt(a), a∈At pt(a)ℓt(a),

for a ∈ At, for a ∈/ At.

Step 3. Make updates. Calculate Q′t+1 ∈ ∆Π: Q′t+1(π) ∝ Q′t(π) exp −ηℓt(π(xt)) .
for i ∈ Pt do V i ← V i − (ℓt(at) − mit(at))2.
Update active set Pt+1 = i ∈ Pt : V i ≥ 0 .

we show that this can in fact be implemented efﬁciently by trying all the K possibilities for At: {b1}, {b1, b2}, . . . , {b1, b2, . . . , bK }, where b1, . . . , bK are such that mt(b1) ≤ mt(b2) ≤ . . . ≤ mt(bK ). The concrete procedure is detailed in Step 1 of Algorithm 5, and we prove in the following
lemma that it does exactly what we want.

Lemma 15 Deﬁne Mt, Lt ∈ RK, Qt ∈ ∆Π, and qt ∈ ∆At as

Mt(a) = mt(π(xt)), if π(xt) ∈ At, , and Lt(a) = ℓt(π(xt)), if π(xt) ∈ At, ,

pt, mt , otherwise

pt, ℓt , otherwise

29

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Qt(π) ∝ Q′t(π) exp (−ηMt(π)) , and

qt(a) =

π:π(xt)=a Qt(π) . π:π(xt)∈At Qt(π)

Then Algorithm 5 ensures the following properties:

Q′t+1(π) ∝ Q′t(π) exp (−ηLt(π)) ,

(16)

pt(a) = (01, − µ)qt(a) + |Aµt| , eiflsae,∈ At, (17)

At = {a ∈ [K] : mt(a) ≤ pt, mt + σ}.

(18)

Proof The ﬁrst property on Q′t+1 is simply by the deﬁnition of Lt and pt, ℓt = The second equation is also clear by the deﬁnition of Qt:

a∈At pt(a)ℓt(a).

qt(a) =

π:π(xt)=a Qt(π) = π:π(xt)∈At Qt(π)

π:π(xt)=a Q′t(π) exp π:π(xt)∈A Q′t(π) exp

− ηmt(π(xt)) − ηmt(π(xt)) .

The last equation clearly holds when j < K and the condition mt(bj) ≤ p, mt + σ ≤ mt(bj+1) holds and triggers the “break” statement, so it remains to prove Eq. (18) if the “break” statement is triggered in the last iteration when j = K, in which case we have for all j < K,

pj, mt + σ < mt(bj ) or pj, mt + σ > mt(bj+1)

(19)

where pj is the value of p in the j-th iteration. Note that for all k ≤ j, we have pj(bk) ≥ pj+1(bk) by the deﬁnition of p, and also pj+1(bj+1) = k≤j(pj(bk) − pj+1(bk)). With these facts we prove pj+1, mt ≥ pj, mt below:

pj+1, mt

= pj+1(bj+1)mt(bj+1) + pj+1(bk)mt(bk)
k≤j

=

pj (bk) − pj+1(bk) mt(bj+1) + pj+1(bk)mt(bk)

k≤j

≥

pj (bk) − pj+1(bk) mt(bk) + pj+1(bk)mt(bk)

k≤j

= pj, mt .

(mt(bj+1) ≥ mt(bk), ∀k ≤ j)

Therefore, realizing Eq. (19), we have

p1, mt + σ = mt(b1) + σ > mt(b1) and thus p2, mt + σ ≥ p1, mt + σ > mt(b2),

p1, mt

+ σ > mt(b2) by

which in turn further implies (by repeatedly using Eq. (19) and pj+1, mt ≥ pj, mt )

p3, mt + σ ≥ p2, mt + σ > mt(b3)

··· ,

30

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

pK , mt + σ ≥ pK−1, mt + σ > mt(bK ). The last statement proves Eq. (18) again.

With this fact, the analysis of the algorithm follows similar steps as in the proof of Theorem 4. First, we apply Lemma 14 to prove the following.

Lemma 16 Algorithm 5 ensures for any π∗ ∈ Π,

T
E

Qt(π)ℓt(π(xt)) ≤ E T ℓt(π∗(xt)) + O lnηN + ηKµ2E∗ + ηKMµ E∗ .

t=1 π∈Π

t=1

Proof

We apply Lemma 14 with Mt and Lt deﬁned in Lemma 15. First note that



ℓt(at) − mt(at),

Lt(π)

−

Mt(π)

=

ℓt (at )−mt (at )
 pt(at)

0,

if π(xt) ∈/ At, if π(xt) = at, if at = π(xt) ∈ At.

Therefore, when ℓt(at) ≥ mt(at), the condition L(π)−Mt(π) ≥ −1/η holds and we apply Eq. (9) and bound the last term by

η Qt(π) (Lt(π) − Mt(π))2

π∈Π



= η

(ℓt(at) − mt(at))2

Qt(π)

p2t (at)

+

Qt(π) (ℓt(at) − mt(at))2

π:π(xt)=at

π :π (xt )∈/ At

≤ η qt(at) (ℓt(at) − mt(at))2 + (ℓt(at) − mt(at))2 p2t (at)

≤ η (ℓt(at) − mt(at))2 + (ℓt(at) − mt(at))2 (1 − µ)pt(at)

(by Eq. (17))

≤η·O

(ℓt(at) − mt(at))2 pt(at)

(µ ≤ 1/2)

≤ ηK · O (ℓt(at) − mt(at))2 µ

(pt(at) ≥ µ/K)

On the other hand, if ℓt(at) ≤ mt(at), we apply Eq. (8) and bound the last term by

2η ξt(π) (Lt(π) − Mt(π))2

π∈Π



= 2η 

(ℓt(at) − mt(at))2

ξt(π)

p2t (at)

+

ξt(π) (ℓt(at) − mt(at))2

π:π(xt)=at

π:π(xt)∈/At

≤ 2η

(ℓt(at) − mt(at))2 + (ℓt(at) − mt(at))2 p2t (at)

31

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤ ηK · O µ
≤ ηK · O µ

(ℓt(at) − mt(at))2 , pt(at)
ℓt(at) − mit∗ (at) 2 . pt(at)

(pt(at) ≥ µ/K) (ℓt(at) ≤ mt(at) ≤ mit∗ (at))

Combining the two situations, setting Q∗ to concentrate on π∗, and summing over t show:

T Qt(π)ℓt(π(xt)) ≤ T ℓt(π∗(xt)) + lnηN

t=1 π∈Π

t=1

ηK

T ℓt(at) − mit∗ (at) 2

2

+ µ ·O

pt(at)

+ (ℓt(at) − mt(at)) .

t=1

Taking expectation on both sides we have

T
E

Qt(π)ℓt(π(xt)) ≤ E

T ∗

ln N

ℓt(π (xt)) + η

t=1 π∈Π



t=1



+ ηK · O E  T

ℓ (a) − mi∗(a) 2 + (ℓ (a ) − m (a ))2

µ

t

t

tt

tt

t=1 a∈[K]

≤ E T ℓt(π∗(xt)) + lnηN + O ηKµ2E∗ + ηKMµ E∗ ,
t=1

where in the last step we use Lemma 19. This ﬁnishes the proof.

Next, we relate the term E[

T t=1

π∈Π Qt(π)ℓt(π(xt)] to the loss of the algorithm, and the

term E[

T t=1

ℓt(π∗(xt))]

to

the

loss

of

the

best

policy,

in

the

following

two

lemmas

respectively.

Lemma 17 Algorithm 5 ensures

T

T

E

ℓt(at) ≤ E

Qt(π)ℓt(π(xt)) + O

t=1

t=1 π∈Π

µM E∗T + µT σ + µ2T .

Proof With Zt = π:π(xt)∈At Qt(π) so that Ztqt(a) = loss of the algorithm as

π:π(xt)=a Qt(π), we rewrite the expected

E [ℓt(at)]

=E

pt(a)ℓt(a)

a∈At

= E (1 − µ) qt(a)ℓt(a) + µ

ℓt(a)

a∈At

|At| a∈At

= E (1 − µ) (Ztqt(a) + (1 − Zt) qt(a)) ℓt(a) + µ

ℓt(a)

a∈At

|At| a∈At

(by Eq. (17))

32

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

= E (1 − µ)Zt qt(a)ℓt(a) + (1 − Zt)

a∈At

a∈At

pt(a) − µ |At|

µ ℓt(a) + |At| ℓt(a)
a∈At
(by Eq. (17))

= E Zt qt(a)ℓt(a) + (1 − Zt) pt, ℓt + µZt

a∈At

a∈At

ℓt(a) − qt(a)ℓt(a) |At|

=E

Qt(π)ℓt(π(xt)) + µZt

ℓt(a) − qt(a)ℓt(a) ,

π∈Π a∈At |At|

where in the last step we use the fact

Qt(π)ℓt(π(xt)) =

Qt(π)ℓt(a) +

Qt(π) pt, ℓt

π∈Π

a∈At π:π(xt)=a

π:π(xt)∈/At

= Zt qt(a)ℓt(a) + (1 − Zt) pt, ℓt
a∈At

by the deﬁnition of ℓt. It thus remains to bound E we decompose into four terms:

Tt=1 µZt a∈At ℓ|tA(at|) − qt(a)ℓt(a) , which

T µZt E
|At|

(ℓt(a) − mt(a)) ,

t=1

a∈At

T

E

µZt

mt(a) − qt(a)mt(a) , |At|

t=1

a∈At

T

E

µZt

qt(a)mt(a) − qt(a)mit∗ (a) ,

t=1

a∈At

T

E

µZt

qt(a)mit∗ (a) − qt(a)ℓt(a) .

t=1

a∈At

The ﬁrst term can be bounded as

T µZt E
|At|

(ℓt(a) − mt(a))

t=1

a∈At

Tµ

≤E

|At|

|ℓt(a) − mt(a)|

t=1

a∈At



T
≤ E

µT |At|

µ (ℓt(a) − mt(a))2 |At|

t=1 a∈At

t=1 a∈At





T

≤ µT · E 

pt(a)(ℓt(a) − mt(a))2

t=1 a∈At

(Cauchy-Schwarz inequality) (pt(a) ≥ µ/|At|)

33

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

T
≤ µT · E (ℓt(at) − mt(at))2
t=1
≤ O µM E∗T .

(Jensen’s inequality) (by Lemma 19)

The second term can be bounded as

T

E

µZt

t=1

a∈At

mt(a) − qt(a)mt(a) |At|

≤E

T

µZt

t=1

a∈At

pt, mt + σ − qt(a)mt(a) |At|

T

≤ µT σ + E µZt (pt(a) − qt(a)) mt(a)

t=1

a∈At

= µT σ + µ2E

T
Zt
t=1 a∈At

1 − qt(a) |At|

mt(a)

≤ µT σ + µ2T.

(by Eq. (18)) (by Eq. (17))

The third term is simply non-positive by the deﬁnition of mt, and ﬁnally the four term can be bounded by (using Cauchy-Schwarz inequality again)

T

E

µZt

qt(a)mit∗(a) − qt(a)ℓt(a)

t=1

a∈At

T

∗

√

≤ µE

ℓt − mit ∞ ≤ µ E∗T ,

t=1

which can be absorbed by the bound of the ﬁrst term since µ ≤ 1. Combining all the bounds proves the lemma.

Lemma 18 Algorithm 5 ensures

E T ℓt(π∗(xt)) ≤ T ℓt(π∗(xt)) + O MσE∗ .

t=1

t=1

Proof Note that

E ℓt(π∗(xt)) = ℓt(π∗(xt)) + E [½[π∗(xt) ∈/ At] (ℓt(at) − ℓt(π∗(xt)))] ,

where the second term is bounded as

E ½[π∗(xt) ∈/ At] ℓt(at) − mt(at) + mt(at) − mt(π∗(xt)) + mit∗(π∗(xt)) − ℓt(π∗(xt))

(mt(a) ≤ mit∗(a))

= E ½[π∗(xt) ∈/ At] ℓt(at) − mt(at) + pt, mt − mt(π∗(xt)) + mit∗(π∗(xt)) − ℓt(π∗(xt))

≤ E |ℓt(at) − mt(at)| + ℓt − mit∗ ∞ − σ

(by Eq. (18))

34

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

= E |ℓt(at) − mt(at)| − σ2 + ℓt − mit∗ ∞ − σ2

≤E

(ℓt(at) − mt(at)|)2 +

ℓt − mit∗

2 ∞

.

2σ

2σ

(AM-GM inequality)

Summing over t and using the fact

T t=1

ℓt − mit∗

2 ∞

=

E∗

and

Lemma

19

complete

the

proof.

In the proofs of all the three lemmas above, we have used the following fact: Lemma 19 Algorithm 5 ensures Tt=1(ℓt(at) − mt(at))2 ≤ M (E∗ + 1). Proof Let Ti = {t ∈ T : i ∈ Pt} be the time steps where predictor i is active. Then

T

T

(ℓt(at) − mt(at))2 ≤

(ℓt(at) − mit(at))2

t=1

t=1 i∈Pt

=

(ℓt(at) − mit(at))2 ≤ M (E∗ + 1),

i∈[M ] t∈Ti

where the last step uses the fact t∈Ti (ℓt(at) − mit(at))2 ≤ E∗ + 1 since the last term in the summation is bounded by one, while the rest cannot exceed E∗ because i has not been removed from the active set yet.

Finally, we are ready to prove Theorem 10. Proof [of Theorem 10] Combining Lemmas 16, 17, and 18, we have

Reg = O ln N + ηK2E∗ + ηKM E∗ + µM E∗T + µT σ + µ2T + M E∗ .

η

µ

µ

σ

With M ′ = max{K, M }, setting

gives Reg = O

µ = min 1 , d , σ = 2T
√M ′E∗(dT ) 14 + √dM ′E∗ + d .

ME∗,η = µT

µ ln N KM′E∗ ,

Appendix D. Omitted Details for Stochastic Environments
In this section, we provide omitted details for the stochastic case, including proofs for results with known E and a single predictor (Section D.1), the adaptive version of Algorithm 2 and its analysis when E is unknown (Section D.2), and the algorithm and analysis for multiple predictors (Section D.3).

35

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

D.1. Proofs of Lemma 7 and Theorems 6 and 8

First, we prove Lemma 7 which certiﬁes the efﬁciency and (approximate) correctness of the binary

search procedure for ﬁnding the policy with the smallest Catoni’s mean (Algorithm 3).

Proof [of Lemma 7] The fact that the algorithm stops after log2 2T

K µ

+

1

= O(ln(KT /µ))

iterations is clear due to the initial value of zleft and zright, and the precision 1/T .

To prove the approximate optimality of the output πt, note that the algorithm maintains the

following loop invariants:

min ψ α ℓs(φs(π(xs))) − zleft ≥ 0
π∈Π s<t

and

min ψ α ℓs(φs(π(xs))) − zright
π∈Π s<t

≤ 0.

Therefore, by the monotonicity of ψ, all policies have Catoni’s mean larger than zleft, and there exists a policy

argmin ψ α ℓs(φs(π(xs))) − zright
π∈Π s<t

with Catoni’s mean smaller than zright. These two facts imply that both Catoniα ℓs(φs(πt(xs))) s<t and minπ∈Π Catoniα ℓs(φs(π(xs))) s<t are between zleft and zright, and are thus 1/T away from each other since we have zright − zleft ≤ 1/T after the algorithm stops.

To prove both Theorem 6 and Theorem 8, we introduce the following notation.

Deﬁnition 20 Denote by L(π) E(xt,mt,ℓt)∼D[ℓt(π(xt))] the expected loss of policy π, and by L(π) E(xt,mt,ℓt)∼D[ℓt(φt(π(xt)))] the expected loss of policy π after remapping.
For both theorems we make use of the following lemmas.

Lemma 21 Algorithm 2 (with either Option I or Option II) ensures

T

T

√

E ℓt(at) ≤ E L(πt) + µT σ + 2µ ET .

t=1

t=1

Proof Denote the conditional expectation given the history up to the beginning of time t by Et[·]. By the choice of at we have

Et [ℓt(at)] = (1 − µ)Et [ℓ(φt(πt(xt))] + Et

µ |At| ℓt(a)
a∈At

= L(πt) + Et

µ

(ℓt(a) − ℓt(φt(πt(xt))))

|At| a∈At

≤ L(πt) + µEt sup |ℓt(a) − ℓt(a′)|
a,a′ ∈At

36

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤ L(πt) + µEt sup |ℓt(a) − mt(a)| + |mt(a) − mt(a′)| + |mt(a′) − ℓt(a′)|
a,a′ ∈At

≤ L(πt) + µEt [σ + 2 ℓt − mt ∞]

(by the deﬁnition of At)

= L(πt) + µσ + 2µEt [ ℓt − mt ∞] .

Summing over T and applying Cauchy-Schwarz inequality:

ﬁnish the proof.

T

E

ℓt − mt ∞ ≤

t=1

T

√

TE

ℓt − mt

2 ∞

=

ET

t=1

Lemma 22 Algorithm 2 (with either Option I or Option II) ensures

T (L(π∗) − L(π∗)) ≤ E . σ

Proof The proof is exactly the same as the adversarial case (cf. Eq. (12)). First rewrite L(π∗) − L(π∗) as E [ℓt(φt(π∗(xt))) − ℓt(π∗(xt))]. When π∗(xt) = φt(π∗(xt)) we have φt(π∗(xt)) = a∗t , mt(a∗t ) ≤ mt(π∗(xt)) − σ, and

ℓt(φt(π∗(xt))) − ℓt(π∗(xt))

= ℓt(a∗t ) − mt(a∗t ) + mt(a∗t ) − mt(π∗(xt)) + mt(π∗(xt)) − ℓt(π∗(xt)),

≤ 2 ℓt − mt ∞ − σ ≤

ℓt − mt

2
∞,

σ

where the last step is by the AM-GM inequality. When π∗(xt) = φt(π∗(xt)), the above holds trivially. Plugging the deﬁnition of E then ﬁnishes the proof.

We are now ready to prove Theorems 6 and 8, using different concentrations according to the two different ways of calculating πt.

Proof [of Theorem 6] First, for any ﬁx π and t, we invoke Lemma 12 with Xs = ℓs(φs(π(xs))) − L(π) + E(x,ℓ,m)∼D[mina m(a)] for s = 1, . . . , t, b = O( Kµ ), and Vt = O( KµTEt + σ2t) (see Eq. (4)). Together with a union bound over all t and π, we have with probability at least 1 − 1/T ,

1t

t

ℓs(φs(π(xs))) − L(π) + E(x,ℓ,m)∼D[min m(a)]
a

s=1

(20)

= O KE + σ2 log(N T ) + K log(N T )

µT t t

µt

for all t ∈ [T ] and π ∈ Π. Therefore, we have

L(πt)

37

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

1t

≤t

ℓs(φs(πt(xs))) + E[min m(a)] + O
a

s=1

≤ 1 t ℓ (φ (π∗(x ))) + E[min m(a)] + O

t

ss

s

a

s=1

KE + σ2 µT t t KE + σ2 µT t t

≤ L(π∗) + O

KE + σ2 log(N T ) + K log(N T ) .

µT t t

µt

log(N T ) + K log(N T ) µt (by Eq. (20))
log(N T ) + K log(N T ) µt
(by the optimality of πt)
(by Eq. (20))

Combining Lemma 21, the inequality above, and Lemma 22, we arrive at

T

T

√

E ℓt(at) ≤ E L(πt) + µT σ + 2µ ET

t=1

t=1

≤ T L(π∗) + O

√

T

µT σ + µ ET +

t=1

KE + σ2 log(N T ) + K log(N T )

µT t t

µt

= T L(π∗) + O µT σ + µ√ET + dE + σ√dT + d

(21)

µ

µ

T

√

≤ E ℓt(π∗(xt)) + O µT σ + µ ET +

t=1

dE + σ√dT + d + E ,

µ

µσ

which ﬁnishes the proof.

Proof [of Theorem 8] First, for any ﬁx π and t, we invoke Lemma 13 with Xs = ℓs(φs(π(xs))) for s = 1, . . . , t, µ1 = · · · = µt = µ = L(π) − E(x,ℓ,m)∼D[mina m(a)], and V = O( KµE + σ2t) (see Eq. (4) for the variance calculation). Together with a union bound over all t and π, and the value of α speciﬁed in Algorithm 2, we have with probability at least 1 − 2/T ,

Catoniα

ℓs(φs(π(xs)))

s≤t

− L(π) + E(x,ℓ,m)∼D[min m(a)]
a

1

2 log(N T 2)

KE σ2

(22)

= t αV + α

=O

µt2 + t log(N T )

for all t ≥ α2V + 2 log(N T 2) = 4 log(N T 2) and π ∈ Π. Therefore, we have for t ≥ 4 ln(N T 2),

L(πt)

≤ Catoniα

ℓs(φs(πt(xs)))

s≤t

+ E[min m(a)] + O
a

KE σ2 µt2 + t

log(N T ) (by Eq. (22))

≤ Catoniα

ℓs(φs(π∗(xs)))

s≤t

+ E[min m(a)] + O
a

KE σ2 µt2 + t

log(N T ) + 1 T
(by Lemma 7)

38

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤ L(π∗) + O

KE σ2

1

µt2 + t log(N T ) + T .

(by Eq. (22))

Combining Lemma 21, the inequality above, and Lemma 22, we arrive at

T

T

√

E ℓt(at) ≤ E L(πt) + µT σ + 2µ ET

t=1

t=1

≤ T L(π∗) + O 4 ln(N T 2) + µT σ + µ√ET + T

t=1

= T L(π∗) + O µT σ + µ√ET +

dE + σ√dT µ

KE σ2 µt2 + t

T

√

≤ E ℓt(π∗(xt)) + O µT σ + µ ET +

t=1

dE + σ√dT + E

µ

σ

which ﬁnishes the proof.

log(N T ) ,

D.2. Adaptive Version of Algorithm 2

The pseudocode of the adaptive version of Algorithm 2 in shown in Algorithm 6. To prove its regret

guarantee, we make use of the following useful lemmas. The ﬁrst one shows the concentration of

αi

around

αi

=

1 K

K a=1

Pr

|ℓt(a) − mt(a)| ∈ (2−i−1, 2−i]

.

Lemma 23 Algorithm 6 ensures:

•

If αi

>

360 log B

T

,

then

with

probability

at

least

1

−

1/T

,

αi − 30 log T ≥ 1 αi;

(23)

B +3

• With probability 1 − 1/T ,

αi − 30 log T ≤ 3 αi.

(24)

B +2

Proof

Clearly, E[αi] = αi. By Freedman’s inequality (Lemma 12), with probability 1 −

1 T

,

|αi − αi| ≤ 2 αi log T + log T

B

B

≤ αi + 30 log T ,

2

B

(AM-GM inequality)

implying both αi

≤

3 2

αi

+

30

log B

T

and

αi 2

≤

αi

+

30

log B

T

.

The former implies Eq. (24).

Rearranging

the

latter

gives

αi

−

30

log B

T

≥

αi 2

−

60 log T B

.

If

αi

>

360 log B

T

,

then

αi 2

−

60 log T B

can further be lower

bounded

by

αi 2

−

αi 6

=

αi 3

,

thus

proving

Eq.

(23).

The next lemma shows that E is essentially an underestimator of E.

39

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Algorithm 6 ǫ-GREEDY.VAR: ǫ-Greedy with Variance-adaptivity and Action Remapping
for t = 1, . . . , B do Draw at ∼ Uniform([K]).
Let

αi = 1

B
½ |ℓt(at) − mt(at)| ∈ (2−i−1, 2−i] ,

B t=1

⌈log2 T ⌉
E =T
i=0

αi − 30 log T B

2−2i.
+

Run Algorithm 2 for the remaining rounds with Option I, σ =

min

d

2 3

/T

1 3

,

1

.

E

(dT

)−

1 3

,

and

µ

=

Lemma 24

With

probability

1

−

1 T

,

E

≤

6E .

Proof

By Lemma 23 and the deﬁnition of E, with probability 1 −

1 T

we have

⌈log2 T ⌉ 3 −2i

⌈log2 T ⌉ −2i−2

E ≤T

2 αi2 = 6T

αi2

i=0

i=0

⌈log2 T ⌉

= 6T

E

1

K
½

K

i=0

a=1

= 6T E  1

K ⌈log2 T ⌉
½

K a=1 i=0

|ℓt(a) − mt(a)| ∈ (2−i−1, 2−i] |ℓt(a) − mt(a)| ∈ (2−i−1, 2−i]

2−2i−2 
2−2i−2

≤ 6T E ≤ 6T E

K1 K (ℓt(a) − mt(a))2
a=1

ℓt − mt

2 ∞

= 6E.

The ﬁnal lemma analyzes the bias due to remapping with the new value of σ, which replaces the role of Lemma 22 when analyzing Algorithm 6.

Lemma 25 Algorithm 6 ensures:

(T − B)(L(π∗) − L(π∗)) = O

K

2

√ E

(dT

)

1 3

+

K

ET

.

B

Proof First we bound (T − B)(L(π∗) − L(π∗)) by E

T t=B+1

(2

ℓt − mt

∞ − σ)

, following

the exact same argument as in the proof of Lemma 22. We then further bound the latter by

T

E

(2 ℓt − mt 1 − σ)

t=B+1

40

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING





 

T
≤ E

2 K ⌈log2 T ⌉ 2−i½[|ℓt(a) − mt(a)| ∈ (2−i−1, 2−i]] + O T1  − σ

t=B+1 a=1 i=0


T
= E


⌈log2 T ⌉

(the O 

1 T

term incurs when all indicators are zero)

2K

2−iFt(i) − σ + O(K),

t=B+1

i=0

where we deﬁne Ft(i) above into two parts:
T
E
t=B+1

1 K

K a=1

½[|ℓt(a)−mt(a)|

∈

(2−i−1,

2−i]].

We

decompose

the

summation

K 2−iFt(i) − σ
i∈I





T

+E

K

 2−iFt(i)

t=B+1

i∈I

where I

{i

≤

⌈log2

T⌉

:

αi

>

360 log T B

}

and

I

ﬁrst term as:

{i

≤

⌈log2

T⌉

:

αi

≤

360 log T B

}.

We

bound

the

T
K 2−iFt(i) − σ

t=B+1

i∈I

TK ≤
t=B+1

i∈I 2−iFt(i) 2 4σ

≤ T K2(log2 T ) i∈I 2−2iFt(i)2 t=B+1 4σ

=O

T
K2
t=B+1

i∈I 2−2iFt(i) . σ

(AM-GM inequality) (Cauchy-Schwarz) (0 ≤ Ft(i) ≤ 1)

Now we take the expectation conditioned on all history before time B and the high probability event

in Lemma 23. Noting that E[Ft(i)] = αi and plugging the value of σ, we arrive at





K2T O

i∈I αi2−2i

≤ O

K2T i∈I αi2−2i



E

(dT

)−

1 3

T

i∈I

αi2−2i(dT

)−

1 3





(Eq. (23))

= O K2 T

αi

2−

2i

(dT

)

1 3



i∈I

=O

K

2

√

E

(dT

)

1 3

.

We continue to bound the second term:





T

E

K

 2−iFt(i) ≤ KT

2−iαi

t=B+1

i∈I

i∈I

41

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING



1 

1

2

2

≤ K T αi T 2−2iαi

i∈I

i∈I

= O K T × √E . B

(Cauchy-Schwarz) (deﬁnition of I)

Combining the two terms ﬁnishes the proof.

Proof [of Theorem 9] By the exact same argument as the proof of Theorem 6 (cf. Eq. (21)), we bound the expected loss of the second phase of the algorithm by

T

E

ℓt(at)

t=B+1

= (T − B)L(π∗) + O

√ µT σ + µ ET +

dE + σ√dT + d .

µ

µ

Further applying Lemma 25 and bounding the regret of the ﬁrst phase of the algorithm trivially by B, we have

Reg = O =O =O

√ µT σ + µ ET +

dE

√ + σ dT

+

d

+

K

2

√ E

(dT

)

1 3

+K

µ

µ

ET + B B

1√

1

E(dT ) 3 + E(dT ) 6 +

E

(dT

)

1 6

+

1
(dT ) 3

+

K

2

√

E

(dT

)

1 3

+

K

ET + B B

(by our choices of µ and σ deﬁned in Algorithm 6)

K

2

√

E

(dT

)

1 3

+

K

ET + B

.

B

(Lemma 24)

This ﬁnishes the proof.

D.3. Algorithms and Analysis for Multiple Predictors
In this section, we provide the complete pseudocode of our algorithm for learning with multiple predictors in the stochastic setting (Algorithm 7) and its analysis. As mentioned in Section 5, there are several extra ingredients compared to Algorithm 2. First, just as in Algorithm 5, we maintain an active set of predictors Pt by bookkeeping the remaining error budget Vti for each predictor i. One difference is that the budget starts from 2E∗ + 8 log T , which takes into account a direct deviation bound. Another difference is that whenever the set Pt is updated, we discard previous data and run the algorithm from scratch (see Step 3 of Algorithm 7). The reason to do so is to make sure that the data {xs, ℓs, ms}ts=tb are i.i.d., where mt(a) = mini∈Pt mit(a) depends on Pt.
Second, at the beginning of each round, we check if all predictors are consistent to some extent. If not, that is, if there exist two predictors who disagree with each other by σ/3 on some action, then we simply choose this action deterministically, since this guarantees to reveal which predictor makes a large error for this round. See Step 1 of Algorithm 7. In this case, we set the loss estimators to be zero.
Finally, in the case when all predictors are consistent, instead of doing ǫ-greedy as in Algorithm 2, we deploy similar ideas as the minimax optimal algorithm ILOVETOCONBANDITS (Agarwal et al.,
42

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Optimization Problem (to solve for Qt)

Parameter: 0 < α ≤ min

µT 65K E

∗

,

1 325K

σ

2

,

µ2 1300σ2

,

1

.

Deﬁne: b is such that t ∈ [tb, tb+1), P[−1,1](X) [−1, 1], and

max{min{X, 1}, −1} is the projection onto

t−1
Ct(π) = P[−1,1] Catoniα ℓs (φs(π(xs))) s=tb , Regt(π) = Ct(π) − min Ct(π′).
π′
Let Qt be a solution of Q that satisﬁes (25), (26), (27).

Q ∈ ∆Π,

(25)

Q(π)Regt(π) ≤ 240αKσ2 log T,

(26)

π∈Π

∀π ∈ Π,

1 t−1

1

≤ 2K + Regt(π) , (27)

t − tb s=tb Qµ φs(π(xs)) xs, ms

120ασ2 log T

where

Qµ(a | xs, ms) = (1 − µ) Q(π′)½[φs(π′(xs)) = a] + µ ½[a ∈ As]. (28)

π ′ ∈Π

|As|

Figure 1: An Optimization Problem for Algorithm 7

2014) to come up with a sparse distribution Qt over the polices, computed by solving an optimization problem described in Figure 1. At a high level, the optimization problem tries to ﬁnd a policy with low empirical regret (Eq. (26)) and low empirical variance (Eq. (27)) simultaneously. The difference compared to (Agarwal et al., 2014) is that we apply action remapping as well as (clipped) Catoni’s estimators. The fact that this optimization problem can be solved efﬁciently is by the original arguments in (Agarwal et al., 2014) and the binary search procedure we develop in Algorithm 3 (details omitted).
To analyze the regret of Algorithm 7 and prove Theorem 11, we introduce some deﬁnitions and useful lemmas.

Deﬁnition 26 For some epoch b of Algorithm 7 (with a corresponding ﬁxed active set Ptb ), deﬁne

C(b)(π) where t = tb, and

E(xt,mt,ℓt)∼D Bt ℓt(φt(π(xt))) − min mt(a)
a∈[K ]

Reg(b)(π)

C(b)(π) − min C(b)(π′).
π′∈Π

43

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Algorithm 7 ILTCB.MARC: ILOVETOCONBANDITS with Action Remapping and Catoni’s
estimator for Multiple predictors
Parameters: E∗, σ ∈ [0, 1], µ ∈ [0, 1] Initialization: V1i = 2E∗ + 8 log T for all i ∈ [M ]. P1 = [M ]. t1 = 1 for b = 1, 2, . . . do
for t = tb, . . . do Receive xt and mit for all i ∈ [M ]. Let mt(a) = mini∈Pt mit(a) for all a ∈ [K]. Deﬁne a∗t , At, φt according to Eq. (3).

Step 1. Check if the predictors are consistent, and calculate pt

Let

Bt

=

½[∀a

∈

[K], ∀i, j

∈

Pt

,

|m

i t

(a

)

−

mjt (a)|

≤

σ3 ].

Let Qt be a solution of the Optimization Problem deﬁned in Figure 1, and deﬁne

pt(a) =

½[a = a′]
Qµt (a | xt, mt)

if Bt = 0 (a′ is such that ∃i, j ∈ Pt, |mit(a′) − mjt (a′)| > σ3 ) if Bt = 1 (see Eq.(28) for the deﬁnition of Qµt (a | xt, mt))

Step 2. Choose an action and construct loss estimators Sample at ∼ pt and receive ℓt(at). Deﬁne

ℓt(a) = (ℓt(a) − mt(a))½[at = a] + mt(a) − mt(a∗) Bt

pt(a)

t

Step 3. Make updates for i ∈ Pt do
Vti+1 ← Vti − (ℓt(at) − mit(at))2 Pt+1 = i ∈ Pt : Vti+1 ≥ 0 .
if Pt+1 = ∅ then Pt+1 ← [M ], Vti+1 ← 2E∗ + 8 log T, ∀i ∈ [M ].
if Pt+1 = Pt then tb+1 = t + 1 break

Also, deﬁne constant C0 log(8T 4N 2).4 Lemma 27 The Optimization problem deﬁned in Figure 1 admits a solution.
4. Recall that mt does not depend on history once Ptb is ﬁxed, and hence can be treated as jointly i.i.d. along with xt, ℓt over an epoch with a ﬁxed active set.
44

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Proof

The proof follows Lemma 1 of (Luo, 2017) (with β

=

1 120ασ2

log

T

).

Lemma 28 For any δ ∈ (0, 1), with probability at least 1 − δ,

T

ℓt − m∗t

2 ∞

≤

2E ∗

+

8

log(1/δ).

t=1

Proof This is by the deﬁnition of E∗ and a direct application of Bernstein’s inequality:

T

ℓt − m∗t

2 ∞

≤

E∗

+

4

t=1

T

log(1/δ)

E[

ℓt − m∗t

4 ∞

]

+

4

log(1/δ)

t=1

≤ E∗ + 4 log(1/δ)E∗ + 4 log(1/δ)

≤ 2E∗ + 8 log(1/δ),

where the last step uses AM-GM inequality.

Lemma 29

With

probability

at

least

1

−

1 T

,

for

all

j,

t

∈

[tj, tj+1),

all

Q

∈

∆Π,

and

all

π

∈

Π,

the following holds

E(xt,mt,ℓt)

1 Qµ(φt(π(xt)) | xt, mt)

6.4 t−1

1

80C0

≤ t − tj Qµ(φs(π(xs)) | xs, ms) + (t − tj)µ2 ,

s=tj

where C0 = log 8T 4N 2 .

Proof This lemma has appeared several times in the literature such as (Dudik et al., 2011, Theorem 6), (Agarwal et al., 2014, Lemma 10), and (Chen et al., 2019, Lemma 13). Basically this is a consequence of the contexts being i.i.d. generated, and is not related to the algorithm.

Lemma 30

With

probability

at

least

1

−

1 T

,

we

have

for

any

π,

j,

and

t

∈

[tj ,

tj+1),

V(xt,m·t,ℓt,at) ℓt(φt(π(xt))) ≤ 4KµTE∗ + 20Kσ2 + 61.240Rαelgotg(πT) + (t80−σt2jC)µ0 2 .

Proof We prove the lemma by the following sequence of direct calculations:

V(xt,mt,ℓt,at) ℓt(φt(π(xt)))

≤ 2E(xt,mt,ℓt,at)

(ℓt(φt(π(xt))) − mt(φt(π(xt)))½[at = φt(π(xt))] 2

pt(φt(π(xt)))

Bt

+ 2E(xt,mt) (mt(φt(π(xt))) − mt(a∗t ))2

≤ 4E(xt,mt,ℓt,at)

(ℓt(φt(π(xt))) − m∗t (φt(π(xt))))½[at = φt(π(xt))]

2
B

pt(φt(π(xt)))

t

45

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

+ 4E(xt,mt,ℓt,at)

(m∗t (φt(π(xt))) − mt(φt(π(xt))))½[at = φt(π(xt))]

2
B

pt(φt(π(xt)))

t

+ 2σ2

≤ 4E(xt,m·t,ℓt)

(ℓt(φt(π(xt))) − m∗t (φt(π(xt))))2 Bt pt(φt(π(xt)))

+ 4E(xt,mt,ℓt)

(m∗t (φt(π(xt))) − mt(φt(π(xt))))2 Bt pt(φt(π(xt)))

+ 2σ2

≤ 4KE∗ + 4E

( σ3 )2

+ 2σ2

µT

(xt,mt,ℓt)


Qµt (φt(π(xt)) | xt, mt)



≤ 4KE∗ + 2σ2 + σ2  6.4 t−1

1

+ 80C0 

µT

t − tj s=t Qµt (φs(π(xs)) | xs, ms) (t − tj )µ2

j

(Lemma 29)

≤ 4KE∗ + 2σ2 + σ2 6.4 × 2K + Regt(π)

µT

120ασ2 log T

+ 80C0 (t − tj )µ2

(Eq. (27))

≤ 4KE∗ + 20Kσ2 + 6.4Regt(π) + 80σ2C0 .

µT

120α log T (t − tj)µ2

Lemma 31 For any π, j, and t ∈ [tj, tj+1), we have E(xt,mt,ℓt,at) ℓt(φt(π(xt))) = C(j)(π).

(Recall the deﬁnition of C(j)(π) in Deﬁnition 26.)

Proof By direct calculation, we have

E(xt,mt,ℓt,at) ℓt(φt(π(xt)))

= E(xt,mt,ℓt,at)

(ℓt(φt(π(xt))) − mt(φt(π(xt))))½[at = φt(π(xt))] + mt(φt(π(xt))) − mt(a∗) Bt

pt(φt(π(xt)))

t

= E(xt,mt,ℓt) [(ℓt(φt(π(xt))) − mt(φt(π(xt))) + mt(φt(π(xt))) − mt(a∗t )) Bt]

= E(xt,mt,ℓt) Btℓt(φt(π(xt))) − Bt min mt(a)
a
= C(j)(π),

ﬁnishing the proof.

46

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Lemma 32

Recall

the

deﬁnition

of

Reg(j)(π)

in

Deﬁnition

26.

With

probability

at

least

1

−

1 T

,

we

have for any j and t ∈ [tj, tj+1),

Reg(j)(π) ≤ 2Reg (π) + 40αKE∗ + 200αKσ2 + 800ασ2C0 log T + 20 log(N T 2) log T ,

t

µT

(t − tj )µ2

α(t − tj)

(29)

Reg (π) ≤ 2Reg(j)(π) + 40αKE∗ + 200αKσ2 + 800ασ2C0 log T + 20 log(N T 2) log T .

t

µT

(t − tj )µ2

α(t − tj)

(30)

Proof We ﬁrst notice that when t−tj ≤ 20 log(N T 2) log T , both inequalities hold trivially because the left-hand side is at most 1 ≤ 20 log(Nt−Ttj2) log T ≤ 20 logα((Nt−Tt2j))log T . Thus we only need to consider the case t − tj ≥ 20 log(N T 2) log T .
We prove them by induction on t. Let π∗ = argminπ′ C(j)(π′). Assume (29) and (30) hold for tj, . . . , t − 1. By the induction hypothesis and Lemma 30, for any π, the conditional variance of
ℓs(φs(π(xs))) can be upper bounded as follows:

V ℓs(φs(π(xs)))

≤ 4KE∗ + 20Kσ2 + 6.4Regs(π) + 80σ2C0

µT

120α log T (s − tj)µ2

≤ 4KE∗ + 20Kσ2 + 80σ2C0

µT

(s − tj )µ2

+ 6.4 120α log T

2Reg(j)(π) + 40αKE∗ + 200αKσ2 + 800ασ2C0 log T + 20 log(N T 2) log T

µT

(s − tj)µ2

α(s − tj)

≤ Reg8(jα)(π) + 6.5µKTE∗ + 32.5Kσ2 + (1s3−0σt2jC)µ02 + 3.2α52l(osg−(NtjT)2) Vs.

Let V = ts−=1tj Vs. We ﬁrst verify that t − tj ≥ α2V + 2 log(N T 2). This can be seen by the following:

α2V + 2 log(N T 2)

t−1
≤ α2
s=tj

Reg(j)(π) + 6.5KE∗ + 32.5Kσ2 + 130σ2C0 + 3.25 log(N T 2)

8α

µT

(s − tj)µ2

α2(s − tj )

+ 2 log(N T 2)

≤ α2(t − tj) 81α + 6.5µKTE∗ + 32.5Kσ2 + 13(0tσ−2Ct0j)lµog2 T + 3.25 lαog2((tN−Tt2j))log T + 2 log(N T 2) ≤ (t − tj) α8 + 6.5αµ2TKE∗ + 32.5α2Kσ2 + 130(αt2−σ2tCj)0µl2og T + 3.25 lo(gt(N− Ttj2)) log T + 2 log(N T 2)

≤ (t − tj) ≤ (t − tj)

1 + 0.1 + 0.1 + C0 log T + 3.25 + 0.1(t − tj)

8

10(t − tj) 20

(by the constraints on α)

1

16 log(N T 2) log T

3.25

8 + 0.1 + 0.1 + 10 × 20 log(N T 2) log T + 20 + 0.1(t − tj)

(by the deﬁnition of C0)

47

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤ t − tj.

(31)

Because of Eq. (31), we are now able to use Lemma 13 for the samples

δ

=

N

1 T

2

.

By

Lemmas

13

and

31,

we

have

with

probability

1

−

δ

that

t−1

ℓs(φs(π(xs)))

with

s=tj

Reg(j)(π) = C(j)(π) − C(j)(π∗)

≤ Ct(π) − Ct(π∗)

α t−1 + t − tj
s=tj

Reg(j)(π) + Reg(j)(π∗) + 13KE∗ + 65Kσ2 + 260σ2C0 + 6.5 log(N T 2)

8α

8α

µT

(s − tj)µ2 α2(s − tj)

+ 4 log(N T 2) α(t − tj)

≤ Reg (π) + 1 Reg(j)(π) + 13αKE∗ + 65αKσ2 + 260ασ2C0 log T + 10.5 log(N T 2) log T .

t

8

µT

(t − tj)µ2

α(t − tj)

(using Reg(j)(π∗) = 0)

Rearranging the above inequality gives

Reg(j)(π) ≤ 8 Reg (π) + 15αKE∗ + 75αKσ2 + 300ασ2C0 log T + 12 log(N T 2) log T ,

7t

µT

(t − tj)µ2

α(t − tj)

proving Eq. (29). Similarly,

Regt(π) = Ct(π) − Ct(π)

≤ Ct(π) − Ct(π)

α t−1 + t − tj
s=tj

Reg(j)(π) + Reg(j)(π) + 13KE∗ + 65Kσ2 + 260σ2C0 + 6.5 log(N T 2)

8α

8α

µT

(s − tj )µ2 α2(s − tj)

4 log(N T 2) + α(t − tj)

≤ 9 Reg(j)(π) + 13αKE∗ + 65αKσ2 + 260ασ2C0 log T + 10.5 log(N T 2) log T

8

µT

(t − tj )µ2

α(t − tj)

+ 1 2Reg (π) + 40αKE∗ + 200αKσ2 + 800ασ2C0 log T + 20 log(N T 2) log T

8

t

µT

(t − tj )µ2

α(t − tj)

(using (29), which we just proved above)

≤ 9 Reg(j)(π) + 18αKE∗ + 90αKσ2 + 360ασ2C0 log T + 13 log(N T 2) log T .

8

µT

(t − tj )µ2

α(t − tj)

(using Regt(π) = 0)

This

proves

Eq.

(30)

and

ﬁnishes

the

induction.

Recall

that

we

pick

δ

=

N

1 T

2

.

Thus

the

total

failure

probability

is

at

most

1 NT2

×

TN

≤

1 T

.

48

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

Lemma 33

With

probability

at

least

1

−

1 T

,

we

have

for

any

π∗,

j,

and

t

∈

[tj ,

tj+1),

E(xt,mt,ℓt,at) [Btℓt(at)] ≤ E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))]

+ O αKE∗ + αKσ2 + ασ2 log N + log N + µ E∗ + µσ .

µT

(t − tj)µ2 α(t − tj)

T

Proof By the way at is chosen when Bt = 1, we have

E(xt,mt,ℓt,at)

[Btℓt(at)] 



= E(xt,mt,ℓt) Bt

pt(a)ℓt(a)

a∈[K ]

= (1 − µ)E(xt,mt,ℓt) Bt Qt(π)ℓt(φt(π(xt))) + µE(xt,mt,ℓt) |ABtt|

ℓt(a) . (32)

π∈Π

a∈At

We continue to bound the ﬁrst term in Eq. (32) as:

Qt(π)E(xt,mt,ℓt) [Btℓt(φt(π(xt)))]
π∈Π

= Qt(π)C(j)(π) + Emt Bt min mt(a)
a π∈Π

(Deﬁnition 26)

≤ Qt(π)Reg(j)(π) + C(j)(π∗) + Emt Bt min mt(a)
a π∈Π

(Deﬁnition 26)

≤ Qt(π) 2Regt(π) + 40αµKT E∗ + 200αKσ2 + 800(αt σ−2Ctj0)µlo2g T + 20 logα((Nt −T 2t)j)log T
π∈Π

+ E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))]

(Lemma 32 and Deﬁnition 26)

=O

αKE∗ + αKσ2 + ασ2C0 log T + log(N T 2) log T

µT

(t − tj)µ2

α(t − tj)

+ E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))] .

(Eq.(26))

=O

αKE∗ + αKσ2 + ασ2 log N + log N

µT

(t − tj)µ2 α(t − tj)

+ E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))] .

The second term in Eq. (32) can be bounded as follows (without the µ factor):

E(xt,mt,ℓt)

1 |At| ℓt(a)Bt
a∈At

= E(xt,mt,ℓt)

1

(ℓt(a) − mt(a) + mt(a) − mt(φt(π∗(xt)))) Bt

|At| a∈At

+ E(xt,mt,ℓt)

1

(mt(φt(π∗(xt))) − ℓt(φt(π∗(xt)))) Bt

|At| a∈At

+ E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))]

49

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING

≤ E(xt,mt,ℓt) ≤ E(xt,mt,ℓt)

2 max |ℓt(a) − mt(a)|Bt + σ + E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))]
a

2 max |ℓt(a) − m∗t (a)|Bt + 2σ + σ + E(x ,m ,ℓ ) [Btℓt(φt(π∗(xt)))]

a

3

t tt

(deﬁnition of Bt)

≤ 2 ET∗ + σ + E(xt,mt,ℓt) [Btℓt(φt(π∗(xt)))] . (deﬁnition of E∗ and Jensen’s inequality)

Combining these two bounds ﬁnishes the proof.

Lemma 34 Algorithm 7 ensures for any π∗,

T

E

Btℓt(at) − Btℓt(φt(π∗(xt)))

t=1

≤O

αK E ∗

+ αT Kσ2

+

M ασ2 log N

+

M

log N

√ + µ TE∗

+ T µσ

µ

µ2

α

Proof This is proven by summing the statement of Lemma 33 over t and noticing that with proba-

bility

1

−

1 T

,

there

exists

a

predictor

with

T t=1

ℓt − mit

∞ ≤ 2E∗ + 8 log T and thus there are at

most M episodes (see Lemma 28).

Lemma 35 Algorithm 7 ensures

E T Btℓt(φt(π∗(xt))) − Btℓt(π∗(xt)) ≤ O Eσ∗ .
t=1

Proof The proof is similar to that of Lemma 22:

T

E

Btℓt(φt(π∗(xt))) − Btℓt(π∗(xt))

t=1

T
= E Bt ½[π∗(xt) ∈/ At] (ℓt(a∗t ) − ℓt(π∗(xt)))

t=1

T

=E

Bt½[π∗(xt) ∈/ At] (ℓt(a∗t ) − mt(a∗t ) + mt(a∗t ) − mt(π∗(xt)) + mt(π∗(xt)) − ℓt(π∗(xt)))

t=1

T

≤E

Bt½[π∗(xt) ∈/ At] (−σ + 2 ℓt − m∗t ∞ + 2 mt − m∗t ∞)

t=1

≤ E T Bt½[π∗(xt) ∈/ At] 2 ℓt − m∗ ∞ − σ

t

3

t=1

(Deﬁnition of Bt)

≤O E

T

ℓt − m∗t

2 ∞

t=1 σ

(AM-GM)

50

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING = O E∗ .
σ

Lemma 36

With

probability

1

−

1 T

,

T

M (1 + E∗)

(1 − Bt) ≤ O

σ2

.

t=1

Proof

With

probability

1

−

1 T

,

there

exists

a

predictor

with

T t=1

ℓt − mit ∞ ≤ 2E∗ + 8 log T and

thus there are at most M episodes (see Lemma 28). Under this event, every time when Bt = 0, there

exist i, i′ ∈ Pt such that |mit(at) − mit′(at)| ≥ σ3 . Therefore, the total budget i∈Pt Vi decreases

by at least

ℓt(at) − mit(at) 2 +

ℓt(at) − mit′(at)

2
≥

1 2

mit(at) − mit′(at)

2
≥

σ182 . Realizing

that the initial total budget is O(M (1 + E∗)) ﬁnishes the proof.

Finally, we are ready to prove Theorem 11. Proof [of Theorem 11] Combining Lemmas 34, 35, 36 and picking the optimal parameters in each step, we bound the regret as:

T

E

ℓt(at) − ℓt(π∗(xt))

t=1

≤O

αT Kσ2

+

αK E ∗

+

M ασ2 log N

+

M

log N

√ + µ TE∗

+ T µσ +

M (1 + E∗)

µ

µ2

α

σ2

=O

M K(log N )T σ2 +

M K(log N )E∗

+

M (log

N )σ

+

√ µ TE∗

+

T µσ

+

M (1

+

E∗)

µ

µ

σ2

=O =O =O =O

+ O M log N

KE∗ + √Kσ2 + σ

µT

µ

(picking the optimal α under the constraints of α)

√ M dT σ2 +

M dE∗

+

M dσ

+

√ µ TE∗

+

T µσ

+

M (1

+

E∗)

µ

µ

σ2

(assume T ≥ M log N )

√ M dT σ

+M

1 1√ 3 d3 E∗T

1 6

+

M

1 3

d

1 3

E

∗

1 3

σ

1 3

T

1 3

+

√

M

dE

∗

1 4

T

1√ 4σ

+

M (1

+ E∗)

σ2

(picking the optimal µ)

(M

2

d)

1 3

(1

+

E

∗

)

1 3

T

1 3

+

1√ (M d) 3 E∗T

1 6

+

(M

3

d2

)

1 7

(1

+

E

∗

)

3 7

T

2 7

+

M

32
5 d 5 (1

+

E

∗

)

2 5

T

1 5

(picking the optimal σ)

M

2 3

d

2 5

(1

+

E∗)

1 3

T

1 3

,

51

LEVERAGING LOSS PREDICTORS IN CONTEXTUAL BANDIT LEARNING where the last step uses the fact that we only care about the case when 1 + E∗ ≤ √T to simplify the bound.
52

