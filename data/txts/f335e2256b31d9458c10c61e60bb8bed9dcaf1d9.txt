arXiv:2003.00857v3 [cs.CL] 9 Mar 2020

Multi-View Learning for Vision-and-Language Navigation
Qiaolin Xia♣∗ Xiujun Li♠♦∗ Chunyuan Li♦ Yonatan Bisk♠♦♥ Zhifang Sui♣ Jianfeng Gao♦ Noah A. Smith♠♥ Yejin Choi♠♥ ♠Paul G. Allen School of Computer Science & Engineering, University of Washington ♣MOE Key Laboratory of Computational Linguistics, Peking University
♦Microsoft Research AI ♥Allen Institute for Artiﬁcial Intelligence {xiujun,ybisk,nasmith,yejin}@cs.washington.edu xql@pku.edu.cn {xiul,chunyl,jfgao}@microsoft.com
Abstract
Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and underspeciﬁed. In this paper, we present a novel training paradigm, LEARN FROM EVERYONE (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent1 (25.3% → 41.4%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+. It improves generalization on unseen environments when a single instruction is used in testing, and pushes the R2R benchmark to 62% when multiple instruction are used.
1 Introduction
Learning to navigate in a visual environment based on natural language instructions has attracted increasing research interest in artiﬁcial intelligence (Savva et al., 2017; Kolve et al., 2017; Das et al., 2018; Anderson et al., 2018b; Chen et al., 2010), as it provides insight into core scientiﬁc questions about multimodal representations and takes a step toward real-world applications such as personal assistants and in-home robots. Navigating from language instructions presents a challenging reasoning problem for agents, as natural language instructions are highly variable, inherently ambiguous, and frequently underspeciﬁed. We see this clearly when analyzing the instructions provided in the Room-to-Room (R2R) Vision-and-Language Navigation (VLN) task (Anderson et al., 2018b), where each desired navigation path is paired with highly variable instructions (Instruction A and B in Figure 1).
Most previous approaches build on the sequence-to-sequence architecture (Sutskever et al., 2014), where the instruction is encoded as a sequence of words, and the navigation trajectory is decoded as a sequence of actions, enhanced with better attention mechanisms (Anderson et al., 2018b; Wang et al., 2019; Ma et al., 2019a) and beam search (Fried et al., 2018). While a number of approaches (Misra et al., 2017; Monroe et al., 2017; Wang et al., 2018) have been proposed to reduce the language ambiguity, common to all existing work is that the agent considers each instruction in isolation without collectively reasoning about other alternative instructions for each desired navigation trajectory
However, each instruction in practice only loosely aligns with the desired navigation path, making the existing learning paradigm that considers one instruction at a time less than ideal. This is because every instruction views different components of the trajectory as necessary to mention or obvious to omit, thus they mention and elide different details. For example, sentences like “Turn right go down the hallway” (Instruction B of Figure 1) may confuse an agent as it is applicable in a wide range of visual contexts (potentially multiple within the same house).
∗Authors contributed equally. 1Using FOLLOWER (Fried et al., 2018) as the base agent.

Instruction A: Walk to the left of the clock and down the hallway to the right. Turn right before the shelf and stop in the doorway of the bedroom.

Instruction B: Exit the room going straight. Turn right go down the hallway until you get to a black bookcase. Turn right and continue going down the hallway until you get to a bedroom. Wait at the entrance.

(a) Greedy agent (b) Beam search (c) Ground-Truth (d) Greedy agent (e) Beam search (f) LEO
Figure 1: Instructions A and B correspond to the same expert trajectory (c), but use different visual cues. Instruction A alone is speciﬁc enough for agents to reach the target successfully with either a (a) greedy or (b) beam search strategy. In contrast, Instruction B, in particular the underlined phrase, is ambiguous and causes existing agents (e.g. SPEAKER-FOLLOWER with beam size=40 (e)) to fail. Our LEO approach (f) shares information across instructions to learn groundings from ambiguous contexts. We indicate the start ( ), target ( ) and failure ( ) of agents in an unseen environment.
In order to address this natural variability of instructions more effectively, we propose LEARN FROM EVERYONE (LEO), where each instruction is considered as a different “view” of the same “label” (trajectory), thus formulating the VLN problem in the “multi-view” learning paradigm. The complexity of VLN learning can then be reduced by eliminating hypotheses which lack consensus from multiple views. Our new framework, LEO, effectively aggregates multiple instructions for each navigation trajectory during training. Each instruction plays the role of a teacher, contributing a different clue for completing the task. LEO allows the agent to fuse the complementary information from multiple teachers when reasoning about the desired trajectory. More concretely, our LEO navigator encodes multiple instructions of the same trajectory via shared parameterization, and aggregates them via a parameter-free pooling function, before ﬁnally generating the action sequence.
Comprehensive experiments demonstrate strong empirical performance of LEO, even when we carefully control the number of instructions during testing for fair comparisons. On the R2R benchmark dataset, our new training strategy dramatically improve a seq2seq greedy agent from 25% to 41% on SPL in the unseen environments. Moreover, since LEO is easily applicable to most existing approaches to VLN, we also apply LEO to two state-of-the-art agents: SMNA (Ma et al., 2019a) and EnvDrop (Tan et al., 2019), obtaining signiﬁcant improvements: 9% and 12% respectively. Finally, using the same data augmentation as the existing work, the resulting LEO+ navigator achieves 62% on the SPL metric,2 with 9% absolute gain over the previous best result.
2 Preliminaries
The VLN task can be formulated as a Markov Decision Process (MDP) M = S, A, Ps, r , where S is the visual state space, A is a discrete action space, Ps is the unknown environment distribution from which we draw the next state, and r ∈ R is the reward function. At each time step t, the agent ﬁrst observes an RGB image st ∈ S, then takes an action at ∈ A. This leads the simulator to generate a new image observation st+1 ∼ Ps(·|st, at) as the next state. The agent interacts with the environment sequentially, and generates a trajectory of length T , τ = [s0, a0, s1, a1, · · · , sT , aT ]. The episode ends when the agent selects the special STOP action, or when a pre-deﬁned maximum trajectory length is reached. The navigation is successfully completed, if the trajectory τ terminates at the intended target location.
The success of VLN largely depends on correct grounding of natural language instructions (Thomason et al., 2019). In a typical VLN setting, the instructions are represented as a set X = {xi}M i=1, where M is the number of alternative instructions, and each instruction xi consists of a sequence of Li word tokens, xi = [xi,1, xi,2, ..., xi,Li ]. The training dataset DE = {τ , X } consists of pairs of the instruction set X together with its corresponding expert trajectory τ . To simplify the learning, it
2Among all the public results at the time of this submission.

is widely assumed that the language instructions are independent and identically distributed (iid), and therefore each of them can fully represent the task goal. The agent then learns to navigate via performing maximum likelihood estimation (MLE) of the policy π, based on the individual sequences:

1M

max Lθ(τ , X ), whereLθ = log πθ(τ |X ) ≈

θ

M

log πθ(τ |xi),

i=1

and θ are the policy parameters. The baseline Seq2Seq (Anderson et al., 2018b; Fried et al., 2018) methods belong to this single teacher learning paradigm.
In this paper, we re-examine the iid assumption in VLN, and raise the concern that policy learning with individual language sequences over-simpliﬁes the dependency across alternative instructions, leading to degraded performance.

3 LEO: Learning from EveryOne

3.1 An information theoretic perspective
We propose to consider the VLN task from the “multi-view” learning perspective (Blum and Mitchell, 1998; Xu et al., 2013), where different instruction contains complementary information to reach the task goal. The basic intuition is that the complexity of the learning problem can be reduced considerably by eliminating hypotheses from each instruction that conﬂict with those of other alternative instructions (Sridharan and Kakade, 2008). Thus, we propose LEO, which learns to navigate based on multiple alternative instructions X = {xi}M i=1 jointly, without making the iid assumption in (1).
From an information-theoretic perspective, we can show the connections between the multi-instruction in LEO and the single instruction training in terms of the conditional entropy (CE):

H(τ |X ) = H(τ |x1) − I(τ , x2, · · · , xM |x1)

(1)

Multi-inst. CE Single inst. CE

Mutual Information

A detailed proof is provided in Appendix A.1. It is simple to show that the additional conditioning in LEO can reduce the entropy in trajectory generation compared to the single instruction setting:

H(τ |X ) ≤ H(τ |xi), ∀i = 1, · · · , M

(2)

Equality holds if the agent trajectory and other instructions are independent, after the agent observes one speciﬁc instruction. We hypothesize this is often false, as many single instructions are too ambiguous to uniquely determine the corresponding agent trajectory. LEO provides the agent an easier task than the single instruction learning, as the prediction uncertainty is reduced when conditioning on more instructions.

3.2 LEO Navigator

Based on this multi-view learning perspective, we design LEO navigator on top of the seq2seq base-
line (Anderson et al., 2018b; Fried et al., 2018). To generate a trajectory, the agent takes action at conditioned on the current state st and the instruction set X :

T

T

log πθ(τ |X ) = EPs(st|st−1,at−1) log πθ(at|st, X ) = log πθ(at|st, at−1X )

(3)

t=1

t=1

The second equality holds because the simulator has a deterministic transition function, and we follow (Fried et al., 2018) to formulate at−1 into the policy learning. Each conditional probability in (3) is modeled as a function πθ(at|st, at−1, X ) = fθ(st, at−1, X ). In this paper, we use the LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder framework to parameterize fθ. The encoder takes all language instructions as input and provides the joint instruction representations. The
LSTM decoder predicts the probability over the navigable directions, based on its understanding of the
task instruction and the agent’s current status.

Trajectory History Context. The agent maintains a memory vector ht, summarizing the history of its trajectory τ ≤t = [s0, a0, · · · , st, at] through step t. Once the agent takes a step, the surrounding visual scene changes accordingly. It ﬁrst performs one-hop visual attention to look at all of the surrounding view

angles, based on its previous memory vector ht−1. Speciﬁcally, the current visual state st is updated as the weighted sum of the panoramic features, st = j γt,jst,j. The attention weight γt,j for the j-th visual feature st,j represents its importance with respect to the previous history context ht−1, computed as γt,j = Softmax((Whht−1)⊤Wsst,j) (Fried et al., 2018) where Softmax(rj) = exp(rj)/ j′ exp(rj′), Wh and Ws are trainable projection matrices. Based on st, the agent updates its memory vector via an
LSTM:

ht = fθD ([st, at−1], ht−1)

(4)

where at−1 is the action taken at previous step, and θD are the LSTM decoder parameters.

Memory-attended Language Context. Knowing where to navigate requires a dynamic understanding

of the language instruction, according to the agent’s current status. The memory vector ht enables the

agent to keep track of its status, and adaptively focus on whichever part of the instructions are currently

most relevant.

We use an LSTM to encode each language instruction x = [x1, · · · , xL] into a sequence of textual features [e1, · · · , eL]; el corresponds to the hidden units for word token xl: el = fθE (xl, el−1), where θE are the parameters of the LSTM language encoder.
At each time step t, the textual context for the i-th instruction xi is computed as weighted sum of word

features in the sequence:

L

ct,i = αlel, where αl = Softmax(h⊤t el)

(5)

l=1

Note that ct,i places more weight on the words that are most relevant to the agent’s current status.

Aggregated Instruction Context. Key to our approach is utilizing the multiple natural language

instructions provided for each trajectory as every annotator makes different choices about descrip-
tions/landmarks to include or elide. At time step t, the textual context ct,i for different language se-
quences xi may characterize different aspects of the task instructions. A natural question is how to aggregate Ct = {ci,t}M i=1 into a joint context zt. In this paper, we investigate several parameter-free aggregation functions:

zt = g(Ct),

(6)

where three schemes are considered:

•

Mean-pooling:

zt

=

1 M

M i=1

ci,t;

• Max-pooling: The most salient features are kept by taking the maximum value along each dimension

of the language contexts;

• Concatenation: zt = [c1,t, · · · , cM,t]. It keeps the all the information of language contexts from multiple teachers, but potentially increases the learning burden in action selection.

Action Selection. Actions are predicted based on both the memory vector ht and the aggregated in-

struction context zt. The action predictor produces a probability for each navigable direction using a

bi-linear dot product:

pk = Softmax((Wz[ht, zt])⊤Wuuk)

(7)

where uk is the action embedding that represents the k-th navigable direction, and Wz & Wu are trainable projection matrices. The action embedding is the concatenation of an visual feature vector (CNN feature vector extracted from the image patch around that view angle or direction) and a 4-dimensional orientation feature vector [sin ψ; cos ψ; sin ω; cos ω] where ψ and ω are the heading and elevation angles, respectively (Fried et al., 2018).

Figure 2: Illustration of learning to navigate with LEO. The action (red circle) is selected, based on both the visual scene states (blue circle) and textual language instructions (green circle). At time step t, LEO aggregates multiple instructions {xi}M i=1 together to generate one action at.
Learning & Inference. We summarize the LEO navigator in Figure 2. At one time step t, the agent digests the multiple instructions X = {xi}M i=1 as Ct = {ci,t}M c=1, infers a joint representation zt, and executes an action at. When M = 1, LEO simpliﬁes to the single instruction baseline. The model parameters θ = {θE, θD, Wh, Ws, Wz, Wu} are trained end-to-end with MLE. Note that θE is shared among different language instructions, and a parameter-free aggregation function g(·) is employed. In testing, this allows to feed an arbitrary number of instruction sequences into the learned policy to produce a single trajectory for evaluation.
4 Experiments
Dataset. The Room-to-Room (R2R) dataset (Anderson et al., 2018b) is built upon the Matterport3D dataset (Chang et al., 2017), which consists of 10,800 panoramic views (each panoromic view has 36 images) and 7,189 trajectories. Each trajectory is paired with three natural language instructions. The R2R dataset consists of four splits: train, validation seen and validation unseen, test unseen. At the beginning of each navigation task, the agent starts at a speciﬁc location in an environment (one room). The goal of the agent is to follow natural language instructions to navigate to the target location as quickly as possible.
Implementation. In all of our experiments, we use a single-layer LSTM for the shared language encoder, and a second single-layer LSTM for the action decoder, with hidden size 512. Following (Fried et al., 2018), we use a panoramic action space. We use the ResNet visual representations provided by (Anderson et al., 2018b). LEO+ is built on top of LEO and trained with the mixture loss (supervised learning and reinforcement learning) and data augmentation. We augmented the instructions of the trajectory dataset that (Fried et al., 2018) provided. Different from speaker-follower, we generate multiple instructions for each trajectory. The number of original and augmented instructions for each trajectory is M = 3 and M ′ = 3, respectively. The source code will be made publicly available on GitHub.
Evaluation Metrics. We evaluate our agent on the following metrics:
TL Trajectory Length measures the average length of the navigation trajectory. NE Navigation Error is the mean of the shortest path distance in meters between the agent’s ﬁnal
location and the target location. SR Success Rate is the percentage of the agent’s ﬁnal location that is less than 3 meters away from the
target location. SPL Success weighted by Path Length (Anderson et al., 2018a) trades-off SR against TL. Higher
score represents more efﬁciency in navigation.
Among these metrics, SPL is the recommended primary metric, other metrics are considered as auxiliary measures.
Baselines. We compare our approach with nine recently published systems:

Setting

Validation Seen Validation Unseen SR ↑ SPL ↑ SR ↑ SPL ↑

Model

Validation Seen Validation Unseen SR ↑ SPL ↑ SR ↑ SPL ↑

(A) seq2seq LEO
(B) seq2seq LEO

51

46

52 (+1) 47 (+1)

49

44

63 (+14) 58 (+14)

32

25

36 (+4) 31 (+6)

33

26

48 (+15) 41 (+15)

SMNA + LEO
EnvDrop (IL) + LEO

63

56

44

30

76 (+13) 68 (+12) 51 (+7) 39 (+9)

48

46

43

40

58 (+10) 56 (+10) 53 (+10) 50 (+10)

Table 1: Comparison of LEO and seq2seq. Set- EnvDro+p L(IELO+RL) tings (A) and (B) correspond to single- and multi-

55

53

64 (+9) 62 (+9)

46

43

59 (+13) 55 (+12)

instruction respectively, as elaborated in Section Table 2: Improvement on the existing SoTA models

Results.

(without Data Augmentation).

• RANDOM: an agent that randomly selects a direction and moves ﬁve step in that direction (Anderson et al., 2018b).
• S2S-ANDERSON: the best performing sequence-to-sequence model using a limited discrete action space, proposed by Anderson et al. as a baseline for the R2R benchmark (Anderson et al., 2018b).
• RPA (Wang et al., 2018): is an agent which combines model-free and model-based reinforcement learning, using a look-ahead module for planning.
• SPEAKER-FOLLOWER (Fried et al., 2018): an agent trained with data augmentation from a speaker model on the panoramic action space.
• SMNA (Ma et al., 2019a): an agent trained with a visual-textual co-grounding module and a progress monitor on the panoramic action space.
• RCM+SIL (Wang et al., 2019): an agent trained with cross-modal grounding locally and globally via reinforcement learning.
• REGRETFUL (Ma et al., 2019b): an agent with a trained progress monitor heuristic for search that enables backtracking.
• FAST (Ke et al., 2019): an agent which uses a fusion function to combine global and local knowledge to score and compare partial trajectories of different lengths, which enables the agent to efﬁciently backtrack after a mistake.
• ENVDROP (Tan et al., 2019): an agent is trained with environment dropout, which can generate more environments based on the limited seen environments.
• PRESS (Li et al., 2019): an agent is trained with pre-trained language models and stochastic sampling to generalize well in the unseen environment.
• PREVALENT (Hao et al., 2020): a generic agent is pre-trained with image-language-action triples, and ﬁne-tuned on the R2R task.
4.1 Results
As the key contribution of this work is to develop a new learning paradigm LEO for VLN, we aim to answer the following research questions via experiments: (i) Is LEO more effective than the traditional single instruction learning, even when the total number of provided instructions are controlled to be identical? (ii) Is LEO general enough to allow for easy integration with existing techniques and lead to performance boost? (iii) How well does LEO perform when compared with state-of-the-art methods on the unseen test split?
The Effectiveness of LEO. We compare LEO against the seq2seq greedy agent3 on the validation splits (seen and unseen). Our agent obtains the aggregated representation of multiple instructions via a parameter-shared encoder and parameter-free pooling function. It allows the model to take an arbitrary number of instructions as input.
We keep the experimental settings the same with seq2seq, except that we add the aggregation function in LEO. Note that there are three different instructions that correspond to one single ground-truth navigation trajectory in the validation/testing dataset splits. To ensure a fair comparison, we test LEO
3Seq2seq is denoted as baseline agent that has a panoramic action space, i.e., the FOLLOWER model in the SPEAKERFOLLOWER (without data augmentation) (Fried et al., 2018).

Model

Validation Seen

Validation Unseen

Test Unseen

TL ↓ NE ↓ SR ↑ SPL ↑ TL ↓ NE ↓ SR ↑ SPL ↑ TL ↓ NE ↓ SR ↑ SPL ↑

Approaches that do not explore the test environments during training, and utilizes one instruction during testing

RANDOM (Anderson et al., 2018b)

9.58 9.45 16 -

9.77 9.23 16 -

9.93 9.77 13 12

S2S-ANDERSON (Anderson et al., 2018b) 11.33 6.01 39 -

8.39 7.81 22 -

8.13 7.85 20 18

RPA (Wang et al., 2018)

- 5.56 43 -

- 7.65 25 -

9.15 7.53 25 23

SPEAKER-FOLLOWER (Fried et al., 2018) - 3.36 66 -

- 6.62 35 - 14.82 6.62 35 28

SMNA (Ma et al., 2019a)

- -- -

- - - - 18.04 5.67 48 35

RCM+SIL (Wang et al., 2019)

10.65 3.53 67 - 11.46 6.09 43 - 11.97 6.12 43 38

REGRETFUL (Ma et al., 2019b)

- 3.23 69 63

- 5.32 50 41 13.69 5.69 48 40

FAST (Ke et al., 2019)

- - - - 21.17 4.97 56 43 22.08 5.14 54 41

ENVDROP (Tan et al., 2019)

11.00 3.99 62 59 10.70 5.22 52 48 11.66 5.23 51 47

PRESS (Li et al., 2019)

10.57 4.39 58 55 10.36 5.28 49 45 10.77 5.49 49 45

AUXRN(*) (Zhu et al., 2019)

- 3.33 70 67

- 5.28 54 50

- 5.15 55 51

PREVALENT (Hao et al., 2020)

10.32 3.67 69 65 10.19 4.71 58 53 10.51 5.30 54 51

Approaches that do not explore the test environments during training, and utilizes three instruction during testing

PRESS (Li et al., 2019)

10.35 3.09 71 67 10.06 4.31 59 55 10.52 4.53 57 53

PREVALENT (Hao et al., 2020)

10.31 3.31 67 63 9.98 4.12 60 57 10.21 4.52 59 56

LEO+ (Ours)

10.41 2.30 81 78 10.06 3.35 70 65 10.24 3.76 65 62

Approaches that do explore the test environments during training, and utilizes one instruction during testing

RCM+SIL (Wang et al., 2019)

10.13 2.78 73 -

9.12 4.17 61 -

9.48 4.22 61 59

ENVDROP (Tan et al., 2019)

9.92 4.84 55 52 9.57 3.78 65 61 9.79 3.97 64 61

AUXRN(*) (Zhu et al., 2019)

- -- -

- ---

- 3.69 68 65

Human

- -- -

- - - - 11.85 1.61 86 76

Table 3: Comparison with the previous SoTA methods. Bold indicates best value. LEO+ with multiintruction setting achieves near-SoTA scores, which are higher all previous SoTA methods that do not explore test enviroments. (*) indicates unpublished works.

and the baseline seq2seq agent in two different evaluation settings: (A) A single instruction is provided to the agent at a time. Thus, three separate navigation trajectories are generated corresponding to three alternative instructions in this setting. We report the averaged performance over three separate runs. (B) All three instructions are provided to the agent at once. Due to lack of instruction aggregation mechanism in the standard seq2seq models, we report its performance for the single trajectory with maximum likelihood. As for LEO, the mean-pooling function is used to aggregate the instructions, based on which one single trajectory is generated.
The results are summarized in Table 1. Our approach outperforms seq2seq in both settings. It is interesting to observe that LEO can provide decent improvement over seq2seq even in setting (A). Even when both agents take one single instruction per navigation, LEO generalizes better than seq2seq in unseen environments. This demonstrates the advantage of “multi-view” learning, where an agent that learns under the guidance of multiple instructions in training should generalize better. In setting (B), LEO yields signiﬁcantly higher performance than seq2seq, in terms of both efﬁciency (SPL) and success rate.
LEO as a General Learning Paradigm. LEO can serve as a basic building block for many existing techniques in VLN. We demonstrate this by applying LEO on two state-of-the-art systems: SMNA (Ma et al., 2019a) and EnvDrop (Tan et al., 2019). In Table 2, with LEO, both show signiﬁcant improvement (50 and 55).
Comparison with SoTA. Table 3 compares the performance of our agent against the existing published top systems.4 With the same data augmentation as the existing work, our LEO+ agent signiﬁcantly outperforms the existing models on nearly all the metrics, includes the two agents which explored test unseen environments (with one available instruction) and two systems trained which do not (with access to three instructions during testing).
4The full leaderboard is available: https://evalai.cloudcv.org/web/challenges/challenge-page/97/leaderboard/270

Mean Max Cat

Validation Seen SR ↑ SPL ↑
63 58 66 58 64 57

Validation Unseen SR ↑ SPL ↑

48

41

41

34

36

28

Table 4: Performance of aggregation schemes.

Encoder
Shared Multi-Arm

Validation seen SR ↑ SPL ↑
63 58 62 57

Validation Unseen SR ↑ SPL ↑

48

41

34

28

Table 5: Comparison of encoder architectures.

4.2 Ablation Analysis
We investigate the effects of several alternatives, and perform ablation studies on the base LEO model.
Aggregating Schemes. In Table 4 we investigate the impact of various context aggregation schemes detailed in Section 3. A shared encoder is employed for different instructions in this experiment. The three schemes perform similarly on the seen environments. However, in unseen environments, the meanpooling scheme yields the highest success rate and SPL. This indicates that mean-pooling provides the robust generalization across environments. One might view mean-pooling as a conservative strategy to calculate the representative features over available instructions, compared to the aggressive strategy of max-pooling which only focuses on the most prominent features. Note the concatenation scheme introduces additional trainable parameters, thereby increasing the learning burden for its intermediate layer. Therefore, we consider the mean-pooling aggregation throughout all our experiments by default.
Multi-Arm Encoders. Given a group of instructions-trajectory pairs, our LEO agent employs a simple strategy to extract the features for different instructions: a shared language encoder. An alternative is to train a multi-arm encoder, where each arm has its own trainable parameters, and process its own instruction. We compare the two strategies in Table 5. Mean-pooling aggregation is used when merging these instruction context. The shared encoder is signiﬁcantly better than the multi-arm encoder on the unseen environments, though they perform similarly on the seen environments. we conjecture that the parameter-shared encoder does not over-ﬁt on the training dataset as much. Our work focuses on opening the door for the research in joint instruction reasoning for VLN; we leave more advanced choices for encoder design as future work.
Qualitative Examples. We visualize the step-by-step navigation of our LEO agent and SPEAKERFOLLOWER in Figure 3, following Instruction B from Figure 1 in an unseen environment. One may refer the attention heatmap showing the step-by-step agent actions with three instructions in Appendix A.2 for better illustration. The two agents take the same actions for the ﬁrst three steps, as the instruction “Exit the room going straight” is quite speciﬁc. However, at the fourth step, the SPEAKERFOLLOWER agent turns right, which eventually leads to a failure. While, our LEO agent takes the second right at the seventh step and quickly ﬁnds the target location. More qualitive examples are provided in Appendix A.3.
5 Related Work
Vision-Language Navigation. Most existing approaches to VLN are based on seq2seq architecture (Anderson et al., 2018b). (Fried et al., 2018) introduced a panoramic action space and a “speaker” model for data augmentation. (Ke et al., 2019) proposed a novel neural decoding scheme with search to balance global and local information. To improve the alignment of the instruction and visual scenes, visual-textual co-grounding attention mechanism was proposed in (Ma et al., 2019a), which is further improved with a progress monitor (Ma et al., 2019b). To improve the generalization of the learned policy to unseen environments, reinforcement learning has been considered, including planning (Wang et al., 2018), and exploration of unseen environments using a off-policy method (Wang et al., 2019). (Tan et al., 2019) proposed an environment dropout to generate more environments based on the limited environments, so that it can generalize well to unseen environments. All these approaches assume a training regime where a single instruction is considered in isolation from

Instruction B: Exit the room going straight. Turn right go down the hallway until you get to a black bookcase. Turn right and continue going down the hallway until you get to a bedroom. Wait at the entrance.
Figure 3: Step-by-Step navigation views of LEO (left) and SPEAKER-FOLLOWER (right) following the Instruction B in Figure 1. For the top three views above the dashed line, actions taken by two agents are identical. The views of executing the instruction “turn right” are highlighted for each agent with a red circle: the 7th step of LEO and the 4th step of SPEAKER-FOLLOWER. other related instructions. The proposed LEO paradigm presents the ﬁrst work to leverage the joint representation of multiple related instructions in one navigation, and can be easily combined with previous methods to achieve improved performance. Multi-View Learning. Multi-view learning has been applied successfully in a number of real-world applications (Xu et al., 2013), such as web-page classiﬁcation (Blum and Mitchell, 1998), information retrieval (Wang et al., 2010), and face detection (Li et al., 2002). It has recently been integrated with deep neural networks for ﬂexible representation learning (Wang et al., 2015; Kan et al., 2016). By exploring the consistency and complementary properties of different views, LEO leads to more effective generalization compared to single-view learning (Sridharan and Kakade, 2008). The success of VLN requires precise reasoning over highly variable and under-speciﬁed language instructions, in order to ground them to the visual environment and action prediction. LEO demonstrates the advantage of multiview learning, and exhibits strong generalization in unseen environments.
6 Conclusion
We present LEO, a new training paradigm that leverages mutual agreement across instruction variants. This allows for more effective use of the limited training data to improve generalization to the previously unseen environments. Empirical results on the R2R benchmark demonstrate that LEO signiﬁcantly improves over the traditional single instruction paradigm. Further, LEO can be easily plugged into many existing models to boost their performance, as we demonstrate via LEO+, a navigator enhanced with data augmentation/ It improves the agent’s generalization ability in unseen environments when a single instruction is used in testing, and can largely boost the performance when multiple instructions are avaiable.

References
Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Zamir. 2018a. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757.
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Su¨nderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018b. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, volume 2.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT. ACM.
Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017. Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV).
Howard Chen, Alane Shur, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2010. Touchdown: Natural language navigation and spatial reasoning in visual street environments. CVPR.
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied question answering. In CVPR.
Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. 2018. Speaker-follower models for vision-andlanguage navigation. NIPS.
Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. 2020. Towards learning a generic agent for vision-and-language navigation via pre-training. CVPR.
Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation.
Meina Kan, Shiguang Shan, and Xilin Chen. 2016. Multi-view deep network for cross-view classiﬁcation. In CVPR.
Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. 2019. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. CVPR.
Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. AI2-THOR: An interactive 3D environment for visual AI. arXiv preprint arXiv:1712.05474.
Stan Z Li, Long Zhu, ZhenQiu Zhang, Andrew Blake, HongJiang Zhang, and Harry Shum. 2002. Statistical learning of multi-view face detection. In ECCV. Springer.
Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, and Yejin Choi. 2019. Efﬁcient navigation with language pre-training and stochastic sampling. In EMNLP.
Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. 2019a. Self-monitoring navigation agent via auxiliary progress estimation. ICLR.
Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. 2019b. The regretful agent: Heuristic-aided navigation through progress estimation. CVPR.
Dipendra Misra, John Langford, and Yoav Artzi. 2017. Mapping instructions and visual observations to actions with reinforcement learning. EMNLP.
Will Monroe, Robert XD Hawkins, Noah D Goodman, and Christopher Potts. 2017. Colors in context: A pragmatic neural model for grounded language understanding. TACL.
Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. 2017. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv preprint arXiv:1712.03931.
Karthik Sridharan and Sham M Kakade. 2008. An information theoretic framework for multi-view learning. COLT.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS.

Hao Tan, Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learning to navigate unseen environments: Back translation with environmental dropout. In NAACL.
Jesse Thomason, Daniel Gordon, and Yonatan Bisk. 2019. Shifting the baseline: Single modality performance on visual navigation & qa. In NAACL.
Kuansan Wang, Xiaolong Li, and Jianfeng Gao. 2010. Multi-style language model for web scale information retrieval. In SIGIR. ACM.
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. 2015. On deep multi-view representation learning. In ICML.
Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. 2018. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. ECCV.
Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. 2019. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. CVPR.
Chang Xu, Dacheng Tao, and Chao Xu. 2013. A survey on multi-view learning. arXiv preprint arXiv:1304.5634.
Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. 2019. Vision-language navigation with self-supervised auxiliary reasoning tasks. arXiv preprint arXiv:1911.07883.

A Appendices
A.1 Proof of entropy reduction

H(τ |X )

(8)

= − p(τ , X ) log p(τ |x1, · · · , xM )

(9)

τ ,X

= − p(τ , X ) log p(τ , x1, · · · , xM )p(x1)

τ ,X

p(x1, · · · , xM )p(τ , x1)

− p(τ , X ) log p(τ , x1)

(10)

τ ,X

p(x1)

= −I(τ , x2, · · · , xM |x1) + H(τ |x1)

(11)

≤ H(τ |x1)

(12)

A.2 Attention Visualization
We visualize three attention heatmaps between the language instruction and action trajectory in Figure 4. Note that these three instructions correspond to one single trajectory, with the step-by-step top-down view in Figure 1 and the panorama view in Figure 3. The three instructions provide complementary information to guide the agent to navigate towards the target location. For example, at the ﬁrst two steps, the agent attends to a wide range of tokens in instruction B (many elements in the ﬁrst two columns are highlighted), meaning the agent has difﬁculties in understanding which words of instruction B to focus on. However, when taking the same action, the agent can clearly concentrate on a few words (such as “walk” and “exit”) at the beginning of instruction A and C.

Figure 4: An attention heatmap showing the step-by-step agent actions with three instructions.

A.3 Examples of Successes and Failures
We provide examples of successful trajectories ( in Figures 5 and 6) and failures (in Figure 7 and 8). The highlighted instruction is executed by the baseline agent.
For each success case, the top-down views highlight the trajectories of Greedy vs. LEO, where Greedy agent fails, LEO succeeds. And we also provide a step-by-step navigation view of LEO.
For each failure case, we show the top-down views for the trajectories of Ground-Truth, Greedy and LEO, respectively. We also provide a step-by-step navigation view for LEO. Interestingly, on the cases that LEO agent fails, the greedy agent would usually fail, too.

Instruction 1: Exit the bathroom, and walk through the closet. Make a left just before the bed. Exit the bedroom, and make a right. Walk through the open bedroom door on the right. Wait in the door’s threshold. Instruction 2: Leave the bathroom and closet. Exit the bedroom and take a right. Enter the room on the right and stop in the doorway. Instruction 3: Walk straight ahead until you reach the bed. Turn left and walk out the door to the left. Once out, turn right and then enter the door on your right and stop once you enter.
Figure 5: Top-Down view and Step-by-Step navigation view of Example (path 2489, instruction 2) in an unseen environment of R2R. indicates the Start, indicates the Target, indicates the incorrect end point. Red arrow indicates the direction to go next.

Instruction 1: Exit the bedroom, enter the bathroom, wait at the toilet. Instruction 2: Walk out of the bedroom and take a right into the bathroom. In the bathroom take your ﬁrst right into the water closet and stop in front of the door. Instruction 3: Walk through the archway to the right and into the bathroom. Wait in the room to the right with the toilet.
Figure 6: Top-Down view and Step-by-Step navigation view of Example (path 321, instruction 1) in an unseen environment of R2R. indicates the Start, indicates the Target, indicates the incorrect end point. Red arrow indicates the direction to go next.

Instruction 1: Go straight into the door in front of you, turn left and then turn left again to go into the bathroom. Wait by the second sink. Instruction 2: Walk down the hallway past the painting and along the mirrored wall into the bedroom. Walk beside the bed and into the bathroom. Wait inside the bathroom, next to the shower. Instruction 3: Head into the bedroom. Turn left and go into the bathroom. Stop in front of the shower.
Figure 7: Top-Down view and Step-by-Step navigation view of Example (path 2755, instruction 2) in an unseen environment of R2R. indicates the Start, indicates the Target, indicates the incorrect end point. Red arrow indicates the direction to go next.

Instruction 1: Walk out into the yard. Take a left. Take another left, and go around the house. Stop before you go up the second stair. Instruction 2: Go past the table and chairs and down the steps. Turn left and then turn left to go around the house. Wait there. Instruction 3: Walk off the deck, turn left to walk around the house. Stop and wait near the window.
Figure 8: Top-Down view and Step-by-Step navigation view of Example (path 5476, instruction 1) in an unseen environment of R2R. indicates the Start, indicates the Target, indicates the incorrect end point. Red arrow indicates the direction to go next.

