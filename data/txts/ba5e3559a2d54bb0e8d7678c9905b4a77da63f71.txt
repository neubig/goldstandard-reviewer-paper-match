arXiv:1507.07045v5 [cs.GT] 24 Feb 2022

The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
Vijay Kamble The University of Illinois at Chicago
kamble@uic.edu Nihar Shah
Carnegie Mellon University nihars@cs.cmu.edu
David Marn University of California, Berkeley
marn@berkeley.edu
Abhay Parekh University of California, Berkeley
yahbaa@gmail.com Kannan Ramchandran University of California, Berkeley kannanr@eecs.berkeley.edu
A major challenge in obtaining evaluations of products or services on e-commerce platforms is eliciting informative responses in the absence of veriﬁability. This paper proposes the Square Root Agreement Rule (SRA): a simple reward mechanism that incentivizes truthful responses to objective evaluations on such platforms. In this mechanism, an agent gets a reward for an evaluation only if her answer matches that of her peer, where this reward is inversely proportional to a popularity index of the answer. This index is deﬁned to be the square root of the empirical frequency at which any two agents performing the same evaluation agree on the particular answer across evaluations of similar entities operating on the platform. Rarely agreed-upon answers thus earn a higher reward than answers for which agreements are relatively more common.
We show that in the many tasks regime, the truthful equilibrium under SRA is strictly payoﬀ-dominant across large classes of natural equilibria that could arise in these settings, thus increasing the likelihood of its adoption. While there exist other mechanisms achieving such guarantees, they either impose additional assumptions on the response distribution that are not generally satisﬁed for objective evaluations or they incentivize truthful behavior only if each agent performs a prohibitively large number of evaluations and commits to using the same strategy for each evaluation. SRA is the ﬁrst known incentive mechanism satisfying such guarantees without imposing any such requirements. Moreover, our empirical ﬁndings demonstrate the robustness of the incentive properties of SRA in the presence of mild subjectivity or observational biases in the responses. These properties make SRA uniquely attractive for administering reward-based incentive schemes (e.g., rebates, discounts, reputation scores, etc.) on online platforms.
1

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
2
1. Introduction
Reputation systems, in which people provide feedback for products or services based on their personal experiences, are a critical component of online platforms and marketplaces (Resnick et al. 2000, Jøsang et al. 2007, Tadelis 2016, Luca 2017). These systems improve the overall quality of transactions, increase trust, and thus play a key role in determining the success of these platforms in the long run. A major practical challenge in these systems is that of eliciting truthful and high-quality responses from the agents. In the absence of appropriate incentives, agents could shirk investing eﬀort, provide uninformative feedback, or even exploit these systems for selﬁsh motives, thus undermining their utility. For instance, signiﬁcant empirical evidence of bias in user ratings has been found on many online platforms (Hu et al. 2017, Filippas et al. 2018, Nosko and Tadelis 2015). This work describes a simple and intuitive reward mechanism that attempts to address this concern.
We consider a setting where an online platform is interested in obtaining responses for a large number of evaluations pertaining to the products or the services being oﬀered on the platform from a pool of customers, whom we refer to as agents. We focus on objective but unveriﬁable evaluations, i.e., evaluations in which the answers can, in principle, be objectively veriﬁed, but such veriﬁcation is infeasible for the platform. This is the case for evaluations comprising of questions like:
1. What was your waiting time to get a table in the restaurant? (Less than 15 mins/Between 15-30 mins/More than 30 mins)
2. Did the plumber show up within 5 mins of your appointed time? (Yes/No) 3. How long did the moving company take to respond with a quote? (1 day/2 days/3 days/more
than three days) 4. Did the dimensions of the received product exceed the dimensions given by the seller? (Yes/No) 5. How long did it take for the product to arrive after the purchase was made from the seller? (1
week/2 weeks/3 or more weeks) In each of these questions, the evaluating agent is being asked to truthfully report an observation about the entity being evaluated. The main property of such evaluations is that each evaluating agent’s observation is an independent sample from an unknown distribution of behaviors speciﬁc to the entity being evaluated. In other words, the true responses of agents for a ﬁxed evaluation task are conditionally independent and identically distributed (conditional on the unknown distribution of responses). For example, in the ﬁrst situation, we can assume that each customer experiences an independently sampled waiting time from a common unknown distribution speciﬁc to that restaurant. In the second situation, the customer’s experience is sampled from the distribution of whether or not the plumber is punctual. Similarly, in the remaining questions, the customer’s true experience is a sample of the moving company’s or the seller’s business practices. We will refer to this property

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
3
as the responses being homogeneous for the rest of the paper, informally referring to the fact that the true responses of any set of agents to a ﬁxed evaluation task are statistically exchangeable.
In such scenarios, we hope to achieve the following informal goals through the design of an eﬀective incentive mechanism: (a) incentivize agents to participate in the provision of feedback in online platforms, i.e., improve response rates, and (b) conditional on participation, incentivize agents to report true observations while overcoming any observation or reporting bias. The second goal is arguably more critical and challenging since an easy way of achieving the ﬁrst goal is to give everyone a ﬁxed reward for participation. As one can imagine, such a naïve reward scheme may not necessarily lead to a high quality of responses.
If the platform could verify the responses to the evaluations, it can simply reward the agents based on whether or not they reported their true observations. But such veriﬁcation is infeasible for questions such as the ones mentioned above since these evaluations are based on interactions that take place outside the platform. In these cases, inducing truthful behavior is a challenging problem. A common approach to this problem, ﬁrst described in the pioneering work of Miller et al. (2005), is to reward the agents’ responses based on comparisons with the responses of other agents who have performed the same evaluation task. Such mechanisms have come to be referred to as peer-prediction mechanisms in subsequent literature (after the original mechanism called the peer-prediction method described in Miller et al. (2005)). Informally, such reward mechanisms leverage the property that the true response of any agent is correlated with the response of some other agent for the same question.1 The situation is then inherently strategic, in which one hopes to sustain truthful reporting as an equilibrium of the game that the reward mechanism induces. It is additionally desirable that such an equilibrium is preferable to the agents over other non-truthful equilibria that may arise in the game.
Our contribution. In this paper, we propose the Square Root Agreement Rule (SRA): a new peerprediction mechanism for online platforms that truthfully elicits objective but unveriﬁable responses at equilibrium. In the setting of our interest, i.e., elicitation on online platforms, we show that the truthful equilibrium under SRA satisﬁes a key dominance property; namely, it yields the agents the highest payoﬀ amongst all symmetric equilibrium payoﬀs in the system limit where there are a large number of evaluation tasks. In addition, the truthful equilibrium payoﬀ is strictly higher than that under any symmetric equilibrium strategy proﬁle that incurs any degree of information loss in the
1 With homogeneous responses, the structure of the correlation between an agent’s true response and the true response of a typical agent in the population is identical across agents. This feature contrasts with the case when the responses are heterogeneous, i.e., when the agents’ true responses strongly depend on their characteristics that vary widely across the population. In these cases, designing mechanisms without obtaining requisite ﬁne-grained information about agent heterogeneity or without making any regularity assumptions on the agent responses, e.g., ‘self-predicting responses’ (Radanovic et al. 2016) or ‘categorical responses’ (Dasgupta and Ghosh 2013, Shnayder et al. 2016) (these assumptions are discussed in Section 2), is known to be impossible (Radanovic and Faltings 2015).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
4
reports. In keeping with the existing terminology in the literature, we refer to this property as SRA being asymptotically strongly truthful across symmetric equilibria. Such a dominance property is crucial in these settings since it hinders the emergence of low-eﬀort equilibria with poorly informative reports – such as everyone reporting the same answer irrespective of their true evaluation – known to plague many other incentive mechanisms, e.g., the peer-prediction method of Miller et al. (2005).
Moreover, under certain additional assumptions satisﬁed in applications such as crowdsourcing,2 we show that the truthful equilibrium under SRA gives the highest payoﬀ amongst all equilibrium payoﬀs (and not just symmetric equilibrium payoﬀs) in the large system limit, i.e., SRA is asymptotically strongly truthful under these assumptions.
While such strong truthfulness guarantees are satisﬁed by existing mechanisms, they either impose conditions on the response distributions that are not satisﬁed in general for objective evaluations (Dasgupta and Ghosh 2013, Radanovic et al. 2016) or they require a prohibitively large number of evaluations from each agent and assume that the agent uses the same reporting strategy for each evaluation (Kong and Schoenebeck 2019, Kong 2020). SRA is the ﬁrst known mechanism that achieves this guarantee for objective evaluations without imposing any such constraints; in particular, SRA is the ﬁrst mechanism satisfying strong truthfulness guarantees for objective evaluations that incentivizes truthful behavior even among agents who perform a single evaluation.
This result is arguably non-trivial. The dominant existing framework for designing strongly truthful peer-prediction mechanisms is due to Kong and Schoenebeck (2019), which incentivizes truthful behavior only if the agents perform multiple evaluations (at least twice the number of possible responses to any evaluation; see Kong (2020)) and additionally commit to using the same reporting strategy for each evaluation. In Section 2 and Section D.4 in the Appendix, we show that even if one leverages the homogeneous responses property satisﬁed by objective evaluations, it is not possible to generically adapt the approach of Kong and Schoenebeck (2019) to incentivize truthfulness in a single evaluation. By designing SRA, we nevertheless demonstrate that this is indeed possible. In showing this result, we make novel information-theoretic contributions that are of interest beyond this work.
The fact that SRA strongly incentivizes even a single response is vital since requiring multiple evaluations from any agent to incentivize truthfulness is impractical in platforms where customers interact with the marketplace relatively rarely, e.g., in vacation rental platforms such as Airbnb. In these settings, requiring a single additional evaluation from each user may prohibitively increase the evaluation period’s duration. Such delay and resulting changes in market characteristics over time
2 Crowdsourcing on labor platforms such as Amazon Mechanical Turk is an important means for sourcing the largescale execution of information-oriented micro-tasks, such as obtaining labeled data for training machine learning algorithms. Incentivizing truthful, high-quality responses from participants is a key concern in these applications.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
5
undermine the platform’s ability to procure the most up-to-date feedback information reliably. While there are mechanisms that incentivize truthfulness in a single evaluation under the homogeneous responses assumption (see Section 2 for a discussion), none of these mechanisms satisfy the crucial strong truthfulness guarantees that SRA satisﬁes.
Background. To appropriately position our contribution, we ﬁrst present a brief discussion of the main existing approaches in incentive design for the elicitation of unveriﬁable responses.
In a pioneering work in this domain, Miller et al. (2005) considered the case of a single evaluation task and homogeneous responses and described the so-called peer-prediction method that incentivizes truthful answers. The main requirement is that there is a commonly known prior on the unknown distribution from which the agents’ true observations are sampled, and this prior is known to the principal (who, in our case, is the platform). Truthfulness is achieved by rewarding/scoring an agent’s posterior probability distribution of her peer’s answer conditioned on her own answer, using a Proper Scoring Rule (PSR). PSRs are a well-known class of payment/scoring rules that incentivize truthful elicitation of probabilities of events that can be observed at a future date (Brier 1950, Gneiting and Raftery 2007, Savage 1971). This approach is infeasible in platforms since a prior on the distribution of evaluations is typically not a priori available, and even if it is, it may not be common knowledge across all agents and the platform. Moreover, this mechanism is known to induce uninformative equilibria that yield the agents a higher payoﬀ than the truthful equilibrium payoﬀ (Jurca and Faltings 2005).
Another inﬂuential design in this domain, the Bayesian Truth Serum (BTS) (Prelec 2004), and its subsequent reﬁnements and generalizations (Witkowski and Parkes 2012, Radanovic and Faltings 2013, Schoenebeck and Yu 2020), preserved the common prior assumption but relaxed the requirement that the principal needs to know this prior. These mechanisms instead require the agents to make extraneous reports about their beliefs in addition to their answers. In particular, they are asked to report a prediction of the empirical distribution of answers reported by other agents. Again, requiring customers on platforms to provide such extraneous information about their beliefs is a tall order given the already low response rates seen for simpler forms of feedback, e.g., ratings. Unfortunately, such extraneous reports of beliefs, although undesirable, are indispensable for incentivizing a single evaluation; it has been shown that it is impossible to design mechanisms that incentivize truthfulness without obtaining some information about the prior distribution (Jurca and Faltings 2011) (which is obtained via agents’ reports of their beliefs in BTS).
Mechanisms that do not assume that the principal knows the prior on the distribution of answers have been referred to as detail-free in the literature and those that do not require any extraneous reporting from the agents apart from the evaluations are called minimal. Ideally, for reputation systems, we need incentive mechanisms that are both detail-free and minimal, and that do not rely on

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
6
the assumption of the existence of a commonly known prior across agents. In light of the impossibility result mentioned above, this seems like a challenging task, if not entirely impossible. The earliest known minimal and detail-free incentive mechanism for a single evaluation task was designed by Jurca and Faltings (2008), in which respondents arrive sequentially and the distributional knowledge of responses is obtained and leveraged by the mechanism in an online fashion; however, this mechanism critically relies on the assumption of binary evaluations.
This is where a key feature of online platforms can be exploited: they typically host a large number of similar products or services. For instance, there are thousands of similar restaurants listed on review platforms like Yelp that users rate. Online marketplaces like Amazon or eBay would like to obtain reviews for many existing sellers on these platforms. Online labor platforms like Thumbtack and Handy would like to get performance metrics for thousands of workers and service providers that operate on these platforms.
The presence of multiple similar evaluation tasks hints at an approach for designing detail-free mechanisms that are also minimal: the missing information about the prior can be obtained from consistent statistical estimates of the distribution of agent responses derived from the response data across multiple tasks. Witkowski and Parkes (2013) ﬁrst explored such a possibility in the context of crowdsourcing, for eliciting binary (e.g., yes/no) responses. A potential concern with this approach is that it assumes that the response data is truthfully generated. But it turns out that in these situations, with careful design, truthfulness can become a self-fulﬁlling prophecy – truthful behavior is an equilibrium in the induced game when the mechanism assumes that these reports are truthful. This is the basic principle underlying the design of Witkowski and Parkes (2013) that forms the foundation of our design, resulting in the fact that SRA is both minimal and detail-free. Mechanisms exploiting this principle are commonly referred to as multi-task peer prediction mechanisms in the literature.
Structure of SRA. SRA is a multi-task peer-prediction mechanism that builds upon the structure of output agreement mechanisms (Von Ahn and Dabbish 2008, 2004), which are simple and intuitive mechanisms that have been quite popular in crowdsourcing practice, except they suﬀer from a critical drawback of being susceptible to strategic manipulations. In an output agreement mechanism, two agents answer the same question, and they are both rewarded if their answers match. There are two critical drawbacks of this scheme: (a) truthful behavior may not necessarily be an equilibrium (see Section 4.1 for an example) and (b) there is always an undesirable equilibrium in the game it induces, in which every person reports the same answer irrespective of their true evaluation. This equilibrium guarantees each person the highest possible payoﬀ rewarded by the mechanism. Our mechanism overcomes these drawbacks by giving proportionately lower rewards for answers that turn out to be more popular on other similar evaluation tasks. This is achieved by

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
7
inversely scaling the rewards for agreement by a popularity index for each answer, thus discouraging blind convergence on a single answer. This is not a new idea: such a biased output agreement scheme, called the Peer Truth Serum (PTS), was ﬁrst introduced by Jurca and Faltings (2011), and was further reﬁned by Radanovic et al. (2016) and Faltings et al. (2017).
The key innovation in our design is in the way these popularity indices are deﬁned. All the strong incentive properties of our mechanism trace their origin to this novel deﬁnition. In our mechanism, these indices are certain second-order population statistics that capture how frequently two people performing the same task agree on a particular answer on average across all tasks. Formally, the popularity index of an answer is the square root of the estimate of the probability of agreement on that answer obtained from response data. Thus rare agreements receive higher rewards than agreements that are relatively common. As the number of tasks increases, the accuracy of these indices improves, and truthfulness is obtained as a Bayes-Nash equilibrium when the number of tasks is large enough. A common prior is not necessary for this result; it should just be common knowledge amongst agents that the prior satisﬁes a certain non-degeneracy property.
Strong truthfulness. A crucial concern in any reward mechanism is that the induced game may possess multiple equilibria. In such cases, there needs to be an adequate rationale for the truthful equilibrium to be selected. Indeed, the theory of equilibrium selection, i.e., justifying certain equilibria as more likely to arise than others, occupies an important position in game theory; see Harsanyi et al. (1988) and Van Damme (2002), and references therein. It is known that elicitation mechanisms for a single evaluation task with no extraneous reporting (which includes Miller et al. (2005)) possess uninformative equilibria that give a higher expected payoﬀ to each agent than in the truthful equilibrium (Jurca and Faltings 2005). Moreover, these equilibria involve simple strategies such as every agent reporting the same answer, due to which these mechanisms are particularly vulnerable to uninformative feedback. The Bayesian Truth Serum demonstrated that this issue could be overcome by requiring extraneous reports of beliefs; the truthful equilibrium under BTS gives the highest expected payoﬀ to an agent across all equilibria, and in particular, this payoﬀ is strictly higher than that under any equilibrium strategy proﬁle that is not fully informative. Mechanisms that satisfy this property are called strongly truthful mechanisms in the literature. Dasgupta and Ghosh (2013) ﬁrst showed that such strong truthfulness properties could be obtained in the multi-task setting without requiring extraneous reports from the agents. Kong and Schoenebeck (2019) describe a general information-theoretic analysis of incentive mechanisms in this space, and show that most mechanisms achieve such properties by (implicitly or explicitly) connecting the loss in the agents’ expected payoﬀ relative to the truthful equilibrium to some form of mutual-information loss or correlation loss in the population due to deviation from truthfulness.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
8
We show that SRA achieves a vanishing uniform upper bound (in the number of tasks) on the diﬀerence between the expected payoﬀ obtained by the truthful equilibrium and that obtained under any other symmetric equilibrium (equilibrium in which all players choose the same reporting strategy). As the number of tasks grows, asymptotically, the expected payoﬀ in the limit under a truthful strategy proﬁle is higher than that under any other symmetric strategy proﬁle. Moreover, this payoﬀ is strictly higher than that under any symmetric equilibrium strategy proﬁle that is not fully informative. A fully informative strategy proﬁle is one where each agent applies a common permutation map to her observation to generate her report (essentially amounting to relabeling the set of responses). In other words, SRA is asymptotically strongly truthful across all symmetric equilibria. As a dual to this property, under a mild assumption on the strategy spaces, we also show that any symmetric equilibrium that gives the highest expected reward to an agent across all symmetric equilibria must be close, in a well-deﬁned sense, to being fully informative when the number of tasks is large. Such properties hinder the rise of potential “obviously attractive” symmetric equilibria such as all agents reporting the same answer for every evaluation.
The restriction to symmetric equilibrium payoﬀs in the equilibrium dominance property of SRA may seem undesirable. However, this restriction stems from a crucial diﬀerence in our setting compared to the settings considered by other mechanisms that are mainly motivated by crowdsourcing applications. In our setting, identifying information for the diﬀerent entities to be evaluated is available to the agents (captured by the task number in our formal setup). Moreover, allocations of evaluation tasks to the agents are exogenously speciﬁed. Thus, in our setting, the agents are free to choose reporting strategies that depend on the identities of the tasks they perform to coordinate their reports with other agents, in addition to the (desirable) coordination that can be achieved by reporting their observations truthfully. Due to the possibility of such extraneously achieved coordination, it is well known that it is impossible to elicit truthful evaluations under a payoﬀ-dominant truthful equilibrium in general (Gao et al. 2019).3 Other mechanisms get around this diﬃculty by making certain assumptions that eliminate the possibility of the agents choosing reporting strategies that depend on task identity. Although such assumptions may be justiﬁable in applications such as crowdsourcing, we do not rely on such assumptions since they are inappropriate in the context of eliciting feedback on platforms; see Section 5.3 for a discussion. In Section 5.3, we additionally show that if such assumptions are made, then SRA is indeed asymptotically strongly truthful.
In a similar spirit as the framework of Kong and Schoenebeck (2019), these strong truthfulness guarantees are obtained by showing that the expected payoﬀ of an agent under a particular strategy
3 The argument for this impossibility is the following: suppose that there is a mechanism that ensures that the payoﬀs that agents obtain by truthfully signaling an extraneous feature of an entity being evaluated via their reports are always lower than those obtained by truthfully reporting the feature that they are supposed to report. Then one obtains a contradiction by exchanging the role of the extraneous feature and the feature to be reported.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
9
proﬁle under SRA is proportional to a novel notion of a square root agreement measure (SRAM) between two independent responses, which we show to be monotonically decreasing in unilateral information loss in the responses. Both the measure and this monotonicity property are new and of independent interest. Moreover, in Section 2, we show that a generic adaptation of the framework of Kong and Schoenebeck (2019) to the homogeneous response setting along the lines of SRA does not yield truthfulness under any arbitrary mutual information measure. SRAM is thus, arguably, the “right" notion of an agreement measure for objective evaluations. We discuss this aspect in more detail in Section 2.
Robustness. We ﬁnally perform numerical experiments on synthetic as well as real data to test SRA’s robustness in incentivizing truthful behavior in ﬁnite-data settings featuring deviations from the homogeneous responses assumption. This is practically important since, despite making a faithful eﬀort to obtain and report true observations, agents may have residual biases in their observations and reports. Such biases could also capture mild subjectivity in responding to objective evaluations. Hence, it is desirable that SRA incentivizes each agent to be truthful even when the agent accounts for such biases in other agents. We ﬁnd that SRA exhibits a high degree of robustness to these practical concerns and generates strong incentives for truthful behavior.
Reward mechanisms in practice. Non-monetary rewards for incentivizing informative feedback, e.g., coupons, badges, or some form of a reputation score, are commonly seen in crowdsourced review forums like Yelp, Tripadvisor, etc. A prominent example of monetary incentives is the “Rebate for Feedback (RFF)” program that was launched by Taobao.com (one of the world’s largest ecommerce websites), on March 1, 2012.4 In this program, sellers can set a rebate amount in the form of cashback or a store coupon for any items they sell, as a reward for a buyer’s feedback after purchasing that item. If a seller opts for RFF, then Taobao ensures that the rebate is transferred from the seller’s account to a buyer who leaves high-quality feedback. The feedback quality is determined by a machine learning algorithm depending on attributes like the length of the feedback, whether or not certain key features of the item (e.g., longevity, whether or not it is true to size, etc.) are mentioned, etc. The main contention of the present work is that strategic considerations are paramount in incentivizing informative feedback. For example, it is easy to give untruthful feedback that appears to be of high quality to a machine learning algorithm; this is especially a concern for objective evaluations with a ﬁxed, ﬁnite set of answers. SRA can thus be an eﬀective approach to administer such rebate schemes in a manner that is robust to strategic behavior.
Organization of the paper. The remainder of the paper is organized as follows. In Section 2, we discuss related mechanisms and their comparisons with SRA. Section 3 presents a formal description
4 See https://bit.ly/2GVntzC, and also Li et al. (2020).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
10

Mechanism SRA Vanilla output agreement Witkowski and Parkes (2013) Dasgupta and Ghosh (2013) Peer Truth Serum for Crowdsourcing Kong and Schoenebeck (2019) Correlated Agreement (CA) CA-HR (Appendix Section D.1 ) Multi-task Peer Prediction Method Radanovic and Faltings (2015)

(PTSC)

Incentivizes truthful homogeneous responses ✓
✗ ✗ (needs binary responses) ✗ (needs categorical responses) ✗ (needs self-predicting responses) ✓
✓
✓
✓
✓

Incentivizes single evaluations ✓ ✓ ✓ ✓ ✓ ✗ ✗ (≥ 2) ✓ ✓ ✓

Dominance property for truthful equilibrium Strongly truthful across symmetric equilibria None None Strongly truthful across symmetric equilibria Strongly truthful across symmetric equilibria Strongly truthful* (see Section 3) Informed truthful across symmetric equilibria Informed truthful across symmetric equilibria None None

Table 1 Properties of diﬀerent multi-task peer-prediction mechanisms in the many tasks regime in our setting.

of the model considered in the paper. Section 4 presents the SRA mechanism and its main incentive property. In Section 5 we address the issue of equilibrium selection. In Section 6, we perform numerical experiments in a practically motivated experimental setup to test the robustness of SRA in incentivizing truthful behavior to deviations from our main assumptions. We ﬁnally summarize our contributions and conclude in Section 7. The proofs of all of our results can be found in the Appendix.
2. Related literature
As a minimal, detail-free, multi-task mechanism, the key feature of SRA is that it strongly incentivizes truthful responses in homogeneous response settings, even among agents who have performed a single evaluation. We now discuss this property in relation to the properties satisﬁed by other existing multi-task mechanisms. Table 1 summarizes the diﬀerences between SRA and these mechanisms at a high-level.
2.1. Existing strongly truthful multi-task mechanisms We ﬁrst discuss mechanisms that achieve strong truthfulness guarantees, focusing on distinctions from SRA. 1. Dasgupta and Ghosh (2013). Dasgupta and Ghosh (2013) proposed the ﬁrst known detailfree and minimal multi-task peer-prediction mechanism that is also strongly truthful, assuming that agents do not choose reporting strategies contingent on task identities; in the absence of this assumption, it is strongly truthful across symmetric equilibria. The original paper restricted the setting to binary evaluations; however, it was later shown by Shnayder et al. (2016) that the mechanism is strongly truthful for non-binary responses under the condition that the responses are “categorical.” This condition says that conditional on an agent’s answer, the posterior probability of all other agents’ answers must reduce relative to the prior. That is, if P r(y) is the prior probability of an answer y and P r(y|y′) is the conditional probability that some other agent has answer y given that one agent has answer y′, then P r(y′|y) ≤ P r(y′) for all y′ = y. Except for the case of binary evaluations, this condition is not satisﬁed in general under homogeneous responses (see Remark 3 in Section 6).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
11
2. Peer Truth Serum for crowdsourcing (PTSC) (Radanovic et al. 2016). PTSC is the multi-task extension of the PTS mechanism, originally deﬁned by Jurca and Faltings (2011) for the case where the prior distribution of responses is known to the principal. Both these mechanisms operate in the homogeneous responses setting. PTS has a biased output agreement structure requiring only one evaluation per agent, where the popularity index of each answer is the prior probability of an agent reporting that answer. In PTSC, this prior probability is replaced by its estimate computed from the response data obtained from a large number of tasks. In order to obtain truthfulness, PTS/PTSC requires that the agent responses satisfy a “self-prediction” condition, which says that P r(y|y)/P r(y) ≥ P r(y′|y)/P r(y′) for any y′ = y. This is equivalent to saying that P r(y|y) ≥ P r(y|y′) for any y′ = y We can show that this condition is weaker than the categorical responses condition required by the mechanism of Dasgupta and Ghosh (2013) for incentivizing truthfulness; see Proposition E.1 in the Appendix. However, except for the case of binary evaluations, this condition is also not satisﬁed in general under homogeneous responses (see Remark 3 in our numerical evaluations). If this condition is satisﬁed, PTSC has been shown to be asymptotically strongly truthful while restricting to symmetric strategy proﬁles, or in other words, it is strongly truthful across symmetric equilibria.
3. Kong and Schoenebeck (2019). The underlying principle leading to the strong truthfulness property of SRA is closely related to the mechanism design paradigm of Kong and Schoenebeck (2019) (KS), who provide an information-theoretic framework for designing strongly truthful mechanisms for general settings with non-homogeneous responses. Their mechanism operates on a pair of agents, and the payment to each agent is deﬁned to be a scaling of an unbiased estimate of some mutual information measure between the two agents’ response distributions, constructed using their reported responses to a common set of evaluation tasks. A variety of mechanisms can be obtained by varying the information measure. Strong truthfulness follows from the fact that the mutual information measure is monotonically decreasing with respect to loss of informativeness in the agents’ responses. We note that strong truthfulness here rests on the assumption that agents use the same reporting strategy for all tasks. However, in our setting, since agents are allowed to choose reporting strategies contingent on task identities, this mechanism is strongly truthful only across equilibria in which agents choose a common reporting strategy for all tasks that they perform (which includes symmetric equilibria).
Until recently, all known mutual information measures required an unboundedly large number of responses per agent (a large fraction of which must be commonly performed by the two agents)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
12
to construct unbiased estimates.5 Kong (2020) recently proposed a mutual information measure, of which an unbiased estimator can be constructed using a ﬁnite number of per-agent responses. Nevertheless, this construction still requires a number of per-agent responses at least twice the number of possible answers (e.g., 4 responses for binary evaluations). In Section D.3 in the Appendix, we show that there can be no mutual information measure satisfying information monotonicity whose unbiased estimate can be constructed from two agents’ responses to a single evaluation task, even in the homogeneous, binary responses setting. Thus, the KS mechanism design framework fails to incentivize truthful behavior in a single evaluation even in the homogeneous responses setting.
Via the design of SRA, we eﬀectively show that if one leverages distributional information obtained from multiple tasks in the homogeneous responses setting, then there is a mutual information measure (the square root agreement measure) and a deviation from the KS mechanism design framework that utilizes the learned distributional information along with agent responses to compute payments, such that the mechanism strongly incentivizes truthful behavior in even a single evaluation.
Based on SRA’s design, one may wonder if it is possible to obtain a generic adaptation of the KS mechanism to the multi-task, homogeneous responses setting, which utilizes distributional information (obtained from many tasks) to incentivize truthful single responses under any mutual information measure. In Section D.4 in the Appendix, we show that this is not true by considering the Shannon mutual information (Cover and Thomas 2012): we show that a mechanism along the lines of SRA that leverages the Shannon mutual information instead of the square root agreement measure is not truthful in general for homogenous responses. This underscores the importance of the discovery of SRAM and shows that it is arguably the “right” agreement measure for the purpose of strongly incentivizing objective evaluations.
2.2. Other prominent multi-task mechanisms We now discuss multi-task mechanisms that are not known to be strongly truthful in general. 1. Witkowski and Parkes (2013). The mechanism proposed by Witkowski and Parkes (2013) is minimal and requires each agent to perform only one task; however, it requires that the responses are binary, and hence it is not uniformly applicable to the homogeneous responses setting. Additionally, no equilibrium dominance properties are known for this mechanism.
2. Correlated Agreement mechanism (CA) (Shnayder et al. 2016). CA is a multi-task mechanism that incentivizes truthful behavior in the general heterogeneous responses setting. CA operates on a pair of agents, and it requires at least two evaluations per agent. As originally
5 Schoenebeck and Yu (2021) recently proposed a sample-eﬃcient approach to directly learn an appropriate scoringrule for scoring the agents’ reports that implements the mutual-information mechanism of Kong and Schoenebeck (2019), rather than learning the joint distribution and then estimating the mutual information.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
13
described, it assumes that certain information about the joint distribution of the agents’ responses is known to the principal. However, this knowledge assumption can be relaxed in the multi-task setting with homogeneous responses, since this distribution can be estimated from the response data obtained from a large number of tasks (appealing to the self-fulﬁlling prophecy of truthful behavior). Additionally, the requirement of two evaluations per agent can also be relaxed: in Section D.1 in the Appendix, we describe an adaptation of CA to our setting that only requires one evaluation per agent. We call this mechanism CA for homogenous responses (CA-HR).
In the setting in which CA is originally deﬁned, it is assumed that there is no extraneous identifying information for the evaluation tasks and the task allocation is randomized across agents, in eﬀect eliminating the need to consider task-contingent reporting strategies of the kind allowed in our setting. In this setting, CA satisﬁes the dominance property of informed truthfulness, which is weaker than strong truthfulness. Under informed truthfulness, the truthful equilibrium yields the highest equilibrium payoﬀ to each agent, which is higher than an agent’s payoﬀ in any equilibrium where her reporting strategy is fully uninformative, i.e., her reports are independent of her observation. However, in contrast to strong truthfulness, there could be other equilibrium strategy proﬁles that are not fully informative, which yield the same payoﬀ as the truthful equilibrium. In particular, although the informed truthfulness property precludes fully uninformative equilibria where all observations map to a single response, the CA mechanism remains vulnerable to equilibria where agents map smaller sets of responses to a single response (e.g. if the responses are {1, 2, 3, 4} then {1, 2} map to 1 and {3, 4} map to 4), which a strongly truthful mechanism precludes (i.e., strictly payoﬀ-dominates). Such types of equilibria are payoﬀ-equivalent to the truthful equilibrium under CA if the joint distribution of observations of a pair of agents for a ﬁxed evaluation task is “clustered,” as deﬁned in Deﬁnition 10 (from Shnayder et al. (2016)) in the Appendix. In our practically motivated experimental setup in Section 6, we show that instances with clustered observations are encountered with a very high frequency; see Remark 4.
In our setting, since reporting strategies contingent on task identities are allowed, CA and CA-HR are not informed truthful; we present an example Section D.2 in the Appendix illustrating this fact. But they can be shown to be (asymptotically) informed truthful across symmetric equilibria. On the other hand, in Section 5.3, we show that if strategies contingent on task identities are disallowed in our setting and tasks are randomly allocated across agents, the SRA is (asymptotically) strongly truthful across all equilibria. Thus, SRA satisﬁes a stronger equilibrium dominance property compared to CA or CA-HR for homogeneous responses.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
14
3. Multi-task extension of the peer-prediction method (Miller et al. 2005). The peerprediction method is a minimal mechanism and incentivizes truthful responses with only one evaluation per agent in the homogeneous responses setting. As originally described, it assumes that the joint distribution of the agents’ responses is commonly known to the principal and the agents. However, this assumption can be relaxed in the multi-task setting with homogeneous responses since this distribution can be estimated from the response data obtained from a large number of tasks. As we discussed in Section 1, this mechanism achieves truthfulness by rewarding an agent’s predicted probability distribution of her peer’s answer, as implied by her own answer, using a Proper Scoring Rule (PSR).
This mechanism, however, does not satisfy any equilibrium dominance properties: in particular, there exist uninformative equilibria that yield the highest possible payoﬀ to each agent, irrespective of the PSR utilized.6 For example, for any PSR, the highest possible utility under this mechanism (in the many tasks limit) is achieved when all agents simply report the same answer irrespective of their observation. To see this, note that under such a symmetric strategy, in the many tasks limit, the joint distribution of responses estimated by the principal puts a unit probability mass on the event of the two peer agents reporting the ﬁxed answer. Hence, the conditional distribution on the peer’s response implied by an agent’s response perfectly predicts the peer’s response and thus, achieves the highest score. In contrast, we show that such equilibrium leads to a strictly lower payoﬀ than the truthful equilibrium under SRA when the number of tasks is large enough (see Remark 2).
4. Radanovic and Faltings (2015). Radanovic and Faltings (2015) describe a mechanism for the homogeneous responses setting that only utilizes one evaluation per agent. This mechanism is a multi-task extension of the peer-prediction method that relaxes the common prior assumption: they show how an unbiased estimate of the payoﬀ under the peer-prediction method utilizing the quadratic scoring rule (a PSR) can be constructed in the homogeneous responses setting with response data from a random but almost surely ﬁnite number of tasks. However, the mechanism inherits the chief concern regarding the peer-prediction method that we discuss above, in that the strategy proﬁle where everyone reports a ﬁxed answer irrespective of their observations results in the highest possible score and thus a higher payoﬀ than the truthful equilibrium.
6 This observation has already been made for the original non-detail-free version of the peer-prediction method by (Jurca and Faltings 2005).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
15
3. Model
We now describe the details of our model. Operational details. We consider a setting with N evaluation tasks denoted by the set N and
labeled as i = 1, · · · , N . Let M denote the population of M agents, labeled as j = 1, . . . , M . Let Mi ⊆ M denote the subset of agents that perform task i, and let Nj be the set of tasks that an agent j performs. We assume that the sets Mi and Nj are exogenously speciﬁed. The set of possible observations and the set of possible answers in each evaluation task is assumed to be the same ﬁnite set, denoted as Y. A generic element of Y will be denoted as y.
Statistical assumptions. The distribution of observations of agents performing task i is speciﬁed by an unobservable type of the entity being evaluated in task i, denoted as the random variable Xi, which takes values in the ﬁnite set X. A generic element of X will be denoted as x. This set of possible types X is common across all tasks. The distribution of the observations of the agents for any task, as a function of the type x ∈ X of the entity being evaluated in that task, is denoted as p(x) = (py(x); y ∈ Y). The observation of an agent j in Mi is denoted as the random variable Yji, which is independently drawn from p(Xi) for each such agent. This implies that the observations of diﬀerent agents for a single task i are independent conditioned on Xi, but may be dependent otherwise.7 Further, we assume that the types of entities being evaluated in the diﬀerent tasks are independently sampled from a common distribution, PX. We refer to this property as the tasks being statistically similar.
Finally, we assume that from the perspective of any agent j, there are no other observable features of the evaluation task i except the observation Yji. The probability distribution over types, PX, and the function p together form a probability generating model (henceforth referred to as the generating model) of the agent observations, denoted as the pair (PX, p). In particular, this pair fully speciﬁes a joint distribution on the underlying types of the diﬀerent entities being evaluated and the diﬀerent agents’ observations across tasks. The following example illustrates this model.
Example 1. Consider a situation where a labor platform wants to obtain feedback on punctuality of plumbers that operate on the platform. Suppose that each plumber could be of 2 possible unobservable types X = {Punctual, Not Punctual}, with PX(Punctual) = PX(Not Punctual) = 0.5. Each plumber’s type is independently sampled from the distribution PX. The question is “Did the plumber show up within 5 minutes of his/her appointed time?”. The two possible observations/answers are Y =
7 Note that {Yji; j ∈ Mi} is a set of exchangeable random variables by the virtue of the fact that they are i.i.d. conditioned on the unobservable entity type Xi. Instead of explicitly assuming the existence of such a type, we can assume that the set of observations made by any and potentially an inﬁnite number of agents for an entity i are exchangeable random variables, which a property that is expected to hold in practice for objective evaluations. De Finetti’s theorem (Aldous 1985) would then imply the existence of a type for each entity such that the observations of the agents are conditionally i.i.d.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
16

{Yes, No}. And the distributions of true answers as a function of type are p(Punctual) = (0.95, 0.05) and p(Not Punctual) = (0.5, 0.5).
We want to remark that the above example is merely illustrative: the type of plumber need not have any semantic interpretation. It may lie in some abstract space. More importantly, we emphasize that this type is unobservable, both, to the agents and the principal.
The payment mechanism. Our goal is to design a payment mechanism that elicits observations from the agents. For any j ∈ Mi, let rji denote agent j’s reported answer for task i. We deﬁne a payment mechanism as follows.
Definition 1. A payment (or reward/scoring) mechanism is a set of functions {τj : j ∈ M}, one for each person in the population, that map the reports {rji : j ∈ M, i ∈ Nj} to a real valued payment (or score).
Agents’ strategies. An agent j’s strategy is a set of mappings {qij : i ∈ Nj} where qij(y) = (qyij′(y); y′ ∈ Y) is the probability distribution over answers for evaluation task i ∈ Nj conditional on the observation being y. We emphasize that agents are allowed to choose diﬀerent reporting strategies for the diﬀerent tasks, i.e., their reporting strategy can be task-contingent. We however restrict ourselves to considering task-contingent reporting strategies in which the reported answer of an evaluation only depends on the observation for that evaluation instead of potentially depending on the observations for all the other evaluations that the agent performs. This restriction is simply for the ease of exposition. All the incentive properties continue to hold for our proposed mechanism even if such strategies are allowed. This is simply because the payment in our mechanism to any agent is additive over the tasks that she performs. Hence, by the expectation operator’s additivity, only the marginal distributions of the responses for the diﬀerent tasks matter in determining the total expected payoﬀ to an agent. Thus, choosing a reporting strategy for a given task that depends on the observations for other tasks is equivalent in terms of expected payoﬀ to choosing the reporting strategy based on the output of some randomization device that produces values that are identically distributed to the observations for these other tasks. Such a reporting strategy is already included in the space of strategies for each agent.
Equilibrium notion. We deﬁne the following notion of a Bayes-Nash equilibrium (Myerson 2013) in the game induced by a payment mechanism.
Definition 2 (Bayes-Nash Equilibrium). Given a generating model (PX, p) that is common knowledge amongst the agents, we say that a strategy proﬁle {qij : j ∈ M, i ∈ Nj} comprises a Bayes-Nash equilibrium in the game induced by the payment mechanism if for each j ∈ M,

E τj {qij′ (Yji′ ) : j′ ∈ M, i ∈ Nj′ }

≥ E τj {q¯ij (Yji) : i ∈ Nj} ∪ {qij′ (Yji′ ) : j′ ∈ M, j′ = j, i ∈ Nj′ } ,

(1)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
17
for each {q¯ij : i ∈ Nj} = {qij : i ∈ Nj}, where the expectation is with respect to the joint distribution on the responses of the population speciﬁed by the generating model (PX, p). We say that the strategy proﬁle is a strict Bayes-Nash equilibrium if the above inequality is strict.
In words, this says that assuming all the other agents adhere to the reporting strategy proﬁle {qij : j ∈ M, i ∈ Nj}, each agent maximizes her expected reward by also adhering to the prescriptions of the strategy proﬁle. Next, we deﬁne Bayes-Nash incentive compatibility, which is the property that truthful reporting is a Bayes-Nash equilibrium of the game induced by the reward mechanism.
Definition 3 (Bayes-Nash Incentive compatibility). We say that a payment mechanism is Bayes-Nash incentive-compatible with respect to the generating model (PX, p) if the truthful reporting strategy proﬁle, i.e., where qyij′(y) = 1{y′=y} for all j ∈ M and i ∈ Nj, is a Bayes-Nash equilibrium in the game induced by the mechanism. If this equilibrium is strict, we say that the mechanism is strictly Bayes-Nash incentive compatible.
Informational assumptions. We make the following informational assumptions. 1. The principal is not assumed to know PX or p. Hence, (PX, p) is not an input to the payment
mechanism. 2. We assume that the structure of the underlying generating model, i.e., the existence of some
prior distribution PX from which the type of any evaluated entity is drawn, and the function p that captures the conditional distribution of observations given the type, that is common across entities being evaluated, is common-knowledge across all agents. In particular, this means that all the agents commonly know that all the tasks are statistically similar, and the observations of agents performing each evaluation are statistically homogeneous. Additionally, we will also assume that it is commonly known to all the agents that the generating model (PX, p) satisﬁes a certain separation property, which we deﬁne in Section 4.2 (Deﬁnition 5), where we also discuss simple interpretations of this property. Finally, we assume that the payment mechanism, once ﬁxed by the principal, is publicly announced and is commonly known to all agents. Note that the deﬁnitions of Bayes-Nash equilibrium and Bayes-Nash incentive compatibility (Definitions 2 and 3) assume that the generating model is commonly known to the agents. However, we show that SRA is Bayes-Nash incentive-compatible with respect to any commonly known generating model that satisﬁes the separation property (discussed in Sections 4.2; Deﬁnition 5). Consequently, it is only necessary for the agents to commonly know that this separation property is satisﬁed by the generating model to obtain truthful behavior. Similarly, we show that the other properties we discuss concerning the Bayes-Nash equilibria in the game induced by SRA hold for any commonly known generating model that satisﬁes the separation property. Hence, to obtain these properties, we only need to assume that it is common knowledge amongst the agents that the generating model satisﬁes this property.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
18
4. The square root agreement rule (SRA)
Our main proposal, the square root agreement rule (SRA), is formally deﬁned in Mechanism 1. Informally, the mechanism can be described as follows. Consider an agent j who has performed evaluation task i. Suppose that she submits an answer y ∈ Y. Then, she receives payment for this answer only if another agent j′, who has performed the same task i, and who is chosen to be her peer, also reports the same answer y. This payment denoted as ej(y) is inversely proportional to the square root of the empirical frequency at which arbitrarily chosen agents agree on answer y across all tasks that j has not performed; see Equation 3. This empirical frequency is denoted by f¯j(y) and is computed in Equation 2. To ensure that f¯j(y) > 0 and the inverse is well deﬁned, we use a smoothed version of empirical frequency, i.e., we add 1 to the total number of agreements on each answer before dividing by the number of tasks j has not performed. The following example illustrates the mechanism.

Mechanism 1: The square root agreement rule (SRA). Assumes |Mi| ≥ 2 for all i ∈ N.
The responses of agents for the diﬀerent evaluation tasks are solicited. Let these be denoted by {rji : j ∈ M, i ∈ Nj}. An agent j’s payment is computed as follows:

• For each population Mi such that i ∈/ Nj, choose any two agents j1(i), j2(i) ∈ Mi, and for each possible evaluation y ∈ Y, compute the quantity

f¯ (y) = 1 (1 +

1i

1i

).

(2)

j

N − |Nj|

{rj1(i)=y} {rj2(i)=y}

i∈N\Nj

• For each answer y, ﬁx a payment ej(y) deﬁned as

e (y) = K .

(3)

j

f¯j (y)

where K > 0 is any positive constant. f¯j(y) is the popularity index of answer y.

• For computing agent j’s payment for evaluation task i ∈ Nj, choose another agent j′ ∈ Mi, who will be called j’s peer for task i. If their responses match, i.e., if rji = rji′ = y, then j gets a reward of ej(y). If the responses do not match, then j gets 0 payment for
that task.

Example 2 (SRA in action). Consider the labor platform presented in Example 1. During an evaluation period, the platform solicits answers to the question “Did the plumber arrive within 5 minutes of his/her appointed time?” from all the customers who have hired a plumber from a set of a priori similar plumbers operating on the platform (e.g., new plumbers who have recently joined

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
19
the platform) during this period. Suppose a customer, Susan, reports an answer “Yes,” meaning that she reports that the plumber that she hired, Tim, did arrive within 5 minutes of his appointed time. Then, Susan gets a reward only if a randomly chosen customer who has also hired Tim in the same period also reports the answer “Yes.” The reward is inversely proportional to a popularity index of the answer “Yes” across the customer population computed from the response data. SRA deﬁnes this popularity index as the square root of the (smoothed) empirical frequency at which two customers who hire the same plumber both report the answer “Yes” across all the plumbers that Susan hasn’t evaluated. The payment procedure is similar if Susan reports “No” instead.
Remark 1. Note that in SRA, a separate set of popularity indices for the diﬀerent answers is computed for each agent based on population responses for tasks that this agent hasn’t performed. In practice, however, these indices are expected to be almost identical across agents when the total number of tasks is large relative to the number of tasks each agent performs; hence one can potentially calculate a single set of popularity indices of the answers and use them for all agents with negligible impact on incentives. Our theoretical results, however, pertain to SRA as it is formally deﬁned.
4.1. The main ideas behind SRA SRA is a biased output agreement mechanism, i.e., an agent gets paid for her evaluation only if her answer matches the answer reported by her peer agent who has made the same evaluation, where the payment depends on the answer. The simplest description of the core idea of the mechanism is obtained in the hypothetical scenario where the generating model (PX, p) is known to the principal and is commonly known to the agents (this is the setting considered by Miller et al. (2005)). In this setting, a biased output agreement scheme is deﬁned as follows.
1. Each agent j is paired with another randomly chosen peer agent j′, and their responses are compared.
2. If their responses don’t match, then j gets no reward. 3. If their responses match and this common response is y ∈ Y, then j gets a positive reward e(y). The agreement rewards e(y) for y ∈ Y are deﬁned as a function of the generating model of responses. The main innovation in SRA is how these agreement rewards are deﬁned. To motivate their design, we ﬁrst discuss why the naïve output agreement mechanism fails to incentivize truthful behavior. Failure of naïve output agreement. In the naïve output agreement mechanism, e(y) is deﬁned to be a constant K > 0 for all y ∈ Y. This mechanism tries to exploit an intuitive property that one may naïvely expect to hold in many scenarios, which is that the peer agent j′ has the highest conditional likelihood of observing the same answer as that observed by an agent j. However, this property is not always true. For instance, it is violated if, irrespective of the observation made by

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
20

an agent, one “popular” answer has an overwhelming conditional likelihood of being observed by the peer agent. This feature can be observed in Example 1.
Example 3. Consider the setting in Example 1. The question is “Did the plumber show up within 5 minutes of his/her appointed time?” with the possible observations/answers being Y = {Yes, No}. Suppose an agent observes that the plumber did not arrive within 5 minutes of her appointed time, i.e., her observation was “No.” Then the conditional probability of the peer agent, assumed to be truthful, replying “Yes” can be computed to be 0.5409, which is higher than the conditional probability of her replying “No,” which can be computed to be 0.45909. Hence, replying “Yes,” i.e., lying, results in a higher expected payoﬀ than being truthful and replying “No.”
In the example above, plumbers are a priori overwhelmingly likely to turn up on time, to the extent that even if an agent observes that a plumber was late, she will still ﬁnd it more likely that the same plumber will be observed to be on time by her peer agent. Thus, assuming that the peer agent is truthful, it is better to lie and say that the plumber was on time. Summarily, truthful behavior is not an equilibrium in this case.
To overcome this shortcoming of the naïve output agreement scheme, the key obstacle that one must tackle is this tendency of regressing to the conditionally most popular answer. Formally, if the observation of an agent j is Yj = y for some y ∈ Y, her tendency is to report arg maxy′∈Y P (Yj′ = y′ | Yj = y) so as to maximize the probability of agreement. As we saw in the example above, this optimal answer need not necessarily be y, i.e., the inequality

P (Yj′ = y | Yj = y) ≥ P (Yj′ = y′ | Yj = y),

(4)

doesn’t necessarily hold for all y, y′ ∈ Y, even in the binary responses setting where |Y| = 2. Biased output agreement schemes can tackle this issue by scaling the rewards for agreement depending on the answer, essentially lowering rewards for answers that are expected to be more (conditionally) likely and increasing rewards for answers that are less (conditionally) likely. The intuition is that if an agent observes an answer that is less likely to also be observed by her peer, she is still incentivized to report that answer since the matching reward on that answer is higher. Conversely, if she observes an answer that is more likely to also be observed by her peer, she prefers to report that answer despite the low reward for agreement relative to other answers since the probability of agreement is higher. The challenge is to design the reward scalings for diﬀerent answers so that these incentives for truthful reporting for an agent are satisﬁed for each answer (assuming every other agent is truthful).
A straightforward way of making an agent indiﬀerent between diﬀerent reports (and hence weakly incentivize truthful behavior) is by deﬁning the agreement reward for answer y′ ∈ Y to be proportional to 1/P (Yj′ = y′ | Yj = y), where y is the answer observed by the agent. This approach is

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
21

rendered infeasible by observing that y is not known to the mechanism – indeed, the entire exer-

cise is meant for the purpose of eliciting y. With this observation in perspective, we discuss two

approaches of deﬁning the rewards before we present our approach in SRA.

Approach 1: Peer Truth Serum. The PTS mechanism (Jurca and Faltings 2011, Faltings et al. 2017) scales the agreement reward for answer y′ ∈ Y by 1/P (Yj′ = y′), i.e., reward for

agreement on an answer is inversely proportional to the probability that an evaluating agent makes

that observation. We thus obtain truthful behavior from agent j under PTS if for all y ∈ Y and y′ = y,

P (Yj′ = y | Yj = y) P (Yj′ = y′ | Yj = y)

≥ P (Y ′ = y′) , i.e., if,

(5)

P (Yj′ = y)

j

P (Yj′ = y | Yj = y) ≥ P (Yj′ = y | Yj = y′).

(6)

In the literature, this is referred to as the ‘self-predicting responses’ condition (that we discussed ear-

lier in Section 2), which is not satisﬁed in general for homogeneous responses, except when |Y| = 2.

Intuitively, deﬁning the popularity index of an answer to be equal to the marginal probability of a

single agent making that observation does not capture the fact that the agent evaluates the condi-

tional probabilities of agreement for the diﬀerent answers, which depend on the joint probabilities

of a pair of agents making various observations.

Approach 2: A “conditional” peer truth serum (CPTS). With the goal of incorporating

the conditional distribution of the observations in the deﬁnition of the agreement rewards, another proxy for the ideal scaling can be deﬁned to be 1/P (Yj′ = y′ | Yj = y′), i.e., e(y′) is deﬁned to be proportional to 1/P (Yj′ = y′ | Yj = y′). In this case, we obtain truthful behavior from agent j if for all y ∈ Y and y′ = y,

P (Yj′ = y | Yj = y)

P (Yj′ = y′ | Yj = y)

= 1 ≥ P (Y ′ = y′ | Y = y′) , i.e., if,

(7)

P (Yj′ = y | Yj = y)

j

j

P (Yj′ = y′ | Yj = y′) ≥ P (Yj′ = y′ | Yj = y).

(8)

This condition is exactly the self-prediction condition from Equation 6. Thus, the conditions that are necessary for the PTS and CPTS mechanisms to induce truthful behavior are the same. This shows that a naïve incorporation of conditional probabilities in the reward scalings need not provide any advantage over PTS, at least in terms of the conditions required for truthfulness.
SRA’s approach. SRA deﬁnes the rewards for agreement as e(y′) = K/ P (Yj = Yj′ = y′) for each y′ ∈ Y, for some K > 0. To put it simply, the reward for an agreement is inversely proportional to the square root of the probability of that agreement. Thus a more probable agreement earns a

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
22

lower reward than an agreement that is relatively less probable. To see the relation to PTS and CPTS, observe that SRA simply replaces the scaling 1/P (Yj′ = y′) of PTS and the scaling 1/P (Yj′ = y′ | Yj = y′) of CPTS by the product of the square root of the two scalings 1/ P (Yj′ = y′) × 1/ P (Yj′ = y′ | Yj = y′) = 1/ P (Yj = Yj′ = y′). By making this change, SRA explicitly incorporates

the joint probabilities of agreements in the rewards.

It turns out that the scaling under SRA successfully overcomes the shortcoming of the naïve

output agreement scheme in homogeneous responses settings. In particular, truthful behavior is a

Bayes-Nash equilibrium in this mechanism. To see this, consider an agent j, and suppose that all

other agents are truthful. Then if j’s true response is y, her expected reward for a truthful report

is,

K P (Yj′ = y | Yj = y) = K P (Yj′ = Yj = y) .

(9)

P (Yj = Yj′ = y)

P (Yj = y)

Similarly, her reward for any other report y′ is,

K P (Yj′ = y′ | Yj = y) = K P (Yj′ = y′, Yj = y) . (10)

P (Yj = Yj′ = y′)

P (Yj = y) P (Yj = Yj′ = y′)

Thus being truthful gives a higher reward if the quantity in Equation 9 is larger than the quantity

in Equation 10, which is, if,

P (Yj′ = Yj = y) P (Yj = Yj′ = y′) ≥ P (Yj′ = y′, Yj = y).

(11)

This inequality resembles the well-known Cauchy-Schwarz inequality that relates second-order moments of two random variables X and Y as, E[XY ] ≤ E(X2)E(Y 2). Hence, if the joint distribution of responses of the two agents satisﬁes this inequality for each y, y′ ∈ Y, we say that the distribution satisﬁes the “Cauchy-Schwarz” (CS) property. And in this case, truthful equilibrium is a Bayes-Nash equilibrium under SRA.
Now the key interesting fact is that this property is always satisﬁed for homogeneous responses, i.e., for objective evaluations. To see this, the inequality in Equation 11 can be expressed as follows in the homogeneous responses setting.

PX (x)py(x)2

PX (x)py′(x)2 ≥ PX (x)py(x)py′(x).

x∈X

x∈X

x∈X

(12)

But this is precisely the Cauchy-Schwarz inequality that always holds. Hence, truthful behavior is incentivized as a Bayes-Nash equilibrium under SRA. For truthful behavior to be a strict Bayes-Nash equilibrium, we need the above inequality to be strict. We show that this requirement is satisﬁed if the generating model is ‘separated’: a condition we discuss in Section 4.2.
Now implementing the mechanism above in our setting is infeasible since the principal does not know the generating model (PX, p). Moreover, the generating model is not assumed to be commonly

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
23

known to the agents. SRA addresses these issues by replacing the required agreement probabilities

with consistent statistical estimates computed from reports obtained across multiple tasks. Observe

that if everyone except agent j is truthful, then in the deﬁnition of SRA, E[f¯j(y)] = P (Yji1(i) = Yji2(i) = y) + 1/(N − |Nj|). In fact, as N grows large, assuming |Nj| remains bounded, f¯j(y) almost surely

converges to P (Yji1(i) = Yji2(i) = y) by the strong law of large numbers, i.e., f¯j(y) is an asymptotically

consistent

estimate

of

P

(

Y

i j1

(

i)

=

Yji2 (i)

=

y).

For

a

large

enough

N,

we

can

show

that

the

estimate’s

quality is suﬃciently high to ensure that truthfulness is recovered as a strict Bayes-Nash equilibrium

for any ‘separated’ generating model (see our main result in Section 4.3). Thus, the agents only

need to commonly know the generating model’s structure and that it is separated to obtain truthful

behavior.

4.2. Obtaining strictness An important goal for any reward mechanism is to strictly incentivize truthfulness, i.e., in the truthful equilibrium, each agent gets a strictly higher reward by being truthful than by adopting any other strategy. Without this property, trivial mechanisms like the one that gives a ﬁxed payment to each agent regardless of her report, in principle, weakly incentivize truthfulness. For truthfulness to be a strict equilibrium under SRA, we need the Cauchy-Schwarz inequality in Equation 12 to be strict for every pair y, y′ ∈ Y. It will be useful to deﬁne the following notion of the “inequality gap.”
Definition 4 (Cauchy-Schwarz inequality gap). For a generating model (PX, p) deﬁned on X and Y, deﬁne

δ(PX, p) = min y, y′∈Y, y=y′

( PX (x)py(x)2)( PX (x)py′(x)2) − PX (x)py(x)py′(x).

x∈X

x∈X

x∈X

By the Cauchy-Schwarz inequality, δ(PX, p) ≥ 0. If δ(PX, p) > 0 for some generating model (PX, p), then truthfulness is a strict Nash equilibrium in the game induced by the mechanism we described earlier for the case where the principal knows this generating model. Consider the following

deﬁnition, which will be useful for our forthcoming discussion.

Definition 5 (Separation). We say that a generating model (PX, p) is separated if δ(PX, p) > 0. We say that it is α-separated for any α > 0 if δ(PX, p) ≥ α.
To understand whether separation is a reasonable assumption on the generating model, a little

demystiﬁcation of this condition is in order. For any answer y ∈ Y, deﬁne the vector

v(y)

PX(x)py(x); x ∈ X .

(13)

Then the Cauchy-Schwarz inequality says that for any two answers y and y′, the magnitude (in the Euclidean norm) of the projection of the vector v(y) on the unit vector in the direction v(y′) is less than the magnitude of the vector v(y) itself (one can reverse the roles of y and y′), i.e.,
|v(y).v(y′)| ≤ v(y′) , v(y)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
24

or

|v(y).v(y′)| ≤ v(y) v(y′) .

(14)

Let θ(u, v) denote the angle in radians between two vectors u and v, deﬁned as θ(u, v) arccos u.v , (15) uv
when both u and v are non-zero and as 0 when either of them is a zero vector. We can then show that the inequality in Equation 14 is strict if and only if the angle between the vectors v(y) and v(y′) is positive. The following proposition gives a precise statement.

Proposition 4.1 For a generating model (PX, p) deﬁned on X and Y, the following two conditions are equivalent.
1. There is some α > 0 such that (PX, p) is α-separated. 2. There is some γ > 0 such that θ(v(y), v(y′)) ≥ γ for all y, y′ ∈ Y such that y = y′.
Thus, separation is equivalent to assuming that the angle between v(y) and v(y′) is positive for any y = y′. If this is not true for some y and y′, then there is a C ∈ R such that py(x) = Cpy′(x) for each x ∈ X such that PX(x) > 0. But in this case, the responses y and y′ need not be distinguished at all, since they contain the same information about X, and hence about the rest of the random quantities. In particular, P (Xi = x|Yji = y) = P (Xi = x|Yji = y′) for each x ∈ X. Hence, the principal can simply ask the agents to map both these responses to a single response.
In the context of our model, separation is also equivalent to the stochastic relevance condition that is imposed to obtain strictness in several works in this domain, starting from Miller et al. (2005). An agent’s answer to a question is a stochastically relevant random variable if no two answers induce the same conditional distribution on the answers of some other agent who has answered the same question. Clearly, if θ(v(y), v(y′)) = 0, then stochastic relevance is violated, and thus stochastic relevance implies that θ(v(y), v(y′)) > 0, which is equivalent to separation by Proposition 4.1. Showing that separation implies stochastic relevance is less straightforward and we show it in Proposition E.2 in the Appendix.
4.3. Main result The following result presents the main incentive property satisﬁed by SRA.
Theorem 1. Consider an α-separated generating model (PX, p) that is commonly known to the agents. Further, suppose that Nj ≤ n for all j ∈ M and |Mi| ≥ 2 for all i ∈ N. Then for any ω ∈ (0, αK(|Y| − 1)), there exists a positive integer N0 that depends only on ω, α, |Y|, n, and K such that if the number of evaluation tasks N > N0, then

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
25
• SRA is strictly Bayes-Nash incentive compatible with respect to (PX, p), and, • At the truthful Bayes-Nash equilibrium, the expected payoﬀ to an agent under the truthful
strategy is at least ω higher than the expected payoﬀ under any reporting strategy where the agent’s reported response is independent of her true response.
Note that the bounds we derive in the proof of this result can be used to explicitly calculate an N0 as a function of ω, α, |Y|, n, and K. Also note that, although the theorem assumes that the generating model is commonly known to the agents, the mentioned properties hold for any αseparated generating model. In particular, the dependence of N0 on the generating model is only through α. Hence, this result implies that even if only the fact that (PX, p) is α-separated is common knowledge amongst agents, then irrespective of their individual beliefs about the speciﬁcs of the generating model, truthful reporting is strictly incentivized in the game induced by the mechanism for a large enough N .
The second claim in the theorem is crucial too: it says that in the truthful equilibrium, the diﬀerence in the payoﬀs to an agent under the truthful strategy and under any strategy in which an agent’s reported response is independent of her true response, is bounded away from zero. Such strict incentives allow the principal to account for any costs that the agents may incur for their evaluation eﬀort by appropriately scaling the mechanism’s rewards. Note that it is not possible to ensure that the diﬀerence between the expected payoﬀ under truthful behavior and that under any other strategy is bounded away from zero, since one can choose a randomized strategy that chooses a non-truthful response with an arbitrarily small probability. However, our goal here is to deter agents who report an arbitrary answer without investing eﬀort into making an observation. Our result ensures that any perceived cost for such eﬀort can be absorbed in the diﬀerence in the payoﬀ under truthful reporting and under any reporting strategy that ignores the observation, by appropriately scaling the rewards.8
Finally, we note that although the common knowledge assumption above is necessary to obtain the theoretical properties of our mechanism, our numerical experiments in Section 6 test SRA under more practical considerations.
5. Equilibrium selection
In this section, we address the issue of multiplicity of equilibria in the game induced by SRA. First, observe that if truthful behavior is an equilibrium, then so is any symmetric fully informative strategy proﬁle in which all agents apply a common permutation map to the responses they receive. And all such equilibria are payoﬀ-equivalent. But the signiﬁcantly higher degree of coordination needed
8 Liu and Chen (2016) show how to learn such a scaling when there is heterogeneity in the cost for eﬀort in classical output agreement mechanisms. We believe a similar approach can be adopted in our case.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
26

for the agents to play a fully informative equilibrium other than truthful behavior makes it unlikely that such equilibria will emerge in practice. Thus full informativeness shall be our benchmark as we focus on other equilibria that may emerge.
The equilibria that give high expected payoﬀs are arguably the most attractive for the agents and thus can be assumed to have an increased likelihood of being chosen. In what follows, we show that for a large N , the truthful equilibrium is approximately payoﬀ-optimal across all symmetric equilibria, with an approximation error that vanishes in N . In the limit, any symmetric fully informative strategy proﬁle gives a strictly higher expected payoﬀ to any agent than any other symmetric strategy proﬁle. We also show a weak dual to this result: under a certain assumption on the strategy spaces, any symmetric equilibrium that results in the highest expected payoﬀ to an agent across all symmetric equilibria cannot be too “uninformative” when N is large, where “uninformativeness” is a precise notion that we deﬁne.

5.1. Truthfulness vs. symmetric equilibria as N → ∞ Before we discuss the result for a large but ﬁnite N , let us ﬁrst discuss the result in the limiting case as N → ∞, which is easier to obtain, and sheds light on the core idea. Consider a symmetric strategy proﬁle in which every agent adopts a reporting strategy q, where q(y) = (qy′(y); y′ ∈ Y) is the distribution over the reported response conditional on the true response. Let us denote the reported responses under this strategy by the random variables {Zji; i = 1, · · · , N, j ∈ Mi}. Under the truthful strategy proﬁle (or equivalently, any symmetric fully informative strategy proﬁle), in the limit as N → ∞, the expected reward of each agent performing task i converges to (see Equation 9),

P (Yji = y)K
y∈Y

P (Yji′ = Yji = y)

P (Yji = y)

=K
y∈Y

P (Yji′ = Yji = y) = K
y∈Y

PX (x)py (x)2.
x∈X

(16)

Whereas, under any other symmetric strategy proﬁle, the expected reward of each agent converges

to,

K
y∈Y

P (Zji′ = Zji = y) = K
y∈Y

PX (x)( py′(x)qy(y′))2.

x∈X

y′∈Y

(17)

It turns out that the quantity in Equation 17 is, in general, lower than the quantity in Equation

16. How much lower depends on the ‘uninformativeness’ of the strategy q: the more uninformative

the strategy q, the higher is the diﬀerence. We will describe this phenomenon more generally since

we believe it has applications beyond this work (see Appendix B.1 for a discussion). We ﬁrst deﬁne

the following notion of a square root agreement measure.

Definition 6 (Square root agreement measure (SRAM)). Consider a generating model

(PX, p) deﬁned over X and Y, and consider two random responses Y1 and Y2 drawn from this model.

Then the square root agreement measure between Y1 and Y2 is deﬁned as

Γ(Y1, Y2) =
y∈Y

P (Y1 = Y2 = y) =
y∈Y

PX (x)py (x)2
x∈X

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
27

From the deﬁnition of SRAM, it is clear that under any symmetric strategy proﬁle in which every agent adopts a reporting strategy q, the expected payoﬀ to each agent in the limit as N → ∞ is K times the SRAM between the reported responses (see Equation 17). Some important properties of the SRAM are presented in Appendix B.
Next, we deﬁne a new notion of uninformativeness of a reporting strategy. Informally, a reporting strategy is more uninformative if it frequently maps multiple true responses to a single reported response, the extreme case being when a report is chosen independently of the true response. The following deﬁnition formalizes this notion.
Definition 7 (An uninformativeness measure). The uninformativeness of a reporting strategy q is deﬁned as

Ω(q) = 1 |Y|(|Y| − 1) y∈Y y′∈Y, y′′∈Y; y′=y′′

qy (y ′ )qy (y ′′ ).

(18)

We say that a strategy q is ω−uninformative if Ω(q) ≥ ω. Certain important properties of the uninformativeness measure are presented in Appendix C. In particular, Ω(q) = 0 if and only if (q(y); y ∈ Y) have disjoint supports across all y ∈ Y, i.e., if and only if q is fully informative, and Ω(q) attains its highest value of 1, if and only if q(y) = q(y′) for any y = y′, i.e., if the report is chosen independently of the true answer.
We ﬁnally present the following information monotonicity property, which is key to our results.

Proposition 5.1 (A monotonicity property) Consider a generating model (PX, p) deﬁned over

X and Y, and consider two random responses Y1 and Y2 drawn from this model. Also, consider two

random responses Z1 and Z2 obtained by applying a reporting strategy q independently to Y1 and Y2

respectively. Then,

δ(PX, p)Ω(q)2(|Y| − 1)

Γ(Z1, Z2) ≤ Γ(Y1, Y2) −

.

(19)

2 |Y|

To see how this property helps us, recall from Equations 16 and 17 that the expected payoﬀ under

any symmetric fully informative strategy proﬁle is KΓ(Y1, Y2), and that under any other symmetric

strategy proﬁle q is KΓ(Z1, Z2), where Z1 and Z2 is obtained by applying a reporting strategy q

independently to Y1 and Y2. The proposition implies that if δ(PX, p) > 0, then Γ(Y1, Y2) = Γ(Z1, Z2)

only if Ω(q) = 0, i.e., only if q is fully informative. Thus we conclude that if δ(PX, p) > 0, then in

the limit as N → ∞, any fully informative strategy proﬁle gives a strictly higher payoﬀ than any

other symmetric strategy proﬁle that is not fully informative. In other words, SRA is asymptotically

strongly truthful across symmetric equilibria. In the next section, we use Proposition 5.1 to address

the case where N is large but ﬁnite.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
28
5.2. Equilibrium selection in the ﬁnite N regime Now we turn to the ﬁnite N setting. In this case, the expected payoﬀs under the fully informative strategy and any other symmetric strategy will not have converged to Γ(Y1, Y2) and Γ(Z1, Z2) respectively. But for any ﬁxed N , one can obtain concentration bounds on how far the expected payoﬀs will be from these target values. This, in turn, gives us vanishing bounds on how much lower the payoﬀ under the truthful equilibrium could be compared to any other symmetric equilibrium.
Theorem 2. Consider an α-separated generating model (PX, p) that is commonly known to the agents. Further, suppose that Nj ≤ n for all j ∈ M and |Mi| ≥ 2 for all i ∈ N. Then for any ω > 0, there is a positive integer N0 that depends on α, ω, n, K, and |Y|, such that for any N > N0, under SRA,
1. Any symmetric fully informative strategy proﬁle is a strict Bayes-Nash equilibrium, and, 2. Any other symmetric Bayes-Nash equilibrium strategy proﬁle gives an expected payoﬀ at most
ω higher than any symmetric fully informative strategy proﬁle.
Note once again that the bounds we derive in the proof of this result can be used to explicitly calculate an N0 as a function of α, ω, n, K, and |Y|. Also, note that similar to Theorem 1, although Theorem 2 assumes that the generating model is commonly known to the agents, the result holds for any α-separated generating model (since the dependence of N0 on (PX, p) is only through α). Hence, we conclude that these properties presented in Theorem 2 hold even in the game where it is only commonly known to the agents that the generating model is α-separated.
Finally, note that in the statement of Theorem 2, N > N0 suﬃces to ensure that the truthful equilibrium gives a payoﬀ at most ω lower than that under any symmetric equilibrium strategy proﬁle. This is signiﬁcant since, for a large but ﬁxed N , it is not possible to obtain uniformly vanishing concentration bounds on E(ej(y)) (which involves an inverse) across all symmetric reporting strategies. This is because there could be a symmetric strategy proﬁle for which the probability of agreement for an answer y ∈ Y gets arbitrarily close to 0. To overcome this issue, we utilize the fact that under a mixed equilibrium, since the problem of computing the best response is a linear optimization problem, a ﬁxed agent is indiﬀerent between multiple deterministic reporting strategies. This allows us to choose a best-response strategy for a single agent in a way that ensures that the probability of agreement on any answer y is bounded away from zero while ensuring that the expected payoﬀ is same as that under the given symmetric equilibrium.
Can we say anything about the informativeness of the symmetric equilibrium proﬁle that gives the highest expected payoﬀ across all symmetric equilibria? Intuitively, bounds on E(ej(y)) for a large N , coupled with the “inequality gap” characterized in Proposition 5.1 should result in an upper bound on the uninformativeness of any symmetric reporting strategy that gives a higher expected

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
29
payoﬀ than a fully informative strategy. It turns out that in doing so, the same diﬃculty that we described earlier arises in obtaining the requisite concentration bounds, where the symmetric strategy proﬁle could lead to probabilities of agreement arbitrarily close to 0. In this case, we cannot use the trick we used earlier, and instead, we show the following result.
Theorem 3. Consider an α-separated generating model (PX, p). Further, suppose that Nj ≤ n for all j ∈ M and |Mi| ≥ 2 for all i ∈ N. Then for any ω > 0 and η > 0, there is a positive integer N0 that depends on ω, α, η, n, K, and |Y|, such that for any N > N0, under SRA, any symmetric strategy proﬁle in which the probability of reporting any answer y ∈ Y is either 0 or at least η, and that gives a higher expected payoﬀ to an agent than the truthful strategy proﬁle, is at most ω−uninformative.
Remark 2. Theorem 3 implies that for a large enough N , the truthful equilibrium yields a strictly higher payoﬀ to each agent than any equilibrium where all agents report a ﬁxed answer irrespective of the observation. To see this, note that the latter strategy proﬁle is 1-uninformative (see properties of the uninformativeness measure presented in Appendix C); hence, we can choose ω ∈ (0, 1). Moreover, η can be chosen to be any number in (0, 1) since mapping all observations to a single response implies that the probability of any response is either 0 or 1. Choosing such η and ω, we see that the above property will hold for any N larger than the corresponding N0. This property is in contrast to the multi-task extension of the peer-prediction method (Miller et al. 2005) and the mechanism of Radanovic and Faltings (2015), in which each agent reporting a ﬁxed answer is an equilibrium that yields the highest possible payoﬀ. A similar argument shows that for a large enough N , the truthful equilibrium yields a strictly higher payoﬀ to each agent than any equilibrium where all agents map smaller sets of responses to a single response, e.g., if the responses are {a, b, c, d} then {a, b} map to b and {c, d} map to d. This contrasts with the informed truthfulness property of CA, under which the truthful equilibrium doesn’t necessarily strictly payoﬀ-dominate equilibria of this type (particularly in scenarios where the observations are clustered; see Deﬁnition 10 in the Appendix).
5.3. Strong truthfulness vs. Strong truthfulness across symmetric equilibria In this section, we show that the property of strong truthfulness across symmetric equilibria of SRA can be strengthened to strong truthfulness (across all equilibria) under certain assumptions commonly made in the peer-prediction literature. To do so, we ﬁrst discuss what kind of asymmetric equilibria may arise under SRA that yield a higher reward to the agents than the truthful equilibrium.
Example 4. Consider the labor platform setting considered in Example 1. In this example, the observation that is expected to be least frequently agreed upon is “No” (i.e., the plumber did not arrive within 5 mins of his/her appointed time). Let John be a plumber operating on this platform, and

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
30

consider the strategy proﬁle where the customers who evaluate John always report “No” for him, while all customers report truthfully for every other plumber. This is an asymmetric strategy proﬁle: the agents who evaluate John follow a diﬀerent reporting strategy from those who don’t. In the limit where the number of plumbers on the platform is large, under the assumptions of Theorem 1, we claim that this strategy proﬁle constitutes an equilibrium. The key idea is that the popularity indices are not expected to be impacted by John’s reports in this limit. Hence, assuming everyone else adheres to this strategy proﬁle, being truthful is optimal on all tasks other than evaluating John. Moreover, for John’s evaluation, reporting “No” is optimal since it is the only way to obtain a positive payment. Thus this strategy proﬁle constitutes an equilibrium. Moreover, this equilibrium gives a strictly higher reward to agents who have evaluated John than the truthful equilibrium, since the answer “No” is expected to have the lowest popularity index and hence the highest reward for agreement.
The ability to construct such equilibria relies on agents coordinating their behavior on a task or a small subset of tasks. Our model assumes that task-identifying information is available and the task allocation is exogenously speciﬁed, so we cannot preclude this possibility. However, when tasks are randomly allocated to the agents and agents do not make their reporting strategy contingent on the task identity, such equilibria are not expected to arise. Formally, consider the following assumptions.

Assumption 1 (Randomized Task Allocation). Suppose that there are N tasks and M agents. Each task is to be performed by m agents, where we assume that m ≥ 2. Also, suppose that no agent should perform more than n tasks on average. To ensure that this is feasible, we consider a system regime in which N and M simultaneously grow such that M > mN/n. For each task, m distinct agents are uniformly sampled and assigned to that task. Note that each agent gets picked for a task with probability m/M and thus performs mN/M tasks on average, which is less than n.

Note that the average number of tasks performed by an agent, n, can be chosen to be as small as required (at the cost of requiring a large M ) to ensure that each agent doesn’t perform more than one evaluation with a high probability.

Assumption 2 (Task-independent strategies). Assume that each agent j picks a reporting strategy qj before the allocation of evaluation tasks, with the assumption that the same reporting strategy will be applied to every task that the agent performs.

Such assumptions are often justiﬁable in crowdsourcing applications and hence are commonly

made in the multi-task peer-prediction literature as we discussed in Section 2. We can argue that

under such assumptions, the fact that SRA is asymptotically strongly truthful across symmetric

equilibria implies that it is, in fact, asymptotically strongly truthful. To state our result, we need

the notion of the population average reporting strategy given a strategy proﬁle, deﬁned as q¯(y) =

1 M

j∈M qj(y) for all y ∈ Y. We have the following result.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
31
Theorem 4. Consider an α-separated generating model (PX, p) that is commonly known to the agents. Suppose that Assumptions 1 and 2 are satisﬁed. Then for any ω > 0 and η > 0, there is a positive integer N0 that depends on α, η, ω, m, n, K, and |Y|, such that for any N > N0, under SRA, the following holds.
1. Any symmetric fully informative strategy proﬁle is a strict Bayes-Nash equilibrium. 2. For any Bayes-Nash equilibrium strategy proﬁle such that probability of reporting any answer
y ∈ Y is either 0 or at least η under the population average reporting strategy, the expected payoﬀ of any agent is at most ω higher than that under any symmetric fully informative strategy proﬁle. 3. Consider any Bayes-Nash equilibrium strategy proﬁle such that probability of reporting any answer y ∈ Y is either 0 or at least η under the population average reporting strategy, and that yields a weakly higher expected payoﬀ to any agent than that under any symmetric fully informative strategy proﬁle. Then the population average reporting strategy under this strategy proﬁle is at most ω-uninformative.
Note that for any ω > 0, we are not able to achieve ω-domination of the fully informative equilibrium over all equilibria for some ﬁnite but large enough N , but only those equilibria in which the population average reporting probabilities are either 0 or bounded away from 0. This is because if the average reporting probability for an answer across the population becomes arbitrarily small in the number of tasks N as N grows, then it is not possible to obtain decaying concentration bounds on the agreement rewards; refer to the similar discussion in the context of Theorem 3 above. However, for any ﬁxed ω > 0, in the limit as N → ∞, all equilibria are ω-dominated by the truthful equilibrium. Moreover, for any ﬁxed ω > 0, in the limit as N → ∞, any equilibrium that yields a weakly higher payoﬀ than the truthful equilibrium is at most ω-uninformative. Since ω is arbitrary, this shows that the mechanism is asymptotically strongly truthful.9
Informally, the idea of the proof of Theorem 4 is the following. The random allocation of tasks, as well as non-contingency of reporting strategies on task identities, imply that from the perspective of each agent, no task is special, i.e., they are expected to be paired with a generic peer from the population of agents irrespective of the identity of the task. The assumption that the number of tasks performed by each agent is bounded on average implies that one agent’s reports are expected to have a vanishing impact on the population indices. Because of these assumptions, from the perspective of each agent, the remainder of the population with an asymmetric strategy proﬁle can be replaced by a population where each agent utilizes the population average reporting strategy q¯,
9 We remark that agents are expected to use simple strategies in practice, and thus strategies in which the reporting probabilities of the answers become arbitrarily close to zero are unlikely to arise in ﬁnite but large N settings under an α-separated generating model.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
32
while (approximately) preserving the payoﬀ structure of the game. Additionally, the fact that each of the diﬀerent strategies utilized by the agents is near-optimal against this average reporting strategy of the population implies that the average strategy q¯ is also near-optimal against q¯, because the payoﬀ of an agent is linear in her reporting strategy. Hence, the agents’ payoﬀ is approximately the same as one in the strategy proﬁle where everyone follows q¯, which constitutes a symmetric strategy proﬁle (with the approximation error in each of the above arguments vanishing in the large tasks limit). The result then essentially follows from the fact that SRA is asymptotically strongly truthful across symmetric equilibria.
We argue that Assumptions 1 and 2 are generally not appropriate in the context of feedback elicitation on online platforms. In this context, tasks are not randomly allocated but are chosen by the agents themselves. Also, entities being evaluated have a multitude of extraneous identifying features, e.g., the name of a plumber, the race or ethnicity of the Airbnb property owner, etc. Given this information, one cannot preclude the possibility of the agents choosing their reporting strategy based on such extraneous features and achieving coordination in their reports, potentially achieving higher payoﬀs compared to truthful reporting of their observations.
However, such extraneous coordination could be more or less likely depending on the context. For instance, in the example above, the possibility that agents coordinate their behavior with respect to this one plumber, John, out of potentially hundreds seems unlikely. However, such coordination may not be unreasonable in reviewing a popular neighborhood restaurant amongst people residing in that neighborhood.
To ensure that practitioners concerned with our target applications are not misled by SRA’s properties reported in our work, we choose not to make restricting assumptions that disallow such coordination and content ourselves with the weaker property of strong truthfulness across symmetric equilibria. As we argued in Section 2, in the absence of such assumptions, the dominance properties of other mechanisms are also restricted.
6. Numerical Evaluation
In this section, we numerically evaluate the performance of SRA, both, in the case of objective evaluations using synthetic data (Section 6.1) and in the case of subjective evaluations using real data from online platforms (Section 6.2). In the latter case, we also compare SRA’s performance to other related mechanisms.
6.1. Performance on objective evaluations In this section, we test SRA’s robustness in inducing truthful behavior in the ﬁnite N regime, in settings where population homogeneity may not necessarily hold exactly for objective evaluations due to observation or reporting biases. To do so, we assume the perspective of a single agent

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
33

operating in a platform environment where SRA is deployed, and examine her incentives for truthful reporting given her beliefs about the other agents’ generating model of the observations and their reporting behavior. This exercise also serves as a practical alternative to arguing about whether the common knowledge assumptions that are required for our results hold in practice; we demonstrate numerically that each agent has strong practical incentives to be truthful under SRA if she believes that agents are largely unbiased and they report their observations faithfully. We begin by deﬁning our experimental setup and the performance measures that we consider.
Setting. We consider the setting of an online service platform such as Thumbtack,10 on which a large number of moving companies oﬀer local moving services. The platform seeks to collect information about how punctual the diﬀerent moving companies are in adhering to the committed time to start the move. Poor scheduling and information collection practices can result in large variability in the start and ﬁnish times of diﬀerent moves in a day, potentially leading to disgruntled customers.
In our simulation setup, we assume that a moving company’s delay in showing up is an exponentially distributed random variable with a certain mean. We assume that there are |X| = 5 types of moving companies, where

X = {1 (Mostly Punctual), 2 (Somewhat Punctual), 3 (Mostly Tardy), 4 (Tardy), 5 (Very Tardy)} .

Each of these types is associated with a mean for the distribution of delay. We denote these means as µ = {µ1, µ2, µ3, µ4, µ5} , where we assume that µ1 < µ2 < µ3 < µ4 < µ5. The distribution of these ﬁve types across the population is denoted as PX. The platform asks the customers the following question: “How long (in minutes) after the scheduled time of appointment did the movers show up?” We consider two settings that diﬀer in the possible answers to this question.
1. The number of answers is |Y| = 3 where, Y = {0 to 30, 30 to 60, 60 to ∞}. 2. The number of answers is |Y| = 5 where, Y = {0 to 15, 15 to 30, 30 to 45, 45 to 60, 60 to ∞}. Given the type of a mover x ∈ X, the delay is exponentially distributed with mean µx, and hence the conditional probability of making an observation y = l to u is given by:

p (x) = exp(− l ) − exp(− u ).

(20)

y

µx

µx

Instances. An instance is speciﬁed by the probability distribution of the types PX, and the mean delays associated with the types µ. We generate 10000 instances. In each instance, PX is determined by sampling 5 numbers independently and uniformly in [0, 1] and dividing them by their total to

10 Thumbtack is an online service platform that matches customers with local professionals; see https://www.thumbtack.com/

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
34

obtain a probability distribution. µ is obtained by sampling 5 numbers independently and uniformly in [0, 60] (hence the mean delay of a moving company can be at most 60 min) and sorting them in an increasing order to satisfy the requirement that that µ1 < µ2 < µ3 < µ4 < µ5. Once PX and µ are thus speciﬁed, the conditional distributions over observations, p(x) for each x ∈ X, get speciﬁed as well according to Equation 20, for both the settings of |Y| = 3 and |Y| = 5.
Remark 3. We note that for |Y| = 3, 9787 out of the 10000 instances thus generated failed to satisfy the self-predicting responses condition (see item 2 in Section 2.1) and 9995 instances failed to satisfy the categorical responses condition (see item 1 in Section 2.1). For |Y| = 5, none of the 10000 instances satisﬁed either of the two conditions. Hence, the PTSC mechanism (Radanovic et al. 2016) and the mechanism of Dasgupta and Ghosh (2013) are inapplicable with a high frequency in this setting. This ﬁnding continues to hold if PX is chosen to be more “regular.” We sampled 10000 instances in two additional settings when |Y| = 5: (a) one where PX(x) is decreasing in x, i.e., types with higher mean delay are more rare, and (b) PX(x) is decreasing in x, i.e., types with higher mean delay are more frequent. In both these settings, none of the instances satisﬁed the self-predicting responses or categorical responses conditions.
Remark 4. The CA-HR mechanism described in Section D.1 in the Appendix is informed truthful across symmetric equilibria in general. However, if the instance is such that the joint distribution of observations of a pair of agents for a common evaluation task is not “clustered,” as deﬁned in Deﬁnition 10 in the Appendix, then the mechanism is strongly truthful across symmetric equilibria for that instance. We ﬁnd that for |Y| = 3, 9995 out of the 10000 instances thus generated had clustered observations. For |Y| = 5, 9940 out of the 10000 instances had clustered observations. Thus, in this setting, with a very high frequency, SRA is the only applicable mechanism requiring one task per agent that is also strongly truthful across symmetric equilibria.
Agent beliefs. We focus on a single agent (a customer on the platform), whom we refer to as agent j, and examine her incentives for being truthful under SRA. Each instance that we deﬁne above represents a belief that agent j has about the generating model for the movers’ true delays. Additionally, we allow the agent to account for potential biases in the observation-making process of other agents. In particular, agent j believes that, while she can make perfect observations, other agents do not observe the delay perfectly, but rather observe a biased version. Owing to this, she believes that given a mover type x, the conditional distribution of observations made by a generic agent in the population is not p(x), but p′(x), which is (slightly) diﬀerent. We assume that

p′(x) = (1 − ǫ)p(x) + ǫq′(x),

(21)

where q′ represents agent j’s belief of the bias in the population, and ǫ represents the magnitude of the bias. In each of the 10000 instances, for both the settings of |Y| = 3 and |Y| = 5 answers, we

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
35
independently sample an associated bias q′(x) for each x ∈ X, again by generating 3 and 5 values uniformly in [0, 1] and dividing each by their sum to obtain a distribution. In our analysis, we will independently consider diﬀerent values of ǫ ∈ {0, 0.1, 0.2}, where ǫ = 0 is the case where agent j believes that everyone else makes perfect observations.
Performance measures. We deﬁne two performance measures that capture the relative attractiveness of non-truthful behavior compared to being truthful from the perspective of agent j, assuming every other agent truthfully reports her (potentially biased) observation. Before we deﬁne these measures, we ﬁrst compute the expected reward for agreement on each answer y ∈ Y under the mechanism from the perspective of agent j, i.e., compute E(ej(y)). For each instance, assuming K = 1 and denoting N − |Nj| = N ′, we denote r(y, N ′, ǫ) E(ej(y)) to be the expected payment that agent j receives if she and her peer both give a matching response y, for each y ∈ Y. We consider values of N ′ in the set {200, 400, 600, 800, 1000} and ǫ ∈ {0, 0.1, 0.2}. Note that r(y, N ′, ǫ) can be computed exactly given the generating model.11 Also note that under SRA, ej(y) is computed based on answers of agents other than j; hence, from the perspective of agent j, ej(y) incorporates the bias of the agents in making their observations. This bias is reﬂected in the computation of E(ej(y)) = r(y, N ′, ǫ).
Based on our calculation of r(y, N ′, ǫ), we next deﬁne three quantities that will be utilized to deﬁne our performance measures. In deﬁning all of these quantities, we assume that all agents other than j truthfully report their (potentially biased) observations.
1. Truthful reward. First, for each instance, we deﬁne the expected reward of agent j under truthful behavior:

truthful-reward(N ′, ǫ)

P (Yj = Yj′ = y)r(y, N ′, ǫ).

(22)

y∈Y

Note that in deﬁning this reward, we account for the fact that agent j believes that she makes perfect observations while her peer j′ is potentially biased. In particular, we have,

P (Yj′ = Yj = y) = PX (x)py(x)p′y(x),
x∈X
where p(x) is deﬁned in Equation 20 and p′(x) is deﬁned in Equation 21 (capturing the fact that the peer agent is biased).

11 In SRA, f¯j (y) is a discrete random variable taking N ′ + 1 possible values in the set {1/N ′, 2/N ′, · · · , (N ′ + 1)/N ′}. In particular, f¯j (y) = 1/N ′ + Z(y)/N ′, where Z(y) is a binomial random variable arising from N ′ trials, with probability of success equalling the probability of agreement on y (between two (potentially) biased agents). Thus the expectation of ej (y), which is also a discrete random variable and a function of f¯j (y), can be exactly computed.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
36

2. Optimal Reward. Next, we deﬁne the expected reward of agent j from the optimal reporting

strategy that maximizes her expected reward (which could potentially entail lying). In doing

so, we address an important consideration. Although the question simply asks for the interval

in which the true delay of the mover lies, agent j can actually observe the true delay. Thus

the optimal report must be determined conditioned not on the true answer, but the true delay

(from which the true answer can be determined). We assume that agent j can accurately

observe the delay to within a minute and she stops observing if the delay is larger than 180

min (i.e., 3 hours).12 Formally, we assume that the observations of the delay lie in the ﬁnite set

Y = {a to a + 1; for a ∈ {0, 1, · · · , 179}} ∪ {180 to ∞}. We denote the observed delay of agent

j by the random element Y j ∈ Y. As before, since the delay is exponentially distributed with

mean µx given the type x ∈ X, the conditional probability of observing the delay y = l to u ∈ Y

is given by:

l

u

p¯y(x) = exp(− µx ) − exp(− µx ). (23)

Accounting for this consideration, we ﬁnally deﬁne the expected reward from the optimal

reporting strategy as:

optimal-reward(N ′, ǫ)

P (Y j = y) max P (Yj′ = y′|Y j = y)r(y′, N ′, ǫ)

(24)

y′∈Y

y∈Y

= max P (Yj′ = y′, Y j = y)r(y′, N ′, ǫ). y′∈Y y∈Y

(25)

Here we have,
P (Yj′ = y′, Y j = y) = PX (x)p¯y(x)p′y′(x),
x∈X
where p′(x) is deﬁned in Equation 21 (capturing the fact that the peer agent is biased) and

p¯(x) is deﬁned in Equation 23 (capturing the fact that the agent observes the true delay in

the set Y ).

3. Eﬀortless reward. Finally, as a baseline, we deﬁne the reward obtained by agent j by choosing

an answer y ∈ Y uniformly at random without making any observation:

eﬀortless-reward(N ′, ǫ) |Y1| P (Yj′ = y)r(y, N ′, ǫ), (26) y∈Y

where we have,
P (Yj′ = y) = PX (x)p′y(x),
x∈X
where p′(x) is deﬁned in Equation 21.

12 This assumption is for simplicity of computation of the optimal reporting strategy as a function of the observed delay. Our ﬁndings are not expected to change signiﬁcantly if the optimal reports are computed conditioned on ﬁner feedback information.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
37

We ﬁnally deﬁne our two main performance measures.

1. Lying gain. The ﬁrst measure we deﬁne captures the percentage gain in expected reward of

agent j by reporting optimally rather than simply being truthful. It is deﬁned as, lying-gain(N ′, ǫ) optimal-reward(N ′, ǫ) − truthful-reward(N ′, ǫ) × 100%. (27) truthful-reward(N ′, ǫ)
Ideally, we would like this gain to be small.

2. Truthful coverage. Although the measure that we deﬁne above is a natural one to consider,

it provides at best a partial picture of the incentives generated by the mechanism. In particular,

one way of ensuring a small lying-gain is to simply add a very large ﬁxed reward to the reward

under the mechanism so that the denominator in Equation 27 becomes large. By scaling this

ﬁxed reward, one can ensure that the lying-gain is as small as one desires without changing

the incentive properties of the mechanism. By doing so, even mechanisms with poor incentives

for truthful behavior can result in small lying-gain. In other words, although the lying-gain

is invariant to multiplicative scaling of the rewards under the mechanism, it is not invariant

to additive shifts of the rewards. To address this concern, we deﬁne the following relative

performance measure, which is invariant to both additive shifts as well as multiplicative scaling

of the rewards.

truthful-coverage(N ′, ǫ)

truthful-reward(N ′, ǫ) − eﬀortless-reward(N ′, ǫ) optimal-reward(N ′, ǫ) − eﬀortless-reward(N ′, ǫ) × 100%.
(28)

This quantity measures the fraction of the incremental gain resulting from optimal reporting

as compared to reporting randomly, that can be attained by truthful reporting. This quantity

should ideally be large.

We note that due to the bias in the population, and given that agent j evaluates the incentives

for lying conditioned on the observed delay as opposed to the true answer, being truthful is not guaranteed to be optimal for agent j′ under SRA even in the N ′ → ∞ limit. Hence, it is expected that lying-gain(N ′, ǫ) > 0% and truthful-coverage(N ′, ǫ) < 100%.
We deﬁne avg-lying-gain(N ′, ǫ) to be the average across 10000 instances of the lying-gain(N ′, ǫ). Similarly, we deﬁne avg-truthful-coverage(N ′, ǫ) to be the average across 10000 instances of the truthful-coverage(N ′, ǫ). When the context is clear, for notational con-

venience, we will refer to these aggregate quantities as avg-lying-gain and avg-truthful-coverage

respectively.

Results. The aggregate performance measures and standard errors13 for the diﬀerent values of N ′ and ǫ are presented for the |Y| = 3 setting in Figure 1 and for the |Y| = 5 setting in Figure 2. We

note two main observations.

13 The standard errors of the avg-lying-gain and avg-truthful-coverage are the standard errors of these mean quantities, deﬁned as 1.96 times the empirical standard deviation of the lying-gains or truthful-coverages divided by the square root of the sample size (10000).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
38

3.5

100

ǫ=0

ǫ=0

3.0

ǫ = 0.1

ǫ = 0.1

ǫ = 0.2

95

ǫ = 0.2

2.5

2.0 90

avg-truthful-coverage

avg-lying-gain

1.5 85
1.0 80
0.5

0.0

200

400

600

800

1000

N′

75

200

400

600

800

1000

N′

Figure 1

(a) avg-lying-gain (|Y| = 3)

(b) avg-truthful-coverage (|Y| = 3)

The avg-lying-gains along with standard errors under SRA for N ′ ∈ {100, 200, 300, 400, 500} (X-axis) for

diﬀerent values of ǫ and for the |Y| = 3 setting.

avg-lying-gain

4.5

ǫ=0

4.0

ǫ = 0.1

ǫ = 0.2 3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0

200

400

600

800

1000

N′

avg-truthful-coverage

100

ǫ=0

ǫ = 0.1

95

ǫ = 0.2

90

85

80

75

70

200

400

600

800

1000

N′

Figure 2

(a) avg-lying-gain (|Y| = 5)

(b) avg-truthful-coverage (|Y| = 5)

The avg-lying-gains (Y-axis) along with standard errors under SRA for N ′ ∈ {100, 200, 300, 400, 500}

(X-axis) for diﬀerent values of ǫ, for the |Y| = 5 setting.

First, SRA displays reasonably good performance in incentivizing truthful behavior despite the limited number of tasks, even when agent j believes that the rest of the population is biased. For instance, for |Y| = 3 and N ′ = 1000, the avg-lying-gain is at most about 2.5% across all settings of biases. Correspondingly, for |Y| = 5 and N ′ = 1000, the avg-lying-gain is at most about 2.7%. Additionally, SRA displays reasonably good truthful-coverage, e.g., for |Y| = 3 and N ′ = 1000, truthful behavior attains about 86% of the maximal potential gain over random reporting on average. For |Y| = 5 and N ′ = 1000, truthful behavior attains about 84% of the maximal potential gain over random reporting on average.
Second, we note an interesting phenomenon in the |Y| = 5 setting: when agent j’s belief about the bias in the population increases, i.e., ǫ increases from 0 to 0.2, her incentive to lie seems to decrease under almost all performance measures, especially for N ′ ∈ {600, 800, 1000}. In particular, the lying-gain is smaller and truthful-coverage is larger in aggregate for ǫ = 0.1 as compared to ǫ = 0.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
39
This is quite unlike the |Y| = 3 setting, in which the incentive to lie increases with ǫ, as expected, under all measures.
This observation is surprising, given that population homogeneity is crucial to the properties of SRA. It can be explained in light of the fact that adding a small random bias typically increases the probability of rare answers in the population. Informally, adding a small observation noise makes the distribution of answers “better mixed,” since the reporting probability of answers with a low probability of occurrence improves simply because of the noise. This results in a higher inequality gap in the Cauchy-Schwarz inequality, as shown in Proposition 4.1. In other words, the biased generating model is better separated on average than the unbiased model. In turn, this fact results in faster convergence of the popularity indices of the answers to their stable values. In Figure 3, we show the average across the 10000 instances of the smallest probability of observation (in Y) made by a generic biased agent in the population, where we observe that this average indeed increases with ǫ when ǫ is small, before decreasing (the mixing eﬀect is highest around ǫ ≈ 0.4). Thus, in ﬁnite N ′ settings, the incentive to lie can potentially be higher when the population is assumed to be unbiased since the population indices are expected to be farther from their asymptotic stable values, as compared to the case where the population is expected to be mildly biased.

0.095

0.090

0.085

0.080

0.075

0.070

0.065

Average of miny∈Y P (Yj′ = y)

0.060 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

ǫ

Figure 3 For the |Y| = 5 setting, the average across 10000 instances of miny∈Y P (Yj′ = y) (Y axis) for some generic agent j′ in the population as ǫ (X axis), i.e., the magnitude of the bias in the population, increases.

However, as the bias in the population becomes large, i.e., ǫ increases, this eﬀect is overpowered by the increased incentive to lie, since the generating model of the peer agent’s observations starts to look starkly diﬀerent from agent j’s model, i.e., the response homogeneity assumption is violated to a higher degree. We indeed verify this to be the case. In Figure 4, we plot the various aggregate measures in the |Y| = 5 setting for ǫ = 1, i.e., when the distributions of observations conditioned on the type are completely uncorrelated for agent j and her randomly chosen peer agent. As expected, the incentive to lie is signiﬁcantly higher across all values of N ′ in this case compared to settings with smaller values of ǫ.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
40

avg-lying-gain 16

15

14

13

12

11

10

9

200

400

600

800

1000

N′

(a) |Y| = 5 and ǫ = 1 (aggregate lying-gains)

0.75

avg-truthful-coverage

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−1.25

200

400

600

800

1000

N′

(b) |Y| = 5 and ǫ = 1 (aggregate truthfulcoverages)

Figure 4 For |Y| = 5, the diﬀerent aggregate measures (Y-axis) along with standard errors under SRA for N ′ ∈ {100, 200, 300, 400, 500} (X-axis) in the fully biased setting, i.e., ǫ = 1 (the distributions of observations of agent j
and her randomly chosen peer are independent).

Overall, these observations suggest that the belief that there exists a mild observation bias in the population may in fact improve incentives for truthful behavior in ﬁnite N ′ settings, as long as this bias is small enough.
6.2. Performance on subjective evaluations As we have discussed in Section 2, strongly truthful or informed truthful mechanisms for eliciting subjective evaluations from heterogeneous agents require multiple evaluations from each agent. Moreover, these mechanisms require that each agent uses the same strategy for each evaluation. These constraints may hinder the practical applicability of these mechanisms in many platform environments. In the face of these drawbacks, mechanisms tailored to homogeneous response settings that require a single task per agent could be a practical alternative. In this section, we hence evaluate the performance of SRA, PTSC, and CA-HR for incentivizing truthful responses to subjective evaluations in real settings.
Datasets. We test these mechanisms using publicly available rating datasets from diﬀerent online platforms. In particular, we consider the following three datasets.
1. Goodreads. We consider book rating data from Goodreads, which is a popular book review platform.14 We restrict our attention to books belonging to two largest and similarly-sized categories: (a) romance and (b) fantasy and paranormal. We assume books in each of these categories to be a priori statistically similar and we test the performance of diﬀerent mechanisms for these two categories independently.
14 The data is publicly available at https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home. The source requires us to cite Wan and McAuley (2018) and Wan et al. (2019).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
41

2. Amazon. We next consider product rating data from the e-commerce platform, Amazon.15 We restrict ourselves to the “Clothing, Shoes, and Jewelery” (CSJ) category, which is by far the largest product category other than books.16
3. Netﬂix. We ﬁnally consider movie rating data from the streaming platform, Netﬂix, that was released as part of the Netﬂix Prize challenge.17
In all of the above cases, the ratings are integers on a scale from 1 to 5. Moreover, the ratings are expected to have a strongly subjective inﬂuence, especially so in the case of books and movies. Table 2 provides some basic information about these datasets.

Table 2

Properties of datasets. The rating strength represents the highest lower bound on the number of ratings given by the top 1000 high-contributing users.

Goodreads: romance Goodreads: fantasy/paranormal Amazon: CSJ Netﬂix

No. of entities
334957 books 258212 books 2681297 products 17770 movies

No. of users
198141 256088 12483678 480189

No. of ratings
3565378 3424641 32292099 100480507

Rating strength
472 278 104 2087

Testing procedure. In each of the above cases, we focus on the top 1000 users who have rated the most number of entities (books, movies, or products). Assuming that all ratings in the dataset are truthful, we estimate the reporting behavior of these users and investigate their incentives for lying under the various mechanisms. Formally, let H denote the set of high-contributing users and consider a user i ∈ H. Based on i’s ratings across the books they have rated and the ratings of randomly chosen peers for these books, we estimate the joint distribution of the rating of i and that of a randomly chosen peer agent for a random book that they rate. Let us denote this estimate as (Qi(y, y′))y,y′∈Y. Similarly, we estimate the joint distribution of the ratings of two randomly chosen agents for a random book by sampling two agents at random for each book in the data set and computing the empirical distribution of the resulting answers. Let us denote this estimate as Q(y, y′) y,y′∈Y. The estimates Qi and Q are expected to be diﬀerent, in line with the expectation that agent i’s responses are statistically diﬀerent from a randomly chosen agent in the population due to the subjectivity of responses.
Based on the estimates Qi and Q, we can estimate the truthful-coverage of each mechanism for each agent i ∈ H, assuming that (a) i’s belief about the joint distribution of her rating and that of
15 The data is publicly available at https://nijianmo.github.io/amazon/index.html. The source requires us to cite Ni et al. (2019). 16 The Goodreads data is richer than Amazon’s rating data for books. 17 The data is publicly available at https://www.kaggle.com/netflix-inc/netflix-prize-data. Information about the Netﬂix prize is available at https://en.wikipedia.org/wiki/Netflix_prize.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
42
a random peer for a random book is identical to Qi, and (b) her belief about the joint distribution of the ratings of two randomly chosen agents for a random book is identical to Q. Note that we focus on truthful-coverage since, as we discussed earlier, the lying-gain is not invariant to additive shifts in the rewards.

Results. The results are shown in Figure 5. First, we observe that SRA achieves an average truthful-coverage of at least 50% across all settings except for Amazon. In this latter case, the performance of all mechanisms is relatively poor. The reason for this may be that the deﬁnition of the CSJ category is quite broad, and signiﬁcant diversity is expected across the products in this category; hence, the assumption of the products being a priori statistically similar likely doesn’t hold.
Next, we observe that SRA outperforms PTSC in all settings except for the case of Netﬂix, where their performance is statistically similar. SRA outperforms CA-HR in the Goodreads setting for the romance category and in the case of Amazon, while their performance is statistically similar in the other two cases. To investigate the diﬀerence in SRA and PTSC, we consider the estimate Q of the joint distribution of two agents’ responses to a common evaluation. The incentive to lie for an agent i stems from two sources: (a) Qi may be diﬀerent from Q, and (b) Q may not satisfy the conditions necessary for inducing truthful behavior even when all agents are identical. We ﬁnd that in all settings, Q satisﬁes the Cauchy-Schwarz property required for SRA to be truthful (Equation 11), while the self-prediction property that is needed for PTSC to be truthful (see item 2 in Section 2.1) is not satisﬁed in any of the settings; see Table 3.
Moreover, we ﬁnd that the response distribution Q is ‘clustered’ in all settings, as deﬁned in Deﬁnition 10 (from Shnayder et al. (2016)) in the Appendix. This implies that under the CAHR mechanism, the agents can achieve the same payoﬀ by merging their responses. For example, considering the Q from the Netﬂix setting, we ﬁnd that agents need not distinguish between the ratings 4 and 5 under the CA-HR mechanism (see Table 3). This points to the importance of the distinction between strong and informed truthfulness in these settings.

Table 3 Properties of population average joint distribution of responses Q.

Goodreads: romance Goodreads: fantasy/paranormal Amazon: CSJ Netﬂix

Satisﬁes CS property
Yes Yes Yes Yes

Satisﬁes self-prediction
No No No No

Clustered ratings
(1, 2) (1, 2, 3)
(2, 3) (4, 5)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
43

avg-truthful-coverage

80

60

40

20

0 SRA

PTSC

CA-HR

(a) Goodreads: romance

avg-truthful-coverage

80

60

40

20

0 SRA

PTSC

CA-HR

(b) Goodreads: fantasy/paranormal

80

80

avg-truthful-coverage

avg-truthful-coverage

60

60

40

40

20

20

0 SRA

PTSC

CA-HR

0 SRA

PTSC

CA-HR

Figure 5

(c) Amazon: CSJ

(d) Netﬂix

The avg-truthful-coverages along with standard errors under the diﬀerent mechanisms in diﬀerent settings.

7. Discussion and Conclusion
In the paper, we focus on the practical setting of reputation systems in online platforms where objective evaluations must be strongly incentivized; ideally, without imposing any constraints on the number of evaluations performed by each agent. Our results show that SRA is the ﬁrst mechanism that achieves this goal.
While there are other mechanisms, such as those of Kong and Schoenebeck (2019), Kong (2020), or CA (Shnayder et al. 2016), that incentivize truthful behavior despite response heterogeneity across agents, these mechanisms incur a high operational cost of requiring multiple evaluations from each agent, which could be prohibitive in many scenarios, including in online platforms. On the other hand, our numerical evaluations show that the truthfulness property of SRA is robust to mild degrees of heterogeneity and subjectivity in the population. This observation overall suggests that SRA can be a simpler alternative to these more complex mechanisms in settings where response homogeneity is a reasonable approximation to the mild degree of heterogeneity and subjectivity expected in the evaluations. Eliciting objective evaluations in online platforms is one such setting that we focused on in the paper. Additionally, our tests on real data show that SRA generates strong incentives for truthful behavior even when evaluations are expected to be highly subjective.
At the same time, we acknowledge that there are settings where other mechanisms could be preferable over SRA. Moreover, metrics such as lying-gain or truthful-coverage may not adequately

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
44
inform the practical utility of mechanisms in these settings and other operational considerations may take precedence. For example, in applications such as crowdsourcing and peer-grading,18 agents typically perform several evaluations in a short span of time. Moreover, subjectivity in evaluations could be a major concern in settings like peer-grading for courses in the arts and the humanities. In this case, Kong (2020)’s mechanism would provide strong truthfulness guarantees without requiring the homogeneity assumption and hence could be preferable over SRA, even if, hypothetically, it turns out to be the case that SRA achieves better truthful-coverage or lying-gain than Kong’s mechanism in homogeneous settings with comparable data. As another example, if the responses can be validated to be self-predicting, then PTSC may be preferable owing to the simpler description of the agreement rewards.
Eﬀective feedback and reputation systems are fundamental to the eﬃcient functioning of online platforms. The impact of user feedback and peer-reviews on customer decisions is evident in the success of independent reputation systems like Yelp and TripAdvisor, which are used by millions of people across the world. But as has been recently shown, these systems are currently fraught with several operational, behavioral, and strategic concerns (Hu et al. 2017, Filippas et al. 2018, Nosko and Tadelis 2015). We believe that appropriate incentive mechanisms that are simple and intuitive can go a long way in addressing some of these concerns, and hence our mechanism has strong practical signiﬁcance. We emphasize here that rather than thinking of our mechanism as a fully speciﬁed solution in any setting, it is more useful to think of it as a framework that provides conceptual guidelines for platform designers as they undertake their design decisions.
Our work presents many avenues for future exploration. For instance, in our model, we assume that the task allocations are exogenously speciﬁed. But for a platform that is interested in learning the underlying distributions of responses for each task, some of these distributions may be more diﬃcult to learn than others, and thus may need more evaluations. Moreover, the agents may be willing to strategically respond to diﬀerences in potential rewards across tasks by choosing which tasks to evaluate. It is important to understand the fundamental tradeoﬀs faced by dynamic mechanisms that balance incentives with diﬀerent statistical accuracy objectives in such situations. We are optimistic that our framework and insights can be used as building blocks in this pursuit.

References
Aldous, D. J. (1985). Exchangeability and related topics. XIII–1983, pages 1–198. Springer.

In École d’Été de Probabilités de Saint-Flour

18 Peer-grading is the idea of having students grade each others’ assignments and examinations. This idea is key in obtaining a scalable solution to the problem of grading in massive open online courses (MOOC). Incentivizing students to grade truthfully is an important concern in such settings.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
45
Brier, G. W. (1950). Veriﬁcation of forecasts expressed in terms of probability. Monthly weather review, 78(1):1–3.
Cover, T. and Thomas, J. (2012). Elements of Information Theory. Wiley.
Dasgupta, A. and Ghosh, A. (2013). Crowdsourced judgement elicitation with endogenous proﬁciency. In Proceedings of the 22nd International Conference on World Wide Web, pages 319–330.
Faltings, B., Jurca, R., and Radanovic, G. (2017). Peer truth serum: incentives for crowdsourcing measurements and opinions. arXiv preprint arXiv:1704.05269.
Filippas, A., Horton, J. J., and Golden, J. (2018). Reputation inﬂation. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 483–484.
Gao, X. A., Wright, J. R., and Leyton-Brown, K. (2019). Incentivizing evaluation with peer prediction and limited access to ground truth. Artiﬁcial Intelligence, 275:618–638.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359–378.
Harsanyi, J. C., Selten, R., et al. (1988). A general theory of equilibrium selection in games. MIT Press Books, 1.
Hu, N., Pavlou, P. A., and Zhang, J. J. (2017). On self-selection biases in online product reviews. MIS Q., 41(2):449–471.
Jøsang, A., Ismail, R., and Boyd, C. (2007). A survey of trust and reputation systems for online service provision. Decision support systems, 43(2):618–644.
Jurca, R. and Faltings, B. (2005). Enforcing truthful strategies in incentive compatible reputation mechanisms. In International Workshop on Internet and Network Economics, pages 268–277. Springer.
Jurca, R. and Faltings, B. (2008). Incentives for expressing opinions in online polls. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 119–128.
Jurca, R. and Faltings, B. (2011). Incentives for answering hypothetical questions. In Workshop on Social Computing and User Generated Content, EC-11.
Kong, Y. (2020). Dominantly truthful multi-task peer prediction with a constant number of tasks. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2398– 2411. SIAM.
Kong, Y. and Schoenebeck, G. (2019). An information theoretic framework for designing information elicitation mechanisms that reward truth-telling. ACM Transactions on Economics and Computation (TEAC), 7(1):1–33.
Li, L., Tadelis, S., and Zhou, X. (2020). Buying reputation as a signal of quality: Evidence from an online marketplace. The RAND Journal of Economics, 51(4):965–988.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
46
Liu, Y. and Chen, Y. (2016). Learning to incentivize: eliciting eﬀort via output agreement. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence, pages 3782–3788.
Luca, M. (2017). Designing online marketplaces: Trust and reputation mechanisms. Innovation Policy and the Economy, 17(1):77–93.
Miller, N., Resnick, P., and Zeckhauser, R. (2005). Eliciting informative feedback: The peer-prediction method. Management Science, 51(9):1359–1373.
Myerson, R. B. (2013). Game theory. Harvard University Press.
Ni, J., Li, J., and McAuley, J. (2019). Justifying recommendations using distantly-labeled reviews and ﬁnegrained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 188–197.
Nosko, C. and Tadelis, S. (2015). The limits of reputation in platform markets: An empirical analysis and ﬁeld experiment. Technical report, National Bureau of Economic Research.
Prelec, D. (2004). A Bayesian truth serum for subjective data. Science, 306(5695):462–466. Radanovic, G. and Faltings, B. (2013). A robust bayesian truth serum for non-binary signals. In Proceedings
of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence, pages 833–839. Radanovic, G. and Faltings, B. (2015). Incentives for subjective evaluations with private beliefs. In Proceed-
ings of the 29th AAAI Conference on Artiﬁcial Intelligence (AAAI’15).
Radanovic, G., Faltings, B., and Jurca, R. (2016). Incentives for eﬀort in crowdsourcing using the peer truth serum. ACM Transactions on Intelligent Systems and Technology (TIST), 7(4):48.
Resnick, P., Kuwabara, K., Zeckhauser, R., and Friedman, E. (2000). Reputation systems. Communications of the ACM, 43(12):45–48.
Savage, L. J. (1971). Elicitation of personal probabilities and expectations. Journal of the American Statistical Association, 66(336):783–801.
Schoenebeck, G. and Yu, F.-Y. (2020). Two strongly truthful mechanisms for three heterogeneous agents answering one question. In International Conference on Web and Internet Economics, pages 119–132. Springer.
Schoenebeck, G. and Yu, F.-Y. (2021). Learning and strongly truthful multi-task peer prediction: A variational approach. In 12th Innovations in Theoretical Computer Science Conference (ITCS 2021).
Shnayder, V., Agarwal, A., Frongillo, R., and Parkes, D. C. (2016). Informed truthfulness in multi-task peer prediction. In Proceedings of the 2016 ACM Conference on Economics and Computation, pages 179–196. ACM.
Tadelis, S. (2016). Reputation and feedback systems in online platform markets. Annual Review of Economics, 8:321–340.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
47
Van Damme, E. (2002). Strategic equilibrium. Handbook of Game Theory with Economic Applications, 3:1521–1596.
Von Ahn, L. and Dabbish, L. (2004). Labeling images with a computer game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 319–326. ACM.
Von Ahn, L. and Dabbish, L. (2008). Designing games with a purpose. Communications of the ACM, 51(8):58–67.
Wan, M. and McAuley, J. (2018). Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 86–94.
Wan, M., Misra, R., Nakashole, N., and McAuley, J. (2019). Fine-grained spoiler detection from largescale review corpora. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2605–2610.
Witkowski, J. and Parkes, D. C. (2012). A robust bayesian truth serum for small populations. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence, pages 1492–1498.
Witkowski, J. and Parkes, D. C. (2013). Learning the prior in minimal peer prediction. In 3rd Workshop on Social Computing and User Generated Content at the ACM Conference on Electronic Commerce.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
48

Appendix
A. Proofs Proof of Proposition 4.1 Let m = miny∈Y v(y) . To see that 2 implies 1, note that θ(v(y), v(y′)) ≥ γ for
all y, y′ ∈ Y such that y = y′, implies that m > 0 and that v(y).v(y′) ≤ cos γ, v(y) v(y′)
Multiplying throughout by v(y) v(y′) , we have:

v(y) v(y′) − v(y).v(y′) ≥ (1 − cos γ) v(y) v(y′) ≥ (1 − cos γ)m2 > 0.

To show that 1 implies 2 is less straightforward and this is where we need to use the fact that v(y) ≤ 1 for all y ∈ Y. First of all
|v(y).v(y′)| ≤ v(y) v(y′) − α,

implies that both v(y) and v(y′) are non-zero. Then dividing on both sides, we get:

|v(y ).v(y ′ )| v(y′) v(y)

≤1− α v(y′) v(y)
≤1−α

where the last inequality holds since v(y) ≤ 1 for all y ∈ Y. In other words:

cos θ(v(y), v(y′)) ≤ 1 − α,

This implies that θ(v(y), v(y′))) ≥ arccos(1 − α). Note that α > 0 so that arccos(1 − α) > 0.

Proof of Theorem 1 First, note that the payments ej(y) for the diﬀerent y ∈ Y are independent of the

reports of agent j for any reporting strategy. This is because {ej(y) : y ∈ Y} are computed only based on

evaluation tasks that j does not perform. Next, suppose that everyone but agent j is truthful. Recalling the

deﬁnition of v(y)

PX (x)py(x); x ∈ X , we have,

E(f¯j(y) − 1 ) = E N − |Nj|

N −1|N | 1 1 j i∈N\Nj {rji1(i′)=y} {rji2(i′)=y}

= PX (x)py(x)2 =
x∈X

v(y) 2

g(y).

In the proof of Proposition 4.1, we have seen that δ(PX, p) > α implies that v(y) > α, and thus we have g(y) > α2 > 0 for all y ∈ Y. Next, recall that

ej(y) = Let N ′ = N − |Nj|. Then we have for any ǫ ∈ (0, 1):

K f¯ (y) .
j

E(ej(y)) ≥ P (f¯j(y) − 1/N ′ ∈ [g(y)(1 − ǫ), g(y)(1 + ǫ)])

K

g(y)(1 + ǫ) + 1/N ′

(a)
≥ (1 − 2 exp(−ǫ2g(y)2N ′))

K g(y)(1 + ǫ) + 1/N ′

≥ (1 − 2 exp(−ǫ2α4N ′))

K g(y)(1 + ǫ) + 1/N ′

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
49

≥

K

− 2 exp(−ǫ2α4N ′) K

g(y)(1 + ǫ) + 1/N ′

α (1 + ǫ)

≥

K

− 2 exp(−ǫ2α4N ′) K

g(y)(1 + ǫ + 1/(g(y)N ′)

α (1 + ǫ)

≥

K

− 2 exp(−ǫ2α4N ′) K

g(y)(1 + ǫ + 1/(α2N ′)

α (1 + ǫ)

(b)
≥

K

(1 − ǫ − 1/(α2N ′)) − 2 exp(−ǫ2α4N ′) K

g(y)

α

(for large enough N ′)

≥ K (1 − ǫ − 1/(α2(N − n))) − 2 exp(−ǫ2α4(N − n)) K

g(y)

α

(for large enough N ). (29)

Here

(a)

follows

from

Hoeﬀding’s

inequality,

and

(b)

is

because

√1 1+a

≥1−a

for

every

a ∈ (0, 1).

The

other

inequalities result from the fact that g(s) ≥ α2 and |Nj| ≤ n. Taking ǫ = (N − n)−1/4, we obtain:

E(ej(y)) ≥

K − o(N ). g(s)

Next, we also have,

E(ej(y)) ≤ P (f¯j(y) − 1/N ′ ∈ [g(y)(1 − ǫ), g(y)(1 + ǫ)]) K g(y)(1 − ǫ)

+ E 1{f¯j (y)−1/N′∈/[g(y)(1−ǫ),g(y)(1+ǫ)]}

K f¯j (y )

(a)

K

√

≤

+ P 1f¯j (y)−1/N′∈/[g(y)(1−ǫ),g(y)(1+ǫ)] K N ′

g(y)(1 − ǫ)

(b)
≤

K

√ + 2K N ′ exp(−ǫ2g(y)2N ′)

g(y)(1 − ǫ)

≤

K

+ 2K√N ′ exp(−ǫ2α4N ′)

g(y)(1 − ǫ)

(c)
≤

K

(1

+

ǫ

+

w(ǫ))

+

√ 2K N ′

exp(−ǫ2α4N ′)

g(y) 2

≤

K

+

ǫK

+

|w(ǫ)|K

√ + 2K N ′

exp(−ǫ2α4N ′)

g(y) 2α

α

≤

K

+

ǫK

+

|w(ǫ)|K

+

√ 2K N

exp(−ǫ2α4(N

−

n)).

(30)

g(y) 2α

α

Here, (a) results from the fact that f¯j(y) ≥ 1/N ′ (because of the smoothing). (b) follows from Hoeﬀding’s √
inequality, and (c) follows from the Taylor approximation of the function 1/ 1 − ǫ, where w(ǫ) = o(ǫ). Now

choosing ǫ = (N − n)−1/4, we get:

E(ej(y)) ≤

K + o(N ). g(y)

Thus, we ﬁnally have |E(ej(y)) − √K | ≤ σ(N ) = o(N ), where σ(N ) ≥ 0 is a function of N that depends g(y)
only on α, n and K and not on y (note that our bounds explicitly deﬁne this function: we have w(ǫ) < ǫ/2

for ǫ < 1/2 and thus w(ǫ) can be replaced by ǫ/2 for N − n ≥ 24 = 16)).

Assuming everyone else is truthful, the expected reward of person j for evaluating object i if she chooses

a reporting strategy qij is,

R(qij )

P (Yji′ = y, Yji = y)E(rj (y)) = E(rj (y)) PX (x)py(x) py′ (x)qyij (y′).

y∈Y

y∈Y

x∈X

y′ ∈Y

(31)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
50

Thus the agent solves maxqij R(qij). The objective is linear in qij, and further, qij(y) lies on a unit simplex for each y ∈ Y. Thus the optimal reporting strategy chooses qij(y) to be one of the extreme points of the simplex for each y ∈ Y, i.e., the optimal reporting strategy is deterministic. Now let t be the truthful strategy, i.e., ty′(y) = 1{y=y′}. Then for any deterministic reporting strategy qij , we have,

R(qij ) = E(ej(y)) qyij(y′) PX (x)py(x)py′ (x)

y∈Y

y′∈Y

x∈X

(a)
≤ E(ej(y)) qyij(y′)

y∈Y

y′∈Y

PX (x)py (x)2
x∈X

PX (x)py′ (x)2 − α1{y=y′}
x∈X

≤(
y∈Y

K + σ(N )) g(y)

qyij (y′)

y′∈Y

g(y)g(y′) − α1{y=y′}

≤K
y′∈Y

g(y′) − αK

1{y=y′}qyij (y′) + σ(N )

qyij (y′)

y′∈Y y∈Y

y∈Y y′∈Y

g(y)g(y′)

(b)
≤K
y′∈Y

g(y′) − αK1{qij=t} + |Y|σ(N ).

(32) (33)

Here, (a) follows from the Cauchy-Schwarz inequality, from the deﬁnition of δ(Px, p), and the fact that δ(Px, p) > α. (b) follows from the fact that qij is deterministic and so is t. While we have,

Thus we have,

R(t) = E(ej(y)) PX (x)py(x)2

y∈Y

x∈X

≥(
y∈Y

K − σ(N ))g(y) g(y)

≥

g(y) − |Y|σ(N ).

y∈Y

R(qij ) ≤ R(t) − αK1{qij=t} + 2|Y|σ(N )

Since σ(N ) depends only on δ and K and σ(N ) = o(1), there is an N1 that depends only on α, K, n and |Y| such that for all N > N1, 2|Y|σ(N ) < Kα, which means that truthful behavior is a strict Bayes-Nash equilibrium. To prove the second statement, suppose that qij is a strategy in which reports are chosen independently of the true answers. Denote qyij qyij(y′) since qyij(y′) = qyij(y′′) for all y, y′, y′′ ∈ Y. Then in (32),

αK

1{y=y′}qyij (y′) = αK

1{y =y ′ } qyij

y′∈Y y∈Y

y′∈Y y∈Y

= αK(|Y| − 1).

And thus,

R(qij) ≤ R(t) − αK(|Y| − 1) + 2|Y|σ(N ).

Thus for any ω ∈ (0, αK(|Y| − 1)), there is a positive integer N2 depending on ω, α, K, n and |Y| such that for any N > N2, R(qij) ≤ R(t) − ω. Choosing N0 = max(N1, N2) proves the result.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
51

Proof of Proposition 5.1 We have,

Γ(Z1, Z2) =
y∈Y

PX (x)py1 (x)py2 (x)qy(y1)qy(y2)
x∈X,y1∈Y,y2∈Y

=
y∈Y

qy(y1)qy(y2) PX (x)py1 (x)py2 (x)

y1 ∈Y,y2 ∈Y

x∈X

(a)
≤
y∈Y

qy (y1 )qy (y2 )
y1 ∈Y,y2 ∈Y

PX (x)py1 (x)2
x∈X

PX (x)py2 (x)2 − δ(PX , p)1{y1=y2}
x∈X

=
y∈Y

qy (y1 )
y1∈Y

PX (x)py1 (x)2
x∈X

2

− δ(PX , p)

qy (y1 )qy (y2 )1{y1 =y2 }

y1∈Y,y2∈Y

(b)

≤

qy (y1 )

y∈Y,y1∈Y

PX (x)py (x)2 − δ(PX , p)

1

2

x∈X

y∈Y

y′∈Y,y′′∈Y qy(y′)qy(y′′)1{y′=y′′}

y1∈Y qy(y1)

x∈X PX (x)py1 (x)2

(c)
≤

Γ(Y1,

Y2)

−

δ(PX ,

p)

2 y∈Y

y′∈Y,y′′∈Y qy (y′)qy (y′′)1{y′=y′′} Γ(Y1, Y2)

(d)
≤

Γ(Y1,

Y2)

−

δ(PX ,

p)

qy (y ′ )qy (y ′′ )1{y′ =y′′ }

2 |Y| y∈Y y′∈Y,y′′∈Y

(e)

δ(PX , p)Ω(q)2(|Y| − 1)

≤ Γ(Y1, Y2) −

.

2 |Y|

Here, (a) follows from the Cauchy-Schwarz inequality and the deﬁnition of δ(PX , p). (b) follows from the fact that for a, b > 0 and a > b, √a − b ≤ √a − b/(2√a). (c) follows from the fact that qy(y1) ≤ 1 and from the

deﬁnition of Γ(Y1, Y2). (d) follows from the fact that Γ(Y1, Y2) ≤ |Y|. (e) holds since, by Jensen’s inequality,

Ω(q)2 =

|Y| |Y|2(|Y| − 1)
y∈Y y′∈Y,y′′∈Y

2
qy (y′)qy (y′′)1{y′=y′′}

≤1

qy(y′)qy (y′′)1{y′=y′′}

|Y| − 1 y∈Y y′∈Y,y′′∈Y

Proof of Theorem 2 The ﬁrst statement follows from Theorem 1: there is an N1 such that for all N ≥ N1, the truthful strategy proﬁle is a Bayes-Nash equilibrium. We focus on the second claim. With some abuse of notation, we denote etj(y) to be the agreement scores for an agent j under the truthful equilibrium, and esj(y) to be the scores under a ﬁxed symmetric equilibrium strategy proﬁle where each agent follows the reporting strategy q.
We have shown in the proof of Theorem 1 that if everyone is truthful, then |E(etj(y))− √Kg(y) | ≤ σ(N ) = o(1), where σ(N ) ≥ 0 is some function of N that depends only on α, n and K and not on y.
Let us denote x∈X PX (x)( y′∈Y py′ (x)qy (y′))2 s(y) and denote x∈X PX (x) y′∈Y py′ (x)qy (y′) b(y). By Jensen’s inequality, we have s(y) ≥ b(y)2. Then using arguments similar to the ones leading up to (30)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
52

in the proof of Theorem 1, we can show that for all y ∈ Y such that b(y) ≥ δ(PX , p)2/|Y| (and hence, s(y) ≥ δ(PX , p)4/|Y|2), and for any ǫ ∈ (0, 1),

E(e

s j

(y

))

−

K ≤ σ′(N ), s(y)

where |σ′(N )| = o(1), and it depends on α, K, n and |Y|. Consider the strategy q and consider a y ∈ Y, such that b(y) > 0 but b(y) < δ(PX, p)2/|Y|. Then one can construct another strategy q′ such that a) a ﬁxed agent j is indiﬀerent between choosing q and q′ assuming everyone else is playing q, and, 2) for all y such that b(y) < δ(PX , p)2/|Y|, qy′ (y′) = 0 for all y′ ∈ Y. To show this, observe that for each y′, q(y′) cannot have support only on those y for which b(y) < δ(PX , p)2/|Y|. This is because if that is the case then P (Yji = y′) = P (Yji = y′) y∈Y;b(y)<δ(PX,p)2/|Y| qy(y′) ≤ y∈Y;b(y)<δ(PX,p)2/|Y| b(y) < δ(PX , p)2, which contradicts the fact that P (Yji = y′) ≥ δ(PX , p)2 as we have seen in the proof of Proposition 4.1. So then deﬁne q′(y′) to have support only on the y ∈ Y for which b(y) ≥ δ(PX, p)2/|Y| by transferring the probability masses. If we

deﬁne G(q) to be the expected payment to a ﬁxed agent j for a ﬁxed task i under the symmetric equilibrium

under strategy q, and deﬁne G(q′, q−j) to be the expected payment to j if she plays q′ while others play q, then we have G(q) = G(q′, q−j). Let us deﬁne x∈X PX (x)( y′∈Y py′ (x)qy′ (y′))2 s′(y). Then we have,

G(q) = G(q′, q−j)

≤

E(esj (y)) PX (x)[ py1 (x)qy′ (y1)][ py2 (x)qy (y2)]

y∈Y; b(y)≥δ(PX,p)2/|Y|

x∈X

y1 ∈Y

y2 ∈Y

(a)

≤

E(esj(y)) s(y)s′(y)

y∈Y; b(y)≥δ(PX,p)2/|Y|

≤

(

y∈Y; b(y)≥δ(PX,p)2/|Y|

K + σ′(N )) s(y)

s(y)s′(y)

≤

K s′(y) + |Y|σ′(N )

y∈Y; b(y)≥δ(PX,p)2/|Y|

(=b) K

s′(y) + |Y|σ′(N ).

y∈Y

(34)

Here (a) follows from the Cauchy-Schwarz inequality and (b) follows from the fact that s′(y) = 0 for all y such that b(y) < δ(PX , p)2/|Y| by construction of the strategy q′. Let G(t) be the expected payment to agent j for task i under the truthful equilibrium. Let j′ be j’s peer for task i. Then we have,

G(t) = E(etj(y))g(y)
y∈Y

≥ K g(y) − σ(N )g(y)

y∈Y

y∈Y

≥ KΓ(Yji, Yji′ ) − |Y|σ(N )

(a)

≥K

s′(y) − |Y|σ(N ).

(35)

y∈Y

Here, (a) follows from Proposition 5.1. Finally, (35) and (34) together imply that, for a large enough N ,

G(t) ≥ G(q) − |Y|(σ(N ) + σ′(N )).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
53
Thus for any ω > 0, there exists some N2 such that for any N ≥ N2, the payoﬀ under the truthful equilibrium is less than that under any other symmetric strategy proﬁle by at most ω. Taking N0 = max(N1, N2) proves our claim.

Proof of Theorem 3 As before, we denote etj(y) to be the agreement scores for an agent j under a fully informative equilibrium, and esj(y) to be the scores under a ﬁxed symmetric strategy proﬁle where each agent follows the reporting strategy q. We denote x∈X PX (x)( y′∈Y py′(x)qy(y′))2 s(y) and denote
x∈X PX (x) y′∈Y py′(x)qy(y′) b(y). By our assumption, b(y) ≥ η if b(y) = 0, and since s(y) ≥ b(y)2, we have s(y) ≥ η2 if b(y) = 0. Then using arguments similar to the ones leading up to (30) in the proof of
Theorem 1, we can show that for all y ∈ Y, |E(etj(y)) − √Kg(y) | ≤ σ(N ) = o(1), and for all y ∈ Y such that b(y) = 0, |E(esj(y)) − √Ks(y) | ≤ σ′(N ) = o(1), where σ(N ) ≥ 0 is some function of N that depends only on α, n and K, and σ′(N ) ≥ 0 is some function of N that depends only on α, η, n and K. Neither of these functions
depend on y. Let G(t) and G(q) be the expected payments to agent j for task i under the truthful strategy
proﬁle and the symmetric proﬁle q, respectively. Let j′ be j’s peer for task i. Let Zji and Zji′ be the reported answers of j and j′ for task i under q. Then we have,

G(q) = E(esj(y))s(y)

y∈Y

(a)
≤ K s(y) + s(y)σ′(N ))

y∈Y

y∈Y

≤ KΓ(Zji, Zji′ ) + |Y|σ′(N ).

(36)

Here, (a) follows from the fact that if b(y) = 0, then s(y) = 0 and moreover, for any y such that b(y) = 0, we have |E(esj(y)) − √Ks(y) | ≤ σ′(N ) from above. Similarly, we can show that

G(t) = E(est (y))g(y)
y∈Y

≥ K g(y) − g(y)σ(N )

y∈Y

y∈Y

≥ KΓ(Yji, Yji′ ) − |Y|σ(N ) ≥ KΓ(Zji, Zji′) + Kδ(PX , p2)Ω(|qY)|2(|Y| − 1) − |Y|σ(N ). (37)

Thus if G(q) ≥ G(t) for any strategy q, then this implies that,

KΓ(Zji, Zji′ ) + Kδ(PX , p2)Ω(|qY)|2(|Y| − 1) − |Y|σ(N ) ≤ KΓ(Zji, Zji′ ) + |Y|σ′(N ),

which implies that

Kδ(PX , p)Ω(q)2(|Y| − 1) ≤ |Y|(σ(N ) + σ′(N )), 2 |Y|

or that,

Ω(q) ≤ 2|Y|3/2(σ(N ) + σ′(N )) < 2|Y|3/2(σ(N ) + σ′(N )) . (38)

Kδ(PX , p)(|Y| − 1)

Kα(|Y| − 1)

Now the quantity on the right is o(1) (depending only on α, η, n, |Y|, and K). Thus for any ω > 0 and η > 0,

there exists some N0 such that for any N ≥ N0, any symmetric strategy proﬁle in which the probability of

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
54
reporting any answer y ∈ Y is either 0 or at least η, and that gives a higher expected payoﬀ to each agent than the truthful strategy proﬁle, is at most ω−uninformative. Since truthful reporting is a Bayes-Nash equilibrium for a large enough N , this implies the result.

Proof of Theorem 4. We will use the following notion for the proof.

Definition 8. For any strategy proﬁle (qj)j∈M across agents, the average reporting strategy excluding

the set of agents J is deﬁned as

q¯−J(y) = 1

qj′ (y).

M − |J| j′∈M\J

Manipulating this deﬁnition, we have,

q¯−J(y) = M q¯(y) − 1 qj(y) .

(39)

M − |J|

M j′∈J

We then directly have that

q¯−J(y) ≥ q¯(y) − |J| . M

Next, since 1/(1 − |MJ| ) is 1 + |MJ| + o( |MJ| ) as M → ∞ (from the Taylor series expansion), we can conclude that

there exists some κ > 1 such that for any M large enough, we have

q¯−J(y) ≤ q¯(y) + κ|J| .

(40)

M

To summarize, for some κ > 1 and any M large enough, we thus have

q¯(y) − κ|J| ≤ q¯−J(y) ≤ q¯(y) + κ|J| .

(41)

M

M

We now present the proof of Theorem 4. Let j be a ﬁxed agent evaluating a ﬁxed task i. Let J be her

(random) peer on task i. Now upon observing y, her expected reward on reporting any y′ such that the

probability of reporting y′ is 0 under the population average strategy is 0 (since there is no hope of matching y′ with any peer). We thus focus on only those y′ ∈ Y such that their reporting probability is at least η. For

any such y′, j’s expected reward on reporting y′ when she observed y is given by,





½ Gji (y, y′) = E  ri =y′ J ½ ½ 

1+

K N − |Wj|
i′ ∈N\Wj rJi′1(i′ )=y′

| Yji = y  rJi′2(i′ )=y′

(=a) E 

1+

K N − |W |j
½ ½  E ½ | Y = y . i′∈N\Wj rJi′1(i′)=y′ rJi′2(i′)=y′

i rJi =y′ j B (probability of matching y′)

A (expected reward from matching y′)

(42) (43)

Here, J1(i′) and J2(i′) are the (random) agents who have evaluated task i′, chosen to compute the agreement rewards for j. (a) results from the fact that the agreement rewards are independent of Yji and YJi: the former because of the fact that the agreement rewards only depend on the tasks that j does not perform, and the

latter because of the random task allocation policy (YJi may contain information about J, but that doesn’t give any information about agents who will be utilized in computing the agreement rewards since the agent

allocation to each task is i.i.d.).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
55

We ﬁrst focus on term A in Equation 43, which is the expected reward for matching on the answer y′.

Note that the random variables ½ri′ ½ =y′ ri′ =y′ across i′ are i.i.d. owing to our random task allocation

J1(i′ )

J2 (i′ )

policy with ½ E( ri′ ½ =y′ ri′ =y′) deﬁned as follows (for notational simplicity we drop the dependence on

j1 (i′ )

j2 (i′ )

i′).

½ ½ E( rJ1 =y′ rJ2 =y′ ) = E

PX (x)py1 (x)py2 (x)qyJ′1 (y1)qyJ′2 (y2) .

x∈X,y1 ,y2 ∈Y

(44)

Here, the latter expectation is over the random choice of J1 and J2. Now we have for a large enough M ,

E

PX (x)py1 (x)py2 (x)qyJ′1 (y1)qyJ′2 (y2)

x∈X,y1 ,y2 ∈Y

(=a) E

PX (x)py1 (x)py2 (x)qyJ′1 (y1)q¯y−′{J1,j}(y2)
x∈X,y1 ,y2 ∈Y

(b)
≤E

PX (x)py1 (x)py2 (x)qyJ′1 (y1) (q¯y′ (y2) + 2κ/M )
x∈X,y1 ,y2 ∈Y

(c)
≤E

PX (x)py1 (x)py2 (x)qyJ′1 (y1)q¯y′ (y2) + 2κ/M

x∈X,y1 ,y2 ∈Y

(d)

≤

PX (x)py1 (x)py2 (x)q¯y−′j (y1)q¯y′ (y2) + 2κ/M

x∈X,y1 ,y2 ∈Y

(e)

≤

PX (x)py1 (x)py2 (x)(q¯y′ (y1) + κ/M )q¯y′(y2) + 2κ/M

x∈X,y1 ,y2 ∈Y

≤

PX (x)py1 (x)py2 (x)q¯y′ (y1)q¯y′ (y2) + 3κ/M.

x∈X,y1 ,y2 ∈Y

(45)

Here, (a) follows from the fact that, J2 is equally likely to be any of the remaining agents other than J1 and

j, again by the random task allocation policy. (b) follows from Equation 41. (c) follows from the fact that

the coeﬃcient of 2κ/M after the expansion is at most 1. (d) follows from taking expectation over J1, who is

equally likely to be any agent other than j. Finally, (e) again follows from Equation 41. Similarly, we have

E

PX (x)py1 (x)py2 (x)qyJ′1 (y1)qyJ′2 (y2) ≥

PX (x)py1 (x)py2 (x)q¯y′ (y1)q¯y′ (y2) − 3κ/M

x∈X,y1 ,y2 ∈Y

x∈X,y1 ,y2 ∈Y

for any large enough M . Let ½ E( ri′ ½ =y ri′ =y) be denoted as h(y), and deﬁne

J1 (i′ )

J2(i′ )

(46)

s(y) =

PX (x)py1 (x)py2 (x)q¯y(y1)q¯y(y2).

x∈X,y1 ,y2 ∈Y

(a)
We have then concluded that |h(y′) − s(y′)| ≤ 3κ/M for each y′ ∈ Y. We also have that s(y) ≥
2
x∈X,y′∈Y PX (x)py′ (x)q¯y ≥ η2, where (a) follows from Jensen’s inequality applied to the function f (x) =

x2. Now by the multiplicative Hoeﬀding’s inequality, for any ǫ > 0, we have,





½ ½ P 
i′ ∈N\Wj

rJi′1(i′ ) =y′

ri′ =y′ ≥ (N − |Wj|)h(y′)(1 + ǫ) ≤ exp(−ǫ2h(y′)/3), and, J2 (i′ )





(47)

½ ½ P 
i′ ∈N\Wj

rJi′1(i′ ) =y′

ri′ =y′ ≤ (N − |Wj|)h(y′)(1 − ǫ) ≤ exp(−ǫ2h(y′)/3). J2 (i′ )

(48)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
56

Thus, for any ǫ > 0, and N large enough, we have,





E

1+

N − |W |j
½ ½  i′∈N\Wj rJi′1(i′)=y′ rJi′2(i′)=y′

(a)
≤E

N − |Wj|

+ exp(−ǫ2h(y′)(N − |W |)/3) N − |W |

1 + (N − |Wj|)h(y′)(1 − ǫ)

j

j





(b)
≤ E

(c)
≤ E

N − |Wj|

+ exp(− ǫ2(s(y′) − 3κ/M )(N − |Wj|) )

1 + (N − |Wj|)(s(y′) − 3Mκ )(1 − ǫ) 3

N − |Wj|



N − |Wj|

+E

1

+

(N

−

|Wj |)(s(y ′ )

−

3κn mN

)(1

−

ǫ)

exp(−

ǫ2(η2

−

3κn mN

)(N

−

|Wj |)

)

3

N − |Wj|

(d)
≤

1

+E

exp(−

ǫ2(η2

−

3κn mN

)(N

−

|Wj|) )

√ N

(s(y′) − 3mκNn )(1 − ǫ) 3

=

1

+ √N exp(− ǫ2(η2 − 3mκNn )N )E exp( ǫ2(η2 − m 3κNn )|Wj | ) .

(s(y′) − 3κn )(1 − ǫ)

3

3

mN

(49)

Here, (a) results from Equation 48 and the fact that in the worst case, the left hand side is at most N − |Wj|. (b) results from the fact that |h(y′) − s(y′)| ≤ 3κ/M , and (c) results from the fact that s(y) ≥ η2 and

M > mN/n. All the expectations are with respect to the randomness in |Wj|. (d) follows from (i) noting that s(y′) ≥ η2 > m 3κNn for N large enough, (ii) ignoring the constant 1 in the denominator of the ﬁrst term, and (iii) ignoring |Wj| ≥ 0 in the second term.

Now, due to the randomized task allocation policy, |Wj| is distributed as Binomial(N, m/N ). Since

the moment generating function of a Binomially distributed random variable X with parameters (n, p) is

E(exp(Xt)) = (1 − p + pet)n, we have that

E exp( ǫ2(η2 − m 3κNn )|Wj | ) = 1 − m + m exp( ǫ2(η2 − m 3κNn ) ) N .

3

NN

3

(50)

Choosing ǫ = N −1/4, we have that

lim
N →∞

1

−

m

+

m

(η2 exp(

− √

m 3κNn ) )

NN

3N

N
= lim N →∞

1

+

m

(exp(

(η2

− √

3κn mN

)

)

−

1)

N

3N

N
≤ lim N →∞

1+ m N

N
= exp(m).
(51)

Thus, choosing ǫ = N −1/4 in Equation 49, and combining Equation 51 with the fact that

√ (η2 − 3κn )√N

lim N exp(−

mN

) = 0,

N →∞

3

we have that



lim E 

N →∞

1+

 N − |W |j
½ ½  ≤ i′∈N\Wj rJi′1(i′)=y′ rJi′2(i′)=y′

1. s(y′)

(52)

Next, we also have that, for a large enough N ,





E

1+

N − |W |j
½ ½  i′∈N\Wj rJi′1(i′)=y′ rJi′2(i′)=y′

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
57

(a)
≥E

(b)
≥ E

(c)
≥ E

N − |Wj|

(1 − exp(−ǫ2h(y′)(N − |W |)/3))

1 + (N − |Wj|)h(y′)(1 + ǫ)

j



N − |Wj|

− exp(− ǫ2(s(y′) − 3κ/M )(N − |Wj|) )

1 + N (s(y′) + 3κ )(1 + ǫ)

3

M

N − |Wj| 
1 + N (s(y′) − 3Mκ )(1 + ǫ)





N − |Wj|



−

E

exp(−

ǫ2(η2

−

3κn mN

)(N

−

|Wj |)

)

1 + N (s(y′) + 3mκNn )(1 + ǫ) 3

N − |Wj| 
1 + N (η2 − m 3κNn )(1 + ǫ)

≥ E 1 − |WNj| − E exp( ǫ2(η2 − m 3κNn )(|Wj |) ) exp(− ǫ2(η2 − 3mκNn )N ) √N

1/N + (s(y′) + 3κn )(1 + ǫ)

3

3

1 + N (η2 − 3κn )(1 + ǫ)

mN

mN

(d) E 1 − |WNj|

mm

ǫ2(η2 − 3κn ) N

ǫ2(η2 − 3κn )N

√ N

≥

− 1 − + exp(

mN ) exp(−

mN )

.

1/N + (s(y′) + 3mκNn )(1 + ǫ) N N 3

3 1 + N (η2 − m 3κNn )(1 + ǫ)

(53)

Here, (a) follows from Equation 47, and the fact that the agreement rewards are always positive. (b) results from the fact that |h(y′) − s(y′)| ≤ 3κ/M and by ignoring the |Wj| term in the denominator, and (c) results from the fact that s(y) ≥ η2 and M > mN/n. (d) follows from Equation 50. We once again choose ǫ = N −1/4. Then, by Equation 51, the second term in Equation 53 converges to 0 as N → ∞. We now focus on the ﬁrst term. The denominator of this term clearly converges to s(y′). It is now easy to show that the numerator converges to 1. This is because |Wj| is distributed as Binomial(N, m/N ), and thus |Wj|/N converges in
√ distribution to the constant 0. Since f (x) = 1 − x is a bounded, continuous function on the domain [0, 1], it follows (by the Portmanteau’s theorem on the equivalence of deﬁnitions of convergence in distribution) that E 1 − |Wj|/N converges to 1 as N → ∞. Thus, we ﬁnally have,



lim E 

N →∞

1+

 N − |W |j
½ ½  ≥ i′∈N\Wj rJi′1(i′)=y′ rJi′2(i′)=y′

1. s(y′)

(54)

Thus, from Equations 52 and 54, we ﬁnally have,

 E

1+

 K N − |W |j
½ ½  − i′∈N\Wj rJi′1(i′)=y′ rJi′2(i′)=y′

K ≤ σ(N ). s(y′)

(55)

where σ(N ) = o(1). Now before we proceed, note that the convergence of the expected matching reward for answer y to K/ s(y) for each y ∈ Y is all that is required for strict truthfulness to follow for a large enough N , as we show in the proof of Theorem 1. For the truthful strategy proﬁle, by the α-separation assumption, we have that s(y) ≥ α2 for all y ∈ Y. Thus, by replacing η with α in the arguments leading up to Equation 55, we can conclude the convergence of the matching rewards to K/ s(y) for each y ∈ Y. Thus, there exists N1 such that for N ≥ N1, the truthful strategy proﬁle is a Bayes-Nash equilibrium. We will not repeat the proof here for conciseness. The ﬁrst statement of the theorem thus follows and we hence focus on proving the second and third statement.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
58

To that eﬀect, we now proceed to focus on term B in Equation 43. We have

P (rJi = y′, Yji = y) (=a)

PX (x)py (x)py2 (x)q¯y−′{j}(y2)

x∈X,y2 ∈Y

(b)

≤

PX (x)py(x)py2 (x)(q¯y′ (y2) + κ/M )

x∈X,y2∈Y

≤

PX (x)py (x)py2 (x)q¯y′ (y2) + κ/M.

x∈X,y2∈Y

(56)

Here, (a) follows from the fact that J is equally likely to be any agent other than j, by the random task

allocation policy. (b) again follows from Equation 41. Similarly, we have

P (rJi = y′, Yji = y) ≥

PX (x)py(x)py2 (x)q¯y′ (y2) − κ/M.

(57)

x∈X,y2∈Y

Thus, we have, for all y′ such that the population average probability of reporting is at least η, we have

P (Yji = y)Gji (y, y′) = (

K + o(1))(

PX (x)py(x)py2 (x)q¯y′ (y2) + o(1))

s(y′)

x∈X,y2 ∈Y

K( =

x∈X,y2∈Y PX (x)py(x)py2 (x)q¯y′ (y2)) + o(1). s(y′)

(58)

Here, we use the fact that s(y) ≥ η2. For every other y′ such that the population average probability of

reporting is 0, we have

P (Yji = y)Gji (y, y′) = 0.

(59)

Let Y′ denote the set of responses such that the population average probability of reporting the response is at least η. Then the expected payoﬀ of agent j on task i under policy qj (ﬁxing everyone else’s policy) is:

Gji (qj ) =

P (Yji = y)Gji (y, y′)qyj′ (y)

y,y′∈Y

(a)

K(

=

y∈Y, y′∈Y′

x∈X,y2∈Y PX (x)py(x)py2 (x)q¯y′ (y2)) qj (y) + o(1)

s(y′)

y′

(60) (61)

(=b) max

P (Yji = y)Gji (y, y′)qyj′ (y)

q

y,y′∈Y

(c)

K(

= max

q

y∈Y, y′∈Y′

x∈X,y2∈Y PX (xs)(pyy′)(x)py2 (x)q¯y′ (y2)) qy′ (y) + o(1).

(62) (63)

Here, (a) follows from Equation 58, (b) follows from the fact that qj is a best-response strategy, and (c)

again follows from Equation 58. Note that the ﬁnal right hand side neither depends on the identity of agent

j nor does it depend on the identity of task i. It only depends on the population average strategy q¯. Since, each policy qj′ optimizes Gji′ (q), we have that

Gji (q¯) =

P (Yji = y)Gji (y, y′)q¯y′ (y)

y,y′∈Y

(a)

K(

=

y∈Y, y′∈Y′

x∈X,y2∈Y PX (x)py(x)py2 (x)q¯y′ (y2)) q¯ (y) + o(1)

s(y′)

y′

(b)

K(

= max

q

y∈Y, y′∈Y′

x∈X,y2∈Y PX (xs)(pyy′)(x)py2 (x)q¯y′ (y2)) qy′ (y) + o(1)

= Gji (qj) + o(1).

(64) (65) (66) (67)

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
59

Here, (a) results from Equation 58. (b) results from averaging the expression in Equation 61 and the expression in Equation 63 across all agents and realizing that the expression in Equation 63 is identical across the agents. Hence, we have that |Gji (qj) − Gji (q¯)| = o(1). Hence, we ﬁnally have,

Gji (qj ) = Gji (q¯) + o(1) K(
=
y∈Y, y′∈Y′

x∈X,y2∈Y PX (x)py(x)py2 (x)q¯y′ (y2)) q¯ (y) + o(1)

s(y′)

y′

K =
y ′ ∈Y′

x∈X,y2∈Y,y∈Y PX (x)py(x)py2 (x)q¯y′ (y2)q¯y′ (y) + o(1) s(y′)

= Ks(y′) + o(1) y′∈Y′ s(y′)

(=a) K s(y′) + o(1)

y′∈Y

(≤b) KΓ(Yji, Yji′ ) − Kδ(PX , p2)Ω(|q¯Y)|2(|Y| − 1) + o(1)

(c)

Kδ(PX , p)Ω(q¯)2(|Y| − 1)

= G(t) −

+ o(1)

2 |Y|

(68) (69) (70) (71) (72) (73) (74)

where G(t) is the expected reward to each agent for an evaluation task under the truthful strategy proﬁle. Here, (a) follows from the fact that s(y) = 0 for all y ∈ Y \ Y′. (b) follows from Proposition 5.1, and (c) follows from the fact that G(t) = KΓ(Yji, YJi) + o(1). This latter conclusion results from the fact that under the truthful strategy proﬁle, because of α-separation of the generating model, the probability of reporting any answer y ∈ Y is at least α2 (see proof of Proposition 4.1). We can thus use the same arguments as that used for deriving the expression in Equation 72 as the expected payoﬀ of each agent, while replacing η with α.
Now the second statement of the theorem immediately follows, since for any ω > 0, there is an N2 such that for any N > N2 we have that Gji (qj) ≤ G(t) + ω. Moreover, we have that if Gji (qj) > G(t), then, Equation 74 allows us to conclude that

Kδ(PX , p)Ω(q¯)2(|Y| − 1) ≤ o(1), or, (75) 2 |Y|

Ω(q¯) ≤ 2 |Y|o(1) ≤ 2 |Y|o(1) . (76) Kδ(PX, p)(|Y| − 1) Kα(|Y| − 1)
Thus for any ω > 0, there is an N3 such that for any N > N3, we have that Ω(q¯) ≤ ω for any population strategy proﬁle where (a) the average probability of reporting any answer y is either 0 or at least η, and (b) there exists an agent whose expected payoﬀ is larger than the expected payoﬀ under the truthful strategy proﬁle. Thus, all the statements of the theorem hold for any N larger than N0 = max(N1, N2, N3).

B. Properties of the square-root agreement measure The SRAM has the following properties.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
60

1. Γ(Y1, Y2) ≥ 1. To see this, note that Jensen’s inequality implies that

PX (x)py(x)2 ≥

PX (x)py(x) = 1.

y∈Y x∈X

y∈Y x∈X

In fact Γ(Y1, Y2) = 1 only when Y1 and Y2 are independent.

2. Γ(Y1, Y2) ≤ |Y|. To see this, note that Jensen’s inequality implies that

PX (x)py(x)2 ≤ |Y| |Y1|

PX (x)py(x)2

y∈Y x∈X

y∈Y x∈X

1

≤ |Y| |Y|

PX (x)py(x) = |Y|.

y∈Y x∈X

In fact Γ(Y1, Y2) = |Y| only when Y1 and Y2 are identical and they are distributed uniformly, i.e., Y1 = Y2 and P (Y1 = y) = 1/|Y| for all y ∈ Y. We also prove the following inequality satisﬁed by the SRAM, which generalizes Proposition 5.1 without the characterizing the inequality gap.

Proposition B.1 (A general monotonicity property) Consider a generating model (PX, p) deﬁned

over X and Y, and consider two random responses Y1 and Y2 drawn from this model. Also, consider two

random responses Z1 and Z2 obtained by applying a reporting strategies q and q′ independently to Y1 and

Y2 respectively. Then,

P (Z1 = Z2 = y) ≤ Γ(Y1, Y2).

(77)

y∈Y

Moreover, if δ(PX, p) > 0, then the above inequality is an equality if and only if q = q′ and Ω(q) = 0, i.e., if

and only if the two reporting strategies are identical and fully informative.

Proof of Proposition B.1 We have,

P (Z1 = Z2 = y) =

PX (x)py1 (x)py2 (x)qy(y1)qy′ (y2)

y∈Y

y∈Y x∈X,y1∈Y,y2∈Y

=
y∈Y

qy(y1)qy′ (y2) PX (x)py1 (x)py2 (x)

y1 ∈Y,y2 ∈Y

x∈X

(a)
≤
y∈Y

qy(y1)qy′ (y2)
y1 ∈Y,y2∈Y

PX (x)py1 (x)2
x∈X

PX (x)py2 (x)2
x∈X

=
y∈Y

qy (y1 )
y1∈Y

PX (x)py1 (x)2
x∈X

qy′ (y2)
y2∈Y

PX (x)py2 (x)2
x∈X

(b) 1

≤2

qy (y1 )

y∈Y,y1∈Y

PX (x)py (x)2 + 1

1

2

qy′ (y2)

x∈X

y∈Y,y2∈Y

=1 2 y1∈Y

PX (x)py (x)2 + 1

1

2

x∈X

y2∈Y

PX (x)py2 (x)2
x∈X

= Γ(Y1, Y2) + Γ(Y1, Y2)

2

2

= Γ(Y1, Y2)

PX (x)py2 (x)2
x∈X

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
61

Here, (a) follows from the Cauchy-Schwarz inequality, and (b) results from the fact that the arithmetic mean of two numbers is no less than the geometric mean.
Now suppose that δ(PX , p) > 0. Then (a) is an equality if and only if qy(y1)qy′ (y2) = 0 for every y and every y1 = y2. Further, (b) is an equality, i.e., arithmetic mean equals geometric mean, if and only if all the terms are equal. This means for all y ∈ Y,

i.e., if

qy (y1 )
y1∈Y

PX (x)py1 (x)2 = qy′ (y2)

x∈X

y2∈Y

PX (x)py2 (x)2,
x∈X

(qy(y′) − qy′ (y′))
y′∈Y

PX (x)py′ (x)2 = 0.
x∈X

(78)

Squaring both sides, we obtain, for all y ∈ Y,

(qy(y′) − qy′ (y′))2( PX (x)py′ (x)2)

y′∈Y

x∈X

+

(qy(y′) − qy′ (y′))(qy (y′′) − qy′ (y′′))

y′=y′′

PX (x)py′ (x)2
x∈X

Substituting qy(y′)qy′ (y′′) = 0 for all y′ = y′′, we obtain,

(qy(y′) − qy′ (y′))2( PX (x)py′ (x)2)

y′∈Y

x∈X

PX (x)py′′ (x)2 = 0.
x∈X

(79)

+

(qy(y′)qy(y′′) + qy′ (y′)qy′ (y′′))

y′=y′′

PX (x)py′ (x)2
x∈X

PX (x)py′′ (x)2 = 0.
x∈X

(80)

But if δ(PX, p) > 0, then we know from Proposition 4.1 that

x∈X PX (x)py′ (x)2 ≥ x∈X PX (x)py′ (x) > 0

for all y′ ∈ Y. Hence, we conclude that if δ(PX , p) > 0, then y∈Y P (Z1 = Z2 = y) = Γ(Y1, Y2) holds, if and

only if all the terms in Equation 80 are 0, i.e., if and only if

1. qy(y′) = qy′ (y′) for all y, y′ ∈ Y, i.e., q and q′ are identical, and,

2. qy(y′)qy(y′′) = 0 for all y ∈ Y and y′ = y′′, i.e., Ω(q) = 0. This ﬁnishes the proof.

B.1. Utility of the square-root agreement measure beyond our work
Deﬁnition 6 essentially deﬁnes an agreement measure between any two random variables that are independent and identically distributed conditioned on some latent random variable. But we could just as well deﬁne an agreement measure between any two random variables that take values in some common ﬁnite set.
Definition 9. Consider two random variables X and X′, which take values in a ﬁnite set S. Then the square-root agreement measure between X and X′ is deﬁned as

Γ(X, X′) =
s∈S

P (X = X′ = s).

Proposition B.1 implies that if X → X′ → Y form a Markov chain, i.e., X is conditionally independent of

Y given X′, and if, conditioned on some latent random variable U , X and X′ are independent and identically

distributed random variables, then,

Γ(X, Y ) ≤ Γ(X, X′).

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
62
Inequalities of this form are called data processing inequalities and they have several applications in information theory, statistics, causal inference, and related ﬁelds. For example, such inequalities provide testable hypotheses to determine the validity of conditional independence assumptions across variables from data. Several mutual information measures between two random variables are known to satisfy such inequality. These measures are typically constructed from two classes of divergences or distance notions between probability distributions, called f-divergences and Bregman divergences; see Kong and Schoenebeck (2019) and references therein. It is interesting to note that our SRAM does not result from such a construction, and to the best of our knowledge, the resulting data-processing inequality was not known in the literature. Moreover, typical mutual information measures depend on the entire joint distribution of two variables, i.e., to estimate these measures from data, one typically needs to learn |S|2 probability values where S is the support set of each variable. On the other hand, the SRAM only depends on the diagonal values of the joint probability distribution, i.e., only the probabilities of agreement matter. Hence, to estimate the SRAM from data, one only needs to learn |S| probability values. It is important to note that for the data processing inequality to hold for the SRAM, X and X′ need to be conditionally independent and identically distributed (conditioned on some latent random variable). There is typically no such requirement for other measures. To show that this condition is necessary, consider the following counterexample. Suppose that X is uniformly distributed on the discrete set {−1, +1}, and X′ = −X. Thus Γ(X, X′) = 0. Whereas if
√ Y = −X′, then it is true that X → X′ → Y forms a Markov chain, and Γ(X, Y ) = Γ(X, X) = 2. Hence, Γ(X, X′) < Γ(X, Y ).

C. Properties of the uninformativeness measure

The uninformativeness measure has the following properties. 1. Clearly, Ω(q) = 0 if and only if (q(y); y ∈ Y) have disjoint supports across all y ∈ Y, i.e., if and only if q is fully informative.

2. Ω(q) attains its highest value of 1, if and only if q(y) = q(y′) for any y = y′, i.e., if the report is chosen

independently of the true answer. To see this, observe that,

1 |Y|(|Y| − 1) y∈Y y′∈Y,y′′∈Y

qy (y′)qy (y′′)1{y′=y′′}

(≤a) 1 |Y|(|Y| − 1)
y∈Y

qy (y′)1{y′=y′′}
y′∈Y,y′′∈Y

qy (y′′)1{y′=y′′}
y ′ ∈Y,y ′′ ∈Y

=1 |Y|(|Y| − 1) y∈Y

(|Y| − 1)2

=1

qy (y ′ )

|Y| y∈Y y′∈Y

= 1.

2
qy (y ′ )
y′∈Y

(81)

Here, (a) follows from the Cauchy-Schwarz inequality.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
63

D. Miscellaneous remarks on existing mechanisms

D.1.

An adaptation of the Correlated Agreement (CA) mechanism to the homogeneous responses setting

In this section, we present an adaptation of CA to the homogeneous responses setting, which induces truthful behavior while only requiring one evaluation per agent. In order to deﬁne this mechanism, we ﬁrst present the original CA mechanism.
Original CA mechanism. CA operates on a pair of agents. Both agents perform a common “bonus” evaluation task, say A, and individually perform one independent “penalty” evaluation task that the other agent doesn’t perform; say agent 1 performs B, and agent 2 performs C. In keeping with our notation, let rji for j = 1, 2, and i ∈ {A, B, C}, be the response of agent j in task i, where the responses are taken to be the null φ if an agent doesn’t perform the corresponding task (hence r1C = r2B = φ). CA deﬁnes an intermediate scoring function that maps two responses to a real number, which, informally, is monotonically increasing in the expected correlation between the responses, i.e., S is higher if the two responses are expected to frequently occur together. Formally, denoting ∆ab = P (Yji = a, Yji′ = b) − P (Yji = a)P (Yji′ = b), for any two responses a, b (where it is assumed that agents j and j′ have both performed task i), the intermediate score for these responses is deﬁned to be:
S(a, b) = sgn(∆ab),

where sgn is the sign function. In the multi-task, homogeneous responses setting, this scoring function can be estimated from the response data obtained from the large number of other participants (i.e., excluding the two agents under consideration) operating on the platform, and relying on the “self-fulﬁlling prophecy of truthfulness” to assume truthful behavior.19 The ﬁnal score/payment to an agent i is then deﬁned to be

S(r1A, r2A) − S(r1B, r2C ),

i.e., the ﬁnal score/payment is the diﬀerence between the bonus score and the penalty score. The intuition is that the payment scheme rewards incremental correlation in the responses to the bonus task over what is expected anyway from the responses to two independent evaluation tasks.
An adaptation of CA for homogeneous responses requiring one evaluation per agent. Consider the following adaptation of CA. Consider an agent, say 1, who has performed evaluation task A, and whose payment needs to be determined. Let 2 be another agent who has performed task A. Let 3 be a third agent who has performed some task B that 1 hasn’t performed. Then the payment to agent 1 is deﬁned to be:

S(r1A, r2A) − S(r1A, r3B).
Here, it is assumed that these scoring functions are calculated as in the original CA mechanism based on the responses data from all tasks other than A and B. In a natural practical implementation of this mechanism, to calculate the payment of an agent j, the platform would randomly pick a peer agent who has performed the
19 In the general setting with non-homogeneous responses, this function can be estimated by having the two agents perform a large number of overlapping and disjoint tasks.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
64

same evaluation to calculate the bonus score and similarly, randomly pick another agent who has performed some task that the agent j hasn’t performed to calculate the penalty score. These scoring functions are calculated from the response data of all tasks that j hasn’t performed. We call this mechanism CA for homogeneous responses (CA-HR).
It is clear that this mechanism only requires each agent to perform one evaluation as long as (a) each task is performed by at least two agents, and (b) there is a large number of tasks while each agent performs only a small number (so that the scores can be estimated accurately, independently of the agent’s reports). These assumptions are almost the same as that required by SRA for its properties, and they are easily satisﬁed on most platforms.
It is easy to argue that truthful behavior is an equilibrium under CA-HR (in the large tasks regime where the scoring function estimates are reasonably accurate) in the homogeneous responses setting. This is because, due to the statistical indistinguishability of agent 2 and 3’s responses to an arbitrary task assuming that they are truthful, replacing agent 2 with agent 3 in the calculation of the penalty score is inconsequential from the perspective of agent 1. All that matters from the perspective of agent 1 (in terms of aligning with the incentives generated by the original CA mechanism) is that this penalty score is computed on the basis of some agent’s response to a task that 1 hasn’t performed. Thus, the fact that truthful behavior is a best response under the original CA mechanism implies that it is a best response under this modiﬁcation as well.
D.2. Remarks on the properties of CA/CA-HR in our setting.
Although CA is informed truthful in the setting in which it is originally deﬁned, neither CA nor CA-HR are informed truthful in our setting. This is because in our setting, task allocations are exogenously speciﬁed and agents can choose task-contingent reporting strategies based on task identities. We present an example below that shows this for CA-HR.
Example 5. Consider the setting in Example 1 again. For the sake of the present discussion, suppose that the plumbers are numbered i = 1, · · · , N (just as the tasks are numbered in our formal model). If everyone is truthful, the accurate scoring function is S(a, b) = 1{a=b} − 1{a=b}. Suppose that j has evaluated plumber A. j′ is her randomly chosen peer who also has also evaluated A. Let j′′ be another randomly chosen peer who has evaluated plumber B, whom j hasn’t evaluated. Then the (random) payment of agent j under CA-HR is

1 − 1 − 1 + 1 . {YjA=YjA′ }

{YjA=YjA′ }

{YjA=YjB′′ }

{YjA=YjB′′ }

Thus, the expected payment of agent j can be determined to be

2

P

(YjA

=

YjA′ )

−

P

(YjA

=

YB j′′

)

This can be computed to be 0.2025, given the generating model. On the other hand, consider the following strategy proﬁle. For all even tasks, agents report ‘Yes,’ and for all odd tasks, agents report ‘No.’ We ﬁrst argue that this strategy proﬁle is an equilibrium in the many tasks regime. Note that under this strategy proﬁle, the scoring function that will be estimated by the platform is S¯(a, b) = 1{a=b} − 1{a=b}, same as that under truthful behavior. Thus, the expected payment of an agent for reporting ‘No’ on an even task (or reporting ‘Yes’ on an odd task) is −1, whereas the expected payment from following the prescribed strategy

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
65
is 1. This argument shows both, that (a) this strategy proﬁle constitutes an equilibrium and (b) the expected payoﬀ to any agent under this strategy proﬁle (which is 1) is strictly higher than the expected payment under the truthful equilibrium (which is 0.2025). Thus CA-HR is not informed truthful. The same construction of a non-truthful strategy proﬁle also shows that CA is also not informed truthful in our setting.
Moreover, unlike the mechanism of Kong and Schoenebeck (2019) (see Section 3), in our setting, neither CA nor CA-HR are informed truthful across all equilibria where agents choose the same reporting strategy for each task they perform. This is because task allocations are exogenously speciﬁed: in the example above, it could very well be the case that every agent performs exactly one task under CA-HR. In this case, the non-truthful equilibrium strategy proﬁle constructed above respects the constraint that each agent chooses the same reporting strategy for each task they perform, simply because each agent performs only one task. A similar argument shows this for CA by considering a situation in which each agent performs either even tasks only or odd tasks only.
Although CA and CA-HR are not informed truthful in our setting, Lemma 5.12 in Shnayder et al. (2016) implies that these mechanisms are informed truthful across symmetric equilibria, i.e., they are informed truthful when restricted to symmetric equilibria, in the many tasks limit.
Next, we discuss why CA and CA-HR are not (asymptotically) strongly truthful across symmetric equilibria in general for homogeneous responses, i.e., there could be symmetric strategy proﬁles that are not fully informative, that asymptotically yield the same payoﬀ as the truthful equilibrium. The existence of such strategy proﬁles is related to the following notion of “clustered observations” as deﬁned in Shnayder et al. (2016).
Definition 10. (Shnayder et al. 2016) A distribution of two agents’ observations for a common evaluation is said to be clustered if there exist at least two identical rows in the matrix [sgn(∆yy′)]y∈Y, y′∈Y. (Note that [sgn(∆yy′ )]y∈Y, y′∈Y is a symmetric matrix under homogeneous responses)
In the presence of clustered observations, there are symmetric equilibrium strategy proﬁles that are not fully informative, that yield the same payoﬀ asymptotically as the truthful equilibrium under CA/CAHR. To see this for CA-HR, suppose that y and y¯ are two observations for which the corresponding rows (sgn(∆yy′); y′ ∈ Y) and (sgn(∆y¯y′); y′ ∈ Y) are identical. Then, if all agents report a ﬁxed observation, e.g., y, irrespective of whether they observe y or y¯, the scoring function estimated by the platform under CA-HR is the same as that under truthful behavior, except with the answer y¯ eliminated as a possible report. However, if everyone else was truthful, the bonus and penalty scores obtained by an agent would have anyway been identical irrespective of whether any of the three agents involved in computing a payment report y or y¯. Thus the payments to all agents remain the same if everyone reports y irrespective of whether they observe y or y¯. It thus follows that this strategy proﬁle is an equilibrium under CA-HR, which yields the same expected payoﬀ to any agent as the truthful equilibrium. This strategy proﬁle is not a fully informative strategy proﬁle, and hence, CA-HR is not asymptotically strongly truthful across symmetric equilibria. The mechanism is essentially incapable of identifying the diﬀerence between y and y′ since it depends only on the sign structure of the ∆ matrix and not the values themselves.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
66

If an instance does not possess clustered observations, CA and CA-HR are strongly truthful across symmetric equilibria. In our practically motivated experimental setup, however, we ﬁnd that clustered observations are encountered with a high frequency; see Remark 4.

D.3.

Insuﬃciency of a single evaluation per agent with homogenous responses in the Kong and Schoenebeck (2019) (KS) mechanism design framework

In this section, we show that it is impossible to design a mechanism within the KS mechanism design framework in the homogeneous responses setting, that incentivizes truthfulness with one evaluation per agent. The KS framework operates on a pair of agents and the payment of each agent is deﬁned to be some scaling of an unbiased estimate of some mutual information measure constructed from their responses to a common set of tasks. The suﬃciently of a single response per agent within this framework implies that the payment must be decided based only on the pair of agents’ responses to a single task. We argue that such a payment scheme cannot strictly incentivize truthful behavior even in the homogenous, binary response setting. This result is not new; it has been shown in the general homogeneous responses setting in Jurca and Faltings (2011) (Theorem 1). We present a proof of the simpler binary responses case below for completeness. This result implies that there cannot be any mutual information measure satisfying information monotonicity, whose unbiased estimate can be constructed based on two agents’ responses to a single evaluation task.

Proposition D.1 (Jurca and Faltings 2011) In any truthful mechanism in the homogenous, binary responses setting that calculates the payment of an agent only as a function of the responses of the agent and her peer to a single evaluation task, the payment to the agent does not depend on her own responses.
Proof. Consider an evaluation task with only two responses: Y = {Yes, No}. The payment scheme that depends on the responses of an agent and her peer to a common task is a speciﬁcation of payment to the agent for every possible pair of responses. One of these payments can be 0 without loss of generality since additive shifts of payments across all possibilities do not change the incentive structure of the game. Let us suppose that the payments are as shown in Table 4, where it is assumed that the agent is the row player.

Table 4

Yes No Yes a b No c 0
The payments to the row agent corresponding to the pair of responses for the common evaluation task.

Let the generating model have two possible types X = {A, B}, with PX = (1/2, 1/2), p(A) = (p, 1 − p), and p(B) = (q, 1 − q). The expected payment of the agent if she reports ‘Yes’ on observing ‘Yes’ can be determined to be:
a(p2/2 + q2/2) + b(p(1 − p)/2 + q(1 − q)/2). (82) p/2 + q/2
The expected payment of the agent if she reports ‘No’ on observing ‘Yes’ can be determined to be: c(p2/2 + q2/2) . (83) p/2 + q/2

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
67

Thus reporting ‘Yes’ on observing ‘Yes’ yields a higher expected payment if

(c − a)(p2 + q2) ≤ b(p(1 − p) + q(1 − q)).

(84)

The expected payment of the agent if she reports ‘No’ on observing ‘No’ can be determined to be: c(p(1 − p)/2 + q(1 − q)/2). (85) (1 − p)/2 + (1 − q)/2
The expected payment of the agent if she reports ‘Yes’ on observing ‘No’ can be determined to be: a(p(1 − p)/2 + q(1 − q)/2) + b((1 − p)2/2 + (1 − q)2/2) . (86) (1 − p)/2 + (1 − q)/2
Thus reporting ‘No’ on observing ‘No’ yields a higher expected payment if

b((1 − p)2 + (1 − q)2) ≤ (c − a)(p(1 − p) + q(1 − q)).

(87)

If we set p and q such that p2 + q2 = p(1 − p) + q(1 − q) (e.g., p = q = 0.5), then from Equation 84 we obtain b ≥ c − a on the other hand, if we set p and q such that (1 − p)2 + (1 − q)2 = p(1 − p) + q(1 − q) (e.g., p = q = 0.5), then from Equation 91 we obtain b ≤ c − a. Thus, we have b = c − a.
Next, if b = c − a > 0, then Equations 84 and 91, reduce to:

p2 + q2 ≤ p(1 − p) + q(1 − q),

(88)

(1 − p)2 + (1 − q)2 ≤ p(1 − p) + q(1 − q).

(89)

In this case, setting p = q = 0.25 violates the second inequality. If b = c − a < 0, then Equations 84 and 91, reduce to:

p2 + q2 ≥ p(1 − p) + q(1 − q),

(90)

(1 − p)2 + (1 − q)2 ≥ p(1 − p) + q(1 − q).

(91)

In this case, setting p = q = 0.25 violates the ﬁrst inequality. Hence, we have that b = c − a = 0, i.e., the mechanism’s payments are independent of the reports of the agent.

D.4.

Infeasibility of a generic adaptation of the KS framework to multi-task, homogeneous responses settings and the special role of the square-root agreement measure (SRAM)

The design of SRA suggests that perhaps a generic adaptation of the KS mechanism to homogeneous

responses setting that incentivizes single evaluations is possible under any mutual information measure. We

argue that this is not true via the example of Shannon mutual information (Cover and Thomas 2012). For

two random variables Y1 and Y2 taking values in ﬁnite sets Y1 and Y2 respectively, the Shannon mutual

information is deﬁned to be,

I(Y1; Y2) =

P (Y1 = y, Y2 = y′) log P (Y1 = y, Y2 = y′) .

y∈Y1, y′∈Y2

P (Y1 = y)P (Y2 = y′)

(92)

Suppose that the distribution of two agents’ responses to a common evaluation task is available to the

platform (estimated from a large number of evaluation tasks). Then, along the lines of SRA, the mutual

information measure above suggests the following mechanism.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
68

1. Each agent j is paired with another randomly chosen agent j′, and their responses are compared.

2. If the response of agent j is y and that of agent j′ is y′, then j gets a reward K log PP(Y(Yj=j=y)yP, Y(jY′j=′ =y′y)′) , where K is some positive constant.

Under this mechanism, if j’s true response is y and j′ is truthful, her expected reward for a truthful report

is,

K P (Y = y′ | Y = y) log P (Yj = y, Yj′ = y′) = K P (Yj′ = y′, Yj = y) log P (Yj = y, Yj′ = y′) .

j′

j

P (Yj = y)P (Yj′ = y′)

P (Yj = y)

P (Yj = y)P (Yj′ = y′)

y′∈Y

y′∈Y

(93)

Similarly, her reward for any other report y¯ is,

K P (Yj′ = y′, Yj = y) log P (Yj = y¯, Yj′ = y′) .

P (Yj = y)

P (Yj = y¯)P (Yj′ = y′)

y′∈Y

(94)

Thus being truthful yields a higher reward if for any y¯ = y, expression in Equation 93 is higher than the one

in Equation 94, which simpliﬁes to the condition,

K P (Y = y, Y = y′) log P (Yj = y, Yj′ = y′) − KP (Y = y) log P (Yj = y) ≥ 0.

(95)

j

j′

P (Yj = y¯, Yj′ = y′)

j

P (Yj = y¯)

y′∈Y

This inequality is not satisﬁed in general for homogeneous responses. We tested this condition in our

experimental setup of Section 6. Assuming that there are |Y| = 5 responses as deﬁned in that section, and

two types of moving companies with delays exponentially distributed and mean delays drawn uniformly

in [0, 60] (in minutes), we found that 629 of 10000 instances we generated violated the inequality in

Equation 95.

E. Auxillary results Proposition E.1 If responses are categorical then they are self-predicting.

Proof. For any two responses y and y′, the categorical responses condition says that,

P (Yj′ = y′ | Yj = y) ≤ P (Yj′ = y′).

(96)

However, this implies that P (Yj′ = y′) ≤ P (Yj′ = y′ | Yj = y′). This means that for any two responses y and y′,

P (Yj′ = y′ | Yj = y) ≤ P (Yj′ = y′ | Yj = y′).

(97)

But this is exactly the self-prediction condition.

Proposition E.2 Consider two exchangeable random variables, Y1 and Y2, taking values in a ﬁnite set Y. If their distribution satisﬁes the strict Cauchy-Schwarz property:

P (Y1 = Y2 = y) P (Y1 = Y2 = y′) > P (Y1 = y, Y2 = y′),

(98)

for each y, y′ ∈ Y, then Y1 and Y2 are stochastically relevant random variables.

Kamble et. al.: The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms
69

Proof. We will show that stochastic irrelevance for two values y and y′ implies that the CS property is satisﬁed for these values with an equality. Stochastic irrelevance for y and y′ implies that the conditional distributions of Y2 given Y1 = y and Y1 = y′ are identical. This implies that there is some constant C > 0 such that (P (Y1 = y′, Y2 = a); a ∈ Y) = C × (P (Y1 = y, Y2 = a); a ∈ Y). In particular we have that:

P (Y1 = Y2 = y′) = C × P (Y1 = y, Y2 = y′) and P (Y1 = y′, Y2 = y) = C × P (Y1 = Y2 = y).

(99) (100)

We thus have,

P (Y1 = Y2 = y)

P (Y1 = Y2 = y′) = P (Y1 = Y2 = y) C × P (Y1 = y, Y2 = y′) = P (Y1 = Y2 = y) P (Y1 = y′, Y2 = y) × P (Y1 = y, Y2 = y′) P (Y1 = Y2 = y) = P (Y1 = y′, Y2 = y)P (Y1 = y, Y2 = y′) (=a) P (Y1 = y, Y2 = y′).

(101) (102) (103) (104)

Here (a) follows from exchangeability of Y1 and Y2. Thus the CS property is satisﬁed with an equality for y and y′.

