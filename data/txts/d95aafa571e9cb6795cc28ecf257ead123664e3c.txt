1

Kernel Cuts: MRF meets Kernel & Spectral Clustering

Meng Tang∗

Dmitrii Marin∗

Ismail Ben Ayed†

Yuri Boykov∗

∗Computer Science, University of Western Ontario, Canada †E´ cole de Technologie Supe´ rieure, University of Quebec, Canada mtang73@csd.uwo.ca dmitrii.a.marin@gmail.com ismail.benayed@etsmtl.ca yuri@csd.uwo.ca

!

arXiv:1506.07439v6 [cs.CV] 21 Sep 2016

Abstract—We propose a new segmentation model combining common regularization energies, e.g. Markov Random Field (MRF) potentials, and standard pairwise clustering criteria like Normalized Cut (NC), average association (AA), etc. These clustering and regularization models are widely used in machine learning and computer vision, but they were not combined before due to signiﬁcant differences in the corresponding optimization, e.g. spectral relaxation and combinatorial max-ﬂow techniques. On the one hand, we show that many common applications using MRF segmentation energies can beneﬁt from a high-order NC term, e.g. enforcing balanced clustering of arbitrary high-dimensional image features combining color, texture, location, depth, motion, etc. On the other hand, standard clustering applications can beneﬁt from an inclusion of common pairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency, label cost, etc. To address joint energies like NC+MRF, we propose efﬁcient Kernel Cut algorithms based on bound optimization. While focusing on graph cut and move-making techniques, our new unary (linear) kernel and spectral bound formulations for common pairwise clustering criteria allow to integrate them with any regularization functionals with existing discrete or continuous solvers.
1 INTRODUCTION AND MOTIVATION
The terms clustering and segmentation are largely synonyms. The latter is common speciﬁcally for computer vision when data points are intensities or other features Ip ∈ RN sampled at regularly spaced image pixels p ∈ RM . The pixel’s location is essential information. Many image segmentation methods treat Ip as a function I ∶ RM → RN and process intensity and location in fundamentally different ways. This applies to many regularization methods including discrete MRF-based techniques [1] and continuous variational methods [2]. For example, such methods often use pixel locations to represent the segment’s geometry/shape and intensities to represent its appearance [3], [4], [5], [6].
The term clustering is often used in the general context where data point Ip is an arbitrary observation indexed by integer p. Data clustering techniques [7] are widely used outside of vision. Variants of K-means, spectral or other clustering methods are also used for image segmentation. They often join the pixel’s location and its feature into one combined data point in RN+M . We focus on a well-known general group of pairwise clustering methods [8] based on some estimated afﬁnities between all pairs of points.
While independently developed from different methodologies, standard regularization and pairwise clustering methods are deﬁned by objective functions that have many complementary

properties reviewed later. Our goal is to combine these functions
into a joint energy applicable to image segmentation or general
clustering problems. Such objectives could not be combined be-
fore due to signiﬁcant differences in the underlying optimization
methods, e.g. combinatorial graph cut versus spectral relaxation.
While focused on MRF regularization, our approach to integrating
pairwise clustering is based on a bound formulation that easily
extends to any regularization functional with existing solvers.
We use basic notation applicable to either image segmentation or general data clustering. Let Ω be a set of pixels, voxels, or any other points p. For example, for 2D images Ω could be a subset of regularly spaced points in R2. Set Ω could also represent data points indices. Assume that every p ∈ Ω comes with an observed feature Ip ∈ RN . For example, Ip could be a greyscale intensity in R1, an RGB color in R3, or any other high-dimensional observation. If needed, Ip could also include the pixel’s location.
A segmentation of Ω can be equivalently represented either as a labeling S ∶= (Sp p ∈ Ω) including integer node labels 1 ≤ Sp ≤ K or as a partitioning {Sk} of set Ω into K non-overlapping subsets or segments Sk ∶= {p ∈ Ω Sp = k}. With minor abuse of notation, we will also use Sk as indicator vectors in {0, 1} Ω .
We combine standard pairwise clustering criteria such as Average Association (AA) or Normalized Cut (NC) [8] and common
regularization functionals such as pairwise or high-order MRF
potentials [1], [9]. The general form of our joint energy is

E(S) = EA(S) + γ Ec(Sc)

(1)

c∈F

where the ﬁrst term is some pairwise clustering objective based on
data afﬁnity matrix or kernel A ∶= [Apq] with Apq ∶= A(Ip, Iq) deﬁned by some similarity function A(⋅, ⋅). The second term in (1) is a general formulation of MRF potentials [10], [11], [12].
Constant γ is a relative weight of this term. Subset c ⊂ Ω represents a factor typically consisting of pixels with nearby
locations. Factor labels Sc ∶= (Sp p ∈ c) is a restriction of labeling S to c. Potentials Ec(Sc) for a given set of factors F represent various forms of unary, second, or higher order constraints, where
factor size c deﬁnes the order. Factor features {Ip p ∈ c} often work as parameters for potential Ec. Section 1.1 reviews standard
MRF potentials. Note that standard clustering methods encourage
balanced segments using ratio-based objectives. They correspond
to high-order potential EA(S) of order Ω in (1). Sections 1.2 and 1.3 review standard clustering objectives used in our paper.

1.1 Overview of MRF regularization
Probably the most basic MRF regularization potential corresponds to the second-order Potts model [10] used for edge alignment

Ec(Sc) =

wpq ⋅ [Sp ≠ Sq] ≈ ∂S

(2)

c∈F

pq∈N

where a set of pairwise factors F = N includes edges c = {pq} between pairs of neighboring nodes and [⋅] are Iverson brackets. Weight wpq is a discontinuity penalty between p and q. It could
be a constant or may be set by a decreasing function of intensity
difference Ip − Iq attracting the segmentation boundary to image contrast edges [4]. This is similar to the image-based boundary
length in geodesic contours [3], [5]. A useful bin consistency constraint enforced by the P n-Potts
model [11] is deﬁned over an arbitrary collection of high-order
factors F. Factors c ∈ F correspond to predeﬁned subsets of nodes such as superpixels [11] or bins of pixels with the same
color/feature [13], [14]. The model penalizes inconsistency in
segmentation of each factor

Ec(Sc) = min{T, c − Sc ∗}

(3)

c∈F

c∈F

where T is some threshold and Sc ∗ ∶= maxk Sk ∩ c is the cardinality of the largest segment inside c. Potential (3) has its
lowest value (zero) when all nodes in each factor are within the
same segment.
Standard label cost [12] is a sparsity potential deﬁned for a single high-order factor c = Ω. In its simplest form this potential penalizes the number of distinct segments (labels) in S

EΩ(S) = hk ⋅ [ Sk > 0]

(4)

k

where hk could be a constant or a cost for each speciﬁc label. Potentials (2), (3), (4) are only a few examples of regular-
ization terms widely used in combination with powerful discrete solvers like graph cut [10], belief propagation [15], TRWS [16], LP relaxation [17], [18], or continuous methods [19], [20], [21].
Image segmentation methods often combine regularization with a likelihood term integrating segments/objects color models. For example, [4], [22] used graph cuts to combine second-order edge alignment (2) with a unary (ﬁrst-order) appearance term

−

log P k(Ip)

(5)

k p∈Sk

where {P k} are given probability distributions. Unary terms like (5) are easy to integrate into any of the solvers above.
If unknown, parameters of the models {P k} in a regularization energy including (5) are often estimated by iteratively minimizing the energy with respect to S and model parameters [23], [24],
[25], [26], [12]. In presence of variable model parameters, (5) can
be seen as a maximum likelihood (ML) model-ﬁtting term or a
probabilistic K-means clustering objective [27]. The next section
reviews K-means and other standard clustering methods.

1.2 Overview of K-means and clustering

Many clustering methods are based on K-means (KM). The most basic iterative KM algorithm [28] can be described as the blockcoordinate descent for the following mixed objective

F (S, m) ∶=

Ip − mk 2

(6)

k p∈Sk

2

combining discrete variables S = {Sk}Kk=1 with continuous variables m = {mk}Kk=1 representing cluster “centers”. Norm . denotes the Euclidean metric. For any given S the optimal centers arg minm F (S, m) are the means
µSk ∶= ∑q∈SSkk Iq (7)

where Sk denotes the segment’s cardinality. Assuming current segments Stk the update operation giving arg minS F (S, µSt )

basic KM procedure

Sp ← arg min Ip − µSk

(8)

k

t

deﬁnes the next solution St+1 as per standard K-means algorithm. This greedy descent technique converges only to a local minimum of KM objective (6), which is known to be NP hard to optimize. There are also other approximation methods. Below we review the properties of KM objective (6) independently of optimization.
The optimal centers mk in (7) allow to represent (6) via an equivalent objective of a single argument S

Ip − µSk 2 ≡

Sk ⋅ var(Sk).

(9)

k p∈Sk

k

The sum of squared distances between data points {Ip p ∈ Sk} and mean µSk normalized by Sk gives the sample variance denoted by var(Sk). Formulation (9) presents the basic KM objective as a standard variance criterion for clustering. That is, K-means attempts to ﬁnd K compact clusters with small variance.
K-means can also be presented as a pairwise clustering criteria with Euclidean afﬁnities. The sample variance can be expressed as the sum of distances between all pairs of the points. For example, plugging (7) into (9) reduces this KM objective to

∑pq∈Sk Ip − Iq 2 . (10)

k

2 Sk

Taking the square in the denominator transforms (10) to another equivalent KM energy with Euclidean dot-product afﬁnities

=c − ∑pq∈Sk ⟨Ip, Iq⟩ .

(11)

k

Sk

Note that we use =c and ≈c for “up to additive constant” relations. Alternatively, K-means clustering can be seen as Gaussian
model ﬁtting. Formula (5) for normal distributions with variable means mk and some ﬁxed variance

−

log N (Ip mk)

(12)

k p∈Sk

equals objective (6) up to a constant. Various extensions of objectives (6), (9), (10), (11), or (12)
lead to many powerful clustering methods such as kernel K-means, average association, and Normalized Cut, see Tab.1 and Fig.34.

1.2.1 Probabilistic K-means (pKM) and model ﬁtting
One way to generalize K-means is to replace squared Euclidean distance in (6) by other distortion measures d leading to a general distortion energy commonly used for clustering

Ip − mk d.

(13)

k p∈Sk

The optimal value of parameter mk may no longer correspond to a mean. For example, the optimal mk for non-squared L2 metric

3

(a) initialization

(b) histogram ﬁtting

(a) Input and initialization

(c) basic K-means

(d) elliptic K-means

(b) GMM ﬁtting in RGB (GrabCut without edges)

(e) GMM: local min (f) GMM: from gr. truth

(c) Normalized Cut in RGB
Fig. 2: Without edge alignment (2) iterative GMM ﬁtting [26] shows stronger data over-ﬁt compared to pairwise clustering [8].

(g) K-modes ∼ mean-shift (h) kernel K-means
Fig. 1: Model ﬁtting (pKM) (14) vs kernel K-means (kKM) (22). Histogram ﬁtting converges in one step assigning initially dominant bin label (a) to all points in the bin (b): energy (14,15) is minimal at any volume-balanced solution with one label inside each bin [27]. Basic and elliptic K-means (one mode GMM) under-ﬁt the data (c,d). Six mode GMMs over-ﬁt (e) as in (b). GMMs have local minima issues; ground-truth initialization (f) yields lower energy (14,15). Kernel K-means (21,22) with Gaussian kernel k in (h) outperforms pKM with distortion k in (g) related to K-modes or mean-shift (weak kKM, see Sec.1.2.3).
is a geometric median. For exponential distortions the optimal mk may correspond to modes [29], [30], see Appendix B.
A seemingly different way to generalize K-means is to treat both means and covariance matrices for the normal distributions in (12) as variables. This corresponds to the standard elliptic Kmeans [31], [32], [12]. In this case variable model parameters θk = {mk, Σk} and data points Ip are not in the same space. Yet, it is still possible to present elliptic K-means as distortion clustering (13) with “distortion” between Ip and θk deﬁned by an operator ⋅ − ⋅ d corresponding to a likelihood function
Ip − θk d ∶= − log N (Ip θk).

clustering (13) generalizes to ML model ﬁtting objective

Ip − θk d ≡ −

log P (Ip θk) (14)

k p∈Sk

k p∈Sk

which is (5) with explicit model parameters θk. This formulation suggests probabilistic K-means1 (pKM) as a good idiomatic name for ML model ﬁtting or distortion clustering (13), even though the corresponding parameters θk are not “means”, in general.
Probabilistic K-means (14) is used in image segmentation with models such as elliptic Gaussians [31], [32], [12], gamma/exponential [25], or other generative models [33]. ZhuYuille [23] and GrabCut [26] use pKM with highly descriptive probability models such as GMM or histograms. Information theoretic analysis in [27] shows that in this case pKM objective (14) reduces to the standard entropy criterion for clustering

Sk ⋅ H(Sk)

(15)

k

where H(Sk) is the entropy of the distribution for {Ip p ∈ Sk}. Intuitively, minimization of the entropy criterion (15) favors
clusters with tight or “peaked” distributions. This criterion is
widely used in categorical clustering [34] and decision trees [35],
[36] where the entropy evaluates histograms over “naturally” dis-
crete features. However, the entropy criterion with either discrete
histograms or continuous GMM densities has limitations in the context of continuous feature spaces, see Appendix C. Iterative
ﬁtting of descriptive models is highly sensitive to local minima
[14], [37] and easily over-ﬁts even low dimentional features in R2 (Fig.1b,e) or in R3 (RGB colors, Fig.2b). This may explain why this approach to clustering is not too common in the learning

Similar distortion measures can be deﬁned for arbitrary probability distributions with any variable parameters θk. Then, distortion

1. The name probabilistic K-means in the general clustering context was coined by [27]. They formulated (14) after representing distortion energy (13) as ML ﬁtting of Gibbs models Z1d e− x−m d for arbitrary integrable metrics.

community. As proposed in (1), instead of entropy criterion we will combine MRF regularization with general pairwise clustering objectives EA widely used for balanced partitioning of arbitrary high-dimensional features [8].

1.2.2 Kernel K-means (kKM) and pairwise clustering
This section reviews pairwise extensions of K-means (11) such as kernel K-means (kKM) and related clustering criteria. In machine learning, kKM is a well established data clustering technique [42], [43], [44], [40], [45], [46] that can identify non-linearly separable structures. In contrast to pKM based on complex models, kKM corresponds to complex (nonlinear) mappings

φ ∶ RN → H
embedding data {Ip p ∈ Ω} ⊂ RN as points φp ≡ φ(Ip) in a higher-dimensional Hilbert space H. The original non-linear problem can often be solved by simple linear separators of the embedded points {φp p ∈ Ω} ⊂ H. Kernel K-means corresponds to the basic K-means (6) in the embedding space

F (S, m) =

φp − mk 2.

(16)

k p∈Sk

Optimal segment centers mk corresponding to the means µSk = ∑q∈SSkk φq . (17)

reduce (16) to kKM energy of the single variable S similar to (9)

F (S) =

φp − µSk 2.

(18)

k p∈Sk

Similarly to (10) and (11) one can write pairwise clustering
criteria equivalent to (18) based on Euclidean distances φ(Ip) − φ(Iq) or inner products ⟨φ(Ip), φ(Iq)⟩, which are commonly represented via kernel function k(x, y)

k(x, y) ∶= ⟨φ(x), φ(y)⟩.

(19)

The (non-linear) kernel function k(x, y) corresponds to the inner product in H. It also deﬁnes Hilbertian metric2

x−y

2 k

∶=

φ(x) − φ(y) 2

≡ k(x, x) + k(y, y) − 2k(x, y) (20)

isometric to the Euclidean metric in the embedding space. Then,

pairwise formulations (10) and (11) for K-means in the embedding

space (18) can be written with respect to the original data points

using isometric kernel distance

2 k

in

(20)

∑pq∈Sk

Ip − Iq

2 k

F (S) ≡

2 Sk

(21)

k

or using kernel function k in (19)

F (S) =c − ∑pq∈Sk k(Ip, Iq) .

(22)

k

Sk

The deﬁnition of kernel k in (19) requires embedding φ. Since pairwise objectives (21) and (22) are deﬁned for any kernel function in the original data space, it is possible to formulate kKM by directly specifying an afﬁnity function or kernel k(x, y) rather

2. Such metrics can be isometrically embedded into a Hilbert space [47].

4

than embedding φ(x). This is typical for kKM explaining why the method is called kernel K-means rather than embedding K-means3.
Given embedding φ, kernel function k deﬁned by (19) is positive semi-deﬁnite (p.s.d), that is k(x, y) ≥ 0 for any x, y. Moreover, Mercer’s theorem [50] states that p.s.d. condition for any given kernel k(x, y) is sufﬁcient to guarantee that k(x, y) is an inner product in some Hilbert space. That is, it guarantees existence of some embedding φ(x) such that (19) is satisﬁed. Therefore, kKM objectives (18), (21), (22) are equivalently deﬁned either by embeddings φ or p.s.d. kernels k. Thus, kernels are
commonly assumed p.s.d. However, as discussed later, pairwise
clustering objective (22) is also used with non p.s.d. afﬁnities. To optimize kKM objectives (18), (21), (22) one can use the
basic KM procedure (8) iteratively minimizing mixed objective (16) explicitly using embedding φ

explicit kKM procedure

Sp ← arg min φp − µSk

(23)

k

t

where µSk is the mean (17) for current segment Stk. Equivalently, t
this procedure can use kernel k instead of φ. Indeed, as in Section 8.2.2 of [51], the square of the objective in (23) is

2

′

2

φp′φStk Stk′φ′φStk

φ¨p¨ − 2φp µSk + µSk

¨

t

t

= −2 Sk +

Sk 2

t

t

where we use segment Sk as an indicator vector, embedding φ

as an embedding matrix φ ∶= [φp] where points φp ≡ φ(Ip) are columns, and ′ denotes the transpose. Since the crossed term is a

constant at p, the right hand side gives an equivalent objective for

computing Sp in (23). Using kernel matrix K ∶= φ′φ and indicator vector 1p for element p we get

⎛ imkKplMicit ⎞ Sp ← arg min Stk′KStk − 2 1p′KStk (24)

⎝procedure⎠

k

Stk 2

Stk

where the kernel matrix is directly determined by kernel k

Kpq ≡ φ′pφq = ⟨φp, φq⟩ = k(Ip, Iq).
Approach (24) has quadratic complexity O( Ω 2) iterations. But, it avoids explicit high-dimensional embeddings φp in (23) replacing them by kernel k in all computations, a.k.a. the kernel trick.
Note that the implicit kKM procedure (24) is guaranteed to decrease pairwise kKM objectives (21) or (22) only for p.s.d. kernels. Indeed, equation (24) is derived from the standard greedy K-means procedure in the embedding space (23) assuming kernel (19). The backward reduction of (24) to (23) can be done only for p.s.d. kernels k when Mercer’s theorem guarantees existence of some embedding φ such that k(Ip, Iq) = ⟨φ(Ip), φ(Iq)⟩.
Pairwise energy (21) helps to explain the positive result for kKM with common Gaussian kernel k = exp −(I2pσ−2Iq)2 in Fig.1(h). Gaussian kernel distance (red plot below)
Ip − Iq 2k ∝ 1 − k(Ip, Iq) = 1 − exp −(Ip2σ−2Iq)2 (25)
is a “robust” version of Euclidean metric (green) in basic K-means (10). Thus, Gaussian kKM ﬁnds clusters with small local variances, Fig.1(h). In contrast, basic K-means (c) tries to ﬁnd good clusters with small global variances, which is impossible for non-compact clusters.

3. This could be a name for some clustering techniques constructing explicit embeddings [48], [49] instead of working with pairwise afﬁnities/kernels.

5 A. basic K-means (KM) (e.g. [28])

∑k ∑p∈Sk Ip − µSk 2

∑pq∈Sk Ip−Iq 2

= ∑k

2 Sk

c

∑pq∈Sk ⟨Ip,Iq ⟩

= − ∑k

Sk

c
=

− ∑k ∑p∈Sk ln N (Ip µSk )

Variance criterion ∑k Sk ⋅ var(Sk)

B. probabilistic K-means (pKM) equivalent energy formulations:
∑k ∑p∈Sk Ip − θk d = − ∑k ∑p∈Sk ln P(Ip θk)
related examples: elliptic K-means [31], [32] geometric model ﬁtting [12] K-modes [29] or mean-shift [39] (weak kKM) Entropy criterion ∑k Sk ⋅ H(Sk) [23], [26]
for highly descriptive models (GMMs, histograms)

C. kernel K-means (kKM) equivalent energy formulations:

∑k ∑p∈Sk φ(Ip) − µSk 2

∑pq∈Sk

Ip −Iq

2 k

= ∑k

2 Sk

c

∑pq∈Sk k(Ip,Iq )

= − ∑k

Sk

related examples:

Average Association or Distortion [38]

Average Cut [8]

Normalized Cut [8], [40] (weighted kKM)

Gini criterion ∑k Sk ⋅ G(Sk) [35], [41]
for small-width normalized kernels (see Sec.5.1)

TABLE 1: K-means and related clustering criteria: Basic K-means (A) minimizes clusters variances. It works as Gaussian model ﬁtting. Fitting more complex models like elliptic Gaussians [31], [32], [12], exponential distributions [25], GMM or histograms [23], [26] corresponds to probabilistic K-means [27] in (B). Pairwise clustering via kernel K-means (C) using more complex data representation.

Average association (AA) or distortion (AD): Equivalent

pairwise objectives (21) and (22) suggest natural extensions of

kKM. For example, one can replace Hilbertian metric

2 k

in

(21) by an arbitrary zero-diagonal distortion matrix D = [Dpq]

generating average distortion (AD) energy

Ead(S) ∶= ∑pq∈Sk Dpq

(26)

k 2 Sk

reducing to kKM energy (21) for Dpq = Ip − Iq 2k. Similarly, p.s.d. kernel k in (22) can be replaced by an arbitrary pairwise
similarity or afﬁnity matrix A = [Apq] deﬁning standard average association (AA) energy

Eaa(S) ∶= − ∑pq∈Sk Apq

(27)

k

Sk

reducing to kKM objective (22) for Apq = k(Ip, Iq). We will also use association between any two segments Si and Sj

assoc(Si, Sj) ∶=

Apq ≡ Si′ASj

(28)

p∈S i ,q ∈S j

allowing to rewrite AA energy (27) as

Eaa(S) ≡ − assoc(Sk, Sk) ≡ − Sk′ASk (29)

k

Sk

k 1′Sk

The matrix expressions in (28) and (29) represent segments Sk as indicator vectors such that Spk = 1 iff Sp = k and symbol ′ means a transpose. Matrix notation as in (29) will be used for all pairwise
clustering objectives discussed in this paper.
kKM algorithm (24) is not guaranteed to decrease (27) for improper (non p.s.d.) kernel matrix K = A, but general AA and AD energies could be useful despite optimization issues. However,
[38] showed that dropping metric and proper kernel assumptions
are not essential; there exist p.s.d. kernels with kKM energies

equivalent (up to constant) to AD (26) and AA (27) for arbitrary associations A and zero-diagonal distortions D, see Fig. 3.
For example, for any given afﬁnity A in (27) the diagonal shift trick of Roth et al. [38] generates the “kernel matrix”

K = A +2A′ + δ ⋅ I. (30)
For sufﬁciently large scalar δ matrix K is positive deﬁnite yielding a proper discrete kernel k(Ip, Iq) ≡ Kpq

k(Ip, Iq) ∶ χ × χ → R

for ﬁnite set χ = {Ip p ∈ Ω}. It is easy to check that kKM energy
(22) with kernel k ≡ K in (30) is equivalent to AA energy (27) with afﬁnity A, up to a constant. Indeed, for any indicator X ∈ {0, 1} Ω we have X′X = 1′X implying

X1′′KXX = 2X(1′A′XX) + 2X(′1A′X′X) + δ X1′′XX = X1′′AXX + δ.

Also, Section 3.1 uses eigen decomposition of K to construct an explicit ﬁnite-dimensional Euclidean embedding4 φp ∈ R Ω satisfying isometry (20) for any p.d. discrete kernel k ≡ K. Minimizing kKM energy (18) over such embedding isometric to

K in (30) is equivalent to optimizing (22) and, therefore, (27).

Since average distortion energy (26) for arbitrary D is equiva-

lent

to

average

association

for

A

=

−

D 2

,

it

can

also

be

converted

to

kKM with a proper kernel [38]. Using the corresponding kernel

matrix (30) and (20) it is easy to derive Hilbertian distortion

(metric) equivalent to original distortions D

Ip − Iq 2k ∶= D +2 D′ + 2δ(1 ⋅ 1′ − I). (31)

4. Mercer’s theorem is a similar eigen decomposition for continuous p.d. kernels k(x, y) giving inﬁnite-dimensional Hilbert embedding φ(x). Discrete kernel embedding φp ≡ φ(Ip) in Sec. 3.1 (56) has ﬁnite dimension Ω , which is still much higher than the dimension of points Ip, e.g. R3 for colors. Sec. 3.1 also shows lower dimensional embeddings φ˜p approximating isometry (20).

6

Fig. 3: Equivalence of pairwise clustering methods: kernel K-means (kKM), average distortion (AD), average association (AA) based on Roth et al. [38], see (30), (31). Equivalence of these methods in the general weighted case is discussed in Appendix A (Fig. 33).

For simplicity and without loss of generality, the rest of the paper assumes symmetric afﬁnities A = A′ since non-symmetric ones can be equivalently replaced by A+2A′ . However, we do not assume positive deﬁniteness and discuss diagonal shifts, if needed.
Weighted kKM and weighted AA: Weighted K-means [28] is
a common extension of KM techniques incorporating some given point weights w = {wp p ∈ Ω}. In the context of embedded points φp it corresponds to weighted kKM iteratively minimizing the
weighted version of the mixed objective in (16)

F w(S, m) ∶=

wp φp − mk 2.

(32)

k p∈Sk

Optimal segment centers mk are now weighted means

µwSk = ∑q∈Sk wqφq ≡ φ W Sk

(33)

∑q∈Sk wq

w′Sk

where the matrix formulation has weights represented by column vector w ∈ R Ω and diagonal matrix W ∶= diag(w). Assuming a ﬁnite dimensional data embedding φp ∈ Rm this formulation uses embedding matrix φ ∶= [φp] with column vectors φp. This
notation implies two simple identities used in (33)

wq ≡ w′Sk and

wqφp ≡ φW Sk. (34)

q∈Sk

q∈Sk

Inserting weighted means (33) into mixed objective (32) produces a pairwise energy formulation for weighted kKM similar to (22)

F w(S) ∶=

wp φp − µwSk 2

(35)

k p∈Sk

=c − ∑pq∈Sk wpwqKpq

(36)

k

∑p∈Sk wp

≡ − Sk′W K WSk

k

w′Sk

where p.s.d kernel matrix K = φ′φ corresponds to the dot products in the embedding space, i.e. Kpq = φ′pφq.
Replacing the p.s.d. kernel with an arbitrary afﬁnity matrix A
deﬁnes a weighted AA objective generalizing (27) and (29)

w

Sk′W AW Sk

Eaa(S) ∶= −

w′Sk .

(37)

k

Weighted AD can also be deﬁned. Equivalence of kKM, AA, and AD in the general weighted case is discussed in Appendix A.
Other pairwise clusteing criteria: Besides AA there are many other standard pairwise clustering criteria deﬁned by afﬁnity matrices A = [Apq]. For example, Average Cut (AC)

Eac(S) ∶= =

assoc(Sk, S¯k) ≡

k

Sk

Sk′(D − A)Sk

k

1′Sk

Sk′A(1 − Sk)

k

1′Sk

(38)

where D ∶= diag(d) is a degree matrix deﬁned by node degrees vector d ∶= A1. The formulation on the last line (38) comes from the following identity valid for Boolean X ∈ {0, 1} Ω

X′DX = X′d.

Normalized Cut (NC) [8] in (39) is another well-known pairwise clustering criterion. Both AC and NC can also be reduced to kKM [38], [52], [40]. However, spectral relaxation [8] is more common optimization method for pairwise clustering objectives than iterative kKM procedure (24). Due to popularity of NC and spectral relaxation we review them in a dedicated Section 1.3.

1.2.3 Pairwise vs. pointwise distortions
Equivalence of kKM to pairwise distortion criteria (26) helps to juxtapose kernel K-means with probabilistic K-means (Sec.1.2.1)

from one more point of view. Both methods generalize the basic

K-means (6), (10) by replacing the Euclidean metric with a more

general distortion measure d. While pKM uses “pointwise” for-

mulation (13) where d measures distortion between a point and

a model, kKM uses “pairwise” formulation (21) where

d=

2 k

measures distortion between pairs of points.

These two different formulations are equivalent for Euclidean

distortion (i.e. basic K-means), but the pairwise approach is strictly

stronger than the pointwise version using the same Hilbertian

distortion

d=

2 k

in

non-Euclidean

cases

(see

Appendix

B).

The corresponding pointwise approach is often called weak kernel

K-means. Interestingly, weak kKM with standard Gaussian kernel

can be seen as K-modes [29], see Fig. 1(g). Appendix B also de-

tails relations between K-modes and popular mean-shift clustering

[39]. An extended version of Table 1 including weighted KM and

weak kKM techniques is given in Figure 34.

1.3 Overview of Normalized Cut and spectral clustering
Section 1.2.2 has already discussed kKM and many related pairwise clustering criteria based on speciﬁed afﬁnities A = [Apq]. This section is focused on a related pairwise clustering method, Normalized Cut (NC) [8]. Shi and Malik [8] also popularized pairwise clustering optimization via spectral relaxation, which is different from iterative K-means algorithms (23) (24). Note that there are many other popular optimization methods for different clustering energies using pairwise afﬁnities [53], [54], [55], [56], [57], which are outside the scope of this work.

1.3.1 NC objective and its relation to AA and kKM Normalized Cut (NC) energy [8] can be written as

Enc(S) ∶= − assoc(Sk, Sk) ≡ − Sk′ASk (39)

k assoc(Ω, Sk)

k 1′ASk

where association (28) is deﬁned by a given afﬁnity matrix A.
The second matrix formulation above shows that the difference
between NC and AA (29) is in the normalization. AA objective uses denominator 1′Sk ≡ Sk , which is k-th segment’s size. NC (39) normalizes by weighted size. Indeed, using d ∶= A′1

1′ASk ≡ d′Sk ≡

dp

p∈Sk

where weights d = {dp p ∈ Ω} are node degrees

dp ∶= Apq.

(40)

q∈Ω

For convenience, NC objective can be formatted similarly to (29)

S k ′ AS k

Enc(S) ≡ − d′Sk .

(41)

k

For some common types of afﬁnities where dp ≈ const, e.g. K-nearest neighbor (KNN ) graphs, NC and AA objectives
(41) and (29) are equivalent. More generally, Bach & Jordan [52],
Dhillon et al. [40] showed that NC objective can be reduced to
weighted AA or kKM with speciﬁc weights and afﬁnities.
Our matrix notation makes equivalence between NC (41) and
weighted AA (37) straightforward. Indeed, objective (41) with afﬁnity A coincides with (37) for weights w and afﬁnity A˜

w = d = A′1 and A˜ = W −1AW −1.

(42)

7

The weighted version of kKM procedure (24) detailed in Appendix A minimizes weighted AA (37) only for p.s.d. afﬁnities, but positive deﬁniteness of A is not critical. For example, an extension of the diagonal shift (30) [38] can convert NC (41) with arbitrary (symmetric) A to an equivalent NC objective with p.s.d. afﬁnity

K=A+δ⋅D

(43)

using degree matrix D ∶= diag(d) ≡ W and sufﬁciently large δ. Indeed, for indicators X ∈ {0, 1} Ω we have X′DX = d′X and

Xd′′KXX = Xd′′AXX + δ Xd′′DXX = Xd′′AXX + δ.

Positive deﬁnite K (43) implies p.d. afﬁnity (42) of weighted AA

K˜ = D−1KD−1 = D−1AD−1 + δD−1.

(44)

The weighted version of kKM procedure (24) for this p.d. kernel [58] greedily optimizes NC objective (41) for any (symmetric) A.

1.3.2 Spectral relaxation and other optimization methods

There are many methods for approximate optimization of NP-hard pairwise clustering energies besides greedy K-mean procedures. In particular, Shi, Malik, and Yu [8], [59] popularized spectral relaxation methods in the context of normalized cuts. Such methods also apply to AA and other problems [8]. For example, similarly to [59] one can rewrite AA energy (27) as

Eaa(S) = − tr(Z′AZ)

for Z ∶= ⎡⎢⎢⎢. . . , ⎢⎣

Sk , . . . ⎤⎥⎥ Sk ⎥⎥⎦

where Z is a Ω × K matrix of normalized indicator vectors Sk. Orthogonality (Si)′Sj = 0 implies Z′Z = IK where IK is an identity matrix of size K × K. Minimization of the trace energy above with relaxed Z constrained to a “unit sphere” Z′Z = IK is a
simple representative example of spectral relaxation in the context

of AA. This relaxed trace optimization is a generalization of

Rayleigh quotient problem that has an exact closed form solution

in terms of K largest eigenvectors for matrix A. This approach

extends to general multi-label weighted AA and related graph

clustering problems, e.g. AC and NC [8], [59]. The main computa-

tional difﬁculties for spectral relaxation methods are explicit eigen

decomposition for large matrices and integrality gap - there is a

ﬁnal heuristics-based discretization step for extracting an integer

solution for the original combinatorial problem from an optimal

relaxed solution. For example, one basic discretization heuristic is

to run K-means over the row-vectors of the optimal relaxed Z.

Other optimization techniques are also used for pairwise

clustering. Buhmann et al. [60], [38] address the general AD

and AA energies via mean ﬁeld approximation and deterministic annealing. It can be seen as a fuzzy version5 of the kernel K-

means algorithm. In the context of normalized cuts Dhillon et al.

[40], [58] use spectral relaxation to initialize kKM algorithm.

In computer vision it is common to combine various con-

straints or energy terms into one objective function. Similar efforts

were made in the context of pairwise clustering as well. For exam-

ple, to combine kKM or NC objectives with Potts regularization

[62] normalizes the corresponding pairwise constraints by cluster

sizes. This alters the Potts model to ﬁt the problem to a standard

trace-based formulation.

Adding non-homogeneous linear constraints into spectral re-

laxation techniques also requires approximations [63] or model

5. Fuzzy version of K-means in Duda et al. [61] generalizes to kKM.

modiﬁcations [64]. Exact optimization for the relaxed quadratic ratios (including NC) with arbitrary linear equality constraints is possible by solving a sequence of spectral problems [65].
Our bound optimization approach allows to combine many standard pairwise clustering objectives with any regularization terms with existing solvers. In our framework such pairwise clustering objectives are interpreted as high-order energy terms approximated via linear upper bounds during optimization.
1.4 Our approach summary
We propose to combine standard pairwise clustering criteria such as AA, AC, or NC with standard regularization constraints such as geometric or MRF priors. To achieve this goal, we propose unary (linear) bounds for the pairwise clustering objectives that are easy to integrate into any standard regularization algorithms. Below we summarize our motivation and our main technical contributions.
1.4.1 Motivation and Related work
Due to signiﬁcant differences in existing optimization methods, pairwise clustering (e.g. NC) and Markov Random Fields (MRF) techniques are used separately in many applications of vision and learning. They have complementary strengths and weaknesses.
For example, NC can ﬁnd a balanced partitioning of data points from pairwise afﬁnities for high-dimensional features [8], [66], [67]. In contrast, discrete MRF as well as continuous regularization methods commonly use probabilistic K-means [27] or model ﬁtting to partition image features [24], [23], [26], [12]. Clustering data by ﬁtting simple parametric models seems intuitively justiﬁed when data supports such simple models, e.g. Gaussians [24] or lines/planes [12]. But, clustering arbitrary data by ﬁtting complex models like GMM or histograms [23], [26] seems like an idea bound to over-ﬁt the data. Indeed, over-ﬁtting happens even for low dimensional color features [14], see also Fig.1(b,e) and Fig.2(b). Our energy (1) allows a general MRF framework to beneﬁt from standard pairwise clustering criteria, e.g. NC, widely used for complex data. In general, kernel-based clustering methods are a prevalent choice in the learning community as model ﬁtting (e.g. EM) becomes intractable for high dimensions. We show potent segmentation results for basic formulations of energy (1) with higher-dimensional features like RGBXY, RGBD, RGBM where standard MRF methods using model-ﬁtting fail.
On the other hand, standard NC applications can also beneﬁt from additional constraints [63], [65], [68]. We show how to add a wide class of standard MRF potentials. For example, standard NC segmentation has weak alignment to contrast edges [67]. While this can be addressed by post-processing, inclusion of the standard pair-wise Potts term [10], [4] offers a principled solution. We show beneﬁts from combining NC with lower and higher-order constraints, such as sparsity or label costs [12]. In the context of a general graph clustering, higher-order consistency terms based on a P n-Potts model [11] also give signiﬁcant improvements.
The synergy of the joint energy (1) is illustrated by juxtaposing the use of the pixel location information (XY) in standard NC and MRF techniques. The basic pairwise Potts model typically works on the nearest-neighbor pixel grids N4 or N8 where XY information helps contrast/edge alignment and enforces “smooth” segment boundaries. Wider connectivity Potts leads to denser graphs with slower optimization and poorer edge localization. In contrast, common NC methods [8] augment pixel color features with location using relatively wide bandwidth for the XY dimension. This encourages segments with spatially “compact” regions.

8
Narrower XY kernels improve edge alignment [67], but weaken regional color/feature consistency. On the other hand, very large XY kernels produce color-only clustering with spatially incoherent segments. Combining regional color consistency with spatial coherence in a single NC energy requires a compromise when selecting XY bandwidth. Our general energy (1) can separate the regional consistency (e.g. NC clustering term) from the boundary smoothness or edge alignment (e.g. Potts potential). Interestingly, it may still be useful to augment colors with XY in the NC term in (1) since XY kernel bandwidth σ allows to separate similarappearance objects at distances larger than σ, see Sec.6.2.3.
Adding high-order term EA to MRF potentials in (1) differs from fully-connected CRF [69]. Like many MRF/CRF models the method in [69] lacks balancing. It is a denser version of the pairwise Potts model giving a trivial solution without a data term. In contrast, ratio-based objectives EA are designed for balanced clustering. In fact, our unary bounds for such objectives allow to combine them with fully connected CRF or any other MRF/CRF model with known solvers, e.g. mean ﬁeld approximation [69].
Some earlier work also motivates a combination of pairwise clustering (e.g. NC) with MRF potentials like the Potts model [62]. They alter the Potts model to ﬁt it to the standard trace-based formulation of NC. Our general bound approach can combine many pairwise clustering methods with any solvable discrete or continuous regularization potentials.
1.4.2 Main contributions
We propose a new energy model (1) for multi-label image segmentation and clustering, efﬁcient bound optimization algorithms, and demonstrate many useful applications. Our preliminary results appear in [41] and [70]. Our main contributions are:
● We propose a general multi-label segmentation or clustering energy (1) combining pairwise clustering (e.g. NC) with standard second or higher-order MRF regularization potentials. A pairwise clustering term can enforce balanced partitioning of observed image features and MRF terms can enforce many standard regularization constraints.
● We obtain kernel (exact) and spectral (approximate) bounds for common pairwise clustering criteria providing two auxiliary functions for joint energy (1). In the context of standard MRF potentials (e.g. Potts, robust P n-Potts, label cost) we propose move-making algorithms for energy (1) generalizing α-expansion and αβ-swap moves6.
● For our spectral bound7 we derive a new low-dimensional data embedding φp minimizing isometry errors w.r.t. afﬁnities. These optimal embeddings complete the gap between standard KM discretization heuristics in spectral relaxation [8] and the exact kKM formulations [52], [40].
● Our experiments demonstrate that typical NC applications beneﬁt from extra MRF constraints, as well as, MRF segmentation beneﬁt from the high-order NC term encouraging balanced partitioning of image features. In particular, our NC+MRF framework works for higher-dimensional features (e.g. RGBXY, RGBD, RGBM) where standard model-ﬁtting clustering [23], [26], [12] fails.
6. Our bounds for pairwise criteria can be also integrated into auxiliary functions with other standard regularization potentials (truncated, cardinality, TV) addressed by discrete (e.g. message passing, relaxations, mean-ﬁeld approximations) or continuous (e.g. convex, primal-dual) algorithms.
7. Our term spectral bound means “spectral” auxiliary function in the context of bound optimization, not to be confused with bounds on eigenvalues.

at(S)

at+1(S)

(I)

E(S)

(II)

9
(I) at(S) (II)
F(S) at+1(S)

St

St+1

St

St+1

Fig. 4: Iteration t of the general bound optimization procedure for
function E(S) using auxiliary functions (bounds) at(S). Step I minimizes at(S). Step II computes the next bound at+1(S).

The rest of the paper is organized as follows. Sections 2 and 3 present our kernel and spectral bounds for (1) and detail combinatorial move making graph cut algorithms for its optimization. Section 4 discusses a number of extensions for our bound optimization approach. Sections 5 analyses kernel and bandwidth selection strategies. Section 6 presents many experiments where either pairwise clustering criteria beneﬁt from the additional MRF constraints or common MRF formulations beneﬁt from an additional clustering term for various high-dimensional features.

2 KERNEL BOUNDS
First, we review the general bound optimization principle and present basic K-means as an example. Section 2.2 derives kernel bounds for standard pairwise clustering objectives. Without loss of generality, we assume symmetric afﬁnities A = A′ since nonsymmetric ones can be equivalently replaced by A+2A′ , e.g. see (30) in Sec.1.2.2. Positive deﬁniteness of A is not assumed and diagonal shifts are discussed when needed. Move-making bound optimization for energy (1) is discussed in Section 2.3.

2.1 Bound optimization and K-means

In general, bound optimizers are iterative algorithms that optimize
auxiliary functions (upper bounds) for a given energy E(S) assuming that these auxiliary functions are more tractable than the
original difﬁcult optimization problem [71], [72], [73], [37]. Let t
be a current iteration index. Then at(S) is an auxiliary function of E(S) at current solution St if

E(S) ≤ at(S) ∀S E(St) = at(St).

(45a) (45b)

The auxiliary function is minimized at each iteration t (Fig. 4)

St+1 = arg min at(S).

(46)

S

This procedure iteratively decreases original function E(S) since

E(St+1) ≤ at(St+1) ≤ at(St) = E(St).

We show that standard KM procedures (23), (24) correspond to bound optimization for K-means objective (18). Note that variables mk in mixed objective F (S, m) (16) can be seen as

Fig. 5: K-means as linear bound optimization: As obvious from
formula (49), the bound at(S) in Theorem 1 is a unary function of S. That is, KM procedures (23,24) correspond to optimization
of linear auxiliary functions at(S) for KM objectives. Optimum St+1 is ﬁnite since optimization is over Boolean Sk ∈ {0, 1} Ω .

relaxations of segment means µSk (17) in single-variable KM objective F (S) (18) since

µSk = arg min

φp − mk 2

mk p∈Sk

and F (S) = min F (S, m).

(47)

m

Theorem 1 (bound for KM). Standard iterative K-means procedures (23,24) can be seen as bound optimization methods for K-means objectives F (S) (18,22) using auxiliary function

at(S) = F (S, µt)

(48)

at any current segmentation St = {Stk} with means µt = {µSk }. t

Proof. Equation (47) implies at(S) ≥ F (S). Since at(St) = F (St) then at(S) is an auxiliary function for F (S). Resegmentation step (23) gives optimal segments St+1 minimizing
the bound at(S). The re-centering step minimizing F (St+1, m) for ﬁxed segments gives means µt+1 deﬁning bound at+1(S) for the next iteration. These re-segmentation (I) and re-centering (II)
steps are also illustrated in Fig. 4 and Fig. 5.

Theorem 1 could be generalized to probabilistic K-means [27]

by stating that block-coordinate descent for distortion clustering

or ML model ﬁtting (14) is a bound optimization [37], [41].

Theorem 1 can also be extended to pairwise and weighted versions

of KM. For example, one straightforward extension is to show that

F

w

(

S

,

µ

w t

)

(32)

with

weighted

means

µwt

=

{µwSk }

(33)

is

a

bound

for

weighted

KM

objective

F

w (S )

(35),

t
e.g. see

Th.

6

in

Appendix A. Then, some bound for pairwise wkKM energy (36)

can also be derived (Cor. 1, Appendix A). It follows that bounds

can be deduced for many pairwise clustering criteria using their

reductions to various forms of kKM reviewed in Sec.1.2.2 or 1.3.1.

Alternatively, the next Section 2.2 follows a more direct and in-

tuitive approach to deriving pairwise clustering bounds motivated

by the following simple observation. Note that function at(S) in Theorem 1 is unary with respect to S. Indeed, functions F (S, m) (16) or F w(S, m) (32) can be written in the form

F (S, m) ≡

φp − mk 2 Spk

(49)

kp

F w(S, m) ≡

wp φp − mk 2 Spk

(50)

kp

highlighting the sum of unary terms for variables Spk. Thus, bounds for KM or weighted KM objectives are modular (linear)

10

objective EA(S)

matrix formulation e(X) in ∑k e(Sk)

concave relaxation e(X) (51)

K and w in Lemma 1

kernel bound for EA(S) at St

AA (29)

X ′ AX
− 1′X

X ′ (δ I+A)X − 1′X

K = δI + A, w = 1

AC (38) NC (41)

X ′ (D−A)X 1′ X
X ′ AX
− d′X

X ′ (δ I+A−D)X

−

1′ X

K = δI + A − D, w = 1

X ′ (δ D+A)X

−

d′ X

K = δD + A, w = d

∑k ∇e(Stk)′ Sk + const for ∇e in (52)

TABLE 2: Kernel bounds for different pairwise clustering objectives EA(S). The second column shows formulations of these objectives EA(S) ≡ ∑k e(Sk) using functions e over segment indicator vectors Sk ∈ {0, 1} Ω . The last column gives a unary (linear) upper bound for EA(S) at St based on the ﬁrst-order Taylor approximation of concave relaxation function e ∶ RΩ → R1 (51).

function of S. This simple technical fact has several useful implications that were previously overlooked. For example,
● in the context of bound optimization, KM can be integrated with many regularization potentials whose existing solvers can easily work with extra unary (linear) terms
● in the context of real-valued relaxation of indicators Sk, linearity of upper bound at(S) (48) implies that the bounded function F (S) ∈ C1 (22) is concave, see Fig.5.
In Section 2.2 we conﬁrm that many standard pairwise clustering objectives in Sections 1.2.2 and 1.3.1 have concave relaxations. Thus, their linear upper bounds easily follow from the corresponding ﬁrst-order Taylor expansions, see Figure 5 and Table 2.

2.2 Kernel Bounds for AA, AC, and NC

The next lemma is needed to state a bound for our joint energy (1) in Theorem 2 for clustering terms AA, AC, or NC in Table 2.

Lemma 1 (concave relaxation). Consider function e ∶ RΩ → R1 deﬁned by matrix K and vector w as
e(X) ∶= − Xw′K′XX . (51)

Function e(X) above is concave over region w′X > 0 assuming that (symmetric) matrix K is positive semi-deﬁnite. (See Fig. 6)

Proof. Lemma 1 follows from the following expression for the

Hessian of function e for symmetric K

∇2∇e = − wK′X + KX(ww′′+Xw)2X′K − w(Xw′′KXX)3w′

1

Xw′ ′

Xw′

≡ − w′X I − w′X K I − w′X

which is negative semi-deﬁnite for w′X > 0 for p.s.d. K. The ﬁrst-order Taylor expansion at current solution Xt
Tt(X) ∶= e(Xt) + ∇e(Xt)′ (X − Xt) is a bound for the concave function e(X) (51). Its gradient8
∇e(Xt) = w (Xwt′′XKXt)2t − KXt w′2Xt (52)

8. Function e and gradient ∇e are deﬁned only at non-zero indicators Xt where w′Xt > 0. We can formally extend e to X = 0 and make the bound Tt work for e at Xt = 0 with some supergradient. However, Xt = 0 is not a problem in practice since it corresponds to an empty segment.

Fig.

6:

Example:

concave

function

e(X )

=

−

X′X 1′ X

for

X

∈

[0, 1]2.

Note that convexity/concavity of similar rational functions with

quadratic enumerator and linear denominator is known in other

optimization

areas,

e.g.

[74,

p.72]

states

convexity

of

x2 y

for

y

>

0

and

[75,

exercise

3.14]

states

convexity

of

(v′ X )2 w′X

for

w′X

> 0.

implies linear bound Tt(X) for concave function e(X) at Xt

Tt(X) ≡ ∇e(Xt)′ X.

(53)

As shown in the second column of Table 2, common pairwise clustering objectives deﬁned by afﬁnity matrix A such as AA (29), AC (38), and NC (41) have the form
EA(S) = e(Sk)
k
with function e(X) as in (51) from Lemma 1. However, arbitrary afﬁnity A may not correspond to a positive semi-deﬁnite K in (51) and e(X) may not be concave for X ∈ R Ω . However, the diagonal shift trick [38] in (30) works here too. The third column in Table 2 shows concave function e(X) that equals e(X) for any non-zero Boolean X ∈ {0, 1} Ω , up to a constant. Indeed, for AA
e(X) = − X′(δ1I′+XA)X = − X1′′AXX − δ =c e(X)
since X′X = 1′X for Boolean X. Clearly, δI + A is p.s.d. for sufﬁciently large δ and Lemma 1 implies that the ﬁrst-order Taylor expansion Tt(X) (53) is a linear bound for concave function e(X). Equivalence between e and e over Booleans allows to

Algorithm 1: α-Expansion for Kernel Cut
Input : Afﬁnity Matrix A of size Ω × Ω ; initial labeling S01, ..., S0K Output: S1, ..., SK : partition of the set Ω Find p.s.d. matrix K as in Table 2. Set t ∶= 0; while not converged do
Set at(S) to be kernel bound (55) at current partition St; for each label α ∈ L = {1, ..., K} do
Find St ∶= arg min at(S) within one α expansion of St; end Set t ∶= t + 1; end

Energy

-12.5 -13
-13.5 -14
-14.5

11
,-expansion ,-expansion* ,--swap

use Tt(X) as a bound for e when optimizing over indicators X. Function e ∶ R Ω → R1 can be described as a concave relaxation of the high-order pseudo-boolean function e ∶ {0, 1} Ω → R1.
Concave relaxation e for AC in Table 2 follows from the same
diagonal shift δI as above. But NC requires diagonal shift δD
with degree matrix D = diag(d) as in (43). Indeed,

e(X) = − X′(δDd′X+ A)X = − Xd′′AXX − δ =c e(X) (54)

since X′DX ≡ X′diag(d)X = d′X for any Boolean X. Clearly, δD + A is p.s.d. for sufﬁciently large δ assuming dp > 0 for all p ∈ Ω. Concave relaxations and the corresponding Taylor-based bounds for EA(S) in Table 2 imply the following theorem.

Theorem 2 (kernel bound). For any (symmetric) afﬁnity matrix

A and any current solution St the following is an auxiliary function for energy (1) with any clustering term EA(S) from Tab.2

at(S) = ∇e(Stk)′ Sk + γ Ec(Sc) (55)

k

c∈F

where e and ∇e are deﬁned in (51), (52) and δ is large enough so that the corresponding K in Table 2 is positive semi-deﬁnite.

2.3 Move-making algorithms
Combination (55) of regularization potentials with a unary/linear bound ∑k ∇e(Stk)′ Sk for high-order term EA(S) can be optimized with many standard discrete or continuous multi-label methods including graph cuts [10], [76], message passing [77], LP relaxations [17], or well-known continuous convex formulations [19], [20], [21]. We focus on MRF regularizers (see Sec.1.1) commonly addressed by graph cuts [10]. We discuss some details of kernel bound optimization technique using such methods.
Step I of the bound optimization algorithm (Fig.4) using auxiliary function at(S) (55) for energy E(S) (1) with regularization potentials reviewed in Sec.1.1 can be done via move-making methods [10], [11], [12]. Step II requires re-evaluation of the ﬁrst term in (55), i.e. the kernel bound for EA. Estimation of gradients ∇e(Stk) in (52) has complexity O(K Ω 2).
Even though the global optimum of at at step I (Fig.4) is not guaranteed for general potentials Ec, it sufﬁces to decrease the bound in order to decrease the energy, i.e. (45a) and (45b) imply
at(St+1) ≤ at(St) ⇒ E(St+1) ≤ E(St).
For example, Algorithm 1 shows a version of our kernel cut algorithm using α-expansion [10] for decreasing bound at(S) in (55). Other moves are also possible, for example αβ-swap.
In general, tighter bounds work better. Thus, we do not run iterative move-making algorithms for bound at until convergence before re-estimating at+1. Instead, one can reestimate the bound either after each move or after a certain number of moves. One should decide the order of iterative move making and bound

-15 0

100

200

300

400

500

600

Time (s)

(a) Versions of Kernel Cut

Compare αexpansion αexpansion

against αβ-swap
α-expansion∗

# of wins 135/200 182/200‡

p-value† 10−6
10−34‡

† The probability to exceed the given number of wins by random chance. ‡ The algorithm stopped due to time limit (may cause incorrect number of wins).

(b) BSDS500 training dataset

Fig. 7: Typical energy evolution wrt different moves and frequency of bound updates. α-expansion updates the bound after a round of expansions, α-expansion* updates the bound after each expansion move. Initialization is a regular 5×5 grid of patches.

evaluation. In the case of α-expansion, there are at least three options: updating the bound after a single expansion step, or after a single expansion loop, or after the convergence of α-expansion. More frequent bound recalculation slows down the algorithm, but makes the bound tighter. The particular choice generally depends on the trade-off between the speed and solution quality. However, in our experiments more frequent update does not always improve the energy, see Fig.7. We recommend updating the bound after a single loop of expansions, see Alg.1. We also evaluated a swap move version of our kernel cut method with bound re-estimation after a complete αβ-swaps loop, see Fig.7.
3 DATA EMBEDDINGS AND SPECTRAL BOUNDS
This section shows a different bound optimization approach to pairwise clustering and joint regularization energy (1). In contrast to the bounds explicitly using afﬁnity A or kernel matrices K in Sec.2.2, the new approach is based on explicit use of isometric data embeddings φ. While the general Mercer theorem guarantees existence of such possibly inﬁnite dimensional Hilbert space embedding, we show ﬁnite dimensional Euclidean embedding
φ ∶= [φp] where {φp p ∈ Ω} ⊂ R Ω
with exact isometry (19,20) to kernels K in Table 2 and lower dimensional embeddings
φ˜ ∶= [φ˜p] where {φ˜p p ∈ Ω} ⊂ Rm for m ≤ Ω
that can approximate the same isometry with any accuracy level. The embeddings use eigen decompositions of the kernels.
Explicit embeddings allow to formulate exact or approximate spectral bounds for standard pairwise clustering objectives like

AA, AC, NC. This approach is very closely related to spectral
relaxation, see Sec. 3.3. For example, optimization of our approximate spectral bounds for m = K is similar to standard discretization heuristics using K-means over eigenvectors [8].
Thus, our bound optimization framework provides justiﬁcation for
such heuristics. More importantly, our spectral bounds allow to
optimize joint energy (1) combing pairwise clustering objectives
with common regularization terms.
Spectral bound is a useful alternative to kernel bounds in
Sec. 2.2 with different complexity and other numerical properties.
In particular, spectral bound optimization using lower dimensional Euclidean embeddings φ˜ for m ≪ Ω is often less sensitive to local minima. This may lead to better solutions, even though such φ˜ only approximately isometric to given pairwise afﬁnities. For m = Ω the spectral bound is mathematically equivalent to the kernel bound, but numerical representation is different.

12
(a) decomposition K = V ′ΛV
(b) decomposition K˜ = (V m)′ΛV m for m < Ω

3.1 Exact and approximate embeddings φ for kKM
This section uses some standard methodology [78] to build the ﬁnite-dimensional embedding φp ≡ φ(Ip) with exact or approximate isometry (19,20) to any given positive deﬁnite kernel k over ﬁnite data set {Ip p ∈ Ω}. As discussed in Sec. 1.2.2, kKM and other pairwise clustering methods are typically deﬁned by afﬁnities/kernels k and energy (22) rather than by high-dimensional embeddings φ with basic KM formulation (18). Nevertheless, data embeddings φp could be useful and some clustering techniques explicitly construct them [8], [79], [38], [48], [52], [49]. In particular, if dimensionality of the embedding space is relatively low then the basic iterative KM procedure (23) minimizing (18) could be more efﬁcient than its kernel variant (24) for quadratic formulation (22). Even when working with a given kernel k it may be algorithmically beneﬁcial to build the corresponding isometric embedding φ. Below we discuss ﬁnite-dimensional Euclidean embeddings in Rm (m ≤ Ω ) allowing to approximate standard pairwise clustering via basic KM.
First, we show an exact Euclidean embedding isometric to a given kernel. Any ﬁnite data set {Ip p ∈ Ω} and any given kernel k deﬁne a positive deﬁnite kernel matrix9
Kpq = k(Ip, Iq)
of size Ω × Ω . The eigen decomposition of this matrix
K = V ′ΛV
involves diagonal matrix Λ with non-negative eigenvalues and orthogonal matrix V whose rows are eigenvectors, see Fig.8(a). Non-negativity of th√e eig√envalues is important for obtaining decomposition Λ = Λ ⋅ Λ allowing to deﬁne the following Euclidean space embedding
φp ∶= √ΛVp ∈ R Ω (56)
where Vp are column of V , see Fig.8(a). This embedding satisﬁes isometry (19,20) since
⟨φp, φq⟩ = (√ΛVp)′(√ΛVq) = Kpq = k(Ip, Iq).
Note that (56) deﬁnes a simple ﬁnite dimensional embedding φp ≡ φ(Ip) only for subset of points {Ip p ∈ Ω} in RN based
9. If k is given as a continuous kernel k(x, y) ∶ RN × RN → R matrix K can be seen as its restriction to ﬁnite data set {Ip p ∈ Ω} ⊂ RN .

Fig. 8: Eigen decompositions for kernel matrix K (a) and its rank m approximation K˜ (b) minimizing Frobenius errors (57) [78]. Decompositions (a,b) allow to build explicit embeddings (56,59)
isometric to the kernels, as implied by the Mercer theorem.

on a discrete kernel, i.e. matrix Kpq. In contrast, Mercer’s theorem should produce a more general inﬁnite dimensional Hilbert embedding φ(x) for any x ∈ RN by extending the eigen decomposition to continuous kernels k(x, y). In either case, however, the embedding space dimensionality is much higher than the original
data space. For example, φp in (56) has dimension Ω , which is
much larger than the dimension of data Ip, e.g. 3 for RGB colors.
Embedding (56) satisfying isometry (19,20) is not unique. For
example, any decomposition K = G′G, e.g. Cholesky [80], deﬁnes a mapping φGp ∶= Gp with desired properties. Also, rotational matrices R generate a class of isometric embeddings φRp ∶= Rφp.
It is easy to build lower dimensional embeddings by weaken-
ing the exact isometry requirements (19,20) following the standard
multi-dimensional scaling (MDS) methodology [78], as detailed below. Consider a given rank m < Ω approximation K˜ for kernel matrix K minimizing Frobenius norm errors [78]

K − K˜ F ∶=

(Kpq − K˜pq)2.

(57)

pq∈Ω

It is well known [78], [80] that the minimum Frobenius error is achieved by
K˜ = (V m)′ΛmV m
where V m is a submatrix of V including m rows corresponding to the largest m eignenvalues of K and Λm is the diagonal matrix of these eigenvalues, see Fig.8(b). The corresponding minimum Frobenius error is given by the norm of zeroed out eigenvalues

K − K˜ F = λ2m+1 + ⋅ ⋅ ⋅ + λ2Ω .

(58)

13
small enough eigenvalues contributing low weight in the distortion above. This is equivalent to K-means (16) over points (59).

(a) data {Ip p ∈ Ω} (b) Gaussian kernel matrix K

(c) 2D embedding φ˜(Ip) (d) 3D embedding φ˜(Ip)
Fig. 9: Low-dimensional Euclidean embeddings (59) for m = 2 and m = 3 in (c,d) are approximately isometric to a given afﬁnity matrix (b) over the data points in (a). The approximation error (58) decreases for larger m. While generated by standard MDS methodology [78], it is intuitive to call embeddings φ in (56) and (59) as (exact or approximate) isometry eigenmap or eigen isomap.

It is easy to check that lower dimensional embedding

φ˜p ∶= √ΛmVpm ∈ Rm

(59)

is isometric with respect to approximating kernel K˜, that is

⟨φ˜p, φ˜q⟩ = K˜pq ≈ Kpq.

(60)

Fig. 9 shows examples of low-dimensional approximate isometry embeddings (59) for a Gaussian kernel. Note that φ˜p ∈ Rm (59) can be obtained from φp ∈ R Ω (56) by selecting coordinates corresponding to dimensions of the largest m eigenvalues.
According to (58) lower dimensional embedding φ˜p in (59)
is nearly-isometric to kernel matrix K if the ignored dimensions have sufﬁciently small eigenvalues. Then (59) may allow efﬁcient
approximation of kernel K-means. For example, if sufﬁciently
many eigenvalues are close to zero then a small rank m approximation Kˆ will be sufﬁciently accurate. In this case, we can use a basic iterative K-means procedure directly in Rm with O( Ω m) complexity of each iteration. In contrast, each iteration of the standard kernel K-means (22) is O( Ω 2) in general10.
There is a different way to justify approximate lowdimensional embedding φ˜p ignoring small eigenvalue dimensions
in φp. The objective in (22) for exact kernel K is equivalent to the basic K-means (16) over points φp (56). The latter can be shown
to be equivalent to (probabilistic) K-means (13) over columns Vp
in orthonormal matrix V using weighted distortion measure

Ω 2

2

√2

Vp − µ Λ ∶= λi(Vp[i] − µ[i]) = φp − Λµ

i=1

where index [i] speciﬁes coordinates of the column vectors. Thus, a good approximation is achieved when ignoring coordinates for

10. Without KNN or other special kernel accelerations.

3.2 Spectral Bounds for AA, AC, and NC

The last Section showed that kKM clustering with given p.s.d.
kernel K can be approximated by basic KM over low-dimensional Euclidean embedding φ˜ ∈ Rm (59) with approximate isometry to K (60). Below we use equivalence of standard pairwise clustering criteria to kKM, as discussed in Sections 1.2.2 and 1.3.1, to derive

the corresponding low-dimensional embeddings for AA, AC, NC.

Then, equivalence of KM to bound optimization (Theorem 1)

allows to formulate our approximate spectral bounds for the

pairwise clustering and joint energy (1). The results of this Section

are summarized in Table 3. For simplicity, assume symmetric afﬁnity matrix A. If not, equivalently replace A by A+2A′ .
Average association (AA): Diagonal shift K = δI + A in (30)

converts AA (29) with A to equivalent kKM (22) with p.d. kernel

K. We seek rank-m approximation K˜ minimizing Frobenius error K − K˜ F . Provided eigen decomposition A = V ′ΛV , equation

(59) gives low-dimensional embedding (also in Tab. 3)

φ˜p = √δIm + ΛmVpm

(61)

corresponding to optimal approximation kernel K˜. It follows that KM (23) over this embedding approximates AA objective (22).

Note that the eigenvectors (rows of matrix V , Fig. 8) also solve the

spectral relaxation for AA in Tab. 4. However, ad hoc discretization by KM over points VpK may differ from the result for points (61).
Average cut (AC): As follows from objective (38) and diago-

nal shift (30) [38], average cut clustering for afﬁnity A is equiv-

alent to minimizing kKM objective with kernel K = δI + A − D where D is a diagonal matrix of node degrees dp = ∑q Apq. Diagonal shift δI is needed to guarantee positive deﬁniteness of

the kernel. Eigen decomposition for D − A = V ′ΛV implies K = V ′(δI − Λ)V . Then, (59) implies rank-m approximate

isometry embedding (also in Tab. 3)

φ˜p = √δIm − ΛmVpm

(62)

using the same eigenvectors (rows of V ) that solve AC’s spectral

relaxation in Tab. 4. However, standard discretization heuristic using KM over φ˜p = VpK may differ from the results for our approximate isometry embedding φ˜p (62) due to different weighting.

Normalized cut (NC): According to [40] and a simple deriva-

tion in Sec.1.3.1 normalized cut for afﬁnity A is equivalent to
weighted kKM with kernel K = δD−1 + D−1AD−1 (44) and node weights wp = dp based on their degree. Weighted kKM (36) can be interpreted as KM in the embedding space with weights wp

for each point φp as in (32,33). The only issue is computing m-

dimensional embeddings approximately isometric to K. Note that previously discussed solution φ˜ in (59) uses eigen decomposition

of matrix K to minimize the sum of quadratic errors between Kpq and approximating kernel K˜pq = ⟨φ˜p, φ˜q⟩. This solution may still be acceptable, but in the context of weighted points it seems

natural to minimize an alternative approximation measure taking

wp into account. For example, we can ﬁnd rank-m approximate afﬁnity matrix K˜ minimizing the sum of weighted squared errors

wpwq(Kpq − K˜pq)2 =

D

1 2

(K

−

K˜ )D

1 2

F.

(63)

pq∈Ω

Substituting K = δD−1 + D−1AD−1 gives an equivalent objective

1
D− 2 (δD

1
+ A)D− 2

−

D

1 2

K˜ D

1 2

F.

14

objective EA(S)

matrix formulation e(X) in ∑k e(Sk)

equivalent kKM (22,36) as in [38], [58]

eigen decomposition V ′ΛV = . . .

embedding in Rm, m ≤ Ω with approx. isometry (60)

spectral bound for EA(S) at St

AA (29) AC (38)

X ′ AX
− 1′X
X ′ (D−A)X 1′ X

K = δI + A K = δI + A − D

A D−A

φ˜p

=

√ δIm

+

Λm Vpm

(61)

φ˜p

=

√ δIm

−

Λm Vpm

(62)

F (S, µt) (49) for points φ˜p

NC (41)

X ′ AX
− d′X

K = δD−1 + D−1AD−1 weighted, wp = dp

D−

1 2

AD−

1 2

φ˜p = δImd+pΛm Vpm (66)

F w(S, µwt ) (50) for points φ˜p

TABLE 3: Spectral bounds for objectives EA(S). The third column shows p.d. kernel matrices K for the equivalent kKM energy (22).

Eigen decomposition for matrices in the forth column deﬁnes our Euclidean embedding φ˜p ∈ Rm (ﬁfth column) isometric to K (60).

Thus, K-means over φ˜p approximates kKM (22). Bounds for KM (last column) follow from Th. 1 & 6 where µt = {µSk } are means

(17)

and

µwt

=

{µwSk }

are

weighted

means

(33).

Functions

F

(S,

m)

and

F

w (S,

m)

are

modular

(linear)

w.r.t.

S,

see

t
(49,50).

t

spectral relaxation [8] common discretization heuristic [81] (embedding & K-means)

AA

Au = λu

φ˜p ∶= UpK ≡ VpK

⇐ V ′ΛV = A

AC

(D − A)u = λu

φ˜p ∶= UpK ≡ VpK

⇐ V ′ΛV = D − A

NC

(D − A)u = λDu

φ˜p ∶= UpK

≡

[V

K

D−

1 2

rn
]p

⇐

V ′ΛV

=

D−

1 2

AD−

1 2

TABLE 4: Spectral relaxation and discretization heuristics for objectives for pairwise clustering objectives EA(S) for afﬁnity A. The corresponding degree matrix D is diagonal with elements dp ∶= ∑q Apq. To extract integer labeling from the relaxed solutions produced by the eigen systems (second column), spectral methods often apply basic KM to some ad hoc data embedding φ˜ (last column) based on the ﬁrst K unit eigenvectors u, the rows of matrix U K . While our main text discusses some variants, the most basic idea [8], [81] is to use the columns of U K as embedding φ˜p. For easier comparison, the last column also shows equivalent representations of
this embedding based on the same eigen decompositions V ′ΛV as those used for our isometry eigenmaps in Tab. 3. In contrast, our
embeddings are derived from justiﬁed approximations of the original non-relaxed AA, AC, or NC objectives. Note that NC corresponds
to a weighted case of K-means with data point weights wp = dp [52], [40], see (42) in Section 1.3.1.

Consider

rank-m

matrix

M˜

∶=

D

1 2

K˜ D

1 2

as

a

new

minimization

variable. Its optimal value (V m)′(δIm + Λm)V m follows from

1

1

D− 2 (δD + A)D− 2 = V ′(δI + Λ)V for eigen decomposition

1

1

D− 2 AD− 2 ≡ V ′ΛV.

(64)

Thus, optimal rank-m approximation kernel K˜ is

K˜

=

D−

1 2

(V

m)′(δIm

+

Λm)V

mD−

1 2

.

(65)

It is easy to check that m-dimensional embedding (also in Tab. 3)

φ˜p = δImd+p Λm Vpm (66)
is isometric to kernel K˜, that is ⟨φ˜p, φ˜q⟩ = K˜pq. Therefore, weighted KM (32) over low-dimensional embedding φ˜p (66) with weights wp = dp approximates NC objective (41).
Summary: The ideas above can be summarized as follows. Assume AA, AC, or NC objectives EA(S) with (symmetric) A. The third column in Table 3 shows kernels K for equivalent kKM objectives F (S) (22,36). Following eigenmap approach (Fig.8), we ﬁnd rank-m approximate kernel K˜ ≈ K minimizing Frobenius error K˜ − K F (57) or its weighted version (63) and deduce embeddings φ˜p ∈ Rm (61), (62), (66) satisfying isometry
φ˜′pφ˜q = K˜pq ≈ Kpq.
Basic K-means objective F˜(S, m) (16,32) for {φ˜p} is equivalent to kKM energy F˜(S) (22,36) for kernel K˜ ≈ K and, therefore, approximates the original pairwise clustering objective
F˜(S, µS) =c F˜(S) ≈ F (S) =c EA(S).

Theorem 1 gives unary (linear) bound F˜(S, µt) (49,50) for objective F˜(S) (16,32). We refer to F˜(S, µt) as a spectral auxiliary function for approximate optimization of EA(S) (last column in Table 3). We will also simply call F˜(S, µt) a spectral bound, not to be confused with a similar term used for matrix eigenvalues.
Similarly to kernel bound in Section 2.2, spectral bound is
useful for optimizing (1). In fact, for m = Ω the spectral bounds (Tab.3) are algebraically equivalent to the kernel bounds (Tab.2) since K˜ = K, see (58). For m < Ω we have K˜ ≈ K and F˜ ≈ F . Therefore, we can iteratively minimize energy E(S) (1) by applying bound optimization to its spectral approximation

E˜(S) = F˜(S) + γ Ec(Sc)

(67)

c∈F

or its weighted spectral approximation

E˜(S) = F˜w(S) + γ Ec(Sc).

(68)

c∈F

Theorem 3 (spectral bound). For any (symmetric) afﬁnity matrix A assume sufﬁciently large diagonal shift δ generating p.s.d. kernel K as in Table 3. Then, auxiliary function

a˜t(S) = F˜(S, µt) + γ Ec(Sc)

(69)

c∈F

using F˜(S, m) (49,50) with embedding {φ˜p} ⊂ Rm in Tab. 3 is a bound for joint energy (67,68) approximating (1) as m → Ω .

Approximation quality (58) depends on omitted eigenvalues λi
for i > m. Representative examples in Fig.10 show that relatively few eigenvalues may dominate the others. Thus, practically good

Algorithm 2: α-Expansion for Spectral Cut

Input : Afﬁnity Matrix A of size Ω × Ω ; initial labeling S01, ..., S0K Output: S1, ..., SK : partition of the set Ω Find top m eigenvalues/vectors Λm, V m for a matrix in the 4th col. of Tab. 3 ;

Compute

embedding

{φ˜p }

⊂

m
R

for

some

δ

and

set

t

∶=

0;

while not converged do

Set a˜t(S) to be the spectral bound (69) at current partition St;

for each label α ∈ L = {1, ..., K} do

Find St ∶= arg min a˜t(S) within one α expansion of St;

end

Set t ∶= t + 1; end

approximation with small m is possible. Larger m are computationally expensive since more eigenvalues/vectors are needed. Interestingly, smaller m may give better optimization since Kmeans in higher-dimensional spaces may be more sensitive to local minima. Thus, spectral bound optimization for smaller m may ﬁnd solutions with lower energy, see Fig.11, even though the quality of approximation is better for larger m.
Similarly to the kernel bound algorithms discussed in Section 2.3 one can optimize the approximate spectral bound (69) for energy (1) using standard algorithms for regularization. This follows from the fact that the ﬁrst term in (69) is unary (linear). Algorithms 2 shows a representative (approximate) bound optimization technique for (1) using move-making algorithms [82]. Note that for γ = 0 (no regularization terms) our bound optimization Algorithm 2 reduces to basic K-means over approximate isometry embeddings {φ˜p} ⊂ Rm similar but not identical to common discretization heuristics in spectral relaxation methods.

3.3 Relation to spectral clustering

Our approximation of pairwise clustering such as NC via basic KM over low dimensional embeddings φ˜p is closely related to popular spectral clustering algorithms [8], [79], [48] using eigen decomposition for various combinations of kernel, afﬁnity, distortion, laplacian, or other matrices. Other methods also build low-dimensional Euclidean embeddings [79], [48], [49] for basic KM using motivation different from isometry and approximation errors with respect to given afﬁnities. We are mainly interested in discussing relations to spectral methods approximately optimizing pairwise clustering criteria such as AA, AC, and NC [8].
Many spectral relaxation methods also use various eigen decompositions to build explicit data embeddings followed by basic K-means. In particular, the smallest or largest eigenvectors for the (generalized) eigenvalue problems in Table 4 give wellknown exact solutions for the relaxed problems. In contrast to our approach, however, the ﬁnal K-means stage in spectral methods is often presented without justiﬁcation [8], [81], [67] as a heuristic for quantizing the relaxed continuous solutions into a discrete labeling. It is commonly understood that

“... there is nothing principled about using the K-means

algorithm in this step”

(Sec. 8.4 in [81])

or that

“... K-means introduces additional unwarranted as-

sumptions.”

(Sec. 4 in [59])

Also, typical spectral methods use K eigenvectors solving the relaxed K-cluster problems followed by KM quantization. In contrast, we choose the number of eigenvectors m based on Frobenius error for isometry approximation (58). Thus, the number m is independent from the predeﬁned number of clusters.

15

Below we juxtapose our approximate isometry low-
dimensional embeddings in Table 3 with embeddings used for ad-
hoc discretization by the standard spectral relaxation methods in
Table 4. While such embeddings are similar, they are not identical.
Thus, our Frobenius error argument offers a justiﬁcation and minor corrections for KM heuristics in spectral methods, even though the
corresponding methodologies are unrelated. More importantly, our
bound formulation allows integration of pairwise clustering with
additional regularization constraints (1). Embeddings in spectral methods for NC: Despite similarity,
there are differences between our low-dimensional embedding (66) provably approximating kernel K = δD−1 + D−1AD−1 for the kKM formulation of NC [52], [40] and common adhoc embeddings used for KM discretization step in the spectral
relaxation methods. For example, one such discretization heuristic [8], [81] uses embedding φ˜p (right column in Tab. 4) deﬁned by the columns of matrix U K whose rows are the K top (unit) eigen-
vectors of the standard eigen system (left column). It is easy to 1
verify that the rows of matrix V D− 2 are non-unit eigenvectors for the generalized eigen system for NC. The following relationship

φ˜p

=

UK

≡

[V

K

D−

1 2

]rn

where operator [⋅]rn normalizes matrix rows, demonstrates certain differences between ad hoc embeddings used by many spectral

relaxation methods in their heuristic K-means discretization step

and justiﬁed approximation embedding (66) in Tab. 3. Note that

our formulation scales each embedding dimension, i.e. rows in

matrix

V

K D−

1 2

,

according

to

eigenvalues

instead

of

normalizing

these rows to unit length.

There are other common variants of embeddings for the K-

means discretization step in spectral relaxation approaches to the

normalized cut. For example, [83], [66], [67] use

φ˜p

=

[Λ−

1 2

U

]Kp

for discretization of the relaxed NC solution. The motivation

comes from the physics-based mass-spring system interpretation

[83] of the generalized eigenvalue system.

Some spectral relaxation methods motivate their discretization

procedure differently. For example, [59], [52] ﬁnd the closest

integer solution to a subspace of equivalent solutions for their

particular very similar relaxations of NC based on the same eigen

decomposition (64) that we used above. Yu and Shi [59] represent

the subspace via matrix

X′

≡

[√ΛmV

mD−

1 2

]cn

where columns differ from our embedding φ˜(Ip) in (66) only by normalization. Theorem 1 by Bach and Jordan [52] equivalently
reformulates the distance between the subspace and integer label-
ings via a weighted K-means objective for embedding

φ˜p = d1p Vpm (70)
and weights wp = dp. This embedding is different from (66) only by eigenvalue scaling.
Interestingly, a footnote in [52] states that NC objective (41) is equivalent to weighted KM objective (32) for exact isometry embedding
φp = d1p Gp ∈ R Ω (71)

16

(a) Gaussian for data in Fig. 9 (b) KNN for data in Fig. 9

(c) mPb kernel [67] for image (d) KNN kernel for image
Fig. 10: Spectrum of eigenvalues of typical kernel matrices for synthetic data (top row) or real image color (bottom row). This helps us to select approximate embedding so as to have small approximation error (58). For example, with ﬁxed width gaussian kernel in (a), it sufﬁces to select a few top eigenvectors since the remaining eigenvalues are negligible. Note that the spectrum elevates with increasing diagonal shift δ in (61). In principle, we can ﬁnd the optimal shift for a given number of dimensions m to minimize approximation error.

Fig. 11: For data and afﬁnity matrix in Fig. 9, we run weighted

K-means with our approximate embedding. The approximation

errors

K − K˜

2 F

K

2 F

for

3,

6,

10

and

50

dim.

embedding

are

58%, 41%, 27% and 3% respectively. We compute weighted K-

means energy (up to a const) and normalized cuts energy for

solution obtained at each iteration. We observed that normalized

cuts energy indeed tends to decrease during iterations of K-

means. Even 10 dim. embedding gives good alignment between

K-means energy and normalized cuts energy. Higher dimensional

embedding gives better energy approximation, but not necessarily

better solution with lower energy.

based

on

any

decomposition

A

≡

G′G.

Fo√r

example, 1

our

exact

isometry map (66) for m = Ω and G = ΛV D 2 is a special

case. While [52] reduce NC to K-means11, their low-dimensional

embedding φ˜ (70) is derived to approximate the subspace of

relaxed NC solutions. In contrast, low-dimensional embedding

(66) approximates the exact esometry map φ ignoring relaxed

solutions. It is not obvious if decomposition A ≡ G′G for the exact embedding (71) can be used to ﬁnd any approximate lower-

dimensional embeddings like (66).

4 OTHER OPTIMIZATION IDEAS
This Section outlines several extensions for optimization methods presented in Sec.2 and 3. For example, we vary diagonal shifts δ to reduce Frobenius approximation error K˜ − K F (58) between the optimal rank-m matrix K˜ and kernels K = A+δI, see Sec.4.1. We also discuss pseudo-bound [37] and trust region [84] as alternative approximate optimization concepts that may improve our bound optimization framework for high-order energy (1), see Sec.4.2.

4.1 Extensions for spectral bound optimization
We derive low-dimensional embeddings (61),(62), (66) in a principled way allowing to estimate approximation errors with respect to the exact pairwise clustering criteria. Frobenius errors (58) depend on embedding dimensionality m and diagonal shift δ that can increase or decrease all eigenvalues λi, see Fig.10. Unlike typical discretization heuristics in spectral relaxation methods, our choice of m is not restricted to the ﬁxed number of clusters K. Moreover,
11. KM procedure (23) (weighted version) is not practical for objective (32) for points φp in R Ω . Instead, Dhillon et al. [40] later suggested pairwise KM procedure (24) (weighted version) using kernel Kpq ≡ ⟨φp, φq⟩.

for each ﬁxed m we can ﬁnd an optimal shift δ minimizing the
sum of squared norms of omitted eigenvalues as follows. For simplicity we focus on AA with symmetric afﬁnity
A = V ′ΛV even though our technique can be easily extended to AC and NC. Diagonal shift δ ensures positive semi-deﬁniteness of the kernel matrix K(δ) = δI + A for an equivalent kKM. For approximate low dimensional embedding (61) it is enough
to guarantee positive semi-deﬁniteness of rank-m approximating kernel K˜(δ)
K˜(δ) = (V m)′(δIm + Λm)V m.

Thus one should use δ such that all eigenvalues λi for i ≤ m are non-negative. Given this restriction one can choose δ to minimize
Frobenius approximation error (58):

min K(δ) − K˜(δ) F , s.t. δ ≥ − min λi.

(72)

δ

i≤m

Making use of (58), the objective in (72) is ∑iΩ=m+1(λi + δ)2 giving optimum

δ∗ = − ∑iΩ=m+1 λi = − tr(K(0) − K˜(0))

Ω −m

Ω −m

automatically satisfying the constraint in (72). Indeed, −δ∗ is the mean value of the discarded eigenvalues λi, so the shifted
discarded eigenvalues (λi + δ∗) must contain both positive and negative values. Since K˜(δ∗) and Λm use the largest eigenvalues {λi i ≤ m} we have

min(λi + δ∗) ≥ max(λi + δ∗) ≥ 0

i≤m

i>m

and the constraint in (72) is satisﬁed. In practice the effect of the diagonal shift mainly depends on the whole spectrum of eigenvalues for a particular afﬁnity matrix, see Figure 10.

4.2 Pseudo-bound and trust region methods
Our bound optimization approach to energy (1) can be extended in several ways. For example, following pseudo-bound optimization ideas [37], parametric methods [56] can be used to optimize our bounds with additional perturbation terms reducing sensitivity to local minima. It also makes sense to use our (spectral) bounds as an approximating functional for (1) in the context of trust region approach to optimization [84]. We also note that parametric methods can explore all diagonal shifts δ alleviating the need for expensive eigen decomposition of large matrices.
Pseudo-bound optimization: Consider the following deﬁnition introduced in [37].
Deﬁnition 1. (pseudo-bound) Given energy E(S) and parameter η ∈ R, functional Bt(S, η) is a pseudo-bound for energy E(S) at current solution St if there exists at least one η′ such that Bt(S, η′) is an auxiliary function of E(S) at St.
Instead of using an auxiliary function, one can optimize a family of pseudo-bounds that includes at least one proper bound. This guarantees that original functional E(S) decreases when the best solution is selected among the global minima for the whole family [37]. In the meanwhile, such pseudo-bounds may approximate E(S) better than a single auxiliary function, even though they come from the same class of sub-modular (globally optimizable) functionals. The experiments of [37] conﬁrmed that pseudo-bounds signiﬁcantly outperform the optimization quality obtained by a single auxiliary function in the context of several high-order segmentation functionals, e.g., entropy [85], Bhattacharyya measure [86] and KL divergence [73]12. If the pseudobounds are monotone w.r.t. parameter η, we can ﬁnd all global minima for the whole family in polynomial time via parametric max-ﬂow algorithm [56]. This practical consideration is important when building a pseudo-bound family. For instance, [37] built pseudo-bounds by simply adding monotone unary terms η Sk to the auxiliary functions. We can use a similar perturbation term in the context of auxiliary functions in Table 2 and 3.
As a proof-of-the-concept, we included Figure 12 demonstrating one synthetic binary clustering example. It uses standard NC objective (41) with p.d. Gaussian kernel (requiring no diagonal shift) and no additional regularization terms. Basic kernel bound optimization converges to a weak local minimum, while pseudobound optimization over all parameters η in perturbation term η Sk achieves a much better solution with lower energy. This toy example suggests that pseudo-bound approach may compete with standard spectral relaxation methods [8].
A different perturbation term is motivated by using diagonal shift as a perturbation parameter η = δ. Besides improving optimization, efﬁcient search over δ may make it unnecessary to compute expensive eigen decompositions when estimating diagonal shift for proper p.s.d. kernels/afﬁnities in the third columns of Table 2 and 3. Consider function e(X) deﬁned by symmetric afﬁnities A and weights w
X ′ AX e(X) ∶= − w′X
which is similar to e(X) in (51) except that A is not necessarily positive deﬁnite. Thus, e(X) may not be concave as a function of relaxed continuous argument X, see Lemma 1. Pseudo-bound

17
for clustering objectives in the general form EA(S) = ∑k e(Sk) can be derived from diagonal shift δW + A for W ∶= diag(w) resulting in equivalent objectives.

Theorem 4 (pseudo-bound). Consider pairwise clustering objectives in form EA(S) ∶= ∑k e(Sk). The following is a pseudobound of EA(S) at St

Bt(S, δ) =c ∇e(Stk)′Sk + δ (1 − 2Stk)′W Sk (73)

k

k

w′Stk

becoming

a

proper

auxiliary

function

for

δ

≥

−λ0(W

−1 2

AW

−1 2

)

where λ0 denotes the smallest eigenvalue of the matrix. The

corresponding pseudo-bound for the joint energy combining high-

order clustering term EA with regularization terms in (1) is

Bt(S, δ) + γ Ec(Sc).

(74)

c∈F

Proof. Implied by Lemma 2 (Appendix D) after omitting δK, which is a constant w.r.t. S not affecting optimization.

Theorem 4 provides pseudo-bound (74) for joint energy (1)

combining AA, AC, NC objectives in Table 2 with regularization

potentials. When the number of segments is K = 2 parametric max-ﬂow methods [56] could be used to explore all distinct opti-

mal solutions S(δ) for pseudo-bound (74) for all δ. In particular,

this search covers δ where (74) is a proper bound removing the

need

for

explicitly

evaluating

λ0

(D

−1 2

AD

−1 2

)

via

expensive

eigen

decomposition. However, in contrast to the common perturbation

term η Sk [37], it is easy to check that the second term in

Bt(S, δ) (73) is not monotonic [56] with respect to δ. Thus, there is no guarantee that optimal object segments Sk(δ) form a nested sequence w.r.t. paprameter δ and that the number of such distinct

discrete solutions is bounded by 1 + Ω . Note that a parametric search could also be implemented for

K > 2 in the context of move making algorithms, see Section 2.3, using parametric max-ﬂow [56] to explore all δ at each expansion

or other move [10]. If the number of distinct solutions for all δ gets

too large at each expansion due to lack of monotonicity, one can

use simple practical heuristics. For example, restricted expansion

moves can be limited to “monotone” subsets of pixels with either

positive or negative unary potential with respect to δ.

Pseudo-bound Bt(S, δ) (73) could be useful for pairwise clustering without regularization when monotonicity is not needed

to guarantee efﬁciency. Indeed, for K = 2 a unary potential for each pixel in (73) changes its sign only once as parameter δ

increases. It is enough to sort all critical values of δ changing

at least one pixel in order to traverse all (at most 1 + Ω ) distinct solutions for pseudo bound (73) in a linear time. For K > 2 it is
easy to check that the optimal label for each pixel changes at most
K − 1 times as δ increases13. Critical values of δ for all pixels can be sorted so that all (at most 1 + (K − 1) Ω ) distinct solutions for
pseudo bound (73) can be explored in a linear time.

Trust region optimization: Non-monotonicity of the second (perturbation) term in (73) follows from the factor (1 − 2Stk)
assigning unary potentials of different signs to pixels inside and outside the current segment Stk. This term can be represented as

δ (1 − 2Stk)′W Sk

k

w′Stk

≡ δ Stk − Sk w
k

12. The segmentation functionals considered in [37] are completely different

13. The optimal value of a unary potential for each pixels is the lower

from the pairwise clustering energies we consider in this work

envelope of K linear functions of δ, which has at most K − 1 breakpoints.

(a) Initialization (energy: 0.404)

(b) Bound optimization (energy: 0.350)

18
This probabilistic interpretation of kKM gives an additional point of view for comparing it with pKM clustering with loglikelihood energy (14). Instead of parametric ML models kKM uses Parzen density estimates. Another difference is absence of the log in (76). Omitting the log reduces the weight of low probability points, that is, outliers. In contrast, log-likelihoods in (14) are highly sensitive to outliers. To address this problem, pKM methods often use heuristics like mixing the desired probability model P with a uniform distribution, e.g. P˜ (⋅ θ) ∶= + (1 − )P(⋅ θ).

(c) Pseudo-bound optimization (energy: 0.094)

(d) Spectral method (energy: 0.115)

Fig. 12: Normalized cut (NC) objective (41). This proof-ofconcept example shows that our pseudo-bound optimization (c) is less sensitive to local minima compared to the standard kKM algorithm (bound optimizer) in (b). Pseudo-bound approach can also compete with spectral relaxation methods [8] in (d).

where Stk − Sk w is a weighted Hamming distance between Sk and the current segment Stk. Thus, instead of bounds, functions (73) and (74) can be treated as Lagrangians for the constrained optimization of the unary or higher-order approximations of the corresponding objectives over trust region [84]
Stk − Sk w ≤ τ.
In this case, parameter δ is a Lagrange multiplier that can be adaptively changed from iteration to iteration [84], rather than exhaustively explored at each iteration. Note that it may still be useful to add a common monotone perturbation term as in [37]
η Sk ≡ η1′Sk
that can be efﬁciently explored at each iteration. This corresponds to a combination of pseudo-bound and trust region techniques.

5 PARZEN ANALYSIS & BANDWIDTH SELECTION

This section discusses connections of kKM clustering to Parzen densities providing probabilistic interpretations for many equivalent pairwise clustering objectives discussed in our work. In particular, this section gives insights on practical selection of kernels or their bandwidth. We discuss extreme cases and analyze adaptive strategies. For simplicity, we mainly focus on Gaussian kernels, even though the analysis applies to other types of positive normalized kernels.
Note that standard Parzen density estimate for the distribution of data points within segment Sk can be expressed using normalized Gaussian kernels [87], [44]

Pσ(Ip Sk) = ∑q∈Sk Sk(kIp, Iq) . (75)

It is easy to see that kKM energy (22) is exactly the following high-order Parzen density energy

F (S) =c −

Pσ(Ip Sk).

(76)

k p∈Sk

5.1 Extreme bandwidth cases

Parzen energy (76) is also useful for analyzing two extreme cases of kernel bandwidth: large kernels approaching the data range and small kernels approaching the data resolution. This section analyses these two extreme cases.
Large bandwidth and basic K-means: Consider Gaussian kernels of large bandwidth σ approaching the data range. In this case Gaussian kernels k in (75) can be approximated (up to a scalar) by Taylor expansion 1 − Ip2−σI2q 2 . Then, Parzen density energy (76) becomes (up to a constant)

∑pq∈Sk Ip − Iq 2

k

2σ2 Sk

which is proportional to the pairwise formulation for the basic Kmeans or variance criteria in Tab.1 with Euclidean metric . That is, kKM for large bandwidth Gaussian kernels reduces to the basic K-means in the original data space instead of the high-dimensional embedding space.
In particular, this proves that as the bandwidth gets too large kKM looses its ability to ﬁnd non-linear separation of the clusters. This also emphasizes the well-known bias of basic K-means to equal size clusters [27], [88].
Small bandwidth and Gini criterion: Very different properties could be shown for the opposite extreme case of small bandwidth approaching data resolution. It is easy to approximate Parzen formulation of kKM energy (76) as

F (S) ≈c − Sk ⋅ ⟨Pσ(Sk), dks ⟩

(77)

k

where Pσ(Sk) is kernel-based density (75) and dks is a “true” continuous density for the sample of intensities {Ip p ∈ Sk} in segment Sk. Approximation (77) follows directly from the same
Monte-Carlo estimation argument in Appendix C with the only difference being f = −Pσ(Sk) instead of − log P(θS).
If kernels have small bandwidth optimal for accurate Parzen density estimation14 we get Pσ(Sk) ≈ dks further reducing (77) to approximation
≈c − Sk ⋅ ⟨dks , dks ⟩
k

that proves the following property.

Property 1. Assume small bandwidth Gaussian kernels optimal for accurate Parzen density estimation. Then kernel K-means energy (76) can be approximated by the standard Gini criterion for clustering [35]:

EG(S) ∶= Sk ⋅ G(Sk)

(78)

k

14. Bandwidth near inter-point distances avoids density oversmoothing.

where G(Sk) is the Gini impurity for the data points in Sk
G(Sk) ∶= 1 − ⟨dks , dks ⟩ ≡ 1 − dks (x)2dx. (79)
x
Similarly to entropy, Gini impurity G(Sk) can be viewed as a measure of sparsity or “peakedness” for continuous or discrete distributions. Both Gini and entropy clustering criteria are widely used for decision trees [35], [36]. In this discrete context Breiman [35] analyzed theoretical
properties of Gini criterion (78) for the case of histograms Ph where G(Sk) = 1 − ∑x Ph(x Sk)2. He proved that for K = 2 the minimum of the Gini criterion is achieved by sending all data points within the highest-probability bin to one cluster and the remaining data points to the other cluster, see the color encoded illustration above. We extend Brieman’s result to the continuous Gini criterion (78)-(79).
Theorem 5 (Gini Bias, K = 2). Let dΩ be a continuous probability density function over domain Ω ⊆ Rn deﬁning conditional density dks (x) ∶= dΩ(x x ∈ Sk) for any non-empty subset Sk ⊂ Ω. Then, continuous version of Gini clustering criterion (78) achieves its optimal value at the partitioning of Ω into regions S1 and S0 = Ω ∖ S1 such that
S1 = arg max dΩ(x).
x
Proof. See Appendix E and Proposition 2.
The bias to small dense clusters is practically noticeable for small bandwidth kernels, see Fig.14(d). Similar empirical bias to tight clusters was also observed in the context of average association (22) in [8]. As kernel gets wider the continuous Parzen density (75) no longer approximates the true distribution ds and Gini criterion (78) is no longer valid as an approximation for kKM energy (76). In pratice, Gini bias gradually disappears as bandwidth gets wider. This also agrees with the observations for wider kernel in average association [8]. As discussed earlier, in the opposite extreme case when bandwidth get very large (approaching data range) kKM converges to basic K-means or variance criterion, which has very different properties. Thus, kernel K-means properties strongly depend on the bandwidth.
The extreme cases for kernel K-means, i.e. Gini and variance criteria, are useful to know when selecting kernels. Variance criteria for clustering has bias to equal cardinality segments [27], [88]. In contrast, Gini criteria has bias to small dense clusters (Theorem 5). To avoid these biases kernel K-means should use kernels of width that is neither too small nor too large. Our experiments compare different strategies with ﬁxed and adaptivewidth kernels (Sec.5.2). Equivalence of kernel-K-means to many standard clustering criteria such as average distortion, average association, normalized cuts, etc(see Sec.1.2.2) also suggest kernel selection strategies based on practices in the prior literature.
5.2 Adaptive kernels via Nash embedding and KNN
As discussed in Sec.5.1, kernel width should neither be too small nor too large. We propose adaptive kernels designed to equalize the density in highly dense regions in the color space.

space of points I with Riemannian metric

19
transformed points I′ with Euclidian metric

unit balls in Riemannian metric deﬁned by tensor Σ

unit balls in Euclidean metric

Fig. 13: Nash embedding: adaptive non-normalized Gaussian kernels deﬁne isometric transformation of the color space modifying density. Ellipsoids are mapped to balls.

The following equation interprets adaptive Gaussian kernels via Riemannian distances in the color space (left picture in Fig.13)

− Ip −Iq 2

kp(Ip, Iq) = e 2σp2

−(Ip −Iq )′ Σ−p1 (Ip −Iq )

=e

2

.

According to Nash embedding theorem [89], this Riemannian color space can be isometrically embedded into a Euclidean space, so that the last expression above is equal to

− Ip′ −Iq′ 2
=e 2

=

k

(

Ip′

,

I

′ q

)

where k is a ﬁxed-width Gaussian kernel in the new transformed space (right picture in Fig.13). Thus, non-normalized Gaussian kernels of adaptive width σp (or covariance matrix Σp, in general) deﬁne some color space transformation, Nash embedding, that locally stretches or shrinks the space. After this transformation, clustering is done using a ﬁxed (homogeneous) Gaussian kernel of constant width.
Figure 13 helps to illustrate how Nash embedding changes the color space density. The number of points in a unit (Euclidean) ball neighborhood in the transformed space is equal to the number of points in the corresponding unit (Riemannian) ball in the original space:
K = d′ ⋅ V1 = d ⋅ Vσ

where d and d′ are local densities in the original and transformed spaces. Thus, kernel width σp can be selected adaptively based on any desired transformation of density d′(d) according to formula

σp ∼ n d′(ddpp) (80)

where dp ∶= d(Ip) is an observed local density for points in the color space. This local density can be evaluated using any common
estimator, e.g. Parzen approach gives

1

− Ip −Iq 2 2

d(Ip) ∼ q ∆nq ⋅ e 2∆q

(81)

where ∆q could be adaptive or ﬁxed ∆q = const, according to any standard technique for density estimation [87].

To address Breiman bias one can use density equalizing

transforms

d′(d)

=

const

or

d′

=

1 α

log(1

+

αd),

which

even

up the highly dense parts of the color space. Formula (80) works

for any target transform d′(d). Once adaptive kernels σp are

Fig. 14: (a)-(d): Breiman bias for ﬁxed (small) kernel. (e) Empirical density transform for adaptive kernels (80) with d′(d) = const. Color space density equalization counters Breiman bias.
(f) Segmentation for adaptive kernels.

chosen, Nash theory also allows to obtain empirical scatter plots
d′(d)Ω ∶= {(d′(Ip′ ), d(Ip)) p ∈ Ω}, for example, to compare it with the selected “theoretical” plot d′(d). Estimates d(Ip) are provided in (81) and the density estimate for Nash embedding are

d′(Ip′ ) ∼

e = e . − Ip′ −Iq′ 2 2
q

− Ip −Iq 2 2σq2
q

(82)

Note the difference between empirical density estimates for d′ in (82) and d in (81): the former uses the sum of non-normalized kernels of selected adaptive width σq in (80) and the latter is the sum of normalized kernels of width ∆q based on chosen density estimator. While parameter σq directly controls the density transformation, ∆ plays a fairly minor role concerning the quality of estimating density d.
Figure 14(e) illustrates the empirical density mapping d′(d)Ω induced by adaptive kernels (80) for d′(d) = const. Notice a density-equalization effect within high density areas in the color
space addressing the Breiman bias. The const density mapping can be approximated using KNN
graph. To be speciﬁc, the symmetric KNN kernel in this paper is
deﬁned as follows:

kpq = k(fp, fq) = [fp ∈ KNN (fq)]

+ [fq ∈ KNN (fp)]

(83)

where KNN (fp) is a set of K nearest neighbors of fp. The afﬁnity between fp and fq achieves maximum value of 2 if they
are mutually each other’s KNN s.

6 EXPERIMENTS
This section is divided into two parts. The ﬁrst part (Sec.6.1) shows the beneﬁts of extra MRF regularization for kernel & spectral clustering, e.g. normalized cut. We consider pairwise Potts, label cost and robust bin consistency term, as discussed in Sec.1.1. We compare to spectral clustering [8], [66] and kernel K-means [40], which can be seen as degenerated versions for spectral and kernel cuts (respectively) without MRF terms. We show that MRF helps kernel & spectral clustering in segmentation and image clustering. In the second part (Sec.6.2) we replace the

20

log-likelihoods in model-ﬁtting methods, e.g. GrabCut [26], by

pairwise clustering term, e.g. AA and NC. This is particularly ad-

vantageous for high dimension features (location, depth, motion).

Implementation details: For segmentation, our kernel cut

uses KNN kernel (83) for pixel features Ip, which can be

concatenation of LAB (color), XY (location) and M (motion or

optical ﬂow) [90]. We choose 400 neighbors and randomly sample

50 neighbors for each pixel. Sampling does not degrade our

segmentation but expedites bound evaluation. We also experiment

with popular mPb contour based afﬁnities [67] for segmentation.

The window radius is set to 5 pixels.

For contrast-sensitive regularization, we use standard penalty

wpq =

1 d

e−0.5

Ip −Iq

2 2

η

for (2), where η is the average of

Ip −

pq

Iq 2 over a 8-connected neighborhood and dpq is the distance

between pixels p and q in the image plane. We set wpq = d1pq for

length regularization.

For GrabCut, we used histogram-based probability model, as

is common in the literature [85], [91]. We tried various bin size

for spatial and depth channels.

With ﬁxed width Gaussian kernel used in Fig. 14, the time

complexity of the naive implementation of kernel bound evaluation in (55) is O( Ω 2). The bottleneck is the evaluation of KXt and Xt′KXt in derivative ∇e(Xt) (52). In this case, we
resort to fast approximate dense ﬁltering method in [92], which

takes O( Ω ) time. Also notice that the time complexity of the approach in [92] grows exponentially with data dimension N . A

better approach for high-dimensional dense ﬁltering is proposed

in [93], which is of time O( Ω × N ). We stick to [92] for lowdimensional color space in experiments in Fig. 14.

6.1 MRF helps Kernel & Spectral Clustering
Here we add MRF regulation terms to typical normalized cut applications, such as unsupervised multi-label segmentation [67] and image clustering [94]. Our kernel and spectral cuts are used to optimize the joint energy of normalized cut and MRF (1) or (68).
6.1.1 Normalized Cut with Potts Regularization
Spectral clustering [8] typically solves a (generalized) eigen problem, followed by simple clustering method such as K-means on the eigenvectors. However, it is known that such paradigm results in undesirable segmentation in large uniform regions [67], [66], see examples in Fig. 15. Obviously such edge mis-alignment can be penalized by contrast-sensitive Potts term. Our spectral and kernel cuts get better segmentation boundaries. As is in [40] we use spectral initialization.
Tab.5 gives quantitative results on BSDS500 dataset. Number of segments in ground truth is provided to each method. It shows that kernel and spectral cuts give better covering, PRI (probabilistic rand index) and VOI (variation of information) than spectral clustering. Fig.15 gives sample results. Kernel K-means [40] gives results similar to spectral clustering and hence are not shown.
6.1.2 Normalized Cuts with Label Cost [12]
Unlike spectral clustering, our kernel and spectral cuts do not need the number of segments beforehand. We use kernel cut to optimize a combination of the normalized cut, Potts model and label costs terms. The label cost (4) penalizes each label by constant hk. The energy is minimized by α-expansion and αβ-swap moves in Sec.2.3. We sample initial models from patches, as in [12].

21

Fig. 15: Sample results on BSDS500. Top row: spectral clustering. Middle & Bottom rows: our Kernel & Spectral Cuts.

method

Covering PRI VOI

Spectral Clustering

0.34

0.76 2.76

Our Kernel Cut

0.41

0.78 2.44

Our Spectral Cut

0.42

0.78 2.34

TABLE 5: Results of spectral clustering (K-means on eigenvectors) and our Kernel Cut & Spectral Cuts on BSDS500 dataset. For this experiment mPb-based kernel is used [67].

Results with different label cost are shown in Fig.16. Due to sparsity prior, our kernel and spectral cuts automatically prune weak models and determine the number of segments, yet yield regularized segmentation. We use KNN afﬁnity for normalized cut and mPb [67] based Potts regularization.
6.1.3 Normalized Cut with High Order Consistency Term [11], [13], [14]
It is common that images come with multiple tags, such as those in Flickr platform or the LabelMe dataset [95]. We study how to utilize tag-based group prior for image clustering [94].
We experiment on the LabelMe dataset [95] which contains 2,600 images of 8 scene categories (coast, mountain, forest, open country, street, inside city, tall buildings and highways). We use the same GIST feature, afﬁnity matrix and group prior as used in [94]. We found the group prior to be noisy. The dominant category in each group occupies only 60%-90% of the group. The high-order consistency term is deﬁned on each group. For each group, we introduce an energy term that is akin to the robust P n-Potts [11], which can be exactly minimized within a single αβ-swap or α-expansion move. Notice that here we have to use robust consistency potential instead of rigid ones.
Our kernel cut minimizes NC plus the robust P n-Potts term. Spectral cut minimizes energy of (67). Normalized mutual information (NMI) is used as the measure of clustering quality. Perfect clustering with respect to ground truth has NMI value of 1.
Spectral clustering and kernel K-means [40] give NMI value of 0.542 and 0.572 respectively. Our kernel cut and spectral cut signiﬁcantly boost the NMI to 0.683 and 0.681. Fig.17 shows the results with respect to different amount of image tags used. The left most points correspond to the case when no group prior is given. We optimize over the weight of high order consistency

term, see Fig.17. Note that it’s not the case the larger the weight the better since the grouping prior is noisy.
We also utilize deep features, which are 4096 dimensional fc7 layer from AlexNet [96]. We either run plain K-means, or construct a KNN kernel on deep features. These algorithms are denoted as deep K-means, deep spectral cut or deep kernel cut in Fig. 17. Incorporating group prior indeed improved clustering. The best NMI of 0.83 is achieved by our kernel cut and spectral cut for KNN kernel on deep features.
6.2 Kernel & Spectral Clustering helps MRF
In typical MRF applications we replace the log-likelihood terms by average association or normalized cut. We evaluate our Kernel Cut (ﬁxed width kernel or KNN ) in the context of interactive segmentation, and compare with the commonly used GrabCut algorithm [26]. In Sec. 6.2.1, we show that our kernel cut is less sensitive to choice of regularization weight γ. We further report results on the GrabCut dataset of 50 images and the Berkeley dataset in Sec. 6.2.2. We experiment with both (i) contrastsensitive edge regularization, (ii) length regularization and (iii) color clustering (i.e., no regularization) so as to assess to what extent the algorithms beneﬁt from regularization.
From Sec. 6.2.3 to Sec. 6.2.6, we also report segmentation results of our kernel cut with high-dimensional features Ip, including location, texture, depth, and motion respectively.
6.2.1 Robustness to regularization weight
We ﬁrst run all algorithms without smoothness. Then, we experiment with several values of γ for the contrast-sensitive edge term. In the experiments of Fig. 18 (a) and (b), we used the yellow boxes as initialization. For a clear interpretation of the results, we did not use any additional hard constraint. In Fig. 18, ”KernelCut-KNNAA” means Kernel Cut with KNN kernel for average association (AA). Without smoothness, our Kernel Cut yields much better results than Grab Cut. Regularization signiﬁcantly beneﬁted the latter, as the decreasing blue curve in (a) indicates. For instance, in the case of the zebra image, model ﬁtting yields a plausible segmentation when assisted with a strong regularization. However, in the presence of noisy edges and clutter, as is the case of the chair image in (b), regularization does not help as much. Note that for small regularization weights γ our method is substantially

boundary
smoothness
none Euclidean length contrast-sensitive

GrabCut
27.2 13.6 8.2

color clustering term

KernelCut KernelCut

-Gau-AA -Gau-NC

20.4

17.6

15.1

16.0

9.7

13.8

KernelCut -KNN-AA
12.2 10.2 7.1

TABLE 6: Box-based interactive segmentation (Fig.21). Error rates (%) are averaged over 50 images in GrabCut dataset. KernelCut-Gau-NC means KernelCut for ﬁxed width Gaussian kernel based normalized cut objective.

better than model ﬁtting. Also, the performance of our method is less dependent on regularization weight and does not require ﬁne tuning of γ.
6.2.2 Segmentation on GrabCut & Berkeley datasets.
First, we report results on the GrabCut database (50 images) using the bounding boxes provided in [97]. For each image the error is the percentage of mis-labeled pixels. We compute the average error over the dataset.
We experiment with four variants of our Kernel Cut, depending on whether to use ﬁxed width Gaussian kernel or KNN kernel, and also the choice of normalized cut or average association term. We test different smoothness weights and plot the error curves15 in Fig.19. Table 6 reports the best error for each method. For contrastsensitive regularization GrabCut gets good results (8.2%). However, without edges (Euclidean or no regularization) GrabCut gives much higher errors (13.6% and 27.2%). In contrast, KernelCutKNN-AA (Kernel Cut with adaptive KNN kernel for AA) gets only 12.2% doing a better job in color clustering without any help from the edges. In case of contrast-sensitive regularization, our method outperformed GrabCut (7.1% vs. 8.2%) but both methods beneﬁt from strong edges in the GrabCut dataset. Fig .20 shows that our Kernel Cut is also robust to the hyper-parameter, i.e. K for nearest neighbours, unlike GrabCut.
Figure 21 gives some sample results. The top row shows a failure case for GrabCut where the solution aligns with strong edges. The second row shows a challenging image where our KernelCut-KNN-AA works well. The third and fourth rows show
15. The smoothness weights for different energies are not directly comparable; Fig. 19 shows all the curves for better visualization.

22

boundary smoothness
none contrast-sensitive

color clustering term

KernelCut BJ GrabCut -KNN-AA

12.4 12.4

7.6

3.2

3.7

2.8

TABLE 7: Seeds-based interactive segmentation (Fig.22). Error rates (%) are averaged over 82 images from Berkeley database. Methods get the same seeds entered by four users. We removed 18 images with multiple nearly-identical objects (see Fig.24) from 100 image subset in [98]. (GrabCut and KernelCut-KNN-AA give 3.8 and 3.0 errors on the whole database.)

failure cases for ﬁxed-width Gaussian kernel in kernel cut due to Breiman’s bias (Th.5) where image segments of uniform color are separated; see green bush and black suit. Adaptive kernel (KNN) addresses this bias.
We also tested seeds-based segmentation on a different database [98] with ground truth, see Tab.7 and Fig.22.
6.2.3 Segmentation of similar appearance objects
Even though objects may have similar appearances or look similar to the background (e.g. the top row in Fig.24), we assume that the objects of interest are compact and have different locations. This assumption motivates using XY coordinates of pixels as extra features for distinguishing similar or camouﬂaged objects. XY features have also been used in [99] to build space-variant color distribution. However, such distribution used in MRF-MAP inference [99] would still over-ﬁt the data [41]. Let Ip ∈ R5 be the augmented color-location features Ip = [lp, ap, bp, βxp, βyp] at pixel p where [lp, ap, bp] is its color, [xp, yp] are its image coordinates, and β is a scaling parameter. Note that the edge-based Potts model [4] also uses the XY information. Location features in the clustering and regularization terms have complementary effect: the former solves appearance camouﬂage while the latter gets edge alignment.
We test the effect of adding XY into feature space for GrabCut and Kernel Cut. We try various β for Kernel Cut. Fig.23 shows the effect of different β on KNN s of a pixel. For histogram-based GrabCut we change spatial bin size for the XY channel, ranging from 30 pixels to the image size. We report quantitative results on 18 images with similar objects and camouﬂage from the Berkeley database [100]. Seeds are used here. Fig. 25 shows average errors for multi-object dataset, see example segmentations in Fig. 24.
Fig. 26 gives multi-label segmentation of similar objects in one image with seeds using our algorithm. We optimize kernel bound

Fig. 16: Segmentation using our kernel cut with label cost. We experiment with increasing value of label cost hk for each label (from left to right)

NMI NMI

0.8 0.7 0.6 0.5 0.4
0

Deep Kernel Cut Deep Spectral Cut Deep K-means Kernel Cut Spectral Cut

20

40

60

80

100

Percent of Tags Used (%)

0.7 0.65

Spectral Cut

0.6

0.55

0.5 0

0.05

0.1

0.15

Weight of Group Prior Term

Fig. 17: Incorporating group prior achieves better NMI for image clustering. Here we use tags-based group prior. Our method achieved better NMI when more images are tagged. The right plot shows how the weight of bin consistency term affects our method.

23

contrast-sensitive regularization 0.3

GrabCut

KernelCut-Gau-AA

0.25

KernelCut-KNN-AA KernelCut-KNN-NC

0.2

Error rate

0.15

0.1

0.05 0
0.3 0.25

20

40

60

80

100

Smoothness Weight

lenghth regularization

GrabCut KernelCut-Gau-AA KernelCut-KNN-AA KernelCut-KNN-NC

Error rate

0.2
(a)
0.15

0.1

0.05 0

20

40

60

80

100

Smoothness Weight

Fig. 19: Average error vs. regularization weights for different variants of our KernelCut on the GrabCut dataset.

(b)
Fig. 18: Illustration of robustness to smoothness weight.
with move-making for NC and smoothness term combination, as discussed in Sec. 2.2. Fig. 26 (c) shows energy convergence.
6.2.4 Texture segmentation The goal of this experiment is to demonstrate scalability of our methods to highly dimensional data. First, desaturated images from GrabCut database [26] are convolved with 48 ﬁlters from [101]. This yields a 48-dimensional descriptor for each pixel. Secondly, these descriptors are clustered into 32 textons by K-means. Thirdly, for each pixel we build a 32-dimensional normalized histogram of textons in 5 × 5 vicinity of the pixel. Then the gray-scale intensity16 of a pixel is augmented by the corresponding texton histogram scaled by a factor w. Finally, resulting 33-dimensional feature vectors are used for segmentation. We show the result of Kernel Cut with respect to w in Fig.27.
16. We found that for the GrabCut database adding texture features to RGB does not improve the results.

Error rate

Number of Bins

16 32 64

128

256

0.15 0.14

KernelCut-KNN-AA GrabCut

0.13

0.12

0.11

0.1

0.09

0.08

0.07 64 256 512

1024

Neighborhood Size K

2048

Fig. 20: Our method aKKM is robust to choice of K while GrabCut is sensitive to bin size for histograms.

We compare our results with GrabCut with various bin sizes for texture features.
6.2.5 Interactive RGBD Images Segmentation
Depth sensor are widely used in vision for 3D modelling [103], [104], semantic segmentation [105], [106], [102], [107], motion ﬂow [108]. We selected 64 indoor RGBD images from semantic segmentation database NYUv2 [102] and provided bounding boxes and ground truth. In contrast to [26], the prepared dataset consists of low-quality images: there are camera motion artifacts, underexposed and overexposed regions. Such artifacts make colorbased segmentation harder.

24

Fig. 28: RGBD+XY examples. The ﬁrst two rows show original images wit bounding box and color-coded depth channel. The third row shows the results of Grabcut, the forth row shows the results of Kernel Cut. Parameters of the methods were independently selected to minimize average error rate over the database. The parameters of the algorithms were selected to minimize the average error over the dataset.

Fig. 22: Sample results for BJ [4], GrabCut [26], and our kernel cut for adaptive KNN kernel, see Tab.7.

Fig. 21: Sample results for GrabCut and our kernel cut with ﬁxed width Gaussian or adaptive width KNN kernel, see Tab.6.

We compare GrabCut to Kernel Cut over joint features Ip = [Lp, ap, bp, βDp] as in Sec.6.2.3. Fig.28 shows the error statistics and segmentation examples. While Kernel Cut takes advantage of the additional channel, GrabCut fails to improve.
6.2.6 Motion segmentation
Besides the location and depth features, we also test segmentation with motion features. Figs. 30, 31 and 32 compare motion segmentations using different feature spaces: RGB, XY, M (optical ﬂow) and their combinations (RGBM or RGBXY or RGBXYM). Abbreviation +XY means Potts regularization. We apply kernel cut (Alg.1) to the combination of NC with the Potts term.

Fig. 23: Visualization of a pixel’s K-Nearest-Neighbours for RGB feature (left) or RGBXY feature (right).
Challenging video examples: For videos in FBMS-59 dataset [109], our algorithm runs on individual frames instead of 3D volume. Segmentation of previous frame initializes the next frame. The strokes are provided only for the ﬁrst frame. We use the optical ﬂow algorithm in [90] to generate M features. Selected frames are shown in Figs. 30 and 31. Instead of tracks from all frames in

25

(a) seeds (b) ground truth (c) GrabCut (d)Kernel Cut Fig. 24: Sample results using RGBXY+XY.

Energy

(a) Seeds
80
60
40

(b) Our solution

Bin size in pixels

0

100

200

300

400

500

7

GrabCut

Kernel Cut 6

5

20 12345678 Swap Move Iteration
(c) Energy minimization for NC plus smoothness
Fig. 26: Multi-label segmentation for similar objects.

Error rate (%)

4

3

0

100

200

300

400

500

Connection range in pixels

Fig. 25: Error on Multi-objects dataset. We vary spatial bin-size for GrabCut and weight β in [l, a, b, βX, βY ] for Kernel Cut. The connection range is the average geometric distance between a pixel and its kth nearest neighbor. The right-most point of the
curves corresponds to the absence of XY features. GrabCut
does not beneﬁt from XY features. Kernel Cut achieves the best
error rate of 2.9% with connection range of 50 pixels.

[110], our segmentation of each frame uses only motion estimation between two consecutive frames. Our approach jointly optimizes normalized cut and Potts model. In contrast, [110] ﬁrst clusters semi-dense tracks via spectral clustering [109] and then obtains dense segmentation via regularization.
Kitti segmentation example: We also experiment with Kitti dataset [111]. Fig.32 shows the multi-label segmentation using either color information RGB+XY (ﬁrst row) or motion MXY+XY (second row). The ground-truth motion ﬁeld works as M channel. Note that the motion ﬁeld is known only for approximately 20% of the pixels. To build an afﬁnity graph, we construct a KNN graph from pixels that have motion information. The regularization over 8-neighborhood on the pixel grid interpolates the segmentation labels during the optimization procedure.

APPENDIX A (WEIGHTED KM AND AA)
This Appendix reviews weighted kernel K-means (wkKM) in detail. In particular, it describes generalizations of KM procedures (23) and (24) to the weighted case and shows that they also correspond to linear bound optimization by extending Theorem 1 in Sec. 2.1. We provide an alternative derivation for the kernel bound for NC. The Appendix also discusses equivalence of weighted AA with arbitrary afﬁnity to wkKM with p.s.d. kernel and explains the corresponding diagonal shift.

Number of bins in extra dimensions

2

4

6

8

10

50

GrabCut

Kernel Cut

40

Error rate (%)

30

20

11:9%

10 12:6%

0

0.2

0.4

0.6

0.8

1

Weight of extra dimensions

Fig. 27: The average errors of GrabCut a Kernel Cut methods for texture segmentation over 50 desaturated images from GrabCut database [26]. We optimize the result of GrabCut with respect to smoothness weight and bin sizes in the intensity dimension. We optimize the result of Kernel Cut with respect to smoothness weight.

Weighted kernel K-means (wkKM): As discussed in Section 1.2.2, weighted K-means corresponds to objective (36)

F w(S) =

wp φp − µwSk 2
k p∈Sk

=c − Sk′W K W Sk

k

Sk′w

(A-1) (A-2)

where . is the Euclidean norm, w ∶= {wp p ∈ Ω} are predeﬁned weights, φp ≡ φ(Ip) is an embedding of data points in some highdimensional space, and µwSk is a weighted cluster mean (33). Consistently with Sec. 1.2.2 we use diagonal matrix W = diag(w) and embedding matrix φ ∶= [φp] implying identities (34) and
matrix formulation (A-2) with p.s.d. kernel

K = φ′φ

of dot products Kpq = φ′pφq. The constant connecting equivalent objectives (A-1) and (A-2) is ∑p∈Ω wp φp 2.

26

(a) frames

(b) optical ﬂow [90]

(c) M+XY

(d) RGB+XY

(e) RGBM+XY

Fig. 30: Motion segmentation using our framework for the sequence horses01 in FBMS-59 dataset [109]. Motion feature alone (M+XY in (c)) is not sufﬁcient to obtain ﬁne segmentation. Our framework successfully utilize motion feature (optical ﬂow) to separate the horse from the barn, which have similar appearances.

(a) frames

(b) optical ﬂow [90]

(c) RGBXY+XY

(d) RGBXYM+XY

Fig. 31: Multi-label motion segmentation using our framework for the sequence ducks01 in FBMS-59 dataset [109]. This video is challenging since the ducks here have similar appearances and even spatially overlap with each other. However, different ducks come with different motions, which helps our framework to better separate individual ducks.

27

Motion Flow

RGB+XY

MXY+XY

Fig. 32: Motion segmentation for image 000079 10 from KITTI [111] dataset. The ﬁrst row shows the motion ﬂow. Black color codes the pixels that do not have motion information. The second row shows color-based segmentation. The third row shows motion based segmentation with location features. We also tried M+XY segmentation, but it does not work as well as MXY+XY above. The results for RGBMXY+XY were not signiﬁcantly different from MXY+XY.

Number of bins in depth dimension

0

500 1000 1500 2000 2500 3000

30

GrabCut Kernel Cut 28

Error rate (%)

26

24

22

20

0

2000 4000 6000 8000 10000

Weight of depth dimension

Fig. 29: The average errors of GrabCut a Kernel Cut methods over 64 images selected from NYUv2 database [102].

In the context of weighted energy (A-1) the basic KM algorithm [28] is the block-coordinate descent for mixed objective (32)

F w(S, m) ∶= =c

wp φp − mk 2
k p∈Sk
w mk 2 − 2W φ′mk ′ Sk
k

(A-3)

where the second linear algebraic formulation17 generalizes (49)
and highlights modularity (linearity) with respect to S. Variables mk can be seen as “relaxed” segment means µwSk in (A-1). Yet, energies (A-3) and (A-1) are equivalent since their global
minimum is achieved at the same optimal segmentation S. Indeed,

µwSk = arg min wp φp − mk 2
mk p∈Sk

⇒ F w(S) = min F w(S, m).
m

(A-4)

Weighted KM and bound optimization: The weighted case of procedure (23) replaces µSk (17) by weighted mean µwSk (33)

weighted KM procedure

Sp ← arg min φp − µwSk .

k

t

(A-5)

17. It is obtained by opening the square of the norm and applying algebraic identities (34). Formulation (6) omits the same constant as (A-2).

Assuming φp ∈ Rm each iteration’s complexity O( Ω Km) is linear with respect to the number of data points Ω .
Implicit kKM procedure (24) generalizes to the weighted case
as follows. Similarly to Sec.8.2.2 in [51] and our derivation of (24)
in Sec.1.2.2, the square of the objective in (A-5) and (33) give

2

′

2

φp′φW Stk Stk′W φ′φW Stk

φ¨p¨− 2φp µSk + µSk

¨

t

t

= −2 Sk′w +

(S k ′ w)2

.

t

t

Since the crossed term is a constant at p, the right hand side gives
an equivalent objective for computing Sp in (A-5). Using K = φ′φ and indicator vector 1p for element p we get generalization of (24)

⎛ wekigKhMted ⎞ Sp ← arg min Stk′W KW Stk − 2 1p′KW Stk .

⎝ procedure ⎠

k (Stk′w)2

Stk ′ w

(A-6)

In contrast to procedure (A-5), this approach has iterations of quadratic complexity O( Ω 2). However, it avoids the explicit use of high-dimensional embeddings φp replacing them by the kernel

matrix K in all computations, a.k.a. the kernel trick. Generalizing Theorem 1 in Sec. 2.1 we can interpret weighted

KM procedures (A-5,A-6) as linear bound optimization.

Theorem 6 (bound for (35,A-1)). Weighted KM procedures (A-5)
or (A-6) can be seen as bound optimization methods for weighted K-means objective F w(S) (35) or (A-1) using auxiliary function

at(S)

∶=

F

w

(

S

,

µ

w t

)

(A-7)

at any current segmentation St = {Stk} with means µwt = {µwSk }. t

Proof. Indeed, (A-4) implies at(S) ≥ F w(S). Since at(St) = F w(St) then at(S) is a proper bound for F w(S). Resegmentation step (A-5) produces optimal segments St+1
minimizing the bound at(S). Re-centering step minimizing F w(St+1, m) for ﬁxed segments produces means µwt+1 deﬁning the bound at+1(S) for the next iteration.

Since algebraic formulations (A-3) and (A-2) omit the same constant we also get the following Corollary.

Corollary 1 (bound for (36,A-2)). Weighted KM procedures
(A-5) or (A-6) can be seen as bound optimization methods for wkKM objective F w(S) in (A-2) using auxiliary function

at(S) ∶=

w µwSk 2 − 2W φ′µwSk ′ Sk

kt t

(A-8)

≡

⎛w Stk′W KW Stk − W KW Stk

2

′
⎞ Sk

(A-9)

k ⎝ (w′Stk)2

w′Stk ⎠

at any current segmentation St ∶= {Stk} with means µwt ∶= {µwSk }. t
Proof. The ﬁrst expression follows from Th. 6 and formula (A-3) for F w(S, m) at m = µwt . Also, (33) and K = φ′φ imply the second expression for bound at(S).

The second expression for at(S) in Corollary 1 allows to obtain a linear bound for NC objective (41). For simplicity, assume positive deﬁnite afﬁnity A. As follows from [52], [40], [58] and
a simple derivation in our Sec. 1.3.1, normalized cut (41) with p.d. afﬁnity A is equivalent to wkKM (36,A-2) with weights and
kernel in (42)

w = d ∶= A1

and

K = W −1AW −1.

28

Then, (A-9) implies the following linear bound for NC

⎛ Stk′AStk

′
k2⎞ k

k

d ⎝

(d′ Stk )2

− ASt

d′Stk ⎠

S

that agrees with the kernel bound for NC in Table 2. Equivalence of weighted AA and AD to wkKM: Figure 33
extends Figure 3 by relating weighted generalizations of standard pairwise clustering objectives to wkKM. The equivalence of objectives in Figure 33 can be veriﬁed by simple algebra. One additional simple property below is also needed. It is easy to prove.

Proposition 1. (e.g. Roth et al. [38]) For any symmetric matrix

M deﬁne

M˜ ∶= M + δI

where I is an identity matrix. Then, matrix M˜ is positive semi-
deﬁnite (psd) for any scalar δ ≥ −λ0(M ) where λ0 is the smallest eigenvalue of its argument.

APPENDIX B (WEAK KERNEL K-MEANS)

For Hilbertian distortions

d=

2 k

with

p.s.d.

kernels

we

can

show that pairwise kKM approach (21) is “stronger” than a

pointwise pKM approach (13) using the same metric. In this case

pKM can be called weak kernel K-means, see Figure 34. Equiv-

alent kKM formulation (18) guarantees more complex decision

boundaries, see Fig.1(h), compared to pKM energy (13)

Ip − mk

2 k

k p∈Sk

(B-1)

≡

φ(Ip) − φ(mk) 2

k p∈Sk

with isometric kernel distance k and some point mk in the

original space, Fig.1(g). Indeed, any mk in the original space

corresponds to some search point φ(mk) in the high-dimensional embedding space, while the opposite is false. Thus, optimization

of (21) and (18) has larger search space than (B-1). It is also easy

to check that energy (B-1) is an upper bound for (21) at any S. For

this reason we refer to distortion energy (B-1) with kernel distance

k and explicit mk in the original space as weak kernel K-means.

Pointwise energy (B-1) is an example of pKM (13), while pairwise

energy (21) with the same kernel metric is kKM.

Note that weak kernel K-means (B-1) for Gaussian kernel

corresponds to K-modes closely related to mean-shift [114], [39],

as discussed below. Some results comparing K-modes (weak

kKM) to regular kKM are shown in Fig.1(g,h). Figure 34 illustrates

general relations between kernel K-means (21) and probabilistic

K-means (13,14). It includes a few examples discussed earlier and

some more examples discussed below.

K-modes and mean-shift: Weak kernel K-means using unary

formulation (B-1) with kernel distance k and explicit optimiza-

tion of mk in the original data space is closely related to K-modes

approach to clustering continuous [30] or discrete [115], [116]

data. For example, for Gaussian kernel distance k energy (B-1)

becomes

− e − Ip−mk 2 2σ2 k p∈Sk

or, using Parzen densities Pσ( ⋅ Sk) for points {Ip p ∈ Sk},

= − Sk ⋅ Pσ(mk Sk).
k

(B-2)

29

Fig. 33: Equivalence of kernel K-means (kKM), average distortion (AD), average association (AA) for the general case of weighted

points. Typically, kKM is associated with positive semi-deﬁnite (p.s.d.) or conditionally positive-deﬁnite (c.p.d.) kernels [112] and

Hilbertian distortions [47]. The above formulations of AA and AD make no assumptions for association matrix A and distortions D

except for zero diagonal in D. Then, equivalence of AD and AA objectives (up to a constant) is straightforward. Roth et al. [38]

reduce non-weighted case of AD to kKM. For arbitrary D they derive Hilbertian distortion

2 k

with

an

equivalent

AD

energy

(up

to

a constant) and explicitly construct the corresponding embedding φ. We show Hilbertian metric

2 k

for

the

general

weighted

case

of

AD, see AD→kKM above. Dhillon et al. [40], [58] prove that normalized cuts is a special case of weighted kKM and construct the corresponding p.d. kernel. Sec.1.3.1 shows a simpler reduction of normalized cuts to weighted kKM. Similarly to [38], an equivalent

p.d. kernel could be constructed for any association matrix A, see AA→kKM above. Note that the formulas for A-equivalent p.d. kernel

and D-equivalent Hilbertian metric require some sufﬁciently large diagonal shift δ. Roth et al. [38] relate proper δ to the smallest

eigenvalue

of

A

=

−

D 2

.

Our

weighted

formulation

requires

the

eigenvalue

of

1
diag(W)− 2

⋅

A

⋅

1
diag(W)− 2 .

pointwise distortions (likelihoods) energy (A) ∑k ∑p∈Sk wp Ip − θSk d

pairwise distortions energy

(B)

∑k

∑pq∈Sk wpwq Ip−Iq

2

w

d

≡

∑k

Sk′W DW Sk 2w′ S k

∑p∈Sk p

Fig. 34: Clustering with (A) pointwise and (B) pairwise distortion energies generalizing (13) and (26) for points with weights w = {wp}. Pointwise distortion relates a data point and a model as log-likelihood function Ip − θ d = − ln P(Ip θ). Pairwise distortion is deﬁned by matrix Dpq = Ip − Iq d. Weighted AD or AA for arbitrary metrics are equivalent to weighted kKM, see Figures 3 and 33. As shown in [38], [40] average cut, normalized cut [8], and spectral ratio cut [113] are examples of (weighted) kKM.

Clearly, optimal mk are modes of Parzen densities in each

segment. K-modes objective (B-2) can be seen as an energy-

based formulation of mean-shift clustering [114], [39] with a

ﬁxed number of clusters. Formal objective allows to combine

color clustering via (B-2) with geometric regularization in the

image domain [29]. If needed, the number of clusters (modes) can

be regularized by adding label cost [12]. In contrast, mean-shift

segmentation [39] clusters RGBXY space combining color and

spatial information. The number of clusters is indirectly controlled

by the bandwidth.

Note that K-modes energy (B-2) follows from a weak kKM

approach (B-1) for arbitrary positive normalized kernels. Such

kernels deﬁne different Parzen densities, but they all lead to energy

(B-2) where optimal mk are modes of the corresponding densities.

Therefore, different kernels in (B-1) give different modes in (B-2).

Many optimization methods can be used for K-modes energy.

For example, it is possible to use iterative (block-coordinate

descent) approach typical of K-means methods: one step reclusters

points and the other step locally reﬁnement the modes, e.g. using

mean-shift operation [29]. For better optimization, local reﬁne-

ment of the mode in each cluster can be replaces by the best

mode search tracing all points within each cluster using mean-

shift. RANSAC-like sampling procedure can be used for some

compromise between speed and quality. It is also possible to use

exhaustive search for the strongest mode in each cluster over

observed discrete features and then locally reﬁne each cluster’s

mode with mean-shift.

It is also interesting that discrete version of K-modes for

histograms

[115],

[116]

deﬁne

modes

mk

=

(m

1 k

,

...,

m

j k

,

...

)

combining marginal modes for all attributes or dimensions j.

Implicitly, they use distortion k for discrete kernel k(x, y) = ∑j[xj = yj] where [⋅] are Iverson brackets. Marginal modes could be useful for aggregating sparse high-dimensional data.

Analogously, we can also deﬁne a continuous kernel for

marginal modes as

k(x, y) =

e . −(xj −yj )2 2σ2

j

(B-3)

Note that this is different from the standard Gaussian kernel

e = − x−y 2 2σ2

e , −(xj −yj )2 2σ2

j

which leads to regular modes energy (B-2). It is easy to check that kernel (B-3) corresponds to weak kKM energy

−
k

e −(Ipj −mjk )2 2σ2
j p∈Sk

= − Sk ⋅ Pjσ(mjk Sk)

k

j

where Pσj is a marginal Parzen density for dimension j.

APPENDIX C (ENTROPY CLUSTERING)
First, we show that (14) reduces to (15) for descriptive models. Indeed, assume P(θ) ≡ P(⋅ θ) is a continuous density of a sufﬁciently descriptive class (e.g. GMM). For any function f (x) Monte-Carlo estimation gives for any subset S ⊂ Ω

f (Ip) ≈ S ⋅
p∈S

f (x)ds(x)dx ≡ S ⋅ ⟨f, ds⟩

30
Fig. 35: Histograms in color spaces. Entropy criterion (15) with histograms can not tell a difference between A and B: bin permutations do not change the histogram’s entropy.
where ds is a “true” density for intensities {Ip p ∈ S} and ⟨, ⟩ is a dot product. If f = − log P(θs) and ds ≈ P(θs) then (14) implies (15) for differential entropy H(S) ∶= H(P(θs)). For histograms Ph(S) ≡ Ph(⋅ S) entropy-based interpretation (15) of (14) is exact for discrete entropy
H(S) ∶= − Ph(x S) ⋅ log Ph(x S) ≡ −⟨Ph(S), log Ph(S)⟩.
x
Intuitively, minimization of the entropy criterion (15) favors clusters with tight or “peaked” distributions. This criterion is widely used in categorical clustering [34] or decision trees [35], [36] where the entropy evaluates histograms over “naturally” discrete features. Below we discuss limitations of the entropy clustering criterion with either discrete histograms or continuous GMM densities in the context of color feature spaces.
The case of histograms: In this case the key problem for color space clustering is illustrated in Fig.35. Once continuous color space is broken into bins, the notion of proximity between the colors in the nearby bins is lost. Since bin permutations do not change the histogram entropy, criterion (15) can not distinguish the quality of clusterings A and B in Fig.35; some permutation of bins can make B look very similar to A.
The case of GMM densities: In this case the problem of entropy clustering (15) is different. In general, continuous density estimators commonly use Gaussian kernels, which preserve the notion of continuity in the color space. Indeed, the (differential) entropy for any reasonable continuous density estimate will see a signiﬁcant difference between the clusters in A and B, see Fig.35.
We observe that the main issue for entropy criterion (15) with GMM densities is related to optimization problems. In this case high-order energies (15) or (14) require joint optimization of discrete variables Sp and a large number of additional continuous parameters for optimum GMM density P(⋅ θS). That is, the use of complex parametric probability models leads to complex highorder mixed objective functions. Typical block coordinate descent methods [23], [26] iterating optimization of S and θ are sensitive to local minima, see Figures 2 and 1(e). Better solutions like Figure 1(f) have lower energy, but they can not be easily found unless initialization is very good.
These problems of pKM with histograms or GMM may explain why descriptive model ﬁtting is not common in the learning community for clustering high-dimensional continuous spaces. Instead of pKM they often use a different extension of K-means, that is kernel K-means (kKM) or related pairwise clustering criteria like Normalize Cut (NC), see Sec.1.2.2 and 1.3.1.

APPENDIX D (PSEUDO BOUND LEMMA)
Lemma 2. Consider function e ∶ {0, 1} Ω → R1 deﬁned by any symmetric matrix A and (strictly) positive vector w as

X ′ AX e(X) = − w′X .

Function Tt is a pseudo-bound of e(X) at Xt for W ∶= diag(w)

Tt(X, δ) ∶= ∇e(Xt)′X + δ (1 − 2wX′Xt)t′W X + 1 (D-1)

where ∇e(X) = w (Xw′′AXX)2 − AX w2′X . Furthermore, Tt(X, δ)

is

an

auxiliary

function

for

e(X )

for

all

δ

≥

−λ0(W

−1 2

AW

−1 2

)

where λ0 denotes the smallest eigenvalue of the matrix.

Proof. Diagonal shift δW for matrix A deﬁnes function

eδ(X) ∶= − X′(δWw′X+ A)X ≡ e(X) − δ Xw′W′XX .

According to Lemma 1 function eδ is concave for any δ ≥ −λ0 since (δW + A) is p.s.d. for such δ. Thus, (53) deﬁnes a Taylorbased linear upper bound for eδ for δ ≥ −λ0

∇eδ(Xt)′ X.

We have eδ(X) = e(X) − δ for Boolean X where X′W X = w′X. Thus, the following upper bound is valid for optimizing e(X) over X ∈ {0, 1} Ω at δ ≥ −λ0

Tt(X, δ) = ∇eδ(Xt)′ X + δ.

(D-2)

where the deﬁnition of eδ above yields gradient expression

∇eδ(Xt) = ∇e(Xt) − δ 2 ($w′$X$t) W Xt − ($Xt$′W$X$t) w (w′Xt)2£
= ∇e(Xt) + δ W (w1 ′−X2tXt)

where W 1 = w for matrix W = diag(w) and Xt′W Xt = w′Xt for any Xt since all iterations explore only Boolean solutions.

APPENDIX E (PROOF OF Gini Bias THEOREM 5)

Let dΩ be a continuous probability density function over domain Ω ⊆ Rn deﬁning conditional density

ds(x) ∶= dΩ(x x ∈ S)

(E-1)

for any non-empty subset S ⊂ Ω and expectation

Ez ∶= z(x)dΩ(x)dx

for any function z ∶ Ω → R1. Suppose Ω is partitioned into two sets S and S¯ such that S ∪
S¯ = Ω and S ∩ S¯ = ∅. Note that S here and in the statement of Theorem 5 is not a discrete set of observations, which is what S
means in the rest of the paper. Theorem 5 states a property of a
fully continuous version of Gini criterion (78) that follows from
an additional application of Monte-Carlo estimation allowing to replace discrete set cardinality S by probability w of a continuous subset S

w ∶= dΩ(x)dx = dΩ(x) ⋅ [x ∈ S]dx = E[x ∈ S].
S
Then, minimization of EG(S) in (78) corresponds to maximization of the following objective function

L(S) ∶= w ds2(x)dx + (1 − w) d¯s2(x)dx. (E-2)

31

Note that conditional density ds in (E-1) can be written as

ds(x) = dΩ(x) ⋅ [x w∈ S]

(E-3)

where [⋅] is an indicator function. Eqs. (E-3) and (E-2) give

1 L(S) = w

d2Ω(x)[x ∈ S]dx + 1 −1 w

d2Ω(x)[x ∈ S¯]dx.

Introducing notation

I ∶= [x ∈ S] and F ∶= dΩ(x)
allows to further rewrite objective function L(S) as
L(S) = EEIIF + EF1 −(1E−II) . Without loss of generality assume that EF1−(E1−II) ≤ EEFII (the opposite case would yield a similar result). We now need the following lemma.

Lemma 3. Let a, b, c, d be some positive numbers, then

ac b≤d

⇒ ab ≤ ab ++ dc ≤ dc .

Proof. Use reduction to a common denominator.

Lemma 3 implies inequality
EF1 −(1E−II) ≤ EF ≤ EEFII which is needed to prove the Proposition below.

(E-4)

Proposition 2. (Gini-bias) Assume that subset Sε ⊂ Ω is Sε ∶= {x ∶ dΩ(x) ≥ sup dΩ(x) − ε}.
x
Then

(E-5)

sup L(S) = lim L(Sε) = EdΩ + sup dΩ(x).

S

ε→0

x

(E-6)

Proof. Due to monotonicity of expectation we have

EEFII ≤ E (I supExIdΩ(x)) = suxp dΩ(x).

(E-7)

Then (E-4) and (E-7) imply

L(S) = EEFII + EF1 −(1E−II) ≤ suxp dΩ(x) + EF.
That is, the right part of (E-6) is an upper bound for L(S). Let Iε ≡ [x ∈ Sε]. It is easy to check that
lim EF (1 − Iε) = EF. ε→0 1 − EIε Deﬁnition (E-5) also implies

(E-8) (E-9)

lim EF Iε ≥ lim E(supx dΩ(x) − ε)Iε = sup dΩ(x).

ε→0 EIε ε→0

EIε

x

This result and (E-7) conclude that

lim EF Iε = sup dΩ(x).

ε→0 EIε

x

(E-10)

Finally, the limits in (E-9) and (E-10) imply

lim L(Sε) = lim EF (1 − Iε) + lim EF Iε

ε→0

ε→0 1 − EIε ε→0 EIε

= EdΩ + sup dΩ(x).
x

This equality and bound (E-8) prove (E-6).

ACKNOWLEDGEMENTS
We would like to greatly thank Carl Olsson (Lund University,
Sweden) for many hours of stimulating discussions, as well as
for detailed feedback and valuable recommendations at different
stages of our work. We also appreciate his tremendous patience
when our thoughts were much more confusing than they might be
now. Ivan Stelmakh (PhysTech, Russia) also gave helpful feedback
on our draft and caught several errors. Anders Eriksson (Lund University, Sweden) helped with related work on NC with constraints.
We also thank Jianbo Shi (UPenn, USA) for his feedback and his
excellent and highly appreciated spectral-relaxation optimization
code for normalized cuts.
REFERENCES
[1] S. Geman and D. Geman, “Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images,” IEEE transactions on Pattern Analysis and Machine Intelligence, vol. 6, pp. 721–741, 1984. 1
[2] D. Mumford and J. Shah, “Optimal approximations by piecewise smooth functions and associated variational problems,” Comm. Pure Appl. Math., vol. 42, pp. 577–685, 1989. 1
[3] V. Caselles, R. Kimmel, and G. Sapiro, “Geodesic active contours,” International Journal of Computer Vision, vol. 22, no. 1, pp. 61–79, 1997. 1, 2
[4] Y. Boykov and M.-P. Jolly, “Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images,” in ICCV, vol. I, July 2001, pp. 105–112. 1, 2, 8, 22, 24
[5] Y. Boykov and V. Kolmogorov, “Computing geodesics and minimal surfaces via graph cuts,” in International Conference on Computer Vision, vol. I, 2003, pp. 26–33. 1, 2
[6] T. Pock, A. Chambolle, D. Cremers, and H. Bischof, “A convex relaxation approach for computing minimal partitions,” in IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2009. 1
[7] C. C. Aggarwal and C. K. Reddy, Eds., Data Clustering: Algorithms and Applications. Chapman & Hall / CRC, 2014. 1
[8] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactionson Pattern Analysis and Machine Intelligence (PAMI), vol. 22, pp. 888–905, 2000. 1, 3, 4, 5, 6, 7, 8, 12, 14, 15, 17, 18, 19, 20, 29
[9] S. Li, Markov Random Field Modeling in Image Analysis, 3rd ed. Springer-Verlag, 2009. 1
[10] Y. Boykov, O. Veksler, and R. Zabih, “Fast approximate energy minimization via graph cuts,” IEEE transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 11, pp. 1222–1239, November 2001. 1, 2, 8, 11, 17
[11] P. Kohli, P. H. Torr et al., “Robust higher order potentials for enforcing label consistency,” International Journal of Computer Vision, vol. 82, no. 3, pp. 302–324, 2009. 1, 2, 8, 11, 21
[12] A. Delong, A. Osokin, H. Isack, and Y. Boykov, “Fast Approximate Energy Minization with Label Costs,” Int. J. of Computer Vision (IJCV), vol. 96, no. 1, pp. 1–27, January 2012. 1, 2, 3, 5, 8, 11, 20, 30
[13] K. Park and S. Gould, “On learning higher-order consistency potentials for multi-class pixel labeling,” in ECCV, 2012. 2, 21
[14] M. Tang, L. Gorelick, O. Veksler, and Y. Boykov, “Grabcut in one cut,” in International Conference on Computer Vision (ICCV), Sydney, Australia, December 2013. 2, 3, 8, 21
[15] J. S. Yedidia, W. T. Freeman, and Y. Weiss, “Constructing free-energy approximations and generalized belief propagation algorithms,” IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282–2312, 2005. 2
[16] V. Kolmogorov, “Convergent tree-reweighted message passing for energy minimization,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 28, no. 10, pp. 1568–1583, 2006. 2
[17] T. Werner, “A linear programming approach to max-sum problem: A review,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 7, pp. 1165–1179, 2007. 2, 11
[18] J. H. Kappes, B. Andres, F. A. Hamprecht, C. Schno¨rr, S. Nowozin, D. Batra, S. Kim, B. X. Kausler, T. Kro¨ger, J. Lellmann et al., “A comparative study of modern inference techniques for structured discrete energy minimization problems,” International Journal of Computer Vision, vol. 115, no. 2, pp. 155–184, 2015. 2
[19] A. Chambolle, “An algorithm for total variation minimization and applications,” Journal of Mathematical imaging and vision, vol. 20, no. 1-2, pp. 89–97, 2004. 2, 11

32
[20] A. Chambolle and T. Pock, “A ﬁrst-order primal-dual algorithm for convex problems with applications to imaging,” Journal of Mathematical Imaging and Vision, vol. 40, no. 1, pp. 120–145, 2011. 2, 11
[21] D. Cremers, M. Rousson, and R. Deriche, “A review of statistical approaches to level set segmentation: integrating color, texture, motion and shape,” International journal of computer vision, vol. 72, no. 2, pp. 195–215, 2007. 2, 11
[22] Y. Boykov and G. Funka-Lea, “Graph cuts and efﬁcient N-D image segmentation,” International Journal of Computer Vision (IJCV), vol. 70, no. 2, pp. 109–131, 2006. 2
[23] S. C. Zhu and A. Yuille, “Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 18, no. 9, pp. 884–900, Sept. 1996. 2, 3, 5, 8, 30
[24] T. Chan and L. Vese, “Active contours without edges,” IEEE Trans. Image Processing, vol. 10, no. 2, pp. 266–277, 2001. 2, 8
[25] I. B. Ayed, A. Mitiche, and Z. Belhadj, “Multiregion level set partitioning of synthetic aperture radar images,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 27, no. 5, pp. 793—800, 2005. 2, 3, 5
[26] C. Rother, V. Kolmogorov, and A. Blake, “Grabcut - interactive foreground extraction using iterated graph cuts,” in ACM trans. on Graphics (SIGGRAPH), 2004. 2, 3, 5, 8, 20, 21, 23, 24, 25, 30
[27] M. Kearns, Y. Mansour, and A. Ng, “An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering,” in Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), August 1997. 2, 3, 5, 8, 9, 18, 19
[28] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classiﬁcation. John Wiley & Sons, 2001. 2, 5, 6, 27
[29] M. B. Salah, A. Mitiche, and I. B. Ayed, “Effective level set image segmentation with a kernel induced data term,” IEEE Transactions on Image Processing, vol. 19, no. 1, pp. 220–232, 2010. 3, 5, 7, 30
[30] M. A. Carreira-Perpinan and W. Wang, “The K-Modes Algorithm for Clustering,” in arXiv:1304.6478v1 [cs.LG], April 2013. 3, 28
[31] K. K. Sung and T. Poggio, “Example based learning for viewbased human face detection,” IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 20, pp. 39–51, 1995. 3, 5
[32] M. Rousson and D. R., “A variational framework for active and adaptative segmentation of vector valued images,” in Workshop on Motion and Video Computing, 2002. 3, 5
[33] A. Mitiche and I. B. Ayed, Variational and Level Set Methods in Image Segmentation. Springer, 2010. 3
[34] T. Li, S. Ma, and M. Ogihara, “Entropy-based criterion in categorical clustering,” in Int. Conf. on M. Learning, 2004. 3, 30
[35] L. Breiman, “Technical note: Some properties of splitting criteria,” Machine Learning, vol. 24, no. 1, pp. 41–47, 1996. 3, 5, 18, 19, 30
[36] G. Louppe, L. Wehenkel, A. Sutera, and P. Geurts, “Understanding variable importances in forests of randomized trees,” in NIPS, 2013, pp. 431–439. 3, 19, 30
[37] M. Tang, I. B. Ayed, and Y. Boykov, “Pseudo-bound optimization for binary energies,” in European Conference on Computer Vision (ECCV), 2014, pp. 691–707. 3, 9, 16, 17, 18
[38] V. Roth, J. Laub, M. Kawanabe, and J. Buhmann, “Optimal cluster preserving embedding of nonmetric proximity data,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 25, no. 12, pp. 1540—1551, 2003. 5, 6, 7, 10, 12, 13, 14, 28, 29
[39] D. Comaniciu and P. Meer, “Mean shift: a robust approach toward feature space analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 24, no. 5, pp. 603–619, 2002. 5, 7, 28, 30
[40] I. Dhillon, Y. Guan, and B. Kulis, “Kernel k-means, spectral clustering and normalized cuts,” in KDD, 2004. 4, 5, 6, 7, 8, 13, 14, 15, 16, 20, 21, 28, 29
[41] M. Tang, I. B. Ayed, D. Marin, and Y. Boykov, “Secrets of grabcut and kernel k-means,” in International Conference on Computer Vision (ICCV), Santiago, Chile, December 2015. 5, 8, 9, 22
[42] V. Vapnik, Statistical Learning Theory. Wiley, 1998. 4 [43] K. Muller, S. Mika, G. Ratsch, K. Tsuda, and B. Scholkopf, “An in-
troduction to kernel-based learning algorithms,” IEEE Trans. on Neural Networks, vol. 12, no. 2, pp. 181–201, 2001. 4 [44] M. Girolami, “Mercer kernel-based clustering in feature space,” IEEE Transactions on Neural Networks, vol. 13, no. 3, pp. 780–784, 2002. 4, 18 [45] R. Chitta, R. Jin, T. C. Havens, and A. K. Jain, “Scalable kernel clustering: Approximate kernel k-means,” in KDD, 2011, pp. 895–903. 4

[46] S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi, “Kernel methods on riemannian manifolds with gaussian rbf kernels,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. In press, 2015. 4
[47] M. Hein, T. N. Lal, and O. Bousquet, “Hilbertian metrics on probability measures and their application in svms,” Pattern Recognition, vol. LNCS 3175, pp. 270—277, 2004. 4, 29
[48] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural computation, vol. 15, no. 6, pp. 1373–1396, 2003. 4, 12, 15
[49] Y. Yu, C. Fang, and Z. Liao, “Piecewise ﬂat embedding for image segmentation,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1368–1376. 4, 12, 15
[50] K.-R. Mu¨ller, S. Mika, G. Ra¨tsch, K. Tsuda, and B. Scho¨lkopf, “An introduction to kernel-based learning algorithms,” IEEE Transactions on Neural Networks, vol. 12, no. 2, pp. 181–201, 2001. 4
[51] J. Shawe-Tayler and N. Cristianini, Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. 4, 28
[52] F. Bach and M. Jordan, “Learning spectral clustering,” Advances in Neural Information Processing Systems (NIPS), vol. 16, pp. 305–312, 2003. 6, 7, 8, 12, 14, 15, 16, 28
[53] I. Cox, S. Rao, and Y. Zhong, ““Ratio Regions”: A Technique for Image Segmentation,” in International Conference on Pattern Recognition (ICPR), 1996, pp. 557–564. 7
[54] I. Jermyn and H. Ishikawa, “Globally optimal regions and boundaries as minimum ratio weight cycles,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 23, no. 10, pp. 1075–1088, 2001. 7
[55] S. Wang and J. M. Siskind, “Image segmentation with ratio cut,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 25, no. 6, pp. 675–690, 2003. 7
[56] V. Kolmogorov, Y. Boykov, and C. Rother, “Applications of parametric maxﬂow in computer vision,” in IEEE International Conference on Computer Vision (ICCV), 2007. 7, 17
[57] D. S. Hochbaum, “Polynomial time algorithms for ratio regions and a variant of normalized cut,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 5, pp. 889–898, 2010. 7
[58] I. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors: A multilevel approach,” IEEE Transactions on Pattern Analysis and Machine Learning (PAMI), vol. 29, no. 11, pp. 1944– 1957, November 2007. 7, 14, 28, 29
[59] S. Yu and J. Shi, “Multiclass spectral clustering,” in International Conference on Computer Vision (ICCV), 2003. 7, 15
[60] T. Hofmann and J. Buhmann, “Pairwise data clustering by deterministic annealing,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 19, no. 1, pp. 1–14, January 1997. 7
[61] R. Duda, P. Hart, and D. Stork, Pattern classiﬁcation. John Wiley & Sons, 2001. 7
[62] B. Kulis, S. Basu, I. Dhillon, and R. Mooney, “Semi-supervised graph clustering: a kernel approach,” Machine Learning, vol. 74, no. 1, pp. 1–22, January 2009. 7, 8
[63] S. X. Yu and J. Shi, “Segmentation given partial grouping constraints,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 26, no. 2, pp. 173–183, 2004. 7, 8
[64] L. Xu, W. Li, and D. Schuurmans, “Fast normalized cut with linear constraints,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 2866–2873. 8
[65] A. Eriksson, C. Olsson, and F. Kahl, “Normalized cuts revisited: A reformulation for segmentation with linear grouping constraints,” Journal of Mathematical Imaging and Vision, vol. 39, no. 1, pp. 45– 61, 2011. 8
[66] J. Malik, S. Belongie, T. Leung, and J. Shi, “Contour and texture analysis for image segmentation,” International journal of computer vision, vol. 43, no. 1, pp. 7–27, 2001. 8, 15, 20
[67] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical image segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 5, pp. 898–916, 2011. 8, 15, 16, 20, 21
[68] S. E. Chew and N. D. Cahill, “Semi-supervised normalized cuts for image segmentation,” in The IEEE International Conference on Computer Vision (ICCV), December 2015. 8
[69] P. Krahenbuhl and V. Koltun, “Efﬁcient inference in fully connected CRFs with Gaussian edge potentials,” in NIPS, 2011. 8
[70] M. Tang, D. Marin, I. B. Ayed, and Y. Boykov, “Normalized Cut meets MRF,” in European Conference on Computer Vision (ECCV), Amsterdam, Netherlands, October 2016. 8

33
[71] K. Lange, D. R. Hunter, and I. Yang, “Optimization transfer using surrogate objective functions,” Journal of Computational and Graphical Statistics, vol. 9, no. 1, pp. 1–20, 2000. 9
[72] M. Narasimhan and J. A. Bilmes, “A submodular-supermodular procedure with applications to discriminative structure learning,” in UAI, 2005, pp. 404–412. 9
[73] I. Ben Ayed, L. Gorelick, and Y. Boykov, “Auxiliary cuts for general classes of higher order functionals,” in IEEE conference on Computer Vision and Pattern Recognition (CVPR), Portland, Oregon, June 2013, pp. 1304–1311. [Online]. Available: http: //www.csd.uwo.ca/∼yuri/Abstracts/cvpr13-auxcut-abs.shtml 9, 17
[74] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004. 10
[75] M. S. Bazaraa, H. D. Sherali, and C. M. Shetty, Nonlinear Programming: Theory and Algorithms. Wiley, 2006. 10
[76] H. Ishikawa, “Exact optimization for Markov Random Fields with convex priors,” IEEE transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 10, pp. 1333–1336, 2003. 11
[77] V. Kolmogorov, “Convergent Tree-Reweighted Message Passing for Energy Minimization,” IEEE transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 10, pp. 1568–1583, October 2006. 11
[78] T. F. Cox and M. A. Cox, Multidimensional scaling. CRC Press, 2000. 12, 13
[79] A. Ng, M. Jordan, and Y. Weiss, “On spectral clustering: analysis and an algorithm,” in Advances in neural information processing systems (NIPS), vol. 2, 2002, pp. 849–856. 12, 15
[80] G. H. Golub and C. F. Van Loan, Matrix computations. JHU Press, 2012, vol. 3. 12
[81] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and computing, vol. 17, no. 4, pp. 395–416, 2007. 14, 15
[82] Y. Boykov, O. Veksler, and R. Zabih, “Fast approximate energy minimization via graph cuts,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 23, no. 11, pp. 1222–1239, 2001. 15
[83] S. Belongie and J. Malik, “Finding boundaries in natural images: A new method using point descriptors and area completion,” in Proceedings of the European Conference on Computer Vision (ECCV), 1998. 15
[84] L. Gorelick, F. R. Schmidt, and Y. Boykov, “Fast trust region for segmentation,” in IEEE conference on Computer Vision and Pattern Recognition (CVPR), Portland, Oregon, June 2013, pp. 1714– 1721. [Online]. Available: http://www.csd.uwo.ca/∼yuri/Abstracts/ cvpr13-ftr-abs.shtml 16, 17, 18
[85] S. Vicente, V. Kolmogorov, and C. Rother, “Joint optimization of segmentation and appearance models,” in International Conf. on Computer Vision (ICCV), 2009. 17, 20
[86] I. Ben Ayed, H.-M. Chen, K. Punithakumar, I. Ross, and S. Li, “Graph cut segmentation with a global constraint: Recovering region distribution via a bound of the bhattacharyya measure,” in CVPR, 2010, pp. 3288–3295. 17
[87] C. M. Bishop, Pattern Recognition and Machine Learning. Springer, August 2006. 18, 19
[88] Y. Boykov, H. Isack, C. Olsson, and I. B. Ayed, “Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions,” in International Conference on Computer Vision (ICCV), December 2015. 18, 19
[89] J. Nash, “The imbedding problem for riemannian manifolds,” Annals of Mathematics, vol. 63, no. 1, pp. 20–63, 1956. 19
[90] T. Brox and J. Malik, “Large displacement optical ﬂow: descriptor matching in variational motion estimation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 3, pp. 500–513, 2011. 20, 24, 26
[91] V. Lempitsky, A. Blake, and C. Rother, “Image segmentation by branchand-mincut,” in ECCV, 2008. 20
[92] S. Paris and F. Durand, “A fast approximation of the bilateral ﬁlter using a signal processing approach,” in Computer Vision–ECCV 2006. Springer, 2006, pp. 568–580. 20
[93] A. Adams, J. Baek, and M. A. Davis, “Fast high-dimensional ﬁltering using the permutohedral lattice,” Computer Graphics Forum, vol. 29, no. 2, pp. 753–762, 2010. 20
[94] M. D. Collins, J. Liu, J. Xu, L. Mukherjee, and V. Singh, “Spectral clustering with a convex regularizer on millions of images,” in Computer Vision–ECCV 2014. Springer, 2014, pp. 282–298. 20, 21
[95] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic representation of the spatial envelope,” International journal of computer vision, vol. 42, no. 3, pp. 145–175, 2001. 21

[96] [97] [98] [99]
[100]
[101] [102] [103]
[104]
[105] [106]
[107] [108] [109] [110]
[111] [112] [113] [114] [115] [116]

A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105. 21 V. Lempitsky, P. Kohli, C. Rother, and T. Sharp, “Image segmentation with a bounding box prior,” in Int. Conference on Computer Vision (ICCV), 2009, pp. 277–284. 22 K. McGuinness and N. E. Oconnor, “A comparative evaluation of interactive segmentation algorithms,” Pattern Recognition, vol. 43, no. 2, pp. 434–444, 2010. 22 C. Nieuwenhuis and D. Cremers, “Spatially varying color distributions for interactive multilabel segmentation,” IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 5, pp. 1234–1247, 2013. 22 D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,” in Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, vol. 2. IEEE, 2001, pp. 416–423. 22 M. Varma and A. Zisserman, “A statistical approach to texture classiﬁcation from single images,” International Journal of Computer Vision, vol. 62, no. 1-2, pp. 61–81, 2005. 23 P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “Indoor segmentation and support inference from rgbd images,” in ECCV, 2012. 23, 27 M. Dou, J. Taylor, H. Fuchs, A. Fitzgibbon, and S. Izadi, “3d scanning deformable objects with a single rgbd sensor,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 493–501. 23 R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon, “Kinectfusion: Real-time dense surface mapping and tracking,” in Mixed and augmented reality (ISMAR), 2011 10th IEEE international symposium on. IEEE, 2011, pp. 127–136. 23 Z. Deng, S. Todorovic, and L. J. Latecki, “Semantic segmentation of rgbd images with mutex constraints,” in International Conference on Computer Vision (ICCV), Santiago, Chile, December 2015. 23 V. Gulshan, V. Lempitsky, and A. Zisserman, “Humanising grabcut: Learning to segment humans using the kinect,” in Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on. IEEE, 2011, pp. 1127–1133. 23 X. Ren, L. Bo, and D. Fox, “Rgb-(d) scene labeling: Features and algorithms,” in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2759–2766. 23 J.-M. Gottfried, J. Fehr, and C. S. Garbe, “Computing range ﬂow from multi-modal kinect data,” in Advances in Visual Computing. Springer, 2011, pp. 758–767. 23 T. Brox and J. Malik, “Object segmentation by long term analysis of point trajectories,” in Computer Vision–ECCV 2010. Springer, 2010, pp. 282–295. 24, 25, 26 P. Ochs and T. Brox, “Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions,” in Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 1583–1590. 25 M. Menze and A. Geiger, “Object scene ﬂow for autonomous vehicles,” in Conference on Computer Vision and PatternRecognition (CVPR), 2015. 25, 27 B. Scho¨lkopf, “The kernel trick for distances,” in Advances in Neural Information Processing Systems (NIPS), 2001, pp. 301–307. 29 P. Chan, M. Schlag, and J. Zien, “Spectral k-way ratio cut partitioning,” vol. 13, pp. 1088–1096, 1994. 29 Y. Cheng, “Mean shift, mode seeking, and clustering,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 17, no. 8, pp. 790–799, 1995. 28, 30 Z. Huang, “Extensions to the k-means algorithm for clustering large data sets with categorical values,” Data mining and knowledge discovery, vol. 2, no. 3, pp. 283–304, 1998. 28, 30 A. Chaturvedi, P. E. Green, and J. D. Caroll, “K-modes clustering,” Journal of Classiﬁcation, vol. 18, no. 1, pp. 35–55, 2001. 28, 30

34

optimization

problems

Meng Tang is a PhD candidate in computer science at the University of Western Ontario, Canada, supervised by Prof. Yuri Boykov. He obtained MSc in computer science in 2014 from the same institution for his thesis titled ”Color Separation for Image Segmentation”. Previously in 2012 he received B.E. in Automation from the Huazhong University of Science and Technology, China. He is interested in image segmentation and semi-supervised data clustering. He is also obsessed and has experiences on discrete for computer vision and machine learning.

Dmitrii Marin received Diploma of Specialist from the Ufa State Aviational Technical University in 2011, and M.Sc. degree in Applied Mathematics and Information Science from the National Research University Higher School of Economics, Moscow, and graduated from the Yandex School of Data Analysis, Moscow, in 2013. In 2010 obtained a certiﬁcate of achievement at ACM ICPC World Finals, Harbin. His is a PhD candidate at the Department of Computer Science, University of Western Ontario under supervision of Yuri Boykov. His research is focused on designing general unsupervised and semi-supervised methods for accurate image segmentation and object delineation.

Ismail Ben Ayed received the PhD degree (with the highest honor) in computer vision from the Institut National de la Recherche Scientiﬁque (INRS-EMT), Montreal, QC, in 2007. He is currently Associate Professor at the Ecole de Technologie Superieure (ETS), University of Quebec, where he holds a research chair on Artiﬁcial Intelligence in Medical Imaging. Before joining the ETS, he worked for 8 years as a research scientist at GE Healthcare, London, ON, conducting research in medical image analysis. He also holds an adjunct professor appointment at the University of Western Ontario (since 2012). Ismail’s research interests are in computer vision, optimization, machine learning and their potential applications in medical image analysis.

Yuri Boykov received ”Diploma of Higher Education” with honors at Moscow Institute of Physics and Technology (department of Radio Engineering and Cybernetics) in 1992 and completed his Ph.D. at the department of Operations Research at Cornell University in 1996. He is currently a full professor at the department of Computer Science at the University of Western Ontario. His research is concentrated in the area of computer vision and biomedical image analysis. In particular, he is interested in problems of early vision, image segmentation, restoration, registration, stereo, motion, model ﬁtting, feature-based object recognition, photo-video editing and others. He is a recipient of the Helmholtz Prize (Test of Time) awarded at International Conference on Computer Vision (ICCV), 2011 and Florence Bucke Science Award, Faculty of Science, The University of Western Ontario, 2008.

