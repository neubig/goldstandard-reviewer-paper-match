Which Evaluations Uncover Sense Representations that Actually Make Sense?
Fenfei Guo, Jordan Boyd-Graber, Mohit Iyyer, Leah Findlater

arXiv:1804.08077v2 [cs.CL] 16 Dec 2019

Abstract
Text representations are critical for modern natural language processing. One form of text representation, sense-specific embeddings, reflect a word’s sense in a sentence better than single-prototype word embeddings tied to each type. However, existing sense representations are not uniformly better: although they work well for computer-centric evaluations, they fail for human-centric tasks like inspecting a language’s sense inventory. To expose this discrepancy, we propose a new coherence evaluation for sense embeddings. We also describe a minimal model (Gumbel Attention for Sense Induction) optimized for discovering interpretable sense representations that are more coherent than existing sense embeddings.
1 Context, Sense, and Representation
Computers need to represent the meaning of words in context. bert (Devlin et al., 2019) and elmo (Peters et al., 2018) have dramatically changed how natural language processing represents text. Rather than one-size-fits-all word vectors that ignore the nuance of how words are used in context, these new representations have topped the leaderboards for question answering, inference, and classification.
Contextual representations have supplanted multisense embeddings (Camacho-Collados and Pilehvar, 2018). While these methods learn a vector for each sense, they do not work encode meanings in downstream tasks as well as contextual representations (Peters et al., 2018).
However, computers are not the only consumer of text representations. Humans also use word representations to understand diachronic

G Gumbel softmax M Marginalization

P

(

s

i k

|

w

i

,

c˜i

)

sense attention ...

P (cij |wi) predict contexts
M

c¯i

G

ci1

cim

. . .

...

si1 chemical si2 007
siK ﬁnancial

context embeddings lookup

C

sense embeddings lookup S

context words c˜i

center word wi

You only live twice, Mr. Bond
Figure 1: Network structure with an example of

our gasi model which learns a set of global context

embeddings C and a set of sense embeddings S.

drift, investigate a language’s sense inventory, or to cluster and explore documents. Thus, a primary role for multisense word embeddings is human understanding of word meanings. Unfortunately, multisense models have only been evaluated on computer -centric dimensions and have ignored the question of sense interpretability.
We first develop measures for how well models encode and explain a word’s meaning to a human (Sec. 4). Existing multisense models do not necessarily fare best on this evaluation; our simpler model (Gumbel Attention for Sense Induction: gasi, Sec. 2) that focuses on discrete sense selection can better capture humaninterpretable representations of senses; comparing against traditional evaluations (Sec. 5), gasi has better contextual word similarity and competitive non-contextual word similarity. Finally, we discuss the connections between representation learning and how modern contextual representations could better capture interpretable senses (Sec. 6).

2 Attentional Sense Induction
Before we explore human interpretability of sense induction, we first describe our simple models to disentangle word senses. Our two

models are built on Word2Vec (Mikolov et al., 2013a,b), which we review in Sec. 2.1. Both models use a straightforward attention mechanism to select which sense is used in a token’s context, which we contrast to alternatives for sense selection (Sec. 2.3). Building on these foundations, we introduce our model, gasi, and along the way introduce a soft-attention stepping-stone (sasi).

2.1 Foundations: Skip-Gram and Gumbel

Word2Vec jointly learns word embeddings W ∈ R|V |×d and context embeddings C ∈ R|V |×d.
More specifically, given a vocabulary V and
embedding dimension d, it maximizes the likelihood of the context words cij that surround a given center word wi in a context window c˜i,

J(W, C) ∝ ∑ ∑ log P (cij | wi; W, C),
wi∈V cij ∈c˜i
(1) where P (cij | wi) is over the vocabulary,

exp

(

c

i j

⊤

wi

)

P (cij | wi; W, C) = ∑c∈V exp (c⊤wi) . (2)

In practice, log P (cij | wi) is approximated by negative sampling. We extend it to learn rep-
resentations for individual word senses.

2.2 Gumbel Softmax
As we introduce word senses, our model will need to select which sense is relevant for a context. The Gumbel softmax (Jang et al., 2016; Maddison et al., 2016) approximates the sampling of discrete random variables; we use it to select the sense. Given a discrete random variable X with P (X = k) ∝ αk, αk ∈ (0, ∞), the Gumbel-max (Gumbel and Lieblein, 1954) refactors the sampling of X into

X = arg max(log αk + gk),

(3)

k

where the Gumbel noise gk = − log(− log(uk)) and uk are i.i.d. from Uniform(0, 1). The Gumbel softmax approximates sampling one_hot(arg maxk(log αk + gk)) by

yk = softmax((log αk + gk)/τ ). (4)

Unlike soft selection of senses, the Gumbel softmax can make harder selections, which will be more interpretable to humans.

2.3 Why Attention? Musing on Alternatives
For fine-grained sense inventories, it makes sense to have graded assignment of tokens to senses (Erk et al., 2009; Jurgens and Klapaftis, 2015). However, for coarse senses—except for humor (Miller et al., 2017)—words typically are associated with a single sense, often a single sense per discourse (Gale et al., 1992). A good model should respect this. Previous models either use non-differentiable objectives or—in the case of the current state of the art, muse (Lee and Chen, 2017)—reinforcement learning to select word senses. By using Gumbel softmax, our model both approximates discrete sense selection and is differentiable.
As we argue in the next section, applications with a human in the loop are best assisted by discrete senses; the Gumbel softmax, which we develop for our task here, helps us discover these discrete senses.
2.4 Attentional Sense Induction
Embeddings We learn a context embedding matrix C ∈ R|V |×d and a sense embedding tensor S ∈ R|V |×K×d. Unlike previous work (Neelakantan et al., 2014; Lee and Chen, 2017), no extra embeddings are kept for sense induction.
Number of Senses For simplicity and consistency with previous work, our model has K fixed senses. Ideally, if we set a large number of K, with a perfect pruning strategy, we can estimate the number of senses per type by removing duplicated senses.
However, this is challenging (McCarthy et al., 2016); instead we use a simple pruning strategy. We estimate a pruning threshold λ by averaging the estimated duplicate sense and true neighbor distances,
1 λ = 2 (mean(Ddup) + mean(Dnn)), (5)
where Ddup are the cosine distances for duplicated sense pairs and Dnn is that of true neighbors (different types). We sample 100 words and if two senses are top-5 nearest neighbors of each other, we consider them duplicates.
After pruning duplicated senses with λ, we can retrain a new model with estimated number of senses for each type by masking the sense

attentions.1 Results in Table 2 and 5 validate our pruning strategy.

Sense Attention in Objective Function

Assuming a center word wi has senses

{

si1

,

si2

,

.

.

.

,

s

i K

}

,

the

original

Skip-Gram

like-

lihood becomes a marginal distribution over

all senses of wi with sense induction probability P (sik | wi); we focus on the disambiguation given local context c˜i and estimate P (sik | wi) ≈ P (sik | wi, c˜i); and thus,

K

P (cij | wi) ≈ ∑ P (cij | sik) P (sik | wi, c˜i), (6)

k=1

  
attention

Replacing P (cij | wi) in Equation 1 with Equation 6 gives our objective function J(S, C) ∝

K
∑ ∑ log ∑ P (cij | sik)P (sik | wi, c˜i). (7)
wi∈V cij ∈c˜i k=1

Modeling Sense Attention We can model the contextual sense induction distribution with soft attention; we call the resulting model softattention sense induction (sasi); although it is a stepping stone to our final model, we compare against it in our experiments as it isolates the contributions of hard attention. In sasi, the sense attention is conditioned on the entire local context c˜i with softmax:

P (si | w , c˜ ) =

exp

(

c¯⊤i

s

i k

)

,

(8)

k i i ∑K exp (c¯⊤si )

k=1

ik

where c¯i is the mean of the context vectors in c˜i. A derivation of how this affects negative sampling is in Appendix A.

2.5 Scaled Gumbel Softmax for Sense Disambiguation
To learn distinguishable sense representations, we implement hard attention in our full model, Gumbel Attention for Sense Induction (gasi). While hard attention is conceptually attractive, it can increase computational difficulty: discrete choices are not differentiable and thus incompatible with modern deep learning frameworks. To preserve differentiability (and resorting to equally complex reinforcement learning),
1More details and analysis about pruning are in Appendix C

we apply the Gumbel softmax reparameterization trick to our sense attention function (Equation 8).

Vanilla Gumbel The discrete sense sampling from Equation 8 can be refactored

zi = one_hot(arg max(c¯i⊤sik + gk)), (9)
k

and the hard attention approximated

yki = softmax((c¯i⊤sik + gk)/τ ).

(10)

Scaled Gumbel Gumbel softmax learns a
flat distribution over senses even with low temperatures: the dot product c¯⊤i sik is too small2 compared to the Gumbel noise gk. Thus we use a scaling factor β to encourage sparser distributions,3

γki = softmax((c¯i⊤sik + βgk)/τ ), (11)

and tune it as a hyperparameter. We append gasi-β to the name of models with a scaling factor. This is critical for learning distinguishable senses (Figure 2, Table 5, and Table 2). Our final objective function for gasi-β is

K

∑ J(S, C) ∝

∑ ∑ γki log P (wc | sik).

wi∈V wc∈ci k=1

(12)

3 Data and Training
For fair comparisons, we try to remain consistent with previous work (Huang et al., 2012; Neelakantan et al., 2014; Lee and Chen, 2017) in all aspects of training. In particular, we train gasi on the same April 2010 Wikipedia snapshot (Shaoul C., 2010) with 1B tokens and the same vocabulary released by Neelakantan et al. (2014); set the number of senses K = 3 and dimension d = 300 for each word unless otherwise specified. More details are in the Appendix. Following Maddison et al. (2016), we fix the temperature τ = 0.5, and tune the scaling factor β = 0.4 using grid search within {0.1 . . . 0.9} on AvgSimC for contextual word similarity (Section 5); this tuning preceded all
2This is from float32 precision and saturation of log(σ(·)); detailed further in Figure 3 in Appendix.
3Normalizing c¯⊤i sik or directly using log P (sik | wi, c˜i) results in a similar outcome.

top neighbors of bond_0

blofeld_0

thunderball_1 octopussy_0 octopussy_1

moonraker_2 goldﬁnger_2

007_1

bond_1 bond_2 bond_0

MUSE

top neighbors of bond_1 top neighbors of bond_2

bond_0
securities_1 mortgage-backed_2
repo_1 repurchase_2 coupon_2

transition_0 bond_1
hydrogen_1 atoms_0 bonding_0
covalent_1

GASI-β0

octopussy_2 moonraker_1

goldeneye_0 goldﬁnger_0 007_2 bond_2

Model
muse mssg-30k
gasi-β

Sense Accuracy
67.33 69.33 71.33

Judgment Accuracy
62.89 66.67 67.33

Agreement
0.73 0.76 0.77

Table 1: Word intrusion evaluations on top ten nearest neighbors of sense embeddings. Users find misfit words most easily with gasi-β, suggesting these representations are more interpretable.

Figure 2: t-sne projections of nearest neighbors for “bond” by hard-attention models: muse (rlbased) and our gasi-β. Trained on same dataset and vocabulary, both models learn three vectors per word (bond_i is ith sense vector). gasi (right) learns three distinct senses of “bond” while muse (left) learns overlapping senses.
interpretability experiments. If not reprinted, numbers for competing models are either computed with pre-trained embeddings released by authors or trained on released code.
4 Evaluating Interpretability
We turn to traditional evaluations of sense embeddings later (Section 5), but our focus is on human interpretability. If you show a human the senses, can they understand why a model would assign a sense to that context? This section evaluates whether the representations make sense to human consumers of multisense models.
In the age of bert and elmo, these are the dimensions that are most critical for multisense representations. While contextual word vectors are most useful for computer understanding of meaning, humans often want an overview of word meanings for other tasks.
Sense representations are useful for humanin-the-loop applications. They help understand semantic drift (Hamilton et al., 2016): how do the meanings of “gay” reflect social progress? They help people learn languages (Noraset et al., 2017): what does it mean when someone says that I “embarrassed” them? They help linguists understand the sense inventory of a language (Kawahara et al., 2014): what are the frames that can be used by the verb “participate”? These questions (and human understanding) are helped by discrete senses, which the Gumbel softmax uncovers.

More broadly, this is the goal of interpretable machine learning (Doshi-Velez and Kim, 2017). While downstream models do not always need an interpretable explanation of why a model uses a particular representation, interactive machine learning and explainable machine learning do. To date, multisense representations ignore this use case.
Qualitative analysis Previous papers use nearest neighbors of a few examples to qualitatively argue that their models have captured meaningful senses of words. We also give an example in Figure 2, which provides an intuitive view on how the learned senses are clustered by visualizing the nearest neighbors of word “bond” using t-sne projection (Maaten and Hinton, 2008). Our model (right) disentangles the three sense of “bond” clearly.
However, examples can be cherry-picked. This problem bedeviled topic modeling until rigorous human evaluation was introduced (Chang et al., 2009). We adapt both aspects of their evaluations: word intrusion (Schnabel et al., 2015) to evaluate whether individual senses are coherent and topic intrusion—rather sense intrusion in this setting—to evaluate whether humans agree with models’ sense assignments in context. Using crowdsourced evaluations from Figure-Eight, we compare our models with two previous state-of-the-art sense embeddings models, i.e., mssg (Neelakantan et al., 2014) and muse (Lee and Chen, 2017).4
4.1 Word Intrusion for Sense Coherence
Schnabel et al. (2015) suggest a “good” word embedding should have coherent neighbors and evaluate coherence by word intrusion. They
4mssg has two settings; we run human evaluation with mssg-30K which has higher correlation with MaxSimC on scws.

Model
muse mssg-30K gasi (no β)
gasi-β
gasi-β-pruned

Accuracy
28.0 44.5 33.8 50.0
75.2

P
0.33 0.37 0.33 0.48
0.67

Agreement
0.68 0.73 0.68 0.75
0.96

Table 2: Human-model consistency on contextual word sense selection; P is the average probability assigned by the model to the human choices. gasiβ is most consistent with crowdworkers. Reducing sense duplications by retraining our model with pruning mask improves significantly human-model agreement.

present crowdworkers four words: three are close in embedding space while one of which is an “intruder”. If the embedding makes sense, contributors will easily spot the word that “does not belong”.
Similarly, we examine the coherence of ten nearest neighbors of senses in the contextual word sense selection task (Section 4.2) and replace one neighbor with an “intruder”. We generate three intruders for each sense and collect three judgments per intruder. To account for variation in users and intruders, we count an instance as “correct” if two or more crowdworkers correctly spot the intruder.
Like Chang et al. (2009), we want the “intruder” to be about as frequent as the target but not too similar. For sense smi of word type wi, we randomly select a word from the neighbors of another sense sni of wi.
All models have comparable model accuracy. gasi-β learns senses that have the highest coherence while muse learns mixtures of senses (Table 1).
We use the aggregated confidence score provided by Figure-Eight to estimate the level of inter-rater agreement between multiple contributors (Figure Eight, 2018). The agreement is high for all models and gasi-β has the highest agreement, suggesting that the senses learned by gasi-β are easier to interpret.
4.2 Contextual Word Sense Selection
The previous task measures whether individual senses are coherent. Now we evaluate models’ disambiguation of senses in context.
Task Description Given a target word in context, we ask a crowdworker to select which

word overlap
Glove cosine

agree disagree
agree disagree

muse
4.78 5.43
0.86 0.88

mssg
0.39 0.98
0.33 0.57

gasi-β
1.52 6.36
0.36 0.81

Table 3: Similarities of human and model choices when they agree and disagree for two metrics: simple word overlap (top) and Glove cosine similarity (bottom). Humans agree with the model when the senses are distinct.

The real question is - how are those four years used and what is their value as training?
s1 hypothetical, unanswered, topic, answered, discussion, yes/no, answer, facts
s2 toss-up, answers, guess, why, answer, trivia, caller, wondering, answering
s3 argument, contentious, unresolved, concerning, matter, regarding, debated, legality
Table 4: A case where mssg has low overlap but confuses raters (agreement 0.33); model chooses s1.

sense group best fits the sentence. Each sense group is described by its top ten distinct nearest neighbors, and the sense group order is shuffled.
Data Collection We select fifty nouns with five sentences from SemCor 3.0 (Miller et al., 1994). We first filter all word types with fewer than ten sentences and select the fifty most polysemous nouns from WordNet (Miller and Fellbaum, 1998) among the remaining senses. For each noun, we randomly select five sentences.
Metrics For each model, we collect three judgments for each question. We consider a model correct if at least two crowdworkers select the same sense as the model.
Sense Disambiguation and Interpretability If humans consistently pick the same sense as the model, they must first understand the choices, thus implying the nearest neighbor words were coherent. Moreover, they also agree that among those senses, that sense was the right choice for this token. gasi-β selections are most consistent with humans’; it has the highest accuracy and assigns the largest probability assigned to the human choices (Table 2). Thus, gasi-β produces sense embeddings that are both more interpretable and distinguishable. gasi without a scaling factor, however,

has low consistency and flat sense distribution.
Model Confidence However, some contexts are more ambiguous than others. For finegrained senses, best practice is to use graded sense assignments (Erk et al., 2013). Thus, we also show the model’s probability of the top human choice; distributions close to K1 (0.33) suggest the model learns a distribution that cannot disambiguate senses. We consider granularity of senses further in Sec. 6.
Inter-rater Agreement We use the confidence score computed by Figure-Eight to estimate the raters’ agreement for this task. gasi-β has the highest human-model agreement, while both Muse and gasi without scaling have the lowest.
Error Analysis Next, we explore why crowdworkers disagree with the model even though the senses are interpretable (Table 1). Is it that the model has learned duplicate senses that both the users and model cannot distinguish (the senses are all bad or identical) or is it that crowdworkers agree with each other but disagree with the model (the model selects bad senses)?
Two trends suggest duplicate senses cause disagreement both for humans with models and humans with each other. For two measures of sense similarity—simple word overlap and glove similarity—similarity is lower when users and models agree (Table 3). Humans also agree with each other more. For gasi-β, pairs with perfect agreement have a word overlap of around 2.5, while the senses with lowest agreement have overlap around 5.5.
To reduce duplicated senses, we retrain the model with pruning (Section 2.4, Equation 5). We remove a little more than one sense per type on average. To maintain the original setting, for word types that have fewer than three senses left, we compute the nearest neighbors to dummy senses represented by random embeddings. Our model trained with pruning mask (gasi-β-pruned) reaches very high inter-rater agreement and higher human-model agreement than models with a fixed number of senses (Table 2, bottom).

5 Word Similarity Evaluation

gasi and gasi-β are interpretable, but how do they fare on standard word similarity tasks?

Contextual Word Similarity Tailored for

sense embedding evaluation, Stanford Contex-

tual Word Similarities (Huang et al., 2012,

scws) has 2003 word pairs tied to context sentences. These tasks assign a pair of word

types (e.g., “green” and “buck”) a similar-

ity/relatedness score. Moreover, both words

in the pair have an associated context. These

contexts disambiguate homonymous and pol-

ysemous word types and thus captures sense-

specific similarity. Thus, we use this dataset to

tune our hyperparameters, comparing Spearman’s rank correlation ρ between embedding similarity and the gold similarity judgments:

higher scores imply the model captures seman-

tic similarities consistent with the trusted simi-

larity scores.

To compute the word similarity with senses

we use two metrics (Reisinger and Mooney,

2010) that take context and sense disambigua-

tion into account: MaxSimC computes the

cosine

similarity

cos

(s

∗ 1

,

s∗2

)

between

the

two

most probable senses s∗1 and s∗2 that maximizes

P (sik | wi, c˜i). AvgSimC weights average

similarity over the combinations of all senses

∑K
i=1

∑K
i=j

P

(s1i

|

w1,

c˜1)P

(s2j

|

w2,

c˜2)

cos(s1i s2j ).

We compare variants of our model with ex-

isting sense embedding models (Table 5), in-

cluding two previous sotas: the clusteringbased Multi-Sense Skip-Gram model (Neelakan-

tan et al., 2014, mssg) on AvgSimC and the rl-based Modularizing Unsupervised Sense Embeddings (Lee and Chen, 2017, muse) on MaxSimC. gasi better captures similarity than sasi, corroborating that hard attention aids word sense selection. gasi without scaling has the best MaxSimC; however, it learns a flat sense distribution (Figure 2). gasi-β has the best AvgSimC and a competitive MaxSimC.

While muse has a higher MaxSimC than gasiβ, it fails to distinguish senses as well (Figure 2, Section 4).

We also evaluate the retrained model with pruning mask on this dataset. gasi-β-pruned has the same AvgSimC as gasi-β and higher local similarity correlation (Table 5, bottom),

validating our pruning strategy (Section 2.4).

Model
Huang et al. (2012)-50d mssg-6k mssg-30k
Tian et al. (2014) Li and Jurafsky (2015)
Qiu et al. (2016) Bartunov et al. (2016)
muse_Boltzmann
sasi gasi (w/o scaling)
gasi-β
gasi-β-pruned

MaxSimC
26.1 57.3 59.3 63.6 66.6 64.9 53.8 67.9
55.1 68.2 66.4
67.0

AvgSimC
65.7 69.3 69.2 65.4 66.8 66.1 61.2 68.7
67.8 68.3 69.5
69.5

Table 5: Spearman’s correlation 100ρ on scws (trained on 1B token, 300d vectors except for Huang et al.). gasi and gasi-β both can disambiguate the sense and correlate with human ratings. Retraining the model with pruned senses furthur improves local similarity correlation.

Word Sense Selection in Context scws evaluates models’ sense selection indirectly. We further compare gasi-β with previous sota, mssg-30k and muse, on the Word in Context dataset (Pilehvar and Camacho-Collados, 2018, wic) which requires the model to identify whether a word has the same sense in two contexts. To reduce the variance in training and to focus on evaluating the sense selection module, we use an evaluation suited for unsupervised models: if the model selects different sense vectors given contexts, we mark that the word has different senses.5 For muse, mssg and gasi-β, we use each model’s sense selection module; for DeConf (Pilehvar and Collier, 2016) and sw2v (Mancini et al., 2017), we follow Pilehvar and Camacho-Collados (2018) and Pelevina et al. (2016) by selecting the closest sense vectors to the context vector. DeConf results are comparable to supervised results (59.4± 0.7). gasi-β has the best result (55.3) apart from DeConf itself (58.55)(full results in Table 8 in appendix), which uses the same sense inventory (Miller and Fellbaum, 1998, WordNet) as wic.
Non-Contextual Word Similarity While contextual word similarity is best suited for our model and goals, other datasets without contexts (i.e., only word pairs and a rating) are both larger and ubiquitous for word vec-
5For monosemous or out of vocab words, we choose randomly.

Dataset
SimLex-999 WS-353 MEN-3k MC-30 RG-65 YP-130 MT-287 MT-771 RW-2k

muse
39.61 68.41 74.06 81.80 81.11 43.56 67.22 64.00 48.46

sasi
31.56 58.31 65.07 70.81 74.38 48.28 64.54 55.00 45.03

gasi
40.14 68.49 73.13 82.47 77.19 49.82 67.37 66.65 47.22

gasi-β
41.68 69.36 72.32 85.27 79.77 56.34 66.13 66.70 47.69

PFT-GM
40.19 68.6 77.40 74.63 79.75 59.39 69.66 68.91 45.69

Table 6: Spearman’s correlation on non-contextual word similarity (MaxSim). gasi-β has higher correlation on three datasets and is competitive on the others. pft-gm is trained with two components/senses while other models learn three. A full version including mssg is in appendix.

tor evaluations. To evaluate the semantics captured by each sense-specific embeddings, we compare the models on non-contextual word similarity datasets.6 Like Lee and Chen (2017) and Athiwaratkun et al. (2018), we compute the word similarity based on senses by MaxSim (Reisinger and Mooney, 2010), which maximizes the cosine similarity over the combination of all sense pairs and does not require local contexts,
MaxSim(w1, w2) = max cos(s1i , s2j ).
0≤i≤K,0≤j≤K
(13) gasi-β has better correlation on three datasets, is competitive on the rest (Table 6), and remains competitive without scaling. gasi is better than muse, the other hard-attention multi-prototype model, on six datasets and worse on three. Our model can reproduce word similarities as well or better than existing models through our sense selection.7
5.1 Word Similarity vs. Interpretability
Word similarity tasks (Section 5) and human evaluations (Section 4) are inconsistent. gasi, gasi-β and muse are all competitive in word
6RG-65 (Rubenstein and Goodenough, 1965); SimLex-999 (Hill et al., 2015); WS-353 (Finkelstein et al., 2002); MEN-3k (Bruni et al., 2014); MC30 (Miller and Charles, 1991); YP-130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk771 (Halawi et al., 2012); RW-2k (Luong et al., 2013)
7Given how good pdf-gm is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties gasi-β.

similarity (Table 5 and Table 6), but only gasiβ also does well in the human evaluations (Table 2). Both gasi without scaling and muse fail to learn distinguishable senses and cannot disambiguate senses. High word similarities do not necessarily indicate “good” sense embeddings quality; our human evaluation— contextual word sense selection—is complementary.
6 Related Work: Representation, Evaluation
Schütze (1998) introduces context-group discrimination for senses and uses the centroid of context vectors as a sense representation. Other work induces senses by context clustering (Purandare and Pedersen, 2004) or probabilistic mixture models (Brody and Lapata, 2009). Reisinger and Mooney (2010) first introduce multiple sense-specific vectors for each word, inspiring other multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015) or external sense inventories (Labutov and Lipson, 2013; Chen et al., 2014; Jauhar et al., 2015; Chen et al., 2015; Wu and Giles, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014; Šuster et al., 2016; Ettinger et al., 2016).
We contrast our gasi to unsupervised monolingual multi-prototype models along two dimensions: sense induction methodology and differentiability.
On the dimension of sense induction methodology, Huang et al. (2012) and Neelakantan et al. (2014) induce senses by context clustering; Tian et al. (2014) model a corpus-level sense distribution; Li and Jurafsky (2015) model the sense assignment as a Chinese Restaurant Process; Qiu et al. (2016) induce senses by minimizing an energy function on a context-depend network; Bartunov et al. (2016) model the sense assignment as a steak-breaking process; Nguyen et al. (2017) model the sense embeddings as a weighted combination of topic vectors with pre-computed weights by topic models; Athiwaratkun et al. (2018) model word representa-

tions as Gaussian Mixture embeddings where each Gaussian component captures different senses; Lee and Chen (2017) compute sense distribution by a separate set of sense induction vectors. The proposed gasi marginalizes the likelihood of contexts over senses and induces senses by local context vectors; the most similar sense selection module is a bilingual model (Šuster et al., 2016) except that it does not introduce lower bound for negative sampling but uses weighted embeddings, which results in mixed senses.
On the dimension of differentiability, most sense selection models are non-differentiable and discretely select senses, with two exceptions: Šuster et al. (2016) use weighted vectors over senses; Lee and Chen (2017) implement hard attention with rl to mitigate the nondifferentiability. In contrast, gasi keeps full differentiability by reparameterization and approximates discrete sense sampling with the scaled Gumbel softmax.
However, the elephants in the room are bert and elmo. While there are specific applications where humans might be better served by multisense embeddings, computers seem to be consistently better served by contextual representations. A natural extension is to use the aggregate representations of word senses from these models. Particularly for elmo, one could cluster individual mentions (Chang, 2019), but this is unsatisfying at first blush: it creates clusters more specific than senses. bert is even more difficult: the transformer is a dense, rich representation, but only a small subset describes the meaning of individual words. Probing techniques (Perone et al., 2018) could help focus on semantic aspects that help humans understand word usage.
6.1 Granularity
Despite the confluence of goals, there has been a disappointing lack of cross-fertilization between the traditional knowledge-based lexical semantics community and the representationlearning community. We, following the trends of sense learning models, have—from the perspective of those used to VerbNet or WordNet— used far too few senses per word. While there is disagreement about sense inventory, “hard” and “line” (Leacock et al., 1998) definitely have more than three senses. Expanding to granular

senses presents both challenges and opportunities for future work.
While moving to a richer sense inventory is valuable future work, it makes human annotation more difficult (Erk et al., 2013)—while we can expect humans to agree on which of three senses are used, we cannot for larger sense inventories. In topic models, Chang et al. (2009) develop topic log odds (in addition to the more widely used model precision) to account for graded assignment to topics. Richer user models would need to capture these more difficult decisions.
However, moving to more granular senses requires richer modeling. Bayesian nonparametrics (Orbanz and Teh, 2010) can determine the number of clusters that best explain the data. Combining online stick breaking distributions (Wang et al., 2011) with gasi’s objective function could remove unneeded complexity for word types with few senses and consider the richer sense inventory for other words.
7 Conclusion
The goal of multi-sense word embeddings is not just to win word sense evaluation datasets. Rather, they should also describe language: given millions of tokens of a language, what are the patterns in the language that can help a lexicographer or linguist in day-to-day tasks like building dictionaries or understanding semantic drift. Our differentiable Gumbel Attention Sense Induction (gasi) offers comparable word similarities with multisense representations while also learning more distinguishable, interpretable senses.
However, simply asking whether word senses look good is only a first step. A sense induction model designed for human use should be closely integrated into that task. While we use a Word2Vec-based objective function in Section 2, ideally we should use a human-driven, task-specific metric (Feng and Boyd-Graber, 2019) to guide the selection of senses that are distinguishable, interpretable, and useful.
References
Ben Athiwaratkun, Andrew Wilson, and Anima Anandkumar. 2018. Probabilistic fasttext for multi-sense word embeddings. In Proceedings of

Empirical Methods in Natural Language Processing.
Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, and Dmitry Vetrov. 2016. Breaking sticks and ambiguities with adaptive skip-gram. In Proceedings of Artificial Intelligence and Statistics.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the European Chapter of the Association for Computational Linguistics.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49.
José Camacho-Collados and Mohammad Taher Pilehvar. 2018. From word to sense embeddings: A survey on vector representations of meaning. CoRR, abs/1805.04032.
Henry Chang. 2019. Visualizing ELMo contextual vectors.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of Advances in Neural Information Processing Systems.
Tao Chen, Ruifeng Xu, Yulan He, and Xuan Wang. 2015. Improving distributed representation of word sense via WordNet gloss composition and context clustering. In Proceedings of the Association for Computational Linguistics, pages 15– 20. Association for Computational Linguistics.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A unified model for word sense representation and disambiguation. In Proceedings of Empirical Methods in Natural Language Processing. Citeseer.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics.
Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of the Association for Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2013. Measuring word meaning in context. Computational Linguistics, 39(3):511–554.

Allyson Ettinger, Philip Resnik, and Marine Carpuat. 2016. Retrofitting sense-specific word vectors using parallel text. In Conference of the North American Chapter of the Association for Computational Linguistics.
Shi Feng and Jordan Boyd-Graber. 2019. What ai can do for me: Evaluating machine learning interpretations in cooperative play. In International Conference on Intelligent User Interfaces.
Figure Eight. 2018. How to calculate a confidence score. Https://success.figure-eight.com/hc/enus/articles/201855939-How-to-Calculate-aConfidence-Score.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1).
William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language.
Yoav Goldberg and Omer Levy. 2014. word2vec explained: Deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.
Emil Julius Gumbel and Julius Lieblein. 1954. Statistical theory of extreme values and some practical applications: a series of lectures. Technical report, US Government Printing Office Washington.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources. In Proceedings of International Conference on Computational Linguistics.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. 2012. Large-scale learning of word relatedness with constraints. In Knowledge Discovery and Data Mining.
William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic word embeddings reveal statistical laws of semantic change. In Proceedings of the Association for Computational Linguistics.
Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4).
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the Association for Computational Linguistics.

Ignacio Iacobacci, Taher Mohammad Pilehvar, and Roberto Navigli. 2015. Sensembed: Learning sense embeddings for word and relational similarity. In Proceedings of the Association for Computational Linguistics, pages 95–105. Association for Computational Linguistics.
Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbelsoftmax. arXiv preprint arXiv:1611.01144.
Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy. 2015. Ontologically grounded multisense representation learning for semantic vector space models. In Conference of the North American Chapter of the Association for Computational Linguistics.
David Jurgens and Ioannis Klapaftis. 2015. Semeval-2013 task 13: Word sense induction for graded and non-graded senses. In Proceedings of the Workshop on Semantic Evaluation.
Daisuke Kawahara, Daniel Peterson, Octavian Popescu, and Martha Palmer. 2014. Inducing example-based semantic frames from a massive amount of verb uses. In Proceedings of the European Chapter of the Association for Computational Linguistics.
Igor Labutov and Hod Lipson. 2013. Reembedding words. In Proceedings of the Association for Computational Linguistics.
Claudia Leacock, George A. Miller, and Martin Chodorow. 1998. Using corpus statistics and wordnet relations for sense identification. Computational Linguistics, 24(1):147–165.
Guang-He Lee and Yun-Nung Chen. 2017. MUSE: Modularizing unsupervised sense embeddings. In Proceedings of Empirical Methods in Natural Language Processing.
Jiwei Li and Dan Jurafsky. 2015. Do multi-sense embeddings improve natural language understanding? arXiv preprint arXiv:1506.01070.
Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. In Conference on Computational Natural Language Learning.
Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov).
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. 2016. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712.
Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci, and Roberto Navigli. 2017. Embedding words and senses together via joint

knowledge-enhanced training. In Conference on Computational Natural Language Learning.
Diana McCarthy, Marianna Apidianaki, and Katrin Erk. 2016. Word sense clustering and clusterability. Computational Linguistics, 42(2):245–275.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of Advances in Neural Information Processing Systems.
George Miller and Christiane Fellbaum. 1998. Wordnet: An electronic lexical database. MIT Press Cambridge.
George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive processes, 6.
George A Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G Thomas. 1994. Using a semantic concordance for sense identification. In Proceedings of the workshop on Human Language Technology.
Tristan Miller, Christian Hempelmann, and Iryna Gurevych. 2017. SemEval-2017 task 7: Detection and interpretation of English puns. In Proceedings of the Workshop on Semantic Evaluation.
Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient non-parametric estimation of multiple embeddings per word in vector space. In Proceedings of Empirical Methods in Natural Language Processing.
Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. 2017. A mixture model for learning multi-sense word embeddings. In Proceedings of the Joint Conference on Lexical and Computational Semantics.
Thanapon Noraset, Chen Liang, Lawrence A Birnbaum, and Douglas C Downey. 2017. Definition modeling: Learning to define word embeddings in natural language. In Association for the Advancement of Artificial Intelligence.
P. Orbanz and Y. W. Teh. 2010. Bayesian Nonparametric Models. Springer.
Maria Pelevina, Nikolay Arefiev, Chris Biemann, and Alexander Panchenko. 2016. Making sense of word embeddings. In Workshop on Representation Learning for NLP.

Christian S Perone, Roberto Silveira, and Thomas S Paula. 2018. Evaluation of sentence embeddings in downstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Conference of the North American Chapter of the Association for Computational Linguistics.
Mohammad Taher Pilehvar and Jose CamachoCollados. 2018. Wic: 10,000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv:1808.09121.
Mohammad Taher Pilehvar and Nigel Collier. 2016. De-conflated semantic representations. In Proceedings of Empirical Methods in Natural Language Processing.
Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Conference on Computational Natural Language Learning.
Lin Qiu, Kewei Tu, and Yong Yu. 2016. Contextdependent sense embedding. In Proceedings of Empirical Methods in Natural Language Processing.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of the World Wide Web Conference.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Conference of the North American Chapter of the Association for Computational Linguistics.
Sascha Rothe and Hinrich Schütze. 2017. Autoextend: Combining word embeddings with semantic resources. Computational Linguistics, 43(3).
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10).
Tobias Schnabel, Igor Labutov, David M. Mimno, and Thorsten Joachims. 2015. Evaluation methods for unsupervised word embeddings. In Proceedings of Empirical Methods in Natural Language Processing.
Hinrich Schütze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1).
Westbury C Shaoul C. 2010. The Westbury Lab Wikipedia Corpusa.

Simon Šuster, Ivan Titov, and Gertjan van Noord. 2016. Bilingual learning of multi-sense embeddings with discrete autoencoders. In Conference of the North American Chapter of the Association for Computational Linguistics.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A probabilistic model for learning multi-prototype word embeddings. In Proceedings of International Conference on Computational Linguistics.
Chong Wang, John Paisley, and David M. Blei. 2011. Online variational inference for the hierarchical Dirichlet process. In Proceedings of Artificial Intelligence and Statistics.
Zhaohui Wu and C Giles. 2015. Sense-aaware semantic analysis: A multi-prototype word representation model using Wikipedia. In Association for the Advancement of Artificial Intelligence.
Dongqiang Yang and David Martin Powers. 2006. Verb similarity on the taxonomy of WordNet. Masaryk University.

Appendix
Equation and figure numbers continue from main submission.

A Derivation Desiderata

Like the Skip-Gram objective (Equation 2), we
model the likelihood of a context word given the center sense P (cij | sik) using softmax,

exp

(

c

i j

⊤

s

i k

)

P (cij | sik) =

( ) , (14)

∑|V |
j=1

exp

c⊤j sik

where the bold symbol sik is the vector representation of sense sjk from S, and cj is the context embedding of word cj from C.
Computing the softmax over the vocabulary
is time-consuming. We want to adopt negative sampling to approximate log P (cij | sik), which does not exist explicitly in our objective function (Equation 7).8
However, given the concavity of the loga-
rithm function, we can apply Jensen’s inequal-
ity,

[K

]

log ∑ P (cij | sik)P (sik | wi, c˜i) ≥ (15)

k=1

K
∑ P (sik | wi, c˜i) log P (cij | sik),

k=1

and create a lower bound of the objective. Maximizing this lower bound gives us a tractable objective, J(S, C) ∝

K

∑

∑

∑

P

(s

i k

| wi, c˜i) log

P (cij

| sik),

wi∈V cij ∈c˜i k=1

(16)

where log P (cij | sik) is estimated by negative sampling (Mikolov et al., 2013b),
n
log σ(cij ⊤sik) + ∑ Ecj∼Pn(c)[log σ(−c⊤j sjk))]
j=1

B Training Details
During training, we fix the window size to five and the dimensionality of the embedding space to 300 for comparison to previous work. We
8Deriving the negative sampling requires the logarithm of a softmax (Goldberg and Levy, 2014).

Figure 3: Our hard attention mechanism is approx-
imated with Gumbel softmax on the context-sense dot product c¯⊤i sik (Equation 10), whose mean and std plotted here as a function of iteration. The
shadowed area shows that it has a smaller scale than the Gumbel noise gk, such that gk, rather than the embeddings, dominates the sense atten-
tion.

# of senses left

2.75

2.50

GASI-0.4, K=5 GASI-0.4, K=3

2.25

2.00

1.75

1.50

1.25

1.00 rank by freq (high to low)

Figure 4: Histogram of number of senses left after post-training pruning for two models: gasi-0.4 initialized with three senses and gasi-0.4 initialized with five senses. We rank the number of senses of words by their frequency from high to low.

initialize both sense and context embeddings randomly within U(-0.5/dim, 0.5/dim) as in Word2Vec. We set the initial learning rate to 0.01; it is decreased linearly until training concludes after 5 epochs. The batch size is 512, and we use five negative samples per center word-context pair as suggested by Mikolov et al. (2013a). The subsample threshold is 1e4. We train our model on the GeForce GTX 1080 Ti, and our implementation (using pytorch 3.0) takes ∼ 6 hours to train one epoch on the April 2010 Wikipedia snapshot (Shaoul C., 2010) with 100k vocabulary. For comparison, our implementation of Skip-Gram on the same framework takes ∼ 2 hours each epoch.

Probability

GASI-β (0.4)

0.8

τ = 0.5

0.6

0.4

0.2

0.0 sense0 sense1 sense2

GASI-β (0.5) τ = 0.5
sense0 sense1 sense2

GASI-β (0.7) τ = 0.5
sense0 sense1 sense2

GASI (no scaling) τ = 0.1 τ = 0.5 τ = 1.0
sense0 sense1 sense2

Figure 5: As the scale factor β increases, the sense selection distribution for “bond” given examples from SemCor 3.0 for synset “bond.n.02” becomes flatter, indicating less disambiguated sense vectors.

Dataset
SimLex-999 WS-353 MEN-3k MC-30 RG-65 YP-130 MT-287 MT-771 RW-2k

mssg-30k
31.80 65.69 65.99 67.79 73.90 40.69 65.47 61.26 42.87

mssg-6k
28.65 67.42 67.10 76.02 64.97 42.68 64.04 58.83 39.24

muse_Boltzmann
39.61 68.41 74.06 81.80 81.11 43.56 67.22 64.00 48.46

sasi
31.56 58.31 65.07 70.81 74.38 48.28 64.54 55.00 45.03

gasi
40.14 68.49 73.13 82.47 77.19 49.82 67.37 66.65 47.22

gasi-β
41.68 69.36 72.32 85.27 79.77 56.34 66.13 66.70 47.69

PFT-GM
40.19 68.6 77.40 74.63 79.75 59.39 69.66 68.91 45.69

Table 7: Spearman’s correlation 100ρ on non-contextual word similarity (MaxSim). gasi-β outperforms the other models on three datasets and is competitive on others. pft-gm is trained with two components/senses while other models learn three.

C Number of Senses
For simplicity and consistency with most of previous work, we present our model with a fixed number of senses K.
C.1 Post-training Pruning and Retraining
For words that do not have multiple senses or have most senses appear very low-frequently in corpus, our model (as well as many previous models) learns duplicate senses. Ideally, if we set a large number of K, with a perfect pruning strategy, we can estimate the number of senses per type by removing duplicated senses and retrain a new model with the estimated number of senses instead of a fixed number K.
However, this is challenging (McCarthy et al., 2016); instead we use a simple pruning strategy and remove duplicated senses with a threshold λ. Specifically, for each word wi, if the cosine distance between any of its sense embeddings (sim, sin) is smaller than λ, we consider them to be duplicates. After discovering all duplicate pairs, we start pruning with the sense sik that has the most duplications and keep pruning with the same strategy until no more duplicates remain.

Model-specific pruning We estimate a model-specific threshold λ from the learned embeddings instead of deciding it arbitrary. We first sample 100 words from the negative sampling distribution over the vocabulary. Then, we retrieve the top-5 nearest neighbors (from all senses of all words) to each sense of each sampled word. If one of a word’s own senses appears as a nearest neighbor, we append the distance between them to a sense duplication list Ddup. For other nearest neighbors, we append their distances to the word neighbor list Dnn. After populating the two lists, we want to choose a threshold that would prune away all of the sense duplicates while differentiating sense duplications with other distinct neighbor words. Thus, we compute
1 λ = 2 (mean(Ddup) + mean(Dnn)). (17)
C.2 Number of Senses vs. Word Frequency
It is a common assumption that more frequent words have more senses. Figure 4 shows a histogram of the number of senses left for words ranked by their frequency, and the results agree with the assumption. Generally, the model learns more sense for high frequent words, except for the most frequent ones. The most fre-

accuracy 0.6 0.5 0.4 0.3 0.2 0.33

0.67

# of questions

250

MUSE

MSSG

GASI150
MUSE

MSSG

50

GASI-

1.00 (rater agreement)

Figure 6: Higher inter-rater agreement correlates with higher human-model consistency.

quent words are usually considered stopwords, such as “the”, “a” and “our’, which have only one common meaning. Moreover, we compare our model initialized with three senses (gasi-0.4, K = 3) against the one that has five (gasi-0.4, K = 5). Initializing with a larger number of senses, the model is able to uncover more senses for most words.
C.3 Duplicated Senses and Human-Model Agreement
We measure distinctness both by counting shared nearest neighbors and the average cosine similarities of GloVe embeddings.9 Specifically, muse learns duplicate senses for most words, preventing users from choosing appropriate senses and preventing human-model agreement. gasi-β learns some duplicated senses and some distinguishable senses. mssg appears to learn the fewest duplicate senses, but they are not distinguishable enough for humans. Users disagree with each other (0.33 agreement) even when the number of overlaps is very small (Figure 7). Table 4 shows an intuitive example. If we use rater agreement to measure how distinguishable the learned senses are to humans, gasi-β learns the most distinguishable senses.
The model is more likely to agree with humans when humans agree with each other (Figure 6), i.e., human-model consistency correlates with rater agreement (Figure 6). mssg disagrees with humans more even when raters agree with each other, indicating worse sense selection ability.

Model

Accuracy(%)

unsupervised multi-prototype models

mssg-30k muse_Boltzmann
gasi-β

54.00 52.14 55.27

semi-supervised with lexical resources

DeConf sw2v

58.55 54.56

Table 8: Sense selection on Word in Context (wic) dataset.

cosine sim overlaps

Average in-word sense similarities 5.0 2.5

0.75 0.5 0.33

0.67

MUSE MSSG GASI-
1.00 (rater agreement)

Figure 7: Overall, human agree more with each other when the senses are more distinct (less word overlaps and smaller cosine similarities)

9Different models learn different representations; we use GloVe for a uniform basis of comparison.

