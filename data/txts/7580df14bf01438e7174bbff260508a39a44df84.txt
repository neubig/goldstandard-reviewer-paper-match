arXiv:1509.01858v1 [cs.IT] 6 Sep 2015

Optimal Systematic Distributed Storage Codes with
Fast Encoding
Preetum Nakkiran, KV Rashmi, Kannan Ramchandran University of California, Berkeley
Abstract
Erasure codes are being increasingly used in distributed-storage systems in place of data-replication, since they provide the same level of reliability with much lower storage overhead. We consider the problem of constructing explicit erasure codes for distributed storage with the following desirable properties motivated by practice: (i) MaximumDistance-Separable (MDS): to provide maximal reliability at minimum storage overhead, (ii) Optimal repair-bandwidth: to minimize the amount of data needed to be transferred to repair a failed node from remaining ones, (iii) Flexibility in repair: to allow maximal ﬂexibility in selecting subset of nodes to use for repair, which includes not requiring that all surviving nodes be used for repair, (iv) Systematic Form: to ensure that the original data exists in uncoded form, and (v) Fast encoding: to minimize the cost of generating encoded data (enabled by a sparse generator matrix). Existing constructions in the literature satisfy only strict subsets of these desired properties.
This paper presents the ﬁrst explicit code construction which theoretically guarantees all the ﬁve desired properties simultaneously. Our construction builds on a powerful class of codes called Product-Matrix (PM) codes. PM codes satisfy properties (i)-(iii), and either (iv) or (v), but not both simultaneously. Indeed, native PM codes have inherent structure that leads to sparsity, but this structure is destroyed when the codes are made systematic. We ﬁrst present an analytical framework for understanding the interaction between the design of PM codes and the systematic property. Using this framework, we provide an explicit code construction that simultaneously achieves all the above desired properties. We also present general ways of transforming existing storage and repair optimal codes to enable fast encoding through sparsity. In practice, such sparse codes result in encoding speedup by a factor of about 4 for typical parameters.
I. Introduction
Erasure codes are being increasingly used in distributed-storage systems instead of replication, since they provide the same level of reliability with much less storage overhead. Large scale distributed-storage systems have many practical requirements that guide the design of distributed-storage codes.

(a)

(b)

Fig. 1: (a) Encoding and decoding for an [n, k] systematic MDS code, (b) Node repair: Connecting to d = (n − 2) helper nodes to repair failed node 1.

In large-scale systems, storage is a critical resource. For this reason, Maximum-Distance-Separable (MDS) codes such as Reed-Solomon codes, which require the minimal storage overhead to achieve a desired level of reliability, are a popular choice [1]–[3]. An [n, k] MDS code allows the data to be stored across n nodes such that the entire data can be recovered from the encoded data stored in any k (out of n) nodes. This is depicted in Figure 1a. Another critical resource in distributed-storage systems is network bandwidth. In large-scale systems, failures are the norm rather than the exception, and repair operations run continuously in the background [4]. When nodes fail, they must be repaired by downloading some data from the remaining nodes. These nodes are termed helper nodes. Figure 1b depicts a repair

operation where node 1 is being repaired with the help of nodes {2, . . . , n − 1}. In large-scale systems, the repair operations consume a signiﬁcant amount of network bandwidth, and this has been one of main deterrents to using classical MDS erasure codes in such systems [4]. Hence, it is important for storage codes to also minimize the amount of bandwidth consumed during repair.
Another important system consideration is that the code not force the requirement that all surviving (n − 1) nodes be needed to repair a single failed node. If d denotes the number of helper nodes required for repair, then this property requires d < (n − 1), as illustrated in Figure 1b. This property is crucial to allow redundant requests to be sent during a repair operation, which is an eﬀective approach to reducing latency in practical systems [5]–[9]. That is, a failed node can request help from many helpers, and can repair as soon as enough nodes respond. This property is even more critical for degraded reads [4], where a repair operation is performed to serve a read request for data stored in a busy or otherwise unavailable node. Latency is crucial for degraded reads to meet the service level agreement in large scale systems.
Another practical requirement of storage codes is that of being in systematic form. That is, the original data must exist in the system in uncoded form. Figure 1a shows a systematic code wherein the ﬁrst k nodes store the original data. This is essential when serving read requests, since if the code is systematic, read requests can be served by simply reading the data in systematic nodes. Otherwise the system must perform a decoding operation to retrieve the original data for every read request.
Finally, one of the most frequent operations performed in many distributed-storage systems is the encoding of new data entering the system. This encoding cost is a non-issue when using replication, but can be signiﬁcant when using erasure codes. Thus it is desirable for the code to support fast encoding operations. For linear codes, encoding the original data can be represented as multiplication between a generator matrix and the data vector [10]. This encoding operation will be fast if the generator matrix is sparse, since this reduces the number of computations performed. Informally, the sparsity of the generator matrix dictates how many data symbols need to be touched in order to generate each encoded symbol.
This forms the motivation for this paper: to construct storage codes that satisfy all the above system-driven constraints. That is, storage codes having the following ﬁve properties: (i) Minimum storage for a targeted level of reliability (MDS), (ii) Minimal repair bandwidth, (iii) Flexible repair parameters: d < (n − 1), (iv) Systematic form of encoded data, and (v) Fast encoding, enabled by a sparse generator matrix.
There has been considerable interest in the recent past in constructing such erasure codes for distributed storage [11]–[15]. However, to the best of our knowledge, all existing constructions in the literature address only a strict subset of the above desired properties. This paper presents the ﬁrst explicit codes which theoretically guarantee all the ﬁve desired properties simultaneously.
Our constructions are based on a powerful class of storage codes called Product-Matrix (PM) codes [16]. PM codes are MDS 1 , and hence are optimal w.r.t. storage overhead. They also have optimal bandwidth consumed during repair, since they meet the lower-bound presented in [17]. PM codes belong to a general class of codes known as Regenerating codes [17], which meet this lower-bound. Moreover, PM codes support a wide range of values for d : (2k − 2) ≤ d ≤ (n − 1). Finally, the special structure of PM codes makes their generator matrix sparse, leading to fast encoding [18]. Thus PM codes satisfy Properties (i)-(iii) and (v).
Native PM codes, however, are not systematic. They can be converted to systematic form using a generic transformation termed “systematic-remapping” [16]. However, this remapping does not respect the inherent structure of PM codes, and thus often destroys its sparsity. Thus naively performing a remapping transform causes PM codes to be systematic, at the expense of fast encoding. For example, an [n, k, d = 2k − 1] PM code requires a block-length of k2 symbols. That is, each stored symbol can be, in general, a function of up to k2 data symbols. However, due to the sparse structure of native PM codes, each stored symbol is a function of only O(k) of these data symbols. This is no longer true after systematic remapping, and in general each parity symbol becomes a (dense) function of k2 symbols. This results in signiﬁcantly higher encoding time for systematic PM codes constructed in this manner [18], [19].
In this paper, ﬁrst, we present an analytical framework for studying and understanding the interaction between the design of PM codes and the systematic-remapping transformation. Using this, we provide an explicit construction of PM codes which remains sparse after systematic-remapping, for d = (2k − 2). In particular, each parity symbol in this construction depends on only d = O(k) data symbols. 2
1 We use “PM codes” here to refer to the MDS version of Product-Matrix codes, termed PM-MSR in [16]. 2 Note that for a systematic [n, k, d] MDS code, a sparsity of at least k symbols is necessary for each encoded symbol in the parity nodes.

Second, we consider the sparsity of codes supporting repair-by-transfer. A node assisting in a repair operation is said to perform repair-by-transfer if it does not perform any computation, and merely transfers one of its stored symbols to the failed node [20]. Storage codes which support repair-by-transfer are appealing in practice, since they also minimize the amount of data read during repairs. There have been a number of works in the recent past on constructing such storage codes [20]–[25]. We show that a particular type of repair-by-transfer property leads to sparsity in any MDS regenerating code. This provides a general way of constructing sparse MDS regenerating codes.
Third, using the above result, we construct explicit sparse systematic PM codes for all d ≥ (2k − 2). For example, the generator matrix of a [n = 17, k = 8, d = 15] systematic-remapped PM code as in [16] is ∼ 11% sparse, while our construction is ∼ 77% sparse.
We note that the construction provided in this paper is similar to the codes considered in [21], wherein the authors present codes supporting repair-by-transfer for achieving savings in disk I/O. For d = (2k − 2), the construction provided in the present paper is also similar to the recent construction in [19] by Le Scouarnec. In [19], the author presents a sparse PM code and computationally validates its properties for a ﬁxed range of k. In fact, the results presented in this paper provide a theoretical proof of sparsity for the constructions in both the above works [19], [21].
The remainder of this paper is organized as follows: Section II contains a review of Product-Matrix codes, systematicremapping, and other necessary background and notation. Section III contains a motivating example. Section IV illustrates the main ideas of our approach to understanding sparsity, by showing that a simple form of PM encoding matrix leads to partial sparsity. These techniques are extended in Section V, to give an explicit construction of sparse systematic PM codes for d = (2k − 2). In Section VI we consider more general regenerating codes, and show that regenerating codes possessing a certain repair-by-transfer property are necessarily sparse. We apply this in Section VII to construct explicit sparse systematic PM codes for d ≥ (2k − 2). Finally in Section VIII, we show that the two presented constructions of sparse PM codes for d = (2k − 2) are in fact equivalent in a certain sense.

II. Background

A. Product-Matrix Codes

Product-Matrix (PM) codes [16] are an explicit family of linear MDS codes which minimize bandwidth consumed in repair, and exist for all [n, k, d ≥ 2k − 2].

Let the message to be stored consist of B symbols from the ﬁnite ﬁeld Fq. An [n, k, d](α) PM code allows the message

to be stored across n nodes, each storing α encoded symbols. All the B symbols can be recovered from the data stored

in any k of the total n nodes. Further, any node’s data may be exactly recovered by connecting to any d other nodes,

and downloading one symbol from each. These d nodes are known as “helper nodes.” The symbols transferred from a

helper node during node repair will be a linear function of the data stored in it. PM codes are storage-optimal and

hence

B = kα.

(1)

The parameter α is induced by [n, k, d] as

α = d − k + 1.

(2)

We now describe the construction of PM codes. In general, a PM code is described by an (n × d) encoding matrix Ψ and a (d × α) message matrix M , yielding an (n × α) code matrix C deﬁned by

C := ΨM.

(3)

Let cTi denote the ith row of the code matrix C. Then the ith node stores cTi = ψiT M .
Here we review PM codes for d = (2k − 2), but the construction can applied to d > (2k − 2) by the shortening procedure of [16], which we review in Section VII-B. 3

For d = (2k − 2), we have α = (d − k + 1) = (k − 1). For these parameters, the encoding matrix Ψ is of the form:

Ψ = Φ ΛΦ

(4)

where Φ is an (n × α) matrix and Λ is an (n × n) diagonal matrix, with the following properties:

3 Constructions without puncturing were subsequently shown in [26] and [27].

(1) Any α rows of Φ are linearly independent (2) Any d rows of Ψ are linearly independent (3) The diagonal elements of Λ are all distinct.

These requirements can be met, for example, by choosing Ψ to be a Vandermonde matrix with elements chosen carefully to satisfy the third condition.

We will now specify the structure of the message matrix M . Recall for d = (2k − 2), we have α = (k − 1), d = 2α, and B = kα = α(α + 1). The (d × α) message matrix M is constructed as

Sa

M = Sb

(5)

where Sa and Sb are (α × α) symmetric matrices. The matrices Sa and Sb together have precisely α(α + 1) distinct entries, which are now populated by the B = α(α + 1) message symbols.

Let ψiT denote the ith row of Ψ, and φTi denote the ith row of Φ. Thus, under this encoding mechanism, node i (1 ≤ i ≤ n),

stores the α symbols

cTi

=

ψ

T i

M

=

φ

T i

S

a

+ λiφTi Sb

.

(6)

Under this encoding, the data in any k nodes suﬃce to reconstruct the B = kα message symbols. The original paper [16] presents an explicit reconstruction algorithm for general PM codes, relying on Properties 1 and 3 above.

PM codes allow repair of any failed node, by downloading one symbol from any d other helper nodes. For repairing node f , helper node i sends the single symbol

c

T i

φ

f

=

ψ

T i

M

φ

f

.

(7)

Upon receiving d such helper symbols, failed node f will have ΨdM φf , where Ψd is some d rows of Ψ. It can then

invert Ψd (by Property 2) to compute

Saφf

M φf = Sbφf .

(8)

And thus can recover its data as

cTf

=

(Saφf )T

+ λf (Sbφf )T

=

φ

T f

S

a

+ λf φTf Sb

(9)

(follows by symmetry of the matrices Sa and Sb).

B. Systematic Codes and Remapping
It is often desirable to have the B original message symbols included in the encoded symbols (in uncoded form). Such codes are called systematic codes. Throughout the paper we consider systematic codes in which the ﬁrst k nodes store the uncoded symbols. These nodes are thus referred to as “systematic nodes.”
Any linear MDS erasure code can be generically transformed into a systematic code, as follows. First, any linear code taking B message symbols to nα encoded symbols can be represented by an (nα × B) generator matrix G, such that for a message-vector m of length B, the encoded nα symbols are given by Gm.
A code can be made systematic through a “systematic remapping”: Let Gk be a (B × B) matrix consisting of the ﬁrst B rows of the original generator matrix G. To encode message m, ﬁrst “remap” the message vector to m := G−k 1m, then encode as Gm. Consider the resulting ﬁrst B encoded symbols: the message m is ﬁrst transformed by G−k 1, then transformed by Gk during encoding. Therefore the ﬁrst B encoded symbols are exactly the message symbols m, making the code systematic. Notice that the entire encoding operation now is equivalent to encoding the original message m with generator matrix Gsys := GG−k 1, which will have the ﬁrst (B × B) block as identity by construction. Observe that the systematic remapping operation applies G−k 1, and hence can be thought of as decoding the message from the ﬁrst k nodes under the original encoding with generator matrix G.
The above transform can be applied to the vanilla PM codes discussed in Section II-A and [16], to yield systematic PM codes. However, as shown in the examples below, applying systematic remapping to traditional PM codes often destroys their sparsity – leading to increased computational complexity.

C. Notation
We will use the concept of an inclusion map. In general, an inclusion map is a map which injectively embeds one space into another space, by simply changing representation (not performing any non-trivial transformation). For example, the following is an inclusion map from vectors of length 3 to symmetric (2 × 2) matrices:
a a b b →− b c
c
Inclusion maps will be denoted by hooked arrows ( →− ) as above.
For notational simplicity, we will often abuse notation by using the same symbols to denote a space as well as a vector in the space. For example, the systematic-remapping transformation of a message vector m, as in Section II-B, will be written as a function f : m → m. The (i, j)th entry of a matrix M is denoted Mi,j. All vectors are column-vectors unless otherwise noted, and T denotes transpose throughout.

III. Motivating Example
A. Example
To better understand the issues of sparsity and systematic remapping in PM codes, let us consider a particular [n = 8, k = 4, d = 6] PM code. For these parameters, each node stores α = 3 symbols, and the number of message symbols is B = 12. Let {m0, . . . , m11} denote these message symbols. Let us work in ﬁeld F11 4. As described in Section II-A, we have:

1 1 1 1 1 1
2 4 8 5 10 9 3 9 5 4 1 3 4 5 9 3 1 4 Ψ = 5 3 4 9 1 5, 6 3 7 9 10 5 7 5 2 3 10 4
8 9 6 4 10 3

m0 m1 m2 

m1 m3 m4 

m2 m4 M =
m6 m7

m5

 

 m8 





m7 m9 m10

m8 m10 m11

Recall from Section II-A that node i stores the i-th row of Ψ times M , so the entire code is C = ΨM .

As in Section II-B, we can generically represent the encoding operation as an (nα × B) = (24 × 12) generator matrix G

times the message vector m, with entries mi. That is, we can “unwrap” the matrix-matrix multiplication C = ΨM

into each of nα = 24 encoded symbols. For example, the ﬁrst α = 3 rows of G correspond to the 3 linear combinations

stored by the ﬁrst node:

1 1 1 0 0 0 1 1 1 0 0 0

0 1 0 1 1 0 0 1 0 1 1 0

001011001011

And the next 3 rows of G correspond to the 3 linear combinations stored by the second node:
2 4 8 0 0 0 5 10 9 0 0 0 0 2 0 4 8 0 0 5 0 10 9 0
0 0 2 0 4 8 0 0 5 0 10 9

Notice that the submatrix of the generator matrix corresponding to each node is d-sparse, with the same sparsity pattern. The entire generator matrix and its sparsity pattern are as follows:
4This is the smallest prime ﬁeld which will allow the PM construction of [16] for this parameter regime.

1 1 1 0 0 0 1 1 1 0 0 0

∗ ∗ ∗

∗∗∗



0101100 1 0 1 1 0

∗ ∗∗ ∗ ∗∗

0010110 0 1 0 1 1

∗ ∗∗ ∗ ∗∗

 2 4 8 0 0 0 5 10 9 0 0 0 

∗ ∗ ∗

∗∗∗



 0 2 0 4 8 0 0 5 0 10 9 0 

 ∗ ∗∗ ∗ ∗∗ 

 0 0 2 0 4 8 0 0 5 0 10 9 

 ∗ ∗ ∗ ∗ ∗ ∗

3 9 5 0 0 0 4 1 3 0 0 0

∗ ∗ ∗

∗∗∗



0 3 0 9 5 0 0 4 0 1 3 0

 ∗ ∗∗ ∗ ∗∗ 

0 0 3 0 9 5 0 0 4 0 1 3

 ∗ ∗ ∗ ∗ ∗ ∗

4 5 9 0 0 0 3 1 4 0 0 0

∗ ∗ ∗

∗∗∗



0 4 0 5 9 0 0 3 0 1 4 0

 ∗ ∗∗ ∗ ∗∗ 

G =  05 03 44 00 50 90 09 01 35 00 10 40  ∼  ∗ ∗ ∗∗ ∗ ∗ ∗ ∗ ∗∗ ∗ ∗ 

0 5 0 3 4 0 0 9 0 1 5 0

 ∗ ∗∗ ∗ ∗∗ 

0 0 5 0 3 4 0 0 9 0 1 5

 ∗ ∗ ∗ ∗ ∗ ∗

 6 3 7 0 0 0 9 10 5 0 0 0 

∗ ∗ ∗

∗∗∗



 0 6 0 3 7 0 0 9 0 10 5 0 

 ∗ ∗∗ ∗ ∗∗ 

 0 0 6 0 3 7 0 0 9 0 10 5 

 ∗ ∗ ∗ ∗ ∗ ∗

 7 5 2 0 0 0 3 10 4 0 0 0 

∗ ∗ ∗

∗∗∗



 0 7 0 5 2 0 0 3 0 10 4 0 

 ∗ ∗∗ ∗ ∗∗ 

 0 0 7 0 5 2 0 0 3 0 10 4 

 ∗ ∗ ∗ ∗ ∗ ∗

8 9 6 0 0 0 4 10 3 0 0 0

∗∗∗

∗∗∗

0 8 0 9 6 0 0 4 0 10 3 0

∗ ∗∗ ∗ ∗∗

0 0 8 0 9 6 0 0 4 0 10 3

∗ ∗∗ ∗ ∗∗

(10)

This code is not systematic, since it does not contain the uncoded message symbols. To make it systematic, we perform
the systematic remapping of Section II-B: Let Gk be the (B × B) matrix consisting of the ﬁrst B rows of the generator matrix G (above the line in (10)). The systematic generator matrix is Gsys = GG−k 1, which in our case is:

 1 0 0 0 0 0 0 0 0 0 0 0

∗



0 1 0 00 0 0 0 0 0 0 0

∗

0 0 1 00 0 0 0 0 0 0 0

∗

 0 0 0 1 0 0 0 0 0 0 0 0



∗



 0 0 0 0 1 0 0 0 0 0 0 0



∗



 0 0 0 0 0 1 0 0 0 0 0 0



∗



 0 0 0 0 0 0 1 0 0 0 0 0



∗



 0 0 0 0 0 0 0 1 0 0 0 0



∗



 0 0 0 0 0 0 0 0 1 0 0 0



∗



 0 0 0 0 0 0 0 0 0 1 0 0



∗

 0 0 0 0 0 0 0 0 0 0 1 0



∗

Gsys =  04 02 01 02 07 08 00 00 03 02 05 17  ∼  ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗∗ 

 8 0 0 7 1 9 10 0 3 9 2 5   1 6 10 7 8 10 0 10 4 10 3 9   5 7 4 2 3 0 1 3 5 4 4 9  9 2 10 9 0 3 9 4 8 3 4 4   9 2 9 3 0 0 10 2 7 8 7 2   10 7 7 4 6 8 5 10 5 10 0 4   5 7 4 4 0 8 7 4 4 8 10 0   9 9 0 6 8 9 4 2 7 0 8 3
7 5 0 5 4 6 2 7 2 10 3 7 5 8 5 7 6 0 1 9 9 0 10 3 8 0 8 4 6 10 5 3 8 6 3 6

∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗  ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗
∗∗ ∗∗∗∗∗∗∗∗∗ ∗∗∗∗∗ ∗∗∗ ∗∗ ∗ ∗∗∗∗∗∗∗∗∗∗

Notably, the parity nodes are now almost entirely dense. 5

(11)

B. Discussion

As seen here, traditional PM codes begin sparse, but become dense after systematic-remapping. We may expect this, since the initial sparsity of PM codes comes from their product-matrix structure, but the systematic-remapping operates generically on linear codes, not necessarily respecting the product-matrix structure. To address this, we need to understand the eﬀect of systematic remapping on Product-Matrix codes.

Traditionally, remapping is viewed as just decoding from the ﬁrst k nodes, as discussed in Section II-B. In this case, understanding decoding is suﬃcient to understand systematic remapping. This is well-suited for classical codes, where the message-space and the code-space have the same structure. However, this is not true for Product-Matrix Codes.

In Product-Matrix Codes, encoding takes a (structured) message-matrix M to a code-matrix C. In the example code above, encoding the data of the ﬁrst k = 4 nodes is a map:

m0 m1 m2 

m1 m3 m4 

c0 c1 c2 

m2 m4

m5

 

c3 c4 c5 

M =

→C =



m6 m7 m8 

c6 c7 c8 





m7 m9 m10

c9 c10 c11

m8 m10 m11

And decoding the B = 12 message symbols from the ﬁrst k = 4 nodes is the inverse map C → M , whose explicit structure follows from the decoding algorithm in [16]. However, understanding the explicit structure of the decoding

5In general, they will be entirely dense – the small sparsities here are incidental, due to small ﬁeld size.

map does not immediately aid in understanding systematic-remapping. This is because remapping is most naturally viewed as a transformation between message-matrices M → M .
We address the above challenge by presenting a framework for understanding systematic remapping for product-matrix codes, and we further use this to construct PM codes which remain sparse after systematic remapping. An example of this construction is provided below.

C. Sparse, Systematic PM Code
In Sections V and VII, we present explicit constructions of sparse systematic PM codes. Here we show the code construction presented in Section V, instantiated for the same parameters as the example of Section III-A: [n = 8, k = 4, d = 6].
The encoding matrix Ψ is chosen as:

 1 0 0 1 0 0

 0 1 0 0 8 0

 

0

0

1

0 0 5

 

4

Ψ =  4

5 4 3 1 3 2 10 5 8 7

 

3

10

9

10 4 8

 

4

4

2

8 8 4

10 3 1 5 7 6

This yields the following (non-systematic) generator matrix:  1 0 0 0 0 0 1 0 0 0 0 0
0 1 0 0 0 0 0 1 0 000 0 0 1 0 0 0 0 0 1 000
 0 1 0 0 0 0 0 8 0 0 0 0  0 0 0 1 0 0 0 0 0 8 0 0  0 0 0 0 1 0 0 0 0 0 8 0  0 0 1 0 0 0 0 0 5 0 0 0  0 0 0 0 1 0 0 0 0 0 5 0  0 0 0 0 0 1 0 0 0 0 0 5  4 5 4 0 0 0 3 1 3 0 0 0  0 4 0 5 4 0 0 3 0 1 3 0 G =  40 20 140 00 05 40 05 08 37 00 01 30  ∼  0 4 0 2 10 0 0 5 0 8 7 0   0 0 4 0 2 10 0 0 5 0 8 7   3 10 9 0 0 0 10 4 8 0 0 0   0 3 0 10 9 0 0 10 0 4 8 0   0 0 3 0 10 9 0 0 10 0 4 8   4 4 2 0 0 0 8 8 4 0 0 0  0 4 0 4 2 0 0 8 0 8 4 0  0 0 4 0 4 2 0 0 8 0 8 4
10 3 1 0 0 0 5 7 6 0 0 0 0 10 0 3 1 0 0 5 0 7 6 0 0 0 10 0 3 1 0 0 5 0 7 6

∗

∗



∗

∗

∗

∗

∗

∗





∗

∗



∗

∗

∗

∗





∗

∗



∗

∗

∗ ∗ ∗

∗∗∗



 ∗ ∗∗ ∗ ∗∗ 

 ∗ ∗ ∗∗ ∗ ∗ ∗ ∗ ∗∗ ∗ ∗ 





 ∗ ∗∗ ∗ ∗∗ 

 ∗ ∗ ∗ ∗ ∗ ∗

∗ ∗ ∗

∗∗∗



 ∗ ∗∗ ∗ ∗∗ 

 ∗ ∗ ∗ ∗ ∗ ∗

∗ ∗ ∗

∗∗∗



 ∗ ∗∗ ∗ ∗∗ 

∗ ∗∗ ∗ ∗∗

∗ ∗ ∗

∗∗∗



∗ ∗∗ ∗ ∗∗

∗ ∗∗ ∗ ∗∗

After systematic-remapping, the ﬁnal generator matrix is:

1 0 0 0 0 0 0 0 0 0 0 0

∗



0 1 00 0 0 0 0 0 0 00

∗

0 0 10 0 0 0 0 0 0 00

∗

0 0 0 1 0 0 0 0 0 0 0 0



∗



0 0 0 0 1 0 0 0 0 0 0 0



∗



0 0 0 0 0 1 0 0 0 0 0 0



∗



0 0 0 0 0 0 1 0 0 0 0 0



∗



0 0 0 0 0 0 0 1 0 0 0 0



∗



0 0 0 0 0 0 0 0 1 0 0 0



∗



0 0 0 0 0 0 0 0 0 1 0 0



∗

0 0 0 0 0 0 0 0 0 0 1 0



∗

Gsys =  08 02 04 05 00 00 100 00 00 100 00 10  ∼  ∗ ∗ ∗ ∗ ∗ ∗ ∗ 

 0 2 0 4 10 3 0 9 0 0 5 0  0 0 4 0 0 9 8 3 7 0 0 9 9 9 6 3 0 0 9 0 0 4 0 0 0 4 0 7 9 2 0 4 0 0 9 0  0 0 3 0 0 1 1 2 10 0 0 8   9 10 2 3 0 0 5 0 0 7 0 0  0 1 0 9 6 6 0 2 0 0 4 0 0 0 7 0 0 4 4 6 9 0 0 1
1 6 65 0 0 8 0 0 5 00 0 5 01 9 6 0 2 0 0 10 0 0 60 0 7 1 6 9 0 09

 ∗ ∗∗∗ ∗ ∗   ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗   ∗ ∗∗∗ ∗ ∗   ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗   ∗ ∗∗∗ ∗ ∗   ∗ ∗ ∗ ∗ ∗ ∗
∗∗∗∗ ∗ ∗ ∗ ∗∗∗ ∗ ∗ ∗ ∗∗∗∗ ∗

(12)

Notice that in this case (compared to (11)) the sparsity is not lost in systematic-remapping: Each row of Gsys is still d = 6-sparse.

IV. First Step towards Sparsity in PM Codes
In this section we analyze a simple family of encoding matrices Ψ which, after systematic remapping, results in codes with partial sparsity. The tools developed here will be useful in subsequent sections. Recall the structure of the encoding matrix for d = (2k − 2) PM codes from (4):
Ψ = Φ ΛΦ .

Consider a d = (2k − 2) PM code in which the ﬁrst row of Φ is e1 = 1 0 . . . 0 . 6 Then the encoding matrix is of

the form

Ψ = e1 λe1 .

(13)

Φ ΛΦ

We will now show that under such PM codes, the ﬁrst symbol stored in every node is d-sparse after systematic remapping.

Let Ψk denote the ﬁrst k rows of Ψ, that is, the encoding submatrix for the ﬁrst k nodes. Then the ﬁrst k nodes store

Ck = ΨkM.

(14)

Let fe : M → Ck denote the above encoding function for the ﬁrst k nodes. We represent systematic remapping as a linear transformation fS : M → M between the original matrix M and the resultant message matrix after transformation M . After the remapping, the ﬁrst k nodes become systematic (see Section II-B). That is, the transformation fS is such that if we encode the ﬁrst k nodes using message matrix M , we recover the original symbols of M in matrix Ck. Equivalently, for a systematic code, the entire encoding transform:

M −f→S M −f→e Ck

(15)

must act as an inclusion map M →− Ck. This inclusion map “unwraps” the symmetric matrices in M into one matrix Ck with distinct message symbols.

To understand the interaction between the PM code and systematic remapping, we will deﬁne an explicit inclusion
map fι, and decompose the remapping fS into two stages. We ﬁrst represent the matrix M as the matrix Ck using the inclusion map fι, and then “decode” Ck into M using the decoding function fe−1. Note that fe is invertible since it is an MDS encoding, wherein all message symbols can be decoded from any k nodes. The remapping transform thus becomes

fS = fe−1 ◦ fι

(16)

In other words,

fι

fe−1

fS : M −→ Ck −−→ M

(17)

Thus the entire encoding transformation for the ﬁrst k nodes becomes

fι

fe−1

fe

M −→ Ck −−→ M −→ Ck

(18)

Notice that this makes the entire encoding transform M → Ck an inclusion map (equal to fι, in fact), thus resulting in a systematic code as desired.
Remark 1. Any choice of inclusion map in (16) will yield a systematic remapping. However, as we will see, our particular choice of fι will be convenient for proving sparsity results.

At a high level, the key ideas behind our approach for showing sparsity are as follows.
(1) For our choices of Ψ and fι, the systematic remapping fS : M → M is such that the ﬁrst column of M depends only on the ﬁrst column of M (Lemma 2).
(2) The ﬁrst stored symbol in node i is the ith row of Ψ times the ﬁrst column of M . This depends only on the ﬁrst column of M , and therefore (through fS) depends only on the ﬁrst column of M .
And Lemma 2 holds because:
6 Here we assume that such codes exist, and analyze their properties. Explicit constructions of such codes are presented in Section V-B.

(1) The “decoding”, fe−1 : Ck → M is such that the ﬁrst column of M depends only on the ﬁrst row and ﬁrst column of Ck (Lemma 1).
(2) Our inclusion map fι : M →− Ck will be such that the symbols in the ﬁrst row/column of Ck correspond exactly to the ﬁrst column of M .
The sparsity pattern of systematic remapping (Lemma 2) is visualized below:









···

···





 

·

·

·

 

···

 

·

·

·

 









 · · ·  −→fι  · · ·  −f−e−→1  · · · 

 

·

·

·

 

· · ·









 

·

·

·

 





· · ·





···

· · ·





···

···

fι

fe−1

fS : M −→ Ck −−→ M

We now consider each component of the systematic remapping transformation in detail, and then prove the sparsity of the entire encoding.

A. The Triangular Inclusion Map
Here we will deﬁne the inclusion map fι, termed the “triangular inclusion map.” Recall from Section II-A that the message matrix in d = (2k − 2) PM codes is of the form M = SSab , where Sa and Sb are symmetric matrices of message symbols.
To map M →− Ck by inclusion, place the upper-triangular half of Sa on the upper-triangular half of Ck, including the diagonal. Then place the lower-triangular half of Sb on the lower-triangular half of Ck, excluding the diagonal. Finally, place the diagonal of Sb on the last row of Ck. For example, consider a PM code with k = 4, d = 6, for which α = 3 and number of message-symbols B = 12. The triangular inclusion map fι in this case is:

 0

 

1



2

M =

 

6



7 

8



12



3

4

 

0



4 5  →− Ck =  7

7 8

8





9 10 

6



10 11

 12

3

4

 



10 5  

9 11

In the above matrices, numbers refer to symbol indices. Notice that symbols in column i (0 ≤ i ≤ 2) of M correspond exactly to symbols in row i and column i of Ck.

B. The Inverse Map
Here we will consider the structure of the inverse map fe−1 : Ck → M , and show that it has a particular sparsity pattern. Lemma 1. In the inverse transform fe−1 : Ck → M , the ﬁrst column of M depends only on the ﬁrst row and ﬁrst column of Ck.
Proof: Let ψ1 denote the ﬁrst row of Ψ. In the encoding transform fe : M → Ck, notice that the ﬁrst row and ﬁrst column of Ck depend only on the ﬁrst column of M :
• The ﬁrst row of Ck is ψ1 times M . Since ψ1 = e1 λe1 , this only involves symbols in the ﬁrst row of Sa and ﬁrst row of Sb. Or equivalently, the ﬁrst column of M .
• The ﬁrst column of Ck = ΨM clearly depends only on the ﬁrst column of M .

Therefore, we can consider the restriction of the map fe : M → Ck to symbols in the ﬁrst column of M , and the ﬁrst row/column of Ck. There are d symbols in both domain and co-domain. Further, this map is full-rank by construction, since it is a restriction of MDS encoding. Therefore this map is invertible, and the ﬁrst column of M can be recovered from the ﬁrst row/column of Ck.

C. Sparsity

Here we combine the above maps and show that the entire encoding transform has a certain sparsity. The following lemma serves as our main tool.
Lemma 2. In the systematic-remapping transform fS : M → M , a symbol in the ﬁrst column of M only depends on symbols in the ﬁrst column of M .

fι

fe−1

Proof: From (17), we write the remapping transform as M −→ Ck −−→ M . From Lemma 1, the ﬁrst column of M

depends only on the ﬁrst row/column of Ck. And by the triangular inclusion map fι as deﬁned in Section IV-A, the

ﬁrst row/column of Ck corresponds to the ﬁrst column of M .

We can then show partial sparsity of the entire encoding:

Theorem 1. Consider a d = (2k − 2) PM code in which the ﬁrst row of Φ is e1 = 1 0 . . . 0 . When this code is made systematic, the ﬁrst symbol stored in every node is d-sparse.

Proof: The ﬁrst symbol of each node is a row of Ψ times the ﬁrst column of M . But the ﬁrst column of M depends

only on the ﬁrst column of M (by Lemma 2), so the ﬁrst symbol of each node is d-sparse w.r.t. symbols in M . Essentially,

the sparsity occurs because the sparsity patterns of the following two transformations, restricted to the ﬁrst column of

M , are aligned:

M → M → ΨM .

(19)

Remark 2. An analogous argument shows that if one of the ﬁrst k rows of Φ is ei, then the i-th symbol of every node is d-sparse.
Remark 3. It may seem that the above sparsity argument only works with our particular inclusion map fι, but in fact it applies to any systematic remapping. Notice that the systematic remapping function is unique up to permutation of the kα message symbols. Therefore, if some ﬁnal encoded symbol is a function of d original message symbols for a particular systematic-remapping function, it will remain a function of some d (permuted) message symbols in any other systematic-remapping.

V. Explicit Sparse, Systematic PM Codes for d = (2k − 2)
In this section, we ﬁrst consider a particular design of encoding matrices Ψ, and prove that, after systematic remapping, they yield PM codes in which each encoded symbol is d-sparse. We then present explicit constructions of such matrices. Our analysis builds on the techniques presented in the previous section.
In this section, for simplicity of notation, we will write the matrix Ck as simply C, so Ci,j denotes the (i, j)th entry of Ck .

A. Design of the Encoding Matrix and Sparsity

Consider a d = (2k − 2) PM code in which the ﬁrst α rows of Φ form an Identity matrix. In this case, the encoding

matrix for the ﬁrst k nodes is of the form:

IΛ

Ψk = rT λrT

(20)

where r is an α-length vector. We will show under such an encoding matrix, after systematic remapping, every encoded symbol is d-sparse.

From the properties of PM encoding matrices discussed in Section II-A, we have:

• Property 1: The diagonal entries of Λ together with λ are all distinct.

• Property 2: All sub-matrices of rIT are full-rank. In particular, all entries of rT are nonzero.

The corresponding encoding transform for the ﬁrst k nodes, fe : M → C, is:

C = ΨkM

(21)

I Λ Sa

= rT λrT Sb

(22)

Sa + ΛSb

= rT Sa + λrT Sb

(23)

:= C1

(24)

C2

1) The Inverse Map: Here we describe how to recover the message matrices Sa and Sb from Ck, thus specifying the explicit structure of the inverse map fe−1.
First, all the non-diagonal entries of Sa, Sb can be found by solving:

Ci,j = Sai,j + λiSbi,j Cj,i = Sai,j + λj Sbi,j

(25)

(Since λi = λj by Property 1).
For the diagonal entries, we ﬁrst compute Sar and Sbr as follows. First deﬁne the following two vectors, which can be computed directly from C:

c1 := C1r = Sar + ΛSbr

(26)

c2 := C2T = Sar + λSbr

(27)

Then Sar and Sbr can be computed from:

Sar = (Λ − λI)−1(Λc2 − λc1)

(28)

Sbr = (Λ − λI)−1(c1 − c2)

(29)

where the diagonal matrix (Λ − λI) is invertible by Property 1.

Now we compute the i-th diagonal entry of Sa from Sar. Let Sai denote row i of Sa. After computing Sar as above, we can extract Sair = j Sai,jrj. Then Sai,i can be computed as:

Sai,i = (Sair − Sai,j rj )/ri
j=i

(30)

Notice that the non-diagonal elements Sai,j=i are known, and ri = 0 by Property 2. The diagonal elements of Sb can be recovered similarly from Sbr.

2) Sparsity: Using the structure of the inverse map described above, together with the triangular inclusion map deﬁned in Section IV-A, we will show that the entire encoding transform is d-sparse.

Analogous to Lemma 2, we ﬁrst show that the systematic-remapping transform has a certain sparsity.

Lemma 3. In the systematic-remapping transform fS : M → M , the symbol M i,j only depends on symbols in column j of M .

Proof: First notice that the sparsity pattern of fe−1, in recovering Sa from C, is as follows:
• Non-diagonal element Sai,j depends on elements Ci,j and Cj,i, as in (25). • Diagonal element Sai,i depends on row i and column i of C. To see this, ﬁrst compute all non-diagonal entries
Sai,j=i from (25) using row i and column i of C1. Then compute the i-th component of Sar from (28), using the i-th entry of c1 and c2. Finally, compute Sai,i from (30).
And the same sparsity holds for recovering Sb from C as well.

Now let M = SSab . We will show that symbol Sai,j depends only on Sai,j and Sbi,j, and a symmetric argument holds

for

Sbi,j .

Writing

the

systematic-remapping

as

M

fι
−→ C

fe−1
−−→ M ,

there

are

two

cases:

• Non-diagonal element Sai,j depends on Ci,j and Cj,i which, by our inclusion map, correspond to Sai,j and Sbi,j. • Diagonal element Saj,j depend on row j and column j of C, which correspond to column j of M .

This allows us to show sparsity of the entire encoding.
Theorem 2. Consider a d = (2k − 2) PM code in which the ﬁrst α rows of Φ form an Identity matrix. When this code is made systematic, each encoded symbol is d-sparse.

Proof: Each encoded symbol is a row of Ψ times a column of M , by the PM encoding of (3). But each column of M depends only the corresponding column of M (by Lemma 3). Thus we conclude the ﬁnal encoding ΨM is d-sparse w.r.t. symbols of M , since the two maps have aligned sparsity patterns:

M → M → ΨM .

(31)

B. Explicit Construction

We now present explicit constructions of matrices Ψ which conform to the design of Section V-A. This yields explicit systematic d = (2k − 2) PM codes in which each encoded symbol is d-sparse.

Theorem 3. Let Ψ = Φ ΛΦ be the encoding matrix for a d = (2k − 2) PM code, satisfying the properties mentioned in Section II-A. For example, we can let Ψ be a Vandermonde matrix, as given in [16]. Let the (α × α) matrix Φα denote the ﬁrst α rows of Φ. Then the following encoding matrix:

Ψ = ΦΦ−α 1 ΛΦΦ−α 1 := Φ ΛΦ .

(32)

deﬁnes a d = (2k − 2) PM code in which, after systematic remapping, each encoded symbol is d-sparse.

Proof: The matrix Ψ satisﬁes the properties of Section II-A, since multiplication by full-rank Φ−α 1 will not destroy the rank of any submatrices of the original encoding matrix Ψ. Therefore Ψ satisﬁes all properties of a PM encoding matrix. Further, the ﬁrst α rows of Φ are the identity. So by Theorem 2, we conclude that after systematic-remapping, this code will remain d-sparse.
In other words, if we represent the encoding procedure for this systematic code as a (nα × B) generator matrix G mapping B message symbols to nα encoded symbols (α per node), then each row of G will be d-sparse.

VI. Sparsity in Systematic MSR Codes from Repair-By-Transfer
Sections IV and V dealt with constructing sparse systematic PM codes. In this section, we consider sparsity in more general systematic regenerating codes.

A. Background: MSR Codes and Repair-by-Transfer

An [n, k, d](α, β) regenerating code allows the message to be stored across n nodes, each storing α encoded symbols. All the B symbols can be recovered from the data stored in any k of the total n nodes. Further, any node’s data may be exactly recovered by connecting to any d other nodes, and downloading β ≤ α symbols from each. The symbols transferred from a helper node during node repair may in general be some arbitrary function of the data stored in it.

Minimum-Storage-Regenerating (MSR) codes are regenerating codes which are also MDS, and therefore satisfy

B = kα.

(33)

For example, an [n, k, d] PM code is an [n, k, d](α = d − k + 1, β = 1) MSR code. The seminal work by Dimakis et. al. [17] shows that for MSR codes, the parameters above must necessarily satisfy

α = β(d − k + 1).

(34)

During a node-repair operation, a helper node is said to perform repair-by-transfer (RBT) if it does not perform any computation and merely transfers one of its α stored symbols to the failed node. We say a linear [n, k, d](α, β = 1) MSR code supports RBT with the RBT-SYS pattern if every node can help the ﬁrst α nodes via RBT.
B. Sparsity from Repair-by-Transfer
We now present a general connection between sparsity and repair-by-transfer, by showing that an MSR code with a certain RBT property must necessarily be sparse.
Let C be a linear systematic MSR [n, k, d](α, β = 1) code of blocklength B = kα, with (nα × B) generator matrix G. Let G(i) be the (α × B) submatrix corresponding to the i-th node.
Theorem 4. If C supports repair of a systematic node ν via RBT with helper nodes comprising the remaining (k − 1) systematic nodes and d − (k − 1) = α other parity nodes, then for each parity i, the corresponding generator-submatrix G(i) has one row with sparsity ≤ d. In particular, the row of G(i) corresponding to the symbol transferred for the repair of node ν is supported on at most the following coordinates.
• The α coordinates corresponding to symbols stored by node ν.
• For each of the other (k − 1) participating systematic nodes µ = ν: one coordinate corresponding to a symbol stored by node µ.
Proof: Say systematic node 0 fails, and is repaired via RBT by the (k − 1) other systematic nodes, and α other parity nodes. Each helper will send one of its α stored symbols. For the systematic helpers, these symbols correspond directly to message symbols – let S be the set of these message symbol indices. Notice that S is disjoint from the symbols that node 0 stores. For the parity helpers, each transferred symbol is a linear combination of message symbols. We claim that these linear combinations cannot be supported on more than the message symbols that node 0 stores, and the set S. That is, in total the support size can be at most α + (k − 1) = d.
Intuitively, Theorem 4 holds because the symbols from systematic helpers can only “cancel interference” in (k − 1) coordinates (of S), and the α parity helpers must allow the repair of node 0’s α coordinates, and thus cannot contain more interference. This concept of interference-alignment is made precise in [28], and our Theorem 4 follows as a corollary of “Property 2 (Necessity of Interference Alignment)” proved in Section VI.D of [28]. Theorem 5. If C supports the RBT-SYS pattern, then for each parity i, the corresponding generator-submatrix G(i) has min(α, k) rows that are d-sparse. In particular, if d ≤ (2k − 1), then all rows of G are d-sparse.
Proof: In the RBT-SYS pattern, each parity node i helps the ﬁrst α nodes via RBT, including min(α, k) systematic nodes. In each repair of a systematic node, the row of G(i) corresponding to the RBT symbol sent is d-sparse (by Theorem 1). This is true for each of the symbols sent to systematic nodes. These transferred symbols correspond to distinct symbols stored in node i, by Property 3, Section 6 of [28], which states that these symbols must be linearly independent. Therefore, min(α, k) rows of G(i) are d-sparse.
In particular, for an MSR code, d ≤ (2k − 1) implies α ≤ k, so all rows of G are d-sparse in this regime.
VII. Explicit Sparse, Systematic PM Codes for d > (2k − 2)
Section VI provides a strong connection between repair-by-transfer and sparsity in systematic MSR codes. This connection allows us to construct explicit sparse PM codes for d > (2k − 2). First we review how to construct systematic PM codes which support the RBT-SYS pattern, from [21]. We then review the notion of code shortening for PM codes, from [16]. We apply these tools with the results of Section VI to present explicit systematic PM codes in which all encoded symbols are d-sparse.
A. Repair-By-Transfer (RBT) for PM Codes
Recall that in a code that supports the RBT-SYS pattern, if any of the ﬁrst α nodes fail, every remaining node can help it by simply transferring one of its stored symbols.

For any d ≥ (2k − 2), let C = ΨM be the code matrix of a PM code C. Recall from Section II that node i stores a row

cTi = ψiT M . To help repair node f , node i sends cTi µf , for some repair vector µf . In helping the ﬁrst α nodes, node i

would

thus

send

the

α

symbols

c

T i

P

where

P = µ1 · · · µα .

(35)

Deﬁne the RBT-transformed code C as the code C where the data in each node is transformed by P : node i now stores cTi P . Hence the encoding procedure for C results in the code matrix

C = CP = ΨM P.

(36)

Notice that if P is invertible, then C shares the same MDS and repair properties as C. Additionally, in C , node i can

help

repair

any

of

the

ﬁrst

α

nodes

(say,

node

j)

by

simply

transferring

its

j th

symbol:

c

T i

µj

.

For d = (2k − 2) PM codes, as reviewed in Section II, the matrix P = ΦTα , which is invertible by construction.

B. Code Shortening
The notion of code shortening allows us to construct d > (2k − 2) PM codes from a class of d = (2k − 2) PM codes. Here we describe the PM code shortening of [16], stated in terms of generator matrices.
For a generator matrix G , consider the submatrix G obtained by omitting the ﬁrst t rows and ﬁrst t columns of G . We refer to the code deﬁned by G as the code G , shortened by the ﬁrst t symbols.
An [n, k, d > 2k − 2] PM code can be constructed by simply shortening an [n , k , d = (2k − 2)] PM code, as follows.
Lemma 4. (From Theorem 6 of [16]) For any [n, k, d > 2k − 2], let G be the generator matrix of an [n = n + i, k = k + i, d = d + i = (2k − 2)](α, β) systematic PM code, where i := d − (2k − 2). Let G be the submatrix of G obtained by omitting the ﬁrst iα rows and ﬁrst iα columns. Then G deﬁnes a systematic [n, k, d](α, β) PM code.

Proof: Informally, restricting to a submatrix as above can be thought of as considering the subcode of G in which the ﬁrst i nodes store all 0-symbols. (Or equivalently, where the ﬁrst iα message symbols are all 0). The regeneration and repair properties of G still hold in G with i less helpers (k = k − i, d = d − i) since the ﬁrst i “dummy nodes” of G can be assumed to always send 0 when participating in regeneration or repair. Further, this new code still operates at the MSR point, since the number of message symbols is k α − iα = kα.
Formally, the statement follows directly from Theorem 6 and Corollary 8 of [16].

C. Explicit Construction
Sparse systematic d > (2k − 2) MSR codes can be constructed by RBT-transforming a d = (2k − 2) PM code, and then shortening appropriately. The following theorem presents this result.
Theorem 6. Consider a [n, k, d > (2k − 2)] systematic PM code C constructed by shortening a [n = n + i, k = k + i, d = (2k − 2)] systematic PM code C that supports RBT-SYS, where i := (d − (2k − 2)). Let G denote the generator matrix for code C. Letting G(j) denote the (α × kα) submatrix of G for node j, the following sparsity holds for all nodes j.
• The ﬁrst (d − 2k + 2) rows of G(j) are k-sparse. • The remaining (k − 1) rows of G(j) are d-sparse.

Proof: By Lemma 4, the shortened generator matrix G deﬁnes an [n, k, d](α, β) linear systematic MSR code. The sparsity of G follows from applying Theorem 4 to the code G . In particular, the ﬁrst iα columns of G are omitted in G. In the code G , these columns correspond to symbols in the ﬁrst i systematic nodes – we interchangeably denote these columns/nodes by set N .
Consider a row of G corresponding to a symbol transferred for the repair (via RBT) of some systematic node ν ∈ N . By Theorem 4, the restriction of this row to columns outside N must be k-sparse, since it can only be supported on one symbol per systematic node µ ∈ N . There must be |N | = i = (d − 2k + 2) such rows per G(j) since the code G supports RBT-SYS, and symbols transferred from a given node for the repair of two diﬀerent nodes must be linearly independent (in d = (2k − 2) PM codes) by Property 3, Section 6 of [28].
Now consider a row of G corresponding to a symbol transferred for the repair (via RBT) of some systematic node ν ∈ N . By Theorem 4, the restriction of this row to columns outside N must be (α + k − 1) = d-sparse, since it can

only be supported on the α symbols of ν plus one symbol per remaining systematic node µ ∈ N, µ = ν. This comprises the remaining rows of each G(j), similarly by the RBT-SYS property and Property 3, Section 6 of [28].
Remark 4. It is interesting to note that the sparsity provided by the codes of Theorem 6 is greater than the sparsity guaranteed by a generic [n, k, d > (2k −2)](α, β = 1) linear systematic MSR code that supports RBT-SYS. By Theorem 5, such a code would be such that the ﬁrst k symbols stored in every node are d-sparse, while the remaining symbols may be dense. 7

VIII. Equivalence in Sparse Systematic PM Code Constructions

The previous sections present two diﬀerent ways of a constructing sparse d = (2k − 2) PM code from a given d = (2k − 2) PM code:

(1) Apply the RBT-transformation of Section VII-A to yield a code that is sparse (by Theorem 5). (2) Transform the encoding matrix Φ to contain an identity block, as in Equation (32) of Theorem 3.

Interestingly, it turns out that these two constructions are equivalent up to a transform termed symbol-remapping, which is deﬁned below.

Symbol-remapping is deﬁned as any invertible transformation on the message-space of a code. For example, systematic-

remapping is a special case of symbol-remapping for achieving systematic codes. Two codes with encoding functions f1

and f2 are equivalent up to symbol-remapping if

f1 = f2 ◦ T

(37)

for some invertible transform T .

Theorem 7. For a given d = (2k − 2) PM code C with encoding matrix Ψ = Φ ΛΦ , consider a related code C wherein the data in each node is further transformed by an invertible linear transformation P . That is, the entire encoding operation is C = ΨM P . Then C is equivalent to a PM code with the below encoding matrix Ψ up to symbol-remapping.

Ψ := ΦP −T ΛΦP −T

(38)

Proof: Consider transforming each message-submatrix Sa and Sb by

Sa → Sa := P −T SaP −1

(39)

Notice that this transformation is invertible and preserves symmetry, so it is a symbol-remapping on the message-space of PM codes.

If we then encode C using message matrices Sa and Sb, the entire encoding operation will be:

Sa

SaP

Ψ

P =Ψ

(40)

Sb

SbP

P −T Sa = Ψ P −T Sb

(41)

= ΦP −T ΛΦP −T SSab (42)

Sa

= Ψ Sb

(43)

The above form is native PM encoding with the original message matrix M = Ψ := ΦP −T ΛΦP −T .

Sa Sb , and the new encoding matrix

Notice that if P is chosen to support RBT-SYS (as in Section VII-A), then P T will be the ﬁrst α rows of Φ, and the

encoding matrix

Ψ = ΦP −T ΛΦP −T = ΦΦ−α 1 ΛΦΦ−α 1

(44)

7 It turns out that the uniﬁed PM codes presented in [26] also have a certain degree of inherent sparsity, although not as sparse as the codes of Theorem 6. It can be shown using an inclusion map argument that the codes of [26], in systematic form, have the following sparsity pattern: the last (α − k) symbols stored in every node are k-sparse. Interestingly, the RBT-transformed version of these codes have essentially the complementary sparsity pattern (by the present remark).

is identical to the encoding matrix (32) of the explicit sparse codes of Theorem 3.
Thus these two methods of constructing sparse codes are equivalent up to symbol-remapping.
References
[1] S. Ghemawat, H. Gobioﬀ, and S.-T. Leung, “The google ﬁle system,” in ACM SIGOPS operating systems review, vol. 37, no. 5. ACM, 2003, pp. 29–43.
[2] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson, “Diskreduce: Raid for data-intensive scalable computing,” in Proceedings of the 4th Annual Workshop on Petascale Data Storage. ACM, 2009, pp. 6–10.
[3] D. Borthakur, R. Schmidt, R. Vadali, S. Chen, and P. Kling, “Hdfs raid,” in Hadoop User Group Meeting, 2010. [4] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran, “A solution to the network challenges of data recovery
in erasure-coded distributed storage systems: A study on the Facebook warehouse cluster,” in Proc. USENIX HotStorage, Jun. 2013. [5] A. Vulimiri, O. Michel, P. Godfrey, and S. Shenker, “More is less: Reducing latency via redundancy,” in 11th ACM Workshop on Hot
Topics in Networks, Oct. 2012, pp. 13–18. [6] G. Liang and U. C. Kozat, “Fast cloud: Pushing the envelope on delay performance of cloud storage with coding,” Networking, IEEE/ACM
Transactions on, vol. 22, no. 6, pp. 2012–2025, 2014. [7] G. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica, “Why let resources idle? Aggressive cloning of jobs with Dolly,” in USENIX
HotCloud, Jun. 2012. [8] J. Dean and L. A. Barroso, “The tail at scale,” Communications of the ACM, vol. 56, no. 2, pp. 74–80, 2013. [9] N. B. Shah, K. Lee, and K. Ramchandran, “When do redundant requests reduce latency?” in Communication, Control, and Computing
(Allerton), 2013 51st Annual Allerton Conference on. IEEE, 2013, pp. 731–738. [10] S. Lin and D. Costello, Error Control Coding: Fundamentals and Applications, ser. Prentice-Hall computer applications in electrical
engineering series. Prentice-Hall, 1983. [11] K. V. Rashmi, N. B. Shah, P. V. Kumar, and K. Ramchandran, “Explicit and optimal exact-regenerating codes for the minimum-
bandwidth point in distributed storage,” in Proc. IEEE International Symposium on Information Theory (ISIT), Austin, Jun. 2010, pp. 1938–1942. [12] N. B. Shah, K. V. Rashmi, P. V. Kumar, and K. Ramchandran, “Interference alignment in regenerating codes for distributed storage: Necessity and code constructions,” IEEE Transactions on Information Theory, vol. 58, no. 4, pp. 2134–2158, Apr. 2012. [13] I. Tamo, Z. Wang, and J. Bruck, “Zigzag codes: MDS array codes with optimal rebuilding,” Information Theory, IEEE Transactions on, vol. 59, no. 3, pp. 1597–1616, 2013. [14] D. Papailiopoulos, A. Dimakis, and V. Cadambe, “Repair optimal erasure codes through Hadamard designs,” IEEE Transactions on Information Theory, vol. 59, no. 5, pp. 3021–3037, May 2013. [15] V. Cadambe, C. Huang, J. Li, and S. Mehrotra, “Polynomial length MDS codes with optimal repair in distributed storage,” in Forty Fifth Asilomar Conference on Signals, Systems and Computers, Nov. 2011, pp. 1850–1854. [16] K. Rashmi, N. B. Shah, and P. V. Kumar, “Optimal exact-regenerating codes for distributed storage at the msr and mbr points via a product-matrix construction,” Information Theory, IEEE Transactions on, vol. 57, no. 8, pp. 5227–5239, 2011. [17] A. G. Dimakis, P. Godfrey, Y. Wu, M. J. Wainwright, and K. Ramchandran, “Network coding for distributed storage systems,” Information Theory, IEEE Transactions on, vol. 56, no. 9, pp. 4539–4551, 2010. [18] S. Jiekak, A.-M. Kermarrec, N. Le Scouarnec, G. Straub, and A. Van Kempen, “Regenerating codes: A system perspective,” ACM SIGOPS Operating Systems Review, vol. 47, no. 2, pp. 23–32, 2013. [19] N. L. Scouarnec, “Fast product-matrix regenerating codes,” CoRR, vol. abs/1412.3022, 2014. [20] K. Rashmi, N. B. Shah, P. V. Kumar, and K. Ramchandran, “Explicit construction of optimal exact regenerating codes for distributed storage,” in Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on. IEEE, 2009, pp. 1243–1249. [21] K. Rashmi, P. Nakkiran, J. Wang, N. B. Shah, and K. Ramchandran, “Having your cake and eating it too: Jointly optimal erasure codes for i/o, storage, and network-bandwidth,” in 13th USENIX Conference on File and Storage Technologies (FAST 15), Santa Clara, CA, 2015. [22] S. El Rouayheb and K. Ramchandran, “Fractional repetition codes for repair in distributed storage systems,” in Allerton Conference on Control, Computing, and Communication, Urbana-Champaign, Sep. 2010. [23] S. Pawar, N. Noorshams, S. El Rouayheb, and K. Ramchandran, “Dress codes for the storage cloud: Simple randomized constructions,” in Proc. IEEE International Symposium on Information Theory (ISIT), St. Petersburg, Aug. 2011. [24] Y. Hu, P. P. Lee, and K. W. Shum, “Analysis and construction of functional regenerating codes with uncoded repair for distributed storage systems,” in INFOCOM, 2013 Proceedings IEEE. IEEE, 2013, pp. 2355–2363. [25] K. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran, “A hitchhiker’s guide to fast and eﬃcient data reconstruction in erasure-coded data centers,” in Proceedings of the 2014 ACM conference on SIGCOMM. ACM, 2014, pp. 331–342. [26] S.-J. Lin, W.-H. Chung, Y. S. Han, and T. Y. Al-Naﬀouri, “A uniﬁed form of exact-msr codes via product-matrix frameworks,” Information Theory, IEEE Transactions on, vol. 61, no. 2, pp. 873–886, 2015. [27] M. Kurihara and H. Kuwakado, “Generalization of Rashmi-Shah-Kumar Minimum-Storage-Regenerating Codes,” arXiv preprint arXiv:1309.6701, 2013. [28] N. B. Shah, K. Rashmi, P. V. Kumar, and K. Ramchandran, “Interference alignment in regenerating codes for distributed storage: Necessity and code constructions,” Information Theory, IEEE Transactions on, vol. 58, no. 4, pp. 2134–2158, 2012.

