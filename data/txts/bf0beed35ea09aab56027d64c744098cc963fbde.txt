arXiv:1711.02186v2 [math.ST] 12 Dec 2018

Quickest Change Detection under Transient Dynamics: Theory and Asymptotic Analysis
Shaofeng Zou†, Georgios Fellouris*, Venugopal V. Veeravalli*
†University at Buﬀalo, the State University of New york
*University of Illinois at Urbana-Champaign
Email: szou3@buﬀalo.edu, fellouri@illinois.edu, vvv@illinois.edu
Abstract
The problem of quickest change detection (QCD) under transient dynamics is studied, where the change from the initial distribution to the ﬁnal persistent distribution does not happen instantaneously, but after a series of transient phases. The observations within the diﬀerent phases are generated by diﬀerent distributions. The objective is to detect the change as quickly as possible, while controlling the average run length (ARL) to false alarm, when the durations of the transient phases are completely unknown. Two algorithms are considered, the dynamic Cumulative Sum (CuSum) algorithm, proposed in earlier work, and a newly constructed weighted dynamic CuSum algorithm. Both algorithms admit recursions that facilitate their practical implementation, and they are adaptive to the unknown transient durations. Speciﬁcally, their asymptotic optimality is established with respect to both Lorden’s and Pollak’s criteria as the ARL to false alarm and the durations of the transient phases go to inﬁnity at any relative rate. Numerical results are provided to demonstrate the adaptivity of the proposed algorithms, and to validate the theoretical results.
1 Introduction
In the problem of quickest change detection (QCD), a decision maker obtains observations sequentially, and at some unknown time (change-point), an event occurs and causes the distribution of the subsequent observations to undergo a change. The objective of the decision maker is to ﬁnd a stopping rule that detects the change as quickly as possible, subject to a constraint on the false alarm rate. In classical QCD formulations [2–7], the statistical behavior of the samples is characterized by the pre-change distribution and one post-change distribution, which generate the samples before and after the change-point, respectively. However, there are many practical applications with more involved statistical behavior after the change-point. For example, when a line outage occurs in a power system, the system goes through multiple transient phases before entering a persistent phase [8].
Motivated by this type of applications, in this work we study the problem of QCD under transient post-change dynamics, in which the pre-change distribution does not change to the persistent
The material in this paper was presented in part at the IEEE International Symposium on Information Theory (ISIT), Aachen, Germany, June 2017 [1].
The work of S. Zou and V. V. Veeravalli was supported in part by the National Science Foundation (NSF) under grants CCF 16-18658 and CIF 15-14245, by the Air Force Oﬃce of Scientiﬁc Research (AFOSR) under grant FA9550-16-1-0077, and by ARL under cooperative agreement W911NF-17-2-0196, through the University of Illinois at Urbana-Champaign. The work of G. Fellouris was supported by the NSF under grant CIF 15-14245, through the University of Illinois at Urbana-Champaign.
1

distribution instantaneously, but after a number of transient phases. Within the transient and persistent phases, the observations are generated by distributions diﬀerent from the initial one, and the problem is to detect the change as soon as possible either during a transient phase or during the persistent phase.
1.1 Related Work
We ﬁrst stress that the QCD problem under transient post-change dynamics that we study in this work is diﬀerent from the problem of detecting transient changes, studied in [9] and [10], in which the system goes back to its pre-change mode after a single transient phase, and where it is only possible to detect the change within the transient phase. Moreover, the setup in this paper is fundamentally diﬀerent from the model selection setup in [11, 12], where the minimum description length (MDL) principle is used to estimate the number of transient phases and the location of the change-points, with a ﬁxed number of observations. Here, we consider the case where the observations are collected sequentially, and we are only interested in detecting in real time whether the distributions no longer follow the pre-change distribution.
The QCD problem under transient dynamics was studied in [13] when there is only one transient phase that lasts for a single observation, where a generalization of Page’s Cumulative Sum (CuSum) algorithm [14] is proposed and shown to be optimal under Lorden’s criterion [15]. A Bayesian formulation is proposed in [16], in which it is assumed that there is an arbitrary, yet known, number of transient phases, whose durations are geometrically distributed. The proposed algorithm in [16] is a generalization of the Shiryaev-Roberts rule [17, 18]. A non-Bayesian formulation is considered in [8], where it is assumed that the durations are deterministic and completely unknown. The proposed algorithm in [8] is a generalization of Page’s CuSum test, called the dynamic CuSum (D-CuSum) algorithm. The algorithms in [8] and [16] are shown to admit a recursive structure, but are not supported by any theoretical performance analysis.
To be more precise, the D-CuSum algorithm was derived in [8] by reformulating the QCD problem as a dynamic composite hypothesis testing problem, where a hypothesis test is conducted at each time instant k until a stopping criterion is met. At each time k, the null hypothesis corresponds to the case that the change from the pre-change distribution has not occurred yet, and the alternative hypothesis corresponds to the case that the change has already occurred. Under the null hypothesis, all samples are distributed according to the pre-change distribution; under the alternative hypothesis, the distribution of the samples up to time k depends on the unknown change-point and the unknown durations of the transient phases, and is thus composite. The test statistic at time k is the generalized likelihood ratio between the two hypotheses, and the corresponding stopping rule is obtained by comparing the test statistic against a pre-speciﬁed threshold. We stress that the implementation of this algorithm does not require knowledge of the transient durations, which are considered to be deterministic and completely unknown.
Since the post-change distribution in our formulation is determined by the unknown durations of the transient phases, the proposed problem falls into the framework of QCD with a composite post-change distribution [19–21]. Our work diﬀers from this literature in three major ways. First, thanks to the special structure of our problem, the proposed detection statistics enjoy recursions, a very important feature for practical implementation that is typically absent in [19–21]. Second, our asymptotic analysis is novel and challenging due to the fact that it requires not only the ARL to false alarm, but also the parameters of the post-change distribution (transient durations) to go to
2

inﬁnity. Third, the distribution of the samples within each phase can be arbitrary (may not belong to an exponential family), and the parameters of the post-change distribution (transient durations) are discrete and do not belong to a compact parameter space.
1.2 Main Contributions
The ﬁrst contribution of this work is that, under a certain condition on the pre/post-change distributions, we obtain a lower bound on the ARL to false alarm of the D-CuSum algorithm, which can be used for an explicit selection of the threshold.
The second contribution of this paper is that we propose an alternative algorithm, to which we refer as weighted D-CuSum (WD-CuSum) algorithm, which also admits a recursive structure and for which we derive a universal lower bound on the ARL. This algorithm is a modiﬁcation of the D-CuSum algorithm, where the test statistic at time k is a weighted generalized likelihood ratio between the two hypotheses described above. The key idea is that instead of taking a maximum likelihood approach with respect to the composite alternative hypothesis as in the D-CuSum algorithm, we take a mixture approach and then replace the sum in the mixture with a max in order to obtain a recursive structure for the resulting algorithm.
The third contribution of this work is that we conduct an asymptotic analysis for the performance of D-CuSum and WD-CUSUM, which demonstrates the statistical eﬃciency and adaptivity of both algorithms to the unknown transient durations. For this analysis, we adopt a worst-case scenario for the unknown change-point, and the performance metrics of interest that we consider are the worsecase average detection delays (WADD) as deﬁned by Lorden [15] and Pollak [22]. As mentioned earlier, the exact minimizer of Lorden’s WADD has been derived only in the special case of a single transient phase that lasts for a single observation [13]. As the proposed algorithms do not make any assumptions regarding the (deterministic) durations of the transient phases, our goal is to show that they have “good” performance under any possible transient durations. In order to do so, we obtain an asymptotic lower bound of the WADD as the durations of the transient phases and the ARL go to inﬁnity at any relative rate, and we show that this lower bound is achieved by the WD-CuSum algorithm. We stress that this asymptotic optimality is achieved for any divergence rate of the transient phases, implying the adaptivity of the algorithm to the unknown transient durations. Similar asymptotic optimality results are also obtained for the D-CuSum algorithm.
In order to demonstrate the performance of the proposed algorithms, we conduct a simulation study which illustrates how both the D-CuSum and the WD-CuSum algorithms are adaptive to the unknown transient durations and have similar performances for any practical purposes. Moreover, we propose a heuristic approach for the selection of the weights of the WD-CuSum algorithm in the ﬁnite regime, which balances the performance within the transient and persistent phases.
1.3 Paper Organization
The remainder of this paper is organized as follows. In Section 2, we formulate the problem mathematically. In Section 3, we introduce the D-CuSum and the WD-CuSum algorithms. In Section 4 we establish lower bounds on the ARL to false alarm for both algorithms. In Section 5, we demonstrate the asymptotic optimality of both algorithms. In Section 6, we present the numerical results and propose a heuristic approach of choosing weights for the WD-CuSum algorithm. Finally, in Section 7, we provide some concluding remarks.
3

2 Problem Model

Consider a sequence of independent random variables {Xk}∞ k=1, observed sequentially by a decision maker. At an unknown change-point v1, an event occurs and {Xk}∞ k=v1 undergoes a change in distribution from the initial distribution, f0. It is assumed that this change goes through L − 1 transient phases before entering a persistent phase. Each phase i begins at an unknown starting
point vi, and the observations within this phase are generated by a known distribution fi, for 1 ≤ i ≤ L. The duration of i-th transient phase is denoted by di = vi+1 − vi, for 1 ≤ i ≤ L − 1. More speciﬁcally, the observations are distributed as follows:

Xk ∼ fi, if vi ≤ k < vi+1,

(1)

for 0 ≤ i ≤ L, where v0 = 1, v1 ≤ v2 ≤ · · · ≤ vL, and vL+1 = ∞. We assume that L is known in advance and so are the densities fi, 0 ≤ i ≤ L. On the other hand, the change point v1 and the vector of transient durations d = {di, 1 ≤ i ≤ L − 1} are assumed to be deterministic and completely unknown.

The goal is to detect the change reliably and quickly based on the sequentially acquired observa-
tions. That is, if Fk is the σ-algebra generated by the ﬁrst k observations, i.e., Fk = σ(X1, . . . , Xk), where k = 1, 2, . . ., we want to ﬁnd a {Fk}k∈N-stopping time that achieves “small” detection delay, while controlling the rate of false alarms.

To be more speciﬁc, we denote by P∞ and E∞ the probability measure and the corresponding expectation when v1 = ∞, i.e., when there is no change, and for any stopping time τ we deﬁne the ARL to false alarm as follows:

ARL(τ ) =E∞[τ ].

(2)

The ﬁrst requirement for a stopping rule is to control the expected time to false alarm above a user-speciﬁed level, γ > 1, i.e., to belong to Cγ = {τ : ARL(τ ) ≥ γ}.
In order to quantify the performance of a stopping rule, we denote by Pdv1 the probability measure with the change-point at v1 and the vector of transient durations d, and we denote by Edv1 the corresponding expectation. Then, for a given vector of transient durations d, the worst-case average detection delay of a stopping time τ under Pollak’s criterion [22] is

JPd(τ ) = sup Edv1 [τ − v1|τ ≥ v1],

(3)

v1 ≥1

and under Lorden’s criterion [15]
JLd(τ ) = sup ess sup Edv1 [(τ − v1)+|X1, . . . , Xv1−1], (4)
v1 ≥1
where (τ − v1)+ = max{τ − v1, 0}. Thus, for any given vector of transient durations d, we have the following two optimization problems:

inf JPd(τ ),

(5)

τ ∈Cγ

inf JLd(τ ).

(6)

τ ∈Cγ

An exact solution to (6) has been obtained for any given γ only when L = 1 and d1 = 1 [13]. To the best of our knowledge, problem (5) has not been solved for any value of L and d. However, our

4

interest in this work is on the case that the vector of transient durations d is completely unknown, thus, our goal is not on solving (6) or (5) for a particular choice of d. Instead, our goal is to obtain algorithms that (i) control the ARL to false alarm and (ii) have a small WADD for any value of d. Speciﬁcally, we will introduce two stopping rules (Section 3), show how to design them in order to belong to Cγ for any user-speciﬁed γ > 1 (Section 4), and also show that they attain (5) and (6) up to a ﬁrst-order approximation as γ → ∞ and d → ∞ (Section 5).

2.1 Notation

For i = 1, . . . , L, we denote by

Ii = fi log dfi df0

the Kullback-Leibler (KL) divergence between fi and f0, which we assume to be positive and ﬁnite. For i = 1, . . . , L, we set

Zi(Xk) = log fi(Xk) ,

(7)

f0 (Xk )

i.e., Zi(Xk) the log-likelihood ratio between fi and f0 for sample Xk, i = 1, . . . , L, k = 1, 2, . . .. Moreover, we set

k2 fi(Xj )

k2−1 fi(Xj )

Λi[k1, k2] =

f0(Xj ) , Λi[k1, k2) =

f0(Xj ) .

(8)

j =k1

j =k1

We denote the largest integer that is smaller than x as ⌊x⌋, and the smallest integer that is larger

than x as ⌈x⌉. We deﬁne nj=2 n1 Xj = 0 and nj=2 n1 Xj = 1 if n1 > n2. We denote x = o(1), as c → c0 if ∀ǫ > 0, ∃δ > 0, s.t., |x| ≤ ǫ if |c − c0| < δ. We denote g(c) ∼ h(c) as c → c0, if

limc→c0

f (c) g(c)

=

1.

3 The Algorithms

In this section, we introduce the proposed algorithms, we show that they admit simple recursive structures. In order to do so, we reformulate the QCD problem as a dynamic composite hypothesis testing problem, as in [8], where which at each time instant k we distinguish the following two hypotheses

H0k : k < v1,

H1k : k ≥ v1.

(9)

The process stops once a decision in favor of the alternative hypothesis is reached; otherwise, a new sample is taken. Under H0k, the samples X1, . . . , Xk are distributed according to f0. The alternative hypothesis H1k is composite, since it depends on v1, d, which are unknown.

Let Γ(k, v1, d) denote the likelihood ratio of the ﬁrst k observations, X1, . . . , Xk, for ﬁxed v1, d, i.e.,

Γ(k, v1, d)

=

P

d v1

(X1

,

.

.

.

,

Xk

)

.

(10)

P∞(X1, . . . , Xk)

5

When vi ≤ k < vi+1 for some 1 ≤ i ≤ L,

i−1

Γ(k, v1, d) = Λi[vi, k] · Λj[vj , vj+1).

(11)

j=1

For the special case with L = 2,

Γ(k, v1, d1)

= Λ1[v1, k],

if v1 + d1 > k,

(12)

Λ1[v1, v1 + d1)Λ2[v1 + d1, k], if v1 + d1 ≤ k.

We note that ν1 can be equal to ν2, i.e., d1 can be equal to 0. This implies that for a given pair of (v1, k), k ≥ ν1, depending on the time that the second change takes place, there are k − v1 + 2 possible values of Γ(k, v1, d1). In general, when L ≥ 2, for a given pair of (k, v1), k ≥ ν1, depending on the time that the second to the L-th changes take place, there are also ﬁnitely many possible
values of Γ(k, v1, d).

3.1 D-CuSum

The D-CuSum [8] detection statistic at time k is the generalized log-likelihood ratio with respect to both v1 and d, for the above hypothesis testing problem:

W [k] = max max log Γ(k, v1, d).

(13)

1≤v1≤k d∈NL−1

As we explained above, there are ﬁnitely many subhypotheses under H1k, which implies that Γ(k, v1, d) has ﬁnitely many values, and the maximization in (13) is over ﬁnitely many terms. More
speciﬁcally, equation (13) is equivalent to the one which takes maximization over

{(v1, . . . , vL) : 1 ≤ v1 ≤ k, v1 ≤ v2 ≤ · · · ≤ vL ≤ k + 1},

(14)

in which, each tuple of (v1, . . . , vL) corresponds to a distinct value of Γ(k, v1, d).
The corresponding stopping time is given by comparing W [k] against a pre-determined positive threshold:

τ (b) = inf{k ≥ 1 : W [k] > b}.

(15)

Since b > 0, without loss of generality we can adopt the positive part of W [k] as the detection statistic. It can be shown that

(W [k])+

L
= max log i=1
1≤v1 ≤···≤vL ≤k+1

jm=inv{i vi+1−1,k} fi(Xj ) kj=v1 f0(Xj )

min{v2 −1,k}

k

=

max

1≤v1 ≤···≤vL ≤k+1

j =v1

Z1(Xj) + · · · + ZL(Xj).
j =vL

(16)

6

It is shown in [8, Appendix] that (W [k])+ has a recursive structure:

(W [k])+ = max Ω(1)[k], Ω(2)[k], . . . , Ω(L)[k], 0 ,

(17)

where for 1 ≤ i ≤ L, we set Ω(i)[0] = 0 and

Ω(i)[k] = max 0, Ω(1)[k − 1], . . . , Ω(i)[k − 1] + Zi(Xk).

(18)

Remark 1. The L-dimensional random vector {Ω(1)[k], . . . , Ω(L)[k]} depends on X1, . . . , Xk−1 only through {Ω(1)[k − 1], . . . , Ω(L)[k − 1]}; thus, it is a Markov process. When all of its components are simultaneously non-positive, or equivalently when (W [k])+ equals 0 at some k, then (W [k])+ forgets all previous observations and restarts from zero, i.e., it regenerates.

3.2 WD-CuSum

If we take a mixture approach with respect to d, combined with a maximum likelihood approach with respect to v1, this suggests the following stopping rule:

τ ′(b) = inf{k ≥ 1 : W ′[k] ≥ b},

(19)

where b is a positive threshold and the detection statistic is





W ′[k] = max log 

Γ(k, v1, d)g(d) ,

(20)

1≤v1 ≤k

d∈NL−1

and g is a pmf on NL−1. Recall that for a given pair of (k, ν1), k ≥ ν1, there are ﬁnitely many

possible values of Γ(k, v1, d). Therefore, this mixture in (20) is equivalent to a sum over ﬁnitely

many terms. Here, we denote the total number of all possible values of Γ(k, v1, d) by n(k, ν1), and

denote each distinct value of Γ(k, v1, d) by λ(k, v1, j), 1 ≤ j ≤ n(k, ν1). Then,





n(k,v1 )

W ′[k] = max log 

λ(k, v1, j)gj  ,

(21)

1≤v1≤k j=1

where

gj =

g(d).

d:Γ(k,v1 ,d)=λ(k,v1 ,j )

(22)

Replacing the sum with a maximum, we obtain

W [k] = max max log λ(k, v1, j)gj ,

(23)

1≤v1≤k 1≤j≤n(k,v1)

which leads to the following stopping rule:

τ (b) = inf{k ≥ 1 : W [k] ≥ b}.

(24)

We refer to this stopping rule in (24) as the WD-CuSum algorithm. We note that the reason we replace the sum with a maximum is that with a particular choice of g, the resulting algorithm has a recursive structure, which can be updated eﬃciently.

7

In the following, we focus on τ for a particular choice of g, which yields a recursive structure for W . In particular, if we choose

L−1

g(d) = ρi(1 − ρi)di ,

(25)

i=1

for some ρi ∈ (0, 1), 1 ≤ i ≤ L − 1, and consider the positive part of W [k] (since b > 0), then

where for i = 1, . . . , L,

(W [k])+ =

max

log

1≤v1 ≤···≤vL ≤k+1

L i=1

Bi

,

kj=v1 f0(Xj )





min{vi+1 −1,k}

Bi= 

fi(Xj

)(1

−

ρi)

ρ1{k≥vi+1 }
i

,

j =vi

(26) (27)

with vL+1 = ∞ and ρL = 0. Following steps similar to those in [8, Appendix], it can be shown that

(W [k])+ = max Ω(1)[k], . . . , Ω(L)[k], 0 ,

(28)

where for 1 ≤ i ≤ L,

Ω(i)[k] = max
0≤j≤i

i−1
Ω(j)[k − 1] + log ρℓ
ℓ=j

+ Zi(Xk) + log(1 − ρi),

(29)

with Ω(0)[k] = 0, for all k, and ρ0 = 1. Example 1. When L = 2, setting G(x) =

k>x g(k), we have

W ′[k] = max log
1≤v1 ≤k

k−ν1
g(d1)Λ1[v1, v2)Λ2[v2, k]
d1 =0

+ G(k − ν1)Λ1[v1, k] ,

W [k] = max log max max g(d1)Λ1[v1, v2)

1≤v1 ≤k

0≤d1 ≤k−ν1

× Λ2[v2, k], G(k − ν1)Λ1[v1, k] .

(30)

4 Lower Bounds on the ARL
In this section, we obtain non-asymptotic lower bounds on the ARL to false alarm for the D-CuSum algorithm and the WD-CuSum algorithm.
8

4.1 D-CuSum

It is interesting to point out that unlike the classical CuSum statistic, which we recover by setting L = 1, {Ω(1)[k], . . . , Ω(L)[k]} does not always regenerate under P∞ for L ≥ 2. Denote by Y the ﬁrst
regeneration time, i.e.,

Y = inf{k ≥ 1 : (W [k])+ = 0}.

(31)

The following example shows that Y is not always ﬁnite.

Example 2. Suppose that L = 2 and f0, f1, f2 are chosen such that

f0(x) = 0.5 × 1{x∈[0,2]}, f1(x) = 0.8 × 1{x∈[0,1]} + 0.2 × 1{x∈(1,2]}, f2(x) = 0.2 × 1{x∈[0,1]} + 0.8 × 1{x∈(1,2]}.

(32)

Then, max {Z1(x), Z2(x)} > 0, ∀x ∈ [0, 2], which implies that for all k ≥ 1, we have pathwise

(W [k])+ = max Ω(1)[k], Ω(2)[k], 0 > 0.

(33)

If we assume that the pre- and post-change distributions satisfy the following condition:

P∞(Y > m) ≤ e−αm, ∀m ≥ 1,

(34)

where α > 0 is a constant, then Y is ﬁnite with probability one. In other words, the probability that (W [k])+ regenerates within ﬁnite time is one. Moreover, the expectation of the regeneration time Y is upper bounded by a ﬁnite constant:
E∞[Y ] ≤ 1 + ∞ e−αm ≤ 1 + α1 < ∞. (35)
m=1

Therefore, (W [k])+ is regenerative, and the ARL of the D-CuSum algorithm is lower bounded as in the following proposition. See Appendix C for an example of suﬃcient conditions for (34) to hold.

Proposition 1. Consider the QCD problem under transient dynamics described in Section 2. Assume that the pre- and post-change distributions satisfy condition (34). If the D-CuSum algorithm is applied with a threshold b, then the ARL is lower bounded as follows:

eb

E∞[τ (b)] ≥ 1 + (b/α)L+1 .

(36)

Proof. See Appendix A.
Corollary 1. Assume that the pre- and post-change distributions satisfy condition (34). To guarantee E∞[τ (b)] ≥ γ, it suﬃces to choose b such that
eb (b/α)L+1 + 1 = γ, and b ∼ log γ.

Proof. The result follows from Proposition 1.

9

4.2 WD-CuSum

Theorem 1. Consider the QCD problem under transient dynamics described in Section 2. Assume that the WD-CuSum algorithm in (24) is applied with threshold b and any ρi ∈ (0, 1), 1 ≤ i ≤ L − 1. Then, the ARL of the WD-CuSum algorithm is lower bounded as follows:

E∞[τ (b)] ≥ 1 eb.

(37)

2

Proof. See Appendix B.
Remark 2. The lower bound can be further tightened to eb by using Doob’s optional sampling theorem [23] instead of the submartingale inequality. However, this does not provide order-level improvement.
Corollary 2. To guarantee E∞[τ (b)] ≥ γ, it suﬃces to choose

b = log γ + log 2 ∼ log γ.

(38)

Proof. The result follows from Theorem 1.

5 Asymptotic Analysis

In this section we study the asymptotic performance of the proposed algorithms and demonstrate their asymptotic optimality. For our asymptotic analysis to be non-trivial, we let not only the prescribed lower bound on the ARL, γ, go to inﬁnity, but also the transient durations. Indeed, if the latter are ﬁxed as γ goes to inﬁnity, then the CuSum algorithm that detects the change from f0 to fL, completely ignoring the transient phases, can be shown to be asymptotically optimal using the techniques in [19]. Therefore, in order to perform a general and relevant asymptotic analysis, we let d1, . . . , dL−1 go to inﬁnity with γ. Without loss of generality, we assume that there are constants ci ∈ [0, ∞], i = 1, . . . , L − 1 so that

di ∼ ci log γ ,

(39)

Ii

where cL = ∞. Note that if di = o(log γ), then ci = 0, whereas if log γ = o(di), then ci = ∞. We stress that {ci, i = 1, . . . , L − 1} are unknown, since the transient durations are unknown, and are not utilized in the design of the proposed rules. However the optimal asymptotic performance will
turn out to be a function of these constants.

We start with the case of a single transient phase (L = 2), since it captures the essential features of the analysis, and then present the generalization to L > 2.

5.1 Asymptotic Universal Lower Bound on the WADD
Consider the case with L = 2, for which d = d1. As will be shown in the following, the optimal asymptotic performance depends on whether c1 ≥ 1 or c1 < 1. This dichotomy can be seen in the following asymptotic universal lower bound on the WADD.
10

Theorem 2. Consider the QCD problem under transient dynamics described in Section 2 with L = 2. Suppose that (39) holds, i.e., d1 ∼ c1 log γ/I1.

(i) If c1 ≥ 1, then as γ → ∞,

inf JLd1 (τ ) ≥ inf JPd1 (τ )

τ ∈Cγ

τ ∈Cγ

≥ log γ (1 − o(1));

(40)

I1

(ii) if c1 < 1, then as γ → ∞,

inf JLd1 (τ ) ≥ inf JPd1 (τ )

τ ∈Cγ

τ ∈Cγ

≥ log γ 1 − c1 + c1 (1 − o(1)).

(41)

I2

I1

Proof. See Appendix E.

Theorem 2 suggests that to meet the asymptotic universal lower bound on the WADD, an algorithm should be adaptive to the unknown d1.
The proof of the asymptotic universal lower bound is based on a change-of-measure argument and the Weak Law of Large Numbers for log-likelihood ratio statistics, similarly to [19]. However, a major diﬀerence is that when changing measures, the post-change statistic is more complicated, due to the cascading of the transient and persistent distributions. In the proof, a decomposition of the sum of the log-likelihood of the samples is necessary before the application of the Weak Law of Large Numbers.

5.2 Asymptotic Upper Bounds on the WADD

We now establish asymptotic upper bounds on the WADD of the proposed algorithms for a threshold b. Again, we start with the case case of a single transient phase (L = 2), in which the WD-CuSum algorithm depends only on the parameter ρ1. Speciﬁcally,

(W [k])+ = max{Ω(1)[k], Ω(2)[k], 0},

(42)

where

Ω(1)[k] =

Ω(1)[k − 1]

+
+ Z1(Xk) + log(1 − ρ1),

Ω(2)[k] = max log ρ1, Ω(1)[k − 1] + log ρ1, Ω(2)[k − 1]

+ Z2(Xk).

(43)

As we can observe from (43), the “drift” of the WD-CuSum algorithm for the samples within the transient phase is I1 + log(1 − ρ1), and there is a negative constant log ρ1 added to Ω(2)[k]. To
meet the asymptotic universal lower bound on the WADD, which does not depend on ρ1, we need to mitigate the eﬀect of ρ1 on the performance. If we choose ρ1 such that as b → ∞,

ρ1 → 0 and log ρ1 → 0,

(44)

b

11

e.g., ρ1 = 1/b, then the “eﬀective drift” within the transient phase is I1(1 − o(1)), and the “eﬀective threshold” is b(1 + o(1)), asymptotically. In this way, the eﬀect of the weights on the upper bound is asymptotically negligible.

Furthermore, suppose that there is a constant c′1 ∈ [0, ∞], such that d1 ∼ c′1 Ib1 . (45)
If we choose b ∼ log γ, then c1 = c′1, where c1 is deﬁned in (39).
The following theorem characterizes asymptotic upper bounds on the WADD for the WD-CuSum and D-CuSum algorithms.

Theorem 3. Consider the QCD problem under transient dynamics described in Section 2 with L = 2. Suppose that (44) and (45) hold. Consider the WD-CuSum algorithm in (24), and the D-CuSum algorithm in (15).

(i) If c′1 > 1, then as b → ∞,

Jd1(τ (b)) = Jd1(τ (b)) ≤ b (1 + o(1)),

(46)

L

P

I1

Jd1(τ (b)) = Jd1(τ (b)) ≤ b (1 + o(1));

(47)

L

P

I1

(ii) if c′1 ≤ 1, then as b → ∞,

J d1 (τ (b)) = J d1 (τ (b)) ≤ b c′1 + 1 − c′1 (1 + o(1)),

(48)

L

P

I1

I2

J d1 (τ (b)) = J d1 (τ (b)) ≤ b c′1 + 1 − c′1 (1 + o(1)).

(49)

L

P

I1

I2

Proof. By Remark 1 it follows that for the D-CuSum the worse-case scenario for the observations up to the change-point v1 is when W [v1] = 0, and consequently for every b > 0 and d we have

JLd(τ (b)) = JPd(τ (b)) = Ed1 [τ (b)].

(50)

Similarly we can argue that

JLd(τ (b)) = JPd(τ (b)) = Ed1 [τ (b)].

(51)

Thus, the WADD for the D-CuSum and the WD-CuSum algorithms is achieved when v1 = 1 under both Lorden’s and Pollak’s criteria. Moreover, by the construction of the D-CuSum and the WD-CuSum algorithms in (15) and (24), for any k ≥ 1 we have

W [k] ≤ W [k],

(52)

which is due to the fact that the weights in the WD-CuSum algorithm are less than one. Therefore, with the same threshold b, the WD-CuSum algorithm will always stop later than the D-CuSum algorithm, thus

Ed11 [τ (b)] ≤ Ed11 [τ (b)],

(53)

and

it

suﬃces

to

upper

bound

E

d1 1

[τ

(b

)],

which

is

done

in

Appendix

F.

12

The proof of the asymptotic upper bounds on WADD is based on an argument of partitioning the samples into independent blocks and the Law of Large Numbers for log-likelihood ratio statistics similar to those in [19, Theorem 4]. The major diﬃculty is due to the more complicated post-change statistic, which is a cascading of the transient and persistent distributions. In the proof, a novel approach of partitioning samples is needed to guarantee large probability of crossing the threshold within each block. Moreover, a decomposition of the sum of log-likelihood of the samples from f1 and f2, respectively, is also necessary before the application of the Law of Large Numbers.
The WADD is upper bounded diﬀerently in two regimes, depending on c′1, which determines the scaling behavior between d1 and b. If d1 is “large”, then the WD-CuSum algorithm stops within the transient phase with high probability, such that the asymptotic upper bound only depends on I1; if d1 is “small”, then the WD-CuSum algorithm stops within the persistent phase with high probability, such that the asymptotic upper bound depends on a mixture of I1 and I2. This is consistent with the insights gained from the asymptotic universal lower bound in Theorem 2.

5.3 Asymptotic Optimality

We are now ready to establish the asymptotic optimality of the proposed rules with respect to both Lorden’s and Pollak’s criteria under every possible post-change regime.

Theorem 4. Consider the QCD problem under transient dynamics described in Section 2 with L = 2. Suppose that d1, γ → ∞ according to (39).
(i) If b can be selected so that τ (b) ∈ Cγ and b ∼ log γ as γ → ∞, then

JLd1 (τ (b)) ∼ inf JLd1 (τ ) ∼ JPd1 (τ (b)) ∼ inf JPd1 (τ )

τ ∈Cγ

τ ∈Cγ

 

log

γ

,

if c1 > 1,

∼ I1

(54)

 log γ c1 + 1 − c1 , if c1 ≤ 1.

I1

I2

(ii) Suppose that b is selected so that E∞[τ (b)] ≥ γ and b ∼ log γ as γ → ∞. If ρ1 → 0 according to (44), then

JLd1 (τ (b)) ∼ inf JLd1 (τ ) ∼ JPd1 (τ (b)) ∼ inf JPd1 (τ )

τ ∈Cγ

τ ∈Cγ

 

log

γ

,

if c1 > 1,

∼ I1

(55)

 log γ c1 + 1 − c1 , if c1 ≤ 1.

I1

I2

Proof. The results follow from Proposition 1 and Theorems 1, 2, and 3.

A heuristic explanation for the dichotomy in Theorem 4 is as follows (see also Fig. 1). If we wish to detect a change from f0 to f1 with ARL γ, we have WADD∼ log γ/I1 (see, e.g., Theorem 1 in [19]). However, we only have d1 samples from f1 within the transient phase. If d1 ≥ log γ/I1,

13

WADD

log

Figure 1: A heuristic explanation for the dichotomy in Theorem 4.

i.e., c1 ≥ 1, then the problem is similar to one of testing the change from f0 to f1, and WADD increases when log γ increases with slope 1/I1, i.e.,

WADD ∼ log γ .

(56)

I1

If d1 < log γ/I1, i.e., c1 < 1, we then need further information from f2, and WADD increases when log γ increases with slope 1/I2. To obtain the overall slope, it then follows that

d1I1 + (WADD − d1)I2 ≈ log γ,

(57)

which implies that

WADD ≈ d1 + log γ − d1I1 I2

∼ log γ 1 − c1 + c1 .

(58)

I2

I1

5.4 Generalization to Arbitrary L
The asymptotic universal lower bound on the WADD can be extended to the case with arbitrary L.

14

Theorem 5. Consider the QCD problem under transient dynamics described in Section 2 with an

arbitrary L ≥ 2. Suppose that (39) holds. If h = min{1 ≤ j ≤ L :

j i=1

ci

≥

1},

then

as

γ

→

∞

inf JLd(τ ) ≥ inf JPd(τ )

τ ∈Cγ

τ ∈Cγ

≥ log γ h−1 ci + 1 −

h−1 i=1

ci

(1 − o(1)).

(59)

i=1 Ii Ih

Proof. The proof is a cumbersome but straightforward generalization of the proof of Theorem 2, and is omitted.

We further assume that there is a constant c′i ∈ [0, ∞] such that

di ∼ c′i Ib1 , (60)
for 1 ≤ i ≤ L − 1. If we choose b ∼ log γ, then ci = c′i, 1 ≤ i ≤ L − 1.
As in the case with L = 2, we need to design the weights {ρi, 1 ≤ i ≤ L − 1} in the WD-CuSum algorithm so that their eﬀect is asymptotically negligible. Similarly to (44), we choose ρi such that as b → ∞,

ρi → 0, and − log ρi → 0,

(61)

b

for i = 1, . . . , L − 1. We then obtain the following asymptotic upper bounds on the WADD of the D-CuSum and the WD-CuSum algorithms.

Theorem 6. Consider the QCD problem under transient dynamics described in Section 2 with an

arbitrary L. Suppose (60) and (61) hold. Let h = min{1 ≤ j ≤ L :

j i=1

c′i

≥

1},

then

as

γ

→

∞

JLd(τ (b)) = JPd(τ (b)) ≤ JLd(τ (b)) = JPd(τ (b))

≤ b h−1 c′i + 1 −

h−1 i=1

c′i

(1 + o(1)).

i=1 Ii

Ih

(62)

Proof. The proof is a cumbersome but straightforward generalization of the proof of Theorem 3, and is omitted.

We are then ready to establish the asymptotic optimality of the proposed algorithms with respect to both Lorden’s and Pollak’s criteria under every possible post-change regime for L ≥ 2.

Theorem 7. Consider the QCD problem under transient dynamics described in Section 2 with

L ≥ 2. Assume that (39) is satisﬁed, as γ, d → ∞. Let h = min{1 ≤ j ≤ L :

j i=1

ci

≥

1}.

(i) If ∃b ∼ log γ such that E∞[τ (b))] ≥ γ. Then, as γ → ∞,

JLd(τ (b)) ∼ inf JLd(τ ) ∼ JPd(τ (b)) ∼ inf JPd(τ )

τ ∈Cγ

τ ∈Cγ

∼ log γ h−1 ci + 1 −

h−1 i=1

ci

.

i=1 Ii Ih

(63)

15

(ii) Choose ρi for 1 ≤ i ≤ L − 1 such that (61) is satisﬁed and b ∼ log γ such that E∞[τ (b)] ≥ γ. Then, as γ → ∞,

JLd(τ (b)) ∼ inf JLd(τ ) ∼ JPd(τ (b)) ∼ inf JPd(τ )

τ ∈Cγ

τ ∈Cγ

∼ log γ h−1 ci + 1 −

h−1 i=1

ci

.

i=1 Ii Ih

Proof. The results follow from Proposition 1 and Theorems 1, 5 and 6.

A heuristic explanation for the polychotomy for the general case with arbitrary L in Theorem
7 is as follows (see also Fig. 2). If we wish to test a change from f0 to f1 with ARL γ, we have WADD∼ log γ/I1. If d1 < log γ/I1, we further need samples from f2. If d1I1 + d2I2 is still less than log γ, we then use samples from f3. Up to the h-th transient phase, we have collected suﬃcient number of samples such that

h

diIi > log γ.

(64)

i=1

To obtain the overall slope, it then follows that

h−1
diIi +
i=1

h−1
WADD − di
i=1

Ih ≈ log γ,

(65)

which implies that

WADD ≈ log γ −

h−1 i=1

diIi

h−1
+d

Ih

i

i=1

∼ log γ

1−

h−1 i=1

ci

h−1
+

ci

.

Ih i=1 Ii

(66)

6 Numerical Studies
In this section, we present some numerical results. We focus on the case with L = 2 to illustrate the performance of the algorithms and demonstrate our theoretical assertions. Together with the insights gained from the theoretical results, we also propose a heuristic approach to assign the weights for the WD-CuSum algorithm.
In Fig. 3, we plot the evolution paths of the WD-CuSum and D-CuSum algorithms. We choose f0 = N (0, 1), f1 = N (3, 1) and f2 = N (1, 1). We assume that the change happens at v1 = 20 and the persistent phase starts at v2 = 40. We choose ρ1 = 1/1000 for the WD-CuSum algorithm, which is small enough compared to I1. It can be seen that the values of both the WD-CuSum and DCuSum algorithms stay close to zero before the change-point v1 and grow after the change-point v1
16

WADD

Figure 2: A heuristic explanation for the results of the general case with arbitrary L in Theorem 7.
with diﬀerent drifts in the transient and persistent phases. Both algorithms are seen to be adaptive to the unknown transient duration d1. Furthermore, within the transient phase, the WD-CuSum and D-CuSum algorithms have close evolution paths. After v2, there is a gap of roughly | log ρ1| between the two evolution paths. These observations reﬂect the diﬀerence between the WD-CuSum and D-CuSum algorithms. For the D-CuSum algorithm, the drift is I1 within the transient phase, and I2 within the persistent phase. Recall that for the WD-CuSum algorithm, the drift within the transient phase is reduced from I1 by | log(1 − ρ1)|. Since ρ1 is chosen to be small compared to I1, the change of drift is not signiﬁcant in the ﬁgure. Furthermore, the value of the WD-CuSum statistic is reduced by | log ρ1| within the persistent phase. Therefore, the diﬀerence between the values of the D-CuSum and the WD-CuSum statistics is roughly | log ρ1| , as shown in the ﬁgure.
We next compare the performance of the WD-CuSum algorithms with diﬀerent ρ1 and the DCuSum algorithm. The goal is to check how diﬀerent choices of ρ1 aﬀect the performance of the WD-CuSum algorithm relative to the D-CuSum algorithm. We choose f0 = N (0, 1), f1 = N (0.3, 1) and f2 = N (−0.3, 1). For the WD-CuSum algorithm, we consider three diﬀerent choices of ρ1, i.e., ρ1 = 0.01, 0.02 and 0.04. We choose d1 = 40 and d1 = ∞, and plot the WADD versus the ARL in Fig. 4 and Fig. 5, respectively. Fig. 4 and Fig. 5 show that if the algorithms stop within the transient phase, i.e., WADD≤ d1, the WD-CuSum algorithm has a better performance than the D-CuSum algorithm. Fig. 4 also shows that if the algorithms stop within the persistent phase, i.e., WADD> d1, the D-CuSum and the WD-CuSum algorithms have similar performance.
In Fig. 4, when the algorithms stop within the persistent phase, i.e., WADD> d1, the WD-CuSum algorithm has a better performance if ρ1 is larger. This is due to the fact that the value of the WD-CuSum statistic is reduced by | log ρ1| in the persistent phase, which slows down the detection. With a larger ρ1, this eﬀect is mitigated, which results in better performance for the WD-CuSum algorithm in the persistent phase.
17

120

D-CuSum

100

WD-CuSum

80

|log 1|

Test Statistic

60 40 v1=20 20

v2=40

0

0

10

20

30

40

50

60

k

Figure 3: Evolution paths of the WD-CuSum and D-CuSum algorithms

In Fig. 4 and more clearly in Fig 5, when the algorithms stop within the transient phase, i.e.,
WADD≤ d1, the WD-CuSum algorithm has a better performance if ρ1 is smaller. This is due to the fact that the drift of the WD-CuSum algorithm is reduced by | log(1 − ρ1)| in the transient phase, which also slows down the detection. With a smaller ρ1, this eﬀect is reduced, which results in better performance for the WD-CuSum algorithm in the transient phase.

As can be observed in Fig. 4 and Fig. 5, the performance of the WD-CuSum algorithm depends on the choice of ρ1, but not monotonically. A smaller ρ1 yields a better performance for the WD-CuSum algorithm in the transient phase, and a larger ρ1 yields a better performance for the WD-CuSum algorithm in the persistent phase. However, since d1 is not known in advance, it is not clear in which regime the WD-CuSum algorithm will stop. Therefore, we propose a moderate way to choose ρ1 that balances the performance within the transient and persistent phases.
Since the lower bound on the ARL in Theorem 1 does not depend on ρ1, we choose b ∼ log γ. We choose ρ1 to be small but not too small such that the WD-CuSum algorithm is robust to the unknown d1, i.e., the WD-CuSum algorithm has a good performance in both the transient and persistent phases. Recall that the drift within the transient phase is reduced from I1 by | log(1−ρ1)|. From our asymptotic analysis, we would like to have

− log(1 − ρ1) → 0, as b → ∞.

(67)

I1

Therefore, we let

− log(1 − ρ1) ≤ δ1I1,

(68)

for some δ1 ∈ (0, 1), such that the drift is reduced by a small fraction of I1. Furthermore, within the persistent phase the value of the WD-CuSum statistic is reduced by | log ρ1|. From our asymptotic

18

120

WD-CuSum with 1=0.01

100

WD-CuSum with 1=0.02

WD-CuSum with 1=0.04

D-CuSum

80

WADD

60
D1=40 40

20

0

100

101

102

103

104

ARL

Figure 4: WADD versus ARL for the WD-CuSum and D-CuSum algorithms with d1 = 40.

analysis, we would like to have

− log ρ1 → 0, as b → ∞.

(69)

b

Therefore, we let

− log ρ1 ≤ δ2b,

(70)

for some δ2 ∈ (0, 1), such that | log ρ1| is a small perturbation compared to b. Therefore, ρ1 is chosen such that

e−δ2b < ρ1 < 1 − e−δ1I1 .

(71)

For example, we let δ1 = δ2 = 0.3. Assume that I1 = 0.045 (as in Fig. 4 and Fig. 5) and the required ARL is 107. Then we can choose b = log(107) and ρ1 ∈ [0.008, 0.134].

7 Conclusions
In this paper, we studied a variant of the QCD problem that arises in a number of engineering applications. Our problem formulation captures the scenarios with transient dynamics after a change. We studied two algorithms for this formulation, the D-CuSum and the WD-CuSum algorithms. We established bounds on the ARL to false alarm for these algorithms that can be used to set the thresholds of these algorithms in application settings. We also established the asymptotic optimality of the D-CuSum and the WD-CuSum algorithms up to a ﬁrst-order asymptotic
19

120

WD-CuSum with 1=0.01

100

WD-CuSum with 1=0.02

WD-CuSum with 1=0.04

DCuSum

80

WADD

60

40

20

0

100

101

102

103

104

ARL

Figure 5: WADD versus ARL for the WD-CuSum and D-CuSum algorithms with d1 = ∞.

approximation. Both algorithms admit recursions that facilitate implementation and are adaptive to unknown transient dynamics.
We have shown that the asymptotic optimal performance follows a polychotomy as illustrated in Fig. 2. In particular, for the case with only one transient phase, the asymptotic optimal performance follows a dichotomy: if the duration of the transient phase is “large”, then the WADD only depends on the distribution associated with the transient phase; otherwise, the WADD depends on the distributions associated with both the transient and the persistent phases.
We note that in this paper, our asymptotic analysis is up to a ﬁrst-order approximation. When the threshold or the transient durations are small, such an approximation may not be precise enough. It is therefore of interest to develop more accurate approximations for the delay and false alarm rate of the algorithms.
A possible extension of the problem formulation studied in this paper is a generalization to the case where the observations within each transient phase are not i.i.d., as in the observation model studied by Lai [19]. Another extension is the scenario in which prior statistical knowledge of the change-point and durations of the transients is available. In this case, such prior knowledge should be incorporated into the design of algorithms to improve performance, while taking into account computational eﬃciency. We also note that the generalization to the case in which the distribution within each transient phase is composite is also of interest in practice, an example of which is the sequentially detection of a propagating event with an unknown propagation pattern in sensor networks [24].
Appendix

20

A Proof of Proposition 1

Under (34), (W [k])+

is regenerative. Deﬁne the following regenerative times:

k≥1

σ1 = inf k : (W [k])+ = 0 , (inf ∅ = ∞)

(72)

and

σn+1 = inf k > σn : (W [k])+ = 0 ,

(73)

for n ≥ 1. Let

N = inf n ≥ 0 : σn ≤ ∞ and (W [k])+ ≥ b

for some σn < k ≤ σn+1

(74)

denote the index of the cycle in which (W [k])+ crosses b. Then

∞

E∞[τˆ(b)] ≥ E∞[N ] = P∞(N ≥ n).

(75)

n=0

For any m ≥ 1,

P∞(τ (b) < Y )

= P∞(τ (b) < Y, Y ≤ m) + P∞(τ (b) < Y, Y > m)

≤ P∞(τ (b) < m) + P∞(Y > m)

≤ mL+1e−b + e−αm,

(76)

where the last inequality is due to condition (34) and the following fact:

P∞(τ (b) < m)

= P∞ max W [k] > b
1≤k<m

= P∞ max max

max

Γ(k, v1, d) > eb

1≤k<m 1≤v1≤k v1≤v2≤···≤vL≤k+1

(a)

≤

P∞ Γ(k, v1, d) > eb

1≤k<m 1≤v1≤k v1≤v2≤···≤vL≤k+1

(b)
≤ mL+1e−b,

(77)

and (a) is due to the Boole’s inequality [25] and (b) is due to Markov’s inequality [26] and the fact that E∞[Γ(k, v1, d)] = 1. By choosing m = b/α, it follows that
P∞(τ (b) < Y ) ≤ e−b αb L+1 + 1 . (78)

21

Next,

P∞(N ≥ n)

= P∞((W [k])+ < b, ∀k ≤ σn)

= P∞((W [k])+ < b, ∀σm−1 ≤ k ≤ σm, ∀1 ≤ m ≤ n)

n
= P∞((W [k])+ < b, ∀σm−1 ≤ k ≤ σm)

m=1

≥ 1 − e−b (b/α)L+1 + 1

n
,

(79)

where the last equality is due to the independence among the cycles [27, Chapter 6.4]. combining (75) and (79), it follows that

∞

n

E∞[τˆ(b)] ≥

1 − e−b (b/α)L+1 + 1

n=0

eb = 1 + (b/α)L+1 ,

Hence, (80)

where the last step is due to the fact that for large b, e−b (b/α)L+1 + 1 < 1. This concludes the proof.

B Proof of Theorem 1

For every k ∈ N we have

W [k] ≤ W ′[k]





= max log 

Γ(k, v1, d)g(d)

1≤v1 ≤k



d∈NL−1



k

≤ log 

Γ(k, v1, d)g(d)

v1=1 d∈NL−1

≡ log R[k],

(81)

where W ′[k] is as in (20), and the ﬁrst inequality follows by the construction of the detection
statistics. Note that R[k] is a mixture Shiryaev-Roberts statistics, and therefore {R[k] − k}k≥1 is a martingale under P∞ [28]. Thus, for every b > 0 and k ∈ N we have by Doob’s submartingale inequality [25] that

P∞(τ (b) ≤ k)

= P∞ max W [s] ≥ b
1≤s≤k

≤ P∞ max R[s] ≥ eb
1≤s≤k

≤ ke−b,

(82)

22

which implies that

∞

E∞[τ (b)] = P∞(τ (b) > k)

k=0 ∞
≥ (1 − ke−b)+

k=0

eb
=

1 − ke−b

k=0

≥ eb . (83) 2

C A Suﬃcient Condition for (34)

Let

Φ(Xj) = log max1≤i≤L fi(Xj ) .

(84)

f0(Xj )

For any (v1, d, k), it follows from (10) and (84) that

k

log Γ(k, v1, d) ≤ Φ(Xj).

(85)

j =v1

This further implies that

k

W [k] ≤ max

Φ(Xj ).

1≤v1≤k j=v1

(86)

Let Y ′ = inf k ≥ 1 : max1≤v1≤k kj=v1 Φ(Xj) ≤ 0 . Then by (86), Y ′ ≥ Y . It then follows that

P∞(Y > m)

≤ P∞(Y ′ > m)





k

= P∞  max

Φ(Xj) > 0, ∀1 ≤ k ≤ m

1≤v1≤k j=v1





k

(=a) P∞  Φ(Xj) > 0, ∀1 ≤ k ≤ m

 j=1 
m
≤ P∞  Φ(Xj) > 0

j=1 
m

= P∞  Φ(Xj) − E∞[Φ(Xj )] > −mE∞[Φ(Xj)] ,

(87)

j=1

23

where (a) is by applying the following argument recursively: P∞ Φ(X1) > 0 Φ(X2) > 0 Φ(X1) + Φ(X2) > 0

= P∞ Φ(X1) > 0 Φ(X2) > 0

Φ(X1) > 0 Φ(X1) + Φ(X2) > 0

= P∞ Φ(X1) > 0 Φ(X1) + Φ(X2) > 0 .

(88)

If Ef0 [Φ(Xj)] < 0, and where

−α = inf θ(t) + tEf0[Φ(Xj)] < 0,
t>0

θ(t) = log Ef0 exp t Φ(Xj) − Ef0 [Φ(Xj)] ,

(89)

then by the Chernoﬀ bound [29], (34) holds.

D A Useful Lemma

We ﬁrst recall the following useful corollary of the Strong Law of Large Numbers, which will be used extensively.

Lemma 1. [30, Lemma A.1] Suppose random variables Y1, Y2, . . . , Yk are i.i.d. on (Ω, F, P) with

E[Yi] = µ > 0, and denote Sk =

k i=1

Yi,

then

for

any

ǫ

>

0,

as

n

→

∞,

P max1≤k≤n Sk − µ > ǫ → 0.

(90)

n

E Proof of Theorem 2

Recall from (39) that d1 ∼ c1 Ilo1g γ for some c1 ∈ [0, ∞]. We deﬁne Kγ as follows:

 

log

γ

,

c1 ∈ [1, ∞];

Kγ = I1

(91)

 1 − c1 + c1 log γ, c1 ∈ [0, 1).

I2

I1

Fix any small enough ǫ > 0. By Markov’s inequality, we have

Edv11 [τ − v1|τ ≥ v1] ≥ Pdv11 τ − v1 ≥ (1 − ǫ)Kγ |τ ≥ v1 (1 − ǫ)Kγ . (92)

24

It then suﬃces to show sup Pdv11(τ − v1 < (1 − ǫ)Kγ |τ ≥ v1) → 0 as γ → ∞. (93)
τ ∈Cγ
We will consider two cases depending on c1 ≥ 1 or c1 < 1. Case 1 : Consider c1 ≥ 1. Then (1 − ǫ)Kγ < d1 for large γ. We ﬁrst have for every a > 0, Pdv11 (v1 ≤ τ < v1 + (1 − ǫ)Kγ|τ ≥ v1) = Pdv11 v1 ≤ τ < v1 + (1 − ǫ)Kγ, log Λ1[v1, τ ] ≥ a τ ≥ v1

+ Pdv11 v1 ≤ τ < v1 + (1 − ǫ)Kγ, log Λ1[v1, τ ] < a τ ≥ v1

≤ Pdv1

max log Λ1[v1, v1 + j] ≥ a τ ≥ v1

1 0≤j<(1−ǫ)Kγ

+ Pdv11 v1 ≤ τ < v1 + (1 − ǫ)Kγ, Λ1[v1, τ ] < ea τ ≥ v1

(=a) Pvd1 1

max log Λ1[v1, v1 + j] ≥ a
0≤j<(1−ǫ)Kγ

+ Pdv11 v1 ≤ τ < v1 + (1 − ǫ)Kγ, Λ1[v1, τ ] < ea τ ≥ v1 ,

(94)

where (a) is due to the fact that log Λ1[v1, v1 + j] is independent of X1, . . . , Xv1−1, ∀0 ≤ j < (1 − ǫ)Kγ, and the fact that the event {τ ≥ v1} only depends on the random variables X1, X2, . . . , Xv1−1.

By changing the measure Pdv11 to P∞ [5, Proof of Theorem 7.1.3], it follows that

Pdv11 (v1 ≤ τ < v1 + (1 − ǫ)Kγ , Λ1[v1, τ ] ≤ ea)

≤ eaEdv11

1 1{v1≤τ <v1+(1−ǫ)Kγ ,Λ1[v1,τ ]≤ea} Λ1[v1, τ ]

≤ eaEdv11

1 1{v1≤τ <v1+(1−ǫ)Kγ } Λ1[v1, τ ]

= eaE∞ 1{v1≤τ <v1+(1−ǫ)Kγ}

= eaP∞(v1 ≤ τ < v1 + (1 − ǫ)Kγ ).

(95)

The event {τ ≥ v1} only depends on the random variables X1, X2, . . . , Xv1−1 that follow the same distribution f0 under both P∞ and Pdv11. This implies that
Pdv11(τ ≥ v1) = P∞(τ ≥ v1). (96)

It then follows from (95) that

Pdv11 v1 ≤ τ < v1 + (1 − ǫ)Kγ , Λ1[v1, τ ] ≤ ea τ ≥ v1

≤ eaP∞(v1 ≤ τ < v1 + (1 − ǫ)Kγ |τ ≥ v1).

(97)

Combining (94) and (97) yields that

Pdv11 (v1 ≤ τ < v1 + (1 − ǫ)Kγ |τ ≥ v1) ≤ eaP∞(v1 ≤ τ < v1 + (1 − ǫ)Kγ |τ ≥ v1)

+

Pd1
v

max log Λ1[v1, v1 + j] ≥ a .

1 0≤j<(1−ǫ)Kγ

(98)

25

Since E∞[τ ] ≥ γ, then for each m < γ, there exists some v1 ≥ 1, such that

P∞(τ ≥ v1) > 0 and P∞(τ < v1 + m|τ ≥ v1) ≤ m ,

(99)

γ

which can be shown by contradiction as in [19, Theorem 1]. Hence, for m = (1 − ǫ)Kγ , there exists v1 such that

P∞(v1 ≤ τ < v1 + (1 − ǫ)Kγ |τ ≥ v1) ≤ (1 − ǫ)Kγ . γ

(100)

Set a = (1 − ǫ2) log γ, then

eaP∞(v1 ≤ τ < v1 + (1 − ǫ)Kγ |τ ≥ v1) ≤ γ1−ǫ2 (1 − ǫ) log γ → 0, as γ → ∞.
γI1

(101)

We next show that the second term in (98) converges to zero as γ → ∞. Because c1 ≥ 1, for large γ, d1 > (1 − ǫ)Kγ, such that Xj, for v1 ≤ j < v1 + (1 − ǫ)Kγ, are i.i.d. generated by f1. Therefore, Z1(Xj), for v1 ≤ j < v1 + (1 − ǫ)Kγ, are also i.i.d. with expectation I1. Rewrite a = (1 − ǫ2) log γ = (1 − ǫ)KγI1(1 + ǫ), then

Pd1
v

max log Λ1[v1, v1 + k] ≥ a

1 0≤k<(1−ǫ)Kγ





v1+k

=

Pd1
v



max

1 0≤k<(1−ǫ)Kγ

Z1(Xj) ≥ a

j =v1

 max

v1+k Z1(Xj )



= Pd1  0≤k<(1−ǫ)Kγ j=v1

− I1 ≥ I1ǫ

v1

(1 − ǫ)Kγ

→ 0, as γ → ∞,

(102)

where the last step is by Lemma 1 and the fact that Z1(Xj), for v1 ≤ j < v1 + (1 − ǫ)Kγ, are i.i.d. with expectation I1.

Combining (98), (101) and (102) yields

Pdv11 (τ − v1 < (1 − ǫ)Kγ |τ ≥ v1) → 0, as γ → ∞.

(103)

Case 2 : Consider c1 < 1. Note that for any c1 < 1, we have a small enough ǫ such that

(1 − ǫ)

c1 + 1 − c1

I1

I2

> c1 . I1

It then follows that (1 − ǫ)Kγ − d1 → ∞ as γ → ∞. By a change-of-measure argument similar to case 1, we obtain for any a′ > 0,

Pdv11 (τ < v1 + (1 − ǫ)Kγ |τ ≥ v1) ≤ ea′ P∞(τ < v1 + (1 − ǫ)Kγ|τ ≥ v1)

+

Pd1
v

max log Γ(v1 + j, v1, d1) > a′ .

1 0≤j<(1−ǫ)Kγ

(104) (105)

26

Set

a′ = (1 − ǫ1) log γ,

(106)

where ǫ1 = (1−2c1)ǫ , and let m = ( 1−I2c1 + cI11 )(1 − ǫ) log γ in (99). Then, there exists v1, such that as γ→∞

ea′ P∞(T < v1 + (1 − ǫ)Kγ |T ≥ v1) ≤ (1 −γǫǫ1)Kγ → 0.

(107)

We next show that the second term in (105) converges to zero as γ → ∞. It can be shown that

max log Γ(v1 + j, v1, d1) 0≤j<(1−ǫ)Kγ 
v1 +min{d1 −1,j }


v1 +j

= max 
0≤j<(1−ǫ)Kγ

k=v1

Z1(Xk) +

Z2 (Xk )

k=v1 +d1

v1 +min{d1 −1,j }

≤ max
0≤j<(1−ǫ)Kγ

k=v1

Z1 (Xk )

v1 +j

+ max

Z2 (Xk )

0≤j<(1−ǫ)Kγ k=v1+d1

v1 +j

v1 +j

(=a) max

Z1(Xk) +

max

Z2 (Xk )

0≤j ≤d1 −1 k=v1

d1−1≤j<(1−ǫ)Kγ k=v1+d1

v1+j

= max

Z1 (Xk )

0≤j ≤d1 −1

k=v1

v1 +d1 +j −1

+

max

Z2 (Xk ),

0≤j<(1−ǫ)Kγ −d1+1 k=v1+d1

(108)

where (a) is due to the fact that if j > d1 − 1, min{d1 − 1, j} = d1 − 1, and the fact that if j < d1, kv1=+vj1+d1 Z2(Xk) = 0. By deﬁnition of a′ in (106),

a′ = (1 − ǫ1) log γ ≥ E1 + E2,

(109)

where

E1 = (1 + ǫ1) c1 log γ

∼(1 + ǫ1)d1I1,

E2 = (1 − ǫ)

c1 + 1 − c1

I1

I2

− c1 I1

∼(1 + ǫ1) (1 − ǫ)Kγ − d1 I2.

I2 log γ (1 + ǫ1)

(110)

27

Then,

Pd1
v

max log Γ(v1 + j, v1, d1) > a′

1 0≤j<(1−ǫ)Kγ

≤ Pdv11

v1 +j −1

max

Z1 (Xk )

1≤j ≤d1

k=v1

v1 +d1 +j −1

+

max

Z2(Xk) > a′

0≤j<(1−ǫ)Kγ −d1+1 k=v1+d1

≤ Pdv11

v1 +j −1

max

Z1 (Xk )

1≤j ≤d1

k=v1

v1 +d1 +j −1

+

max

Z2(Xk) > E1 + E2

0≤j<(1−ǫ)Kγ −d1+1 k=v1+d1

(a)
≤ Pdv11

v1 +j −1

max

Z1(Xk) > E1

1≤j ≤d1

k=v1

+ Pdv11

v1 +d1 +j −1

max

Z2(Xk) > E2

0≤j<(1−ǫ)Kγ −d1+1 k=v1+d1

→(b) 0, as γ → ∞,

(111)

where (a) is due to the fact that P(Y1 + Y2 > y1 + y2) ≤ P(Y1 > y1) + P(Y2 > y2) for any random variables Y1, Y2 and constants y1, y2, and (b) is due to Lemma 1. This completes the proof.

  log w[k1, k2, v2] =  ρ1  log

jm=ink{1v2−1,k2} f1(Xj )(1 − ρ1) ρ11 {k2≥v2} kj=2 k1 f0(Xj )
kj=2 k1 f2(Xj ) , if k1 > v2, kj=2 k1 f0(Xj )

k2 j=v

f2(Xj )

2

, if k1 ≤ v2,

(112)

F Proof of Theorem 3

We ﬁrst show the asymptotic upper bound on the WADD for the WD-CuSum algorithm. Then the results for the D-CuSum algorithm naturally follows from (53).

For notational convenience, deﬁne w[k1, k2, v2] as in (112), i.e., w[k1, k2, v2] is the logarithm of the weighted likelihood ratio of the samples Xk1, . . . , Xk2 with the change-point v1 = 1 and the starting point of the persistent phase being v2.

We further note that the test statistic in (26) is equivalent to

(W [k])+ = max w[k1, k, v2].
1≤k1 ≤v2 ≤k+1

(113)

28

Due to the Markov property and the recursive structure of {Ω(1)[k], Ω(2)[k]}k≥1, it is clear that the WADD is achieved when v1 = 1, i.e.,

JLd1 (τ (b)) = JPd1 (τ (b)) = Ed11 [τ (b)].

(114)

It

then

suﬃces

to

upper

bound

E

d1 1

[τ

(b

)].

When

ρ1

→0

and

log ρ1 b

→0

as

b→∞

and

by

the

fact

that d1 ∼ c′1b/I1, we have

d1 ∼ c′1 I1 + logb(1 − ρ1) .

(115)

Depending

on

the

value

of

c′1,

we

bound

E

d1 1

[τ

(b

)]

in

the

following

two

cases.

Case 1: Consider c′1 > 1. Our goal is to show that as b → ∞,

Ed1[τ (b)] ≤ b (1 + o(1)).

1

I1

In

the

following,

we

choose

ǫ

>

0

such

that

1

<

1+ǫ 1−ǫ

≤

c′1,

i.e.,

c′1(1−ǫ) 1+ǫ

≥

1,

and

denote

b(1 + ǫ) nb = I1 + log(1 − ρ1) , cǫ = c′1 (11 +− ǫǫ) .

(116)
(117) (118)

We ﬁrst have

Ed1 τ (b) 1 nb

∞

=

Pd11

0

τ (b) > x dx nb

∞

≤ Pd11 (τ (b) > nbi)

i=0

cǫ

∞

= 1 + Pd11 (τ (b) > nbi) +

Pd11 (τ (b) > nbi) .

i=1

i=cǫ+1

(119)

It then suﬃces to bound Pd11 (τ (b) > nbi) for the two regimes, i ≤ cǫ and i > cǫ. We note that the event {τ (b) > nbi} only depends on the samples X1, . . . , Xnbi.

29

For 1 ≤ i ≤ cǫ, X1, . . . , Xnbi are i.i.d. generated from f1 under Pd11. Therefore, Pd11 (τ (b) > nbi)

= Pd11 max (W [k])+ ≤ b
1≤k≤nbi

= Pd11 max

max w[k1, k, v2] ≤ b

1≤k≤nbi 1≤k1≤v2≤k+1

≤ Pd11 (w[(u − 1)nb + 1, unb, d1 + 1] ≤ b, ∀1 ≤ u ≤ i)





unb

= Pd11 

Z1(Xj) + log(1 − ρ1) ≤ b, ∀1 ≤ u ≤ i

j =(u−1)nb +1

(=a) i Pd1  1 1 nb

unb
Z1(Xj) + log(1 − ρ1)

u=1

j=(u−1)nb+1

 ≤ b
nb

(b)
≤ δi,

(120)

where δ can be arbitrarily small for large b, (a) is due to the fact that {X1+(u−1)nb , . . . , Xunb } are independent from {X1+(u′−1)nb , . . . , Xu′nb } for any u = u′, and (b) is by the Weak Law of Large Numbers.

For i > cǫ, nbi > d1 for large b, then the samples X1, . . . , Xnbi are generated from diﬀerent distributions, either f1 or f2. We then deﬁne

t=

I1

+ 1.

min{I1, I2}

(121)

We note that t is a constant that only depends on I1 and I2.

Figure 6: Illustration of partitioning of samples into blocks. Here cǫ + t ≤ i < cǫ + 2t. We partition the samples up to inb into cǫ blocks with size nb, and one block with size tnb. Then, the probability that the sum of log-likelihood of the samples within each block is less than b is asymptotically
small by the Weak Law of Large Numbers. The choice of block size tnb for the samples after cǫnb is due to the fact that the samples are generated from f1 and f2, and the need to guarantee an asymptotically small probability that the sum of log-likelihood of the samples within this block is
less than b.

Consider any i such that cǫ + (ℓ − 1)t ≤ i ≤ cǫ + ℓt − 1, for any ℓ ≥ 1, then

Pd11 (τ (b) > nbi) = Pd11 max (W [k])+ ≤ b
1≤k≤nbi
≤ Pd11 (A ∩ B) = Pd11 (A) Pd11 (B) ,

(122)

30

where

A = {w [1 + (u − 1)nb, unb, d1 + 1] ≤ b, ∀1 ≤ u ≤ cǫ} , B = w [(cǫ + (u − 1)t) nb + 1, (cǫ + ut) nb, d1 + 1] ≤ b,
∀1 ≤ u ≤ ℓ − 1 ,

(123) (124)

and the last equality is due to the fact that the events A and B are independent. See Fig. 6 for an illustration of partitioning the samples up to nbi into blocks with diﬀerent sizes.

Similarly to (120), we obtain that

Pd11(A) ≤ δcǫ .

(125)

Furthermore, by the Weak Law of Large Numbers, ∀1 ≤ u ≤ ℓ − 1, we have that as b → ∞,

w [(cǫ + (u − 1)t) nb + 1, (cǫ + ut) nb, d1 + 1] −p→. I2. tnb

(126)

As b → ∞,

w [(cǫ + (u − 1)t) nb + 1, (cǫ + ut) nb, d1 + 1]
b −p→. I2t(1 + ǫ) ≥ 1 + ǫ.
I1

(127)

Thus,

P1d1 (w [(cǫ + (u − 1)t) nb + 1, (cǫ + ut) nb, d1 + 1] ≤ b) ≤ δ,

(128)

where δ can be arbitrarily small for large b. Then, it follows from similar arguments of independence that

Pd11(B) ≤ δℓ−1.

(129)

Combining (125) and (129) further implies that Pd11 (τ (b) > nbi) ≤ δcǫ+ℓ−1.

(130)

Hence, by (119), (120) and (130), we have

Ed1

τ (b)

cǫ

∞

≤ δi + tδcǫ+ℓ−1

1 nb

i=0

ℓ=1

= 1 + tδcǫ + (t − 1)δcǫ+1 1

1−δ

1−δ

=∆ 1 + δ′,

(131)

where δ′ can be arbitrarily small for large b due to the facts that cǫ ≥ 1 and δ can be arbitrarily small for large b. Therefore, as b → ∞,

Ed1[τ (b)] ≤ b (1 + o(1)).

1

I1

(132)

31

Case 2: If c′1 ≤ 1, our goal is to show that as b → ∞,

Ed11[τ (b)] ≤ b

c′1 + 1 − c′1

I1

I2

(1 + o(1)).

Let

n′b = d1 + b − log ρ1 − d1(II21 + log(1 − ρ1))

∼ b c′1 + 1 − c′1 (1 + ǫ).

I1

I2

(1 + ǫ),

Then, we have

lim n′b = 1 + 1 − 1 I1 (1 + ǫ) > 1,

b→∞ d1

c′1

I2

which implies that for large b, n′b > d1, and n′b − d1 → ∞ as b → ∞.

To

bound

E

d1 1

[τ

(b

)],

we

ﬁrst

obtain

Ed1

τ (b)

∞
≤ Pd1 τ (b) > n′ i

1 n′b

1 i=0

b

∞

= 1 + Pd11 τ (b) > n′bi .

i=1

(133) (134) (135)
(136)

32

If i = 1,

Pd11 τ (b) > n′b

= Pd11 ≤ Pd11 = Pd11

max (W [k])+ ≤ b
1≤k≤n′b

w 1, n′b, d1 + 1 ≤ b

d1
Z1(Xj) + log(1 − ρ1)
j=1

n′b

+

Z2(Xj ) ≤ b

j =d1 +1

d1

= Pd11

Z1(Xj) + log(1 − ρ1)

j=1

+ log ρ1

n′b

+

Z2(Xj )

j =d1 +1

≤ d1(I1 + log(1 − ρ1)) + (n′b − d1)I2 − ǫC

(a)

d1

≤ Pd11

Z1(Xj) + log(1 − ρ1)

j=1

≤ d1(I1 + log(1 − ρ1)) − ǫC

2





n′b
+ Pd1 

Z (X ) ≤ (n′ − d )I − ǫC 

1

2j

b 12 2

j =d1 +1

(b)
≤ δ,

(137)

where C = d1I2 + b − d1(I1 + log(1 − ρ1)) − log ρ1, δ can be arbitrarily small for large b, (a) is due to the fact that for any random variables X, Y and constants x, y, P(X + Y ≤ x + y) ≤ P(X ≤ x) + P(Y ≤ y), and (b) is due to the Weak Law of Large Numbers.

Deﬁne

 t′ = 



1

′

′

 + 1,

Ic11 + 1−I2c1 min{I1, I2} 

(138)

which only depends on c′1, I1 and I2. Following arguments similar to those in (122)-(129), we can show that if (ℓ − 1)t + 1 ≤ i ≤ ℓt, for any ℓ ≥ 1,

Pd11 τ (b) > n′bi ≤ t′δℓ.

(139)

33

Combining (137) with (139) implies that

Ed1

τ (b)

∞
≤ 1 + δ + t′δj−1

1 n′b

j=2

= 1 + t′δ + (t′ − 1) δ2

1−δ

1−δ

=∆ 1 + δ′′,

where δ′′ can be arbitrarily small for large b. Therefore, as b → ∞

Ed11[τ (b)] ≤ b

c′1 + 1 − c′1

I1

I2

(1 + o(1)).

(140) (141)

References
[1] S. Zou, G. Fellouris, and V. V. Veeravalli, “Asymptotic optimality of D-CuSum for quickest change detection under transient dynamics,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT), Aachen, Germany, 2017, IEEE, pp. 156–160.
[2] V. V. Veeravalli and T. Banerjee, “Quickest change detection,” Academic press library in signal processing: Array and statistical signal processing, vol. 3, pp. 209–256, 2013.
[3] H. V. Poor and O. Hadjiliadis, Quickest Detection, Cambridge University Press, 2009.
[4] M. Basseville and I. V. Nikiforov, Detection of abrupt changes: Theory and application, vol. 104, Prentice Hall Englewood Cliﬀs, 1993.
[5] A. Tartakovsky, I. Nikiforov, and M. Basseville, Sequential analysis: Hypothesis testing and changepoint detection, CRC Press, 2014.
[6] S. Aminikhanghahi and D. J. Cook, “A survey of methods for time series change point detection,” Knowledge and Information Systems, vol. 51, no. 2, pp. 339–367, 2017.
[7] C. Truong, L. Oudre, and N. Vayatis, “A review of change point detection methods,” arXiv preprint: 1801.00718, 2018.
[8] G. Rovatsos, J. Jiang, A. D. Dom´ınguez-Garc´ıa, and V. V. Veeravalli, “Statistical power system line outage detection under transient dynamics,” IEEE Trans. Signal Proc., vol. 65, no. 11, pp. 2787–2797, June 2017.
[9] B. K. Gu´epi´e, L. Fillatre, and I. Nikiforov, “Sequential detection of transient changes,” Sequential Analysis, vol. 31, no. 4, pp. 528–547, 2012.
[10] E. Ebrahimzadeh and A. Tchamkerten, “Sequential detection of transient changes in stochastic systems under a sampling constraint,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT). IEEE, 2015, pp. 156–160.
[11] S. van der Pas and P. Gru¨nwald, “Almost the best of three worlds: Risk, consistency and optional stopping for the switch criterion in nested model selection,” Statistica Sinica, vol. 28, no. 1, pp. 229–255, Jan. 2018.
34

[12] R. A. Davis and C. Y. Yau, “Consistency of minimum description length model selection for piecewise stationary time series models,” Electronic Journal of Statistics, vol. 7, pp. 381–411, 2013.
[13] G. V. Moustakides and V. V. Veeravalli, “Sequentially detecting transitory changes,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT). IEEE, 2016, pp. 26–30.
[14] E. S. Page, “Continuous inspection schemes,” Biometrika, vol. 41, pp. 100–115, 1954.
[15] G. Lorden, “Procedures for reacting to a change in distribution,” The Annals of Mathematical Statistics, pp. 1897–1908, 1971.
[16] G. Rovatsos, S. Zou, and V. V. Veeravalli, “Quickest change detection under transient dynamics,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), New Orleans, USA, Mar. 2017, IEEE, pp. 4785–4789.
[17] A. N. Shiryaev, “On optimum methods in quickest detection problems,” Theory of Prob. and App., vol. 8, no. 1, pp. 22–46, 1963.
[18] A. N. Shiryaev, Optimal stopping rules, New York: Springer-Verlag, 1978.
[19] T. L. Lai, “Information bounds and quick detection of parameter changes in stochastic systems,” IEEE Trans. Inform. Theory, vol. 44, no. 7, pp. 2917–2929, 1998.
[20] T. L. Lai, “Sequential changepoint detection in quality control and dynamical systems,” Journal of the Royal Statistical Society. Series B (Methodological), pp. 613–658, 1995.
[21] D. Siegmund and E. S. Venkatraman, “Using the generalized likelihood ratio statistic for sequential detection of a change-point,” The Annals of Statistics, pp. 255–271, 1995.
[22] M. Pollak, “Optimal detection of a change in distribution,” The Annals of Statistics, pp. 206–227, 1985.
[23] Y. S. Chow, H. Robbins, and D. Siegmund, Great Expectations: The Theory of Optimal Stopping, Houghton-Niﬄin, 1971.
[24] S. Zou and V. V. Veeravalli, “Quickest detection of dynamic events in sensor networks,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), Alberta, Canada, April 2018, IEEE.
[25] R. Durrett, Probability: Theory and Examples, Cambridge University Press, 2010.
[26] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd Edition, John Wiley & Sons, 2006.
[27] S. Asmussen, Applied probability and queues, vol. 51, Springer Science & Business Media, 2008.
[28] M. Pollak, “Average run lengths of an optimal method of detecting a change in distribution,” The Annals of Statistics, pp. 749–779, 1987.
[29] M. Raginsky and I. Sason, Concentration of measure inequalities in information theory, communications, and coding, vol. 10, Now Publishers, 2013.
[30] G. Fellouris and A. Tartakovsky, “Multichannel sequential detection—Part I: Non-iid data,” IEEE Trans. Inform. Theory, vol. 63, no. 7, pp. 4551–4571, 2017.
35

