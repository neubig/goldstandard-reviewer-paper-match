RUN-AND-BACK STITCH SEARCH: NOVEL BLOCK SYNCHRONOUS DECODING FOR STREAMING ENCODER-DECODER ASR
Emiru Tsunoo⋆ Chaitanya Narisetty† Michael Hentschel⋆ Yosuke Kashiwagi⋆ Shinji Watanabe†
⋆ Sony Group Corporation, Japan † Carnegie Mellon University, USA

arXiv:2201.10190v1 [eess.AS] 25 Jan 2022

ABSTRACT
A streaming style inference of encoder–decoder automatic speech recognition (ASR) systems is important for reducing latency, which is essential for interactive use cases. To this end, we propose a novel blockwise synchronous decoding algorithm with a hybrid approach that combines endpoint prediction and endpoint post-determination. In the endpoint prediction, we compute the expectation of the number of tokens that are yet to be emitted in the encoder features of the current blocks using the CTC posterior. Based on the expectation value, the decoder predicts the endpoint to realize continuous block synchronization, as a running stitch. Meanwhile, endpoint post-determination probabilistically detects backward jump of the source–target attention, which is caused by the misprediction of endpoints. Then it resumes decoding by discarding those hypotheses, as back stitch. We combine these methods into a hybrid approach, namely run-and-back stitch search, which reduces the computational cost and latency. Evaluations of various ASR tasks show the efﬁciency of our proposed decoding algorithm, which achieves a latency reduction, for instance in the Librispeech test set from 1487 ms to 821 ms at the 90th percentile, while maintaining a high recognition accuracy.
Index Terms— Streaming automatic speech recognition (ASR), encoder–decoder, end-to-end, Transformer, CTC
1. INTRODUCTION
In recent years, end-to-end automatic speech recognition (ASR) has garnered signiﬁcant attention. For interactive use cases in particular, streaming style inference is essential; thus, several approaches have been discovered for both the encoder–decoder (Enc–Dec) [1, 2, 3] and transducer models [4, 5]. Blockwise processing can be easily introduced to the encoders of both models [6, 7, 8, 9, 10]. Although transducers are efﬁcient for streaming ASR owing to framesynchronous decoding, they are less accurate than Enc–Dec [11] and Enc–Dec can be used additionally to achieve higher performance [12]. However, during blockwise Enc–Dec inference, it is still challenging for the decoder to know when to stop decoding, that is, endpoints with limited encoder features in the currently given blocks.
Several studies on streaming Enc–Dec ASR have introduced additional training or modules to predict endpoints. By predicting endpoints, the decoder realizes continuous block synchronization with the encoder, which we refer to as the running stitch approach. Monotonic chunkwise attention (MoChA) [13] is a popular approach for achieving online processing [6, 14, 15]. However, it makes training complicated and sometimes degrades accuracy [14, 15]. A triggered attention mechanism [7] and its modiﬁcation with a scout network [16] are further methods for predicting the endpoints, but they also require additional modules in the network and training criteria.

As an alternative approach, block synchronization with the encoder can also be established by examining the tendency of hypotheses that exceed the endpoint [17, 18]. If an exceeded hypothesis is detected, the decoder turns back to the previous hypotheses and restarts the beam search with the subsequent encoder features, which we refer to as a back stitch approach. Tian et. al. trained the model to emit eos tokens when the decoder exceeded endpoints [17]. In our previous work, we revealed that detecting only eos is insufﬁcient for some ASR tasks, and showed that it is also necessary to detect repeated tokens during decoding [18]. This method achieved better performance than most running-stitch approaches, and it did not require any additional training. However, hypotheses might be discarded during decoding and latency can become large, particularly for long utterances because there are real token repetitions.
In this study, we propose a novel blockwise synchronous decoding algorithm that does not require any additional training, with a hybrid approach combining endpoint prediction and endpoint postdetermination, namely the run-and-back stitch (RABS) search. To realize endpoint prediction, the CTC posterior and source–target (ST) attention of the decoder within the CTC/attention framework [3] are used. We compute the expectation of the number of tokens to be emitted in the current encoder output blocks to predict the endpoint. We recover a possible misprediction of this endpoint using the post-determination approach. To achieve this, we explicitly compute the probability of a backward jump in the ST attention to improve on our previous repeated phrase detection. By combining the running stitch and backward stitch in the hybrid RABS search, we can reduce the computational cost and latency. Experiments on the Librispeech English, AISHELL-1 Mandarin, and CSJ Japanese tasks demonstrate that the proposed RABS search achieves a reduction in latency without signiﬁcant degradation in word error rates (WERs). We particularly reduce the 90th percentile latency, in the Librispeech test set for instance, from 1487 ms to 821 ms.
2. STREAMING ENCODER-DECODER ASR
To realize a streaming Enc–Dec ASR system, both the encoder and decoder must be processed online synchronously. A simple way to process the encoder online is through blockwise computation, as in [6, 7, 8]. However, the global channel, speaker, and linguistic context are also important for local phoneme classiﬁcation. Therefore, a context inheritance mechanism for block processing was proposed in [9, 10] by introducing an additional context embedding vector in the encoder. Thus, the encoder sequentially computes encoded features h1:Tb from the currently given b block input x1:Tb .
The synchronous decoding in this work bases on the Transformer architecture proposed in [18]. The decoder predicts the probability of the subsequent character from the previous output

(a) ST attention of running stitch search. The decoder repeats a sequence after approximately hypothesis 900.

(b) ST attention of block synchronous beam search. The decoder struggles after approximately hypothesis 400, whereas the encoder proceeds forward.
Fig. 1: Examples of ST attentions.

(c) ST attention of run-and-back stitch search. The attention is synchronously aligned to the blockwise encoder.

CTC Posterior

_IN _IS _IT <blk>

for Attention weights for
for

Fig. 2: Endpoint prediction by expecting the number of tokens in the encoded features using a CTC posterior and a ST attention.

characters y0:i−1 and the current encoder output blocks h1:Tb , as

p(yi|y0:i−1, h1:Tb ) = Dec(y0:i−1, h1:Tb ).

(1)

Self-attention in the decoder attends to the output history, y0:i−1, and the subsequent ST attention is directed to the encoder output sequence, h1:Tb . In addition to the Enc–Dec Transformer, a linear layer is added to the encoder to project h1:Tb onto the token probability for CTC, which is jointly trained as in [3].

3. RABS DECODING SEARCH

3.1. Running stitch approach: endpoint prediction with CTC posterior

Endpoint prediction realizes continuous and efﬁcient block synchro-

nization. It would be a trivial problem to predict endpoints if the

system knows the number of tokens to be emitted from the given

limited partial encoder features; the decoder should only stop after a

speciﬁc number of tokens are decoded. However, because the length

of the input sequence and that of the output sequence differ and de-

pend on token granularity, the number of tokens in the encoded fea-

tures is unknown. Although most studies introduce an additional

endpoint predictor into the ASR model [6, 7, 13], we predict end-

points without additional training or modules, within the standard

CTC/attention architecture. We compute an expectation value of the

number of tokens following the currently attending time frame in the

currently given encoded blocks. The expectation value is calculated

by combining a CTC posterior and an ST attention.

CTC computes a frame-wise token emission posterior, including

a

blank

label.

Let

p

ct t

c

(

y

|h1:

Tb

)

be

a

CTC

posterior

of

frame

t

given

encoded block h1:Tb , as shown in Fig. 2. In CTC, the same consecu-

tive tokens are merged, and only the tokens transiting from the other

tokens or the blank token are counted. Because the probability of the

previous

token

not

being

y

is

1

−

p

ctc t−1

(

y

|h1:

Tb

)

,

the

probability

of

emitting token y in frame t is described as follows:

ey(t) = 1 − pct−tc1(y|h1:Tb ) · pcttc(y|h1:Tb )

(2)

Let Nbemit(t) be the remaining number of all the tokens except for the blank to be emitted from the encoded features h1:Tb after frame t. The number can be expressed as an accumulation of the token
emission probability (2) as follows:

Tb −t

Nbemit(t) =

ey(t + τ )

(3)

τ =1 y= blk

In the case of Fig. 2, to compute the number of tokens after t = 5, all the posterior except for the blank in the yellow box is accumulated for Nbemit(t = 5). Nbemit(t) is shown as the blue graph in Fig 2.
We use ST attention to know the time frame currently attended
to. An averaged attention weight for the current hypothesis can be
computed from ST attention as follows:

M

qm,i−1 km,t

ai(t) = softmax

√

,

(4)

m=1

d

where qm,i−1 denotes the query value of the last output in the hy-

pothesis, yi−1, km,t is the key value of encoded frame t and of multi-

head m of M heads, and d is the dimensionality of both vectors.

In Transformer, the ST attention of each head is not always mono-

tonic or aligned, because of the multi-head and residual connections.

However, empirically, we found that the ST attention of the last de-

coder layer tends to be monotonic. Thus, we compute the expecta-

tion of the remaining number of tokens after currently attending time

frame as follows:

Tb

E[Nbemit]i = ai(t)Nbemit(t)

(5)

t=1

As shown as the green graph in Fig. 2, the expectation values E[Nbemit]i decrease step by step. If E[Nbemit]i is less than the predeﬁned threshold ν, we assume that current decoding step i reaches
an endpoint of the current b blocks, and the decoder stops until the
encoder outputs the next block hTb+1:Tb+1 .

3.2. Back stitch approach: endpoint post-determination
3.2.1. Block synchronous beam search
CTC and ST attention are not explicitly trained to be aligned; thus, the aforementioned endpoint prediction can cause some errors. Fig. 1a shows an example of ST attention of endpoint prediction search. The horizontal axis denotes the encoder frame and the vertical axis indicates the hypothesis number. In the earlier hypotheses, the decoder uses only the limited encoded features, e.g., h1:T5 = h1:96 for hypothesis 200. In this example, the ST attention jumps back and the decoder repeats a sequence after approximately hypothesis 900. The endpoint post-determination can recover such error. For this purpose, block synchronous (BS) beam search [18] can be applied, in which the repeated tokens are considered as well as eos to evaluate the excess of the endpoints. Detecting such tokens in the hypotheses is regarded as signs of exceeding the endpoint, and thus, we discard the hypotheses containing such tokens and resume decoding after the next block is encoded.
Fig. 1b shows an example of ST attention in the BS search, which shows another problem caused by repeated token detection. The decoder struggles after hypothesis 400, whereas the encoder proceeds forward. After the encoder reaches the end of the utterance at approximately hypothesis 1000, the decoder consumes all the remaining encoded features, which adversely impacts the latency. This particularly occurs in long utterances because there are several real token repetitions, which make the decoder discard hypotheses falsely and be left behind by the encoder. In this study, we improve the BS search by explicitly evaluating ST attention to ﬁnd the back-jump phenomenon in Fig. 1a.

3.2.2. Attention back jump detection

When the back-jump phenomenon occurs, the peak of the current
ST attention focuses on encoder features prior to the last decoded hypothesis. Thus, the probability of the attention back jump, pjiu,bmp, is calculated by accumulating all the ST attention that is concentrated
behind the previous attention, as follows.

Tb

Tb −t

pjiu,bmp = ai(t)

ai−1(t + τ )

(6)

t=1

τ =1

If the back jump probability is greater than a threshold, that is, pjiu,bmp > υ, the current hypotheses ending with yi are regarded as a repeated sequence and discarded. The decoder stops until the next
block hTb+1:Tb+1 is encoded.

3.3. Hybrid RABS beam search
To realize efﬁcient yet accurate ASR decoding, we propose to combine the endpoint predictor using CTC posterior and the endpoint post-determination with attention back jump detection. The proposed hybrid beam search algorithm, namely RABS search, is summarized in Algorithm 1. In every beam search step, ﬁrst, eos and back jump probability of ST attention are evaluated in the current hypotheses (line 6). If eos is found in the hypotheses or pjiu,bmp > υ, the decoder is considered to exceed the endpoint and stops decoding until the next block hTb+1:Tb+1 to be encoded. Subsequently, endpoint prediction is performed by evaluating the expectation of the remaining number of tokens to be emitted, as in (5), with a CTC posterior and a ST attention (line 14). An example of ST attention in the proposed RABS search is shown in Fig. 1c in which the attention of the decoder proceeds synchronously with the encoder.

Algorithm 1 Hybrid run-and-back stitch beam search

Input: encoder feature blocks h, total block number B, beam width K, max

output length Imax Output: Ωˆ : complete hypotheses

1: Initialize: y0 ← sos , Ω0 ← {y0}, b ← 1, i ← 1

2: while b < B do

3: NextBlock ← f alse

4: Ωi ← SearchK (Ωi−1, h1:Tb )

5: for y0:i ∈ Ωi do 6: if yi = eos or pjiu,bmp > υ then

⊲ back-stitch search

7:

NextBlock ← true

8:

end if

9: end for

10: if NextBlock then

11:

b ← b + 1 ⊲ discard current hypotheses and wait for the next

block

12: else

13:

if E[Nbemit]i < ν then

14:

b ← b+1

⊲ running-stitch search ⊲ wait for the next block

15:

end if

16:

i ← i+1

17: end if

18: end while

19: while i < Imax unless EndingCriterion(Ωi−1) do decoding follows to obtain Ωˆ after b = B

⊲ ordinary

20: Ωi ← SearchK (Ωi−1, h1:TB )

21: for y0:i ∈ Ωi do

22:

if yi = eos then

23:

Ωˆ ← Ωˆ ∪ y0:i

24:

end if

25: end for

26: end while 27: return Ωˆ

4. EXPERIMENTS
4.1. Experimental Setup
We conducted experiments using the English LibriSpeech dataset [21], AISHELL-1 [22] Mandarin tasks, and the Japanese CSJ dataset [23]. The input acoustic features were 80-dimensional ﬁlter bank features and the pitch. Regular, small, and large models were trained using multitask learning with CTC loss as in [3] with a weight of 0.3. We used the Adam optimizer and Noam learning rate decay, and applied SpecAugment [24].
We adopted contextual block processing with a Transformer encoder, following [18]. For the regular and small models, we trained a 12-layer encoder with 2048 units, d = 256 of the hidden dimension size, and M = 4 multihead attention. The large model was trained with d = 512 and M = 8 and used HuBERT [25] features 1 pretrained on Libri-light [26].
The decoder had six layers with 2048 units for the regular and large models, and two layers for the small model. The parameters for the ordinary (batch) decoder were directly used in the proposed hybrid RABS algorithm of the decoder. We set the parameters for the RABS search as ν = 1.0 and υ = 0.5.
The real-time factor (RTF) and latency were measured with our implementation of the proposed search algorithm in C++, using a subset of each task with a beam size of 10. We only evaluated regular and small models because we did not implement the large model with HuBERT in C++. We adopted EP latency, following [20], which is the time required to emit eos token after the end of each utterance. The 50th and 90th percentile latencies are shown.
1We used causal feaures for decoding.

Table 1: WERs and computation efﬁciency in the LibriSpeech task. (∗: There is no description of the beam size in the literature. †: The EPs were evaluated on a different dataset and by simulation without considering computation time.)

Beam 30

Beam 10

RTF

test-clean test-other test-clean test-other

Latency

EP50

EP90

Avg. last steps

Triggered Attention [7]

2.8

HS-DACS [19]

2.7

Scout Network [16]

—

Emformer Transducer (pretrained in hybrid ASR) [10]

—

FastEmit Conformer Transducer [20]

—

Regular model (6-layer decoder with d = 256 and M = 4)

BS-Dec [18]

2.7

Running-stitch search (Sec. 3.1)

3.0

Back-stitch search (Sec. 3.2)

2.7

RABS search (proposed)

2.8

Small model (2-layer decoder with d = 256 and M = 4)

BS-Dec [18]

2.9

RABS search (proposed)

3.0

Large model (6-layer decoder with d = 512 and M = 8)

HuBERT BS-Dec + Transformer LM

2.2

HuBERT RABS search + Transformer LM (proposed)

2.2

7.3

—

—

—

—

—

—

6.6

—

—

—

—

—

—

—

2.7

6.4

—

—

—

—

—

2.4∗

6.1∗

—

—

—

—

—

3.5∗

9.1∗

— 290† ms 660† ms

—

7.1

3.0

7.7

0.25 552 ms 1487 ms 7.95

7.7

3.3

8.5

0.24 349 ms 508 ms

3.09

7.1

3.0

7.7

0.25 497 ms 919 ms

5.22

7.2

3.0

7.8

0.24 491 ms 821 ms

4.71

7.5

3.4

8.4

0.15 341 ms 857 ms

8.29

7.6

3.4

8.3

0.14 246 ms 354 ms

3.25

4.3

2.2

4.4

—

—

—

12.28

4.3

2.3

4.5

—

—

—

1.52

Table 2: CERs and latency in the AISHELL-1 task

Dev Test EP50 EP90

RNN-T [27]

10.1 11.8 —

—

Sync-Transformer (6-layer) [17] 7.9 8.9

—

—

HS-DACS [19]

6.2 6.8

—

—

BS-Dec [18]

5.8 6.4 439 ms 570 ms

RABS search (proposed)

5.8 6.4 326 ms 471 ms

Table 3: CERs and latency in the CSJ task

eval 1 eval 2 eval 3 EP50

BS-Dec [18]

5.5 4.2 4.7 978 ms

RABS search (proposed) 5.6 4.2 4.7 582 ms

EP90
2616 ms 1079 ms

The RTF and latency were measured with an 8 core 3.60 GHz Intel i9-9900K processor.
4.2. Librispeech English results
For LibriSpeech, we adopted the byte-pair encoding subword tokenization [28], which has 5000 token classes. A language model (LM) of a four-layer LSTM with 2048 units was fused with a weight of 0.6 for the regular and small models. For the large model, a 16layer Transformer LM was fused as in [18]. CTC weight was set as 0.4. We compare WERs with a beam size of 30 and that of 10. The running-stitch approach described in Sec. 3.1, the back-stitch approach described in Sec. 3.2, and their combination, that is, RABS search (Sec. 3.3), were compared with our previous BS search [18].
We also evaluated the average number of last decoding steps after the encoder reached the utterance end, that is, the number of steps after line 19 in Algorithm 1. We desire as few decoding steps as possible, but if the encoder and decoder are not sufﬁciently synchronous, as shown in Fig. 1b, the value increases.
The results are listed in Table 1 along with other streaming approaches with a larger number of parameters [7, 10, 16, 19, 20]. The results of the regular models show that the running-stitch search is efﬁcient as RTF and latency decreased, particularly at EP90. However, WERs increased owing to the CTC posterior and ST attention misalignment. Back-stitch search successfully decreased the average number of last steps, by replacing repeated token detection with the proposed attention back jump probability while maintaining the WERs of BS search. With the RABS search, we observed a WER re-

duction compared with the running-stitch search because it successfully recovered the error by the back-stitch approach. In addition, the latency of RABS search improved over the back-stitch search, and the EP90 latency decreased compared with BS search, from 1487 ms to 821 ms. The same tendency can be found in the small model. In particular, the 10-beam RABS search with the small model was faster and more accurate than the FastEmit transducer [20]. We also conﬁrmed that our proposed approach can be applied to the large model with HuBERT features, which drastically reduced the number of last steps without signiﬁcant WER degradation.
4.3. AISHELL-1 Mandarin and CSJ Japanese results
We used the regular model architectures for AISHELL-1 Mandarin and CSJ Japanese tasks to conﬁrm the effectiveness of our proposed method in other languages, i.e., in various token granularity. For the Mandarin task, 4231 character classes were used with parameters {CTC weight, beam width, LM weight} = {0.5, 10, 0.7}. For CSJ, the dataset had 3260 Japanese character classes, and parameters were set as {CTC weight, beam width, LM weight} = {0.3, 10, 0.3}. We used an external two-layer LSTM LM with 650 units for each tasks.
The results for AISHELL-1 are summarized in Table 2 and for CSJ are in Table 3. We observed a similar tendency as on Librispeech; the proposed RABS search reduced latency while WERs were maintained. In the Mandarin task, we achieved better performance compared to other streaming methods, such as headsynchronous decoding (HS-DACS) [19]. We conﬁrmed that our proposed method is consistently effective in various languages.
5. CONCLUSION
We have proposed a novel blockwise synchronous decoding algorithm for Enc–Dec ASR, called RABS search, which is a hybrid approach combining endpoint prediction and endpoint postdetermination. In the endpoint prediction, the expectation of the number of tokens that are yet to be emitted in the encoder features of currently given blocks is calculated. The endpoint postdetermination recovers errors from endpoint misprediction, by copmputing backward jump probability of the ST attention. The RABS search successfully combined the advantages of both, reducing the computational cost and latency, and maintaining accuracy at the same time.

6. REFERENCES
[1] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Proc. of NIPS, 2015, pp. 577– 585.
[2] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. of ICASSP, 2016, pp. 4960–4964.
[3] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[4] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. of ICASSP, 2013, pp. 6645–6649.
[5] Kanishka Rao, Has¸im Sak, and Rohit Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer,” in Proc. of ASRU Workshop, 2017, pp. 193–199.
[6] Haoran Miao, Gaofeng Cheng, Zhang Pengyuan, and Yonghong Yan, “Transformer online CTC/attention end-to-end speech recognition architecture,” in Proc. of ICASSP, 2020, pp. 6084–6088.
[7] Niko Moritz, Takaaki Hori, and Jonathan Le Roux, “Streaming automatic speech recognition with the transformer model,” in Proc. of ICASSP, 2020, pp. 6074–6078.
[8] Daniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and Sanjeev Khudanpur, “A time-restricted self-attention layer for ASR,” in Proc. of ICASSP, 2018, pp. 5874–5878.
[9] Emiru Tsunoo, Yosuke Kashiwagi, Toshiyuki Kumakura, and Shinji Watanabe, “Transformer ASR with contextual block processing,” in Proc. of ASRU Workshop, 2019, pp. 427–433.
[10] Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer, “Emformer: Efﬁcient memory transformer based acoustic model for low latency streaming speech recognition,” in Proc. of ICASSP, 2021, pp. 6783–6787.
[11] Jinyu Li, Yu Wu, Yashesh Gaur, Chengyi Wang, Rui Zhao, and Shujie Liu, “On the comparison of popular end-to-end models for large scale speech recognition,” arXiv preprint arXiv:2005.14327, 2020.
[12] Tara N Sainath, Ruoming Pang, David Rybach, Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirko´ Visontai, Qiao Liang, Trevor Strohman, Yonghui Wu, et al., “Two-pass end-to-end speech recognition,” in Proc. of Interspeech, 2019, pp. 2773– 2777.
[13] Chung-Cheng Chiu and Colin Raffel, “Monotonic chunkwise attention,” arXiv preprint arXiv:1712.05382, 2017.
[14] Kwangyoun Kim, Kyungmin Lee, Dhananjaya Gowda, Junmo Park, Sungsoo Kim, Sichen Jin, Young-Yoon Lee, Jinsu Yeo, Daehyun Kim, Seokyeong Jung, et al., “Attention based ondevice streaming speech recognition with large speech corpus,” in Proc. of ASRU Workshop, 2019, pp. 956–963.
[15] Hirofumi Inaguma, Yashesh Gaur, Liang Lu, Jinyu Li, and Yifan Gong, “Minimum latency training strategies for streaming sequence-to-sequence ASR,” in Proc. of ICASSP, 2020, pp. 6064–6068.

[16] Chengyi Wang, Yu Wu, Liang Lu, Shujie Liu, Jinyu Li, Guoli Ye, and Ming Zhou, “Low Latency End-to-End Streaming Speech Recognition with a Scout Network,” in Proc. of Interspeech, 2020, pp. 2112–2116.
[17] Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai Zhang, and Zhengqi Wen, “Synchronous transformers for endto-end speech recognition,” in Proc. of ICASSP, 2020, pp. 7884–7888.
[18] Emiru Tsunoo, Yosuke Kashiwagi, and Shinji Watanabe, “Streaming transformer asr with blockwise synchronous beam search,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 22–29.
[19] Mohan Li, Ca˘ta˘lin Zorila˘, and Rama Doddipatla, “Headsynchronous decoding for transformer-based streaming ASR,” in Proc. of ICASSP, 2021, pp. 5909–5913.
[20] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang, Tara N Sainath, Yanzhang He, Arun Narayanan, Wei Han, Anmol Gulati, Yonghui Wu, et al., “Fastemit: Low-latency streaming asr with sequence-level emission regularization,” in Proc. of ICASSP, 2021, pp. 6004–6008.
[21] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “LibriSpeech: an ASR corpus based on public domain audio books,” in Proc. of ICASSP, 2015, pp. 5206– 5210.
[22] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng, “AIShell-1: An open-source Mandarin speech corpus and a speech recognition baseline,” in Oriental COCOSDA, 2017, pp. 1–5.
[23] Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara, “Spontaneous speech corpus of Japanese,” in Proc. of the International Conference on Language Resources and Evaluation (LREC), 2000, pp. 947–9520.
[24] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le, “SpecAugment: A simple data augmentation method for automatic speech recognition,” in Proc. of Interspeech, 2019.
[25] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” arXiv preprint arXiv:2106.07447, 2021.
[26] Jacob Kahn, Morgane Rivie`re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazare´, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al., “Libri-light: A benchmark for ASR with limited or no supervision,” in Proc. of ICASSP, 2020, pp. 7669–7673.
[27] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, and Zhengqi Wen, “Self-attention transducers for end-to-end speech recognition,” in Proc. of Interspeech, 2019, pp. 4395–4399.
[28] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neural machine translation of rare words with subword units,” in Proc. of the Association for Computational Linguistics, 2016, vol. 1, pp. 1715–1725.

