Large Dual Encoders Are Generalizable Retrievers
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan,
Keith B. Hall, Ming-Wei Chang, Yinfei Yang Google Research
Mountain View, CA

arXiv:2112.07899v1 [cs.IR] 15 Dec 2021

Abstract
It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the ﬁnal score is simply a dotproduct between a query vector and a passage vector, is too limited to make dual encoders an effective retrieval model for out-ofdomain generalization. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck embedding size ﬁxed. With multi-stage training, surprisingly, scaling up the model size brings signiﬁcant improvement on a variety of retrieval tasks, especially for out-ofdomain generalization. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform existing sparse and dense retrievers on the BEIR dataset (Thakur et al., 2021) signiﬁcantly. Most surprisingly, our ablation study ﬁnds that GTR is very data efﬁcient, as it only needs 10% of MS Marco supervised data to achieve the best out-of-domain performance.1
1 Introduction
Typical neural retrieval models follow a dual encoder paradigm (Gillick et al., 2018; Yang et al., 2020; Karpukhin et al., 2020). In this setup, queries and documents are encoded separately into a shared ﬁxed-dimensional embedding space where relevant queries and documents are represented in each other’s proximity. Then, approximated nearest neighbor search (Vanderkam et al., 2013; Johnson et al., 2021) is applied to efﬁciently retrieve relevant documents given an encoded input query.
While dual encoders are popular neural retrievers, the expressiveness of the model is limited by a bottleneck layer consisting of only a simple dotproduct between query embeddings and passage
1All the GTR models are released at https://tfhub. dev/google/collections/gtr/1.

Figure 1: The average Recall@100 and NDCG@100 on all BEIR tasks excluding MS Marco. Scaling up consistently improves dual encoders’ out-of-domain performance.
embeddings. Several papers (Lu et al., 2021; Khattab and Zaharia, 2020) have discussed that the simple dot-product (or cosine similarity) between the embeddings might not be powerful enough to capture semantic relevance. Thakur et al. (2021) studied whether the retriever models can generalize to other domains and conclude that dual encoder models have “issues for out-of-distribution data”, and showed that models with more interactions between queries and documents have better generalization ability.
In this paper, we challenge this belief by scaling up the dual encoder model size while keeping the bottleneck embedding size ﬁxed. Note that scaling up a dual encoder is different from scaling up pretrained language models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) because of the presence of the bottleneck layer. While increasing the model size can greatly increase the capacity of the model, for dual encoders, where the embedding size is ﬁxed, the interactions between queries and documents are still limited by a simple dot-product.
In order to test this hypothesis, we take advantage of the existing T5 model architecture and checkpoints, which allows us to build encoders

Q_Emb (D=768)
T5-base Question Q_Emb (D=768)

D_Emb (D=768)
T5-base Passage D_Emb (D=768)

Q_Emb (D=768)
T5-3B Question Q_Emb (D=768)

D_Emb (D=768)
T5-3B Passage D_Emb (D=768)

T5-large

T5-large

T5-11B

T5-11B

Question

Passage

Question

Passage

Figure 2: Architecture of Generalizable T5-based dense Retrievers. The research question we ask is: can scaling up dual encoder model size improve the retrieval performance while keeping the bottleneck layers ﬁxed? Only the encoder is taken from the pre-train T5 models, and the question tower and document tower of the dual encoder share parameters.

of up to 5 billion parameters while keeping the bottleneck embedding dimension of 768 in all conﬁgurations, as illustrated in Figure 2. Following Ni et al. (2021), we build dual encoders by taking the encoder part of T5. For effectively using the power of large models, we collect roughly two billion community question-answer pairs as generic pre-training data. By combining pre-training using generic training data and ﬁne-tuning using MS Marco (Nguyen et al., 2016), we are able to train large-scale dual encoder retrieval models. We call the resulting models Generalizable T5-based dense Retrievers (GTR).
We evaluate the zero-shot performance of GTR on the BEIR benchmark (Thakur et al., 2021), which consists of 18 selected information retrieval tasks across 9 domains.2 Our results show that, surprisingly, scaling up of dual encoders leads to better generalizability despite the ﬁxed bottleneck embedding dimension. Second, pre-training on community question-answer pairs and ﬁne-tuning on human curated data are both important to fully utilize the power of the scaled up model. In addition, with scaling and pre-training, we found GTR to be highly data efﬁcient in terms of human annotated queries, as it only needs to use 10% of MS Marco to match the overall out-of-domain performance.
2We focus on evaluating the performance on the 18 BEIR (Thakur et al., 2021) tasks other than MS Marco and we did not use in-domain training data or question generation (Ma et al., 2021).

2 Background
2.1 Dual Encoder and dense retrieval
Classic retrieval models such as BM25 (Robertson and Zaragoza, 2009) relies on lexical overlap, term frequency heuristics, inverse document frequency and document length. This type of retrieval models does not require any training and can generalize reasonably well due to its emphasis on lexical match. However, these retrieval models fall short of ﬁnding documents that are only semantically related to the query and have low lexical overlap.
This is where the dense retrieval models come into play. The retrieval process is dense because both queries and documents are embedded into lowdimensional dense representations, in contrast to the high-dimensional sparse representations used in lexical based retrieval functions. Such an encoding process is often accomplished by a dual encoder architecture, with one of the encoders tailored to the queries and the other to the documents. Matching using dense representations enables dense retrieval models to go beyond lexical overlap to retrieve semantically similar documents. This powerful capability of semantic matching, however, often requires relatively large amounts of training data.
Another critical challenge for dual encoder models is that the performance is possibly bounded by the dot-product similarity function. As such, there is growing interest in applying lightweight interaction layers to replace the single dot-product function. On the one hand, Luan et al. (2020) pro-

poses a multi-vector encoding model, which represents each document as a ﬁxed-size set of multiple vectors, and calculate the relevance scores as the maximum inner product over this set. This model combines the efﬁciency of dual encoders with some of the expressiveness of attention based architectures. On the other hand, ColBERT (Khattab and Zaharia, 2020) proposes to learn embeddings for each token and then use a “MaxSim” operation to select the best candidate. These models achieve signiﬁcant improvement but also introduce a large latency overhead. In this paper, we take a step back and aim to empirically study how to improve the performance of single dot-product based methods. Speciﬁcally, we study whether simply scaling up the model capacity can lead to better ﬁxed embeddings to improve the performance of single dot-product retrievers.
2.2 BEIR generalization task
For evaluation in this paper we use BEIR, a heterogenous benchmark for zero-shot evaluation of information retrieval models. The BEIR zero-short evaluation suit contains 18 information retrieval datasets3 across 9 domains, including Bio-Medical, Finance, News, Twitter, Wikipedia, StackExchange, Quora, Scientiﬁc, and Misc. The majority of the datasets have binary relevancy labels indicating whether a document is relevant to a given query or not. A small part of the datasets have 3-level or 5-level relevancy judgements. We refer readers to the original BEIR paper (Thakur et al., 2021) for more details.
3 Generalizable T5 Retriever
3.1 T5 dual encoder
We use the dual encoder framework to train dense retrieval models. We follow prior work (Xiong et al., 2020; Hofstätter et al., 2021) to initialize dual encoders from pre-trained language models. In this work, we found convenient to use the pretrained T5 model family as our backbone encoder because the T5 model family provides off-the-shelf pre-trained models (e.g. T5, mT5, byT5) with a wide range of model capacity from millions to billions of parameters (Raffel et al., 2020; Xue et al., 2020, 2021). The architectures of our models are illustrated in ﬁg. 2.
3MS Marco is excluded from the zero-shot comparison as many baseline model used it as training data.

T5
Question

Shared

T5
Passage

Pre-training

Web dataset (Mined from web)

Fine-tuning

Search dataset (Human annotated)

Figure 3: Multi-stage training for generalizable T5 dual encoder.

Let paired examples D = {(qi, p+i )} be the training set, where qi is an input question and p+i is a related passage (e.g., a semantically relevant pas-
sage to the question). Following Ni et al. (2021), we encode the question qi and passage p+i into embeddings by feeding them to the T5 encoder and
taking the mean pooling of the encoder as output.
In all outr experiments, we ﬁx the output embed-
dings to be of size 768.
We train the model using an in-batch sampled
softmax loss (Henderson et al., 2017):

esim(qi,p+i )/τ

L=

+,

(1)

j∈B esim(qi,pj )/τ

where the similarity scoring function sim is the
cosine similarity between the embeddings of qi and p+i . B is a mini-batch of examples and τ is the softmax temperature.
Additional negatives p−j can be given for input question q. The loss is computed by including them
in the denominator:

esim(qi,p+i )/τ

L=

+

− . (2)

j∈B esim(qi,pj )/τ + esim(qi,pj )/τ

We also apply a bi-directional in-batch sampled softmax loss, where we compute losses for both question to document matching and document to question matching.

3.2 Multi-stage training
As shown in ﬁg. 3, we use a multi-stage dual encoder training approach to achieve generalizable retrieval models.

The training process includes a pre-training stage on a web-mined corpus and a ﬁne-tuning stage on search datasets. The web-mined corpus provides a large amount of semi-structured data pairs (such as question-answer pairs and conversations), which can provide rich semantic relevance information. It is easy to collect but it is often not well annotated, if at all. The search datasets are often annotated by humans, and the queries and documents are also authored by humans. These datasets are of high quality but costly to collect.
In this work, for dual encoder pre-training, we initialize the dual encoders from the T5 models and train on question-answer pairs collected from the Web. Recently, Sentence-T5 (Ni et al., 2021) explored different ways to extract strong text embeddings and achieved remarkable performance on SentEval and Sentence Textual Similarity tasks. We follow that setting to encode queries and passages via mean pooling from the T5 encoders and focus on the dense retrieval tasks.
For ﬁne-tuning, our aim is to adapt the model to retrieval using a high quality search corpus so the model can learn to better match generic queries to documents. In this paper, we consider two datasets for ﬁne-tuning: MS Marco (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019).
4 Experimental setup
4.1 Training Data
Community QA. In order to leverage most of the power from the large scale models, we collect input-response pairs and question-answer pairs from online forums and QA websites including Reddit, Stack-Overﬂow, etc. This results in 2 billion question-answer pairs that we use to pre-train the dual encoder.
MS Marco. We consider the MS Marco dataset (Nguyen et al., 2016), which includes 532K query and document pairs, as search data for ﬁnetuning. The dataset is sampled from Bing search logs, which covers a broad range of domains and concepts. Most of the neural models compared in (Thakur et al., 2021) are trained on MS Marco, including DeepCT (Dai and Callan, 2020), DocT5Query (Nogueira, 2019), ANCE (Xiong et al., 2020) and ColBERT (Khattab and Zaharia, 2020). Some of these models have shown great generalization with comparable or even better performance relative to BM25.

GTR Models # of params

Base 110M

Large 335M

XL 1.24B

XXL 4.8B

Table 1: Number of parameters in the GTR models.

Models
ColBERT DPR, ANCE, TAS-B, GenQ, GTR BM25, DocT5Query

Dim. size
128 768
-

Table 2: Dimension size of different models. Most dual encoder models set the embedding dimension size to 768.

Natural Questions. In the ﬁne-tuning stage, we also consider the Natural Questions dataset (Kwiatkowski et al., 2019) , which has been widely used in the dense retrieval literature (Karpukhin et al., 2020; Xiong et al., 2020). It consists of 130k query and passage pairs which are also humanannotated.
4.2 Conﬁgurations
We implement GTR models in JAX4 and train them on Cloud TPU-V8. We consider different sizes of the T5 transformer (Vaswani et al., 2017) architecture including Base, Large, XL and XXL. Their number of parameters are listed in table 1.
Note that we only use the encoder portion of the T5 models and thus the number of parameters are less than half of the full model size. We use the offthe-shelf checkpoints as the initial parameters and use the same sentencepiece vocabulary model.5
During pre-training and ﬁne-tuning, we set the batch size to 2048 and use a softmax temperature τ of 0.01. We use Adafactor optimizer (Shazeer and Stern, 2018) and set the initial learning rate to 1e-3 with a linear decay. We train the model for 800K steps and 20K steps for the pre-training and ﬁne-tuning stages, respectively.
For ﬁne-tuning, we use the hard negatives released by RocketQA (Qu et al., 2021) when ﬁnetuning with MS Marco data and the hard negatives release by (Lu et al., 2021) for Natural Questions, which were proven to lead to better retriever performance. By default, we use the complete MS Marco dataset and the NQ dataset for ﬁne-tuning.
When evaluating on the BEIR benchmark, we use sequences of 64 tokens for the questions and 512 for the documents in all datasets except Trec-
4https://github.com/google/jax 5https://github.com/google-research/ text-to-text-transfer-transformer

NDCG@10 / Model
MS Marco
Trec-Covid BioASQ NFCorpus NQ HotpotQA FiQA-2018 Signal-1M Trec-News Robust04 ArguAna Touché-2020 Quora DBPedia-entity SCIDOCS Fever Climate-Fever SciFact CQADupStack
Avg Avg w/o MS Marco

Lexical / Sparse

BM25 docT5query

0.228

0.338

0.656 0.465 0.325 0.329 0.603 0.236 0.330 0.398 0.408 0.315 0.367 0.789 0.313 0.158 0.753 0.213 0.665 0.299

0.713 0.431 0.328 0.399 0.58 0.291 0.307 0.42 0.437 0.349 0.347 0.802 0.331 0.162 0.714 0.201 0.675 0.325

0.413 0.423

0.429 0.434

DPR
0.177
0.332 0.127 0.189 0.474 0.391 0.112 0.155 0.161 0.252 0.175 0.131 0.248 0.263 0.077 0.562 0.148 0.318 0.153
0.234 0.237

ANCE
0.388
0.654 0.306 0.237 0.446 0.456 0.295 0.249 0.382 0.392 0.415 0.240 0.852 0.281 0.122 0.669 0.198 0.507 0.296
0.389 0.389

Dense

TAS-B GenQ

0.408 0.408

0.481 0.383 0.319 0.463 0.584 0.300 0.289 0.377 0.427 0.429 0.162 0.835 0.384 0.149 0.700 0.228 0.643 0.314

0.619 0.398 0.319 0.358 0.534 0.308 0.281 0.396 0.362 0.493 0.182 0.830 0.328 0.143 0.669 0.175 0.644 0.347

0.414 0.415

0.410 0.410

ColBERT
0.401
0.677 0.474 0.305 0.524 0.593 0.317 0.274 0.393 0.391 0.233 0.202 0.854 0.392 0.145 0.771 0.184 0.671 0.350
0.429 0.431

GTR-Base
0.420
0.539 0.271 0.308 0.495 0.535 0.349 0.261 0.337 0.437 0.511 0.205 0.881 0.347 0.149 0.660 0.241 0.600 0.357
0.416 0.416

Ours

GTR-Large GTR-XL

0.430

0.439

0.557 0.320 0.329 0.547 0.579 0.424 0.265 0.343 0.470 0.525 0.219 0.890 0.391 0.158 0.712 0.262 0.639 0.384

0.584 0.317 0.343 0.559 0.591 0.444 0.268 0.350 0.479 0.531 0.230 0.890 0.396 0.159 0.717 0.270 0.635 0.388

0.444 0.445

0.452 0.453

GTR-XXL
0.442
0.501 0.324 0.342 0.568 0.599 0.467 0.273 0.346 0.506 0.540 0.256 0.892 0.408 0.161 0.740 0.267 0.662 0.399
0.457 0.458

Table 3: NDCG@10 on the BEIR benchmark. The best result on a given dataset is marked in bold. GTR models are pre-trained on CommunityQA dataset and the complete MS Marco dataset. GTR models achieve better NDCG when increasing size from Base to XXL, outperforming the previous best sparse model DocT5Query and dense retrieval model TAS-B.

News, Robust-04 and ArguAna. In particular, we set the document length to 768 for Trec-News and Robust-04 while setting the question length to 512 for ArguAna, in accordance to the average query and document lengths in these datasets.
4.3 Models for comparison
We consider various baselines, including sparse retrieval models: BM25, DocT5Query, and dense retrieval models: DPR, ANCE, TAS-B, and GenQ (Thakur et al., 2021).
We conduct experiments on four different sizes of our GTR models (GTR-Base, GTR-Large, GTRXL, and GTR-XXL). We consider three different settings for GTR to investigate the scaling up effect for different training stages:
• GTR. This is the full GTR model that conducts both pre-training and ﬁne-tuning.
• GTR-FT. This is a ﬁne-tune only version of GTR where the T5 dual encoders on the MS Marco dataset.
• GTR-PT. This is a pre-training only version of GTR where the T5 dual encoders is only pre-trained on the CommunityQA dataset.
We evaluate baseline and our models on the BEIR generalization task (Thakur et al., 2021) as discussed in section 2.2,. We consider two main

retrieval metrics: NDCG@10 and Recall@100 following BEIR (Thakur et al., 2021). Due to space limitations, we report NDCG@10 in the main section of the paper and include Recall@100 results in appendix A.
5 Evaluation Results
We present three groups of experiments to study the a) in-domain performance of the GTR models on MS Marco, b) their out-of-domain generalization performance on BEIR, and c) their data efﬁciency.
5.1 Results on MS Marco
We ﬁrst analyze in-domain performance based on the evaluation results on MS Marco. As show in table 3, with scaling up, the models achieve consistent improvement on NDCG@10. We observe similar improvements on other evaluation metrics including MRR@10 and Recall@1000 and reported the numbers in table 7 of appendix A. This shows that increasing model capacity leads to better indomain performance.
5.2 Results on BEIR generalization tasks
The next set of experiments investigates the effect of increasing model capacity on out-of-domain (OOD) performance.
As shown in table 3, we observe a clear gain on out-of-domain performance in terms of NDCG@10 when the model size increases. The GTR-Large

MS MARCO
0.420.430.440.44
0.23
HotpotQA
0.580.590.6 0.6 0.54
ArguAna
0.510.520.530.54
0.32
Fever
0.710.720.740.75 0.66

Trec-Covid
0.66 0.540.560.58
0.5
FiQA-2018
0.420.440.47 0.35
0.24
Touché-2020
0.37
0.2 0.220.230.26
Climate-Fever
0.260.270.27 0.24
0.21

BioASQ
0.46
0.320.320.32 0.27
Signal-1M
0.33
0.260.260.270.27
Quora
0.880.890.890.89 0.79
SciFact
0.640.640.660.66 0.6

NFCorpus
0.330.340.340.32 0.31
Trec-News
0.4
0.340.340.350.35
DBPedia-entity
0.390.4 0.41 0.35
0.31
CQADupStack
0.380.390.4 0.36
0.3

NQ
0.550.560.57 0.5
0.33
Robust04
0.51 0.470.48 0.44
0.41
SCIDOCS
0.160.160.160.16 0.15

Avg

0.440.450.46

0.42

0.41

BaseLarge XL XXLBM25 BaseLarge XL XXLBM25 BaseLarge XL XXLBM25 BaseLarge XL XXLBM25 BaseLarge XL XXLBM25

Figure 4: Comparison with BM25 on NDCG@10. The GTR-Base model outperforms BM25 on 9 datasets and the larger GTR models continue to improve on these 9 tasks. The GTR-XXL model catches up or surpasses BM25 on the other 5 datasets and only under-performs on 5 of the remaining tasks.

model already outperforms the previous best dense retrieval model TAS-B as well as the best sparse model DocT5Query. Scaling up to GTR-XXL leads to another jump in retrieval performance. Similar improvements are found on Recall@100 as shown in the Appendix’s table 8. On average, the scaling up process demonstrates an encouraging ascending trend that eventually outperforms all baseline methods on all evaluation metrics. This conﬁrms that scaling up is a valid path towards generalizability.
Previously, dual encoders failed to match the performance of BM25 for tasks that require better lexical matching capabilities. Thus, we wanted to investigate what kind of tasks can get improved by scaling up the model size. Figure 4 presents a detailed comparison of all sizes of GTR models against the BM25 baseline.
For tasks like NQ where dual encoders have been previously shown to be more effective than BM25, increasing the model size continues to advance the performance of dual encoders. This suggests scaling up can further boost the head start of dense models over sparse models on these datasets.
For tasks like BioASQ and NFCorpus, where dual encoders previously struggled to match the performance of BM25 for inherent reasons, we discovered that scaling up consistently improves the retrieval performance. In particular, for NFCorpus, our Base model under-performs BM25 but the

GTR-FT

GTR

Ratio of data Large XL Large XL XXL

NDCG@10 on MS Marco

10% 100%

0.402 0.397 0.428 0.426

-

0.415 0.418 0.430 0.439 0.430

Zero-shot average NDCG@10 w/o MS Marco

10% 100%

0.413 0.418 0.452 0.462 0.465 0.412 0.433 0.445 0.453 0.458

Table 4: Comparisons of NDCG@10 for GTR models trained with different amount of ﬁne-tuning data. With only 10% of the MS Marco data, both GTR-FT and GTR large and XL models achieve slightly worse indomain performance; meanwhile they obtain comparable or even superior out-of-domain performance than using the complete MS Marco data.

XL model outperforms BM25 by 5.5% (0.343 vs. 0.325). This exciting ﬁnding veriﬁes our assumption that scaling up can further exploit the powerful semantic matching capabilities of the dual encoder models and enable them to ultimately outperform BM25.
5.3 Data efﬁciency for large retrievers
To better understand the data efﬁciency for large dual encoders, we trained models using different portions of the MS Marco dataset during ﬁnetuning. In particular, we sampled a subset of the training data by keeping only 10% of the training queries as well as their relevant (positive) passages and irrelevant (hard negative) passages.
As shown in table 4, using 10% of training data reduces the in-domain performance of the GTR models on MS Marco. For the GTR-FT (ﬁnetuning only) models, using 10% of the data leads to a mixed result of out-of-domain performance.
On the other hand, for full GTR models, using 10% of the MS Marco dataset is sufﬁcient for ﬁnetuning. In particular, the GTR-Large, XL and XXL models achieve comparable or even better OOD performance than ﬁne-tuning on the complete MS Marco dataset. This might suggest that GTR models have the beneﬁt of data efﬁciency and could use less training data for domain adaptation.
6 Ablation Study and Analysis
In this section we present ablations and analysis to further understand the effects of scaling up, the impact of ﬁne-tuning and pre-training, and the trends of the GTR model on different experimental conditions.

GTR-FT GTR-PT GTR

Fine-tuning







NDCG@10 on MS Marco

Base Large XL XXL

0.400 0.415 0.418 0.422

0.258 0.262 0.259 0.252

0.420 0.430 0.439 0.442

Zero-shot average NDCG@10 w/o MS Marco

Base Large XL XXL

0.387 0.412 0.433 0.430

0.295 0.315 0.315 0.332

0.416 0.445 0.453 0.458

Table 5: Comparisons (NDCG@10) of the models trained with and without pre-training and ﬁne-tuning. Notably, the GTR-FT XL model already achieves an average zero-shot NDCG@10 of 0.433, which outperforms the previous best dual encoder model TAS-B (NDCG@10=0.415).

6.1 Effect of scaling up for different training stages
The ﬁrst ablation study aims to investigate how scaling up effects dual encoder pre-training and ﬁne-tuning. Results are listed in table 5.
For ﬁne-tuning only models, scaling up beneﬁts both in-domain and out-of-domain performance. For pre-training only models, the improvement on in-domain performance is not obvious; meanwhile for out-of-domain tasks, scaling up also improves the generalization. Finally with both pre-training and ﬁne-tuning, GTR models consistently improve over GTR-FT models of all sizes. This shows the power of combining scaling up and a generic pretraining stage.
6.2 Importance of the ﬁne-tuning dataset
In table 5, we compare GTR and GTR-PT on the BEIR benchmark to understand the importance of ﬁne-tuning on MS Marco. The table shows that there is a clear gap between GTR models before and after ﬁne-tuning. The result shows the necessity of leveraging a high quality dataset (e.g. search data) to ﬁne-tune the dual encoders.
In table 6, we compare ﬁne-tuning GTR on NQ instead of MS Marco. Compared to MS Marco, NQ only covers Wikipedia documents and is much smaller in size, which allows us to investigate the performance of GTR when ﬁne-tuned on a less generalizable dataset. In addition, ﬁne-tuning on NQ can give us a fair comparison with DPR.
As shown in table 6, the GTR-base model ﬁne-

Model
DPR GTR-Base GTR-Large GTR-XL
GTR-Large GTR-XL

Fine-tuning dataset
NQ NQ NQ NQ
MS Marco MS Marco

Zero-shot average NDCG@10
0.237 0.360 0.379 0.407
0.445 0.453

Table 6: Comparisons of GTR models ﬁne-tuned on MS Marco and NQ. We report the zero-shot average NDCG@10. Scaling up improves model performance both on NQ and MS Marco.

tuned on NQ outperforms the original DPR model, which uses a BERT-Base model as the encoder backbone. This demonstrates the effectiveness of our pre-training on the Web dataset as well as the hard negatives introduced from Lu et al. (2021) for NQ. Fine-tuning on NQ leads to inferior performance compared to ﬁne-tuning on MS Marco, which is consistent with prior work (Thakur et al., 2021). However, importantly, scaling up GTR size improves zero-shot performance on BEIR when ﬁne-tuning on NQ. This shows that the beneﬁt of scaling up holds for different ﬁne-tuning datasets. Furthermore, when scaling from Large to XL, we observe a larger gain when ﬁne-tuning with NQ than with MS Marco, indicating that scaling up helps more when using weaker ﬁne-tuning data.
6.3 Document length vs model capacity
Previously, BEIR has shown that models trained with cosine similarity prefer short documents while those trained with dot-product prefer long documents (Thakur et al., 2021). We investigate whether scaling up affect this observation. Speciﬁcally, we compute the median lengths (in words) of the top10 retrieved documents for all queries. Results are shown in ﬁg. 5.
Though all GTR models are trained using cosine similarity, we found that scaling up the model size has inﬂuence over the lengths of retrieved documents. We observe an increasing trend of document length for DB-Pedia, Fever, HotpotQA, Signal-1M, Trec-News, and Web-Touche2020 with scaling up. In particular, for Web-Touche2020, the lengths of the retrieved documents grow drastically as the models scale up: The largest GTR-XXL retrieves documents that are on average twice as long compared with the smallest GTR-Base. This plays in our favor since Thakur et al. (2021) show that the majority of relevant documents in Web-

ArguAna 173 176 173 172

BioASQ 176 179 173 182

Climate-Fever 228 244 234
206

DBPedia-entity 60 62 62
57

Fever 104 105
97
88

FiQA-2018 156 151 154
136

HotpotQA 61 63
53 56

MS Marco 49 50 50 50

NFCorpus

NQ

236 243 245 243 75 78 80 81

Quora 999 8

Robust04 525 490
453 410

SCIDOCS 166 170 169 168

SciFact 211 216 221 219

15 Trec-Covid 12
10 9 BaseLarge XL XXL

Trec-News Touché-202602

707 700 708

678

41

32 32

BaseLarge XL XXL BaseLarge XL XXL

CQADupStack 116 117 116
111
BaseLarge XL XXL

Signal-1M 10 10 10 10

Figure 5: Median lengths (in words) of top-10 retrieved documents for all queries.

Touche2020 are longer. On the other hand, the only exception we ob-
serve is the Trec-Covid dataset, where GTR-XXL model retrieves much shorter documents than those retrieved by the smaller size counterparts. This may explain the inferior performance of GTR-XXL on Trec-Covid shown in table 3 and table 8. We leave it as future work to explore the effects of using the dot-product as similarity function for large dual encoders.
7 Related Work
Neural information retrieval. Document retrieval is an important task in the NLP and information retrieval (IR) communities. The goal is to ﬁnd the relevant document from a large corpus given a query. Traditionally, lexical based approaches trying to match the query and document based on term overlap, such as TF-IDF and BM25 (Robertson and Zaragoza, 2009), have achieved great success in this task. Recently, neural based approaches, which go beyond the simple term matching, are being quickly adopted by the community and achieve state-of-the-art performance on multiple retrieval tasks, such as passage retrieval (Karpukhin et al., 2020), question answering (Ahmad et al., 2019), conversational question answering (Qu et al., 2020) and bitext retrieval (Feng et al., 2020).
Dual encoders for neural retrieval. Dual encoders have demonstrated to be one type of neural retrievers that can achieve great performance compared to traditional sparse models such as BM25 for a wide range of retrieval tasks (Karpukhin et al., 2020; Gillick et al., 2018). One key aspect to their

success is the adoption of pre-trained language models, which enables the dual encoders to have backbone contextual embeddings to initialize from. Other techniques such as negative mining (Xiong et al., 2020; Lu et al., 2021; Sachan et al., 2021) and large training batch sizes (Qu et al., 2021) have also shown great effectiveness. However, few of the previous works have discussed the effect of the backbone model’s capacity.
Zero-shot neural retrieval. Recent works have shown great improvement under the zero-shot setting for dual encoders by leveraging distillation and synthetic data generation (Thakur et al., 2021; Hofstätter et al., 2021; Ma et al., 2020). Both of these techniques, and scaling up backbone models, are effective ways to close the gap between dual encoders and the upper bound of the singleproduct approaches with ﬁxed-dimension embeddings. On the other hand, multi-vector approaches introduce more interactions between dense embeddings, which could also beneﬁt from scaling up the backbone multi-vector encoders. We hope that our observation about scaling up model sizes for single dot-product based methods can be combined with these techniques and further push the frontier of neural retrieval models.
8 Inference latency
One caveat for scaling up model size is the increment in the latency overhead. We investigate the inference speed in terms of microseconds (ms) for all GTR models with batch size 1 and input length 128. We found the latency increases from 17 ms, 34 ms, 96 ms to 349 ms. The GTR-Base model has close latency compared to TAS-B while the largest GTR-XXL model has a similar latency to the re-ranking models (Thakur et al., 2021). With the recent work towards making large models efﬁcient from angles such as sparsity, distillation and prompt-tuning, we hope the inference time for large dual encoders can be signiﬁcantly reduced in the future.
9 Conclusion
This paper presents the Generalizable T5 Retriever (GTR), a scaled-up dual encoder model with a ﬁxed-size bottleneck layer. We show that scaling up the model size brings signiﬁcant improvement on retrieval performance across the board on the BEIR zero-shot retrieval benchmark, especially

for out-of-domain generalization. The GTR-XXL model achieves state-of-the-art performance on the BEIR benchmark, outperforming many models that use earlier interactions between queries and documents. This sheds light on the research direction to keep improving the single vector representation model through better backbone encoders. The ﬁndings here are also complementary with other recent works that improve the dual encoder training, including distilling from a ranker / scorer model, using a better contrasting pre-training objective and scaling up the encoders for multi-vector retrieval models.
Acknowledgments
We thank Chris Tar and Don Metzler for feedback and suggestions.
References
Amin Ahmad, Noah Constant, Yinfei Yang, and Daniel Cer. 2019. ReQA: An evaluation for end-to-end answer retrieval models. In Workshop on Machine Reading for Question Answering.
Anonymous. 2022. Contrastive pre-training for zeroshot information retrieval. In Submitted to The Tenth International Conference on Learning Representations. Under review.
Zhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for ﬁrst stage passage retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’20, page 1533–1536, New York, NY, USA. Association for Computing Machinery.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Languageagnostic bert sentence embedding.
D. Gillick, A. Presta, and Gaurav Singh Tomar. 2018. End-to-end retrieval in continuous space. ArXiv, abs/1811.08008.
Matthew Henderson, Rami Al-Rfou, B. Strope, YunHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and R. Kurzweil. 2017. Efﬁcient natural language response suggestion for smart reply. ArXiv, abs/1705.00652.
Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efﬁciently teaching an effective dense retriever with

balanced topic aware sampling. arXiv preprint arXiv:2104.06967.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769– 6781, Online. Association for Computational Linguistics.
Omar Khattab and Matei Zaharia. 2020. Colbert: Efﬁcient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39–48.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466.
Jing Lu, Gustavo Hernández Ábrego, Ji Ma, Jianmo Ni, and Yinfei Yang. 2021. Multi-stage training with improved negative contrast for neural passage retrieval. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6091–6103.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020. Sparse, dense, and attentional representations for text retrieval. arXiv preprint arXiv:2005.00181.
Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2020. Zero-shot neural retrieval via domain-targeted synthetic query generation. arXiv e-prints, pages arXiv–2004.
Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot neural passage retrieval via domain-targeted synthetic question generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1075–1088, Online. Association for Computational Linguistics.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset.
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. 2021. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877.

Rodrigo Nogueira. 2019. From doc2query to doctttttquery.
Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. 2020. Open-retrieval conversational question answering. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835–5847.
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed textto-text transformer. JMLR, 21/140.
Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389.
Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamilton, and Bryan Catanzaro. 2021. End-to-end training of neural retrievers for open-domain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6648–6662, Online. Association for Computational Linguistics.
Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596–4604. PMLR.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).
Dan Vanderkam, Rob Schonberger, Henry Rowley, and Sanjiv Kumar. 2013. Nearest neighbor search in google correlate. Technical report, Google.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.

Linting Xue, Aditya Barua, Noah Constant, Rami AlRfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2021. Byt5: Towards a token-free future with pre-trained byte-to-byte models. arXiv preprint arXiv:2105.13626.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.
Yinfei Yang, Daniel Matthew Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, G. Ábrego, Steve Yuan, C. Tar, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2020. Multilingual universal sentence encoder for semantic retrieval. In ACL.

Model
ANCE TAS-Balanced ColBERT RocketQA GTR-Base GTR-Large GTR-XL GTR-XXL

NDCG@10
0.388 0.408 0.401
/ 0.420 0.430 0.439 0.442

MRR@10
0.330 0.340 0.360 0.370 0.366 0.379 0.385 0.388

Recall@1000
0.959 0.975 0.968 0.979 0.983 0.991 0.989 0.990

Table 7: Comparisons of different models on MS Marco. Scaling up can improve GTR models’ indomain performance.

A More results
A.1 Comparisons on MS Marco
Table 7 shows the comparisons of GTR models and the baselines. Note that the best RocketQA model used additional augmented data other than MS Marco to improve the model performance while all others do not. Our best GTR-XXL models outperforms RocketQA on both MRR and recall.
A.2 Comparison of different dual encoder pre-training strategies
In a concurrent work (Anonymous, 2022), researchers proposed to conduct contrastive learning (CL) pre-training for improving the generalizability of neural retrievers. The paired data for contrastive training is constructed from C4 and Wiki dataset in an unsupervised way. In particular, they construct pairs by randomly choosing two spans from a single document and conduct word deletion or replacement to each span. We compare the performance of our GTR models to their models to gain insights into different pretraining strategies for dual encoders.
As shown in Figure 6, on over half of the datasets, models with our pre-training approach under-perform CL-Pretrain with the base size; while as the model size increases, GTR-Large and -XXL models show signiﬁcant gains over CLPretrain. The best GTR-XXL model achieves 0.49 for NDCG@10 on average while CL-Pretrain achieves 0.46. This demonstrates that scaling up can mitigate the disadvantage of the potentially inferior pre-training approach. Note that our pretraining is additive to CL-Pretrain and we can leverage the pre-training on C4 and Wiki to further improve the results. We leave this exploration as future work.

Trec-Covid
0.66 0.540.560.58 0.5
0A.5r1g0u.5A20n.a530.54

0.32

Fever

0.75

0.74

0.710.72

0.66

NFCorpus
0.340.34 0.32 0.33
0.31
0.3T7ouché-2020
0.26 0.2 0.220.23
Climate-Fever
0.260.270.27 0.24 0.21

0N.5Q50.560.57
0.5

0.33
Quora
0.880.890.890.89
0.79

SciFact

0.66

0.66

0.640.64

0.6

HotpotQA
0.6 0.580.590.6 0.54
DBPedia-entity
0.390.4 0.41 0.35 0.31
CQADupStack
0.380.390.4 0.36
0.3

FiQA-20180.47
0.420.44 0.35
0.24
SCIDOCS
0.16 0.160.160.16 0.15
Avg
0.440.450.46 0.410.42

CLBaseLarge XL XXL CLBaseLarge XL XXL CLBaseLarge XL XXL CLBaseLarge XL XXL CLBaseLarge XL XXL

Figure 6: Comparison with Anonymous (2022) on NDCG@10. “CL” denotes Anonymous (2022) with contrastive learning on C4 and Wiki while others denote our GTR models with different sizes. Note that they only report results on 15 datasets of the BEIR benchmark.

A.3 Recall on BEIR
Table 8 presents the Recall@100 of GTR models and the baselines. Similar to NDCG@10, we observe that scaling up dual encoders lead to signiﬁcant gains on the BEIR benchmark in terms of recall.

Recall@10 / Model
MS Marco Trec-Covid BioASQ NFCorpus NQ HotpotQA FiQA-2018 Signal-1M Trec-News Robust04 ArguAna Touché-2020 Quora DBPedia-entity SCIDOCS Fever Climate-Fever SciFact CQADupStack
Avg Avg w/o MS Marco

Lexical / Sparse

BM25 docT5query

0.658 0.498 0.714 0.250 0.760 0.740 0.539 0.370 0.422 0.375 0.942 0.538 0.973 0.398 0.356 0.931 0.436 0.908 0.606

0.819 0.541 0.646 0.253 0.832 0.709 0.598 0.351 0.439 0.357 0.972 0.557 0.982 0.365 0.360 0.916 0.427 0.914 0.638

0.601 0.598

0.615 0.603

DPR
0.552 0.212 0.256 0.208 0.880 0.591 0.342 0.162 0.215 0.211 0.751 0.301 0.470 0.349 0.219 0.840 0.390 0.727 0.403
0.425 0.418

ANCE
0.852 0.457 0.463 0.232 0.836 0.578 0.581 0.239 0.398 0.274 0.937 0.458 0.987 0.319 0.269 0.900 0.445 0.816 0.579
0.559 0.543

Dense

TAS-B GenQ

0.884 0.387 0.579 0.280 0.903 0.728 0.593 0.304 0.418 0.331 0.942 0.431 0.986 0.499 0.335 0.937 0.534 0.891 0.622

0.884 0.456 0.627 0.280 0.862 0.673 0.618 0.281 0.412 0.298 0.978 0.451 0.988 0.431 0.332 0.928 0.450 0.893 0.654

0.610 0.594

0.605 0.590

ColBERT
0.865 0.464 0.645 0.254 0.912 0.748 0.603 0.283 0.367 0.31 0.914 0.439 0.989 0.461 0.344 0.934 0.444 0.878 0.624
0.604 0.590

GTR-Base
0.898 0.411 0.441 0.275 0.893 0.676 0.670 0.263 0.475 0.324 0.974 0.281 0.996 0.418 0.340 0.923 0.522 0.872 0.681
0.596 0.580

Ours

GTR-Large GTR-XL

0.908 0.434 0.490 0.298 0.930 0.725 0.742 0.261 0.525 0.365 0.978 0.282 0.996 0.480 0.358 0.941 0.552 0.899 0.714

0.911 0.457 0.483 0.318 0.936 0.739 0.755 0.268 0.512 0.364 0.980 0.297 0.997 0.480 0.358 0.944 0.569 0.911 0.729

0.625 0.609

0.632 0.616

GTR-XXL
0.916 0.407 0.483 0.300 0.946 0.752 0.780 0.268 0.544 0.372 0.983 0.301 0.997 0.494 0.366 0.947 0.556 0.900 0.740
0.634 0.619

Table 8: Recall@100 on the BEIR benchmark. The best result on a given dataset is marked in bold.

