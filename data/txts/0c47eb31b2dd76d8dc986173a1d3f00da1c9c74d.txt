Efﬁcient Nearest Neighbor Language Models
Junxian He†, Graham Neubig†, Taylor Berg-Kirkpatrick‡ †Language Technologies Institute, Carnegie Mellon University ‡Department of Computer Science and Engineering, University of California San Diego {junxianh, gneubig}@cs.cmu.edu, tberg@eng.ucsd.edu

arXiv:2109.04212v3 [cs.CL] 15 Nov 2021 ppl

Abstract
Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, signiﬁcantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model (Khandelwal et al., 2020) as an example, exploring methods to improve its efﬁciency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efﬁcient non-parametric NLMs.1
1 Introduction
Language models (LMs) are one of the most fundamental technologies in NLP, with applications spanning text generation (Bahdanau et al., 2015; Rush et al., 2015), representation learning (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019), and few-shot learning (Radford et al., 2019; Brown et al., 2020). Modern neural language models (NLMs) based on recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) or selfattentional (Vaswani et al., 2017; Al-Rfou et al., 2019) neural networks are mostly parametric, where the predictions are solely dependent on the model parameters given the input data.
In contrast, recent non-parametric LMs (Guu et al., 2018; Khandelwal et al., 2020; He et al.,
1Code is available at https://github.com/jxhe/efﬁcientknnlm.

19 NLM
18

Datastore

17 Vanilla Compression

kNNLM

Dimension

x Reduction

x x Adaptive

Retrieval

16

300 500

1000

tokens/s

Combined 2000 3000

Figure 1: Perplexity (ppl) and evaluation speed for different models. Circled points represent the neural language model (NLM) and k-nearest neighbors language model (kNN-LM ) baselines respectively, while others are the methods that we propose and explore in this paper.

2020) model text distributions by referencing both the parameters of the underlying model and examples from an external datastore. Non-parametric LMs are appealing since they allow for effective language modeling – particularly for rarer patterns – through explicit memorization via a datastore, which mitigates the burden on model parameters to learn to encode all information from a large dataset. One effective and representative example is the k-nearest neighbors LM (kNN-LM, Khandelwal et al. (2020)). The kNN-LM computes the probability of the next token by interpolating a parametric LM with a distribution calculated from the k nearest context-token pairs in the datastore, as demonstrated in Figure 2. This model is particularly notable for its large improvements in performance – it outperforms the previous best parametric LMs by a large margin in standard language modeling benchmarks, in domain adaptation settings, and on other conditional generation tasks such as machine translation (Khandelwal et al., 2021).
However, one downside to the kNN-LM is that the datastore stores high-dimensional dense vectors

Datastore Pruning

Dimension Reduction
cat Alex Steve …
Bill Datastore Apple was founded by

Index

p k N N < l a t e x i t s h a 1 _ b a s e 6 4 = " b 8 n P u C K B + x x W C 4 f 3 E d T H Z p A U f R o = " > A A A 1 r H i c l V v b c h v L d a X t J H a Y O D 5 2 H v M y F U p l O U W x R J 2 j c i p P 5 g W 8 i C A J k u B F E n R U A 2 B j M O L c O N 0 Y A p w g z / 4 E v 9 p / l b / J 7 u 7 p 2 b s H Q 5 0 K q 0 R O r 7 W 7 p y + r e y 8 M R s M s C o V 8 8 + Z / f / b z X / z d 3 / / D L 3 / 1 j + v / 9 M + / / p f f f P f b 3 9 2 I d J a P 4 H q U R m l + N / Q F R G E C 1 z K U E d x l O f j x M I L b 4 f 2 e 4 m 8 L y E W Y J n 2 5 y O B z 7 A d J O A l H v k T o x + x L e T + Q M J f l 2 d l y + e W 7 j T d b b / S P t 3 q x X V 1 s r F U / v S + / / e H P g 3 E 6 m s W Q y F H k C / F p + 0 0 m P 5 d + L s N R B M v 1 w U x A 5 o / u / Q A + 4 W X i x y A + l 7 r b S + 8 l I m N v k u b 4 L 5 G e R n m N 0 o + F W M R D j I x 9 O R V N T o F t 3 K e Z n P z n 5 z J M s p m E Z G R u N J l F n k w 9 N Q f e O M x h J K M F X v i j P M S + e q O p n / s j i T O 1 v v 5 S / X h n n V v v d K d / 5 O 1 3 D o 7 P j v v H 5 2 d X n q b W 2 z q y i X / V M M T m M F 5 i G 9 6 p n 9 9 7 A u + D 8 y y 8 d O K N / M x c q x H n M I E 8 D 5 N A d W o c F q G w Y Z M w m O W A A 0 r g c Z T G s Z + M y w G C E U z k s i w H E H u v u n j 9 h + V y J W a E 6 w C 5 j d r T p b a 4 P A y m d W O X q t A W J d P M x v T T r C 1 i m E q Z x j Z o V 5 d W 4 q p x + z b M f y 5 i a C O G z 0 W M b M T o u Y i x j R i r C F y G I x x d p E b o + R 7 G q 0 W H C W 6 W s Y d z E 7 t t 4 L U C l 5 + 2 P 2 M r w 4 m 3 s a 0 a a Q 5 7 v i w H s Z 8 H K D A / L w + O 7 5 p 9 w W s n B K X U D O m f 7 5 / r + + j d p 6 V f 5 o C 9 V 8 R / m R u 7 b X Z 0 k 3 K a Z u W g 0 2 Q 7 D 8 h 2 v p S D v H g a i D D 2 H v C 6 y K b h 8 p W C / h t / z V e m r J M 9 N W p l q p a c g v S / X a + u N q + q j V + 1 R j 4 8 4 V y 1 d s W N y 1 T c M z d v R M 6 f V i L n K v J p N X I 1 c C V m r I P G 7 a S 6 0 6 u 2 p h + e m q N q R q i J R b i B z j U 6 b 6 A L j S 4 a a K z R u K k C j S b N 2 J l U 4 p h h l + a b n h 3 x S l D m B K m + N 0 L G p i L e x h 9 G P s 1 d M 0 x V Z U F t L S G 2 0 h h i v H e 4 O Q / 0 W W c O Q z y p Y d O L 0 k f I X 4 8 w o 2 2 t D 3 C n 6 t M K J h v b p T k X / 2 e A p V J v j 7 b q e A q E 0 o + 2 v A M 8 Y 4 X E P K S O V K E O Q u R N i w e 2 x Y N m i 5 q W j 6 m 9 5 8 b b 6 q 7 C s 0 E e D q 8 q v L U 1 H m b + m K p s f L / x w 0 q 1 z b q O v f q e N / W D H s 6 V S R b f n A 5 M K K b z V W Z x 5 q O l A T s h p v a V r X 3 V U v v S 1 t J 5 8 j G t k 9 d W P T H m 7 k L P T J 3 a n p m a Z o P T H K D Z J G t v 4 / v V F m n W W N v f r 7 b t J x 7 g I q j K L V M G D 2 b M N u T 5 Q T v t z L I M c k + 1 Y 5 r p V M 1 0 2 p r Z 8 X L / k e a 9 0 d j r 1 6 / 9 I g 3 H 3 k y o j B 9 O v C w V I k S P Z p r O I h 8 z U t X + 8 7 1 T J i X D B N U y R s W Y 6 l X M / 3 u Q V U N 7 d U N 7 P 9 k Q j j k J Q F s b E y t M G x q u e 4 R S s b R t 6 v X r Z 2 W C v f O j I E V T N o 1 b x o m c 6 V 0 d 9 M 2 B s q Z W R r p j m 9 p p a c o K 3 t 4 P B 1 G 3 9 e 3 D o O 9 U 2 v n J S i u T i o Z B V i N n 6 l O o 6 a 6 6 + t a i m P p N 9 f b q + j 2 3 v h 1 p f Q P s t b p + t s O V 4 C C M l F g j d Y F 2 B Q P U V d X e J E r T X N P 6 y v D 6 s g p A a h i X K y Z H 5 r g R K p 8 z 8 q N y v x l Q + F E 4 5 g F f z H U e l 4 Z a r j Q J Q r Z X 0 M y y H h F k Q l n H T I R R m m j b h 1 O L T a S x V / h 5 i E k M r L 4 x f 5 X G u C V p H m O r L w Y I v V j a 6 c w b t E / M 0 G W G x I x c Z k T M 2 G X G x I D L A D E T l 5 k Q E 7 h M Q M z U Z a b E h C 4 T E v P V Z b 4 S c + 8 y 9 8 R E L h M t t Y z z 2 A s F 7 l j 8 9 D p e q M P O r O C m 9 3 U m p D d O k 9 9 L T 3 1 + R D k u 1 M n j L I w X V 2 0 n b t s J 3 T V 1 m Z S Y z G U y Y h 5 c 5 o G Y 3 G V y Y o T L C G K k y 0 h i Z i 4 z I 6 Z w m Y K Y R 5 d 5 J G b u M n N i F i 6 z I O b J Z Z 6 W x q D Z D Y C Z O a 2 P 9 6 L a J K X Z S s M J 2 z Z 1 v 7 X L Y x G V 6 6 t 5 x n F 4 S D D b G 8 W I Y L Y x i j H B b F c U Q D D b E s W E Y L Y f i o B g t h m K K c F s J x Q z g t k 2 K L 4 S z P Z A c U 8 w 2 w B F R H D E 4 J j g m M F s o v k M p w Q z M R c Z w U z J x Q P B T M Z F T j D T c C E I F n x R C Z b t c 8 K l W x D M d F s 8 E s x E W 8 w J Z o o t F g Q z u R Z P B F u t d i J Q z 6 H 0 Q 5 S 8 R b d g R N d 6 L o N R X u v J D E Z + r W c z G A 2 2 n s 5 g h N h 6 P o N R Y + s J D U a S r W c 0 G F 2 2 n t L I P X t O g 1 F o 6 0 k N R q a t Z z U Y r T Z P a 8 v F L h d z 7 t m T G I x 0 W 8 9 i M P p t P Y 3 B i L j 1 P A a j 5 N Y T G Y y c W 8 9 k M J p u P Z X B C L v 1 X A a j 7 t a T G Y z E W 8 9 m M D p v P Z 3 B i L 3 1 f A a j + O d P a N w L e T i q H U q 8 Q / t j h 7 Z N v E v w L o P 3 C N 5 j 8 D 7 B + w z u E N x h 8 A H B B w w + J P i Q w U c E H z H 4 m O B j B r 8 n + D 2 D T w g + Y X C X 4 C 6 D T w k + Z f A Z w W c M P i f 4 n M E 9 g n s M v i D 4 g s G X B F 8 y + I r g K w b 3 C e 4 z + J r g a w b f E H z D 4 F u C b x l 8 R / A d g z 8 Q / I H B H w n + + P z x 6 o o O j O q Y R n e Y f r X 0 G L f L u T 2 X 2 + P c v s v t c 6 7 j c h 3 O H b j c A e c O X e 6 Q c 0 c u d 8 S 5 Y 5 c 7 5 t x 7 l 3 v P u R O X O + F c 1 + W 6 n D t 1 u V P O n b n c G e f O X e 6 c c z 2 X 6 3 H u w u U u O H f p c p e c u 3 K 5 K 8 7 1 X a 7 P u W u X u + b c j c v d c O 7 W 5 W 4 5 d + d y d 5 z 7 4 H I f O P f R 5 a z s b 7 i F K J 5 A f 4 7 A z 6 5 v 6 r p F m k B p P 8 9 a L J 4 Z a B B T 0 q g 9 s c J d P 6 y e j V a E f p p q 4 S q a B Q 4 N Q v Z E m x N E y J R o S 4 I I W Z G i 6 i A Z E G 0 / E C H b o U 0 H I m Q 2 t N V A h C x G U X W S 9 f C r Q c h O a D O B C J k I b S E Q i d j 0 G I Q M g 7 Y L i C R s W g 2 S s k k y C F k C b Q g Q I S O g b Q A i l P 5 1 8 k d E s H U w C K X 6 o l o t t l a F Q S i t 6 6 S O C C V z n c o R o R S u E z g i l L h 1 2 k a k z a S 6 7 r T w o 2 y q 1 l v / r Y V Z D C v N V A / i D U i f w O i B R U V F f j w c q x r m g o g 0 h k D h + i / B W q l K p R b A B h H B 3 w S J M I h V V f 2 X Y K v n + l u C a i B l y f t f K r H a E o p 1 R C U U 6 p g N q l Q C t S U U 6 I R K K M 6 A S i j M K Z W w u 6 y v K M i v V E I x 3 r O 5 K Z U I 6 5 G X S o C 2 h J P J Z h H F l 7 I p K Z X o b A l F 9 0 A l F F z O Z q p U Q q s n q F Q i s y W c a D b N K L C C S i i u R y q h s O Z U Q l E t q I S C e l p W 3 z B j + p 0 b X K d e 1 B m l X J 1 w E a F E q 9 M s I p R e d X J F h J K q T q m I U C r V i R Q R S q A 6 f S J C a V M n T U Q o W e p U i Q i l S J 0 g E a H E q N M i I p Q O d T J E h J K g T o G I U O r T i Q 8 R S n g 6 3 S F C a U 4 n O U Q o u e n U h g i l N J 3 Q E K F E p t M Y I p S + d P J C h J K W T l m I U K r S i Q o R S l A 6 P S F C a U k n J U Q o G e l U h A i l I J 2 A E P n I V p D S x Z B n i 7 h X Z 4 s e y x Z x 1 2 5 9 x X S r 7 V 8 P r t r D i r s y + 1 i r q A + J U O 9 d 7 M M o 8 n N A U U 1 3 1 A m E d z Q e U E x C 9 Q Q V k l E 6 D p M A G / N n k U L E p L 6 O l 6 V Q D 3 + v Q D 7 X w D C N x j / V z H C + L J t f b k r s n / m m X K f T q j 3 9 8 L o a m j S 2 M x F M / X L X Y q R / u W c x 2 g F y 3 2 K 0 B 2 T H Y r Q L 5 I H F a B / I Q 4 v R T p B H F q O 9 I I 8 t R r t B v r c Y 7 Q d 5 Y j H a E b J r M d o T 8 t R i t C v k m c V o X 8 h z i 9 H O k D 2 L 0 d 6 Q F x a j 3 S E v L U b 7 Q 1 5 Z j H a I 7 F u M 9 o i 8 t h j t E n l j M d o n 8 t Z i t F P k n c V o r 8 g P F q P d I j 9 a z B g 1 F P J h 7 m d T w w b 2 4 + / I + R Q S 7 D K Y d B H s M Z i k E e w z m N Q R d B h M A g k O G E w a C Q 4 Z T D I J j h h M S g m O G U x i C d 4 z m P Q S n D C Y J B N 0 G U y q C U 4 Z T M I J z h h M 2 g n O G U z y C X o M J g U F F w w m E Q W X D C Y d B V c M J i k F f Q a T m o J r B p O g g h s G k 6 a C W w a T r I I 7 B p O y g g 8 M J n E F H x l s P w j g 0 V Z Z N V E / X B k y c Y l d Q k l b Y o 9 Q k p b Y J 1 Q r 6 6 W 3 r 7 / g m A n w f E + A 9 P D W E Y y 9 z q Y 3 h J G v c D k N h f e Y z q I x Q l g C T + i v Q 9 B L z n J P v S i X R t i Q e r s M 5 h l 6 S / 0 d r / 2 m v U N 3 J N G K A 0 J J s + K Q U J K s O C K U F C u O C S X B i v e E k l 7 F C a E k V 9 E l l N Q q T g k l s Y o z Q k m r 4 p x Q k q r o E U p K F R e E k l D F J a G k U 3 F F K M l U 9 A k l l Y p r Q k m k 4 o Z Q 0 q i 4 J Z Q k K u 4 I J Y W K D 4 S S Q M V H Q u v n M w m 6 Q d A f L H z z Z K a y h k C + o O t + J F C m c Y d K K O B d K q F w 9 6 i E g t 2 n E o q p Q y U U 0 Q G V U D y H V E L R H F E J x X J M J R T J e y q h O E 6 o h K L o U g n F c E o l F M E Z l X D x z 6 m E i 9 6 j E i 7 2 B Z V w k S + p h I t 7 R S V c 1 D 6 V c D G v q Y S L e E M l X L x b K u G i 3 V E J F + s D l X C R P r L 7 V f 6 r 8 l 5 q y Y A v m T Q + D A 8 a t a v 1 G 7 G 4 t Q 2 6 6 T 2 G c p r O p I c m y H v E R J d B 7 t o k I J / k e K T q 9 r L W g A 5 c s Y e g T R Q 0 X B R o G w U N H w X a S E H D S Y G 2 U t D w U q D N F D T c F G g 7 B Q 0 / B d p Q Q c N R g b Z U 0 P B U o E 0 V N F w V a F s F D V 8 F 2 l h B w 1 m B t l b Q 8 F a g z R U 0 3 B V o e w U N f w X a Y E H D Y Y G 2 W N D w W K B N F j R c F m i b B Q 2 f B d p o Q c N p g b Z a 0 P B a o M 0 W N N w W a L s F D b 8 F 2 n B B w 3 G B t l z Q 8 F y g T R c 0 X B d o 2 w X M d + H n B 0 x E M p + B N 0 v G k E c L 9 Y b T 2 J e + F 0 A C O e Y g V Q 4 F K n 0 4 U w m p + Q a m r 1 4 S V C 9 q 5 n G p C z o d q l Y h z s I 8 x E T o 1 K / f 3 x 0 u d B L U 7 4 y o m 2 D W b L R t X y e Z + h I / v 7 u 3 c C J 7 P L K 3 b O t M n I 4 h + t Z A d E A 9 E l N a u U 8 V 1 P t W U C b D a A x V 5 E A X 6 t 7 X N f C Y k O l o 6 g v 1 8 r o / k 6 n + X A W 5 0 8 P G S + S Z i a n 7 W F V Z 7 c A Y n D h T b I n L k c B D x 8 a Z I m p h p J + s u c G R n 0 X + C J b 1 6 z f d C l h 6 L 7 3 q 2 p 3 e x k v H S + 5 f X K 4 r 2 B s + 3 S Z 7 u e S 5 v X F q x h m b 4 w Y Z 5 U v 7 N M 4 l c g i W 9 f O 1 J j W S N E Z V C i c h 5 M 2 m R T q R s T + n S A s 0 4 z B Z p P q N J / P o b b W V L J q p 0 T + p 5 w M u e 9 J d 8 t e d T r o r C 3 j j 5 9 Q D V W i 2 L / G P n + P a 5 y m L v F p Z g L 2 0 I F o V l E B v 0 2 i S + 7 F 6 T D V 9 T H O 0 r c J f C O 9 F 9 8 e 3 L 9 S r P v o / f s w S 8 8 q q y H D 9 h X 7 V 7 M U A o o j F 2 M e k L 7 1 d T I C 4 5 R P 1 a 4 H 7 H W L 1 y p v y x q Z R F q 3 e N 0 1 n g c 6 Z 2 i q H E j Z 1 8 y L 1 x i m o 5 h 7 D + z C D c e h v N V 6 y T v M 4 U k / 6 l 2 X 3 x z f L F j J N Q H H b b Z x 8 1 P X e t n G Z Y r I W R m u h + + M g T C Z y 0 d w 6 m Z + r R 8 Z 4 b P h q s 1 w B n r X C D 8 A L E y 9 J K 5 s v Y b 7 l 7 U 1 T o a Y n V Q Z w N P X 2 8 R N x A r 8 X 3 j B N 7 7 f W n Y c 8 5 5 k 6 n d P 8 P 1 D j e a A 7 g H 8 H m + r q W 4 H q n D S B e N X e p F Y r h u n f z 0 T 0 U V B 9 9 T p g B H L g D 3 G f R e n j M A f / f t 1 g q T o E M 7 n A Y 3 1 g L w x T w 4 W f 4 / i n e P i v r 3 / 5 b m O 7 + T + V V i 9 u 3 m 5 t v 9 n a v v h h 4 0 + 7 1 f 9 i + t X a v 6 3 9 + 9 q r t e 2 1 P 6 7 9 a e 1 o r b d 2 v T Z a y 9 f + s v b X t b + 9 2 3 r X f / f p 3 W c T + v O f V X X + d c 3 5 e T f 5 P x 0 R 0 B o = < / l a t e x i t >

query

p
< l a t e x i t s h a 1 _ b a s e 6 4 = " 0 I Z A 8 q I s 4 y t Y 0 X N A E o H 9 F 4 p C y y U = " > A A A 1 0 X i c l V t b b + N K c v Z u b h s n m 5 x N H v N C x D P Y 2 V 2 P M Z 5 z B g k C B F h f 5 M t Y v k q + z B z N G V B S i e K Y N 7 N b t G R G i 8 U + L Z A f k F + T 1 + Q 3 5 N 9 s d T e b V U 3 R c x A f H J v 9 f d X F v n z V V a I 4 w y w K h X z z 5 v 9 + 8 t M / + / O / + M u / + t l f r / / N 3 / 7 8 7 / 7 + m 1 / 8 w 4 1 I Z / k I r k d p l O Z 3 Q 1 9 A F C Z w L U M Z w V 2 W g x 8 P I 7 g d 3 u 8 p / r a A X I R p 0 p e L D D 7 F f p C E k 3 D k S 4 Q + f 7 O d e f / u D f C / 7 H N 5 P 5 A w l + X Z 2 X L p / c Z 7 t e 2 9 1 s y v k K q Y 7 u l y + f m b j T d b b / S P t 3 q x X V 1 s r F U / F 5 9 / 8 d 0 f B + N 0 N I s h k a P I F + L 7 7 T e Z / F T 6 u Q x H E S z X B z M B m T + 6 9 w P 4 H i 8 T P w b x q d R z W 3 o v E R l 7 k z T H / x P p a Z T 3 K P 1 Y i E U 8 R M v Y l 1 P R 5 B T Y x n 0 / k 5 N / / V S G S T a T k I z M j S a z y J O p p x b K G 4 c 5 j G S 0 w A t / l I c 4 V m 8 0 9 X N / J H E 5 1 9 d f q h / v r H P r n e 7 0 j 7 z 9 z s H x 2 X H / + P y s 5 2 l q v W 0 g m / h X T U N s D u M l + v B O / f z e E 3 g f 3 A z h p R N v 5 G f m W s 0 4 h w n k e Z g E a l D j s A i F N Z u E w S w H n F A C j 6 M 0 j v 1 k X A 4 Q j G A i l 2 U 5 g N h 7 1 c X r X y 2 X K z Y j 3 A f I r d W e b r X Z 5 W E w r Z 1 d q U a b l U w z a 9 N P s z a L Y S p l G l u j X d 1 a s a v m 7 V s z / z m L o b U Y P m c x s h a j 5 y z G 1 m K s L H A b j n B 2 k Z q h 5 3 t o r z Y d J h h R Y w / X J n Z 9 4 L U C l 9 9 v f 0 I v w 4 m 3 s a 2 c N K c 9 X 5 a D 2 M 8 D F J i f l w f H d 8 2 x 4 L V j g l J q m v T P 9 8 / 1 f X T 4 a e m X O e D o F f F v 5 s a u z 4 5 2 K a d p V g 4 6 T b b z g G w H g z k v n g Y i j L 0 H v C 6 y a b h 8 p a D / w F / z l S X r Z E + N X u o 4 K O Q U p P / 1 f n W 3 e d V t / K r V 8 u E J 1 6 p 1 K K 5 d p u y e u X n D c v 6 0 Y j l X l k + r l q u G K z Z j b T R u J 9 W d X r W 5 f n h q z q p p o R Y W 4 Q Y 6 1 + i 8 g S 4 0 u m i g s U b j p g o 0 m j R t Z 1 K J Y 4 Z D m m 9 6 d s Y r R p l j p M b e M B m b j n g b f x j 5 t H Z N M 9 W V G b V 5 Q m z F G W J 8 d B i c B / q s M 4 c h n t S w 6 U X p I + S v R 5 j 2 t t Y H G K n 6 t I L J x n Z p z s X f D b B V 6 v B o 6 4 6 n Q C j 9 a M s 7 w D N W S M x D 6 k g V 6 i B E 3 n g 8 s B 4 P m h 4 1 L R 9 T e 8 + N t 9 V d h W e N P J x e 1 X h r e z z M / D F 1 2 f h 2 4 7 u V b p t 1 H 3 v 1 L X f 1 n Z 5 O z y S L r y 4 H J h Q z + C q z O O v R 4 s A u i O n d s 7 1 7 L b 2 v b C + d J x / T O n l t 1 Q t j 7 i 7 0 y t S p 7 Z m l a T q c 5 g B N l 8 z f x r e r H m n V m O 9 v V 3 3 7 i Q e 4 C a p z y 5 L B g 5 m z N X l + 0 o 6 f W Z Z B 7 i k / x k 2 n c t N p c 7 P j 5 f 4 j r X v D 2 e v X r / 0 i D c f e T K i M H 0 6 8 L B U i x E L O u M 4 i H z N S 5 f / 5 0 a k i J c M E 1 T J H x Z j u l c 3 / e 5 K V o 7 3 a 0 d 6 P O s I 5 J w H o 0 s b Y C u N D w / W I U C q W t q 5 e v 3 5 W J j g 6 P w p S L M q m c c s 8 k T O j q 4 2 + O l H m a m W m O 9 b V T o s r K 3 h 7 P 5 x E 7 e v r h 0 H f 6 b T z o 5 1 W F h U L B l n N n K l P o W a 4 6 u p r m 2 L 6 N 9 V 7 U f e / c P v b m d Y 3 w F G r 6 2 c H X A k O w k i J N V I X W K 6 g g b q q / E 2 i N M 0 1 r a 8 M r y 8 r A 6 S G c b l S 5 M g c A 6 G q c 0 Z + V O 4 3 D Q o / C s f c 4 L O 5 z u P S U M s V l y B k e w f N L O s Z Q S Z U 6 Z i J M E o T X f b h 0 q K L N P Y K P w 8 x i Y H V N + a v 0 h R u S Z r H 6 P X F A K E X S 7 u c e Y P 2 i R m 6 z J C Y k c u M i B m 7 z J g Y c B k g Z u I y E 2 I C l w m I m b r M l J j Q Z U J i v r j M F 2 L u X e a e m M h l o q W W c R 5 7 o c C I x Y + 4 4 4 U 6 7 M w O b n p f Z k J 6 4 z T 5 p f T U 5 0 e U 4 0 K d P M 7 G e H H l O 3 F 9 J 3 T X 1 G V S Y j K X y Y h 5 c J k H Y n K X y Y k R L i O I k S 4 j i Z m 5 z I y Y w m U K Y h 5 d 5 p G Y u c v M i V m 4 z I K Y J 5 d 5 W p o C z Q Y A Z u a 0 P t 6 L K k h K E 0 r D C Q u b e t y 6 y m M W V d V X 8 4 z j 8 J B g F h v F i G A W G M W Y Y B Y V B R D M Q q K Y E M z i o Q g I Z s F Q T A l m k V D M C G Z h U H w h m M V A c U 8 w C 4 A i I j h i c E x w z G C 2 0 H y F U 4 K Z m I u M Y K b k 4 o F g J u M i J 5 h p u B A E C 7 6 p B M v 2 N e H S L Q h m u i 0 e C W a i L e Y E M 8 U W C 4 K Z X I s n g q 1 W O x G o 5 1 D 6 I U r e o l s w o m s 9 l 8 E o r / V k B i O / 1 r M Z j A Z b T 2 c w Q m w 9 n 8 G o s f W E B i P J 1 j M a j C 5 b T 2 n k n j 2 n w S i 0 9 a Q G I 9 P W s x q M V p u n t e V i l 4 s 5 9 + x J D E a 6 r W c x G P 2 2 n s Z g R N x 6 H o N R c u u J D E b O r W c y G E 2 3 n s p g h N 1 6 L o N R d + v J D E b i r W c z G J 2 3 n s 5 g x N 5 6 P o N R / P M n N M Z C H o 7 q C i X e o f j Y o b C J d w n e Z f A e w X s M 3 i d 4 n 8 E d g j s M P i D 4 g M G H B B 8 y + I j g I w Y f E 3 z M 4 P c E v 2 f w C c E n D O 4 S 3 G X w K c G n D D 4 j + I z B 5 w S f M / i C 4 A s G X x J 8 y e A r g q 8 Y 3 C O 4 x + A + w X 0 G X x N 8 z e A b g m 8 Y f E v w L Y P v C L 5 j 8 A e C P z D 4 I 8 E f n z 9 e X d G B U R 3 T 6 A 7 T r 5 Y e 4 3 Y 5 t + d y e 5 z b d 7 l 9 z n V c r s O 5 A 5 c 7 4 N y h y x 1 y 7 s j l j j h 3 7 H L H n H v v c u 8 5 d + J y J 5 z r u l y X c 6 c u d 8 q 5 M 5 c 7 4 9 y 5 y 5 1 z 7 s L l L j h 3 6 X K X n L t y u S v O 9 V y u x 7 m + y / U 5 d + 1 y 1 5 y 7 c b k b z t 2 6 3 C 3 n 7 l z u j n M f X O 4 D 5 z 6 6 n J X 9 D S 8 h i i f Q n y P w s + u b u m + R J l D a z 7 M W i 2 c G G s S U N O q a W O F u P a y e j V a E f p p q 4 c q a G Q 4 N Q u W J L k 4 Q o a J E l y S I U C l S V A O k A k S X H 4 h Q 2 a G L D k S o 2 N C l B i J U Y h T V I N k I v x i E y g l d T C B C R Y Q u I R C J 2 P I Y h A o G X S 4 g k r B l N U j K F s k g V B L o g g A R K g R 0 G Y A I p X + d / B E R b B 8 M Q q m + q H a L 7 V V h E E r r O q k j Q s l c p 3 J E K I X r B I 4 I J W 6 d t h F p K 1 L d 6 r T w o 2 y q 9 l v / r Y V Z D C v N V A / i D U i f w O i B R U V F f j w c q x 7 m g o g 0 h k D h + i / B W q l K p R Z A h 4 j g b 4 J E G M S q q / 5 L s N V z / S 1 B N Z G y 5 O M v l V h t C 8 U 6 o h Y K d c w m V S q B 2 h Y K d E I t F G d A L R T m l F o 4 X D Z W F O Q X a q E Y 7 9 n a l E q E 9 c x L J U D b w s V k q 4 j i S 9 m S l E p 0 t o W i e 6 A W C i 5 n K 1 U q o d U L V C q R 2 R Y u N F t m F F h B L R T X I 7 V Q W H N q o a g W 1 E J B P S 2 r b 5 g x / c 4 N r l M v 6 o x S r k 6 4 i F C i 1 W k W E U q v O r k i Q k l V p 1 R E K J X q R I o I J V C d P h G h t K m T J i K U L H W q R I R S p E 6 Q i F B i 1 G k R E U q H O h k i Q k l Q p 0 B E K P X p x I c I J T y d 7 h C h N K e T H C K U 3 H R q Q 4 R S m k 5 o i F A i 0 2 k M E U p f O n k h Q k l L p y x E K F X p R I U I J S i d n h C h t K S T E i K U j H Q q Q o R S k E 5 A i H x k O 0 j p Y s i z R X x R Z 4 s L l i 3 i r g 1 9 x X S r 8 K 8 n V 8 W w 4 n o m j r W K + p A I 9 d 7 F P o w i P w c U 1 X R H n U B 4 R 1 M D i k m o n q B C M k r H Y R K g M 3 8 W K U R M 6 u t 4 W Q r 1 8 L c H 8 j k H w z Q a / 5 i b 4 X x Z N r / c l D g + 8 0 2 5 T q e V P / 3 w u p q a N G V n I p j 6 5 a 7 F S P 9 y z 2 I U A X L f Y h Q D s m M x i g J 5 Y D G K A 3 l o M Y o E e W Q x i g V 5 b D G K B v n e Y h Q P 8 s R i F B G y a z G K C X l q M Y o K e W Y x i g t 5 b j G K D H l h M Y o N e W k x i g 5 5 Z T G K D 9 m z G E W I 7 F u M Y k R e W 4 y i R N 5 Y j O J E 3 l q M I k X e W Y x i R X 6 w G E W L / G g x U 6 i h k A 9 z P 5 s a N r A f f 0 f O p 5 B g l 8 G k i 2 C P w S S N Y J / B p I 6 g w 2 A S S H D A Y N J I c M h g k k l w x G B S S n D M Y B J L 8 J 7 B p J f g h M E k m a D L Y F J N c M p g E k 5 w x m D S T n D O Y J J P c M F g U l B w y W A S U X D F Y N J R 0 G M w S S n o M 5 j U F F w z m A Q V 3 D C Y N B X c M p h k F d w x m J Q V f G A w i S v 4 y G D 7 Q Q C P t q p U E / X D l S E T l 9 g l l L Q l 9 g g l a Y l 9 Q r W y X n r 7 + g u O m Q D P 9 w R I D 2 8 d w d j r b H p D G P k K l 9 N Q e I / p L B o j h C 3 w h P 4 6 B G v J W e 6 p F + X S C B 2 p t 8 t g n m F t q b / j t d + 0 d + i O J F p x Q C h p V h w S S p I V R 4 S S Y s U x o S R Y 8 Z 5 Q 0 q s 4 I Z T k K r q E k l r F K a E k V n F G K G l V n B N K U h U X h J J S x S W h J F R x R S j p V P Q I J Z m K P q G k U n F N K I l U 3 B B K G h W 3 h J J E x R 2 h p F D x g V A S q P h I a P 1 8 J s F q E P Q H C 9 8 8 m a l K Q 6 C 6 o O t + J F B F 4 w 6 1 U M C 7 1 E L h 7 l E L B b t P L R R T h 1 o o o g N q o X g O q Y W i O a I W i u W Y W i i S 9 9 R C c Z x Q C 0 X R p R a K 4 Z R a K I I z a u H m n 1 M L N / 2 C W r j Z l 9 T C T b 6 i F m 5 u j 1 q 4 q X 1 q 4 W Z e U w s 3 8 Y Z a u H m 3 1 M J N u 6 M W b t Y H a u E m f W T 3 q + q v q v Z S W w Z 8 y 6 S p w / C g U V G t 3 4 j F 0 D b o p v c Y y m k 6 k x 4 W Q d 4 j J r o M c r d M A q q T n B q p u r 2 s N a A N V 8 p D 0 E U U N K o o 0 G U U N O o o 0 I U U N C o p 0 K U U N G o p 0 M U U N K o p 0 O U U N O o p 0 A U V N C o q 0 C U V N G o q 0 E U V N K o q 0 G U V N O o q 0 I U V N C o r 0 K U V N G o r 0 M U V N K o r 0 O U V N O o r 0 A U W N C o s 0 C U W N G o s 0 E U W N K o s 0 G U W N O o s 0 I U W N C o t 0 K U W N G o t 0 M U W N K o t 0 O U W N O o t 0 A U X N C o u 0 C U X N G o u 0 E U X N K o u 0 G U X s L o L P z 9 g I p L 5 D L x Z M o Y 8 W q g 3 n M a + 9 L 0 A E s g x B 6 l 2 K F D p w 5 l K S M 0 3 M H 3 1 k q B 6 U T O P S 9 3 Q 6 V B 5 h T g L 8 x A T o d O / f n 9 3 u N B J U L 8 z o m 6 C W b P h 2 7 5 O M v U l f n 5 3 b + F Y X n D L i 2 X b Y O J 0 D N H X J q I N 6 p m Y 1 s p 9 K q O L r x l l M o z G U F k O d K M e f d 0 D j w m Z j q a + U C + v + z O Z 6 s 9 V k D s j b L x E n h m b e o x V l 9 U B j M G x M 8 0 W u x w J P H S s n W m i F k b 6 y Z p r H P l Z 5 I 9 g W b 9 + 0 6 2 A p f f S q 6 7 d 5 W 2 8 d L z k 9 Y v L d Q V 7 w 6 f b Z K + W P L c 3 T s 0 4 Y 2 v c I K N 8 a Z / G u U Q O w b J + v t a k R p L m q F r h J I S 8 6 V q k E x n 7 c 7 K 0 Q N M O k 0 W q 3 3 g y j 9 5 W v W T R T M 3 + S T 0 f c N m T 7 p K / 7 n T S X d n A G z + n E a h G 0 7 / E P 3 6 O e 5 + n z L K 3 s g F 7 a U G 0 a i i B 3 q b R J P d j 9 Z h q + p j m W L Y K f y G 8 F 9 0 f 3 r 5 Q r / r o f / g x S 8 w r q y L D / R f 6 V b M X A 4 g i Z m M f k 7 7 0 d j E B Y s g n 6 t c C 4 x 1 i 9 c q b q o 2 N U 2 a t 3 j d N Z 4 H O m b p U D i V s a v c i 9 c Y p K H e P 4 X 2 Y w T j 0 t x o v W a d 5 H K k n / c u y + 8 O b Z Q u Z J q C 4 7 T Z O P u p + b 9 u 4 T D F Z C 6 O 1 0 P 1 h E C Y T u W i G T u b n 6 p E x H h u + C p Y e 4 F k r / A C 8 M P G S t C r z J c y 3 v L 1 p K t T y p K o A H E 2 9 f f x E n M A v h T d M 0 / u t d e c h z 3 m m T u c 0 / z V q P A / 0 A P D v Y F N d f c 1 Q n Z P G E K / a X W q 1 o p n + / Y x F H w X V V 6 8 D R i A H / h D j L E o f h z n 4 9 + s G S 9 U h m M k F H u s D e 2 G Y G i 7 8 H O c / x c N / f f 3 z N x v b z X + p t H p x 8 3 Z r + 8 3 W 9 u V 3 G 7 / d r f 4 V 0 8 / W / m n t n 9 d e r W 2 v / c v a b 9 e O 1 i 7 W r t d G a / + 1 9 t 9 r / 7 P 2 v + 9 6 7 x b v f v / u D 8 b 0 p z + p + v z j m v P z 7 j / / B E y l 2 6 c = < / l a t e x i t >

= < l a t e x i t s h a 1 _ b a s e 6 4 = " M a 5 d m B C L e H Q V E k + 1 g 0 x r 7 6 f 4 m 8 4 = " > A A A 1 p X i c l V t b d 9 v K d V a S N k n V J j 1 J H v O C V d n r u F m y l m Q f n y R 9 i i 7 U x a L u 1 M U + 9 P E C w U 0 Q F m 7 C D C F S L P v c 1 7 6 2 v 6 z / p n t m M N h 7 Q M h n V W t Z w n z f n s F c v p n 9 E Y Q H e R w J u b n 5 v z / 7 + S / + 7 u 9 / + a t f / 8 P q P / 7 T b 3 7 7 z 9 / 8 7 v c 3 I p s U A V w H W Z w V d w N f Q B y l c C 0 j G c N d X o C f D G K 4 H d z v K v 6 2 h E J E W d q T s x w + J X 6 Y R q M o 8 C V C t / 0 Y Q 4 f + 5 2 / W N j f + u v n m 7 d t N b 3 N j c / P P b z a / V x f f b 7 7 9 y / f e F i L q Z 2 2 l + j n / / L v v / r M / z I J J A q k M Y l + I H 7 Y 2 c / l p 7 h c y C m J Y r P Y n A n I / u P d D + A E v U z 8 B 8 W m u + 7 v w X i I y 9 E Z Z g f 9 S 6 W m U 1 5 j 7 i R C z Z I C R i S / H o s k p s I 3 7 Y S J H f / k 0 j 9 J 8 I i E N z I 1 G k 9 i T m a c G 7 w 2 j A g I Z z / D C D 4 o I + + o F Y 7 / w A 4 l T t L r 6 U v 1 4 p 5 1 b 7 2 S 7 d + j t d f a P T o 9 6 R 2 e n V 5 6 m V t s 6 s o 5 / 1 T D E + i B Z Y B v e i V / c e w L v g x M s v G z k B X 5 u r t W I C x h B U U R p q D o 1 j M p I 2 L B R F E 4 K w A G l 8 B h k S e K n w 3 k f w R h G c j G f 9 y H x X n X x + l 8 X i 6 W Y A N c B C h u 1 q 0 t t c U U U j u v G L l W h L U p m u Y 3 p Z X l b x C C T M k t s 0 I 4 u L c V V 4 / Z t m P 9 c x M B G D J 6 L C G x E 8 F z E 0 E Y M V Q Q u w y G O L l Y j 9 H w P 4 9 W i w w h 3 y d D D u U n c N v B a g Y s f t j 5 h K 4 O R t 7 a l G m k O e 7 q Y 9 x O / C F F g f j H f P 7 p r 9 g W v n R C U U j O k d 7 Z 3 p u / T l z C V W v r z A r D 3 i v g 3 c 2 O 3 z Y 5 u U o 6 z f N 7 v N N n O A 7 K d z / N + U T 7 1 R Z R 4 D 3 h d 5 u N o 8 U p B / 4 6 / p k t T 1 s m f G r V y V U u O Q f p f r 1 d X m 1 b V h q 9 a I x + e c K 5 a u + L G 5 S r u m Z s 3 I q d P S 5 F T F f m 0 H L k c u B Q z 1 E H D d l L d 6 V V b 0 w 9 P z V E 1 I 9 T E I t x A p x q d N t C Z R m c N N N F o 0 l S B R t N m 7 E Q q c U y w S 9 N 1 z 4 5 4 K S h 3 g l T f G y F D U x F v 4 w 9 i n + a u G a a q s q C 2 l h B b a g w x 3 j v c n P v 6 r D O H I Z 7 U s O 7 F 2 S M U r w N M Z R u r f d y p + r S C 0 d r W 3 J y L / 9 H H 0 l x v j 7 b q e A p E 0 o 8 3 v H 0 8 Y 4 X E P K S O V K E O Q u R N i / u 2 x f 1 m i 5 q W j 5 m 9 5 9 q b 6 q 7 C s 0 E e D q 8 q v L E 1 H i b + k K q s v V 3 7 b q n a e l 3 H X r 3 l T X 2 n h 3 N l k s V X p w M T i u l 8 l V m c + W h p w E 6 I q X 1 l a 1 + 1 1 L 6 0 t X S e f M z q 5 L V R T 4 y 5 u 9 A z U 6 e 2 Z 6 a m 2 e C 4 A G g 2 y d p b e 7 v c I s 0 a a / v t c t t + 6 g E u g q r c M m X w Y M Z s Q 5 4 f t N P O J M + h 8 F Q 7 p p l O 1 U y n r Z l t r / A f a d 4 b j b 1 + / d o v s 2 j o T Y T K + N H I y z M h I j R n p u k 8 9 j E j V e 0 / 3 z t l U n J M U C 1 j V I y p X s X 8 v w d Z N b R b N 7 T 7 k w 3 h m N M Q t L U x s c K 0 o e G 6 R y g V S 9 u m X r 9 + V i b Y O z 8 O M z R l 4 6 R l n M i Z 3 t V B X x 0 o a 2 p p p N u 2 q e 2 W p q z g 7 f 1 w E H V b X z 8 M e k 6 l 7 Z + s t D S p a B h k N X K m P o W a 7 q q r r y 2 K q d 9 U 7 3 l d / 9 y t b 0 d a 3 w B 7 r a 6 f 7 X A l O I h i J d Z Y X a B d w Q B 1 V b U 3 i r O s 0 L S + M r y + r A K Q G i T z J Z M j C 9 w I l c 8 J / H i + 1 w w o / T g a 8 o D P 5 r p I 5 o Z a L D U J Q r Z X 0 M y i H h H k Q l n H X E R x l m r b h 1 O L T W S J V / p F h E k M r L 4 x f 8 2 N c U u z I s F W X / Q R e r G w 0 1 k 0 a J + Y g c s M i A l c J i B m 6 D J D Y s B l g J i R y 4 y I C V 0 m J G b s M m N i I p e J i P n i M l + I u X e Z e 2 J i l 4 k X W s Z F 4 k U C d y x + b B 3 O 1 G F n V n D d + z I R 0 h t m 6 b f S U 5 8 f U Y 4 z d f I 4 C + M l V d u p 2 3 Z K d 8 1 c J i M m d 5 m c m A e X e S C m c J m C G O E y g h j p M p K Y i c t M i C l d p i T m 0 W U e i Z m 6 z J S Y m c v M i H l y m a e F M W h 2 A 2 B m z u r j v a w 2 y d x s p c G I b Z u 6 3 9 r l s Y j K 9 d U 8 4 z g 8 I J j t j T I g m G 2 M c k g w 2 x U l E M y 2 R D k i m O 2 H M i S Y b Y Z y T D D b C e W E Y L Y N y i 8 E s z 1 Q 3 h P M N k A Z E x w z O C E 4 Y T C b a D 7 D G c F M z G V O M F N y + U A w k 3 F Z E M w 0 X A q C B V 9 U g m X 7 n H D p l g Q z 3 Z a P B D P R l l O C m W L L G c F M r u U T w V a r n R j U c y j 9 E K V o 0 S 0 Y 0 b W e y 2 C U 1 3 o y g 5 F f 6 9 k M R o O t p z M Y I b a e z 2 D U 2 H p C g 5 F k 6 x k N R p e t p z R y z 5 7 T Y B T a e l K D k W n r W Q 1 G q 8 3 T 2 n K J y y W c e / Y k B i P d 1 r M Y j H 5 b T 2 M w I m 4 9 j 8 E o u f V E B i P n 1 j M Z j K Z b T 2 U w w m 4 9 l 8 G o u / V k B i P x 1 r M Z j M 5 b T 2 c w Y m 8 9 n 8 E o / v k T G v d C E Q W 1 Q 0 m 2 a X 9 s 0 7 Z J d g j e Y f A u w b s M 3 i N 4 j 8 E d g j s M 3 i d 4 n 8 E H B B 8 w + J D g Q w Y f E X z E 4 P c E v 2 f w M c H H D O 4 S 3 G X w C c E n D D 4 l + J T B Z w S f M f i c 4 H M G X x B 8 w e B L g i 8 Z f E X w F Y N 7 B P c Y f E 3 w N Y N v C L 5 h 8 C 3 B t w y + I / i O w R 8 I / s D g j w R / f P 5 4 d U U H R n V M o 9 t M v 1 p 6 j N v h 3 K 7 L 7 X J u z + X 2 O N d x u Q 7 n 9 l 1 u n 3 M H L n f A u U O X O + T c k c s d c e 6 9 y 7 3 n 3 L H L H X O u 6 3 J d z p 2 4 3 A n n T l 3 u l H N n L n f G u X O X O + f c h c t d c O 7 S 5 S 4 5 d + V y V 5 z r u V y P c 9 c u d 8 2 5 G 5 e 7 4 d y t y 9 1 y 7 s 7 l 7 j j 3 w e U + c O 6 j y 1 n Z 3 3 A L U T 6 B / h y B n 1 0 3 6 7 p l l s L c f p 6 1 W D I x U D + h p F F 7 Y o W 7 f l g 9 G 6 0 I / T T V w l U 0 C x w Y h O y J N i e I k C n R l g Q R s i J l 1 U E y I N p + I E K 2 Q 5 s O R M h s a K u B C F m M s u o k 6 + E X g 5 C d 0 G Y C E T I R 2 k I g E r P p M Q g Z B m 0 X E E n Z t B o k Y 5 N k E L I E 2 h A g Q k Z A 2 w B E K P 3 r 5 I + I Y O t g E E r 1 Z b V a b K 1 K g 1 B a 1 0 k d E U r m O p U j Q i l c J 3 B E K H H r t I 1 I m 0 l 1 3 W n p x / l Y r b f + W w u z H F S a q R 7 E G 5 A + g d E D i 4 o y X + U i Y y 6 I y B I I F a 7 / E q y V q l R q A W w Q E f x N k I j C R F X V f w m 2 e q 6 / J a g G M p / z / s + V W G 0 J x R p Q C Y U 6 Z I O a K 4 H a E g p 0 R C U U Z 0 g l F O a Y S t h d 1 l c U 5 B c q o R j v 2 d z M l Q j r k c + V A G 0 J J 5 P N I o o v Y 1 M y V 6 K z J R T d A 5 V Q c A W b q b k S W j 1 B c y U y W 8 K J Z t O M A i u p h O J 6 p B I K a 0 o l F N W M S i i o p 0 X 1 D T O m 3 6 n B d e p F n V H K 1 Q k X E U q 0 O s 0 i Q u l V J 1 d E K K n q l I o I p V K d S B G h B K r T J y K U N n X S R I S S p U 6 V i F C K 1 A k S E U q M O i 0 i Q u l Q J 0 N E K A n q F I g I p T 6 d + B C h h K f T H S K U 5 n S S Q 4 S S m 0 5 t i F B K 0 w k N E U p k O o 0 h Q u l L J y 9 E K G n p l I U I p S q d q B C h B K X T E y K U l n R S Q o S S k U 5 F i F A K 0 g k I k Y 9 s B S l d D H i 2 S M 7 r b H H O s k X S t V t f M d 1 q + 9 e D q / a w 4 q 7 M P t Y q 6 k E q 1 H s X e x D E f g E o q v G 2 O o H w j s Y D i l G k n q B C G m T D K A 2 x M X 8 S K 0 S M 6 u t k M R f q 4 e 8 V y O c a G G T x 8 K e a G U w X 8 + a X m x L 7 Z 7 4 p 1 + m 0 a k 8 / v K 6 G J o 3 t T A V T v 9 y x G O l f 7 l q M d o D c s x j t A d m x G O 0 C u W 8 x 2 g f y w G K 0 E + S h x W g v y C O L 0 W 6 Q 7 y 1 G + 0 E e W 4 x 2 h O x a j P a E P L E Y 7 Q p 5 a j H a F / L M Y r Q z 5 L n F a G / I C 4 v R 7 p C X F q P 9 I a 8 s R j t E 9 i x G e 0 R e W 4 x 2 i b y x G O 0 T e W s x 2 i n y z m K 0 V + Q H i 9 F u k R 8 t Z o w a C v m g 8 P O x Y U P 7 8 T d w P o W E O w w m X Y S 7 D C Z p h H s M J n W E H Q a T Q M J 9 B p N G w g M G k 0 z C Q w a T U s I j B p N Y w v c M J r 2 E x w w m y Y R d B p N q w h M G k 3 D C U w a T d s I z B p N 8 w n M G k 4 L C C w a T i M J L B p O O w i s G k 5 T C H o N J T e E 1 g 0 l Q 4 Q 2 D S V P h L Y N J V u E d g 0 l Z 4 Q c G k 7 j C j w y 2 H w T w a K u s m q g f r g y Y u M Q O o a Q t s U s o S U v s E a q V 9 d L b 0 1 9 w T A R 4 v i d A e n j r G I Z e Z 9 0 b Q O A r X I 4 j 4 T 1 m k 3 i I E J b A E / r r E P S S k 8 J T L 8 p l M T a k 3 i 6 D a Y 7 e U n / H a 7 9 p 7 9 A d S b R i n 1 D S r D g g l C Q r D g k l x Y o j Q k m w 4 j 2 h p F d x T C j J V X Q J J b W K E 0 J J r O K U U N K q O C O U p C r O C S W l i g t C S a j i k l D S q b g i l G Q q e o S S S s U 1 o S R S c U M o a V T c E k o S F X e E k k L F B 0 J J o O I j o f X z m R T d I O g P F r 5 5 M l N Z Q y B f 0 H U / E i j T u E 0 l F P A O l V C 4 u 1 R C w e 5 R C c X U o R K K a J 9 K K J 4 D K q F o D q m E Y j m i E o r k P Z V Q H M d U Q l F 0 q Y R i O K E S i u C U S r j 4 Z 1 T C R T + n E i 7 2 B Z V w k S + p h I t 7 R S V c 1 B 6 V c D G v q Y S L e E M l X L x b K u G i 3 V E J F + s D l X C R P r L 7 V f 6 r 8 l 5 q y Y A v m T Q + D A 8 a t a v 1 G 7 G 4 t Q 2 6 7 j 1 G c p x N p I c m y H v E R J d D 4 d o k I J / k e K T q 9 r L W g A 5 c s o e g T R Q 0 X B R o G w U N H w X a S E H D S Y G 2 U t D w U q D N F D T c F G g 7 B Q 0 / B d p Q Q c N R g b Z U 0 P B U o E 0 V N F w V a F s F D V 8 F 2 l h B w 1 m B t l b Q 8 F a g z R U 0 3 B V o e w U N f w X a Y E H D Y Y G 2 W N D w W K B N F j R c F m i b B Q 2 f B d p o Q c N p g b Z a 0 P B a o M 0 W N N w W a L s F D b 8 F 2 n B B w 3 G B t l z Q 8 F y g T R c 0 X B d o 2 w X M d + H n B 0 x E s p i A N 0 m H U M Q z 9 Y b T 0 J e + F 0 I K B e Y g V Y 4 E K n 0 w U Q m p + Q a m r 1 4 S V C 9 q F s l c F 3 Q 6 V K 1 C k k d F h I n Q q V + / v z u Y 6 S S o 3 x l R N 8 G s 2 W j b v k 4 y 9 i V + f n d v 4 U S e 8 8 j z R V t n k m w I 8 d c G o g P q k Z j S 0 n 2 q o P O v B e U y i o d Q R f Z 1 o e 5 9 X Q O P C Z k F Y 1 + o l 9 f 9 i c z 0 5 y o o n B 4 2 X i L P T U z d x 6 r K c g e G 4 M S Z Y k t c g Q Q e O j b O F F E L g X 6 y 5 g b H f h 7 7 A S z q 1 2 + 6 F b D w X n r V t T u 9 j Z e O F 9 y / u F x X s D d 8 u k 3 2 c s F z e + P U T H I 2 x w 0 y L h b 2 a Z x L F B A u 6 u d r T S q Q N E Z V i k Y R F M 2 m R T a S i T + l S A s 0 4 z B Z Z P q N J / P o b b m V P J 6 o 0 T + p 5 w M u e 9 x d 8 N e d j r t L C 3 j j F 9 Q D V W i 2 L / G P X + D a F x m L v F p a g N 2 s J F o V l E B v s 3 h U + I l 6 T D V + z A q 0 r c K f C e 9 F 9 8 c 3 L 9 S r P v o / f k x S 8 8 q q y H H 9 h X 7 V 7 E U f 4 p j F 2 M e k L 7 0 d T I C 4 5 V P 1 a 4 b 7 H R L 1 y p v y x q Z R F q 3 e N 8 0 m o c 6 Z 2 i p H E t Z 1 8 y L z h h m o 5 h 6 j + y i H Y e R v N F 6 y z o o k V k / 6 F / P u j 5 u L F j J L Q X F b b Z x 8 1 P X e t H G 5 Y v I W R m u h + 2 M / S k d y 1 t w 6 u V + o R 8 Z 4 b P h q s 1 w B n r X C D 8 G L U i / N K p s v Y b r h 7 Y 4 z o a Y n U w Y w G H t 7 + I k 4 h W + F N 8 i y + 4 1 V 5 y H P W a 5 O 5 6 z 4 E 2 q 8 C H U H 8 G 9 / X V 1 9 L V C d k y Y Q r 9 q b 1 G r F M P 3 7 m Y g e C q q n X g e M Q f b 9 A e 6 z O H s c F O D f r x o s U 4 d g L m d 4 r P f t h W F q u P Q L H P 8 Y D / / V 1 c / f r N n / o O Q 9 f 3 H z Z m N r c 2 P r 4 r u 1 v + 1 U / 4 v p 1 y t / X P m X l V c r W y t / X v n b y u H K + c r 1 S r B y v / J f K / + 9 8 j / v v n 1 3 8 q 7 3 7 s a E / v x n V Z 0 / r D g / 7 z 7 / H 4 f o z O E = < / l a t e x i t >

pkNN + (1

) p N L M < l a t e x i t s h a 1 _ b a s e 6 4 = " M a 5 d m B C L e H Q V E k + 1 g 0 x r 7 6 f 4 m 8 4 = " > A A A 1 p X i c l V t b d 9 v K d V a S N k n V J j 1 J H v O C V d n r u F m y l m Q f n y R 9 i i 7 U x a L u 1 M U + 9 P E C w U 0 Q F m 7 C D C F S L P v c 1 7 6 2 v 6 z / p n t m M N h 7 Q M h n V W t Z w n z f n s F c v p n 9 E Y Q H e R w J u b n 5 v z / 7 + S / + 7 u 9 / + a t f / 8 P q P / 7 T b 3 7 7 z 9 / 8 7 v c 3 I p s U A V w H W Z w V d w N f Q B y l c C 0 j G c N d X o C f D G K 4 H d z v K v 6 2 h E J E W d q T s x w + J X 6 Y R q M o 8 C V C t / 0 Y Q 4 f + 5 2 / W N j f + u v n m 7 d t N b 3 N j c / P P b z a / V x f f b 7 7 9 y / f e F i L q Z 2 2 l + j n / / L v v / r M / z I J J A q k M Y l + I H 7 Y 2 c / l p 7 h c y C m J Y r P Y n A n I / u P d D + A E v U z 8 B 8 W m u + 7 v w X i I y 9 E Z Z g f 9 S 6 W m U 1 5 j 7 i R C z Z I C R i S / H o s k p s I 3 7 Y S J H f / k 0 j 9 J 8 I i E N z I 1 G k 9 i T m a c G 7 w 2 j A g I Z z / D C D 4 o I + + o F Y 7 / w A 4 l T t L r 6 U v 1 4 p 5 1 b 7 2 S 7 d + j t d f a P T o 9 6 R 2 e n V 5 6 m V t s 6 s o 5 / 1 T D E + i B Z Y B v e i V / c e w L v g x M s v G z k B X 5 u r t W I C x h B U U R p q D o 1 j M p I 2 L B R F E 4 K w A G l 8 B h k S e K n w 3 k f w R h G c j G f 9 y H x X n X x + l 8 X i 6 W Y A N c B C h u 1 q 0 t t c U U U j u v G L l W h L U p m u Y 3 p Z X l b x C C T M k t s 0 I 4 u L c V V 4 / Z t m P 9 c x M B G D J 6 L C G x E 8 F z E 0 E Y M V Q Q u w y G O L l Y j 9 H w P 4 9 W i w w h 3 y d D D u U n c N v B a g Y s f t j 5 h K 4 O R t 7 a l G m k O e 7 q Y 9 x O / C F F g f j H f P 7 p r 9 g W v n R C U U j O k d 7 Z 3 p u / T l z C V W v r z A r D 3 i v g 3 c 2 O 3 z Y 5 u U o 6 z f N 7 v N N n O A 7 K d z / N + U T 7 1 R Z R 4 D 3 h d 5 u N o 8 U p B / 4 6 / p k t T 1 s m f G r V y V U u O Q f p f r 1 d X m 1 b V h q 9 a I x + e c K 5 a u + L G 5 S r u m Z s 3 I q d P S 5 F T F f m 0 H L k c u B Q z 1 E H D d l L d 6 V V b 0 w 9 P z V E 1 I 9 T E I t x A p x q d N t C Z R m c N N N F o 0 l S B R t N m 7 E Q q c U y w S 9 N 1 z 4 5 4 K S h 3 g l T f G y F D U x F v 4 w 9 i n + a u G a a q s q C 2 l h B b a g w x 3 j v c n P v 6 r D O H I Z 7 U s O 7 F 2 S M U r w N M Z R u r f d y p + r S C 0 d r W 3 J y L / 9 H H 0 l x v j 7 b q e A p E 0 o 8 3 v H 0 8 Y 4 X E P K S O V K E O Q u R N i / u 2 x f 1 m i 5 q W j 5 m 9 5 9 q b 6 q 7 C s 0 E e D q 8 q v L E 1 H i b + k K q s v V 3 7 b q n a e l 3 H X r 3 l T X 2 n h 3 N l k s V X p w M T i u l 8 l V m c + W h p w E 6 I q X 1 l a 1 + 1 1 L 6 0 t X S e f M z q 5 L V R T 4 y 5 u 9 A z U 6 e 2 Z 6 a m 2 e C 4 A G g 2 y d p b e 7 v c I s 0 a a / v t c t t + 6 g E u g q r c M m X w Y M Z s Q 5 4 f t N P O J M + h 8 F Q 7 p p l O 1 U y n r Z l t r / A f a d 4 b j b 1 + / d o v s 2 j o T Y T K + N H I y z M h I j R n p u k 8 9 j E j V e 0 / 3 z t l U n J M U C 1 j V I y p X s X 8 v w d Z N b R b N 7 T 7 k w 3 h m N M Q t L U x s c K 0 o e G 6 R y g V S 9 u m X r 9 + V i b Y O z 8 O M z R l 4 6 R l n M i Z 3 t V B X x 0 o a 2 p p p N u 2 q e 2 W p q z g 7 f 1 w E H V b X z 8 M e k 6 l 7 Z + s t D S p a B h k N X K m P o W a 7 q q r r y 2 K q d 9 U 7 3 l d / 9 y t b 0 d a 3 w B 7 r a 6 f 7 X A l O I h i J d Z Y X a B d w Q B 1 V b U 3 i r O s 0 L S + M r y + r A K Q G i T z J Z M j C 9 w I l c 8 J / H i + 1 w w o / T g a 8 o D P 5 r p I 5 o Z a L D U J Q r Z X 0 M y i H h H k Q l n H X E R x l m r b h 1 O L T W S J V / p F h E k M r L 4 x f 8 2 N c U u z I s F W X / Q R e r G w 0 1 k 0 a J + Y g c s M i A l c J i B m 6 D J D Y s B l g J i R y 4 y I C V 0 m J G b s M m N i I p e J i P n i M l + I u X e Z e 2 J i l 4 k X W s Z F 4 k U C d y x + b B 3 O 1 G F n V n D d + z I R 0 h t m 6 b f S U 5 8 f U Y 4 z d f I 4 C + M l V d u p 2 3 Z K d 8 1 c J i M m d 5 m c m A e X e S C m c J m C G O E y g h j p M p K Y i c t M i C l d p i T m 0 W U e i Z m 6 z J S Y m c v M i H l y m a e F M W h 2 A 2 B m z u r j v a w 2 y d x s p c G I b Z u 6 3 9 r l s Y j K 9 d U 8 4 z g 8 I J j t j T I g m G 2 M c k g w 2 x U l E M y 2 R D k i m O 2 H M i S Y b Y Z y T D D b C e W E Y L Y N y i 8 E s z 1 Q 3 h P M N k A Z E x w z O C E 4 Y T C b a D 7 D G c F M z G V O M F N y + U A w k 3 F Z E M w 0 X A q C B V 9 U g m X 7 n H D p l g Q z 3 Z a P B D P R l l O C m W L L G c F M r u U T w V a r n R j U c y j 9 E K V o 0 S 0 Y 0 b W e y 2 C U 1 3 o y g 5 F f 6 9 k M R o O t p z M Y I b a e z 2 D U 2 H p C g 5 F k 6 x k N R p e t p z R y z 5 7 T Y B T a e l K D k W n r W Q 1 G q 8 3 T 2 n K J y y W c e / Y k B i P d 1 r M Y j H 5 b T 2 M w I m 4 9 j 8 E o u f V E B i P n 1 j M Z j K Z b T 2 U w w m 4 9 l 8 G o u / V k B i P x 1 r M Z j M 5 b T 2 c w Y m 8 9 n 8 E o / v k T G v d C E Q W 1 Q 0 m 2 a X 9 s 0 7 Z J d g j e Y f A u w b s M 3 i N 4 j 8 E d g j s M 3 i d 4 n 8 E H B B 8 w + J D g Q w Y f E X z E 4 P c E v 2 f w M c H H D O 4 S 3 G X w C c E n D D 4 l + J T B Z w S f M f i c 4 H M G X x B 8 w e B L g i 8 Z f E X w F Y N 7 B P c Y f E 3 w N Y N v C L 5 h 8 C 3 B t w y + I / i O w R 8 I / s D g j w R / f P 5 4 d U U H R n V M o 9 t M v 1 p 6 j N v h 3 K 7 L 7 X J u z + X 2 O N d x u Q 7 n 9 l 1 u n 3 M H L n f A u U O X O + T c k c s d c e 6 9 y 7 3 n 3 L H L H X O u 6 3 J d z p 2 4 3 A n n T l 3 u l H N n L n f G u X O X O + f c h c t d c O 7 S 5 S 4 5 d + V y V 5 z r u V y P c 9 c u d 8 2 5 G 5 e 7 4 d y t y 9 1 y 7 s 7 l 7 j j 3 w e U + c O 6 j y 1 n Z 3 3 A L U T 6 B / h y B n 1 0 3 6 7 p l l s L c f p 6 1 W D I x U D + h p F F 7 Y o W 7 f l g 9 G 6 0 I / T T V w l U 0 C x w Y h O y J N i e I k C n R l g Q R s i J l 1 U E y I N p + I E K 2 Q 5 s O R M h s a K u B C F m M s u o k 6 + E X g 5 C d 0 G Y C E T I R 2 k I g E r P p M Q g Z B m 0 X E E n Z t B o k Y 5 N k E L I E 2 h A g Q k Z A 2 w B E K P 3 r 5 I + I Y O t g E E r 1 Z b V a b K 1 K g 1 B a 1 0 k d E U r m O p U j Q i l c J 3 B E K H H r t I 1 I m 0 l 1 3 W n p x / l Y r b f + W w u z H F S a q R 7 E G 5 A + g d E D i 4 o y X + U i Y y 6 I y B I I F a 7 / E q y V q l R q A W w Q E f x N k I j C R F X V f w m 2 e q 6 / J a g G M p / z / s + V W G 0 J x R p Q C Y U 6 Z I O a K 4 H a E g p 0 R C U U Z 0 g l F O a Y S t h d 1 l c U 5 B c q o R j v 2 d z M l Q j r k c + V A G 0 J J 5 P N I o o v Y 1 M y V 6 K z J R T d A 5 V Q c A W b q b k S W j 1 B c y U y W 8 K J Z t O M A i u p h O J 6 p B I K a 0 o l F N W M S i i o p 0 X 1 D T O m 3 6 n B d e p F n V H K 1 Q k X E U q 0 O s 0 i Q u l V J 1 d E K K n q l I o I p V K d S B G h B K r T J y K U N n X S R I S S p U 6 V i F C K 1 A k S E U q M O i 0 i Q u l Q J 0 N E K A n q F I g I p T 6 d + B C h h K f T H S K U 5 n S S Q 4 S S m 0 5 t i F B K 0 w k N E U p k O o 0 h Q u l L J y 9 E K G n p l I U I p S q d q B C h B K X T E y K U l n R S Q o S S k U 5 F i F A K 0 g k I k Y 9 s B S l d D H i 2 S M 7 r b H H O s k X S t V t f M d 1 q + 9 e D q / a w 4 q 7 M P t Y q 6 k E q 1 H s X e x D E f g E o q v G 2 O o H w j s Y D i l G k n q B C G m T D K A 2 x M X 8 S K 0 S M 6 u t k M R f q 4 e 8 V y O c a G G T x 8 K e a G U w X 8 + a X m x L 7 Z 7 4 p 1 + m 0 a k 8 / v K 6 G J o 3 t T A V T v 9 y x G O l f 7 l q M d o D c s x j t A d m x G O 0 C u W 8 x 2 g f y w G K 0 E + S h x W g v y C O L 0 W 6 Q 7 y 1 G + 0 E e W 4 x 2 h O x a j P a E P L E Y 7 Q p 5 a j H a F / L M Y r Q z 5 L n F a G / I C 4 v R 7 p C X F q P 9 I a 8 s R j t E 9 i x G e 0 R e W 4 x 2 i b y x G O 0 T e W s x 2 i n y z m K 0 V + Q H i 9 F u k R 8 t Z o w a C v m g 8 P O x Y U P 7 8 T d w P o W E O w w m X Y S 7 D C Z p h H s M J n W E H Q a T Q M J 9 B p N G w g M G k 0 z C Q w a T U s I j B p N Y w v c M J r 2 E x w w m y Y R d B p N q w h M G k 3 D C U w a T d s I z B p N 8 w n M G k 4 L C C w a T i M J L B p O O w i s G k 5 T C H o N J T e E 1 g 0 l Q 4 Q 2 D S V P h L Y N J V u E d g 0 l Z 4 Q c G k 7 j C j w y 2 H w T w a K u s m q g f r g y Y u M Q O o a Q t s U s o S U v s E a q V 9 d L b 0 1 9 w T A R 4 v i d A e n j r G I Z e Z 9 0 b Q O A r X I 4 j 4 T 1 m k 3 i I E J b A E / r r E P S S k 8 J T L 8 p l M T a k 3 i 6 D a Y 7 e U n / H a 7 9 p 7 9 A d S b R i n 1 D S r D g g l C Q r D g k l x Y o j Q k m w 4 j 2 h p F d x T C j J V X Q J J b W K E 0 J J r O K U U N K q O C O U p C r O C S W l i g t C S a j i k l D S q b g i l G Q q e o S S S s U 1 o S R S c U M o a V T c E k o S F X e E k k L F B 0 J J o O I j o f X z m R T d I O g P F r 5 5 M l N Z Q y B f 0 H U / E i j T u E 0 l F P A O l V C 4 u 1 R C w e 5 R C c X U o R K K a J 9 K K J 4 D K q F o D q m E Y j m i E o r k P Z V Q H M d U Q l F 0 q Y R i O K E S i u C U S r j 4 Z 1 T C R T + n E i 7 2 B Z V w k S + p h I t 7 R S V c 1 B 6 V c D G v q Y S L e E M l X L x b K u G i 3 V E J F + s D l X C R P r L 7 V f 6 r 8 l 5 q y Y A v m T Q + D A 8 a t a v 1 G 7 G 4 t Q 2 6 7 j 1 G c p x N p I c m y H v E R J d D 4 d o k I J / k e K T q 9 r L W g A 5 c s o e g T R Q 0 X B R o G w U N H w X a S E H D S Y G 2 U t D w U q D N F D T c F G g 7 B Q 0 / B d p Q Q c N R g b Z U 0 P B U o E 0 V N F w V a F s F D V 8 F 2 l h B w 1 m B t l b Q 8 F a g z R U 0 3 B V o e w U N f w X a Y E H D Y Y G 2 W N D w W K B N F j R c F m i b B Q 2 f B d p o Q c N p g b Z a 0 P B a o M 0 W N N w W a L s F D b 8 F 2 n B B w 3 G B t l z Q 8 F y g T R c 0 X B d o 2 w X M d + H n B 0 x E s p i A N 0 m H U M Q z 9 Y b T 0 J e + F 0 I K B e Y g V Y 4 E K n 0 w U Q m p + Q a m r 1 4 S V C 9 q F s l c F 3 Q 6 V K 1 C k k d F h I n Q q V + / v z u Y 6 S S o 3 x l R N 8 G s 2 W j b v k 4 y 9 i V + f n d v 4 U S e 8 8 j z R V t n k m w I 8 d c G o g P q k Z j S 0 n 2 q o P O v B e U y i o d Q R f Z 1 o e 5 9 X Q O P C Z k F Y 1 + o l 9 f 9 i c z 0 5 y o o n B 4 2 X i L P T U z d x 6 r K c g e G 4 M S Z Y k t c g Q Q e O j b O F F E L g X 6 y 5 g b H f h 7 7 A S z q 1 2 + 6 F b D w X n r V t T u 9 j Z e O F 9 y / u F x X s D d 8 u k 3 2 c s F z e + P U T H I 2 x w 0 y L h b 2 a Z x L F B A u 6 u d r T S q Q N E Z V i k Y R F M 2 m R T a S i T + l S A s 0 4 z B Z Z P q N J / P o b b m V P J 6 o 0 T + p 5 w M u e 9 x d 8 N e d j r t L C 3 j j F 9 Q D V W i 2 L / G P X + D a F x m L v F p a g N 2 s J F o V l E B v s 3 h U + I l 6 T D V + z A q 0 r c K f C e 9 F 9 8 c 3 L 9 S r P v o / f k x S 8 8 q q y H H 9 h X 7 V 7 E U f 4 p j F 2 M e k L 7 0 d T I C 4 5 V P 1 a 4 b 7 H R L 1 y p v y x q Z R F q 3 e N 8 0 m o c 6 Z 2 i p H E t Z 1 8 y L z h h m o 5 h 6 j + y i H Y e R v N F 6 y z o o k V k / 6 F / P u j 5 u L F j J L Q X F b b Z x 8 1 P X e t H G 5 Y v I W R m u h + 2 M / S k d y 1 t w 6 u V + o R 8 Z 4 b P h q s 1 w B n r X C D 8 G L U i / N K p s v Y b r h 7 Y 4 z o a Y n U w Y w G H t 7 + I k 4 h W + F N 8 i y + 4 1 V 5 y H P W a 5 O 5 6 z 4 E 2 q 8 C H U H 8 G 9 / X V 1 9 L V C d k y Y Q r 9 q b 1 G r F M P 3 7 m Y g e C q q n X g e M Q f b 9 A e 6 z O H s c F O D f r x o s U 4 d g L m d 4 r P f t h W F q u P Q L H P 8 Y D / / V 1 c / f r N n / o O Q 9 f 3 H z Z m N r c 2 P r 4 r u 1 v + 1 U / 4 v p 1 y t / X P m X l V c r W y t / X v n b y u H K + c r 1 S r B y v / J f K / + 9 8 j / v v n 1 3 8 q 7 3 7 s a E / v x n V Z 0 / r D g / 7 z 7 / H 4 f o z O E = < / l a t e x i t >

Adaptive Retrieval

NLM

p N L M < l a t e x i t s h a 1 _ b a s e 6 4 = " c 3 t 6 j U g I 4 O f V 4 C e k u 2 K R m O a k u M k = " > A A A 1 r H i c l V t b c x v L c a b t J H a Y O D 5 2 H v O y F U p l O U W x R J 2 j c i p P 5 g W 8 i C A J k u B F E n R U C 6 C x W H F v 3 B k s A W 6 Q Z / 8 E v 9 r / K v 8 m P T M 7 2 z 2 L p U 6 F V S J 3 v q + n d y 5 f T z c W q 2 E W h U K + e f O / P / v 5 L / 7 u 7 / / h l 7 / 6 x / V / + u d f / 8 t v v v v t 7 2 5 E O s t H c D 1 K o z S / G / o C o j C B a x n K C O 6 y H P x 4 G M H t 8 H 5 P 8 b c F 5 C J M k 7 5 c Z P A 5 9 o M k n I Q j X y L 0 Y / a l H E i Y y / K s e 7 p c f v l u 4 8 3 W G / 3 j r V 5 s V x c b a 9 V P 7 8 t v f / j z Y J y O Z j E k c h T 5 Q n z a f p P J z 6 W f y 3 A U w X J 9 M B O Q + a N 7 P 4 B P e J n 4 M Y j P p R 7 2 0 n u J y N i b p D n + S 6 S n U d 6 j 9 G M h F v E Q L W N f T k W T U 2 A b 9 2 k m J / / 5 u Q y T b C Y h G Z k b T W a R J 1 N P r Y E 3 D n M Y y W i B F / 4 o D 3 G s 3 m j q 5 / 5 I 4 k q t r 7 9 U P 9 5 Z 5 9 Y 7 3 e k f e f u d g + O z 4 / 7 x + d m V p 6 n 1 t o F s 4 l 8 1 D b E 5 j J f o w z v 1 8 3 t P 4 H 1 w n Y W X T r y R n 5 l r N e M c J p D n Y R K o Q Y 3 D I h T W b B I G s x x w Q g k 8 j t I 4 9 p N x O U A w g o l c l u U A Y u 9 V F 6 / / s F y u 2 I x w H y C 3 V n u 6 1 W a X h 8 G 0 d n a p G m 1 W M s 2 s T T / N 2 i y G q Z R p b I 1 2 d W v F r p q 3 b 8 3 8 5 y y G 1 m L 4 n M X I W o y e s x h b i 7 G y w G 0 4 w t l F a o a e 7 6 G 9 2 n S Y Y L C M P V y b 2 P W B 1 w p c f t r + j F 6 G E 2 9 j W z l p T n u + L A e x n w c o M D 8 v D 4 7 v m m P B a 8 c E p d Q 0 6 Z / v n + v 7 6 P D T 0 i 9 z w N E r 4 r / M j V 2 f H e 1 S T t O s H H S a b O c B 2 Q 4 G c 1 4 8 D U Q Y e w 9 4 X W T T c P l K Q f + N v + Y r S 9 b J n h q 9 1 H F Q y C l I / 9 v 9 6 m 7 z q t v 4 V a v l w x O u V e t Q X L t M 2 T 1 z 8 4 b l / G n F c q 4 s n 1 Y t V w 1 X b M b a a N x O q j u 9 a n P 9 8 N S c V d N C L S z C D X S u 0 X k D X W h 0 0 U B j j c Z N F W g 0 a d r O p B L H D I c 0 3 / T s j F e M M s d I j b 1 h M j Y d 8 T b + M P J p 7 Z p m q i s z a v O E 2 I o z x P j o M D g P 9 F l n D k M 8 q W H T i 9 J H y F + P M K N t r Q 8 w U v V p B Z O N 7 d K c i / 8 z w F a p w 6 O t O 5 4 C o f S j L e 8 A z 1 g h M Q + p I 1 W o g x B 5 4 / H A e j x o e t S 0 f E z t P T f e V n c V n j X y c H p V 4 6 3 t 8 T D z x 9 R l 4 / u N H 1 a 6 b d Z 9 7 N X 3 3 N U P e j p X J l l 8 c z k w o Z j B V 5 n F W Y 8 W B 3 Z B T O 8 r 2 / u q p f e l 7 a X z 5 G N a J 6 + t e m H M 3 Y V e m T q 1 P b M 0 T Y f T H K D p k v n b + H 7 V I 6 0 a 8 / 3 9 q m 8 / 8 Q A 3 Q X V u W T J 4 M H O 2 J s 9 P 2 v E z y z L I P e X H u O l U b j p t b n a 8 3 H + k d W 8 4 e / 3 6 t V + k 4 d i b C Z X x w 4 m X p U K E W K M Z 1 1 n k Y 0 a q / D 8 / O l W k Z J i g W u a o G N O 9 s v l / T 7 J y t F c 7 2 v t J R z j n J A B d 2 h h b Y X x o u B 4 R S s X S 1 t X r 1 8 / K B E f n R 0 G K R d k 0 b p k n c m Z 0 t d E 3 J 8 p c r c x 0 x 7 r a a X F l B W / v h 5 O o f X 3 7 M O g 7 n X Z + s t P K o m L B I K u Z M / U p 1 A x X X X 1 r U 0 z / p n p 7 d f + e 2 9 / O t L 4 B j l p d P z v g S n A Q R k q s k b r A c g U N 1 F X l b x K l a a 5 p f W V 4 f V k Z I D W M y 5 U i R + Y Y C F W d M / K j c r 9 p U P h R O O Y G X 8 x 1 H p e G W q 6 4 B C H b O 2 h m W c 8 I M q F K x 0 y E U Z r o s g + X F l 2 k s V f 4 e Y h J D K y + M X + V p n B L 0 j x G r y 8 G C L 1 Y 2 u X M G 7 R P z N B l h s S M X G Z E z N h l x s S A y w A x E 5 e Z E B O 4 T E D M 1 G W m x I Q u E x L z 1 W W + E n P v M v f E R C 4 T L b W M 8 9 g L B U Y s f n o d L 9 R h Z 3 Z w 0 / s 6 E 9 I b p 8 n v p a c + P 6 I c F + r k c T b G i y v f i e s 7 o b u m L p M S k 7 l M R s y D y z w Q k 7 t M T o x w G U G M d B l J z M x l Z s Q U L l M Q 8 + g y j 8 T M X W Z O z M J l F s Q 8 u c z T 0 h R o N g A w M 6 f 1 8 V 5 U Q V K a U B p O W N j U 4 9 Z V H r O o q r 6 a Z x y H h w S z 2 C h G B L P A K M Y E s 6 g o g G A W E s W E Y B Y P R U A w C 4 Z i S j C L h G J G M A u D 4 i v B L A a K e 4 J Z A B Q R w R G D Y 4 J j B r O F 5 i u c E s z E X G Q E M y U X D w Q z G R c 5 w U z D h S B Y 8 E 0 l W L a v C Z d u Q T D T b f F I M B N t M S e Y K b Z Y E M z k W j w R b L X a i U A 9 h 9 I P U f I W 3 Y I R X e u 5 D E Z 5 r S c z G P m 1 n s 1 g N N h 6 O o M R Y u v 5 D E a N r S c 0 G E m 2 n t F g d N l 6 S i P 3 7 D k N R q G t J z U Y m b a e 1 W C 0 2 j y t L R e 7 X M y 5 Z 0 9 i M N J t P Y v B 6 L f 1 N A Y j 4 t b z G I y S W 0 9 k M H J u P Z P B a L r 1 V A Y j 7 N Z z G Y y 6 W 0 9 m M B J v P Z v B 6 L z 1 d A Y j 9 t b z G Y z i n z + h M R b y c F R X K P E O x c c O h U 2 8 S / A u g / c I 3 m P w P s H 7 D O 4 Q 3 G H w A c E H D D 4 k + J D B R w Q f M f i Y 4 G M G v y f 4 P Y N P C D 5 h c J f g L o N P C T 5 l 8 B n B Z w w + J / i c w T 2 C e w y + I P i C w Z c E X z L 4 i u A r B v c J 7 j P 4 m u B r B t 8 Q f M P g W 4 J v G X x H 8 B 2 D P x D 8 g c E f C f 7 4 / P H q i g 6 M 6 p h G d 5 h + t f Q Y t 8 u 5 P Z f b 4 9 y + y + 1 z r u N y H c 4 d u N w B 5 w 5 d 7 p B z R y 5 3 x L l j l z v m 3 H u X e 8 + 5 E 5 c 7 4 V z X 5 b q c O 3 W 5 U 8 6 d u d w Z 5 8 5 d 7 p x z P Z f r c e 7 C 5 S 4 4 d + l y l 5 y 7 c r k r z v V d r s + 5 a 5 e 7 5 t y N y 9 1 w 7 t b l b j l 3 5 3 J 3 n P v g c h 8 4 9 9 H l r O x v e A l R P I H + H I G f X d / U f Y s 0 g d J + n r V Y P D P Q I K a k U d f E C n f r Y f V s t C L 0 0 1 Q L V 9 b M c G g Q K k 9 0 c Y I I F S W 6 J E G E S p G i G i A V I L r 8 Q I T K D l 1 0 I E L F h i 4 1 E K E S o 6 g G y U b 4 1 S B U T u h i A h E q I n Q J g U j E l s c g V D D o c g G R h C 2 r Q V K 2 S A a h k k A X B I h Q I a D L A E Q o / e v k j 4 h g + 2 A Q S v V F t V t s r w q D U F r X S R 0 R S u Y 6 l S N C K V w n c E Q o c e u 0 j U h b k e p W p 4 U f Z V O 1 3 / p v L c x i W G m m e h B v Q P o E R g 8 s K i r y 4 + F Y 9 T A X R K Q x B A r X f w n W S l U q t Q A 6 R A R / E y T C I F Z d 9 V + C r Z 7 r b w m q i Z Q l H 3 + p x G p b K N Y R t V C o Y z a p U g n U t l C g E 2 q h O A N q o T C n 1 M L h s r G i I L 9 S C 8 V 4 z 9 a m V C K s Z 1 4 q A d o W L i Z b R R R f y p a k V K K z L R T d A 7 V Q c D l b q V I J r V 6 g U o n M t n C h 2 T K j w A p q o b g e q Y X C m l M L R b W g F g r q a V l 9 w 4 z p d 2 5 w n X p R Z 5 R y d c J F h B K t T r O I U H r V y R U R S q o 6 p S J C q V Q n U k Q o g e r 0 i Q i l T Z 0 0 E a F k q V M l I p Q i d Y J E h B K j T o u I U D r U y R A R S o I 6 B S J C q U 8 n P k Q o 4 e l 0 h w i l O Z 3 k E K H k p l M b I p T S d E J D h B K Z T m O I U P r S y Q s R S l o 6 Z S F C q U o n K k Q o Q e n 0 h A i l J Z 2 U E K F k p F M R I p S C d A J C 5 C P b Q U o X Q 5 4 t 4 l 6 d L X o s W 8 R d G / q K 6 V b h X 0 + u i m H F X Z k 4 1 i r q Q y L U e x f 7 M I r 8 H F B U 0 x 1 1 A u E d T Q 0 o J q F 6 g g r J K B 2 H S Y D O / F m k E D G p r + N l K d T D 3 y u Q z z k Y p t H 4 p 9 w M 5 8 u y + e W m x P G Z b 8 p 1 O q 3 8 6 Y f X 1 d S k K T s T w d Q v d y 1 G + p d 7 F q M I k P s W o x i Q H Y t R F M g D i 1 E c y E O L U S T I I 4 t R L M h j i 1 E 0 y P c W o 3 i Q J x a j i J B d i 1 F M y F O L U V T I M 4 t R X M h z i 1 F k y J 7 F K D b k h c U o O u S l x S g + 5 J X F K E J k 3 2 I U I / L a Y h Q l 8 s Z i F C f y 1 m I U K f L O Y h Q r 8 o P F K F r k R 4 u Z Q g 2 F f J j 7 2 d S w g f 3 4 O 3 I + h Q S 7 D C Z d B H s M J m k E + w w m d Q Q d B p N A g g M G k 0 a C Q w a T T I I j B p N S g m M G k 1 i C 9 w w m v Q Q n D C b J B F 0 G k 2 q C U w a T c I I z B p N 2 g n M G k 3 y C H o N J Q c E F g 0 l E w S W D S U f B F Y N J S k G f w a S m 4 J r B J K j g h s G k q e C W w S S r 4 I 7 B p K z g A 4 N J X M F H B t s P A n i 0 V a W a q B + u D J m 4 x C 6 h p C 2 x R y h J S + w T q p X 1 0 t v X X 3 D M B H i + J 0 B 6 e O s I x l 5 n 0 x v C y F e 4 n I b C e 0 x n 0 R g h b I E n 9 N c h W E v O c k + 9 K J d G 6 E i 9 X Q b z D G t L / R 2 v / a a 9 Q 3 c k 0 Y o D Q k m z 4 p B Q k q w 4 I p Q U K 4 4 J J c G K 9 4 S S X s U J o S R X 0 S W U 1 C p O C S W x i j N C S a v i n F C S q u g R S k o V F 4 S S U M U l o a R T c U U o y V T 0 C S W V i m t C S a T i h l D S q L g l l C Q q 7 g g l h Y o P h J J A x U d C 6 + c z C V a D o D 9 Y + O b J T F U a A t U F X f c j g S o a d 6 i F A t 6 l F g p 3 j 1 o o 2 H 1 q o Z g 6 1 E I R H V A L x X N I L R T N E b V Q L M f U Q p G 8 p x a K 4 4 R a K I o u t V A M p 9 R C E Z x R C z f / n F q 4 6 T 1 q 4 W Z f U A s 3 + Z J a u L l X 1 M J N 7 V M L N / O a W r i J N 9 T C z b u l F m 7 a H b V w s z 5 Q C z f p I 7 t f V X 9 V t Z f a M u B b J k 0 d h g e N i m r 9 R i y G t k E 3 v c d Q T t O Z 9 L A I 8 h 4 x 0 W W Q u 2 U S U J 3 k 1 E j V 7 W W t A W 2 4 U h 6 C L q K g U U W B L q O g U U e B L q S g U U m B L q W g U U u B L q a g U U 2 B L q e g U U + B L q i g U V G B L q m g U V O B L q q g U V W B L q u g U V e B L q y g U V m B L q 2 g U V u B L q 6 g U V 2 B L q + g U V + B L r C g U W G B L r G g U W O B L r K g U W W B L r O g U W e B L r S g U W m B L r W g U W u B L r a g U W 2 B L r e g U W + B L r i g U X G B L r m g U X O B L r q g U X W B L r u A 1 V 3 4 + Q E T k c x n 4 M 2 S M e T R Q r 3 h N P a l 7 w W Q Q I 4 5 S L V D g U o f z l R C a r 6 B 6 a u X B N W L m n l c 6 o Z O h 8 o r x F m Y h 5 g I n f 7 1 + 7 v D h U 6 C + p 0 R d R P M m g 3 f 9 n W S q S / x 8 7 t 7 C 8 e y x y 1 7 y 7 b B x O k Y o m 9 N R B v U M z G t l f t U R r 1 v G W U y j M Z Q W Q 5 0 o x 5 9 3 Q O P C Z m O p r 5 Q L 6 / 7 M 5 n q z 1 W Q O y N s v E S e G Z t 6 j F W X 1 Q G M w b E z z R a 7 H A k 8 d K y d a a I W R v r J m m s c + V n k j 2 B Z v 3 7 T r Y C l 9 9 K r r t 3 l b b x 0 v O T 1 i 8 t 1 B X v D p 9 t k L 5 c 8 t z d O z T h j a 9 w g o 3 x p n 8 a 5 R A 7 B s n 6 + 1 q R G k u a o W u E k h L z p W q Q T G f t z s r R A 0 w 6 T R a r f e D K P 3 l a 9 Z N F M z f 5 J P R 9 w 2 Z P u k r / u d N J d 2 c A b P 6 c R q E b T v 8 Q / f o 5 7 n 6 f M 8 m p l A / b S g m j V U A K 9 T a N J 7 s f q M d X 0 M c 2 x b B X + Q n g v u j + + f a F e 9 d H / 8 W O W m F d W R Y b 7 L / S r Z i 8 G E E X M x j 4 m f e n t Y g L E k E / U r w X G O 8 T q l T d V G x u n z F q 9 b 5 r O A p 0 z d a k c S t j U 7 k X q j V N Q 7 h 7 D + z C D c e h v N V 6 y T v M 4 U k / 6 l 2 X 3 x z f L F j J N Q H H b b Z x 8 1 P 3 e t n G Z Y r I W R m u h + + M g T C Z y 0 Q y d z M / V I 2 M 8 N n w V L F e A Z 6 3 w A / D C x E v S q s y X M N / y 9 q a p U M u T q g J w N P X 2 8 R N x A r 8 X 3 j B N 7 7 f W n Y c 8 5 5 k 6 n d P 8 P 1 D j e a A H g H 8 H m + r q W 4 b q n D S G e N X u U q s V z f T v Z y z 6 K K i + e h 0 w A j n w h x h n U f o 4 z M G / X z d Y q g 7 B T C 7 w W B / Y C 8 P U c O H n O P 8 p H v 7 r 6 1 + + 2 9 h u / k + l 1 Y u b t 1 v b b 7 a 2 L 3 7 Y + N N u 9 b + Y f r X 2 b 2 v / v v Z q b X v t j 2 t / W j t a 6 6 1 d r 4 3 W 8 r W / r P 1 1 7 W / v t t 7 1 3 3 1 6 9 9 m Y / v x n V Z 9 / X X N + 3 k 3 + D 3 T m z / o = < / l a t e x i t >

Figure 2: Illustration of kNN-LM . The datastore consists of paired context representations and the corresponding next tokens. The index represents a method that performs approximate k-nearest neighbors search over the datastore. Red and bolded text represent three dimensions that we explore in this paper to improve the efﬁciency. In adaptive retrieval, we use a predictive model to decide whether or not to query the datastore.

for each token in the training data; this can easily scale to hundreds of millions or even billions of records. As a result, the extra retrieval step from such datastores greatly decreases model efﬁciency at test time. For example, a 100M-entry datastore can lead to an over 10x slow-down compared to parametric models (§3.3) as shown in Figure 1. This issue poses a serious hurdle for the practical deployment of non-parametric LMs, despite their effectiveness.
In this paper, we attempt to address this issue of test-time inefﬁciency and make non-parametric LMs more applicable in real-world settings. We take kNN-LM as an example, ﬁrst analyzing the evaluation overhead, and raise three questions that we aim to answer in this paper: (1) Do we really need to perform retrieval on the prediction of every single token? (2) Can we identify and prune redundant records from the datastore? (3) Is it possible to further compress the datastore by reducing the vector dimensionality without losing performance? We propose and explore potential solutions for each question to aid efﬁciency. Speciﬁcally, we (1) show that a lightweight network can be learned to automatically prune unnecessary retrieval operations (adaptive retrieval, §4.1), (2) explore several different methods for datastore pruning based on clustering, importance-guided ﬁltering, or greedy merging (§4.2), and (3) empirically demonstrate that simple dimension reduction techniques are able to improve both the performance and speed (§4.3). Figure 1 illustrate the overall performance of these methods. Our experiments on the WikiText-103 language modeling benchmark (Merity et al., 2017) and a training-free domain-adaptation setting demonstrate speed improvements of up to 6x with comparable perplexity to the kNN-LM. On a higher level, we expect the empirical results and analysis in the paper to help researchers better understand the speed-performance tradeoff in non-parametric

NLMs, and provide a springboard for future research on more efﬁcient non-parametric LMs.
2 k-Nearest Neighbors Language Model
In this section, we overview kNN-LM (Khandelwal et al., 2020) and its implementation details.
Formulation. kNN-LM (Khandelwal et al., 2020) is an LM that estimates token distributions by interpolating a pre-trained autoregressive NLM’s distribution with another distribution computed using an external datastore. Speciﬁcally, given a sequence of context tokens ct = (w1, · · · , wt−1), the kNN-LM’s next token probability p(wt|ct) is calculated through the interpolation of the probability estimated by a standard parametric NLM pNLM(wt|ct) and a probability computed using an external datastore pkNN (wt|ct) (detailed later):2
p(wt|ct) (1)
= λpkNN(wt|ct) + (1 − λ)pNLM(wt|ct),
where λ is the interpolation hyperparameter. Note that the application of kNN-LM requires no additional training; the parameters of the NLM remain as-is, and Eq. 1 is only applied at test time. The workﬂow of kNN-LM is shown in Figure 2
Datastore. The datastore in kNN-LM stores context vectors from the pretrained NLM as keys, and their corresponding next tokens as values. Formally, let f be the key function that maps context sequence c to a ﬁxed-size vector, then the datastore (K, V) contains all the key-value pairs constructed from the entire training examples D:
(K, V) = {(f (ct), wt)|(ct, wt) ∈ D}. (2)
The size of such a datastore is almost equal to the number of training tokens because the context ct is (nearly) unique due to the large context
2Below, we sometimes ignore the subscript to simplify notation when there is no confusion.

window size in modern recurrent (Sundermeyer et al., 2012) or self-attentional (Vaswani et al., 2017) NLMs. This suggests that the datastore can easily scale to hundreds of millions or even billions of records. Also, each f (ct) is a highdimensional dense vector, which makes the datastore difﬁcult to ﬁt in memory. For example, a datastore from a 100M-token training dataset, using 1024-dimension context vectors at 16-bit precision, could require 200GB of memory.3
The Nearest Neighbor Distribution pkNN(wt|ct). At inference time, the kNN-LM (1) computes the context vector f (c) from the current sequence using the pretrained NLM, (2) uses f (c) as the query to retrieve k nearest neighbors N = {(qi, vi)|i = 1, · · · , k} from the datastore, and (3) aggregates the retrieved tokens to form the distribution pkNN(w|c) to be used in Eq. 1 as:
pkNN(w = y|c)
∝ Ivi=y exp(−d(qi, f (c))). (3)
(qi,vi)∈N
d(·, ·) is a distance function between the two vectors, and L2 was shown to be more effective than other alternatives (Khandelwal et al., 2020). Intuitively, kNN-LM ﬁnds context sequences in the datastore that are similar to the test context, and then utilizes the next tokens observed after these contexts to help prediction. Such a mechanism allows language modeling through explicit memorization from the datastore, and may be particularly helpful for patterns rarely seen by the pretrained NLM (Khandelwal et al., 2020, 2021).
Sources of Inference Overhead. The extra inference overhead stems from the kNN search process in pkNN(wt|ct) computation. We denote the inference time per token as t = tNLM+tkNN. While tNLM remains constant with different datasets, tkNN unfortunately grows as the datastore scales.
In practice, the kNN search process is often performed only approximately (ANN, Gionis et al. (1999); Muja and Lowe (2009)) to reduce computational cost. Khandelwal et al. (2020) implemented ANN search in kNN-LM4 using FAISS (Johnson
3Note that the dataset to contruct the datastore may not necessarily be the training data that trains the parametric NLM in Eq. 1 – a separate dataset may be used for the datastore construction which would lead to potential applications such as training-free domain adaptation or a gradient-free way to utilize extra training data (Khandelwal et al., 2020).
4https://github.com/urvashik/knnlm.

et al., 2019), which combines inverted ﬁle systems (Sivic and Zisserman, 2003) and product vector quantization (Jegou et al., 2010). This type of index reduces memory usage by only storing quantized vectors and accelerates kNN search by pre-clustering the datastore vectors; interested readers can refer to (Jegou et al., 2010) for more details. For the purpose of this paper we study kNN-LM using this indexing method as a black box, aiming to improve efﬁciency in an index-agnostic way. At the same time, we note that building fast and accurate indexing methods remains an active area of research (André et al., 2019; Guo et al., 2020), and selection or improvement of the index itself (possibly in concert with the methods proposed in this paper) is an interesting avenue for future work.
Distance Recomputation. The distances to the nearest neighbors are required to compute pkNN(wt|ct) as shown in Eq. 3. However, as described above, kNN-LM’s nearest neighbor search process performs search over quantized vectors, and as a result it can only return approximate distances. While it is possible to compute the accurate distances by reading the full-precision vectors from the datastore after retrieval, this presents challenges as well: (1) storing the entire datastore in memory is not scalable for large datastores, (2) reading the vectors from a large datastore on disk on-the-ﬂy is too slow to be practical (< 1 token per second).5 Therefore, in this paper we use the approximate distances directly to compute pkNN. This comes at the cost of a minor performance loss, as we will show in §3.3. Similar approximations were adopted to apply kNN-LM to machine translation tasks (Khandelwal et al., 2021).
3 The Efﬁciency of kNN-LM
In this section, we ﬁrst introduce the datasets and setup that we will use throughout the paper, and then compare the inference speed of kNN-LM to parametric NLMs.
3.1 Datasets
We study kNN-LM in two different settings: (1) the standard setting where the datastore is constructed from the same data used to train the NLM, and (2) a domain adaptation setting where the datastore is based on the training data in the test domain, in
5Disk random I/O is another aspect that may be improved by further engineering effort, which is also interesting future work.

which case the NLM never sees the examples included in the datastore. The following two datasets are used for the two settings respectively:
WikiText-103 (Merity et al., 2017) is a standard language modeling benchmark from Wikipedia that has 250K word-level vocabulary. It consists of 103M training tokens, and thus leads to a datastore that has 103M records and takes 200G space. Following (Khandelwal et al., 2020), we use the transformer-based (Vaswani et al., 2017) language model checkpoint released by (Baevski and Auli, 2019) as the underlying pretrained NLM, which is trained on the WikiText-103 training split.
Law-MT is an English-German machine translation dataset in the law domain originally released by (Koehn and Knowles, 2017) and resplit by (Aharoni and Goldberg, 2020). We only use the English text for language modeling. The training set consists of 19M tokens which we use to build the datastore that occupies 55G space. To inspect the domain-adaptation performance, our pretrained NLM is a 12-layer transformer model trained on WMT News Crawl6 released by (Ng et al., 2019).
3.2 Setup
Throughout the rest of the paper, we adopt the same hyperparameters and index as (Khandelwal et al., 2020) for kNN-LM.7 Speciﬁcally, the number of nearest neighbors is set to 1024 during evaluation.8 Our pretrained NLMs are the state-of-theart decoder-only transformers as mentioned above, and the key function f (c) to obtain context vectors is the input to the ﬁnal layer’s feedforward network. The context vectors are 1024-dimensional and 1536-dimensional for WikiText-103 and LawMT respectively. Given a dataset, we tune the interpolation weight λ on validation set in terms of the vanilla kNN-LM performance, and ﬁx it unless otherwise speciﬁed. Complete details on the setup can be found in Appendix A.
Evaluation efﬁciency is benchmarked on 32 CPU cores (1.5 GHz AMD EPYC 7282) and 1 NVIDIA RTX 3090 GPU which represents a normalized environment – the index searching uses all the CPU cores while neural network computation is based
6http://data.statmt.org/news-crawl/ 7We directly base our experiments on the original kNNLM implementation. 8The perplexity continues improving as k grows as shown in (Khandelwal et al., 2020) and conﬁrmed by us. Yet k does not have an effect on the evaluation speed in the range [8, 1024] from our observation.

Table 1: Evaluation performance and speed of baseline models. The ppl numbers of kNN-LM ∗ (exact) are from (Khandelwal et al., 2020), which recomputes accurate distances.

Model
kNN-LM ∗ (exact) NLM kNN-LM

WikiText-103 ppl tokens/s

16.12

<1

18.66 16.65

3847 277

Law-MT ppl tokens/s

–

–

106.25 12.32

28K 1052

on the GPU. Running retrieval on 32 CPU cores is also used by the FAISS repo9 as a standard setting to benchmark large-scale retrieval.
3.3 Baseline Speed
We measure the perplexity (ppl) and speed of evaluation in term of tested tokens per second, and Table 1 reports the results on the test set of the two datasets. We also include “kNN-LM (exact)” for reference, which represents the kNN-LM variant that re-computes accurate distances as explained in §2. While very effective with 2 ppl points gains on WikiText-103 and over 90 points gains on LawMT in a domain-adaptation setting, kNN-LM is 10x – 30x slower to evaluate on these datasets because of the extra retrieval step. When exact distances are computed by reading vectors from the disk on-the-ﬂy, kNN-LM (exact) takes over 1 second to evaluate a single token.
4 The Remedies
In this section we propose and explore several different methods that may potentially improve the efﬁciency of kNN-LM along three axes: (1) adaptive retrieval, (2) datastore pruning, and (3) dimension reduction. We analyze the performance of each method on WikiText-103, trying to conclude the best practices that we will evaluate in §5.
4.1 Adaptive Retrieval
Just as humans refer to books only when they are uncertain in an open-book quiz, the parametric NLMs may not always need help from the external datastore. To inspect this hypothesis, we compare pkNN(w|c) and pNLM(w|c) for every token in the WikiText-103 validate set. Interestingly, pkNN(w|c) ≥ pNLM(w|c) only 39% of the time – the likelihood of 61% of the tokens becomes
9https://github.com/facebookresearch/faiss/wiki/ Indexing-1G-vectors

Table 2: The features used to train the retrieval adaptor.

Feature f (c) conf(c) ent(c)
log freq(c[−n :])
log fert(c[−n :])

Description
the context embeddings from pretrained NLM the maximal value (conﬁdence) of pNLM the entropy of the distribution pNLM
log of frequency of the immediate n context tokens computed from the training data. n = 1, 2, 3, 4 which leads to four scalar features.
fert(c[−n :]) is the number of unique word (fertility) that succeeds the immediate n context tokens computed from the training data. n = 1, 2, 3, 4 which leads to four scalar features.

19.0 18.5 18.0 17.5

Random Adaptor

(3847, 18.66), 1 (1567, 18.44), 0.9

(794, 18.02), 0.7

(530, 17.6), 0.5

(1567, 17.59), 0.9

ppl

17.0

(794, 16.99), 0.7

16.5 (277, 16.65), 0
(277, 16.35), 0
16.0 300

(530, 16.67), 0.5

500

1000

tokens/s

2000 3000

Figure 3: Perplexity and speed results of adaptive retrieval on WikiText-103 test set. We annotate the coordinates of some points and the third number in the annotation is the fraction of retrieval operations that are removed.

worse after interpolation despite the overall improvement. This indicates that if we were able to identify these locations perfectly, 61% of the retrieval operations could be removed completely and we would achieve even better perplexity. Inspired by this observation, we aim to automatically identify and prune unnecessary retrieval operations to speed up inference.
Methods: We propose to train a light neural network, the retrieval adaptor, to identify when we should remove the retrieval operation. Speciﬁcally, given the context c as the input, the retrieval adaptor may be trained with either (1) a classiﬁcation objective to predict whether pkNN(w|c) ≥ pNLM(w|c), or (2) a likelihood maximization objective to predict the interpolation weight λ(c) and maximize the overall likelihood of kNN-LM as in Eq. 1. In our preliminary results the classiﬁcation method performs only on par with a random removal baseline, partially due to the discretized noisy supervision. Therefore, we directly maximize the kNN-LM log likelihood by modeling λ as a function of the con-

text:

1 L=
Tt

log p(wt|ct; λθ(ct)) − a · λθ(ct) , (4)

where only θ – the parameters of the retrieval adaptor – are updated. The second term is an L1 regu-

larizer that encourages learning sparse weights for

pkNN, which we ﬁnd helpful to prune unnecessary

retrievals. At inference time, we prune a given frac-

tion of retrievals with the smallest kNN weight λ(c)

by resetting λ(c) to zero. The hyperparameters of

the retrieval adaptor network including the regu-

larizer coefﬁcient, a, are tuned on the validation

set in terms of perplexity at 50% retrieval prun-

ing. Learning the interpolation weights to prune

is related to (Johansen and Socher, 2017) where

they learn to skip text for classiﬁcation tasks. Opti-

mizing the interpolation weights in kNN-LM has

also been applied at training time to train the NLM

jointly (Yogatama et al., 2021).

Architecture and Input Features: The retrieval adaptor is a light MLP network with linear transformation followed by ReLU activation at each layer. The output layer maps the hidden representation to a 2-dimensional vector followed by a LogSoftmax layer to yield log(λ) and log(1−λ) respectively. Complete details on the retrieval adaptor can be found in Appendix A.2. We concatenate several neural and count-based features as input to the retrieval adaptor as shown in Table 2. For the scalar features (basically all the features excluding f (c)) , we found it helpful to map them to a vector with a small network before concatenation. We note that all the features are trivial to obtain at test time – the neural features are from intermediate computation of pNLM(w|c) and count-based features are looked-up values. Ablation analysis on these features can be found in Appendix B.

Training: During training, only the retrieval adaptor is updated while the pretrained NLM is ﬁxed. Note that it is inappropriate to train the retrieval adaptor on the training dataset, which would lead to biased solutions since pNLM may have already overﬁt on the training data and the datastore includes the training example itself. To generalize to the test data, we hold out 10% of the validation data for validation and use the remaining 90% to train the retrieval adaptor. The retrieval adaptor is light and converges quickly; it took several minutes to train it on WikiText-103 with a single GPU.

Results: Figure 3 shows the perplexity and evaluation speed of adaptive retrieval on the test set of WikiText-103, varying the percent of removed retrieval operations. The different threshold values of λ used to cut off retrieval is selected based on the synthetic validation set mentioned above. We also add a random retrieval baseline which uniformly selects a certain fraction of retrieval operations to discard. We observe that adaptive retrieval (AR) exhibits a much ﬂatter increase of perplexity than the random baseline when the number of removed retrievals grows. Notably, AR is able to achieve comparable perplexity to the original kNN-LM model (16.67 vs. 16.65) while being nearly 2x faster (530 vs. 277 tokens/s) through removing 50% of the operations. AR’s gain comes from both the smart pruning mask and optimized λ. We perform an ablation study on this in Appendix B.
4.2 Datastore Pruning
The information present in a large training dataset is often redundant, which suggests that a datastore constructed from training tokens may be pruned with no or only minor performance cost. To validate this hypothesis, we propose several different methods to prune the number of entries and reduce the datastore size:
Random Pruning: As a simple baseline, a certain fraction of the datastore entries are randomly selected. Random pruning has been shown to work well with a billion-scale datastore in machine translation tasks (Khandelwal et al., 2021).
k-Means Pruning: Clustering is a common technique to prune redundant vectors by only keeping the centroids of the clusters. Yet in our task speciﬁcally, we note that a general clustering on the context vectors is not directly applicable since the vectors in the same cluster may still correspond to various target tokens, as language use in context is not deterministic. Therefore, we propose to perform target-aware k-means clustering – for a word wi in the vocabulary, we perform a separate k-means clustering for all the context vectors that have wi as the target token, then we only keep centroids of each cluster as well as saving the cluster size s. The (centroid vector, cluster size, target token) triples form a new compressed datastore. Since we approximate multiple vectors in the same cluster with the centroid and only save the centroid vector once in the new datastore, the

computation of the kNN distribution pkNN needs to be rectiﬁed as:
pkNN(w = y|c)
∝ Ivi=y si · exp(−d(qi, f (c))), (5)
(qi,vi)∈N
the cluster size si acts like weights for each datastore entry. Eq. 5 recovers Eq. 1 when every cluster is of size 1.10 In practice, we perform 5000 separate k-means clustering passes only for the most frequent 5000 words due to high computational cost, which accounts for 84% of all the training tokens. For other vectors we treat each of them as a separate clusters with size 1. The number of clusters in k-means are set to 1/20 of the number of vectors to be clustered, which produces a 5x smaller datastore overall. We did not intensively tune the k-means hyperparameters due to the computational burden. We note that the clustering here is different from the pre-clustering in the ANN index with inverted ﬁle systems mentioned in §2– the index’s pre-clustering does not actually reduce size and is just for lookup.

Algorithm 1 Greedy Merging

1: (K, V) = {(qi, vi)}Ni=1 ← the old datastore

2: s ← 1

weight vector with size N

3: for (qi, vi) ∈ (K, V) do

4:

retrieve

K

neighbors

of

(qi,

vi)

as

{qtk ,

v

t

k

}

K k=1

5: for k = 1, 2, · · · , K do

6:

if stk = 1 & vi = vtk & tk = i then merge

condition

7:

si ← si + 1 merge (qtk , vtk ) into (qi, vi)

8:

stk ← stk − 1

Remove record tk

9:

end if

10: end for

11: end for

12: Save datastore {(qi, vi, si)|si > 0, i = 1, · · · , N }

Greedy Merging: Generally we aim to merge records that share the same target token while being close to each other in vector space. Token-aware clustering is an attempt to achieve this goal, but forcing all points to participate in clustering – and the resulting large clusters – causes some points within the same cluster to be distant in some clusters with high variance. Thus approximating all the vectors with the cluster centroids may lead to large errors. To address this issue, we propose a simple approach, greedy merging (GM), which inspects every record in the datastore and greedily
10In addition, the centroid formulation is roughly equivalent to saving vectors within the same cluster as the centroids multiple times without pruning in the original formulation.

merges their nearest neighbors if a merging condition is satisﬁed. The detailed algorithm is shown in Algorithm 1. Intuitively, GM is density-based to group points with nearest neighbors, but the merging operation only happens locally between a point and its nearest neighbors – it never propagates to merge the nearest neighbors of nearest neighbors unlike typical density-based clustering methods (Ester et al., 1996) which may amplify errors. Similar to k-means pruning, we also compute the weights si of each entry in the compressed datastore to correct pkNN computation using Eq. 5. Without a global clustering mechanism, this approach ensures that the merging vectors are close enough by inspecting only a small number of nearest neighbors. In the following analysis we vary the number of nearest neighbors K within range [2,100] to achieve different compression rates.
Rank-based Pruning: It is well known that embedding spaces contain “hubs” which are nearest neighbors of many other embeddings (Tomasev et al., 2013), and other points that are not nearest neighbors of any other points. We hypothesize that these entries which are rarely nearest neighbors may be removed without signiﬁcant impact on the performance. To verify this assumption, we iterate every (ci, wi) pair in the training data as queries to search their k nearest neighbors from the datastore (k is set to a large number as 1024 here). In this process we compute an “importance score” for every entry in the datastore as g = i 1/ranki, where ranki is the rank of this entry among the nearest neighbors of the query f (ci). rank= +∞ if it is not in the retrieval results. Intutively, the “importance score” up-weights the datastore records that appear more often with lower ranks in the retrieval results. Then we sort all the datastore records in terms of g and remove the ones with small scores, varying the compression rate. This method shares spirit with the technique in (Min et al., 2020) which ﬁlters out the articles that are never retrieved in memoryconstrained open-domain question answering tasks.
Results: Figure 4 demonstrates the perplexity v.s. speed results on Wikitext-103 validation set of different datastore pruning methods described above. Only one solution point is reported for k-means since we do not vary the hyperparameters of kmeans for different compression rate, given that its computational cost is much higher than other methods. Using 20% of the original datstore, k-means

ppl

19.0 18.5

(789, 18.63), 0.2

18.0 NLM (3847, 17.97), 0

17.5

(544, 17.55), 0.4

(1088, 18.04), 0.2 (1013, 17.74), 0.2

17.0

(597, 17.25), 0.4 (598, 16.86), 0.41

16.5
kNN-LM

(446, 16.65), 0.6

16.0

200

400

600

800

tokens/s

Kmeans Random Rank Greedy

1000

1200

Figure 4: Perplexity and speed results of datastore pruning methods on WikiText-103 validation set. We annotate the coordinates of some points and the third number in the annotation is the compression rate (fraction of records remained).

20

(2691, 19.39), 16

19

18

(1657, 18.11), 64

(2337, 18.69), 32 NLM
(3847, 17.97)

ppl

(1049, 17.35), 128

17 kNN-LM
(277, 16.46)

(1016, 16.54), 256

(991, 16.25), 512

16

0

1000

2000

tokens/s

3000

4000

Figure 5: PCA dimension reduction results on WikiText-103 validation set. We annotate the coordinates of the points and the third number in the annotation is the PCA dimensions.

even underperforms the vanilla NLM baseline, suggesting that the cluster centroids approximation may lead to large distance errors which reduce the accuracy of the kNN distribution. Surprisingly, the simple random pruning method outperforms more complicated ones such as k-means and rank-based pruning. The best approach is greedy merging, which demonstrates a relatively ﬂat curve compared with others.
4.3 Dimension Reduction
The context vectors f (c) from large NLMs are often high-dimensional. For example, the pretrained NLMs that we use produce vectors of 1024 and 1536 dimensions in WikiText-103 and Law-MT respectively, which incurs signiﬁcant datastore space and distance computation cost. To mitigate this issue, we empirically explore the effect of dimension reduction in kNN-LM . Speciﬁcally, we use prin-

cipal component analysis (PCA), an efﬁcient and scalable dimension reduction algorithm, to reduce the dimensions and generate a new compressed datastore. We vary the new PCA dimensions as the hyperparameter and report the results.
Results: As shown in Figure 5, the evaluation becomes faster as expected with smaller dimensions, yet a too aggressive compression (dimension < 256) incurs large perplexity cost and even loses advantages over NLM when the dimension is smaller than 128. However, at 256 and 512 dimensions PCA is able to achieve comparable or even better performance than the original 1024-dim vectors, while attaining 3x-4x speed-up.11
5 Putting it All Together
Based on the analysis results in §4, in this section we combine best practices in adaptive retrieval, datastore pruning, and dimension reduction to assess the performance. We select the retrieval pruning rate r, datastore pruning rate n, and the reduced dimensions d on the validation set,12 so that they achieve the largest speed-up at the cost of <= 0.1 perplexity compared to vanilla kNN-LM . We report the results on the test set.
Results: Table 3 shows the results on the test set of WikiText-103 and Law-MT, where we assess the combination of all three different strategies. Separate performance for each strategy is also included for reference points. On WikiText-103, adaptive retrieval is able to remove 50% of the retrieval and achieve nearly 2x speed-up, greedy merging prunes 40% of the datastore at the cost of 0.2 perplexity points. The dimension reduction method PCA leads to a minor improvement of perplexity over kNN-LM while being 3.6x faster. Combination of all the three techniques yields comparable perplexity to vanilla kNN-LM (16.67 v.s. 16.65) and a 6.6x speed-up (1835 v.s. 277).
Different from WikiText-103 where the datastore is contructed from the data that trains the pretrained NLM, in the Law-MT domain adaptation setting the datastore represents the domain-speciﬁc knowledge that the pretrained NLM never sees during
11The tool we use for PCA, the FAISS PCA implementation, applies random rotation to the PCA output vectors by default to re-balance variances of components of a vector (Gong et al., 2012), which may provide additional beneﬁts over vanilla PCA on product vector quantization inside the index.
12Adaptive retrieval uses part of the validation data to training the retrieval adaptor network, thus we select r separately on its own held-out validation and then combine it to others.

Table 3: Perplexity and speed results on the test set of WikiText-103 and Law-MT. AR, GM, DR denote adaptive retrieval, datastore pruning, and dimension reduction respectively, “+All” denotes the combination of all the three technique.

Methods
NLM kNN-LM +AR (r = 0.5) +GM (n = 0.6) +DR (d = 512) +All

ppl tokens/s

WikiText-103

18.66 3847

16.65

277

16.67

530

16.86

446

16.40

991

16.67 1835

NLM NLM (ﬁne-tuned) kNN-LM +AR (r = 0.1) +GM (n = 0.6) +DR (d = 512) +All

Law-MT
106.56 8.61 12.64 12.74
13.33 11.59 12.29

27.8K 27.8K 1052 1290 1451 3420 5708

speedup
13.9x 1x
1.9x 1.6x 3.6x 6.6x
264.3x 264.3x
1x 1.2x 1.4x 3.3x 5.4x

training and thus is critical to produce good perplexity. This may be inferred from by the large ppl gains that the datastore offers (94 points). From another perspective though, the big improvement from the datastore retrieval leads to difﬁculties removing retrieval operations adaptively13 – our learned retrieval adaptor is able to remove only 10% of the retrieval operations costing 0.1 ppl points. Greedy merging is able to prune 40% of the datastore losing 0.7 ppl points. We suspect that the Law-MT datastore is more vulnerable to pruning than the WikiText-103 one because of its smaller size (19M v.s. 103M) and corresponding lack of redundancy. Interestingly, the PCA dimension reduction yields 1 point ppl gain over the vanilla kNN-LM while achieving 3.3x speed-up, consistent with WikiText-103. This implies that a PCA transformation may be able to produce a new vector space that is more appropriate for deﬁning pkNN with L2 distances, we leave the underlying reasons for future work to discuss. Finally, a combination of the three allows kNN-LM to be evaluated 5.4x faster and even obtain superior perplexity.

13This can be reﬂected from the oracle comparison: pkNN(w|c) ≥ pNLM(w|c) 76% of the time compared to 39% in WikiText-103.

6 Implications and Future Work
In this paper, we explore several different ways to improve efﬁciencies of the k-nearest neighbors language model, achieving up to 6x speed-up while attaining comparable performance. As for future work, it is interesting to explore features from the datastore side to better know when to retrieve, and the gap between retrieval-based NLMs and parametric NLMs may be further reduced by combining more optimized indexing methods and the approaches in this paper.
Acknowledgements
We thank the anonymous reviewers for their comments, Emma Strubell, André Martins, Pedro Martins, and Uri Alon for helpful advice and discussions, and Wanzhen He for help with ﬁgure plotting. This material is based upon work supported by the National Science Foundation under Grant 1815287.
References
Roee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. In Proceedings of ACL.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. In Proceedings of AAAI.
Fabien André, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. 2019. Quicker adc: Unlocking the hidden potential of product quantization with simd. IEEE transactions on pattern analysis and machine intelligence.
Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In Proceedings of ICLR.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL.

Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of KDD.
Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. 1999. Similarity search in high dimensions via hashing. In Proceedings of VLDB.
Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. 2012. Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. IEEE transactions on pattern analysis and machine intelligence, 35(12):2916–2929.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization. In Proceedings of ICML.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2018. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450.
Junxian He, Taylor Berg-Kirkpatrick, and Graham Neubig. 2020. Learning sparse prototypes for text generation. In Proceeings of NeurIPS.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128.
Alexander Johansen and Richard Socher. 2017. Learning when to skim and when to read. In Proceedings of the 2nd Workshop on Representation Learning for NLP.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In Proceeings of ICLR.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In Proceedings of ICLR.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.
Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In Proceedings of ICLR.
Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan Cˇ ernocky`, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association.
Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palomaki, et al. 2020. Neurips 2020 efﬁcientqa competition: Systems, analyses and lessons learned. arXiv preprint arXiv:2101.00133.
Marius Muja and David G Lowe. 2009. Fast approximate nearest neighbors with automatic algorithm conﬁguration. In Proceedings of the International Conference on Computer Vision Theory and Applications.
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook fair’s wmt19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1).
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of EMNLP.
Josef Sivic and Andrew Zisserman. 2003. Video google: A text retrieval approach to object matching in videos. In Proceedings of ICCV.
Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. 2012. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association.
Nenad Tomasev, Milos Radovanovic, Dunja Mladenic, and Mirjana Ivanovic. 2013. The role of hubness in clustering high-dimensional data. IEEE transactions on knowledge and data engineering, 26(3):739–751.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NeurIPS.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.

2019. XLNet: Generalized autoregressive pretraining for language understanding. In Proceedings of NeurIPS.
Dani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. 2021. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics, 9:362–373.

A Experimental Setup Details
A.1 General Setup Details
The interpolation hyperparameter λ is tuned in the range [0,1, 0.9] with interval 0.05 on the validation split of each dataset separately. As a result, λ = 0.25 in WikiText-103 and λ = 0.9 in Law-MT.
A.2 Adaptive Retreival
We use the same adaptive retrieval conﬁguration hyperparameters for different datasets, which are validated on the WikiText-103 dev set: the retrieval adaptor is a MLP network with 4 hidden layers, 1 input layer and 1 output layer. Each layer is a linear transformation followed by the ReLU non-linear activation, and a dropout layer with 0.2 dropout probability, except for the output layer where the hidden unites are transformed to 2 dimensions followed by a log softmax to produce log λ and log(1 − λ). The number of hidden units in each layer is 128. Before passing the input features to MLP, we transform each of the scalar features (all the features except for f (c)) into an m-dim vector, where m = dim(f (c))/n and n is the number of scalar feature types. This is to balance the context vector feature and other features. The scalar-feature transformation is performed with an one-layer Linear(in, out)-ReLU-Linear(out, out) network. We also tried using LSTM (Hochreiter and Schmidhuber, 1997) network to capture the temporal relations yet found it leads to very unstable training and fails to converge, though we note that MLP is faster at test time and the f (c) feature already captures the temporal correlations between tokens. The coefﬁcient of the L1 regularizer a is tuned on WikiText-103 validation set among {0.01, 0.05, 0.1, 0.2, 0.5, 1} and ﬁxed as 0.05 for both WikiText-103 and Law-MT. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with learning rate 0.0005. The checkpoint with the best validation perplexity at 50% pruning is saved.
B Ablation Analysis
Input features to retrieval adaptor: We analyze the effect of different input features to the retrieval adaptor by removing a subset of features. We report the perplexities at 50% retrieval pruning, because using different features only has a marginal effect on the evaluation speed. Results on

Table 4: Results of kNN-LM + adaptive retrieval using different input features. The perplexities are based on removing 50% of the retrieval operations after training retrieval adaptor.

Features
f (c) + conf + ent + log freq + log fert -log freq -log fert -ent, conf -f (c) -conf, ent, log freq, log fert

ppl
16.63 16.67 16.72 16.77 16.71 17.03

19.0

18.5

18.0

ppl

17.5

17.0 16.5 16.0

Random mask, constant weight Learned mask, constant weight Random mask, learned weight Learned mask, learned weight

300 500

1000

tokens/s

2000 3000

Figure 6: Perplexity and speed results of adaptive retrieval on WikiText-103 test set. This ﬁgure includes different variants of adaptive retrieval for ablation analysis.

the WikiText-103 test set are shown in Table 4. All features together produce the best results, while the perplexity is relatively robust to removal of a single feature. In our experiments (§4 and §5) we drop off the log freq feature and use the others to save memory while achieving comparable perplexities to using all features.
Effect of learnable interpolation weights: In the adaptive retrieval analysis (§4.1), we observed gains of a learned retrieval adaptor over a random baseline at different fractions of retrieval prunning. However, the advantages may come from two sources: (1) the automatically identiﬁed prunning masks agains the random masks, and (2) the learned interpolation weights on the remaining retrievals against the constant weights that random baseline uses. To separate the two effects, we perform an ablation study to analyze the results of (1) random mask, constant weight (“Random” in §4.1), (2) random mask, learned weight – the weights are from the trained retrieval adaptor, (3) learned mask, constant weight, and (4) learned mask, learned weight (“Adaptor” in §4.1). The results are shown in Figure 6, “learned mask, learned weight” performs

the best. While minor gains are from the automatically learned weights (“Random mask, learned weights”), most of the superiority can be attained with the smart pruning strategy even with constant weights (“Learned mask, constant weights”).

