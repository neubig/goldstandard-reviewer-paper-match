TOWARDS ONLINE END-TO-END TRANSFORMER AUTOMATIC SPEECH RECOGNITION
Emiru Tsunoo1, Yosuke Kashiwagi1, Toshiyuki Kumakura1, Shinji Watanabe2
1Sony Corporation, Japan 2Johns Hopkins University, USA

arXiv:1910.11871v1 [eess.AS] 25 Oct 2019

ABSTRACT
The Transformer self-attention network has recently shown promising performance as an alternative to recurrent neural networks in end-to-end (E2E) automatic speech recognition (ASR) systems. However, Transformer has a drawback in that the entire input sequence is required to compute self-attention. We have proposed a block processing method for the Transformer encoder by introducing a context-aware inheritance mechanism. An additional context embedding vector handed over from the previously processed block helps to encode not only local acoustic information but also global linguistic, channel, and speaker attributes. In this paper, we extend it towards an entire online E2E ASR system by introducing an online decoding process inspired by monotonic chunkwise attention (MoChA) into the Transformer decoder. Our novel MoChA training and inference algorithms exploit the unique properties of Transformer, whose attentions are not always monotonic or peaky, and have multiple heads and residual connections of the decoder layers. Evaluations of the Wall Street Journal (WSJ) and AISHELL-1 show that our proposed online Transformer decoder outperforms conventional chunkwise approaches.
Index Terms— Speech Recognition, End-to-end, Transformer, Self-attention Network, Monotonic Chunkwise Attention
1. INTRODUCTION
End-to-end (E2E) automatic speech recognition (ASR) has been attracting attention as a method of directly integrating acoustic models (AMs) and language models (LMs) because of the simple training and efﬁcient decoding procedures. In recent years, various models have been studied, including connectionist temporal classiﬁcation (CTC) [1–4], attention-based encoder–decoder models [5–9], their hybrid models [10, 11], and the RNN-transducer [12–14]. Transformer [15] has been successfully introduced into E2E ASR by replacing RNNs [16–20], and it outperforms bidirectional RNN models in most tasks [21]. Transformer has multihead self-attention network (SAN) layers, which can leverage a combination of information from completely different positions of the input.
However, similarly to bidirectional RNN models [22], Transformer has a drawback in that the entire utterance is required to compute self-attention, making it difﬁcult to utilize in online recognition systems. Also, the memory and computational requirements of Transformer grow quadratically with the input sequence length, which makes it difﬁcult to apply to longer speech utterances. A simple solution to these problems is block processing as in [17, 19, 23]. However, it loses global context information and its performance is degraded in general.
We have proposed a block processing method for the encoder– decoder Transformer model by introducing a context-aware inheritance mechanism, where an additional context embedding vector

handed over from the previously processed block helps to encode not only local acoustic information but also global linguistic, channel, and speaker attributes [24]. Although it outperforms naive blockwise encoders, the block processing method can only be applied to the encoder because it is difﬁcult to apply to the decoder without knowing the optimal chunk step, which depends on the token unit granularity and the language.
For the attention decoder, various online processes have been proposed. In [5, 25, 26], the chunk window is shifted from an input position determined by the median or maximum of the attention distribution. Monotonic chunkwise attention (MoChA) uses a trainable monotonic energy function to shift the chunk window [27]. MoChA has also been extended to make it stable while training [28] and to be able to change the chunk size adaptively to the circumstances [29]. [30] proposed a unique approach that uses a trigger mechanism to notify the timing of the attention computation. However, to the best of our knowledge, such monotonic chunkwise approaches have not yet been applied to Transformer.
In this paper, we extend our previous context block approach towards an entire online E2E ASR system by introducing an online decoding process inspired by MoChA into the Transformer decoder. Our contributions are as follows. 1) Triggers for shifting chunks are estimated from the source–target attention (STA), which uses queries and keys, 2) all the past information is utilized according to the characteristics of the Transformer attentions that are not always monotonic or locally peaky, and 3) a novel training algorithm of MoChA is proposed, which extends to train the trigger function by dealing with multiple attention heads and residual connections of the decoder layers. Evaluations of the Wall Street Journal (WSJ) and AISHELL-1 show that our proposed online Transformer decoder outperforms conventional chunkwise approaches.
2. TRANSFORMER ASR
The baseline Transformer ASR follows that in [21], which is based on the encoder–decoder architecture. An encoder transforms a T length speech feature sequence x = (x1, . . . , xT ) to an L-length intermediate representation h = (h1, . . . , hL), where L ≤ T due to downsampling. Given h and previously emitted character outputs yi−1 = (y1, . . . , yi−1), a decoder estimates the next character yi.
The encoder consists of two convolutional layers with stride 2 for downsampling, a linear projection layer, positional encoding, followed by Ne encoder layers and layer normalization. Each encoder layer has a multihead SAN followed by a position-wise feedforward network, both of which have residual connections. Layer normalization is also applied before each module. In the SAN, attention weights are formed from queries (Q ∈ Rtq×d) and keys (K ∈ Rtk×d), and applied to values (V ∈ Rtv×d) as

¦§
Outputs (£)

¨©



…

Encoder layers (¡ ¢)

Encoder Layer
Encoder Layer
Encoder Layer

Encoder Layer
Encoder Layer
Encoder Layer

Encoder

…

Layer

… Encoder Layer

… Encoder Layer

Co n t ex t embedding

Downsampled

…

in p ut s

¤¥





Fig. 1. Context inheritance mechanism of the encoder.

QKT

Attention(Q, K, V) = softmax √ V,

(1)

d

where typically d = dmodel/M for the number of heads M . We utilized multihead attention denoted, as the MHD(·) function, as follows:
MHD(Q, K, V) = Concat(head1, . . . , headM )WOn , (2) headm = Attention(QWQn ,m, KWK n ,m, VWVn,m). (3)

In (2) and (3), the nth layer is computed with the projection matrices WQn ,m ∈ Rdmodel×d, WK n ,m ∈ Rdmodel×d, WVn,m ∈ Rdmodel×d, and WOn ∈ RMd×dmodel . For all the SANs in the encoder, Q, K, and V are the same matrices, which are the inputs of the SAN. The position-wise feedforward network is a stack of linear layers.
The decoder predicts the probability of the following character from previous output characters yi−1 and the encoder output h, i.e., p(yi|yi−1, h). The character history sequence is converted to character embeddings. Then, Nd decoder layers are applied, followed by the linear projection and Softmax function. The decoder layer consists of a SAN and a STA, followed by a position-wise feedforward network. The ﬁrst SAN in each decoder layer applies attention weights to the input character sequence, where the input sequence of the SAN is set as Q, K, and V. Then, the following STA attends to the entire encoder output sequence by setting K and V to be the encoder output h.
The SAN can leverage a combination of information from completely different positions of the input. This is due to the multiple heads and residual connections of the layers that complement each other, i.e., some attend monotonically and locally while others attend globally. Transformer requires the entire speech utterance for both the encoder and decoder; thus, they are processed only after the end of the utterance, which causes a huge delay. To realize an online ASR system, both the encoder and decoder are processed online.

3. CONTEXTUAL BLOCK PROCESSING OF ENCODER
A simple way to process the encoder online is blockwise computation, as in [17, 19, 23]. However, the global channel, speaker, and

(a)

(b)

y y

20

20

40

40

60

60

80

80

100

100

120

120

50 100 150 200 250 h

50 100 150 200 250 h

Fig. 2. Examples of attentions in a Transformer decoder layer. (a) is a head having wider attentions, and (b) is a head attending a certain area of h.

linguistic context are also important for local phoneme classiﬁcation.

We have proposed a context inheritance mechanism for block pro-

cessing by introducing an additional context embedding vector [24].

As shown in the tilted arrows in Fig. 1, the context embedding vector

is computed in each layer of each block and handed over to the upper

layer of the following block. Thus, the SAN in each layer is applied

to the block input sequence using the context embedding vector.

The context embedding vector is introduced into the original formulation in Sec. 2. Denoting the context embedding vector as cnb , the augmented variables satisfy Q˜ nb = [Znb −1 cnb −1] and K˜ nb = V˜ bn = [Znb −1 cnb−−11], where the context embedding vector of the previous block (b − 1) of the previous layer (n − 1) is used. Znb is
the output of the nth encoder layer of block b, which is computed simultaneously with the context embedding vector cnb as

[Znb cnb ] = max(0, Z˜ nb,int.W1n + v1n)W2n + v2n + Z˜ nb,int. (4)

Z˜ nb,int. = MHD(Q˜ nb , K˜ nb , V˜ bn) + V˜ bn,

(5)

where W1n, W2n, v1n, and v2n are trainable matrices and biases. The output of the SAN does not only encode input acoustic features but also delivers the context information to the succeeding layer as shown by the tilted red arrows in Fig. 1.

4. ONLINE PROCESS FOR DECODER
4.1. Online Transformer Decoder based on MoChA
The decoder of Transformer ASR is incremental at test time, especially for the ﬁrst SAN of each decoder layer. However, the second STA requires the entire sequence of the encoded features h. Blockwise attention mechanisms cannot be simply applied with a ﬁxed step size, because the step size depends on the output token granularity (grapheme, character, (sub-)word, and so forth) and language. In addition, not all the STAs are monotonic, because the other heads and layers complement each other. Typically, in the lower layer of the Transformer decoder, some heads attend wider areas, and some attend a certain area constantly, as shown in Fig. 2. Therefore, chunk shifting and the chunk size should be adaptive.
For RNN models, the median or maximum of the attention distribution is used as a cue for shifting a ﬁxed-length chunk, where the parameters of the original batch models are reused [5, 25, 26]. MoChA further introduces the probability distribution of chunking to train the monotonic chunking mechanism. In this paper, we propose a novel online decoding method inspired by MoChA.
MoChA [27] splits the input sequence into small chunks over which soft attention is computed. It learns a monotonic alignment between the encoder features h and the output sequence y, with wlength chunking. “Soft” attention is efﬁciently utilized with back-

Algorithm 1 MoChA Inference for n-th Transformer Decoder Layer

Input: encoder features h, length L, chunk size w

1: Initialize: y0 = sos , tm,0 = 1, i = 1

2: while yi−1 = eos do

3: for m = 1 to M do

4:

for j = tm,i−1 to L do

5:

pm,i,j = σ(Energym(zSAN,i, hj ))

6:

if pm,i,j ≥ 0.5 then

7:

tm,i = j

8:

break

9:

end if

10:

end for

11:

if pm,i,j < 0.5, ∀j ∈ {tm,i−1, . . . , L} then

12:

tm,i = tm,i−1

13:

end if

14:

r = tm,i − w + 1 // or r = 1

15:

for k = r to ti do

16:

um,i,k = ChunkEnergym(zSAN,i, hk)

17:

end for

18: headm,i = tki=r tle=ixrpe(xupi,(ku)i,l) vm,k

19: end for

20: zSTA,i = STA(yi−1, head1,i, . . . , headM,i), i = i + 1

21: end while

propagation to train chunking parameters. At the test time, online “hard” chunking is used to realize online ASR, which achieves almost the same performance as the soft attention model.
Since Transformer has unique properties, the conventional MoChA cannot be simply applied. One property is that the STA is computed using queries and keys, while MoChA is formulated on the basis of the attention using a hidden vector of the RNN and tanh. Another property is that not all the STAs are monotonic, because the other heads and layers complement each other, as examples shown in Fig. 2. We modify the training algorithm of MoChA to deal with these characteristics.

4.2. Inference Algorithm

The inference process for decoder layer n is shown in Algorithm 1.

The differences from the original MoChA are highlighted in red

color. In our case, MoChA decoding is introduced into the second

STA of each decoder layer; the vector zSAN,i in Algorithm 1 is the

output of the ﬁrst SAN in the decoder layer. STA(·) in line 20 con-

catenates and computes an output of the STA network, zSTA,i, in

each decoder layer, as in (2). MoChA can be applied independently

to each head; thus, we added line 3. In line 18, the attention weight is

applied to the selected values vm,k = hkWV,m to compute headm

in (3), and the chunk of selection shifts monotonically.

pm,i,j in line 5 is regarded as a trigger function at head m to

move the computing chunk, which is estimated from an Energy

function. For the Energy and ChunkEnergy (in line 16) functions,

the original MoChA utilizes tanh because it is used as a nonlinear

function in RNNs. However, in Transformer, attentions are com-

puted using queries and keys as in (1). Therefore, we modify them

for the head m as

qi,m kjT,m

Energym(zSAN,i, hj ) = gm √

+ rm,

(6)

d||qi,m ||

qi,m kjT,m

ChunkEnergym(zSAN,i, hj) = √ ,

(7)

d

Algorithm 2 MoChA Training for n-th Transformer Decoder Layer

Input: encoder features h, length L, chunk size w, Gauss. noise ǫ

1: Initialize: y0 = sos , α0,0 = 1, α0,k = 0(k = 0), i = 1

2: while yi−1 = eos do

3: for m = 1 to M do

4:

for j = 1 to L do

5:

pm,i,j = σ(Energym(zSAN,i, hj ) + ǫ)

6:

qm,i,j = Lk=j+1(1 − pm,i,k)

7:

αm,i,j = pm,i,j

j k=1

αm,i−1,k

jl=−k1(1 − pm,i,l)

+qm,i,j αm,i−1,j

8:

end for

9:

for j = 1 to L do

10:

um,i,k = ChunkEnergym(zSAN,i, hk)

β = 11:

m,i,j

j+w−1 k=j

αm,i,j exp(um,i,k )

k

exp(u

)

l=k−w+1

m,i,l

12:

end for

13:

headm,i =

L j=1

βm,i,j

vj

14: end for

15: zSTA,i = STA(yi−1, head1,i, . . . , headM,i), i = i + 1

16: end while

where gm and rm are trainable scalar parameters, qi,m = zSAN,iWQ,m, and kj,m = hj WK,m as in (3).
Note that, the exception in lines 11–13, where the trigger never ignites in frame i, sets headm,i as 0 in the original MoChA. However, we compute headm,i using the previous tm,i−1 (line 12) because the exception often occurs in Transformer. Also, for online processing, all the past frames of encoded features h are also available without any latency, while the original MoChA computes attentions within the ﬁxed-length chunk. Taking into account the property that Transformer attentions tend to be distributed widely and are not always monotonic, we also consider utilizing the past frames. We optionally modify line 14 by setting r = 1 and test both cases in Sec. 5.

4.3. Training Algorithm

MoChA strongly relies on the monotonicity of the attentions, and it also forces attentions to be monotonic, while Transformer has a ﬂexible attention mechanism that may be able to integrate information of various positions without the monotonicity. Further more, the Transformer decoder has both multihead and residual connections. Therefore, typically, not all the attentions become monotonic, as in Fig. 2.
The original MoChA training computes a variable αi,j , which is a cumulative probability of computing the local chunk attention at ti = j, deﬁned as

j

j−1

αi,j = pi,j

αi−1,k (1 − pi,l) .

(8)

k=1

l=k

When pi,j ≈ 0 for all j, which occurs frequently in Transformer because the other heads and layers complement each other for this frame, αi,j rapidly decays after i. An example is shown in Fig. 3. The top left shows pm,i,j in Algorithm 1, which has monotonicity. The top right is the original αi,j in (8), in which the value decreases immediately after around frame 50 of the target y and does not recover.
Therefore, we introduce a probability of the trigger not igniting as qm,i,j into computation of αm,i,j . Thus, the new training algorithm for Transformer is shown in Algorithm 2, which encourages

y

y

p i,j original α i,j

20
40
60
80
100
120 50 100 150 200 250 h modified α i,j

y

20
40
60
80
100
120 50 100 150 200 250 h β i,j

20

20

40

40

60

y

60

80

80

100

100

120 50 100 150 200 250 h

120 50 100 150 200 250 h

Fig. 3. Example of expected attention in the Transformer decoder. Top left: pi,j in Algorithm 2; top right: original αi,j in (8); bottom left: our modiﬁed αi,j in Algorithm 2; bottom right: expected attention βi,j . Head index m is omitted for simplicity.

MoChA to exploit the ﬂexibility of the SAN in Transformer (colored lines are new to the original MoChA). An example of our modiﬁed αm,i,j is shown in the bottom left of Fig. 3, which maintains the monotonicity. The bottom right shows the expected attention βm,i,j .
5. EXPERIMENTS
5.1. Experimental Setup
We carried out experiments using the WSJ English and AISHELL1 Mandarin data [31]. The input acoustic features were 80dimensional ﬁlter banks and the pitch, extracted with a hop size of 10 ms and a window size of 25 ms, which were normalized with the global mean and variance. For the WSJ English setup, the number of output classes was 52, including symbols. We used 4,231 character classes for the AISHELL-1 Mandarin setup.
For the training, we utilized multitask learning with CTC loss as in [11, 21] with a weight of 0.1. A linear layer was added onto the encoder to project h to the character probability for the CTC. The Transformer models were trained over 100 epochs for WSJ and 50 epochs for AISHELL-1, with the Adam optimizer and Noam learning rate decay as in [15]. The learning rate was set to 5.0 and the minibatch size to 20. SpecAugment [?] was applied to only WSJ.
The parameters of the last 10 epochs were averaged and used for inference. The encoder had Ne = 12 layers with 2048 units and the decoder had Nd = 6 layers with 2048 units, with both having a dropout rate of 0.1. We set dmodel = 256 and M = 4 for the multihead attentions. We trained three types of Transformer, namely, baseline Transformer [21], Transformer with the contextual block processing encoder (CBP Enc. + Batch Dec.) [24], and the proposed entire online model with the online decoder (CBP Enc. + Proposed Dec.). The training was carried out using ESPNet [32] with the PyTorch backend. The median based chunk shifting [5] with a window of 16 frames was also applied to the Batch Dec. with and without past frames for the fair comparison (CBP Enc. + Median Dec.).
For the CBP Enc. models, we set the parameters as Lblock = 16

Table 1. Word error rates (WERs) in the WSJ and AISHELL-1 evaluation task.

WSJ (WER) AISHELL-1 (CER)

Batch processing

biLSTM [11]

6.7

9.2

uniLSTM

8.4

11.8

Transformer [21]

4.9

6.7

CBP Enc. + Batch Dec. [24]

6.0

7.6

Online processing

CBP Enc. + median Dec. [5]

9.9

25.0

—with past frames

7.9

24.2

CBP Enc. + Proposed Dec.

8.8

18.7

—with past frames

6.6

9.7

and Lhop = 8. For the initialization of context embedding, we utilized the average of the input features to simplify the implementation. The decoder was trained with the proposed MoChA architecture using w = 8. The STA were computed within each chunk, or using all the past frames of encoded features as described in Sec. 4.2.
The decoding was performed alongside the CTC, whose probabilities were added with weights of 0.3 for WSJ and 0.7 for AISHELL-1 to those of Transformer. We performed decoding using a beam search with a beam size of 10. An external word-level LM, which was a single-layer LSTM with 1000 units, was used for rescoring using shallow fusion [33] with a weight of 1.0 for WSJ. A character-level LM with the same structure was fused with a weight of 0.5 for AISHELL-1.
For comparison, unidirectional and bidirectional LSTM models were also trained as in [11]. The models consisted of an encoder with a VGG layer, followed by LSTM layers and a decoder. The numbers of encoder layers were six and three, with 320 and 1024 units for WSJ and AISHELL-1, respectively. The decoders were an LSTM layer with 300 units for WSJ and two LSTM layers with 1024 units for AISHELL-1.
5.2. Results
Experimental results are summarized in Table 1. The chunk hopping using the median of attention worked well in the English task but poorly in the Chinese task. This was because Chinese requires a wider area of the encoded features to emit each character. On the other hand, our proposed decoder prevented the degradation of performance. In particular, using all the past frames of encoded features, our proposed decoder achieved the highest accuracy among the online processing methods. This indicated that the new decoding algorithm was able to exploit the wider attentions of Transformer.
6. CONCLUSION
We extended our previous Transformer, which adopted a contextual block processing encoder, towards an entirely online E2E ASR system by introducing an online decoding process inspired by MoChA into the Transformer decoder. The MoChA training and inference algorithms were extended to cope with the unique properties of Transformer whose attentions are not always monotonic or peaky and have multiple heads and residual connections of the decoder layers. Evaluations of WSJ and AISHELL-1 showed that our proposed online Transformer decoder outperformed conventional chunkwise approaches. Thus, we realize the entire online processing of Transformer ASR with reasonable performance.

7. REFERENCES
[1] Alex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber, “Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in Proc. of the 23rd International Conference on Machine Learning, 2006, pp. 369– 376.
[2] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in International Conference on Machine Learning, 2014, pp. 1764–1772.
[3] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “EESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding,” in Proc. of IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop, 2015, pp. 167–174.
[4] Dario Amodei et al., “Deep Speech 2: End-to-end speech recognition in English and Mandarin,” in Proc. of the 33rd International Conference on Machine Learning, 2016, vol. 48, pp. 173–182.
[5] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems, 2015, pp. 577–585.
[6] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4960– 4964.
[7] Liang Lu, Xingxing Zhang, and Steve Renais, “On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5060– 5064.
[8] Albert Zeyer, Kazuki Irie, Ralf Schlu¨ter, and Hermann Ney, “Improved training of end-to-end attention models for speech recognition,” in Proc. of Interspeech 2018, 2018, pp. 7–11.
[9] Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, et al., “State-of-the-art speech recognition with sequence-to-sequence models,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 4774–4778.
[10] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 4835–4839.
[11] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[12] Alex Graves, “Sequence transduction with recurrent neural networks,” in ICML Representation Learning Workshop, 2012.
[13] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645–6649.
[14] Kanishka Rao, Has¸im Sak, and Rohit Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer,” in Proc. of IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop, 2017, pp. 193–199.
[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems, 2017, pp. 5998–6008.
[16] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-transformer: a norecurrence sequence-to-sequence model for speech recognition,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5884–5888.

[17] Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian Stu¨ker, and Alex Waibel, “Self-attentional acoustic models,” in Proc. of Interspeech, 2018, pp. 3723–3727.
[18] Julian Salazar, Katrin Kirchhoff, and Zhiheng Huang, “Self-attention networks for connectionist temporal classiﬁcation in speech recognition,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 7115–7119.
[19] Linhao Dong, Feng Wang, and Bo Xu, “Self-attention aligner: A latency-control end-to-end model for ASR using self-attention network and chunk-hopping,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 5656– 5660.
[20] Yuanyuan Zhao, Jie Li, Xiaorui Wang, and Yan Li, “The Speechtransformer for large-scale Mandarin Chinese speech recognition,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 7095–7099.
[21] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al., “A comparative study on transformer vs RNN in speech applications,” arXiv preprint arXiv:1909.06317, 2019.
[22] Mike Schuster and Kuldip K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, 1997.
[23] Navdeep Jaitly, David Sussillo, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, and Samy Bengio, “A neural transducer,” arXiv preprint arXiv:1511.04868, 2015.
[24] Emiru Tsunoo, Yosuke Kashiwagi, Toshiyuki Kumakura, and Shinji Watanabe, “Transformer ASR with contextual block processing,” arXiv preprint arXiv:1910.07204, 2019.
[25] William Chan and Ian Lane, “On online attention-based speech recognition and joint Mandarin character-Pinyin training,” in Proc. of Interspeech, 2016, pp. 3404–3408.
[26] Andre´ Merboldt, Albert Zeyer, Ralf Schlu¨ter, and Hermann Ney, “An analysis of local monotonic attention variants,” Proc. of Interspeech 2019, pp. 1398–1402, 2019.
[27] Chung-Cheng Chiu and Colin Raffel, “Monotonic chunkwise attention,” arXiv preprint arXiv:1712.05382, 2017.
[28] Haoran Miao, Gaofeng Cheng, Pengyuan Zhang, Ta Li, and Yonghong Yan, “Online hybrid CTC/attention architecture for end-to-end speech recognition,” Proc. of Interspeech 2019, pp. 2623–2627, 2019.
[29] Ruchao Fan, Pan Zhou, Wei Chen, Jia Jia, and Gang Liu, “An online attention-based model for speech recognition,” Proc. of Interspeech 2019, pp. 4390–4394, 2019.
[30] Niko Moritz, Takaaki Hori, and Jonathan Le Roux, “Triggered attention for end-to-end speech recognition,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 5666–5670.
[31] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng, “AIShell1: An open-source Mandarin speech corpus and a speech recognition baseline,” in Oriental COCOSDA, 2017, pp. 1–5.
[32] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. of Interspeech, 2019, pp. 2207–2211.
[33] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, ZhiJeng Chen, and Rohit Prabhavalkar, “An analysis of incorporating an external language model into a sequence-to-sequence model,” in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5824–5828.

