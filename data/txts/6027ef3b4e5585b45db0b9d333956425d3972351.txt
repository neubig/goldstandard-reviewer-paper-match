NAACL 2021
Differentiable Open-Ended Commonsense Reasoning
Bill Yuchen Lin1∗, Haitian Sun2, Bhuwan Dhingra2, Manzil Zaheer2, Xiang Ren1, William W. Cohen2
1 University of Southern California 2 Google Research
{yuchen.lin,xiangren}@usc.edu {haitiansun,bdhingra,manzilzaheer,wcohen}@google.com

arXiv:2010.14439v2 [cs.CL] 6 Jun 2021

Abstract
Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic and useful, we propose to study open-ended commonsense reasoning (OpenCSR) — the task of answering a commonsense question without any predeﬁned choices — using as a resource only a knowledge corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DRFACT, an efﬁcient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt three popular multiple-choice datasets, and collect multiple new answers to each test question via crowd-sourcing. Experiments show that DRFACT outperforms strong baseline methods by a large margin.1
1 Introduction
The conventional task setting for most current commonsense reasoning research is multiplechoice question answering (QA) — i.e., given a question and a small set of pre-deﬁned answer choices, models are required to determine which of the candidate choices best answers the question. Existing commonsense reasoning models usually work by scoring a question-candidate pair (Lin et al., 2019; Lv et al., 2020; Feng et al., 2020). Hence, even an accurate multiple-choice
∗ The work was mainly done during Bill Yuchen Lin’s internship at Google Research.
1Our code and data are available at the project website — https://open-csr.github.io/. The human annotations were collected by the USC-INK group.

Q: What can help alleviate global warming?
Multiple-Choice CSR
(reason w/ question+choice)
Open-Ended CSR (A) air cooler (B) fossil fuel
(reason w/ question) (C) renewable energy (D) water
a large text corpus of commonsense facts
carbon dioxide is the major greenhouse Multi-Hop gas contributing to global warming .
Reasoning
trees remove carbon dioxide from the atmosphere through photosynthesis .
Output: …, renewable energy, tree, solar battery, …
a ranked list of concepts as answers.
Figure 1: We study the task of open-ended commonsense reasoning (OpenCSR), where answer candidates are not provided (as in a multiple-choice setting). Given a question, a reasoner uses multi-hop reasoning over a knowledge corpus of facts, and outputs a ranked list of concepts from the corpus.
QA model cannot be directly applied in practical applications where answer candidates are not provided (e.g., answering a question asked on a search engine, or during conversation with a chat-bot).
Because we seek to advance commonsense reasoning towards practical applications, we propose to study open-ended commonsense reasoning (OpenCSR), where answers are generated efﬁciently, rather than selected from a small list of candidates (see Figure 1). As a step toward this, here we explore a setting where the model produces a ranked list of answers from a large question-independent set of candidate concepts that are extracted ofﬂine from a corpus of common-sense facts written in natural language.
The OpenCSR task is inherently challenging. One problem is that for many questions, ﬁnding an answer requires reasoning over two or

more natural-language facts from a corpus. In the multiple-choice QA setting, as the set of candidates is small, we can pair a question with an answer, and use the combination to retrieve relevant facts and then reason with them. In the open-ended setting, this is impractical: instead one needs to retrieve facts from the corpus using the question alone. In this respect, OpenCSR is similar to multi-hop factoid QA about named entities, e.g. as done for HotpotQA (Yang et al., 2018).
However, the underlying reasoning chains of most multi-hop factoid QA datasets are relatively clear and context-independent, and are thus easier to infer. Commonsense questions, in contrast, exhibit more variable types of reasoning, and the relationship between a question and the reasoning to answer the question is often unclear. (For example, a factoid question like “who starred in a movie directed by Bradley Cooper?” clearly suggests following a directed-by relationship and then a starred-in relationship, while the underlying reasoning chains of a question like “what can help alleviate global warming?” is relatively implicit from the question.) Furthermore, annotations are not available to identify which facts are needed in the latent reasoning chains that lead to an answer — the only supervision is a set of questions and their answers. We discuss the formulation of OpenCSR and its challenges further in Section 3.
As shown in Fig. 1, another challenge is that many commonsense questions require reasoning about facts that link several concepts together. E.g., the fact “trees remove carbon dioxide from the atmosphere through photosynthesis” cannot be easily decomposed into pairwise relationships between “trees”, “carbon dioxide”, “the atmosphere”, and “photosynthesis”, which makes it more difﬁcult to store in a knowledge graph (KG). However, such facts have been collected as sentences in common-sense corpora, e.g., GenericsKB (Bhakthavatsalam et al., 2020). This motivates the question: how can we conduct multi-hop reasoning over such a knowledge corpus, similar to the way multi-hop reasoning methods traverse a KG? Moreover, can we achieve this in a differentiable way, to support end-to-end learning?
To address this question, we extend work by Seo et al. (2019) and Dhingra et al. (2020), and propose an efﬁcient, differentiable multi-hop reasoning method for OpenCSR, named DRFACT (for Differentiable Reasoning over Facts). Speciﬁcally,

we formulate multi-hop reasoning over a corpus as an iterative process of differentiable fact-following operations over a hypergraph. We ﬁrst encode all fact sentences within the corpus as dense vectors to form a neural fact index, such that a fast retrieval can be done via maximum inner product search (MIPS). This dense representation is supplemented by a sparse fact-to-fact matrix to store symbolic links between facts (i.e., a pair of facts are linked if they share common concepts). DRFACT thus merges both neural and symbolic aspects of the relationships between facts to model reasoning in an end-to-end differentiable framework (Section 4).
To evaluate OpenCSR methods, we construct new OpenCSR datasets by adapting three existing multiple-choice QA datasets: QASC (Khot et al., 2020), OBQA (Mihaylov et al., 2018), and ARC (Clark et al., 2018). Note that unlike factoid questions that usually have a single correct answer, open-ended commonsense questions can have multiple correct answers. Thus, we collect a collection of new answers for each test question by crowd-sourcing human annotations. We compare with several strong baseline methods and show that our proposed DRFACT outperforms them by a large margin. Overall DRFACT gives an 4.6% absolute improvement in Hit@100 accuracy over DPR (Karpukhin et al., 2020), a state-of-the-art text retriever for QA, and 3.2% over DrKIT (Dhingra et al., 2020), a strong baseline for entitycentric multi-hop reasoning. With a relatively more expensive re-ranking module, the gap between DRFACT and others is even larger. (Sec. 5)
2 Related Work
Commonsense Reasoning. Many recent commonsense-reasoning (CSR) methods focus on multiple-choice QA. For example, KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) use an external commonsense knowledge graph as structural priors to individually score each choice. These methods, though powerful in determining the best choice for a multi-choice question, are less realistic for practical applications where answer candidates are typically not available. UniﬁedQA (Khashabi et al., 2020) and other closed-book QA models (Roberts et al., 2020) generate answers to questions by ﬁne-tuning a text-to-text transformer such as BART (Lewis et al., 2020a) or T5 (Raffel et al., 2020), but a

disadvantage of closed-book QA models is that they do not provide intermediate explanations for their answers, i.e., the supporting facts, which makes them less trustworthy in downstream applications. Although closed-book models exist that are augmented with an additional retrieval module (Lewis et al., 2020b), these models mainly work for single-hop reasoning.
QA over KGs or Text. A conventional source of commonsense knowledge is triple-based symbolic commonsense knowledge graphs (CSKGs) such as ConceptNet (Speer et al., 2017). However, the binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or more concepts. Formalized in this way, OpenCSR is a question answering task requiring (possibly) iterative retrieval, similar to other open-domain QA tasks (Chen et al., 2017) such as HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). As noted above, however, the surface of commonsense questions in OpenCSR have fewer hints about kinds of multi-hop reasoning required to answer them than the factoid questions in open-domain QA, resulting in a particularly challenging reasoning problem (see Sec. 3).
Multi-Hop Reasoning. Many recent models for open-domain QA tackle multi-hop reasoning through iterative retrieval, e.g., GRAFT-Net (Sun et al., 2018), MUPPET (Feldman and El-Yaniv, 2019), PullNet (Sun et al., 2019), and GoldEn (Qi et al., 2019). These models, however, are not endto-end differentiable and thus tend to have slower inference speed, which is a limitation shared by many other works using reading comprehension for multi-step QA (Das et al., 2019; Lee et al., 2019). As another approach, Neural Query Language (Cohen et al., 2020) designs differentiable multi-hop entity-following templates for reasoning over a compactly stored symbolic KG, but this KG is limited to binary relations between entities from an explicitly enumerated set.
DrKIT (Dhingra et al., 2020) is the most similar work to our DRFACT, as it also supports multi-hop reasoning over a corpus. Unlike DRFACT, DrKIT is designed for entity-centric reasoning. DrKIT

begins with an entity-linked corpus, and computes both sparse and dense indices of entity mentions (i.e., linked named-entity spans). DrKIT’s fundamental reasoning operation is to “hop” from one weighted set of X entities to another, by 1) ﬁnding mentions of new entities x that are related to some entity in X, guided by the indices, and then 2) aggregating these mentions to produce a new weighted set of entities. DrKIT’s operations are differentiable, and by learning to construct appropriate queries to the indices, it can be trained to answer multi-hop entity-related questions.
Prior to our work DrKIT been applied only on factoid questions about named entities. In CSR, the concepts that drive reasoning are generally less precise than entities, harder to disambiguate in context, and are also much more densely connected, so it is unclear to what extent DrKIT would be effective. We present here novel results using DrKIT on OpenCSR tasks, and show experimentally that our new approach, DRFACT, improves over DrKIT. DRFACT mainly differs from DrKIT in that its reasoning process learns to “hop” from one fact to another, rather than from one entity to another, thus effectively using the full information from a fact for multi-hop reasoning.
3 Open-Ended Commonsense Reasoning
Task Formulation. We denote a corpus of knowledge facts as F, and use V to denote a vocabulary of concepts; both are sets consisting of unique elements. A fact fi ∈ F is a sentence that describes generic commonsense knowledge, such as “trees remove carbon dioxide from the atmosphere through photosynthesis.” A concept cj ∈ V is a noun or base noun phrase mentioned frequently in these facts (e.g., ‘tree’ and ‘carbon dioxide’). Concepts are considered identical if their surface forms are the same (after lemmatization). Given only a question q (e.g., “what can help alleviate global warming?”), an openended commonsense reasoner is supposed to answer it by returning a weighted set of concepts, such as {(a1=‘renewable energy’, w1), (a2=‘tree’, w2), . . . }, where wi ∈ R is the weight of the predicted concept ai ∈ V.
To learn interpretable, trustworthy reasoning models, it is expected that models can output intermediate results that justify the reasoning process — i.e., the supporting facts from F. E.g., an explanation for ‘tree’ to be an answer to the ques-

= trees remove carbon dioxide from the atmosphere through photosynthesis .

tree photosynthesis

= carbon dioxide is the major greenhouse gas contributing to global warming .

carbon dioxide atmosphere

water

oxygen

global warming

Modeling a knowledge corpus as a hypergraph.

greenhouse gas

= the atmosphere contains oxygen, carbon dioxide, and water.

Question: What can help
alleviate global warming?
tree
DrFact: Multi-hop reasoning as
recursive fact-following operations.

Figure 2: A motivating example of how DrFact works for OpenCSR. We model the knowledge corpus as a hypergraph consisting of concepts in V as nodes and facts in F as hyperedges. Then, we develop a differentiable reasoning method, DrFact, to perform multi-hop reasoning via fact-following operations (e.g., f1 → f2).

tion above can be the combination of two facts: f1 = “carbon dioxide is the major ...” and f2 = “trees remove ...”, as shown in Figure 1.
Implicit Multi-Hop Structures. Commonsense questions (i.e., questions that need commonsense knowledge to reason) contrast with betterstudied multi-hop factoid QA datasets, e.g., HotpotQA (Yang et al., 2018), which primarily focus on querying about evident relations between named entities. For example, an example multihop factoid question can be “which team does the player named 2015 Diamond Head Classic’s MVP play for?” Its query structure is relatively clear and self-evident from the question itself: in this case the reasoning process can be decomposed into q1 = “the player named 2015 DHC’s MVP” and q2 = “which team does q1. answer play for”.
The reasoning required to answer commonsense questions is usually more implicit and relatively unclear. Consider the previous example in Fig. 1, q = ‘what can help alleviate global warming?’ can be decomposed by q1 = “what contributes to global warming” and q2 = “what removes q1. answer from the atmosphere” — but many other decompositions are also plausible. In addition, unlike HotpotQA, we assume that we have no ground-truth justiﬁcations for training, which makes OpenCSR even more challenging.
4 DrFact: An Efﬁcient Approach for Differentiable Reasoning over Facts
In this section we present DRFACT, a model for multi-hop reasoning over facts. More implementation details are in Appendix B.

4.1 Overview
In DRFACT, we propose to model reasoning as traversing a hypergraph, where each hyperedge corresponds to a fact in F, and connects the concepts in V that are mentioned in that fact. This is shown in Figure 2. Notice that a fact, as a hyperedge, connects multiple concepts that are mentioned, while the textual form of the fact maintains the contextual information of the original natural language statement, and hence we do not assume a ﬁxed set of relations.
Given such a hypergraph, our open-ended reasoning model will traverse the hypergraph starting from the question (concepts) and ﬁnally arrive at a set of concept nodes by following multiple hyperedges (facts). A probabilistic view of this process over T hops is:

P (c | q) = P (c | q, FT )

T t=1

P (Ft

|

q,

Ft−1)P

(F0

|

q)

Intuitively, we want to model the distribution of a concept c ∈ V being an answer to a question q as P (c | q). This answering process can be seen as a process of multiple iterations of “fact-following,” or moving from one fact to another based on shared concepts, and ﬁnally moving from facts to concepts. We use Ft to represent a weighted set of retrieved facts at the hop t, and F0 for the initial facts below. Then, given the question and the current retrieved facts, we iteratively retrieve the facts for the next hop. Finally, we score a concept using retrieved facts.

4.2 Pre-computed Indices
Dense Neural Fact Index D. We pre-train a bi-encoder architecture over BERT (Devlin et al.,

Concept-to-Fact

!$

#

|< l a t e x i t s h a 1 _ b a s e 6 4 = " 8 z L Q J 1 N f l b b + E + 4 r R v n i E + D g 8 w 0 = " > A A A C C 3 i c b V D L S s N A F J 3 4 r P U V d e l m a B E E o S S C 6 D I o i M s K 9 g F N K J P p p B 0 6 m Y S Z i R D S 7 N 2 4 8 z v c u F D E r T / g r n / j p C 2 o r Q c u H M 6 5 l 3 v v 8 W N G p b K s s b G 0 v L K 6 t l 7 a K G 9 u b e / s m n v 7 T R k l A p M G j l g k 2 j 6 S h F F O G o o q R t q x I C j 0 G W n 5 w 6 v C b 9 0 T I W n E 7 1 Q a E y 9 E f U 4 D i p H S U t e s j N w Q q Q F G L G v m I + g q G h I J f 8 T r f N Q 1 q 1 b N m g A u E n t G q k 7 F P X k a O 2 m 9 a 3 6 5 v Q g n I e E K M y R l x 7 Z i 5 W V I K I o Z y c t u I k m M 8 B D 1 S U d T j v R K L 5 v 8 k s M j r f R g E A l d X M G J + n s i Q 6 G U a e j r z u J G O e 8 V 4 n 9 e J 1 H B h Z d R H i e K c D x d F C Q M q g g W w c A e F Q Q r l m q C s K D 6 V o g H S C C s d H x l H Y I 9 / / I i a Z 7 W 7 L O a d W t X n U s w R Q k c g g o 4 B j Y 4 B w 6 4 A X X Q A B g 8 g G f w C t 6 M R + P F e D c + p q 1 L x m z m A P y B 8 f k N H k 2 e t Q = = < / l a t e x i t > V

Sparse Matrix
! DrFact $ // 1. Initial Facts.
| ⇥ |F |

for

t< l a t e x i t s h a 1 _ b a s e 6 4 = " Y r b P 6 3 7 L W / m I v b u 3 n B H / T E 2 c j m Y = " > A A A B 6 X i c b Z B N S 8 N A E I Y n 9 a v G r 6 p H L 8 E i e C q J I H o R i 1 4 8 V r E f 0 I a y 2 W 7 a p Z t N 2 J 0 I J f Q f e P G g i N f + G O 9 e x H / j p u 1 B q y 8 s P L z v D D s z Q S K 4 R t f 9 s g p L y y u r a 8 V 1 e 2 N z a 3 u n t L v X 0 H G q K K v T W M S q F R D N B J e s j h w F a y W K k S g Q r B k M r / O 8 + c C U 5 r G 8 x 1 H C / I j 0 J Q 8 5 J W i s O 7 S 7 p b J b c a d y / o I 3 h / L l u 3 2 R T D 7 t W r f 0 0 e n F N I 2 Y R C q I 1 m 3 P T d D P i E J O B R v b n V S z h N A h 6 b O 2 Q U k i p v 1 s O u n Y O T J O z w l j Z Z 5 E Z + r + 7 M h I p P U o C k x l R H C g F 7 P c / C 9 r p x i e + x m X S Y p M 0 t l H Y S o c j J 1 8 b a f H F a M o R g Y I V d z M 6 t A B U Y S i O U 5 + B G 9 x 5 b / Q O K l 4 p x X 3 1 i t X r 2 C m I h z A I R y D B 2 d Q h R u o Q R 0 o h P A I z / B i D a 0 n 6 9 V 6 m 5 U W r H n P P v y S N f k G d e G Q T g = = < / l a t e x i t >

in [1, … , %]{ //

2.

Fact-Follow.

< l a t e x i t s h a 1 _ b a s e 6 4 = " 6 9 2 O s z H 2 o g 0 R y r v m G v H B b K j K T m g = " > A A A C G H i c b V B N S 8 N A E N 3 4 b f 2 q e v S y W A Q F r Y k o e h G K Q v G o Y F V o S t l s J + 3 i J h t 3 J 2 o J + R l e / C t e P C j i t T f / j d v a g 1 8 P B h 7 v z T A z L 0 i k M O i 6 H 8 7 I 6 N j 4 x O T U d G F m d m 5 + o b i 4 d G F U q j n U u J J K X w X M g B Q x 1 F C g h K t E A 4 s C C Z f B 9 X H f v 7 w F b Y S K z 7 G b Q C N i 7 V i E g j O 0 U r O 4 X W 1 m m B / 6 C P e Y V R n H r a q S U t 3 l v o Q Q 1 / v u l p d v 0 h t f i 3 Y H N 5 r F k l t 2 B 6 B / i T c k J T L E a b P Y 8 1 u K p x H E y C U z p u 6 5 C T Y y p l F w C X n B T w 0 k j F + z N t Q t j V k E p p E N H s v p m l V a N F T a V o x 0 o H 6 f y F h k T D c K b G f E s G N + e 3 3 x P 6 + e Y n j Q y E S c p A g x / 1 o U p p K i o v 2 U a E t o 4 C i 7 l j C u h b 2 V 8 g 7 T N h + b Z c G G 4 P 1 + + S + 5 2 C l 7 e 2 X 3 b L d U O R r G M U V W y C p Z J x 7 Z J x V y Q k 5 J j X D y Q J 7 I C 3 l 1 H p 1 n 5 8 1 5 / 2 o d c Y Y z y + Q H n N 4 n g p O g C A = = < / l a t e x i t >
Ft

= Fact-Follow (Ft

1, q)

A< l a t e x i t s h a 1 _ b a s e 6 4 = " j 2 N X o Y o Y R d q w v r D p s q G a w 4 G o L x 4 = " > A A A B 8 X i c b Z D L S g M x F I Y z 9 V b r r S q 4 c R M s g q s y I 4 h u h F p R X L Z g L 9 g O Q y b N t K G Z z J C c E c r Q t 3 D j Q h G 3 4 l v 4 B O 7 c + C y m l 4 W 2 / h D 4 + P 9 z y D n H j w X X Y N t f V m Z h c W l 5 J b u a W 1 v f 2 N z K b + / U d Z Q o y m o 0 E p F q + k Q z w S W r A Q f B m r F i J P Q F a / j 9 y 1 H e u G d K 8 0 j e w i B m b k i 6 k g e c E j D W 3 Y U H + B x f X X v g 5 Q t 2 0 R 4 L z 4 M z h U J p r / r N 3 8 s f F S / / 2 e 5 E N A m Z B C q I 1 i 3 H j s F N i Q J O B R v m 2 o l m M a F 9 0 m U t g 5 K E T L v p e O I h P j R O B w e R M k 8 C H r u / O 1 I S a j 0 I f V M Z E u j p 2 W x k / p e 1 E g j O 3 J T L O A E m 6 e S j I B E Y I j x a H 3 e 4 Y h T E w A C h i p t Z M e 0 R R S i Y I + X M E Z z Z l e e h f l x 0 T o p 2 1 S m U y m i i L N p H B + g I O e g U l d A N q q A a o k i i B / S E n i 1 t P V o v 1 u u k N G N N e 3 b R H 1 l v P 9 w O k 2 8 = < / l a t e x i t > t

= EFt

// 3. Emit Concepts.

}
< l a t e x i t s h a 1 _ b a s e 6 4 = " T j E i y g I G S w o w z d l U k b x f y A K 6 Y / U = " > A A A C B n i c b V C 7 S g N B F J 3 1 G d f X q q U I g y F g F X Y F 0 S a Q a G M Z I S / I x m V 2 M k m G z D 6 Y u S u E J Z W N P + E H 2 F g o Y u s 3 2 K g f Y u / k U W j i g Q u H c + 7 l 3 n v 8 W H A F t v 1 l L C w u L a + s Z t b M 9 Y 3 N r W 1 r Z 7 e m o k R S V q W R i G T D J 4 o J H r I q c B C s E U t G A l + w u t + / G P n 1 G y Y V j 8 I K D G L W C k g 3 5 B 1 O C W j J s w 5 K u I B d l Q R e C g V n e F 3 B L h F x j 3 i A S x 5 4 V t b O 2 2 P g e e J M S b a Y + / 7 8 u D e 7 Z c 9 6 d 9 s R T Q I W A h V E q a Z j x 9 B K i Q R O B R u a b q J Y T G i f d F l T 0 5 A E T L X S 8 R t D n N N K G 3 c i q S s E P F Z / T 6 Q k U G o Q + L o z I N B T s 9 5 I / M 9 r J t A 5 a 6 U 8 j B N g I Z 0 s 6 i Q C Q 4 R H m e A 2 l 4 y C G G h C q O T 6 V k x 7 R B I K O j l T h + D M v j x P a s d 5 5 y R v X z n Z 4 j m a I I P 2 0 S E 6 Q g 4 6 R U V 0 i c q o i i i 6 R Q / o C T 0 b d 8 a j 8 W K 8 T l o X j O n M H v o D 4 + 0 H u Z + b y A = = < / l a t e x i t >
A

=

PTt=1 ↵tAt

//

4.

Final

answers.

!

|< l a t e x i t s h a 1 _ b a s e 6 4 = " O q o C w i n 5 r t i K t 4 D E 3 a U 3 O t v s O W 0 = " > A A A B / n i c b V D L S s N A F L 3 x W e s r K q 7 c D B b B V U l E 0 W V R E J c V 7 A O a U C b T S T t 0 M g k z E 6 G k B X / F j Q t F 3 P o d 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 2 l 5 Z X V t f X S R n l z a 3 t n 1 9 7 b b 6 o 4 l Y Q 2 S M x j 2 Q 6 w o p w J 2 t B M c 9 p O J M V R w G k r G N 7 k f u u R S s V i 8 a B H C f U j 3 B c s Z A R r I 3 X t w 7 E X Y T 0 g m G e 3 k 7 G n W U Q V 6 n X t i l N 1 p k C L x C 1 I B Q r U u / a X 1 4 t J G l G h C c d K d V w n 0 X 6 G p W a E 0 0 n Z S x V N M B n i P u 0 Y K r A 5 4 2 f T + B N 0 Y p Q e C m N p n t B o q v 7 e y H C k 1 C g K z G S e V c 1 7 u f i f 1 0 l 1 e O V n T C S p p o L M D o U p R z p G e R e o x y Q l m o 8 M w U Q y k x W R A Z a Y a N N Y 2 Z T g z n 9 5 k T T P q u 5 F 1 b k / r 9 S u i z p K c A T H c A o u X E I N 7 q A O D S C Q w T O 8 w p v 1 Z L 1 Y 7 9 b H b H T J K n Y O 4 A + s z x + r 6 J X v < / l a t e x i t > F

|

⇥

d

!!"#

!

Dense Index of Fact Vectors

|< l a t e x i t s h a 1 _ b a s e 6 4 = " p + m 3 D v 0 E d 4 B u 2 f H m F k C u L R X 1 g 8 4 = " > A A A B 9 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l R l R d F k U x G U F + 4 B 2 K J k 0 0 4 Z m k j H J F M q 0 3 + H G h S J u / R h 3 / o 2 Z d h b a e i B w O O d e 7 s k J Y s 6 0 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 t E w U o X U i u V S t A G v K m a B 1 w w y n r V h R H A W c N o P h b e Y 3 R 1 R p J s W j G c f U j 3 B f s J A R b K z k T z o R N g O C e X o 3 n X R L Z b f i z o C W i Z e T M u S o d U t f n Z 4 k S U S F I R x r 3 f b c 2 P g p V o Y R T q f F T q J p j M k Q 9 2 n b U o E j q v 1 0 F n q K T q 3 S Q 6 F U 9 g m D Z u r v j R R H W o + j w E 5 m G f W i l 4 n / e e 3 E h N d + y k S c G C r I / F C Y c G Q k y h p A P a Y o M X x s C S a K 2 a y I D L D C x N i e i r Y E b / H L y 6 R x X v E u K + 7 D R b l 6 k 9 d R g G M 4 g T P w 4 A q q c A 8 1 q A O B J 3 i G V 3 h z R s 6 L 8 + 5 8 z E d X n H z n C P 7 A + f w B R W 2 S b A = = < / l a t e x i t > F

|

|< l a t e x i t s h a 1 _ b a s e 6 4 = " o M 1 6 2 e A 1 Z l O + V Y W n v H j 2 V I 8 a x 9 M = " > A A A C C X i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 W N R E I 8 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 0 J J e / X i X / H i Q R G v / g N v / h s 3 b Q 6 2 9 c H A 4 7 0 Z Z u b 5 s e A a H O f H K q y s r q 1 v F D d L W 9 s 7 u 3 v 2 / k F D R 4 m i r E 4 j E a m W T z Q T X L I 6 c B C s F S t G Q l + w p j + 8 y f z m I 1 O a R / I B R j H z Q t K X P O C U g J G 6 N h 5 3 Q g I D S k R 6 O x l 3 g I d M z 0 l d u + x U n C n w M n F z U k Y 5 a l 3 7 u 9 O L a B I y C V Q Q r d u u E 4 O X E g W c C j Y p d R L N Y k K H p M / a h k p i N n r p 9 J M J P j F K D w e R M i U B T 9 W / E y k J t R 6 F v u n M b t S L X i b + 5 7 U T C K 6 8 l M s 4 A S b p b F G Q C A w R z m L B P a 4 Y B T E y h F D F z a 2 Y D o g i F E x 4 J R O C u / j y M m m c V d y L i n N / X q 5 e 5 3 E U 0 R E 6 R q f I R Z e o i u 5 Q D d U R R U / o B b 2 h d + v Z e r U + r M 9 Z a 8 H K Z w 7 R H K y v X y y b m 0 U = < / l a t e x i t > F

|

⇥

|F |

"
Sparse Matrix of Fact Links

d< l a t e x i t s h a 1 _ b a s e 6 4 = " p E w l D p + F c 5 G j f 2 F n 9 f I o g c A v O c g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 W P R i 8 c W 7 A e 0 o W w 2 k 3 b t Z h N 2 N 0 I p / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A q u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 k m m G D Z Z I h L V C a h G w S U 2 D T c C O 6 l C G g c C 2 8 H o b u a 3 n 1 B p n s g H M 0 7 R j + l A 8 o g z a q z U C P v l i l t 1 5 y C r x M t J B X L U + + W v X p i w L E Z p m K B a d z 0 3 N f 6 E K s O Z w G m p l 2 l M K R v R A X Y t l T R G 7 U / m h 0 7 J m V V C E i X K l j R k r v 6 e m N B Y 6 3 E c 2 M 6 Y m q F e 9 m b i f 1 4 3 M 9 G N P + E y z Q x K t l g U Z Y K Y h M y + J i F X y I w Y W 0 K Z 4 v Z W w o Z U U W Z s N i U b g r f 8 8 i p p X V S 9 q 6 r b u K z U b v M 4 i n A C p 3 A O H l x D D e 6 h D k 1 g g P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D y T u M 7 A = = < / l a t e x i t >

< l a t e x i t s h a 1 _ b a s e 6 4 = " i R V o y / e A i k 5 + A 8 L o f + h / W Z 8 8 a o 8 = " > A A A C K H i c b Z D L S s N A F I Y n 9 V 5 v V Z d u B o t Q Q U s i i m 5 E 0 Y 1 L B X u B J o T J d N I O T i 7 O n A g 1 5 H H c + C p u R B R x 6 5 M 4 q S l o 6 w 8 D P 9 8 5 h z n n 9 2 L B F Z j m p 1 G a m p 6 Z n Z t f K C 8 u L a + s V t b W m y p K J G U N G o l I t j 2 i m O A h a w A H w d q x Z C T w B G t 5 t x d 5 v X X P p O J R e A O D m D k B 6 Y X c 5 5 S A R m 7 l t G c L 5 k P N D g j 0 P T 9 9 y N w U 9 q x s F 4 / I X U 4 y W / J e H 3 Z O R r R f 9 L m V q l k 3 h 8 K T x i p M F R W 6 c i u v d j e i S c B C o I I o 1 b H M G J y U S O B U s K x s J 4 r F h N 6 S H u t o G 5 K A K S c d H p r h b U 2 6 2 I + k f i H g I f 0 9 k Z J A q U H g 6 c 5 8 T z V e y + F / t U 4 C / r G T 8 j B O g I X 0 5 y M / E R g i n K e G u 1 w y C m K g D a G S 6 1 0 x 7 R N J K O h s y z o E a / z k S d P c r 1 u H d f P 6 o H p 2 X s Q x j z b R F q o h C x 2 h M 3 S J r l A D U f S I n t E b e j e e j B f j w / j 8 a S 0 Z x c w G + i P j 6 x v O R 6 e p < / l a t e x i t >
g

(zt

1, qt) = ht

1

< l a t e x i t s h a 1 _ b a s e 6 4 = " v I k N Q 5 C D Z Z C V r 9 Q B G P F V W 1 o M O + g = " > A A A C H n i c b V B N a 9 t A F F y 5 a e q 6 T a K m x 1 6 W m o I L j Z F C T X I 0 S Q 4 t I e C S + g M s I 1 b r J 3 v x S i t 2 n w p G 6 J f 0 k r + S S w 8 t p Z B T + 2 + y c n x I n A 4 s D D P v s f M m y q Q w 6 H n / n N q T r a f b z + r P G y 9 e 7 u z u u a / 2 B 0 b l m k O f K 6 n 0 K G I G p E i h j w I l j D I N L I k k D K P F a e U P v 4 E 2 Q q V f c Z n B J G G z V M S C M 7 R S 6 H Y C l Y F m q H T K E i g u P v c u y / A 8 k B B j 6 + w D D R K G 8 y g u 5 m V Y 4 I F f B l r M 5 v g + d J t e 2 1 u B P i b + m j T J G r 3 Q v Q m m i u c J p M g l M 2 b s e x l O C q Z R c A l l I 8 g N Z I w v 2 A z G l l Z Z z K R Y n V f S d 1 a Z 0 l h p + 1 K k K / X + R s E S Y 5 Z J Z C e r u G b T q 8 T / e e M c 4 + N J I d I s R 0 j 5 3 U d x L i k q W n V F p 0 I D R 7 m 0 h H E t b F b K 5 0 w z j r b R h i 3 B 3 z z 5 M R k c t v 1 O 2 / v y s d k 9 W d d R J 2 / I W 9 I i P j k i X f K J 9 E i f c P K d X J O f 5 J d z 5 f x w f j t / 7 k Z r z n r n N X k A 5 + 8 t l N u i v g = = < / l a t e x i t >
MIPSK

! (D, ht 1) #
!
Mixing

!!"

!!

|< l a t e x i t s h a 1 _ b a s e 6 4 = " p + m 3 D v 0 E d 4 B u 2 f H m F k C u L R X 1 g 8 4 = " > A A A B 9 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l R l R d F k U x G U F + 4 B 2 K J k 0 0 4 Z m k j H J F M q 0 3 + H G h S J u / R h 3 / o 2 Z d h b a e i B w O O d e 7 s k J Y s 6 0 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 t E w U o X U i u V S t A G v K m a B 1 w w y n r V h R H A W c N o P h b e Y 3 R 1 R p J s W j G c f U j 3 B f s J A R b K z k T z o R N g O C e X o 3 n X R L Z b f i z o C W i Z e T M u S o d U t f n Z 4 k S U S F I R x r 3 f b c 2 P g p V o Y R T q f F T q J p j M k Q 9 2 n b U o E j q v 1 0 F n q K T q 3 S Q 6 F U 9 g m D Z u r v j R R H W o + j w E 5 m G f W i l 4 n / e e 3 E h N d + y k S c G C r I / F C Y c G Q k y h p A P a Y o M X x s C S a K 2 a y I D L D C x N i e i r Y E b / H L y 6 R x X v E u K + 7 D R b l 6 k 9 d R g G M 4 g T P w 4 A q q c A 8 1 q A O B J 3 i G V 3 h z R s 6 L 8 + 5 8 z E d X n H z n C P 7 A + f w B R W 2 S b A = = < / l a t e x i t > F

|

Figure 3: The overall workﬂow of DRFACT. We encode the hypergraph (Fig. 2) with a concept-to-fact sparse matrix E and a fact-to-fact sparse matrix S. The dense fact index D is pre-computed with a pre-trained bi-encoder. A weighed set of facts is represented as a sparse vector F . The workﬂow (left) of DRFACT starts mapping a question to a set of initial facts that have common concepts with it. Then, it recursively performs Fact-Follow operations (right) for computing Ft and At. Finally, it uses learnable hop-weights αt to aggregate the answers.

2019), which learns to maximize the score of facts that contain correct answers to a given question, following the steps of Karpukhin et al. (2020) (i.e., dense passage retrieval), so that we can use MIPS to do dense retrieval over the facts. After pre-training, we embed each fact in F with a dense vector (using the [CLS] token representation). Hence D is a |F| × d dense matrix.
Sparse Fact-to-Fact Index S. We pre-compute the sparse links between facts by a set of connection rules, such as fi → fj when fi and fj have at least one common concept and fj introduces at least two more new concepts that are not in fi (see Appendix B (2) for more). Hence S is a binary sparse tensor with the dense shape |F| × |F|.
Sparse Index of Concept-to-Fact Links E. As shown in Figure 2, a concept can appear in multiple facts and a fact also usually mentions multiple concepts. We encode these co-occurrences between each fact and its mentioned concepts into a sparse matrix with the dense shape |V| × |F| — i.e., the concept-to-fact index.
4.3 Differentiable Fact-Following Operation
The most important part in our framework is how to model the fact-following step in our formulation, i.e., P (Ft | Ft−1, q). For modeling the translation from a fact to another fact under the context of a question q, we propose an efﬁcient approach with a differentiable operation that uses both neural embeddings of the facts and their symbolic connections in the hypergraph.
The symbolic connections between facts are represented by the very sparse fact-to-fact matrix

S, which in our model is efﬁciently implemented with the tf.RaggedTensor construct of TensorFlow (Dhingra et al., 2020). S stores a precomputed dependency between pairs of facts, Sij. Intuitively, if we can traverse from fi to fj these facts should mention some common concepts, and also the facts’ semantics are related, so our Sij will reﬂect this intuition. The fact embeddings computed by a pre-trained bi-encoder are in the dense index of fact vectors D, which contains rich semantic information about each fact, and helps measure the plausibility of a fact in the context of a given question.
The proposed fact-follow operation has two parallel sub-steps: 1) sparse retrieval and 2) dense retrieval. The sparse retrieval uses a fact-to-fact sparse matrix to obtain possible next-hop facts. We can compute Fts = Ft−1S efﬁciently thanks to the ragged representation of sparse matrices.
For the neural dense retrieval, we use a maximum inner product search (MIPS) (Johnson et al., 2019; Guo et al., 2020) over the dense fact embedding index D:
zt−1 = Ft−1D ht−1 = g(zt−1, qt)
Ftd = MIPSK (ht−1, D)
We ﬁrst aggregate the dense vectors of the facts in Ft−1 into the dense vector zt−1, which is fed into a neural layer with the query embedding at the current step, qt (encoded by BERT), to create a query vector ht−1. Here g(·) is an MLP that maps the concatenation of the two input vectors to

a dense output with the same dimensionality as the
fact vectors, which we named to be fact-translating
function. Finally, we retrieve the next-hop top-K facts Ftd with the MIPSK operator.
To get the best of both symbolic and neural
world, we use element-wise multiplication to combine the sparse and dense retrieved results: Ft = Fts Ftd. We summarize the fact-following operation with these differentiable steps:

Ft = Fact-Follow(Ft−1, q)

(1)

= Ft−1S MIPSK (g(Ft−1D, qt), D)

After each hop, we multiply Ft with a pre-

computed fact-to-concept matrix E, thus generat-

ing At, a set of concept predictions. To aggregate

the concept scores, we take the maximum score

among the facts that mention a concept c. Finally

we take the weighted sum of the concept predic-

tions at all hops as the ﬁnal weighted concept sets

A=

T t=1

αtAt,

where

αt

is

a

learnable

parame-

ter. Please read Appendix B for more details.

Equation 1 deﬁnes a random-walk process on

the hypergraph associated with the corpus. We

found that performance was improved by making

this a “lazy” random walk—in particular by aug-

menting Ft with the facts in Ft−1 which have a

weight higher than a threshold τ :

Ft = Fact-Follow(Ft−1, q) + Filter(Ft−1, τ ).

We call this as self-following, which means that Ft contains highly-relevant facts for all distances t < t, and thus improve models when there are variable numbers of “hops” for different questions.
Initial Facts. Note that the set of initial facts F0 is computed differently, as they are produced using the input question q, instead of a previous-hop Ft−1. We ﬁrst use our pre-trained bi-encoder and the associated index D via MIPS query to ﬁnds facts related to q, and then select from the retrieved set those facts that contain question concepts (i.e.,
concepts that are matched in the question text), using the concept-to-fact index E.

4.4 Auxiliary Learning with Distant Evidence
Intermediate evidence, i.e., supporting facts, is signiﬁcant for guiding multi-hop reasoning models during training. In a weakly supervised setting, however, we usually do not have ground-truth annotations as they are expensive to obtain.
To get some noisy yet still helpful supporting facts, we use as distant supervision dense retrieval

based on the training questions. Speciﬁcally, we

concatenate the question and the best candidate

answer to build a query to our pre-trained index D,

and then we divide the results into four groups de-

pending on whether they contain question/answer

concepts: 1) question-answer facts, 2) question-

only facts, 3) answer-only facts, and 4) none-facts.

Then, to get a 2-hop evidence chain, we ﬁrst

check if a question-only fact can be linked to an

answer-only fact through the sparse fact-to-fact

matrix S. Similarly, we can also get 3-hop distant

evidence. In this manner, we can collect the set of

supporting facts at each hop position, denoted as

{F1∗

,

F2∗

,

.

.

.

,

F

∗ T

}.

The ﬁnal learning objective is thus to optimize

the sum of the cross-entropy loss l between the ﬁ-

nal weighed set of concepts A and the answer set A∗, as well as the auxiliary loss from distant ev-

idence — i.e., the mean of the hop-wise loss be-

tween the predicted facts Ft and the distant supporting facts at that hop Ft∗, deﬁned as follows:

L = l(A, A∗) + T1 T l(Ft, Ft∗)
t=1

5 Experiments

5.1 Experimental Setup
Fact corpus and concept vocabulary
We use the GenericsKB-Best corpus as the main knowledge source2. In total, we have 1,025,413 unique facts as our F. We use the spaCy toolkit to prepossess all sentences in the corpus and then extract frequent noun chunks within them as our concepts. The vocabulary V has 80,524 concepts, and every concept is mentioned at least 3 times.

Datasets for OpenCSR
To facilitate the research on open-ended commonsense reasoning (OpenCSR), we reformatted three existing multi-choice question answering datasets to allow evaluating OpenCSR methods. We choose three datasets: QASC, OBQA, and ARC, as their questions require commonsense knowledge about science and everyday objects and are presented in natural language. By applying a set of ﬁlters and rephrasing rules, we selected those open-ended commonsense questions that query concepts in our vocabulary V.
2It was constructed from multiple commonsense knowledge corpora and only kept naturally occurring generic statements, which makes it a perfect ﬁt for OpenCSR.

Stat. \ Data # All Examples
# Training Set # Validation Set
# Test Set
Avg.#Answers
Single-hop %

ARC 6,600 5,355 562 683 6.8 66.91%

QASC 8,443 6,883 731 829 7.6 59.35%

OBQA 5,288 4,199 463 626 7.7 50.80%

Overall 20,331 16, 437 1,756 2,138
7.5 59.02%

Table 1: Statistics of datasets for OpenCSR (v1.0).

As we know that there can be multiple correct answers for a question in OpenCSR, we employed crowd-workers to collect more answers for each test question based on a carefully designed annotation protocol. In total, we collect 15,691 answers for 2,138 rephrased questions for evaluation, which results in 7.5 answers per question on average. Please ﬁnd more details about crowdsourcing and analysis in Appendix A.
We show some statistics of the OpenCSR datasets and our new annotations in Table 1. To understand the multi-hop nature and the difﬁculty of each dataset, we use a heuristic to estimate the percentage of “single-hop questions”, for which we can ﬁnd a fact (from top-1k facts retrieved by BM25) containing both a question concept and an answer concept. The ARC dataset has about 67% one-hop questions and thus is the easiest, while OBQA has only 50%.
Evaluation metrics.
Recall that, given a question q, the ﬁnal output of every method is a weighted set of concepts A = {(a1, w1), . . . }. We denote the set of true answer concepts, as deﬁned above, as A∗ = {a∗1, a∗2, . . . }. We deﬁne Hit@K accuracy to be the fraction of questions for which we can ﬁnd at least one correct answer concept a∗i ∈ A∗ in the top-K concepts of A (sorted in descending order of weight). As questions have multiple correct answers, recall is also an important aspect for evaluating OpenCSR, so we also use Rec@K to evaluate the average recall of the top-K proposed answers.
5.2 Baseline Methods
We present baseline methods and an optional reranker component for boosting the performance on OpenCSR. Table 3 shows a summary of the comparisions of the three methods and our DrFact.
Direct Retrieval Methods. The most straightforward approach to the OpenCSR task is to directly

Methods Knowledge Corpus Structure Multi-hop Formulation Index for Dense Retrieval
Sparse Retrieval Method
# models for Multi-Hop Intermediate Supervision

BM25 A set of
docs N/A N/A
BM25
N/A N/A

DPR A set of
docs N/A Dense Fact Embeddings
N/A
N/A

DrKIT
Mention-Entity Bipartite Graph
EntityFollowing
Dense Mention Embeddings
EntityEntity/Mention Co-occurrence
Multiple Models

N/A

N/A

DrFact (ours) Concept-Fact Hypergraph
Fact-Following
Dense Fact Embeddings Fact-to-Fact, Concept-to-Fact
Matrix A single model (self-following)
Auxiliary Learning

Table 3: Comparisons of the four retrieval methods.

retrieve relevant facts, and then use the concepts mentioned in the top-ranked facts as answer predictions. BM25 is one of the most popular unsupervised method for retrieval, while the Dense Passage Retrieval (DPR) model is a state-of-theart trainable, neural retriever (Karpukhin et al., 2020). Following prior work with DPR, we used BM25-retrieved facts to create positive and (hard)negative examples as supervision. For both methods, we score a concept by the max3 of the relevance scores of retrieved facts that mention it.
DrKIT. Following Dhingra et al. (2020), we use DrKIT for OpenCSR, treating concepts as entities. DrKIT is also an efﬁcient multi-hop reasoning model that reasons over a pre-computed indexed corpus, which, as noted above (Sec. 2), differs from our work in that DrKIT traverses a graph of entities and entity mentions, while DRFACT traverses a hypergraph of facts.
Multiple-choice style re-ranking (MCQA). A conventional approach to multiple-choice QA (MCQA) is to ﬁne-tune a pre-trained language model such as BERT, by combining a question and a particular concept as a single input sequence in the form of “[CLS]question[SEP]choice” and using [CLS] vectors for learning to score choices. We follow this schema and train4 such a multiplechoice QA model on top of BERT-Large, and use this to re-rank the top-K concept predictions.
5.3 Results and Analysis
Main results. For a comprehensive understanding, we report the Hit@K and Rec@K of all methods, at K=50 and K=100, in Table 2. The overall results are the average over the three datasets.
3We also tried mean and sum, but max performs the best. 4Speciﬁcally, we ﬁne-tune BERT-Large to score truth answers over 9 sampled distractors, and use it to rank the top500 concepts produced by each above retrieval method.

Metric = Hit@K (%)
BM25 (off-the-shelf) DPR (Karpukhin et al., 2020) DrKIT (Dhingra et al., 2020)
DRFACT (Ours)
BM25 + MCQA Reranker DPR + MCQA Reranker DrKIT + MCQA Reranker DRFACT + MCQA Reranker
Metric = Rec@K (%)
BM25 (off-the-shelf) DPR (Karpukhin et al., 2020) DrKIT (Dhingra et al., 2020)
DRFACT (Ours)
BM25 + MCQA Reranker DPR + MCQA Reranker DrKIT + MCQA Reranker DRFACT + MCQA Reranker

ARC

H@50 H@100

56.95 68.67 67.63 71.60

67.35 78.62 77.89 80.38

76.87 76.72 78.44 84.19

80.38 83.16 83.37 89.90

R@50
21.12 28.93 27.57 31.48
39.11 43.78 43.14 47.73

R@100
28.08 38.63 37.29 40.93
42.96 51.56 49.17 55.20

QASC

H@50 H@100

58.50 69.36 67.49 72.01

66.71 78.89 81.63 84.56

75.75 81.66 84.00 89.87

80.22 87.45 86.83 93.00

R@50
16.33 23.19 21.25 23.29
29.03 40.72 39.20 44.30

R@100
20.13 32.12 30.93 33.60
32.11 48.25 44.37 50.30

OBQA

H@50 H@100

53.99 62.30 61.74 69.01

66.29 73.80 75.92 80.03

79.23 77.16 79.25 85.78

84.03 83.39 84.03 90.10

R@50
14.27 18.11 18.18 21.27
36.38 36.18 35.12 39.60

R@100
20.21 26.83 27.10 30.32
39.46 43.61 39.85 45.24

Overall

H@50 H@100

56.48 66.78 65.62 70.87

66.78 77.10 78.48 81.66

77.28 78.51 80.56 86.61

81.54 84.67 84.74 91.00

R@50
17.24 23.41 22.33 25.35
34.84 40.23 39.15 43.88

R@100
22.81 32.53 31.77 34.95
38.18 47.81 44.46 50.25

Table 2: Results of the Hit@K and Rec@K (K=50/100) on OpenCSR (v1.0). We present two groups of methods with different inference speed levels. The upper group is retrieval-only methods that are efﬁcient (< 0.5 sec/q), while the bottom group are augmented with a computationally expensive answer reranker (≥ 14 sec/q).

We can see that DRFACT outperforms all baseline methods for all datasets and metrics. Comparing with the state-of-the-art text retriever DPR, DRFACT improves by about 4.1% absolute points in Hit@50 accuracy overall. With the expensive yet powerful MCQA reranker module DRFACT gives an even large gap (∼ 8% gain in H@50 acc).
The performance gains on the QASC and OBQA datasets are larger than the one on ARC. This observation correlates the statistics that the former two have more multi-hop questions and thus DRFACT has more advantages. As shown in Figure 4, we can see that DRFACT consistently outperforms other retrieval methods at different K by a considerable margin.
Interestingly, we ﬁnd that with the MCQA reranker, DrKIT does not yield a large improvement over DPR, and it usually has a lower than other methods. We conjecture this is because that entity-centric reasoning schema produces too many possible concepts and thus is more likely to take more irrelevant concepts at the top positions.
The results on Rec@K in bottom section of Table 2 show that even our DRFACT+MCQA model only recalls about 50% of the correct answers in top-100 results on average. This suggests that OpenCSR is still a very challenging problem and

Hit@K Accuracy (%)

0.9

0.8

0.7

0.6

0.5

BM25

BM25+MCQA

0.4

DPR

DPR+MCQA

DrKIT

DrKIT+MCQA

0.3

DrFact

DrFact+MCQA

10 20 30 40 50 K 60 70 80 90 100

Figure 4: The curve of Hit@K accuracy in overall. Please ﬁnd the curve of Rec@K in Figure 7.

future works should focus on improving the ability of ranking more correct answers higher.
Run-time efﬁciency analysis. We use Table 4 to summarize the online inference speed of each OpenCSR method. At inference time, DPR will make one call to BERT-base for encoding a question and do one MIPS search. Similarly, DrKIT and DRFACT with T hops will make one call to BERT-base for query encoding and do T MIPS searches. However, since the entity-to-mention

Methods
BM25 DPR DrKIT DRFACT
X+ MCQA

Major Computations
Sparse Retrieval BERT-base + MIPS BERT-base + T *(MIPS+ spe2m) BERT-base + T *(MIPS+ spf2f )
X + K * BERT-Large

Speed (sec/q)
0.14 0.08 0.47 0.23
+ 14.12

Table 4: The major competitions of each method and their online (batch-size=1) inference speed in sec/q.

T =1 T =2 T =3 
w/o. Self-follow w/o. Aux. loss

ARC
69.3% 71.1% 71.6%
70.9% 70.6%

QASC
70.1% 72.2% 72.0%
70.4% 70.1%

OBQA
65.0% 68.3% 69.0%
68.4% 68.0%

Overall
68.1% 70.5% 70.9%
69.9% 69.6%

Table 5: Ablation study of DRFACT (H@50 test acc).

matrix (spe2m) of DrKIT is much larger than the fact-to-fact matrix (spf2f ) of DRFACT, DrKIT is about twice as slow as DRFACT. The MCQA is much more computationally expensive, as it makes K calls to BERT-Large for each combination of question and choice. Note that in these experiments we use T =2 for DrKIT, T =3 for DRFACT and K=500 for the MCQA re-rankers.5
Ablation study. Varying the maximum hops (T={1,2,3}) — i.e., the number of calls to Fact-Follow — indicates that overall performance is the best when T=3 as shown in Table 5. The performance with T=2 drops 0.7% point on OBQA. We conjecture this is due to nature of the datasets, in particular the percentage of hard questions. We also test the model (with T=3) without the auxiliary learning loss (Sec. 4.4) or the selffollowing trick. Both are seen to be important to DRFACT. Self-following is especially helpful for QASC and OBQA, where there are more multihop questions. It also makes learning and inference more faster than an alternative approach of ensembling multiple models with different maximum hops as done in some prior works.
Qualitative analysis. We show a concrete example in Fig. 5 to compare the behaviour of DPR and DRFACT in reasoning. DPR uses purely dense retrieval without any regularization, yielding irrelevant facts. The fact f2 matches the phrase “sepa-
5We note the MCQA-reranker could be speed up by scoring more choices in parallel. All run-time tests were performed on NVIDIA V100 (16GB), but MCQA with batchsize of 1 requires only ∼5GB. This suggests more parallel inference on a V100 could obtain 4.5 sec/q for MCQA.

Q: “What will separate iron filings from sand? ”

f1= angle irons reinforce the thinnest section of the ring .” f2= sieves are used for separating fossils from sand...” f3= stainless steel has a rough surface just after filing .” DPR

iron filings show the magnetic fields . (in F0)

DrFact

magnets produce a magnetic field with a north … (in F1)

magnets attract magnetic metals through magnetism (in F2)

Figure 5: A case study to compare DPR and DRFACT.

rating...from sand,” but does not help reason about the question. The f3 shows here for the semantic relatedness of “steel” and “iron” while “ﬁlling” here is not related to question concepts. Our DRFACT, however, can faithfully reason about the question via fact-following over the hypergraph, and use neural fact embeddings to cumulatively reason about a concept, e.g., magnet. By backtracking with our hypergraph, we can use retrieved facts as explanations for a particular prediction.
6 Conclusion
We introduce and study a new task — open-ended commonsense reasoning (OpenCSR) — which is both realistic and challenging. We construct three OpenCSR versions of widely used datasets targeting commonsense reasoning with a novel crowdsourced collection of multiple answers, and evaluate a number of baseline methods for this task. We also present a novel method, DRFACT. DRFACT is a scalable multi-hop reasoning method that traverses a corpus (as a hypergraph) via a differentiable “fact-following” reasoning process, employing both a neural dense index of facts and sparse tensors of symbolic links between facts, using a combination of MIPS and sparse-matrix computation. DRFACT outperforms several strong baseline methods on our data, making a signiﬁcant step towards adapting commonsense reasoning approaches to more practical applications. Base on the multi-hop reasoning framework of DRFACT, we hope the work can beneﬁt future research on neural-symbolic commonsense reasoning.
Acknowledgments
Xiang Ren is supported in part by the Ofﬁce of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, the DARPA MCS program under Contract No. N660011924033 with the United States Ofﬁce Of

f1=heterogeneou mixture of iron f f2=…a soil textu

Naval Research, the Defense Advanced Research Projects Agency with award W911NF-19-20271, and NSF SMA 18-29268. We thank all reviewers for their constructive feedback and comments.
* Ethical Considerations
Crowd-workers. This work presents three datasets for addressing a new problem, open common-sense reasoning. The datasets are all derived from existing multiple-choice CSR datasets, and were produced by ﬁltering questions and using crowd-workers to annotate common-sense questions by suggesting additional answers. Most of the questions are about elementary science and common knowledge about our physical world. None of the questions involve sensitive personal opinions or involve personally identiﬁable information. We study posted tasks to be completed by crowd-workers instead of crowd-workers themselves, and we do not retrieve any identiﬁable private information about a human subject.
Data bias. Like most crowdsourced data, and in particular most common-sense data, these crowdsourced answers are inherently subject to bias: for example, a question like “what do people usually do at work” might be answered very differently by people from different backgrounds and cultures. The prior multiple-choice CSR datasets which our datasets are built on are arguably more strongly biased culturally, as they include a single correct answer and a small number of distractor answers, while our new datasets include many answers considered correct by several annotators. However, this potential bias (or reduction in bias) has not been systematically measured in this work.
Sustainability. For most of the experiments, we use the virtual compute engines on Google Cloud Platform, which “is committed to purchasing enough renewable energy to match consumption for all of their operations globally.”6 With such virtual machine instances, we are able to use the resources only when we have jobs to run, instead of holding them all the time like using physical machines, thus avoiding unnecessary waste.
Application. The work also evaluates a few proposed baselines for OpenCSR, and introduced a new model which outperforms them. This raises the question of whether harm might arise from applications of OpenCSR—or more generally, since
6https://cloud.google.com/ sustainability

OpenCSR is intended as a step toward making multiple-choice CSR more applicable, whether harm might arise more generally from CSR methods. Among the risks that need to be considered in any deployment of NLP technology are that responses may be wrong, or biased, in ways that would lead to improperly justiﬁed decisions. Although in our view the current technology is still relatively immature, and unlikely to be ﬁelded in applications that would cause harm of this sort, it is desirable that CSR methods provide audit trails, and recourse so that their predictions can be explained to and critiqued by affected parties. Our focus on methods that provide chains of evidence is largely a reﬂection of this perceived need.
References
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. 2020. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870– 1879, Vancouver, Canada. Association for Computational Linguistics.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
William W. Cohen, Haitian Sun, R. Alex Hofer, and Matthew Siegler. 2020. Scalable neural methods for reasoning with a symbolic knowledge base. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020. OpenReview.net.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. 2019. Multi-step retrieverreader interaction for scalable open-domain question answering. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of

deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, and William W. Cohen. 2020. Differentiable reasoning over a virtual knowledge base. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Yair Feldman and Ran El-Yaniv. 2019. Multi-hop paragraph retrieval for open-domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2296–2309, Florence, Italy. Association for Computational Linguistics.
Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. 2020. Scalable multi-hop relational reasoning for knowledge-aware question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1295–1309, Online. Association for Computational Linguistics.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3887–3896. PMLR.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3929–3938. PMLR.
Jeff Johnson, Matthijs Douze, and Herve´ Je´gou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769– 6781, Online. Association for Computational Linguistics.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Find-

ings of the Association for Computational Linguistics: EMNLP 2020, pages 1896–1907, Online. Association for Computational Linguistics.
Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. QASC: A dataset for question answering via sentence composition. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8082–8090. AAAI Press.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence, Italy. Association for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockta¨schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2829–2839, Hong Kong, China. Association for Computational Linguistics.
Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang,

Guihong Cao, and Songlin Hu. 2020. Graphbased reasoning over heterogeneous external knowledge for commonsense question answering. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8449–8456. AAAI Press.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381–2391, Brussels, Belgium. Association for Computational Linguistics.
Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. 2019. Answering complex open-domain questions through iterative query generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2590–2602, Hong Kong, China. Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426, Online. Association for Computational Linguistics.
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase index. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4430–4441, Florence, Italy. Association for Computational Linguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the ThirtyFirst AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 4444–4451. AAAI Press.
Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. PullNet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the

9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2380– 2390, Hong Kong, China. Association for Computational Linguistics.
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4231– 4242, Brussels, Belgium. Association for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics.

Appendix
In this appendix, we show more details of our dataset construction (Appx. A), details of model implementation and experiments for reproduciblility (Appx. B), and more related works (Appx. C). As we have submitted our code as supplementary material with detailed instructions for running baselines, we will skip some minor details here. We will make our code and data public after the anonymity period.
A Constructing OpenCSR Datasets
A.1 Reformatting Questions and Answers
In this section, we introduce how we reformat the existing three datasets and crowd-source annotations of multiple answers for evaluating OpenCSR. To convert a multiple-choice question to an open-ended question, we ﬁrst remove questions where the correct answer does not contain any concept in V and the few questions that require comparisons between original choices, as they are designed only for multiple-choice QA, e.g., “which of the following is the most . . . ” Then, we rephrase questions with long answers to be an open-ended question querying a single concept.
For example, an original question-answer pair such as (Q:“The Earth revolving around the sun can cause ”, A:“constellation to appear in one place in spring and another in fall”) is now rephrased to (Q*=“The Earth revolving around the sun can cause what to appear in one place in spring and another in fall?”, A*=“constellation”). Specifically, we combine the original question (Q) and original correct choice (A) to form a long statement and rephrase it to be a new question (Q*) querying a single concept (A*) in the original answer, where we use the least frequent concept as the target. This question-rephrasing largely improve the number of answerable questions, particularly for the OBQA dataset. All are English data.
A.2 Crowd-sourcing More Answers
Note that there can be multiple correct answers to an open-ended question in OpenCSR while the original datasets only provide a single answer. Thus, we use Amazon Mechanical Turk7 (AMT) to collect more answers for the test questions to have a more precise OpenCSR evaluation.
7https://www.mturk.com/

0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02 0.00 0 5 10 15 20 25 30
# Answer Concepts
Figure 6: Distribution of # answers of test questions.
We design a three-stage annotation protocol as follows:
• S1) Multiple-Choice Sanity Check. We provide a question and 4 choices where only one choice is correct and the other 3 are randomly sampled. Only the workers who passed this task, their following annotations will be considered. This is mainly designed for avoiding noise from random workers.
• S2) Selection from Candidates. To improve the efﬁciency of annotation, we take the union of top 20 predictions from BM25, DPR, DrKIT, and DrFact and randomly shufﬂe the order of these concepts (most of them are about 60∼70 candidates). workers can simply input the ids of the concepts that they think are good answers to the question (i.e., a list of integers separated by comma). There are three different workers for each question and we take the candidates which are selected by at least two workers. Note that we also put the correct answer we already have in the candidates and use them as another sanity check to ﬁlter out noisy workers.
• S3) Web-based Answer Collection. We generate an URL link to Google Search of the input question to help workers to use the Web for associating more correct answers to the question (the input here is a string for a list of concepts separated by comma). We also provide our concept vocabulary as a web-page so one can quickly check if a concept is valid.
After careful post-processing and multiple rounds of re-assignment, we have in total 15k answers for 2k questions, and the distribution of number of answers are in Figure 6 and Table 1.

B Details of Implementation and Our Experiments
B.1 DrFact Implementation
We present some concrete design choices within our DrFact implementation which are abstractly illustrated in the main content of the paper.
(1) Pre-training Dense Fact Index D. As we mentioned in Sec. 4, we follow the steps of Karpukhin et al. (2020) to pre-train a biencoder question answering model on top of BERT (Devlin et al., 2019). To create negative examples, we use the BM25 results which do not contain any answer concept. We use BERT-base (uncased L-12 H-768 A-12) in our implementation and thus d = 768 in our experiments.
(2) Sparse Fact-to-Fact Index S. We use a set of rules to decide if we can create a link fi → fj (i.e., Sij = 1) as follows:
• i = j. We do not allow self-link here but use self-following as we described in Sec. 4.
• |I| >= 1 where I is the set of concepts that are mentioned in both fi and fj. Note that we remove the most frequent 100 concepts (e.g., human) from I.
• |I| < |fi|. We do not create links when all concepts in fi are mentioned in fj, which are usually redundant.
• |fj| − |I| >= 2. We create links only when there are more than two unseen concepts in fj which are not in fi, such that the fact-to-fact links create effective reasoning chains.
We also limit that a fact can be followed by at most 1k different facts. Additionally, we append the links from our distant supervision of justiﬁcations as well if they were ﬁltered out before.
(3) Hop-wise Question Encoding qt. We encode the question q with BERT-base and then use its [CLS] token vector as the dense representation for q. For each hop, we append a hop-speciﬁc layer to model how the question context changes over the reasoning process — qt = MLPθt(q). (4) Fact Translating Function g. The translating function accepts both the vector representation of previous-hop facts Ft−1 and the hop-wise question vector qt and uses an MLP to map the concatenation of them to a vector used for a MIPS query: ht−1 = MLPθg ([Ft−1; qt]). Thus, ht−1 has the same dimension as a fact vector in U .
(5) Hop-wise Answer Weights αt. We use the shared query vector to learn how to aggregate pre-

Rec@K (%)

0.5

0.4

0.3

0.2

BM25

BM25+MCQA

DPR

DPR+MCQA

0.1

DrKIT DrFact

DrKIT+MCQA DrFact+MCQA

10 20 30 40 50 K 60 70 80 90 100

Figure 7: The curve of Rec@K in overall data.

dictions at different hops. For a T -hop DrFact model, we learn to transform the q to a T -dim vector where αt is the t-th component.
B.2 Hyper-parameters and Training Details
We now present the details and ﬁnal hyperparameters that we used in our experiments. For all methods, we tune their hyper-parameters on the validation set and then use the same conﬁgurations to train them with the combination of the training and validation sets for the same steps. BM25. We use the off-the-shelf implementation by elasticsearch8, which are open-source and unsupervised. For the run-time analysis, we use Intel(R) Xeon(R) CPU @ 2.00GHz and the localhost webserver for data transfer. DPR. We use the source code9 released by the original authors. The creation of negative contexts are the same when we pre-train our dense fact index D, which are sampled from BM25 results. DrKIT. We use the ofﬁcial source code10 for our experiments. We did minimal modiﬁcations on their code for adapt DrKIT towards building dense index of mentions for the OpenCSR corpus and datasets. For fair comparisions between DPR, DrKIT and DrFact, we all use BERT-base as question and mention/fact encoder. We use 200 as the dimension of mention embeddings and T=2 as the maximum hops. We found that using T=3 will cause too much memory usage (due to denser entity-to-mention matrix) and also result
8https://github.com/elastic/ elasticsearch
9https://github.com/facebookresearch/ DPR
10https://github.com/google-research/ language/tree/master/language/labs/drkit

in a very slow training speed. Non-default hyperparameters are: train batch size=8 due to the limit of our GPU memory, entity score threshold=5e-3 (out of {5e-2, 5e-3, 5e-4, 1e-4}) to ﬁlter numerous long-tail intermediate concepts for speeding up training and inference.
DrFact. Similar to DrKIT, we also implement DrFact in TensorFlow for its efﬁcient implementation of tf.RaggedTensor which are essential for us to compute over large sparse tensors. We record the default hyper-parameters in our submitted code. We use a single V100 GPU (16GB) for training with batch size of 24 (using 15GB memory) and learning rate as 3e-5, selected from {1e-5, 2e-5, 3e-5, 4e-5, 5e-5}. The entity score threshold=1e-4, and fact score threshold=1e-5, which are all selected from {1e-3, 1e-4, 1e-5} based on the dev set.
Model Parameters. DPR, DrKIT and DrFact are all based on the BERT-base, which are 110 million parameters (after pre-training index). DrKIT and DrFact additionally have several MLP layers on top of ‘[CLS]’ token vectors, which are all less than 1 million parameters. The MCQA-reranker model is based on BERT-Large, and thus has 345 million parameters.
C Discussion on Other Related Work
Other Open-Domain QA models. Recent open-domain QA models such as REALM (Guu et al., 2020), Path-Retriever (Asai et al., 2020), ORQA (Lee et al., 2019), and RAG (Lewis et al., 2020b), mainly focus on QA over the full Wikipedia corpus like DrKIT (Dhingra et al., 2020) does. Some of them explicitly use the links between pages to form reasoning chain, while a few them rely on expensive QA-oriented pretraining. Moreover, as DPR (Karpukhin et al., 2020) already shows better performance (see their Table 4) than most prior works with a simpler method, we thus use DPR as the major baseline for evaluation in this work.

