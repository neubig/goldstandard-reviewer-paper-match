Are Perceptually-Aligned Gradients a General Property of Robust Classiﬁers?

arXiv:1910.08640v2 [cs.LG] 23 Oct 2019

Simran Kaur Carnegie Mellon University
skaur@cmu.edu

Jeremy Cohen Carnegie Mellon University
jeremycohen@cmu.edu

Zachary C. Lipton Carnegie Mellon University
zlipton@cmu.edu

Abstract
For a standard convolutional neural network, optimizing over the input pixels to maximize the score of some target class will generally produce a grainy-looking version of the original image. However, Santurkar et al. (2019) demonstrated that for adversarially-trained neural networks, this optimization produces images that uncannily resemble the target class. In this paper, we show that these perceptuallyaligned gradients also occur under randomized smoothing, an alternative means of constructing adversarially-robust classiﬁers. Our ﬁnding supports the hypothesis that perceptually-aligned gradients may be a general property of robust classiﬁers. We hope that our results will inspire research aimed at explaining this link between perceptually-aligned gradients and adversarial robustness.
1 Introduction
Classiﬁers are called adversarially robust if they achieve high accuracy even on adversariallyperturbed inputs [1, 2]. Two effective techniques for constructing robust classiﬁers are adversarial training and randomized smoothing. In adversarial training, a neural network is optimized via a min-max objective to achieve high accuracy on adversarially-perturbed training examples [1, 3, 4]. In randomized smoothing, a neural network is smoothed by convolution with Gaussian noise [5, 6, 7, 8]. Recently, [9, 10, 11] demonstrated that adversarially-trained networks exhibit perceptually-aligned gradients: iteratively updating an image by gradient ascent so as to maximize the score assigned to a target class will render an image that perceptually resembles the target class.
In this paper, we show that smoothed neural networks also exhibit perceptually-aligned gradients. This ﬁnding supports the conjecture in [9, 10, 11] that perceptually-aligned gradients may be a general property of robust classiﬁers, and not only a curious consequence of adversarial training. Since the root cause behind the apparent relationship between adversarial robustness and perceptual alignment remains unclear, we hope that our ﬁndings will spur foundational research aimed at explaining this connection.
Perceptually-aligned gradients Let f : Rd → Rk be a neural network image classiﬁer that maps from images in Rd to scores for k classes. Naively, one might hope that by starting with any image x0 ∈ Rd and taking gradient steps so as to maximize the score of a target class t ∈ [k], we would produce an altered image that better resembled (perceptually) the targeted class. However, as shown in Figure 1, when f is a vanilla-trained neural network, this is not the case; iteratively following the gradient of class t’s score appears perceptually as a noising of the image. In the nascent literature on the explainability of deep learning, this problem has been addressed by adding explicit regularizers to the optimization problem [12, 13, 14, 15]. However, [10] showed that for adversarially-trained neural networks, these explicit regularizers aren’t needed — merely following the gradient of a target class t will render images that visually resemble class t.
NeurIPS 2019 Workshop on “Science Meets Engineering of Deep Learning.”

Figure 1: Large- targeted adversarial examples for a vanilla-trained network, an adversarially trained network, and a smoothed network. Adversarial examples for both robust classiﬁers visually resemble the targeted class, while adversarial examples for the vanilla classiﬁer do not. All of these adversarial examples have perturbation size = 40 (on images with pixels scaled to [0, 1]).

Randomized smoothing Across many studies, adversarially-trained neural networks have proven empirically successful at resisting adversarial attacks within the threat model in which they were trained [16, 17]. Unfortunately, when the networks are large and expressive, no known algorithms are able to provably certify this robustness [18], leaving open the possibility that they will be vulnerable to better adversarial attacks developed in the future.

For this reason, a distinct approach to robustness called randomized smoothing has recently gained
traction in the literature [5, 6, 7, 8]. In the 2-robust version of randomized smoothing, the robust classiﬁer fˆσ : Rd → Rk is a smoothed neural network of the form:

fˆσ(x) = Eε∼N (0,σ2I)[f (x + ε)]

(1)

where f : Rd → Rk is a neural network (ending in a softmax) called the base network. In other words, fˆσ(x), the smoothed network’s predicted scores at x, is the weighted average of f within the neighborhood around x, where points are weighted according to an isotropic Gaussian centered at x with variance σ2. A disadvantage of randomized smoothing is that the smoothed network fˆσ cannot be evaluated exactly, due to the expectation in (1), and instead must approximated via Monte Carlo sampling. However, by computing fˆσ(x) one can obtain a guarantee that fˆσ’s prediction is constant within an 2 ball around x; in contrast, it is not currently possible to obtain such certiﬁcates using neural network classiﬁers. See Appendix B for more background on randomized smoothing.
How to best train the base network f to maximize the certiﬁed accuracy of the smoothed network fˆσ remains an open question in the literature. In [5, 7], the base network f was trained with Gaussian data augmentation. However, [19, 6] showed that training f instead using stability training [20] resulted in substantially higher certiﬁed accuracy, and [8] showed that training f by adversarially training fˆσ also outperformed Gaussian data augmentation. Our main experiments use a base network trained with Gaussian data augmentation. In Appendix C we compare against the network from [8].

2 Experiments
In this paper, we show that smoothed neural networks exhibit perceptually-aligned gradients. By design, our experiments mirror those conducted in [10]. To begin, we synthesize large- targeted adversarial examples for a smoothed (σ = 0.5) ResNet-50 trained on ImageNet [21, 22]. Given some source image x0, we used projected gradient descent (PGD) to ﬁnd an image x∗ within 2 distance of x0 that the smoothed network fˆσ classiﬁes conﬁdently as target class t. Speciﬁcally, decomposing

Figure 2: Large- adversarial examples for a smoothed neural network. Each row is a (random) starting image, each column is a (random) target class. See Figures 5-6 in Appendix A for more.
2

Figure 3: Class-conditional image synthesis using a smoothed NN. To synthesize an image from class t, we sampled a seed image from a multivariate Gaussian ﬁt to images from class t, and then performed PGD to maximize the score of class t. Figures 7-8 in Appendix A have more examples.

f as f (x) = softmax(logits(x)), we solve the problem:

x∗ = arg max Eε∼N (0,σ2I)[logits(x + ε)t].

(2)

x: x−x0 ≤

We ﬁnd that optimizing (2) yields visually more compelling results than minimizing the cross-entropy loss of fˆσ. See Appendix C for a comparison between (2) and the cross-entropy approach.

The gradient of the objective (2) cannot be computed exactly, due to the expectation over ε, so

we instead used an unbiased estimator obtained by sampling N = 20 noise vectors ε1, . . . , εN ∼

N (0, σ2I) and computing the average gradient N1

N i=1

∇x

logits(x

+

εi

)t

.

Figure 1 depicts large- targeted adversarial examples for a vanilla-trained neural network, an adversarially trained network [4], and a smoothed network. Observe that the adversarial examples for the vanilla network do not take on coherent features of the target class, while the adversarial examples for both robust networks do. Figure 2 shows large- targeted adversarial examples synthesized for the smoothed network for a variety of different target classes.

Next, as in [10], we use the smoothed network to class-conditionally synthesize images. To generate an image from class t, we sample a seed image x0 from a multivariate Gaussian ﬁt to images from class t, and then we iteratively take gradient steps to maximize the score of class t using objective (2). Figure 3 shows two images synthesized in this way from each of seven ImageNet classes. The synthesized images appear visually similar to instances of the target class, though they often lack global coherence — the synthesized solar dish includes multiple overlapping solar dishes.

Noise Level σ Smoothed neural networks have a hyperparameter σ which controls a robustness/accuracy tradeoff: when σ is high, the smoothed network is more robust, but less accurate [5, 7]. We investigated the effect of σ on the perceptual quality of generated images. Figure 4 shows large- adversarial examples crafted for smoothed networks with σ varying in {0.25, 0.50, 1.00}. Observe that when σ is large, PGD tends to paint single instance of the target class; when σ is small, PGD tends to add spatially scattered features.

Other concerns In Appendix C, we study the effects of the following factors on the perceptual quality of the generated images: the number of Monte Carlo noise samples N , the loss function used for PGD, and whether the base network f is trained using Gaussian data augmentation [5, 7] or SMOOTHADV [8].

Figure 4: Large- adversarial examples crafted for smoothed neural networks with different settings of the smoothing scale hyperparameter σ. More examples are in Figures 9-11 in Appendix A.
3

References
[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.
[2] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndic´, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. Joint European Conference on Machine Learning and Knowledge Discovery in Database, 2013.
[3] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
[4] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
[5] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certiﬁed robustness to adversarial examples with differential privacy. In IEEE Symposium on Security and Privacy (SP), 2019.
[6] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certiﬁed adversarial robustness with additive gaussian noise. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[7] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized smoothing. In Proceedings of the 36th International Conference on Machine Learning, 2019.
[8] Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien Bubeck. Provably robust deep learning via adversarially trained smoothed classiﬁers. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[9] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SyxAb30cY7.
[10] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Image synthesis with a single (robust) classiﬁer. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[11] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint arXiv:1906.00945, 2019.
[12] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi: 10.23915/distill.00007. https://distill.pub/2017/feature-visualization.
[13] Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence predictions for unrecognizable images. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[14] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2015. doi: 10.1109/cvpr.2015.7299155. URL http://dx.doi.org/10.1109/CVPR. 2015.7299155.
[15] Audun M. Øygard. Visualizing googlenet claasses. https://www.auduno.com/2015/07/ 29/visualizing-googlenet-classes/, 2015. [Online; accessed 30-August-2019].
[16] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, 2018.
[17] Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, and Matthias Bethge. Accurate, reliable and fast robustness evaluation. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
4

[18] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness veriﬁcation of neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[19] Y. Carmon, A. Raghunathan, L. Schmidt, P. Liang, and J. C. Duchi. Unlabeled data improves adversarial robustness. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[20] Stephan Zheng, Yang Song, Thomas Leung, and Ian J. Goodfellow. Improving the robustness of deep neural networks via stability training. In Computer Vision and Pattern Recognition, 2016.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2016.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
[23] Alexander Levine, Sahil Singla, and Soheil Feizi. Certiﬁably robust interpretation in deep learning. arXiv preprint arXiv:1905.12105, 2019.
[24] Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to deep neural networks via region-based classiﬁcation. 33rd Annual Computer Security Applications Conference, 2017.
[25] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via random self-ensemble. In The European Conference on Computer Vision (ECCV), September 2018.
[26] Y. Zhang and P. Liang. Defending against whitebox adversarial attacks via randomized discretization. In Artiﬁcial Intelligence and Statistics (AISTATS), 2019.
[27] Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cédric GouyPailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[28] Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S. Jaakkola. A stratiﬁed approach to robustness for randomly smoothed classiﬁers. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
5

A Additional images
Figure 5: Large- adversarial examples for a smoothed neural network (part 1 / 2). Each row is a randomly chosen starting image, each column is a randomly chosen target class.
6

Figure 6: Large- adversarial examples for a smoothed neural network (part 2 / 2). Each row is a randomly chosen starting image, each column is a randomly chosen target class.
7

(a) cat
(b) panda
(c) barber shop
(d) mug Figure 7: Class-conditional synthesized images (part 1 / 2). To synthesize an image from class t, we sampled a seed image from a multivariate Gaussian distribution ﬁt to class t, and then performed PGD to maximize the score which a smoothed neural network assigns to class t. The top row shows the seed image, the bottom row shows the result of PGD.
8

(a) computer
(b) solar dish
(c) broccoli Figure 8: Class-conditional synthesized images (part 2 / 2). To synthesize an image from class t, we sampled a starting image from a multivariate Gaussian distribution ﬁt to class t, and then performed PGD to maximize the score which a smoothed neural network assigns to class t. The top row shows the seed image, the bottom row shows the result of PGD.
9

Figure 9: Large- adversarial examples crafted for smoothed neural networks with different settings of the smoothing scale hyperparameter σ (part 1 / 3). Images and target classes were randomly chosen. When σ is large, the adversary tends to paint a single, coherent instance of the target class; when σ is small, the adversary tends to paint scattered features of the target class. Note that σ = 0.0 corresponds to a vanilla-trained network.
10

Figure 10: Large- adversarial examples crafted for smoothed neural networks with different settings of the smoothing scale hyperparameter σ (part 2 / 3). Images and target classes were randomly chosen. When σ is large, the adversary tends to paint a single, coherent instance of the target class; when σ is small, the adversary tends to paint scattered features of the target class. Note that σ = 0.0 corresponds to a vanilla-trained network.
11

Figure 11: Large- adversarial examples crafted for smoothed neural networks with different settings of the smoothing scale hyperparameter σ (part 3 / 3). Images and target classes were randomly chosen. When σ is large, the adversary tends to paint a single, coherent instance of the target class; when σ is small, the adversary tends to paint scattered features of the target class. Note that σ = 0.0 corresponds to a vanilla-trained network.
12

Figure 12: Large- targeted adversarial examples for a vanilla-trained network, an adversarially trained network [4], and a smoothed network. Adversarial examples for both robust classiﬁers visually resemble the targeted class, while adversarial examples for the vanilla classiﬁer do not. All of these adversarial examples have perturbation size = 40 (on images with pixels scaled to [0, 1]).
13

B Randomized Smoothing

Randomized smoothing is relatively new to the literature, and few comprehensive references exist. Therefore, in this appendix, we review some basic aspects of the technique.

Preliminaries Randomized smoothing refers to a class of adversarial defenses in which the robust classiﬁer g : Rd → [k] that maps from an input in Rd to a class in [k] := {1, . . . , k} is deﬁned as:

g(x) = arg max ET [f (T (x))]y.
y∈[k]

Here, f : Rd → ∆k is a neural network “base classiﬁer” which maps from an input in Rd to a vector

of class scores in ∆k := {z ∈ Rk : z ≥ 0,

k j=1

zj

=

1},

the

probability

simplex

of

non-negative

k-vectors that sum to 1. T is a randomization operation which randomly corrupts inputs in Rd to

other inputs in Rd, i.e. for any x, T (x) is a random variable.

Intuitively, the score which the smoothed classiﬁer g assigns to class y for the input x is deﬁned to be the expected score that the base classiﬁer f assigns to the class y for the random input T (x).

The requirement that f returns outputs in the probability simplex ∆k can be satisﬁed in either of two ways. In the “soft smoothing” formulation (presented in the main paper), f is a neural network which ends in a softmax. In the “hard smoothing” formulation, f returns the indicator vector for a particular class, i.e. a length-k vector with one 1 and the rest zeros, without exposing the intermediate class scores. In the hard smoothing formulation, since the expectation of an indicator function is a probability, the smoothed classiﬁer g(x) can be interpreted as returning the most probable prediction by the classiﬁer f over the random variable T (x). Note that no papers have yet studied soft smoothing as a certiﬁed defense, though [8] approximated a hard smoothing classiﬁer with the corresponding soft classiﬁer in order to attack it.

When the base classiﬁer f is a neural network, the smoothed classiﬁer g cannot be evaluated exactly, since it is not possible to exactly compute the expectation of a neural network’s prediction over a random input. However, by repeatedly sampling the random vector f (T (x)), one can obtain upper and lower bounds on the expected value of each entry of that vector, which hold with high probability over the sampling procedure. In the hard smoothing case, since each entry of f (T (x)) is a Bernoulli random variable, one can use standard Bernoulli conﬁdence intervals like the Clopper-Pearson, as in [5, 7]. In the soft smoothing case, since each entry of f (T (x)) is bounded in [0, 1], one can use Hoeffding-style concentration inequalities to derive high-probability conﬁdence intervals for the entries of f (T (x)) .

Gaussian smoothing When T is an additive Gaussian corruption, T (x) = x + ε, ε ∼ N (0, σ2I),

the robust classiﬁer g : Rd → [k] is given by:

g(x) = arg max fˆσ(x) where fˆσ(x) = Eε∼N (0,σ2I)[f (x + ε)].

(3)

j∈[k]

Gaussian-smoothed classiﬁers are certiﬁably robust under the 2 norm: for any input x, if we know fˆσ(x), we can certify that g’s prediction will remain constant within an 2 ball around x:

Theorem 1 (Extension to “soft smoothing” of Theorem 1 from [7]; see also Appendix A in [8]). Let

f : Rd → ∆k be any function, and deﬁne g and fˆσ as in (3). For some x ∈ Rd, let y1, y2 ∈ [k] be

the indices of the largest and second-largest entries of fˆσ(x). Then g(x + δ) = y1 for any δ with

σ δ 2≤

Φ−1(fˆσ(x)y ) − Φ−1(fˆσ(x)y )

.

2

1

2

Theorem 1 is easy to prove using the following mathematical fact:

Lemma 2 (Lemma 2 from [8], Lemma 1 from [23]). Let h : Rd → [0, 1] be any function, and deﬁne its Gaussian convolution hˆσ as hˆσ(x) = Eε∼N (0,σ2I)[h(x + ε)]. Then, for any input x ∈ Rd and any perturbation δ ∈ Rd,

Φ Φ−1(hˆσ(x)) − δ 2 ≤ hˆσ(x + δ) ≤ Φ Φ−1(hˆσ(x)) + δ 2 .

σ

σ

14

Intuitively, Lemma 2 says that hˆσ(x + δ) cannot be too much larger or too much smaller than hˆσ(x). If this has the feel of a Lipschitz guarantee, there is good reason: Lemma 2 is equivalent to the statement that the function x → Φ−1(hˆσ(x)) is 1/σ-Lipschitz.
Theorem 1 is a direct consequence of Lemma 2:

Proof of Theorem 1. Since the outputs of fˆσ live in the probability simplex, for each class j the function fˆσ(·)j has output bounded in [0, 1], and hence can be viewed as a function hˆσ for which the condition of Lemma 2 applies.
Therefore, from applying Lemma 2 to fˆσ(·)y1 , we know that:

fˆσ(x + δ)y ≥ Φ Φ−1(fˆσ(x)y ) − δ 2

1

1

σ

and, for any j, from applying Lemma 2 to fˆσ(·)j, we know that:

Φ Φ−1(fˆσ(x)j) + δ 2 σ

≥ fˆσ(x + δ)j.

Combining these two results, it follows that a sufﬁcient condition for fˆσ(x + δ)y1 ≥ fˆσ(x + δ)j is:

Φ Φ−1(fˆσ(x)y ) − δ 2 ≥ Φ Φ−1(fˆσ(x)j) + δ 2 ,

1

σ

σ

or equivalently,

δ 2 ≤ σ (Φ−1(fˆσ(x)y − Φ−1(fˆσ(x)j)).

2

1

Hence, we can conclude that fˆσ(x + δ)y1 ≥ maxj=y1 fˆσ(x + δ)j so long as

δ 2 ≤ min σ (Φ−1(fˆσ(x)y − Φ−1(fˆσ(x)j)) = σ (Φ−1(fˆσ(x)y − Φ−1(fˆσ(x)y ))

j=y1 2

1

2

1

2

Training Given a dataset, a base classiﬁer architecture, and a smoothing level σ > 0, it currently an active research question to ﬁgure out the best way to train the base classiﬁer f so that the smoothed classiﬁer g will attain high certiﬁed or empirical robust accuracies. The original randomized smoothing paper [5] proposed training f with Gaussian data augmentation and the standard crossentropy loss. However, [8] and [6, 19] showed that alternative training schemes yield substantial gains in certiﬁed accuracy. In particular, [8] proposed training f by performing adversarial training on g, and [6, 19] proposed training f via stability training [20].
Related work Gaussian smoothing was ﬁrst proposed as a certiﬁed adversarial defense by [5] under the name “PixelDP,” though similar techniques had been proposed earlier as a heuristic defenses in [24, 25]. Subsequently, [6] proved a stronger robustness guarantee, and ﬁnally [7] derived the tightest possible robustness guarantee in the “hard smooothing” case, which was extended to the “soft smoothing” case by [23, 8].
Concurrently, [26] proved a robustness guarantee in ∞ norm for Gaussian smoothing; however, since Gaussian smoothing speciﬁcally confers 2 (not ∞) robustness [7], the certiﬁed accuracy numbers reported in [26] were weak.
[27] gave theoretical and empirical arguments for an adversarial defense similar to randomized smoothing, but did not position their method as a certiﬁed defense.
[28] have extended randomized smoothing beyond Gaussian noise / 2 norm by proposing a randomization scheme which allows for certiﬁed robustness in the 0 norm.

15

C Details on Generating Images

This appendix details the procedure used to generate the images that appeared in this paper.
As in [10], to generate an image x∗ ∈ Rd near the starting image x0 that is classiﬁed by a smoothed neural network fˆσ as some target class t, we use projected steepest descent to solve the optimization problem:

x∗ = arg min L(fˆσ, x, t)

(4)

x: x−x0 2≤

where L is a loss function measuring the extent to which fˆσ classiﬁes x as class t.
The two big choices which need to be made are: which loss function to use, and how to compute its gradient?

Loss functions for adversarially-trained networks We ﬁrst review two loss functions for generating images using adversarially-trained neural networks. Our loss functions for smoothed neural networks (presented below) are inspired by these.
The ﬁrst is the cross-entropy loss. If f adv : Rd → ∆k is an (adversarially trained) neural network classiﬁer that ends in a softmax layer (so that its output lies on the probability simplex ∆k), the cross-entropy loss is deﬁned as:
LCE(f adv, x, t) := − log f adv(x)t.

The second is the “target class max” (TCM) loss. If we write f adv as f adv(x) = softmax(logits(x)), where logits : Rd → Rk is f adv minus the ﬁnal softmax layer, then the TCM loss is deﬁned as:
LTCM(f adv, x, t) := − logits(x)t.
In other words, minimizing LTCM will maximize the score that logits assigns to class t. Since f adv is just a neural network, computing the gradients of these loss functions can be easily done using automatic differentiation. (The situation is more complicated for smoothed neural networks.)
We note that [10] used LCE in their experiments.

Loss functions for smoothed networks Our loss functions for smoothed neural networks are inspired by those described above for adversarially trained networks. If fˆσ is a smoothed neural network of the form fˆσ(x) = Eε∼N (0,σ2I)[f (x + ε)], with f a neural network that ends in a softmax
layer, then the cross-entropy loss is deﬁned as:

LCE(fˆσ, x, t) := − log fˆσ(x)t = − log Eε∼N (0,σ2I)[f (x + ε)t].

(5)

If we decompose f as f (x) = softmax(logits(x)), where logits : Rd → Rk is f minus the softmax layer, then the TCM loss is deﬁned as:

LTCM(fˆσ, x, t) := −Eε∼N (0,σ2I)[logits(x + ε)t].

(6)

In other words, minimizing LTCM will maximize the expected logit of class t for the random input x+ε

Gradient estimators To solve problem (4) using PGD, we need to be able to compute the gradient of the objective w.r.t x. However, for smoothed neural networks, it is not possible to exactly compute the gradient of either LCE or LTCM. We therefore must resort to gradient estimates obtained using Monte Carlo sampling.

For LTCM, we use the following unbiased gradient estimator:

∇L

(fˆ , x, t) ≈ − 1

N
∇

logits(x + ε ) ,

x TCM σ

N

x

it

i=1

εi ∼ N (0, σ2I)

16

This estimator is unbiased since

Eε1,...,εN ∼N (0,σ2I)

1N − N ∇x logits(x + εi)t
i=1

= Eε∼N (0,σ2I) [−∇x logits(x + ε)t] = ∇x Eε∼N (0,σ2I) [− logits(x + ε)t] .

For LCE, we are unaware of any unbiased gradient estimator, so, following [8], we use the following biased “plug-in” gradient estimator:

∇xLCE(fˆσ, x, t) ≈ ∇x − log

1N N f (x + εi)t
i=1

, εi ∼ N (0, σ2I)

Experimental comparison between loss functions Figure 13 shows large- adversarial examples
crafted for a smoothed neural network using both LTCM and LCE. The adversarial examples crafted using LTCM seem to better perceptually resemble the target class. Therefore, in this work we primarily use LTCM.

Experimental comparison between training procedures For most of the ﬁgures in this paper, we used a base classiﬁer from [7] trained using Gaussian data augmentation. However, in Figures 15-17, we compare large- adversarial examples for this base classiﬁer to those synthesized for a base classiﬁer trained using the SMOOTHADV procedure from [8], which was shown in that paper to attain much better certiﬁed accuracies than the network from [7]. We ﬁnd that there does not seem to be a large difference in the perceptual quality of the generated images. Therefore, throughout this paper we used the network from [7], since we wanted to emphasize that perceptually-aligned gradients arise even with robust classiﬁers that do not involve adversarial training of any kind.

Experimental study of number of Monte Carlo samples One important question is how many Monte Carlo samples N are needed when computing the gradient of LTCM or LCE. In Figure 14 we show large- adversarial examples synthesized using N ∈ {1, 5, 20, 25, 50, 75} Monte Carlo samples. There does not seem to be a large difference between using N = 20 samples or using more than 20. Images synthesized using N = 1 samples do appear a bit less developed than the others (e.g. the terrier with N = 1 is has fewer ears than when N is large.) In this work, we primarily used N = 20.

Hyperparameters The following table shows the hyperparameter settings for all of the ﬁgures in this paper.

Figure σ number of PGD steps

PGD step size N

1, 12 0.5

300

40.0 2.8 (vanilla), 0.7 20

2 0.5

300

40.0

0.7

20

3 0.5

300

40.0

0.7

20

4, 9-11 vary

300

40.0 2.8 (σ = 0), 0.7 20

15-17 0.5, 1.0

300

40.0

0.7

20

13 0.5

300

40.0 2.0 (CE), 0.7 20

14 0.5

300

40.0

0.7

vary

Note that Figures 1 and 12 only use stepSize = 2.8 in the Vanilla column, Figure 13 only uses stepSize = 2.0 in the C-E Loss column, and Figures 4 and 9-11 only use stepSize = 2.8 for σ = 0.

17

Figure 13: Here, we compare the perceptual quality of large- adversarial examples (for a smoothed neural network) crafted using the cross-entropy loss LCE to those crafted using the target class max LTCM loss. Observe that adversarial examples crafted using the TCM loss seem to better resemble the targeted class. For this reason, we used the TCM loss function throughout this paper.
18

Figure 14: Large- adversarial examples for a smoothed neural network crafted using different settings of the parameter N , the number of Monte Carlo samples used for gradient estimation.
19

Figure 15: We compare (part 1/3) large- targeted adversarial examples for smoothed networks trained using Gaussian data augmentation [5, 7] (columns “Smooth”) to those for smoothed networks trained using the SMOOTHADV algorithm of [8], i.e. adversarial training on the smoothed classiﬁer (columns “Adv. Smooth”).
20

Figure 16: We compare (part 2/3) large- targeted adversarial examples for smoothed networks trained using Gaussian data augmentation [5, 7] (columns “Smooth”) to those for smoothed networks trained using the SMOOTHADV algorithm of [8], i.e. adversarial training on the smoothed classiﬁer (columns “Adv. Smooth”).
21

Figure 17: We compare (part 3/3) large- targeted adversarial examples for smoothed networks trained using Gaussian data augmentation [5, 7] (columns “Smooth”) to those for smoothed networks trained using the SMOOTHADV algorithm of [8], i.e. adversarial training on the smoothed classiﬁer (columns “Adv. Smooth”).
22

