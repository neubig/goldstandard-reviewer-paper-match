A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution

Valts Blukis1,2, Chris Paxton1, Dieter Fox1,3, Animesh Garg1,4, Yoav Artzi2

1NVIDIA

2Cornell University

3University of Washington

4University of Toronto, Vector Institute

arXiv:2107.05612v3 [cs.RO] 28 Nov 2021

Abstract: Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over speciﬁc robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions. https://hlsm-alfred. github.io/
Keywords: vision and language, spatial representations
1 Introduction
Mobile manipulation in a home environment requires addressing multiple challenges, including exploration and making long-term inference about actions to perform. In addition to reasoning, robots require an accessible, yet sufﬁciently expressive interface to specify their tasks. Natural Language provides an intuitive mechanism for task speciﬁcation, and coupled with advances in automated language understanding, is increasingly applied to embodied agents [e.g., 1–11].
In this paper, we study the problem of learning to map high-level natural language instructions to low-level mobile manipulation actions in an interactive 3D environment [12]. Existing work largely studies language tightly aligned to the robot actions, either using single-sentence instructions [e.g., 1, 2, 5, 9] or sequences of instructions [13–18]. In contrast, we focus on high-level instructions, which provide more efﬁcient human-robot communication, but require long-horizon reasoning across layers of abstraction to generate actions not explicitly speciﬁed in the instruction.
Robust reasoning about manipulation goals from unrestricted high-level natural language instructions has a variety of open challenges. Consider the instruction secure two discs in a bedroom safe (Figure 1). The robot must ﬁrst locate the safe in the bedroom. It then needs to distribute the actions entailed by secure to two objects (two discs), each requiring a distinct sequence of actions, but targeting the same safe. It is also required to map the verb secure to its action space. In parallel, the robot must address mobile manipulation challenges, and often can only identify required actions as it observes and manipulates the world (e.g., if the safe needs to be opened).
We propose to construct and continually update a spatial semantic representation of the world from robot observations (Figure 2). Similar to widely used map representations [19–22], we retain the spatial properties of the environment, allowing the robot to navigate and reason about relations between objects, as required to accomplish its task. We propose the Hierarchical Language-conditioned Spatial Model (HLSM), a hierarchical approach that uses our spatial representation as a long-term memory to solve long-horizon tasks. HLSM consists of a high-level controller that generates subgoals, and a low-level controller that generates sequences of actions to accomplish them. In our example (Figure 1), the sequence of subgoals is pick up a CD, open the safe, put the CD in the safe, . . . , each requiring a sequence of actions. The spatial representation allows selecting subgoals that use previously observed objects outside of the agent’s view, or to decide about needed exploration.
We evaluate our approach on the ALFRED [12] benchmark and achieve state-of-the-art results without using the low-level instructions used by previous work [16–18, 23], neither during training

Time

Egocentric RGB
Observations

Subgoals Actions

ROTATELEFT

...

...

...

...

......

PICKUPOBJECT(CD)

MOVEFORWARD

ROTATERIGHT

PICKUPOBJECT

ROTATELEFT

OPENOBJECT(SAFE) LOOKDOWN

OPENOBJECT

PUTOBJECT(SAFE)
PUTOBJECT

Task: Secure two discs in a bedroom safe
Figure 1: Illustration of the task and our hierarchical formulation. The agent receives a high-level task in natural language. It needs to map RGB images to navigation and manipulation actions to complete the task.
nor at test-time. This paper makes three key contributions: (a) a modular representation learning approach for the problem of mapping high-level natural language task descriptions to actions in a 3D environment; (b) a method for utilizing a spatial semantic representation within a hierarchical model for solving mobile manipulation tasks; and (c) state-of-the-art performance on the ALFRED benchmark, even outperforming all approaches that use detailed sequential instructions.

2 Related Work
Natural language has been extensively studied in robotics research, including with focus on instruction [1, 24], reference resolution [25], question generation [26–28], and dialogue [4, 29, 30]. Most work in this area has considered either synthetic instructions of relatively simple goals [7, 31–33], or natural language instructions where all intermediate steps are explained in detail [5, 12–14, 34–38]. In contrast, we focus on high-level instructions, which are more likely in home environments [39].
Representation of world state, action history, and language semantics plays a central role in robot systems and their algorithm design. Symbolic representations have been extensively studied for instruction following agents [1–4, 19, 20, 39–45]. While they simplify the symbol grounding problem and enable robustness, the ontologies on which they rely on are laborious to scale to new, unstructured environments and language. Representation learning presents an alternative by learning to map observations and language directly to actions [5, 8, 9, 11, 13, 34]. World state and language semantics are represented with vectors [13] or by memorizing past observations [8, 17]. Modelling improvements have enabled these approaches to achieve good performance on complex navigation tasks [7, 9, 11, 13, 14, 37], a success that has not yet translated to mobile manipulation [12, 46, 47].
We propose integrating a semantic voxel map state representation within a hierarchical representation learning system. Similar semantic 2D maps have been successfully used in navigation [7, 8, 48, 49] and more recently even in mobile manipulation instruction-following tasks [23]. We extend these maps to 3D and show state-of-the-art results on a challenging mobile manipulation benchmark. Our map design is related to sparse metric, topological and semantic maps [10, 19– 21, 50] that have enabled grounding symbolic instruction representations. Our map does not impose a topological structure or require reasoning about object instances, instead modelling a distribution over semantic classes for every voxel.

3 Problem Deﬁnition

Let A be the set of agent actions, and S the set of world states. Given a natural language
instruction L and an initial state s0 ∈ S, the agent’s goal is to generate an execution Ξ = s0, a0, s1, a1, . . . , sT , aT , where at ∈ A is an action taken by the agent at time t, st ∈ S is the state before taking at, and st+1 = T (st, at) under environment dynamics T : S × A → S. The state st is deﬁned by the environment layout and the poses and states of all objects and the agent. The agent does not have access to the state st, but only to an observation ot. An observation ot = (It, Pt, vtS, L) includes a ﬁrst-person RGB camera image It, the agent’s pose Pt, a one-hot encoding of the object class the agent is holding vtS, and the instruction L.The task is considered successful if all goal-conditions corresponding to the task L are true at the ﬁnal state sT . Partial success is measured as the fraction of goal-conditions that have been achieved.

The ALFRED dataset includes sets of seen and unseen environments. The set

of actions A = Anav ∪ Aint includes parameter-free navigation actions Anav =

{MOVEAHEAD, ROTATELEFT, ROTATERIGHT} and interaction actions Aint

=

2

Current RGB Observation
𝐼𝑡
Observation Model 𝐹

Environment

Safe Desk

Mirror Floor Wall

Laundry Hamper

Pillow Bed

Action 𝑎𝑡
MOVEAHEAD
𝜋𝐿
Subgoal 𝑔𝑘
OPENOBJECT(SAFE)

Semantic Segmentation

𝐼𝑡𝑆

Depth 𝐼𝑡𝐷

Semantic Voxel Map

𝑉𝑡𝑆

Agent Position

𝑃𝑡

𝜋𝐻

Instruction 𝐿 Figure 2: Model architecture consisting of an observation model, high-level controller (πH ), and low-level controller (πL). The observation model updates the semantic voxel map state representation from RGB observations. πH predicts the next subgoal given the instruction and the map. πL outputs a sequence of actions to achieve the subgoal. The semantic voxel map is visualized in the middle with agent position illustrated as a black pillar, ans the current sugoal argument mask in yellow. Other colors are different segmentation classes. Saturated voxels are observed in the current timestep.
{PICKUP, PUT, TOGGLEON, TOGGLEOFF, OPEN, CLOSE, SLICE} parameterized by a binary mask that identiﬁes the object of the interaction in the agent’s current ﬁrst-person view. We compute Pt and vtS using dead-reckoning from RGB observations and actions.

4 Hierarchical Model with a Persistent Spatial Semantic Representation

We model the agent behavior with a policy π that maps an instruction L and the observation ot at time t to an action at. The policy π is made of an observation model F and two controllers: a high-level controller πH and a low-level controller πL. The observation model builds a spatial state representation sˆt that captures the cumulative agent knowledge of the world at time t. sˆt is used by both πH for high-level long-horizon task planning, and πL for near-term reasoning, such as object search, navigation, collision avoidance, and manipulation. Figure 2 illustrates the policy.
The high-level controller πH computes a probability over subgoals. A subgoal g is a tuple (type, argC , argM ), where type ∈ Aint is an interaction type (e.g., OPEN, PICKUP), argC is the semantic class of the interaction argument (e.g., SAFE, CD), and argM is a 3D mask identifying the location of the argument instance. In ALFRED, each interaction action in the set Aint corresponds to a subgoal type. When predicting the k-th subgoal at time t, πH considers the instruction L, the current state representation sˆt, and the sequence of past subgoals gi, i<k. During inference, we sample from πH . Unlike arg max, sampling allows the agent to re-try the same or different subgoal incase of a potentially random failure (e.g., if a MUG was not found, pick up a CUP).
The low-level controller πL is given the subgoal gk as its goal speciﬁcation at time t. At every timestep j > t, πL maps the state representation sˆj and subgoal gk to an action aj, until it outputs one of the stop actions: aPASS or aFAIL to indicate successful or failed subgoal completion.
The execution ﬂow is as follows. At time t = 0 the initial observation o0 is received. At each timestep, we update the state representation sˆt using the observation model. If there is no currently active subgoal, we sample a new subgoal gk from πH , and then sample an action at from πL. If at is aPASS, we increment subgoal counter k. If it is aFAIL, we discard the current subgoal k. We repeat sampling subgoals and actions until an executable action at is sampled. We execute at, increment the timestep t, and receive the next observation ot. The episode ends when the subgoal gSTOP is sampled or the horizon Tmax is exceeded. Algorithm 1 in Appendix A.4 describes this process.

4.1 State Representation
The state representation sˆt at time t captures the agent’s current understanding of the state of the world, including the locations of objects observed and the agent’s relation to them. The state representation is a tuple (VtS, VtO, vtS, Pt). The semantic map VtS ∈ [0, 1]X×Y ×Z×C is a 3D voxel map that for every position indicates which of the c ∈ [1, C] object classes are present in the voxel. The

3

observability map VtO ∈ {0, 1}X×Y ×Z is a 3D voxel map that indicates whether the corresponding position has been observed. The inventory vector vtS ∈ {0, 1}C indicates which of the C object classes the agent is currently holding. The agent pose Pt = (x, y, ωp, ωy) is speciﬁed by the 2D position (x, y), pitch angle ωp, and yaw angle ωy.
We also compute 2D state affordance features AFFORD(sˆt) ∈ [0, 1]7×X×Y in a top-down view that represent each position with one or more of seven affordance classes {pickable, receptacle, togglable, openable, ground, obstacle, observed}. Each [AFFORD(sˆt)](τ,x,y) = 1.0 if at least one of the voxels at position (x, y) has affordance class τ , otherwise it is zero. AFFORD(sˆt) is suited for object class agnostic reasoning, for example predicting a pose to pick up an object. 1

4.2 Observation Model

The observation model F (sˆt−1, ot, gk) updates the state representation with new observations. It considers the current subgoal gk to actively acquire information relevant to gk. The computation of F consists of three steps: perception, projection, accumulation.

Perception Step We predict semantic segmentation ItS and depth map ItD from the RGB observation It. We use neural networks pre-trained in the ALFRED environment. The semantic segmentation [ItS](u,v) is a distribution over C object classes at pixel (u, v). The depth map [ItD](u,v) is a binned distribution over B bins.2 We also heuristically compute a binary mask MtD that indicates which pixels have conﬁdent depth readings. We allow more conﬁdence slack in pixels that correspond to the current subgoal argument argCt according to ItS. Appendix A.3 provides further details. We use perception models based on the U-Net [51] architecture, but our framework supports other,
potentially more powerful models as well (e.g. [52, 53]).

Projection Step We use a pinhole camera model to convert depth ItD and segmentation ItS to a

point cloud that represents each image pixel (u, v) with a 3D position (x, y, z) ∈ RX×Y ×Z and

a

semantic

distribution

[ItS ](u,v).

We

use

arg

m

axB

(I

D t

)

to

compute

the

3D

positions,

and

discard

points at pixels (u, v) when the binary mask value is [MtD](u,v) = 0. We construct a discrete

semantic voxel map VˆtS ∈ [0, 1]X×Y ×Z×C , where X, Y , and Z are the width, height, and length. The value at each voxel [VˆtS](x,y,z) is the element-wise maximum of the segmentation distributions

[ItS](u,v) across all points (u, v) within the voxel. We additionally compute a binary observability

map VˆtO ∈ {0, 1}X×Y ×Z that indicates the voxels observed at time t. A voxel is observed if it

contains points, or if a ray cast from the camera through the voxel centroid has expected depth

greater than the distance from the camera to the centroid.

Accumulation Step We integrate VˆtS and VˆtO into a persistent state representation:

VtS = VˆtS × VˆtO + VtS−1 × (1 − VˆtO)

VtO = max(VtO−1, VˆtO) .

(1)

This operation updates each voxel with the most recent semantic distribution, while retaining the
values of all voxels not visible at time t. The output of the observation model is the spatial state representation sˆt = (VtS, VtO, vtS, Pt). The inventory vtS and pose Pt are taken directly from ot.

4.3 High-level Controller (πH )

At timestep t, when invoked for the k-th time, the input to πH is the instruction L, the sequence of past subgoals gi i<k, and the current state representation sˆt. The output is the next subgoal gk = (typek, argCk , argM k ). Figure 3 illustrates the high-level controller architecture.
Input Encoding We encode the text L using a pre-trained BERT [54] model that we ﬁne-tune during training. We use the CLS token embedding as the task embedding φL. We encode the state representation sˆt to account for classes of all observed objects, and the object that the agent is holding: φs(sˆt) = [vtS; max(x,y,z)(VtS)], where max(x,y,z) is a max-pooling operation over spatial dimensions and [·; ·] denotes concatenation. We compute the representations of previous subgoals as REPR(gi) ki=−01, where REPR(gi) is the sum of a sinusoidal positional encoding [55] of index i and
1We assume a known mapping between object semantic classes and affordance classes. 2We use B uniformly spaced depth bins {0, ∆D, 2∆D, . . . , (B − 1)∆D}, where ∆D is a depth resolution. We suggest ∆D should be less than 50% of the voxel size. We used voxels with edge length 0.25m.

4

Semantic Map 𝑉𝑡𝑠 Inventory 𝑣𝑡𝑠
Language 𝐿
Subgoal History (type𝑖𝐶, arg𝑖𝐶) 𝑖𝑘=−01
𝐇𝑘−1

Spatial MaxPool 𝜙𝑡𝑠
State Encoder
𝜙𝐿 BERT CLS
Language Encoder

𝐇𝑘−1 Subgoal history tensor

AFFORD State features AFFORD(𝑠ෝ𝑡)

Slice

Argument class

[𝑉𝑡𝑠](𝑎𝑟𝑔𝐶) 𝑘

mask

⨀

Birds-eye view representation
𝐱𝑘 EGOCENTRIC
TRANSFORM
REFINER

Transformer Encoder
Subgoal History Encoder

𝜙𝑘𝑔−1

Dense MLP

Sample
𝑃(type𝑡𝐻| ⋅) 𝑃(arg𝑡𝐻|type𝑡𝐻,⋅)

Subgoal Predictor

Figure 3: Illustration of the high-level controller πH (Section 4.3).

Subgoal 𝑔𝑘 = arg𝑘𝑀 arg𝑘𝐶
type𝑘

learned embeddings for typei and argCi . We process this sequence with a two-layer Transformer autoregressive encoder [55] to compute φgi ki=−01. We take φgk−1 as the subgoal history embedding

vector. We additionally encode the argument mask information argM i from the subgoal history in
an integer-valued subgoal history tensor Hk−1 ∈ NK×X×Y where [Hk−1](τ,x,y) is the number of

times an interaction action type τ was performed at 2D position (x, y) in the birds-eye view:

k−1

[Hk−1](τ,x,y) =

max([argM i ](x,y,z)) .

(2)

z

i=0...k−1

argCi =τ

Subgoal Prediction We concatenate the three representations h(t,k)

=

[

φ

L

;

φ

s t

;

φ

g k

−

1

].

We use a

densely connected multi-layer perceptron [56] to predict two distributions P (typek | h(t,k)) and

P (argCk | typek, h(t,k)), from which we sample a subgoal type typek and argument class argCk .

The remaining component of the subgoal is the action argument mask argM k . Let [VtS](argC) be a k
voxel map that only retains the object information for objects of class argCk in the semantic map VtS. We reﬁne it to identify a single object instance. We compute a birds-eye view representation:

xt = [AFFORD(sˆt); Hk−1; max([VtS ](argC )) ⊗ 1typek ]

(3)

z

k

where AFFORD(sˆt) is a birds-eye view state affordance feature map (Section 4.1) and 1typek is a
one-hot encoding of typek.3 Finally, we compute the 3D argument mask argM k ∈ [0, 1]X×Y ×Z :

argM k = REFINER(EGOTRANSFORM(xt, Pt), φL) ,

(4)

where EGOTRANSFORM(x, Pt) transforms the map x to the agent egocentric pose Pt, REFINER is a neural network based on the LingUNet architecture [36], and φL is the language embedding. The reﬁned argM k is a [0, 1]-valued 3D mask that identiﬁes the instance of the interaction argument object. If the object is believed to be unobserved, then argM k contains all zeroes. The controller output is the subgoal gk = (typek, argCk , argM k ).
4.4 Low-level Controller (πL)

The low-level controller πL is conditioned on the most recent subgoal gk = (typek, argCk , argM k ). At time t, it maps the state representation sˆt to an action at. It combines engineered and learned components. Appendix A.6 provides the implementation details. The controller πL invokes a set of procedures: NavigateTo, SampleExplorationPosition, SampleInteractionPose, and InteractMask. Their invokation follows a pre-speciﬁed execution ﬂow across multiple timesteps. First, we perform a 360° rotation to observe the nearby environment. If no objects of type argCk are observed, we explore the environment by sampling a position (x, y) = SampleExplorationPosition(sˆt), navigating there using the procedure NavigateTo(x, y, sˆt), and performing a 360° rotation. We repeat exploration until a voxel in VtS contains the class argCk with >50% probability. To interact with an object, we sample an interaction pose (x, y, ωy, ωp) = SampleInteractionPose(sˆt, gk), invoke NavigateTo(x, y, sˆt) to reach the position (x,y), and then rotate according to yaw and pitch angles (ωy, ωp). Finally, we generate the egocentric interaction mask maskt = InteractMask(sˆt, argM k ), and output the interaction action (typek, maskt).
3⊗ denotes multiplication of a X × Y tensor with a K-dimensional vector to obtain a K × X × Y tensor. [·; ·; ·] denotes channel-wise concatenation.

5

All procedures use the spatial representation sˆt. NavigateTo navigates to a goal position using a value iteration network (VIN) [57] that reasons over obstacle and observability maps from sˆt. SampleExplorationPosition samples positions on the boundary of observed space in sˆt. SampleInteractionPose uses a learned neural network NAVMODEL to predict a distributon of poses from which the interaction gk will likely succeed. InteractMask uses the segmentation image ItS and the 3D argument mask argM t to compute the ﬁrst-person mask of the target object.

5 Learning

The policy contains four learned models: the segmentation and depth networks, πH , and the navigation model NAVMODEL used by πL. We train all four networks independently using supervised learning. We assume access to a training dataset D = {(L(j), Ξ(j))}Nj=D1 of high-level natural lan-
guage instructions L(j) paired with demonstration execution Ξ(j) in a set of seen environments.
Each execution Ξ(j) is a sequence of states and actions s(0j), a(0j), . . . , s(Tj), a(Tj) . We denote NP the total number of states in dataset D, and NG the total number of subgoals.

We process D into three datasets.

The perception dataset DP

=

{([I

]

(i

)

,

[

I

D

]

(i

)

,

[

I

S

]

(i

)

}

NP i=1

in-

cludes RGB images [I](i) with ground truth depth [ID](i) and segmentation [IS](i). The subgoal

dataset Dg = {(L(i), sˆ(ti), gj(i) kj=0)}Ni=G1 contains natural language instructions L(i), state repre-

sentations sˆ(ti) at the start of k-th subgoal execution, and sequences of the ﬁrst k subgoals

gj(i)

k j=0

extracted from Ξ(j). The navigation dataset DN = {(sˆ(i), g(i), P (i))}Ni=P1 consists of state representations sˆ(i), subgoals g(i), and agent poses P (i) at the time of taking the interaction action

corresponding to subgoal g(i). The state representations sˆ(·) in datasets Dg and DN are constructed

using the observation model (Section 4.2), but using ground-truth depth and segmentation images.

We train the perception models on DP and the πH on Dg to predict the k-th subgoal by optimizing cross-entropy losses. We use DN to train the navigation model NAVMODEL by optimizing a cross-
entropy loss for positions and yaw angles, and an L2 loss for the pitch angle.

6 Experimental Setup
Environment, Data, and Evaluation We evaluate our approach on the ALFRED [12] benchmark. It contains 108 training scenes, 88/4 validation seen/unseen scenes, and 107/8 test seen/unseen scenes. There are 21,023 training tasks, 820/821 validation seen/unseen tasks, and 1533/1529 test seen/unseen tasks. Each task is speciﬁed with a high-level natural language instruction. The goal of the agent is to map raw RGB observations to actions to complete the task. ALFRED also provides detailed low-level step-by-step instructions, which simplify the reasoning process. We do not use these instructions for training or evaluation. We collect a training dataset of languagedemonstration pairs for learning (Section 5). To extract subgoal sequences, we label each interaction action at = (typet, maskt) and any preceding navigation actions with a single subgoal of type = typet. We compute the subgoal argument class argC and 3D mask argM labels from the ﬁrst-person mask maskt, and ground truth segmentation and depth. Completing a task requires satisfying several goal conditions. Following the common evaluation [58, 59], we report two metrics. Success rate (SR) is the fraction of tasks for which all goal conditions were satisﬁed. Goal condition rate (GC) is the fraction of goal-conditions satisﬁed across all tasks.
Systems We compare our approach, the Hierarchical Language-conditioned Spatial Model (HLSM) to others on the ALFRED leaderboard that only use the high-level instructions. At the time of writing, the only such published approach is HiTUT [47], an approach that uses a ﬂat BERT [54] architecture to model a hierarchical task structure without using a spatial representation. See Appendix A.2 for a detailed comparison. We also compare to approaches that use the step-by-step instructions, which puts our method at a disadvantage. Of these, LAV [60] also imposes a hierarchical task structure and uses pre-trained depth and segmentation models, but without using a spatial state representation.
Additionally, we perform ablations and study sensory oracles. To study the observation model, we compare to using sensory oracles for ground truth depth, ground truth segmentation, and both. We report high-level controller ablations that remove the subgoal encoder, language encoder, and state

6

Success

✔

✔

✔

✔

✔

Task: Secure two discs in the bedroom safe

✔

✔

✔

12
PICKUPOBJECT

51
OPENOBJECT

61
PUTOBJECT

72
CLOSEOBJECT

232
PICKUPOBJECT

272
OPENOBJECT

Correctly identified the safe, but failed to open it due to agent blocking the safe door

286
OPENOBJECT

295
PUTOBJECT

308
CLOSEOBJECT

2nd attempt succeeded from a different agent pose

Perception failure

RGB Image Input

Predicted Segmentation

Predicted Depth

Task: Hold the clock and turn on a lamp

Semantic Voxel Map (3D)

Navigation Value Function (2D)

Non-fatal perception error

RGB Image Input

Semantic Voxel Map (3D)

Unfamiliar alarm clock and room

Alarm clock mistaken as “bed”

Depth prediction error

Agent is “blocked” from moving to its goal by the imagined obstacle

186

Mirror Agent
Wall

Agent is facing a mirror, but interprets it as another room behind the wall

Grounding failure

Task: Put a washed bowl away in a kitchen cabinet
✔

Subgoal prediction failure

Task: Put an egg in a bowl on the counter

1

161

✔

PICKUPOBJECT (BOWL)

Agent wrongly picks up a cup instead of a bowl

162
PUTOBJECT (SINK)

175

176
TOGGLEON (FAUCET)

Continues unaware of the mistake

1
PICKUPOBJECT (CD)

108

109
PUTOBJECT (BOWL)

Looks for a CD instead of Egg. Thinks vase base is a CD.

Figure 4: Qualitative results showcasing successes and failures of our approach. Top row: snapshots of every interaction action taken during a successful task. Action argument masks are overlaid in red over the RGB images. The white numbers are timesteps. Middle-right: illustration of a non-fatal perception error. Middleleft: illustration of a fatal perception error. The agent incorrectly interprets the reﬂection on the alarm clock as an obstacle, causing the agent (blue star) to believe that the path to the goal (green star) is blocked off. This is reﬂected in the navigation value function computed by the value iteration network (VIN) [57], where black cells are obstacles with value −1. White cell is the goal with value 1. Bottom-left: grounding failure. The agent wrongly picks up the cup instead of a bowl. Predicted subgoals are shown in green. Bottom-right: high-level controller and percepton failure. πH predicts the wrong subgoal argument class (CD instead of EGG). The segmentation model then mistakes the vase for a CD.

representation encoder as used for predicting subgoal type typek and argument class argCk , while still using the state representation sˆt to predict the subgoal argument mask argM k . We also study a low-level controller ablation that removes the exploration procedure.

7 Results

Table 1 shows test and validation results. Our approach achieves state-of-the-art performance across both seen and unseen environments in the setting with only high-level instructions. We achieve 10.04% absolute (98.1% relative) improvement in SR on the test unseen split, and 11.53% absolute (62.6% relative) improvement in SR on the test seen split compared to HiTUT G-only.

Our approach performs competitively even when compared to approaches that also use the low-level step-by-step instructions. We achieve 4.84% absolute (31.4% relative) improvement in SR on the test unseen split compared to ABP [61]. On the test seen split, our approach performs reasonably well, however ABP [61] and LWIT [18] perform better, reﬂecting potentially stronger scene overﬁtting.

Tables 2 and 3 show development results. We performed ﬁve runs of the full HLSM model on the validation unseen data and found the sample standard deviation of the success rate is 1.1% (absolute). All other results are from a single-evaluation runs. Ground truth depth alone (+ gt depth) does not signiﬁcantly affect performance. Ground truth segmentation (+ gt seg) provides 6.6%/16.4% absolute improvement in seen/unseen scenes. Using both (+ gt depth, gt seg) provides 11.1%/21.9% absolute improvement and narrows the seen/unseen gap from 11.3% to 0.5%. This points to perception being the main bottleneck in generalization to unseen scenes.
We report high-level controller πH input encoder ablations. The poor performance without the language encoder reﬂects task difﬁculty. Zeroing the input to the subgoal history encoder (but keeping position encodings) does not signiﬁcantly affect performance, showing that knowing the index of the current subgoal in addition to the state representation is often sufﬁcient. Not using the state representation for predicting subgoal type and argument class gives mixed results in seen

7

Method

Test

Seen

Unseen

SR GC

SR GC

Low-level Sequential Instructions + High-level Goal Instruction

Validation

Seen

Unseen

SR GC

SR GC

SEQ2SEQ [12] MOCA [46] E.T. [17] E.T. + synth. data [17] LWIT [62] HITUT[47] ABP [61]

3.98 22.05 28.77 38.42 30.92 21.27 44.55

9.42 28.29 36.47 45.44 45.44 29.97 51.13

0.39 5.30 5.04 8.57 9.42 13.87 15.43

7.03 14.28 15.01 18.6 20.91 20.31 24.76

3.70 19.15 33.78 46.59 33.70 25.24 42.93

10.00 28.5 42.48 52.82 43.10 34.85 50.45

0.00 3.78 3.17 7.32 9.70 12.44 12.55

6.90 13.4 13.12 20.87 23.10 23.71 25.19

High-level Goal Instruction Only

HITUT G-only[47] LAV [60] HLSM (Ours)

18.41 13.35 29.94

25.27 23.21 41.21

10.23 6.38 20.27

20.27 17.27 30.31

13.63 12.7 29.63

21.11 23.4 38.74

11.12 -
18.28

17.89 -
31.24

Table 1: Test results. Test seen/unseen and validation seen/unseen splits. Top section approaches use sequential step-by-step instructions. The bottom section uses only high-level instructions. Best results using only highlevel instructions and using both types of instructions are highlighted.

Method HLSM

Validation

Seen

Unseen

SR GC

SR GC

29.6 38.8

18.3 31.2

+ gt depth + gt depth, gt seg. + gt seg.

29.6 40.5 40.7 50.4 36.2 47.0

20.1 33.7 40.2 52.2 34.7 47.8

w/o language enc. 0.9 8.6 w/o subg. hist. enc. 29.4 38.5 w/o state repr enc. 30.0 40.6

0.2 7.5 16.6 29.2 18.9 30.8

w/o exploration

32.2 42.4

18.1 31.3

Table 2: Development results on validation split. Per-
formance of our full approach, with perception oracles, a perception ablation, πH ablations, and πL ablations

Task Type
Overall
Examine Pick & Place Stack & Place Clean & Place Cool & Place Heat & Place Pick 2 & Place

Validation

Seen

Unseen

SR GC

SR GC

29.6 38.7

18.3 31.2

46.8 59.0 57.0 57.0 13.0 27.0 25.0 39.5 17.5 33.8
9.3 29.1 34.7 51.9

36.6 59.9 34.8 34.8
4.4 14.3 11.3 25.8 14.8 39.6 0.0 17.0 18.0 34.7

Table 3: Performance breakdown per task type on the validation split.

and unseen scenes, but without a signiﬁcant difference in performance. Therefore, predicting the
sequence of subgoal types and argument classes (i.e., what to do) is at times possible without spatial
reasoning, while grounding the subgoal (i.e., where to do it) requires spatial information. Removing random exploration from πL does not signiﬁcantly affect unseen performance.

Figure 4 illustrates the model behavior, showing both successes and common failures. The main failures in valid unseen scenes are due to (1) perception errors that result in missing or extraneous obstacles or picking up wrong objects; (2) insufﬁciency of random exploration (e.g., not searching inside cabinets); (3) navigation model errors (e.g., blocking objects from opening); (4) subgoal prediction errors (e.g., picking up wrong objects); and (5) lack of state-aware multi-step planning and backtracking. More qualitative results are available in Appendix A.10.

8 Discussion and Limitations
We showed that a persistent spatial semantic representation enables a hierarchical model to achieve state-of-the-art performance on a challenging instruction-following mobile manipulation task. The main performance bottlenecks include long-horizon exploration, perception generalization to unseen environments, and low-level motion planning for continuous collision avoidance. In terms of learning, incorporating reinforcement learning to train πH , πL, and observation model F jointly could improve robustness. We deﬁned the interface to πL to be faithful to skills available on physical robots, but the exact implementation of πL is not the focus of our work. Physical deployment would require changes to πL, and study on robustness to errors in continuous environments, such as localization or motion uncertainty.

8

9 Acknowledgements
This research was supported by ARO W911NF-21-1-0106, a Google Focused Award, and NSF under grant No. 1750499. Animesh Garg is supported in part by CIFAR AI Chair and NSERC Discovery Grant. A signiﬁcant part of the work was done during the ﬁrst author’s internship at Nvidia. We thank the authors of ALFRED for maintaining the benchmark. We thank Mohit Shridhar and Jesse Thomason for their help answering our questions, and the anonymous reviewers for their helpful comments.
References
[1] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. Gopal Banerjee, S. Teller, and N. Roy. ”Approaching the Symbol Grounding Problem with Probabilistic Graphical Models. AI Magazine, 2011.
[2] C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. Learning to parse natural language commands to a robot control system. In ISER, 2012.
[3] D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions. In RSS, 2014.
[4] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone. Learning to interpret natural language commands through human-robot dialog. In IJCAI, 2015.
[5] D. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.
[6] D. Nyga, S. Roy, R. Paul, D. Park, M. Pomarlan, M. Beetz, and N. Roy. Grounding robot plans from natural language instructions with incomplete world knowledge. In CoRL, 2018.
[7] V. Blukis, N. Brukhim, A. Bennet, R. Knepper, and Y. Artzi. Following high-level navigation instructions on a simulated quadcopter with imitation learning. In RSS, 2018.
[8] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi. Mapping navigation instructions to continuous control actions with position-visitation prediction. In CoRL, 2018.
[9] V. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language instructions to physical quadcopter control using simulated ﬂight. In CoRL, 2019.
[10] S. Patki, E. Fahnestock, T. M. Howard, and M. R. Walter. Language-guided semantic mapping and mobile manipulation in partially observable environments. In CoRL, 2019.
[11] V. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding and mapping for natural language robot instruction following. In CoRL, 2020.
[12] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In CVPR, 2020.
[13] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018.
[14] H. Tan, L. Yu, and M. Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In NAACL-HLT, 2019.
[15] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Simto-real transfer for vision-and-language navigation. In CoRL, 2020.
[16] B. Kim, S. Bhambri, K. P. Singh, R. Mottaghi, and J. Choi. Abp, alfred leaderboard, may 10th 2021. https://leaderboard.allenai.org/alfred/submission/ c2t70j37q4q5ci4so89g. Accessed: June 16th, 2021.
9

[17] A. Pashevich, C. Schmid, and C. Sun. Episodic Transformer for Vision-and-Language Navigation, 2021.
[18] Anonymous. Lwit, alfred leaderboard, may 10th 2021. https://leaderboard.allenai. org/alfred/submission/bvppcin94ro4j7j0jqlg. Accessed: May 25th, 2021.
[19] M. R. Walter, S. Hemachandra, B. Homberg, S. Tellex, and S. Teller. Learning Semantic Maps from Natural Language Descriptions. In RSS, 2013.
[20] S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter. Learning models for following natural language directions in unknown environments. In ICRA, 2015.
[21] S. Patki, A. F. Daniele, M. R. Walter, and T. M. Howard. Inferring compact representations for efﬁcient natural language understanding of robot instructions. In 2019 International Conference on Robotics and Automation (ICRA), pages 6926–6933. IEEE, 2019.
[22] I. Kostavelis and A. Gasteratos. Semantic mapping for mobile robotics tasks: A survey. Robotics and Autonomous Systems, 2015.
[23] H. Saha, F. Fotouhi, Q. Liu, and S. Sarkar. A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment. arXiv preprint arXiv:2101.07891, 2021.
[24] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In HRI, 2010.
[25] C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. A Joint Model of Language and Perception for Grounded Attribute Learning. In ICML, 2012.
[26] S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics. In RSS, 2014.
[27] R. A. Knepper, S. Tellex, A. Li, N. Roy, and D. Rus. Recovering from Failure by Asking for Help. Autonomous Robots, 2015.
[28] Z. Gong and Y. Zhang. Temporal spatial inverse semantics for robots communicating with humans. In ICRA, 2018.
[29] T. Brick and M. Scheutz. Incremental natural language processing for hri. In HRI, 2007.
[30] S. Tellex, P. Thaker, R. Deits, D. Simeonov, T. Kollar, and N. Roy. Toward information theoretic human-robot dialog. Robotics, 2013.
[31] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.
[32] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. AAAI, 2018.
[33] C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Fox. Prospection: Interpretable plans from language by predicting the future. In ICRA, 2019.
[34] A. Suhr and Y. Artzi. Situated mapping of sequential instructions to actions with single-step reward observation. In ACL, 2018.
[35] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navigation. In NeurIPS, 2018.
[36] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkin, and Y. Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018.
[37] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In ICLR, 2019.
10

[38] H. Chen, A. Suhr, D. Misra, and Y. Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In CVPR, 2019.
[39] D. K. Misra, K. Tao, P. Liang, and A. Saxena. Environment-driven lexicon induction for highlevel instructions. In ACL, 2015.
[40] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.
[41] S. R. K. Branavan, L. S. Zettlemoyer, and R. Barzilay. Reading between the lines: Learning to map high-level instructions to commands. In ACL, 2010.
[42] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011.
[43] F. Duvallet, T. Kollar, and A. Stentz. Imitation learning for natural language direction following through unknown environments. In ICRA, 2013.
[44] Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 2013.
[45] E. C. Williams, N. Gopalan, M. Rhee, and S. Tellex. Learning to parse natural language to grounded reward functions with weak supervision. In ICRA, 2018.
[46] K. P. Singh, S. Bhambri, B. Kim, R. Mottaghi, and J. Choi. Moca: A modular object-centric approach for interactive instruction following. arXiv preprint arXiv:2012.03208, 2020.
[47] Y. Zhang and J. Chai. Hierarchical task learning from language instructions with uniﬁed transformers and self-monitoring. ACL Findings, 2021.
[48] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018.
[49] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee. Chasing ghosts: Instruction following as bayesian state tracking. In NeurIPS, 2019.
[50] S. Hemachandra, M. R. Walter, S. Tellex, and S. Teller. Learning spatial-semantic representations from natural language descriptions and scene classiﬁcations. In ICRA, 2014.
[51] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015.
[52] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison. Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation? In ICCV, 2017.
[53] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Girshick. Detectron2. https://github.com/ facebookresearch/detectron2, 2019.
[54] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
[55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS, 2017.
[56] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.
[57] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel. Value iteration networks. In NeurIPS, 2016.
[58] M. Shridhar and D. Hsu. Interactive visual grounding of referring expressions for human-robot interaction. In RSS, 2018.
[59] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred leaderboard. https://leaderboard.allenai.org/alfred/. Accessed: June 16th, 2021.
11

[60] K. Nottingham, L. Liang, D. Shin, C. C. Fowlkes, R. Fox, and S. Singh. Modular framework for visuomotor language grounding. arXiv preprint arXiv:2109.02161, 2021.
[61] B. Kim, S. Bhambri, K. P. Singh, R. Mottaghi, and J. Choi. Agent with the big picture: Perceiving surroundings for interactive instruction following. In Embodied AI Workshop CVPR, 2021.
[62] V.-Q. Nguyen, M. Suganuma, and T. Okatani. Look wide and interpret twice: Improving performance on interactive instruction-following tasks. In IJCAI, 2021.
[63] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis. Learning navigation behaviors end-to-end with autorl. RA-L, 2019.
[64] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox. Contact-graspnet: Efﬁcient 6-dof grasp generation in cluttered scenes. arXiv preprint arXiv:2103.14127, 2021.
[65] D. Meagher. Geometric modeling using octree encoding. Computer graphics and image processing, 1982.
[66] T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. Loop, D. Nowrouzezahrai, A. Jacobson, M. McGuire, and S. Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In CVPR, 2021.
[67] H. P. Grice. Logic and conversation. In Speech acts. 1975.
[68] J. Roh, C. Paxton, A. Pronobis, A. Farhadi, and D. Fox. Conditional driving from natural language instructions. In CoRL, pages 540–551, 2020.
[69] C. Matuszek, D. Fox, and K. Koscher. Following directions using statistical machine translation. In HRI, 2010.
[70] N. Gopalan, D. Arumugam, L. L. Wong, and S. Tellex. Sequence-to-sequence language grounding of non-markovian task speciﬁcations. In RSS, 2018.
[71] D. Bahdanau, F. Hill, J. Leike, E. Hughes, A. Hosseini, P. Kohli, and E. Grefenstette. Learning to understand goal speciﬁcations by modelling reward. In ICLR, 2018.
[72] P. Goyal, S. Niekum, and R. J. Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. CoRL, 2020.
[73] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Visionand-language navigation in continuous environments. arXiv preprint arXiv:2004.02857, 2020.
[74] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge. Stay on the path: Instruction ﬁdelity in vision-and-language navigation. In ACL, 2019.
[75] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal grounding. In EMNLP, 2020.
[76] M. Persson, T. Duckett, C. Valgren, and A. Lilienthal. Probabilistic semantic mapping with a virtual sensor for building/nature detection. In Computational Intelligence in Robotics and Automation, 2007.
[77] H. Zender, O. M. Mozos, P. Jensfelt, G.-J. Kruijff, and W. Burgard. Conceptual spatial representations for indoor mobile robots. Robotics and Autonomous Systems, 2008.
[78] A. Pronobis, O. Martinez Mozos, B. Caputo, and P. Jensfelt. Multi-modal semantic place classiﬁcation. IJRR, 2010.
[79] A. Pronobis. Semantic mapping with mobile robots. PhD thesis, KTH Royal Institute of Technology, 2011.
12

A Appendix
A.1 Frequently Asked Questions
• Are the ALFRED sequential instructions needed during training? The sequential stepby-step instructions are not needed neither during training, nor at test-time.
• What has to be done to apply this approach to a real robot? The observation model, high-level controller, state representation, and the interface to the low-level controller together constitute our contribution and are intended to generalize to physical robots. Deployment on a real robot would require an implementation of the low-level controller designed for continuous motion in cluttered environments, and an implementation of the ALFRED interface to enable execution of manipulation actions such as PICKUP and TOGGLEON. Such physical robot capabilities are subject of ongoing research [63, 64].
• Does this simulated environment result constitute progress towards real-world capabilities? Real-robot operation is the long-term motivation of this work and has been carefully considered in the design of the representation and the approach. However, we do not claim to execute high-level natural language mobile manipulation instructions from raw vision on real robots in unseen environments. To date, such capabilities haven’t been demonstrated even in simulated environments, such as ALFRED. Even in this scenario, though our method achieves better results than existing work, it can still only solve 18.28% of problems in unseen environments.
• Would the system scale to physically larger environments? The main bottleneck towards scaling to larger environments is the memory constraint of the semantic memory. While our implementation is likely restricted to interior scenes when using commodity hardware, follow-up work could address this, perhaps using multi-scale representations such as Octress [65, 66].
• How are the state dynamics modeled? Are they assumed to be known or are they learned? The GOTO procedure in the low-level controller is based on a value-iteration network that utilizes a deterministic grid-navigation dynamics model on the internal representation, which is a crude approximation of the dynamics of the ROTATELEFT, ROTATERIGHT, MOVEAHEAD navigation commands. Other than that, the dynamics of the environment are assumed to be completely unknown to the agent, and are not explicitly learned or modeled.
• How would localization uncertainty affect the approach? Our representation approach assumes a reliable robot pose estimate. Precisely studying the effects of pose errors would require integration into a system for continuous environments. Intuitively, voxels further away are affected by pose errors more, but may better tolerate it due to being used mainly to decide navigation goals. Voxels close to the agent require more precision as they are used for object instance mask generation, but would be less affected by pose errors. Our voxel map uses a relatively coarse 25cm resolution.
• Which model was used to obtain test results? The full HLSM model was evaluated on the test set, even though the model without state representation encoding input to the high-level controller performed better in unseen environments on the validation set.
• Why does the ablation without state representation encodings perform better in unseen environments? In unseen environments, the semantic segmentation is erroneous due to the generalization gap, resulting in state encodings that contain errors. This may affect perfromance of the high-level controller that was trained on data with perfect segmentation, and thus with perfect state encodings.
• What is the beneﬁt of sampling the subgoals instead of attempting execution from most to least likely in order? There are two types of subgoal execution failures: systematic and random. An example of a systematic failure is the selection of an incorrect subgoal. For example, TOGGLEON(FLOORLAMP) would fail if a FLOORLAMP does not exist in the environment. An example of a random failure is the low-level controller sampling an interaction pose for which the interaction fails (e.g., Figure 4, row 1, timestep 272). A next-best approach would alleviate a systematic failure, but a sampling approach alleviates both: the systematic failures by trying different subgoals, and random failures by potentially sampling the same subgoal multiple times.
13

A.2 Extended Related Work
Grounding High-level Language to Actions in Robotics In order for natural language humanrobot interfaces to be useful and widely adopted in practice, they should support instructions that are as brief as possible while still being informative of the task, i.e., that adhere to Grice’s maxim of quantity [67]. Following such high-level instructions requires bridging the gap from high-level language to long sequences of low-level actions. This is commonly achieved using temporal abstraction, where subgoals or options abstract over sequences of low-level actions, reducing the effective time horizon of the problem. Most work on instruction following in robotics utilizes temporal abstraction [1, 3, 10, 20, 21, 33, 36, 39, 42, 44, 68].
Various methods explicitly model correspondences between linguistic constituents in a symbolic instruction representation, environment percepts in the world model, and subgoals (behavior primitives) [1, 3, 20, 24, 39, 69]. This requires the instruction to at least mention each subgoal, and precludes instructions that omit intermediate goals that are expected to be inferred. This limitation can be overcome by directly mapping from language to reward speciﬁcations [45, 70–72] or postconditions [39], and then using a planner [39] or learning a task-speciﬁc policy [71, 72] to solve for the sequence of actions. Both are difﬁcult in practice. Planning requires a compact, symbolic environment representation with an underlying ontology that is hard to construct for unstructured environments, such as the household environment studied in this work. Policy learning is computationally expensive, and poorly adapts to novel tasks speciﬁed in natural language in real-time.
Recently, methods that map language and observations directly to actions using neural networks have seen rising popularity and success on simulated [8, 13, 14, 38, 73–75] and real-robot [9, 11, 15] navigation, as well as simulated manipulation [5] tasks. Simulated mobile manipulation is a promising next frontier [12, 46, 47]. Representation learning approaches avoid planning, by using a direct sequence-to-sequence formulation and a data-driven approach that theoretically permits mapping arbitrarily terse input text to arbitrarily long action sequences that potentially include any necessary intermediate steps not explicitly mentioned in the text. In practice, however, most research has focuses on relatively detailed step-by-step instructions, sometimes using modelling tools such as attention [13, 17] and progress monitoring [37] to leverage the sequential nature of the instructions.
We learn to follow high-level instructions in an interactive mobile manipulation 3D environment. To bridge the gap between language and actions, we use temporal abstraction, where the high-level controller predicts subgoals that abstract over sequences of actions, and the low-level controller generates actions to fulﬁl each subgoal. The controllers rely on a spatial-semantic state representation to enable reasoning about what subgoals make progress towards the high-level task, and what actions make progress towards the speciﬁc subgoal, given all past sensory observations. The persistent representation enables operation over long time horizons. Using a shared world representation for both the high- and low-level controllers reduces representation engineering effort and error accumulation typically associated with pipeline approaches.
Semantic Maps for Language Grounding in Robotics The idea of building maps that combine spatial and semantic information [19, 50, 76–79] and using them for following natural language instructions [7, 9, 10, 20, 21, 49] has a long history in robotics. Common approaches can be classiﬁed into sparse topological and dense grid-based maps.
Walter et al. [19] introduced a sparse semantic graph that combines pose, semantic, and topological information, extracted from sensory observations and speech descriptions along a route. Hemachandra et al. [50] added a spatial map layer, and fused language with other sensory modalities. Hemachandra et al. [20] used these representations for grounding natural language route instructions. More recently, Patki et al. [21] extended this framework to build compact world models speciﬁc to the input instruction, and Patki et al. [10] enabled supporting previously unseen environments. This class of sparse topological maps are well suited for probabilistic language grounding from symbolic representations.
Dense grid-based 2D semantic maps are suited for downstream processing using learned neural network modules, and have been used in modular neural network approaches for language grounding [8, 9, 23, 48, 49]. Saha et al. [23] used a grid-based spatial representation and a map ﬁltering method, showing promising early results on a subset of the ALFRED dataset. We extend this line of work to 3D voxel maps, add explicit tracking of occupancy and observability, and maintain the representation through time to facilitate grounding high-level language over long time horizons. Our
14

dense representation has a number of advantages. First, it is easy to build in real-time from RGBD data using segmentation models and geometric operations. Second, it captures structures found in indoor environments, such as L-shaped countertops or kitchen islands with sinks that are hard to represent topologically. Third, it encodes spatial object relationships without requiring an ontology of spatial relations, or even tracking of object instances. The main limitation of our approach is a memory footprint that scales with the physical size of the environment, making it less suited for outdoor or ﬁeld applications. Follow-up work could address this limitation, for example by using multi-scale representations such as octrees [65, 66].
Detailed comparison to HiTUT We provide a detailed technical comparison between our approach and HiTUT [Hierarchical Task Learning from Language Instructions with Uniﬁed Transformers and Self-Monitoring; 47], our main point of comparison.
Both our approach and HiTUT use a hierarchical task decomposition of goals into sequences of subgoals, and subgoals into sequences of actions. The set of subgoals assumed by our approach and HiTUT have differences. HiTUT has an additional subgoal GOTO(LOCATION), while we view any navigation as a means to an end of a manipulation subgoal, and therefore do not have an explicit GOTO subgoal. HiTUT additionally has subgoals for CLEAN and HEAT, (e.g., CLEAN(OBJ) usually abstracts over the sequence PUT(SINK), TOGGLEON(FAUCET), TOGGLEOFF(FAUCET), PICKUP(OBJ)), while our high-level policy would have to predict this entire sequence.
In terms of the model architecture, we use a hierarchical model with high-level and low-level controllers to mimic the task structure. In contrast, HiTUT uses a ﬂat transformer model to jointly solve high-level subgoal planning and low-level action prediction. One of their main contributions is showing how a ﬂat transformer model can be used to model a hierarchical task structure. The beneﬁt of our hierarchical model decomposition in combination with a shared spatial state representation is its ability to solve low-level navigation and manipulation problems with specialized modules, while avoiding the representational error accumulation and representation engineering issues typically associated with modular pipeline approaches.
In terms of inference, HiTUT and our approach both sample subgoals one at a time, dynamically responding to changes in environment and execution. Both approaches perform backtracking to previous subgoals upon subgoal failure.
In terms of perception, our approach requires a pre-trained segmentation model, while HiTUT requires a pre-trained object detection model to generate object entity information that is fed into the transformer.
A.3 Observation Model Details

𝐼𝑡
RGB Input

𝐼𝑡𝐷 /𝐼𝑡𝑆
Depth / Segmentation Output

Legend: Conv 3x3
Conv 3x3 Stride=2
LeakyReLU
InstanceNorm2D Upscale 2x Softmax
Figure 5: Illustration of the U-Net architecture used in the depth and segmentation networks.

At time t, during the perception step, we predict ﬁrst-person semantic segmentation ItS and depth ItD from the observation ot = (It, Pt, vtS, L), from the RGB image It with neural network models pre-trained in the ALFRED environment. Each pixel [ItS](u,v) at coordinates (u, v) is a distribution over C object classes. Likewise, [ItD](u,v) is a distribution over B uniformly spaced depth bins
{0, ∆D, 2∆D, . . . , (B −1)∆D}, where ∆D is a depth resolution. In early experiments, we observed
that ∆D should be less than 50% of the voxel size. We use ∆D = 0.1m, B = 50, and voxel size of 0.25m. We also heuristically compute a binary mask MtD that indicates which pixels have conﬁdent depth readings. We allow more conﬁdence slack in pixels that correspond to the current subgoal

15

argument argCt according to ItS. The mask MtD is used in the projection step to discard points (x, y, z) that correspond to pixels (u, v) for which [M D](u,v) = 0. The mask computation is:

MtD = (W90[ItD] < c1 E[ItD]) ∨ ((W90[ItD] < c2 E[ItD]) ∧ ([ItS]argC > 0.5)) ,

(5)

t

where W90[ItD] is the width of the 90% conﬁdence interval at each pixel, E[ItD] is the expected depth at each pixel, and [ItS]argC is a 0-1 valued segmentation mask of the class of the current subgoal
t
argument. We set the hyperparameters c1 = 0.3 and c2 = 1.0 to allow higher depth uncertainty for points corresponding to the subgoal argument.

If the agent is currently holding an object (i.e. i[[vtS](i)] > 0), we also discard points closer than 0.7m to the camera to make sure that the object in the agent inventory does not get added to the voxel map.

We use custom models based on the U-Net architecture [51] for depth and segmentation networks. The architecture is illustrated in Figure 5. It consists of a cascade of ﬁve downscale blocks followed by ﬁve upscale blocks with skip-connections. Each block includes two convolutions, two leakyReLU activations, and an instance normalization layer. The upscale blocks contain a 2x spatial upscaling operation. We found that training a separate network for depth and segmentation worked better than sharing one network for both tasks.

A.4 Model Execution Flow

Algorithm 1 Execution Flow

Input: Instr. L, Horizon Tmax.

1: g0,1,2..., sˆ−1 ← null

2: k ← 1

3: Observe initial o0.

4: for t = 0, 1, 2, . . . H do

5: sˆt ← F (sˆt−1, ot, gk)

6: do

7: if gk = null then

8:

gk ∼ πH (L, sˆt, gi i<k)

9:

if gk = gSTOP then

10:

End episode

11: at ∼ πL(gk, sˆt)

12: if at = aPASS then

13: k ← k + 1

14: if at = aFAIL then 15: gk ← null

16: while at ∈ {aFAIL, aPASS} 17: Perform at, observe ot+1

18: End episode

Algorithm 1 describes the execution ﬂow. At time t = 0 the initial observation o0 is received. At each timesep, we update the state representation sˆt (Line 5). If needed, we sample a new subgoal gk from πH (Line 8), and then sample an action at from πL. If at is aPASS, we increment subgoal counter k (Line 13). If it is aFAIL, we discard the current subgoal k (Line 15). We repeat Lines 8–15 until an executable action at is sampled. We execute at, receive the next observation (Line 17), and proceed to the next timestep. The episode ends when the subgoal gSTOP is sampled (Line 10) or the horizon Tmax is exceeded (Line 18).
A.5 High-Level Controller Details
Subgoals are predicted periodically. Let gk = (typek, argCk , argM k ) be the k-th subgoal predicted at time t. Predicting the subgoal type typek and the argument class argCk is described in the main paper (Section 4.3). This section provides further details of REFINER, the model we use to generate argM k . The mask reﬁner REFINER has four inputs:4 (a) a spatial feature map xetgo ∈ [0, 1]N×W ×L oriented in the agent egocentric reference frame; (b) [VtS](argC) ∈ [0, 1]W ×L×H , a
k
4Errata: Equation 4 in the main paper is missing [VtS](argC) and Pt arguments to the REFINER. k

16

Input Context Vector

Legend: Linear
Conditional Conv 1x1

Input Image

Output Image

Conv 3x3

Conv 3x3 Stride=2
LeakyReLU
InstanceNorm2D

Upscale 2x
Figure 6: Illustration of the LingUNet architecture used for as part of REFINER within the high-level controller πH , and as part of the navigation model NAVMODEL within the low-level controller. The conditional
convolutions parameters are computed during the network forward pass.

3D mask indicating all voxels that contain objects of class argCk in the voxel map VtS; (c) the agent’s pose Pt; and (d) a vector representation of the instruction φL. It outputs a 3D mask argM k ∈ [0, 1]W ×L×H that identiﬁes the subgoal argument object. Formally, the computation is:5

REFINER(xetgo, [VtS ](argC ), Pt, φL) =

(6)

k

ALLOTRANSFORM(LINGUNETm(xetgo, φL), Pt) ⊗ [VtS ](argC) , k

where ALLOTRANSFORM transforms a spatial 2D map from an egocentric to the global reference frame, and LINGUNETm is a language-conditioned image-to-image encoder-decoder [36]. The architecture of LINGUNETm is illustrated in Figure 6.

A.6 Low-Level Controller Details

We describe the implementation of each of the low-level controller procedures. This implementation is not the focus of this paper, and could be improved or replaced with other algorithms. Some of the procedures cause actions in the AI2Thor environment, others simply process data to pass between procedures.
The procedures are NavigateTo, SampleExplorationPosition, SampleInteractionPose, and InteractMask. The low-level controller receives the subgoal gk, and follows a pre-speciﬁed execution ﬂow across multiple timesteps to complete it. The execution ﬂow (Figure 7) consists of an exploration and interaction phase. In the exploration phase, we perform a 360° rotation by generating a sequence of three ROTATELEFT actions to observe the environment and add information to the semantic map. If the semantic map indicates that no object of type argCk , the action argument, is observed, we explore the environment by sampling a position (x, y) = SampleExplorationPosition(sˆt), navigating there using NavigateTo(x, y, sˆt), and performing another 360° rotation. We repeat this process until a voxel in VtS contains the class argCk with >50% probability, at which point we move on to the interaction phase. In the interaction phase, we sample an interaction pose (x, y, ωy, ωp) = SampleInteractionPose(sˆt, gk), invoke NavigateTo(x, y, sˆt) to reach the position (x,y), and rotate according to yaw and pitch angles (ωy, ωp). Finally, we generate the egocentric interaction action mask maskt = InteractMask(sˆt, argM k ), and execute the interaction action (typek, maskt) in the ALFRED environment. We output aPASS or aFAIL depending if the interaction action has succeeded, and pass control back to the high-level controller to sample the next subgoal.

A.6.1 NavigateTo Procedure
At time t, the NavigateTo procedure maps a 2D navigation goal position (x, y) and the state representation sˆt to one of the actions: {ROTATELEFT, ROTATERIGHT, MOVEAHEAD, aSTOP}. We implement it with a Value Iteration Network [VIN; 57] that solves a 2D grid-MDP to predict navigation actions using fast GPU-accelerated convolution and max-pooling operations. The VIN parameters are pre-deﬁned, and not learned. Other motion planners such as A∗ could be used as well.
5⊗ is an operation that multiplies a W × L matrix by a W × L × H tensor to obtain a W × L × H tensor

17

Start

RotateInPlace

Yes

𝒂𝒓𝒈𝒌𝑪 found in 𝑽𝒕𝑺

No

Sample

Exploration

Position

Sample Interaction
Pose

Interaction Mask

INTERACTION SUCCESS?

Yes No

END
𝑎𝑝𝑎𝑠𝑠

Exploration Phase

NavigateTo

NavigateTo

Interaction Phase

END
𝑎𝑓𝑎𝑖𝑙

Figure 7: Illustration of the low-level controller execution ﬂow, showing the order in which procedures are used to complete a subgoal gk.

Current Heading North East South West

VIN Action
WEST NORTH EAST or SOUTH
NORTH EAST SOUTH or WEST
EAST SOUTH WEST or NORTH
SOUTH WEST NORTH or EAST

AI2Thor Action
ROTATELEFT MOVEAHEAD ROTATERIGHT
ROTATELEFT MOVEAHEAD ROTATERIGHT
ROTATELEFT MOVEAHEAD ROTATERIGHT
ROTATELEFT MOVEAHEAD ROTATERIGHT

Table 4: Mapping from VIN actions to AI2Thor actions.

The VIN is deﬁned by a state-space Svin, action space Avin, transition function T vin : Svin × Avin → Svin, a reward function Rvin : Svin × Avin → Avin, and terminal state set Mvin. At
each timestep t, VIN performs value iteration to compute the Q-function Qvin : Svin × Avin → R
that estimates the expected sum of future discounted rewards for taking action avt in ∈ Avin in state svt in ∈ Svin, and thereafter following a greedy policy: avi in = arg maxavin∈Avin Q(svi in, avin), i > t. We implement the state space Svin as a 2D grid of shape W vin × Hvin. Each state svin is tagged with three 0-1 valued attributes: OBSTACLE, UNOBSERVED, GOAL. At each timestep t, we set the values of state attributes according to the most recent state representation sˆt and current navigation goal (x, y). States svin with occupied voxels in the height range [0, 1.75m] are tagged OBSTACLE(svin) = 1, otherwise OBSTACLE(svin) = 0. States with all voxels unobserved are tagged UNOBSERVED(svin) = 1, otherwise UNOBSERVED(svin) = 0. The state at the goal position is tagged GOAL(svin) = 1, for all others GOAL(svin) = 0. The action space is Avin = {NORTH, EAST, SOUTH, WEST, STOP}. The transition function encodes epsilon-
greedy grid navigation dynamics: (1) the action NORTH moves the agent one state north and likewise for other actions, and (2) with probability = 8% a random transition to a neighboring state occurs. Visiting any terminal state svin ∈ Mvin or executing the action STOP ter-
minates the episode. Terminal states are all states tagged with attributes OBSTACLE and GOAL, Mvin = {svin ∈ Svin | (OBSTACLE(svin) > 0.5) ∧ (GOAL(svin) > 0.5)}.
The reward function assigns different rewards for visiting states with different attributes:

Rvin(svin, avin) = −0.9 · OBSTACLE(svin) + 1.0 · GOAL(svin)
− 0.02 · UNOBSERVED(svin) + 0.001 · 1avin=STOP . (7)
OBSTACLE states receive reward −0.9, GOAL states receive reward 1.0, and UNOBSERVED states receive reward −0.02. Taking the STOP action in any state gives reward 0.001, which has the effect of the agent stopping in unsolvable cases. We use the VIN iteratively for N vin iterations, and predict an action avin = arg maxavin∈Avin (Qvin(svt in, avin). We map from the VIN action avin to a single valid AI2Thor navigation action using a deterministic mapping (Table 4).

18

A.6.2 SampleExplorationPosition
The SampleExplorationPosition procedure maps a state representation sˆt to a discrete 2D position pexplore = (x, y). Let Ps be the set of 2D positions corresponding to voxel centroids in the voxel map along the horizontal axes, and the ground set Pg as the set of all unoccupied positions that have the class FLOOR or RUG in at least one voxel. A position is unoccupied if all voxels in the height range [0, 1.75m] are free of obstacles. We deﬁne a frontier set Pf as the set of all positions Pg for which at least one immediately neighboring position contains zero observed voxels. If Pf is non-empty, we sample the position pexplore uniformly at random from Pf . Otherwise, we sample pexplore uniformly at random from Pg.

A.6.3 SampleInteractionPose

The SampleInteractionPose procedure maps the state representation sˆt and subgoal gk = (typek, argCk , argM k ) to a pose P = (x, y, ωy, ωp), where (x, y) is a discrete 2D position, ωy is the agent yaw angle, and ωp is the agent camera pitch angle. The pose is predicted such that upon reaching it, the interaction action of type typek is likely to succeed on the object of class argCk at location identiﬁed by the mask argM k .
We use a neural network model NAVMODEL to predict expected pitch E(ωp|x, y; gk, sˆt) and a
distribution P (x, y, ωy|gk, sˆt), factored as:

P (x, y, ωy|gk, sˆt) = P (ωy|x, y; gk, sˆt)P (x, y|gk, sˆt)

(8)

The network NAVMODEL is based on the LingUNet architecture (Figure 6):

NAVMODEL(sˆt, gk) = LINGUNET(AFFORD(sˆt), LINEAR([LUTT (typek); LUTC (argCk )])) , (9)
where AFFORD is an affordance feature map (Section 4.1), LINEAR is a linear layer with bias, LUTT and LUTC are embedding lookup tables, and [·; ·] is a vector concatenation.
To sample a pose P , we ﬁrst sample a position (x, y) ∼ P (x, y|gk, sˆt), then sample a yaw angle
ωy ∼ P (ωy|x, y; gk, sˆt), and ﬁnally lookup a pitch angle ωp = E(ωp|x, y; gk, sˆt).

A.6.4 InteractionMask

The

InteractionMask

procedure

maps

a

state

representation

sˆt

=

(

V

S t

,

VtO

,

vtS

,

Pt

)

,

the

most

recent RGB observation It, the most recent predicted segmentation ItS, and a subgoal gk =

(typek, argCk , argM k ) to a 0-1 valued mask maskt ∈ [0, 1]H×W that identiﬁes the interaction ob-

ject in the ﬁrst-person view observation. The interaction mask maskt is in the format expected by

ALFRED. Formally, it is computed in three steps:

maskAt = [ItS ]argC

(10)

k

maskBt = PINHOLECAM(argM k , Pt)

(11)

maskt = maskAt · maskBt ,

(12)

where PINHOLECAM projects the 0-1 valued 3D voxel map argM k to the agent’s camera plane according to the pose Pt. The mask maskAt is an egocentric 0-1 valued mask that identiﬁes all objects of class argCk in the image It. The maskBt is an egocentric 0-1 valued mask that identiﬁes the voxels argM k . For each pixel (u, v), the value [maskBt ](u,v) is the maximum of all values [argM k ](x,y,z) over voxels with coordinates (x, y, z) that the ray cast from the camera through the pixel (u, v) intersects
with. The ﬁnal mask maskt is a 0-1 valued mask that identiﬁes not only the correct object class, but also the correct instance according to the voxel mask argM k .

A.7 Additional Learning Details

A.7.1 Observation Model Learning

Data As described in Section 5, we use a perception dataset DP for training depth and segmentation models. The dataset DP = {([I](i), [ID](i), [IS](i)}Ni=P1 includes RGB images [I](i) with ground

19

Figure 8: Examples of images produced with our AUGMENT procedure. The top row shows raw RGB images from ALFRED. The bottom row shows images generated by our segmentation-aware data augmentation method. Objects like walls, sinks, ﬂoors, and furniture randomly change color, while the apple and the spoons do not.
truth depth [ID](i) and segmentation [IS](i). The ground truth depth [ID](i) at each pixel (u, v) is a distribution [ID](((i)u,v)) over B depth bins, where 100% of the probability mass is assigned to the bin containing the reference depth value. The ground truth segmentation [IS](i) is likewise at each pixel (u, v) a one-hot vector indicating the object class that pixel belongs to.
Data Augmentation The ALFRED dataset consists of 108 different training scenes, where each scene has a ﬁxed furniture and light ﬁxtures. Observations are highly correlated within each scene, which greatly reduces the effective size of the perception dataset and hurts generalization to unseen scenes. We use a custom segmentation-aware data augmentation strategy that increases the diversity of RGB observations. We compute an augmented RGB image I˜ = AUGMENT(I, IS, Ov) that maps the image I, groundtruth segmentation IS, and a set of semantic classes Ov to a new image I˜. Ov is the set of object classes that are likely to appear in different colors. Ov includes classes like FLOOR, COUNTERTOP, CABINET, VASE, SOAPBOTTLE, ALARMCLOCK that come in different designs and colors, but not classes like BANANA, APPLE, SPOON that tend to have even appearance. Algorithm 2 shows the implementation of AUGMENT. It emulates more diverse environments by applying a different random color offset to each object class in the RGB image. Figure 8 shows examples of images produced with this augmentation procedure.
During training, we apply AUGMENT with 50% probability to each training example. Additionally, with 50% probability we perform a horizontal ﬂip.
A.8 Additional Experimental Details
We collect a training dataset of language-demonstration pairs as described in Section 5. The demonstrations in ALFRED typically navigate while looking down at the ﬂoor, likely a side-effect of the PDDL planner that had access to the world state during data generation, and as such has no need to explore or observe the visual environment. We modify the demonstration trajectories to get more informative ﬁrst-person observations. First, we insert four ROTATELEFT actions at the start of each trajectory. Second, we maintain a nominal camera pitch angle of 30°during navigation, by inserting LOOKDOWN and LOOKUP actions before and after every interaction action. We discard trajectories for which these modiﬁcations cause failures. These modiﬁcations result in observations that are more useful for learning and constructing our persistent spatial representation.
A.9 Hyperparameters
Table 5 shows hyperparameter values. The hyperparameters were hand-tuned on the validation unseen split.
A.10 Additional Results
Additional qualitative results are available at: https://hlsm-alfred.github.io/.
20

Algorithm 2 AUGMENT

Input: RGB Image I, ground truth segmentation IS, set of object classes Ov. 1: I˜ ← I
2: for c ∈ Ov do 3: » Extract a binary mask corresponding to object class c 4: Mc ← [IS ](c) 5: » Apply modiﬁcations to the image, masked by the segmentation mask Mc 6: » multiplies a N -dimensional vector with a H × W tensor to compute a N × H × W tensor
7: » · is an elementwise multiplication
8: if randomBernoulli(0.5) then 9: » Sample an additive color offset for class c from a normal distribution.
10: » I3 is the 3 × 3 identity matrix. −→
11: a ∼ N ( 0 , σaI3) 12: I˜ ← I˜ + a Mc

13: if randomBernoulli(0.5) then

14: » Sample additive gaussian noise for each pixel (u, v) for class c

15: for each pixel (u, v) do

16:

gu,v ∼ N (1, σg)

17:

[I˜](u,v) ← [I˜](u,v) + gu,v · [Mc](u,v)

18: if randomBernoulli(0.5) then 19: » Sample an multiplicative color offset for class c from a normal distribution
−→ 20: m ∼ N ( 0 , σmI3) 21: I˜ ← I˜ · (m Mc)
22: clamp image I˜ within 0-1 bounds 23: return I˜

A successful example of task execution is available at: https://drive.google.com/file/d/ 1APKe3cR_-vliyU2elT5Un30w7PvkEdYs/view?usp=sharing

A failed example of task execution is available at: https://drive.google.com/file/d/1j8BJ_ ALoXGyf8a-IOkmQAg38awSWYt6f/view?usp=sharing

21

Hyperparameter

Value

Observation Model

Number of depth bins B Depth resolution ∆D

50 0.1m

Spatial State Representation

Voxel Size Voxel Map Dimensions in Voxels Voxel Map Dimensions in Meters
Number of semantic classes C

0.25m 61 × 61 × 10 15.25m × 15.25m × 2.5m
123

High-level Controller

Subgoal history encoder hidden dimension

128

Subgoal history encoder transformer layers

2

Subgoal predictor dense MLP layers

3

Subgoal predictor dense MLP hidden dimension

128

Low-level Controller
Number of VIN iterations N vin VIN state space size

122 61 × 61

Development Environment

Programming Language ML and Math Library
Table 5: Hyperparameter values.

Python PyTorch

22

