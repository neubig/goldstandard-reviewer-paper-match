Towards Zero-shot Language Modeling
Edoardo M. Ponti1, Ivan Vulic´1, Ryan Cotterell2, Roi Reichart3, Anna Korhonen1 1Language Technology Lab, TAL, University of Cambridge 2Computer Laboratory, University of Cambridge
2Faculty of Industrial Engineering and Management, Technion, IIT 1,2{ep490,iv250,rdc42,alk23}@cam.ac.uk 3roiri@ie.technion.ac.il

arXiv:2108.03334v1 [cs.CL] 6 Aug 2021

Abstract
Can we construct a neural model that is inductively biased towards learning human languages? Motivated by this question, we aim at constructing an informative prior over neural weights, in order to adapt quickly to heldout languages in the task of character-level language modeling. We infer this distribution from a sample of typologically diverse training languages via Laplace approximation. The use of such a prior outperforms baseline models with an uninformative prior (so-called ‘ﬁnetuning’) in both zero-shot and few-shot settings. This shows that the prior is imbued with universal phonological knowledge. Moreover, we harness additional language-speciﬁc side information as distant supervision for held-out languages. Speciﬁcally, we condition language models on features from typological databases, by concatenating them to hidden states or generating weights with hypernetworks. These features appear beneﬁcial in the few-shot setting, but not in the zero-shot setting. Since the paucity of digital texts affects the majority of the world’s languages, we hope that these ﬁndings will help broaden the scope of applications for language technology.
1 Introduction
With the success of recurrent neural networks and other black-box models on core NLP tasks, such as language modeling, researchers have turned their attention to the study of the inductive bias such neural models exhibit (Linzen et al., 2016; Marvin and Linzen, 2018; Ravfogel et al., 2018). A number of natural questions have been asked. For example, do recurrent neural language models learn syntax (Marvin and Linzen, 2018)? Do they map onto grammaticality judgments (Warstadt et al., 2019)? However, as Ravfogel et al. (2019) note, “[m]ost of the work so far has focused on English.” Moreover, these studies have almost always focused on train-

ing scenarios where a large number of in-language sentences are available.
In this work, we aim to ﬁnd a prior distribution over network parameters that generalize well to new human languages. The recent vein of research on the inductive biases of neural nets implicitly assumes a uniform (unnormalizable) prior over the space of neural network parameters (Ravfogel et al., 2019, inter alia). In contrast, we take a Bayesian-updating approach: First, we approximate the posterior distribution over the network parameters using the Laplace method (Azevedo-Filho and Shachter, 1994), given the data from a sample of seen training languages. Afterward, this distribution serves as a prior for maximum-a-posteriori (MAP) estimation of network parameters for the held-out unseen languages.
The search for a universal prior for linguistic knowledge is motivated by the notion of Universal Grammar (UG), originally proposed by Chomsky (1959). The presence of innate biological properties of the brain that constrain possible human languages was posited to explain why children learn languages so quickly despite the poverty of the stimulus (Chomsky, 1978; Legate and Yang, 2002). In turn, UG has been connected with Greenberg (1963)’s typological universals by Grafﬁ (1980) and Gilligan (1989): this way, the patterns observed in cross-lingual variation could be explained by an innate set of parameters wired into a languagespeciﬁc conﬁguration during the early phases of language acquisition.
Our study explores the task of character-level language modeling. Speciﬁcally, we choose an open-vocabulary setup, where no token is treated as unknown, to allow for a fair comparison among the performances of different models across different languages (Gerz et al., 2018a,b; Cotterell et al., 2018; Mielke et al., 2019). We run experiments under several regimes of data scarcity for the held-out

languages (zero-shot, few-shot, and joint multilingual learning) over a sample of 77 typologically diverse languages.
As an orthogonal contribution, we also note that realistically we are not completely in the dark about held-out languages, as coarse-grained grammatical features are documented for most world’s languages and available in typological databases such as URIEL (Littell et al., 2017). Hence, we also explore a regime where we condition the universal prior on typological side information. In particular, we consider concatenating typological features to hidden states (Östling and Tiedemann, 2017) and generating the network parameters with hypernetworks receiving typological features as inputs (Platanios et al., 2018).
Empirically, given the results of our study, we offer two ﬁndings. The ﬁrst is that neural recurrent models with a universal prior signiﬁcantly outperform baselines with uninformative priors both in zero-shot and few-shot training settings. Secondly, conditioning on typological features further reduces bits per character in the few-shot setting, but we report negative results for the zero-shot setting, possibly due to some inherent limitations of typological databases (Ponti et al., 2019).
The study of low-resource language modeling also has a practical impact. According to Simons (2017), 45.71% of the world’s languages do not have written texts available. The situation is even more dire for their digital footprint. As of March 2015, just 40 out of the 188 languages documented on the Internet accounted for 99.99% of the web pages.1 And as of April 2019, Wikipedia is translated only in 304 out of the 7097 existing languages. What is more, Kornai (2013) prognosticates that the digital divide will act as a catalyst for the extinction of many of the world’s languages. The transfer of language technology may help reverse this course and give space to unrepresented communities.
2 LSTM Language Models
In this work, we address the task of character-level language modeling. Whereas word lexicalization is mostly arbitrary across languages, phonemes allow for transferring universal constraints on phonotactics2 and language-speciﬁc sequences that may be shared across languages, such as borrowings and
1https://w3techs.com/technologies/ overview/content_language/all
2E.g. with few exceptions (Evans and Levinson, 2009, sec. 2.2.2), the basic syllabic structure is vowel–consonant.

cognates (Brown et al., 2008). Since languages are mostly recorded in text rather than phonemic symbols (IPA), however, we focus on characters as a loose approximation of phonemes.
Let Σ be the set of characters for language . Moreover, consider a collection of languages T E partitioned into two disjoint sets of observed (training) languages T and held-out (evaluation) languages E. Then, let Σ = ∪ ∈(T E)Σ be the union of character sets in all languages. A universal, character-level language model is a probability distribution over Σ∗.3 Let x ∈ Σ∗ be a sequence of characters. We write:

n

p(x | w) = p(xt | x<t, w)

(1)

t=1

where t is a time step, x0 is a distinguished beginning-of-sentence symbol, w are the parameters, and every sequence x ends with a distinguished end-of-sentence symbol xn.
We implement character-level language models with Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). These encode the entire history x<t as a ﬁxed-length vector ht by manipulating a memory cell ct through a set of gates. Then we deﬁne

p(xt | x<t, w) = softmax(W ht + b). (2)

LSTMs have an advantage over other recurrent architectures as memory gating mitigates the problem of vanishing gradients and captures long-distance dependencies (Pascanu et al., 2013).

3 Neural Language Modeling with a Universal Prior
The fundamental hypothesis of this work is that there exists a prior p(w) over the weights of a neural language model that places high probability on networks that describe human-like languages. Such a prior would provide an inductive bias that facilitates learning unseen languages. In practice, we construct it as the posterior distribution over the weights of a language model of seen languages. Let D be the examples in language , and let the examples in all training languages be D = ∪ ∈T D . Taking a Bayesian approach, the posterior over
3Note that Σ is also augmented with punctuation and white space, and distinguished beginning-of-sequence and end-ofsequence symbols, respectively.

weights is given by Bayes’ rule:

p(w | D) ∝ p(D | w) p(w) (3)
∈T

posterior

likelihood

prior

We take the prior of eq. (3) to be a Gaussian with zero mean and covariance matrix σ2 I, i.e.

p(w) = √21πσ2 exp − 2σ12 ||w||22 . (4)
However, computation of the posterior p(w | D) is woefully intractable: recall that, in our setting, each p(x | w) is an LSTM language model, like the one deﬁned in eq. (2). Hence, we opt for a simple approximation of the posterior, using the classic Laplace method (MacKay, 1992). This method has recently been applied to other transfer learning or continuous learning scenarios in the neural network literature (Kirkpatrick et al., 2017; Kochurov et al., 2018; Ritter et al., 2018).
In §3.1, we ﬁrst introduce the Laplace method, which approximates the posterior with a Gaussian centered at the maximum-likelihood estimate.4 Its covariance matrix is amenable to be computed with backpropagation, as detailed in §3.2. Finally, we describe how to use this distribution as a prior to perform maximum-a-posteriori inference over new data in §3.3.

3.1 Laplace Method
First, we (locally) maximize the logarithm of the RHS of eq. (3):

L(w) = log p(D | w) + log p(w) (5)
∈T

We note that L(w) is equivalent to the log-posterior up to an additive constant, i.e.

log p(w | D) = L(w) − log p(D) (6)

where the constant log p(D) is the log-normalizer. Let w be a local maximizer of L.5 We now approximate the log-posterior with a second-order Taylor expansion around w :

log p(w | D) ≈

(7)

L(w ) + 1 (w−w ) H (w − w ) − log p(D) 2

4Note that, in general, the true posterior is multi-modal. The Laplace method instead approximates it with a unimodal distribution.
5In practice, non-convex optimization is only guaranteed to reach a critical point, which could be a saddle point. However, the derivation of Laplace’s method assumes that we do reach a maximizer.

where H is the Hessian matrix. Note that we have omitted the ﬁrst-order term, since the gradient ∇L(w) = 0 at the local maximizer w . This quadratic approximation to the log-posterior is Gaussian, which can be seen by exponentiating the RHS of eq. (7):

exp

−

1 2

(w

−

w

)

(−H)(w − w )

(2π)d |−H|−1

N (w , −H−1)

(8)

where exp(L(w )) is simpliﬁed from both numerator and denominator. Since w is a local maximizer, H is a negative semi-deﬁnite matrix.6 The full derivation is given in App. C.
In principle, computing the Hessian is possible by running backpropagation twice: This yields a matrix with d2 entries. However, in practice, this is not possible. First, running backpropagation twice is tedious. Second, we can not easily store a matrix with d2 entries since d is the number of parameters in the language model, which is exceedingly large.

3.2 Approximating the Hessian
To cut the computation down to one pass, we exploit a property from theoretical statistics: Namely, that the Hessian of the log-likelihood bears a close resemblance to a quantity known as the Fisher information matrix. This connection allows us to develop a more efﬁcient algorithm that approximates the Hessian with one pass of backpropagation.
We derive this approximation to the Hessian of L(w) here. First, we note that due to the linearity of ∇2, we have
H = ∇2L(w)

= ∇2

log p(D | w) + log p(w)

∈T

= ∇2 log p(D | w) + ∇2 log p(w) (9)

∈T

likelihood

prior

Note that the integral over languages ∈ T is a discrete summation, so we may exchange addends and derivatives such as is required for the proof.
We now discuss each term of eq. (9) individually. First, to approximate the likelihood term, we draw on the relation between the Hessian and the Fisher
6Note that, as a result, our representation of the Gaussian is non-standard; generally, the precision matrix is positive semi-deﬁnite.

information matrix. A basic fact from information theory (Cover and Thomas, 2006) gives us that the Fisher information matrix may be written in two equivalent ways:

− E ∇2 log p(D | w)

(10)

= E ∇ log p(D | w) ∇ log p(D | w)

expected Fisher information matrix

This equality suggests a natural approximation of the expected Fisher information matrix—the observed Fisher information matrix

−1

∇2 log p(x | w)

(11)

|D| x∈D

≈ 1 ∇ log p(x | w) ∇ log p(x | w) |D| x∈D

observed Fisher information matrix

which is tight in the limit as |D| → ∞ due to the law of large numbers. Indeed, when we have a large number of training exemplars, the average of the outer products of the gradients will be a good approximation to the Hessian. However, even this approximation still has d2 entries, which is far too many to be practical. Thus, we further use a diagonal approximation. We denote the diagonal of the observed Fisher information matrix as the vector f ∈ Rd, which we deﬁne as

f=

1 ∇ log p(x | w) 2 (12)

∈T x∈D |T | · |D |

where the (·)2 is applied point-wise. Computation of the Hessian of the prior term in eq. (9) is more straightforward and does not require approximation. Indeed, generally, this is the negative inverse of the covariance matrix, which in our case means

∇2 log p(w) = − 1 I

(13)

σ2

Summing the (approximate) Hessian of the loglikelihood in eq. (12) and the Hessian of the prior in eq. (13) yields our approximation to the Hessian of the log-posterior

H˜ = −diag(f ) −

1 I

(14)

σ2

The full derivation of the approximated Hessian is available in App. D.

3.3 MAP Inference
Finally, we harness the posterior p(w | D) ≈ N (w , −H˜ −1) as the prior over model parameters for training a language model on new, held-out languages via MAP estimation. This is only an approximation to full Bayesian inference, because it does not characterize the entire distribution of the posterior, just the mode (Gelman et al., 2013).
In the zero-shot setting, this boils down to using the mean of the prior w as network parameters during evaluation. In the few-shot setting, instead, we assume that some data for the target language
∈ E is available. Therefore, we maximize the log-likelihood given the target language data plus a regularizer that incarnates the prior, scaled by a factor of λ:

L(w) =

log p(D | w)
∈E
+ λ (w − w ) 2

(15) H˜ (w − w )

We denote the the prior N (w , −H˜ −1) that features in eq. (15) as UNIV, as it incorporates universal linguistic knowledge. As a baseline for this objective, we perform MAP inference with an uninformative prior N (0, I), which we label NINF. In the zero-shot setting, this means that the parameters are sampled from the uninformative prior. In the few-shot setting, we maximize

L(w) = log p(D | w) − λ2 ||w||22 (16)
∈E

Note that, owing to this formulation, the uninformed NINF model does not have access to the posterior of the weights given the data from the training languages.
Moreover, as an additional baseline, we consider a common approach for transfer learning in neural networks (Ruder, 2017), namely ‘ﬁne-tuning.’ After ﬁnding the maximum-likelihood value w on the training data, this is simply used to initialize the weights before further optimizing them on the held-out data. We label this method FITU.

4 Language Modeling Conditioned on Typological Features
Realistically, the prior over network weights should also be augmented with side information about the general properties of the held-out language to be learned, if such information is available. In fact,

linguists have documented such information even for languages without plain digital texts available and stored it in the form of attribute–value features in publicly accessible databases (Croft, 2002; Dryer and Haspelmath, 2013).
The usage of such features to inform neural NLP models is still scarce, partly because the evidence in favor of their effectiveness is mixed (Ponti et al., 2018, 2019). In this work, we propose a way to distantly supervise the model with this side information effectively. We extend our non-conditional language models outlined in §3 (BARE) to a series of variants conditioned on language-speciﬁc properties, inspired by Östling and Tiedemann (2017) and Platanios et al. (2018). A fundamental difference from these previous works, however, is that they learn such properties in an end-to-end fashion from the data in a joint multilingual learning setting. Obviously, this is not feasible for the zeroshot setting and unreliable for the few-shot setting. Rather, we represent languages with their typological feature vector, which we assume to be readily available both for both training and held-out languages.
Let t ∈ [0, 1]f be a vector of f typological features for language ∈ T E. We reinterpret the conditional language models within the Bayesian framework by estimating their posterior probability
p(w | D, F) ∝ p(D | w) p(w | t ) (17)
∈T
We now consider two possible methods to estimate p(w | t ). For both of them, we ﬁrst encode the features through a non-linear transformation f (t ) = ReLU(W t + b), where W ∈ Rr×f and b ∈ Rr, r f . A ﬁrst variant, labeled OEST, is based on Östling and Tiedemann (2017). Assuming the standard LSTM architecture where ot is the output gate and ct is the memory cell, we modify the equation for the hidden state ht as follows:
ht = ot tanh(ct) ⊕ f (t ) (18)
where stands for the Hadamard product and ⊕ for concatenation. In other words, we concatenate the typological features to all the hidden states.
Moreover, we experiment with a second variant where the parameters of the LSTM are generated by a hyper-network (i.e., a simple linear layer with weight W ∈ R|w|×r) that transforms f (t ) into w. This approach, labeled PLAT, is inspired by

Platanios et al. (2018), with the difference that they generate parameters for an encoder-decoder architecture for neural machine translation.
On the other hand, we do not consider the conditional model proposed by Sutskever et al. (2014), where f (t ) would be used to initialize the values for h0 and c0. During the evaluation, for all time steps t, ht and ct are never reset on sentence boundaries, so this model would ﬁnd itself at a disadvantage because it would require either to erase the sequential history cyclically or to lose memory of the typological features.
5 Experimental Setup
Data The source for our textual data is the Bible corpus7 (Christodouloupoulos and Steedman, 2015).8 We exclude languages that are not written in the Latin script and duplicate languages, resulting in a sample of 77 languages.9 Since not all translations cover the entire Bible, they vary in size. The text from each language is split into training, development, and evaluation sets (80-10-10 percent, respectively). Moreover, to perform MAP inference in the few-shot setting, we randomly sample 100 sentences from the train set of each held-out language.
We obtain the typological feature vectors from URIEL (Littell et al., 2017).10 We include the features related to 3 levels of linguistic structure, for a total of 245 features: i) syntax, e.g. whether the subject tends to precede the object. These originate from the World Atlas of Language Structures (Dryer and Haspelmath, 2013) and the Syntactic Structures of the World’s Languages (Collins and Kayne, 2009); ii) phonology, e.g. whether a language has distinctive tones; iii) phonological inventories, e.g. whether a language possesses the retroﬂex approximant /õ/. Both ii) and iii) were originally collected in PHOIBLE (Moran et al., 2014). Missing values are inferred as a weighted average of the 10 nearest neighbor languages in terms of family, geography, and typology.
7http://christos-c.com/bible/ 8This corpus is arguably representative of the variety of the world’s languages: it covers 28 families, several geographic areas (16 languages from Africa, 23 from Americas, 26 from Asia, 33 from Europe, 1 from Oceania), and endangered or poorly documented languages (39 with less than 1M speakers). 9These are identiﬁed with their 3-letter ISO 639-3 codes throughout the paper. For the corresponding language names, consult www.iso.org/standard/39534.html. 10www.cs.cmu.edu/~dmortens/uriel.html

NINF

UNIV

BARE BARE OEST

acu 8.491 3.244 3.472

afr 8.607 3.229 3.995

agr 8.603 3.779 3.946

ake 8.602 5.753 6.281

alb 8.490 4.571 5.017

amu 8.610 4.912 5.959

bsn 8.591 5.046 5.695

cak 8.603 4.068 4.326

ceb 8.488 3.668 3.850

ces 8.600 4.369 4.461

cha 8.594 4.366 4.353

chq 8.598 6.940 7.623

cjp 8.494 4.600 4.985

cni 8.604 3.740 4.651

dan 8.593 3.471 4.599

deu 8.599 4.102 4.214

dik 8.490 4.447 4.533

dje 8.603 3.725 3.996

djk 8.592 3.663 3.874

dop 8.609 5.950 7.351

eng 8.488 3.816 4.028

epo 8.605 3.818 4.116

est 8.606 6.807 8.261

eus 8.605 4.118 4.321

ewe 8.490 5.049 5.497

ﬁn 8.604 4.308 4.338

NINF

UNIV

BARE BARE OEST

fra 8.587 4.066 4.467

gbi 8.610 3.823 3.912

gla 8.490 4.179 3.956

glv 8.606 4.349 4.612

hat 8.594 4.186 4.620

hrv 8.606 4.050 3.441

hun 8.493 4.836 5.030

ind 8.604 3.796 4.311

isl 8.596 5.039 5.629

ita 8.605 4.023 3.752

jak 8.488 4.051 4.793

jiv 8.601 3.866 4.039

kab 8.596 4.659 5.400

kbh 8.607 4.663 4.950

kek 8.491 4.666 4.944

lat 8.601 3.703 4.093

lav 8.588 5.415 6.130

lit 8.602 4.794 4.853

mam 8.488 4.292 5.076

mri 8.606 3.440 4.074

nhg 8.588 4.323 4.450

nld 8.601 3.851 4.326

nor 8.492 3.174 3.902

pck 8.603 4.053 4.233

plt 8.603 4.364 4.648

pol 8.601 5.158 5.556

NINF

UNIV

BARE BARE OEST

por 8.491 3.751 4.219

pot 8.600 5.336 5.359

ppk 8.596 4.506 4.599

quc 8.605 4.063 4.118

quw 8.488 3.560 4.027

rom 8.603 3.669 4.056

ron 8.588 5.011 5.690

shi 8.601 5.496 5.946

slk 8.491 4.304 4.512

slv 8.604 3.661 4.106

sna 8.596 4.146 4.283

som 8.614 4.159 4.470

spa 8.489 3.645 4.020

srp 8.604 3.414 3.437

ssw 8.593 4.064 3.780

swe 8.605 4.210 3.892

tgl 8.487 3.639 3.878

tmh 8.602 4.830 4.711

tur 8.592 5.574 5.935

usp 8.604 4.127 4.337

vie 8.490 7.137 7.484

wal 8.605 4.027 4.585

wol 8.607 4.290 4.420

xho 8.602 4.171 4.276

zul 8.488 3.218 4.109

ALL 8.572 4.343 4.691

Table 1: BPC scores (lower is better) for the ZERO-SHOT learning setting, with the uninformed prior (NINF) and the universal prior (UNIV): see §2 for the descriptions of the priors. Note that for NINF there is no difference between a BARE model and a conditional model (OEST). Colors deﬁne the partition in which each language (rows) has been held out.

BARE OEST
acu 1.413 1.308 afr 1.471 1.457 agr 1.701 1.581 ake 1.453 1.377 alb 1.590 1.552 amu 1.402 1.340 bsn 1.232 1.172 cak 1.281 1.221 ceb 1.193 1.185 ces 1.872 1.795 cha 1.934 1.790 chq 1.265 1.220 cjp 1.706 1.565 cni 1.348 1.290 dan 1.727 1.693 deu 1.532 1.512 dik 1.979 1.835 dje 1.570 1.550 djk 1.515 1.435 dop 1.810 1.676

BARE OEST
eng 1.355 1.350 epo 1.471 1.450 est 0.333 0.150 eus 1.763 1.635 ewe 2.084 1.944 ﬁn 1.716 1.680 fra 1.465 1.432 gbi 1.398 1.331 gla 3.403 1.839 glv 1.932 1.644 hat 1.480 1.454 hrv 2.059 1.974 hun 1.887 1.847 ind 1.356 1.336 isl 1.845 1.808 ita 1.615 1.583 jak 1.415 1.322 jiv 1.705 1.572 kab 1.955 1.791 kbh 1.436 1.371

BARE OEST
kek 1.131 1.133 lat 1.792 1.758 lav 2.146 1.931 lit 1.895 1.833 mam 1.654 1.548 mri 1.342 1.330 nhg 1.302 1.238 nld 1.621 1.601 nor 1.623 1.590 pck 1.731 1.711 plt 1.296 1.286 pol 1.743 1.698 por 1.586 1.552 pot 2.484 2.144 ppk 1.538 1.439 quc 1.393 1.291 quw 1.498 1.418 rom 1.706 1.587 ron 1.572 1.537 shi 2.057 1.903

BARE OEST
slk 1.844 1.754 slv 1.848 1.793 sna 1.489 1.457 som 1.477 1.468 spa 1.559 1.525 srp 1.832 1.756 ssw 1.890 1.697 swe 1.619 1.595 tgl 1.221 1.210 tmh 2.786 2.301 tur 1.801 1.773 usp 1.290 1.214 vie 1.648 1.637 wal 1.561 1.457 wol 2.053 1.890 xho 1.680 1.634 zul 1.880 1.620 ALL 1.652 1.550

Table 2: BPC results (lower is better) for the JOINT learning setting, with the uninformed NINF prior. These results constitute the expected ceiling performance for language transfer models.

NINF FITU

UNIV

BARE OEST BARE OEST

acu 4.203 2.117 2.551 2.136

afr 4.423 3.620 3.042 2.773

agr 4.268 3.282 3.403 2.457

ake 4.318 2.168 2.238 2.180

alb 4.544 3.186 3.302 3.084

amu 4.486 2.820 3.948 2.080

bsn 4.546 1.861 2.678 1.850

cak 4.426 1.994 2.053 1.956

ceb 4.084 2.562 2.595 2.470

ces 4.984 4.651 4.190 3.680

cha 4.329 2.546 2.899 2.525

chq 4.941 1.948 2.078 1.963

cjp 4.424 2.389 2.880 2.393

cni 4.185 2.797 3.018 1.982

dan 4.719 3.211 3.127 3.180

deu 4.589 3.103 3.007 2.953

dik 4.380 2.640 3.020 2.667

dje 4.382 3.815 3.398 2.898

djk 4.130 2.064 2.446 2.085

dop 4.508 2.506 2.562 2.448

eng 4.436 2.808 2.913 2.719

epo 4.469 3.609 3.511 2.825

est 3.618 1.952 2.487 1.962

eus 4.354 2.628 2.705 2.567

ewe 4.590 2.806 3.336 2.786

ﬁn 4.385 4.339 3.830 3.312

fra 4.551 3.086 3.276 2.981

gbi 4.250 2.138 2.170 2.054

gla 4.159 2.377 2.835 2.395

glv 4.346 3.523 3.702 2.644

hat 4.468 2.929 3.048 2.849

hrv 4.615 3.845 3.608 3.588

hun 4.806 3.589 3.709 3.522

ind 4.377 3.317 3.258 2.420

isl 4.744 3.174 3.703 3.101

ita 4.370 3.384 3.196 3.178

jak 4.532 2.113 2.650 2.126

jiv 4.338 3.413 3.475 2.504

kab 4.649 2.783 3.574 2.800

NINF FITU

UNIV

BARE OEST BARE OEST

kbh 4.644 2.362 2.434 2.288

kek 4.613 2.809 3.015 2.714

lat 4.239 4.342 3.416 3.202

lav 4.765 2.867 3.842 2.917

lit 4.769 3.752 3.592 3.668

mam 4.525 2.274 2.873 2.363

mri 3.795 3.482 3.010 2.459

nhg 4.373 2.004 2.480 1.965

nld 4.469 3.008 2.908 2.903

nor 4.453 3.152 2.954 3.054

pck 4.246 4.011 3.532 3.030

plt 4.201 2.532 2.742 2.490

pol 4.853 3.852 3.620 3.788

por 4.446 3.231 3.198 3.098

pot 4.299 3.773 3.944 2.763

ppk 4.439 2.220 2.736 2.236

quc 4.538 2.154 2.242 2.108

quw 4.223 2.196 2.547 2.158

rom 4.378 3.121 3.257 2.455

ron 4.579 3.273 3.734 3.216

shi 4.509 2.963 3.092 2.970

slk 4.873 3.722 3.812 3.631

slv 4.633 4.630 3.527 3.501

sna 4.455 2.910 3.114 2.870

som 4.257 3.048 2.908 2.934

spa 4.507 3.223 3.149 3.090

srp 4.561 4.467 3.367 3.380

ssw 4.370 2.611 2.924 2.570

swe 4.657 3.266 3.184 3.177

tgl 4.060 2.546 2.592 2.436

tmh 4.618 4.087 4.218 3.125

tur 4.846 3.509 4.282 3.552

usp 4.529 2.114 2.189 2.073

vie 5.185 3.018 3.751 3.015

wal 4.398 2.986 3.623 2.278

wol 4.621 2.898 2.968 2.826

xho 4.561 3.415 3.208 3.289

zul 4.564 2.625 2.866 2.622

ALL 4.467 3.007 3.120 2.731

Table 3: BPC scores (lower is better) for the FEW-SHOT learning setting, with NINF, FITU and UNIV priors. Colors deﬁne the partition in which each language (rows) has been held out.

Language Model We implement the LSTM following the best practices and choosing the hyper-parameter settings indicated by Merity et al. (2018b,a). Speciﬁcally, we optimize the neural weights with Adam (Kingma and Ba, 2014) and a non-monotonically decayed learning rate: its value is initialized as 10−4 and decreases by a factor of 10 every 1/3rd of the total epochs. The maximum number of epochs amounts to 6 for training on DT , with early stopping based on development set performance, and the maximum number of epochs is 25 for few-shot learning on D ∈E .

For each iteration, we sample a language proportionally to the amount of its data: p( ) ∝ |D |, in order not to exhaust examples from resourcelean languages in the early phase of training. Then, we sample without replacement from D a minibatch of 128 sequences with a variable maximum sequence length.11 This length is sampled from a distribution m ∼ N (µ = 125, σ = 5).12 Each epoch ends when all the data sequences have been
11This avoids creating insurmountable boundaries to backpropagation through time (Tallec and Ollivier, 2017).
12The learning rate is therefore scaled by mµ · |T|D|·|TD| | , where · is an operator that rounds to the closest integer.

sampled. We apply several techniques of dropout for regu-
larization, including variational dropout (Gal and Ghahramani, 2016), which applies an identical mask to all the time steps, with p = 0.1 for character embeddings and intermediate hidden states and p = 0.4 for the output hidden states. DropConnect (Wan et al., 2013) is applied to the model parameters U of the ﬁrst hidden layer with p = 0.2.
Following Merity et al. (2018b), the underlying language model architecture consists of 3 hidden layers with 1,840 hidden units each. The dimensionality of the character embeddings is 400. We tie input and output embeddings following Merity et al. (2018a). For conditional language models, the dimensionality of f (t ) is set to 115 for the OEST method based on concatenation (Östling and Tiedemann, 2017), and 4 (due to memory limitations) in the PLAT method based on hyper-networks (Platanios et al., 2018). For the regularizer in eq. (15), we perform grid search over the hyper-parameter λ: we ﬁnally select a value of 105 for UNIV and 10−5 for NINF.
Regimes of Data Paucity We explore different regimes of data paucity for the held-out languages: • ZERO-SHOT transfer setting: we split the sample of 77 languages into 4 partitions. The languages in each subset are held out in turn, and we use their test set for evaluation.13 For each subset, we further randomly choose 5 languages whose development set is used for validation. The training set of the rest of the languages is used to estimate a prior over network parameters via the Laplace approximation. • FEW-SHOT transfer setting: on top of the zeroshot setting, we use the prior to perform MAP inference over a small sample (100 sentences) from the training set of each held-out language. • JOINT multilingual setting: the data includes the full training set for all 77 languages, including held-out languages. This serves as a ceiling for the model performance in cross-lingual transfer.
6 Results and Analysis
The results for our experiments are grouped in Table 1 for the ZERO-SHOT regime, in Table 3 for the FEW-SHOT regime, and in Table 2 for the JOINT multilingual regime, which constitutes a ceiling to cross-lingual transfer performances. The scores
13Holding out each language individually would not increase the sample of training languages signiﬁcantly, while inﬂating the number of experimental runs needed.

represent Bits Per Character (BPC; Graves, 2013): this metric is simply deﬁned as the negative loglikelihood of test data divided by ln 2. We compare the results along the following dimensions:
Informativeness of Prior Our main result is that the UNIV prior consistently outperforms the NINF prior across the board and by a large margin in both ZERO-SHOT and FEW-SHOT settings. The scores of the naïvest baseline, ZERO-SHOT NINF BARE, are considerably worse than both ZERO-SHOT UNIV models: this suggests that the transfer of information on character sequences is meaningful. The lowest BPC reductions are observed for languages like Vietnamese (15.94% error reduction) or Highland Chinantec (19.28%) where character inventories differ the most from other languages. Moreover, the ZERO-SHOT UNIV models are on a par or better than even the FEW-SHOT NINF models. In other words, the most helpful supervision comes from a universal prior rather than from a small in-language sample of sentences. This demonstrates that the UNIV prior is truly imbued with universal linguistic knowledge that facilitates learning of previously unseen languages.
The averaged BPC score for the other baseline without a prior, FINE-TUNE, is 3.007 for FEWSHOT OEST, to be compared with 2.731 BPC of UNIV. Note that ﬁne-tuning is an extremely competitive baseline, as it lies at the core of most stateof-the-art NLP models (Peters et al., 2019). Hence, this result demonstrates the usefulness of Bayesian inference in transfer learning.
Conditioning on Typological Information Another important result regards the fact that conditioning language models on typological features yields opposite effects in the ZERO-SHOT and FEWSHOT settings. Comparing the columns of the BARE and OEST models in Table 1 reveals that the non-conditional baseline BARE is superior for 71 / 77 languages (the exceptions being Chamorro, Croatian, Italian, Swazi, Swedish, and Tuareg). On the other hand, the same columns in Table 3 and Table 2 reveal an opposite pattern: OEST outperforms the BARE baseline in 70 / 77 languages. Finally, OEST surpasses the BARE baseline in the JOINT setting for 76 / 77 languages (save Q’eqchi’).
We also also take into consideration an alternative conditioning method, namely PLAT. For clarity’s sake, we exclude this batch of results from Table 1 and Table 3, as this method proves to be

consistently worse than OEST. In fact, the average BPC of PLAT amounts to 5.479 in the ZERO-SHOT setting and 3.251 in the FEW-SHOT setting. These scores have to be compared with 4.691 and 2.731 for OEST, respectively.
The possible explanation behind the mixed evidence on the success of typological features points to some intrinsic ﬂaws of typological databases. Ponti et al. (2019) has shown how their feature granularity may be too coarse to liaise with datadriven probabilistic models, and inferring missing values due to the limited coverage of features results in additional noise. As a result, language models seem to be damaged by typological features in absence of data, whereas they beneﬁt from their guidance when at least a small sample of sentences is available in the FEW-SHOT setting.
Data Paucity Different regimes of data paucity display uneven levels of performance. The best models for each setting (ZERO-SHOT UNIV BARE, FEW-SHOT UNIV OEST, and JOINT OEST) reveal large gaps between their average scores. Hence, inlanguage supervision remains the best option when available: transferred language models always lag behind their supervised equivalents.
7 Related Work
LSTMs have been probed for their inductive bias towards syntactic dependencies (Linzen et al., 2016) and grammaticality judgments (Marvin and Linzen, 2018; Warstadt et al., 2019). Ravfogel et al. (2019) have extended the scope of this analysis to typologically different languages through synthetic variations of English. In this work, we aim to model the inductive bias explicitly by constructing a prior over the space of neural network parameters.
Few-shot word-level language modeling for truly under-resourced languages such as Yongning Na has been investigated by Adams et al. (2017) with the aid of a bilingual lexicon. Vinyals et al. (2016) and Munkhdalai and Trischler (2018) proposed novel architectures (Matching Networks and LSTMs augmented with Hebbian Fast Weights, respectively) for rapid associative learning in English, and evaluated them in few-shot cloze tests. In this respect, our work is novel in pushing the problem to its most complex formulation, zero-shot inference, and in taking into account the largest sample of languages for language modeling to date.
In addition to those considered in our work, there are also alternative methods to condition language

models on features. Kalchbrenner and Blunsom (2013) used encoded features as additional biases in recurrent layers. Kiros et al. (2014) put forth a log-bilinear model that allows for a ‘multiplicative interaction’ between hidden representations and input features (such as images). With a similar device, but a different gating method, Tsvetkov et al. (2016) trained a phoneme-level joint multilingual model of words conditioned on typological features from Moran et al. (2014).
The use of the Laplace method for neural transfer learning has been proposed by Kirkpatrick et al. (2017), inspired by synaptic consolidation in neuroscience, with the aim to avoid catastrophic forgetting. Kochurov et al. (2018) tackled the problem of continuous learning by approximating the posterior probabilities through stochastic variational inference. Ritter et al. (2018) substitute diagonal Laplace approximation with a Kronecker factored method, leading to better uncertainty estimates. Finally, the regularizer proposed by Duong et al. (2015) for cross-lingual dependency parsing can be interpreted as a prior for MAP estimation where the covariance is an identity matrix.
8 Conclusions
In this work, we proposed a Bayesian approach to transfer language models cross-lingually. We created a universal prior over neural network weights that is capable of generalizing well to new languages suffering from data paucity. The prior was constructed as the posterior of the weights given the data from available training languages, inferred via the Laplace method. Based on the results of character-level language modeling on a sample of 77 languages, we demonstrated the superiority of this prior imbued with universal linguistic knowledge over uninformative priors and unnormalizable priors (i.e., the widespread ﬁne-tuning approach) in both zero-shot and few-shot settings. Moreover, we showed that adding language-speciﬁc side information drawn from typological databases to the universal prior further increases the levels of performance in the few-shot regime. While cross-lingual transfer still lags behind supervised learning when sufﬁcient in-language data are available, our work is a step towards bridging this gap in the future.
Acknowledgements
This work is supported by the ERC Consolidator Grant LEXICAL (no 648909). RR was partially

funded by ISF personal grants No. 1625/18.
References
Oliver Adams, Adam Makarucha, Graham Neubig, Steven Bird, and Trevor Cohn. 2017. Cross-lingual word embeddings for low-resource language modeling. In Proceedings of EACL, pages 937–947.
Adriano Azevedo-Filho and Ross D. Shachter. 1994. Laplace’s method approximations for probabilistic inference in belief networks with continuous variables. In Proceedings of UAI, pages 28–36.
Cecil H. Brown, Eric W. Holman, Søren Wichmann, and Viveka Velupillai. 2008. Automated classiﬁcation of the world’s languages: A description of the method and preliminary results. STUF-Language Typology and Universals Sprachtypologie und Universalienforschung, 61(4):285–308.
Noam Chomsky. 1959. A review of B.F. Skinner’s Verbal Behavior. Language, 35(1):26–58.
Noam Chomsky. 1978. A naturalistic approach to language and cognition. Cognition and Brain Theory, 4(1):3–22.
Christos Christodouloupoulos and Mark Steedman. 2015. A massively parallel corpus: The Bible in 100 languages. Language Resources and Evaluation, 49(2):375–395.
Chris Collins and Richard Kayne. 2009. Syntactic structures of the world’s languages. http:// sswl.railsplayground.net/.
Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. 2018. Are all languages equally hard to language-model? In Proceedings of NAACL-HLT, pages 536–541.
Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information Theory. Wiley-Interscience.
William Croft. 2002. Typology and Universals. Cambridge University Press.
Matthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online. Max Planck Institute for Evolutionary Anthropology.
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser. In Proceedings of ACL, pages 845–850.
Nicholas Evans and Stephen C. Levinson. 2009. The myth of language universals: Language diversity and its importance for cognitive science. Behavioral and Brain Sciences, 32(5):429–448.
Yarin Gal and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recurrent neural networks. In Proceedings of NeurIPS, pages 1019–1027.

Andrew Gelman, Hal S. Stern, John B. Carlin, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian data analysis. Chapman and Hall/CRC.

Daniela Gerz, Ivan Vulic´, Edoardo Ponti, Jason Naradowsky, Roi Reichart, and Anna Korhonen. 2018a. Language modeling for morphologically rich languages: Character-aware modeling for word-level prediction. Transactions of the Association of Computational Linguistics, 6:451–465.

Daniela Gerz, Ivan Vulic´, Edoardo Maria Ponti, Roi Reichart, and Anna Korhonen. 2018b. On the relation between linguistic typology and (limitations of) multilingual language modeling. In Proceedings of EMNLP, pages 316–327.

Gary Martin Gilligan. 1989. A cross-linguistic approach to the pro-drop parameter. Ph.D. thesis, University of Southern California.

Giorgio Grafﬁ. 1980. Universali di Greenberg e grammatica generativa in la nozione di tipo e le sue articolazioni nelle discipline del linguaggio. Lingua e Stile Bologna, 15(3):371–387.

Alex Graves. 2013. Generating sequences with

recurrent neural networks.

arXiv preprint

arXiv:1308.0850.

Joseph H. Greenberg. 1963. Some universals of grammar with particular reference to the order of meaningful elements. Universals of Language, 2:73–113.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of EMNLP, pages 1700–1709.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Proceedings of ICLR.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526.

Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel. 2014. Multimodal neural language models. In Proceedings of ICML, pages 595–603.

Max Kochurov, Timur Garipov, Dmitry Podoprikhin, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. 2018. Bayesian incremental learning for deep neural networks. In Proceedings of ICLR (Workshop Papers).

András Kornai. 2013. Digital language death. PloS One, 8(10):e77056.

Julie Anne Legate and Charles D. Yang. 2002. Empirical re-assessment of stimulus poverty arguments. The Linguistic Review, 18(1-2):151–162.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521– 535.
Patrick Littell, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. 2017. URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. In Proceedings of EACL, pages 8–14.
David JC MacKay. 1992. A practical Bayesian framework for backpropagation networks. Neural computation, 4(3):448–472.
Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of EMNLP, pages 1192–1202.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018a. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018b. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations.
Sabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner. 2019. What kind of language is hard to language-model? In Proceedings of ACL, pages 4975–4989.
Steven Moran, Daniel McCloy, and Richard Wright, editors. 2014. PHOIBLE Online. Max Planck Institute for Evolutionary Anthropology, Leipzig.
Tsendsuren Munkhdalai and Adam Trischler. 2018. Metalearning with Hebbian fast weights. arXiv preprint arXiv:1807.05076.
Robert Östling and Jörg Tiedemann. 2017. Continuous multilinguality with language vectors. In Proceedings of the EACL, pages 644–649.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difﬁculty of training recurrent neural networks. In Proceedings of ICML, pages 1310– 1318.
Matthew E Peters, Sebastian Ruder, and Noah A Smith. 2019. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of RepL4NLP-2019, pages 7–14.
Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, and Tom Mitchell. 2018. Contextual parameter generation for universal neural machine translation. In Proceedings of EMNLP, pages 425–435.

Edoardo Maria Ponti, Helen O’horan, Yevgeni Berzak, Ivan Vulic´, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. 2019. Modeling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics, 45(3):559–601.
Edoardo Maria Ponti, Roi Reichart, Anna Korhonen, and Ivan Vulic´. 2018. Isomorphic transfer of syntactic structures in cross-lingual NLP. In Proceedings of ACL, pages 1531–1542.
Shauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019. Studying the inductive biases of RNNs with synthetic variations of natural languages. In Proceedings of NAACL-HLT, pages 3532–3542.
Shauli Ravfogel, Yoav Goldberg, and Francis Tyers. 2018. Can LSTM learn to capture agreement? The case of Basque. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 98–107.
Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018. Online structured Laplace approximations for overcoming catastrophic forgetting. In Proceedings of NIPS, pages 3738–3748.
Sebastian Ruder. 2017. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098.
Gary F. Simons. 2017. Ethnologue: Languages of the world, 22nd edition. Dallas, Texas: SIL International.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of NIPS, pages 3104–3112.
Corentin Tallec and Yann Ollivier. 2017. Unbiasing truncated backpropagation through time. arXiv preprint arXiv:1705.08209.
Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. 2016. Polyglot neural language models: A case study in cross-lingual phonetic representation learning. In Proceedings of NAACL-HLT, pages 1357–1366.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. Matching networks for one shot learning. In Proceedings of NIPS, pages 3630–3638.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. 2013. Regularization of neural networks using DropConnect. In Proceedings of ICML, pages 1058–1066.
Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641.

A Character Distribution
Even within the same setting, BPC scores vary enormously across languages in both the ZEROSHOT and FEW-SHOT settings, which requires an explanation. Similarly to Gerz et al. (2018a,b), we run a correlation analysis between language modeling performance and basic statistics of the data. In particular, we ﬁrst create a vector of unigram character counts for each language, shown in Fig. 1. Then we estimate the cosine distance between the vector of each language and the average of all the others in our sample. This cosine distance is a measure of the ‘exoticness’ of a language’s character distribution.
Pearson’s correlation between such cosine distance and the perplexity of UNIV BARE in each language reveals a strong correlation coefﬁcient ρ = 0.53 and a statistical signiﬁcance of p < 10−6 in the ZERO-SHOT setting. On the other hand, such correlation is absent (ρ = −0.13) and insigniﬁcant p > 0.2 in the FEW-SHOT setting. In other words, if a few examples of character sequences are provided for a target language, language modeling performance ceases to depend on its unigram character distribution.
B Probing of Learned Posteriors
Finally, it remains to establish which sort of knowledge is embedded in the universal prior. How to probe a probability distribution over weights in the non-conditional UNIV BARE language model? First, we study the signal-to-noise ratio of each parameter wi, computed as |µσii| , in each of the 4 splits. Intuitively, this metric quantiﬁes the ‘informativeness’ of each parameter, which is proportional to both the absolute value of the mean and the inverse standard deviation of the estimate. The probability density function of the signal-to-noise ratio is shown in Fig. 2. From this plot, it emerges that the estimated uncertainty is generally low (small σi denominators yield high values). Most crucially, the signal-to-noise values concentrate on the left of the spectrum. This means that most weights will not incur any penalty for changing during few-shot learning based on eq. (15); on the other hand, there is a bulk of highly informative parameters on the right of the spectrum that are very likely to remain ﬁxed, thus preventing catastrophic forgetting. All splits display such a pattern, although somewhat shifted.
Second, to study the effect of conditioning the

universal prior on typological features, I generate random sequences of 25 characters from the learned prior in each language. The ﬁrst character is chosen uniformly at random, and the subsequent ones are sampled from the distribution given by eq. (1) with a temperature of 1. The resulting texts are shown in Table 4. Although this would warrant a more thorough and systematic analysis, from a cursory view it is evident of the sequences abide with universal phonological patterns, e.g. favoring vowels as syllabic nuclei and ordering consonants based on sonority hierarchy. Moreover, the language-speciﬁc information clearly steers predicted sequences towards the correct inventory of characters, as demonstrated by Vietnamese (VIE) and Lukpa (DOP) in Table 4.

LIT NOR KEK JIV DJE SLK CES POR SPA GLV POL QUC WAL XHO SOM TGL CJP ACU FIN MRI SLV HRV EPO AMU KBH CEB GBI ENG ISL SNA RON KAB NHG DAN PPK SSW WOL DEU CAK

javen šuksyr sun siriai tes pije nuks s hech far binje alrn bre a ver e hior sx er taj chan linam laj âtebke naque da tum suuam sιtas nekkin una tekaru ni a ciya toi milkak mo to yen nga suci o je to temokoé lostave sa jesé gukli e je jek jem neuteN rekssýj jazá níb ws ucˇ somo ai jegparase saves e iper to esquár y lues dusme allis nencec adi ayr shz˙i ayn ai sephson a gil or geee eteni na hidi cếho oz˙ swchj jeci i cil ûs xe cä wija ro pio kin cbi’ ij jejac banjake la dos que benthi shivegina ukayla azigeecoa kosubentisiili jen maky ao kun adku i sir jija i befey yadui ikugy peo asha atan kao amai kain ak a pae yei aje kin trheka pän awawa ri s animmhi mustatur tukaw aants aastasai a
i koin suu meit ja ii soi tetot jasw oki ka benoka ai ki kimanka pikaka ko cˇicˇvim koko si necˇe pau ku meta noj ne
ca ka te zet jon jem nezin isak ve u j li inij keris ec xom el e sepon kaj m´ ibinya na ñero melee cano’ ndo’ cy’oc x¨e aquangmomnaynangmuacha tojam abithon kayay isa atoug giraban sula fuma ome pani de imoako kema kaye ntul g ban urse auth ahen ant msesher at nhe j noka nie leli maken ti aide ni itsim a xe yare ske tengker ci bendar nu derbe ma awa nasil ko khe ni koy koj tikis t je cana ka casa chomdis mear de ber h chun neyal den ma kashtaka asa as riste dnepse aa aye sas ningli inas giksaj abe ios yena mona kemewascoj ni ne maa nta yoti gesi kela nii ikasgaber ni tus alen kokpan fed man benu pei ei kestam ke giko si obi rer nin eber tun ke ele tej je awem titoj lunik c’u chis m ni

SHI JAK SWE DIK EWE ALB CNI POT ZUL QUW AGR DOP EUS HUN GLA PCK AFR USP IND ROM TMH ITA SRP NLD LAT MAM VIE
EST CHA FRA DJK LAV BSN HAT TUR AKE CHQ PLT

ereswrin an daγtartnaas ni mad yanó ﬁ pelo ayok musam nejaz jih tewat ushi ssiar rˇades perdeshen heklui tart si a e wEn ke nuN ni piyitia de run ye e ke å mula pe ose le ake mente amesa ke kul
I kur je ki thet je ji tin nuk t tho u pen mireshisinoe airitcsa ateani yi neta ynimka nekin linaayi meu carií a ởnakan kuná bencro krileke konusti k ai chimira kachisinyra poi apre asyu ji ica ama kujaa muri wajetar aumam hu
btElO ι telaγa kO n ι zûγι nEk@ pO cerer nagcermac istirinun qatserite elyet a bukot aky azraá ot mu háláj y o e kere hhó sho dhöìr te ilailui a tu a u gihiha ki mi dhia mea la hen a puh ih mal hoor in e sheei wer var buerkeas en okan mi ykis ris rajajkujij taka ja t berka duhah menkad kemia ukus keri ya hal kus seke nukertia dehe neshes hos n @rofm sibarn awigtir li d usi leped tri cordia io si si conse de namni nel e se a nil do zasom kuz je sefe nij hocˇ e suet en de semeshord ak abaido zin ifte quissi fetam remnas emens in timnex í la Nil a cheh tjea nut tej quxen kaj
hẩ kì đãi bi ầt ni γì sa hiổ vu¯ r
inam acha dius dempegun geben parug j ê duka ka kina kia nextis ne aka nisa
dis assan in man usia issokoj mulel e me okrana anginar matom iliantarinta a non
ilu kagsa eriri isi paj ewri bus os as juhma yainawa nusa wali apai basti a kuneati ua veskos oramaj meseqen ye k che a shachmo ềspi meng rinnaj e ish em n jes silem semmo caja arka wagtoa doo shas nej neysakun kina alistad mesabe Vwi meyak me imai anet alavis edte kin

Table 4: Randomly generated text on observed languages (top) and held-out languages (bottom) in the 4th split.

DDFIXU DDNJHU DPDOEX EFDVNQ FFHHEV FFKKTD FFMQSL GGDHQX GGLMNH GGRMNS HHQSJR HHXVVW HZILQH JIUEDL JJOOYD KKUDYW KLQXGQ LLWVDO MMDLNY NNEDEK NOHDNW ODOLYW PPDPUL QQKOGJ SQFRNU SSROWO SSRRUW STSXNF TURXPZ UVRKQL VVOONY VVRQPD VVSUSD VVZVZH WPWJKO XWVXSU ZYLDHO Z[KRRO ]XO
   " ? AEHKNQT WZ]`  ilor t èy|~ê











Figure 1: Unigram character distribution (x-axis) per language (y-axis). Note how some rows stand out as outliers.

pdf

102 101 100 10−1 10−2 10−3 10−4 10−5
10−5

10−4
|µ| σ

10−3

Figure 2: Probability density function of the signal-to-noise ratio for each parameter of the learned posteriors in the UNIV BARE language models on splits 1 (blue), 2 (red), 3 (green), 4 (gold). The plot is in log-log scale.

C Derivation of the Laplace Approximation

p(w | D) =

exp L(w) exp L(w) dw

Bayes rule

≈

exp L(w ) + (w − w )

∇L(w

)+

1 2

(w

−

w

)

H (w − w )

Taylor expansion

exp L(w ) + (w − w )

∇L(w

)+

1 2

(w

−

w

)

H (w − w )

dw

exp

L(w

)+

1 2

(w

−

w

)

H (w − w )

= exp L(w ) + 1 (w − w ) H (w − w ) dw

2

∇L(w)|w = 0

exp

L(w )

exp

−

1 2

(w

−

w

)

(−H) (w − w )

= exp L(w )

exp − 1 (w − w ) (−H)(w − w ) dw

2

exponential of sum

= exp − 12 (w − w ) (−H)(w − w ) integration and simpliﬁcation

(2π)d |−H|−1

N (w , −H−1) (19)

D Derivation of the Approximated Hessian

We assume w ∼ N (0, σ2I). Given the relationship among the expected Fisher Information I(w), the observed Fisher Information J (w), the observed Fisher Information based on |D| samples JD(w), and the Hessian H:

−I(w) = −EJ (w) ≈ − 1 JD(w) = 1 H = 1 ∇2L(w)

(20)

|D|

|D|

|D|

we can derive our approximation of |D1 | H:

1 ∇2L(w) |D|

= 1 ∇2 |D|

log p(D | w) + log p(w)
∈T

deﬁnition of L(w)

=

1 ∇2 log p(x | w) + ∇2 log p(w) linearity of ∇2

∈T x∈D |T | · |D |

=

1∇

∈T x∈D |T | · |D |

∇p(x | w) p(x | w)

+ ∇2 log p(w)

derivative of logarithm

1 p(x | w)∇2p(x | w) − ∇p(x | w)∇p(x | w)

= |T | · |D |

p(x | w)2

∈T x∈D

+ ∇2 log p(w) quotient rule

1 = |T | · |D |
∈T x∈D

∇2p(x | w) p(x | w) −

∇p(x | w) p(x | w)

∇p(x | w) p(x | w)

+ ∇2 log p(w) rearrange and simplify

1

∇2p(x | w)

=

|T | · |D | p(x | w) − ∇ log p(x | w) ∇ log p(x | w)

∈T x∈D

+ ∇2 log p(w) derivative of logarithm

(21)





1

∇2p(x | w) 1

≈ |T | Ex∼ p(·|w) p(x | w) − |D | ∇ log p(x | w) ∇ log p(x | w) 

∈T

x∈D

+ ∇2 log p(w) sample average as expectation

 1 = |T | 
∈T



∇2p(x | w)

1

p(x | w) p(x | w) dx − |D | ∇ log p(x | w) ∇ log p(x | w) 

x∈D

+ ∇2 log p(w) expectation as integral





= |T1 | ∇2
∈T

p(x | w) dx − 1 |D |

∇ log p(x | w) ∇ log p(x | w) 

x∈D

+ ∇2 log p(w) simplify

=

−1 ∇ log p(x | w) ∇ log p(x | w) + ∇2 log p(w) derivative of constant

∈T x∈D |T | · |D |

≈

−1

2
diag ∇ log p(x | w) + ∇2 log p(w)

∈T x∈D |T | · |D |

diagonal approximation

−1

21

=

|T | · |D | diag ∇ log p(x | w) − σ2 I

∈T x∈D

second derivative of log-probability

