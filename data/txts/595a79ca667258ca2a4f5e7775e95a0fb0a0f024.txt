arXiv:2111.09456v3 [cs.GT] 6 Apr 2022

Improved Rates for Derivative Free Gradient Play
in Strongly Monotone Games∗
Dmitriy Drusvyatskiy†, Maryam Fazel‡, Lillian J. Ratliﬀ‡
The inﬂuential work of Bravo et al. (2018) shows that derivative free gradient play in strongly monotone games has complexity O(d2/ε3), where ε is the target accuracy on the expected squared distance to the solution. This paper shows that the eﬃciency estimate is actually O(d2/ε2), which reduces to the known eﬃciency guarantee for the method in unconstrained optimization. The argument we present simply interprets the method as stochastic gradient play on a slightly perturbed strongly monotone game to achieve the improved rate.
1 Introduction
Game theoretic abstractions are foundational in many application domains ranging from machine learning to reinforcement learning to control theory. For instance, in machine learning, game theoretic abstractions are used to develop solutions to learning from adversarial or otherwise strategically generated data (see, e.g., Madry et al. (2018); Fiez et al. (2020); Goodfellow et al. (2014); Narang et al. (2022)). Analogously, in reinforcement learning and control theory, game theoretic abstractions are used to develop robust algorithms and policies. (see, e.g., Ratliﬀ et al. (2016); Zhou et al. (2017); Ratliﬀ and Fiez (2020); Li and Marden (2011); Yekkehkhany et al. (2021); Zhang et al. (2020, 2019)). Additionally, game theoretic abstractions are used to capture interactions between multiple decision making entities and to model asymmetric information and incentive problems (see, e.g., Ratliﬀ and Fiez (2020); Li and Marden (2011); Savas et al. (2019)).
In such game theoretic abstractions, each decision-maker or ‘player’ faces an optimization problem that is dependent on the decisions of other players in the game. Learning as a tool for ﬁnding equilibria, or explaining how players in a game arrive at an equilibrium through a process of taˆtonnement, is a long studied phenomenon Fudenberg et al. (1998); Cesa-Bianchi and Lugosi (2006). Gradient-based learning algorithms form a very natural class of learning algorithms for games on continuous actions spaces with suﬃciently smooth cost functions. Additionally, in both control theory and machine learning, typically gradient-based methods are used since they scale well.
There is an extensive body of literature—too vast to cite all relevant work—analyzing stochastic gradient play and its variants in diﬀerent classes of games, ranging from zero-sum to general-sum. The majority of the work on stochastic gradient play assumes access to a gradient oracle that provides an unbiased estimate of each player’s individual gradient—i.e., the partial gradient of a player’s cost with respect to their own choice variable.
Counter to this, we are motivated by settings in which non-cooperative players interact in extremely low-information environments: in this paper, we examine the long-run behavior of learning with so-called “bandit feedback” in strongly monotone games. Speciﬁcally, players have access only to a loss function oracle, and use responses to queries (of the loss function value) to construct a gradient estimate. The bandit feedback setting has been studied extensively in the single player case Agarwal et al. (2010); Shamir (2017); Flaxman et al. (2005); Nesterov and Spokoiny (2017), and over the last few years it has been extended to the multi-agent setting Bravo et al. (2018); Tatarenko and Kamgarpour (2020, 2019).
∗Drusvyatskiy’s research was supported by NSF DMS-1651851 and CCF-2023166 awards. Ratliﬀ’s research was supported by NSF CNS-1844729 and Oﬃce of Naval Research YIP Award N000142012571. Fazel’s research was supported in part by awards NSF TRIPODS II-DMS 2023166, NSF TRIPODS-CCF 1740551, and NSF CCF 2007036.
†Drusvyatskiy is in the Department of Mathematics at the University of Washington, Seattle. email: ddrusv@uw.edu. ‡Ratliﬀ and Fazel are in the Department of Electrical and Computer Engineering at the University of Washington, Seattle. email: {ratliffl,mfazel}@uw.edu.
1

In particular, the inﬂuential work of Bravo et al. (2018) studies derivative free “gradient play” wherein
players formulate a gradient estimate using a single-point query to their loss function. While in a general game such algorithms (even with perfect gradient information) may not converge, for strongly monotone games,
which admit a unique Nash equilibrium, the authors show convergence to the Nash equilibrium. Moreover, they show that the iteration complexity is O(d2/ε3), where ε is the target accuracy on the expected squared distance to the solution, and d is the problem dimension. It was conjectured in Bravo et al. (2018) that this rate should match that of single player optimization, which is known to be O(1/ε2) in terms of target accuracy.
In this paper, we resolve this open question by showing that the iteration complexity is in fact O(d2/ε2).
Our proof deviates signiﬁcantly from the analysis in Bravo et al. (2018). In particular, we take the unique perspective that the update players are executing is simply stochastic gradient play on a slightly perturbed
strongly monotone game, and this tighter analysis leads to the optimal rate result.

2 Problem Setup and Algorithm

In this paper, we consider an n-player game deﬁned by cost functions fi : Xi → R and sets of strategies Xi ⊂ Rdi. Thus each player i ∈ {1, . . . , n} seeks to solve the problem

min fi(xi, x−i),
xi∈Xi
where x−i denotes the actions of all the players excluding player i. A vector of strategies x⋆ = (x⋆1, . . . , x⋆n) is a Nash equilibrium if each player i has no incentive to unilaterally change their strategy, that is

x⋆i ∈ argmin fi(xi, x⋆−i).
xi∈Xi

Throughout, the symbol ∇i(·) denotes the partial derivative of the argument (·) with respect to xi. Set

d=

n i=1

di,

and

let

Si

and

Bi

denote

the

unit

sphere

and

unit

ball

in

Rdi ,

respectively.

Additionally,

throughout we impose the following convexity and smoothness assumptions.

Assumption 1 (Standing). There exist constants α, β, L ≥ 0 such that for each i ∈ {1, . . . , n}, the following hold:
(a) The set Xi ⊂ Rdi is closed and convex, and there exist constants r, R > 0 satisfying rB ⊆ X ⊆ RB where X := X1 × · · · × Xn.
(b) The function fi(xi, x−i) is convex and C1-smooth in xi and the gradient ∇ifi(x) is β-Lipschitz continuous in x.
(c) The Jacobian of the map ∇ifi(x) is L-Lipschitz continuous, meaning

∇(∇ifi)(x) − ∇(∇ifi)(x′) op ≤ L x − x′ , ∀ x, x′ ∈ X .

(d) The map deﬁned by the vector of individual partial gradients, g(x) := (∇1f1(x), ∇2f2(x), . . . , ∇nfn(x)),
is α-strongly monotone on X , meaning that the following inequality holds: g(x) − g(x′), x − x′ ≥ α x − x′ 2 ∀x, x′ ∈ X .

(e) For each i ∈ {1, . . . , n} and x ∈ X , the function fi satisﬁes |fi(x)| < ∞ and we set F∗ := maxi maxx∈X |fi(x)|2.
Items (a)–(b) and (d)–(e) are identical to those in Bravo et al. (2018). In contrast, item (c) is not assumed in Bravo et al. (2018), but will be important in what follows. We note that when applied to the single player setting n = 1, none of our results require item (c) and it may be dropped entirely.

2

Algorithm 1: Derivative Free Gradient Play (Bravo et al., 2018): DFO(x0, {ηt}t≥1, δ, T )

Input: Horizon T ∈ N, step-sizes ηt > 0, radius δ ∈ (0, r), initial strategies x0 ∈ (1 − δ)X .

1 for t = 0, . . . , T − 1 do

2 for i = 1, . . . , n do

3

Sample vit ∈ Si uniformly at random;

4

Play xti + δvit;

5

Compute

gˆit

=

di δ

fi

(xti

+

δvit, xt−i

+

δv−t i)vit;

6

Update xti+1 = proj(1−δ)Xi (xi − ηtgˆit)

7 end

8 end

Output: xT = (xT1 , . . . , xTn )

Classical results, such as those in the seminal work by Rosen (1965), guarantee that the game admits a unique Nash equilibrium under Assumption 1. In this work, we study a derivative-free algorithm proposed by Bravo et al. (2018) for ﬁnding the Nash equilibrium of the game. We note that the gradient estimator used by Bravo et al. (2018) is motivated by the analogous construction introduced by Flaxman et al. (2005) for the single player setting. The procedure is recorded as Algorithm 1.
In each iteration t, Algorithm 1 samples vt ∈ S1 × · · · × Sn uniformly at random and then declares

xt+1 = proj (xt − ηtgt)

(1)

(1−δ)X

where

gt := (gˆ1t , . . . , gˆnt ).

The reason for projecting onto the set (1 − δ)X is simply to ensure that in the next iteration t + 1, the action proﬁle is valid in the sense that xti+1 + δvit+1 lies in Xi for each player i ∈ {1, . . . , n}.
Bravo et al. (2018) show that with appropriate parameter choices, Algorithm 1 will ﬁnd a point x sat-

isfying E[

x − x⋆

2] ≤ ε after

O

(

d2 ε3

)

iterations,

and leave it as an

open question if this

result is

tight.

We

provide a

diﬀerent

convergence argument

that

yields

an

improved eﬃciency

estimate

O

(

d2 ε2

).

The

estimate

matches the known rate of convergence of the method for unconstrained optimization problems (i.e., n = 1

and X = Rd) established by Agarwal et al. (2010).1 We note that compared to Bravo et al. (2018) our results

do rely on a slightly stronger assumption on the second-order smoothness of the loss functions, summarized

in item 1 of Assumption 1. This assumption is not needed in the single player setting n = 1.

Theorem 1 (Informal). For suﬃciently small ε > 0, there exists a choice of δ > 0 and ηt > 0 such that Algorithm 1 generates a sequence xt satisfying

E xt − x⋆ 2 ≤ ε

for

t

on

the

order

of

d2 ε2

.

We provide the formal statement of the main result as well as the proof in the next section. A key idea is to interpret Algorithm 1 as stochastic gradient play applied to a slightly perturbed strongly monotone game.

3 Main Result

The starting point is the very motivation for the update (1), which is that the vector gt is an unbiased estimator of the gradient map for a diﬀerent game. Namely, for each player i, deﬁne the smoothed cost

fiδ(xi, x−i) := E fi(xi + δwi, x−i + δw−i),
w∼Ui

1Formally, the paper of Agarwal et al. (2010) shows the bound of O dε22 the minimizer because the analysis is based on bounding the regret.

on the squared distance of the average iterate to

3

where Ui denotes the uniform distribution on

× Bi ×

Sj .

j=i

Thus w is a vector of size d, where recall that d =

n i=1

di.

The

following

is

proved

in

(Bravo

et

al.,

2018,

Lemma C.1) and follows closely the argument in Flaxman et al. (2005).

Lemma 1 (Unbiased gradient estimator). For each index i ∈ {1, . . . , n}, let vi be sampled uniformly from

Si and deﬁne the random vector

gˆi = di fi(xi + δvi, x−i + δv−i)vi. δ

The following equality holds,

E[gˆi] = ∇ifiδ(x).

The path forward is now clear: we interpret Algorithm 1 as stochastic gradient play on the perturbed game deﬁned by the losses fiδ over the smaller set (1 − δ)X . To this end, deﬁne the perturbed gradient map

gδ(x) = (∇1f1δ(x), . . . , ∇nfnδ(x)).

Lemmas 2 and 3 estimate the smoothness and strong monotonicity constants of the perturbed game, thereby allowing us to invoke classical convergence guarantees for stochastic gradient play on the perturbed game.
Lemma 2 (Smoothness of the perturbed game). For each index i ∈ {1, . . . , n}, the loss fiδ(x) is diﬀerentiable and the map x → ∇ifiδ(x) is β–Lipschitz continuous. Moreover the following estimate holds,
g(x) − gδ(x) ≤ βδn ∀x ∈ X .

Proof. For any points x, x′ ∈ X , we successively estimate

∇ifiδ(x) − ∇ifiδ(x′) ≤ E [ ∇ifi(xi + δwi, x−i + δw−i) −∇ifi(x′i + δwi, x′−i + δw−i) ≤ β x − x′ .
wi ∼Ui

Thus ∇ifiδ is β-Lipschitz continuous. Next, we estimate

∇ifi(x) − ∇ifiδ(x)

≤ E [ ∇ifi(x + δw) − ∇ifi(x) ]
w∼Ui

≤β E w√∼Ui
≤ βδ n.

wi 2 + (n − 1)

Therefore we deduce as claimed.

g(x) − gδ(x) =

n
∇ifi(x) − ∇ifiδ(x) 2 ≤ βδn,
i=1

Observe that in the single player case n = 1, the function f δ is trivially α-strongly convex for any δ > 0. For general n > 1, the following lemma shows that the perturbed map gδ is strongly monotone for all
suﬃciently small δ.

Lemma

3

(Strong

monotonicity of the

smoothed

game).

Choose

δ

≤

cα Ln3/2

for

some

constant

c ∈ (0, 1).

Then the gradient map gδ is strongly monotone over X with parameter (1 − c)α.

Proof. Fix an index i and let us ﬁrst estimate the Lipschitz constant of the diﬀerence map

Hi(x) := ∇ifiδ(x) − ∇ifi(x).

4

To this end, we compute

∇Hi(x) = E [∇(∇ifi)(x + δw) − ∇(∇ifi)(x)].
w∼Ui

Taking into account that the map x → ∇(∇ifi)(x) is L-Lipschitz continuous, we deduce

√

∇Hi(x) op ≤ E [ (∇(∇ifi)(x + δw) − ∇(∇ifi)(x)) op] ≤ δL E w ≤ δL n.

w∼Ui

w∼Ui

Thus the map Hi is Lipschitz continuous with parameter δL√n. We therefore compute

gδ(x) − gδ(x′), x − x′

n
= ∇ifiδ(x) − ∇ifiδ(x′), xi − x′i

i=1

n

n

= ∇ifi(x) − ∇ifi(x′), xi − x′i − Hi(x′) − Hi(x), xi − x′i

i=1

i=1

≥ (α − Ln3/2δ) x − x′ 2.

The proof is complete.

The last ingredient, summarized in Lemma 4, is to estimate the distance between the Nash equilibria of the original and the perturbed games. Henceforth, let x⋆δ be the Nash equilibrium of the game with losses fiδ over the set (1 − δ)X .

Lemma 4 (Distance between equilibria). Choose any δ < min

r,

α Ln3/2

. Then the following estimate holds,

x⋆ − x⋆δ ≤ δ

β√n 1+ α

x⋆ + βn . α

Proof. Lemma 3 and our choice of δ ensures that gδ is strongly monotone and therefore that x⋆δ is welldeﬁned. There are two sources of perturbation: one replacing X with (1 − δ)X and the other in replacing
fi with fiδ. We deal with these in turn. To this end, set γ := 1 − δ and let x˜ be the Nash equilibrium of the original game deﬁned by the losses fi over the shrunken set γX . Thus x˜ satisﬁes the inclusion

0 ∈ g(x˜) + NγX (x˜),

where NγX (x˜) denotes the normal cone to γX at x˜. The triangle inequality directly gives

x⋆ − x⋆δ ≤ x⋆ − x˜ + x˜ − x⋆δ .

(2)

Let us bound the ﬁrst term on the right side of (2). To this end, since the map x → g(x) + NγX (x) is α-strongly monotone, we deduce

α x˜ − γx⋆ ≤ dist(0, g(γx⋆) + NγX (γx⋆)).

(3)

Let us estimate the right-hand side of (3). Since x⋆ is a Nash equilibrium of the original game over X , the inclusion
0 ∈ g(x⋆) + NX (x⋆)
holds. Taking into account the identity NγX (γx⋆) = NX (x⋆), we deduce

d(0, g(γx⋆) + NγX (γx⋆)) = d(0, g(γx⋆) + NX (x⋆))

≤ g(γx⋆) − g(x⋆)

√ ≤ δβ n

x⋆

,

where the last inequality follows from g being β√n-Lipschitz continuous. Appealing to (3) and using the triangle inequality, we therefore deduce

√

x⋆ − x˜ ≤ x˜ − γx⋆ + δ x⋆ ≤ δ(1 + β αn ) x⋆ .

(4)

5

It remains to upper bound x˜ − x⋆δ . The deﬁnition of x˜ as a Nash equilibrium ensures

−g(x˜), x − x˜ ≤ 0, ∀ x ∈ γX .

(5)

Analogously, the deﬁnition of x∗δ as a Nash equilibrium ensures

−gδ(x⋆δ ), x − x⋆δ ≤ 0, ∀ x ∈ γX .

(6)

Then by strong monotonicity of the game and estimates (5) and (6), we get

α x˜ − x⋆δ 2 ≤ g(x˜) − g(x⋆δ ), x˜ − x⋆δ ≤ gδ(x⋆δ ) − g(x⋆δ ), x˜ − x⋆δ ≤ gδ(x⋆δ ) − g(x⋆δ ) · x˜ − x⋆δ ≤ βδn x˜ − x⋆δ ,

where the last inequality follows from Lemma 2. Rearranging, we conclude

x˜ − x⋆ ≤ βδn ,

δ

α

which combined with (2) and (4) completes the proof.

With the observations that the game deﬁned by smoothed functions fiδ is strongly monotone and the individual gradients of the smoothed loss functions are Lipschitz, we arrive at the following eﬃciency guar-
antee.

Theorem

2.

Suppose

δ

≤

min{r,

α 2Ln3/2

}

and

set

ηt

=

α2t .

Then

the

following

holds,

E[ xt − x⋆ 2] ≤ max{δ2α2 x1 − x⋆δ 2, 8F∗d2n} + 2δ2 δ2α2t

β√n 1+ α

x⋆

+ βn

2
.

α

Proof. Using the inequality (a + b)2 ≤ 2a2 + 2b2 and Lemma 4, we estimate the one step progress

1 xt+1 − x⋆ 2 ≤ xt+1 − x⋆ 2 + x⋆ − x⋆ 2

2

δ

δ

√

≤ xt+1 − x⋆ 2 + δ2 1 + β n

δ

α

x⋆ + βn3/2 2 . α

Next we continue with the standard estimate using non-expansiveness of the projection:

E[ xt+1 − x⋆δ 2] = E

2
proj (xt − ηtgt(xt)) − x⋆δ
(1−δ)X

≤ E[ xt − x⋆δ − ηtgt(xt) 2]

(7)

≤ E[ xt − x∗δ 2] + ηt2 E[ gt(xt) 2] − 2ηt E[ gt(xt), xt − x⋆δ ]

≤ E[ xt − x∗δ 2] − 2ηt[ gδ(xt), xt − x⋆δ ] + ηt2 E[ gt(xt) 2].

Since

gδ

is

strongly

monotone

with

parameter

α 2

(Lemma

3)

and

x∗δ

is

the

Nash

equilibrium

of

the

smoothed

game, we deduce

gδ(xt), xt − x⋆ ≥ gδ(xt) − gδ(x∗), xt − x⋆ ≥ α xt − x⋆ 2.

δ

δ

δ2

δ

Returning to (7), we conclude

E[ xt+1 − x⋆δ A standard inductive argument shows

2] ≤ (1 − αηt) E

xt − x∗δ

2 + η2 F∗d2n . t δ2

E[ xt − x⋆ 2] ≤ max{δ2α2 x1 − x⋆δ 2, 8F∗d2n}

δ

δ2α2t

for all t ≥ 1. The proof is complete.

6

Main Result. The following is now the formal statement and proof of Theorem 1.

Corollary 1 (Main Result). Fix a target accuracy

ε

<

((α

+

√ β n)R

+

βn)2

·

min{

1

, 4r2 },

L2n3 α2

and set

δ=

α√ ε/4

(α + β n)R + βn

and ηt = α2t . Then, the estimate E[ xt − x⋆ 2] ≤ ε holds for all

max{32α4εR2, 64((α + β√n)R + βn)2F∗d2n}

t≥

α4ε2

.

In the single player setting n = 1, the conclusion holds for any ε < 4r2((1 + αβ ))R + αβ )2 and Assumption 1 (item c) may be dropped.

Proof.

The

assumed

upper

bound

on

ε

directly

implies

δ≤

α 2Ln3/2

and

δ < r.

An

application

of

Theorem 2

yields the estimate

E[ xt − x⋆ 2] ≤ max{δ2α2 x1 − x∗δ 2, 8F∗d2n} + ε .

δ2α2t

2

Setting the right side to ε, solving for t, and using the trivial upper bound x1 − x∗δ ≤ 2R completes the proof. The claims for the single player setting n = 1 follows by noting that the conclusions of Lemmas 3–4 and Theorem 2 hold for any δ ∈ (0, r), since f δ is strongly convex for any δ > 0.

Observe that the sequence of actions that each player i is actually playing is xti + δvit. The eﬃciency guarantee in the preceding corollary remains intact for this sequence with a slightly diﬀerent choice of
parameters. Indeed, the following estimate holds:

1 xt+1 + δvt+1 − x⋆ 2 ≤ xt+1 − x⋆ + x⋆ − x⋆ 2 + δ2 vt+1 2

2

δ

δ

β√n

βn3/2 √ 2

≤ xt+1 − x⋆δ 2 + δ2 1 + α

x⋆ + α + n .

√
α ε/4

√

√

2

Hence, if δ = ((α+β√n)R+βn+α√n) and ε < ((α + β

n)R + βn + α

n)2

·

min

{

L

1 2 n3

,

4r α2

},

then

an

analogous

argument to what is presented in the proof of Corollary 1 holds. In fact, one can choose parameters such

that both guarantees holds as is summarized in the following corollary.

Corollary 2. Fix a target accuracy ε < ((α+β√n)R+βn)2·min

1 L2 n3

,

4r2 α2

√ , query radius δ = (α+βα√nε)R/4+βn) ,

and step-size ηt = α2t . Then, the estimates E[ xt − x⋆ 2] ≤ ε and E[ xt + δvt − x⋆ 2] ≤ ε hold for all

max{32α4εR2, 64 · ((α + β√n)R + βn + √nα)2 · F∗d2n}

t≥

α4ε2

.

In practice it may be advantageous to not specify ε at the onset and instead allow the algorithm to run indeﬁnitely. This can be easily achieved without sacriﬁcing eﬃciency simply by restarting the algorithm periodically while shrinking δ by a constant fraction. The resulting process and its convergence rate is summarized in the following corollary.

Corollary 3 (Eﬃciency without target accuracy). Deﬁne the following constants:

8F∗d2n A = α2 , B = 2

β√n

βn 2

1+ α R+ α ,

α

A 4R2

δ1 = min r, 2Ln3/2 , T1 = max Bδ4 , Bδ2 .

1

1

7

Fix

a

fraction

q

∈ (0, 1),

and

set

y0

= x0

and

ηt

=

2 αt

for

each

index

t ≥ 1.

Consider

the

following

process:

 

yk

=

DFO(yk−1,

{ηt}t≥1,

δk ,

Tk)

 

 δk+1 = q · δk





A 4R2  .

(8)

Tk+1 = max Bδ4 , Bδ2



k+1

k+1

For every k ≥ 1, the iterate yk satisﬁes E[ yk − x⋆ 2] ≤ εk, where εk := 2Bδ12q2(k−1), while the total number of steps of Algorithm 1 needed to generate yk is at most

1 + 1 log 2

2Bδ12 εk

+ 4Bq−A4q−−41δ12 · ε−k 2 + 8Rq−2q2−−2δ11−2 ε−k 1.

In the single player setting (i.e., n = 1), we may instead set δ1 = r.

Proof. Theorem 2 directly guarantees

E[ yk − x⋆ 2] ≤ max{A, 4R2δk2} + Bδ2 ≤ 2Bδ2.

δk2 Tk

k

k

The total number of steps of Algorithm 1 needed to generate yk is bounded as follows:

k

k

Ti ≤

i=1

i=1

A δ−4 · q−4(i−1) + 4R2 δ−2 · q−2(i−1)

B1

B1

≤ k + Aδ1−4 k−1 q−4j + k + 4R2δ1−2 k−1 q−2j

B j=0

B j=0

≤ k + Aδ1−4 · q−4k + 4R2δ1−2 q−2k.

B(q−4 − 1)

B(q−2 − 1)

Rewriting the right-side in terms of εk completes the proof.

4 Discussion and Future Directions
A promising future research direction is to examine the beneﬁts of using a two-point (and multi-point) estimate for the gradient. In a two-point method, once a random perturbation direction is chosen, two function evaluations are performed along that direction, for example f (x + δu) − f (x − δu), as examined by Agarwal et al. Agarwal et al. (2010) and by Nesterov and Spokoiny Nesterov and Spokoiny (2017). The use of this symmetric two-point estimate can improve the convergence rate via improving the constants— however, the dependence of the rate on problem dimension d still is not optimal. We remark that this symmetric expression yields an unbiased estimate of the gradient of f , and its extension to the game setting is straightforward; i.e., the proof approach in the current paper can be applied.
One point to note when considering the extension of two-point methods to the multiplayer (game) setting is to ensure each player can indeed evaluate their cost function fi(xi, x−i) at two points—player i can certainly vary its own action xi, but can it do so while keeping the other players’ actions x−i ﬁxed? This type of method requires more explicit coordination between players; however, there are practical settings in machine learning where such coordination is possible. One example is multiplayer performative prediction which is introduced in the recent work Narang et al. (2022), wherein players have a loss function oracle and observe their competitors’ actions. This would enable players to form estimates using query responses of the form fi(xi + δui, x−i). For instance, the two point method of Agarwal et al. (2010) takes the form
gˆi = di (fi(xi + δui, x−i) − fi(xi − δui, x−i)) . 2δ
8

√The analysis in Duchi et al. (2015) improves the rate in Agarwal et al. (2010) signiﬁcantly by a factor of d, by employing a one-sided two-point estimate of the gradient. This introduces additional bias in the gradient estimate that they cleverly handle. It is an interesting direction to adapt this algorithm to the game setting. However, due to the asymmetry and hence bias in the estimate of Duchi et al. (2015), a new proof approach is needed. We leave this topic to future work.
References
M. Bravo, D. S. Leslie, and P. Mertikopoulos, “Bandit learning in concave n-person games,” Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2018.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models resistant to adversarial attacks,” Proceedings of the International Conference Learning Representation (ICLR), 2018.
T. Fiez, B. Chasnov, and L. Ratliﬀ, “Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study,” in International Conference on Machine Learning. PMLR, 2020, pp. 3133–3144.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 2014.
A. Narang, E. Faulkner, D. Drusvyatskiy, M. Fazel, and L. J. Ratliﬀ, “Learning in strongly monotone decision-dependent games,” Proceedings of the Artiﬁcial Intelligence and Statistics Conference (AIStats), 2022.
L. J. Ratliﬀ, S. A. Burden, and S. S. Sastry, “On the characterization of local nash equilibria in continuous games,” IEEE transactions on automatic control, vol. 61, no. 8, pp. 2301–2307, 2016.
Z. Zhou, P. Mertikopoulos, A. L. Moustakas, N. Bambos, and P. Glynn, “Mirror descent learning in continuous games,” in Proceedings of the 56th IEEE Annual Conference on Decision and Control (CDC), 2017, pp. 5776–5783.
L. J. Ratliﬀ and T. Fiez, “Adaptive incentive design,” IEEE Transactions on Automatic Control, vol. 66, no. 8, pp. 3871–3878, 2020.
N. Li and J. R. Marden, “Designing games for distributed optimization,” in Proceedings of the 50th IEEE Conference on Decision and Control, 2011.
A. Yekkehkhany, H. Feng, and J. Lavaei, “Adversarial attacks on computation of the modiﬁed policy iteration method,” in Proceedings of the IEEE Conference on Decision and Control, 2021.
K. Zhang, Z. Yang, and T. Ba¸sar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” Springer Studies in Systems, Decision and Control, Handbook on RL and Control, 2020.
K. Zhang, Z. Yang, and T. Basar, “Policy optimization provably converges to nash equilibria in zero-sum linear quadratic games,” Advances in Neural Information Processing Systems, vol. 32, 2019.
Y. Savas, V. Gupta, M. Ornik, L. J. Ratliﬀ, and U. Topcu, “Incentive design for temporal logic objectives,” in Proceedings of the 58th IEEE Conference on Decision and Control, 2019, pp. 2251–2258.
D. Fudenberg, F. Drew, D. K. Levine, and D. K. Levine, The theory of learning in games. MIT press, 1998, vol. 2.
N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games. Cambridge university press, 2006.
A. Agarwal, O. Dekel, and L. Xiao, “Optimal algorithms for online convex optimization with multi-point bandit feedback.” in Proceedings of the Conference on Learning Theory (COLT), 2010, pp. 28–40.
O. Shamir, “An optimal algorithm for bandit and zero-order convex optimization with two-point feedback,” The Journal of Machine Learning Research, vol. 18, no. 1, pp. 1703–1713, 2017.
9

A. D. Flaxman, A. T. Kalai, and H. B. McMahan, “Online convex optimization in the bandit setting: gradient descent without a gradient,” Proceedings of the sixteenth annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.
Y. Nesterov and V. Spokoiny, “Random gradient-free minimization of convex functions,” Foundations of Computational Mathematics, vol. 17, no. 2, pp. 527–566, 2017.
T. Tatarenko and M. Kamgarpour, “Bandit online learning of nash equilibria in monotone games,” arXiv preprint arXiv:2009.04258, 2020.
——, “Learning Nash equilibria in monotone games,” in Proceedings of the 58th IEEE Conference on Decision and Control (CDC), 2019, pp. 3104–3109.
J. B. Rosen, “Existence and uniqueness of equilibrium points for concave n-person games,” Econometrica: Journal of the Econometric Society, pp. 520–534, 1965.
J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono, “Optimal rates for zero-order convex optimization: The power of two function evaluations,” IEEE Transactions on Information Theory, vol. 61, no. 5, pp. 2788–2806, 2015.
10

