Contextual Encoding for Translation Quality Estimation
Junjie Hu, Wei-Cheng Chang, Yuexin Wu, Graham Neubig Language Technologies Institute, Carnegie Mellon University {junjieh, wchang2, yuexinw, gneubig}@cs.cmu.edu

arXiv:1809.00129v1 [cs.CL] 1 Sep 2018

Abstract
The task of word-level quality estimation (QE) consists of taking a source sentence and machine-generated translation, and predicting which words in the output are correct and which are wrong. In this paper, propose a method to effectively encode the local and global contextual information for each target word using a three-part neural network approach. The ﬁrst part uses an embedding layer to represent words and their part-of-speech tags in both languages. The second part leverages a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking ﬁrst in three of the six tracks.1
1 Introduction
Quality estimation (QE) refers to the task of measuring the quality of machine translation (MT) system outputs without reference to the gold translations (Blatz et al., 2004; Specia et al., 2013). QE research has grown increasingly popular due to the improved quality of MT systems, and potential for reductions in post-editing time and the corresponding savings in labor costs (Specia, 2011; Turchi et al., 2014). QE can be performed on multiple granularities, including at word level, sentence level, or document level. In this paper, we focus on quality estimation at word level, which is framed as the task of performing binary classiﬁcation of translated tokens, assigning “OK” or “BAD” labels.
1Our software is available at https://github.com/ junjiehu/CEQE.

Early work on this problem mainly focused on hand-crafted features with simple regression/classiﬁcation models (Uefﬁng and Ney, 2007; Bic¸ici, 2013). Recent papers have demonstrated that utilizing recurrent neural networks (RNN) can result in large gains in QE performance (Martins et al., 2017). However, these approaches encode the context of the target word by merely concatenating its left and right context words, giving them limited ability to control the interaction between the local context and the target word.
In this paper, we propose a neural architecture, Context Encoding Quality Estimation (CEQE), for better encoding of context in word-level QE. Speciﬁcally, we leverage the power of both (1) convolution modules that automatically learn local patterns of surrounding words, and (2) handcrafted features that allow the model to make more robust predictions in the face of a paucity of labeled data. Moreover, we further utilize stacked recurrent neural networks to capture the long-term dependencies and global context information from the whole sentence.
We tested our model on the ofﬁcial benchmark of the WMT18 word-level QE task. On this task, it achieved highly competitive results, with the best performance over other competitors on English-Czech, English-Latvian (NMT) and English-Latvian (SMT) word-level QE task, and ranking second place on English-German (NMT) and German-English word-level QE task.
2 Model
The QE module receives as input a tuple s, t, A , where s = s1, . . . , sM is the source sentence, t = t1, . . . , tN is the translated sentence, and A ⊆ {(m, n)|1 ≤ m ≤ M, 1 ≤ n ≤ N } is a set of word alignments. It predicts as output a sequence yˆ = y1, . . . , yN , with each yi ∈ {BAD, OK}. The

overall architecture is shown in Figure 1 CEQE consists of three major components: (1)
embedding layers for words and part-of-speech (POS) tags in both languages, (2) convolution encoding of the local context for each target word, and (3) encoding the global context by the recurrent neural network.

2.1 Embedding Layer
Inspired by (Martins et al., 2017), the ﬁrst embedding layer is a vector representing each target word tj obtained by concatenating the embedding of that word with those of the aligned words sA(:,tj) in the source. If a target word is aligned to multiple source words, we average the embedding of all the source words, and concatenate the target word embedding with its average source embedding. The immediate left and right contexts for source and target words are also concatenated, enriching the local context information of the embedding of target word tj. Thus, the embedding of target word tj, denoted as xj, is a 6d dimensional vector, where d is the dimension of the word embeddings. The source and target words use the same embedding parameters, and thus identical words in both languages, such as digits and proper nouns, have the same embedding vectors. This allows the model to easily identify identical words in both languages. Similarly, the POS tags in both languages share the same embedding parameters. Table 1 shows the statistics of the set of POS tags over all language pairs.

Language Pairs En-De (SMT) En-De (NMT) De-En En-Lv (SMT) En-Lv (NMT) En-Cz

Source 50 49 58 140 167 440

Target 57 58 50 38 43 57

Table 1: Statistics of POS tags over all language pairs

2.2 One-dimensional Convolution Layer
The main difference between the our work and the neural model of Martins et al. (2017) is the onedimensional convolution layer. Convolutions provide a powerful way to extract local context features, analogous to implicitly learning n-gram features. We now describe this integral part of our model.

After embedding each word in the target sentence {t1, . . . , tj, . . . , tN }, we obtain a matrix of embeddings for the target sequence,
x1:N = x1 ⊕ x2 . . . ⊕ xN ,
where ⊕ is the column-wise concatenation operator. We then apply one-dimensional convolution (Kim, 2014; Liu et al., 2017) on x1:N along the target sequence to extract the local context of each target word. Speciﬁcally, a one-dimensional convolution involves a ﬁlter w ∈ Rhk, which is applied to a window of h words in target sequence to produce new features.
ci = f (w · xi:i+h−1 + b),
where b ∈ R is a bias term and f is some functions. This ﬁlter is applied to each possible window of words in the embedding of target sentence {x1:h, x2:h+1, . . . , xN−h+1:N } to produce features
c = [c1, c2, . . . , cN−h+1].
By the padding proportionally to the ﬁlter size h at the beginning and the end of target sentence, we can obtain new features cpad ∈ RN of target sequence with output size equals to input sentence length N . To capture various granularities of local context, we consider ﬁlters with multiple window sizes H = {1, 3, 5, 7}, and multiple ﬁlters nf = 64 are learned for each window size.
The output of the one-dimensional convolution layer, C ∈ RN×|H|·nf , is then concatenated with the embedding of POS tags of the target words, as well as its aligned source words, to provide a more direct signal to the following recurrent layers.
2.3 RNN-based Encoding
After we obtain the representation of the sourcetarget word pair by the convolution layer, we follow a similar architecture as (Martins et al., 2017) to reﬁne the representation of the word pairs using feed-forward and recurrent networks.
1. Two feed-forward layers of size 400 with rectiﬁed linear units (ReLU; Nair and Hinton (2010));
2. One bi-directional gated recurrent unit (BiGRU; Cho et al. (2014)) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization (Ba et al., 2016).

Figure 1: The architecture of our model, with the convolutional encoder on the left, and stacked RNN on the right.

Category Binary Binary Binary Binary Float Float Float One-hot
One-hot
One-hot
One-hot

Description target word is a stopword target word is a punctuation mark target word is a proper noun target word is a digit backoff behavior of ngram wi−2 wi−1 wi (wi is the target word) backoff behavior of ngram wi−1 wi wi+1 backoff behavior of ngram wi wi+1 wi+2 highest order of ngram that includes target word and its left context highest order of ngram that includes target word and its right context highest order of ngram that includes source word and its left context highest order of ngram that includes source word and its right context

Table 2: Baseline Features

3. Two feed-forward layers of hidden size 200 with rectiﬁed linear units;
4. One BiGRU layer with hidden size 100 using the same conﬁguration of the previous BiGRU layer;
5. Two feed-forward layers of size 100 and 50 respectively with ReLU activation.
We concatenate the 31 baseline features extracted by the Marmot2 toolkit with the last 50 feedforward hidden features. The baseline features are listed in Table 2. We then apply a softmax layer on the combined features to predict the binary labels.

2https://github.com/qe-team/marmot

3 Training
We minimize the binary cross-entropy loss between the predicted outputs and the targets. We train our neural model with mini-batch size 8 using Adam (Kingma and Ba, 2015) with learning rate 0.001 and decay the learning rate by multiplying 0.75 if the F1-Multi score on the validation set decreases during the validation. Gradient norms are clipped within 5 to prevent gradient explosion for feed-forward networks or recurrent neural networks. Since the training corpus is rather small, we use dropout (Srivastava et al., 2014) with probability 0.3 to prevent overﬁtting.
4 Experiment
We evaluate our CEQE model on the WMT2018 Quality Estimation Shared Task3 for wordlevel English-German, German-English, EnglishCzech, and English-Latvian QE. Words in all languages are lowercased. The evaluation metric is the multiplication of F1-scores for the “OK” and “BAD” classes against the true labels. F1-score is the harmonic mean of precision and recall. In Table 3, our model achieves the best performance on three out of six test sets in the WMT 2018 wordlevel QE shared task.
4.1 Ablation Analysis
In Table 4, we show the ablation study of the features used in our model on English-German, German-English, and English-Czech. For each
3http://statmt.org/wmt18/quality-estimation-task.html

Language Pairs En-De (SMT) En-De (NMT) De-En En-Lv (SMT) En-Lv (NMT) En-Cz

F1-BAD 0.5075 0.3565 0.4906 0.4211 0.5192 0.5882

F1-OK 0.8394 0.8827 0.8640 0.8592 0.8268 0.8061

F1-Multi 0.4260 0.3147 0.4239 0.3618 0.4293 0.4741

Rank 3 2 2 1 1 1

Table 3: Best performance of our model on six datasets in the WMT2018 word-level QE shared task on the leader board (updated on July 27th 2018)

language pair, we show the performance of CEQE without adding the corresponding components speciﬁed in the second column respectively. The last row shows the performance of the complete CEQE with all the components. As the baseline features released in the WMT2018 QE Shared Task for English-Latvian are incomplete, we train our CEQE model without using such features. We can glean several observations from this data:
1. Because the number of “OK” tags is much larger than the number of “BAD” tags, the model is easily biased towards predicting the “OK” tag for each target word. The F1-OK scores are higher than the F1-BAD scores across all the language pairs.
2. For German-English, English Czech, and English-German (SMT), adding the baseline features can signiﬁcantly improve the F1BAD scores.
3. For English-Czech, English-German (SMT), and English-German (NMT), removing POS tags makes the model more biased towards predicting “OK” tags, which leads to higher F1-OK scores and lower F1-BAD scores.
4. Adding the convolution layer helps to boost the performance of F1-Multi, especially on English-Czech and English-Germen (SMT) tasks. Comparing the F1-OK scores of the model with and without the convolution layer, we ﬁnd that adding the convolution layer help to boost the F1-OK scores when translating from English to other languages, i.e., English-Czech, English-German (SMT and NMT). We conjecture that the convolution layer can capture the local information more effectively from the aligned source words in English.

F1-Multi

0.7 Train-0.7 Train-0.3 Train-0.1
0.6 Valid-0.7 Valid-0.3
0.5 Valid-0.1

0.4

0.3

0

1000 2000 3000 4000

Iteration

Figure 2: Effect of the dropout rate during training.

5 Case Study
Table 5 shows two examples of quality prediction on the validation data of WMT2018 QE task for English-Czech. In the ﬁrst example, the model without POS tags and baseline features is biased towards predicting “OK” tags, while the model with full features can detect the reordering error. In the second example, the target word “panelu” is a variant of the reference word “panel”. The target word “znaky” is the plural noun of the reference “znak”. Thus, their POS tags have some subtle differences. Note the target word “zmnit” and its aligned source word “change” are both verbs. We can observe that POS tags can help the model capture such syntactic variants.
5.1 Sensitivity Analysis
During training, we ﬁnd that the model can easily overﬁt the training data, which yields poor performance on the test and validation sets. To make the model more stable on the unseen data, we apply dropout to the word embeddings, POS embeddings, vectors after the convolutional layers and the stacked recurrent layers. In Figure 2, we examine the accuracies dropout rates in [0.1, 0.3, 0.7]. We ﬁnd that adding dropout alleviates overﬁtting issues on the training set. If we reduce the dropout rate to 0.1, which means randomly setting some values to zero with probability 0.1, the training F1Multi increases rapidly and the validation F1-multi score is the lowest among all the settings. Preliminary results proved best for a dropout rate of 0.3, so we use this in all the experiments.

Language Pairs De-En En-Cz En-De (SMT) En-De (NMT)

Method - (Convolution + POS + features) - (POS + features) - features - POS CEQE - (Convolution + POS + features) - (POS + features) - features - POS CEQE - (Convolution + POS + features) - (POS + features) - features - POS CEQE - (Convolution + POS + features) - (POS + features) - features - POS CEQE

F1-BAD 0.4774 0.4948 0.5095 0.4906 0.5233 0.5748 0.5628 0.5777 0.5192 0.5884 0.4677 0.4768 0.4902 0.5047 0.5075 0.3545 0.3404 0.3565 0.3476 0.3481

F1-OK 0.8680 0.8474 0.8735 0.8640 0.8721 0.7622 0.8000 0.7997 0.8268 0.7991 0.8038 0.8166 0.8230 0.8431 0.8394 0.8396 0.8752 0.8827 0.8948 0.8835

F1-Multi 0.4144 0.4193 0.4450 0.4239 0.4564 0.4381 0.4502 0.4620 0.4293 0.4702 0.3759 0.3894 0.4034 0.4255 0.4260 0.2976 0.2979 0.3147 0.3111 0.3075

Table 4: Ablation study on the WMT18 Test Set

6 Conclusion
In this paper, we propose a deep neural architecture for word-level QE. Our framework leverages a one-dimensional convolution on the concatenated word embeddings of target and its aligned source words to extract salient local feature maps. In additions, bidirectional RNNs are applied to capture temporal dependencies for better sequence prediction. We conduct thorough experiments on four language pairs in the WMT2018 shared task. The proposed framework achieves highly competitive results, outperforms all other participants on English-Czech and English-Latvian word-level, and is second place on English-German, and German-English language pairs.
Acknowledgements
The authors thank Andre Martins for his advice regarding the word-level QE task.
This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Ofﬁce (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial poli-

cies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
References
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR, abs/1607.06450.
Ergun Bic¸ici. 2013. Referential translation machines for quality estimation. In Proceedings of the eighth workshop on statistical machine translation, pages 343–351.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Uefﬁng. 2004. Conﬁdence estimation for machine translation. In Proceedings of the 20th international conference on Computational Linguistics, page 315. Association for Computational Linguistics.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724– 1734, Doha, Qatar. Association for Computational Linguistics.

Source MT Reference no POS & features CEQE Source MT Reference no POS & features CEQE

specify the scope of blending options : urcˇete rozsah prolnut´ı voleb : urcˇete rozsah voleb prolnut´ı : urcˇete rozsah prolnut´ı voleb : urcˇete rozsah prolnut´ı voleb : use the Character panel and Paragraphs panel to change the appearance of text . pomoc´ı panelu znaky a odstavce , chcete - li zmeˇnit vzhled textu . pouzˇijte panel znak a panel odstavce , chcete - li zmeˇnit vzhled textu . pomoc´ı panelu znaky a odstavce , chcete - li zmnit vzhled textu . pomoc´ı panelu znaky a odstavce , chcete - li zmeˇnit vzhled textu .

Table 5: Examples on WMT2018 validation data. The source and translated sentences, the reference sentences, the predictions of the CEQE without and with POS tags and baseline features are shown. Words predicted as OK are shown in green, those predicted as BAD are shown in red, the difference between the translated and reference sentences are shown in blue.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1746–1751.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.
Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. 2017. Deep learning for extreme multi-label text classiﬁcation. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 115–124. ACM.
Andre´ Martins, Marcin Junczys-Dowmunt, Fabio Kepler, Ramo´n Astudillo, Chris Hokamp, and Roman Grundkiewicz. 2017. Pushing the limits of translation quality estimation. Transactions of the Association for Computational Linguistics, 5:205–218.
Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814.
Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings of the 15th Conference of the European Association for Machine Translation, pages 73–80.
Lucia Specia, Kashif Shah, Jose GC Souza, and Trevor Cohn. 2013. Quest-a translation quality estimation framework. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 79–84.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958.

Marco Turchi, Antonios Anastasopoulos, Jose´ GC de Souza, and Matteo Negri. 2014. Adaptive quality estimation for machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 710–720.
Nicola Uefﬁng and Hermann Ney. 2007. Wordlevel conﬁdence estimation for machine translation. Computational Linguistics, 33(1):9–40.

