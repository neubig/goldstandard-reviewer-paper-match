arXiv:2001.02360v3 [cs.SD] 27 Apr 2021

Automatic Melody Harmonization with Triad Chords: A Comparative Study
Yin-Cheng Yeh, Wen-Yi Hsiao, Satoru Fukayama, Tetsuro Kitahara, Benjamin Genchel, Hao-Min Liu, Hao-Wen Dong, Yian Chen, Terence Leong, and Yi-Hsuan Yang
ARTICLE HISTORY Compiled April 28, 2021
ABSTRACT Several prior works have proposed various methods for the task of automatic melody harmonization, in which a model aims to generate a sequence of chords to serve as the harmonic accompaniment of a given multiple-bar melody sequence. In this paper, we present a comparative study evaluating and comparing the performance of a set of canonical approaches to this task, including a template matching based model, a hidden Markov based model, a genetic algorithm based model, and two deep learning based models. The evaluation is conducted on a dataset of 9,226 melody/chord pairs we newly collect for this study, considering up to 48 triad chords, using a standardized training/test split. We report the result of an objective evaluation using six diﬀerent metrics and a subjective study with 202 participants.
KEYWORDS Symbolic music generation; automatic melody harmonization; functional harmony
1. Introduction
Automatic melody harmonization, a sub-task of automatic music generation (Fern´andez & Vico, 2013), refers to the task of creating computational models that can generate a harmonic accompaniment for a given melody (Chuan & Chew, 2007; Simon, Morris, & Basu, 2008). Here, the term harmony, or harmonization, is used to refer to chordal accompaniment, where an accompaniment is deﬁned relative to the melody as the supporting section of the music. Figure 1 illustrates the inputs and outputs for a melody harmonization model.
Melody harmonization is a challenging task as there are multiple ways to harmonize the same melody; what makes a particular harmonization pleasant is subjective, and often dependent on musical genre and other contextual factors. Tonal music, which encompasses most of Western music, deﬁnes speciﬁc motivic relations between chords based on scales such as those deﬁned in functional harmony (Riemann, 1893). While these relations still stand and are taught today, their application towards creating
Yeh, Hsiao, Liu, Dong and Yang are with Academia Sinica, Taiwan ({ycyeh, wayne391, paul115236, salu133445, yang}@citi.sinica.edu.tw); Fukayama is with National Institute of Advanced Industrial Science and Technology, Japan (satoru s.fukayama@aist.go.jp); Kitahara is with Nihon University, Japan (kitahara@chs.nihon-u.ac.jp); Genchel is with Georgia Institute of Technology, USA (benjiegenchel@gmail.com); Chen and Leong are with KKBOX Inc., Taiwan (annchen@kkbox.com, terenceleong@kkboxgroup.com)

Figure 1. Diagram of the slightly modiﬁed version of the bidirectional long short-term memory network (BiLSTM) based model (Lim et al., 2017) for melody harmonization. The input to the model is a melody sequence. With two layers of BiLSTM and one fully-connected (FC) layer, the model generates as output a sequence of chord labels (e.g., Cm or B chords), one for each half bar. See Section 2.4 for details.
pleasant music often depends on subtleties, long term dependencies and cultural contexts which may be readily accessible to a human composer, but very diﬃcult to learn and detect for a machine. While a particular harmonization may be deemed technically correct in some cases, it can also be seen as uninteresting in a modern context.
There have been several eﬀorts made towards this task in the past (Makris, Kayrdis, & Sioutas, 2016). Before the rise of deep learning, the most actively employed approach is based on hidden Markov models (HMMs). For example, Paiement, Eck, and Bengio (2006) proposed a tree-structured HMM that allows for learning the non-local dependencies of chords, and encoded probabilities for chord substitution taken from psycho-acoustics. They additionally presented a novel representation for chords that encodes relative scale degrees rather than absolute note values, and included a subgraph in their model speciﬁcally for processing it. Tsushima, Nakamura, Itoyama, and Yoshii (2017) similarly presented a hierarchical tree-structured model combining probabilistic context-free grammars (PCFG) for chord symbols and HMMs for chord rhythms. Temperley (2009) presented a statistical model that would generate and analyze music along three sub-structures: metrical structure, harmonic structure, and stream structure. In the generative portion of this model, a metrical structure deﬁning the emphasis of beats and sub-beats is ﬁrst generated, and then harmonic structure and progression are generated conditioned on that metrical structure.
There are several previous works which attempt to formally and probabilistically analyze tonal harmony and harmonic structure. For example, Rohrmeier and Cross (2008) applied a number of statistical techniques to harmony in Bach chorales in order to uncover a proposed underlying harmonic syntax that naturally produces common perceptual and music theoretic patterns including functional harmony. Jacoby, Tishby, and Tymoczko (2015) attempted to categorize common harmonic symbols (scale degrees, roman numerals, or sets of simultaneous notes) into higher level functional
2

groups, seeking underlying patterns that produce and generalize functional harmony. Tsushima, Nakamura, Itoyama, and Yoshii (2018) used unsupervised learning in training generative HMM and PCFG models for harmonization, showing that the patterns learned by these models match the categorizations presented by functional harmony.
More lately, people have begun to explore the use of deep learning for a variety of music generation tasks (Briot, Hadjeres, & Pachet, 2017). For melody harmonication, Lim et al. (2017) proposed a model that employed two bidirectional long short-term memory (BiLSTM) recurrent layers (Hochreiter & Schmidhuber, 1997) and one fullyconnected layer to learn the correspondence between pairs of melody and chord sequences. The model architecture is depicted in Figure 1. According to the experiments reported in (Lim et al., 2017), this model outperforms a simple HMM model and a more complicated DNN-HMM model (Hinton et al., 2012) for melody harmonization with major and minor triad chords.
Moreover, melody harmonization is also relevant to four-part chorale harmonization (Allan & Williams, 2005; Ebcioˇglu, 1988; Hadjeres & Pachet, 2017; C.Z. A. Huang, Cooijmans, Roberts, Courville, & Eck, 2017) and accompaniment generation (Chuan & Chew, 2007; Dong, Hsiao, Yang, & Yang, 2018; Simon et al., 2008). Although these three tasks share the same input, monophonic melodies, they have distinct outputs—lead sheets, four-part chorales and full arrangements, respectively. Another relevant topic is harmonic analysis (T.-P. Chen & Su, 2018; De Haas, Magalh˜aes, Wiering, & Veltkamp, 2014; Harte, Sandler, & Gasser, 2006; Raphael & Stoddard, 2004; Temperley, 2009). Despite having similar input and output spaces, melody harmonization is fundamentally diﬀerent from harmonic analysis. On one hand, harmonic analysis takes polyphonic music as inputs, while melody harmonization takes monophonic melodies as inputs and is considered more diﬃcult as less harmonic information is given. On the other hand, there is in general a correct answer for harmonic analysis, while there are no strict answers for melody harmonization.
We note that, while many new models are being proposed for melody harmonization, at present there is no comparative study evaluating a wide array of diﬀerent approaches for this task, using the same training set and test set. Comparing models trained on diﬀerent training sets is problematic as it is hard to have a standardized deﬁnition of improvement and quality. Moreover, as there is to date no standardized test set for this task, it is hard to make consistent comparison between diﬀerent models.
In this paper, we aim to bridge this gap with the following three contributions:
(1) We implement in total ﬁve melody harmonization models that span a number of canonical approaches to the task, including template matching, hidden Markov model (HMM) (Simon et al., 2008), genetic algorithm (GA) (Kitahara, Giraldo, & Ramirez, 2018), and two variants of deep recurrent neural network (RNN) models (Lim et al., 2017). We then present a comparative study comparing the performance of these models. To our best knowledge, a comparative study that considers such a diverse set of approaches for melody harmonization using a standardized dataset has not been attempted before. As we follow fairly faithfully the implementation proposed in the original publications, these models diﬀer in terms of not only the model architectures but also the employed features. Therefore, we have to admit that our study cannot decouple the eﬀects of the model architectures and the features. Yet, we note that the comparison of the ﬁrst four models is an architecture-vs-architecture comparison, while the comparison of the two RNN models is a feature-vs-feature comparison.
3

(2) We compile a new dataset, called the Hooktheory Pianoroll Triad Dataset (HTPD3), to evaluate the implemented models over well-annotated lead sheet samples of music. A lead sheet is a form of musical notation that speciﬁes the essential elements of a song—the melody, harmony, and where present, lyrics (Liu & Yang, 2018). HTPD3 provides melody lines and accompanying chords specifying both chord symbol and harmonic function useful for our study. We consider 48 triad chords in this study, including major, minor, diminished, and augmented triad chords. We use the same training split of HTPD3 to train the implemented models and evaluate them on the same test split.
(3) We employ six objective metrics for evaluating the performance of melody harmonization models. These metrics consider either the distribution of chord labels in a chord sequence, or how the generated chord sequence ﬁts with the given melody. In addition, we conduct an online user study and collect the feedback from 202 participants around the world to assess the quality of the generated chordal accompaniment.
We discuss the ﬁndings of comparative study, hoping to gain insights into the strength and weakness of the evaluated methods. Moreover, we show that incorporating the idea of functional harmony (T.-P. Chen & Su, 2018) while harmonizing melodies greatly improves the result of the model presented by (Lim et al., 2017).
In what follows, we present in Section 2 the models we consider and evaluate in this comparative study. Section 3 provides the details of the HTPD3 dataset we build for this study, and Section 4 the objective metrics we consider. Section 5 presents the setup and result of the study. We discuss the ﬁndings and limtiations of this study in Section 6, and then conclude the paper in Section 7.
2. Automatic Melody Harmonization Models
A melody harmonization model takes a melody sequence of T bars as input and generates a corresponding chord sequence as output. Chord Sequence is deﬁned here as a series of chord labels Y = y1, y2, . . . , yM , where M denotes the length of the sequence. In this work, each model predicts a chord label for every half bar, i.e. M = 2T . Each label yj is chosen from a ﬁnite chord vocabulary C. To reduce the complexity of this task, we consider here only the triad chords, i.e., chords composed of three notes. Speciﬁcally, we consider major, minor, diminished, and augmented triad chords, all in root position. We also consider No Chord (N.C.), or rest, so the size of the chord vocabulary is |C| = 49. Melody Sequence is a time-series of monophonic musical notes in MIDI format. We compute a sequence of features as X = x1, x2, . . . , xN to represent the melody and use them as the inputs to our models. Unless otherwise speciﬁed, we set N = M , computing a feature vector for each half bar.
Given a set of melody and corresponding chord sequences, a melody harmonization model f (·) can be trained by minimizing the loss computed between the ground truth Y∗ and the model output Yˆ∗ = f (X∗), where X∗ is the input melody.
We consider three non-deep learning based and two deep learning based models in this study. While the majority are adaptation of existing methods, one (deep learning based) is a novel method which we introduce in this paper (see Section 2.5). All models are carefully implemented and trained using the training split of HTPD3. We present the technical details of these models below.
4

2.1. Template Matching-based Model
This model is based on an early work on audio-based chord recognition (Fujishima, 1999). The model segments training melodies into half-bars, and constructs a pitch proﬁle for each segment. The chord label for a new segment is then selected based on the label for the training segment whose pitch proﬁle it most closely matches. When there is more than one possible chord template that has the highest matching score, we choose a chord randomly based on uniform distribution among the possibilities. We refer to this model as template matching-based as the underlying method compares the proﬁle of a given melody segment with those of the template chords.
We use Fujishima’s pitch class proﬁle (PCP) (Fujishima, 1999) as the pitch proﬁle representing respectively the melody and chord for each half-bar. A PCP is a 12dimensional feature vector x ∈ [0, 1]12 where each element corresponds to the activity of a pitch class. The PCP for each of the |C| chord labels is constructed by setting the elements corresponding to the pitch classes that are part of the chord to one, and all the others to zero. Because we consider only triad chords in this work, there will be exactly three one’s in the PCP of a chord label for each half bar. The PCP for melody is constructed similarly, but additionally considering the duration of notes. Speciﬁcally, the activity of the k-th pitch class, i.e., xk ∈ [0, 1], is set by the ratio of time the pitch class is active during the corresponding half bar.
The result of this model are more conservative by design, featuring intensive use of chord tones. And, this model sets the chord label independently for each half bar, without considering the neighboring chord labels, or the chord progression over time.
We note that, to remove the eﬀect of the input representations on the harmonization result, we use PCP as the model input representation for all the other models we implement for melody harmonizationm.

2.2. HMM-based Model
HMM is a probabilistic framework for modeling sequences with latent or hidden variables. Our HMM-based harmonization model regards chord labels as latent variables and estimates the most likely chord sequence for a given set of melody notes. Unlike the template matching-based model, this model considers the relationship between neighboring chord labels. HMM-based models similar to this one were widely used in chord generation and melody harmonization research before the current era of deep learning (Raczyn´ski, Fukayama, & Vincent, 2013; Simon et al., 2008).
We adopt a simple HMM architecture employed in (Lim et al., 2017). This model makes the following assumptions:
1. The observed melody sequence X = x1, . . . , xM is statistically biased due to the hidden chord sequence Y = y1, . . . , yM , which is to be estimated.
2. xm depends on only ym, ∀m ∈ [1, M ]. 3. ym depends on only ym−1, ∀m ∈ [2, M ].
The task is to estimate the most likely hidden sequence Yˆ = yˆ1, . . . , yˆM given X. This amounts to maximizing the posterior probability:

Yˆ = arg max P (Y |X) = arg max P (X|Y )P (Y )

Y

Y

M

= arg max P (xm|ym)P (ym|ym−1),

(1)

Y m=1

5

where P (y1|y0) is equal to P (y1). The term P (xm|ym) is also called the emission probability, and the term P (ym|ym−1) is called the transition probability. This optimization problem can be solved by the Viterbi algorithm (Forney, 1973).
Departing from the HMM in (Lim et al., 2017), our implementation uses the PCPs described in Section 2.1 to represent melody notes, i.e., to compute xm. Accordingly, we use multivariate Gaussian distributions to model the emission probabilities, as demonstrated by Fujishima (Sheh & Ellis, 2003). For each chord label, we set the covariance matrix of the corresponding Gaussian distribution to be a diagonal matrix, and calculate the mean and variance for each dimension from the PCP features of melody segments that are associated with that chord label in the training set.
To calculate the transition probabilities, we count the number of transitions between successive chord labels (i.e., bi-grams), then normalize those counts to sum to one for each preceding chord label. A uniform distribution is used when there is no bigram count for the preceding chord label. To avoid zero probabilities, we smooth the distribution by interpolating P (ym|ym−1) with the prior probability P (ym) as follows,

P (ym|ym−1) = (1 − β)P (ym) + βP (ym|ym−1) ,

(2)

yielding the revised transition probability P (ym|ym−1). The hyperparameter β is empirically set to 0.08 via experiments on a random 10% subset of the training set.

2.3. Genetic Algorithm (GA)-based Model
A GA is a ﬂexible algorithm that generally maximizes an objective function or ﬁtness function. GAs have been used for melody generation and harmonization in the past (de Le´on, In˜esta, Calvo-Zaragoza, & Rizo, 2016; Phon-Amnuaisuk & Wiggins, 1999), justifying their inclusion in this study. A GA can be used in both rule-based and probabilistic approaches. In the former case, we need to design a rule set of what conditions must be satisﬁed for musically acceptable melodies or harmonies—the ﬁtness function is formulated based on this rule set. In the latter, the ﬁtness function is formulated based on statistics of a data set.
Here, we design a GA-based melody harmonization model by adapting the GAbased melody generation model proposed by (Kitahara et al., 2018). Unlike the other implemented models, the GA-based model takes as input a computed feature vector for every 16-th note (i.e., 1/4 beats). Thus, the melody representation has a temporal resolution 8 times that of the chord progression (i.e., N = 8M ). This means that x8m and ym point to the same temporal position.
Our model uses a probabilistic approach, determining a ﬁtness function based on the following elements. First, the (logarithmic) conditional probability of the chord progression given the melody is represented as:

N

F1(X, Y ) = log P (y n/8 |xn),

(3)

n=1

where is the ceiling function. The chord transition probability is computed as:

M

F2(Y ) = log P (ym|ym−2, ym−1) .

(4)

m=3

6

The conditional probability of each chord given its temporal position is deﬁned as:

M

F3(Y ) = log P (ym|Posm) ,

(5)

m=1

where Posm is the temporal position of the chord ym. For simplicity, we deﬁned Posm = mod(m, 8), where mod is the modulo function. With this term, the model may learn that the tonic chord tends to appear at the ﬁrst half of the ﬁrst bar, while the dominant (V ) chord tends to occur at the second half of the second bar.
Finally, we use the entropy to evaluate a chord sequence’s complexity, which should not be too low as to avoid monotonous chord sequences. The entropy is deﬁned as E(Y ) = − ci∈C P (Y = ci) log P (Y = ci). In the ﬁtness function, we evaluate how likely this entropy E(Y ) is in a given data set.

F4(Y ) = log P (E = E(Y )) ,

(6)

where E is the random variable of the entropy of chord progressions and is discritized by 0.25. Its probability distribution is obtained from the training data.
The ﬁtness function F (Y ) is calculated as:

F (Y ) = w1F1(X, Y ) + w2F2(Y ) + w3F3(Y ) + w4F4(Y ) .

(7)

We simply set all the weights w1, w2, w3, w4 to 1.0 here.

2.4. Deep BiLSTM-based Model
This ﬁrst deep learning model is adapted from the one proposed by (Lim et al., 2017), which uses BiLSTM layers. This model extracts contextual information from the melody sequentially from both the positive and negative time directions. The original model makes chord prediction for every bar, using a vocabulary of only the major and minor triad chords (i.e., |C| = 24). We slightly extend this model such that the harmonic rhythm is a half bar, and the output chord vocabulary includes diminished and augmented chords, and the N.C. symbol (i.e., |C| = 49).
As shown in Figure 1, this model has two BiLSTM layers, followed by a fullyconnected layer. Dropout (Srivastava, Hinton, Krizhevsky, Sutskever, & Salakhutdinov, 2014) is applied with probability 0.2 at the output layer. This dropout rate, as well as the number of hidden layers and hidden units, are empirically chosen by maximizing the chord prediction accuracy on a random held-out subset of the training set. We train the model using minibatch gradient descent with categorical cross entropy as the the cost function. We use Adam as the optimizer and regularize by early stopping at the 10-th epoch to prevent over-ﬁtting.

2.5. Deep Multitask Model: MTHarmonizer
From our empirical observation on the samples generated by the aforementioned BiLSTM model, we ﬁnd that the model has two main defects for longer phrases:
(1) overuse of common chords—common chords like C, F, and G major are repeated and overused, making the chord progression monotonous.

7

(2) incorrect phrasing—non-congruent phrasing between the melody and chords similarly results from the frequent occurrence of common chords. The resulting frequent occurrence of progressions like F→C or G→C in generated sequences implies a musical cadence in an unﬁt location, potentially bringing an unnecessary sense of ending in the middle of a chord sequence.
We propose an extension of the BiLSTM model to address these two defects. The core idea is to train the model to predict not only the chord labels but also the chord functions (T.-P. Chen & Su, 2018), as illustrated in Figure 2. We call the resulting model a deep multitask model, or MTHarmonizer, since it deals with two tasks at the same time. We note that the use of the chord functions for melody harmonization has been found useful by Tsushima et al. (2018), using an HMM-based model.
Functional harmony elaborates the relationship between chords and scales, and describes how harmonic motion guides musical perception and emotion (T.-P. Chen & Su, 2018). While a chord progression consisting of randomly selected chords generally feels aimless, chord progressions which follow the rules of functional harmony establish or contradict a tonality. Music theorists annotate each scale degree into functions, such as tonic and dominant, based on the association between chord and degree in a particular scale. These functions explain the role a given scale degree, and its associated chord relative to the scale, plays in musical phrasing and composition. Speciﬁcally, we consider the following functions:
• The tonic function serves to stabilize and reinforce the tonal center. • The dominant function provides a strong sense of motion back to tonal center.
For example, a progression that moves from a dominant function scale degree chord to a tonal scale degree chord ﬁrst creates tension, then resolves it. • The others that encompasses all the other chords that are neither tonic nor dominant, such as the subdominant chords.
As will be introduced in Section 3, all the pieces in HTPD3 are in either C Major or c minor. Therefore, all chords share the same tonal center. We can directly map the chords into ‘tonic,’ ‘dominant,’ and ‘others’ functional groups, by name, without worrying about their relative functions in other keys, for other tonal centers. Speciﬁcally, we consider C, Am, Cm, A as tonic chords, and G and B diminished as dominant chords. The other chords all fall into the others category.
We identify two potential beneﬁts of adding chord functions to the target output. First, in contrast to the distribution of chord labels, the distribution of chord functions is relatively balanced, making it easier for the model to learn the chord functions. Second, as the chord functions and chord labels are interdependent, adding the chord functions as a target informs the model which chord labels share the same function and may therefore be interchangeable. We hypothesize that this multi-task learning will help our model learn proper functional progression, which in turn will produce better harmonic phrasing relative to the melody. Speciﬁcally, the loss function is deﬁned as:

L∗ = Lchord + γLfunction = H(Yˆchord, Ychord) + γH(Yˆfunction, Yfunction)

= H(f (X), Ychord) + γH(g(X), Yfunction) ,

(8)

where H(·) denotes the categorical cross entropy function, f (·) the chord label prediction branch, and g(·) the chord function prediction branch. When γ = 0, the model reduces to the uni-task model proposed by Lim et al. (2017), and we can simply write

8

Figure 2. Diagram of the proposed MTHarmonizer, a deep multitask model extended from the model (Lim et al., 2017) depicted in Figure 1. See Section 2.5 for details.

Ychord as Y . In our work, we set γ = 1.5 to ensure the loss value from Lchord and Lfunction are equally scaled. The two branches f and g share the two BiLSTM layers but not the fully-connected layer. Empirically, we found that if γ is too small, the model will tend to harmonize the melody with the chords with tonic and dominant functions; the resulting chord sequences would therefore lack diversity.
The outputs of f and g are likelihood values for each chord label and chord function given an input melody. As Figure 2 shows, in predicting the ﬁnal chord sequence, we rely on a weighted combination of the outputs of f and g in the following way:

M

Yˆ = arg max (P (yˆm = f (xm)) ∗ αmP (yˆm = h(g(xm)))) ,

(9)

yˆ1,yˆ2,...,yˆM m=1

where h(·) is simply a look-up table that maps the three chord functions to the |C| chord labels, and αm is a pre-deﬁned hyperparameter that allows us to boost the importance of correctly predicting the chord function over that of correctly predicting the chord label, for each chord. In our implementation, we set αm = 1.0 for the tonic and dominant chords, and αm = 1.8 for the other chords, to encourage the model to select chord labels that have lower likelihood, i.e., to use the “others” chords. This would more likely aﬀect the middle part of a chord sequence, because this is where the likelihood to observe a chord from the three functions to be likely similar, so applying diﬀerent αm makes a diﬀerence. In contrast, in the beginning or the end of a phrase, the likelihood of observing the “others” chords would tend to be low anyway, even after we boost it with αm. As we will mainly add diversity to the middle part of a chord sequence, we would not compromise the overall chord progression and phrasing.

9

3. Proposed Dataset
For the purpose of this study, we ﬁrstly collect a new dataset called the Hooktheory Lead Sheet Dataset (HLSD), which consists of lead sheet samples scraped from the online music theory forum called TheoryTab, hosted by Hooktheory (https:// www.hooktheory.com/theorytab), a company that produces pedagogical music software and books. The majority of lead sheet samples found on TheoryTab are usercontributed.1 Each piece contains high-quality, human-transcribed melodies alongside their corresponding chord progressions, which are speciﬁed by both literal chord symbols (e.g., Gmaj7), and chord functions (e.g., VI7) relative to the provided key.Chord symbols specify inversion if applicable, and the full set of chord extensions (e.g., #9, b11). The metric timing/placement of the chords is also provided. Due to copyright concerns, TheoryTab prohibits uploading full length songs. Instead, users upload snippets of a song (here referred to as lead sheet samples), which they voluntarily annotate with structural labels (e.g. “Intro,” “Verse,” and “Chorus”) and genre labels. A music piece can be associated with multiple genres.
As the samples in this dataset are segments of pop songs, e.g., a verse or a chorus, temporary changes of the tonal center should be rare.
We note that HLSD contains music of various genres, and only a few of them are classical music.2 As discussed in (de Clercq & Temperley, 2011), the rules of classical harmony are much less often followed in pop music. In addition, melodies in pop/rock music are more independent of the harmony than the case in classical music (Nobile, 2015; Temperley, 2007). It therefore remains to be studied whether the consideration of functional harmony improves our task here.
HLSD contains 11,329 lead sheets samples, all in 4/4 time signature. It contains up to 704 diﬀerent chord classes, which is deemed too many for the current study. We therefore take the following steps to process and simplify HLSD, resulting in the ﬁnal HTPD3 dataset employed in the performance study.
• We remove lead sheet samples that do not contain a suﬃcient number of notes. Speciﬁcally, we remove samples whose melodies comprise of more than 40% rests (relative to their lengths). One can think of this as correcting class imbalance, another common issue for machine learning models—if the model sees too much of a single event, it may overﬁt and only produce or classify that event.
• We then ﬁlter out lead sheets that are less than 4 bars and longer than 32 bars, so that 4 ≤ T ≤ 32. This is done because 4 bars is commonly seen as the minimum length for a complete musical phrase in 4/4 time signature. At the other end, 32 bars is a common length for a full lead sheet, one that is relatively long. Hence, as the majority of our dataset consists of mere song sections, we are inclined for not including samples longer than 32 bars.
• The HLSD provides the key signatures of every samples. We transpose every samples to either C major or c minor based on the provided key signatures.
• In general, a chord label can be speciﬁed by the pitch class of its root note (among
1We note that we do not own the copyrights of the lead sheets so we cannot further redistribute them. We collected the lead sheets only for academic research.
2Here is a random sample of 10 songs from the dataset: Love Grows Where My Rosemary Goes by Edison Lighthouse (1972), La Bamba by Ritchie Valen (1987), Palm Tree Paradise-Wario Land 4 by Ryoji Yoshitomi (2001), Forever and Always by Taylor Swift (2008), Trails of the Past by Sbtrkt (2011), I’ve Run Away to Join the Fairies by Magnetic Fields (2012), Semi Automatic by Twenty One Pilots (2013), Adventure of A Lifetime by Coldplay (2015), Same Drugs by Chance the Rapper (2016), Feel Good-Brooks Remix by Gryﬃn And Illenium (2017).
10

12 possible pitch classes, i.e., C, C#, . . . , B, in a chromatic scale), and its chord quality, such as ‘triad’, ‘sixths’, ‘sevenths’, and ‘suspended.’ HLSD contains 704 possible chord labels, including inversions. However, the distribution of these labels is highly skewed. In order to even out the distribution and simplify our task, we reduce the chord vocabulary by converting each label to its root position triad form, i.e., the major, minor, diminished, and augmented chords without 7ths or additional extensions. Suspended chords are mapped to the major and minor chords. As a result, only 48 chord labels (i.e., 12 root notes by 4 qualities) and N.C. are considered (i.e., |C| = 49). • We standardize the dataset so that a chord change can occur only every bar or every half bar.
We do admit that this simpliﬁcation can decrease the chord color and reduce the intensity of tension/release patterns, and can sometimes convert a vibrant, subtle progression into a monotonous one (e.g., because both CMaj7 and C7 are mapped to C chord). We plan to make full use of the original chord vocabulary in future works.
Having pre-deﬁned train and test splits helps to facilitate the use of HTPD3 for evaluating new models of melody harmonization via the standardization of training procedure. As HTPD3 includes paired melody and chord sequences, it can also be used to evaluate models for chord-conditioned melody generation as well. With these use cases in mind, we split the dataset so that the training set contains 80% of the pieces, and the test set contains 10% of the pieces. There are in total 923 lead sheet samples in the test set. The remaining 10% is reserved for future use. When splitting, we imposed the additional requirement that lead sheet samples from the same song are in the same subset.
4. Proposed Objective Metrics
To our knowledge, there are at present no standardized, objective evaluation metrics for the melody harmonization task. The only objective metric adopted by (Lim et al., 2017), in evaluating the models they built is a categorical cross entropy-based chord prediction error, representing the discrepancy between the ground truth chords Y∗ and predicted chords Yˆ∗ = f (X∗). The chord prediction error is calculated for each half bar individually and then got averaged, not considering the chord sequence as a whole. In addition, it does not directly measure how the generated chord sequence ﬁts with the given melody. What’s more, when calculating the chord prediction error, the underlying assumption is that the “ground truth” chord sequence Y∗ is the only feasible one to harmonize the given melody X∗. This is not true in general.
For the comparative study, we introduce here a set of six objective metrics deﬁned below. These metrics are split into two categories, namely three chord progression metrics and three chord/melody harmonicity metrics. Please note that we do not evaluate the melody itself, as the melody is provided by the ground truth data.
Chord progression metrics evaluate each chord sequence as a whole, independent from the melody, and relate to the distribution of chord labels in a sequence.
• Chord histogram entropy (CHE): Given a chord sequence, we create a histogram of chord occurrences with |C| bins. Then, we normalize the counts to sum
11

to 1, and calculate its entropy:

|C|

H = − pi log pi ,

(10)

i=1

where pi is the relative probability of the i-th bin. The entropy is greatest when the histogram follows a uniform distribution, and lowest when the chord sequence uses only one chord throughout. • Chord coverage (CC): The number of chord labels with non-zero counts in the chord histogram in a chord sequence. • Chord tonal distance (CTD): The tonal distance proposed by (Harte et al., 2006) is a canonical way to measure the closeness of two chords. It is calculated by ﬁrstly calculating the PCP features of two chords, projecting the PCP features to a derived 6-D tonal space, and ﬁnally calculating the Euclidean distance between the two 6-D feature vectors. CTD is the average value of the tonal distance computed between every pair of adjacent chords in a given chord sequence. The CTD is highest when there are abrupt changes in the chord progression (e.g., from C chord to B chord).
Chord/melody harmonicity metrics, on the other hand, aims to evaluate the degree to which a generated chord sequence successfully harmonizes a given melody sequence.
• Chord tone to non-chord tone ratio (CTnCTR): In reference to the chord sequence, we count the number of chord tones, and non-chord tones in the melody sequence. Chord tones are deﬁned as melody notes whose pitch class are part of the current chord (i.e., one of the three pitch classes that make up a triad) for the corresponding half bar. All the other melody notes are viewed as non-chord tones. One way to measure the harmonicity is to simply computing the ratio of the number of the chord tones (nc) to the number of the non-chord tones (nn). However, we ﬁnd it useful to further take into account the number of a subset of non-chord tones (np) that are two semitones within the notes which are right after them, where subscript p denotes a “proper” non-chord tone. We deﬁne CTnCTR as
nc + np . (11) nc + nn
CTnCTR equals one when there are no non-chord tones at all, or when np = nn. • Pitch consonance score (PCS): For each melody note, we calculate a conso-
nance score with each of the three notes of its corresponding chord label. The consonance scores are computed based on the musical interval between the pitch of the melody notes and the chord notes, assuming that the pitch of the melody notes is always higher. This is always the case in our implementation, because we always place the chord notes lower than the melody notes. The consonance score is set to 1 for consonance intervals including unison, major/minor 3rd, perfect 5th, major/minor 6th, set to 0 for a perfect 4th, and set to –1 for other intervals, which are considered dissonant. PCS for a pair of melody and chord sequences is computed by averaging these consonance scores across a 16th-note windows, excluding rest periods. • Melody-chord tonal distance (MCTD): Extending the idea of tonal distance, we represent a melody note by a PCP feature vector (which would be a one-hot

12

Figure 3. A harmonization example (in major key) from The Beatles: Hey Jude. We can see that, while the non-deep learning models change the harmonization in diﬀerent phrases, the MTharmonizer generates a V-I progression nicely to close the phrase.
vector) and compare it against the PCP of a chord label in the 6-D tonal space (Harte et al., 2006) to calculate the closeness between a melody note and a chord label. MCTD is the average of the tonal distance between every melody note and corresponding the chord label calculated across a melody sequence, with each distance weighted by the duration of the corresponding melody note.
5. Comparative Study
We train all the ﬁve models described in Section 2 using the training split of HTPD3 and then apply them to the test split of HTPD3 to get the predicted chord sequences for each melody sequence. Examples of the harmonization result of the evaluated models can be found in Figures 3 and 4.
We note that, since one cannot judge the full potential of each algorithm only from our simpliﬁed setting of melody harmonization, we do not intend to ﬁnd what method is the best in general. We rather attempt a challenge to compare diﬀerent harmonization method which have not been directly compared because of the diﬀerent context that each approach assumes.
In what follows, we use the harmonization result for a random subset of the test set comprising 100 pieces in a user study for subjective evaluation. The result of this subjective evaluation is presented in Section 5.1. Then, in Section 5.2, we report the results of an objective evaluation wherein we compute the mean values of the chord/melody harmonicity and chord progression metrics presented in Section 4 for the harmonization results for each test set piece.
13

Figure 4. A harmonization example (in minor key) from ABBA: Gimme Gimme Gimme A Man After Midnight. Similar to the example shown in Figure 3, the result of the MTHarmonizer appears to be more diverse and functionally correct. We also see that the result of GA is quite “interesting”—e.g., with nondiatonic chord D ﬂat Major and close the music phrase with Picardy third (i.e., a major chord of the tonic at the end of a chord sequence that is in a minor key). We also see that the non-deep learning methods seem to be weaker in handling the tonality of music.
5.1. Subjective Evaluation
We conducted an online survey where we invited human subjects to listen to and assess the harmonization results of diﬀerent models. The subjects evaluated the harmonizations in terms of the following criteria:
• Harmonicity: The extent to which a chord progression successfully or pleasantly harmonizes a given melody. This is designed to correspond to what the melody/chord harmonicity metrics described in Section 4 aim to measure.
• Interestingness: The extent to which a chord progression sounds exciting, unexpected and/or generates “positive” stimulation. This criterion corresponds to the chord-related metrics described in Section 4. Please note that we use a less technical term “interestingness” here since we intend to solicit feedback from people either with or without musical backgrounds.
• The Overall quality of the given harmonization.
Given a melody sequence, we have in total six candidate chord sequences to accompany it: those generated by the ﬁve models presented in Section 2, and the humancomposed, ground-truth progression retrieved directly from the test set. We intend to compare the results of the automatically generated progression with the original human-composed progression. Yet, given the time and cognitive load required, it was not possible to ask each subject to evaluate the results of every model for every piece of music in the test set (there are 6 × 923 = 5, 538 sequences in total). We describe below how our user study is designed to make the evaluation feasible.
14

5.1.1. Design of the User Study
First, we randomly select 100 melodies from the test set of HTPD3. For each human subject, we randomly select three melody sequences from this pool, and present to the subject the harmonization results of two randomly selected models for each melody sequence. For each of the three melodies, the subject listens to the melody without accompaniment ﬁrst, and then the sequence with two diﬀerent harmonizations. Thus, the subject has to listen to nine music pieces in total: three melody sequences and the six harmonized ones. As we have six methods for melody harmonization (including the original human-composed harmonization), we select methods for each set of music such that each method is presented once and only once to each subject. The subjects are not aware of which harmonization is generated by which method, but are informed that at least one of the harmonized sequence is human-composed.
In each set, the subject has to listen to the two harmonized sequences and decide which version is better according to the three criteria mentioned earlier. This ranking task is mandatory. In addition, the subject can choose to further grade the harmonized sequences in a ﬁve-point Likert scale with respect to the criteria mentioned earlier. Here, we break “harmonicity” into the following two criteria in order to get more feedback from subjects:
• Coherence: the coherence between the melody and the chord progression in terms of harmonicity and phrasing.
• Chord Progression: how coherent, pleasant, or reasonable the chord progression is on its own, independent of the melody.
This optional rating task thus has four criteria in total. The user study opens to an “instructions” page, that informs the subjects that we
consider only root-positioned triad chords in the survey. Moreover, they are informed that there is no “ground truth” in melody harmonization—the task is by nature subjective. After collecting a small amount of relevant personal information from the subjects, we present them with a random audio sample and encourage them to put on their headsets and adjust the volume to a comfortable level. After that, they are prompted to begin evaluating the three sets (i.e., one set for each melody sequence), one-by-one on consecutive pages.
We spread the online survey over the Internet openly, without restriction, to solicit voluntary, non-paid participation. The webpage of the survey can be found at https://musicai.citi.sinica.edu.tw/survey mel harm/.
5.1.2. User Study Results
In total, 202 participants from 16 countries took part in the survey. We had more male participants than female (ratio of 1.82:1), and the average age of participants was 30.8 years old. 122 participants indicated that they have music background, and 69 of them are familiar with or expertise in the harmonic theory. The participants took on average 14.2 minutes to complete the survey.
We performed the following two data cleaning steps: First, we discarded both the ranking and rating results from participants who spent less than 3 minutes to complete the survey, which is considered too short. Second, we disregarded rating results when the relative ordering of the methods contradicted that from the ranking results. As a result, 9.1% and 21% of the ranking and rating records were removed, respectively.
We ﬁrst discuss the results of the pairwise ranking task, which is shown in Figure 5. The following observations are made:
15

(a) Harmonicities

(b) Interestingness

(c) Overall
Figure 5. “Win probabilities” of diﬀerent model pairs. Each entry represents the probability that the model in that column scores higher than the model in that row.
16

Figure 6. The mean rating scores in subjective evaluation, along with the standard deviation (the error bars).
• The human-composed progressions have the highest “win probabilities” on average in all the three ranking criteria. It performs particularly well in Harmonicity.
• In general, the deep learning methods have higher probabilities to win over the non-deep learning methods in Harmonicity and Overall.
• For Interestingness, GA performs the best among the ﬁve automatic methods, which we suspect stems from its entropy term (Eq. (6)).
• Among the two deep learning methods, the MTHarmonizer consistently outperforms the BiLSTM in all ranking criteria, especially for Interestingness. We (subjectively) observe that MTHarmonizer indeed generates more diverse chord progressions compared to the vanilla BiLSTM, perhaps due to the consideration of functions.
The results of the rating task shown in Figure 6, on the other hand, lead to the following observations:
• Congruent with the results of the ranking task, the MTHarmonzer model achieves the second best performance here, only losing out to the original humancomposed chord progressions. The MTHarmonzier consistently outperforms the other four automatic methods in all the four metrics. With a paired t-test, we ﬁnd that there is signiﬁcant performance diﬀerence between the MTHarmonzer progressions and the original human-composed progressions in terms of Coherence and Chord Progression (p-value<0.005), but no signiﬁcant diﬀerence in terms of Interestingness and Overall.
• Among the four metrics, the original human-composed progressions score higher in Coherence (3.81) and Overall (3.78), and the lowest in Interestingness (3.43). This suggests that the way we simplify the data (e.g., using only root-positioned triad chords) may have limited the perceptual qualities of the music, in particular its diversity.
• Generally speaking, the results in Chord Progression (i.e., the coherence of the chord progression on its own) seems to correlate better with the results in Coherence (i.e., the coherence between the melody and chord sequences) than the Interestingness of the chord progression. This suggests that a chord progression rated as being interesting may not sound coherent.
• Although the GA performs worse than the MTHarmonizer on all the four metrics, it actually performs fairly well in Interestingness (3.23), as we have observed from the ranking result. A paired t-test showed no signiﬁcant performance difference between the GA generated progressions and original human-composed progressions in Interestingness. A hybrid model that combines GA and deep learning may be a promising direction for future research.
From the rating and ranking tasks, we see that, in terms of harmonicity, automatic
17

Melody/chord harmonicity metrics

CTnCTR PCS MCTD

Human-composed
Template matching HMM (adapted from (Lim et al., 2017)) GA-based (adapted from (Kitahara et al., 2018)) BiLSTM (adapted from (Lim et al., 2017)) MTHarmonizer (proposed here)

0.74
0.91 0.89 0.74 0.87 0.82

1.42
1.97 1.93 0.43 1.84 1.77

1.03
0.83 0.85 1.31 0.91 0.94

Chord progression metrics

CHE CC CTD

Human-composed

1.28 2.62 0.88

Template matching HMM (adapted from (Lim et al., 2017)) GA-based (adapted from (Kitahara et al., 2018)) BiLSTM (adapted from (Lim et al., 2017)) MTHarmonizer (proposed here)

1.01 0.88 1.58 1.07 1.29

1.70 1.89 2.47 2.07 2.31

0.65 0.56 0.96 0.71 1.02

Table 1. Objective evaluation scores for diﬀerent melody harmonization models. The closer the values to that of the human-composed samples, the better the model is in modeling the training data. The bold values indicate the closet value to that of the human-composed samples per metric. And, 1) higher values in CTnCTR and PCS and lower values in MCTD may suggest that the melody/chord harmonicity is high; 2) higher values in CHE and CC and lower values in CTD may suggest that the diversity (which can be related to the interestingness) of the chord progression is high. See Section 4 for the deﬁnitions of the metrics.

methods still fall behind the human composition. However, the results of the two deep learning based methods are closer to that of the human-composed ones.
5.2. Objective Evaluation
The results are displayed in Table 1. We discuss the result of the melody/chord harmonicity metrics ﬁrst. We can see that the results for the two deep learning methods are in general closer to the results for the original human-composed progressions than those of the three non-deep learning methods for all three harmonicity metrics, most signiﬁcantly on the latter two. The template matching-based and HMM-based methods scores high in PCS and low in MCTD, indicating that the harmonization these two methods generate may be too conservative. In contrast, the GA scores low in PCS and high in MCTD, indicating overly low harmonicity. These results are consistent with the subjective evaluation, suggesting that these metrics can perhaps reﬂect human perception of the harmonicity between melody and chords.
From the result of the chord progression metrics, we also see from CHE and CC that the progressions generated by the template matching-based and HMM-based methods seem to lack diversity. In contrast, the output of GA features high diversity.
As the GA based method was rated lower than the template matching and HMM methods in terms of the Overall criterion in our subjective evaluation, it seems that the subjects care more about the harmonicity than the diversity of chord progressions.
Comparing the two deep learning methods, we see that the MTHarmonizer uses more non-chord tones (smaller CTnCTR) and uses a greater number of unique chords

18

(larger CC) than the BiLSTM model. The CHE of the MTHarmonizer is very close to that of the original human-composed progressions.
In general, the results of the objective evaluation appear consistent with those of the subjective evaluation. It is diﬃcult to quantify which metrics are better for what purposes, and how useful and accurate these metrics are overall. Therefore, our suggestion is to use them mainly to gain practical insights into the results of automatic melody harmonization models, rather than to judge their quality. As pointed out by (Dong et al., 2018), objective metrics can be used to track the performance of models during development, before committing to running the user study. Yet, human evaluations are still needed to evaluate the quality of the generated music.
Finally, although we have argued earlier that there is no ground truth in melody harmonization and that it is not adequate to rely solely on chord prediction error to evaluate the models, we ﬁnd that the MTHarmonizer also achieves the lowest chord prediction error (i.e., a 48-class classiﬁcation problem) and chord function prediction error (a 3-class classiﬁcation problem). On the test set, the chord prediction accuracy for the ﬁve models (template matching, HMM, GA, BiLSTM, and MTHarmonizer) is 29%, 31%, 20%, 35%, and 38%, respectively, while the accuracy for a random guess baseline is only 2%. And, the chord function prediction accuracy is 62%, 61%, 55%, 65%, 69%, respectively, while the accuracy for a random guess baseline is 51%.
6. Discussions
We admit that the comparative study presented above has some limitations. First, because of the various preprocessing steps taken for data cleaning and for making the melody harmonization task manageable (cf. Section 3), the “human-composed” harmonizations are actually simpliﬁed versions of those found on TheoryTab. We considered triad chords only, and we did not consider performance-level attributes such as velocity and rhythmic pattern of chords. This limits the perceptual quality of the human-composed chord progression, and therefore also limits the results that can be achieved by automatic methods. The reduction from extended chords to triads reduces the “color” of the chords and creates many innacurate chord repetitions in the dataset (e.g., both the alternated CMaj7 and C7 will be reduced to C triad chord). We believe it is important to properly inform the human subjects of such limitations as we did in the instruction phase of our user study. We plan to compile other datasets from HLSD to extend the comparative study in the future.
Second, in our user study we asked human subjects to rank and rate the results of two randomly chosen methods in each of the three presented sets. After analyzing the results, we found that the subject’s ratings are in fact relative. For example, the MTHarmonizer’s average score in Overall is 3.04 when presented alongside the humancomposed progressions, and 3.57 when confronted with the genetic algorithm-based model. We made sure in our user study that all the methods are equally likely to be presented together with every other method, so the average rating scores presented in Figure 6 do not favor a particular method. Still, caution is needed when interpreting the rating scores. Humans may not have a clear idea of how to consistently assign a score to a harmonization. While it is certainly easier to objectively compare multiple methods with the provided rating scores, we still recommended asking human subjects to make pairwise rankings in order to make the result more reliable.
Third, as the aim of this paper is to sample representative models from the literature, the current setting cannot decouple the eﬀects of the model architectures (e.g.,
19

HMMs, GAs and BiLSTMs) and the music knowledge induced into the model (e.g., the concept of functional harmonic in MTHarmonizer). Moreover, for similar reasons, the models we implemented in this paper do not include extensions that might lead to better performance. For example, we could further improve the HMM model by using trigrams or extending the hidden layers as discussed in the literature (Paiement et al., 2006; Temperley, 2009; Tsushima et al., 2017). It may also be possible to incorporate some of the proposed objective metrics (such as the chord tone to non-chord tone ratio) as additional loss terms. The aim of this paper is to observe how diﬀerent categories of models characterize the harmonization results rather than to explore the full potentials of each presented model.
Reviewing the properties of harmonization algorithms which imitate styles in a dataset as in our research still holds its importance, although recent music generation research is shifting towards measuring how systems can generate content that extrapolates meaningfully from what the model have learned (Zacharakis, KaliakatsosPapakostas, Tsougras, & Cambouropoulos, 2018). Extrapolation could be based on the model which also achieves interpolation or maintaining particular styles among data points. We believe we can further discuss extrapolation based on the understanding of how methods imitate data.
7. Conclusion
In this paper, we have presented a comparative study implementing and evaluating a number of canonical methods and one new method for melody harmonization, including deep learning and non-deep learning based approaches. The evaluation has been done using a lead sheet dataset we newly collected for training and evaluating melody harmonization. In addition to conducting a subjective evaluation, we employed in total six objective metrics with which to evaluate a chord progression given a melody. Our evaluation shows that deep learning models indeed perform better than non-deep learning ones in a variety of aspects, including harmonicity and interestingness. Moreover, a deep learning model that takes the function of chords into account reaches the best result among the evaluated models.
Future work can be directed toward at least the following three directions. First, it is important to extend the chord vocabulary to include more complicated chords. Second, as self-attention based neural sequence models such as the Transformers (Vaswani et al., 2017) have been shown powerful alternatives to RNNs for various automatic music generation tasks (Y.-H. Chen, Huang, Hsiao, & Yang, 2020; Donahue, Mao, Li, Cottrell, & McAuley, 2019; Y.-S. Huang & Yang, 2020; Ren et al., 2020), it might be interesting to investigate Transformer-based models for melody harmonization. Finally, other than the melody harmonization task (i.e., generating chords given a melody) addressed in the paper, it would also be interesting to study chord-conditioned melody generation (i.e., generating a melody given chords) (Genchel, Pati, & Lerch, 2019; Trieu & Keller, 2018; Yang, Chou, & Yang, 2017), or simultaneously generating both melody and harmony from scratch (Jiang, Xia, Carlton, Anderson, & Miyakawa, 2020; Liu & Yang, 2018; Wu & Yang, 2020).
References
Allan, M., & Williams, C. K. I. (2005). Harmonising chorales by probabilistic inference. In
20

Proc. Advances in Neural Information Processing Systems. Briot, J.-P., Hadjeres, G., & Pachet, F. (2017). Deep learning techniques for music generation:
A survey. arXiv preprint arXiv:1709.01620 . Chen, T.-P., & Su, L. (2018). Functional harmony recognition of symbolic music data with
multi-task recurrent neural networks. In Proc. Int. Soc. Music Information Retrieval Conf. (pp. 90–97). Chen, Y.-H., Huang, Y.-S., Hsiao, W.-Y., & Yang, Y.-H. (2020). Automatic composition of guitar tabs by Transformers and groove modeling. In Proc. Int. Soc. Music Information Retrieval Conf. Chuan, C.-H., & Chew, E. (2007). A hybrid system for automatic generation of style-speciﬁc accompaniment. In Proc. Int. Joint Workshop on Computational Creativity. de Clercq, T., & Temperley, D. (2011). A corpus analysis of rock harmony. Popular Music, 30 (1), 47–70. De Haas, W. B., Magalh˜aes, J. P., Wiering, F., & Veltkamp, R. C. (2014). Automatic functional harmonic analysis. Computer Music Journal , 37 (4), 37–53. de Leo´n, P. J. P., In˜esta, J. M., Calvo-Zaragoza, J., & Rizo, D. (2016). Data-based melody generation through multi-objective evolutionary computation. J. Mathematics and Music, 10 (2), 173-192. Donahue, C., Mao, H. H., Li, Y. E., Cottrell, G. W., & McAuley, J. (2019). LakhNES: Improving multi-instrumental music generation with cross-domain pre-training. In Proc. Int. Soc. Music Information Retrieval Conf. (pp. 685–692). Dong, H.-W., Hsiao, W.-Y., Yang, L.-C., & Yang, Y.-H. (2018). MuseGAN: Symbolic-domain music generation and accompaniment with multi-track sequential generative adversarial networks. In Proc. AAAI Conf. Artiﬁcial Intelligence. Ebciogˇlu, K. (1988). An expert system for harmonizing four-part chorales. Computer Music Journal , 12 (3), 43–51. Fern´andez, J. D., & Vico, F. (2013). AI methods in algorithmic composition: A comprehensive survey. J. Artiﬁcial Intelligence Research, 48 (1), 513–582. Forney, G. D. (1973). The viterbi algorithm. Proceedings of the IEEE , 61 , 268–278. Fujishima, T. (1999). Realtime chord recognition of musical sound: A system using common Lisp. In Proc. Int. Computer Music Conf. (pp. 464–467). Genchel, B., Pati, A., & Lerch, A. (2019). Explicitly conditioned melody generation: A case study with interdependent rnns. In Proc. Computer Simulation of Music Creativity Conf. Hadjeres, G., & Pachet, F. (2017). DeepBach: a steerable model for Bach chorales generation. In Proc. Int. Conf. Machine Learning. Harte, C., Sandler, M., & Gasser, M. (2006). Detecting harmonic change in musical audio. In Proc. Int. Soc. Music Information Retrieval Conf. Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., . . . Kingsbury, B. (2012). Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29 (6), 82-97. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computing, 9 (8), 1735–1780. Huang, C.-Z. A., Cooijmans, T., Roberts, A., Courville, A., & Eck, D. (2017). Counterpoint by convolution. In Proc. Int. Soc. Music Information Retrieval Conf. (pp. 211–218). Huang, Y.-S., & Yang, Y.-H. (2020). Pop Music Transformer: Beat-based modeling and generation of expressive Pop piano compositions. In Proc. ACM Multimedia. Jacoby, N., Tishby, N., & Tymoczko, D. (2015). An information theoretic approach to chord categorization and functional harmony. J. New Music Research, 44 (3), 219–244. Jiang, J., Xia, G. G., Carlton, D. B., Anderson, C. N., & Miyakawa, R. H. (2020). Transformer VAE: A hierarchical model for structure-aware and interpretable music representation learning. In Proc. Int. Conf. Acoustics, Speech and Signal Processing (p. 516-520). Kitahara, T., Giraldo, S., & Ramirez, R. (2018). JamSketch: Improvisation support system with GA-based melody creation from user’s drawing. In Proc. Int. Symp. Computer Music Multidisciplinary Research (pp. 509–521).
21

Lim, H., Rhyu, S., & Lee, K. (2017). Chord generation from symbolic melody using BLSTM networks. In Proc. Int. Soc. Music Information Retrieval Conf. (pp. 621–627).
Liu, H.-M., & Yang, Y.-H. (2018). Lead sheet generation and arrangement by conditional generative adversarial network. In Proc. IEEE Int. Conf. Machine Learning And Applications.
Makris, D., Kayrdis, I., & Sioutas, S. (2016). Automatic melodic harmonization: An overview, challenges and future directions. In Trends in Music Information Seeking, Behavior, and Retrieval for Creativity (pp. 146–165). IGI Global.
Nobile, D. F. (2015). Counterpoint in rock music: Unpacking the “melodic-harmonic divorce”. Music Theory Spectrum, 37 (2), 189–203.
Paiement, J.-F., Eck, D., & Bengio, S. (2006). Probabilistic melodic harmonization. In Proc. Conf. Canadian Society for Computational Studies of Intelligence (pp. 218–229).
Phon-Amnuaisuk, S., & Wiggins, G. (1999). The four-part harmonisation problem: a comparison between genetic algorithms and a rule-based system. In Proc. AISB Symp. Musical Creativity.
Raczyn´ski, S., Fukayama, S., & Vincent, E. (2013). Melody harmonisation with interpolated probabilistic models. J. New Music Research, 42 , 223-235.
Raphael, C., & Stoddard, J. (2004). Harmonic analysis with probabilistic graphical models. Computer Music Journal , 28 (3), 45–52.
Ren, Y., He, J., Tan, X., Qin, T., Zhao, Z., & Liu, T.-Y. (2020). PopMAG: Pop music accompaniment generation. In Proc. ACM Int. Conf. Multimedia.
Riemann, H. (1893). Vereinfachte harmonielehre, oder die lehre von den tonalen funktionen der akkorde. 1896. Tr. H. Bewerunge.
Rohrmeier, M., & Cross, I. (2008). Statistical properties of tonal harmony in bach’s chorales. In Proc. Int. Conf. Music Perception and Cognition (pp. 619–627).
Sheh, A., & Ellis, D. P. (2003). Chord segmentation and recognition using EM-trained hidden Markov models. In Proc. Int. Soc. Music Information Retrieval Conf. (pp. 185–191).
Simon, I., Morris, D., & Basu, S. (2008). MySong: Automatic accompaniment generation for vocal melodies. In Proc. SIGCHI Conf. Human Factors in Computing Systems (p. 725-734).
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overﬁtting. J. Machine Learning Research, 15 (1), 1929–1958.
Temperley, D. (2007). The melodic-harmonic ‘divorce’ in rock. Popular Music, 26 (2), 323–342. Temperley, D. (2009). A uniﬁed probabilistic model for polyphonic music analysis. J. New
Music Research, 38 (1), 3–18. Trieu, N., & Keller, R. M. (2018). JazzGAN: Improvising with generative adversarial networks.
In Proc. Int. Workshop on Musical Metacreation. Tsushima, H., Nakamura, E., Itoyama, K., & Yoshii, K. (2017). Function- and rhythm-aware
melody harmonization based on tree-structured parsing and split-merge sampling of chord sequences. In Proc. Int. Soc. Music Information Retrieval Conf. Tsushima, H., Nakamura, E., Itoyama, K., & Yoshii, K. (2018). Generative statistical models with self-emergent grammar of chord sequences. J. New Music Research, 47 (3), 226–248. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is all you need. In Proc. Advances in Neural Information Processing Systems (pp. 5998–6008). Wu, S.-L., & Yang, Y.-H. (2020). The Jazz Transformer on the front line: Exploring the shortcomings of ai-composed music through quantitative measures. In Proc. Int. Soc. Music Information Retrieval Conf. Yang, L.-C., Chou, S.-Y., & Yang, Y.-H. (2017). MidiNet: A convolutional generative adversarial network for symbolic-domain music generation. In Proc. Int. Soc. Music Information Retrieval Conf. Zacharakis, A., Kaliakatsos-Papakostas, M., Tsougras, C., & Cambouropoulos, E. (2018). Musical blending and creativity: An empirical evaluation of the CHAMELEON melodic harmonisation assistant. Musicae Scientiae, 11 (1), 119-144.
22

