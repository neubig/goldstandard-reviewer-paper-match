Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering
Wenhu Chen, Pat Verga, Michiel de Jong†, John Wieting, William W. Cohen Google Research, University of Southern California†
{wenhuchen,patverga,jwieting,wcohen}@google.com, msdejong@usc.edu

arXiv:2204.04581v2 [cs.CL] 11 May 2022

Abstract
Existing state-of-the-art methods for opendomain question-answering (ODQA) use an open book approach in which information is ﬁrst retrieved from a large text corpus or knowledge base (KB) and then reasoned over to produce an answer. A recent alternative is to retrieve from a collection of previouslygenerated question-answer pairs. This has several practical advantages, including being more memory- and compute-efﬁcient. Question-answer pairs are also appealing in that they seem to be an intermediate between text and KB triples: like KB triples, they usually concisely express a single relationship, but like text, they have good coverage. We describe a new QA system which augments a text-to-text model with a large memory of question-answer pairs, and a new pre-training task for the latent step of question retrieval. The pre-training task substantially simpliﬁes training and greatly improves performance on smaller QA benchmarks. Unlike prior systems of this sort, our QA system can also answer multi-hop questions that do not explicitly appear in the collection of stored questionanswer pairs.
1 Introduction
Open-domain question answering (ODQA) is a well-studied knowledge-intensive task, and state-of-the-art methods require retrieving relevant knowledge from a large corpus or datastore and then performing reasoning over this retrieved evidence. Most existing methods retrieve documents (Chen et al., 2017a; Lee et al., 2019; Karpukhin et al., 2020) or structured KB triples (Verga et al., 2021). Recently, a few works have proposed retrieving from a collection of question-answer pairs—an approach made feasible by advances in scalable automatic question generation. In this setting, a new question is answered by retrieving relevant question paraphrases from

an index, and aggregating their associated answers (Xiao et al., 2021; Lewis et al., 2021). Notably, the RePAQ system from (Lewis et al., 2021) won the 2020 EfﬁcientQA competition (Min et al., 2021), outperforming closed-book QA (CBQA) models by a signiﬁcant margin and matching the prior SoTA performance on NQ (Kwiatkowski et al., 2019).
A collection of question-answer (QA) pairs is appealing for several reasons. In particular, QA pairs are concise, and also tend to express a single relationship, much like a KB triple—but unlike KB triples, question-answer collections appear to have good coverage of standard open QA datasets. RePAQ demonstrated several concrete and practical advantages of retrieval of question-answer pairs. This approach is memory-efﬁcient and computationally efﬁcient, relative to text retrieval. RePAQstyle systems are also often able to determine when no indexed question matches a test question, and thus can be accurate “selective QA systems”—i.e., their precision can be increased by selectively abstaining, instead of returning a low-conﬁdence answer. Finally, RePAQ systems can be effectively ensembled with text-retrieval QA systems, leading to a hybrid system that outperforms either.
However, question-retrieval QA systems have several limitations as well. First, there is no large-scale supervised data for the latent questionretrieval step of the system. This contrasts with the step of retrieving text given a question, where supervised data is used to build retrievers like DPR (Karpukhin et al., 2020). To address this, RePAQ uses a latent-retrieval training process (similar to REALM (Lee et al., 2019)), in which the retriever is trained using loss transferred from the downstream QA task. This requires asynchronously updating the index as training proceeds, a process that is complex and computationally expensive. This is also a problem for domains with limited QA data: as we will show, RePAQ’s performance is disappointing on smaller datasets like WebQues-

tions (Berant et al., 2013), which contains only 3K training instances. To address this problem, we introduce a novel pre-training task for question retrieval, which can be applied to any questionanswer dataset generated from a corpus. Using this pre-training task, we are able to train a retriever for a new task with a ﬁxed series of three training phases, simplifying implementation, and greatly improving performance on smaller datasets.
A second problem is that RePAQ systems are limited to answering questions that are explicitly stored in the index, or paraphrases of such questions. This contrasts with QA systems that retrieve from KBs, which can typically generate complex queries that combine the atomic triples in the KB. To address this, we present an extended model that answers multi-hop questions by retrieval from a question-answer corpus, the ﬁrst question-retrievalbased QA system (to our knowledge) that addresses this task.
In more detail, we propose a new QA-MemoryAugmented Transformer (QAMAT) with better compositionality paired with a lower complexity training strategy. QAMAT is based on T5-encoderdecoder (Raffel et al., 2020) paired with an integrated key-value memory (Khandelwal et al., 2019; Borgeaud et al., 2021) populated with questionanswer pairs (See Figure 1). Given an input, the encoder generates a query representation scored against the QA memory to retrieve the top-K relevant QA pairs. The encoder then reprocesses the input along with the retrievals forming a QA-injected representation which is passed to the decoder to attend to and generate.
To reduce the training (ﬁne-tuning) sample complexity, we propose to ﬁrst pre-train QAMAT on a large-scale corpus to teach the model how to search and interpret QA-pairs and then ﬁne-tune it for different downstream tasks. We construct the pretraining corpus by leveraging existing methods for question generation, producing a very large set of potentially interesting questions from text passages (Zhou et al., 2017; Alberti et al., 2019; Lewis et al., 2021). For each QA pair, we take the passage it was generated from and mask the answer. The model is then trained by retrieving and using an appropriate QA pair to recover the answer. We show that pre-training greatly boosts the model’s performance and helps the model generalize to different domains. For example, the pre-trained model can achieve zero-shot performance of 40% EM on NQ

and TriviaQA without any ﬁne-tuning. The effectiveness of this pre-training task means
that we can avoid the expensive latent training procedure used by RePAQ, and instead use an efﬁcient two-stage training pipeline. In the ﬁrst stage, we use a small local in-batch memory of QA pairs to optimize the QA pair encoder. We then freeze the encoder and construct the index for the global memory. In the second stage, we retrieve from this ﬁxed global memory and continue to optimize remaining parameters—including the parameters used to construct queries to the global memory—for better performance.
Lastly, to enable QAMAT to perform compositional reasoning on multi-hop ODQA, we propose an extension QAMAT+, which can iteratively retrieve from the memory to augment the query. The augmented query will be fed to the QAMAT to generate an output. We demonstrate that QAMAT+ can effectively chain multiple QA-pairs together to answer multi-hop questions in HotpotQA (Yang et al., 2018) and Musique (Trivedi et al., 2021). Such compositional reasoning capability is nonexistent in RePAQ (Lewis et al., 2021).
In summary, we extend the lines of research considering QA pairs as a representation of knowledge and memory-augmented language models by developing a new QA augmented architecture (section 3). When paired with our proposed pretraining strategy (section 4), we address many of the shortcomings of previous QA-indexing based approaches leading to lower sample complexity training and the ability to perform compositional reasoning (section 6).
2 Related Work
2.1 Retriever-Reader Models
Retrieve-and-read models have been widely studied to address knowledge-intensive tasks and achieve state-of-the-art performance on most QA tasks. These methods use two models, one to retrieve from a passage index based on BM25 (Robertson and Zaragoza, 2009), and one to perform reading comprehension on the returned passages (Chen et al., 2017b). More recently, deep retrieval models have gained more popularity to replace traditional string-similarity retriever.
Retriever DPR (Karpukhin et al., 2020) is a widely used supervised approach to achieve better results than BM25 on a large collection of

Figure 1: During pre-training, the encoder ﬁrst encodes the corrupted paragraph and uses the masked token representation to query the memory keys (questions). It then retrieves and integrates the corresponding top-K memory values (answers) to update the encoder hidden states. Finally, the decoder attends to the updated encoder states to predict the corrupted span. For downstream tasks, the input is a question and the output is the answer.

text retrieval tasks (Thakur et al., 2021). Contrastive learning is used to train the deep retriever model to distinguish between annotated positive and mined negative candidates. More recently, ColBERT (Khattab and Zaharia, 2020) has been proposed to integrate more ﬁne-grained late fusion between query and context to improve DPR. Another line of retriever training is based on selfsupervision without requiring annotated data. For example, ICT (Lee et al., 2019), REALM (Guu et al., 2020) and SPIDER (Ram et al., 2021) proposed to optimize the retriever on pseudo retrieval data in either latent or supervised fashion.
Reader Retrieval Augmented Generation (RAG) (Lewis et al., 2020), Fusion-in-Decoder (FiD) (Izacard and Grave, 2021) and End-to-end training of Multi-Document Reader and Retriever (EmDR) (Singh et al., 2021) are proposed to read retrievals to extract or generate answers. These models require a pre-trained retriever and reranker to obtain top-K results, which are fed to the reader to generate the answer. As discussed in section 1, our model provides better interpretability due to atomic knowledge representation. In subsection 5.4, we also demonstrate that our model’s inference speed is 5x faster.
2.2 Question Generation
The problem of question generation (Zhou et al., 2017) has attracted attention from the community in recent years. It has been used for data augmentation (Alberti et al., 2019) to improve current QA systems or to improve retrieval systems (Nogueira et al., 2019). Pan et al. (2021) also demonstrated

that by connecting generated single-hop questions, we can train zero-shot multi-hop question answering systems. Besides QA, it has also been widely used in other domains like evaluating factual consistency of summarization (Eyal et al., 2019; Wang et al., 2020) or enhancing contextualized representation (Jia et al., 2021). Most related to our work is PAQ (Lewis et al., 2021), which aims to generate and use QA pairs as retrieval units for question answering. The efﬁcacy of this data was further veriﬁed when it was used to train DPR, yielding better domain generalization (Og˘uz et al., 2021).
2.3 Memory-Augmented Language Models
Another line of work inspiring ours is end-toend memory-augmented language models, which can jointly train memories that have mostly focused on storing entities (Févry et al., 2020), entity mentions (Dhingra et al., 2019; Sun et al., 2021; de Jong et al., 2022) or knowledge triples (Verga et al., 2021). Memory attention layers are then used to inﬂuence the computation of transformer layers. These entities and fact-centric memories are naturally atomic and interpretable, and models employing them have shown competitive performance on entity-focused QA datasets like Web-Question-SP (Yih et al., 2016) and ComplexWebQuestions (Talmor and Berant, 2018). However, these models are limited to integrating entitycentric knowledge and classifying the answer w.r.t a pre-deﬁned entity list. For example, these models cannot handle questions with non-entity answers, e.g. number, date, noun phrases, etc, which are ubiquitous in various QA datasets like

NQ (Kwiatkowski et al., 2019), SQuAD (Rajpurkar et al., 2016), or HotpotQA (Yang et al., 2018).
3 Basic Model: QAMAT
3.1 Problem Deﬁnition
The input to our model is a piece of text X = x1, · · · , xn where x1 is a special [CLS] token; X is either a question during ﬁne-tuning or a paragraph in pre-training. Pre-training is formulated as a span corruption task (Raffel et al., 2019): given an example in the pre-training corpus as (X, {Qk, Ak}mk=1), where A1, · · · , Am correspond to spans in the input X. We sample k spans from X as a cloze answer and replace all tokens within a span with a [MASK] token. Consequently, there are multiple [MASK] tokens in the corrupted text X and the model aims to recover all the corrupted spans as Y = [M ASK_1], A1, · · · , [M ASK_k], Ak.
The pre-training/ﬁne-tuning objective function is to maximize the masked language model objectives p(Y |X) = mi∈M p(Y |X, mi)p(mi|X), which marginalizes over the entire memory M . However, due to its intractability in a large-scale memory, we adopt an approximation to only sum over the top-K memory entries T opK(M ).
3.2 Notation Deﬁnition
We deﬁne the encoder function as fθ, which takes an input sequence X as input to generate a sequence of vector Fθ(X) ∈ Rn×d, where n is the input length and d is the hidden size. The designated position of Fθ(X) will be used as the query and memory representation, which are denoted as fθ(X; [M ASK]) ∈ Rd (at [MASK] position) and memory key/value as fθ(mki ; [CLS]) ∈ Rd (at [CLS] position). For brevity, we leave out [MASK] and [CLS] and simply use fθ(·).
We also deﬁne a broadcast operator Bkn(x) to broadcast a vector into a matrix by assigning the vector x to k-th row while ﬁlling the rest with zero, i.e. Bkn(x) = [0, ...xT , ..., 0].
3.3 Dense Retriever
The memory M contains separate key and value components, where the key mki contains a question, and the corresponding value mvi contains the question-answer concatenation. To retrieve the topk QA-pairs from the memory, we use our encoder fθ to encode X and mi separately and select the top-K entries T opK(M ) based on their inner prod-

uct as follows:
T opK (M ) = T opKmi∈M fθ(X) · fθ(mki )
We use the transformer encoder in T5 (Raffel et al., 2019) as our fθ and train it jointly with the encoderdecoder in subsection 3.4 in a latent fashion.
3.4 Encoder-Decoder Memory Integration After the model retrieves the Top-K candidates, their corresponding memory values mvi will be incorporated into the encoder to inﬂuence the decoder outputs. We write our objective p(Y |X) as:

p(Y |X, mi)p(mi|X)

mi∈T opK (M )

=

p(mi|X)gθ(Y |Fθ(X) + Bkn[fθ(mvi )])

mi∈T opK (M )

≈gθ(Y |

p(mi|X)(Fθ(X) + Bkn[fθ(mvi )]))

mi∈T opK (M )

=gθ(Y |Fθ(X) + Bkn[

p(mi|X)fθ(mvi )])

mi∈T opK (M )

p(mi|X) =

efθ (X)·fθ (mki ) e mi∈T opKM fθ (X)·fθ (mki )

The probability p(Y |X, mi) is parmeterized by a decoder function gθ, which takes a memory-infused encoder representation Fθ(X) + Bkn[fθ(mvi )] as input. We approximate this marginal probabil-
ity by pulling weighted summation inside the de-
coder function gθ to derive an aggregated memoryinfused encoder representations Fθ(X) + Bkn[· · · ]. The retrieval weight p(m|X) is calculated as the
softmax over the retrieval score over top-K items.
For simplicity, H(X, T opK(M ), p(m|X)) is used to denote this encoder representation, thus
the objective can be written as follows:

p(Y |X) = gθ(Y |H(X, T opK (M ), p(m|X))) (1)

3.4.1 Retrieval of Neural or Discrete Memory
Similar to prior work (Févry et al., 2020; Verga et al., 2021), we constructed the encoder representation based on the weighted-sum of the retrieval representation mi p(mi|X)fθ(mvi ) and the encoder state Fθ(X), shown in the upper part of Figure 2. The advantage of this approach is that the retrieval scores p(m|X) are directly leveraged into the computation graph, making the framework end-to-end differentiable. This contrasts with other prior work (Lee et al., 2019) in which the discrete tokens of retrieved documents are used.

Figure 2: Architecture: encoder encodes both the query and the memory key. The retrieved memory values are aggregated to form the hybrid representation, which the decoder ground on to generate output Y .

Figure 3: Two stage training procedure. Left: in-batch training with a batch-speciﬁc memory and end-to-end gradient updates. Right: global training with a single global memory and partial gradient updates.

A disadvantage of adopting weighted-sum i p(mi|X)fθ(mvi ) ∈ Rd is that all the information from all of the top-K documents are
overly compressed into a d-dimension vector,
whereas the token retrieval representation con-
tains more information. Therefore, we propose
to add a ﬁne-grained token-wise representation Hˆ (X, T opK(M )) to help the model access the retrieved value mi directly. The token-wise representation is obtained by encoding the concatenation of the input X and retrieval Xˆ =
Concat[mk; · · · ; m1; X]:

Hˆ (X, T opK (M )) = Fθ(Xˆ )

(2)

Hˆ (X, T opK (M )) ∈ R(n+k|m|)×d greatly enriches the token-level representation for mi and enables cross-attention between the query and retrieval, addressing the bottleneck problem. However, since token-wise representation cannot propagate gradients back to the retriever (since retrieval scores p(m|X) does not exist in the computation graph), we retain the original representation H to jointly enable latent retriever training. Formally,

we rewrite our objective function as follows:
p(Y |X) =gθ(Y |H0(X, T opK (M ), p(m|X)) + λHˆ (X, T opK (M )))
where the λ is the balancing factor to weight the two representations. We use H0 = [0; H] to represent the concatenation of zero-matrix 0 ∈ Rk|m|×d, which has consistent dimension with Hˆ . After leveraging Hˆ , our model demonstrates signiﬁcant improvements on the downstream tasks with 14% on TriviaQA and 10% on HotpotQA.
H(X, T opK(M ), p(m|X)) is only used to latently train the retriever, after training, we can drop it and only use the concatenated representation Hˆ (X, T opK(M )) as the encoder representation. The decoder gθ will attend to Hˆ and perform a greedy search over the vocabulary to generate output Y = y1, · · · , yn token by token.
3.5 Model Design Analysis
In document-based retrieve-read models (Izacard and Grave, 2021; Lewis et al., 2020), the reader needs to deal with large retrieval unit mi (i.e. 200 sub-word tokens each), which makes it impossible to perform cross-attention across top-K retrievals

due to the quadratic attention complexity. Thus these models need to encode the retrievals individually with multiple passes. In contrast, QAMAT deals with a much shorter QA-pair as the retrieval unit (i.e. 40 tokens each), which makes it possible to accommodate top-30 retrievals as the input to the cross-attention reader with a single pass. Such design not only unleashes the potential of crossattention but also accelerates the inference.
4 Training

Figure 4: Question generation pipeline: Answers are extracted from passages and then questions are generated conditioned on that contextualized answer. This procedure is used to generate both our model’s QA memory and our pre-training data.
4.1 Pre-training Corpus
Our QA-pairs are constructed by combining 30M deduplicated QA-pairs from PAQ (Lewis et al., 2021)(originally 65M, we delete paraphrases to keep a subset) and 30M additional QA-pairs generated from our own pipeline. The additional QApairs are populated from non-overlapping passage blocks to increase the knowledge coverage over Wikipedia. Our QA generation pipeline is similar to (Lewis et al., 2021) but trained solely on SQuAD 2.0 (Rajpurkar et al., 2018) and ﬁltered with a cheap reading comprehension model rather than FiD (Izacard and Grave, 2021), the details are described in the Appendix. The ﬁnal statistics of our QA-memory is described in Table 1, where the total size is comparable to RePAQ.

Memory Size #Passages Training Data

Dedup-PAQ 30M 10M NaturalQuestions

Additional 30M 10M

SQuAD 2.0

Combined 60M 20M

-

Table 1: The breakdown statistics of our QA corpus. We denote the entire memory as M and formu-

late the pre-training corpus as {X, {Qk, Ak}mk=1}, where X is the passage aligned with multiple QApairs {Qk, Ak}mk=1 generated from it.
4.2 End-to-End Training
During training, the retrieval process is integrated into the model’s training loop. The most widely adopted approach to accomplish this is approximate nearest neighbor search (ANNS) efﬁciently implemented by several libraries like ScaNN (Guo et al., 2020), FAISS (Johnson et al., 2019), etc. These libraries require a ﬁxed set of dense vectors to construct the index and perform a NearestNeighbor search using approximate algorithms. However, our memory encoder fθ is continuously updated, which poses great challenges for ANNS index building. REALM (Guu et al., 2020) and RePAQ (Lewis et al., 2021) use an asynchronous index building sub-process to refresh the index every K steps, which is known to be extremely computationally expensive, especially with a large memory. To avoid such expensive computation overhead, we are inspired by TOME (de Jong et al., 2022) to adopt a two-stage pre-training as shown in Figure 3.
4.3 In-Batch Pre-training
In the ﬁrst stage, instead of using the whole memory, we propose a batch-speciﬁc memory that concatenates the positive, random negative, and hard negative entries from each instance in the batch. Assuming we have a batch size of B containing examples {Xi, {Qki , Aki }Kk=1}Bi=1. For each example there exist K positive QA-pairs generated from the given context Xi. Additionally, we mine K hard negative QA-pairs {Q¯ki , A¯ki }Kk=1 for each input Xi to increase retrieval difﬁculty. This hard negative mining is done with BM25 (Robertson and Zaragoza, 2009) similar to DPR (Karpukhin et al., 2020). We construct the in-batch memory by aggregating the K × B positive QA-pairs and K × B hard negative memory entries, so the inbatch memory Mˆ contains a total of 2K × B QApairs (roughly a few thousand). Due to the small size of the memory, we can construct the memory index very efﬁciently. Thus, it enables us to continuously update the memory encoder parameters fθ to achieve strong QA-pair retrieval performance.
4.4 Global Pre-training and Fine-Tuning
In this stage, we ﬁrst freeze the memory encoder fθ to generate memory-key embedding for the entire memory to build its index. We then incorporate the

on-device approximate search algorithm1 to perform the nearest-neighbor search over the memory index to retrieve the top-K QA-pairs. Formally, we propose to maximize the following objective:
p(Y |X) = gθ(Y |H0(X, T opK (M ), SG(p(m|X))) + λHˆ (X, T opK (M )))
where SG is stop-gradient operator over p(m|X). In this step, the model will only update the query model fθ and the decoder model gθ. During ﬁnetuning, we follow the same recipe as the global pre-training. Instead of feeding masked passages as inputs, we use questions with pseudo [MASK] token in the front as the input.
5 Single-Hop QA Experiments
5.1 Implementation Details
Our model is based on the T5-base or large architecture implemented in JAX2 and pre-trained on 32 TPUs on Google Cloud3. During in-batch training, our query and index encoder fθ are shared and initialized from the T5 encoder (during global training the index encoder is ﬁxed and the query encoder continues to be updated). Our decoder gθ is similarly initialized from the T5 decoder. In total, we construct ∼ 60M question-answer pairs as the global memory. The memory key is the question tokenized by T5 sentencepiece model into 32 tokens, and the memory value is the answer concatenated with its question tokenized into 40 tokens. The memory is indexed by a pre-computed matrix M k ∈ R|M|×d computed based on its keys (questions). The corresponding top-K memory values (question+answer) will be fetched.
During in-batch pre-training, we use a large batch size of 512 and a learning rate of 1e-3, where each example contains a positive Q-A pair and 7 hard negative QA-pairs mined through BM25 (Robertson and Zaragoza, 2009). The inbatch memory contains a total of 4096 entries, we set Top-k of 4 and update over all the modules. After 100K steps of in-batch pre-training, we switch to global pre-training with global memory retrieval. We decrease the batch size to 32 and enlarge Top-K to 16 for larger memory. We update only the query
1https://github.com/google-research/ language/tree/master/language/ mentionmemory
2https://github.com/google-research/ t5x
3https://cloud.google.com/tpu/

encoder and decoder for another 100K steps. Finally, we set K to 32 to ﬁne-tune on downstream datasets with a decreased learning rate of 5e-4.
5.2 Datasets
We ﬁrst evaluate our framework on the three most widely used single-hop open-domain questionanswering datasets including:
NQ-Open The NaturalQuestions (Kwiatkowski et al., 2019) dataset consists of naturally occurring Google queries and their answers. We follow Lee et al. (2019) to keep questions that have a "short answer type". It consists of 79168 training examples, 8757 dev examples, and 3610 test examples.
TriviaQA The TriviaQA dataset is a collection of trivia question-answer pairs that were scraped from the web (Joshi et al., 2017). We use their unﬁltered version to evaluate our model consisting of 78785 training, 8837 dev, and 113313 test examples.
WebQuestions The WebQuestion dataset contains questions that were sampled from Google Suggest API (Berant et al., 2013). The answers are annotated from FreeBase, the training set contains 3417 examples, the dev set contains 361 examples, and the test set contains 2032 examples.
5.3 Baselines
We compare our model with baselines from the following categories. 1) CBQA large language models (T5 XXL), which directly outputs an answer without retrieval. 2) Entity/KG memory-augmented models that use memory attention to incorporate entity-level features into language models (Entitiesas-Experts (EaE) (Févry et al., 2020), Fact-Injected Language Model (FilM) (Verga et al., 2021), MentionMemory (TOME) (de Jong et al., 2022)). 3) Retrieve-and-read model, which retrieves passages to pass to a reader model which predicts the answer. 4) QA-retrieval models, which train a retriever to collect QA-pairs from a large datastore, and then rerank these QA-pairs (top 50-100) with original query with cross-attention. The highest-ranked answer is returned as the ﬁnal answer.
5.4 Main Results
Our results are summarized in Table 2 which reports exact-match (EM) score.
Comparison with RePAQ Our main comparison is with the previous best QA-retrieval-based approach "RePAQ w/ rerank (XXL ALBERT)". This

model has a similar number of parameters to QAMAT (Large). Without using an explicit re-ranking procedure, our model performs slightly worse on NQ but obtains signiﬁcant gains on TriviaQA and WebQuestion. Especially on WebQuestion, which only contains 3K training examples, RePAQ performs signiﬁcantly worse than the other datasets because it requires a high volume of examples to update the retriever from scratch. With our proposed pre-training strategy, QAMAT can initialize from a much better checkpoint to decrease the sample complexity, yielding an absolute 6% EM improvement. Additionally, without any ﬁne-tuning, we demonstrate that our model already achieves promising results across these datasets, nearly matching the performance of "RePAQ w/o rerank"4.
Comparison with retrieve-and-read models In comparison to this class of model, QAMAT roughly matches the performance of RAG, though it still lags behind the SoTA model FiD. However, FiD requires reading 100 passages, i.e. 20K tokens while our best model works more efﬁciently by only reading top-32 QA-pairs, i.e. 1.2K tokens. To investigate the speed difference between these approaches, we compared their inference speeds using the same hardware (32 Google Cloud v3 TPUs). We found that QAMAT can answer 240 Qs/sec, while FiD only answers 50 Qs/sec, a 5x inference time speedup over FiD.
5.5 Ablation Studies
Number of Retrievals To understand the properties of our model better, we ﬁrst investigate the impact of the number of retrievals, K, on the model’s performance. We gradually increase the K from 1 to 30 and collect the recall (EM overlap of any retrieved answers with the gold answer)5 and ﬁnal QA performance (EM of predicted answer to gold answer). The results are shown in Figure 5 where we observe that even though retrieval recall continues to increase beyond K > 30, the ﬁnal EM score saturates much earlier. Future research could improve performance further by developing methods that enable the decoder to more accurately
4It’s worth noting that the question generation models are trained using some of these datasets’ training data so this is not truly “zero-shot” performance.
5This type of recall calculation is upper-bound as there is no guarantee that the retrieved question is relevant to the query even if the answers match directly. ie: The unrelated questions “Where is the movie The Perfect Storm based?” and “Where is the movie Coda based?” have the same answer: Gloucester, MA USA.

Model (Test Set)
T5-3B (Roberts et al., 2020) T5-11B (Roberts et al., 2020)
EaE (Févry et al., 2020) FILM (Verga et al., 2021) TOME-2 (de Jong et al., 2022)
DensePhrases (Lee et al., 2021) REALM (Guu et al., 2020) DPR (Karpukhin et al., 2020) RAG-Seq (Lewis et al., 2020) FiD (Izacard and Grave, 2021)

NQ TQA WQ

30.4 35.1 33.6 32.6 42.3 37.2

-

43.2 -

-

29.1 -

-

53.4 -

40.9 50.7 40.4 55.8 40.7 41.5 57.9 42.4 44.5 56.8 45.2 48.2 65.0 -

RePAQ (Lewis et al., 2021)

41.2 38.8

RePAQ w/ Rerank (Lewis et al., 2021) 47.6 50.7

QAMAT Zero-Shot (Base) QAMAT Zero-Shot (Large) QAMAT Fine-tuned (Base) QAMAT Fine-tuned (Large)

37.9 34.1 39.8 40.0 44.5 53.2 45.5 54.8

29.4† 37.6†
25.9 25.1 43.0 43.6

Table 2: The main experimental results on single-hop question answering datasets (NQ=NaturalQuestions, TQA=TriviaQA, WQ=WebQuestions), † means Besteffort replication using our own implementation.

exploit these larger sets of retrievals.

Figure 5: The retrieval recall and EM score of different retrieval numbers on test sets.
Size of Pre-training Corpus Next, we investigate the impact of the size of the pre-training corpus. As a baseline, we repurpose the aligned querypassage corpus used to train DPR (Karpukhin et al., 2020) which we adapt to our setting by simply reversing the pairs to match our pre-training (ie: the model’s input is a marked passage and it attempts to retrieve the associated query from its memory). This corpus only contains 100K passages aligned with 120K realistic queries from NQ/TQA/WQ, making it signiﬁcantly smaller than our generated corpus. Additionally, we take subsets of our generated pre-training corpus (from 1M to 20M instances) to see its impact on the model’s ﬁnal downstream performance. From Table 3, we can see that

the smaller-sized pre-training corpus can drastically reduce the model’s performance, with up to a 5% drop seen on TriviaQA.

Pre-train Examples NQ TQA WQ

120K

42.5 48.2 39.7

1M

42.8 48.8 40.2

5M

43.8 51.5 41.7

10M

44.3 52.1 42.5

20M

44.5 53.2 43.0

Table 3: Impact of pre-training corpus size on ﬁnal downstream EM performance. The upper portion is pre-trained using the DPR-reverse corpus described in subsection 5.5 and the lower portion uses subsets of our generated pre-training corpus (subsection 4.1)

Importance of Two-Stage Pre-training We next analyze the importance of the two-stage pretraining from section 4 by removing either the inbatch or global stage. From our results shown in Table 4, we can see that using in-batch pre-training alone leads to a degradation in performance when compared to the two-stage approach. This is likely because the model is never exposed to the full set of hard negatives which will be encountered when performing retrieval over the global memory. On the other hand, if we directly pre-train the globalmemory model without any in-batch initialization, the retriever performance is nearly random and the decoder consequently learns to ignore the retrieval and simply memorize question-answer pairs.

Pre-training Stages NQ TQA WQ

In-Batch Global Both

42.1 48.2 39.7 26.0 28.9 26.1 44.5 53.2 43.0

Table 4: Downstream EM performance of models when pre-trained using in-batch, global, or both stages.

that the model is gradually learning to incorporate retrieved information retrievals to assist prediction.
6 Extended Model: QAMAT+
To further extend QAMAT’s capability to perform compositional reasoning, we propose a cascaded architecture (depicted in Figure 7), where the model learns to perform multiple rounds of retrieval before feeding the augmented inputs to the decoder. Speciﬁcally for two-hop reasoning, we use X as the query to retrieve a ﬁrst-round of top-K memory values T opK(M ; 1) with our learned retriever fθ described in subsection 3.3. Next, we augment the query by concatenating the retrieved values as X1 = [T opK(M ; 1); X]. This new query X1 is used to perform a second round of retrieval to obtain additional top-K memory values, T opK(M ; 2). Based on T opK(M ; 2), we compute the hybrid encoder representation H(X1, T opK(M ; 2)) and Hˆ (X1, T opK(M ; 2)) to compute p(Y |X1; θ).
Since the retrieval augmentation process cannot be learned latently, i.e. the gradient propagation is blocked in the concatenation step, we add additional supervision to maximize the ﬁrst-round retrieval objective p(m1|X). Formally, we optimize a combined objective function as follows:
argmaxθ[logp(m1|X; θ) + logp(Y |X1; θ)]; where p(Y |X1; θ) =gθ(Y |H(X1, T opK (M ; 2)) + λHˆ (X1, T opK (M ; 2)))
However, since there exist no annotated QA-pair m1 in the existing datasets, we use lexical-matchbased heuristics to mine the most likely ones as our silver training signal m1 to optimize the ﬁrst round retrieval p(m1|X; θ). The mining algorithm is depicted in Appendix.

Size of Memory Finally, we look at how big of memory we need to reach optimal downstream accuracy and how the model behaves with a smaller memory. As is shown in Figure 6, having a small memory of less than 5M entries does not improve over a model with no memory at all. Due to the lack of coverage, the model does not receive a useful signal from the retrieval and is subsequently not incentivized to utilize those retrievals when making a prediction. However, once the size of the memory increases beyond 15M we observe a steep increase in the ﬁnal performance, indicating

Figure 6: The impact of memory size on downstream QA EM performance.

Figure 7: QAMAT+: the query X will iteratively retrieve from memory for augmentation, and then the augmented query X1 will feed to QAMAT to decode the results.

6.1 Multi-Hop QA Experiments
We evaluate QAMAT+’s multi-hop performance on two popular QA datasets:
HotpotQA The HotpotQA dataset contains questions generated by human workers by reading two passages (Yang et al., 2018). The questions are designed to require multiple hops and include both bridge questions and comparison questions. The training set contains a total of 90564 examples, the dev-set contains 7405 examples for evaluation.
Musique The Musique dataset contains questions created by composing multiple questions from existing single-hop questions and was constructed to contain less bias and artifacts (Trivedi et al., 2021). In our experiments, we consider only the subset of 2-hop questions, resulting in a training set of 14376 examples and a dev set of 1252 examples for evaluation. While the dataset was originally designed as a distractor setting (given a question and a small number of passages, return the answer), we instead consider an open-domain setting.
Setup Since the document corpora source of these two datassets are different, we use our question generation model trained on SQuAD 2.0 (Rajpurkar et al., 2018) to generate questions from the provided documents in these two datasets. To create the document corpora, we gather all of the provided positive and negative documents, obtaining 500K passages for HotpotQA and 85K passages for Musique. We then use the trained generation models to populate 3M QA pairs for HotpotQA and 500K QA pairs for Musique. These QA-pairs are then used as the memory source for QAMAT+, simulating a (slightly smaller) open-domain setup. When training QAMAT+ on Musique, we initialize from HotpotQA’s in-Batch pre-trained checkpoint, which can bring 5-7% F1 improvement.
In Table 5, we show that QAMAT+ achieves promising results on both multi-hop datasets, out-

Model (Dev Set F1 Score)
T5-3B (Roberts et al., 2020) T5-11B (Roberts et al., 2020)
MDR+T5-Decoder (Xiong et al., 2020) RePAQ (Lewis et al., 2021)†
QAMAT QAMAT+

HoPoQ
27.8 30.2
62.6 47.8
42.0 57.6

MusQ
7.5 9.0
26.8 18.6
16.7 29.8

Table 5: The main experimental results on MultiHop QA datasets with QAMAT and QAMAT+, † means Best-effort replication using our own implementation.

performing T5-CBQA and RePAQ by a large margin. Additionally, QAMAT+ performs considerably better than the single-hop QAMAT, demonstrating the effectiveness of performing multiround retrieval. Though QAMAT+ still lags behind the document-based model (MDR+T5 Decoder) on HotpotQA, it surpasses it on the more challenging Musique dataset. These encouraging results suggest the potential for QAMAT+ to perform compositional reasoning over QA-pairs and open the door for future research.
7 Conclusion
In this paper, we aim at proposing a more accurate and efﬁcient architecture to utilize QA-pairs as representation units of knowledge. Our proposed model QAMAT outperforms RePAQ signiﬁcantly while utilizing our proposed less expensive training procedure. Furthermore, we show how a QA-backed model can be used to perform compositional reasoning and address more complex queries. In the future, we hope to further close the gap with state-of-the-art document-based retrieve-and-read models and extend this approach to a broader range of tasks beyond open-domain Question Answering.

References
Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic qa corpora generation with roundtrip consistency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168– 6173.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2021. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017a. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870– 1879.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017b. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870– 1879, Vancouver, Canada. Association for Computational Linguistics.
Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William Cohen. 2022. Mention memory: incorporating textual knowledge into transformers through entity mention attention. International Conference on Learning Representations.
Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, and William W Cohen. 2019. Differentiable reasoning over a virtual knowledge base. In International Conference on Learning Representations.
Matan Eyal, Tal Baumel, and Michael Elhadad. 2019. Question answering as an automatic evaluation metric for news article summarization. In Proceedings of NAACL-HLT, pages 3938–3948.
Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4937–4951.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887–3896. PMLR.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3929–3938. PMLR.
Gautier Izacard and Édouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880.
Robin Jia, Mike Lewis, and Luke Zettlemoyer. 2021. Question answering infused pre-training of generalpurpose contextualized representations. arXiv preprint arXiv:2106.08190.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769– 6781.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations.
Omar Khattab and Matei Zaharia. 2020. Colbert: Efﬁcient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39–48.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466.
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2021. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6634–6647.

Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474.
Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. Paq: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:1098–1115.
Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palomaki, et al. 2021. Neurips 2020 efﬁcientqa competition: Systems, analyses and lessons learned. In NeurIPS 2020 Competition and Demonstration Track, pages 86–111. PMLR.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. arXiv preprint arXiv:1904.08375.
Barlas Og˘uz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Wen-tau Yih, Sonal Gupta, et al. 2021. Domain-matched pretraining tasks for dense retrieval. arXiv preprint arXiv:2107.13602.
Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, and William Yang Wang. 2021. Unsupervised multi-hop question answering by question generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5866–5880.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2021. Learning to retrieve passages without supervision. arXiv preprint arXiv:2112.07708.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426.
Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc.
Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-to-end training of multi-document reader and retriever for opendomain question answering. Advances in Neural Information Processing Systems, 34.
Haitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan Salakhutdinov, and William W Cohen. 2021. Reasoning over virtual knowledge bases with open predicate relations. In International Conference on Machine Learning, pages 9966–9977. PMLR.
Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641–651.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2021. Musique: Multihop questions via single-hop question composition. arXiv preprint arXiv:2108.00573.
Pat Verga, Haitian Sun, Livio Baldini Soares, and William Weston Cohen. 2021. Adaptable and interpretable neural memory over symbolic knowledge. In Proceedings of NAACL-HLT, pages 3678–3691.
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020.
Jinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung Bui, Tong Sun, and Jiawei Han. 2021. Open-domain question answering with pre-constructed question

spaces. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 61–67.
Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela, et al. 2020. Answering complex open-domain questions with multihop dense retrieval. In International Conference on Learning Representations.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380.
Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 201–206.
Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017. Neural question generation from text: A preliminary study. In National CCF Conference on Natural Language Processing and Chinese Computing, pages 662–671. Springer.

A Question Answer Pairs as Knowledge Base
We can see QA-pairs as a virtual knowledge graph, where the question template deﬁnes the relation, the topic entity in the question deﬁnes the head entity node, and the answer denotes the tail entity. A typical example is given in Figure 8, such compositionality makes the QA-pair more controllable and easy to reason over than documents.

jpurkar et al., 2016) could be contextualized or ambiguous, which could lead to ambiguity problems to hurt the retrieval performance. Therefore, we add question ﬁltering to select the most accurate QA pairs.
Question Filtering For the question ﬁltering model, we take the document + generated question to generate an answer. We compare the predicted answer vs. the original answer to see if they match each other. If not, the QA-pair will be ﬁltered based on such inconsistency. We use a reading comprehension model trained with SQuAD to predict the answer. The predicted answer based on the document will match with the original QA-pair to decide its consistency. Such an option runs much faster, providing much higher recall but lower precision compared to the open-domain FiD ﬁltering used in (Lewis et al., 2021).

Figure 8: QA pairs can be seen as virtual knowledge base, where the question can represent complex relations connecting subject and answer.
B Question Generation
Here, we use existing SQuAD datasets’ <Q, A, Document> triples (Rajpurkar et al., 2016) to train answer extraction, question generation model.
Answer Extraction Speciﬁcally, our answer extraction model takes a document as the input and trains an encoder-decoder model to generate a potential answer. We use beam search over the trained model to ﬁnd the highest-likely answers in the given document. In our experiment, the answer extraction model is trained with SQuAD dataset, where the document is given as the input, and the answer spans are the prediction targets.
Question Generation For the question generation model, we take the SQuAD dataset and use document + extracted answer as the input to generate questions as the outputs. This step is also accomplished by an encoder-decoder model. which is mainly purposed for reading comprehension problems, where the annotated questions are highly correlated with the document containing very few hallucinations. However, the questions in SQuAD (Ra-

C Memory Variants
We experiment with two variants of memory to see their performance difference.
PAQ memory The ﬁrst version is the standard PAQ corpus (Lewis et al., 2021) containing 65M QA pairs, where these QA-pairs are generated by models trained on NQ (Kwiatkowski et al., 2019) and ﬁltered through FiD model (Izacard and Grave, 2021) also trained on NQ (Kwiatkowski et al., 2019). This memory is highly precise due to ODQA-ﬁltering process, however, it only covers information from 9M out of the 20M passage blocks used in DPR (Karpukhin et al., 2020).
Our memory Our memory contains 30M PAQ corpus being de-duplicated, i.e. only one question corresponds to an answer span. We generate 30M additional QA-pairs based on the left-out 10M documents from PAQ (Lewis et al., 2021) and add these complementary QA-pairs to form our 60M memory to increase the coverage. However, since our ﬁltering procedure is based on reading comprehension, the precision of QA-pairs is lower than the original PAQ memory.

Memory NQ TriviaQA WebQuestions

PAQ 65M 44.7 48.0

39.4

Ours 60M 44.5 53.2

43.0

Table 6: Impact of different memory over the downstream QA dataset performance.

Figure 9: We ﬁrst ﬁnd the ﬁnal question based on answer string matching with the pre-generated question, and then base on that to trace back the intermediate question.
As can be seen from Table 6, using the most precise but low-coverage PAQ memory from PAQ (Lewis et al., 2021) yields the worse results on TriviaQA and WebQuestions. After adding an additional 30M PAQs to the memory generated by our pipeline, we are able to achieve 4-5% improvements on these two datasets while still maintaining NQ’s performance.
D MultiHop QA Training
In order to train the multi-hop QA model, we need to have intermediate supervision for the query augmentation process. Here we use a string-based match to derive what are the most possible intermediate questions from a collection of pre-generated QA pairs. We depict the mining process as Figure 9.

