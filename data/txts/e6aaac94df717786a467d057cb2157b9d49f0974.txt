Proceedings of Machine Learning Research vol 134:1–35, 2021

34th Annual Conference on Learning Theory

arXiv:2104.12761v2 [cs.GT] 16 Oct 2021

Adaptive Learning in Continuous Games: Optimal Regret Bounds and Convergence to Nash Equilibrium

Yu-Guan Hsieh

YU-GUAN.HSIEH@UNIV-GRENOBLE-ALPES.FR

Univ. Grenoble Alpes, Inria, LJK, 38000 Grenoble, France

Kimon Antonakopoulos

KIMON.ANTONAKOPOULOS@INRIA.FR

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France

Panayotis Mertikopoulos

PANAYOTIS.MERTIKOPOULOS@IMAG.FR

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France, & Criteo AI Lab

Editors: Mikhail Belkin and Samory Kpotufe

Abstract
In game-theoretic learning, several agents are simultaneously following their individual interests, so the environment is non-stationary from each player’s perspective. In this context, the performance of a learning algorithm is often measured by its regret. However, no-regret algorithms are not created equal in terms of game-theoretic guarantees: depending on how they are tuned, some of them may drive the system to an equilibrium, while others could produce cyclic, chaotic, or otherwise divergent trajectories. To account for this, we propose a range of no-regret policies based on optimistic mirror descent, with the following desirable√properties: i) they do not require any prior tuning or knowledge of the game; ii) they all achieve O( T ) regret against arbitrary, adversarial opponents; and iii) they converge to the best response against convergent opponents. Also, if employed by all players, then iv) they guarantee O(1) social regret; while v) the induced sequence of play converges to Nash equilibrium with O(1) individual regret in all variationally stable games (a class of games that includes all monotone and convex-concave zero-sum games).
1. Introduction
A fundamental problem at the interface of game theory and online learning concerns the exact interplay between static and dynamic solution concepts. On the one hand, if all players know the game and are assumed to be rational, the most relevant solution concept is that of a Nash equilibrium: this represents a stationary state from which no player has an incentive to deviate. On the other hand, this knowledge is often unavailable, so players must adapt to each other’s actions in a dynamic manner; in this case, the standard ﬁgure of merit is an agent’s regret, i.e., the cumulative difference in performance between an agent’s trajectory of play and the best action in hindsight. Optimistically, one would expect that the two approaches should yield compatible answers – and, indeed, one direction is clear: Nash equilibrium never incurs any regret. Our paper deals with the converse question, namely: Does no-regret lead to Nash equilibrium?
This question has attracted considerable interest in the literature and the answer can be particularly nuanced. To provide some context, it is well known that the empirical frequency of no-regret play in ﬁnite games converges to the set of coarse correlated equilibria (CCE) – also known as the game’s Hannan set [18, 19]. This is sometimes interpreted as a “universal equilibrium convergence” result, but one needs to keep in mind that a) the type of convergence involved is not the actual, day-to-day
© 2021 Y.-G. Hsieh, K. Antonakopoulos & P. Mertikopoulos.

HSIEH ANTONAKOPOULOS MERTIKOPOULOS
play but the empirical frequency of play; and b) the game’s CCE set may contain elements that fail even the most basic rationalizability axioms. In particular, Viossat and Zapechelnyuk [46] constructed a simple two-player game (a variant of rock-paper-scissors with a feeble twin) that admits CCE supported exclusively on strictly dominated strategies.
This interplay becomes even more involved because the behavior of a no-regret learning algorithm could switch from convergent to non-convergent by a slight variation of its hyperparameters or a small perturbation of the game. As a simple example, optimistic gradient methods are known to converge to Nash equilibrium in smooth convex-concave games, provided that they are tuned appropriately. However, if the algorithm’s step-size is out-of-tune even by a little bit, the trajectory of play could diverge and the players’ mean behavior could converge to an irrelevant off-equilibrium proﬁle (we provide a concrete example of this behavior in Section 3). Equally pernicious examples can be found in symmetric 2 × 2 congestion games: even though such games have a very simple equilibrium structure (a unique, evolutionarily stable equilibrium), running a no-regret learning algorithm – like the popular multiplicative weights update scheme – may lead to chaos [10, 11, 40].
Our contributions and related work. In view of all this, the equilibrium convergence properties of no-regret learning depend crucially on the algorithm’s tuning – and the parameters required for this tuning could be beyond the players’ reach. With this in mind, we propose a range of no-regret policies with the following desirable properties:
1. They do not require any prior tuning or knowledge of the game’s parameters: each player updates their individual step-size with purely local, individual gradient information. √
2. They guarantee an order-optimal O( T ) regret bound against adversarial play, and they further enjoy constant social regret when all players employ one of these algorithms.
3. In any continuous game with smooth, convex losses, the sequence of chosen actions of any player converges to best response against convergent opponents.
4. If all players follow one of these algorithms, the induced trajectory of play converges to Nash equilibrium and the individual regret of each player is bounded as O(1) in all variationally stable games – a large class of games that contains as special cases all convex-concave zero-sum games and monotone / diagonally convex games.
To the best of our knowledge, the proposed methods – optimistic dual averaging (OptDA) and dual stabilized optimistic mirror descent (DS-OptMD) – are the ﬁrst in the literature that concurrently enjoy even a subset of these properties in games with continuous action spaces. To achieve this, they rely on two principal ingredients: a) a regularization mechanism as in the popular “follow the regularized leader” (FTRL) class of policies [6, 44]; and b) a player-speciﬁc adaptive step-size rule inspired by [42]. In this regard, they resemble the policy employed by Syrgkanis et al. [45] who established comparable individual/social regret guarantees for ﬁnite games. Our results extend the analysis of Syrgkanis et al. [45] to games with continuous – and possibly unbounded – action spaces, and, as a pleasing after-effect, they also shave off all logarithmic factors.
Concerning the convergence behavior of optimistic mirror descent (OptMD), it is known that the sequence of realized actions converges to a Nash equilibrium in all variationally stable games, provided that every player runs the algorithm with a sufﬁciently large regularization parameter,
2

ADAPTIVE LEARNING IN CONTINUOUS GAMES

common across all players [21, 35].1 This result cannot be attained by “vanilla” ﬁrst-order methods that do not include an extra-gradient mechanism, but it also comes with several important caveats. First, running OptMD with a constant step-size robs the algorithm of any fallback guarantees: a player’s individual regret may grow linearly if the other players switch to an adversarial behavior (e.g., as part of a “grim trigger” strategy). Second, the method’s convergence is contingent on the players’ using a ﬁne-tuned regularization parameter, depending on the smoothness modulus of their payoff functions. This constant cannot be estimated without prior, global knowledge of the game’s primitives, and if a player misestimates it, the algorithm’s convergence breaks down completely (see Fig. 1 in Section 3).
In terms of trajectory convergence of adaptive methods, the closest antecedents of our results are the recent papers by Lin et al. [29] and Antonakopoulos et al. [2, 3], where the authors propose an adaptive step-size rule for cocoercive games and variational inequalities respectively. However, in both cases, the method’s step-size requires global gradient information, and therefore does not apply to a fully distributed game-theoretic setting.

2. Online learning in games
In this section, we present the necessary background material on normal form games with continuous action spaces and the corresponding learning framework.

2.1. Games with continuous action spaces

Deﬁnitions and examples. Throughout the paper, we focus on normal form games played by a

ﬁnite set of players N := {1, . . . , N }. Each player i ∈ N is associated with a closed convex action

set X i ⊆ Rdi and a loss function i : X → R, where X :=

N i=1

X

i

denotes

the

game’s

joint

action

space. For the sake of clarity, a joint action of multiple players will be typeset in bold; in particular,

the joint action proﬁle of all players will be written as x = (xi, x−i) = (xi)i∈N , where xi and x−i

respectively denote the action of player i and the joint action of all players except player i.

Our blanket assumption concerning the players’ loss functions is the following:

Assumption 1 (Individual convexity + Smoothness). For each i ∈ N , i is continuous in x and
convex in xi – that is, i(·, x−i) is convex for all x−i ∈ j=i X j. Furthermore, the subdifferential ∂i i of i relative to xi admits a Lipschitz continuous selection V i on X .

In the sequel, we will refer to any game that satisﬁes Assumption 1 as a (continuous) convex game. For the sake of concreteness, we brieﬂy discuss below two examples of such games.

Example 1 (Mixed extensions of ﬁnite games). In a ﬁnite game, each player i ∈ N has a ﬁnite

set Ai of pure strategies and no assumptions are made on the loss function i :

N i=1

Ai

→

R.

A

mixed strategy for player i is a probability distribution xi over their pure strategies, so the player

plays k with probability xik (i.e., the k-th coordinate of xi).2 In this case, X i = ∆(Ai) is the set of mixed strategies, the expected loss at a mixed proﬁle is given by i(x) = Es∼x i(s), and the player’s

1. Strictly speaking, [35] analyzes the Mirror-Prox algorithm, but the same arguments apply to OptMD. On the other hand, several other papers have focused on obtaining convergence rate of OptMD in more speciﬁc settings [20, 28, 47].
2. In a slight abuse of notation, a subscript may denote either time or a coordinate, but this should be clear from the context.

3

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

feedback is the observation of the expected loss Es−i∼x−i[ i(k, s−i)] for all k ∈ Ai. Our blanket assumption holds trivially since the mixed losses are multilinear.

Example 2 (Kelly auctions). Consider an auction of K splittable resources among N bidders

(players). For the k-th resource, let qk and ck denote respectively its available quantity and the entry barrier for bidding on it; for the i-th bidder, let bi and gi denote respectively the bidder’s budget and

marginal gain from obtaining a unit of resources. During play, each bidder submits a bid xik for each

resource k with the constraint

K k=1

xik

≤

bi.

Resources

are

then

allocated

to

bidders

proportionally

to their bids, so the i-th player gets ρik = qkxik/(ck +

N i=1

xik )

units

of

the

k-th

resource.

The

utility

of player i ∈ N is given by ui(x) = Kk=1(giρik − xik), and the loss function is i = −ui.

Nash equilibrium. In terms of solution concepts, the most widely used notion is that of a Nash equilibrium, i.e., a strategy proﬁle from which no player has incentive to deviate unilaterally. Formally, a point x ∈ X is a Nash equilibrium if for all i ∈ N and all xi ∈ X i, i(xi , x−i) ≤ i(xi, x−i). For posterity, we will write X for the set of Nash equilibria of the game; by a famous theorem of Debreu [15], X is always nonempty if X is compact.

2.2. Regret minimization

In the multi-agent learning model that we consider, players interact with each other repeatedly via a
continuous convex game. In more detail, during each round t of the process, each player i selects an action xit from their action set X i and suffers a loss i(xt), where xt = (xit)i∈N is the joint action proﬁle of all players. At the end of each round, the players receive as feedback a subgradient vector

gti

= V i(xt) ∈ ∂i

i

(x

i t

,

x

− t

i

),

(1)

and the process repeats. We will also write V = (V i)i∈N for the joint feedback operator. In this low-information setting, the players have no knowledge about the rules of the game,
and can only improve their performance by “learning through play”. It is therefore unrealistic to assume that players can pre-compute their component of an equilibrium proﬁle; however, it is plausible to expect that rational players would always seek to minimize their accumulated losses. This criterion can be quantiﬁed via each player’s individual regret, i.e., the difference between the player’s cumulative loss and the best they could have achieved by playing a given action from a compact comparator set Pi ⊆ X i.
Following Shalev-Shwartz [44], we deﬁne the regret relative to a set of competing actions as

T

Reg

i T

(P

i

)

=

max

pi∈Pi t=1

i(xit, x−t i) −

i

(p

i

,

x

−i t

)

.

Likewise, for P :=

N i=1

Pi

⊆

X

,

we

deﬁne

the

social

regret

by

aggregating

over

all

players,

viz,

N

NT

RegT (P) = RegiT (Pi) = max

p∈P

i=1

i=1 t=1

i(xit, x−t i) −

i

(p

i

,

x

−i t

)

.

In this context, a sequence of play xit of player i incurs no individual regret if RegiT (Pi) = o(T ) for every (compact) set of alternative strategies; correspondingly, xt incurs no social regret if
RegT (P) = o(T ).

4

ADAPTIVE LEARNING IN CONTINUOUS GAMES

In certain classes of games, the growth rate of the social regret can be related to the empirical mean of the players’ social welfare [45]. However, beyond this “aggregate” criterion, having no regret does not translate into any tangible guarantees for the quality of “day-to-day” play [46]. On that account, we will measure the optimality of xit at a given stage t by the gap function

∆iPi (xt) =

i(xit, x−t i) − min

i

(p

i

,

x

−i t

)

,

pi ∈P i

i.e., the best that the player could have gained by switching to any other strategy in Pi at round t. When Pi = X i and ∆iX i(xt) ≤ ε for every i ∈ N , we recover the deﬁnition of an ε-equilibrium.

3. Optimistic mirror descent and its failures

The OptMD template. Our focal point in the sequel will be the optimistic mirror descent (OptMD)
class of algorithms, which, under different assumptions, has been shown to enjoy optimal regret
minimization guarantees [9, 41, 42]. To deﬁne it, assume that each player i ∈ N is equipped with a regularizer hi : X i → R, i.e., a continuous, strongly convex function whose subdifferential ∂hi admits a continuous selection ∇ hi. Then, given a sequence of feedback signals (gti)t∈N (deﬁned in (1) with the notation g0i = 0), the i-th player plays an action xit = Xti+ 1 via the update rule
2

Xti = arg min gti−1, x + λit−1Di(x, Xti−1),
x∈X i

Xti+ 1 = arg min

g

i t−1

,

x

+ λitDi(x, Xti),

2

x∈X i

(OptMD)

where

Di(p, x) = hi(p) − hi(x) − ∇ hi(x), p − x p ∈ X i, x ∈ dom ∂hi,

denotes the Bregman divergence of hi and λit is a player-speciﬁc regularization parameter (more

details on this below). We also stress that, although (OptMD) produces two iterates per step, only one

is actually played and directly contributes to the received feedback – namely, gti = V i(Xti+ 1 , x−t i). 2

Two of the most widely used instances of (OptMD) are

the past extra-gradient (PEG) and optimistic multiplicative 8

last

weights update (OMWU) algorithms, obtained respectively by

average

the quadratic regularizer hi(x) = x 22/2 and the negentropy

function hi(x) =

di k=1

xk

log

xk .

For

a

detailed

discussion,

4

see [13, 17, 20, 31, 42] and references therein.

Failures of OptMD. As we mentioned in the introduction,
the convergence of (OptMD) is only guaranteed as long as the players’ regularization parameter λit has been suitably ﬁnetuned – speciﬁcally, as long as it is sufﬁciently large relative to
the smoothness modulus of the players’ loss functions. How-
ever, this tuning is contingent on a degree of coordination and global knowledge of the game that is often impractical: if λit is not chosen properly, (OptMD) may – and, in fact, does –
fail to converge.
We illustrate this failure in the simple min-max game 1(θ, φ) = θφ = − 2(θ, φ). In this case, if both players run

0

−4

−4

0

4

8

Figure 1: The trajectories of play and its time-average when running PEG for minθ∈[−4,8] maxφ∈[−4,8] θφ w√ith constant stepsize η = 0.7 > 1/ 3. Neither of the two converges to the unique Nash equilibrium at (0, 0).

5

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

√ the PEG instance of (OptMD) with λ > 3, the sequence of play conv√erges to the game’s√unique Nash equilibrium. However, if the players misestimate the critical value 3 and choose λ < 3, the method no longer converges to equilibrium, in either the “ergodic” or “trajectory/last-iterate” sense (for a proof, see e.g., [48]). Moreover, as we show in Fig. 1, this “off-equilibrium” behavior persists even if we restrict the players’ actions to a compact set: in fact, not only does the method fail to converge to equilibrium, its average actually converges to an irrelevant action proﬁle (an artifact of the trajectory’s divergence). This makes such failures particularly spurious and difﬁcult to detect: even though the algorithm stabilizes, the players’ regret continues to accrue at a linear rate.
A simple remedy to the abo√ve would be to run (OptMD) with an increasing regularization schedule, e.g., of the form λit ∝ t. In some cases, this could indeed stabilize the algorithm and ensure convergence, but at a much slower rate – in terms of both regret minimization and convergence speed. An alternative would be to employ an adaptive schedule in the spirit of [42] (see Section 4 for details), but even this is not enough: as was shown by Orabona and Pál [39], when the “Bregman diameter” DX := [2 supp,x D(p, x)]1/2 of X is unbounded, mirror-based methods with an increasing regularization parameter may – and often do – lead to superlinear regret.3 This “ﬁnite Bregman diameter” condition rules out both MWU on the simplex and gradient descent in unbounded domains, and it is the ﬁrst requirement that we relax in the next section.

4. Optimistic averaging, adaptation, and stabilization
4.1. Optimistic dual averaging Viewed abstractly, the failures described above are due to the following aspect of (OptMD):

With an increasing schedule for λt, new information enters (OptMD) with a decreasing weight.

From a learning viewpoint, this behavior is undesirable because it gives more weight to earlier, uninformed updates, and less weight to more recent, more relevant ones (so, mutatis mutandis, an adversary could push the algorithm very far from an optimal point in the starting iterations of a given window of play). To account for this disparity, we build on an idea originally due to Nesterov [38], and introduce the optimistic dual averaging (OptDA) method as:

t−1

Xti = arg min gsi , x + λithi(x),
x∈X i s=1

Xti+ 1 = arg min

g

i t−1

,

x

+ λitDi(x, Xti).

2

x∈X i

(OptDA)

In contrast to (OptMD), the base state Xt of (OptDA) is produced by aggregating all feedback
received with the same weight (the ﬁrst line in the algorithm); subsequently, each player selects an action xit = Xti+ 1 after taking a “conservatively optimistic” step forward (this one with a decreasing
2
step-size, for reasons of stability). As we will show, this different aggregation architecture plays a
crucial role in overcoming the “ﬁnite Bregman diameter” limitation of (OptMD).

3. The precise result of [39] concerns mirror descent; however, it is straightforward to adapt their argument to show that, for example, the PE√G variant of (OptMD) run on X = R against the sequence gti = (−1) (2t−1)/T imposes Ω(T 3/2) regret for both t and adaptive regularization schedules.

6

ADAPTIVE LEARNING IN CONTINUOUS GAMES

From a design perspective, the core elements of (OptDA) are a) the choice of “learning rate” parameters λit (which now acts both as a regularization weight and as an inverse step-size); and b) the choice of regularizer hi, which deﬁnes the “mirror map” Qi : y → arg maxx∈X i y, x − hi(x) that determines the update of the base state Xti of (OptDA). We discuss both elements in detail in the remainder of this section.

Remark (Optimistic FTRL). Another closely related algorithm is the optimistic variant of “follow the regularized leader” (OptFTRL) [1, 36, 45], whose updates follow the recursion

Xi 1 = arg min

t+ 2

x∈X i

t−1
gsi + gti−1, x
s=1

+ λithi(x).

(OptFTRL)

Compared to (OptDA), (OptFTRL) aggregates all the relevant feedback, including gti−1 directly in the dual space. In this way, there is no need to deﬁne Xti, which acts as an auxiliary state to produce the actual iterate Xi 1 in both (OptMD) and (OptDA). Nonetheless, while the regret bounds
t+ 2
presented in Section 5 can also be obtained for adaptive variants of (OptFTRL), the fact that all
updates are performed in the dual space prevents us from proving the last-iterate convergence results
of Section 6.

4.2. Learning rate adaptation √
Since running the algorithm with a t learning rate schedule is, in general, too pessimistic, we will consider an adaptive policy in the spirit of Rakhlin and Sridharan [42], namely

λit =

τi +

t−1 s=1

δti

where δti = gti − gti−1 2(i),∗.

(Adapt)

In the above, τ i > 0 is a player-speciﬁc constant that can be chosen freely by each player, and · (i),∗ : y → max x (i)≤1 y, x is the dual norm of · (i), itself a norm on Rdi. Intuitively, in the favorable case (e.g., when the environment is stationary), the increments δti will eventually vanish, so the policy (Adapt) will be a proxy for the “constant step-size” c√ase. By contrast, in a non-favorable / adversarial setting, we have δti = Θ(1) and λit grows as Θ( t), which makes the algorithm robust.
We should also note here that (Adapt) involves exclusively player-speciﬁc quantities, and its computation only makes use of information that is available to each player locally. This is not
always the case for other adaptive learning rates considered in the game-theoretic literature, e.g., as
in [2, 3, 29]. Even though this “local information” desideratum is very natural, very few algorithms
with this property have been analyzed in the game theory literature.

4.3. Reciprocity and stabilization
In the aggregation step of (OptDA), the mirror map Qi maps a dual vector back to the primal space to obtain Xti. For this reason, to analyze the players’ sequence of play, we will make use of the Fenchel coupling, a “primal-dual” distance of measure ﬁrst introduced in [5, 31, 32]. To deﬁne it, let (hi)∗ be the Fenchel conjugate of hi, i.e., (hi)∗(y) = maxx∈X i y, x − hi(x). The Fenchel coupling induced by hi between a primal point p ∈ X i and a dual vector y ∈ Rdi is then deﬁned as
F i(p, y) = hi(p) + (hi)∗(y) − y, p .

7

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

One key property of the Fenchel coupling is that F i(p, y)

≥

(1/2)

Qi(y) − p

2 (i)

for some norm

· (i) on X i. Therefore, it can be used to measure the convergence of a sequence. In particular,

Qi(Yti) → pi whenever F i(pi, Yti) → 0. For several results concerning the trajectory convergence

of the algorithm, it will also be convenient to assume the converse, that is

Assumption 2 (Fenchel reciprocity [33]). For any i ∈ N , pi ∈ X i, and (Yti)t∈N a sequence of dual vectors such that Qi(Yti) → pi, we have F i(pi, Yti) → 0.

Given the similarity between the Fenchel coupling and the Bregman divergence (which we discuss in detail in Appendix A), Fenchel reciprocity may be regarded as a primal-dual analogue of the more widely used Bregman reciprocity condition [7, 26].
Assumption 2 (Bregman reciprocity). For any i ∈ N , pi ∈ X i, and (Xti)t∈N a sequence of primal points such that Xti → pi, it holds Di(pi, Xti) → 0.

It can be veriﬁed that Bregman reciprocity is indeed implied by Fenchel reciprocity, but the opposite is generally not true. For example, when hi is the quadratic regularizer, Bregman reciprocity always holds while Fenchel reciprocity is only guaranteed when X i is a polytope.
In this regard, it is desirable to devise an algorithm with the same regret guarantees as OptDA
while only requiring the less stringent Bregman reciprocity condition to ensure the convergence of the trajectory. This motivates us to introduce dual stabilized optimistic mirror descent (DS-OptMD), in which player i recursively computes their realized action xit = Xti+ 1 by
2

Xti = arg min
x∈X i

Xi 1 = arg min

t+ 2

x∈X i

g

i t−1

,

x

g

i t−1

,

x

+ λit−1Di(x, Xti−1) + (λit − λit−1)Di(x, X1i ), + λitDi(x, Xti).

(DS-OptMD)

The stabilization step (i.e., the anchoring term that appears in the ﬁrst line of the update) is inspired by Fang et al. [16], and it has been shown to help the algorithm achieve no regret even when the Bregman diameter is unbounded. Moreover, by standard arguments [16, 24, 27, 30], we can show that when the mirror map is interior-valued, i.e., im Qi = ri X i (here ri X i denotes the relative interior of X i), the update of (DS-OptMD) coincides with that of (OptDA).4 One important example which falls into this situation is the (stabilized) OMWU algorithm [13], whose update can be written in a coordinate-wise way as follows

xit,k = Xti+ 1 ,k = 2

exp(−(

t−1 s=1

gs,k

+

gt−1,k )/λit )

di exp(−( t−1 g + g

. )/λi)

l=1

s=1 s,l t−1,l t

(OMWU)

4.4. A template descent inequality
For the results presented in this work, we provide an umbrella analysis for OptDA and DS-OptMD by means of the following energy inequality.

4. Precisely, this requires to set X1 = arg minx∈X i hi(X ) in (DS-OptMD). 8

ADAPTIVE LEARNING IN CONTINUOUS GAMES

Lemma 1. Suppose that player i runs (OptDA) or (DS-OptMD). Then, for any pi ∈ X i, we have

λit+1ψti+1(pi) ≤ λitψti(pi) − gti, Xti+ 1 − pi + (λit+1 − λit)ϕi(pi)

2

(2)

+ gti − gti−1, Xti+ 1 − Xti+1 − λitDi(Xti+1, Xti+ 1 ) − λitDi(Xti+ 1 , Xti),

2

2

2

where:

(i) ψti(pi) = F i(pi, Yti), ϕi(pi) = hi(pi) − min hi for (OptDA). (ii) ψti(pi) = Di(pi, Xti), ϕi(pi) = Di(pi, X1i) for (DS-OptMD).

The proof of Lemma 1 combines several techniques used in the analysis of regularized online learning algorithms and is deferred to Appendix A. As a direct consequence of Lemma 1, we have

T ii

i

i

ii

T

gti − gti−1

2 (i),∗

T λit−1 i

i2

gt, Xt+ 1 − p ≤ λT +1ϕ (p ) + 2

λi

−

8

Xt+ 1 − Xt− 1 (i). (3)

2

2

t=1

t=1

t

t=2

This is very similar to the Regret bounded by Variations in Utilities (RVU) property introduced by Syrgkanis et al. [45], but it now applies to an algorithm with possibly non-constant learning rate (and, of course, to continuous action spaces). By invoking the individual convexity assumption, (3) gives an implicit upper bound on the individual regret of each player. Moreover, (2) relates the distance measure of round t to that of round t + 1. Therefore, we can also leverage Lemma 1 to prove the convergence of the learning dynamics. In Appendix B we explain in detail how this template inequality can be used to derive exactly the same guarantees for other learning algorithms as long as they satisfy a version of (2).

5. Optimal regret bounds
In this section, we derive a series of min-max optimal regret bounds, both when the opponents are adversarial and when all the players interact according to prescribed algorithms. The proofs of our results leverage the template inequality (3) and are deferred to Appendix C.
5.1. Regret guarantees: individual and social
Our ﬁrst result provides a worst-case guarantee for any sequence of play realized by the opponents.
Theorem 2. Suppose that Assumption 1 holds, and a player i ∈ N adopts (OptDA) or (DS-OptMD) with the adaptive learning rate (Adapt). If Pi ⊆ X i i√s bounded and G = supt gti , the regret incurred by the player is bounded as RegiT (Pi) = O(G T + G2).
Theorem 2 is a direct consequence of (3) and the deﬁnition of the adaptive learning rate. It addresses what is traditionally referred to as the adversarial scenario, since we do not make any assumptions on how the opponents’ actions are selected; in particular, they may choose the actions so as to maximize the player’s cumulative loss. Even in this case, Theorem 2 shows that the two adaptive algorithms that we consider would achieve no regret provided that the sequence of feedback is bounded (this is for example the case when X is compact).
We now proceed to show that, if all players adhere to one of the adaptive policies discussed so far, the social regret is at most constant.

9

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

Theorem 3. Suppose that Assumption 1 holds and all players i ∈ N use (OptDA) or (DS-OptMD) with the adaptive learning rate (Adapt). Then, for every bounded comparator set P ⊆ X , the players’ social regret is bounded as RegT (P) = O(1).
The closest result in the literature is that of [45], which proves a constant regret bound for ﬁnite games for all algorithms that satisfy the RVU property. Theorem 3 improves upon this result in two fundamental aspects: First, Theorem 3 applies to any continuous game with smooth and convex losses, not just mixed extensions of ﬁnite games. Second, the proposed policies do not require any prior knowledge about the game’s parameters (such as the relevant Lipschitz constants and the like).
An additional appealing property of our analysis is that, to the best of our knowledge, this is the ﬁrst guarantee that shaves off the logarithmic in T factors in this speciﬁc setting for a method that is robust to adversarial opponents (i.e., Theorem 2). This relies on a careful analysis of (3) with the speciﬁc learning rate (Adapt). We note additionally that, in Theorem 3, the players do not need to use the same regularizer or even the same template algorithm: As a matter of fact, the only requirement for this result to hold is that the players’ sequence of play satisﬁes a version of the inequality (3).

5.2. Individual regret under variational stability

We close this section by zooming in on a class of convex games known as variationally stable:

Deﬁnition 4. A continuous convex game is variationally stable if the set X of Nash equilibria of the game is nonempty and

N

V(x), x − x = V i(x), xi − xi ≥ 0 for all x ∈ X , x ∈ X .

(4)

i=1

The game is strictly variationally stable if (4) holds as a strict inequality whenever x ∈/ X .

A notable family of games that verify the variational stability condition is monotone games (i.e., V is monotone), which includes convex-concave zero-sum games, zero-sum polymatrix games, Cournot oligopolies, and Kelly auctions (Example 2) as several examples. The last two examples satisfy in fact a more stringent diagonal strict concavity condition (Rosen [43]), i.e., the vector ﬁeld V is strictly monotone, which implies the strict variational stability of the game.
Remark. In the literature, the term “variationally stable” frequently signiﬁes what we refer to as “strictly variationlly stable”; this is for example the case in [33], where the concept was ﬁrst introduced.

Under this stability condition, we derive a constant regret bound on the individual regrets of the players when they play against each other using a prescribed algorithm.

Theorem 5. Suppose that Assumption 1 holds and all players i ∈ N use (OptDA) or (DS-OptMD)

with the adaptive learning rate (Adapt). If the game is variationally stable, then, for every bounded

comparator

set

Pi

⊆

X i,

the

individual

regret

of

player

i

∈

N

is

bounded

as

Reg

i T

(

P

i

)

=

O(1).

Theorem 5 extends a range of results previously proved for ﬁnite two-player, zero-sum games for various learning algorithms [14, 25, 42]. It also inherits the appealing attribute of the social regret bound of Theorem 3 – namely, that all logarithmic factors have been shaved off.

10

ADAPTIVE LEARNING IN CONTINUOUS GAMES
The main difﬁculty in the proof of Theorem 5 is to show that the sequence of gradient increments (δti)t∈N is actually summable for all i ∈ N . Equivalently, this implies that each player’s learning rate λit converges to a ﬁnite constant that is automatically adapted to the smoothness landscape of the game. To achieve this, we follow a proof strategy that is similar in spirit to the approach of [3] for solving variational inequalities; however, our setting is considerably more complicated because each player’s learning rate is different.
6. Convergence of the day-to-day trajectory of play
So far, our results have focused on “average” measures of performance, namely the players’ individual and social regret. Even though the derived bounds are sharp, as we discussed in Section 2, they cannot be used to draw meaningful conclusions for the players’ actual sequence of play. Our analysis in this section shows that, in fact, the proposed learning methods actually stabilize to a best response or a Nash equilibrium in a number of relevant cases. The proof details are deferred to Appendix D.
6.1. Convergence to best response against convergent opponents
A fundamental consistency property for online learning in games is that any player should end up “best responding” to the action proﬁle of all other players if their actions stabilize (or are stationary). Formally, a player i ∈ N is said to “converge to best response” if, whenever the action proﬁle x−t i of all other players converges to some limit proﬁle x−∞i ∈ j=i X j, the sequence of actions xit ∈ X i of the focal player i ∈ N converges itself to BR(x−∞i) := arg minxi∈X i i(xi, x−∞i). We establish this key requirement below.
Theorem 6. Suppose that Assumptions 1 and 2 (resp. 2 ) hold, and a player i ∈ N employs (OptDA) (resp. (DS-OptMD)) with the adaptive learning rate (Adapt). If X i is compact, the trajectory of chosen actions of the player in question converges to best response.
Idea of proof. The fact that the opponents are only convergent rather than stationary makes the proof much more challenging and requires a non-standard “trapping” argument.5 Speciﬁcally, we show that when the sequence Xti gets close to a best response (i.e., when minxi ∈BR(x−i) ψti(xi ) ≤ ε for
∞
some ε > 0), all subsequent iterates must remain in this neighborhood provided that t is sufﬁciently large. Subsequently, we also show that the sequence (Xti)t∈N visits any neighborhood of BR(x−∞i) inﬁnitely many times. Therefore, for every neighborhood of BR(x−∞i), the iterates eventually get trapped into that neighborhood, and we conclude by showing Xti+ 1 − Xti (i) converges to zero.
2
As a direct consequence of Theorem 6, we deduce that limt→+∞ ∆iX i(xt) = 0 whenever the opponents’ actions converge. Therefore, the action of the player becomes quasi-optimal as time goes by, in the sense that they would not earn much more by switching to any other strategy in each round.
6.2. Main result: Convergence to Nash equilibrium
Moving forward, we proceed to establish a series of results concerning the convergence of the players’ trajectory of play to Nash equilibrium when all players employ an adaptive learning algorithm.
5. In fact, the compactness assumption in Theorem 6 can be dropped if the opponents are stationary.
11

HSIEH ANTONAKOPOULOS MERTIKOPOULOS
Theorem 7. Suppose that Assumptions 1 and 2 (resp. 2 ) hold and all players i ∈ N use either (OptDA) or (DS-OptMD) (resp. only (DS-OptMD)) with the adaptive learning rate (Adapt). Then the induced trajectory of play converges to a Nash equilibrium provided that either of the following conditions is satisﬁed
a) The game is strictly variationally stable. b) The game is variationally stable and hi is subdifferentiable on all X i, i.e., dom ∂hi = X i.
Idea of proof. The proof of the two cases follow the same schema. We ﬁrst establish that every cluster point of (Xt)t∈N is a Nash equilibrium. This utilizes the fact that λit converges to a ﬁnite constant as shown in the proof of Theorem 5. Then, to prove the sequence actually converges, we leverage the reciprocity conditions discussed in Section 4.1 together with a quasi-Fejér property [12] that we establish for the induced sequence of play relative to a suitable divergence metric.
The convergence to a Nash equilibrium x implies that for every i ∈ N and every compact set Pi ∈ X i, limt→+∞ ∆iPi(xt) = ∆iPi(x ) ≤ 0. Thus, in the long run, the players are individually satisﬁed with their own choices of each play compared to any other action they could have pick from a comparator set. To the best of our knowledge, this is the ﬁrst equilibrim convergence result for online learning in variationally stable games with a player-speciﬁc, adaptive learning rate. The closest antecedent to our result is the recent work of [29] where the authors prove convergence to Nash equilibrium in unconstrained cocoercive games,6 with an adaptive step-size that is the same across player (and which therefore requires access to global information to be computed). In this regard, Theorem 7 extends a wide range of earlier equilibrium convergence results for strictly stable games that were obtained with a constant or diminishing – but not adaptive – step-size.
Despite the generality of Theorem 7, it fails to cover the case where the players are running localized, adaptive versions of OMWU in a game that is variationally stable but not strictly so. The most representative example of this special case is ﬁnite two-player zero-sum games with a mixed equilibrium; we address this case below.
Theorem 8. Suppose that the players of a two-player, ﬁnite zero-sum game follow (OMWU) with the adaptive learning rate (Adapt). Then the induced sequence of play converges to a Nash equilibrium.
The closest results in the literature are [13] and, most recently, [47]. Theorem 8 sharpens these results in two key aspects: i) the players’ learning rate is not contingent on the knowledge of game-speciﬁc constants; and ii) we do not assume the existence of a unique Nash equilibrium.
Finally, following the proof of Theorem 7, we establish below an interesting dichotomy for general convex games with compact action sets (see also Appendix D.3 for a non-compact version).
Theorem 9. Suppose that Assumption 1 holds and all players i ∈ N use (OptDA) or (DS-OptMD) with the adaptive learning rate (Adapt). Assume additionally that X i ⊂ dom ∂hi for every i ∈ N and X is compact. Then one of the following holds: (a) The sequence of realized actions converges to the set of Nash equilibria. Furthermore, for every
i ∈ N , it holds RegiT (X i) = O(1) and lim supt→+∞ ∆iX i(xt) ≤ 0. (b) The social regret tends to minus inﬁnity when t → +∞, i.e., limt→+∞ RegT (X ) = −∞.
6. The class of cocoercive games is deﬁned by the property V(x) − V(z), x − z ≥ (1/β) V(x) − V(z) 2∗.
12

ADAPTIVE LEARNING IN CONTINUOUS GAMES

Action (probability)

0.4 0.3 0.2 0.1 0.0
0
2
1
0 0

1000

2000 3000 # Iterations

4000

5000

player 1 player 2

1000

2000 3000 # Iterations

4000

5000

Individual regret

Action (bid)

1.5 1.0 0.5 0.0
0
800 600 400 200
0 0

player i1 player i2 player i3

2000

4000 6000 # Iterations

2000

4000 6000 # Iterations

8000 10000 8000 10000

Individual regret

Action (probability)

1.0 0.8 0.6 0.4 0.2 0.0
0
0
−50
−100
−150 0

100

200

300

400

500

# Iterations

player 1 player 2 player 3

100

200

300

400

500

# Iterations

Individual regret

Figure 2 - [Illustrative experiments]: The realized actions (top, each line representing a coordinate of xit) and the individual regret (bottom) of a subset of players in a ﬁnite two-player zero-sum game (left), a resource allocation auction (middle), and a three-player matching-pennies game [22] (right). All the players use either adaptive OptDA or adaptive DS-OptMD as their learning strategies. We observe convergence of the realized actions and the regrets in the ﬁrst two examples.

Theorem 9 shows that, if the player’s sequence of actions fails to converge, the social regret goes to −∞; in particular, there is at least one player who beneﬁts more from the actions employed by all other players compared to the regret incurred by all the dissatisﬁed players put together. For this player in question, the individual regret goes to −∞ and the player actually beneﬁts from not converging to a ﬁxed action. We are not aware of any similar result in the literature.

7. Illustrative experiments
In this section we experimentally illustrate our theoretical results through Examples 1 and 2. Precisely, we investigate the following three different setups.
• A ﬁnite zero-sum two-player game with 10 × 10 cost matrix whose elements are drawn uniformly at random from [−1, +1]: We let the two players play DS-OptMD respectively with negative entropy and Euclidean regularizers.7
• A resource allocation auction (Example 2) with 6 resources and 20 bidders: We ﬁx ck = 1, draw qk and gi uniformly at random from [4, 6], and draw bi uniformly at random from [5, 10]. Each player runs either OptDA or DS-OptMD and hi(x) = x 22/2 for all i ∈ N .
• A three-player-matching-pennies game introduced in [22]: Each player has two pure strategies. Player 1 wants to match the pure strategy of player 2; player 2 wants to match the pure strategy of player 3; and player 3 wants to match the opposite of the pure strategy of player 1. Each player receives a loss of −1 if they match as desired, and 1 otherwise. It is straightforward to see that the unique equilibrium is achieved when all the players uniformly randomize. In this game, we let the three players run DS-OptMD with Euclidean regularizer.
7. The convergence of this particular situation can be proved following the proof of Theorem 8.
13

HSIEH ANTONAKOPOULOS MERTIKOPOULOS
As for the learning rates, we ﬁx τ i = 1 and use the Euclidean norm · (i) = · 2 throughout. The results are plotted in Fig. 2. The ﬁrst two games that we consider are variationally stable, and as predicted by our analysis, we observe the convergence of the iterates and the boundedness of the individual regrets. For the three-player-matching-pennies game, all the players oscillate between the two pure strategies, and have their individual regrets tend to minus inﬁnity. This is consistent with our dichotomy result Theorem 9.
8. Concluding remarks
In this work, we have presented a family of adaptive algorithms for online learning in continuous games that solely utilizes local information received by each player. We showed that these algorithms achieve optimal regret bounds under various conditions, and more importantly, lead to Nash equilibrium when employed by all the players in a variationally stable game.
Many interesting questions remain to be answered. For exa√mple, it is known that optimistic algorithms can achieve individual regret much smaller than O( T ) in general-sum ﬁnite games when used by all players [8, 45]. Is this feature shared by our algorithm? Theorem 9 and preliminary experiments suggest that this could be the case. Nonetheless, even this property does not imply that the algorithm effectively generates a ‘good’ sequence of play. In fact, in some cases, the players may beneﬁt more from staying at a Nash equilibrium rather than following a trajectory that lead to −∞ individual regret, and we believe that understanding the dynamics of the algorithm even in the case of no-convergence is an important and challenging direction for future research.
Acknowledgments
This research was partially supported by the COST Action CA16228 “European Network for Game Theory” (GAMENET), and the French National Research Agency (ANR) in the framework of the “Investissements d’avenir” program (ANR-15-IDEX-02), the LabEx PERSYVAL (ANR11-LABX-0025-01), MIAI@Grenoble Alpes (ANR-19-P3IA-0003), and the grants ORACLESS (ANR-16-CE33-0004) and ALIAS (ANR-19-CE48-0018-01).
References
[1] Jacob Abernethy, Kevin A Lai, Kﬁr Y Levy, and Jun-Kun Wang. Faster rates for convex-concave games. In Conference On Learning Theory, pages 1595–1625. PMLR, 2018.
[2] Kimon Antonakopoulos, E. Veronica Belmega, and Panayotis Mertikopoulos. An adaptive mirror-prox algorithm for variational inequalities with singular operators. In NeurIPS ’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019.
[3] Kimon Antonakopoulos, Veronica Belmega, and Panayotis Mertikopoulos. Adaptive extra-gradient methods for min-max optimization and games. In ICLR ’21: Proceedings of the 2021 International Conference on Learning Representations, 2021.
[4] Peter Auer, Nicolo Cesa-Bianchi, and Claudio Gentile. Adaptive and self-conﬁdent on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48–75, 2002.
[5] Mario Bravo and Panayotis Mertikopoulos. On the robustness of learning in games with stochastically perturbed payoff observations. Games and Economic Behavior, 103, John Nash Memorial issue:41–66, May 2017.
[6] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
14

ADAPTIVE LEARNING IN CONTINUOUS GAMES
[7] Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm using Bregman functions. SIAM Journal on Optimization, 3(3):538–543, August 1993.
[8] Xi Chen and Binghui Peng. Hedging in games: Faster convergence of external and swap regrets. 2020. [9] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu.
Online optimization with gradual variations. In COLT ’12: Proceedings of the 25th Annual Conference on Learning Theory, 2012. [10] Thiparat Chotibut, Fryderyk Falniowski, Michał Misiurewicz, and Georgios Piliouras. Family of chaotic maps from game theory. Dynamical Systems, 2020. [11] Thiparat Chotibut, Fryderyk Falniowski, Michał Misiurewicz, and Georgios Piliouras. The route to chaos in routing games: When is price of anarchy too optimistic? In NeurIPS ’20: Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020. [12] Patrick L. Combettes. Quasi-Fejérian analysis of some optimization algorithms. In Dan Butnariu, Yair Censor, and Simeon Reich, editors, Inherently Parallel Algorithms in Feasibility and Optimization and Their Applications, pages 115–152. Elsevier, New York, NY, USA, 2001. [13] Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In ITCS ’19: Proceedings of the 10th Conference on Innovations in Theoretical Computer Science, 2019. [14] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms, pages 235–254. SIAM, 2011. [15] Gérard Debreu. A social equilibrium existence theorem. Proceedings of the National Academy of Sciences of the USA, 38(10):886–893, October 1952. [16] Huang Fang, Nick Harvey, Victor Portella, and Michael Friedlander. Online mirror descent and dual averaging: keeping pace in the dynamic case. In ICML ’20: Proceedings of the 37th International Conference on Machine Learning, pages 3008–3017, 2020. [17] Gauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In ICLR ’19: Proceedings of the 2019 International Conference on Learning Representations, 2019. [18] James Hannan. Approximation to Bayes risk in repeated play. In Melvin Dresher, Albert William Tucker, and P. Wolfe, editors, Contributions to the Theory of Games, Volume III, volume 39 of Annals of Mathematics Studies, pages 97–139. Princeton University Press, Princeton, NJ, 1957. [19] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127–1150, September 2000. [20] Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In NeurIPS ’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 6936–6946, 2019. [21] Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. In NeurIPS ’20: Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020. [22] James S Jordan. Three problems in learning mixed-strategy nash equilibria. Games and Economic Behavior, 5(3): 368–386, 1993. [23] Anatoli Juditsky, Arkadi Semen Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011. [24] Anatoli Juditsky, Joon Kwon, and Éric Moulines. Unifying mirror descent and dual averaging. arXiv preprint arXiv:1910.13742, 2019. [25] Ehsan Asadi Kangarshahi, Ya-Ping Hsieh, Mehmet Fatih Sahin, and Volkan Cevher. Let’s be honest: An optimal no-regret framework for zero-sum games. In ICML ’18: Proceedings of the 35th International Conference on Machine Learning, pages 2488–2496, 2018. [26] Krzysztof C. Kiwiel. Proximal minimization methods with generalized Bregman functions. SIAM Journal on Control and Optimization, 35:1142–1168, 1997.
15

HSIEH ANTONAKOPOULOS MERTIKOPOULOS
[27] Joon Kwon and Panayotis Mertikopoulos. A continuous-time approach to online optimization. Journal of Dynamics and Games, 4(2):125–148, April 2017.
[28] Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In AISTATS ’19: Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics, 2019.
[29] Tianyi Lin, Zhengyuan Zhou, Panayotis Mertikopoulos, and Michael I. Jordan. Finite-time last-iterate convergence for multi-agent learning in games. In ICML ’20: Proceedings of the 37th International Conference on Machine Learning, 2020.
[30] Panayotis Mertikopoulos. Online Optimization and Learning in Games: Theory and Applications. Habilitation à Diriger des Recherches (HDR), Université Grenoble-Alpes, December 2019.
[31] Panayotis Mertikopoulos and William H. Sandholm. Learning in games via reinforcement and regularization. Mathematics of Operations Research, 41(4):1297–1324, November 2016.
[32] Panayotis Mertikopoulos and Mathias Staudigl. On the convergence of gradient-like ﬂows with noisy gradient input. SIAM Journal on Optimization, 28(1):163–197, January 2018.
[33] Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, 173(1-2):465–507, January 2019.
[34] Panayotis Mertikopoulos, Christos H. Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In SODA ’18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms, 2018.
[35] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In ICLR ’19: Proceedings of the 2019 International Conference on Learning Representations, 2019.
[36] Mehryar Mohri and Scott Yang. Accelerating online convex optimization via adaptive prediction. In Artiﬁcial Intelligence and Statistics, pages 848–856, 2016.
[37] Arkadi Semen Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[38] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 109(2):319–344, 2007.
[39] Francesco Orabona and Dávid Pál. Scale-free online learning. Theoretical Computer Science, 716:50–69, 2018. [40] Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update with constant step-
size in congestion games: Convergence, limit cycles and chaos. In NIPS ’17: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017. [41] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In COLT ’13: Proceedings of the 26th Annual Conference on Learning Theory, 2013. [42] Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In NIPS ’13: Proceedings of the 27th International Conference on Neural Information Processing Systems, 2013. [43] J Ben Rosen. Existence and uniqueness of equilibrium points for concave n-person games. Econometrica: Journal of the Econometric Society, pages 520–534, 1965. [44] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107–194, 2011. [45] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in games. In NIPS ’15: Proceedings of the 29th International Conference on Neural Information Processing Systems, pages 2989–2997, 2015. [46] Yannick Viossat and Andriy Zapechelnyuk. No-regret dynamics and ﬁctitious play. Journal of Economic Theory, 148(2):825–842, March 2013. [47] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In ICLR ’21: Proceedings of the 2021 International Conference on Learning Representations, 2021. [48] Guojun Zhang and Yaoliang Yu. Convergence of gradient methods on bilinear zero-sum games. In ICLR ’20: Proceedings of the 2020 International Conference on Learning Representations, 2020.
16

ADAPTIVE LEARNING IN CONTINUOUS GAMES
Appendix A. Proof of Lemma 1
In this appendix, we present several basic properties of the Bregman divergence, the mirror map, and the Fenchel coupling, before proceeding to prove Lemma 1. For ease of notation, the player index will be dropped in the notation. In particular, we will write X and h respectively for the player’s action space and the associated regularizer, and we assume that h is 1-stronlgy convex relative to an ambient norm · .
A.1. Bregman divergence, mirror map, and Fenchel coupling We ﬁrst recall the deﬁnition of the Bregman divergence and the Fenchel coupling,
D(p, x) = h(p) − h(x) − ∇ h(x), p − x , F (p, y) = h(p) + h∗(y) − y, p , where h∗ : y → maxx∈X y, x − h(x) is the Fenchel conjugate of h. We also recall that the mirror map induced by h is deﬁned as
Q(y) = arg min −y, x + h(x).
x∈X
The auxiliary results that we are going to present below concerning these three quantities are not new (see e.g., [23, 33, 37] and references therein); however, the set of hypotheses used to obtain them varies widely in the literature, so we still provide the proofs for the sake of completeness. To begin, our ﬁrst lemma concerns the optimality condition of the mirror map. Lemma 10. Let h be a regularizer on X . Then, for all x ∈ dom ∂h and all y ∈ Rd, we have
x = Q(y) ⇐⇒ y ∈ ∂h(x).
Moreover, if x = Q(y), it holds for all p ∈ X that
∇ h(x), x − p ≤ y, x − p .
Proof. For the ﬁrst claim, we have by the deﬁnition of the mirror map x = Q(y) if and only if 0 ∈ ∂h(x) − y, i.e., y ∈ ∂h(x). For the second claim, it sufﬁces to show it holds for all p ∈ ri X (by continuity). To do so, we can deﬁne
φ(t) = h(x + t(p − x)) − [h(x) + y, x + t(p − x) ].
Since h is strongly convex and y ∈ ∂h(x) by the previous claim, it follows that φ(t) ≥ 0 with equality if and only if t = 0. Moreover, as ri X ⊂ dom ∂h, ∇h(x + t(p − x)) is well-deﬁned and ψ(t) = ∇h(x+t(p−x))−y, p−x is a continuous selection of subgradients of φ. Given that φ and ψ are both continuous on [0, 1], it follows that φ is continuously differentiable and φ = ψ on [0, 1]. Thus, with φ(t) ≥ 0 = φ(0) for all t ∈ [0, 1], we conclude that φ (0) = ∇h(x) − y, p − x ≥ 0, from which our claim follows.
We continue with the “three-point identity” [7] which will be used to derive the recurrent relationship between the divergence measures of different steps.
17

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

Lemma 11. Let h be a regularizer on X . Then, for all p ∈ X and all x, x ∈ dom ∂h, we have

∇ h(x ) − ∇ h(x), x − p = D(p, x ) − D(p, x) − D(x, x ).

(5)

Similarly, writing x = Q(y), for all p ∈ X and all y, y ∈ Rd, we have

y − y, x − p = F (p, y ) − F (p, y) − F (x, y ).

(6)

Proof. We start with the Bregman version. By deﬁnition,

D(p, x ) = h(p) − h(x ) − ∇h(x ), p − x D(p, x) = h(p) − h(x) − ∇h(x), p − x D(x, x ) = h(x) − h(x ) − ∇h(x ), x − x .

The result then follows by adding the two last lines and subtracting the ﬁrst. On the other hand, in order to show the Fenchel coupling version we write
F (p, y ) = h(p) + h∗(y ) − y , p F (p, y) = h(p) + h∗(y) − y, p .

Then, by subtracting the above we obtain
F (p, y ) − F (p, y) = h(p) + h∗(y ) − y , p − h(p) − h∗(y) + y, p = h∗(y ) − h∗(y) − y − y, p = h∗(y ) − y, Q(y) + h(Q(y)) − y − y, p = h∗(y ) − y, x + h(x) − y − y, p = h∗(y ) + y − y, x − y , x + h(x) − y − y, p = F (x, y ) + y − y, x − p

and our proof is complete.

Since x = Q(∇ h(x)) and F (p, ∇ h(x)) = D(p, x), the identity (5) is indeed a special case of (6). In the general case, the Fenchel coupling and the Bregman divergence can be related by the following lemma.

Lemma 12. Let h be a regularizer on P. Then, for all p ∈ P and y ∈ Rd, it holds

F (p, y) ≥ D(p, Q(y)) ≥ Proof. For the ﬁrst inequality we have,

p − Q(y) 2 .
2

F (p, y) = h(p) + h∗(y) − y, p = h(p) − h(Q(y)) + y, Q(y) + y, −p = h(p) − h(Q(y)) − y, p − Q(y)

Since y ∈ ∂h(Q(y)), by Lemma 10 we get

∇h(Q(y)), Q(y) − p ≤ y, Q(y) − p

18

ADAPTIVE LEARNING IN CONTINUOUS GAMES

With all the above we then have
F (p, y) = h(p) − h(Q(y)) − y, p − Q(y) ≥ h(p) − h(Q(y)) − ∇h(Q(y)), p − Q(y) = D(p, Q(y))
and the result follows. The second inequality follows directly from the fact that the regularizer h is 1-strongly convex relative to · .
Remark. From the above proof we see that F (p, y) = h(p) − h(Q(y)) − y, p − Q(y) . Since y ∈ ∂h(Q(y)) by Lemma 10, Fenchel coupling is also closely related to a generalized version of Bregman divergence which is deﬁned for p ∈ X , x ∈ dom ∂h, and g ∈ ∂h(x) by D(p, x; g) = h(p) − h(x) − g, p − x . This deﬁnition is formally introduced in [24], but its use in the literature can be traced back to much earlier work such as [26].
Remark. By using x = Q(∇ h(x)) and F (p, ∇ h(x)) = D(p, x), we see immediately that Bregman reciprocity is implied by Fenchel reciprocity.

A.2. Optimistic dual averaging

We ﬁrst prove Lemma 1 for optimistic dual averaging (OptDA). Its update writes

t−1

Xt = arg min gs, x + λth(x),
x∈X s=1

Xt+ 1 = arg min gt−1, x + λtD(x, Xt).

2

x∈X

(OptDA)

Let us deﬁne Yt = (−1/λt)

t−1 s=1

gs

so

that

Xt

=

Q(Yt).

For any p

∈

X , we can apply the

three-point identity for Fenchel coupling (6) to the update of Xt+1 and get

gt, Xt+1 − p

= λtYt − λt+1Yt+1, Xt+1 − p = λt Yt − Yt+1, Xt+1 − p + (λt+1 − λt) 0 − Yt+1, Xt+1 − p = λt(F (p, Yt) − F (p, Yt+1) − F (Xt+1, Yt)) + (λt+1 − λt)(F (p, 0) − F (p, Yt+1) − F (Xt+1, 0)).

As F (p, 0) = h(p) − h(Q(0)) = h(p) − min h, writing ϕ(p) = h(p) − min h, the above gives

gt, Xt+1 − p ≤ λtF (p, Yt) − λt+1F (p, Yt+1) − λtF (Xt+1, Yt) + (λt+1 − λt)ϕ(p). (7)

As for the update of Xt+ 1 , we note that Xt+ 1 = Q(∇ h(Xt) − gt−1/λt). Therefore, invoking

2

2

Lemma 10 gives

gt−1

∇ h(Xt+ 1 ), Xt+ 1 − p ≤

2

2

∇ h(Xt) −

λt

, Xt+ 1 − p 2

.

For the speciﬁc choice p ← Xt+1, using the three-point identity for Bregman divergence (5) we obtain

gt−1, Xt+ 1 − Xt+1 ≤ λt ∇ h(Xt) − ∇ h(Xt+ 1 ), Xt+ 1 − Xt+1

2

2

2

= λt(D(Xt+1, Xt) − D(Xt+1, Xt+ 1 ) − D(Xt+ 1 , Xt)).

(8)

2

2

19

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

Since F (Xt+1, Yt) ≥ D(Xt+1, Xt) by Lemma 12, combining (7) and (8) leads to

gt, Xt+ 1 − p 2

= gt − gt−1, Xt+ 1 − Xt+1 + gt−1, Xt+ 1 − Xt+1 + gt, Xt+1 − p

2

2

≤ λtF (p, Yt) − λt+1F (p, Yt+1) + (λt+1 − λt)ϕ(p)

+ gt − gt−1, Xt+ 1 − Xt+1 − λtD(Xt+1, Xt+ 1 ) − λtD(Xt+ 1 , Xt).

2

2

2

This proves the generated iterates of OptDA satisfy (2) with ψti = F i(·, Yti) and ϕi = hi − min hi.

A.3. Dual stabilized optimistic mirror descent
We next prove the generated iterates of dual stabilized optimistic mirror descent (DS-OptMD) satisfy (2) with ψti = Di(·, Xti) and ϕi = Di(·, X1i). The algorithm is stated recursively as

Xt+ 1 = arg min gt−1, x + λtD(x, Xt),

2

x∈X

Xt+1 = arg min gt, x + λtD(x, Xt) + (λt+1 − λt)D(x, X1).
x∈X

(DS-OptMD)

By deﬁnition of the Bregman divergence and the mirror map, the second step is equivalent to

Xt+1 = Q λt ∇ h(Xt) + (1 − λt ) ∇ h(X1) − gt .

λt+1

λt+1

λt+1

This shows that the update of Xt+1 consists in fact of a mixing step in the dual space with weight λt/λt+1 followed by a standard mirror descent step. Applying Lemma 10 gives

∇ h(Xt+1), Xt+1 − p ≤

λt ∇ h(Xt) + (1 − λt ) ∇ h(X1) − gt , Xt+1 − p .

λt+1

λt+1

λt+1

We rearrange the terms and use the three-point identity (5) to get

gt, Xt+1 − p ≤ λt ∇ h(Xt) − ∇ h(Xt+1), Xt+1 − p

+ (λt+1 − λt) ∇ h(X1) − ∇ h(Xt+1), Xt+1 − p

≤ λt(D(p, Xt) − D(p, Xt+1) − D(Xt+1, Xt))

+ (λt+1 − λt)(D(p, X1) − D(p, Xt+1) − D(Xt+1, X1))

(9)

Since Xt+ 1 is computed exactly as in (OptDA), inequality (8) still holds. We conclude by putting 2
together (9) and (8)

gt, Xt+ 1 − p 2

= gt − gt−1, Xt+ 1 − Xt+1 + gt−1, Xt+ 1 − Xt+1 + gt, Xt+1 − p

2

2

≤ λtD(p, Xt) − λt+1D(p, Xt+1) + (λt+1 − λt)D(p, X1)

+ gt − gt−1, Xt+ 1 − Xt+1 − λtD(Xt+1, Xt+ 1 ) − λtD(Xt+ 1 , Xt).

2

2

2

This prove Lemma 1 for DS-OptMD.

20

ADAPTIVE LEARNING IN CONTINUOUS GAMES

Appendix B. Adaptive optimistic algorithms

In the remainder of the appendix, we consider a broad family of algorithms which we refer to as “optimistic and compatible with dynamic learning rate”. Given a regularizer h and a sequence of non-decreasing positive numbers (λt)t∈N, an algorithm of this family produces a sequence of iterates (Xs)s∈N/2 satisfying that
1. For some non-negative continuous functions (ψt)t∈N and ϕ deﬁned on X i (the player’s action set), we have, for all p ∈ X i,

λt+1ψt+1(p) ≤ λtψt(p) − gt, Xt+ 1 − p + (λt+1 − λt)ϕ(p) 2

+ gt − gt−1, Xt+ 1 − Xt+1 − λtD(Xt+1, Xt+ 1 ) − λtD(Xt+ 1 , Xt), (10)

2

2

2

where D is the associated Bregman divergence of h.

2. For every t ∈ N, Xt+ 1 is generated by 2

Xt+ 1 = arg min gt−1, x + λtD(x, Xt).

2

x∈X i

By replacing ϕ with max(ϕ, ψ1) if needed, we may assume ψ1 ≤ ϕ without loss of generality. Thanks to Lemma 1, we know that both (DS-OptMD) and (OptDA) are optimistic and compatible
with dynamic learning rate. As another example, it can be proved in a similar way that (OptMD) is optimistic and compatible with dynamic learning rate if supp,x∈X i D(p, x) < +∞. In this case, ψt = D(·, Xt) and ϕ ≡ supp,x∈X i D(p, x).
Since the player’s cost function is convex with respect to its own action by Assumption 1, their regret can be bounded by the linearized regret,8 which, using (10), can be in turn bounded by

T

T

gt, Xt+ 1 − p ≤ λT +1ϕ(p) − λT +1ψT +1(p) + gt − gt−1, Xt+ 1 − Xt+1

2

2

t=1

t=1

T

− λt D(Xt+1, Xt+ 1 ) + D(Xt+ 1 , Xt) .

(11)

2

2

t=1

To further obtain (2), we need to invoke Young’s inequality and the strong convexity of h. More details can be found in the proof of Theorem 3 (Appendix C.2). For those results that require the reciprocity conditions, this translates into the following requirement on ψt(p).

Assumption 3. For some norm · and its associated distance function dist, the sequence (ψt)t∈N satisﬁes
(a) For any t ∈ N, ψt(p) ≥ (1/2) Xt − p 2.
(b) For any compact set K ∈ X i and ε > 0, there exists r > 0 such that if dist(Xt, K) ≤ r then ψt(K) := minp∈K ψt(p) ≤ ε.
8. This argument will be used implicitly throughout the proofs.

21

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

For ψt = D(·, Xt) and ψt = F (·, Yt), Assumption 3(a) is indeed veriﬁed (Lemma 12) and Assumption 3(b) is implied by the corresponding reciprocity condition (this can be proved by using some standard arguments of the point-set topology).
In the sequel, we will restate all our results in the case where players “adopt an adaptive optimistic learning strategy”. This means that the player runs an optimistic algorithm that is compatible with dynamic learning rate with a regularizer hi and the adaptive scheme (Adapt), and plays xit = Xti+ 1 .
2
For ease of presentation, we will take τ i = 1 throughout, and we will assume that hi is 1-strongly convex relative to · (i), but the proof can be easily adapted to general τ i and · (i). It will also be convenient to deﬁne the norm on the joint action space as

N

(xi)i∈N =

xi 2(i).

(12)

i=1

Appendix C. Proofs for regret bounds
C.1. Robustness to adversarial opponent
Theorem 2. Suppose that Assumption 1 holds, and a player i ∈ N adopts an adaptive optimistic learning strategy. If Pi ⊆ X i √is bounded and G = supt gti , the regret incurred by the player is bounded as RegiT (Pi) = O(G T + G2).

Proof. By Young’s inequality and the strong convexity of hi,

gti − gti−1, Xti+ 1 − Xti+1 − λitDi(Xti+1, Xti+ 1 )

2

2

gti − gti−1

2 (i),∗

λit

i

i 2 λit

≤

2λi

+ 2

Xt+ 1 − Xt+1 2

(i) −

2

t

Xti+ 1 − Xti+1 2

2

δti

(i) = 2λi .

t

(13)

From (11) we then obtain

T
gti, Xti+ 1 − pi 2
t=1

i i i 1 T δti ≤ λT +1ϕ (p ) + 2 λi
t=1 t

i i i 1 T δti 1 T 1

1

i

= λT +1ϕ (p ) + 2

λi

+ 2

λi − λi δt

t=1 t+1

t=1 t

t+1

ii

T i

T1

1

2

≤ (1 + ϕ (p )) 1 + δt + 2

λi − λi G

t=1

t=1 t

t+1

≤ (1 + ϕi(pi)) 1 + 4G2T + 2G2

(14)

We have used Lemma 19 in the second to last inequality. We conclude by maximizing the above inequality over pi ∈ Pi.

22

ADAPTIVE LEARNING IN CONTINUOUS GAMES

C.2. Constant bound on social regret
Theorem 3. Suppose that Assumption 1 holds and all players i ∈ N adopt an adaptive optimistic learning strategy. Then, for every bounded comparator set P ⊆ X , the players’ social regret is bounded as RegT (P) = O(1).

Proof. Let p = (pi)i∈N ∈ P. Since P is bounded and ϕi is continuous, there exists M i > 0 such that it always holds ϕi(pi) ≤ M i. We start by rewriting the regret bound (11) as

T
gti, Xti+ 1 − pi 2
t=1

≤ λiT +1ϕi(pi) − λiT +1ψTi +1(pi)

− λi Di(Xi , Xi ) − λiT Di(Xi , Xi )

1

3/2 1

2

T +1 T + 12

T
−
t=2

λit−1 i i i

ii i

i

2

D (Xt , Xt− 1 ) + λtD (Xt+ 1 , Xt )

2

2

T
+
t=1

ii

i

i

λit i i

i

gt − gt−1, Xt+ 1 − Xt+1 2

−

2

D

(Xt+1, Xt+ 1 ) 2

(15)

On one hand, the strong convexity of hi implies

Xi 1 − Xi 1

t+ 2

t− 2

2 (i)

≤

2

Xti+ 1 − Xti

2 (i)

+

2

Xti − Xti− 1

2 (i)

2

2

≤ 4Di(Xti+ 1 , Xti) + 4Di(Xti, Xti− 1 ).

2

2

(16)

On the other hand, similar to (13),

ii

i

i

λit i i

i

gt − gt−1, Xt+ 1 − Xt+1 2

−

2

D

(Xt+1, Xt+ 1 ) 2

≤

gti − gti−1

2 (i),∗

λi

.

t

(17)

Combining (15), (16), (17), we obtain

T
gti, Xti+ 1 − pi 2
t=1

≤ λiT +1ϕi(pi) − λiT +1ψTi +1(pi)

T
+
t=1

gti − gti−1

2 (i),∗

1T i

i

i2

λi

− 8

λt−1 Xt+ 1 − Xt− 1 (i)

2

2

t

t=2

≤ λiT +1M i +

V i(x1)

2 (i),∗

T
+
t=2

V i(xt) − V i(xt−1)

2 (i),∗

λit−1

i

i2

λi

− 8

Xt+ 1 − Xt− 1 (i) .

2

2

t

(18)

In the current setting, the realized joint action is xt = Xt+ 1 . With the norm on X deﬁned in (12),

2

we have

N i=1

Xti+ 1 − Xti− 1

2=

Xt+ 1 − Xt− 1

2. Note that λit ≥ 1 for all t and i by deﬁnition.

2

2

2

2

23

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

Summing (18) from i = 1 to N and maximizing over p ∈ P then gives

N

RegT (P) ≤

λiT +1M i +

V i(x1)

2 (i),∗

i=1

T

N

V i(Xt+ 1 ) − V i(Xt− 1 )

2 (i),∗

1

+

2
λi

2

− 8 Xt+ 12 − Xt− 12 2 .

t=2 i=1

t

(19)

In the remainder of the proof, we show that the right-hand side of (19) is bounded from above by
some constant. Since all the norms are equivalent in a ﬁnite dimensional space, from Assumption 1 we know that for every i ∈ N , there exists Li > 0 such that for all x, x ∈ X ,

V i(x) − V i(x ) (i),∗ ≤ Li x − x .

(20)

Subsequently,

Xt+ 1 − Xt− 1

2

2

2 N1 ≥ N Li2
i=1

V i(Xt+ 1 ) − V i(Xt− 1 )

2

2

2(i),∗.

(21)

It is thus sufﬁcient to show that for each i ∈ N , there exists Ci ∈ R+ such that for all T ∈ N,

i

i

1

T i

i

2

i

λT +1M − 16N Li2

V (Xt+ 1 ) − V (Xt− 1 ) (i),∗ ≤ C ,

2

2

(22)

t=2

T

V i(Xt+ 1 ) − V i(Xt− 1 )

2 (i),∗

1

2
λi

2

− 16N Li2 V i(Xt+ 12 ) − V i(Xt− 12 ) 2(i),∗ ≤ Ci. (23)

t=2

t

To simplify the notation, we will write γi = 1/(16N Li2). We recall that λit =

1+

t−1 s=1

δti

where

δi = gi − gi

2

√

√√

. Using the inequality a + b ≤ a + b, we can bound the left-hand side of

t

t t−1 (i),∗

(22) as following

T

T





T

T

T

M i 1 + δti − γi δti ≤ M i 1 + δ1i + M i

δti − γi δti = f i 

δ

i t



.

(24)

s=1

t=2

s=2

t=2

t=2

where f i : ν ∈ R → −γiν2 + M iν + M i 1 + δ1i is a quadratic function with negative leading

coefﬁcient and is hence bounded from above. This proves (22) by setting Ci ≥ maxν∈R+ f i(ν).

Note that (λit)t∈N is non-decreasing. Therefore, it either converges to some ﬁnite limit or tends to plus inﬁnity. We can thus write limt→+∞ λit = λi ∈ R+ ∪{+∞}. To prove (23), we tackle the two

cases separately:

Case 1, λi ∈ R+: In other words,

+∞ t=2

δti

is

ﬁnite.

Since λit

≥

1, by taking Ci

≥

+∞ t=2

δti

inequality (23) is veriﬁed.

Case 2, λi = +∞: Then limt→+∞ 1/λit = 0. The quantity t = mint{t : 1/λit ≤ γi} is welldeﬁned and the inequality (23) is satisﬁed as long as Ci ≥ tt=−21(1/λit − γi)δti.
To summarize, we have proved that (22) and (23) must hold for some Ci ∈ R+. Therefore, invoking

(19) and (21) we have effectively proved RegT (P) = O(1).

24

ADAPTIVE LEARNING IN CONTINUOUS GAMES

C.3. Individual regret bound in variationally stable games

Lemma 13. Let Assumption 1 holds and that all players i ∈ N adopt an adaptive optimistic learning

strategy. Assume additionally that the game is variationally stable. Then, for every i ∈ N , the

sequence (λit)t∈N converges to a ﬁnite constant λi ∈ R+ (equivalently,

+∞ t=1

δti

<

+∞).

Proof. In this proof we borrow the notations from the proof of Theorem 3. First, summing the

left-hand side of (18) from i = 1 to N leads to

T t=1

V(Xt+ 1 ), Xt+ 1

−p

.

Since

the

game

is

2

2

variationally stable, we may take p ← x ∈ X a Nash equilibrium of the game, which gurantees

that V(x), x − x ≥ 0 for all x ∈ X . Summing (18) from i = 1 to N and using the Lipschitz

continuity of the functions, similar to (19), we obtain

N

0≤

λiT +1ϕi(xi ) +

V i(x1)

2 (i),∗

i=1

T

N

V i(Xt+ 1 ) − V i(Xt− 1 )

2 (i),∗

1

+

2
λi

2

− 8 Xt+ 12 − Xt− 12 2 .

t=2 i=1

t

(25)

Combining (22) and (23) with the above inequality, we deduce that for any i, there exists Ci ∈ R

such that for all T ∈ N,

ϕi(xi )

T

T

1 + δti − γi δti ≥ Ci.

s=1

t=2

Invoking (24) then gives f i

T t=2

δti

≥ Ci. Since f i is a quadratic function with negative

leading coefﬁcient, limν→+∞ f i(ν) = −∞. Accordingly, λi = limt→+∞ λit < +∞.

+∞ t=2

δti

is

ﬁnite,

which

in

turn

implies

Theorem 5. Suppose that Assumption 1 holds and all players i ∈ N adopt an adaptive optimistic

learning strategy. If the game is variationally stable, then, for every bounded comparator set

Pi

⊆

X i,

the

individual

regret

of

player

i

∈

N

is

bounded

as

Reg

i T

(

P

i

)

=

O(1).

Proof. From the ﬁrst line of (14) we have

T ii

i

i i i 1 T δti

gt, Xt+ 1 − p 2

≤ λT +1ϕ (p ) + 2

λi .

(26)

t=1

t=1 t

As ϕi is continuous and Pi is bounded, M i = maxpi∈Pi ϕi(pi) is well-deﬁned. Moreover, 1/λit ≤ 1 for all t. Maximizing (26) over pi ∈ Pi then gives

ii

i

i 1T i

i i 1 +∞ i

RegT (P ) ≤ λT +1M

+ 2

δt ≤ λ M

+ 2

δt ,

t=1

t=1

where λi = limt→+∞ λit and RegiT (Pi) = O(1).

+∞ t=1

δti

are ﬁnite according to Lemma 13.

We have thus proved

25

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

Appendix D. Proofs for last-iterate convergence

D.1. Convergence to best response

In this part, we focus on the learning of a single player when the realized actions of the other players converge asymptotically. For ease of notation, the player index i will be dropped when there is no confusion.

Lemma 14. Let player i adopt an adaptive (optimistic) learning strategy. Then, if the sequence of received feedback is bounded, both the sequences a) (λt+1 − λt)t∈N and b) (δt/λt)t∈N tend to zero.

Proof. This trivially holds if limt→+∞ λt < +∞ (which is equivalent to

+∞ t=1

δt

<

+∞).

Other-

wise, we have λt → +∞. Let G be an upper bound on the received feedback. Since δt ≤ 4G2, we

deduce the sequence b) converges to 0. For the sequence a), we simply note that

λ2t+1 − λ2t

δt

2G2 λt→+∞

λt+1 − λt =

=

≤ −−−−−→ 0.

λt+1 + λt λt+1 + λt λt

Theorem 6. Suppose that Assumption 1 holds, and a player i ∈ N adopts an adaptive optimistic learning strategy that veriﬁes Assumption 3. Assume additionally that X i is compact. Then, if all
other players’ actions converge to a point x−∞i ∈ j=i X j, player i’s realized actions converge to the best response to x−∞i.

Proof. Let xi ∈ X i := BR(x−∞i). From (10) we derive immediately that

λt+1ψt+1(xi ) ≤ λtψt(xi ) + (λt+1 − λt)M + δt λt

− V i(X 1 ), Xi − xi − λt Xi − Xi 2 ,

t+ 2

t+ 12

4 t+1

t+ 12 (i)

(27)

where M = maxxi ∈X i ϕ(xi ). The scalar product term is not necessarily non-negative, but with Xt+ 21 = (Xti+ 12 , x−∞i), x = (xi , x−∞i), and R the diameter of X i, we can decompose

V i(Xt+ 1 ), Xti+ 1 − xi = V i(Xt+ 1 ) − V i(Xt+ 1 ), Xti+ 1 − xi + V i(Xt+ 1 ), Xti+ 1 − xi

2

2

2

2

2

2

2

≥ −R V i(Xt+ 1 ) − V i(Xt+ 1 ) (i),∗ + i(Xt+ 1 ) − i(x ).

2

2

2

(28)

In the inequality we have used the convexity of i(·, x−∞i). Since X i is compact and V i is continuous,

the function

f : x−i → max V i(pi, x−i) − V i(pi, x−∞i) (i),∗
pi∈X i

is continuous by Berge’s maximum theorem. Therefore f (X−i 1 ) converges to 0 when t goes to
t+ 2
inﬁnity. Moreover, from (28) we have

V i(Xt+ 12 ), Xti+ 12 − xi ≥ −Rf (X−t+i 21 ) + i(Xt+ 12 ) − i(x ). (29)

26

ADAPTIVE LEARNING IN CONTINUOUS GAMES

Let us write

i

= minxi∈X i

i

(x

i

,

x

−i ∞

).

Combining (27), (29) and minimizing with respect to

xi ∈ X i leads to

λt+1ψt+1(X i) ≤ λtψt(X i) + (λt+1 − λt)M + δt + Rf (X−i )

λt

t+ 12

− ( i(X 1 ) − i ) − λt Xi − Xi 2 .

(30)

t+ 2

4 t+1

t+ 12 (i)

We deﬁne ζt = (λt+1 − λt)M + δt/λt + Rf (X−i 1 ). As V i is continuous, X i is compact, and the
t+ 2
iterates (x−t i)t∈N converges and is hence bounded, the sequence of feedback received by player i is also bounded. Applying Lemma 14 then gives limt→+∞ ζt = 0.
Let us next prove that for any ε > 0, we have ψt(X i) ≤ ε for all t large enough. Since X i ⊂ X i is a compact set, Assumption 3(b) ensures the existence of r > 0 such that if dist(Xti, X i) ≤ r then ψt(X i) ≤ ε. We distinguish between three different situations:

Case 1, dist(Xti+ 1 , X i) ≥ r/2: By convexity of i(·, x−∞i) this clearly implies the existence c > 0 2
such that i(Xt+ 1 ) − i ≥ c whenever we are in this situation. As limt→+∞ ζt = 0, there exists 2
t1 ∈ N such that for all t ≥ t1, ζt ≤ c/2. For any t ≥ t1, the inequality (30) then gives

λt+1ψt+1(X i) ≤ λtψt(X i) + ζt − c − λt Xi

− Xi

1

2

≤

λtψt(X i)

−

c .

4 t+1

t+ 2 (i)

2

Case 2, dist(Xti+ 1 , X i) ≤ r/2 and Xti+1 − Xti+ 1 (i) ≥ r/2: We deﬁne t2 ∈ N such that for all

2

2

t ≥ t2, ζt ≤ r2/32. Then for t ≥ t2,

i

i

i

i r2

i r2

λt+1ψt+1(X ) ≤ λtψt(X ) + ζt − ( (Xt+ 1 ) − 2

) − 16 ≤ λtψt(X ) − 32 .

Case 3, dist(Xti+ 1 , X i) ≤ r/2 and Xti+1 − Xti+ 1 (i) ≤ r/2: By the triangular inequality this

2

2

implies dist(Xti+1, X i) ≤ r and thus ψt+1(X i) ≤ ε by the choice of r.

Conclude. Let us consider the sequence (ρt)t∈N ∈ (R+)N deﬁned by ρt = λtψt(X i). For t ≥ max(t1, t2), whenever we are in Case 1 or 2, we have ρt+1 ≤ ρt − min(c/2, r2/32). Since (ρt)t∈N

is non-negative, this can not happen for all t ≥ max(t1, t2); this means Case 3 must happen for some

t ≥ max(t1, t2). Note that for both Case 1 and 2 we get ψt+1(X i) ≤ ψt(X i). Therefore, with the

three cases presented above we see that for all t ≥ t + 1 we have ψt(X i) ≤ ε. We have proved

that for any ε > 0, the distance measure ψt(X i) becomes eventually smaller than ε. This means

limt→+∞ ψt(X i) = 0 and accordingly limt→+∞ dist(Xti, X i) = 0 thanks to Assumption 3(a).

We next prove Xti+ 1 − Xti (i) → 0. In (10) we may keep the D(Xti+ 1 , Xti) term, then similar to

2

2

how (30) is derived, we get

λtD(Xti+ 1 , Xti) ≤ λtψt(X i) − λt+1ψt+1(X i) + ζt. 2

This implies

Xi 1 − Xi 2 ≤ 2 ψt(X i) − ψt+1(X i) + ζt .

t+ 2 t (i)

λt

As the right-hand side of the above inequality tends to zero when t goes to inﬁnity, we conclude that

Xti+ 1 − Xti (i) → 0. As a consequence, limt→+∞ dist(Xti+ 1 , X i) = 0.

2

2

27

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

D.2. Convergence to Nash equilibrium

In this part, we show the convergence of the realized actions to a Nash equilibrium when all the
players adopt an adaptive optimistic learning strategy in a variationally stable game. According to Lemma 13, the limit λi = limt→+∞ λit is ﬁnite in this case.

Lemma 15. Let Assumption 1 holds and that all players i ∈ N adopt an adaptive optimistic

learning strategy in a variationally stable game. Then, Xt+ 1 − Xt → 0 and Xt − Xt− 1 → 0

as t → +∞.

2

2

Proof. Let x be a Nash equilibrium. We apply the regret bound (11) to pi ← xi , and sum these bounds for i = 1 to N , with Young’s inequality (17), we get

1T N i i i

i

ii

i

N

i i i +∞ δti

2

λt D (Xt+1, Xt+ 1 ) + D (Xt+ 1 , Xt ) ≤

2

2

λ h (x ) + λi .

t=1 i=1

i=1

t=1 t

(31)

The right-hand side of (31) is ﬁnite by Lemma 13. With strong convexity of hi, this implies

+∞

Xt+1 − Xt+ 1 2 + Xt+ 1 − Xt 2 < +∞.

2

2

t=1

As a consequence, both Xt+ 1 − Xt and Xt − Xt− 1 converge to zero when t → +∞.

2

2

Lemma 16. Let Assumption 1 holds and that all players i ∈ N adopt an adaptive optimistic learning

strategy in a variationally stable game. Then,

N i=1

λiψti(xi

)

converges

for

all

Nash

equilibrium

x ∈X .

Proof. that

Let x

be a Nash equilibrium. From the descent inequality (10), it is straightforward to show

N

N

λit+1ψti+1(xi ) ≤

λitψti(xi ) − V(Xt+ 1 ), Xt+ 1 − x

2

2

i=1

i=1

N i

t ii

δti

+

(λt+1 − λt)ϕ (x ) + 2λi .

i=1

t

By the choice of x , V(Xt+ 1 ), Xt+ 1 − x ≥ 0. On the other hand, thanks to Lemma 13 we know

2

2

that the term on the second line is summable. Therefore, by applying Lemma 20, we deduce the

convergence of

N i=1

λitψti(xi

).

This

in

particular

implies

that

ψti(xi

)

is

bounded

above

for

all

i

and

t; hence

Ni=1(λi − λit)ψti(xi ) converges to zero, and the convergence of

N i=1

λiψti(xi

)

follows

immediately.

Theorem 7. Suppose that Assumption 1 holds and all players i ∈ N adopt an adaptive optimistic learning strategy which veriﬁes Assumption 3. Then the induced trajectory of play converges to a Nash equilibrium provided that either of the following conditions is satisﬁed

a) The game is strictly variationally stable. b) The game is variationally stable and hi is subdifferentiable on all of X i.

28

ADAPTIVE LEARNING IN CONTINUOUS GAMES

Proof. We ﬁrst show that in both cases, a cluster point of (Xt)t∈N is necessarily a Nash equilibrium.

a) Let x∞ be a cluster point of (Xt)t∈N and x be a Nash equilibrium. The point x∞ is also a

cluster point of (Xt+ 1 )t∈N since limt→+∞ Xt+ 1 − Xt = 0. From the proof of Theorem 3, we

2

2

have

T t=1

V(Xt+ 1 ), Xt+ 1

−x

= O(1). As V(Xt+ 1 ), Xt+ 1 − x

≥ 0 for all t, this implies

2

2

2

2

limt→+∞ V(Xt+ 1 ), Xt+ 1 − x = 0. Subsequently, V(x∞), x∞ − x = 0 by the continuity of

2

2

V, which shows that x∞ must be a Nash equilibrium by the strict variational stability of the game.

b) Let x∞ ∈ X be a cluster point of (Xt)t∈N. We recall that Xi 1 is obtained by
t+ 2

Xi 1 = arg min

t+ 2

x∈X i

V i(Xt− 1 ), x + λitDi(x, Xti) . 2

For any pi ∈ X i, the optimality condition Lemma 10 then gives

V i(Xt− 1 ) + λit ∇ hi(Xti+ 1 ) − λit ∇ hi(Xti), pi − Xti+ 1 ≥ 0.

(32)

2

2

2

Let (Xω(t))t∈N be a subsequence that converges to x∞. With limt→+∞ Xt+ 1 − Xt = 0 and 2

limt→+∞ Xt − Xt− 1 = 0 (Lemma 15), we deduce Xω(t)+ 1 → x∞ and Xω(t)− 1 → x∞. Since

2

2

2

both ∇ hi and V i are continuous (∇ hi is a continuous selection of the subgradients of hi) and

X i ⊂ dom ∂hi, by substituting t ← ω(t) in (32) and letting t go to inﬁnity, we get

V i(x∞) + λi ∇ hi(xi∞) − λi ∇ hi(xi∞), pi − xi∞ ≥ 0. In other words, for all pi ∈ X i, it holds that

∇xi i(x∞), pi − xi∞ ≥ 0. Since i is convex in xi by Assumption 1, the above implies

i(pi, x−∞i) ≥ i(x∞).

This is true for all i ∈ N and all pi ∈ X i, which shows that x∞ is indeed a Nash equilibrium.

Conclude. Lemma 16 along with Assumption 3(a) implies the boundedness of (Xt)t∈N. With the above we can readily show that dist(xt, X ) → 0 and lim supt→+∞ ∆iPi(xt) ≤ 0 for all i and every compact set Pi ⊂ X i (xt = Xt+ 1 is the realized action at time t).
2

Below, we further prove the convergence of the iterates to a point using Assumption 3(b) and

Lemma 16. The sequence (Xt)t∈N, being bounded, necessarily possesses a cluster point which we

denote by x∞. We have proved that x∞ must be a Nash equilibrium. Therefore, by Lemma 16 the

sequence

N i=1

λiψti(xi∞)

converges.

In

Assumption

3(b),

we

take

K

←

{xi∞}

and

this

means

that

when Xti is close enough to xi∞, ψti(xi∞) becomes arbitrarily small. Consequently,

N i=1

λiψti(xi∞)

can only converge to zero. By invoking Assumption 3(a), we then get limt→+∞ Xt = x∞, or

equivalently limt→+∞ Xt+ 1 = x∞. 2

29

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

D.2.1. FINITE TWO-PLAYER ZERO-SUM GAMES WITH ADAPTIVE OMWU

We now investigate the speciﬁc case of learning in a ﬁnite two-player zero-sum game with adaptive (OMWU). We consider the saddle-point formulation of the problem. Let us denote respectively by θ ∈ ∆m and φ ∈ ∆n the mixed strategy of the ﬁrst and the second player. A point (θ , φ ) is a Nash equilibrium if for all θ ∈ ∆m and φ ∈ ∆n,

θ Aφ ≤ θ Aφ , θ Aφ ≤ θ Aφ .

(33)

where A is the payoff matrix and without loss of generality we assume A ∞ ≤ 1. We deﬁne v = minθ∈∆m maxφ∈∆n θ Aφ as the value of the game and we will write x[k] for the k-th coordinate of x. A pure strategy αi of player i is called essential if there exists a Nash equilibrium in which player i plays αi with positive probability. We have the following lemma from [34].
Lemma 17. Let A ∈ Rm×n be the game matrix for a ﬁnite two-player zero-sum game with value v. There is a Nash equilibrium (θ , φ ) such that each player plays each of their essential strategies with positive probability, and

∀k ∈/ supp(θ ), (Aφ )[k] > v, ∀l ∈/ supp(φ ), (A θ )[l] < v.

In the following, we will denote by x = (θ , φ ) such an equilibrium. As an immediate consequence, for all k ∈ supp(θ ), (Aφ )[k] = v and for all l ∈ supp(φ ), (A θ )[l] = v. We also deﬁne

ξ = min min (Aφ )[k] − v, v − max (A θ )[l] > 0.

k∈/supp(θ )

l∈/supp(φ )

Moreover,

ξ ≤ mink∈/supp(θ ) (Aφ )[k] − v + v − maxl∈/supp(φ ) (A θ )[l] ≤ Aφ ∞ + A θ ∞ ≤ 1.

2

2

For any θ ∈ ∆m, we denote by
Vθ = {θ ∈ ∆m : supp(θ) ⊂ supp(θ)}.
the set of the points whose support is included in that of θ. For φ ∈ ∆n, Vφ is deﬁned in the same way. The next lemma, extracted from [47], is crucial to our proof. Lemma 18. Let x = (θ, φ) ∈ ∆m × ∆n satisfy that for all (θ, φ) ∈ Vθ × Vφ ,

(θ − θ) Aφ + θ A(φ − φ) ≥ 0.

(34)

Then x = (1 − ξ/2)x + (ξ/2)x is also a Nash equilibrium.

Proof. We rewrite the left-hand side of (34) as

(θ − θ) Aφ + θ A(φ − φ) = θ Aφ − v + v − θ Aφ = θ A(φ − φ ) + (θ − θ) Aφ. (35)

The second inequality holds because (θ, φ) ∈ Vθ × Vφ . With the choice (θ, φ) ← (θ , φ ) and (34) we then get
θ A(φ − φ ) + (θ − θ) Aφ ≥ 0.

30

ADAPTIVE LEARNING IN CONTINUOUS GAMES

This implies

θ A(φ − φ ) = (θ − θ) Aφ = 0

(36)

by the deﬁnition of Nash equilibrium (33). We next prove that (θ , φ ) is also a Nash equilibrium with φ = (1 − ξ/2)φ we already have
θ Aφ = θ Aφ = v = max θ Aφ.
φ∈∆n

+ (ξ/2)φ. From (36)

It remains to show that θ Aφ = minθ∈∆m θ Aφ . By choosing φ = φ in (35), we know that for all θ ∈ Vθ , it holds θ A(φ − φ ) ≥ 0. In other words,

∀k ∈ supp(θ ), (A(φ − φ ))[k] ≥ 0

(37)

Let θ ∈ ∆m. We decompose

θ Aφ =

θ[k](Aφ )[k] +

θ[k](Aφ )[k].

(38)

k∈supp(θ )

k∈/supp(θ )

The ﬁrst term can be bounded below using (37),

θ[k](Aφ )[k] =

k∈supp(θ )

k∈supp(θ )

ξ 2 θ[k](A(φ − φ ))[k] + θ[k](Aφ )[k]

We proceed to lower bound the second term

≥

θ[k]v.

k∈supp(θ )

(39)

θ[k](Aφ )[k] ≥

k∈/supp(θ )

k∈/supp(θ )

≥
k∈/supp(θ )

ξ θ[k](Aφ )[k] − 2 |θ[k](A(φ − φ ))[k]|
ξ θ[k](Aφ )[k] − 2 θ[k] A ∞ φ − φ 1

≥

θ[k]((Aφ )[k] − ξ)

k∈/supp(θ )

≥

θ[k]v.

(40)

k∈/supp(θ )

In the last inequality we use the deﬁnition of ξ. Combining (38), (39), and (40) we have θ Aφ ≥ v = θ Aφ . We have therefore proved that (θ , φ ) is a Nash equilibrium. In the same way we can show that with θ = (1 − ξ/2)θ + (ξ/2)θ, the point (θ , φ ) is also a Nash equilibrium. We then conclude that x = (θ , φ ) is indeed a Nash equilibrium.

In the following we analyse the case where both h1 and h2 are negative entropy regularizers
(i.e., both players play adaptive OMWU). The case where one is negative entropy regularizer and the other satisﬁes that X i ⊂ dom ∂hi can be proved similarly. The Bregman divergence for
the negative entropy regularizer is the KL divergence which we will denote by DKL. We take ∇ h1 : (θ[k])k∈{1,...,m} → (− log θ[k])k∈{1,...,m} and ∇ h2 : (φ[l])l∈{1,...,m} → (− log φ[l])l∈{1,...,n}.

31

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

Theorem 8. Suppose that the players of a two-player, ﬁnite zero-sum game follow (OMWU) with the adaptive learning rate (Adapt). Then the induced sequence of play converges to a Nash equilibrium.

Proof. Consider the solution x = (θ , φ ) that we have chosen using Lemma 17. By Lemma 16 we know that λ1DKL(θ , θt)+λ2DKL(φ , φt) are bounded above. This implies that for all k ∈ supp(θ ) and l ∈ supp(φ ), the coordinates θt,[k] and φt,[l] are bounded below. In particular, for any cluster point x∞ = (θ∞, φ∞), we have supp(θ ) ⊂ supp(θ∞) and supp(φ ) ⊂ supp(φ∞).
We will proceed to prove the sequence of produced iterates only has one cluster point. We ﬁrst use the optimality condition (32) but only apply it to p1 ← θ ∈ V(θ ). This gives

(V 1(Xt− 1 ) + λ1t (log(θt,[k]) − log(θt+ 1 ,[k])))(θ[k] − θt+ 1 ,[k]) ≥ 0

(41)

2 [k]

2

2

k∈supp(θ )

We consider a subsequence that goes to a cluster point x∞ = (θ∞, φ∞). Since θ∞[k] > 0 for all

k ∈ supp(θ ) and both Xt − Xt− 1 and Xt+ 1 − Xt go to zero (Lemma 15), (41) implies

2

2

V 1(x∞)[k](θ[k] − θ∞[k]) ≥ 0.
k∈supp(θ )

Equivalently, (θ − θ∞) Aφ∞ ≥ 0. In the same way, for all φ ∈ V(φ ) we have θ∞A(φ∞ − φ) ≥ 0. We can thus apply Lemma 18 and we know that (θ , φ ) = (1 − ξ/2)x + (ξ/2)x∞ is also a Nash equilibrium. By the choice of x , we have supp(θ ) ⊂ supp(θ ) and supp(φ ) ⊂ supp(φ ). Subsequently, supp(θ ) = supp(θ∞) and supp(φ ) = supp(φ∞).
Using Lemma 16, we can deﬁne

ρ = lim λ1DKL(θ , θt) + λ2DKL(φ , φt)
t→+∞
ρ = lim λ2DKL(θ , θt) + λ2DKL(φ , φt).
t→+∞

Since supp(θ ) = supp(θ∞) and supp(φ ) = supp(φ∞), we can use the continuity of the KL divergence with respect to the second variable and deduce that λ1DKL(θ , θ∞) + λ2DKL(φ , φ∞) = ρ. Similarly, λ1DKL(θ , θ∞) + λ2DKL(φ , φ∞) = ρ . These two equations also hold if we consider
another cluster point x∞ = (θ∞, φ∞). As a consequence,

λ1

θ [k] log θ∞[k] + λ2

φ [l] log φ∞[l]

k∈supp(θ )

l∈supp(φ )

= λ1

θ [k] log θ∞[k] + λ2

φ [l] log φ∞[l],

k∈supp(θ )

l∈supp(φ )

(42)

and

λ1

θ [k] log θ∞[k] + λ2

φ [l] log φ∞[l]

k∈supp(θ )
= λ1

l∈supp(φ )
θ [k] log θ∞[k] + λ2

φ [l] log φ∞[l],

(43)

k∈supp(θ )

l∈supp(φ )

32

ADAPTIVE LEARNING IN CONTINUOUS GAMES

With (θ , φ ) = (1 − ξ/2)x + (ξ/2)x∞ and ξ > 0, using (42) and (43) we get

λ1

θ∞[k] log θ∞[k] + λ2

φ∞[l] log φ∞[l]

k∈supp(θ )

l∈supp(φ )

= λ1

θ∞[k] log θ∞[k] + λ2

φ∞[l] log φ∞[l],

k∈supp(θ )

l∈supp(φ )

Note that we also have supp(θ ) = supp(θ∞) and supp(φ ) = supp(φ∞). The above is thus equivalent to
λ1DKL(θ∞, θ∞) + λ2DKL(φ∞, φ∞) = 0

This shows x∞ = x∞, and therefore (Xt)t∈N has only one cluster point; in other words, the algorithm

converges (recall that limt→+∞ Xt+ 1 − Xt = 0). To conclude, we note that if a no regret learning

2

algorithm converges, it must converge to a Nash equilibrium. In fact, for all θ ∈ ∆m, we have

T t=1

V 1(Xt+ 1 ), θt+ 1

−θ

= o(T ) and thus lim inft→+∞ V 1(Xt+ 1 ), θt+ 1 − θ

≤ 0. However,

2

2

2

2

limt→+∞ V 1(Xt+ 1 ), θt+ 1 − θ = V 1(x∞), θ∞ − θ . This shows V 1(x∞), θ∞ − θ ≤ 0 for all

2

2

θ ∈ ∆m and thus θ∞ is a best response to φ∞. The same argument also applies to the second player;

accordingly, x∞ is indeed a Nash equilibrium.

D.3. A dichotomy result for general convex games
Below we prove a variant of Theorem 9 which does not require the compactness assumption. Theorem 9 is a direct corollary of this variant.
Theorem 9 . Suppose that Assumption 1 holds and all players i ∈ N adopt an adaptive optimistic learning strategy. Assume additionally that X i ⊂ dom ∂hi. Then one of the following holds: (a) For every i ∈ N and every compact set Pi ∈ X i, the individual regret RegiT (Pi) is bounded
above i.e., RegiT (Pi) = O(1). Moreover, every cluster point of the realized actions is a Nash equilibrium of the game.
(b) For every compact set P ⊂ X , the social regret with respect it tends to minus inﬁnity when t → +∞, i.e., limt→+∞ RegT (P) = −∞.

Proof. By Lipschitz continuity of V i, there exists Li > 0 such that

V i(Xt+ 1 ) − V i(Xt− 1 ) (i),∗ ≤ Li Xt+ 1 − Xt− 1 ≤ Li( Xt+ 1 − Xt + Xt − Xt− 1 ).

2

2

2

2

2

2

We set δti = 2Li2( Xt+ 1 − Xt 2 + Xt − Xt− 1 2) for t ≥ 2 so that δti ≤ δti. We also deﬁne

2

2

δ1i = δ1i = V i(x1) 2(i),∗, γi = 1/(16N Li2), and M i = maxpi∈Pi ϕi(pi). Then, from the regret

33

HSIEH ANTONAKOPOULOS MERTIKOPOULOS

bound (11), similar to how (19) is derived, we deduce

N
RegT (P) ≤
i=1

λiT +1M i +

V i(x1)

2 (i),∗

T
+
t=2

N δti 1 λi − 4
i=1 t

Xt+ 1 − Xt 2 + Xt − Xt− 1 2

2

2





N i

T ii

T

i

i

T N δti

ii

≤ M 1 + δt + δ1 − γ δt  +

λi − γ δt .

i=1

t=1

t=2

t=2 i=1 t

Following the reasoning of the proof of Theorem 3, we know there exists a constant C such that for

all T ∈ N,





T

RegT (P) ≤ C + f 1 

δ

1 t



,

t=2

where f 1 : ν ∈ R → −γ1ν2 + M 1ν is quadratic and has negative leading coefﬁcient. Therefore,

RegT (P) → −∞ when

+∞ t=1

δt1

=

+∞,

and

this

corresponds

to

the

situation

(b).

Otherwise, +t=∞1 ( Xt+ 1 − Xt 2 + Xt − Xt− 1 2) < +∞ and this implies

2

2

i) limt→+∞ Xt+ 1 − Xt 2 = 0; 2

ii) limt→+∞ Xt − Xt− 1 2 = 0; 2

iii) for all i ∈ N ,

+∞ t=1

δti

<

+∞

and

hence

λi

=

limt→+∞

λit

<

+∞.

To conclude, we prove the boundedness of individual regrets as in the proof of Theorem 5 and that every cluster point of (Xt+ 1 )t∈N is a Nash equilibrium as in the proof of Theorem 7 (case a).
2

Appendix E. Technical lemmas for numerical sequences

In this appendix we provide two basic lemmas for numerical sequences, one for bounding the adversarial regret of adaptive methods [4, Lemma 3.5], and the other for the analysis of quasi-Fejér sequence [12, Lemma 3.1].

Lemma 19. For any real numbers ν1, . . . , νT such that

t s=1

νs

>

0

for

all

t

∈

{1,

.

.

.

,

T

},

it

holds

T

νt

T

≤2

νt.

t=1

t s=1

νs

t=1

Proof. The function y ∈ R+ → √y being concave and has derivative y → 1/(2√y), it holds for

every z ≥ 0,

√√

1

z ≤ y + 2√y (z − y).

34

ADAPTIVE LEARNING IN CONTINUOUS GAMES

Take y =

t s=1

νs

and

z

=

t−1 s=1

νs

gives

t−1

2

νs +

s=1

νt ≤ 2

t s=1

νs

t
νs.
s=1

√

√

We conclude by summing the inequality from t = 2 to t = T and using ν1 ≤ 2 ν1.

Lemma 20. Let (Dt)t∈N ∈ RN+ be a non-negative sequence and (χt)t∈N ∈ RN+ be summable such

that, for all t ∈ N,

Dt+1 ≤ Dt + χt.

(44)

Then, (Dt)t∈N converges.

Proof. Since (χt)t∈N is summable, we can deﬁne Dt = Dt +

+∞ s=t

χs

∈

R+.

Inequality

(44)

then

implies Dt+1 ≤ Dt. Therefore, (Dt)t∈N converges, and accordingly (Dt)t∈N converges.

35

