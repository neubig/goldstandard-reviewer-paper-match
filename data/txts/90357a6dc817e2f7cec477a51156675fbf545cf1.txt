MERLOT:
Multimodal Neural Script Knowledge Models

arXiv:2106.02636v3 [cs.CV] 21 Oct 2021

Rowan Zellers♠ Ximing Lu♠♥ Jack Hessel♥ Youngjae Yu♥ Jae Sung Park♠ Jize Cao♠♥ Ali Farhadi♠ Yejin Choi♠♥ ♠Paul G. Allen School of Computer Science & Engineering, University of Washington
♥Allen Institute for Artiﬁcial Intelligence
https://rowanzellers.com/merlot

Abstract

As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present, and future. We introduce MERLOT, a model that learns multimodal script knowledge by watching millions of YouTube videos with transcribed speech – in an entirely
label-free, self-supervised manner. By pretraining with a mix of both frame-
level (spatial) and video-level (temporal) objectives, our model not only learns
to match images to temporally corresponding words, but also to contextualize what is happening globally over time. As a result, MERLOT exhibits strong out-of-the-box representations of temporal commonsense, and achieves state-of-
the-art performance on 12 different video QA datasets when ﬁnetuned. It also
transfers well to the world of static images, allowing models to reason about
the dynamic context behind visual scenes. On Visual Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy, outperforming stateof-the-art models of similar size by over 3%, even those that make heavy use of
auxiliary supervised data (like object bounding boxes).
Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the pretraining
video corpus; and 3) using diverse objectives that encourage full-stack multimodal
reasoning, from the recognition to cognition level.

MERLOT

Video QA

TVQA(+) TGIFQA
What’s she holding onto before he leaves?
Which of the chef’s hands has a watch?

6M videos

Commonsense Single-Image QA

VCR
Visual Commonsense Reasoning
Why is the man poin.ng?

Figure 1: Multimodal Event Representation Learning Over Time. We learn representations of multimodal script knowledge from 6 million YouTube videos. These representations can then be
applied to a variety of downstream tasks that require commonsense or temporal visual reasoning.

1 Introduction
The human capacity for commonsense reasoning is shaped by how we experience causes and effects over time. Consider the still image of people dining at a restaurant in the bottom right of Figure 1: while a literal, concrete description like “people sitting at a table eating" might be technically correct for the static scene, it doesn’t capture the richer temporal, commonsense inferences that are nonetheless obvious: before sitting down, the people had to meet up, agree where to go, and enter the
: Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

restaurant; at present, the man is pointing because the server just came to the table, and she might want to know whose food is whose; and after, it is likely the server will return to the kitchen to help another table.
Teaching machines this type of script knowledge [95] is a signiﬁcant challenge in no small part because enumerating all facts, inferences, and counterfactuals is prohibitive. As a result, the highest performing models on vision-and-language tasks, including Visual Commonsense Reasoning (VCR) (where Figure 1’s scene originates from), learn about the visual world exclusively through static images paired with literal captions [108, 22, 69, 75, 119, 36]. Though some captions might hint at the past and future, it is not obvious that even training on, e.g., 400M literal image/text pairs [89] will result in models capable of temporal reasoning.
In this paper, we introduce MERLOT, short for Multimodal Event Representation Learning Over Time. MERLOT is a model that learns commonsense representations of multimodal events by selfsupervised pretraining over 6M unlabelled YouTube videos. With the goal of learning multimodal reasoning capacity beyond static images/literal captions, we train MERLOT to a) match individual video frames with contextualized representations of the associated transcripts, and to b), contextualize those frame-level representations over time by “unmasking" distant word-level corruptions [27] and reordering scrambled video frames.
We validate our model on a diverse suite of video tasks, requiring both recognition- and cognition-level reasoning across long and short timescales; when ﬁnetuned, MERLOT achieves a new state-of-theart on 12 such tasks. Additionally, we show that our script-knowledge representations transfer to the single image domain. On Visual Commonsense Reasoning (VCR; [123]), our model achieves particularly strong performance, outperforming models that require heavy visual supervision (in the form of object detection bounding boxes, or images paired with pristine captions).
Beyond ﬁnetuning, we show both quantitatively and qualitatively that MERLOT has a strong outof-the-box understanding of everyday events and situations. Given a scrambled visual story, [50, 2], MERLOT can sort image sequences to match captions which tell a globally coherent narrative. Despite considerable domain shift from videos to static images, MERLOT outperforms strong baselines like CLIP [89] and UNITER [22], which independently match images to text and thus cannot reason over long-term contexts as effectively. This capacity for temporal coherence emerges during pretraining: analysis of MERLOT’s attention patterns (Figure 11) show that regions attend to captions that are distant in time (and vice versa), allowing it perform cross-modal coreference to piece together a holistic view of situations.
Finally, ablations of MERLOT show that 1) pretraining works better when we train on videos rather than still images, aided crucially by our strategy of corrupting highly visual words in the masked language modeling task, 2) using a diverse set of videos covering many aspects of everyday situations improves downstream performance compared to curated instructional video corpora [107, 80] which both cover a smaller slice of the visual world (conﬁrming hypotheses from past work [47]); and 3) MERLOT’s performance does not saturate even after many epochs of training on the pretraining corpus we curated, YT-Temporal-180M, as it continues to improve performance simply with more pretraining. The combination of these results suggests that learning full-stack visual reasoning and multimodal world knowledge from video data is a promising path forward for future research.
In summary, our main contributions are:
1. MERLOT a performant end-to-end vision and language model, that learns powerful multimodal world representations from videos and their transcripts – using no labeled data.
2. YT-Temporal-180M, a diverse corpus of frames/ASR derived from a ﬁltered set of 6M diverse YouTube videos, which we show greatly aids performance, and
3. A set of experiments/ablations demonstrating the strong performance of MERLOT on a set of 14 tasks, spanning ﬁnetuning and zero-shot transfer, and images and videos.
At rowanzellers.com/merlot, we have released code, data, and models for public research use.
2

2 Related Work
2.1 Joint representations of written text and images
There is a long history of work on learning joint text-image representations [14]. Recently, several papers have proposed “Visual BERT” models [108, 22, 8, 69, 75, 119, 36], trained on image captioning datasets such as MSCOCO [71]. In general, features are extracted using Anderson et al. [10]’s frozen object detector, which was originally trained on Visual Genome [60]. Some exceptions are Zhang et al. [125], who use an even larger object detector trained on more labeled data; Kim et al. [57], who use an ImageNet-pretrained backbone [26], and Shen et al. [100], who study a CLIP backbone [89] pretrained on web image-caption pairs.
Overall, these approaches all learn visual representations of static images, and rely on signiﬁcant human annotation in doing so (e.g. through literal image descriptions). Instead, our approach learns dynamic visual representations purely from videos – their frames, and a transcript of what is said – thus using no human annotation.
2.2 Learning from videos, with automatic speech recognition (ASR) transcripts
Prior works have used web videos with ASR to build weakly-supervised object detectors [87], action detectors/classiﬁers [120, 6, 62, 84], instruction aligners [77, 5, 19], video captioners [96, 46, 86, 101], and visual reference resolvers [49]. Of late, works have sought to learn multimodal representations transferable to many tasks from uncurated sets of (usually how-to) videos [80, 106, 107, 81, 127, 9, 7, 4]; generally these are applied to video understanding tasks like activity recognition. One challenge is designing an appropriate objective for learning video-level representations. Lei et al. [67]’s ClipBERT model learns vision-language representations from image captions, which more literally describe image content versus the longer ASR transcripts we consider. Tang et al. [109] use a pretrained dense image captioner [59] to provide auxiliary labels for web how-to videos. Both approaches use (supervised) ResNets pretrained on ImageNet [43] as their visual backbones. MERLOT is trained using a combination of objectives requiring no manual supervision; it nonetheless outperforms both prior approaches on downstream tasks.
2.3 Temporal ordering and forecasting
There has been a large body of work on analyzing ‘what happens next’ in videos [58]. Some modeling choices include using pixels [34, 113], graphs [11], euclidean distance using sensors [3], or studying cycle consistency across time [32]. In addition to extrapolation, past work has studied deshufﬂing objectives in videos [82, 115], though this has mostly been limited to the visual modality. In contrast to these papers, our goal is learning multimodal script knowledge representations: using both language and vision as complementary views into the world, instead of just tracking what changes on-screen.
3 MERLOT: Multimodal Event Representation Learning Over Time
We now present our uniﬁed model for learning script knowledge through web videos; including our pretraining dataset, architecture, and objectives.
3.1 YT-Temporal-180M
We collect YT-Temporal-180M, a dataset for learning multimodal script knowledge, derived from 6 million public YouTube videos. Our YT-Temporal-180M intentionally spans many domains, datasets, and topics. We began with 27 million candidate video IDs (which we then ﬁltered), including instructional videos from HowTo100M [80], lifestyle vlogs of everyday events from the VLOG dataset [35], and YouTube’s auto-suggested videos for popular topics like ‘science’ or ‘home improvement.’ Our intent (in making the corpus as diverse as possible) was to encourage the model to learn about a broad range of objects, actions, and scenes [47]: we will later show through an ablation that limiting our pretraining to only instructional videos indeed hurts performance downstream.
We ﬁltered videos using the YouTube API, which provides access to videos themselves, their ASR track (automatically transcribed speech tokens), and other metadata. We discard videos 1) without
3

We’re making a
greenhouse.

CLS

Contrastive Frame-caption
matching

Temporal ordering

CLS

I1 < l a t e x i t s h a 1 _ b a s e 6 4 = " D 5 D N A o S + P Z A a 2 P + l e L V q H J B z S R Q = " > A A A D R X i c f V L N b h M x E H a X Q s v y l 8 K R i 0 W E B B y i 3 R a J H i v g A A d E k U h b K R t F s 8 4 k s e K f x Z 6 F h l W e g a f h C k e e g Y f g h r i C k + y B T S g j W f P 5 m z / P e P J C S U 9 J 8 n 0 r u r R 9 + c r O 7 t X 4 2 v U b N 2 + 1 9 m 6 f e F s 6 g V 1 h l X V n O X h U 0 m C X J C k 8 K x y C z h W e 5 t N n C / v p e 3 R e W v O W Z g X 2 N Y y N H E k B F K h B 6 2 G W W z X 0 M x 1 U 9 X I + S H k W 4 g W + 4 2 u G / U G r n X S S p f B N k N a g z W o 5 H u x F 2 9 n Q i l K j I a H A + 1 6 a F N S v w J E U C u d x V n o s Q E x h j L 0 A D W j 0 / W r Z 0 5 z f D 8 y Q j 6 w L x x B f s n 9 H V K D 9 4 n X B U w N N / L p t Q f 7 L 1 i t p d N i v p C l K Q i N W h U a l 4 m T 5 Y k B 8 K E P / p G Y B g H A y v J W L C T g Q F M b Y q K J L R d L Z D 4 1 O K g F K N J m x g 2 I i x X m T d a i 8 / N g c w w U p n a X w X W b c Z H P d v J d O r S W z D j d L 5 N Z O C X J / Y e H n G H 7 L 4 a s w u d c F O i D r H l U Z u L G G 8 3 l V 6 / + 5 S b N y C z q O 4 7 A 3 6 f q W b I K T / U 5 6 0 E n e P G 4 f P a 0 3 a J f d Z f f Y A 5 a y J + y I v W D H r M s E + 8 Q + s y / s a / Q t + h H 9 j H 6 t X K O t O u Y O a 0 j 0 + w / Y w R g i < / l a t e x i t >

I2

CLS

Unmask words 10

mask= saw

Language only encoder …

It’s thin

CLS

plastic, so

it’ll be easy

to cut.

So I’ll cut it

CLS

with a

circular

saw.

CLS

Joint Vision & Language Transformer Encoder

CLS

CLS 1,1 … H,W

CLS 1,1 … H,W CLS 1 … L

CLS 1 … L

Image

Image

Encoder … Encoder …

Word embed …

Word embed

For my

CLS

morning

routine, I …

I1 < l a t e x i t s h a 1 _ b a s e 6 4 = " t H q T J n n l C K H / z d B 2 e C A v j 0 P R 4 S 0 = " > A A A D L H i c f V I 7 b x N B E N 4 c A c L x c q C k O W E h I Q r r D p C g j A I F F I g g 4 S S S b V l z 6 7 G 9 8 j 5 O s 3 N J z M n / h B Z K f g 0 N Q r T 8 D t b 2 F Z x N G G k 1 3 3 7 z 3 N n J C 6 0 8 p + m P n e j K 7 t V r 1 / d u x D d v 3 b 5 z t 7 V / 7 9 i 7 k i R 2 p d O O T n P w q J X F L i v W e F o Q g s k 1 n u S z V 0 v 7 y R m S V 8 5 + 5 H m B A w M T q 8 Z K A g d q 2 G r 1 c 6 d H f m 6 C q t 4 u h t m w 1 U 4 7 6 U q S b Z D V o C 1 q O R r u R 7 v 9 k Z O l Q c t S g / e 9 L C 1 4 U A G x k h o X c b / 0 W I C c w Q R 7 A V o w 6 A f V q v V F 8 i g w o 2 T s K B z L y Y r 9 O 6 I C 4 5 f d B U 8 D P P W b t i X 5 L 1 u v 5 P H L Q a V s U T J a u S 4 0 L n X C L l n O I R k p Q s l 6 H g B I U q H X R E 6 B Q H K Y V q O K K T U r c u e N l 1 Q S t G w y E 4 J i q u R F k y X U X n 1 q j u G S l O Q 4 / I q d N N n c N O 8 l 6 Y 1 k j n C 7 R O 7 c j C H 3 l x Z + j e G 3 C N + F y b 0 v k I A d P a n 6 Q B M D F 4 u q 1 v 9 z U 3 b t F n Q c x 2 F v s s 0 t 2 Q b H T z v Z s 0 7 6 4 X n 7 4 L D e o D 3 x Q D w U j 0 U m X o g D 8 U Y c i a 6 Q 4 k x 8 F l / E 1 + h b 9 D 3 6 G f 1 a u 0 Y 7 d c x 9 0 Z D o 9 x 8 5 + A 2 n < / l a t e x i t >

I2 < l a t e x i t s h a 1 _ b a s e 6 4 = " 0 T 1 E b T j y 0 i P B k Y n M z K e W G u g 0 0 7 U = " > A A A D L H i c f V I 7 b x N B E N 4 c A c L x c q C k O W E h I Q r r L k G C M g I K K B B B w k k k 2 7 L m 1 m N 7 5 X 2 c Z u d C z M n / h B Z K f g 0 N Q r T 8 D t b 2 F Z x N G G k 1 3 3 7 z 3 N n J C 6 0 8 p + m P n e j K 7 t V r 1 / d u x D d v 3 b 5 z t 7 V / 7 8 S 7 k i R 2 p d O O z n L w q J X F L i v W e F Y Q g s k 1 n u a z l 0 v 7 6 T m S V 8 5 + 4 H m B A w M T q 8 Z K A g d q 2 G r 1 c 6 d H f m 6 C q t 4 s h g f D V j v t p C t J t k F W g 7 a o 5 X i 4 H + 3 2 R 0 6 W B i 1 L D d 7 3 s r T g Q Q X E S m p c x P 3 S Y w F y B h P s B W j B o B 9 U q 9 Y X y a P A j J K x o 3 A s J y v 2 7 4 g K j F 9 2 F z w N 8 N R v 2 p b k v 2 y 9 k s f P B 5 W y R c l o 5 b r Q u N Q J u 2 Q 5 h 2 S k C C X r e Q A g S Y V e E z k F A s l h W o 0 q p t S s y H 1 s v K S S o G W T m R A U U y U v m i y h 9 u p T c w y X p C T H 4 V f s p M n m p n k v S W 8 k c 4 T b J X L n Z g y 5 v 7 T w K w y / R f g 2 T O 5 d g Q T s 6 E n V B 5 o Y u F h U t f 6 f m 7 J r t 6 D j O A 5 7 k 2 1 u y T Y 4 O e h k h 5 3 0 / d P 2 0 Y t 6 g / b E A / F Q P B a Z e C a O x G t x L L p C i n P x W X w R X 6 N v 0 f f o Z / R r 7 R r t 1 D H 3 R U O i 3 3 8 A P K 8 N q A = = < / l a t e x i t >

It’s so

thin plastic,

it’ll be easy

to cut.

w 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " s T E L 0 r 8 V D k j Y m m V T w d x H W n v N C + 4 = " > A A A D L H i c f V I 7 b x N B E N 4 c A c L x c q C k O W E h I Q r r D p C g j A I F D S J I O I l k W 9 b c e m y v v I / T 7 F w S 5 + R / Q g s l v 4 Y G I V p + B 2 v 7 C s 4 m j L S a b 7 9 5 7 u z k h V a e 0 / T H T n R t 9 / q N m 3 u 3 4 t t 3 7 t 6 7 3 9 p / c O x d S R K 7 0 m l H p z l 4 1 M p i l x V r P C 0 I w e Q a T / L Z m 6 X 9 5 A z J K 2 c / 8 b z A g Y G J V W M l g Q M 1 b L X 6 u d M j P z d B V e e L Y T Z s t d N O u p J k G 2 Q 1 a I t a j o b 7 0 W 5 / 5 G R p 0 L L U 4 H 0 v S w s e V E C s p M Z F 3 C 8 9 F i B n M M F e g B Y M + k G 1 a n 2 R P A n M K B k 7 C s d y s m L / j q j A + G V 3 w d M A T / 2 m b U n + y 9 Y r e f x 6 U C l b l I x W r g u N S 5 2 w S 5 Z z S E a K U L K e B w C S V O g 1 k V M g k B y m 1 a h i S s 2 K 3 H n j J Z U E L Z v M h K C Y K n n R Z A m 1 V 5 f N M V y R k h y H X 7 G T J p u b 5 r 0 k v Z H M E W 6 X y J 2 b M e T + y s J v M f w W 4 f s w u Q 8 F E r C j Z 1 U f a G L g Y l H V + n 9 u y q 7 d g o 7 j O O x N t r k l 2 + D 4 e S d 7 0 U k / v m w f H N Y b t C c e i c f i q c j E K 3 E g 3 o k j 0 R V S n I n P 4 o v 4 G n 2 L v k c / o 1 9 r 1 2 i n j n k o G h L 9 / g O 3 Z A 3 V < / l a t e x i t >

So

I’ll cut it with

a circular

[MASK.]

w 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " k P 8 H J 8 B k H S g o 7 s j k R 6 u P q D D j a U c = " > A A A D L H i c f V I 7 b x N B E N 4 c A c L x c q C k O W E h I Q r r n C B B G Q E F D S J I O I l k W 9 b c e n x e e R + n 2 b k k 5 u R / Q g s l v 4 Y G I V p + B 2 v 7 C s 4 m j L S a b 7 9 5 7 u x k h V a e 0 / T H T n R t 9 / q N m 3 u 3 4 t t 3 7 t 6 7 3 9 p / c O J d S R J 7 0 m l H Z x l 4 1 M p i j x V r P C s I w W Q a T 7 P Z 6 6 X 9 9 B z J K 2 c / 8 r z A o Y H c q o m S w I E a t V q D z O m x n 5 u g q o v F 6 G D U a q e d d C X J N u j W o C 1 q O R 7 t R 7 u D s Z O l Q c t S g / f 9 b l r w s A J i J T U u 4 k H p s Q A 5 g x z 7 A V o w 6 I f V q v V F 8 i Q w 4 2 T i K B z L y Y r 9 O 6 I C 4 5 f d B U 8 D P P W b t i X 5 L 1 u / 5 M n L Y a V s U T J a u S 4 0 K X X C L l n O I R k r Q s l 6 H g B I U q H X R E 6 B Q H K Y V q O K K T U r c h e N l 1 Q S t G w y O U E x V f K y y R J q r z 4 1 x 3 B F S n I c f s X m T T Y z z X t J e i O Z I 9 w u k T k 3 Y 8 j 8 l Y X f Y P g t w n d h c u 8 L J G B H z 6 o B U G 7 g c l H V + n 9 u y q 7 d g o 7 j O O x N d 3 N L t s H J Q a d 7 2 E k / P G 8 f v a o 3 a E 8 8 E o / F U 9 E V L 8 S R e C u O R U 9 I c S 4 + i y / i a / Q t + h 7 9 j H 6 t X a O d O u a h a E j 0 + w + 6 G w 3 W < / l a t e x i t >

Figure 2: Left: MERLOT learns to match contextualized captions with their corresponding video frames. Right: the same image encoding is provided, along with (masked) word embeddings, into a
joint vision-language Transformer model; it then unmasks ground words (like ‘saw’ in this example)
and puts scrambled video frames into the correct order.

an English ASR track; 2) that are over 20 minutes long; 3) that belong to visually “ungrounded" categories like video game commentaries; and 4) that have thumbnails unlikely to contain objects, according to a lightweight image classiﬁer. We add punctuation to the ASR by applying a sequenceto-sequence model trained to add punctuation to sentences/paragraphs from news articles. Full details of the scraping and ﬁltering are in Appendix A.
Each video V might contain thousands of frames. In this work, we represent a video V as a sequence of consecutive video segments {st}. Each segment st consists of:
a. an image frame It, extracted from the middle timestep of the segment, b. the words wt spoken during the segment, with a total length of L tokens.
To split the videos into segments, we byte-pair-encode (BPE; [97, 88]) each video transcript and align tokens with YouTube’s word-level timestamps. This enables us to split the videos into segments of L=32 BPE tokens each (Appendix A.4); our ﬁnal dataset has 180 million segments of this form.
3.2 MERLOT Architecture
A diagram of MERLOT is given in Figure 2. MERLOT takes a sequence of video frames {st} as input. We encode each frame It using an image encoder, embed the words wt using a learned embedding, and jointly encode both using a Transformer [112]. After pretraining, the architecture can be applied to a variety of vision-and-language tasks with minimal modiﬁcation. For video QA, for example, we pass several video frames to the image encoder, the question to the text encoder, and extract a single vector representation from the CLS token position. For each task, we learn a lightweight classiﬁcation head mapping from this hidden state to the task’s label space; speciﬁc modeling/optimization details are given in Appendix E.2.
Image encoder. We train our image encoder end-to-end, alongside the rest of the model, from random initialization (thus without learning from supervised data). While most performant visionand-language models pre-extract features from a (supervised) object detector [108, 69, 75, 22, 68], for the sake of pre-training efﬁciency we use a grid-based hybrid ResNet/Vision Transformer.1
Speciﬁcally: our encoder uses a ResNet-50 backbone, followed by a 12-layer, 768-dimensional Vision Transformer [43, 112, 31]. We made additional modiﬁcations that improve efﬁciency, including: 1) we trained on smaller, widescreen images of size 192x352 (because most YouTube videos are
1Standard object detectors have expensive operations for proposing regions, and extracting features from those regions (RoI-pooling); our grid approach avoids these. Recent work has proposed using ‘grid features’ broadly [53], yet on tasks like VCR these approaches have so far underperformed the more expensive object detector backbones [123]; our results suggest that ‘grid features’ can perform well broadly.
4

widescreen) using a patch size of 16x16 pixels; 2) we mirror [31]’s alterations of removing the C5 block in ResNet-50; and 3) we save compute further by average-pooling the ﬁnal-layer region cells using a kernel size of 2 × 2. With these modiﬁcations, our image encoder requires 40 gigaFLOPs for a forward pass, which is 2% of the 2 teraFLOPs required for the Faster-RCNN.
In summary: given an image of size W × H, the image encoder will output a W/32×H/32 feature map, along with two CLS hidden states: one for pooling a global representation of the image, and another for pretraining (Task 1.).
Joint Vision-Language Encoder. The joint encoder is a 12-layer, 768-dimensional Transformer [112], mirroring the RoBERTa base architecture [72]; we initialize it with pretrained RoBERTa weights. To compute joint representations, we ﬁrst embed the tokens {wt} via lookup, and then add position embeddings to both language and vision components (i.e., {It}). The position embeddings differ between different segments, so as to distinguish between images and captions at different timesteps. Finally, we pass the independent visual and textual feature maps to our joint encoder.
The tokens wt in each segment begin with a CLS token; recall that the feature maps for each frame It start with one as well. At those positions, we will later pool ﬁnal-layer hidden-state representations, for use in pretraining along with downstream tasks.
3.3 Pretraining Tasks and Objectives
We use the following three objectives to pretrain MERLOT, that cover ‘full-stack’ visual reasoning – from recognition subtasks (like object detection) that operate at the frame level, to more ‘cognitive’ tasks that operate at the video level.
1. Contrastive frame-transcript matching [126, 89]. We want to ensure that the underlying image encoder produces helpful image representations. Thus, we use the video transcript to compute a ‘language-only’ representation of each video segment; and use a contrastive loss to maximize its similarity to corresponding representations from the image encoder.2 Unlike what is the case for many image captions, the words wt in each segment are often not sufﬁcient to describe the gist of It, or even what the key objects might be – for that, video-level contextualization is often required. We thus pass the entire transcript into the language-only encoder, which then extracts hidden states for each segment at the segment-level CLS tokens. Given matching representations for each frame It and caption wt as positive examples, the negative examples come from all other frame-caption pairs in the batch – whether or not they come from the same video. We project both of these representations into a size-768 hidden state which is then unit-L2-normalized, and compute an all-pairs dot-product between all image and text representations. We divide these logits by a temperature of τ = 0.05, and then apply a pairwise cross entropy loss to encourage matching captions and frames.
2. (Attention) Masked Language Modeling When providing words into the joint vision-andlanguage encoder, we randomly replace 20% with a MASK token, a random word, or the same word; MERLOT must then reconstruct the correct word with a cross-entropy loss, following [27]. This approach is commonly used by ‘visual BERT’ models in the image captioning domain, where captions are concise, and thus the identity of masked concrete words is difﬁcult for models to recover given language context alone. However, we observed qualitatively that videos break these assumptions: people tend to ramble, and often mention key objects multiple times. Thus, applying vanilla BERT-style masking often causes ungrounded ﬁllers like ‘umm’ or ‘yeah’ to get masked, while the (repeated) names of important objects are often partially masked, penalizing the learning of multimodal representations. We introduce a simple solution to this problem, that we call attention masking: we use attention weights from a language-only transformer (introduced in the previous objective) as a heuristic for which words are grounded. 50% of the time, we mask out a random token; the other 50% of the time, we mask out one of the top 20% most-attended-to-tokens. We then apply SpanBERT masking [54], randomly corrupting the following or preceding tokens with an average length of 0.5 tokens in each direction; this makes it harder for models to over-rely on BPE artifacts. We show in ablations that this improves performance.
2To save memory, our ‘language-only encoder’ for this subtask shares parameters with the joint vision-andlanguage encoder.
5

Q→A QA→R Q→AR

ViLBERT [75] Unicoder-VL [68] VLBERT [69] UNITER [22] VILLA [36] ERNIE-ViL [119]

73.3 74.6 54.8 73.4 74.4 54.9 73.8 74.4 55.2 75.0 77.2 58.2 76.4 79.1 60.6 77.0 80.3 62.1

MERLOT (base-sized) 80.6 80.4 65.1

Table 1: Results on VCR [123]. We compare
against SOTA models of the same ‘base’ size as
ours (12-layer vision-and-language Transformers). MERLOT performs best on all metrics.

Spearman Pairwise acc Distance

(↑)

(↑)

(↓)

CLIP [89]

.609

UNITER [22] .545

MERLOT

.733

78.7

.638

75.2

.745

84.5

.498

Table 2: Results unscrambling SIND visual
stories[50, 2]. Captions are provided in the cor-
rect order; models must arrange the images temporally. MERLOT performs best on all metrics by reasoning over the entire story, instead of in-
dependently matching images with captions.

3. Temporal Reordering. We have the model order the image frames in a video, forcing it to explicitly learn temporal reasoning and giving it an interface to measure such temporal reasoning. Here, 40% of the time, we randomly pick an integer i between 2 and N (the number of segments provided to the joint encoder). Then we randomly scramble i video frames chosen at random, by replacing the segment-level position embeddings (e.g. [image_t]) for that frame with a random and unique position embedding, e.g. [image_unk_0]). These random position embeddings are learned, and separate from the ‘unshufﬂed’ position embeddings. This allows the model to order each ‘shufﬂed’ frame conditioned on frames provided in the correct order (if any). To compute the reordering loss, we extract hidden states from each frame at the CLS token position. For each pair of frames, we concatenate their hidden states hti and htj and pass the result through a two-layer MLP, predicting if ti < tj or ti > tj. We optimize this using a cross-entropy loss.
3.4 Pretraining MERLOT
We pretrain our model for 40 epochs over our video dataset. We preprocess the dataset into examples with sequences of N =16 video segments each, each containing up to L=32 BPE tokens.3 The language-only encoder computes contrastive representations given this entire sequence, its total length is thus 512 tokens. To save memory, we provide the joint vision-language encoder 4 groups of N = 4 segments each. At an image training resolution of 192 × 352, the joint model’s sequence length is 396 tokens. To combine the losses, we multiply the contrastive loss by a coefﬁcient of 0.25, which we found scaled its gradient magnitudes to roughly the same magnitude as the Mask LM loss.
We train the model using a v3-1024 TPU pod, at a batch size of 1024 sequences (or 16k segments) in total. This pretraining process on this hardware takes 30 hours. We provide additional information about hyperparameters and experimental setup in Appendix E.1.
4 Experiments: Transferring MERLOT to Downstream Tasks
In this section, we explore MERLOT on 14 different tasks, covering vision-language reasoning on static images as well as videos; we present analysis and ablations to dig deeper into our performance.
4.1 Image tasks
VCR. We consider VCR [123], a task and dataset where models must answer commonsense visual questions about images. These questions, about e.g. ‘what might happen next’ or ‘what are people’s intentions,’ force MERLOT to transfer video-level understanding to the world of single images. VCR provides additional ‘referring expression’ information to models in the form of bounding boxes around named entities. For example, if Person1 is referenced in the question, the location of Person1 is also given in the image. We provide this information to models by drawing (in pixel space) a
3To train the model on as much data as possible, we merged together the segments of short videos, and split up longer videos, such that all preprocessed examples in our dataset have exactly N =16 video segments.
6

Tasks

Split Vid. Length ActBERT [127] ClipBERT8x2 [67] SOTA MERLOT

MSRVTT-QA Test Short

-

MSR-VTT-MC Test Short

88.2

37.4

41.5 [118] 43.1

-

88.2 [127] 90.9

TGIF-Action Test Short

-

TGIF-Transition Test Short

-

TGIF-Frame QA Test Short

-

82.8

82.8 [67] 94.0

87.8

87.8 [67] 96.2

60.3

60.3 [67] 69.5

LSMDC-FiB QA Test Short

48.6

LSMDC-MC Test Short

-

-

48.6 [127] 52.9

-

73.5 [121] 81.7

ActivityNetQA Test Long

-

Drama-QA

Val

Long

-

TVQA

Test Long

-

TVQA+

Test Long

-

VLEP

Test Long

-

-

38.9 [118] 41.4

-

81.0 [56] 81.4

-

76.2 [56] 78.7

-

76.2 [56] 80.9

-

67.5 [66] 68.4

Table 3: Comparison with state-of-the-art methods on video reasoning tasks. MERLOT outperforms state of the art methods in 12 downstream tasks that involve short and long videos.

colored highlight around the referenced entity (Appendix E.3.1), this differs from prior works (that integrate these entities into detection architectures).
Our results on the three VCR settings, in comparison to other models at the same (‘base’) scale, are given in Table 1. Our model outperforms these other models, that all learn from exclusively static images (paired with captions and supervised object detections).
Unsupervised ordering of Visual Stories. To probe our model’s ability to do out-of-the-box commonsense reasoning over events in images, we next consider the Visual Storytelling dataset [50, 74]. Each story in this dataset contains ﬁve images and captions in a certain order; the order tells a joint narrative between the captions and the images. Past work has considered unshufﬂing imagecaption pairs [2], but we take a slightly different approach in this work to avoid language-only biases, which can rely on discursive clues to order text [27, 102]. In our formulation, models are given the captions in sorted order, and must match frames to the captions. Our formulation disarms language-only baselines, while still allowing us to quantify MERLOT’s capacity for commonsense temporal reasoning.
We compare MERLOT with two strong out-of-the-box baselines for text-image matching: CLIP [89], which encodes each caption and image separately and computes similarity through a dot product, and UNITER [22] which jointly represents each image/caption pair, and is trained in part using a ‘textimage matching’ objective. We use our temporal reordering loss to ﬁnd the most probable ordering of the video frames (Appendix E.1.1); for CLIP and UNITER we compute a maximum-weight bipartite matching [63] over the pairwise image-text similarity scores.
Results over 5K stories are given in Table 2. MERLOT’s performance in comparison to the algorithms trained from image-literal caption pairs suggests that, with no ﬁne-tuning, our model has strong capability to reason about past and future events expressed in collections of temporal visual stories.
4.2 Video Reasoning
We report results on 12 video reasoning tasks: TVQA [64], TVQA(+) [65], VLEP [66], MSRVTT-QA [117], MSRVTT-Multichoice [121], LSMDC-Multichoice, LSMDC ﬁll-in-the-blank QA [110, 92], ActivityNetQA [122, 45], TGIFQA [52], and DramaQA [23]. We apply MERLOT to these tasks in the same way. We sample a sequence of 5 to 7 still frames from each video clip, initialize new parameters only to map the model’s pooled CLS hidden state into the output labels, and ﬁnetune MERLOT with a softmax cross entropy loss; see Appendix E.2 for details. As shown in Table 3, for all these datasets MERLOT sets a new state-of-the-art. Given the diversity of tasks and the strengths of the comparison models, these results provide strong evidence that MERLOT learned strong multimodal and temporal representations.
4.3 Ablations
We present ablations over VCR and TVQA+ to study the effect of several modeling decisions.
7

Training setup

VCR TVQA+

Training setup

VCR TVQA+

VCR

One segment (N =1)

73.8 75.2

One segment, attention masking

73.5 74.5

Four segments

74.1 73.3

Four segments, attention masking 75.2 75.8

(a) Context helps together with attention masking. Pretraining on more segments at once improves performance, but more context can encourage language-only representation learning. Attention masking counteracts this, giving an additional 1 point boost.

No contrastive V-L loss 57.5 67.6

No temporal ordering loss 75.5 75.6

All losses

75.2 75.8

(b) Contrastive V+L loss is crucial. Removing it makes performance drop signiﬁcantly; the temporal ordering loss is not as important for downstream ﬁnetuning.

No boxes

74.8

Drawn-on boxes 79.4

(c) Drawing on bounding boxes helps, suggesting that our model uses it to decode the ‘referring expression’ information (e.g. person1).

Dataset

VCR

# epochs VCR

Conceptual ∪ COCO

58.9

HowTo100M

66.3

YT-Temporal-180M

75.2

HowTo100M-sized YT-Temporal-180M 72.8

YTT180M, raw ASR

72.8

(d) Diverse (video) data is important. Applying our architecture to caption data leads to poor results. Our model performs better on HowTo100M, yet still below our (more diverse) YT-Temporal-180M, even when controlled for size. Using raw ASR (vs. denoised ASR) reduces performance.

5 epochs 75.2 10 epochs 75.9 20 epochs 77.0 30 epochs 78.5
40 epochs 79.4
(e) Training for longer helps, with performance increasing monotonically over training iterations.

Table 4: Ablation study on the validation set of VCR question answering (Q → A) and TVQA+, in accuraty (%). We put a next to the conﬁgurations we chose for MERLOT.

Context size. Table 4a shows the effect of varying the number of segments N given to the joint vision-and-language encoder during pretraining. In the ﬁrst two rows, we provide only a single video segment (N =1) to the model.4 In this limited regime, we ﬁnd that our ‘attention masking’ approach (preferential masking of tokens that were highly attended-to by the contrastive language-only encoder) does not outperform a strong baseline of masking spans randomly [54]. Yet, when we expand the sequence length to N =4 segments/128 tokens, our masking becomes more effective, improving by 1 point over the baseline. This supports our hypothesis (Section 3.3.2.) that text-only shortcuts become increasingly viable with length, and that our attention-masking approach counteracts them.5
Losses. In Table 4b, we ablate the losses. We ﬁnd that the contrastive frame-transcript matching loss is crucial to performance, suggesting that an explicit objective is critical for the (randomly initialized) image backbone to learn visual representations. The temporal ordering loss appears less critical for downstream tasks; it helps for TVQA but performance drops slightly for VCR. Thus, we ﬁnd that it helps primarily as an interface by which we can query the model about temporal events (i.e. for the story ordering experiments); the model might be learning this information from other objectives.
Drawing bounding boxes. Table 4c shows the effects of providing grounding information to VCR models by drawing boxes. Performance drops 5% when they are removed, suggesting that they help.
Dataset source. In Table 4d, we investigate pretraining MERLOT on two datasets beyond YTTemporal-180M. First, we train on 3 million static image-caption pairs from Conceptual Captions [99] combined with MSCOCO [71]; for fair comparison, we train for the same number of steps as 5 epochs on our dataset. The resulting model achieves 58.9% accuracy on VCR. We suspect this might be due to 1) a smaller context window (Table 4a), and 2) overﬁtting (5 epochs on YT-Temporal180M corresponds to 300 epochs on the caption data). Because our vision pipeline is trained from scratch, the scale of the curated/supervised image pairing corpora is a concern.
We next investigate the impact of video selection, comparing YT-Temporal-180M with HowTo100M [80]. To control for number of videos, we train for an equivalent amount of steps: 5 epochs on our dataset, 30 epochs on HowTo100M, and likewise 30 epochs on a ‘HowTo100M-sized YTTemporal-180M’. Using diverse YT-Temporal-180M data vs. only instructional videos improves VCR performance by 6.5 points. This suggests that the how-to domain is limited in terms of visual
4We keep the effective batch size the same, so that we use 4× the number of sequences at 14 th the length. 5Additional qualitative analyses of the attention patterns produced by the language-only encoder are in Appendix C.1; we ﬁnd that highly attended-to tokens are typically more ‘visual’, and, thus, masking them may make the Masked LM objective require more cross-modal reasoning.
8

Figure 3: Zero-shot story ordering (same setup as Table 2). MERLOT performs temporal commonsense reasoning accross frames. In the ﬁrst row, it uses ‘the old man’ mentioned to identify the ‘kids’ as parent-aged; in the second, it identiﬁes riding a merry-go-round as an activity that takes a while.
phenomena covered, and that other domains (like web dramas and VLOGs) provide helpful signal for tasks like VCR [47]. Using all the data gives an additional 2.4-point performance boost.
Last, we investigate our choice to preprocess the YouTube ASR text with a language model (adding punctuation, etc); using ‘raw ASR’ instead of this preprocessing reduces performance by 2.4 points. Pretraining longer. Last, in Table 4e, we investigate the effect of pretraining MERLOT for longer. The performance increases monotonically and doesn’t begin to plateau, which suggests that had we pretrained MERLOT for even longer, its performance could improve even further.
4.4 Qualitative examples
In Figure 3, we show two qualitative examples of MERLOT’s zero-shot story ordering capability. More examples (and a comparison with the best-scoring baseline, CLIP [89]) are in Appendix C.2. The examples here show that MERLOT has a strong understanding of events, transcending individual frames. In the ﬁrst row, it orders the story correctly, performing vision-and-language coreference across several frames (e.g. frames and captions 2 and 3 use ‘he’ to refer to ‘the old man’ only mentioned in the ﬁrst caption). Without resolving this coreference (establishing the subject as an elderly family member), it seems unlikely that anyone would describe the adults in frame (3) as ‘kids.’ Investigating the attention patterns of MERLOT (Appendix C.3) backs up this claim; they show that MERLOT frequently addresses video tasks by merging attention across (distant) video segments. MERLOT gets the second row ‘wrong’, but for an interesting reason. It reverses the order of frames (3) and (4), which groups the merry-go-round pictures together – even though caption (3) mentions a barn. This seems to capture the temporal commonsense intuition that people might ride a merry-go-round for a while, i.e., it is not an atomic event [25].
5 Conclusion, Limitations, and Broader Impacts
We introduced Multimodal Event Representation Learning Over Time (MERLOT). We trained the model through a combination of self-supervised objectives on 6M YouTube videos, in service of learning powerful multimodal representations that go beyond single frames. The model achieves strong performance on tasks requiring event-level reasoning over videos and static images. We hope that MERLOT can inspire future work for learning vision+language representations in a more human-like fashion compared to learning from literal captions and their corresponding images. There are several potential limitations of MERLOT that would make for promising avenues of future work, including: 1) exploring ﬁner-grained temporal reasoning pretraining objectives vs. frame ordering e.g., a temporal frame localization within transcripts; and 2) learning multilingually from non-English videos and communities on YouTube.
9

Like other pretraining work, MERLOT risks some potential negative impacts. We discuss these in more detail below, in addition to the steps we took to reduce these harms.
5.1 Data collection and privacy.
As with other corpora gathered from the web used for pretraining data, YT-Temporal-180M contains publicly available content posted by users. We thus shaped our data gathering and release strategy to minimize inherent privacy and consent harms (Appendix A.5). Perhaps most importantly, we plan to only share video IDs for download, following a release strategy from prior work [1, 80] and giving users the right to opt out of not just YouTube, but our dataset as well.
5.2 Social biases.
The curation choices we made in this work could cause the model to exhibit undesirable social biases – for this reason, along with others, we do not advocate for deployed use-cases. For example, 30% of the data selected for by our ﬁltering pipeline was local broadcast news (uploaded to YouTube). Including these news videos seems to perform better than ﬁltering them out and only using how-to videos (Table 4b), however, there are risks when training on them. Local broadcast news (at least in the US) dedicates signiﬁcant time to covering crime, sometimes in a racist and sensationalized manner [38, 29, 44]. Indeed, running a topic model over our data identiﬁes several ‘crime’ categories (Appendix B). Past work has shown correlation between watching local news and having more explicit racialized beliefs about crime [28]; it seems likely therefore that training models on this data could teach them learn the same racist patterns.
Additionally, there are inherent social biases on YouTube – and treating these videos as equivalent to ‘the world’ [111] can embed hegemonic perspectives [42, 114, 13]. Most popular YouTubers are men [30] and video practices emerging on YouTube are often gendered [83]. YouTube also has problems with hate, including radical alt-right and ‘alt-lite’ content [90]. These problems – as with other problems in representation and power – are themselves ampliﬁed by the ‘YouTube algorithm’ [15] that recommends content to users. Though we downloaded videos independently of YouTube’s recommender system, by ﬁltering based on what content has views, we are implicitly ﬁltering based on this algorithm. The dynamics of YouTube (i.e., which videos get popular/monetized) inﬂuence the style and content of videos that get made and uploaded to the platform; this in turn shapes and is shaped by culture more broadly [104].
5.3 Dual use.
The video QA tasks that we studied carry risk of dual use, through possible downstream applications like surveillance [91, 128]. It seems unlikely that purely technological ﬁxes and defenses – which themselves can be problematic [40] – could resolve these dynamics. Studying how well video-level pretraining enables surveillance applications might be an important avenue for future work, if only to inform stakeholders and policymakers about these risks.
5.4 Energy consumption.
The pretraining that we used in this work was expensive upfront [105]. Our results suggest that scaling up the amount of data and compute that we used might yield additional performance gains – but at increased environmental cost. To pretrain more efﬁciently, we used a much more lightweight architecture (in terms of FLOPs) than is standard for today’s vision and language models. We hope that our public release of the model (for research use) can further amortize this cost.
5.5 Synthesizing these risks.
With these issues in mind, we release MERLOT and YT-Temporal-180M for researchers. We view our work, and our research artifacts, to be part of a larger conversation on the limits of pretrained ‘foundation models’ [17]. These models have broad impact to real-world areas like healthcare, law, and education. At the same time, these models have signiﬁcant risks, including the harms that we outlined. We believe that further academic research into this video-and-language pretraining paradigm is important – especially to probe its limits and possible harms. We hope that our paper, code, and data release can contribute to this direction.
10

Acknowledgements and Funding Transparency Statement
We thank the anonymous reviewers for their helpful feedback that improved this work, along with Oren Etzioni and Gabriel Ilharco. Thanks also to Zak Stone and the Google Cloud TPU team for providing access to the TPU machines used for conducting experiments, and for help with the computing infrastructure. Last, but not least, thanks to all the YouTubers who share interesting videos with the world. This work was funded by DARPA MCS program through NIWC Paciﬁc (N66001-19-2-4031), and the Allen Institute for AI.
References
[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classiﬁcation benchmark. arXiv preprint arXiv:1609.08675, 2016.
[2] Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, and Mohit Bansal. Sort Story: Sorting Jumbled Images and Captions into Stories. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 925–931, 2016.
[3] Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: experiential learning of intuitive physics. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 5092–5100, 2016.
[4] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: transformers for multimodal self-supervised learning from raw video, audio and text. arXiv preprint arXiv:2104.11178, 2021.
[5] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsupervised learning from narrated instruction videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4575–4583, 2016.
[6] Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Joint discovery of object states and manipulation actions. In ICCV, 2017.
[7] Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovic´, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Selfsupervised multimodal versatile networks. arXiv preprint arXiv:2006.16228, 2020.
[8] Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual question answering. arXiv preprint arXiv:1908.05054, 2019.
[9] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. arXiv preprint arXiv:2003.03186, 2020.
[10] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.
[11] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 4509–4517, 2016.
[12] Emily Bender and Batya Friedman. Data statements for nlp: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 2019.
[13] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610–623, 2021.
11

[14] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8): 1798–1828, 2013.
[15] Sophie Bishop. Anxiety, panic and self-optimization: Inequalities and the youtube algorithm. Convergence, 24(1):69–84, 2018.
[16] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022, 2003.
[17] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv e-prints, pages arXiv–2108, 2021.
[18] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.
[19] Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and Juan Carlos Niebles. D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation. In CVPR, 2019.
[20] Snigdha Chaturvedi, Haoruo Peng, and Dan Roth. Story Comprehension for Predicting What Happens Next. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1603–1614, 2017.
[21] Peihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen, Mingkui Tan, and Chuang Gan. Rspnet: Relative speed perception for unsupervised video representation learning. In AAAI, 2021.
[22] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.
[23] Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Seungchan Lee, Minsu Lee, and Byoung-Tak Zhang. DramaQA: character-centered video story understanding with hierarchical qa. arXiv preprint arXiv:2005.03356, 2020.
[24] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at? an analysis of bert’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286, 2019.
[25] William Croft. Verbs: Aspect and causal structure. OUP Oxford, 2012.
[26] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. Ieee, 2009.
[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[28] Travis L Dixon. Crime news and racialized beliefs: Understanding the relationship between local news viewing and perceptions of african americans and crime. Journal of Communication, 58(1):106–125, 2008.
[29] Travis L Dixon and Daniel Linz. Overrepresentation and underrepresentation of african americans and latinos as lawbreakers on television news. Journal of communication, 50(2): 131–154, 2000.
[30] Nicola Döring and M Rohangis Mohseni. Male dominance and sexism on youtube: results of three content analyses. Feminist Media Studies, 19(4):512–524, 2019.
12

[31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[32] Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun. Learning temporal dynamics from cycles in narrated video. arXiv preprint arXiv:2101.02337, 2021.
[33] Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C Lawrence Zitnick, et al. Visual storytelling. arXiv preprint arXiv:1604.03968, 2016.
[34] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 64–72, 2016.
[35] David F Fouhey, Wei-cheng Kuo, Alexei A Efros, and Jitendra Malik. From lifestyle vlogs to everyday interactions. In CVPR, 2018.
[36] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Largescale adversarial training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.
[37] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumeé III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.
[38] Franklin D Gilliam Jr, Shanto Iyengar, Adam Simon, and Oliver Wright. Crime in black and white: The violent, scary world of local news. Harvard International Journal of press/politics, 1(3):6–23, 1996.
[39] Jonathan Gordon and Benjamin Van Durme. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25–30. ACM, 2013.
[40] Ben Green. Good” isn’t good enough. In Proceedings of the AI for Social Good workshop at NeurIPS, 2019.
[41] Herbert P Grice. Logic and conversation. In Speech acts, pages 41–58. Brill, 1975.
[42] Donna Haraway. Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist studies, 14(3):575–599, 1988.
[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[44] Don Heider. White news: Why local news programs don’t cover people of color. Routledge, 2014.
[45] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961–970, 2015.
[46] Jack Hessel, Bo Pang, Zhenhai Zhu, and Radu Soricut. A case study on combining ASR and visual features for generating instructional video captions. In CoNLL, November 2019.
[47] Jack Hessel, Zhenhai Zhu, Bo Pang, and Radu Soricut. Beyond instructional videos: Probing for more diverse visual-textual grounding on youtube. In EMNLP, 2020.
[48] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.
13

[49] De-An Huang, Joseph J. Lim, Li Fei-Fei, and Juan Carlos Niebles. Unsupervised visuallinguistic reference resolution in instructional videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
[50] Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, and Margaret Mitchell. Visual storytelling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1233–1239, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1147. URL https://www.aclweb.org/anthology/N16-1147.
[51] Sarthak Jain and Byron C Wallace. Attention is not explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543–3556, 2019.
[52] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017). Honolulu, Hawaii, pages 2680–8, 2017.
[53] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. In defense of grid features for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10267–10276, 2020.
[54] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64–77, 2020.
[55] Ruogu Kang, Laura Dabbish, Nathaniel Fruchter, and Sara Kiesler. “my data just goes everywhere:” user mental models of the internet and implications for privacy and security. In Eleventh Symposium On Usable Privacy and Security ({SOUPS} 2015), pages 39–52, 2015.
[56] Seonhoon Kim, Seohyeong Jeong, Eun-Byul Kim, Inho Kang, and Nojun Kwak. Selfsupervised pre-training and contrastive representation learning for multiple-choice video qa. ArXiv, abs/2009.08043, 2020.
[57] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. arXiv preprint arXiv:2102.03334, 2021.
[58] Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting. In European Conference on Computer Vision, pages 201–214. Springer, 2012.
[59] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. DenseCaptioning Events in Videos. In International Conference on Computer Vision (ICCV), 2017.
[60] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32–73, 2017.
[61] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recognition. In Proceedings of the International Conference on Computer Vision (ICCV), 2011.
[62] Hilde Kuehne, Ahsan Iqbal, Alexander Richard, and Juergen Gall. Mining youtube-a dataset for learning ﬁne-grained action concepts from webly supervised video data. arXiv preprint arXiv:1906.01012, 2019.
[63] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97, 1955.
[64] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. In EMNLP, 2018.
14

[65] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video question answering. In Tech Report, arXiv, 2019.
[66] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. What is more likely to happen next? video-and-language future event prediction. arXiv preprint arXiv:2010.07999, 2020.
[67] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. arXiv preprint arXiv:2102.06183, 2021.
[68] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In AAAI, pages 11336–11344, 2020.
[69] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.
[70] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In Proceedings of the European Conference on Computer Vision (ECCV), pages 513–528, 2018.
[71] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014.
[72] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[73] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
[74] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. In European Conference on Computer Vision, 2016.
[75] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pages 13–23, 2019.
[76] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron C Courville, and Christopher Joseph Pal. A dataset and exploration of models for understanding video data through ﬁllin-the-blank question-answering. In Computer Vision and Pattern Recognition (CVPR), 2017. URL http://openaccess.thecvf.com/content_cvpr_2017/papers/Maharaj_ A_Dataset_and_CVPR_2017_paper.pdf.
[77] Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy. What’s cookin’? interpreting cooking videos using text, speech and vision. In NAACL, 2015.
[78] Alice E Marwick and danah boyd. Networked privacy: How teenagers negotiate context in social media. New media & society, 16(7):1051–1067, 2014.
[79] Andrew Kachites McCallum. Mallet: A machine learning for language toolkit. 2002. URL http://mallet.cs.umass.edu.
[80] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV, 2019.
[81] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.
15

[82] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shufﬂe and learn: unsupervised learning using temporal order veriﬁcation. In European Conference on Computer Vision, pages 527–544. Springer, 2016.
[83] Heather Molyneaux, Susan O’Donnell, Kerri Gibson, Janice Singer, et al. Exploring the gender divide on youtube: An analysis of the creation and reception of vlogs. American Communication Journal, 10(2):1–14, 2008.
[84] Yasufumi Moriya, Ramon Sanabria, Florian Metze, and Gareth JF Jones. Grounding object detections with transcriptions. arXiv preprint arXiv:1906.06147, 2019.
[85] Meinard Müller. Dynamic time warping. Information retrieval for music and motion, pages 69–84, 2007.
[86] Shruti Palaskar, Jindrich Libovicky`, Spandana Gella, and Florian Metze. Multimodal abstractive summarization for how2 videos. arXiv preprint arXiv:1906.07901, 2019.
[87] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio Ferrari. Learning object class detectors from weakly annotated video. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3282–3289. IEEE, 2012.
[88] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical report, OpenAI, 2019.
[89] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.
[90] Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgílio AF Almeida, and Wagner Meira Jr. Auditing radicalization pathways on youtube. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 131–141, 2020.
[91] Neil M Richards. The dangers of surveillance. Harv. L. Rev., 126:1934, 2012.
[92] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Chris Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. Movie description. International Journal of Computer Vision, 2017. URL http://link.springer.com/article/10.1007/ s11263-016-0987-1?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst.
[93] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510–4520, 2018.
[94] Oleksandr Savsunenko. How tensorﬂow’s tf.image.resize stole 60 days of my life. Technical report, Hacker Noon.
[95] Roger C. Schank and Robert P. Abelson. Scripts, plans, and knowledge. In Proceedings of the 4th International Joint Conference on Artiﬁcial Intelligence - Volume 1, IJCAI’75, pages 151–157, San Francisco, CA, USA, 1975. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.cfm?id=1624626.1624649.
[96] Ozan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Unsupervised semantic parsing of video collections. In ICCV, 2015.
[97] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, 2016.
[98] Soﬁa Serrano and Noah A Smith. Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931–2951, 2019.
[99] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565, 2018.
16

[100] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip beneﬁt vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.
[101] Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen, Zhendong Niu, and Ming Zhou. Dense procedure captioning in narrated instructional videos. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6382–6391, 2019.
[102] Wei Shi and Vera Demberg. Next sentence prediction helps implicit discourse relation classiﬁcation within and across domains. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5794–5800, 2019.
[103] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. A dataset of 101 human action classes from videos in the wild. Center for Research in Computer Vision, 2(11), 2012.
[104] Michael Strangelove. Watching YouTube. University of Toronto press, 2020.
[105] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, 2019.
[106] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Contrastive bidirectional transformer for temporal representation learning. arXiv preprint arXiv:1906.05743, 2019.
[107] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A joint model for video and language representation learning. In ICCV, 2019.
[108] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In EMNLP, 2019.
[109] Zineng Tang, Jie Lei, and Mohit Bansal. Decembert: Learning from noisy instructional videos via dense captions and entropy minimization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2415–2426, 2021.
[110] Atousa Torabi, Niket Tandon, and Leon Sigal. Learning language-visual embedding for movie understanding with natural-language. arXiv preprint, 2016. URL http://arxiv.org/pdf/ 1609.08124v1.pdf.
[111] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521–1528. IEEE, 2011.
[112] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
[113] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting from static images using variational autoencoders. In European Conference on Computer Vision, pages 835–851. Springer, 2016.
[114] Zeerak Waseem, Smarika Lulz, Joachim Bingel, and Isabelle Augenstein. Disembodied machine learning: On the illusion of objectivity in nlp. arXiv preprint arXiv:2101.11974, 2021.
[115] Donglai Wei, Joseph J Lim, Andrew Zisserman, and William T Freeman. Learning and using the arrow of time. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8052–8060, 2018.
[116] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 11–20, 2019.
17

[117] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually reﬁned attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645–1653, 2017.
[118] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. arXiv preprint arXiv:2012.00451, 2020.
[119] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020.
[120] Shoou-I Yu, Lu Jiang, and Alexander Hauptmann. Instructional videos for unsupervised harvesting and learning of action examples. In ACM MM, 2014.
[121] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471–487, 2018.
[122] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA: a dataset for understanding complex web videos via question answering. In AAAI, pages 9127–9134, 2019.
[123] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6720–6731, 2019.
[124] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Advances in Neural Information Processing Systems 32, 2019.
[125] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. arXiv preprint arXiv:2101.00529, 2021.
[126] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive learning of medical visual representations from paired images and text. arXiv preprint arXiv:2010.00747, 2020.
[127] Linchao Zhu and Yi Yang. ActBERT: Learning global-local video-text representations. In CVPR, 2020.
[128] Shoshana Zuboff. Big other: surveillance capitalism and the prospects of an information civilization. Journal of Information Technology, 30(1):75–89, 2015.
18

Supplemental Material
We present the following items in the supplemental:
a. Data collection information (Section A) b. An exploration of the data in our corpus (Section B) c. Qualitative analysis of model representations (Section C) d. An exploration of the intermediate visual representations (Section D) e. Hyperparameters and experimental setup used for all experiments (Section E) f. A Datasheet [37] for our YT-Temporal-180M dataset (Section F)
A Collecting Videos and Transcripts from YouTube
We adopt the following high-level process to collect YouTube videos and their accompanying transcripts:
a. Collect channel pages that are likely to cover visually-textually grounded events (A.1), b. Download videos from each channel, while ﬁltering out videos without English ASR
captions, or unlikely to have (changing) real-world scenes and objects (A.2), c. ‘Denoise’ the transcripts – using a language model to rewrite transcripts in a style more
similar to written English, as opposed to spoken English (A.3), d. Last, align words in the transcript to video frames, and extract the segments for pretraining
(A.4).
As we will discuss in more detail in the following subsections, we designed our strategy to preserve user privacy as much as possible – an imperative when constructing a corpus on public-facing multimodal data. We conclude with a high-level summary of these privacy-preserving decisions, as well as about our release strategy (A.5).
A.1 Collecting channel IDs + video IDs
The ﬁrst stage in our pipeline was to collect YouTube video IDs that could potentially be relevant for learning visual-textual relationships. We opted to search for interesting channels rather than search for videos directly, as we found the API limits for searching for videos somewhat restrictive. Once a channel was downloaded, we could then download its videos.
We found channels using YouTube’s auto-generated ‘topic’ pages, corresponding to entries in FreeBase like ‘Science’ or ‘Home Improvement.’ We identiﬁed 18 of these topics, and retrieved the IDs for all channels that were linked to by each topic page. We also used YouTube channels that appeared in the VLOG dataset [35], as well as a selection of viral ‘How-To’ and ‘Cooking’ channels. Last, we searched YouTube for concrete nouns, using the object list from MSCOCO (‘baseball’, ‘snowboard’, etc.) as a starting point; we retrieved channel IDs for each video that appeared.
Channels on YouTube often feature other (often similar) channels; so we downloaded more channel IDs by performing a graph breadth-ﬁrst search over the initial set of channels. We identiﬁed 50k channels total and ﬁltered out any more ‘personal’ channels (with fewer than 10k views between all videos). Last, we gathered all video IDs that came from our list of channels, which left us with 27 million video IDs, which formed our ﬁnal candidate list.
Privacy implications. Our high-level goal was to preserve user privacy by mainly using popular (and more monetized) YouTube videos and channels in our dataset, as opposed to personal ones. The YouTube search algorithm helped us do that, by ordering results (in part) by the popularity of a video / channel. Downloading all videos from a channel, and ﬁltering out channels with fewer than 10k views, favors popular content (like for celebrities, professional YouTubers, and cable news stations). Our analysis in Appendix B shows this strategy was largely successful.
Connection with HowTo100M. As discussed in the paper, we used both a diverse selection of YouTube videos (coming from this process), as well as the video list from HowTo100M [80]. We simply
19

concatenated the video IDs from HowTo100M with the video IDs from this searching step. This means ﬁrst, that the HowTo100M videos were also ﬁltered by the next steps (and thus our copy of HowTo100M is slightly smaller than the original), though we found that the ﬁltering step had minimal impact on those videos (that were already ﬁltered by [80]). Second, it means that the HowTo100M videos do contain some instructional videos from less-popular channels. Our intuition here is that this might be okay from a privacy standpoint: few of these people are discussing personal topics; a typical example might be a grainy video of somebody baking cookies. Nonetheless, given the scale that we operated at ourselves, we tried to be more cautious with the ﬁltering.
A.2 Filtering out videos
After retrieving a set of video IDs, our next step was to download ones likely to be appropriate for pre-training MERLOT. Not all videos would are likely to work well: many videos have no spoken words, are not in English, or otherwise do not have automatically-generated (ASR) captions. Likewise, many videos are not grounded: some just have still images (like podcasts), some are of people talking to each other or to the camera, and many are of people playing video games. Our intention was to ﬁlter out these videos, ideally without having to download them (so as to conserve bandwidth).
For each video ID, we perform the following steps:
• Downloading info: YouTube allows us to download the video metadata separately from each video. We do this ﬁrst as the video info ﬁle is much smaller than the video itself. We thus ﬁrst (try to) download this ﬁle. We exit here if one of the following conditions are met:
– the video was removed, – the video is categorized as a ‘Gaming’ video, – the video does not contain any English ASR captions, – the video is over 20 minutes long (and thus might be overly expensive to download).
• Inspecting thumbnails: the YouTube API has a hidden feature that allows us to download four thumbnails [35]; in terms of bandwidth usage, this is often much cheaper than downloading the whole video. We use these thumbnails as a proxy as to whether the entire video is likely suitable for pretraining.6 We trained a lightweight MobileNet-V2 CNN [93] to score whether a COCO object class is present in an image or not, using a sigmoid cross entropy loss. We exit here if one of the following conditions are met:
– the CNN classiﬁes fewer than four COCO objects as being ‘present’ over the four frames, using a minimum threshold of 30% probability for an object to be counted as being ‘present.’ This is mainly to recognize scenes with people, as opposed to animations, landscape footage, or blank/placeholder slides.
– The average cosine similarity between all feature representations (computed by the classiﬁer) is over 0.9; this allows us to skip videos that have no visual variance (like a person sitting in front of a camera for the whole video, or an album cover while a song is playing).
• Downloading the video: if we have not exited yet, we download the video.
A.3 Denoising ASR Captions
One concern with pretraining on ASR is that written text may differ from spoken text: thus, when transferring to downstream tasks based on written corpora, models pretrained on spoken transcriptions may not transfer well. Also, ASR generated by YouTube does not include punctuation or capitalization. Furthermore, ASR transcripts can contain errors, e.g., by mistranscribing rare words/proper nouns and instead predicting incorrect, but similarly pronounced, words. And ﬁnally, YouTube’s ASR system sometimes attempts to translate text from a different language to English, which is sometimes successful, but other times produces nonsense.
6Note that YouTube thumbnails are also (algorithmically) curated: when thumbnails aren’t hand-selected by the uploader, YouTube’s thumbnail selection algorithm selects high quality, clear frames. https://ai. googleblog.com/2015/10/improving-youtube-video-thumbnails-with.html
20

We aim to sidestep these issues by using a language model to ‘denoise’ ASR text, as well to ﬁlter out excessively noisy transcripts. We use a GROVER-Large language model to do this [124], as it was exclusively pretrained on written text from news articles. Then, we ﬁnetuned it in a sequence-tosequence setting to ‘denoise’ ASR.
We created data for our ‘denoising’ task using the following procedure. Given an article from RealNews [124], we would trim it to 600 BPE tokens, and perform the following corruptions:
• We lowercase all text, and remove all punctuation.
• For each word (splitting by whitespace), we replace it with a random word 1% of the time. Within this 1%, 25% of the time, we use the CMU Pronouncing Dictionary7 to swap-in a word with identical pronunciation (to simulate mistranscriptions), and 75% of the time we use a random sequence of BPE tokens of the same length as the actual word.
• For each word, 1% of the time we insert a ‘ﬁller word’ before it, such as ‘umm,’ ‘hmm,’ or ‘yeah.’
The model was trained to generate the ‘noisy’ news article, followed by a ‘START’ token, then the original ‘clean’ news article, and then an ‘END’ token; all using a standard cross-entropy loss. We prioritize learning the ‘clean’ text by multiplying the loss on the initial ‘noisy’ tokens by 0.01. We trained this model using a batch size of 256 sequences of maximum sequence length 1536, a learning rate of 1e-5, and 80k steps.
The result is a model that not only attempts to ﬁx mistranscriptions and corruptions, but also adds punctuation and capitalization. The model also produces an estimated likelihood of the ASR caption track, which we later use to ﬁlter out videos with very low quality ASR transcripts, e.g., poorly translated transcripts.
We apply the model to each video’s transcript that survived the described ﬁltration, breaking up long transcripts into groups of 512 tokens. These groups are handed as input to the model, and Nucleus Sampling (with p=0.9) [48] is used to generate a cleaned transcript for the group. We exit, ﬁltering out the entire video, if any group has a perplexity of over 200. Finally, we concatenated all the groups together to form a ‘clean’ transcript.
A.4 Putting everything together: aligning videos and cleaned transcripts to frames
To recap, at this stage in the pipeline, for each video, we have the video ﬁle, along with the original ASR transcript (with words, as well as timestamps for each word), and the cleaned ASR caption (without timing info). To estimate timing info for the clean transcript, we align the noisy and cleaned transcripts on a word-by-word level using Dynamic Time Warping [85]; word-word distance is computed using Levenstein distance. The timing estimate for a cleaned token was computed as the average of the noisy tokens assigned to it in this alignment.
Finally, given a video and its cleaned, per-word timed transcript, we sought to extract corresponding video frames – the data format we rely on for pretraining. We start with (empty) buffers of at most L = 32 tokens for both the original, and noisy transcripts. We loop through the (aligned) clean and noisy transcripts, and add the tokens to their respective buffers. If adding the next word would cause the buffer to exceed L = 32 tokens in length, we commit the segment – returning the noisy ASR text, along with the clean text, and timing information. We then extract a frame from the video corresponding to the middle of that segment. We do this until the end of the video. We use the GPT2 BPE encoder for this [97, 88], as was also widely adopted in later work (e.g. RoBERTa [72]).
Not all videos ﬁt neatly into 16 segments, which was the format we used for training. Thus, we merged segments from videos shorter than 16 segments, and for longer videos, we split them into multiple examples. We didn’t use any video sequence-level padding: all of our dataset examples have 16 valid frames, even though we did include padding at the token level (so many segments had fewer than L = 32 tokens).
7http://www.speech.cs.cmu.edu/cgi-bin/cmudict
21

A.5 Summary - scraping while preserving privacy
As we discussed in the sections above, we tailored our scraping process to protect user privacy. It should be mentioned here that we focused on public videos. Possibly due to cues of engagement like view/subscriber counts, users on YouTube appear to understand the privacy implications of uploading a ‘public’ video [55], differentiating YouTube from more private venues, like email and social media. Under Marwick and boyd [78]’s framework of networked privacy, when web users (particularly those with less viewership) upload public videos, they are often ‘being in public without being public.’ The idea behind this distinction is that web users, understanding that their content might be visible to others, tend to avoid sharing overly private data (like their phone number or date of birth); the information that they do share is often encoded (i.e., referring to a friend by their ﬁrst name, not their full name). Finally, we took extra steps to ﬁlter out more ‘personal’ videos (without many views); our analysis in Appendix B shows this strategy was largely successful.
An additional aspect of our approach, as it relates to privacy, was our decision to use a diverse selection of channels. We did this to minimize risks of models ‘overﬁtting’ to speciﬁc individuals – a risk evidenced by a large GPT2 model memorizing users’ phone numbers [18]. We believe that training a base-sized model in a large- and diverse-data regime minimizes many of the harms in this case; that said, the risk in the multimodal (video) space is unclear as of yet, and more research is needed.
Finally, we do not plan on releasing videos for download, only their IDs, following a strategy from prior work [1, 80]. This gives users an explicit ‘right to be forgotten’ not just from YouTube, but our data as well. We understand that this might make exact reproducibility difﬁcult; we address this by releasing code for our ﬁltering process. Thus, if in the future, if N videos get deleted from YT-Temporal-180M, a practitioner can download N new YouTube videos that pass through the same ﬁlters that we used.
B Data Exploration
Curating large pretraining corpora necessitates some ad-hoc decisions, e.g., what data to search for, what data to keep/discard, etc., and our work is no exception. The described data extraction pipeline contains several heuristics that we developed based on our subjective experiences (and per-step, heuristic validations) curating the corpus. While it isn’t computationally feasible ablate each stage of this pipeline (and examine each decision’s effect on downstream performance), we seek to quantify some basic of the properties of the corpus.
Validity Check We randomly sampled 100 videos from the corpus, and answered the following basic questions for each of the videos: Q1: Does the video contain language utterances? Q2: If so, is the language primarily English? Q3: Is the video an instructional video, i.e., is it an attempt to teach the viewer how to undertake a task?8 Q4: What type of entity created the video: a small youtuber (<10K subscribers); a medium youtuber (<100K, >10K subscribers); or a large youtuber (>100K subscribers); a news station; or a media company. Q5: Is the video a music video? Q6: Is the video a video game commentary?
Of the 100 examined videos, none were music videos or video game commentaries (Q5/Q6). The videos were mostly not instructional (84%) (Q3) and mostly in English (86%) (Q2); non-English videos nonetheless can have an English ASR track provided by the YouTube API if the spoken language is transcribed by YouTube via its auto-translate feature. And while all contained language utterances (Q1), at least one translated transcript had a very low quality transcription, which was only loosely semantically related to the underlying content. Finally, the most common video creators were news studios (29%; e.g., local news channels); big YouTubers (26%; e.g., popular vloggers), and media companies (24%; e.g., Major League Baseball). Also included, but in lesser proportion, were small YouTubers (8%), and TV studios (1%; e.g., ofﬁcial movie trailers).
Content Exploration What topics are covered by the corpus? We randomly sampled 55K video transcripts, and ran an LDA topic model [16] implemented in MALLET [79] with 100 topics. We used a vocab size of 25K word types that appear in at least 25 transcripts, but in no more than 10% of
8A similar deﬁnition was proposed in [47].
22

Sports Baking Legal LifeVlog Cooking

Sports Baking Legal LifeVlog Cooking

goal win match points ball games goals played players sugar mix cup butter recipe ﬂour oven dough bowl court law justice judge investigation report prison
excited vlog tomorrow literally camera bed yesterday sauce cook oil chicken salt garlic pepper cooking

Table 5: Several common topics, derived from the transcripts of YTTemporal-180M, represented by the most common words of those topics. Figure 4: TSNE of topic distributions for 7K sampled documents.

transcripts. The topics suggest diverse coverage, e.g., topics about speciﬁc sports (boxing, soccer), US and world politics, fashion, construction, fantasy settings, nail painting, etc. We use TSNE to visualize the per-document topic distributions, and color a sample of documents according to their top topic in Figure 4 (topic details in Table 5).
Overall, the topical coverage of YT-Temporal-180M, at least according to a topic model trained on the transcripts of a sample of videos, is broader than comparable-in-size video corpora like HowTo100M [80]. And, experiments in the main paper demonstrate that this diversity is apparently helpful for a number of downstream tasks.
C Qualitative Analysis of Model Representations
In this section, we provide more qualitative analysis about the representations learned by MERLOT.
C.1 Analysis of the language-only encoder, and attention masking during pretraining
Early on in this project, when inspecting qualitative examples, we observed that using BERT-style masked language modeling [27] – choosing 15% randomly selected BPE tokens as the prediction targets, and replacing them with MASK 80% of the time, or a random token 10% of the time – produced overly easy examples.
This has been observed by other work in the text-only setting: when long words get partially masked, it is often easy to recover the missing BPE token from the context, which motivated Joshi et al. [54]’s choice to mask out entire spans instead. However, our goal in multimodal pretraining is different. We want the model to learn grounded representations of events, such that even when we scale up the number of segments given to the model, the model has to construct a multimodal representation of what happened. Thus, in our setup, we wanted to encourage masking out highly visual words, to learn cross-modal representations.
Instead of masking randomly, recall that we used the attention weights produced by the language-only encoder (trained to match a sequence of captions to individual frames) to inform which tokens to mask. While we do not claim that these attention weights provide a full explanation of the model behavior [51, 98], they do play some role in the model’s decision [116], and we ﬁnd that our masking strategy improves performance on downstream tasks by around 1% (Table 4), versus a SpanBERT baseline [54].
We show qualitative examples that seem to back up our hypothesis in Figures 5 and 6. In Figure 5, for instance, the video shows a VLOG of an adult playing with children and talking the camera. Tokens ﬂagged by our approach as having high attention weights (being in the top 20% of all tokens in the sequence, in terms of other positions attending to that token) include concrete words like ‘scissors’ and ‘toys.’ Even though scissors are not shown in the selected frames, that word might be a good prediction target, insofar as it might complete a picture of what is going on in the ﬁrst few frames: somehow, the adult is able to open the package with the child’s toy, which could require scissors.
23

##<|START|> okay i ##'ll help you i ##'ll help you we ##'re gonna need
scissors for this bad boy
okay

<|START|> okay i'll help you i<|MASK|><|MASK|> you<|MASK|>'re gonna need scissors for this bad boy okay

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> all
right wait hold
on show them other toys
too okay
let ##'s look what ##'s this look

<|START|> all<|MASK|><|MA SK|><|MASK|><|MASK|><|MAS K|> them other<|MASK|> too okay let's look<|MASK |><|MASK|><|MASK|> look

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> lebalroncinkgs we got more species and that ##'s who wawnet that basic#a#ll'ys it thien box this is for me

<|START|> learning blocks we got more species and that's who we<|MASK|> that's basically it in the box this is for me

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> oh
oklaeyt #pl#a'ys
we ##'re gopnlanya
daayll oksaoy gwoet houthsee built this
is gonna outsidgeo

<|START|> oh okay let's orchestr we're gonna play all<|MASK|><|MASK|><|MASK |><|MASK|><|MASK|><|MASK| ><|MASK|> built this is gonna go<|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> but i just
wantetod giviet himto
inside and we can just take
theosuet sdlidooinrgs whenever
we ##'re ready

<|START|> but i just wanted to give<|MASK|><|MASK|> him<|MASK|> and we can just take out these<|MASK|><|MASK|> whenever castles<|MASK|><|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARaTn|>d goti
btuhiilst andi t#ry#in'mg getot himto outsidgeo wmithe wseo csaene yobuigr birthhadpapyy sign

<|START|><|MASK|><|MASK|> got this built and i'm tr ying<|MASK|><|MASK|><|MAS K|> to go<|MASK|> with me so we can<|MASK|><|MASK|> big happy birthday sprink

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTg|>o dotwhne slyidoeu thihnek ctahlliss slidea ansdo dyooun w#a#n'tt gtoo
outsaindde ysoeuer
birhthapdpayy sign

<|START|> go down the BS you think he calls this<|MASK|> slide so and you don<|MASK|> want to go outside and see your happy birthday sign

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> i
want to go
come on let
##'s go
outside no no
there ##'s
a present outside
you go see it

<|START|><|MASK|><|MASK|> to<|MASK|> come on let's go outside no no there's a present outside you go see it

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> no
there ##'as present
out you want
to go see it yeah there ##'s and ninjaa there ##'s a ninja

<|START|> no there<|MASK|>act<|MASK|> out you want to go see it yeah there's and<|MASK|><|MASK|> there's a ninja

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> s
##utton all
right finally finally going outside
oh cool yeah isn ##'t that awesome
s ##utton
happy birthday

<|START|> sutton all right finally<|MASK|> going<|MASK|> oh<|MASK|> yeah isn<|MASK|> that<|MASK|> s Kirk happy<|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTh|>e d#oe#s'nt mcuacrhe tfhoer sibgunt doi justi needtheids phoontoe lhooowk cuothe #th#a'st
adorable

<|START|><|MASK|><|MASK|> <|MASK|> care much for the<|MASK|> but i do i just needed this one photo look how cute oh that's adorable

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> okay let ##'s go back to the
chocolate all
right guys
the crew ##'s
all here
my mom brought cookies

<|START|> okay<|MASK|>'s go back to the<|MASK|><|M ASK|><|MASK|><|MASK|><|MA SK|><|MASK|>'s all<|MASK|> my mom brought cookies

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTm|>y brosuisgthert ##crfaiswh andt ##ayalnodr ##chmelitl brcoaungdhyt thfoer litotnlee juhset woukpe frohmis nap

<|START|> my<|MASK|> brought crawfish and taylor and mitchell<|MASK|><|MASK|> for<|MASK|> little one he just woke up from his<|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTo|>h ##i'st
humsido outsidhei ##aylort literahlley
wjoukset anudp witlhikine secon1d0s gohees insidgeo houthsee

<|START|> oh<|MASK|>'s so<|MASK|><|MASK|> hi t<|MASK|><|MASK|> literally<|MASK|><|MASK|> up<|MASK|> within like 10 seconds he goes<|MASK|><| MASK|><|MASK|> house

0.00

0.01

0.02

0.03

0.04

0.05

##<|START|> and so he ##'s in there
happiethset little boy i
already went in pwahrtayt ##'s all here i
already went in

<|START|> and so he's in there the<|MASK|> little boy i already went in what party's all here i already went in

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARhTa|>d scormawe
demo#lis#hfiesdh wheriet #t#h'es
cookiesi lit#er#a'lmly
abotuot eaaltl oift
thyaonuk makifnogr cookmiees

<|START|><|MASK|> some crawfish demolished it where<|MASK|><|MASK|> gon i'm literally about to eat all of it thank you for making me<|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

24

Figure 5: Attention masking for a video of 16 frames. Our model’s image encoder learns image representations independently for each frame. A language-only encoder model takes in the entire transcript (with 32 words at most per frame) and computes hidden representations for each segment. The language encoder thus takes advantage of the inherent contextuality over time; each individual caption is not enough to understand the frame in isolation. We use the language encoder’s attention weights to mask out words. Tokens that are highly attended to (with the overall attention weights in the middle column) are shown in red and bolded. These tokens tend to be more grounded, e.g. the word ‘toys’ in the second row. The ﬁnal input to the joint vision-and-language model is shown in the third column. We mask out highly attended-to words (except special tokens like ‘START’), 50% of the time, which makes the pretraining objective much more visual than masking out random words (often ﬁllers like ‘on’ or ‘okay’).

##<|STARThTe|>n nowI
sti#rr#jiun'msgt uipt
suugtnhateirl compmleeteltlys
ittnehtaeo naonwd tfuhne p#a#rt, ygoeut poutor otvoepirt thoef

<|START|> Then now I'm just stirring it up until the sugar completely melts into the<|MASK|><|MASK|> now the fun part, you get to<|MASK|> it <|MASK|><| MASK|><|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTic|>e hopefaunlldy #dy#oo'unt slpikilel didI ybouut n#ot#ic'ell somoef tichee goinigs metlot
immediataenldy ednrittnihrkee goinigs bteo

<|START|> ) and hopefully you don<|MASK|><|MASK|> like I did but you'll notice some<|MASK|> the ice is going to melt immediately and the entire<|MASK|> is going to<|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARcTo|l>d pmreutcthy arwigahyt #Ye#s. g#ojoo#bd, JSilol nowI #go#in'mg tshtiactokt
refrigeratthoiner keetpo cooitl whwilee moovtneo

<|START|> cold pretty much right away. Yes, good job<|MASK|> So now I'm<|MASK|> to stick that in the refrigerator to keep it cool<|MASK|><|MAS K|><|MASK|> on to

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTo|u>r nsteexpt #S#o. wyaonut statrot obfyf
placjiunsgt youOr
##r#in#gthine
centeorf tlhide youtor
##asomn ##.

<|START|> our next step. So you want to start off by just placing your<|MAS K|><|MASK|><|MASK|> in the center of<|MASK|> lid to your mason.

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTJa|>r #An#d:
##acinbgr thaitt w#a#y. knYoowu
whethree holies gointgo #b#e, haenrde comthees hpaarrdt ##.

<|START|> Jarcatch<|MASK| ><|MASK|>acing it that way. You<|MASK|><|MASK|>< |MASK|><|MASK|> is going to be, and here comes the hard part.

0.00

0.01

0.02

0.03

0.04

0.05

##<|STAAlrRiDgTah|>dt #ca#n, tyeolul ywohuuys
#do#i'nrge #Y#ou? sta#rt#jiun'rsget woitfhf smallear ##omlo #O#h? y#ea#h, pliitltoleat h#o#le,

<|START|><|MASK|> Dad,<|MASK|> bunker tell us why you're<|MASK|><|MASK|> You're just starting off with a smaller loom? Oh y eah<|MASK|><|MASK|><|MASK |> pilot hole<|MASK|>

0.00

0.01

0.02

0.03

0.04

0.05

##<|STeAaRsTie|>r tfhoer
largbeitr beca#u#se,
lartbghieet jwusiltl arosulindde ri#gh#t, ##?I g#o#n'nma slightgloy lar#g#er, tahnedn ##w'ell gtheet real

<|START|> easier for the larger bit, because the large bit will just slide around, right<|MASK|><|MASK|>'m gonna go slightly larger, and then we'll get the real

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTb|i>g gooinnge #o#n. B#o#y, #th#a'st pltahne L#oo#k. thaist g#u#y, ##h'es pscreatrtyy
leakainngd #o#il.

<|START|> big one going on. Boy, that<|MASK|> breathed<|MASK|>. Look at this guy<|MASK|><|MASK|>'s pretty scary and leaking oil.

0.00

0.01

0.02

0.03

0.04

0.05

##<|STRAeRaTd|>y H#o#ly?
##olmy t#h#at, wasos sc#a#ry. hatedI t#h#at. hatedI
everyatbhoinugt ##it. mOonree t#im#e.

<|START|> Ready?<|MASK|> m<|MASK|><|MASK|><|MASK|> was so<|MASK|><|MASK|> I hated that. I hated everything about it. One more time.

0.00

0.01

0.02

0.03

0.04

0.05

##<|STAROTka|>y Ple#a#se. carefbuel ##.I ##'smo
nervo#u#s. #Th#a'st
probatbhley rigtihpt th#e#re. ##H'de slogwo #u#h, ####h###u#h.

<|START|> Okay. Please be careful. I'm<|MASK|><|MAS K|><|MASK|> That's probably the tip right there. He'd go<|MASK|> assigns uh-huh.

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTTh|>e tfiimrset
throwuegnhtI #an#d, #d#id'ntI
anythhainvge clamtpo witiht ri#gh#t, #B#u?t yoiuf slogwo ajunsdt bsuhrosratt like

<|START|> The first timeosate<|MASK|> through, and I didn't have anything to clamp it with, right? But<|MASK|> you go slow and<|MASK|><|MASK|> short burst like

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARtTh|a>t ##it,
thro#u#gge'hltl #b#ut, #w#o'nt cragzoy yooun ok#a#y, W##he? ###S#wo. oynocue
dboagwcenkt ina
#fe#w, you

<|START|> that, it'll get through, but won<|MASK|> gostanbul on you<|MASK|> okay? Whew. So once<|MASK|> get<|MASK|><|MASK|> in a few, you

0.00

0.01

0.02

0.03

0.04

0.05

##<|SmTAaRnaTg|>e getot doniet #Yo#u.
##ju'rset gointgo aheagdo galuned youOr ##ri#ng#sovtheer hoolen sbidoeths ##.

<|START|> manage to get it<|MASK|>.<|MASK|>'re just going to go ahead and glue your O-rings<|MA SK|><|MASK|><|MASK|> on both sides.

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTS|>o ##'mI
applyjiunsgt litbtlieat gluoef try#in#g, dtoo fairlay
amsomuanltl beca#u#se,
doeist takea whitleo #d#ry, oanncde you

<|START|><|MASK|> I'm just<|MASK|> a little<|MASK|> of<|MASK|>, trying to do a fairly small amount, because it does take a while to dry, and once you

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARhaTv|>e gltuhee
surrounhydooinulegr #yo#u,
##ju'rset gointgo aheagdo plaacned y#o#uOr##rinogn #An#d. thyeonu #go#i'nrge repeatot thoant

<|START|> have the glue<|MASK|> your hole, you're just<|MASK|><|MASK |><|MASK|> ahead and place your O-<|MASK|><|MA SK|><|MASK|> And<|MASK|> you're going to repeat that on

0.00

0.01

0.02

0.03

0.04

0.05

##<|STARTth|>e osthideer #An#d. oynocue hathvee plabcoeadt #yo#u, ##ju'rset gointgo ltehaevme drtyo #An#d.
meantimthiene #w#e,
#go#i'nrtgeo

<|START|> the other side. And once you have the boat placed, you're just going to<|MASK|><|MASK|>< |MASK|><|MASK|>. And in the meantime<|MASK|> we're going to

0.00

0.01

0.02

0.03

0.04

0.05

25

Figure 6: Another example of our masking approach, the same format as Figure 5. This shows an instructional video. Note the highly attended to tokens that get masked out (like ‘ice’, ‘O-ring’ and ‘lid.’) Seeing those objects in the image (not just through reading about them) is key to understand what the video is about – someone making iced tea in a mason jar.

UCF-101 [103] HMDB-51 [61]

Constant
1.1 2.0

RSPNet [21]
61.8 42.8

MERLOT-VizBranch
74.9 49.6

CLIP ViT-B/16 [89]
87.1 62.4

Table 6: Linear probing classiﬁcation accuracy of a MERLOT’s intermediate visual representations (higher=better).

Additionally, in Figure 6, showing an instructional video for both making iced tea and putting it in a sealed mason jar, concrete nouns such as ‘o-rings’ get masked out.
Nevertheless, there are still several cases where the model seems to assign attention weights to apparently non-visual tokens. The model places a lot of attention on the START token, a pattern noticed by prior work as well [24], perhaps because we pool representations from those positions (for matching with the video frames). However, we never select the START token for masking in our work, so this might not highly affect the learning signal. Perhaps more strangely, language-only encoder seems to attend highly to the ﬁnal token in contractions (like ’t and ’s). It is not clear to us whether these represent something important visually, or noise; we leave a more in-depth investigation of this phenomenon to future work.
C.2 More qualitative examples for zero-shot story ordering
In this section, we show more examples of MERLOT unshufﬂing visual stories in SIND [50, 33]. We compare our model’s zero-shot results (using the logits from its temporal-ordering objective) to CLIP’s [89] independent matching of each caption with each image (using the Hungarian algorithm to ﬁnd the best-scoring assignment [63]).
In Figures 7 and 8, we show expanded versions of Figure 3, comparing to CLIP. The examples show that MERLOT has a strong understanding of events that transcends individual frames. Unlike MERLOT, CLIP can only match captions independently to images, so in the ﬁrst row it struggles to connect ‘his kids’ with the middle-aged children of ‘the old man’ In the second row, it matches the barn image with the caption ‘they also had a barn’, while it is unable to keep all the merry-go-round images together (as MERLOT does).
We show additional examples in Figures 9 and 10. Our model provides a reasonable ordering to the ‘kayaking’ example (Figure 9), which is evident of multimodal script knowledge: ﬁrst, people have to get ready to go kayaking (which they do on land!) and then they go out onto the water, and ﬁnally come back. The ordering of the tennis match (Figure ??) seems reasonable as well. Unlike CLIP, MERLOT groups together frames (3) and (4) – the players ﬁrst serving the tennis ball, and then awaiting the return.
C.3 Attention patterns
Finally, we show examples of the attention patterns produced by MERLOT, when it reasons over both vision-and-language content at a video level. Plots are shown in Figure 11. Overall, the model frequently links together visual regions with similar concepts in text, even when they get mentioned far away in time.
Though these attention patterns should be taken with a grain of salt, as they are not necessarily explanatory of the model’s decision [51, 98], we ﬁnd it promising that the model attends globally over all frames and captions – rather than ignoring one modality or ignoring the temporal dimension. We leave further investigation of the model’s attention patterns and behavior to future work.
D Linear Probe of Intermediate Visual Representations
Our goal with MERLOT was to learn about situations expressed through videos and language. However, as it includes a vision encoder that we trained from scratch, a reasonable question is how this visual encoder compares to other encoders (e.g., that were trained through image captions). To this end, we performed linear probing experiments over two activity recognition datasets: HMDB-51
26

Ours

Tthheeeosldcamlaatonrw. as riding

He was almost to the top.

Hthies ktoidps. were already at

Stoopm. eIt pwoaliscea wtrearine at the station.

They then got on the bus.

(1)

(2)

(3)

(4)

(5)

(1)

(3)

(2)

(4)

(5)

Figure 7: Zero-shot story unscrambling; continuation of Figure 3 with the CLIP baseline [89]. MERLOT successfully orders the story, performing cross-modal coreference over several images to note that ‘He’ in image (2) refers to ‘the old man’ mentioned in (1). The narrative that MERLOT generated also makes sense at an event level: people are riding the escalator, then they get to the top, then they
exit and do something else; maximizing caption-image similarity of all pairs independently misses
this event-level coherence.

CLIP

Figure 8: An incorrect story unshufﬂing example – but for an interesting reason. Frames (1), (2), and (4) all involve people riding a merry-go-round, and MERLOT keeps them together even though the ground truth story labels have the ‘barn’ image, (3), in between.
27

Tisogdoaiyn,gthtoe gwohokaleyafakminigly at the lake.

Hinesrteruwcteioanrsefgroemttinthge guide.

Twhaeteyrhsahdoeuss.wear these

Tghroeurep,ghoaevsinsgomfuen.of the

Mthoemhaanngd oDfatdhirsesaullpyegrot fast!

(1)

(2)

(3)

(4)

(5)

Ours

CLIP

Ours

(4)

(2)

(3)

(5)

(1)

Figure 9: A second zero-shot story ordering example. MERLOT unshufﬂes the frames, while grouping together frames (1) and (2) – which make sense as they are in the stage of the event where they are preparing to go. CLIP instead puts frame (4) ﬁrst, which matches caption (1) indepedently,
but doesn’t make sense temporally in context.

Idto'sutbimlese mfoarttchhe. local

Twhaeiticnrgowfodr itsheexmciatetcdhatnod start.

Isteirsvtei.me for the first

Itthgeoyewsaoitveforrththeenreettuarnnd.

Ugentfsorhteuantaetdelyantdhesommaetcohne itnejaumrestetnhneims.selves. Go

(2)

(1)

(3)

(4)

(5)

(3)

(2)

(4)

(1)

(5)

Figure 10: A second zero-shot story ordering example. There are a variety of potential ‘reasonable’ orderings for this example; both models get this one ‘incorrect.’ MERLOT’s ordering suggests someone ﬁrst looking into the tennis match on the outside, and then cutting to watch the match more
closely. On the other hand, CLIP switches between a shot of someone serving, back to the outside
TV, and then inside again.

28

CLIP

Figure 11: Additional qualitative examples of MERLOT’s attention patterns, aggregated over all layers of the joint vision-language encoder. Cells on the top attend to cells on the bottom; we only show three attention edges per query, so as to reduce clutter. In red, we show visual patches attending to other visual patches; in gold, we show tokens attending to visual patches; in teal we show tokens attending to tokens, and in purple we show patches attending to tokens. The ﬁrst row seems to show a tourist in the United Kingdom. In the third segment, the narrator discusses a ‘gothic style house’ even though only the gate is shown in the frame; those tokens attend highly to the house when it is shown in the fourth frame. The second row shows someone at a factory for Dr. Bronner’s Soap. The factory worker in the third frame seems highly attended to, particularly by the tokens ‘applied by hand’ which appear in the second caption. The third row shows a dinner party. The ﬁrst caption mentions ‘nice food’ but no food is shown in the ﬁrst frame. Interestingly, the model has these tokens attend to the ﬁnal frame, where food is shown.

####<<||SMTAASRKT|>|>(table) #Or#g.an #by#ized Aanndna #fro#m<|MASK|> (Kath) TNhe ####<<||MMAASSKK||>> ((####abroraotw) ) Eanxpderience #lo#ts<|MASK|> (having) #go#o<d|MASK|> (of) c#h#a,tter nfoicoed #a #<|MASK|> (and) #go#o<d|MASK|> (generally) t#im#e.

##<|M#A#SK<||>ST(AtaRbTl|e>) Or#ga#n. ##izebdy Anannda
##<|MASK|> (Kfarothm) ThNe
####<<|M|MAASSKK|>|>(#(###abrrooawt)) Experieanncde
##<|MASK|> (havilnogts) ##<|MASK|>go(oofd) cha#tt#er, fnoiocde
##<|MASK|> (anda) ##<|MASK|> (genergaolloyd)
t#im#e.

#it #<|START|> #al#l 's a#b#o<ut|MASK|> (##.) ####<<||MMAASSKK||>> ((Dmuyr)ing) ####<'s|MASK|> (grandfather) ####<, |MASK|> (time) e#v#e<ry|MASK|> (bottle) ####<<||MMAASSKK||>> ((osof)ap) wfilalesd bhaynd wgriathvity ####<fe|dMASK|> (##-) l#ig#h<t |MASK|> (##,) aenvedry #wa#s<|MASK|> (label) asticker

##<|START|>it ##a'lsl
##<|MASK|> a(#bo#u.t) ##<#|#M<A|SMKA|>SK(|D>ur(mingy)) ##<|MASK|> (grandfa#th#er's)
##<|MASK|> (ti#m#e), ##<|MASK|> (beovttelery) ###<#|M<A|MSKA|S>K|(>so(aopf))
fiwlleads hanbdy grawviitthy ##<|MASK|>#(##f#e-d) ##<|MASK|> (#lig#h,t) evaenrdy ##<|MASK|> (lawbeals) stickear

#I #<|START|> rloevaelly JGurdeyen #sc#re'sen ####<<||MMAASSKK||>> ((s#t#or.e) ) #ar#e<|MASK|> (There) adlisffoerent s#e#c-ond ####h<a|MndASK|> (shops) s#e#ll<in|gMASK|> (designer) c#lo#t,hes f#u#rnitures ####,<|MASK|> (cro) ####<er|iMesASK|> (##ck) asond much

##<|START|>I reloavlley GrJueedny sc#re#e'ns
####<<|M|MAASSKK|>|>(s(t#o#re.)) ##<|MASK|> (Thearree)
differaelsnot sec#on#d##<|MASK|>#(#shhoapnsd) ##<|MASK|> (dessieglnlienrg) clot#h#es, ##itufurerns ##<|MASK|> (#cr#o), ##<|MASK|>#(##e#rcieks)
ansdo much

#No#t<|START|> ecovuelrdyone m##ak<e|MASK|> (it) u#n#fo<r|tMunAaStKe|l>y (and) stuormneed #la#te<|MASK|> (up) #Lo#n<do|MnASK|> (like) BGoiral t ahfetrer #ha#d<|MASK|> (car) arather dbalonwgerous #on#out tmhoetor #bu#tway sishe

##<|STARNT|o>t evercyoounlde
##<|MASK|m>a(kite) ##<|MunAfSoKr|t>un(aatnedly)
tusronmede ##<|MASK|> (luapte) ##<|MASK|L>on(ldikoen)
BGoairtl afhteerr ##<|MASK|> (chaard) rathear dangebrolouws ##oount motthoer ##wbauyt shies

#ap#p<lie|SdTART|> bhaynd #Si#nc.e t#h#en<|MASK|> (##,) ####<<||MMAASSKK||>> ((tkoe)ep) uwpith #m#an<|MASK|> (the) ####,<|MASK|> (we) #in#st<al|lMedASK|> (##'ve) f#iv#e<|MASK|> (automated) b#i#lli<ng|MASK|> (machines) #bu#t, wsteill uthseese g#r#av- ity #lin#efsed

##<|SaTpApRlTie|>d hanbdy Si#nc#e.
##<|MASK|> (#th#e,n) ###<#|M<A|MSKA|S>K|(>ke(etop))
wiutph ##<|MASK|> (mthaen) ##<|MASK|> (#w#e), ##<|MASK|> in(#st#a'lvleed) ##<|MASK|> (automatfeivde) ##<|MASK|> (machbiinlliensg)
#b#ut, swtiell thuessee gra#vi#ty##linfeeds

#m#or<e|START|> ####.<|MASK|> (Talking) a#b#o<ut|MASK|> (cute) ####<le|yMsASK|> (al) #I #, caacmroess tchhiasrming coeldnturies pcaulbled tHhoelly B#u#s<h|MASK|> (Pub) #It #. #a #'s rgeeaml aonned Women (of)

##<|STAmRTo|r>e ##<|MASK|> (Talk#in#g.)
##<|MASK|> (acbuotue)t ##<|MASK#|#>le(ayls) ##,I accaromses charmtihnigs centuroields caplluedb Hothlley
##<|MASK|> (BPuusbh) ##It. ##'as greemal aonnde
Women (of)

#ok#a<y |START|> ####.azo (There) wfuans #la#ug, hter #Ch#r,istmas c#r#acekrs #Ch#r,istmas #an#d<|MASK|> (pudding) bjoakdes aas mofeasure ehxoawctly ####<<||MMAASSKK||>> ((bthaed)) c#r#acekr jwoekrees ##.

##<|STAoRkTa|>y ##azo (The#r#e).
wfuans laugh#t#er, Christm##as,
#c#raecrks Christm##as, ##<|MASK|> (puddianngd)
jobkaeds aas
measuroef exahcotlwy ####<<||MMAASSKK||>>((bthade))
c#r#acekr jwokeeres ##.

#fo#r <|START|> hfilalinndg srumnasller #W#he. n wmeade tchoinsversion #we#, r#e#t rained oeumrployees s#o#<|MASK|> (there) ####<<||MMAASSKK||>> ((wnoa)s) #lo#ss<|MASK|> (job) #W#e. jmusatde isto tehvaetry

##<|STARTfo|>r fhilalinngd
smraullnesr W#he#n. mawdee
converstihoins #w#e,
##rainreedt employeoeusr ##<|MASK|> (therseo) ####<<|M|MASAKS|K>|>(w(naos)) ##<|MASK|> (ljoosbs)
#W#e. majudset
siot evthearyt

#He#m<p|START|> ####s<t|eMaAdSK|> (##'s) ####<m|aMbAleSK|> (Instagram) ####<<||MMAASSKK||>> ((stoitoe)s) #M#ov.ing o#n#, Iwalk tBilrl ####ahnecr tsopot tahmisazing Gstoytlheic ####<<||MMAASSKK||>> ((hcaolulesde)) ####<<||MMAASSKK||>> ((tDheel)) rBeupilltacements (##i) itnhe

##<|STAHReTm|>p ##<|MASK|#>#(#st#ea'sd) ##<|MASK|> (In#st#amgraabmle) ####<<|M|MASAKS|K>|>(s(ittoeos))
Mov#in#g. #o#n, walkI tBilrl
####ahnecr spotot
amaztihnigs Gosttyhliec
####<<||MMAASSKK||>> ((hcaolulesde)) ####<<||MMAASSKK||>> ((Dtheel)) replacements (#Bu#ilit) thine

#Le#t<|START|> #he#a'rs o#n#e<|MASK|> (from) ####<<||MMAASSKK||>> ((J#a#s)mine) #th#en<|MASK|> (and) byoeuing ####teurbs iwneevitably w#e#n<t|MASK|> (outside) aanlld s#t#arpthedone (filming) eotahcehr #Yo#u. mwealyl hseaevne soof me

29 ##<|STARTL|e>t #he#a'sr ##<|MASK|> (froomne) ##<|#M#AS<K|M|>AS(#K#|>m(iJnaes)) ##<|MASK|> (tahnedn)
beyinogu ####teurbs inevitabwley ##<|MASK|> (outwsidene)t
anadll ##phone (fsiltmarintegd)
oetahcehr #Y#ou. mwaeyll hseaevne somoef

#pe#rs<o|nSTART|> #ef#fo'rsts wmeurletiplied ####.<|MASK|> (Here) waree itnhe mofiddle tahuetomated b#o#ttling a#r#ea. Aitnd i#s#<|MASK|> (loud) #It #. aulslowed tkoeep rwuinthnoinugt

##<|STpAeRrsTo|>n ef#fo#rt'ss
multipwleierde ##<|MASK|> (He#r#e).
awree thine middolef automattehde ##bliontgt a#re#a. Anidt ##<|MASK|> (loudis) ##It. alloweuds keetop rwuinthnoinugt

#ye#a<r |START|> 1#8#68 #th#is, apmiecaezing oafrchitecture ias mvisuistt #Al#so. #I #, eunpded caapturing H#a#r<ry|MASK|> (Potter) tphicetmureed fmory I#n#st.agram

##<|STARyeTa|>r ##6188 #th#is,
ampazieincge architecturoef
ias mvuissitt A#l#so. ##,I enduedp capturinga ##<|MASK|> (PoHtatrerry) tphiecmtuerde
mfoyr Instagr#a#m.

[61] and UCF-101 [103]. These tasks are 51 and 101 class classiﬁcation tasks, respectively: they challenge algorithms to predict which human activity is present in a video clip. Following prior work, for both datasets, we average over the three standard train/test splits. We evaluate in the linear probe setup, where models represent video clips as a single ﬁxed vector, and a linear maximum entropy classiﬁer is trained on top, freezing the rest of the model’s parameters.
In addition to a random prediction baseline, we compare against [21]’s RSPNet reported results (they use a 3DResNet-18 backbone pretrained on Kinetics400), and CLIP ViT-B/16 [89]. For MERLOT and CLIP, we extract a single central frame from each video, and extract a feature vector from it. For MERLOT, we represent the frame as the concatenation of the two [CLS] tokens (one was for the image-transcript alignment task, the other was for passing to the joint encoder).
The results, shown in Table 6, show that CLIP performs best in this setup – though MERLOT does outperform an RSPNet baseline. At ﬁrst, this might appear surprising, as MERLOT was trained on web videos, which might be closer to activity recognition datasets (as opposed to image captions). However, common benchmarks for activity recognition tend to have strong object and background bias – for example, to recognize the UCF action ‘playing guitar,’ it is sufﬁcient to detect a guitar in an image (as guitars are unlikely to show up for the other activities like ‘playing basketball’) [70]. Temporal self-supervised learning from transcripts may not lead to as powerful zero-shot object detectors because speakers in videos may be less likely to state the obvious [41, 39], e.g., in this case, a speaker is probably unlikely to say ‘I will now play a guitar while sitting in a chair.’
E Experimental setup and hyperparameters
E.1 Hyperparameters used during pretraining
We used AdamW [73] with a learning rate of 3e − 4, weight decay with value 0.1, and set β2=0.98. We used minimal data augmentation on the image frames. We randomly scale them between 1.125 and 1.5 times what would ﬁt in our 192 × 352 resolution, and take a random crop. We use a random resize algorithm when doing this scaling, to make the model robust to different ways of preprocessing images [94]. Last, for 80% of images, we randomly jittered either their brightness or contrast to between 0.7 and 1.3 their original values, which we suspect did not play a major role in performance.
On the text side, we note that we have both the original copies of each transcript – what was retrieved from YouTube – and versions “cleaned up” by our denoisiﬁer. We can use both kinds of transcript as additional data augmentation. However, although the words are time aligned, there might be inconsistencies if alternating between cleaned and noisy versions inside of a single video. Thus, for each iteration, we randomly choose either the ‘clean’ or ‘noisy’ ASR transcript and use that one.
To slightly speed up convergence, we initialize the joint vision-and-language model, and the word embeddings, with parameters from RoBERTa [72]. However, we suspect that due to the scale of our dataset and pretraining time, this might not have been required.
E.1.1 Unsupervised Story Ordering
[20]
For the unsupervised scrambling of visual stories task, we did not do any ﬁnetuning on the SIND dataset [33, 50, 2]. However, there is a slight mismatch between the model that we pretrained initially, and the format of the task – the visual stories in the SIND dataset have 5 images and captions each, whereas we initially pretrained with at most 4 segments. We handled this discrepancy by pretraining MERLOT for 10 more epochs, using a peak learning rate of 2e-5, and a new resolution of 384 x 384. This slightly bigger size was to account for the (not necessarily) widescreen images in SortStory, as opposed to the (mostly) widescreen videos on YouTube.
Recall that MERLOT’s pairwise loss is deﬁned over pairs of segments. However, how to best combine these into a uniﬁed score for story ordering is an open question. To brieﬂy explore this, during this additional pretraining of MERLOT, we applied three variants of our temporal loss: one over caption-caption pairs, one over caption-frame pairs, and one over frame-frame pairs. We also experimented with randomly shufﬂing the captions as well, in the same way as the frames, we found however that this did not boost downstream task performance (perhaps because using shufﬂed captions as input incentivizes models to learn exclusively language-language interactions). The loss
30

is computed the exact same way everywhere; the only differences is that for caption-frame pairs, we have four options:

1. the caption (at ti) and frame (at tj) are of the same segment, so ti = tj, 2. the caption precedes the frame, so ti < tj, 3. the caption comes after the frame, so ti > tj, 4. the caption comes from a different video as the frame, so comparing ti and tj is undeﬁned.

The model learns to distinguish between those four options with a cross-entropy loss. We found that using this version of the temporal loss over vision-language pairs produced slightly better results on story ordering (as judged on the validation set) compared with the loss applied over the frames. We hypothesize that this might be due to the additional ‘ti = tj’ option allowing models to assign a probability to a frame-caption match, but are not sure. With this approach, to produce a uniﬁed score for (length-N ) permutations σL over the captions, and σV over frames, we then sum over pairwise log-probabilities:

N N p(σL(i) > σV (j)) 

score(σ) =

log p(σL(i) = σV (j))

i=1 j=1 p(σL(i) < σV (j))

if σL(i) > σV (j) if σL(i) = σV (j) . if σL(i) < σV (j)

For story ordering, the order of the captions is always ﬁxed: σL = (1, 2, 3, 4, 5) and N = 5; we thus feed MERLOT captions with the correct order. However, the model should have no information about the order of the frames.9 Recall that we handle this through position embeddings (3.3); e.g. one possible ordering might be

[image_unk_3], [image_unk_2], [image_unk_4], [image_unk_1], [image_unk_5],

and those position embeddings would get added to each frame, respectively. This allows the network to disambiguate between distinct frames even though no order is revealed. However, we found that the model was sometimes sensitive to the exact order of these position embedding tokens, and so for each example we randomly sampled two orderings and averaged the model’s pairwise probabilities. We found no difference in performance when using more than two orderings. We hypothesize that this could be an issue with how (absolute) position embeddings are handled by Transformers, but are not fully conﬁdent; we leave a more thorough investigation for future work.

E.2 Per-downstream ﬁne-tuning details.
In this section, we discuss implementation details for ﬁnetuning MERLOT on downstream tasks. For each downstream task, given images I1:N and language context w, we ﬁrst encode I1:N via the image encoder. We concatenate this with word embeddings of w, apply position embeddings, and feed the result into the joint vision-language encoder to extract joint representation. The input images I1:N are either provided by the task or extracted from given video, where we uniformly select N frames from the video clips (spaced evenly, so with an equal amount of time between sequential frames). For supervised tasks, we use as the ‘head’ a two-layer MLP from random initialization on top of the CLS token of the language context together with the rest of MERLOT.
For downstream tasks, we note that we found it effective to ﬁnetune on different resolutions than what we used during pretrianing. Our default image resolution here was 384× 704. To do this, we note that all parameters in the model remain the same, except for position embeddings on the image patches. We expanded the size of the position embedding matrix by initializing the upper-left-side 192x352 region from the pretrained model, and used random initialization for new position embeddings.
For all downstream tasks, we followed the standard training, validation, and test splits of the original datasets. We used the AdamW [73] optimizer, with β2 = 0.98, and warmed up the learning rate linearly for the ﬁrst 10% of iterations, followed by a linear decay of the learning rate (down to 0) for the remaining 90%. For regularization, we used L2 weight decay with a value of 0.01, and a dropout rate of 10%. For tuning other hyperparameters, we ﬁrst did a larger random hyperparameter search over VCR, and used those hyperparameters as defaults for the other tasks. We used a batch size of
9Embarassingly, we found a slight leakage of this in the V1 of this arxiv paper which inﬂated the story ordering performance by a few percentage points (of pairwise accuracy), which we have corrected in this version.

31

64, and searched over learning rates in the range [1e-5, 2e-4] on VCR, we found that 1.2e-5 worked well, so we used it as the default for other tasks. We also trained with early stopping, validating every epoch and returning the best-performing model across epochs. Due to our choice of early stopping, we trained for a slightly larger-than-typical number of epochs (18 by default for every tasks, as we found training longer did not help on VCR).
We follow the standard evaluation metrics for these tasks, which is usually accuracy for QA-style conﬁgurations. Alongside brief descriptions of each downstream task, we provide hyperparameter and training details in the following section.

E.3 Static Image Reasoning Tasks
E.3.1 VCR
VCR [123]contains two different subtasks: question answering (Q→A) and answer justiﬁcation (QA→R), both of which are multiple choice questions over a given image. These subtasks are combined in the joint Q→AR metric, which requires a model to both pick the right answer and the right rationale for the model to get a question ‘right.’ VCR has 290k questions over 110k movie scenes.
As mentioned in the main text, VCR provides bounding boxes around entities, with explicit groundings between those entities and references in questions. We draw colored highlights around the referenced entity directly in the image, with consistent mapping between color code and entity name (e.g. person1 with red box, person2 with green box, etc). Though no text is written on the image, because we always associate each string (e.g. person1) with a deterministic color, the model can learn through ﬁnetuning to associate that color with the entity. Figure 12 illustrates one such example.

What is [person1] thinking?

Q ➜A

a) [person2] is thinking she would rather have gone to the science museum of the beach. b) She wants [person8] to sit back in [chair1] and let her take care of the cleanup c) She can’t believe what she is seeing d) [person1] is wondering if [person2] is going to kiss her

I think so because …

QA ➜ R

a) [person6] is leaning backwards and has an expression of confusion. b) [person1] is staring at [person2] with a questioning expression. c) They are dancing and they are looking intently at each other. d) [person1] is unsure what will happen next.

Figure 12: A VCR example with highlighted image. The image with the drawn-on boxes is what we pass to models.
We jointly ﬁnetune MERLOT on Q→A and QA→R, with two separate MLP heads. We concatenate the question (the question and the ground truth answer) and each answer (rationale) choice from the four possible answer (rationale) candidates. On-top of the CLS token of the question, we train the classiﬁer to predict the conﬁdence for each candidate to be correct with cross-entropy loss, and take softmax over four possible candidates for each question. We used a widescreen resolution of 384×704 set the batch size as 64, and train for 60k training steps, which is roughly 18 epochs. We started with this and then tuned the learning rate (from candidates chosen randomly); here, we found that a learning rate of 1.2e-5 worked well. We then used this learning rate as a default for the other tasks.
Note that our pretraining setup is different from other work. Previous works [22, 36, 119] conduct what they call ‘second-stage pretraining’ with VCR training data. Here, they use a masked language model objective over the VCR dataset (instead of answering the question correctly). In particular, UNITER [22] reports 2.8 % point performance boost due to the second-stage pretraining. We suspect that this might be because the caption data (that models like UNITER rely on) are quite different from VCR. We tried performing secondary pretraining and found it did not help. One possible reason might be that our large-scale pretraining corpus covers diverse and complex event space thus we don’t need additional data domain adaptation.

32

AMU [117]
VQA-T [118] MERLOT

What (50K)
26.2 35.5 37.0

Who (20K)
43.0 51.1 52.9

How (2K)
80.2 -
85.3

When (677)
72.5 81.0 79.2

Where (250)
30.0 43.5 42.8

Overall
30.5 41.5 43.0

Table 7: Per question-category results for MSRVTT-QA.

E.4 Video Reasoning Tasks
MSRVTT-QA [117]
MSRVTT-QA is a question-answering task with 244K questions posed over 10K videos. For each video clip, we uniformly selected 5 image frames (spaced evenly through the video). We follow the protocols of the original work and use an answer vocabulary containing the most common 1K answers in the training set as answer candidates. The questions with out-of-vocabulary answer will automatically get wrong. We encode the answers in a one-hot fashion, and train 2-layer MLP classiﬁer over all answer candidates with a binary cross-entropy loss on-top of the CLS token of the question. We train for 60k training steps with batch size 16. A few additional ﬁne-tuning runs were conducted to examine the effect of changing the resolution from 384×704 to 704×704, a batch size of 16 vs. 32, and and using 1.5K answers instead of 1K, but none had much impact on validation accuracy. We undertook a light hyperparameter optimization over the validation set, wherein we considered 3 possible learning rates (1.2e-5, 6e-5, 2.4e-6), but the default worked best. MSRVTT-QA splits questions by type, and we report our per-type test set results in comparison to [117, 118] in Table 7.
TVQA [64]
TVQA is a multiple choice task with 152K questions posed over 21K video clips. For each clip, we uniformly select 6 image frames. We concatenate the question and each answer choice from the ﬁve possible answer candidates. On-top of the CLS token of the question, we train 2-layer MLP classiﬁer to predict the conﬁdence for each candidate to be correct with cross-entropy loss, and take softmax over ﬁve possible candidates for each question. We set the batch size as 64, and train for 35k training steps (roughly 18 epochs over the corpus). We used the default learning rate of 1.2e-5, and a resolution of 384×704.
TVQA+ [65]
TVQA+ is a subset of TVQA, where bounding boxes are provided in video clips, linking depicted objects to visual concepts in questions and answers. TVQA+ contains 29.4K questions posed over 4.2K video clips. We uniformly select 6 image frames per video, and draw bounding boxes on each frame following the same manner with VCR. We train the classiﬁer in the same way with TVQA. We trained with the same hyperparameters as TVQA, but for 16k steps (18 epochs still).
VLEP [66] VLEP is a binary choice task to infer which of the two events is more likely to happen next following the given video. VLEP contains 28.7K questions posed over 10K video clips. For each clip, we uniformly select 6 image frames. On-top of the CLS token of the event, we train 2-layer MLP classiﬁer to predict the conﬁdence for each event to happen next with cross-entropy loss, and take softmax over two possible events for each instance. We trained the model for 8k steps (18 epochs over the dataset), and with otherwise default hyperparameters.
DramaQA [23]
DramaQA is a multiple choice task with 17.9K questions posed over 23.9K video clips. For each clip, we uniformly select 6 image frames. We concatenate the question and each answer choice from the ﬁve possible answer candidates. On-top of the CLS token of the question, we train 2-layer MLP classiﬁer to predict the conﬁdence for each candidate to be correct with cross-entropy loss, and take softmax over ﬁve possible candidates for each question. We trained for 3.5k steps (18 epochs) with otherwise default hyperparameters. A few additional ﬁne-tuning runs were conducted to examine the effect of changing the resolution between 384×704, 512×512 and 704×704, and we found 512×512 works the best for this task.
33

Learning rate
Weight Decay β2 Warmup ratio

Common hyperparameters
1.2e-5 0.01 0.98 10%

VCR MSRVTT-QA TVQA TVQA+ VLEP DramaQA TGIF-Action TGIF-Trans TGIF-FrameQA ActivityNetQA LSMDC-FIB LSMDC-MC MSRVTT-MC

Resolution
384x704 384x704 384x704 384x704 384x704 512x512 384x704 384x704 384x704 384x704 384x704 384x704 384x704

Batch Size
64 16 64 64 64 64 16 16 16 16 16 16 16

Max Epochs
18 18 18 18 18 18 56 22 56 10 8 12 12

Training Steps
60k 35k 35k 35k 18k 18k 70k 70k 70k 34k 150k 80k 80k

Table 8: Hyperparameters for ﬁnetuning on all downstream tasks. Common hyperparameters are shown to the left, and task-speciﬁc hyperparameters are to the right.

Motion Spatial Temporal Yes-No Color Object Location Number Other All

VQA-T [118] 28.0 17.5

4.9

66.3 34.3 26.7

35.8

50.2 36.8 38.9

MERLOT 33.9 18.1

4.0

72.5 36.2 24.5

36.5

51.7 37.8 41.4

Table 9: Per question-category results for ActivityNetQA

TGIF-QA [52]
TGIF-QA is web GIF VQA, which requires spatio-temporal reasoning from visual frames to answer questions correctly. We ﬁnetuned MERLOT on three tasks in TGIF-QA benchmark,
Action is deﬁned as a multiple choice question about identifying an action that has been repeated in a video.
Transition is asking about transitions of certain states. The benchmark provides a multiple choice question about identifying the state before or after another state.
FrameQA is asking open-ended questions about the given video. The model selects answer from a dictionary of words, given a question in a complete sentence.
For each video clip, we uniformly select 5 image frames. We serialized 5 candidate answers and a question, where we put a special token QSEP between the candidate answers and question to concatenate them into one question. On-top of the CLS token of the question, we trained 2-layer MLP to predict the conﬁdence of the ﬁve candidates with cross-entropy loss. We set the batch size as 16, and train for 70k training steps (Action : 56 epoch, Transition : 22 epoch, FrameQA : 28 epoch) for each task with 1.2e-5 learning rate. We used a longer training duration for each task as we found that performance increased when we did so (and we used the same number of training steps for each TGIF-QA task). All other hyperparameters were default.
ActivityNetQA [45, 122]
ActivityNetQA [122] is a question-answering with 58K questions posed over 5.8K videos. For each video clip, we uniformly select 5 image frames. We use an answer vocabulary containing the most common 1K answers in the training set as answer candidates. The questions with out-of-vocabulary answer will automatically get wrong. We encode the answers in a one-hot fashion, and train 2-layer MLP classiﬁer over all answer candidates with a binary cross-entropy loss on-top of the CLS token of the question. We set the batch size as 16, and train for 34K training steps for each task. We undertook a light hyperparameter optimization over the validation set, wherein we considered 3 possible learning rates (1.2e-5, 6e-5, 2.4e-6), but the default worked best. A few additional ﬁne-tuning runs were conducted to examine the effect of changing the resolution from 384×704 to 704×704, a batch size of 16 vs. 32, and using 1.5K answers instead of 1K, but none had much impact on validation accuracy. ActivityNetQA splits questions by type, and we report our per-type test set results in comparison to [118] in Table 9.
LSMDC FiTB QA [76, 92]
34

The Fill-in-the-blank (FiTB) task is, given a video clip and a sentence with a blank in it, to predict a single correct word for the blank. The test set includes 30,000 examples from 10,000 clips (i.e. 3 blanks for each description). For each clip, we uniformly select 5 image frames. We constructed answer vocabulary containing the most common word for blank in the training set as answer candidates. We replace the blank in the sentence with BLANK token, so the question query should be a blanked sentence with the special token. On-top of the CLS token of the blanked sentence query, we trained 2-layer MLP classiﬁer to predict the word for the blank over answer vocabulary. We set the batch size as 16, and train for 150k training steps (8 epoch) with 1.2e-5 learning rate.
LSMDC Multichoice [110]
Given a video query and 5 candidate captions, the task is to ﬁnd the one that ﬁts the query out of 5 possible candidates. The correct answer is the ground-truth (GT) caption, and four other negatives are chosen from other captions that have different activity-phrase labels from the correct answer. We randomly created 100,000 video and candidates pairs for training. For each video clip, we uniformly select 5 image frames. We put a special token QSEP between the candidate captions to concatenate 5 candidates into one question. At the end of the 5 captions, we put CLS token as an end of the question. On-top of the CLS token, we trained 2-layer MLP to predict the conﬁdence of the ﬁve candidates with cross-entropy loss. We set the batch size as 16, and train for 80k training steps (12 epoch) with 1.2e-5 learning rate.
MSRVTT Multichoice [121]
The task objective for the MSRVTT Multichoice benchmark is identical to those of corresponding tasks in the LSMDC benchmark [110]. The benchmark has 2,990 questions in total for the multiple choice test, using all the test video clips of MSR-VTT. For each test video. We ﬁnetuned our model on MSR-VTT train split, and evaluated on the evaluation set. We trained the same model speciﬁcation as the LSMDC Multichoice task. For training, we set the batch size as 16, and train for 80k training steps (12 epoch) with 1.2e-5 learning rate.
F Datasheet for YT-Temporal-180M
In this section, we present a DataSheet [37, 12] for YT-Temporal-180M, synthesizing many of the other analyses we performed in this paper.
1. Motivation For Datasheet Creation
• Why was the dataset created? In order to investigate learning events from videos – involving a collection of frames and captions over time, that together form a view about the world.
• Has the dataset been used already? No. • What (other) tasks could the dataset be used for? Possibly other types of represen-
tation learning, with or without ASR captions. • Who funded dataset creation? This work was funded by DARPA MCS program
through NIWC Paciﬁc (N66001-19-2-4031), and the Allen Institute for AI.
2. Data composition
• What are the instances? The instances that we consider in this work are videos, paired with ASR transcripts aligned over time.
• How many instances are there? We include 6 million videos. The total length of all the ASR transcripts is 5 billion BPE tokens. Altogether, we extracted 180 million image frames from this data.
• What data does each instance consist of? The instances have ‘raw’ video frames and text, which we preprocess through BPE tokenization and extracting frames for every 32 BPE tokens.
• Is there a label or target associated with each instance? We only use the ASR captions as labels in this work, though it might be also possible to use auxiliary information (like tags or video titles).
• Is any information missing from individual instances? No.
35

• Are relationships between individual instances made explicit? Not applicable – we do not study relations between different videos (e.g. made by the same creator), though this is a possibility for future work
• Does the dataset contain all possible instances or is it a sample? Just a sample.
• Are there recommended data splits (e.g., training, development/validation, testing)? We do not provide recommended data splits at this time, as this data was built only for pretraining rather than evaluation. We suspect that the data is large enough that overﬁtting is not a major concern.
• Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. Yes. YouTube ASR is often noisy, and though we presented a pipeline to correct some of these errors, there are many that we cannot ﬁx.
• Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? The dataset is self-contained. However, we plan to only release the video URLs, rather than the videos themselves, so as to protect user privacy (allowing users to delete videos).
3. Collection Process
• What mechanisms or procedures were used to collect the data? We used the YouTube API and the youtube-dl library.
• How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data? The data was directly observable (from YouTube).
• If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with speciﬁc sampling probabilities)? We used a probabilistic strategy with many heuristics, more details in Appendix A.
• Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? Data collection was primarily done by the ﬁrst authors of this paper.
• Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. The data was collected from November 2020 to April 2021, even though the YouTube videos are often much older (dating back to when the platform was ﬁrst created).
4. Data Preprocessing
• Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? Yes, we discuss this in Appendix A: of note, we use a sequence-to-sequence model to ‘denoise’ ASR transcripts (Appendix A.3), BPE-tokenize text, turn everything into segments, and extract the middle image frame for each video segment.
• Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the ‘raw’ data. The raw data was saved, but at this time we do not plan to release it directly due to copyright and privacy concerns.
• Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. We will make our code public to support future research.
• Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the ﬁrst section of this datasheet? If not, what are the limitations? We believe our dataset does allow for study of our goal – indeed, it covers grounded temporal situations from a variety of domains – but with signiﬁcant limitations. Some of the key ones we are aware of involve various biases on YouTube, which we discuss in Section 5.
5. Dataset Distribution
36

• How will the dataset be distributed? At this time, we plan to distribute all the metadata (transcripts, etc) that we used, as well as links to the YouTube videos that we used. We will do this on our website.
• When will the dataset be released/ﬁrst distributed? What license (if any) is it distributed under? We will release it as soon as possible, using a permissible license for research-based use.
• Are there any copyrights on the data? We believe our use is ‘fair use,’ however, due to an abundance of caution, we will not be releasing any of the videos themselves.
• Are there any fees or access restrictions? No. 6. Dataset Maintenance
• Who is supporting/hosting/maintaining the dataset? The ﬁrst authors of this work. • Will the dataset be updated? If so, how often and by whom? We do not plan to
update it at this time. • Is there a repository to link to any/all papers/systems that use this dataset? Not
right now, but we encourage anyone who uses the dataset to cite our paper so it can be easily found. • If others want to extend/augment/build on this dataset, is there a mechanism for them to do so? Not at this time. 7. Legal and Ethical Considerations • Were any ethical review processes conducted (e.g., by an institutional review board)? No ofﬁcial processes were done, as our research is not on human subjects, but we had signiﬁcant internal deliberation when choosing the scraping strategy. • Does the dataset contain data that might be considered conﬁdential? No, we only use public videos. • Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why Yes – many of these videos exist on YouTube; we discuss this more in Section 5. • Does the dataset relate to people? Yes. • Does the dataset identify any subpopulations (e.g., by age, gender)? Not explicitly (e.g. through labels) • Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? Yes, our data includes celebrities, or other YouTube-famous people. All of the videos that we use are of publicly available data, following the Terms of Service that users agreed to when uploading to YouTube.
37

