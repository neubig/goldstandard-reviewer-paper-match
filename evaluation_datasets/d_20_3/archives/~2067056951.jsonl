{"id": "caabc3d0c5ece9d44fb2216a347362d4609934c1", "content": {"title": "A Systematic Evaluation of Large Language Models of Code", "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.", "year": 2022, "ssId": "caabc3d0c5ece9d44fb2216a347362d4609934c1", "arXivId": "2202.13169", "link": "https://arxiv.org/pdf/2202.13169.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"]}}
{"id": "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09", "content": {"title": "A Study of Dramatic Action and Emotion Using a Systematic Scan of Stick Figure Configurations", "abstract": "Comprehending the meaning of body postures is essential for social organisms such as humans. For example, it is important to understand at a glance whether two people seen at a distance are in a friendly or conflictual interaction. However, it is still unclear what fraction of the possible body configurations carry meaning, and what is the best way to characterize such meaning. Here, we address this by using stick figures as a low-dimensional, yet evocative, representation of body postures. We systematically scanned a set of 1,470 upper-body postures of stick figures in a dyad with a second stick figure with a neutral pose. We asked participants to rate the stick figure in terms of 20 emotion adjectives like sad or triumphant and in terms of eight active verbs that connote intent like to threaten and to comfort. The stick figure configuration space was dense with meaning: people strongly agreed on more than half of the configurations. The meaning was generally smooth in the sense that small changes in posture had a small effect on the meaning, but certain small changes had a large effect. Configurations carried meaning in both emotions and intent, but the intent verbs covered more configurations. The effectiveness of the intent verbs in describing body postures aligns with a theory, originating from the theater, called dramatic action theory. This suggests that, in addition to the well-studied role of emotional states in describing body language, much can be gained by using also dramatic action verbs which signal the effort to change the state of others. We provide a dictionary of stick figure configurations and their perceived meaning. This systematic scan of body configurations might be useful to teaching people and machines to decipher body postures in human interactions.", "year": 2021, "ssId": "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09", "arXivId": null, "link": null, "openAccess": false, "authors": ["Noa Raindel", "Y. Liron", "Uri Alon"]}}
{"id": "48220433a2fb07761b26b2d6aa59b615289a3d4c", "content": {"title": "Single-Node Attack for Fooling Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have shown broad applicability in a variety of domains. Some of these domains, such as social networks and product recommendations, are fertile ground for malicious users and behavior. In this paper, we show that GNNs are vulnerable to the extremely limited scenario of a single-node adversarial example, where the node cannot be picked by the attacker. That is, an attacker can force the GNN to classify any target node to a chosen label by only slightly perturbing another single arbitrary node in the graph, even when not being able to pick that specific attacker node. When the adversary is allowed to pick a specific attacker node, the attack is even more effective. We show that this attack is effective across various GNN types, such as GraphSAGE, GCN, GAT, and GIN, across a variety of real-world datasets, and as a targeted and a non-targeted attack. Our code is available at this https URL .", "year": 2020, "ssId": "48220433a2fb07761b26b2d6aa59b615289a3d4c", "arXivId": "2011.03574", "link": "https://arxiv.org/pdf/2011.03574.pdf", "openAccess": true, "authors": ["Ben Finkelshtein", "Chaim Baskin", "Evgenii Zheltonozhskii", "Uri Alon"]}}
{"id": "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add", "content": {"title": "Neural Edit Completion", "abstract": "We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program's Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task. We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers, and neural CRFs. Our experiments show that our model achieves 28% relative gain over state-of-the-art sequential models and 2$\\times$ higher accuracy than syntactic models that learn to generate the edited code instead of modeling the edits directly. We make our code, dataset, and trained models publicly available.", "year": 2020, "ssId": "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add", "arXivId": "2005.13209", "link": "https://arxiv.org/pdf/2005.13209.pdf", "openAccess": true, "authors": ["Shaked Brody", "Uri Alon", "Eran Yahav"]}}
{"id": "3bfa808ce20b2736708c3fc0b9443635e3f133a7", "content": {"title": "On the Bottleneck of Graph Neural Networks and its Practical Implications", "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.", "year": 2020, "ssId": "3bfa808ce20b2736708c3fc0b9443635e3f133a7", "arXivId": "2006.05205", "link": "https://arxiv.org/pdf/2006.05205.pdf", "openAccess": true, "authors": ["Uri Alon", "Eran Yahav"]}}
{"id": "27724bd19946d6a824d06cdca3cdfe5d40f71003", "content": {"title": "A structural model for contextual code changes", "abstract": "We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program\u2019s Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task. We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers, and neural CRFs. Our experiments show that our model achieves a 28% relative gain over state-of-the-art sequential models and 2\u00d7 higher accuracy than syntactic models that learn to generate the edited code, as opposed to modeling the edits directly. Our code, dataset, and trained models are publicly available at https://github.com/tech-srl/c3po/ .", "year": 2020, "ssId": "27724bd19946d6a824d06cdca3cdfe5d40f71003", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shaked Brody", "Uri Alon", "Eran Yahav"]}}
{"id": "7129b62be18487db5e9602e353bb10a4c79a9b92", "content": {"title": "Neural reverse engineering of stripped binaries using augmented control flow graphs", "abstract": "We address the problem of reverse engineering of stripped executables, which contain no debug information. This is a challenging problem because of the low amount of syntactic information available in stripped executables, and the diverse assembly code patterns arising from compiler optimizations. We present a novel approach for predicting procedure names in stripped executables. Our approach combines static analysis with neural models. The main idea is to use static analysis to obtain augmented representations of call sites; encode the structure of these call sites using the control-flow graph (CFG) and finally, generate a target name while attending to these call sites. We use our representation to drive graph-based, LSTM-based and Transformer-based architectures. Our evaluation shows that our models produce predictions that are difficult and time consuming for humans, while improving on existing methods by 28% and by 100% over state-of-the-art neural textual models that do not use any static analysis. Code and data for this evaluation are available at https://github.com/tech-srl/Nero.", "year": 2020, "ssId": "7129b62be18487db5e9602e353bb10a4c79a9b92", "arXivId": null, "link": null, "openAccess": false, "authors": ["Yaniv David", "Uri Alon", "Eran Yahav"]}}
{"id": "db392858262b17aa9c8ff8659738f68fbf832ebe", "content": {"title": "Structural Language Models of Code", "abstract": "We address the problem of any-code completion - generating a missing piece of source code in a given program without any restriction on the vocabulary or structure. We introduce a new approach to any-code completion that leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling (SLM). SLM estimates the probability of the program's abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary code in any programming language. Our model significantly outperforms both seq2seq and a variety of structured approaches in generating Java and C# code. We make our code, datasets, and models publicly available.", "year": 2019, "ssId": "db392858262b17aa9c8ff8659738f68fbf832ebe", "arXivId": null, "link": null, "openAccess": false, "authors": ["Uri Alon", "Roy Sadaka", "Omer Levy", "Eran Yahav"]}}
{"id": "bbb7eb10c45cabaee6e427242fce7180c0217ef1", "content": {"title": "A general path-based representation for predicting program properties", "abstract": "\n Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is\n how to represent programs in a way that facilitates effective learning\n .\n \n \n We present a\n general path-based representation\n for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens.\n \n We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages.\n We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.", "year": 2018, "ssId": "bbb7eb10c45cabaee6e427242fce7180c0217ef1", "arXivId": null, "link": null, "openAccess": false, "authors": ["Uri Alon", "Meital Zilberstein", "Omer Levy", "Eran Yahav"]}}
{"id": "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a", "content": {"title": "Children\u2019s Object Structure Perspective-Taking: Training and Assessment", "abstract": "Spatial abilities\u2014required in both academic and everyday information processing\u2014are recommended as an important target for explicit instruction in the K-12 curriculum. However, most school curricula do not address this spatial issue, probably because spatial ability is a general rather than domain-specific skill and also due to debate regarding individuals\u2019 ability to transfer general skills to novel tasks. In particular, little is known about elementary school children\u2019s object structure perspective-taking (OSPT) ability. We developed an age-appropriate OSPT assessment tool and examined differences in OSPT ability between 1st and 4th graders who did or did not receive OSPT training; children\u2019s OSPT ability in relation to 3 object types: detailed everyday objects, contour only, and abstract geometrical target objects; the ability to identify a view seen by an observer and the ability to determine the vantage point from which a given view is seen; and problem-solving involving far transfer and difficult items. Problem-solving strategies and relations between these young children\u2019s OSPT performance and their mathematic achievements were examined to further investigate the possible contribution of spatial ability to STEM learning. Our findings are promising in that a relatively short training effected significantly children\u2019s OSPT ability\u2014a general intellectual skill\u2014calling for the development and introduction of OSPT-relevant K-12 curriculum, possibly using our training program as a model. Such curriculum is relevant in particular for girls, of whom OSPT showed significant correlation with mathematic scores for contoured and geometric objects. Fourth graders performed better than 1st graders, but the latter improved more.", "year": 2018, "ssId": "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Billie Eilam", "Uri Alon"]}}
