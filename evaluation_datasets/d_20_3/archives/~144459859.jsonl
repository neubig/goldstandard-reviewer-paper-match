{"id": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30", "content": {"title": "Few-shot Language Coordination by Modeling Theory of Mind", "abstract": "No man is an island. Humans communicate with a large community by coordinating with different interlocutors within short conversations. This ability has been understudied by the research on building neural communicative agents. We study the task of few-shot language coordination: agents quickly adapting to their conversational partners\u2019 language abilities. Different from current communicative agents trained with selfplay, we require the lead agent to coordinate with a population of agents with different linguistic abilities, quickly adapting to communicate with unseen agents in the population. This requires the ability to model the partner\u2019s beliefs, a vital component of human communication. Drawing inspiration from theory-of-mind (ToM; Premack & Woodruff (1978)), we study the effect of the speaker explicitly modeling the listeners\u2019 mental states. The speakers, as shown in our experiments, acquire the ability to predict the reactions of their partner, which helps it generate instructions that concisely express its communicative goal. We examine our hypothesis that the instructions generated with ToM modeling yield better communication performance in both a referential game and a language navigation task. Positive results from our experiments hint at the importance of explicitly modeling communication as a socio-pragmatic progress. Code can be found at https://github.com/CLAW-Lab/ToM.", "year": 2021, "ssId": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30", "arXivId": "2107.05697", "link": "https://arxiv.org/pdf/2107.05697.pdf", "openAccess": true, "authors": ["Hao Zhu", "Graham Neubig", "Yonatan Bisk"]}}
{"id": "d5ec188a5a39e504788c1fe33457eeb816a99f31", "content": {"title": "Dependency Induction Through the Lens of Visual Perception", "abstract": "Most previous work on grammar induction focuses on learning phrasal or dependency structure purely from text. However, because the signal provided by text alone is limited, recently introduced visually grounded syntax models make use of multimodal information leading to improved performance in constituency grammar induction. However, as compared to dependency grammars, constituency grammars do not provide a straightforward way to incorporate visual information without enforcing language-specific heuristics. In this paper, we propose an unsupervised grammar induction model that leverages word concreteness and a structural vision-based heuristic to jointly learn constituency-structure and dependency-structure grammars. Our experiments find that concreteness is a strong indicator for learning dependency grammars, improving the direct attachment score (DAS) by over 50% as compared to state-of-the-art models trained on pure text. Next, we propose an extension of our model that leverages both word concreteness and visual semantic role labels in constituency and dependency parsing. Our experiments show that the proposed extension outperforms the current state-of-the-art visually grounded models in constituency parsing even with a smaller grammar size.", "year": 2021, "ssId": "d5ec188a5a39e504788c1fe33457eeb816a99f31", "arXivId": "2109.09790", "link": "https://arxiv.org/pdf/2109.09790.pdf", "openAccess": true, "authors": ["Ruisi Su", "Shruti Rijhwani", "Hao Zhu", "Junxian He", "Xinyu Wang", "Yonatan Bisk", "Graham Neubig"]}}
{"id": "65c2a39f1579a947926ac5746888445ea4afdf6e", "content": {"title": "The Return of Lexical Dependencies: Neural Lexicalized PCFGs", "abstract": "Abstract In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g., lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs that allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.1", "year": 2020, "ssId": "65c2a39f1579a947926ac5746888445ea4afdf6e", "arXivId": "2007.15135", "link": "https://arxiv.org/pdf/2007.15135.pdf", "openAccess": true, "authors": ["Hao Zhu", "Yonatan Bisk", "Graham Neubig"]}}
{"id": "2873f78efd7adcb118a70f8ea3ca7fa1501e320a", "content": {"title": "FewRel 2.0: Towards More Challenging Few-Shot Relation Classification", "abstract": "We present FewRel 2.0, a more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations? To construct FewRel 2.0, we build upon the FewRel dataset by adding a new test set in a quite different domain, and a NOTA relation choice. With the new dataset and extensive experimental analysis, we found (1) that the state-of-the-art few-shot relation classification models struggle on these two aspects, and (2) that the commonly-used techniques for domain adaptation and NOTA detection still cannot handle the two challenges well. Our research calls for more attention and further efforts to these two real-world issues. All details and resources about the dataset and baselines are released at https://github.com/thunlp/fewrel.", "year": 2019, "ssId": "2873f78efd7adcb118a70f8ea3ca7fa1501e320a", "arXivId": "1910.07124", "link": "https://arxiv.org/pdf/1910.07124.pdf", "openAccess": true, "authors": ["Tianyu Gao", "Xu Han", "Hao Zhu", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou"]}}
{"id": "4cf633d0893a1d3af97723ce1f2fae33c2a30043", "content": {"title": "Quantifying Similarity between Relations with Fact Distribution", "abstract": "We introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases. Specifically, our approach is based on the divergence between the conditional probability distributions over entity pairs. In this paper, these distributions are parameterized by a very simple neural network. Although computing the exact similarity is in-tractable, we provide a sampling-based method to get a good approximation. We empirically show the outputs of our approach significantly correlate with human judgments. By applying our method to various tasks, we also find that (1) our approach could effectively detect redundant relations extracted by open information extraction (Open IE) models, that (2) even the most competitive models for relational classification still make mistakes among very similar relations, and that (3) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes.", "year": 2019, "ssId": "4cf633d0893a1d3af97723ce1f2fae33c2a30043", "arXivId": "1907.08937", "link": "https://arxiv.org/pdf/1907.08937.pdf", "openAccess": true, "authors": ["Weize Chen", "Hao Zhu", "Xu Han", "Zhiyuan Liu", "Maosong Sun"]}}
{"id": "352ac73b7d92afa915c06026a4336927d550cec3", "content": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "In this paper, we propose a novel graph neural network with generated parameters (GP-GNNs). The parameters in the propagation module, i.e. the transition matrices used in message passing procedure, are produced by a generator taking natural language sentences as inputs. We verify GP-GNNs in relation extraction from text, both on bag- and instance-settings. Experimental results on a human-annotated dataset and two distantly supervised datasets show that multi-hop reasoning mechanism yields significant improvements. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "year": 2019, "ssId": "352ac73b7d92afa915c06026a4336927d550cec3", "arXivId": "1902.00756", "link": "https://arxiv.org/pdf/1902.00756.pdf", "openAccess": true, "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-Seng Chua", "Maosong Sun"]}}
{"id": "740182c3aa9a3045fcd9370269d446455ae9f623", "content": {"title": "Neural Finite-State Transducers: Beyond Rational Relations", "abstract": "We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions over pairs of strings. The probability of a string pair is obtained by marginalizing over all its accepting paths in a finite state transducer. In contrast to ordinary weighted FSTs, however, each path is scored using an arbitrary function such as a recurrent neural network, which breaks the usual conditional independence assumption (Markov property). NFSTs are more powerful than previous finite-state models with neural features (Rastogi et al., 2016.) We present training and inference algorithms for locally and globally normalized variants of NFSTs. In experiments on different transduction tasks, they compete favorably against seq2seq models while offering interpretable paths that correspond to hard monotonic alignments.", "year": 2019, "ssId": "740182c3aa9a3045fcd9370269d446455ae9f623", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chu-Cheng Lin", "Hao Zhu", "Matthew R. Gormley", "Jason Eisner"]}}
{"id": "8fc728b71f9e92f91455f957f10c7e496cbe4772", "content": {"title": "Put It Back: Entity Typing with Language Model Enhancement", "abstract": "Entity typing aims to classify semantic types of an entity mention in a specific context. Most existing models obtain training data using distant supervision, and inevitably suffer from the problem of noisy labels. To address this issue, we propose entity typing with language model enhancement. It utilizes a language model to measure the compatibility between context sentences and labels, and thereby automatically focuses more on context-dependent labels. Experiments on benchmark datasets demonstrate that our method is capable of enhancing the entity typing model with information from the language model, and significantly outperforms the state-of-the-art baseline. Code and data for this paper can be found from https://github.com/thunlp/LME.", "year": 2018, "ssId": "8fc728b71f9e92f91455f957f10c7e496cbe4772", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ji Xin", "Hao Zhu", "Xu Han", "Zhiyuan Liu", "Maosong Sun"]}}
{"id": "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95", "content": {"title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation", "abstract": "We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.", "year": 2018, "ssId": "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95", "arXivId": "1810.10147", "link": "https://arxiv.org/pdf/1810.10147.pdf", "openAccess": true, "authors": ["Xu Han", "Hao Zhu", "Pengfei Yu", "Ziyun Wang", "Y. Yao", "Zhiyuan Liu", "Maosong Sun"]}}
{"id": "4b2d583e22f378f9104814d9f63cda411ddd5825", "content": {"title": "Cross-lingual Lexical Sememe Prediction", "abstract": "Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many NLP tasks. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict sememes for words in other languages. We propose a novel framework to model correlations between sememes and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at https://github.com/thunlp/CL-SP.", "year": 2018, "ssId": "4b2d583e22f378f9104814d9f63cda411ddd5825", "arXivId": null, "link": null, "openAccess": false, "authors": ["Fanchao Qi", "Yankai Lin", "Maosong Sun", "Hao Zhu", "Ruobing Xie", "Zhiyuan Liu"]}}
{"id": "f2e7598464a0b9376771ffc4ba243233ee12c677", "content": {"title": "Incorporating Chinese Characters of Words for Lexical Sememe Prediction", "abstract": "Sememes are minimum semantic units of concepts in human languages, such that each word sense is composed of one or multiple sememes. Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks. Recently, the lexical sememe prediction task has been introduced. It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency. However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words. To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words. We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words.", "year": 2018, "ssId": "f2e7598464a0b9376771ffc4ba243233ee12c677", "arXivId": "1806.06349", "link": "https://arxiv.org/pdf/1806.06349.pdf", "openAccess": true, "authors": ["Huiming Jin", "Hao Zhu", "Zhiyuan Liu", "Ruobing Xie", "Maosong Sun", "Fen Lin", "Leyu Lin"]}}
{"id": "89d15c9de3608157ff746af7368556149b50e037", "content": {"title": "Language Modeling with Sparse Product of Sememe Experts", "abstract": "Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution given textual context. Afterwards, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics, and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models. Experiments on language modeling and the downstream application of headline generation demonstrate the significant effectiveness of SDLM.", "year": 2018, "ssId": "89d15c9de3608157ff746af7368556149b50e037", "arXivId": "1810.12387", "link": "https://arxiv.org/pdf/1810.12387.pdf", "openAccess": true, "authors": ["Yihong Gu", "Jun Yan", "Hao Zhu", "Zhiyuan Liu", "Ruobing Xie", "Maosong Sun", "Fen Lin", "Leyu Lin"]}}
{"id": "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc", "content": {"title": "Iterative Entity Alignment via Joint Knowledge Embeddings", "abstract": "Entity alignment aims to link entities and their counterparts among multiple knowledge graphs (KGs). Most existing methods typically rely on external information of entities such as Wikipedia links and require costly manual feature construction to complete alignment. In this paper, we present a novel approach for entity alignment via joint knowledge embeddings. Our method jointly encodes both entities and relations of various KGs into a unified low-dimensional semantic space according to a small seed set of aligned entities. During this process, we can align entities according to their semantic distance in this joint semantic space. More specifically, we present an iterative and parameter sharing method to improve alignment performance. Experiment results on realworld datasets show that, as compared to baselines, our method achieves significant improvements on entity alignment, and can further improve knowledge graph completion performance on various KGs with the favor of joint knowledge embeddings.", "year": 2017, "ssId": "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc", "arXivId": null, "link": null, "openAccess": false, "authors": ["Hao Zhu", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"]}}
{"id": "f75e691daae9133941c9a083e319b39bd837d456", "content": {"title": "Iterative Entity Alignment via Knowledge Embeddings", "abstract": "Entity alignment aims to link entities and their counterparts among multiple knowledge graphs (KGs). Most existing methods typically rely on external information of entities such as Wikipedia links and require costly manual feature construction to complete alignment. In this paper, we present a novel approach for entity alignment via joint knowledge embeddings. Our method jointly encodes both entities and relations of various KGs into a unified low-dimensional semantic space according to a small seed set of aligned entities. During this process, we can align entities according to their semantic distance in this joint semantic space. More specifically, we present an iterative and parameter sharing method to improve alignment performance. Experiment results on realworld datasets show that, as compared to baselines, our method achieves significant improvements on entity alignment, and can further improve knowledge graph completion performance on various KGs with the favor of joint knowledge embeddings.", "year": 2017, "ssId": "f75e691daae9133941c9a083e319b39bd837d456", "arXivId": null, "link": null, "openAccess": false, "authors": ["Hao Zhu", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"]}}
