{"id": "e9d8db4f5b5c106c43a268f635788c0a94b2916a", "content": {"title": "Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods", "abstract": "Stochastic Gradient Descent-Ascent (SGDA) is one of the most prominent algorithms for solving min-max optimization and variational inequalities problems (VIP) appearing in various machine learning tasks. The success of the method led to several advanced extensions of the classical SGDA, including variants with arbitrary sampling, variance reduction, coordinate randomization, and distributed variants with compression, which were extensively studied in the literature, especially during the last few years. In this paper, we propose a unified convergence analysis that covers a large variety of stochastic gradient descent-ascent methods, which so far have required different intuitions, have different applications and have been developed separately in various communities. A key to our unified framework is a parametric assumption on the stochastic estimates. Via our general theoretical framework, we either recover the sharpest known rates for the known special cases or tighten them. Moreover, to illustrate the flexibility of our approach we develop several new variants of SGDA such as a new variance-reduced method (L-SVRGDA), new distributed methods with compression (QSGDA, DIANA-SGDA, VR-DIANA-SGDA), and a new method with coordinate randomization (SEGA-SGDA). Although variants of the new methods are known for solving minimization problems, they were never considered or analyzed for solving min-max problems and VIPs. We also demonstrate the most important properties of the new methods through extensive numerical experiments.", "year": 2022, "ssId": "e9d8db4f5b5c106c43a268f635788c0a94b2916a", "arXivId": "2202.07262", "link": "https://arxiv.org/pdf/2202.07262.pdf", "openAccess": true, "authors": ["Aleksandr Beznosikov", "Eduard A. Gorbunov", "Hugo Berard", "Nicolas Loizou"]}}
{"id": "73e401dead436aabd0cd9c941f7b13bfdeda9861", "content": {"title": "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation", "abstract": "We propose and study a new class of gradient communication mechanisms for communicationefficient training\u2014three point compressors (3PC)\u2014as well as efficient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-K), our class allows the compressors to evolve throughout the training process, with the aim of improving the theoretical communication complexity and practical efficiency of the underlying methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (Richt\u00e1rik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efficient methods. Notably, our approach allows us to improve upon the state of the art in the algorithmic and theoretical foundations of the lazy aggregation literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased.", "year": 2022, "ssId": "73e401dead436aabd0cd9c941f7b13bfdeda9861", "arXivId": "2202.00998", "link": "https://arxiv.org/pdf/2202.00998.pdf", "openAccess": true, "authors": ["Peter Richt'arik", "Igor Sokolov", "I. Fatkhullin", "Elnur Gasanov", "Zhize Li", "Eduard A. Gorbunov"]}}
{"id": "488b1849dd81e63aae2cd327564077ae123c0369", "content": {"title": "Distributed Methods with Absolute Compression and Error Compensation", "abstract": "Distributed optimization methods are often applied to solving huge-scale problems like training neural networks with millions and even billions of parameters. In such applications, commu-nicating full vectors, e.g., (stochastic) gradients, iterates, is prohibitively expensive, especially when the number of workers is large. Communication compression is a powerful approach to al-leviating this issue, and, in particular, methods with biased compression and error compensation are extremely popular due to their practical e\ufb03ciency. Sahu et al. (2021) [29] propose a new analysis of Error Compensated SGD ( EC-SGD ) for the class of absolute compression operators showing that in a certain sense, this class contains optimal compressors for EC-SGD . However, the analysis was conducted only under the so-called ( M, \u03c3 2 )-bounded noise assumption. In this paper, we generalize the analysis of EC-SGD with absolute compression to the arbitrary sampling strategy and propose the \ufb01rst analysis of EC-LSVRG [9] with absolute compression for (strongly) convex problems. Our rates improve upon the previously known ones in this setting. Our theoretical \ufb01ndings are corroborated by several numerical experiments.", "year": 2022, "ssId": "488b1849dd81e63aae2cd327564077ae123c0369", "arXivId": "2203.02383", "link": "https://arxiv.org/pdf/2203.02383.pdf", "openAccess": true, "authors": ["Marina Danilova", "Eduard A. Gorbunov"]}}
{"id": "9ba545841b837fa077579290e252eb00351ebeb0", "content": {"title": "MARINA: Faster Non-Convex Distributed Learning with Compression", "abstract": "We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences that is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al. (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. The communication complexity bounds we prove for MARINA are evidently better than those of all previous first-order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for a partial participation of clients \u2013 a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of oracle/communication complexity. Finally, we provide a convergence analysis of all methods for problems satisfying the Polyak-\u0141ojasiewicz condition.", "year": 2021, "ssId": "9ba545841b837fa077579290e252eb00351ebeb0", "arXivId": "2102.07845", "link": "https://arxiv.org/pdf/2102.07845.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Konstantin Burlachenko", "Zhize Li", "Peter Richt\u00e1rik"]}}
{"id": "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6", "content": {"title": "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices", "abstract": "Training deep neural networks on large datasets can often be accelerated by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-Reduce. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real-world applications, such as federated learning and cloud-based distributed training, operate on unreliable devices with unstable network bandwidth. As a result, these applications are restricted to using parameter servers or gossip-based averaging protocols. In this work, we lift that restriction by proposing Moshpit All-Reduce \u2014 an iterative averaging protocol that exponentially converges to the global average. We demonstrate the efficiency of our protocol for distributed optimization with strong theoretical guarantees. The experiments show 1.3\u00d7 speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5\u00d7 speedup when training ALBERT-large from scratch using preemptible compute nodes.", "year": 2021, "ssId": "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6", "arXivId": "2103.03239", "link": "https://arxiv.org/pdf/2103.03239.pdf", "openAccess": true, "authors": ["Max Ryabinin", "Eduard A. Gorbunov", "Vsevolod Plokhotnyuk", "Gennady Pekhimenko"]}}
{"id": "0e9e334e2647307f8fa7f9937d93f3ca9095e351", "content": {"title": "Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity", "abstract": "Extragradient method (EG) (Korpelevich, 1976) is one of the most popular methods for solving saddle point and variational inequalities problems (VIP). Despite its long history and significant attention in the optimization community, there remain important open questions about convergence of EG. In this paper, we resolve one of such questions and derive the first last-iterate O(1/K) convergence rate for EG for monotone and Lipschitz VIP without any additional assumptions on the operator unlike the only known result of this type (Golowich et al., 2020b) that relies on the Lipschitzness of the Jacobian of the operator. The rate is given in terms of reducing the squared norm of the operator. Moreover, we establish several results on the (non-)cocoercivity of the update operators of EG, Optimistic Gradient Method, and Hamiltonian Gradient Method, when the original operator is monotone and Lipschitz.", "year": 2021, "ssId": "0e9e334e2647307f8fa7f9937d93f3ca9095e351", "arXivId": "2110.04261", "link": "https://arxiv.org/pdf/2110.04261.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Nicolas Loizou", "Gauthier Gidel"]}}
{"id": "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc", "content": {"title": "Near-Optimal High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise", "abstract": "Thanks to their practical efficiency and random nature of the data, stochastic first-order methods are standard for training large-scale machine learning models. Random behavior may cause a particular run of an algorithm to result in a highly suboptimal objective value, whereas theoretical guarantees are usually proved for the expectation of the objective value. Thus, it is essential to theoretically guarantee that algorithms provide small objective residual with high probability. Existing methods for non-smooth stochastic convex optimization have complexity bounds with the dependence on the confidence level that is either negative-power or logarithmic but under an additional assumption of sub-Gaussian (light-tailed) noise distribution that may not hold in practice, e.g., in several NLP tasks. In our paper, we resolve this issue and derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-smooth convex stochastic optimization problems with non-sub-Gaussian (heavy-tailed) noise. To derive our results, we propose novel stepsize rules for two stochastic methods with gradient clipping. Moreover, our analysis works for generalized smooth objectives with H\u00f6lder-continuous gradients, and for both methods, we provide an extension for strongly convex problems. Finally, our results imply that the first (accelerated) method we consider also has optimal iteration and oracle complexity in all the regimes, and the second one is optimal in the non-smooth setting.", "year": 2021, "ssId": "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc", "arXivId": "2106.05958", "link": "https://arxiv.org/pdf/2106.05958.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Marina Danilova", "Innokentiy Shibaev", "P. Dvurechensky", "A. Gasnikov"]}}
{"id": "dda3f2a2803c80e5b3332868bf86901d6239befc", "content": {"title": "Distributed and Stochastic Optimization Methods with Gradient Compression and Local Steps", "abstract": "In this thesis, we propose new theoretical frameworks for the analysis of stochastic and distributed methods with error compensation and local updates. Using these frameworks, we develop more than 20 new optimization methods, including the first linearly converging Error-Compensated SGD and the first linearly converging Local-SGD for arbitrarily heterogeneous local functions. Moreover, the thesis contains several new distributed methods with unbiased compression for distributed non-convex optimization problems. The derived complexity results for these methods outperform the previous best-known results for the considered problems. Finally, we propose a new scalable decentralized fault-tolerant distributed method, and under reasonable assumptions, we derive the iteration complexity bounds for this method that match the ones of centralized Local-SGD.", "year": 2021, "ssId": "dda3f2a2803c80e5b3332868bf86901d6239befc", "arXivId": "2112.10645", "link": "https://arxiv.org/pdf/2112.10645.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov"]}}
{"id": "f784ab218692364b9c8a1f8064809e4524116f3a", "content": {"title": "Secure Distributed Training at Scale", "abstract": "Some of the hardest problems in deep learning can be solved with the combined effort of many independent parties, as is the case for volunteer computing and federated learning. These setups rely on high numbers of peers to provide computational resources or train on decentralized datasets. Unfortunately, participants in such systems are not always reliable. Any single participant can jeopardize the entire training run by sending incorrect updates, whether deliberately or by mistake. Training in presence of such peers requires specialized distributed training algorithms with Byzantine tolerance. These algorithms often sacrifice efficiency by introducing redundant communication or passing all updates through a trusted server. As a result, it can be infeasible to apply such algorithms to large-scale distributed deep learning, where models can have billions of parameters. In this work, we propose a novel protocol for secure (Byzantine-tolerant) decentralized training that emphasizes communication efficiency. We rigorously analyze this protocol: in particular, we provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. To demonstrate its practical effectiveness, we conduct large-scale experiments on image classification and language modeling in presence of Byzantine attackers.", "year": 2021, "ssId": "f784ab218692364b9c8a1f8064809e4524116f3a", "arXivId": "2106.11257", "link": "https://arxiv.org/pdf/2106.11257.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Alexander Borzunov", "Michael Diskin", "Max Ryabinin"]}}
{"id": "87951cea6573eed827986371a35025e478d3c184", "content": {"title": "Stochastic Extragradient: General Analysis and Improved Rates", "abstract": "The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving min-max optimization and variational inequalities problems (VIP) appearing in various machine learning tasks. However, several important questions regarding the convergence properties of SEG are still open, including the sampling of stochastic gradients, mini-batching, convergence guarantees for the monotone finite-sum variational inequalities with possibly non-monotone terms, and others. To address these questions, in this paper, we develop a novel theoretical framework that allows us to analyze several variants of SEG in a unified manner. Besides standard setups, like Same-Sample SEG under Lipschitzness and monotonicity or Independent-Samples SEG under uniformly bounded variance, our approach allows us to analyze variants of SEG that were never explicitly considered in the literature before. Notably, we analyze SEG with arbitrary sampling which includes importance sampling and various mini-batching strategies as special cases. Our rates for the new variants of SEG outperform the current state-of-the-art convergence guarantees and rely on less restrictive assumptions.", "year": 2021, "ssId": "87951cea6573eed827986371a35025e478d3c184", "arXivId": "2111.08611", "link": "https://arxiv.org/pdf/2111.08611.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Hugo Berard", "Gauthier Gidel", "Nicolas Loizou"]}}
{"id": "8306e4a566e2b1279d5d67b40facc8e1e345c4e3", "content": {"title": "EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback", "abstract": "First proposed by Seide et al. (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is O(1/T ), the rate of gradient descent in the same regime is O(1/T )). Recently, Richt\u00e1rik et al. (2021) (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21, all supported by strong convergence theory: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression. Several of these techniques were never analyzed in conjunction with EF before, and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.", "year": 2021, "ssId": "8306e4a566e2b1279d5d67b40facc8e1e345c4e3", "arXivId": "2110.03294", "link": "https://arxiv.org/pdf/2110.03294.pdf", "openAccess": true, "authors": ["I. Fatkhullin", "Igor Sokolov", "Eduard A. Gorbunov", "Zhize Li", "Peter Richt\u00e1rik"]}}
{"id": "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4", "content": {"title": "Recent Theoretical Advances in Non-Convex Optimization", "abstract": "Motivated by recent increased interest in optimization algorithms for non-convex optimization in application to training deep neural networks and other optimization problems in data analysis, we give an overview of recent theoretical results on global performance guarantees of optimization algorithms for non-convex optimization. We start with classical arguments showing that general non-convex problems could not be solved efficiently in a reasonable time. Then we give a list of problems that can be solved efficiently to find the global minimizer by exploiting the structure of the problem as much as it is possible. Another way to deal with non-convexity is to relax the goal from finding the global minimum to finding a stationary point or a local minimum. For this setting, we first present known results for the convergence rates of deterministic first-order methods, which are then followed by a general theoretical analysis of optimal stochastic and randomized gradient schemes, and an overview of the stochastic first-order methods. After that, we discuss quite general classes of non-convex problems, such as minimization of $\\alpha$-weakly-quasi-convex functions and functions that satisfy Polyak--Lojasiewicz condition, which still allow obtaining theoretical convergence guarantees of first-order methods. Then we consider higher-order and zeroth-order/derivative-free methods and their convergence rates for non-convex optimization problems.", "year": 2020, "ssId": "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4", "arXivId": "2012.06188", "link": "https://arxiv.org/pdf/2012.06188.pdf", "openAccess": true, "authors": ["Marina Danilova", "P. Dvurechensky", "A. Gasnikov", "Eduard A. Gorbunov", "S. Guminov", "D. Kamzolov", "Innokentiy Shibaev"]}}
{"id": "dcac1abd2ae5af180e51994a9c8334a6de915765", "content": {"title": "Linearly Converging Error Compensated SGD", "abstract": "In this paper, we propose a unified analysis of variants of distributed SGD with arbitrary compressions and delayed updates. Our framework is general enough to cover different variants of quantized SGD, Error-Compensated SGD (EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive the complexity results for all the methods that fit our framework. For the existing methods, this theorem gives the best-known complexity results. Moreover, using our general scheme, we develop new variants of SGD that combine variance reduction or arbitrary sampling with error feedback and quantization and derive the convergence rates for these methods beating the state-of-the-art results. In order to illustrate the strength of our framework, we develop 16 new methods that fit this. In particular, we propose the first method called EC-SGD-DIANA that is based on error-feedback for biased compression operator and quantization of gradient differences and prove the convergence guarantees showing that EC-SGD-DIANA converges to the exact optimum asymptotically in expectation with constant learning rate for both convex and strongly convex objectives when workers compute full gradients of their loss functions. Moreover, for the case when the loss function of the worker has the form of finite sum, we modified the method and got a new one called EC-LSVRG-DIANA which is the first distributed stochastic method with error feedback and variance reduction that converges to the exact optimum asymptotically in expectation with a constant learning rate.", "year": 2020, "ssId": "dcac1abd2ae5af180e51994a9c8334a6de915765", "arXivId": "2010.12292", "link": "https://arxiv.org/pdf/2010.12292.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "D. Kovalev", "Dmitry Makarenko", "Peter Richt\u00e1rik"]}}
{"id": "4cd92a56dca741190e453b4229eb9851abf6944c", "content": {"title": "Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping", "abstract": "In this paper, we propose a new accelerated stochastic first-order method called clipped-SSTM for smooth convex stochastic optimization with heavy-tailed distributed noise in stochastic gradients and derive the first high-probability complexity bounds for this method closing the gap in the theory of stochastic optimization with heavy-tailed noise. Our method is based on a special variant of accelerated Stochastic Gradient Descent (SGD) and clipping of stochastic gradients. We extend our method to the strongly convex case and prove new complexity bounds that outperform state-of-the-art results in this case. Finally, we extend our proof technique and derive the first non-trivial high-probability complexity bounds for SGD with clipping without light-tails assumption on the noise.", "year": 2020, "ssId": "4cd92a56dca741190e453b4229eb9851abf6944c", "arXivId": "2005.10785", "link": "https://arxiv.org/pdf/2005.10785.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Marina Danilova", "A. Gasnikov"]}}
{"id": "e6924d247b56980260e4c68dbc51b947409e4764", "content": {"title": "Local SGD: Unified Theory and New Efficient Methods", "abstract": "This work was supported by the KAUST baseline research grant of P. Richt\u00b4arik. Part of this work was done while E. Gorbunov was a research intern at KAUST. The research of E. Gorbunov was also partially supported by the Ministry of Science and Higher Education \nof the Russian Federation (Goszadaniye) 075-00337-20-03 and RFBR, project number 19-31-51001.", "year": 2020, "ssId": "e6924d247b56980260e4c68dbc51b947409e4764", "arXivId": "2011.02828", "link": "https://arxiv.org/pdf/2011.02828.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Filip Hanzely", "Peter Richt\u00e1rik"]}}
{"id": "c39ac49e2d3feec992e84868256cb0a0ff028346", "content": {"title": "Recent theoretical advances in decentralized distributed convex optimization.", "abstract": "In the last few years, the theory of decentralized distributed convex optimization has made significant progress. The lower bounds on communications rounds and oracle calls have appeared, as well as methods that reach both of these bounds. In this paper, we focus on how these results can be explained based on optimal algorithms for the non-distributed setup. In particular, we provide our recent results that have not been published yet, and that could be found in details only in arXiv preprints.", "year": 2020, "ssId": "c39ac49e2d3feec992e84868256cb0a0ff028346", "arXivId": "2011.13259", "link": "https://arxiv.org/pdf/2011.13259.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "A. Rogozin", "Aleksandr Beznosikov", "D. Dvinskikh", "A. Gasnikov"]}}
{"id": "1bd7d16340642948142d7608ef8f085d934d94a3", "content": {"title": "A Stochastic Derivative Free Optimization Method with Momentum", "abstract": "We consider the problem of unconstrained minimization of a smooth objective function in $\\mathbb{R}^d$ in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP) \\cite{Bergou_2018}. We show new complexity results for non-convex, convex and strongly convex functions. We test our method on a collection of learning to continuous control tasks on several MuJoCo \\cite{Todorov_2012} environments with varying difficulty and compare against STP, other state-of-the-art derivative-free optimization algorithms and against policy gradient methods. SMTP significantly outperforms STP and all other methods that we considered in our numerical experiments. Our second contribution is SMTP with importance sampling which we call SMTP_IS. We provide convergence analysis of this method for non-convex, convex and strongly convex objectives.", "year": 2019, "ssId": "1bd7d16340642948142d7608ef8f085d934d94a3", "arXivId": "1905.13278", "link": "https://arxiv.org/pdf/1905.13278.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Adel Bibi", "Ozan Sener", "E. Bergou", "Peter Richt\u00e1rik"]}}
{"id": "f762ce106b37728df1126375981a02a589e0497c", "content": {"title": "A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent", "abstract": "In this paper we introduce a unified analysis of a large family of variants of proximal stochastic gradient descent ({\\tt SGD}) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling. As a by-product, we obtain the first unified theory of {\\tt SGD} and randomized coordinate descent ({\\tt RCD}) methods, the first unified theory of variance reduced and non-variance-reduced {\\tt SGD} methods, and the first unified theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop five new variants of {\\tt SGD}, and through numerical experiments demonstrate some of their properties.", "year": 2019, "ssId": "f762ce106b37728df1126375981a02a589e0497c", "arXivId": "1905.11261", "link": "https://arxiv.org/pdf/1905.11261.pdf", "openAccess": true, "authors": ["Eduard A. Gorbunov", "Filip Hanzely", "Peter Richt\u00e1rik"]}}
{"id": "09ec8d8e2251e079abb0e109979f33ee120211fa", "content": {"title": "Reachability of Optimal Convergence Rate Estimates for High-Order Numerical Convex Optimization Methods", "abstract": "The Monteiro\u2013Svaiter accelerated hybrid proximal extragradient method (2013) with one step of Newton\u2019s method used at every iteration for the approximate solution of an auxiliary problem is considered. The Monteiro\u2013Svaiter method is optimal (with respect to the number of gradient and Hessian evaluations for the optimized function) for sufficiently smooth convex optimization problems in the class of methods using only the gradient and Hessian of the optimized function. An optimal tensor method involving higher derivatives is proposed by replacing Newton\u2019s step with a step of Yu.E. Nesterov\u2019s recently proposed tensor method (2018) and by using a special generalization of the step size selection condition in the outer accelerated proximal extragradient method. This tensor method with derivatives up to the third order inclusive is found fairly practical, since the complexity of its iteration is comparable with that of Newton\u2019s one. Thus, a constructive solution is obtained for Nesterov\u2019s problem (2018) of closing the gap between tight lower and overstated upper bounds for the convergence rate of existing tensor methods of order $$p\\; \\geqslant \\;3$$.", "year": 2019, "ssId": "09ec8d8e2251e079abb0e109979f33ee120211fa", "arXivId": null, "link": null, "openAccess": false, "authors": ["A. Gasnikov", "Eduard A. Gorbunov", "D. Kovalev", "A. A. M. Mokhammed", "E. Chernousova"]}}
{"id": "6b387d18bae978202af501c4795f37a0c73781a6", "content": {"title": "Reachability of optimal convergence rate estimates for high-order numerical convex optimization methods", "abstract": "The Monteiro-Svaiter accelerated hybrid proximal extragradient method (2013) with one step of Newton\u2019s method used at every iteration for the approximate solution of an auxiliary problem is considered. The Monteiro-Svaiter method is optimal (with respect to the number of gradient and Hessian evaluations for the optimized function) for sufficiently smooth convex optimization problems in the class of methods using only the gradient and Hessian of the optimized function. An optimal tensor method involving higher derivatives is proposed by replacing Newton\u2019s step with a step of Yu.E. Nesterov\u2019s recently proposed tensor method (2018) and by using a special generalization of the step size selection condition in the outer accelerated proximal extragradient method. This tensor method with derivatives up to the third order inclusive is found fairly practical, since the complexity of its iteration is comparable with that of Newton\u2019s one. Thus, a constructive solution is obtained for Nesterov\u2019s problem (2018) of closing the gap between tight lower and overstated upper bounds for the convergence rate of existing tensor methods of order p \u2265 3.", "year": 2019, "ssId": "6b387d18bae978202af501c4795f37a0c73781a6", "arXivId": null, "link": null, "openAccess": false, "authors": ["A. Gasnikov", "Eduard A. Gorbunov", "D. Kovalev", "A. A. M. Mokhammed", "E. Chernousova"]}}
