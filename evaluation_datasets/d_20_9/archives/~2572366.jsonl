{"id": "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d", "content": {"title": "How reparametrization trick broke differentially-private text representation leaning", "abstract": "As privacy gains traction in the NLP community, researchers have started adopting various approaches to privacy-preserving methods. One of the favorite privacy frameworks, differential privacy (DP), is perhaps the most compelling thanks to its fundamental theoretical guarantees. Despite the apparent simplicity of the general concept of differential privacy, it seems non-trivial to get it right when applying it to NLP. In this short paper, we formally analyze several recent NLP papers proposing text representation learning using DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and reveal their false claims of being differentially private. Furthermore, we also show a simple yet general empirical sanity check to determine whether a given implementation of a DP mechanism almost certainly violates the privacy loss guarantees. Our main goal is to raise awareness and help the community understand potential pitfalls of applying differential privacy to text representation learning.", "year": 2022, "ssId": "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d", "arXivId": "2202.12138", "link": "https://arxiv.org/pdf/2202.12138.pdf", "openAccess": true, "authors": ["Ivan Habernal"]}}
{"id": "2fb44f1317bc51a1e011a5a44d817ad9104e29e8", "content": {"title": "When differential privacy meets NLP: The devil is in the detail", "abstract": "Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users\u2019 original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder\u2019s dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.", "year": 2021, "ssId": "2fb44f1317bc51a1e011a5a44d817ad9104e29e8", "arXivId": "2109.03175", "link": "https://arxiv.org/pdf/2109.03175.pdf", "openAccess": true, "authors": ["Ivan Habernal"]}}
{"id": "75ba422d90c488b1388345865e0525208331bb3d", "content": {"title": "One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks", "abstract": "Preserving privacy in training modern NLP models comes at a cost. We know that stricter privacy guarantees in differentiallyprivate stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the efficiency of DPSGD in NLP is inconclusive or even counterintuitive. In this short paper, we provide a thorough analysis of different privacy preserving strategies on seven downstream datasets in five different \u2018typical\u2019 NLP tasks with varying complexity using modern neural models. We show that unlike standard non-private approaches to solving NLP tasks, where bigger is usually better, privacy-preserving strategies do not exhibit a winning pattern, and each task and privacy regime requires a special treatment to achieve adequate performance.", "year": 2021, "ssId": "75ba422d90c488b1388345865e0525208331bb3d", "arXivId": "2112.08159", "link": "https://arxiv.org/pdf/2112.08159.pdf", "openAccess": true, "authors": ["Manuel Senge", "Timour Igamberdiev", "Ivan Habernal"]}}
{"id": "80b92f762e116d4513da27792822897ca3915247", "content": {"title": "Privacy-Preserving Graph Convolutional Networks for Text Classification", "abstract": "Graph convolutional networks (GCNs) are a 001 powerful architecture for representation learn002 ing on documents that naturally occur as 003 graphs, e.g., citation or social networks. How004 ever, sensitive personal information, such as 005 documents with people\u2019s profiles or relation006 ships as edges, are prone to privacy leaks, 007 as the trained model might reveal the orig008 inal input. Although differential privacy 009 (DP) offers a well-founded privacy-preserving 010 framework, GCNs pose theoretical and prac011 tical challenges due to their training specifics. 012 We address these challenges by adapting 013 differentially-private gradient-based training 014 to GCNs and conduct experiments using two 015 optimizers on five NLP datasets in two lan016 guages. We propose a simple yet efficient 017 method based on random graph splits that not 018 only improves the baseline privacy bounds by 019 a factor of 2.7 while retaining competitive F1 020 scores, but also provides strong privacy guar021 antees of \u03b5 = 1.0. We show that, under certain 022 modeling choices, privacy-preserving GCNs 023 perform up to 90% of their non-private vari024 ants, while formally guaranteeing strong pri025 vacy measures. 026", "year": 2021, "ssId": "80b92f762e116d4513da27792822897ca3915247", "arXivId": "2102.09604", "link": "https://arxiv.org/pdf/2102.09604.pdf", "openAccess": true, "authors": ["Timour Igamberdiev", "Ivan Habernal"]}}
{"id": "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793", "content": {"title": "Why do you think that? Exploring faithful sentence\u2013level rationales without supervision", "abstract": "Evaluating the trustworthiness of a model\u2019s prediction is essential for differentiating between \u2018right for the right reasons\u2019 and \u2018right for the wrong reasons\u2019. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training\u2013framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart\u2019s performance in two cases. We further exploit the transparent decision\u2013making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale\u2013level.", "year": 2020, "ssId": "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793", "arXivId": "2010.03384", "link": "https://arxiv.org/pdf/2010.03384.pdf", "openAccess": true, "authors": ["Max Glockner", "Ivan Habernal", "Iryna Gurevych"]}}
{"id": "7cbb56da008163df09d254f85b7165f11389f298", "content": {"title": "SemEval-2018 Task 12: The Argument Reasoning Comprehension Task", "abstract": "A natural language argument is composed of a claim as well as reasons given as premises for the claim. The warrant explaining the reasoning is usually left implicit, as it is clear from the context and common sense. This makes a comprehension of arguments easy for humans but hard for machines. This paper summarizes the first shared task on argument reasoning comprehension. Given a premise and a claim along with some topic information, the goal was to automatically identify the correct warrant among two candidates that are plausible and lexically close, but in fact imply opposite claims. We describe the dataset with 1970 instances that we built for the task, and we outline the 21 computational approaches that participated, most of which used neural networks. The results reveal the complexity of the task, with many approaches hardly improving over the random accuracy of about 0.5. Still, the best observed accuracy (0.712) underlines the principle feasibility of identifying warrants. Our analysis indicates that an inclusion of external knowledge is key to reasoning comprehension.", "year": 2018, "ssId": "7cbb56da008163df09d254f85b7165f11389f298", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal", "Henning Wachsmuth", "Iryna Gurevych", "Benno Stein"]}}
{"id": "cb53f9558bd13c853026f97dce3bbe3d989ca97d", "content": {"title": "Adapting Serious Game for Fallacious Argumentation to German: Pitfalls, Insights, and Best Practices", "abstract": "As argumentation about controversies is culture- and language-dependent, porting a serious game that deals with daily argumentation to another language requires substantial adaptation. This article presents a study of deploying Argotario (serious game for learning argumentation fallacies) into the German context. We examine all steps that are necessary to end up with a successful serious game platform, such as topic selection, initial data creation, or effective campaigns. Moreover, we analyze users\u2019 behavior and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models.", "year": 2018, "ssId": "cb53f9558bd13c853026f97dce3bbe3d989ca97d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal", "P. Pauli", "Iryna Gurevych"]}}
{"id": "19b6537012412bee0a36e3e271f84b95868fe859", "content": {"title": "Before Name-Calling: Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation", "abstract": "Arguing without committing a fallacy is one of the main requirements of an ideal debate. But even when debating rules are strictly enforced and fallacious arguments punished, arguers often lapse into attacking the opponent by an ad hominem argument. As existing research lacks solid empirical investigation of the typology of ad hominem arguments as well as their potential causes, this paper fills this gap by (1) performing several large-scale annotation studies, (2) experimenting with various neural architectures and validating our working hypotheses, such as controversy or reasonableness, and (3) providing linguistic insights into triggers of ad hominem using explainable neural network architectures.", "year": 2018, "ssId": "19b6537012412bee0a36e3e271f84b95868fe859", "arXivId": "1802.06613", "link": "https://arxiv.org/pdf/1802.06613.pdf", "openAccess": true, "authors": ["Ivan Habernal", "Henning Wachsmuth", "Iryna Gurevych", "Benno Stein"]}}
{"id": "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21", "content": {"title": "Computational Argumentation: A Journey Beyond Semantics, Logic, Opinions, and Easy Tasks", "abstract": "The classical view on argumentation, such that arguments are logical structures consisting of different distinguishable parts and that parties exchange arguments in a rational way, is prevalent in textbooks but nonexistent in the real world. Instead, argumentation is a multifaceted communication tool built upon humans\u2019 capabilities to easily use common sense, emotions, and social context. As humans, we are pretty good at it. Computational Argumentation tries to tackle these phenomena but has a long and not so easy way to go. In this talk, I would like to shed a light on several recent attempts to deal with argumentation computationally, such as addressing argument quality, understanding argument reasoning, dealing with fallacies, and how should we never ever argue online.", "year": 2018, "ssId": "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal"]}}
{"id": "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988", "content": {"title": "Argumentation Quality Assessment: Theory vs. Practice", "abstract": "Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.", "year": 2017, "ssId": "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988", "arXivId": null, "link": null, "openAccess": false, "authors": ["Henning Wachsmuth", "Nona Naderi", "Ivan Habernal", "Yufang Hou", "Graeme Hirst", "Iryna Gurevych", "Benno Stein"]}}
{"id": "ece56ab633f11d1592a3d4f9386412d3f48fcf95", "content": {"title": "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants", "abstract": "Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.", "year": 2017, "ssId": "ece56ab633f11d1592a3d4f9386412d3f48fcf95", "arXivId": "1708.01425", "link": "https://arxiv.org/pdf/1708.01425.pdf", "openAccess": true, "authors": ["Ivan Habernal", "Henning Wachsmuth", "Iryna Gurevych", "Benno Stein"]}}
{"id": "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647", "content": {"title": "The Argument Reasoning Comprehension Task", "abstract": "Reasoning is a crucial part of natural language argumentation. In order to comprehend an argument, one has to reconstruct and analyze its reasoning. As arguments are highly contextualized, most reasoning-related content is left implicit and usually presupposed. Thus, argument comprehension requires not only language understanding and logic skills, but it also heavily depends on common sense. In this article we define a new task, argument reasoning comprehension. Given a natural language argument with a reason and a claim, the goal is to choose the correct implicit reasoning from two options. The challenging factor is that both options are plausible and lexically very close while leading to contradicting claims. To provide an empirical common ground for the task, we propose a complex, yet scalable crowdsourcing process, and we create a new freely licensed dataset based on authentic arguments from news comments. While the resulting 2k high-quality instances are also suitable for other argumentation-related tasks, such as stance detection, argument component identification, and abstractive argument summarization, we focus on the argument reasoning comprehension task and experiment with several systems based on neural attention and language models. Our results clearly reveal that current methods lack the capability to solve the task.", "year": 2017, "ssId": "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal", "Henning Wachsmuth", "Iryna Gurevych", "Benno Stein"]}}
{"id": "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5", "content": {"title": "Argotario: Computational Argumentation Meets Serious Games", "abstract": "An important skill in critical thinking and argumentation is the ability to spot and recognize fallacies. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to \u2018wrong moves\u2019 in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated fallacies empirically. The nonexistence of resources dealing with fallacious argumentation calls for scalable approaches to data acquisition and annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a serious game that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at www.argotario.net.", "year": 2017, "ssId": "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5", "arXivId": "1707.06002", "link": "https://arxiv.org/pdf/1707.06002.pdf", "openAccess": true, "authors": ["Ivan Habernal", "Raffael Hannemann", "Christiana Pollak", "Christopher Klamm", "P. Pauli", "Iryna Gurevych"]}}
{"id": "08f6819e66318cd49cddefd5d690a752d1098da7", "content": {"title": "What is the Essence of a Claim? Cross-Domain Claim Identification", "abstract": "Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.", "year": 2017, "ssId": "08f6819e66318cd49cddefd5d690a752d1098da7", "arXivId": "1704.07203", "link": "https://arxiv.org/pdf/1704.07203.pdf", "openAccess": true, "authors": ["Johannes Daxenberger", "Steffen Eger", "Ivan Habernal", "Christian Stab", "Iryna Gurevych"]}}
{"id": "25ae911c13da7ef9def56ee30170920ebd48a668", "content": {"title": "Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM", "abstract": "We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classification, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation \u201cA is more convincing than B\u201d exhibits properties of total ordering; these findings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all arguments to the topic based on their convincingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman\u2019s correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses.", "year": 2016, "ssId": "25ae911c13da7ef9def56ee30170920ebd48a668", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal", "Iryna Gurevych"]}}
{"id": "9bd170248355047067f05349d57110cc8e4de5cf", "content": {"title": "C4Corpus (publicdomain part)", "abstract": "A large web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs.", "year": 2016, "ssId": "9bd170248355047067f05349d57110cc8e4de5cf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Iryna Gurevych", "Ivan Habernal", "Omnia Zayed"]}}
{"id": "598321d9c3eb5c035b449e19e539b6fa04b3802a", "content": {"title": "C4Corpus (CC BY-NC part)", "abstract": "A large web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs.", "year": 2016, "ssId": "598321d9c3eb5c035b449e19e539b6fa04b3802a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Iryna Gurevych", "Ivan Habernal", "Omnia Zayed"]}}
{"id": "834d68b9befcc6c68415b460b33435a1822799fb", "content": {"title": "Argumentation Mining in User-Generated Web Discourse", "abstract": "The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people\u2019s argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the data, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.", "year": 2016, "ssId": "834d68b9befcc6c68415b460b33435a1822799fb", "arXivId": "1601.02403", "link": "https://arxiv.org/pdf/1601.02403.pdf", "openAccess": true, "authors": ["Ivan Habernal", "Iryna Gurevych"]}}
{"id": "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a", "content": {"title": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation", "abstract": "This article tackles a new challenging task in computational argumentation. Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one. We approach this task in a fully empirical manner by annotating 26k explanations written in natural language. These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws. We create a new crowd-sourced corpus containing 9,111 argument pairs, multi-labeled with 17 classes, which was cleaned and curated by employing several strict quality measures. We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new UKPConvArg2 corpus and software under permissive licenses to the research community.", "year": 2016, "ssId": "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal", "Iryna Gurevych"]}}
{"id": "cdf5eb63e9c2434073e811aba50ae80ede9d15f6", "content": {"title": "New Collection Announcement: Focused Retrieval Over the Web", "abstract": "Focused retrieval (a.k.a., passage retrieval) is important at its own right and as an intermediate step in question answering systems. We present a new Web-based collection for focused retrieval. The document corpus is the Category A of the ClueWeb12 collection. Forty-nine queries from the educational domain were created. The $100$ documents most highly ranked for each query by a highly effective learning-to-rank method were judged for relevance using crowdsourcing. All sentences in the relevant documents were judged for relevance.", "year": 2016, "ssId": "cdf5eb63e9c2434073e811aba50ae80ede9d15f6", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Habernal", "Maria Sukhareva", "Fiana Raiber", "A. Shtok", "Oren Kurland", "H. Ronen", "J. Bar-Ilan", "Iryna Gurevych"]}}
