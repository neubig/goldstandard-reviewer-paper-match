{"id": "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed", "content": {"title": "Confluent Vessel Trees with Accurate Bifurcations", "abstract": "We are interested in unsupervised reconstruction of complex near-capillary vasculature with thousands of bifurcations where supervision and learning are infeasible. Unsupervised methods can use many structural constraints, e.g. topology, geometry, physics. Common techniques use variants of MST on geodesic tubular graphs minimizing symmetric pairwise costs, i.e. distances. We show limitations of such standard undirected tubular graphs producing typical errors at bifurcations where flow \"directedness\" is critical. We introduce a new general concept of confluence for continuous oriented curves forming vessel trees and show how to enforce it on discrete tubular graphs. While confluence is a high-order property, we present an efficient practical algorithm for reconstructing confluent vessel trees using minimum arborescence on a directed graph enforcing confluence via simple flow-extrapolating arc construction. Empirical tests on large near-capillary sub-voxel vasculature volumes demonstrate significantly improved reconstruction accuracy at bifurcations. Our code has also been made publicly available 1.", "year": 2021, "ssId": "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed", "arXivId": "2103.14268", "link": "https://arxiv.org/pdf/2103.14268.pdf", "openAccess": true, "authors": ["Zhongwen Zhang", "D. Marin", "M. Drangova", "Yuri Boykov"]}}
{"id": "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0", "content": {"title": "Token Pooling in Vision Transformers", "abstract": "Despite the recent success in many applications, the high computational requirements of vision transformers limit their use in resource-constrained settings. While many existing methods improve the quadratic complexity of attention, in most vision transformers, self-attention is not the major computation bottleneck, e.g., more than 80% of the computation is spent on fully-connected layers. To improve the computational complexity of all layers, we propose a novel token downsampling method, called Token Pooling, efficiently exploiting redundancies in the images and intermediate token representations. We show that, under mild assumptions, softmax-attention acts as a high-dimensional low-pass (smoothing) filter. Thus, its output contains redundancy that can be pruned to achieve a better trade-off between the computational cost and accuracy. Our new technique accurately approximates a set of tokens by minimizing the reconstruction error caused by downsampling. We solve this optimization problem via cost-efficient clustering. We rigorously analyze and compare to prior downsampling methods. Our experiments show that Token Pooling significantly improves the cost-accuracy trade-off over the state-of-the-art downsampling. Token Pooling is a simple and effective operator that can benefit many architectures. Applied to DeiT, it achieves the same ImageNet top-1 accuracy using 42% fewer computations.", "year": 2021, "ssId": "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0", "arXivId": "2110.03860", "link": "https://arxiv.org/pdf/2110.03860.pdf", "openAccess": true, "authors": ["D. Marin", "J. Chang", "Anurag Ranjan", "Anish K. Prabhu", "Mohammad Rastegari", "Oncel Tuzel"]}}
{"id": "04b364d56995de2228cb1acfb320a935cbcf4440", "content": {"title": "Robust Trust Region for Weakly Supervised Segmentation", "abstract": "Acquisition of training data for the standard semantic segmentation is expensive if requiring that each pixel is labeled. Yet, current methods significantly deteriorate in weakly supervised settings, e.g. where a fraction of pixels is labeled or when only image-level tags are available. It has been shown that regularized losses\u2014originally developed for unsupervised low-level segmentation and representing geometric priors on pixel labels\u2014can considerably improve the quality of weakly supervised training. However, many common priors require optimization stronger than gradient descent. Thus, such regularizers have limited applicability in deep learning. We propose a new robust trust region approach1 for regularized losses improving the state-of-the-art results. Our approach can be seen as a higher-order generalization of the classic chain rule. It allows neural network optimization to use strong low-level solvers for the corresponding regularizers, including discrete ones.", "year": 2021, "ssId": "04b364d56995de2228cb1acfb320a935cbcf4440", "arXivId": "2104.01948", "link": "https://arxiv.org/pdf/2104.01948.pdf", "openAccess": true, "authors": ["D. Marin", "Yuri Boykov"]}}
{"id": "ec99cf93ef22a0c0d669abe90c9509f642b2cf69", "content": {"title": "Efficient Segmentation: Learning Downsampling Near Semantic Boundaries", "abstract": "Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.", "year": 2019, "ssId": "ec99cf93ef22a0c0d669abe90c9509f642b2cf69", "arXivId": "1907.07156", "link": "https://arxiv.org/pdf/1907.07156.pdf", "openAccess": true, "authors": ["D. Marin", "Zijian He", "P\u00e9ter Vajda", "P. Chatterjee", "Sam Tsai", "Fei Yang", "Yuri Boykov"]}}
{"id": "d8704a63517868475b3af7ec25eaa2fb2a44362b", "content": {"title": "ADM for grid CRF loss in CNN segmentation", "abstract": "Variants of gradient descent (GD) dominate CNN loss minimization in computer vision. But, as we show, some powerful loss functions are practically useless only due to their poor optimization by GD. In the context of weakly-supervised CNN segmentation, we present a general ADM approach to regularized losses, which are inspired by well-known MRF/CRF models in \"shallow\" segmentation. While GD fails on the popular nearest-neighbor Potts loss, ADM splitting with $\\alpha$-expansion solver significantly improves optimization of such grid CRF losses yielding state-of-the-art training quality. Denser CRF losses become amenable to basic GD, but they produce lower quality object boundaries in agreement with known noisy performance of dense CRF inference in shallow segmentation.", "year": 2018, "ssId": "d8704a63517868475b3af7ec25eaa2fb2a44362b", "arXivId": "1809.02322", "link": "https://arxiv.org/pdf/1809.02322.pdf", "openAccess": true, "authors": ["D. Marin", "Meng Tang", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "73bbd0b53044e9f518a3596a3607521bbce12fc2", "content": {"title": "Beyond Gradient Descent for Regularized Segmentation Losses", "abstract": "The simplicity of gradient descent (GD) made it the default method for training ever-deeper and complex neural networks. Both loss functions and architectures are often explicitly tuned to be amenable to this basic local optimization. In the context of weakly-supervised CNN segmentation, we demonstrate a well-motivated loss function where an alternative optimizer (ADM) achieves the state-of-the-art while GD performs poorly. Interestingly, GD obtains its best result for a \"smoother\" tuning of the loss function. The results are consistent across different network architectures. Our loss is motivated by well-understood MRF/CRF regularization models in \"shallow\" segmentation and their known global solvers. Our work suggests that network design/training should pay more attention to optimization methods.", "year": 2018, "ssId": "73bbd0b53044e9f518a3596a3607521bbce12fc2", "arXivId": null, "link": null, "openAccess": false, "authors": ["D. Marin", "Meng Tang", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "6b7004138ee2de5ec52e500cae4e65390e961e16", "content": {"title": "Kernel Cuts: Kernel and Spectral Clustering Meet Regularization", "abstract": "This work bridges the gap between two popular methodologies for data partitioning: kernel clustering and regularization-based segmentation. While addressing closely related practical problems, these general methodologies may seem very different based on how they are covered in the literature. The differences may show up in motivation, formulation, and optimization, e.g.\u00a0spectral relaxation versus max-flow. We explain how regularization and kernel clustering can work together and why this is useful. Our joint energy combines standard regularization, e.g.\u00a0MRF potentials, and kernel clustering criteria like normalized cut. Complementarity of such terms is demonstrated in many applications using our bound optimization Kernel Cut algorithm for the joint energy (code is publicly available). While detailing combinatorial move-making, our main focus are new linear kernel and spectral bounds for kernel clustering criteria allowing their integration with any regularization objectives with existing discrete or continuous solvers.", "year": 2018, "ssId": "6b7004138ee2de5ec52e500cae4e65390e961e16", "arXivId": null, "link": null, "openAccess": false, "authors": ["Meng Tang", "D. Marin", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "066f2023b2b5ba5df61dc193c205785fa5e73fed", "content": {"title": "Kernel Cuts: Kernel & Spectral Clustering meet Regularization", "abstract": "This work bridges the gap between two popular methodologies for data partitioning: kernel clustering and regularization-based segmentation. While addressing closely related practical problems, these general methodologies may seem very different based on how they are covered in the literature. The differences may show up in motivation, formulation, and optimization, e.g. spectral relaxation vs maxflow. We explain how regularization and kernel clustering can work together and why this is useful. Our joint energy combines standard regularization, e.g. MRF potentials, and kernel clustering criteria like normalized cut. Complementarity of such terms is demonstrated in many applications using our bound optimization Kernel Cut algorithm for the joint energy (code is publicly available). While detailing combinatorial move-making, our main focus are new linear kernel and spectral bounds for kernel clustering criteria allowing their integration with any regularization objectives with existing discrete or continuous solvers.", "year": 2018, "ssId": "066f2023b2b5ba5df61dc193c205785fa5e73fed", "arXivId": null, "link": null, "openAccess": false, "authors": ["Meng Tang", "D. Marin", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "4f7bbcef3d40cafad17936fdf562a121667af1e8", "content": {"title": "Divergence Prior and Vessel-Tree Reconstruction", "abstract": "We propose a new geometric regularization principle for reconstructing vector fields based on prior knowledge about their divergence. As one important example of this general idea, we focus on vector fields modelling blood flow pattern that should be divergent in arteries and convergent in veins. We show that this previously ignored regularization constraint can significantly improve the quality of vessel tree reconstruction particularly around bifurcations where non-zero divergence is concentrated. Our divergence prior is critical for resolving (binary) sign ambiguity in flow orientations produced by standard vessel filters, \\eg Frangi. Our vessel tree centerline reconstruction combines divergence constraints with robust curvature regularization. Our unsupervised method can reconstruct complete vessel trees with near-capillary details on synthetic and real 3D volumes.", "year": 2018, "ssId": "4f7bbcef3d40cafad17936fdf562a121667af1e8", "arXivId": "1811.09745", "link": "https://arxiv.org/pdf/1811.09745.pdf", "openAccess": true, "authors": ["Zhongwen Zhang", "Egor Chesakov", "D. Marin", "Yuri Boykov"]}}
{"id": "60a9438c24847a949419e0350a61fc2a330e4a09", "content": {"title": "Kernel Clustering: Density Biases and Solutions", "abstract": "Kernel methods are popular in clustering due to their generality and discriminating power. However, we show that many kernel clustering criteria have density biases theoretically explaining some practically significant artifacts empirically observed in the past. For example, we provide conditions and formally prove the density mode isolation bias in kernel K-means for a common class of kernels. We call it Breiman\u2019s bias due to its similarity to the histogram mode isolation previously discovered by Breiman in decision tree learning with Gini impurity. We also extend our analysis to other popular kernel clustering methods, e.g.,\u00a0average/normalized cut or dominant sets, where density biases can take different forms. For example, splitting isolated points by cut-based criteria is essentially the sparsest subset bias, which is the opposite of the density mode bias. Our findings suggest that a principled solution for density biases in kernel clustering should directly address data inhomogeneity. We show that density equalization can be implicitly achieved using either locally adaptive weights or locally adaptive kernels. Moreover, density equalization makes many popular kernel clustering objectives equivalent. Our synthetic and real data experiments illustrate density biases and proposed solutions. We anticipate that theoretical understanding of kernel clustering limitations and their principled solutions will be important for a broad spectrum of data analysis applications across the disciplines.", "year": 2017, "ssId": "60a9438c24847a949419e0350a61fc2a330e4a09", "arXivId": "1705.05950", "link": "https://arxiv.org/pdf/1705.05950.pdf", "openAccess": true, "authors": ["D. Marin", "Meng Tang", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5", "content": {"title": "Kernel clustering: Breiman's bias and solutions", "abstract": "Clustering is widely used in data analysis where kernel methods are particularly popular due to their generality and discriminating power. However, kernel clustering has a practically significant bias to small dense clusters, e.g. empirically observed in (Shi & Malik, TPAMI'00). Its causes have never been analyzed and understood theoretically, even though many attempts were made to improve the results. We provide conditions and formally prove this bias in kernel clustering. Moreover, we show a general class of locally adaptive kernels directly addressing these conditions. Previously, (Breiman, ML'96) proved a bias to histogram mode isolation in discrete Gini criterion for decision tree learning. We found that kernel clustering reduces to a continuous generalization of Gini criterion for a common class of kernels where we prove a bias to density mode isolation and call it Breiman's bias. These theoretical findings suggest that a principal solution for the bias should directly address data density inhomogeneity. In particular, our density law shows how density equalization can be done implicitly using certain locally adaptive geodesic kernels. Interestingly, a popular heuristic kernel in (Zelnik-Manor and Perona, NIPS'04) approximates a special case of our Riemannian kernel framework. Our general ideas are relevant to any algorithms for kernel clustering. We show many synthetic and real data experiments illustrating Breiman's bias and its solution. We anticipate that theoretical understanding of kernel clustering limitations and their principled solutions will be important for a broad spectrum of data analysis applications in diverse disciplines.", "year": 2017, "ssId": "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5", "arXivId": null, "link": null, "openAccess": false, "authors": ["D. Marin", "Meng Tang", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0", "content": {"title": "Normalized Cut Meets MRF", "abstract": "We propose a new segmentation or clustering model that combines Markov Random Field (MRF) and Normalized Cut (NC) objectives. Both NC and MRF models are widely used in machine learning and computer vision, but they were not combined before due to significant differences in the corresponding optimization, e.g. spectral relaxation and combinatorial max-flow techniques. On the one hand, we show that many common applications for multi-label MRF segmentation energies can benefit from a high-order NC term, e.g. enforcing balanced clustering of arbitrary high-dimensional image features combining color, texture, location, depth, motion, etc. On the other hand, standard NC applications benefit from an inclusion of common pairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency, label cost, etc. To address NC+MRF energy, we propose two efficient multi-label combinatorial optimization techniques, spectral cut and kernel cut, using new unary bounds for different NC formulations.", "year": 2016, "ssId": "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0", "arXivId": null, "link": null, "openAccess": false, "authors": ["Meng Tang", "D. Marin", "I. B. Ayed", "Yuri Boykov"]}}
{"id": "d95aafa571e9cb6795cc28ecf257ead123664e3c", "content": {"title": "Kernel Cuts: MRF meets Kernel & Spectral Clustering", "abstract": "We propose a new segmentation model combining common regularization energies, e.g. Markov Random Field (MRF) potentials, and standard pairwise clustering criteria like Normalized Cut (NC), average association (AA), etc. These clustering and regularization models are widely used in machine learning and computer vision, but they were not combined before due to significant differences in the corresponding optimization, e.g. spectral relaxation and combinatorial max-flow techniques. On the one hand, we show that many common applications using MRF segmentation energies can benefit from a high-order NC term, e.g. enforcing balanced clustering of arbitrary high-dimensional image features combining color, texture, location, depth, motion, etc. On the other hand, standard clustering applications can benefit from an inclusion of common pairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency, label cost, etc. To address joint energies like NC+MRF, we propose efficient Kernel Cut algorithms based on bound optimization. While focusing on graph cut and move-making techniques, our new unary (linear) kernel and spectral bound formulations for common pairwise clustering criteria allow to integrate them with any regularization functionals with existing discrete or continuous solvers.", "year": 2015, "ssId": "d95aafa571e9cb6795cc28ecf257ead123664e3c", "arXivId": "1506.07439", "link": "https://arxiv.org/pdf/1506.07439.pdf", "openAccess": true, "authors": ["Meng Tang", "I. B. Ayed", "D. Marin", "Yuri Boykov"]}}
{"id": "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff", "content": {"title": "Thin Structure Estimation with Curvature Regularization", "abstract": "Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples.", "year": 2015, "ssId": "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff", "arXivId": "1506.04654", "link": "https://arxiv.org/pdf/1506.04654.pdf", "openAccess": true, "authors": ["D. Marin", "Yuchen Zhong", "M. Drangova", "Yuri Boykov"]}}
{"id": "1d05e91b6d94f06439b2b41291a8dcc3d8064149", "content": {"title": "Secrets of GrabCut and Kernel K-Means", "abstract": "The log-likelihood energy term in popular model-fitting segmentation methods, e.g. [39, 8, 28, 10], is presented as a generalized \"probabilistic K-means\" energy [16] for color space clustering. This interpretation reveals some limitations, e.g. over-fitting. We propose an alternative approach to color clustering using kernel K-means energy with well-known properties such as non-linear separation and scalability to higher-dimensional feature spaces. Our bound formulation for kernel K-means allows to combine general pair-wise feature clustering methods with image grid regularization using graph cuts, similarly to standard color model fitting techniques for segmentation. Unlike histogram or GMM fitting [39, 28], our approach is closely related to average association and normalized cut. But, in contrast to previous pairwise clustering algorithms, our approach can incorporate any standard geometric regularization in the image domain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) and demonstrate effectiveness of KNN-based adaptive bandwidth strategies. Our kernel K-means approach to segmentation benefits from higher-dimensional features where standard model fitting fails.", "year": 2015, "ssId": "1d05e91b6d94f06439b2b41291a8dcc3d8064149", "arXivId": null, "link": null, "openAccess": false, "authors": ["Meng Tang", "I. B. Ayed", "D. Marin", "Yuri Boykov"]}}
