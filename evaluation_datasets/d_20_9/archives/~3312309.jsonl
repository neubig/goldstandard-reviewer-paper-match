{"id": "499ada382b7ce8f1cbd890e8c21500d95e20f2fe", "content": {"title": "HEAR 2021: Holistic Evaluation of Audio Representations", "abstract": "What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without \ufb01ne-tuning? The aim of the HEAR 2021 NeurIPS challenge is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR 2021 evalu-ates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.", "year": 2022, "ssId": "499ada382b7ce8f1cbd890e8c21500d95e20f2fe", "arXivId": "2203.03022", "link": "https://arxiv.org/pdf/2203.03022.pdf", "openAccess": true, "authors": ["Joseph P. Turian", "Jordie Shier", "H. Khan", "B. Raj", "Bj\u00f6rn Schuller", "C. Steinmetz", "C. Malloy", "G. Tzanetakis", "Gissel Velarde", "K. McNally", "Max Henry", "Nicolas Pinto", "Camille Noufi", "Christian Clough", "Dorien Herremans", "Eduardo Fonseca", "Jesse Engel", "J. Salamon", "P. Esling", "Pranay Manocha", "Shinji Watanabe", "Zeyu Jin", "Yonatan Bisk"]}}
{"id": "2550fafc0cbd8bbf7aadd864ac569596d33db038", "content": {"title": "Grounding \u2018Grounding\u2019 in NLP", "abstract": "The NLP community has seen substantial recent interest in grounding to facilitate interaction between language technologies and the world. However, as a community, we use the term broadly to reference any linking of text to data or non-textual modality. In contrast, Cognitive Science more formally defines \u201cgrounding\u201d as the process of establishing what mutual information is required for successful communication between two interlocutors \u2013 a definition which might implicitly capture the NLP usage but differs in intent and scope. We investigate the gap between these definitions and seek answers to the following questions: (1) What aspects of grounding are missing from NLP tasks? Here we present the dimensions of coordination, purviews and constraints. (2) How is the term \u201cgrounding\u201d used in the current research? We study the trends in datasets, domains, and tasks introduced in recent NLP conferences. And finally, (3) How to advance our current definition to bridge the gap with Cognitive Science? We present ways to both create new tasks or repurpose existing ones to make advancements towards achieving a more complete sense of grounding. github.com/khyathiraghavi/Grounding-Grounding", "year": 2021, "ssId": "2550fafc0cbd8bbf7aadd864ac569596d33db038", "arXivId": "2106.02192", "link": "https://arxiv.org/pdf/2106.02192.pdf", "openAccess": true, "authors": ["Khyathi Raghavi Chandu", "Yonatan Bisk", "A. Black"]}}
{"id": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30", "content": {"title": "Few-shot Language Coordination by Modeling Theory of Mind", "abstract": "No man is an island. Humans communicate with a large community by coordinating with different interlocutors within short conversations. This ability has been understudied by the research on building neural communicative agents. We study the task of few-shot language coordination: agents quickly adapting to their conversational partners\u2019 language abilities. Different from current communicative agents trained with selfplay, we require the lead agent to coordinate with a population of agents with different linguistic abilities, quickly adapting to communicate with unseen agents in the population. This requires the ability to model the partner\u2019s beliefs, a vital component of human communication. Drawing inspiration from theory-of-mind (ToM; Premack & Woodruff (1978)), we study the effect of the speaker explicitly modeling the listeners\u2019 mental states. The speakers, as shown in our experiments, acquire the ability to predict the reactions of their partner, which helps it generate instructions that concisely express its communicative goal. We examine our hypothesis that the instructions generated with ToM modeling yield better communication performance in both a referential game and a language navigation task. Positive results from our experiments hint at the importance of explicitly modeling communication as a socio-pragmatic progress. Code can be found at https://github.com/CLAW-Lab/ToM.", "year": 2021, "ssId": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30", "arXivId": "2107.05697", "link": "https://arxiv.org/pdf/2107.05697.pdf", "openAccess": true, "authors": ["Hao Zhu", "Graham Neubig", "Yonatan Bisk"]}}
{"id": "36c95e3ef362742a5c1844257e8b79d3251a781e", "content": {"title": "Language Grounding with 3D Objects", "abstract": "Seemingly simple natural language requests to a robot are generally underspecified, for example Can you bring me the wireless mouse? Flat images of candidate mice may not provide the discriminative information needed for wireless. The world, and objects in it, are not flat images but complex 3D shapes. If a human requests an object based on any of its basic properties, such as color, shape, or texture, robots should perform the necessary exploration to accomplish the task. In particular, while substantial effort and progress has been made on understanding explicitly visual attributes like color and category, comparatively little progress has been made on understanding language about shapes and contours. In this work, we introduce a novel reasoning task that targets both visual and non-visual language about 3D objects. Our new benchmark ShapeNet Annotated with Referring Expressions (SNARE) requires a model to choose which of two objects is being referenced by a natural language description.2 We introduce several CLIP-based [1] models for distinguishing objects and demonstrate that while recent advances in jointly modeling vision and language are useful for robotic language understanding, it is still the case that these image-based models are weaker at understanding the 3D nature of objects \u2013 properties which play a key role in manipulation. We find that adding view estimation to language grounding models improves accuracy on both SNARE and when identifying objects referred to in language on a robot platform, but note that a large gap remains between these models and human performance.", "year": 2021, "ssId": "36c95e3ef362742a5c1844257e8b79d3251a781e", "arXivId": "2107.12514", "link": "https://arxiv.org/pdf/2107.12514.pdf", "openAccess": true, "authors": ["Jesse Thomason", "Mohit Shridhar", "Yonatan Bisk", "Chris Paxton", "Luke Zettlemoyer"]}}
{"id": "a67face220a88b6b36f3343a6a017a3536562d5b", "content": {"title": "An Empirical Study on the Generalization Power of Neural Representations Learned via Visual Guessing Games", "abstract": "Guessing games are a prototypical instance of the \u201clearning by interacting\u201d paradigm. This work investigates how well an artificial agent can benefit from playing guessing games when later asked to perform on novel NLP downstream tasks such as Visual Question Answering (VQA). We propose two ways to exploit playing guessing games: 1) a supervised learning scenario in which the agent learns to mimic successful guessing games and 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL). We evaluate the ability of both procedures to generalise: an in-domain evaluation shows an increased accuracy (+7.79) compared with competitors on the evaluation suite CompGuessWhat?!; a transfer evaluation shows improved performance for VQA on the TDIUC dataset in terms of harmonic average accuracy (+5.31) thanks to more fine-grained object representations learned via SPIEL.", "year": 2021, "ssId": "a67face220a88b6b36f3343a6a017a3536562d5b", "arXivId": "2102.00424", "link": "https://arxiv.org/pdf/2102.00424.pdf", "openAccess": true, "authors": ["Alessandro Suglia", "Yonatan Bisk", "Ioannis Konstas", "Antonio Vergari", "E. Bastianelli", "Andrea Vanzo", "Oliver Lemon"]}}
{"id": "e79be3f9ce409f1a9b7084ef880298665e5212d0", "content": {"title": "TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment", "abstract": "Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware Cascade contrastive learning (TACo) that improves contrastive learning using two novel techniques. The first is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard negative examples for efficient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we finetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, set-ting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet.", "year": 2021, "ssId": "e79be3f9ce409f1a9b7084ef880298665e5212d0", "arXivId": "2108.09980", "link": "https://arxiv.org/pdf/2108.09980.pdf", "openAccess": true, "authors": ["Jianwei Yang", "Yonatan Bisk", "Jianfeng Gao"]}}
{"id": "afa9364ec48e38d19099cfc22ac9cb679c4baa39", "content": {"title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models", "abstract": "Numerous works have analyzed biases in vision and pre-trained language models individually however, less attention has been paid to how these biases interact in multimodal settings. This work extends text-based bias analysis methods to investigate multimodal language models, and analyzes intraand intermodality associations and biases learned by these models. Specifically, we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene. We demonstrate these findings on a controlled case-study and extend them for a larger set of stereotypically gendered entities.", "year": 2021, "ssId": "afa9364ec48e38d19099cfc22ac9cb679c4baa39", "arXivId": "2104.08666", "link": "https://arxiv.org/pdf/2104.08666.pdf", "openAccess": true, "authors": ["Tejas Srinivasan", "Yonatan Bisk"]}}
{"id": "7668b23aadf43bebe5e2d3abf37938b44bd16200", "content": {"title": "WebQA: Multihop and Multimodal QA", "abstract": "Scaling Visual Question Answering (VQA) to the opendomain and multi-hop nature of web searches, requires fundamental advances in visual representation learning, knowledge aggregation, and language generation. In this work, we introduce WEBQA, a challenging new benchmark that proves difficult for large-scale state-of-the-art models which lack language groundable visual representations for novel objects and the ability to reason, yet trivial for humans. WEBQA mirrors the way humans use the web: 1) Ask a question, 2) Choose sources to aggregate, and 3) Produce a fluent language response. This is the behavior we should be expecting from IoT devices and digital assistants. Existing work prefers to assume that a model can either reason about knowledge in images or in text. WEBQA includes a secondary text-only QA task to ensure improved visual performance does not come at the cost of language understanding. Our challenge for the community is to create unified multimodal reasoning models that answer questions regardless of the source modality, moving us closer to digital assistants that not only query language knowledge, but also the richer visual online world.", "year": 2021, "ssId": "7668b23aadf43bebe5e2d3abf37938b44bd16200", "arXivId": "2109.00590", "link": "https://arxiv.org/pdf/2109.00590.pdf", "openAccess": true, "authors": ["Yingshan Chang", "M. Narang", "Hisami Suzuki", "Guihong Cao", "Jianfeng Gao", "Yonatan Bisk"]}}
{"id": "6fa7de6f3ce3a599de6fab273a0d43939e176e9d", "content": {"title": "A Framework for Learning to Request Rich and Contextually Useful Information from Humans", "abstract": "When deployed, AI agents will encounter problems that are beyond their autonomous problemsolving capabilities. Leveraging human assistance can help agents overcome their inherent limitations and robustly cope with unfamiliar situations. We present a general interactive framework that enables an agent to determine and request contextually useful information from an assistant, and to incorporate rich forms of responses into its decision-making process. We demonstrate the practicality of our framework on a simulated human-assisted navigation problem. Aided with an assistance-requesting policy learned by our method, a navigation agent achieves up to a 7\u00d7 improvement in success rate on tasks that take place in previously unseen environments, compared to fully autonomous behavior. We show that the agent can take advantage of different types of information depending on the context, and analyze the benefits and challenges of learning the assistance-requesting policy when the assistant can recursively decompose tasks into subtasks.", "year": 2021, "ssId": "6fa7de6f3ce3a599de6fab273a0d43939e176e9d", "arXivId": "2110.08258", "link": "https://arxiv.org/pdf/2110.08258.pdf", "openAccess": true, "authors": ["Khanh Nguyen", "Yonatan Bisk", "Hal Daum'e"]}}
{"id": "fc848789b557a7581c51c79fd01897dc5aa7e8a8", "content": {"title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language", "abstract": "The primary focus of recent work with largescale transformers has been on optimizing the amount of information packed into the model\u2019s parameters. In this work, we ask a different question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a novel model Knowledge Augmented Transformer (KAT) which achieves a strong state-of-the-art result (+6 points absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an end to end encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. An additional benefit of explicit knowledge integration is seen in improved interpretability of model predictions in our analysis.", "year": 2021, "ssId": "fc848789b557a7581c51c79fd01897dc5aa7e8a8", "arXivId": "2112.08614", "link": "https://arxiv.org/pdf/2112.08614.pdf", "openAccess": true, "authors": ["Liangke Gui", "Borui Wang", "Qiuyuan Huang", "A. Hauptmann", "Yonatan Bisk", "Jianfeng Gao"]}}
{"id": "e5d143ae82ede67726aa1a9aeac3de4bf53d8920", "content": {"title": "KB-VLP: Knowledge Based Vision and Language Pretraining", "abstract": "Transformer-based pretraining techniques have achieved impressive performance on learning cross-model representations for various multimodal tasks. However, off-the-shelf models do not take advantage of commonsense knowledge and logical reasoning that are crucial to many realworld tasks. To this end, we introduce a novel pretraining approach Knowledge Based Vision and Language Pretraining (KB-VLP) which uses knowledge graph embeddings extracted from text and detected image object tags to enhance the learning of semantically aligned and knowledgeaware representations, and improve the models generalization, and interpretability. KB-VLP is pretrained on a large image-text corpus and automatically extracted knowledge embeddings, and then finetuned on several downstream visionlanguage tasks. Experiments show that KB-VLP significantly improves the performance on VQA, GQA, NLVR and OKVQA tasks compared with the baselines.", "year": 2021, "ssId": "e5d143ae82ede67726aa1a9aeac3de4bf53d8920", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kezhen Chen", "Qiuyuan Huang", "Yonatan Bisk", "Daniel J. McDuff", "Jianfeng Gao"]}}
{"id": "f837bf72e5b864e1c162e924fed59b778e946e23", "content": {"title": "Imagining Grounded Conceptual Representations from Perceptual Information in Situated Guessing Games", "abstract": "In visual guessing games, a Guesser has to identify a target object in a scene by asking questions to an Oracle. An effective strategy for the players is to learn conceptual representations of objects that are both discriminative and expressive enough to ask questions and guess correctly. However, as shown by Suglia et al. (2020), existing models fail to learn truly multi-modal representations, relying instead on gold category labels for objects in the scene both at training and inference time. This provides an unnatural performance advantage when categories at inference time match those at training time, and it causes models to fail in more realistic \u201czero-shot\u201d scenarios where out-of-domain object categories are involved. To overcome this issue, we introduce a novel \u201cimagination\u201d module based on Regularized Auto-Encoders, that learns context-aware and category-aware latent embeddings without relying on category labels at inference time. Our imagination module outperforms state-of-the-art competitors by 8.26% gameplay accuracy in the CompGuessWhat?! zero-shot scenario (Suglia et al., 2020), and it improves the Oracle and Guesser accuracy by 2.08% and 12.86% in the GuessWhat?! benchmark, when no gold categories are available at inference time. The imagination module also boosts reasoning about object properties and attributes.", "year": 2020, "ssId": "f837bf72e5b864e1c162e924fed59b778e946e23", "arXivId": "2011.02917", "link": "https://arxiv.org/pdf/2011.02917.pdf", "openAccess": true, "authors": ["Alessandro Suglia", "Antonio Vergari", "Ioannis Konstas", "Yonatan Bisk", "E. Bastianelli", "Andrea Vanzo", "Oliver Lemon"]}}
{"id": "2db020e3398c06e3a22f12d8caffe76b0d9d1dda", "content": {"title": "Knowledge-driven Self-supervision for Zero-shot Commonsense Question Answering", "abstract": "Recent developments in pre-trained neural language modeling have led to leaps in accuracy on commonsense question-answering benchmarks. However, there is increasing concern that models overfit to specific tasks, without learning to utilize external knowledge or perform general semantic reasoning. In contrast, zero-shot evaluations have shown promise as a more robust measure of a model's general reasoning abilities. In this paper, we propose a novel neuro-symbolic framework for zero-shot question answering across commonsense tasks. Guided by a set of hypotheses, the framework studies how to transform various pre-existing knowledge resources into a form that is most effective for pre-training models. We vary the set of language models, training regimes, knowledge sources, and data generation strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies. We provide empirical results across five commonsense question-answering tasks with data generated from five external knowledge resources. We show that, while an individual knowledge graph is better suited for specific tasks, a global knowledge graph brings consistent gains across different tasks. In addition, both preserving the structure of the task as well as generating fair and informative questions help language models learn more effectively.", "year": 2020, "ssId": "2db020e3398c06e3a22f12d8caffe76b0d9d1dda", "arXivId": "2011.03863", "link": "https://arxiv.org/pdf/2011.03863.pdf", "openAccess": true, "authors": ["Kaixin Ma", "Filip Ilievski", "Jonathan Francis", "Yonatan Bisk", "Eric Nyberg", "A. Oltramari"]}}
{"id": "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9", "content": {"title": "Multi-View Learning for Vision-and-Language Navigation", "abstract": "Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and under-specified. In this paper, we present a novel training paradigm, Learn from EveryOne (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent as the base agent (25.3% $\\rightarrow$ 41.4%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+, which creates the new state of the art, pushing the R2R benchmark to 62% (9% absolute improvement).", "year": 2020, "ssId": "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9", "arXivId": "2003.00857", "link": "https://arxiv.org/pdf/2003.00857.pdf", "openAccess": true, "authors": ["Qiaolin Xia", "Xiujun Li", "Chunyuan Li", "Yonatan Bisk", "Zhifang Sui", "Yejin Choi", "Noah A. Smith"]}}
{"id": "358d7d6333d3edd530e37efd8004cb9da8cfd5d4", "content": {"title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos", "abstract": "Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.", "year": 2020, "ssId": "358d7d6333d3edd530e37efd8004cb9da8cfd5d4", "arXivId": "2005.00706", "link": "https://arxiv.org/pdf/2005.00706.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Lei Ji", "Botian Shi", "Junyi Du", "Graham Neubig", "Yonatan Bisk", "Nan Duan"]}}
{"id": "645bc7a5347a299a1e8aa965867bd097f6f4bddd", "content": {"title": "RMM: A Recursive Mental Model for Dialog Navigation", "abstract": "Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers. Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The navigating agent models the guiding agent to simulate answers given candidate generated questions. The guiding agent in turn models the navigating agent to simulate navigation steps it would take to generate answers. We use the progress agents make towards the goal as a reinforcement learning reward signal to directly inform not only navigation actions, but also both question and answer generation. We demonstrate that RMM enables better generalization to novel environments. Interlocutor modelling may be a way forward for human-agent RMM where robots need to both ask and answer questions.", "year": 2020, "ssId": "645bc7a5347a299a1e8aa965867bd097f6f4bddd", "arXivId": "2005.00728", "link": "https://arxiv.org/pdf/2005.00728.pdf", "openAccess": true, "authors": ["Homero Roman Roman", "Yonatan Bisk", "Jesse Thomason", "Asli Celikyilmaz", "Jianfeng Gao"]}}
{"id": "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d", "content": {"title": "Experience Grounds Language", "abstract": "Successful linguistic communication relies on a shared experience of the world, and it is this shared experience that makes utterances meaningful. Despite the incredible effectiveness of language processing models trained on text alone, today's best systems still make mistakes that arise from a failure to relate language to the physical world it describes and to the social interactions it facilitates. \nNatural Language Processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large text corpora can be deeply enriched from the parallel tradition of research on the contextual and social nature of language. \nIn this article, we consider work on the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and possible progression of how contextual information can factor into our representations, with an eye towards how this integration can move the field forward and where it is currently being pioneered. We believe this framing will serve as a roadmap for truly contextual language understanding.", "year": 2020, "ssId": "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d", "arXivId": "2004.10151", "link": "https://arxiv.org/pdf/2004.10151.pdf", "openAccess": true, "authors": ["Yonatan Bisk", "Ari Holtzman", "Jesse Thomason", "Jacob Andreas", "Yoshua Bengio", "J. Chai", "Mirella Lapata", "Angeliki Lazaridou", "Jonathan May", "Aleksandr Nisnevich", "N. Pinto", "Joseph P. Turian"]}}
{"id": "65c2a39f1579a947926ac5746888445ea4afdf6e", "content": {"title": "The Return of Lexical Dependencies: Neural Lexicalized PCFGs", "abstract": "Abstract In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g., lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs that allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.1", "year": 2020, "ssId": "65c2a39f1579a947926ac5746888445ea4afdf6e", "arXivId": "2007.15135", "link": "https://arxiv.org/pdf/2007.15135.pdf", "openAccess": true, "authors": ["Hao Zhu", "Yonatan Bisk", "Graham Neubig"]}}
{"id": "398a0625e8707a0b41ac58eaec51e8feb87dd7cb", "content": {"title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning", "abstract": "Given a simple request (e.g., Put a washed apple in the kitchen fridge), humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Cote et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, visual scene understanding, and so forth).", "year": 2020, "ssId": "398a0625e8707a0b41ac58eaec51e8feb87dd7cb", "arXivId": "2010.03768", "link": "https://arxiv.org/pdf/2010.03768.pdf", "openAccess": true, "authors": ["Mohit Shridhar", "Xingdi Yuan", "Marc-Alexandre C\u00f4t\u00e9", "Yonatan Bisk", "Adam Trischler", "M. Hausknecht"]}}
{"id": "18e5fb8cec55a75b288a499c57d77ede541dc049", "content": {"title": "Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering", "abstract": "Recent developments in pre-trained neural language modeling have led to leaps in accuracy on commonsense question-answering benchmarks. However, there is increasing concern that models overfit to specific tasks, without learning to utilize external knowledge or perform general semantic reasoning. In contrast, zero-shot evaluations have shown promise as a more robust measure of a model's general reasoning abilities. In this paper, we propose a novel neuro-symbolic framework for zero-shot question answering across commonsense tasks. Guided by a set of hypotheses, the framework studies how to transform various pre-existing knowledge resources into a form that is most effective for pre-training models. We vary the set of language models, training regimes, knowledge sources, and data generation strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies. We provide empirical results across five commonsense question-answering tasks with data generated from five external knowledge resources. We show that, while an individual knowledge graph is better suited for specific tasks, a global knowledge graph brings consistent gains across different tasks. In addition, both preserving the structure of the task as well as generating fair and informative questions help language models learn more effectively.", "year": 2020, "ssId": "18e5fb8cec55a75b288a499c57d77ede541dc049", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kaixin Ma", "Filip Ilievski", "Jonathan Francis", "Yonatan Bisk", "Eric Nyberg", "A. Oltramari"]}}
