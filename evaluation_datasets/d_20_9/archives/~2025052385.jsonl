{"id": "60e339d25d43c026cf96395aa8accf34eae744a5", "content": {"title": "IMDB-WIKI-SbS: An Evaluation Dataset for Crowdsourced Pairwise Comparisons", "abstract": "Today, comprehensive evaluation of large-scale machine learning models is possible thanks to the open datasets produced using crowdsourcing, such as SQuAD, MS COCO, ImageNet, SuperGLUE, etc. These datasets capture objective responses, assuming the single correct answer, which does not allow to capture the subjective human perception. In turn, pairwise comparison tasks, in which one has to choose between only two options, allow taking peoples\u2019 preferences into account for very challenging artificial intelligence tasks, such as information retrieval and recommender system evaluation. Unfortunately, the available datasets are either small or proprietary, slowing down progress in gathering better feedback from human users. In this paper, we present IMDB-WIKI-SbS, a new large-scale dataset for evaluating pairwise comparisons.1 It contains 9,150 images appearing in 250,249 pairs annotated on a crowdsourcing platform. Our dataset has balanced distributions of age and gender using the well-known IMDB-WIKI dataset as ground truth. We describe how our dataset is built and then compare several baseline methods, indicating its suitability for model evaluation.", "year": 2021, "ssId": "60e339d25d43c026cf96395aa8accf34eae744a5", "arXivId": "2110.14990", "link": "https://arxiv.org/pdf/2110.14990.pdf", "openAccess": true, "authors": ["Nikita Pavlichenko", "Dmitry Ustalov"]}}
{"id": "1ccf412212873ae1b020762b8b86291e1fb11f65", "content": {"title": "CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription", "abstract": "Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CROWDSPEECH \u2014 the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VOXDIY \u2014 a counterpart of CROWDSPEECH for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.1", "year": 2021, "ssId": "1ccf412212873ae1b020762b8b86291e1fb11f65", "arXivId": "2107.01091", "link": "https://arxiv.org/pdf/2107.01091.pdf", "openAccess": true, "authors": ["Nikita Pavlichenko", "Ivan Stelmakh", "Dmitry Ustalov"]}}
{"id": "2ec99c834bd67ac64ec04b426e5f9fd04f639024", "content": {"title": "Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription", "abstract": "Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. Crowdsourcing has become one of the standard tools for cheap and time-efficient data collection for simple problems such as image classification: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing advanced aggregation methods is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CROWDSPEECH \u2014 the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of collecting high-quality datasets using crowdsourcing: we develop a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VOXDIY \u2014 a counterpart of CROWDSPEECH for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.1", "year": 2021, "ssId": "2ec99c834bd67ac64ec04b426e5f9fd04f639024", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nikita Pavlichenko", "I. Stelmakh", "Dmitry Ustalov"]}}
{"id": "48e32ba9a891f36183a26f35316e8906d14d83c0", "content": {"title": "A General-Purpose Crowdsourcing Computational Quality Control Toolkit for Python", "abstract": "Quality control is a crux of crowdsourcing. While most means for quality control are organizational and imply worker selection, golden tasks, and post-acceptance, computational quality control techniques allow parameterizing the whole crowdsourcing process of workers, tasks, and labels, inferring and revealing relationships between them. In this paper, we demonstrate Crowd-Kit, a general-purpose crowdsourcing computational quality control toolkit. It provides efficient implementations in Python of computational quality control algorithms for crowdsourcing, including uncertainty measures and crowd consensus methods. We focus on aggregation methods for all the major annotation tasks, from the categorical annotation in which latent label assumption is met to more complex tasks like image and sequence aggregation. We perform an extensive evaluation of our toolkit on several datasets of different nature, enabling benchmarking computational quality control methods in a uniform, systematic, and reproducible way using the same codebase. We release our code and data under an open-source license at https://github.com/Toloka/crowd-kit.", "year": 2021, "ssId": "48e32ba9a891f36183a26f35316e8906d14d83c0", "arXivId": "2109.08584", "link": "https://arxiv.org/pdf/2109.08584.pdf", "openAccess": true, "authors": ["Dmitry Ustalov", "Nikita Pavlichenko", "V. Losev", "Evgeny Tulin", "Iulian Giliazev"]}}
{"id": "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b", "content": {"title": "Spherical convolutions on molecular graphs for protein model quality assessment", "abstract": "Processing information on 3D objects requires methods stable to rigid-body transformations, in particular rotations, of the input data. In image processing tasks, convolutional neural networks achieve this property using rotation-equivariant operations. However, contrary to images, graphs generally have irregular topology. This makes it challenging to define a rotation-equivariant convolution operation on these structures. In this work, we propose Spherical Graph Convolutional Network (S-GCN) that processes 3D models of proteins represented as molecular graphs. In a protein molecule, individual amino acids have common topological elements. This allows us to unambiguously associate each amino acid with a local coordinate system and construct rotation-equivariant spherical filters that operate on angular information between graph nodes. Within the framework of the protein model quality assessment problem, we demonstrate that the proposed spherical convolution method significantly improves the quality of model assessment compared to the standard message-passing approach. It is also comparable to state-of-the-art methods, as we demonstrate on Critical Assessment of Structure Prediction (CASP) benchmarks. The proposed technique operates only on geometric features of protein 3D models. This makes it universal and applicable to any other geometric-learning task where the graph structure allows constructing local coordinate systems.", "year": 2020, "ssId": "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b", "arXivId": "2011.07980", "link": "https://arxiv.org/pdf/2011.07980.pdf", "openAccess": true, "authors": ["Ilia Igashov", "Nikita Pavlichenko", "S. Grudinin"]}}
{"id": "2bbb33ab8124e5078ec39e821a25c24c20a31b9b", "content": {"title": "Preface", "abstract": "The second workshop on Crowd Science is organized in conjunction with the 47th International Conference on Very Large Data Bases (VLDB 2021). This workshop is the second in a series of events that has the goal of helping crowdsourcing \u201ctransition\u201d from art to science, and tackles the research challenges that we face to make crowdsourcing a technology that users can easily access and leverage, and produces results that researchers and businesses can rely on. In addition to regular paper submissions, this year we have organized a crowdsourced audio transcription shared task that attracted 18 participants around the world. Eight submissions have beaten a strong baseline method, thus advancing the state-of-the-art in this challenging task. This workshop features three invited talks, seven paper presentations, an overview of this shared task, as well as a panel discussion. We received 11 submissions, out of which 7 were accepted as the talks at the workshop after peer review. Besides the accepted papers, the volume contains a shared task overview by its organizers. We thank all the authors for their contributions and the effort they put into it, and we are very grateful for the excellent work of our reviewers and program committee members. Last but not least, we would like to thank VLDB organizers for their assistance that made the workshop organization process a pleasure. The 2021 edition of the workshop took place in a hybrid format, and we look forward to the forthcoming (in-person) editions of the Crowd Science workshop.", "year": 2016, "ssId": "2bbb33ab8124e5078ec39e821a25c24c20a31b9b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dmitry Ustalov", "Fabio Casati", "A. Drutsa", "Ivan Stelmakh", "Nikita Pavlichenko", "Daria Baidakova"]}}
