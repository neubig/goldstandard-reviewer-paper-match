{"id": "978582ad754eab481856d62bdc7b0ee5bcf21811", "content": {"title": "Federated Unsupervised Clustering with Generative Models", "abstract": "We consider the problem of clustering unlabeled datasets in the federated environment, where statistical heterogeneity can exist across clients. Compared to the centralized setting, the model-based solution for this problem remains relatively unexplored, possibly due to increased difficulty of training models with FedAvg algorithm under highly heterogeneous setting. A recently proposed Iterative Federated Clustering Algorithm (IFCA) addresses this issue by training multiple models that captures each cluster, and showed its effectiveness on supervised datasets, for the setting when the data are i.i.d. within the same client but separated across the clients. In this work, we develop UIFCA using generative models with IFCA framework, that solves for a more general setting where the data in the same client can also come from different clusters. For synthetic data, we observe that our method can correctly recover the cluster information of individual datapoints. We also provide analysis our method on MNIST dataset.", "year": 2021, "ssId": "978582ad754eab481856d62bdc7b0ee5bcf21811", "arXivId": null, "link": null, "openAccess": false, "authors": ["Jichan Chung", "Kangwook Lee", "K. Ramchandran"]}}
{"id": "e6accbbb366387faf817126dc7b0260c450bd2e6", "content": {"title": "SAFFRON: A Fast, Efficient, and Robust Framework for Group Testing Based on Sparse-Graph Codes", "abstract": "Group testing is the problem of identifying <inline-formula><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> defective items among <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math></inline-formula> items by <italic>pooling</italic> groups of items. In this paper, we design group testing algorithms for approximate recovery with order-optimal sample complexity by leveraging design and analysis tools from modern sparse-graph coding theory. Our algorithm, SAFFRON, recovers at least <inline-formula><tex-math notation=\"LaTeX\">$(1-\\varepsilon)K$</tex-math></inline-formula> defective items w.p. <inline-formula><tex-math notation=\"LaTeX\">$1-K/n^r$</tex-math></inline-formula> with <inline-formula><tex-math notation=\"LaTeX\">$m=2(1+r)C(\\varepsilon)K\\log _2{n}$</tex-math></inline-formula> tests, where <inline-formula><tex-math notation=\"LaTeX\">$\\varepsilon$</tex-math></inline-formula> is an arbitrarily small constant, <inline-formula><tex-math notation=\"LaTeX\">$C(\\varepsilon)$</tex-math></inline-formula> is a precisely characterizable constant, and <inline-formula><tex-math notation=\"LaTeX\">$r$</tex-math></inline-formula> is any positive integer. The decoding complexity is <inline-formula><tex-math notation=\"LaTeX\">$\\Theta (K\\log n)$</tex-math></inline-formula>. We also propose variations of SAFFRON, which are robust to noise and unknown offsets. For example, for <inline-formula><tex-math notation=\"LaTeX\">$n \\simeq 4.3\\times 10^9$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$K = 128$</tex-math></inline-formula>, our algorithm is observed to recover all defective items with <inline-formula><tex-math notation=\"LaTeX\">$m \\simeq 8.3\\times 10^{5}$</tex-math></inline-formula> tests, even in the presence of noisy test results. Moreover, the decoding time takes less than 4 seconds on a laptop with a 2 GHz Intel Core i7 and 8 GB memory.", "year": 2019, "ssId": "e6accbbb366387faf817126dc7b0260c450bd2e6", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Kabir Chandrasekher", "Ramtin Pedarsani", "K. Ramchandran"]}}
{"id": "47234fca1b14666d72bc5df0e2d911ff7cdea688", "content": {"title": "Hypergraph Spectral Clustering in the Weighted Stochastic Block Model", "abstract": "Spectral clustering is a celebrated algorithm that partitions the objects based on pairwise similarity information. While this approach has been successfully applied to a variety of domains, it comes with limitations. The reason is that there are many other applications in which only <italic>multi</italic>way similarity measures are available. This motivates us to explore the multiway measurement setting. In this paper, we develop two algorithms intended for such setting: hypergraph spectral clustering (HSC) and hypergraph spectral clustering with local refinement (HSCLR). Our main contribution lies in performance analysis of the polytime algorithms under a random hypergraph model, which we name the weighted stochastic block model, in which objects and multiway measures are modeled as nodes and weights of hyperedges, respectively. Denoting by <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math></inline-formula> the number of nodes, our analysis reveals the following: 1) HSC outputs a partition which is better than a random guess if the sum of edge weights (to be explained later) is <inline-formula><tex-math notation=\"LaTeX\">$\\Omega (n)$</tex-math> </inline-formula>; 2) HSC outputs a partition which coincides with the hidden partition except for a vanishing fraction of nodes if the sum of edge weights is <inline-formula><tex-math notation=\"LaTeX\">$\\omega (n)$</tex-math> </inline-formula>; and 3) HSCLR exactly recovers the hidden partition if the sum of edge weights is on the order of <inline-formula><tex-math notation=\"LaTeX\">$n \\log n$</tex-math></inline-formula>. Our results improve upon the state of the arts recently established under the model and they first settle the orderwise optimal results for the binary edge weight case. Moreover, we show that our results lead to efficient sketching algorithms for subspace clustering, a computer vision application. Finally, we show that HSCLR achieves the information-theoretic limits for a special yet practically relevant model, thereby showing no computational barrier for the case.", "year": 2018, "ssId": "47234fca1b14666d72bc5df0e2d911ff7cdea688", "arXivId": "1805.08956", "link": "https://arxiv.org/pdf/1805.08956.pdf", "openAccess": true, "authors": ["Kwangjun Ahn", "Kangwook Lee", "Changho Suh"]}}
{"id": "e92677eb974a2814d57de54e2c3733cbd92e2c00", "content": {"title": "Hierarchical Coding for Distributed Computing", "abstract": "Coding for distributed computing supports low-latency computation by relieving the burden of straggling workers. While most existing works assume a simple master-worker model, we consider a hierarchical computational structure consisting of groups of workers, motivated by the need to reflect the architectures of real-world distributed computing systems. In this work, we propose a hierarchical coding scheme for this model, as well as analyze its decoding cost and expected computation time. Specifically, we first provide upper and lower bounds on the expected computing time of the proposed scheme. We also show that our scheme enables efficient parallel decoding, thus reducing decoding costs by orders of magnitude over non-hierarchical schemes. When considering both decoding cost and computing time, the proposed hierarchical coding is shown to outperform existing schemes in many practical scenarios.", "year": 2018, "ssId": "e92677eb974a2814d57de54e2c3733cbd92e2c00", "arXivId": "1801.04686", "link": "https://arxiv.org/pdf/1801.04686.pdf", "openAccess": true, "authors": ["Hyegyeong Park", "Kangwook Lee", "Jy-yong Sohn", "Changho Suh", "J. Moon"]}}
{"id": "650f2afca6d72d6b6e2e08849e1224f1e8b7900c", "content": {"title": "Binary Rating Estimation with Graph Side Information", "abstract": "Rich experimental evidences show that one can better estimate users' unknown ratings with the aid of graph side information such as social graphs. However, the gain is not theoretically quantified. In this work, we study the binary rating estimation problem to understand the fundamental value of graph side information. Considering a simple correlation model between a rating matrix and a graph, we characterize the sharp threshold on the number of observed entries required to recover the rating matrix (called the optimal sample complexity) as a function of the quality of graph side information (to be detailed). To the best of our knowledge, we are the first to reveal how much the graph side information reduces sample complexity. Further, we propose a computationally efficient algorithm that achieves the limit. Our experimental results demonstrate that the algorithm performs well even with real-world graphs.", "year": 2018, "ssId": "650f2afca6d72d6b6e2e08849e1224f1e8b7900c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kwangjun Ahn", "Kangwook Lee", "Hyunseung Cha", "Changho Suh"]}}
{"id": "549df5fc83c382cbdf633dc782fa67bf2f983f2c", "content": {"title": "SGD ON R ANDOM M IXTURES : P RIVATE M ACHINE L EARNING UNDER D ATA B REACH T HREATS", "abstract": "We propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We show that SGDRM converges to the globally optimal point for deep neural networks with linear activations while being differentially private. We also train nonlinear neural networks with private mixtures as the training data, proving the practicality of SGDRM.", "year": 2018, "ssId": "549df5fc83c382cbdf633dc782fa67bf2f983f2c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Kyungmin Lee", "Hoon Kim", "Changho Suh", "K. Ramchandran"]}}
{"id": "1ee276db29ba9127e81d9a7d9cb08f5138339412", "content": {"title": "Straggler-Proofing Massive-Scale Distributed Matrix Multiplication with D-Dimensional Product Codes", "abstract": "Distributed computing allows for large-scale computation and machine learning tasks by enabling parallel computing at massive scale. A critical challenge to speeding up distributed computing comes from stragglers, a crippling bottleneck to system performance [1]. Recently, coding theory has offered an attractive paradigm dubbed as coded computation [2] for addressing this challenge through the judicious introduction of redundant computing to combat stragglers. However, most existing approaches have limited applicability if the system scales to hundreds or thousands of workers, as is the trend in computing platforms. At these scales, previously proposed algorithms based on Maximum Distance Separable (MDS) codes are too expensive due to their hidden cost, i.e., computing and communication costs associated with the encoding/decoding procedures. Motivated by this limitation, we present a novel coded matrix-matrix multiplication scheme based on d-dimensional product codes. We show that our scheme allows for order-optimal computation/communication costs for the encoding/decoding procedures while achieving near-optimal compute time.", "year": 2018, "ssId": "1ee276db29ba9127e81d9a7d9cb08f5138339412", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tavor Z. Baharav", "Kangwook Lee", "Orhan Ocal", "K. Ramchandran"]}}
{"id": "5bca90a331417402f5018f552e1a62656dd7fc5b", "content": {"title": "On the Joint Recovery of Community Structure and Community Features", "abstract": "We study the problem of recovering both K communities and their features from a labeled graph observation. We assume that the edges of an observed graph are generated as per the symmetric Stochastic Block Model (SBM), and that the label of each node is a noisy and partially-observed version of the corresponding community feature. We characterize the information-theoretic limit of this problem, and then propose a computationally efficient algorithm that achieves the information-theoretic limit.", "year": 2018, "ssId": "5bca90a331417402f5018f552e1a62656dd7fc5b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Jisang Yoon", "Kangwook Lee", "Changho Suh"]}}
{"id": "e4de1009eb7b3524bf7d19bdcebced80035a47cf", "content": {"title": "Asynchronous and noncoherent neighbor discovery for the IoT using sparse-graph codes", "abstract": "In this paper, we design a fast and efficient energy-based and asynchronous neighbor discovery protocol for the Internet of Things (IoT). In our solution, we relax the assumption of frame-level synchronization. We formulate a novel asynchronous group testing scheme and apply it to the neighbor discovery problem. We then show that our proposed scheme is able to detect the set of K active neighbors1 among a network of n nodes with codeword length and decoding complexity of \u0398(K log (K) log (n)). Finally, we provide extensive simulation results to verify our theoretical guarantees.", "year": 2017, "ssId": "e4de1009eb7b3524bf7d19bdcebced80035a47cf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kabir Chandrasekher", "Kangwook Lee", "P. Kairouz", "Ramtin Pedarsani", "K. Ramchandran"]}}
{"id": "dc1d1f64503578d9c5d906da4556f631d4178b04", "content": {"title": "Crash To Not Crash: Playing Video Games To Predict Vehicle Collisions", "abstract": "Today\u2019s vehicle collision prediction algorithms are rule-based, and have not benefited from the recent developments in deep learning. This is because it is almost impossible to collect a large amount of collision data from the real world. To address this challenge, we collect a large accident data set using a popular video game named GTA V. Using this accident data set, we develop efficient prediction algorithms based on modern CNN architectures. The performances of our algorithms are compared with the simple rule-based algorithms. We observe that the best CNN-based algorithm among several variants achieves the prediction accuracy of 96.2% while the best rule-based one achieves the accuracy of 89.6%. We also show that our approaches can identify the source of danger when a collision is predicted. Moreover, our approaches are shown to learn how the wheel angles, vehicle orientations, and distances affect the collision probability.", "year": 2017, "ssId": "dc1d1f64503578d9c5d906da4556f631d4178b04", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Hoon Kim", "Changho Suh"]}}
{"id": "a13c580250af3644fe368b08a540f4ea65dac919", "content": {"title": "PhaseCode: Fast and Efficient Compressive Phase Retrieval Based on Sparse-Graph Codes", "abstract": "We consider the problem of recovering a complex signal $ {x} \\in \\mathbb {C}^{n}$ from $m$ intensity measurements of the form $ | {a}_{i} ^{\\mathrm{ H}} {x}|, ~1 \\leq i \\leq m$ , where $ {a}_{i} ^{\\mathrm{ H}}$ is the $i$ th row of measurement matrix $ {A} \\in \\mathbb {C}^{m \\times n}$ . Our main focus is on the case where the measurement vectors are unconstrained, and where $ {x}$ is exactly $K$ -sparse, or the so-called general compressive phase retrieval problem. We introduce PhaseCode , a novel family of fast and efficient algorithms that are based on a sparse-graph coding framework. We show that in the noiseless case, the PhaseCode algorithm can recover an arbitrarily-close-to-one fraction of the $K$ nonzero signal components using only slightly more than $4K$ measurements when the support of the signal is uniformly random, with the order-optimal time and memory complexity of $\\Theta (K)$ . 1 It is known that the fundamental limit for the number of measurements in compressive phase retrieval problem is $4K - o(K)$ for the more difficult problem of recovering the signal exactly and with no assumptions on its support distribution. This shows that under mild relaxation of the conditions, our algorithm is the first constructive capacity-approaching compressive phase retrieval algorithm: in fact, our algorithm is also order-optimal in complexity and memory. Furthermore, we show that for any signal $ {x}$ , PhaseCode can recover a random $(1 - p)$ -fraction of the nonzero components of $ {x}$ with high probability, where $p$ can be made arbitrarily close to zero, with sample complexity $m = c(p)K$ , where $c(p)$ is a small constant depending on $p$ that can be precisely calculated, with optimal time and memory complexity. As a result, assuming that the nonzero components of $ {x}$ are lower bounded by $\\Theta (1)$ and upper bounded by $\\Theta (K^{\\gamma })$ for some positive constant $\\gamma , we are able to provide a strong $\\ell _{1}$ guarantee for the estimated signal $\\hat { {x}}$ as follows: $\\| \\hat { {x}} - {x}\\|_{1} \\leq p \\| {x} \\|_{1}(1 + o(1))$ , where $p$ can be made arbitrarily close to zero. As one instance, the PhaseCode algorithm can provably recover, with high probability, a random $1 - 10^{-7}$ fraction of the significant signal components, using at most $m = 14K$ measurements. Next, motivated by some important practical classes of optical systems, we consider a \u201cFourier-friendly\u201d constrained measurement setting, and show that its performance matches that of the unconstrained setting, when the signal is sparse in the Fourier domain with uniform support. In the Fourier-friendly setting that we consider, the measurement matrix is constrained to be a cascade of Fourier matrices (corresponding to optical lenses) and diagonal matrices (corresponding to diffraction mask patterns). Finally, we tackle the compressive phase retrieval problem in the presence of noise, where measurements are in the form of $y_{i}= | {a}_{i} ^{\\mathrm{ H}} {x}|^{2}+w_{i}$ , and $w_{i}$ is the additive noise to the $i$ th measurement. We assume that the signal is quantized, and each nonzero component can take $L_{m}$ possible magnitudes and $L_{p}$ possible phases. We consider the regime, where $K=\\beta n^\\delta $ , $\\delta \\in (0,1)$ . We use the same architecture of PhaseCode for the noiseless case, and robustify it using two schemes: the almost-linear scheme and the sublinear scheme. We prove that with high probability, the almost-linear scheme recovers $ {x}$ with sample complexity $\\Theta (K \\log (n))$ and computational complexity $\\Theta (L_{m} L_{p} n \\log (n))$ , and the sublinear scheme recovers $ {x}$ with sample complexity $\\Theta (K\\log ^{3}(n))$ and computational complexity $\\Theta (L_{m} L_{p} K\\log ^{3}(n))$ . Throughout, we provide extensive simulation results that validate the practical power of our proposed algorithms for the sparse unconstrained and Fourier-friendly measurement settings, for noiseless and noisy scenarios. 1 Here, we define the notation $ \\mathcal {O}(\\cdot )$ , $\\Theta (\\cdot )$ , and $\\Omega (\\cdot )$ . We have $f= \\mathcal {O}(g)$ if and only if there exists a constant $C_{1}>0$ such that $ \\left |{f/g}\\right | ; $f=\\Theta (g)$ if and only if there exist two constants $C_{1},C_{2}>0$ such that $C_{1} ; and $f=\\Omega (g)$ if and only if there exists a constant $C_{1}>0$ such that $ \\left |{f/g}\\right |>C_{1}$ .", "year": 2017, "ssId": "a13c580250af3644fe368b08a540f4ea65dac919", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ramtin Pedarsani", "Dong Yin", "Kangwook Lee", "K. Ramchandran"]}}
{"id": "54e7209e692ca4f5c85f0e68df34040b3cfa8bad", "content": {"title": "Matrix sparsification for coded matrix multiplication", "abstract": "Coded computation is a framework for providing redundancy in distributed computing systems to make them robust to slower nodes, or stragglers. In a recent work of Lee et al., the authors propose a coded computation scheme for distributedly computing A x x in the presence of stragglers. The proposed algorithm first encodes the data matrix A to obtain an encoded matrix F. It then computes F x x using distributed processors, waits for some subset of the processors to finish their computations, and decodes A x x from the partial computation results. In another recent work, Dutta et al. explore a new tradeoff between the sparsity of the encoded matrix F and the number of processors to wait to compute A x x. They show that one can introduce a large number of zeros into F to reduce the computational overheads while maintaining the number of processors to wait relatively low. Hence, one can potentially further speed up the distributed computation. In this work, motivated by this observation, we study the sparsity of the encoded matrix for coded computation. Our goal is to characterize the fundamental limits on the sparsity level. We first show that the Short-Dot scheme is optimal if an Maximum Distance Separable (MDS) matrix is fixed. Further, by also designing this MDS matrix, we propose a new encoding scheme that can achieve a strictly larger sparsity than the existing schemes. We also provide an information-theoretic upper bound on the sparsity.", "year": 2017, "ssId": "54e7209e692ca4f5c85f0e68df34040b3cfa8bad", "arXivId": null, "link": null, "openAccess": false, "authors": ["Geewon Suh", "Kangwook Lee", "Changho Suh"]}}
{"id": "384bf224d91a1691c9e6384201483121e2e7ddab", "content": {"title": "Information-theoretic limits of subspace clustering", "abstract": "Subspace clustering is a celebrated problem that comes up in a variety of applications such as motion segmentation and face clustering. The goal of the problem is to find clusters in different subspaces from similarity measurements across data points. While the algorithmic aspect of this problem has been extensively studied in the literature, the information-theoretic limit on the number of similarities required for reliable clustering has been unknown. In this paper, we translate the problem into an instance of community recovery in hypergraphs, and characterize the sharp threshold on the limit required for exact subspace clustering. Moreover, we present a computationally efficient algorithm that achieves the fundamental limit.", "year": 2017, "ssId": "384bf224d91a1691c9e6384201483121e2e7ddab", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kwangjun Ahn", "Kangwook Lee", "Changho Suh"]}}
{"id": "96d5e1f691397dfb51e8b818a21a2d11eee46a59", "content": {"title": "Coded computation for multicore setups", "abstract": "Consider a distributed computing setup consisting of a master node and n worker nodes, each equipped with p cores, and a function f (x) = g(f1(x), f2(x),\u2026, fk(x)), where each fi can be computed independently of the rest. Assuming that the worker computational times have exponential tails, what is the minimum possible time for computing f? Can we use coding theory principles to speed up this distributed computation? In [1], it is shown that distributed computing of linear functions can be expedited by applying linear erasure codes. However, it is not clear if linear codes can speed up distributed computation of \u2018nonlinear\u2019 functions as well. To resolve this problem, we propose the use of sparse linear codes, exploiting the modern multicore processing architecture. We show that 1) our coding solution achieves the order optimal runtime, and 2) it is at least \u0398(\u221alog n) times faster than any uncoded schemes where the number of workers is n.", "year": 2017, "ssId": "96d5e1f691397dfb51e8b818a21a2d11eee46a59", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Ramtin Pedarsani", "Dimitris Papailiopoulos", "K. Ramchandran"]}}
{"id": "2821db8962fce43265215a9c4b8d66af02e16ae7", "content": {"title": "On scheduling redundant requests with cancellation overheads", "abstract": "Reducing latency in distributed computing and data storage systems is gaining increasing importance. Several empirical works have reported on the efficacy of scheduling redundant requests in such systems. That is, one may reduce job latency by 1) scheduling the same job at more than one server and 2) waiting only until the fastest of them responds. Inspired by the empirically observed gains of such schemes, several theoretical models have been proposed to explain the power of using redundant requests. Although the proposed models in the literature provide useful insights such as when scheduling redundant requests can be beneficial, all these results rely heavily on a common assumption: all redundant requests of a job can be immediately cancelled as soon as one of them is completed. In this paper, we study how one should schedule redundant requests when such assumption does not hold. This is of great importance in practice since cancellation of running jobs typically incurs non-negligible delays. In order to bridge the gap between the existing models and practice, we propose a new queueing model that captures such cancellation delays. We then find how one can schedule redundant requests to achieve the optimal average job latency when cancellation delay is considered and accounted for. Our results show that even with a small cancellation overhead, the actual optimal scheduling policy differs significantly from the optimal scheduling policy when the overhead is zero. Further, we study optimal dynamic scheduling policies, which appropriately schedule redundant requests based on the number of jobs in the system. Our analysis reveals that for the twoserver case, the optimal dynamic scheduler can achieve 7% to 16% lower average job latency, compared with the optimal static scheduler. This observation is in stark contrast to the known fact that the optimal static scheduler performs as well as the optimal dynamic scheduler when cancellation overhead is ignored, affirming that misleading conclusions result if the cancellation overhead is ignored completely.", "year": 2017, "ssId": "2821db8962fce43265215a9c4b8d66af02e16ae7", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Ramtin Pedarsani", "K. Ramchandran"]}}
{"id": "322ef476e90a487c8f9797bece7799b69af9e5c1", "content": {"title": "High-dimensional coded matrix multiplication", "abstract": "Coded computation is a framework for providing redundancy in distributed computing systems to make them robust to slower nodes, or stragglers. In [1], the authors propose a coded computation scheme based on maximum distance separable (MDS) codes for computing the product ATB, and this scheme is suitable for the case where one of the matrices is small enough to fit into a single compute node. In this work, we study coded computation involving large matrix multiplication where both matrices are large, and propose a new coded computation scheme, which we call product-coded matrix multiplication. Our analysis reveals interesting insights into which schemes perform best in which regimes. When the number of backup nodes scales sub-linearly in the size of the product, the product-coded scheme achieves the best run-time performance. On the other hand, when the number of backup nodes scales linearly in the size of the product, the MDS-coded scheme achieves the fundamental limit on the run-time performance. Further, we propose a novel application of low-density-parity-check (LDPC) codes to achieve linear-time decoding complexity, thus allowing our proposed solutions to scale gracefully.", "year": 2017, "ssId": "322ef476e90a487c8f9797bece7799b69af9e5c1", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Changho Suh", "K. Ramchandran"]}}
{"id": "2cc7db7b17ee7349800334b3a154f708850c6410", "content": {"title": "The MDS Queue: Analysing the Latency Performance of Erasure Codes", "abstract": "In order to scale economically, data centers are increasingly evolving their data storage methods from simple data replication to more powerful erasure codes, which provide the same level of reliability as replication, but at a significantly lower storage cost. In particular, it is well known that maximum-distance-separable (MDS) codes, such as Reed\u2013Solomon codes, can achieve a target reliability with the maximum storage efficiency. While the use of codes for providing improved reliability in archival storage systems, where data is less frequently accessed (or so-called \u201ccold data\u201d), is well understood, the role of codes in storing more frequently accessed and active \u201chot data\u201d, where latency is the key metric, is less clear. In this paper, we study data storage systems based on MDS codes through the lens of queueing theory, and term the queueing system arising under codes as an \u201cMDS queue.\u201d We provide lower and upper bounds on the average job latency for both centralized and decentralized versions of MDS queues. We also provide extensive simulations to corroborate our analysis as well as obtain additional insights.", "year": 2017, "ssId": "2cc7db7b17ee7349800334b3a154f708850c6410", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Nihar B. Shah", "Longbo Huang", "K. Ramchandran"]}}
{"id": "b9f5115b0353c268999fcc2f49c4b8e03a223994", "content": {"title": "Simulating outcomes of interventions using a multipurpose simulation program based on the evolutionary causal matrices and Markov chain", "abstract": "Predicting long-term outcomes of interventions is necessary for educational and social policy-making processes that might widely influence our society for the long term. However, performing such predictions based on data from large-scale experiments might be challenging due to the lack of time and resources. In order to address this issue, computer simulations based on evolutionary causal matrices and Markov chain can be used to predict long-term outcomes with relatively small-scale laboratory data. In this paper, we introduce Python classes implementing a computer simulation model and presented some pilot implementations demonstrating how the model can be utilized for predicting outcomes of diverse interventions. We also introduce the class-structuredsimulation module both with real experimental data and with hypothetical data formulated based on social psychological theories. Classes developed and tested in the present study provide researchers and practitioners with a feasible and practical method to simulate intervention outcomes prospectively.", "year": 2017, "ssId": "b9f5115b0353c268999fcc2f49c4b8e03a223994", "arXivId": "1711.09490", "link": "https://arxiv.org/pdf/1711.09490.pdf", "openAccess": true, "authors": ["Hyemin Han", "Kangwook Lee", "Firat Soylu"]}}
{"id": "5b1c0152bbb12ece2a8817c727e33e6d5c503065", "content": {"title": "Speeding up distributed machine learning using codes", "abstract": "Distributed machine learning algorithms that are widely run on modern large-scale computing platforms face several types of randomness, uncertainty and system \u201cnoise.\u201d These include stragglers1, system failures, maintenance outages, and communication bottlenecks. In this work, we view distributed machine learning algorithms through a coding-theoretic lens, and show how codes can equip them with robustness against this system noise. Motivated by their importance and universality, we focus on two of the most basic building blocks of distributed learning algorithms: data shuffling and matrix multiplication. In data shuffling, we use codes to reduce communication bottlenecks: when a constant fraction of the data can be cached at each worker node, and n is the number of workers, coded shuffling reduces the communication cost by up to a factor \u0398(n) over uncoded shuffling. For matrix multiplication, we use codes to alleviate the effects of stragglers, also known as the straggler problem. We show that if the number of workers is n, and the runtime of each subtask has an exponential tail, the optimal coded matrix multiplication is \u0398(log n) times faster than the uncoded matrix multiplication or the optimal task replication scheme.", "year": 2016, "ssId": "5b1c0152bbb12ece2a8817c727e33e6d5c503065", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Maximilian Lam", "Ramtin Pedarsani", "Dimitris Papailiopoulos", "K. Ramchandran"]}}
{"id": "4f02d8775123624088a91fcfff20625463e5239a", "content": {"title": "Learning analytics: Collaborative filtering or regression with experts?", "abstract": "An intelligent learning analytics based on an enormous amount of education data is a key enabler of the next generation of education; among many tasks of the intelligent learning analytics, personalized prediction of test responses based on the record of each individual learner is of the utmost importance. In recent years, a variety of machine learning algorithms for predicting test outcomes have been proposed, and two of the most prominent approaches are collaborative filtering and logistic regression. Collaborative filtering is fully data-driven since it does not require any extra information other than test outcomes while logistic regression is applicable only when questions can be independently analyzed by experts. In this work, we first propose a new model for test responses, and propose a collaborative filtering algorithm with enhanced human-interpretability based on the new model. Then, we evaluate the prediction performance of these approaches using a large education data set, collected via mobile applications for English Language Learning. Our experimental results show that the fully data-driven collaborative filtering approach can predict test outcomes better than the logistic regression approach.", "year": 2016, "ssId": "4f02d8775123624088a91fcfff20625463e5239a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kangwook Lee", "Changho Suh", "Jichan Chung", "Youngmin Cha"]}}
