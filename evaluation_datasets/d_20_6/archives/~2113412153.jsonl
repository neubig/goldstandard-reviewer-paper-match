{"id": "4e3016617e5e254bafebcbd7e96c509f670bdd37", "content": {"title": "Deep Performer: Score-to-Audio Music Performance Synthesis", "abstract": "Music performance synthesis aims to synthesize a musical score into a natural performance. In this paper, we borrow recent advances in text-to-speech synthesis and present the Deep Performer\u2014a novel system for score-to-audio music performance synthesis. Unlike speech, music often contains polyphony and long notes. Hence, we propose two new techniques for handling polyphonic inputs and providing a finegrained conditioning in a transformer encoder-decoder model. To train our proposed system, we present a new violin dataset consisting of paired recordings and scores along with estimated alignments between them. We show that our proposed model can synthesize music with clear polyphony and harmonic structures. In a listening test, we achieve competitive quality against the baseline model, a conditional generative audio model, in terms of pitch accuracy, timbre and noise level. Moreover, our proposed model significantly outperforms the baseline on an existing piano dataset in overall quality.", "year": 2022, "ssId": "4e3016617e5e254bafebcbd7e96c509f670bdd37", "arXivId": "2202.06034", "link": "https://arxiv.org/pdf/2202.06034.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Cong Zhou", "Taylor Berg-Kirkpatrick", "Julian McAuley"]}}
{"id": "eaa224ae5c969180503dda4972ab86d3a71c888c", "content": {"title": "An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition", "abstract": "Previous work has shown that neural architectures are able to perform optical music recognition (OMR) on monophonic and homophonic music with high accuracy. However, piano and orchestral scores frequently exhibit polyphonic passages, which add a second dimension to the task. Monophonic and homophonic music can be described as homorhythmic, or having a single musical rhythm. Polyphonic music, on the other hand, can be seen as having multiple rhythmic sequences, or voices, concurrently. We first introduce a workflow for creating large-scale polyphonic datasets suitable for end-to-end recognition from sheet music publicly available on the MuseScore forum. We then propose two novel formulations for end-to-end polyphonic OMR\u2014one treating the problem as a type of multi-task binary classification, and the other treating it as multi-sequence detection. Building upon the encoderdecoder architecture and an image encoder proposed in past work on end-to-end OMR, we propose two novel decoder models\u2014FlagDecoder and RNNDecoder\u2014that correspond to the two formulations. Finally, we compare the empirical performance of these end-to-end approaches to polyphonic OMR and observe a new state-of-the-art performance with our multi-sequence detection decoder, RNNDecoder.", "year": 2021, "ssId": "eaa224ae5c969180503dda4972ab86d3a71c888c", "arXivId": "2108.01769", "link": "https://arxiv.org/pdf/2108.01769.pdf", "openAccess": true, "authors": ["Sachinda Edirisooriya", "Hao-Wen Dong", "Julian McAuley", "Taylor Berg-Kirkpatrick"]}}
{"id": "ff3b83ef0a153ed376556057269f3a61da3a103a", "content": {"title": "Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music", "abstract": "Modern keyboards allow a musician to play multiple instruments at the same time by assigning zones\u2014fixed pitch ranges of the keyboard\u2014to different instruments. In this paper, we aim to further extend this idea and examine the feasibility of automatic instrumentation\u2014dynamically assigning instruments to notes in solo music during performance. In addition to the online, real-time-capable setting for performative use cases, automatic instrumentation can also find applications in assistive composing tools in an offline setting. Due to the lack of paired data of original solo music and their full arrangements, we approach automatic instrumentation by learning to separate parts (e.g., voices, instruments and tracks) from their mixture in symbolic multitrack music, assuming that the mixture is to be played on a keyboard. We frame the task of part separation as a sequential multi-class classification problem and adopt machine learning to map sequences of notes into sequences of part labels. To examine the effectiveness of our proposed models, we conduct a comprehensive empirical evaluation over four diverse datasets of different genres and ensembles\u2014Bach chorales, string quartets, game music and pop music. Our experiments show that the proposed models outperform various baselines. We also demonstrate the potential for our proposed models to produce alternative convincing instrumentations for an existing arrangement by separating its mixture into parts. All source code and audio samples can be found at https://salu133445.github.io/arranger/.", "year": 2021, "ssId": "ff3b83ef0a153ed376556057269f3a61da3a103a", "arXivId": "2107.05916", "link": "https://arxiv.org/pdf/2107.05916.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Chris Donahue", "Taylor Berg-Kirkpatrick", "Julian McAuley"]}}
{"id": "eec490a41bdc716fccf98f4a7996c1d31334985a", "content": {"title": "MusPy: A Toolkit for Symbolic Music Generation", "abstract": "In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at this https URL .", "year": 2020, "ssId": "eec490a41bdc716fccf98f4a7996c1d31334985a", "arXivId": "2008.01951", "link": "https://arxiv.org/pdf/2008.01951.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "K. Chen", "Julian McAuley", "Taylor Berg-Kirkpatrick"]}}
{"id": "3c40fc36217a56aafb0abc735ff7d132b17e83a0", "content": {"title": "Automatic melody harmonization with triad chords: A comparative study", "abstract": "The task of automatic melody harmonization aims to build a model that generates a chord sequence as the harmonic accompaniment of a given multiple-bar melody sequence. In this paper, we present a comparative study evaluating the performance of canonical approaches to this task, including template matching, hidden Markov model, genetic algorithm and deep learning. The evaluation is conducted on a dataset of 9226 melody/chord pairs, considering 48 different triad chords. We report the result of an objective evaluation using six different metrics and a subjective study with 202 participants, showing that a deep learning method performs the best.", "year": 2020, "ssId": "3c40fc36217a56aafb0abc735ff7d132b17e83a0", "arXivId": "2001.02360", "link": "https://arxiv.org/pdf/2001.02360.pdf", "openAccess": true, "authors": ["Yin-Cheng Yeh", "Wen-Yi Hsiao", "Satoru Fukayama", "Tetsuro Kitahara", "Benjamin Genchel", "Hao-Min Liu", "Hao-Wen Dong", "Yian Chen", "T. Leong", "Yi-Hsuan Yang"]}}
{"id": "b778a7c4001898a1c3888577154d747522f16db4", "content": {"title": "Towards a Deeper Understanding of Adversarial Losses", "abstract": "Recent work has proposed various adversarial losses for training generative adversarial networks. Yet, it remains unclear what certain types of functions are valid adversarial loss functions, and how these loss functions perform against one another. In this paper, we aim to gain a deeper understanding of adversarial losses by decoupling the effects of their component functions and regularization terms. We first derive some necessary and sufficient conditions of the component functions such that the adversarial loss is a divergence-like measure between the data and the model distributions. In order to systematically compare different adversarial losses, we then propose DANTest, a new, simple framework based on discriminative adversarial networks. With this framework, we evaluate an extensive set of adversarial losses by combining different component functions and regularization approaches. This study leads to some new insights into the adversarial losses. For reproducibility, all source code is available at this https URL .", "year": 2019, "ssId": "b778a7c4001898a1c3888577154d747522f16db4", "arXivId": "1901.08753", "link": "https://arxiv.org/pdf/1901.08753.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Yi-Hsuan Yang"]}}
{"id": "9e172f35b2b0ebcff090f01d40e61fa5aecefa68", "content": {"title": "Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting.", "abstract": "Recent work has proposed various adversarial loss functions for training either generative or discriminative models. Yet, it remains unclear what certain types of functions are valid adversarial losses, and how these loss functions perform against one another. In this paper, we aim to gain a deeper understanding of adversarial losses by decoupling the effects of their component functions and regularization terms. We first derive in theory some necessary and sufficient conditions of the component functions such that the adversarial loss is a divergence-like measure between the data and the model distributions. In order to systematically compare different adversarial losses, we then propose a new, simple comparative framework, dubbed DANTest, based on discriminative adversarial networks (DANs). With this framework, we evaluate an extensive set of adversarial losses by combining different component functions and regularization approaches. Our theoretical and empirical results can together serve as a reference for choosing or designing adversarial training objectives in future research.", "year": 2019, "ssId": "9e172f35b2b0ebcff090f01d40e61fa5aecefa68", "arXivId": null, "link": null, "openAccess": false, "authors": ["Hao-Wen Dong", "Yi-Hsuan Yang"]}}
{"id": "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9", "content": {"title": "Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation", "abstract": "It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at this https URL .", "year": 2018, "ssId": "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9", "arXivId": "1804.09399", "link": "https://arxiv.org/pdf/1804.09399.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Yi-Hsuan Yang"]}}
{"id": "63cd8df0041638b0aa74834a81f99ff136951ff1", "content": {"title": "Training Generative Adversarial Networks with Binary Neurons by End-to-end Backpropagation", "abstract": "We propose the BinaryGAN, a novel generative adversarial network (GAN) that uses binary neurons at the output layer of the generator. We employ the sigmoid-adjusted straight-through estimators to estimate the gradients for the binary neurons and train the whole network by end-to-end backpropogation. The proposed model is able to directly generate binary-valued predictions at test time. We implement such a model to generate binarized MNIST digits and experimentally compare the performance for different types of binary neurons, GAN objectives and network architectures. Although the results are still preliminary, we show that it is possible to train a GAN that has binary neurons and that the use of gradient estimators can be a promising direction for modeling discrete distributions with GANs. For reproducibility, the source code is available at this https URL .", "year": 2018, "ssId": "63cd8df0041638b0aa74834a81f99ff136951ff1", "arXivId": "1810.04714", "link": "https://arxiv.org/pdf/1810.04714.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Yi-Hsuan Yang"]}}
{"id": "70bc4dc0bc72816773006c71b56fa5885c729caa", "content": {"title": "MUSEGAN : DEMONSTRATION OF A CONVOLUTIONAL GAN BASED MODEL FOR GENERATING MULTI-TRACK PIANO-ROLLS", "abstract": "Generating realistic and aesthetic pieces is one of the most exciting tasks in the field. We present in this demo paper a new neural music generation model we recently proposed, called MuseGAN. We exploit the potential of applying generative adversarial networks (GANs) to generate multi-track pop/rock music of four bars, using convolutions in both the generators and the discriminators. Moreover, we propose an efficient approach for pre-processing symbolic data and share the data with the community. Our model can generate music either from scratch, or by following (accompanying) a track given by user.", "year": 2017, "ssId": "70bc4dc0bc72816773006c71b56fa5885c729caa", "arXivId": null, "link": null, "openAccess": false, "authors": ["Hao-Wen Dong", "Wen-Yi Hsiao", "Li-Chia Yang", "Yi-Hsuan Yang"]}}
{"id": "f83ef3250ba1166d7c1c7585da7dd78e0641fae7", "content": {"title": "MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment", "abstract": "Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at this https URL .", "year": 2017, "ssId": "f83ef3250ba1166d7c1c7585da7dd78e0641fae7", "arXivId": "1709.06298", "link": "https://arxiv.org/pdf/1709.06298.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Wen-Yi Hsiao", "Li-Chia Yang", "Yi-Hsuan Yang"]}}
{"id": "4275d4c4bd10742b321467f175f16198ed7d17d7", "content": {"title": "MuseGAN: Symbolic-domain Music Generation and Accompaniment with Multi-track Sequential Generative Adversarial Networks", "abstract": "Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks, with close interaction with one another. Each track has its own temporal dynamics, but collectively they unfold over time interdependently. Lastly, for symbolic domain music generation, the targeted output is sequences of discrete musical events, not continuous values. In this paper, we propose and study three generative adversarial networks (GANs) for symbolic-domain multi-track music generation, using a data set of 127,731 MIDI bars of pop/rock music. The three models, which differ in the underlying model assumption and accordingly the network architecture, are referred to as the jamming model, composer model, and hybrid model, respectively. We propose a few intra-track and inter-track objective metrics to examine and compare their generation result, in addition to a subjective evaluation. We show that our models can learn from the noisy MIDI files and generate coherent music of four bars right from scratch (i.e. without human inputs). We also propose extensions of our models to facilitate human-AI cooperative music creation: given the piano track composed by human we can generate four additional tracks in return to accompany it.", "year": 2017, "ssId": "4275d4c4bd10742b321467f175f16198ed7d17d7", "arXivId": null, "link": null, "openAccess": false, "authors": ["Hao-Wen Dong", "Wen-Yi Hsiao", "Li-Chia Yang", "Yi-Hsuan Yang"]}}
