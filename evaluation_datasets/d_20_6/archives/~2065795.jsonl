{"id": "a7b6802f20c399615dbac161678cd6a6d2df5a97", "content": {"title": "Challenges in Data Production for AI with Human-in-the-Loop", "abstract": "Today, successful Artificial Intelligence applications rely on three pillars: machine learning algorithms, hardware for running them, and data for training and evaluating models. Although algorithms and hardware have already become commodities, obtaining up-to-date and high-quality data at scale is still challenging-but possible by building hybrid human-computer pipelines called human-in-the-loop. This talk will show how to make a significant business impact using human-in-the-loop pipelines that combine machine learning with crowdsourcing. We will share the experience of one of the world's largest search engines, Yandex. After a brief introduction to human-in-the-loop, we will describe two insightful case studies with a significant business impact at Yandex. First, we will show how to use human-in-the-loop with subjective human opinions to gather training data for learning-to-rank models in the online setting, crucial for the recommendation, e-commerce, and search applications. Second, we will show how human-in-the-loop combined with spatial crowdsourcing enables keeping information on brick-and-mortar businesses up-to-date and transformed into structured data, essential for social impactful applications like online maps and directories. Then, we will present the practical challenges of deploying human-in-the-loop pipelines, focusing on common issues with task design and quality control. We will demonstrate the end-to-end task design techniques that better fit for open-ended and subjective questions compared to widely-used classification tasks. We will present our recent advances in this field, including the use of large-scale language models (like BART and T5) for sequence aggregation. Also, we will show the new evaluation datasets for textual and subjective annotation, which are publicly available at https://toloka.ai/datasets. We will discuss the problem of reliable quality control in crowdsourcing by describing the relevant computational methods for aggregation, quality estimation, and model selection. Finally, we will demonstrate Crowd-Kit, an open-source library that offers battle-tested and platform-agnostic implementations of all the above-described methods in Python: https://github.com/Toloka/crowd-kit. Overall, we will share our experience in running impactful human-in-the-loop pipelines in production while overcoming the common practical challenges using the available and reliable open-source technologies, datasets, and tools.", "year": 2022, "ssId": "a7b6802f20c399615dbac161678cd6a6d2df5a97", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dmitry Ustalov"]}}
{"id": "1ccf412212873ae1b020762b8b86291e1fb11f65", "content": {"title": "CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription", "abstract": "Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CROWDSPEECH \u2014 the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VOXDIY \u2014 a counterpart of CROWDSPEECH for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.1", "year": 2021, "ssId": "1ccf412212873ae1b020762b8b86291e1fb11f65", "arXivId": "2107.01091", "link": "https://arxiv.org/pdf/2107.01091.pdf", "openAccess": true, "authors": ["Nikita Pavlichenko", "Ivan Stelmakh", "Dmitry Ustalov"]}}
{"id": "60e339d25d43c026cf96395aa8accf34eae744a5", "content": {"title": "IMDB-WIKI-SbS: An Evaluation Dataset for Crowdsourced Pairwise Comparisons", "abstract": "Today, comprehensive evaluation of large-scale machine learning models is possible thanks to the open datasets produced using crowdsourcing, such as SQuAD, MS COCO, ImageNet, SuperGLUE, etc. These datasets capture objective responses, assuming the single correct answer, which does not allow to capture the subjective human perception. In turn, pairwise comparison tasks, in which one has to choose between only two options, allow taking peoples\u2019 preferences into account for very challenging artificial intelligence tasks, such as information retrieval and recommender system evaluation. Unfortunately, the available datasets are either small or proprietary, slowing down progress in gathering better feedback from human users. In this paper, we present IMDB-WIKI-SbS, a new large-scale dataset for evaluating pairwise comparisons.1 It contains 9,150 images appearing in 250,249 pairs annotated on a crowdsourcing platform. Our dataset has balanced distributions of age and gender using the well-known IMDB-WIKI dataset as ground truth. We describe how our dataset is built and then compare several baseline methods, indicating its suitability for model evaluation.", "year": 2021, "ssId": "60e339d25d43c026cf96395aa8accf34eae744a5", "arXivId": "2110.14990", "link": "https://arxiv.org/pdf/2110.14990.pdf", "openAccess": true, "authors": ["Nikita Pavlichenko", "Dmitry Ustalov"]}}
{"id": "48e32ba9a891f36183a26f35316e8906d14d83c0", "content": {"title": "A General-Purpose Crowdsourcing Computational Quality Control Toolkit for Python", "abstract": "Quality control is a crux of crowdsourcing. While most means for quality control are organizational and imply worker selection, golden tasks, and post-acceptance, computational quality control techniques allow parameterizing the whole crowdsourcing process of workers, tasks, and labels, inferring and revealing relationships between them. In this paper, we demonstrate Crowd-Kit, a general-purpose crowdsourcing computational quality control toolkit. It provides efficient implementations in Python of computational quality control algorithms for crowdsourcing, including uncertainty measures and crowd consensus methods. We focus on aggregation methods for all the major annotation tasks, from the categorical annotation in which latent label assumption is met to more complex tasks like image and sequence aggregation. We perform an extensive evaluation of our toolkit on several datasets of different nature, enabling benchmarking computational quality control methods in a uniform, systematic, and reproducible way using the same codebase. We release our code and data under an open-source license at https://github.com/Toloka/crowd-kit.", "year": 2021, "ssId": "48e32ba9a891f36183a26f35316e8906d14d83c0", "arXivId": "2109.08584", "link": "https://arxiv.org/pdf/2109.08584.pdf", "openAccess": true, "authors": ["Dmitry Ustalov", "Nikita Pavlichenko", "V. Losev", "Evgeny Tulin", "Iulian Giliazev"]}}
{"id": "251a80dd4126fed3d6ae64f00dc24479f0ba5662", "content": {"title": "Summary of Tutorials at The Web Conference 2021", "abstract": "This report summarizes the 23 tutorials hosted at The Web Conference 2021: nine lecture-style tutorials and 14 hands-on tutorials.", "year": 2021, "ssId": "251a80dd4126fed3d6ae64f00dc24479f0ba5662", "arXivId": null, "link": null, "openAccess": false, "authors": ["Robert West", "Smriti Bhagat", "Paul Groth", "M. Zitnik", "Francisco M. Couto", "Pasquale Lisena", "Albert Mero\u00f1o-Pe\u00f1uela", "Xiangyu Zhao", "Wenqi Fan", "Dawei Yin", "Jiliang Tang", "Linjun Shou", "Ming Gong", "J. Pei", "Xiubo Geng", "Xingjie Zhou", "Daxin Jiang", "B. Ricaud", "Nicolas Aspert", "Volodymyr Miz", "J. Dy", "Stratis Ioannidis", "Ilkay Yildiz", "R. Rezapour", "Samin Aref", "Ly Dinh", "Jana Diesner", "Alexey Drutsa", "Dmitry Ustalov", "N. Popov", "Daria Baidakova", "Shubhanshu Mishra", "Arjun Gopalan", "Da-Cheng Juan", "Cesar Ilharco Magalhaes", "Chun-Sung Ferng", "Allan Heydon", "Chun-Ta Lu", "Philip Pham", "George Yu", "Yicheng Fan", "Yueqi Wang", "Florian Laurent", "Yanick Schraner", "C. Scheller", "S. Mohanty", "Jiawei Chen", "Xiang Wang", "Fuli Feng", "Xiangnan He", "Irene Teinemaa", "Javier Albert", "Dmitri Goldenberg", "Flavian Vasile", "David Rohde", "Olivier Jeunen", "Amine Benhalloum", "Otmane Sakhi", "Yu Rong", "Wenbing Huang", "Tingyang Xu", "Yatao Bian", "Hongying Cheng", "Fuchun Sun", "Junzhou Huang", "Shobeir Fakhraei", "C. Faloutsos", "Onur \u00c7elebi", "Martin M\u00fcller", "Manuel Schneider", "Olesia Altunina", "Wolfram Wingerath", "Benjamin Wollmer", "Felix Gessert", "Stephan Succo", "N. Ritter", "Evann Courdier", "Tudor Mihai Avram", "Dragan Cvetinovic", "L. Tsinadze", "Johny Jose", "R. Howell", "Mario Koenig", "M. Defferrard", "K. Kenthapadi", "Ben Packer", "M. Sameki", "Nashlie H. Sephus"]}}
{"id": "61a07d1e4eaa831152e253b96b91808ef3a184b4", "content": {"title": "Crowdsourcing Natural Language Data at Scale: A Hands-On Tutorial", "abstract": "In this tutorial, we present a portion of unique industry experience in efficient natural language data annotation via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practical session, where participants address a real-world language resource production task, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session and we will present useful quality control techniques and provide the attendees with an opportunity to discuss their own annotation ideas.", "year": 2021, "ssId": "61a07d1e4eaa831152e253b96b91808ef3a184b4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Alexey Drutsa", "Dmitry Ustalov", "Valentina Fedorova", "Olga Megorskaya", "Daria Baidakova"]}}
{"id": "2ec99c834bd67ac64ec04b426e5f9fd04f639024", "content": {"title": "Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription", "abstract": "Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. Crowdsourcing has become one of the standard tools for cheap and time-efficient data collection for simple problems such as image classification: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing advanced aggregation methods is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CROWDSPEECH \u2014 the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of collecting high-quality datasets using crowdsourcing: we develop a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VOXDIY \u2014 a counterpart of CROWDSPEECH for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.1", "year": 2021, "ssId": "2ec99c834bd67ac64ec04b426e5f9fd04f639024", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nikita Pavlichenko", "I. Stelmakh", "Dmitry Ustalov"]}}
{"id": "464a75c05a5ce709fc515a2577b43acc8e3d45ce", "content": {"title": "TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration", "abstract": "The Shared Task on Multi-Hop Inference for Explanation Regeneration asks participants to compose large multi-hop explanations to questions by assembling large chains of facts from a supporting knowledge base. While previous editions of this shared task aimed to evaluate explanatory completeness \u2013 finding a set of facts that form a complete inference chain, without gaps, to arrive from question to correct answer, this 2021 instantiation concentrates on the subtask of determining relevance in large multi-hop explanations. To this end, this edition of the shared task makes use of a large set of approximately 250k manual explanatory relevancy ratings that augment the 2020 shared task data. In this summary paper, we describe the details of the explanation regeneration task, the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of participating systems, evaluating various aspects involved in the multi-hop inference process. The best performing system achieved an NDCG of 0.82 on this challenging task, substantially increasing performance over baseline methods by 32%, while also leaving significant room for future improvement.", "year": 2021, "ssId": "464a75c05a5ce709fc515a2577b43acc8e3d45ce", "arXivId": null, "link": null, "openAccess": false, "authors": ["Peter Alexander Jansen", "Mokanarangan Thayaparan", "Marco Valentino", "Dmitry Ustalov"]}}
{"id": "b0d644277933988c00b22d8ae012512fe498ad62", "content": {"title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "abstract": "Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this task. However, (i) the inherent Zipfian distribution of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these languages. Models and system are available online.", "year": 2020, "ssId": "b0d644277933988c00b22d8ae012512fe498ad62", "arXivId": "2003.06651", "link": "https://arxiv.org/pdf/2003.06651.pdf", "openAccess": true, "authors": ["V. Logacheva", "Denis Teslenko", "Artem Shelmanov", "Steffen Remus", "Dmitry Ustalov", "Andrey Kutuzov", "E. Artemova", "Christian Biemann", "Simone Paolo Ponzetto", "Alexander Panchenko"]}}
{"id": "c143d2b09bdfc0dff784dce2668fd5657806dbf2", "content": {"title": "TextGraphs 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration", "abstract": "The 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating large detailed multi-fact explanations for standardized science exam questions. Given a question, correct answer, and knowledge base, models must rank each fact in the knowledge base such that facts most likely to appear in the explanation are ranked highest. Explanations consist of an average of 6 (and as many as 16) facts that span both core scientific knowledge and world knowledge, and form an explicit lexically-connected \u201cexplanation graph\u201d describing how the facts interrelate. In this second iteration of the explanation regeneration shared task, participants are supplied with more than double the training and evaluation data of the first shared task, as well as a knowledge base nearly double in size, both of which expand into more challenging scientific topics that increase the difficulty of the task. In total 10 teams participated, and 5 teams submitted system description papers. The best-performing teams significantly increased state-of-the-art performance both in terms of ranking (mean average precision) and inference speed on this challenge task.", "year": 2020, "ssId": "c143d2b09bdfc0dff784dce2668fd5657806dbf2", "arXivId": null, "link": null, "openAccess": false, "authors": ["Peter Alexander Jansen", "Dmitry Ustalov"]}}
{"id": "4d10d7c02ce01d71f11c296b09b389c6f20b354b", "content": {"title": "Crowdsourcing Practice for Efficient Data Labeling: Aggregation, Incremental Relabeling, and Pricing", "abstract": "In this tutorial, we present a portion of unique industry experience in efficient data labeling via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practice session, where participants will choose one of the real label collection tasks, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session. While the crowd performers are annotating the project set up by the attendees, we will present the major theoretical results in efficient aggregation, incremental relabeling, and dynamic pricing. We will also discuss their strengths and weaknesses as well as applicability to real-world tasks, summarizing our five year-long research and industrial expertise in crowdsourcing. Finally, participants will receive a feedback about their projects and practical advice on how to make them more efficient. We invite beginners, advanced specialists, and researchers to learn how to collect high quality labeled data and do it efficiently.", "year": 2020, "ssId": "4d10d7c02ce01d71f11c296b09b389c6f20b354b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Alexey Drutsa", "Dmitry Ustalov", "Evfrosiniya Zerminova", "Valentina Fedorova", "Olga Megorskaya", "Daria Baidakova"]}}
{"id": "14a6452d6d026a3f384e425add6ab68f8e65037f", "content": {"title": "Practice of Efficient Data Collection via Crowdsourcing: Aggregation, Incremental Relabelling, and Pricing", "abstract": "In this tutorial, we present a portion of unique industry experience in efficient data labelling via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labelling via public crowdsourcing marketplaces and will present key components of efficient label collection. This will be followed by a practice session, where participants will choose one of the real label collection tasks, experiment with selecting settings for the labelling process, and launch their label collection project on Yandex.Toloka, one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session. Finally, participants will receive a feedback about their projects and practical advice to make them more efficient. We expect that our tutorial will address an audience with a wide range of background and interests. We do not require specific prerequisite knowledge or skills. We invite beginners, advanced specialists, and researchers to learn how to efficiently collect labelled data.", "year": 2020, "ssId": "14a6452d6d026a3f384e425add6ab68f8e65037f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Alexey Drutsa", "Valentina Fedorova", "Dmitry Ustalov", "Olga Megorskaya", "Evfrosiniya Zerminova", "Daria Baidakova"]}}
{"id": "e63c9eb5b623baad0a7805e839e5d9fabad37fce", "content": {"title": "TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration", "abstract": "While automated question answering systems are increasingly able to retrieve answers to natural language questions, their ability to generate detailed human-readable explanations for their answers is still quite limited. The Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating detailed gold explanations for standardized elementary science exam questions by selecting facts from a knowledge base of semi-structured tables. Each explanation contains between 1 and 16 interconnected facts that form an \u201cexplanation graph\u201d spanning core scientific knowledge and detailed world knowledge. It is expected that successfully combining these facts to generate detailed explanations will require advancing methods in multi-hop inference and information combination, and will make use of the supervised training data provided by the WorldTree explanation corpus. The top-performing system achieved a mean average precision (MAP) of 0.56, substantially advancing the state-of-the-art over a baseline information retrieval model. Detailed extended analyses of all submitted systems showed large relative improvements in accessing the most challenging multi-hop inference problems, while absolute performance remains low, highlighting the difficulty of generating detailed explanations through multi-hop reasoning.", "year": 2019, "ssId": "e63c9eb5b623baad0a7805e839e5d9fabad37fce", "arXivId": null, "link": null, "openAccess": false, "authors": ["Peter Alexander Jansen", "Dmitry Ustalov"]}}
{"id": "3e2bac2abfb5b33a43fe56db5a868e17e38c616a", "content": {"title": "The Role of Student Projects in Teaching Machine Learning and High Performance Computing", "abstract": "We describe an approach to teaching Machine Learning and High-Performance Computing classes for Master students at Ural Federal University. In addition to the theoretical classes, the students participate in the projects in collaboration with the partner companies and research laboratories of the university and institutes of the Russian Academy of Sciences. The partners provide not only project topics, but also the experienced mentors to assist the students in project work. We discuss the structure of the Project Workshop class that was designed to include the project-based learning into the curriculum. As a result, during the Master studies, the students not only learn the theoretical basis but also gain experience solving real-world problems, which has a positive effect on employment.", "year": 2019, "ssId": "3e2bac2abfb5b33a43fe56db5a868e17e38c616a", "arXivId": null, "link": null, "openAccess": false, "authors": ["A. Sozykin", "A. Koshelev", "Dmitry Ustalov"]}}
{"id": "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c", "content": {"title": "HHMM at SemEval-2019 Task 2: Unsupervised Frame Induction using Contextualized Word Embeddings", "abstract": "We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (Qasem-iZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages.", "year": 2019, "ssId": "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c", "arXivId": "1905.01739", "link": "https://arxiv.org/pdf/1905.01739.pdf", "openAccess": true, "authors": ["S. Anwar", "Dmitry Ustalov", "N. Arefyev", "Simone Paolo Ponzetto", "Chris Biemann", "Alexander Panchenko"]}}
{"id": "7f85b7ee0fc6cdb5b92417035a7049247729545a", "content": {"title": "Artificial Intelligence and Natural Language: 8th Conference, AINL 2019, Tartu, Estonia, November 20\u201322, 2019, Proceedings", "abstract": "This study discusses the effects of training data size and class imbalance on the performance of classifiers. An empirical study was performed on nine classifiers with twenty benchmark datasets. First, two groups of datasets (those with few variables and those with numerous variables) were prepared. Then we progressively increased the class imbalance of each dataset in each group by undersampling both classes so that we could clarify to what extent the predictive power of each classifier was adversely affected. Kappa coefficient (kappa) was chosen as the performance metric, and nemenyi post hoc test was used to find significant differences between classifiers. Additionally, the ranks of nine classifiers in different conditions were discussed. The results indicated that (1) Na\u00efve bayes, logistic regression and logit leaf model are less susceptible to class imbalance; (2) It was assumed that using datasets with balanced class distribution and sufficient instances would be the ideal condition to maximize the performance of classifiers; (3) Increasing the number of instances is more effective than using variables for improving the predictive performance of Random Forest. Furthermore, our experiment clarified the optimal classifiers for four types of datasets.", "year": 2019, "ssId": "7f85b7ee0fc6cdb5b92417035a7049247729545a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Phoebe Chen", "A. Cuzzocrea", "Xiaoyong Du", "Orhun Kara", "Ting Liu", "K. Sivalingam", "D. \u015al\u0119zak", "T. Washio", "Xiaokang Yang", "Junsong Yuan", "Simone Diniz Junqueira Barbosa", "Dmitry Ustalov", "A. Filchenkov", "Lidia Pivovarova"]}}
{"id": "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df", "content": {"title": "Datasets for Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction", "abstract": "This dataset supplements the article \u201cWatset: Local-Global Graph Clustering with Applications in Sense and Frame Induction\u201d published in the Computational Linguistics journal: watset-coli-lcc-performance.tsv : runtime analysis watset-coli-synsets.zip : synset induction experiment (note that pairwise-{en-babelnet,ru-rwn}.pkl files are excluded due to the licensing issues) watset-coli-triframes.zip : semantic frame induction experiment watset-coli-classes.zip : semantic class induction experiment", "year": 2019, "ssId": "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dmitry Ustalov", "Alexander Panchenko", "Chris Biemann", "Simone Paolo Ponzetto"]}}
{"id": "be8d6a8d3dfe87a4d9171f25bf9a18d502498756", "content": {"title": "Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction", "abstract": "We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph, which reflects the \u201cambiguity\u201d of its nodes. Then, it uses hard clustering to discover clusters in this \u201cdisambiguated\u201d intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can also be applied to other networks of linguistic data.", "year": 2019, "ssId": "be8d6a8d3dfe87a4d9171f25bf9a18d502498756", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dmitry Ustalov", "Alexander Panchenko", "Chris Biemann", "Simone Paolo Ponzetto"]}}
{"id": "10e88416035a8a3cbef0e65f8967df650abd0a00", "content": {"title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages", "abstract": "In this paper, we present Watasense, an unsupervised system for word sense disambiguation. Given a sentence, the system chooses the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index.", "year": 2018, "ssId": "10e88416035a8a3cbef0e65f8967df650abd0a00", "arXivId": "1804.10686", "link": "https://arxiv.org/pdf/1804.10686.pdf", "openAccess": true, "authors": ["Dmitry Ustalov", "Denis Teslenko", "Alexander Panchenko", "M. Chernoskutov", "Chris Biemann", "Simone Paolo Ponzetto"]}}
{"id": "eb28e82ca0bbc5d83e1cc07807da16874105d2fa", "content": {"title": "Unsupervised Semantic Frame Induction using Triclustering : Supplementary Materials", "abstract": "We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. This document contains supplementary materials to the main paper. 1 Triple Vector Representation Figure 1 illustrates our approach for triple vector representation. In our representation, given a syntactic subject-verb-object (SVO) triple (people,make,money), we concatenate the word embeddings corresponding to these words into a single vector representing the whole triple. This explains the core assumption underlying in the Triframes approach: triples representing similar roles appear in similar contexts. 2 Implementation Details We use a parallel implementation of the WATSET1 algorithm in Java for graph clustering, the Gensim2 library for handling word embeddings, and the Faiss3 library for indexing of word emhttps://github.com/dustalov/ watset-java https://radimrehurek.com/gensim/ https://github.com/facebookresearch/ faiss Figure 1: Concatenation of the vectors corresponding to the SVO triple elements expresses structural similarity of the triples. Method # of clusters Triframes WATSET 37,535 HOSG 10,000 NOAC 46,984 Triadic Spectral 500 Triadic k-means 500 Triframes CW 1862 LDA-Frames 109 Singletons 648,432 Whole 1 Table 1: Number of induced frames. beddings and retrieval of nearest neighbors. The source code and the data presented in this paper are available online under a permissive license.4 3 Cluster Sizes Table 1 shows the amount of clusters produced by clustering algorithms during the frame induction experiment. Note that the Singletons baseline produced a distinct cluster for each triple and yet received low scores on each scale. https://github.com/uhh-lt/triframes 4 Examples of Induced Frames Figures 2, 3 and 4 demonstrate examples of \u201cgood\u201d frames, i.e. those which are semantically plausible according to our human judgment during a post-hoc manual analysis of clustering results. Figures 5, 6 and 7 show examples of \u201cbad\u201d frames according to the same criteria. All the frames are produced by the Triframes WATSET method ranked best as according to the Frame F1 in the frame induction experiment. In particular, the number of nearest neighbors is n = 30, and the WATSET[CWtop, CWtop] fuzzy clustering algorithm has been used. These frames are available in the file triw2v-watset-n30-top-top-triples.txt available in the \u201cDownloads\u201d section of our GitHub repository (cf. Section 2). Frame # 848 Subjects: Company, firm, company Verbs: buy, supply, discharge, purchase, expect Objects: book, supply, house, land, share, company, grain, which, item, product, ticket, work, this, equipment, House, it, film, water, something, she, what, service, plant, time Figure 2: An example of a \u201cgood\u201d frame. Frame # 849 Subjects: student, scientist, we, pupil, member, company, man, nobody, you, they, US, group, it, people, Man, user, he Verbs: do, test, perform, execute, conduct Objects: experiment, test Figure 3: An example of a \u201cgood\u201d frame. Frame # 3207 Subjects: people, we, they, you Verbs: feel, seek, look, search Objects: housing, inspiration, gold, witness, partner, accommodation, Partner Figure 4: An example of a \u201cgood\u201d frame. Frame # 1 Subjects: you, she, he, return, they, we, themselves, road, help, who Verbs: govern, discourage, resemble, encumber, urge, pummel, . . . 911 more verbs . . . , demolish, swarm, anticipate, spew, derail, emit, snap Objects: you, pass, she, he, it, product, change, solution, total, any, wall, they, something, people, classic, this, interest, itself, flat, place, part, controversy Figure 5: An example of a \u201cbad\u201d frame. Frame # 852 Subjects: Word, glue, pill, speed, drug, pot, they, those, mine, item, resource, this, its, it, something, most, horse, material, chemical, plant, information, word Verbs: use, attach, apply, follow Objects: we, they, you, it, report, he Figure 6: An example of a \u201cbad\u201d frame. Frame # 37535 Subjects: he Verbs: phone, book Objects: you Figure 7: An example of a \u201cbad\u201d frame. Officer|chair|Committee officer|head|team mayor|lead|city officer|lead|company Mayor|lead|city boss|lead|company chairman|lead|company director|lead|department chief|lead|department president|lead|government president|lead|state director|lead|company president|lead|department officer|chair|committee Chief|lead|department chairman|lead|committee Director|lead|Department Director|lead|department Director|lead|agency Director|lead|company minister|lead|team Director|head|team director|head|team Chairman|lead|company Chairman|lead|Committee President|lead|company Director|chair|Committee President|lead|party President|head|team leader|head|team Director|chair|committee director|chair|committee Director|head|Department president|head|team director|head|department director|head|agency director|head|committee Chairman|run|committee Chairman|chair|Committee Chairman|chair|committee President|chair|Committee President|chair|committee Governor|lead|state chairman|head|committee chairman|run|committee president|chair|committee president|head|committee president|chair|Committee Minister|chair|committee representative|chair|committee representative|head|committee General|command|department General|command|Department General|head|Department General|head|department officer|head|department minister|head|department leader|head|agency leader|head|party leader|head|committee leader|head|department minister|head|committee King|run|company leader|head|government Minister|head|government president|head|government Figure 8: Visualization of an SVO triple graph, where edges represent distributional relatedness of the triples estimated using word embeddings. 5 Visualization of Triple Graph Figure 8 presents a densly connected part of the triple graph related to the concept of \u201cleadership\u201d. A similar cluster of triples can represent a semantic frame induced automatically from text using our approach.", "year": 2018, "ssId": "eb28e82ca0bbc5d83e1cc07807da16874105d2fa", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dmitry Ustalov", "Alexander Panchenko", "A. Kutuzov", "Chris Biemann", "Simone Paolo Ponzetto"]}}
