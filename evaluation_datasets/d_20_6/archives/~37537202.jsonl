{"id": "e28b9bc26f5f7eb3b0532d823713400202372da2", "content": {"title": "Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms", "abstract": "The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation. We adopt this viewpoint and model the actor and critic interaction as a two-player general-sum game with a leader-follower structure known as a Stackelberg game. Given this abstraction, we propose a meta-framework for Stackelberg actor-critic algorithms where the leader player follows the total derivative of its objective instead of the usual individual gradient. From a theoretical standpoint, we develop a policy gradient theorem for the refined update and provide a local convergence guarantee for the Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an empirical standpoint, we demonstrate via simple examples that the learning dynamics we study mitigate cycling and accelerate convergence compared to the usual gradient dynamics given cost structures induced by actor-critic formulations. Finally, experiments on OpenAI gym environments show that Stackelberg actor-critic algorithms always perform at least as well and often significantly outperform the standard actor-critic algorithm counterparts.", "year": 2021, "ssId": "e28b9bc26f5f7eb3b0532d823713400202372da2", "arXivId": "2109.12286", "link": "https://arxiv.org/pdf/2109.12286.pdf", "openAccess": true, "authors": ["Liyuan Zheng", "Tanner Fiez", "Zane Alumbaugh", "Benjamin J. Chasnov", "L. Ratliff"]}}
{"id": "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130", "content": {"title": "Data-Dependent Regret Bounds", "abstract": "In this scribe note, we provide an overview of recent work deriving data-dependent bounds for adversarial bandits. This line of work asks the question of whether improved regret bounds can be obtained when the loss sequence is not completely adversarial. In other words, while classical algorithms such as Exp3 obtain worst-case optimal regret of \u00d5( \u221a KT ) where K is the number of actions and T is the time-horizon, this line of work explores algorithms that can adapt to easier loss sequences and provides regret bounds in terms of meaningful characteristics of the loss sequence. Deriving results of this nature requires novel algorithms and analysis methods. In particular, variants of optimistic mirror descent framework have emerged as the primary algorithmic method that can naturally adapt to easy data with careful choices of the parameters and the regularization function. We give a high level overview of these methods and highlight main results.", "year": 2021, "ssId": "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130", "arXivId": null, "link": null, "openAccess": false, "authors": ["Omid Sadeghi", "Max Gray", "Tanner Fiez"]}}
{"id": "e9d26b9f5e6b619bbb759a67560cb949a9f034ba", "content": {"title": "Global Convergence to Local Minmax Equilibrium in Classes of Nonconvex Zero-Sum Games", "abstract": "We study gradient descent-ascent learning dynamics with timescale separation (\u03c4 -GDA) in unconstrained continuous action zero-sum games where the minimizing player faces a nonconvex optimization problem and the maximizing player optimizes a Polyak-\u0141ojasiewicz (P\u0141) or strongly-concave (SC) objective. In contrast to past work on gradient-based learning in nonconvex-P\u0141/SC zero-sum games, we assess convergence in relation to natural game-theoretic equilibria instead of only notions of stationarity. In pursuit of this goal, we prove that the only locally stable points of the \u03c4 -GDA continuous-time limiting system correspond to strict local minmax equilibria in each class of games. For these classes of games, we exploit timescale separation to construct a potential function that when combined with the stability characterization and an asymptotic saddle avoidance result gives a global asymptotic almost-sure convergence guarantee for the discrete-time gradient descent-ascent update to a set of the strict local minmax equilibrium. Moreover, we provide convergence rates for the gradient descent-ascent dynamics with timescale separation to approximate stationary points.", "year": 2021, "ssId": "e9d26b9f5e6b619bbb759a67560cb949a9f034ba", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "L. Ratliff", "Eric V. Mazumdar", "Evan Faulkner", "Adhyyan Narang"]}}
{"id": "73271677da83a3f55523148d1b43a0501f0a35dd", "content": {"title": "Online Learning in Periodic Zero-Sum Games", "abstract": "A seminal result in game theory is von Neumann\u2019s minmax theorem, which states that zero-sum games admit an essentially unique equilibrium solution. Classical learning results build on this theorem to show that online no-regret dynamics converge to an equilibrium in a time-average sense in zero-sum games. In the past several years, a key research direction has focused on characterizing the day-to-day behavior of such dynamics. General results in this direction show that broad classes of online learning dynamics are cyclic, and formally Poincar\u00e9 recurrent, in zero-sum games. We analyze the robustness of these online learning behaviors in the case of periodic zero-sum games with a time-invariant equilibrium. This model generalizes the usual repeated game formulation while also being a realistic and natural model of a repeated competition between players that depends on exogenous environmental variations such as time-of-day effects, week-to-week trends, and seasonality. Interestingly, time-average convergence may fail even in the simplest such settings, in spite of the equilibrium being fixed. In contrast, using novel analysis methods, we show that Poincar\u00e9 recurrence provably generalizes despite the complex, non-autonomous nature of these dynamical systems.", "year": 2021, "ssId": "73271677da83a3f55523148d1b43a0501f0a35dd", "arXivId": "2111.03377", "link": "https://arxiv.org/pdf/2111.03377.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Ryann Sim", "Stratis Skoulakis", "G. Piliouras", "L. Ratliff"]}}
{"id": "dbdefb498b619912a726fec7c85533594a1c6a1b", "content": {"title": "Minimax Optimization with Smooth Algorithmic Adversaries", "abstract": "This paper considers minimax optimization minx maxy f(x, y) in the challenging setting where f can be both nonconvex in x and nonconcave in y. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, many fundamental issues remain in theory, such as the absence of efficiently computable optimality notions, and cyclic or diverging behavior of existing algorithms. Our framework sprouts from the practical consideration that under a computational budget, the max-player can not fully maximize f(x, \u00b7) since nonconcave maximization is NP-hard in general. So, we propose a new algorithm for the min-player to play against smooth algorithms deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles), and to find an appropriate \u201cstationary point\u201d in a polynomial number of iterations. Our framework covers practical settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further provide complementing experiments that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice.", "year": 2021, "ssId": "dbdefb498b619912a726fec7c85533594a1c6a1b", "arXivId": "2106.01488", "link": "https://arxiv.org/pdf/2106.01488.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Chi Jin", "Praneeth Netrapalli", "L. Ratliff"]}}
{"id": "ab42ad9698386cc15a30a8c7885fa82b260f537b", "content": {"title": "Gaussian Mixture Models for Parking Demand Data", "abstract": "To mitigate congestion caused by drivers cruising in search of parking, performance-based pricing schemes have received a significant amount of attention. However, several recent studies suggest location, time-of-day, and awareness of policies are the primary factors that drive parking decisions. Harnessing data provided by the Seattle Department of Transportation and considering the aforementioned decision-making factors, we analyze the spatial and temporal properties of curbside parking demand and propose methods that can improve traditional policies with straightforward modifications by advancing the understanding of where and when to administer pricing policies. Specifically, we develop a Gaussian mixture model based technique to identify zones with similar parking demand as quantified by spatial autocorrelation. In support of this technique, we introduce a metric based on the repeatability of our Gaussian mixture model to investigate temporal consistency.", "year": 2020, "ssId": "ab42ad9698386cc15a30a8c7885fa82b260f537b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "L. Ratliff"]}}
{"id": "568462ab0a0a59a2575b70db2cd9022572526f3f", "content": {"title": "Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study", "abstract": "Contemporary work on learning in continuous games has commonly overlooked the hierarchical decision-making structure present in machine learning problems formulated as games, instead treating them as simultaneous play games and adopting the Nash equilibrium solution concept. We deviate from this paradigm and provide a comprehensive study of learning in Stackelberg games. This work provides insights into the optimization landscape of zero-sum games by establishing connections between Nash and Stackelberg equilibria along with the limit points of simultaneous gradient descent. We derive novel gradient-based learning dynamics emulating the natural structure of a Stackelberg game using the implicit function theorem and provide convergence analysis for deterministic and stochastic updates for zero-sum and general-sum games. Notably, in zero-sum games using deterministic updates, we show the only critical points the dynamics converge to are Stackelberg equilibria and provide a local convergence rate. Empirically, our learning dynamics mitigate rotational behavior and exhibit benefits for training generative adversarial networks compared to simultaneous gradient descent.", "year": 2020, "ssId": "568462ab0a0a59a2575b70db2cd9022572526f3f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "Benjamin J. Chasnov", "L. Ratliff"]}}
{"id": "b8dcc2ae3346e41a421232169c2ca07957c654c4", "content": {"title": "Evolutionary Game Theory Squared: Evolving Agents in Evolving Games", "abstract": "The predominant paradigm in evolutionary game theory and more generally online learning in games is based on a clear distinction between a population of dynamic agents that interact given a fixed, static game. In this paper, we move away from the artificial divide between dynamic agents and static games, to introduce and analyze a large class of competitive settings where both agents and the games they play evolve strategically over time. We focus on arguably the most archetypal game-theoretic setting\u2014zero-sum (as well as network generalizations)\u2014and the most studied evolutionary learning dynamic\u2014replicator, the continuous-time analogue of multiplicative weights. Populations of agents compete against each other in a zero-sum competition that itself evolves adversarially to the current population mixture. Remarkably, despite the chaotic coevolution of agents and games, we prove that the system exhibits a number of regularities. First, the system has conservation laws of an information-theoretic flavor that couple the behavior of all agents and games. Secondly, the system is Poincar\u00e9 recurrent, with effectively all possible initializations of agents and games lying on recurrent orbits that come arbitrarily close to their initial conditions infinitely often. Thirdly, the time-average agent behavior and utility converge to the Nash equilibrium values of the time-average game. Finally, we provide a polynomial time algorithm to efficiently predict this time-average behavior for any such coevolving network game.", "year": 2020, "ssId": "b8dcc2ae3346e41a421232169c2ca07957c654c4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "G. Piliouras", "L. Ratliff"]}}
{"id": "1410f7d9470a24fb4055c6685c2dda758b9d995f", "content": {"title": "Evolutionary Game Theory Squared: Evolving Agents in Endogenously Evolving Zero-Sum Games", "abstract": "The predominant paradigm in evolutionary game theory and more generally online learning in games is based on a clear distinction between a population of dynamic agents that interact given a fixed, static game. In this paper, we move away from the artificial divide between dynamic agents and static games, to introduce and analyze a large class of competitive settings where both the agents and the games they play evolve strategically over time. We focus on arguably the most archetypal game-theoretic setting -- zero-sum games (as well as network generalizations) -- and the most studied evolutionary learning dynamic -- replicator, the continuous-time analogue of multiplicative weights. Populations of agents compete against each other in a zero-sum competition that itself evolves adversarially to the current population mixture. Remarkably, despite the chaotic coevolution of agents and games, we prove that the system exhibits a number of regularities. First, the system has conservation laws of an information-theoretic flavor that couple the behavior of all agents and games. Secondly, the system is Poincare recurrent, with effectively all possible initializations of agents and games lying on recurrent orbits that come arbitrarily close to their initial conditions infinitely often. Thirdly, the time-average agent behavior and utility converge to the Nash equilibrium values of the time-average game. Finally, we provide a polynomial time algorithm to efficiently predict this time-average behavior for any such coevolving network game.", "year": 2020, "ssId": "1410f7d9470a24fb4055c6685c2dda758b9d995f", "arXivId": "2012.08382", "link": "https://arxiv.org/pdf/2012.08382.pdf", "openAccess": true, "authors": ["Stratis Skoulakis", "Tanner Fiez", "Ryan Sim", "G. Piliouras", "L. Ratliff"]}}
{"id": "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe", "content": {"title": "Gradient Descent-Ascent Provably Converges to Strict Local Minmax Equilibria with a Finite Timescale Separation", "abstract": "We study the role that a finite timescale separation parameter $\\tau$ has on gradient descent-ascent in two-player non-convex, non-concave zero-sum games where the learning rate of player 1 is denoted by $\\gamma_1$ and the learning rate of player 2 is defined to be $\\gamma_2=\\tau\\gamma_1$. Existing work analyzing the role of timescale separation in gradient descent-ascent has primarily focused on the edge cases of players sharing a learning rate ($\\tau =1$) and the maximizing player approximately converging between each update of the minimizing player ($\\tau \\rightarrow \\infty$). For the parameter choice of $\\tau=1$, it is known that the learning dynamics are not guaranteed to converge to a game-theoretically meaningful equilibria in general. In contrast, Jin et al. (2020) showed that the stable critical points of gradient descent-ascent coincide with the set of strict local minmax equilibria as $\\tau\\rightarrow\\infty$. In this work, we bridge the gap between past work by showing there exists a finite timescale separation parameter $\\tau^{\\ast}$ such that $x^{\\ast}$ is a stable critical point of gradient descent-ascent for all $\\tau \\in (\\tau^{\\ast}, \\infty)$ if and only if it is a strict local minmax equilibrium. Moreover, we provide an explicit construction for computing $\\tau^{\\ast}$ along with corresponding convergence rates and results under deterministic and stochastic gradient feedback. The convergence results we present are complemented by a non-convergence result: given a critical point $x^{\\ast}$ that is not a strict local minmax equilibrium, then there exists a finite timescale separation $\\tau_0$ such that $x^{\\ast}$ is unstable for all $\\tau\\in (\\tau_0, \\infty)$. Finally, we empirically demonstrate on the CIFAR-10 and CelebA datasets the significant impact timescale separation has on training performance.", "year": 2020, "ssId": "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe", "arXivId": "2009.14820", "link": "https://arxiv.org/pdf/2009.14820.pdf", "openAccess": true, "authors": ["Tanner Fiez", "L. Ratliff"]}}
{"id": "cf7e8f47ad1c57738dc586109dcf28a22ab67b72", "content": {"title": "A SUPER* Algorithm to Optimize Paper Bidding in Peer Review", "abstract": "A number of applications involve sequential arrival of users, and require showing each user an ordering of items. A prime example (which forms the focus of this paper) is the bidding process in conference peer review where reviewers enter the system sequentially, each reviewer needs to be shown the list of submitted papers, and the reviewer then \"bids\" to review some papers. The order of the papers shown has a significant impact on the bids due to primacy effects. In deciding on the ordering of papers to show, there are two competing goals: (i) obtaining sufficiently many bids for each paper, and (ii) satisfying reviewers by showing them relevant items. In this paper, we begin by developing a framework to study this problem in a principled manner. We present an algorithm called SUPER*, inspired by the A* algorithm, for this goal. Theoretically, we show a local optimality guarantee of our algorithm and prove that popular baselines are considerably suboptimal. Moreover, under a community model for the similarities, we prove that SUPER* is near-optimal whereas the popular baselines are considerably suboptimal. In experiments on real data from ICLR 2018 and synthetic data, we find that SUPER* considerably outperforms baselines deployed in existing systems, consistently reducing the number of papers with fewer than requisite bids by 50-75% or more, and is also robust to various real world complexities.", "year": 2020, "ssId": "cf7e8f47ad1c57738dc586109dcf28a22ab67b72", "arXivId": "2007.07079", "link": "https://arxiv.org/pdf/2007.07079.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Nihar B. Shah", "L. Ratliff"]}}
{"id": "f78e5aaf34cc1e4874490e9155c640b73c630021", "content": {"title": "Gradient Conjectures for Strategic Multi-Agent Learning", "abstract": "We introduce a general framework for gradient-based learning that incorporates opponent behavior in continuous, general-sum games. In contrast to much of the work on learning in games, which primarily analyzes agents that myopically update a strategy under the belief opposing players repeat the last joint action, we study agents that model the dynamic behavior of opponents and optimize a surrogate cost. The surrogate cost functions embed conjectures, which anticipate the dynamic behavior of opponents. We show that agents with heterogeneous conjectures can result in a number of game-theoretic outcomes including Nash, Stackelberg, and general conjectural variations equilibrium. We review the suitability of each equilibrium concept for implicit and gradient conjectures and analyze the limiting outcomes of conjectural learning. Moreover, we demonstrate our framework generalizes a number of learning rules from recent years.", "year": 2019, "ssId": "f78e5aaf34cc1e4874490e9155c640b73c630021", "arXivId": null, "link": null, "openAccess": false, "authors": ["Benjamin J. Chasnov", "Tanner Fiez", "L. Ratliff"]}}
{"id": "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4", "content": {"title": "A Perspective on Incentive Design: Challenges and Opportunities", "abstract": "The increasingly tight coupling between humans and system operations in domains ranging from intelligent infrastructure to e-commerce has led to a challenging new class of problems founded on a well-established area of research: incentive design. There is a clear need for a new tool kit for designing mechanisms that help coordinate self-interested parties while avoiding unexpected outcomes in the face of information asymmetries, exogenous uncertainties from dynamic environments, and resource constraints. This article provides a perspective on the current state of the art in incentive design from three core communities\u2014economics, control theory, and machine learning\u2014and highlights interesting avenues for future research at the interface of these domains.", "year": 2019, "ssId": "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4", "arXivId": null, "link": null, "openAccess": false, "authors": ["L. Ratliff", "Roy Dong", "S. Sekar", "Tanner Fiez"]}}
{"id": "7ce80c7df1774e4483b32a813d54a8ff35dd0163", "content": {"title": "Convergence of Learning Dynamics in Stackelberg Games", "abstract": "This paper investigates the convergence of learning dynamics in Stackelberg games. In the class of games we consider, there is a hierarchical game being played between a leader and a follower with continuous action spaces. We establish a number of connections between the Nash and Stackelberg equilibrium concepts and characterize conditions under which attracting critical points of simultaneous gradient descent are Stackelberg equilibria in zero-sum games. Moreover, we show that the only stable critical points of the Stackelberg gradient dynamics are Stackelberg equilibria in zero-sum games. Using this insight, we develop a gradient-based update for the leader while the follower employs a best response strategy for which each stable critical point is guaranteed to be a Stackelberg equilibrium in zero-sum games. As a result, the learning rule provably converges to a Stackelberg equilibria given an initialization in the region of attraction of a stable critical point. We then consider a follower employing a gradient-play update rule instead of a best response strategy and propose a two-timescale algorithm with similar asymptotic convergence guarantees. For this algorithm, we also provide finite-time high probability bounds for local convergence to a neighborhood of a stable Stackelberg equilibrium in general-sum games. Finally, we present extensive numerical results that validate our theory, provide insights into the optimization landscape of generative adversarial networks, and demonstrate that the learning dynamics we propose can effectively train generative adversarial networks.", "year": 2019, "ssId": "7ce80c7df1774e4483b32a813d54a8ff35dd0163", "arXivId": "1906.01217", "link": "https://arxiv.org/pdf/1906.01217.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Benjamin J. Chasnov", "L. Ratliff"]}}
{"id": "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df", "content": {"title": "Sequential Experimental Design for Transductive Linear Bandits", "abstract": "In this paper we introduce the transductive linear bandit problem: given a set of measurement vectors $\\mathcal{X}\\subset \\mathbb{R}^d$, a set of items $\\mathcal{Z}\\subset \\mathbb{R}^d$, a fixed confidence $\\delta$, and an unknown vector $\\theta^{\\ast}\\in \\mathbb{R}^d$, the goal is to infer $\\text{argmax}_{z\\in \\mathcal{Z}} z^\\top\\theta^\\ast$ with probability $1-\\delta$ by making as few sequentially chosen noisy measurements of the form $x^\\top\\theta^{\\ast}$ as possible. When $\\mathcal{X}=\\mathcal{Z}$, this setting generalizes linear bandits, and when $\\mathcal{X}$ is the standard basis vectors and $\\mathcal{Z}\\subset \\{0,1\\}^d$, combinatorial bandits. Such a transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages $\\mathcal{X}$ a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages $\\mathcal{Z}$ that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books $\\mathcal{X}$ a user is queried about may be restricted to well known best-sellers even though the goal might be to recommend more esoteric titles $\\mathcal{Z}$. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we provide the first non-asymptotic algorithm for linear bandits that nearly achieves the information theoretic lower bound.", "year": 2019, "ssId": "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df", "arXivId": "1906.08399", "link": "https://arxiv.org/pdf/1906.08399.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Lalit P. Jain", "Kevin G. Jamieson", "L. Ratliff"]}}
{"id": "45f59bd3ef8e1d76474199c08c140675c04a728c", "content": {"title": "Opponent Anticipation via Conjectural Variations", "abstract": "We introduce a framework for multi-agent learning in which agents anticipate each others\u2019 reactions by forming conjectures about their learning processes, and devise learning rules using a variational perspective relative to these conjectures. The conjecture learning schemes lead to an alternative equilibrium concept, a differential general conjectural variations equilibrium. When compared to simultaneous gradient play, we empirically observe that implicit conjecture learning leads to a more equitable solution in a zero-sum polynomial game, while gradient and fast conjecture learning decrease the rotational components of the joint dynamics. The framework provides techniques for future synthesis of novel heterogeneous multi-agent learning rules.", "year": 2019, "ssId": "45f59bd3ef8e1d76474199c08c140675c04a728c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Benjamin J. Chasnov", "Tanner Fiez"]}}
{"id": "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c", "content": {"title": "Characterizing Equilibria in Stackelberg Games", "abstract": "This paper investigates the convergence of learning dynamics in Stackelberg games on continuous action spaces, a class of games distinguished by the hierarchical order of play between agents. We establish connections between the Nash and Stackelberg equilibrium concepts and characterize conditions under which attracting critical points of simultaneous gradient descent are Stackelberg equilibria in zero-sum games. Moreover, we show that the only stable critical points of the Stackelberg gradient dynamics are Stackelberg equilibria in zero-sum games. Using this insight, we develop two-timescale learning dynamics for which each stable critical point is guaranteed to be a Stackelberg equilibrium in zero-sum games and the dynamics converge to the set of stable attractors in general-sum games.", "year": 2019, "ssId": "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "Benjamin J. Chasnov"]}}
{"id": "30a6a5614727017e7d7981f87df57d17713501a0", "content": {"title": "Combinatorial Bandits for Incentivizing Agents with Dynamic Preferences", "abstract": "The design of personalized incentives or recommendations to improve user engagement is gaining prominence as digital platform providers continually emerge. We propose a multi-armed bandit framework for matching incentives to users, whose preferences are unknown a priori and evolving dynamically in time, in a resource constrained environment. We design an algorithm that combines ideas from three distinct domains: (i) a greedy matching paradigm, (ii) the upper confidence bound algorithm (UCB) for bandits, and (iii) mixing times from the theory of Markov chains. For this algorithm, we provide theoretical bounds on the regret and demonstrate its performance via both synthetic and realistic (matching supply and demand in a bike-sharing platform) examples.", "year": 2018, "ssId": "30a6a5614727017e7d7981f87df57d17713501a0", "arXivId": "1807.02297", "link": "https://arxiv.org/pdf/1807.02297.pdf", "openAccess": true, "authors": ["Tanner Fiez", "S. Sekar", "Liyuan Zheng", "L. Ratliff"]}}
{"id": "045f90129a8d7148eec4a58770bc4166b51330ca", "content": {"title": "Data Driven Spatio-Temporal Modeling of Parking Demand", "abstract": "To mitigate the congestion caused by parking, performance based pricing schemes have received a significant amount of attention. However, several recent studies suggest location, time of day, and awareness of policies are the primary factors that drive parking decisions. In light of this, we provide an extensive study of the spatio-temporal characteristics of parking demand. This work advances the understanding of where and when to set pricing policies, as well as how to target information and incentives to drivers looking to park. Harnessing data provided by the Seattle Department of Transportation, we develop a Gaussian mixture model based technique to identify zones with similar spatial demand as quantified by spatial autocorrelation. In support of this technique we provide a method based on the repeatability of our Gaussian mixture model to show demand for parking is consistent through time.", "year": 2018, "ssId": "045f90129a8d7148eec4a58770bc4166b51330ca", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "L. Ratliff", "Chase P. Dowling", "Baosen Zhang"]}}
{"id": "3e4d80e43346b9538504c0a7ee5562f3c6a09178", "content": {"title": "Incentives in the Dark: Multi-armed Bandits for Evolving Users with Unknown Type", "abstract": "Design of incentives or recommendations to users is becoming more common as platform providers continually emerge. We propose a multi-armed bandit approach to the problem in which users types are unknown a priori and evolve dynamically in time. Unlike the traditional bandit setting, observed rewards are generated by a single Markov process. We demonstrate via an illustrative example that blindly applying the traditional bandit algorithms results in very poor performance as measured by regret. We introduce two variants of classical bandit algorithms, upper confidence bound (UCB) and epsilon-greedy, for which we provide theoretical bounds on the regret. We conduct a number of simulation-based experiments to show how the algorithms perform in comparison to traditional UCB and epsilon-greedy algorithms as well as reinforcement learning (Q-learning).", "year": 2018, "ssId": "3e4d80e43346b9538504c0a7ee5562f3c6a09178", "arXivId": null, "link": null, "openAccess": false, "authors": ["L. Ratliff", "S. Sekar", "Liyuan Zheng", "Tanner Fiez"]}}
