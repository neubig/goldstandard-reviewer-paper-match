{"id": "796f29cee975603c7a1469df1eb21ed5142ecff5", "content": {"title": "RELIC: Retrieving Evidence for Literary Claims", "abstract": "Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work. Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching. We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines; however, experiments and analysis by human domain experts indicate that there is substantial room for improvement over our dense retriever.", "year": 2022, "ssId": "796f29cee975603c7a1469df1eb21ed5142ecff5", "arXivId": "2203.10053", "link": "https://arxiv.org/pdf/2203.10053.pdf", "openAccess": true, "authors": ["Katherine Thai", "Ya-yin Chang", "Kalpesh Krishna", "Mohit Iyyer"]}}
{"id": "1b759204e7c13f0e4af9fe00b052af4456ac3669", "content": {"title": "An Analysis of Frame-skipping in Reinforcement Learning", "abstract": "In the practice of sequential decision making, agents are often designed to sense state at regular intervals of d time steps, d > 1, ignoring state information in between sensing steps. While it is clear that this practice can reduce sensing and compute costs, recent results indicate a further benefit. On many Atari console games, reinforcement learning (RL) algorithms deliver substantially better policies when run with d > 1\u2014in fact with d even as high as 180. In this paper, we investigate the role of the parameter d in RL; d is called the \u201cframe-skip\u201d parameter, since states in the Atari domain are images. For evaluating a fixed policy, we observe that under standard conditions, frame-skipping does not affect asymptotic consistency. Depending on other parameters, it can possibly even benefit learning. To use d > 1 in the control setting, one must first specify which d-step open-loop action sequences can be executed in between sensing steps. We focus on \u201cactionrepetition\u201d, the common restriction of this choice to d-length sequences of the same action. We define a task-dependent quantity called the \u201cprice of inertia\u201d, in terms of which we upper-bound the loss incurred by action-repetition. We show that this loss may be offset by the gain brought to learning by a smaller task horizon. Our analysis is supported by experiments on different tasks and learning algorithms.", "year": 2021, "ssId": "1b759204e7c13f0e4af9fe00b052af4456ac3669", "arXivId": "2102.03718", "link": "https://arxiv.org/pdf/2102.03718.pdf", "openAccess": true, "authors": ["Shivaram Kalyanakrishnan", "Siddharth Aravindan", "Vishwajeet Bagdawat", "Varun Bhatt", "Harshith Goka", "Archit Gupta", "Kalpesh Krishna", "Vihari Piratla"]}}
{"id": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "content": {"title": "Do Long-Range Language Models Actually Use Long-Range Context?", "abstract": "Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).", "year": 2021, "ssId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "arXivId": "2109.09115", "link": "https://arxiv.org/pdf/2109.09115.pdf", "openAccess": true, "authors": ["Simeng Sun", "Kalpesh Krishna", "Andrew Mattarella-Micke", "Mohit Iyyer"]}}
{"id": "9d332ad27bfce66ee725b413aa07bd93c355efdf", "content": {"title": "NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation", "abstract": "Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Pythonbased natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its tranformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robutstness analysis results are available publicly on the NL-Augmenter repository (https://github. com/GEM-benchmark/NL-Augmenter).", "year": 2021, "ssId": "9d332ad27bfce66ee725b413aa07bd93c355efdf", "arXivId": "2112.02721", "link": "https://arxiv.org/pdf/2112.02721.pdf", "openAccess": true, "authors": ["Kaustubh D. Dhole", "Varun Gangal", "Sebastian Gehrmann", "Aadesh Gupta", "Zhenhao Li", "Saad Mahamood", "Abinaya Mahendiran", "Simon Mille", "Ashish Srivastava", "Samson Tan", "Tongshuang Sherry Wu", "J. Sohl-Dickstein", "Jinho D. Choi", "E. Hovy", "Ondrej Dusek", "Sebastian Ruder", "Sajant Anand", "Nagender Aneja", "Rabin Banjade", "Lisa Barthe", "Hanna Behnke", "Ian Berlot-Attwell", "Connor Boyle", "C. Brun", "Marco Antonio Sobrevilla Cabezudo", "Samuel Cahyawijaya", "E. Chapuis", "Wanxiang Che", "Mukund Choudhary", "C. Clauss", "Pierre Colombo", "Filip Cornell", "Gautier Dagan", "M. Das", "Tanay Dixit", "Thomas Dopierre", "Paul-Alexis Dray", "Suchitra Dubey", "Tatiana Ekeinhor", "Marco Di Giovanni", "Rishabh Gupta", "Louanes Hamla", "Sanghyun Han", "Fabrice Harel-Canada", "A. Honor\u00e9", "Ishan Jindal", "Przemyslaw K. Joniak", "Denis Kleyko", "Venelin Kovatchev", "Kalpesh Krishna", "Ashutosh Kumar", "Stefan Langer", "S. Lee", "Corey J. Levinson", "H.-J. Liang", "Kaizhao Liang", "Zhexiong Liu", "Andrey Lukyanenko", "V. Marivate", "Gerard de Melo", "Simon Meoni", "Maxime Meyer", "Afnan Mir", "N. Moosavi", "Niklas Muennighoff", "Timothy Sum Hon Mun", "Kenton W. Murray", "Marcin Namysl", "Maria Obedkova", "Priti Oli", "Nivranshu Pasricha", "J. Pfister", "R. Plant", "Vinay Uday Prabhu", "V. Pais", "Libo Qin", "Shahab Raji", "Pawan Kumar Rajpoot", "Vikas Raunak", "Roy Rinberg", "N. Roberts", "Juan Diego Rodriguez", "C. Roux", "S. VasconcellosP.H.", "Ananya B. Sai", "Robin M. Schmidt", "Thomas Scialom", "T. Sefara", "Saqib Nizam Shamsi", "Xu-dong Shen", "Haoyue Shi", "Y. Shi", "Anna V. Shvets", "Nick Siegel", "Damien Sileo", "Jamie Simon", "Chandan Singh", "Roman Sitelew", "P. Soni", "Taylor M Sorensen", "W. Soto", "Aman Srivastava", "K V Aditya Srivatsa", "Tony Sun", "T. MukundVarma", "A. Tabassum", "Fiona Anting Tan", "Ryan Teehan", "Monalisa Tiwari", "Marie Tolkiehn", "Athena Wang", "Zi-Hao Wang", "Gloria Wang", "Zijie Jay Wang", "Fuxuan Wei", "Bryan Wilie", "Genta Indra Winata", "Xinyi Wu", "Witold Wydma\u0144ski", "Tianbao Xie", "Usama Yaseen", "M. Yee", "Jing Zhang", "Yue Zhang"]}}
{"id": "facefd2fc4b718c6a0d8096b4eb02866028a04c2", "content": {"title": "Weakly-Supervised Open-Retrieval Conversational Question Answering", "abstract": "Recent studies on Question Answering (QA) and Conversational QA (ConvQA) emphasize the role of retrieval: a system first retrieves evidence from a large collection and then extracts answers. This open-retrieval ConvQA setting typically assumes that each question is answerable by a single span of text within a particular passage (a span answer). The supervision signal is thus derived from whether or not the system can recover an exact match of this ground-truth answer span from the retrieved passages. This method is referred to as spanmatch weak supervision. However, information-seeking conversations are challenging for this span-match method since long answers, especially freeform answers, are not necessarily strict spans of any passage. Therefore, we introduce a learned weak supervision approach that can identify a paraphrased span of the known answer in a passage. Our experiments on QuAC and CoQA datasets show that the span-match weak supervisor can only handle conversations with span answers, and has less satisfactory results for freeform answers generated by people. Our method is more flexible as it can handle both span answers and freeform answers. Moreover, our method can be more powerful when combined with the span-match method which shows it is complementary to the span-match method. We also conduct in-depth analyses to show more insights on open-retrieval ConvQA under a weak supervision setting.", "year": 2021, "ssId": "facefd2fc4b718c6a0d8096b4eb02866028a04c2", "arXivId": "2103.02537", "link": "https://arxiv.org/pdf/2103.02537.pdf", "openAccess": true, "authors": ["Chen Qu", "Liu Yang", "Cen-Chieh Chen", "W. Bruce Croft", "Kalpesh Krishna", "Mohit Iyyer"]}}
{"id": "e35357ac461a669fe7e4b877ee1fad0dfda26303", "content": {"title": "Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings", "abstract": "Style transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al., 2021) has attempted \u201cfew-shot\u201d style transfer using just 310 sentences at inference for style extraction. In this work, we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages. Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob. We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model. Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages. To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.1", "year": 2021, "ssId": "e35357ac461a669fe7e4b877ee1fad0dfda26303", "arXivId": "2110.07385", "link": "https://arxiv.org/pdf/2110.07385.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Deepak Nathani", "Xavier Garc\u00eda", "Bidisha Samanta", "P. Talukdar"]}}
{"id": "de0e1f9980afa7949df64d53b8ae7a2f59c55579", "content": {"title": "Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages", "abstract": "Style transfer is the task of rewriting an input sentence into a target style while approximately preserving its content. While most prior literature assumes access to large stylelabelled corpora, recent work (Riley et al., 2021) has attempted \u201cfew-shot\u201d style transfer using only 3-10 sentences at inference for extracting the target style. In this work we consider one such low resource setting where no datasets are available: style transfer for Indian languages. We find that existing fewshot methods perform this task poorly, with a strong tendency to copy inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work using automatic and human evaluations, our model achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across five Indian languages. Moreover, our method is better able to control the amount of style transfer using an input scalar knob. We report promising qualitative results for several attribute transfer directions, including sentiment transfer, text simplification, gender neutralization and text anonymization, all without retraining the model. Finally we found model evaluation to be difficult due to the lack of evaluation datasets and metrics for Indian languages. To facilitate further research in formality transfer for Indic languages, we crowdsource annotations for 4000 sentence pairs in four languages, and use this dataset to design our automatic evaluation suite.", "year": 2021, "ssId": "de0e1f9980afa7949df64d53b8ae7a2f59c55579", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kalpesh Krishna", "Deepak Nathani", "Xavier Garc\u00eda", "Bidisha Samanta", "P. Talukdar"]}}
{"id": "3122a2d7799ba585b993e432b3deb47659b3f3c1", "content": {"title": "Hurdles to Progress in Long-form Question Answering", "abstract": "The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system\u2019s generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.", "year": 2021, "ssId": "3122a2d7799ba585b993e432b3deb47659b3f3c1", "arXivId": "2103.06332", "link": "https://arxiv.org/pdf/2103.06332.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Aurko Roy", "Mohit Iyyer"]}}
{"id": "8ff620f704a4151fd7abba1db792463fbd32bfe5", "content": {"title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models", "abstract": "Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pre-trained abstractive summarizer BART, which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with independent human labeling by domain experts.", "year": 2021, "ssId": "8ff620f704a4151fd7abba1db792463fbd32bfe5", "arXivId": "2103.00751", "link": "https://arxiv.org/pdf/2103.00751.pdf", "openAccess": true, "authors": ["Ahsaas Bajaj", "Pavitra Dangati", "Kalpesh Krishna", "Pradhiksha Ashok Kumar", "Rheeya Uppaal", "Brad Windsor", "Eliot Brenner", "Dominic Dotterrer", "R. Das", "A. McCallum"]}}
{"id": "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e", "content": {"title": "Social Distancing and Face Mask Detection Using Deep Learning", "abstract": "The COVID-19 Pandemic caused by the new Coronavirus is the cause of this 21st-century global health crisis. It has forced the government to impose a lockdown to prevent the transmission of the virus. This led to the unprecedented shutdown of economic activities. The health care system is in crisis. Many different types of safety measures are being taken in order to reduce the risk of the spread of this disease at unprecedented times. Verified reports from renowned scientists and medial health practitioner indicated that wearing a face mask and maintaining social distance reduces the risk of transmitting the virus. Hence, we decided on an approach that is effective and economic by using deep learning techniques to create a safe environment in setups such as manufacturing plants, markets, malls, and other such places. To demonstrate our approach, the training dataset is composed of people, the images with and without the masks, which are collected from a variety of sources and use it in order to build a robust algorithm in order to measure the social distance with the help of the classic geometry methods. Our goal is to determine if a person is wearing a mask, or whether or not they maintain social distance as per protocols and guidelines which are given by leading scientists and governments in this pandemic. We hope that this study will serve as a useful tool for reducing the spread of this dangerous infectious disease in the world.", "year": 2021, "ssId": "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kalpesh Krishna"]}}
{"id": "a997d6e253f08a3e589432c611d6d2a3097d7629", "content": {"title": "Research Exchange-A Collaborative Paper Annotation Tool", "abstract": "This research looks to explore the requirements, development, and efficacy of the online research paper tool-Research Exchange. Based on surveying graduate and undergraduate research students, it was found that getting started with research is a huge undertaking and requires constant guidance of immediate peers and professors. To address these issues, a collaborative online research tool was designed to help young researchers understand concepts on the fly. To understand whether the tool helps achieving the desired goal, usability studies were conducted to understand the usability and functionality issues of the tool. It was found that collaboration using annotations was extremely beneficial for users which were in line with our findings from the requirement phase. Additionally, it was noted that users appreciated the online reader and were open to trying it from the existing pdf solutions. However, many users noted several additional features that would be necessary to make this product useful. Future work involves improving the prototype and integration of the OpenReview APIs for large scale deployment. Our final prototype can be found on http://researchexchange.cs.umass.edu:3000/.", "year": 2020, "ssId": "a997d6e253f08a3e589432c611d6d2a3097d7629", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kalpesh Krishna", "Yue-Cen Liu", "Shashank Mehrotra", "Jiazheng Zhu"]}}
{"id": "ccad27088b9098de4eaca8dc449b18766db4b3ab", "content": {"title": "Reformulating Unsupervised Style Transfer as Paraphrase Generation", "abstract": "Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.", "year": 2020, "ssId": "ccad27088b9098de4eaca8dc449b18766db4b3ab", "arXivId": "2010.05700", "link": "https://arxiv.org/pdf/2010.05700.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "J. Wieting", "Mohit Iyyer"]}}
{"id": "84e566e326b64b105cabf0c47dff336c4f632a1c", "content": {"title": "SunPy: A Python package for Solar Physics", "abstract": "Stuart J. Mumford\u22171, 2, 3, Nabil Freij4, Steven Christe5, Jack Ireland5, Florian Mayer6, V. Keith Hughitt7, Albert Y. Shih5, Daniel F. Ryan8, 5, Simon Liedtke6, David P\u00e9rez-Su\u00e1rez9, Pritish Chakraborty10, Vishnunarayan K I.6, Andrew Inglis11, Punyaslok Pattnaik12, Brigitta Sip\u0151cz13, Rishabh Sharma6, Andrew Leonard3, David Stansby14, Russell Hewett15, Alex Hamilton6, Laura Hayes5, Asish Panda6, Matt Earnshaw6, Nitin Choudhary16, Ankit Kumar6, Prateek Chanda17, Md Akramul Haque18, Michael S Kirk11, Michael Mueller6, Sudarshan Konge6, Rajul Srivastava6, Yash Jain19, Samuel Bennett6, Ankit Baruah6, Will Barnes20, Michael Charlton6, Shane Maloney21, Nicky Chorley22, Himanshu6, Sanskar Modi6, James Paul Mason6, Naman96396, Jose Ivan Campos Rozo23, Larry Manley6, Agneet Chatterjee24, John Evans6, Michael Malocha6, Monica G. Bobra25, Sourav Ghosh24, Airmansmith976, Dominik Sta\u0144czak26, Ruben De Visscher6, Shresth Verma27, Ankit Agrawal6, Dumindu Buddhika6, Swapnil Sharma6, Jongyeob Park28, Matt Bates6, Dhruv Goel6, Garrison Taylor29, Goran Cetusic6, Jacob6, Mateo Inchaurrandieta6, Sally Dacie30, Sanjeev Dubey6, Deepankar Sharma6, Erik M. Bray6, Jai Ram Rideout31, Serge Zahniy5, Tomas Meszaros6, Abhigyan Bose6, Andr\u00e9 Chicrala32, Ankit6, Chlo\u00e9 Guennou6, Daniel D\u2019Avella6, Daniel Williams33, Jordan Ballew6, Nick Murphy34, Priyank Lodha6, Thomas Robitaille6, Yash Krishan6, Andrew Hill6, Arthur Eigenbrot35, Benjamin Mampaey36, Bernhard M. Wiedemann6, Carlos Molina6, Duygu Ke\u015fkek6, Ishtyaq Habib6, Joseph Letts6, Juanjo Baz\u00e1n37, Quinn Arbolante38, Reid Gomillion6, Yash Kothari6, Yash Sharma6, Abigail L. Stevens39, 40, Adrian Price-Whelan41, Ambar Mehrotra6, Arseniy Kustov6, Brandon Stone6, Trung Kien Dang42, Emmanuel Arias6, Fionnlagh Mackenzie Dover1, Freek Verstringe36, Gulshan Kumar43, Harsh Mathur44, Igor Babuschkin6, Jaylen Wimbish6, Juan Camilo Buitrago-Casas6, Kalpesh Krishna45, Kaustubh Hiware46, Manas Mangaonkar6, Matthew Mendero6, Micka\u00ebl Schoentgen6, Norbert G Gyenge47, Ole Streicher48, Rajasekhar Reddy Mekala6, Rishabh Mishra6, Shashank Srikanth43, Sarthak Jain6, Tannmay Yadav49, Tessa D. Wilkinson6, Tiago M. D. Pereira50, 51, Yudhik Agrawal12, jamescalixto6, yasintoda6, and Sophie A. Murray52", "year": 2020, "ssId": "84e566e326b64b105cabf0c47dff336c4f632a1c", "arXivId": null, "link": null, "openAccess": false, "authors": ["S. Mumford", "N. Freij", "S. Christe", "J. Ireland", "F. Mayer", "V. Hughitt", "A. Shih", "D. Ryan", "Simon Liedtke", "D. P\u00e9rez-Su\u00e1rez", "Pritish Chakraborty", "K. Vishnunarayan", "A. Inglis", "Punyaslok Pattnaik", "B. Sipocz", "Rishabh Sharma", "A. Leonard", "D. Stansby", "Russell J. Hewett", "Alex Hamilton", "L. Hayes", "A. Panda", "M. Earnshaw", "Nitin Choudhary", "Ankit Kumar", "Prateek Chanda", "Haque", "M. Kirk", "Michael Mueller", "Sudarshan Konge", "Rajul Srivastava", "Y. Jain", "Samuel Bennett", "Ankit Baruah", "W. Barnes", "Michael C. Charlton", "S. Maloney", "N. Chorley", "Himanshu", "Sanskar Modi", "James Mason", "Naman", "J. Rozo", "L. Manley", "Agneet Chatterjee", "John G Evans", "Michael Malocha", "M. Bobra", "Sourav Ghosh", "Airmansmith", "D. Sta\u0144czak", "R. Visscher", "Shresth Verma", "Ankit Agrawal", "Dumindu Buddhika", "Swapnil Sharma", "Jongyeob Park", "M. Bates", "Dhruv Goel", "Garrison Taylor", "Goran Cetu\u0161i\u0107", "Jacob", "Mateo Inchaurrandieta", "S. Dacie", "Sanjeev Dubey", "Deepankar Sharma", "E. Bray", "J. Rideout", "S. Zahniy", "T. M\u00e9sz\u00e1ros", "Abhigyan Bose", "Andr\u00e9 Chicrala", "Ankit", "C. Guennou", "D. D'Avella", "Daniel Williams", "J. Ballew", "N. Murphy", "P. Lodha", "T. Robitaille", "Yash Krishan", "Andrew Hill", "A. Eigenbrot", "B. Mampaey", "Bernhard M. Wiedemann", "Carlos Molina", "Duygu Ke\u015fkek", "Ishtyaq Habib", "Joseph Letts", "Juanjo Baz\u00e1n", "Quinn Arbolante", "Reid Gomillion", "Yash Kothari", "Yash Sharma", "A. Stevens", "A. Price-Whelan", "Ambar Mehrotra", "A. Kustov", "Brandon Stone", "Trung Kien Dang", "Emmanuel Arias", "F. Dover", "F. Verstringe", "Gulshan Kumar", "Harsh Mathur", "I. Babuschkin", "J. Wimbish", "J. Buitrago-Casas", "Kalpesh Krishna", "Kaustubh Hiware", "Manasi D. Mangaonkar", "Matthew Mendero", "Micka\u00ebl Schoentgen", "N. Gyenge", "O. Streicher", "R. Mekala", "R. Mishra", "Shashank Srikanth", "Sarthak Jain", "Tannmay Yadav", "T. Wilkinson", "T. Pereira", "Yudhik Agrawal", "Jamescalixto", "Yasintoda", "S. Murray"]}}
{"id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27", "content": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.", "year": 2019, "ssId": "ac713aebdcc06f15f8ea61e1140bb360341fdf27", "arXivId": "1910.12366", "link": "https://arxiv.org/pdf/1910.12366.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"]}}
{"id": "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b", "content": {"title": "Trick or TReAT : Thematic Reinforcement for Artistic Typography", "abstract": "An approach to make text visually appealing and memorable is semantic reinforcement - the use of visual cues alluding to the context or theme in which the word is being used to reinforce the message (e.g., Google Doodles). We present a computational approach for semantic reinforcement called TReAT - Thematic Reinforcement for Artistic Typography. Given an input word (e.g. exam) and a theme (e.g. education), the individual letters of the input word are replaced by cliparts relevant to the theme which visually resemble the letters - adding creative context to the potentially boring input word. We use an unsupervised approach to learn a latent space to represent letters and cliparts and compute similarities between the two. Human studies show that participants can reliably recognize the word as well as the theme in our outputs (TReATs) and find them more creative compared to meaningful baselines.", "year": 2019, "ssId": "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b", "arXivId": "1903.07820", "link": "https://arxiv.org/pdf/1903.07820.pdf", "openAccess": true, "authors": ["Purva Tendulkar", "Kalpesh Krishna", "Ramprasaath R. Selvaraju", "Devi Parikh"]}}
{"id": "8da992b611df508b1803f66ffa53bd1fb741a76c", "content": {"title": "Generating Question-Answer Hierarchies", "abstract": "The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., \u201cWhy did Frodo leave the Fellowship?\u201d) to reveal related but more specific questions (e.g., \u201cWho did Frodo leave with?\u201d). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC . We then use these labels as input to a pipelined system centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results.", "year": 2019, "ssId": "8da992b611df508b1803f66ffa53bd1fb741a76c", "arXivId": "1906.02622", "link": "https://arxiv.org/pdf/1906.02622.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Mohit Iyyer"]}}
{"id": "d6e21619df572d04b2b2d97b4c5d1fd604f185fb", "content": {"title": "Syntactically Supervised Transformers for Faster Neural Machine Translation", "abstract": "Standard decoders for neural machine translation autoregressively generate a single target token per timestep, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.", "year": 2019, "ssId": "d6e21619df572d04b2b2d97b4c5d1fd604f185fb", "arXivId": "1906.02780", "link": "https://arxiv.org/pdf/1906.02780.pdf", "openAccess": true, "authors": ["Nader Akoury", "Kalpesh Krishna", "Mohit Iyyer"]}}
{"id": "7a1bcf3c84607f7aeb0601658845ca2083059f43", "content": {"title": "Semi-Supervised Learning for Vision-and-Language Tasks using MixMatch", "abstract": "Semi-supervised learning algorithms attempt to train a model with limited labeled data by leveraging a large amount of unlabelled samples . However, there is limited literature on using such a strategy for visual-language tasks, mainly due to the complexity and the discreteness of the input space. We attempt to reformulate MixMatch, a recently proposed semi-supervised learning strategy, on a state-of-the-art multi-modal framework LXMERT, and report the performance on the NLVR2 dataset. We compare these results with suitable baseline experiments. To assess the applicability of textual interpolation, we conduct an interesting experiment on GPT-2. Towards the end, we propose two more modifications that were planned but couldn\u2019t be executed due to the constraint of time.", "year": 2019, "ssId": "7a1bcf3c84607f7aeb0601658845ca2083059f43", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kalpesh Krishna", "Videsh Suman"]}}
{"id": "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30", "content": {"title": "Hierarchical Multitask Learning for CTC-based Speech Recognition", "abstract": "Previous work has shown that neural encoder-decoder speech recognition can be improved with hierarchical multitask learning, where auxiliary tasks are added at intermediate layers of a deep encoder. We explore the effect of hierarchical multitask learning in the context of connectionist temporal classification (CTC)-based speech recognition, and investigate several aspects of this approach. Consistent with previous work, we observe performance improvements on telephone conversational speech recognition (specifically the Eval2000 test sets) when training a subword-level CTC model with an auxiliary phone loss at an intermediate layer. We analyze the effects of a number of experimental variables (like interpolation constant and position of the auxiliary loss function), performance in lower-resource settings, and the relationship between pretraining and multitask learning. We observe that the hierarchical multitask approach improves over standard multitask training in our higher-data experiments, while in the low-resource settings standard multitask training works well. The best results are obtained by combining hierarchical multitask learning and pretraining, which improves word error rates by 3.4% absolute on the Eval2000 test sets.", "year": 2018, "ssId": "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30", "arXivId": "1807.06234", "link": "https://arxiv.org/pdf/1807.06234.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Shubham Toshniwal", "Karen Livescu"]}}
{"id": "a7f30bae9303825adbc333a8df8a03398dea5151", "content": {"title": "Revisiting the Importance of Encoding Logic Rules in Sentiment Classification", "abstract": "We analyze the performance of different sentiment classification models on syntactically complex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo\u2019s ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.", "year": 2018, "ssId": "a7f30bae9303825adbc333a8df8a03398dea5151", "arXivId": "1808.07733", "link": "https://arxiv.org/pdf/1808.07733.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "P. Jyothi", "Mohit Iyyer"]}}
