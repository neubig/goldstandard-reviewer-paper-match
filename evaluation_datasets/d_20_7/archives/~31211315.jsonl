{"id": "44cabe32482d4b622d9ca00bf23b3ee7950e2710", "content": {"title": "A Unifying Framework for Combining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making", "abstract": "Hybrid human-ML systems are increasingly in charge of consequential decisions in a wide range of domains. A growing body of work has advanced our understanding of these systems by providing empirical and theoretical analyses. However, existing empirical results are mixed, and theoretical proposals are often incompatible with each other. Our goal in this work is to bring much-needed organization to this field by offering a unifying framework for understanding conditions under which combining complementary strengths of human and ML leads to higher quality decisions than those produced by them individually\u2014a state to which we refer to as human-ML complementarity. We focus specifically on the context of human-ML predictive decision-making systems and investigate optimal ways of combining human and ML-based predictive decisions, accounting for the underlying causes of variation in their judgments. Within this scope, we present two crucial contributions. First, drawing upon prior literature in human psychology, machine learning, and human-computer interaction, we introduce a taxonomy characterizing a wide variety of criteria across which human and machine decision-making differ. Building on our taxonomy, our second contribution presents a unifying optimizationbased framework for formalizing how human and ML predictive decisions should be aggregated optimally. We show that our proposed framework encompasses several existing models of human-ML complementarity as special cases. Last but not least, the exploratory analysis of our framework offers a critical piece of insight for future work in this area: the mechanism by which we combine human-ML judgments should be informed by the underlying causes of their diverging decisions.", "year": 2022, "ssId": "44cabe32482d4b622d9ca00bf23b3ee7950e2710", "arXivId": "2204.10806", "link": "https://arxiv.org/pdf/2204.10806.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Liu Leqi", "Kenneth Holstein", "Hoda Heidari"]}}
{"id": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "content": {"title": "Cite-seeing and Reviewing: A Study on Citation Bias in Peer Review", "abstract": "Citations play an important role in researchers\u2019 careers as a key factor in evaluation of scienti\ufb01c impact. Many anecdotes advice authors to exploit this fact and cite prospective reviewers to try obtaining a more positive evaluation for their submission. In this work, we investigate if such a citation bias actually exists: Does the citation of a reviewer\u2019s own work in a submission cause them to be positively biased towards the submission? In conjunction with the review process of two \ufb02agship conferences in machine learning and algorithmic economics, we execute an observational study to test for citation bias in peer review. In our analysis, we carefully account for various confounding factors such as paper quality and reviewer expertise, and apply di\ufb00erent modeling techniques to alleviate concerns regarding the model mismatch. Overall, our analysis involves 1,314 papers and 1,717 reviewers and detects citation bias in both venues we consider. In terms of the e\ufb00ect size, by citing a reviewer\u2019s work, a submission has a non-trivial chance of getting a higher score from the reviewer: an expected increase in the score is approximately 0 . 23 on a 5-point Likert item. For reference, a one-point increase of a score by a single reviewer improves the position of a submission by 11% on average.", "year": 2022, "ssId": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "arXivId": "2203.17239", "link": "https://arxiv.org/pdf/2203.17239.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Ryan Liu", "Shuchi Chawla", "F. Echenique", "Nihar B. Shah"]}}
{"id": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "content": {"title": "To ArXiv or not to ArXiv: A Study Quantifying Pros and Cons of Posting Preprints Online", "abstract": "Double-blind conferences have engaged in debates over whether to allow authors to post their papers online on arXiv or elsewhere during the review process. Independently, some authors of research papers face the dilemma of whether to put their papers on arXiv due to its pros and cons. We conduct a study to substantiate this debate and dilemma via quantitative measurements. Speci\ufb01cally, we conducted surveys of reviewers in two top-tier double-blind computer science conferences\u2014ICML 2021 (5361 submissions and 4699 reviewers) and EC 2021 (498 submissions and 190 reviewers). Our two main \ufb01ndings are as follows. First, more than a third of the reviewers self-report searching online for a paper they are assigned to review. Second, outside the review process, we \ufb01nd that preprints from better-ranked a\ufb03liations see a weakly higher visibility, with a correlation of 0.06 in ICML and 0.05 in EC. In particular, papers associated with the top-10-ranked a\ufb03liations had a visibility of approximately 11% in ICML and 22% in EC, whereas the remaining papers had a visibility of 7% and 18% respectively.", "year": 2022, "ssId": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "arXivId": "2203.17259", "link": "https://arxiv.org/pdf/2203.17259.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Ivan Stelmakh", "Xinwei Shen", "M. Meil\u0103", "F. Echenique", "Shuchi Chawla", "Nihar B. Shah"]}}
{"id": "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9", "content": {"title": "Two-Sample Testing on Pairwise Comparison Data and the Role of Modeling Assumptions", "abstract": "A number of applications require two-sample testing of pairwise comparison data. For instance, in crowdsourcing, there is a long-standing question of whether comparison data provided by people is distributed similar to ratings-converted-to-comparisons. Other examples include sports data analysis and peer grading. In this paper, we design a two-sample test for pairwise comparison data. We establish an upper bound on the sample complexity required to correctly distinguish between the distributions of the two sets of samples. Our test requires essentially no assumptions on the distributions. We then prove complementary information-theoretic lower bounds showing that our results are tight (in the minimax sense) up to constant factors. We also investigate the role of modeling assumptions by proving information-theoretic lower bounds for a range of pairwise comparison models (WST, MST, SST, parameter-based such as BTL and Thurstone).", "year": 2020, "ssId": "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9", "arXivId": null, "link": null, "openAccess": false, "authors": ["Charvi Rastogi", "Sivaraman Balakrishnan", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "content": {"title": "A Large Scale Randomized Controlled Trial on Herding in Peer-Review Discussions", "abstract": "Peer review is the backbone of academia and humans constitute a cornerstone of this process, being responsible for reviewing papers and making the final acceptance/rejection decisions. Given that human decision making is known to be susceptible to various cognitive biases, it is important to understand which (if any) biases are present in the peer-review process and design the pipeline such that the impact of these biases is minimized. In this work, we focus on the dynamics of between-reviewers discussions and investigate the presence of herding behaviour therein. In that, we aim to understand whether reviewers and more senior decision makers get disproportionately influenced by the first argument presented in the discussion when (in case of reviewers) they form an independent opinion about the paper before discussing it with others. Specifically, in conjunction with the review process of ICML 2020 -- a large, top tier machine learning conference -- we design and execute a randomized controlled trial with the goal of testing for the conditional causal effect of the discussion initiator's opinion on the outcome of a paper.", "year": 2020, "ssId": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "arXivId": "2011.15083", "link": "https://arxiv.org/pdf/2011.15083.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Nihar B. Shah", "Aarti Singh", "Hal Daum'e"]}}
{"id": "148efaba70165d9faef0dac28d5fa2538cfa662d", "content": {"title": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making", "abstract": "Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people's perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.", "year": 2020, "ssId": "148efaba70165d9faef0dac28d5fa2538cfa662d", "arXivId": "2010.07938", "link": "https://arxiv.org/pdf/2010.07938.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Yunfeng Zhang", "Dennis Wei", "K. Varshney", "Amit Dhurandhar", "Richard J. Tomsett"]}}
{"id": "803a0d2677a7d6b20c3964533595775fa5c7c750", "content": {"title": "Two-Sample Testing on Ranked Preference Data and the Role of Modeling Assumptions", "abstract": "A number of applications require two-sample testing on ranked preference data. For instance, in crowdsourcing, there is a long-standing question of whether pairwise comparison data provided by people is distributed similar to ratings-converted-to-comparisons. Other examples include sports data analysis and peer grading. In this paper, we design two-sample tests for pairwise comparison data and ranking data. For our two-sample test for pairwise comparison data, we establish an upper bound on the sample complexity required to correctly distinguish between the distributions of the two sets of samples. Our test requires essentially no assumptions on the distributions. We then prove complementary lower bounds showing that our results are tight (in the minimax sense) up to constant factors. We investigate the role of modeling assumptions by proving lower bounds for a range of pairwise comparison models (WST, MST,SST, parameter-based such as BTL and Thurstone). We also provide testing algorithms and associated sample complexity bounds for the problem of two-sample testing with partial (or total) ranking data.Furthermore, we empirically evaluate our results via extensive simulations as well as two real-world datasets consisting of pairwise comparisons. By applying our two-sample test on real-world pairwise comparison data, we conclude that ratings and rankings provided by people are indeed distributed differently. On the other hand, our test recognizes no significant difference in the relative performance of European football teams across two seasons. Finally, we apply our two-sample test on a real-world partial and total ranking dataset and find a statistically significant difference in Sushi preferences across demographic divisions based on gender, age and region of residence.", "year": 2020, "ssId": "803a0d2677a7d6b20c3964533595775fa5c7c750", "arXivId": "2006.11909", "link": "https://arxiv.org/pdf/2006.11909.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Sivaraman Balakrishnan", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "919c929dfa665cb0595a835b4380f96da4cd0143", "content": {"title": "Mobile Sensing of Two-Dimensional Bandlimited Fields on Random Paths", "abstract": "Mobile sensing has been recently proposed for sampling spatial fields, where mobile sensors record the field along various paths for reconstruction. Classical and contemporary sampling typically assumes that the sampling locations are approximately known. This work explores multiple sampling strategies along random paths to sample and reconstruct a two dimensional bandlimited field. Extensive simulations are carried out, with insights from sensing matrices and their properties, to evaluate the sampling strategies. Their performance is measured by evaluating the stability of field reconstruction from field samples. The effect of location unawareness on some sampling strategies is also evaluated by simulations.", "year": 2017, "ssId": "919c929dfa665cb0595a835b4380f96da4cd0143", "arXivId": "1711.04114", "link": "https://arxiv.org/pdf/1711.04114.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Animesh Kumar"]}}
{"id": "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f", "content": {"title": "A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms", "abstract": "This paper proposes a new approach to construct high quality space-filling sample designs. First, we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions. Second, we connect the proposed metric (defined in the spatial domain) to the objective measure of the design performance (defined in the spectral domain). This connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general. Using the theoretical insights provided by this spatial-spectral analysis, we derive the notion of optimal space-filling designs, which we refer to as space-filling spectral designs. Third, we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework to generate high quality space-filling designs. Finally, we carry out a detailed performance comparison on two different applications in 2 to 6 dimensions: a) image reconstruction and b) surrogate modeling on several benchmark optimization functions and an inertial confinement fusion (ICF) simulation code. We demonstrate that the propose spectral designs significantly outperform existing approaches especially in high dimensions.", "year": 2017, "ssId": "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f", "arXivId": "1712.06028", "link": "https://arxiv.org/pdf/1712.06028.pdf", "openAccess": true, "authors": ["B. Kailkhura", "Jayaraman J. Thiagarajan", "Charvi Rastogi", "P. Varshney", "P. Bremer"]}}
