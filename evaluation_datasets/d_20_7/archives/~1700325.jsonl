{"id": "1756376bf7cf0d0a7bec881d663b57907a361ecf", "content": {"title": "Learning Structural Edits via Incremental Tree Transformations", "abstract": "While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e. ''structural edits''). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g. deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.", "year": 2021, "ssId": "1756376bf7cf0d0a7bec881d663b57907a361ecf", "arXivId": "2101.12087", "link": "https://arxiv.org/pdf/2101.12087.pdf", "openAccess": true, "authors": ["Ziyu Yao", "Frank F. Xu", "Pengcheng Yin", "Huan Sun", "Graham Neubig"]}}
{"id": "8512718bafa447f9b433da9e809215dfc28b6b28", "content": {"title": "Towards More Fine-grained and Reliable NLP Performance Prediction", "abstract": "Performance prediction, the task of estimating a system\u2019s performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future.", "year": 2021, "ssId": "8512718bafa447f9b433da9e809215dfc28b6b28", "arXivId": "2102.05486", "link": "https://arxiv.org/pdf/2102.05486.pdf", "openAccess": true, "authors": ["Zihuiwen Ye", "Pengfei Liu", "Jinlan Fu", "Graham Neubig"]}}
{"id": "fcdac45272543b4f8b8eaa59d66044d1b7018494", "content": {"title": "Meta Back-translation", "abstract": "Back-translation (Sennrich et al., 2016) is an effective strategy to improve the performance of Neural Machine Translation (NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results (Edunov et al., 2018). In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudoparallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De\u201914 and WMT En-Fr\u201914, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines.1", "year": 2021, "ssId": "fcdac45272543b4f8b8eaa59d66044d1b7018494", "arXivId": "2102.07847", "link": "https://arxiv.org/pdf/2102.07847.pdf", "openAccess": true, "authors": ["Hieu Pham", "Xinyi Wang", "Yiming Yang", "Graham Neubig"]}}
{"id": "cefd3993db4d065b95ab8f105452fb728c02b60e", "content": {"title": "Can We Automate Scientific Reviewing?", "abstract": "The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question \u201ccan we automate scientific reviewing?\u201d, discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a \u201cgood\u201d review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted \u2217Corresponding author. summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github. com/neulab/ReviewAdvisor as well as a ReviewAdvisor system: http://review.nlpedia.ai/ (See demo screenshot in A.2). The review of this paper (without TL;QR section) written by the system of this paper can be found A.1", "year": 2021, "ssId": "cefd3993db4d065b95ab8f105452fb728c02b60e", "arXivId": "2102.00176", "link": "https://arxiv.org/pdf/2102.00176.pdf", "openAccess": true, "authors": ["Weizhe Yuan", "P. Liu", "Graham Neubig"]}}
{"id": "06e36261b21af2943e464a562c92c09dac292a82", "content": {"title": "Augmenting Decompiler Output with Learned Variable Names and Types", "abstract": "A common tool used by security professionals for reverseengineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4% of the time and the original types 75.8% of the time.", "year": 2021, "ssId": "06e36261b21af2943e464a562c92c09dac292a82", "arXivId": "2108.06363", "link": "https://arxiv.org/pdf/2108.06363.pdf", "openAccess": true, "authors": ["Qibin Chen", "Jeremy Lacomis", "Edward J. Schwartz", "Claire Le Goues", "Graham Neubig", "Bogdan Vasilescu"]}}
{"id": "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713", "content": {"title": "Phoneme Recognition through Fine Tuning of Phonetic Representations: a Case Study on Luhya Language Varieties", "abstract": "Models pre-trained on multiple languages have shown significant promise for improving speech recognition, particularly for low-resource languages. In this work, we focus on phoneme recognition using Allosaurus, a method for multilingual recognition based on phonetic annotation, which incorporates phonological knowledge through a language-dependent allophone layer that associates a universal narrow phone-set with the phonemes that appear in each language. To evaluate in a challenging real-world scenario, we curate phone recognition datasets for Bukusu and Saamia, two varieties of the Luhya language cluster of western Kenya and eastern Uganda. To our knowledge, these datasets are the first of their kind. We carry out similar experiments on the dataset of an endangered Tangkhulic language, East Tusom, a Tibeto-Burman language variety spoken mostly in India. We explore both zero-shot and few-shot recognition by fine-tuning using datasets of varying sizes (10 to 1000 utterances). We find that fine-tuning of Allosaurus, even with just 100 utterances, leads to significant improvements in phone error rates.", "year": 2021, "ssId": "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713", "arXivId": "2104.01624", "link": "https://arxiv.org/pdf/2104.01624.pdf", "openAccess": true, "authors": ["Kathleen Siminyu", "Xinjian Li", "Antonios Anastasopoulos", "David R. Mortensen", "M. Marlo", "Graham Neubig"]}}
{"id": "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8", "content": {"title": "Hierarchical Control of Situated Agents through Natural Language", "abstract": "When humans conceive how to perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, in the literature on natural language (NL) command of situated agents, most works have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a powerful yet intuitive method of representing hierarchical procedural knowledge for agent command and control. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development.", "year": 2021, "ssId": "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8", "arXivId": "2109.08214", "link": "https://arxiv.org/pdf/2109.08214.pdf", "openAccess": true, "authors": ["Shuyan Zhou", "Pengcheng Yin", "Graham Neubig"]}}
{"id": "a77643bff6f50ccc4f80ec081e4d078a2e788ae7", "content": {"title": "Multi-view Subword Regularization", "abstract": "Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.", "year": 2021, "ssId": "a77643bff6f50ccc4f80ec081e4d078a2e788ae7", "arXivId": "2103.08490", "link": "https://arxiv.org/pdf/2103.08490.pdf", "openAccess": true, "authors": ["Xinyi Wang", "Sebastian Ruder", "Graham Neubig"]}}
{"id": "83145b7a391b792e24d8d38f74ed6b6ae7a149dc", "content": {"title": "Measuring and Increasing Context Usage in Context-Aware Machine Translation", "abstract": "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.", "year": 2021, "ssId": "83145b7a391b792e24d8d38f74ed6b6ae7a149dc", "arXivId": "2105.03482", "link": "https://arxiv.org/pdf/2105.03482.pdf", "openAccess": true, "authors": ["Patrick Fernandes", "Kayo Yin", "Graham Neubig", "Andr\u00e9 F. T. Martins"]}}
{"id": "b57da3ccf214e8dad49116c8db9590c2c89629f5", "content": {"title": "MasakhaNER: Named Entity Recognition for African Languages", "abstract": "Abstract We take a step towards addressing the under- representation of the African continent in NLP research by bringing together different stakeholders to create the first large, publicly available, high-quality dataset for named entity recognition (NER) in ten African languages. We detail the characteristics of these languages to help researchers and practitioners better understand the challenges they pose for NER tasks. We analyze our datasets and conduct an extensive empirical evaluation of state- of-the-art methods across both supervised and transfer learning settings. Finally, we release the data, code, and models to inspire future research on African NLP.1", "year": 2021, "ssId": "b57da3ccf214e8dad49116c8db9590c2c89629f5", "arXivId": "2103.11811", "link": "https://arxiv.org/pdf/2103.11811.pdf", "openAccess": true, "authors": ["David Ifeoluwa Adelani", "Jade Z. Abbott", "Graham Neubig", "Daniel D'souza", "Julia Kreutzer", "Constantine Lignos", "Chester Palen-Michel", "Happy Buzaaba", "Shruti Rijhwani", "Sebastian Ruder", "Stephen Mayhew", "Israel Abebe Azime", "Shamsuddeen Hassan Muhammad", "Chris C. Emezue", "J. Nakatumba-Nabende", "Perez Ogayo", "Anuoluwapo Aremu", "Catherine Gitau", "Derguene Mbaye", "Jesujoba Oluwadara Alabi", "Seid Muhie Yimam", "Tajuddeen R. Gwadabe", "I. Ezeani", "Rubungo Andre Niyongabo", "Jonathan Mukiibi", "V. Otiende", "Iroro Orife", "Davis David", "Samba Ngom", "Tosin P. Adewumi", "Paul Rayson", "Mofetoluwa Adeyemi", "Gerald Muriuki", "Emmanuel Anebi", "C. Chukwuneke", "N. Odu", "Eric Peter Wairagala", "S. Oyerinde", "Clemencia Siro", "Tobius Saul Bateesa", "Temilola Oloyede", "Yvonne Wambui", "Victor Akinode", "Deborah Nabagereka", "Maurice Katusiime", "A. Awokoya", "Mouhamadane Mboup", "D. Gebreyohannes", "Henok Tilaye", "Kelechi Nwaike", "Degaga Wolde", "A. Faye", "Blessing K. Sibanda", "Orevaoghene Ahia", "Bonaventure F. P. Dossou", "Kelechi Ogueji", "Thierno Ibrahima Diop", "A. Diallo", "Adewale Akinfaderin", "T. Marengereke", "Salomey Osei"]}}
{"id": "8b20173b98914f36302389e4c761c334fe867dcd", "content": {"title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts", "abstract": "Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L\u2019AMBRE \u2013 a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages.", "year": 2021, "ssId": "8b20173b98914f36302389e4c761c334fe867dcd", "arXivId": "2103.16590", "link": "https://arxiv.org/pdf/2103.16590.pdf", "openAccess": true, "authors": ["Adithya Pratapa", "Antonios Anastasopoulos", "Shruti Rijhwani", "Aditi Chaudhary", "David R. Mortensen", "Graham Neubig", "Yulia Tsvetkov"]}}
{"id": "4a160efbe80c38cd5eb2f92c7c095b49b113397d", "content": {"title": "In-IDE Code Generation from Natural Language: Promise and Challenges", "abstract": "\n A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning\n concept\n into\n code\n , especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from\n natural language queries\n , but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, \u201cAt the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?\u201d To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.\n", "year": 2021, "ssId": "4a160efbe80c38cd5eb2f92c7c095b49b113397d", "arXivId": "2101.11149", "link": "https://arxiv.org/pdf/2101.11149.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Bogdan Vasilescu", "Graham Neubig"]}}
{"id": "25efc17ba82ba4af29f2e03868de74e1ea66d025", "content": {"title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models", "abstract": "This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.", "year": 2021, "ssId": "25efc17ba82ba4af29f2e03868de74e1ea66d025", "arXivId": "2103.08849", "link": "https://arxiv.org/pdf/2103.08849.pdf", "openAccess": true, "authors": ["Po-Yao Huang", "Mandela Patrick", "Junjie Hu", "Graham Neubig", "Florian Metze", "A. Hauptmann"]}}
{"id": "d06493373421c86ba33dbb8834ccb725105a665f", "content": {"title": "When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection", "abstract": "Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun \u201cwall\u201d has different lexical manifestations in Spanish \u2013 \u201cpared\u201d refers to an indoor wall while \u201cmuro\u201d refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, Spanish and Greek, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations.", "year": 2021, "ssId": "d06493373421c86ba33dbb8834ccb725105a665f", "arXivId": "2109.06014", "link": "https://arxiv.org/pdf/2109.06014.pdf", "openAccess": true, "authors": ["Aditi Chaudhary", "Kayo Yin", "Antonios Anastasopoulos", "Graham Neubig"]}}
{"id": "1cf2e9e198feef3893da2800a7949f6880ddc084", "content": {"title": "ExplainaBoard: An Explainable Leaderboard for NLP", "abstract": "With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate \u201coutput-driven\u201d research in the future.", "year": 2021, "ssId": "1cf2e9e198feef3893da2800a7949f6880ddc084", "arXivId": "2104.06387", "link": "https://arxiv.org/pdf/2104.06387.pdf", "openAccess": true, "authors": ["Pengfei Liu", "Jinlan Fu", "Yanghua Xiao", "Weizhe Yuan", "Shuaichen Chang", "Junqi Dai", "Yixin Liu", "Zihuiwen Ye", "Zi-Yi Dou", "Graham Neubig"]}}
{"id": "5ede529879d162d2779d410a5775d3f6cd6be3f4", "content": {"title": "Modeling the Second Player in Distributionally Robust Optimization", "abstract": "Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the \u201cuncertainty set\u201d). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as f -divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines1.", "year": 2021, "ssId": "5ede529879d162d2779d410a5775d3f6cd6be3f4", "arXivId": "2103.10282", "link": "https://arxiv.org/pdf/2103.10282.pdf", "openAccess": true, "authors": ["Paul Michel", "Tatsunori B. Hashimoto", "Graham Neubig"]}}
{"id": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30", "content": {"title": "Few-shot Language Coordination by Modeling Theory of Mind", "abstract": "No man is an island. Humans communicate with a large community by coordinating with different interlocutors within short conversations. This ability has been understudied by the research on building neural communicative agents. We study the task of few-shot language coordination: agents quickly adapting to their conversational partners\u2019 language abilities. Different from current communicative agents trained with selfplay, we require the lead agent to coordinate with a population of agents with different linguistic abilities, quickly adapting to communicate with unseen agents in the population. This requires the ability to model the partner\u2019s beliefs, a vital component of human communication. Drawing inspiration from theory-of-mind (ToM; Premack & Woodruff (1978)), we study the effect of the speaker explicitly modeling the listeners\u2019 mental states. The speakers, as shown in our experiments, acquire the ability to predict the reactions of their partner, which helps it generate instructions that concisely express its communicative goal. We examine our hypothesis that the instructions generated with ToM modeling yield better communication performance in both a referential game and a language navigation task. Positive results from our experiments hint at the importance of explicitly modeling communication as a socio-pragmatic progress. Code can be found at https://github.com/CLAW-Lab/ToM.", "year": 2021, "ssId": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30", "arXivId": "2107.05697", "link": "https://arxiv.org/pdf/2107.05697.pdf", "openAccess": true, "authors": ["Hao Zhu", "Graham Neubig", "Yonatan Bisk"]}}
{"id": "29263fa3632951be0ca617988d7c9ce651e74393", "content": {"title": "Breaking Down Multilingual Machine Translation", "abstract": "While multilingual training is now an essen-tial ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training con-tribute to learning these two components of the MT model. Speci\ufb01cally, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is bene\ufb01cial to encoders in general, while it only bene\ufb01ts decoders for low-resource languages (LRLs). We further \ufb01nd the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and en-ables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).", "year": 2021, "ssId": "29263fa3632951be0ca617988d7c9ce651e74393", "arXivId": "2110.08130", "link": "https://arxiv.org/pdf/2110.08130.pdf", "openAccess": true, "authors": ["Ting-Rui Chiang", "Yi-Pei Chen", "Yi-Ting Yeh", "Graham Neubig"]}}
{"id": "b790c3e712c92065d596364af81a494adbc62c39", "content": {"title": "IN DISTRIBUTIONALLY ROBUST OPTIMIZATION", "abstract": "Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the \u201cuncertainty set\u201d). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as f -divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines1.", "year": 2021, "ssId": "b790c3e712c92065d596364af81a494adbc62c39", "arXivId": null, "link": null, "openAccess": false, "authors": ["Paul Michel", "Tatsunori B. Hashimoto", "Graham Neubig"]}}
{"id": "821532ecef5bc2252823b190c35f1e4c44ddc41c", "content": {"title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora", "abstract": "Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs.", "year": 2021, "ssId": "821532ecef5bc2252823b190c35f1e4c44ddc41c", "arXivId": "2101.08231", "link": "https://arxiv.org/pdf/2101.08231.pdf", "openAccess": true, "authors": ["Zi-Yi Dou", "Graham Neubig"]}}
