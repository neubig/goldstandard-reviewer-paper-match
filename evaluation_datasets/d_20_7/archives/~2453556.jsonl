{"id": "d34712c217046ccf8063efe083fbb1e6cbfc0340", "content": {"title": "C2DN: How to Harness Erasure Codes at the Edge for Ef\ufb01cient Content Delivery", "abstract": "Content Delivery Networks (CDNs) deliver much of the world\u2019s web and video content to users from thousands of clusters deployed at the \u201cedges\u201d of the Internet. Maintain-ing consistent performance in this large distributed system is challenging. Through analysis of month-long logs from over 2000 clusters of a large CDN, we study the patterns of server unavailability. For a CDN with no redundancy, each server unavailability causes a sudden loss in performance as the objects previously cached on that server are not accessible, which leads to a miss ratio spike. The state-of-the-art mitigation technique used by large CDNs is to replicate objects across multiple servers within a cluster. We \ufb01nd that although replication reduces miss ratio spikes, spikes remain a performance challenge. We present C2DN, the \ufb01rst CDN design that achieves a lower miss ratio, higher availability, higher resource ef\ufb01ciency, and close-to-perfect write load balancing. The core of our design is to introduce erasure coding into the CDN architecture and use the parity chunks to re-balance the write load across servers. We implement C2DN on top of open-source production software and demonstrate that compared to replication-based CDNs, C2DN obtains 11% lower byte miss ratio, eliminates unavailability-induced miss ratio spikes, and reduces write load imbalance by 99%.", "year": 2022, "ssId": "d34712c217046ccf8063efe083fbb1e6cbfc0340", "arXivId": null, "link": null, "openAccess": false, "authors": ["Juncheng Yang", "A. Sabnis", "Daniel S. Berger", "K. V. Rashmi", "R. Sitaraman"]}}
{"id": "ab8be9e585e599db99d8451e63a2311d88ff9293", "content": {"title": "A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters at Twitter", "abstract": "Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.", "year": 2021, "ssId": "ab8be9e585e599db99d8451e63a2311d88ff9293", "arXivId": null, "link": null, "openAccess": false, "authors": ["Juncheng Yang", "Yao Yue", "K. V. Rashmi"]}}
{"id": "3f2821dd40c12da560c89b9dad7f95cd4ad9354f", "content": {"title": "PACEMAKER: Avoiding HeART attacks in storage clusters with disk-adaptive redundancy", "abstract": "Data redundancy provides resilience in large-scale storage clusters, but imposes significant cost overhead. Substantial space-savings can be realized by tuning redundancy schemes to observed disk failure rates. However, prior design proposals for such tuning are unusable in real-world clusters, because the IO load of transitions between schemes overwhelms the storage infrastructure (termed transition overload). This paper analyzes traces for millions of disks from production systems at Google, NetApp, and Backblaze to expose and understand transition overload as a roadblock to diskadaptive redundancy: transition IO under existing approaches can consume 100% cluster IO continuously for several weeks. Building on the insights drawn, we present PACEMAKER, a low-overhead disk-adaptive redundancy orchestrator. PACEMAKER mitigates transition overload by (1) proactively organizing data layouts to make future transitions efficient, and (2) initiating transitions proactively in a manner that avoids urgency while not compromising on space-savings. Evaluation of PACEMAKER with traces from four large (110K\u2013450K disks) production clusters show that the transition IO requirement decreases to never needing more than 5% cluster IO bandwidth (0.2\u20130.4% on average). PACEMAKER achieves this while providing overall space-savings of 14\u201320% and never leaving data under-protected. We also describe and experiment with an integration of PACEMAKER into HDFS.", "year": 2021, "ssId": "3f2821dd40c12da560c89b9dad7f95cd4ad9354f", "arXivId": "2103.08191", "link": "https://arxiv.org/pdf/2103.08191.pdf", "openAccess": true, "authors": ["Saurabh Kadekodi", "Francisco Maturana", "Suhas Jayaram Subramanya", "Juncheng Yang", "K. V. Rashmi", "G. Ganger"]}}
{"id": "58961f0ea3291ddab697fbe5be999a0793b0efaf", "content": {"title": "Irregular Array Codes with Arbitrary Access Sets for Geo-Distributed Storage", "abstract": "Distributed storage systems typically use erasure codes to provide tolerance against node failures. An erasure code encodes a message into a codeword made up of several symbols, which are then distributed among nodes in the system. Maximum distance separable (MDS) $[n, k]$ scalar codes are commonly used in practice, which have the property that any subset of $k$ out of $n$ nodes is enough to decode the message. However, in applications such as geo-distributed storage systems, decodability from many of these subsets is unnecessary. In this paper, we study codes where only certain subsets of nodes, named access sets, are required to satisfy decodability. Our analysis focuses on two metrics of practical importance: update cost and storage overhead. For minimizing these metrics, we show that it is necessary to employ irregular array codes. We derive a lower bound on update cost as a function of the required access sets and show that it is achievable. Existing work provides an achievable lower bound on storage overhead. While both lower bounds are individually achievable, we show that they are not simultaneously achievable in general. Due to the premium in wide-area network bandwidth cost over storage cost, we focus on codes with minimum update cost (termed MUC). Finally, we derive a lower bound on the storage overhead of MUC codes and show the existence of MUC codes meeting this lower bound via a randomized construction. Our results thus show that it is possible to achieve significant savings in update cost and storage overhead by tailoring the design of codes to the required access sets.", "year": 2021, "ssId": "58961f0ea3291ddab697fbe5be999a0793b0efaf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Francisco Maturana", "K. V. Rashmi"]}}
{"id": "e59adee86b666ad76164b3446cfee5068a15e5c9", "content": {"title": "Arithmetic-intensity-guided fault tolerance for neural network inference on GPUs", "abstract": "Neural networks (NNs) are increasingly employed in safety-critical domains and in environments prone to unreliability (e.g., soft errors), such as on spacecraft. Therefore, it is critical to impart fault tolerance to NN inference. Algorithm-based fault tolerance (ABFT) is emerging as an efficient approach for fault tolerance in NNs. We propose an adaptive approach to ABFT for NN inference that exploits untapped opportunities in emerging deployment scenarios. GPUs have high compute-to-memory-bandwidth ratios, while NN layers have a wide range of arithmetic intensities. This leaves some layers compute bound and others memory-bandwidth bound, but current approaches to ABFT do not consider these differences. We first investigate ABFT schemes best suited for each of these scenarios. We then propose intensity-guided ABFT, an adaptive, arithmetic-intensity-guided approach that selects the most efficient ABFT scheme for each NN layer. Intensity-guided ABFT reduces execution-time overhead by 1.09--5.3\u00d7 across many NNs compared to traditional approaches to ABFT.", "year": 2021, "ssId": "e59adee86b666ad76164b3446cfee5068a15e5c9", "arXivId": "2104.09455", "link": "https://arxiv.org/pdf/2104.09455.pdf", "openAccess": true, "authors": ["J. Kosaian", "K. V. Rashmi"]}}
{"id": "09fcc7ed1f867bcf9133ab12065ee7366cfaa652", "content": {"title": "ECRM: Efficient Fault Tolerance for Recommendation Model Training via Erasure Coding", "abstract": "Deep-learning-based recommendation models (DLRMs) are widely deployed to serve personalized content to users. DLRMs are large in size due to their use of large embedding tables, and are trained by distributing the model across the memory of tens or hundreds of servers. Server failures are common in such large distributed systems and must be mitigated to enable training to progress. Checkpointing is the primary approach used for fault tolerance in these systems, but incurs significant training-time overhead both during normal operation and when recovering from failures. As these overheads increase with DLRM size, checkpointing is slated to become an even larger overhead for future DLRMs, which are expected to grow in size. This calls for rethinking fault tolerance in DLRM training. We present ECRM, a DLRM training system that achieves efficient fault tolerance using erasure coding. ECRM chooses which DLRM parameters to encode, correctly and efficiently updates parities, and enables training to proceed without any pauses, while maintaining consistency of the recovered parameters. We implement ECRM atop XDL, an open-source, industrial-scale DLRM training system. Compared to checkpointing, ECRM reduces training-time overhead for large DLRMs by up to 88%, recovers from failures up to 10.3\u00d7 faster, and allows training to proceed during recovery. These results show the promise of erasure coding in imparting efficient fault tolerance to training current and future DLRMs.", "year": 2021, "ssId": "09fcc7ed1f867bcf9133ab12065ee7366cfaa652", "arXivId": "2104.01981", "link": "https://arxiv.org/pdf/2104.01981.pdf", "openAccess": true, "authors": ["Kaige Liu", "J. Kosaian", "K. V. Rashmi"]}}
{"id": "305a1251a68fb16835876d8c99de498472c0cd8f", "content": {"title": "A locality-based lens for coded computation", "abstract": "Coded computation is an emerging paradigm for robustness in large-scale distributed computing, which applies principles from coding theory to provide robustness against slow or otherwise unavailable workers. We propose a new approach to view coded computation via the lens of locality of codes. We do so by defining a new notion of locality, called computational locality, via the locality properties of an appropriately defined code for the function being computed. This notion of locality incorporates the unique aspects of locality arising in the context of coded computation. Using this new approach, (1) We demonstrate how to design a coded computation scheme for a function using the local decoding scheme of an appropriately defined code. This rederives the best-known coded computation scheme for multivariate polynomial functions via the viewpoint of locality of the Reed Muller code. (2) We show that the proposed locality-based approach enables coded computation schemes with significantly lower resource overhead than existing schemes. Specifically, matrix multiplication over complex numbers, a common workload in high performance computing, is achieved with 33.3% fewer workers than state-of-the-art coded computation schemes.", "year": 2021, "ssId": "305a1251a68fb16835876d8c99de498472c0cd8f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Michael Rudow", "K. V. Rashmi", "V. Guruswami"]}}
{"id": "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4", "content": {"title": "A large scale analysis of hundreds of in-memory cache clusters at Twitter", "abstract": "Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.", "year": 2020, "ssId": "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Juncheng Yang", "Yao Yue", "K. V. Rashmi"]}}
{"id": "9cdf512f273083efa1ea01f7b31daa97a7bbe884", "content": {"title": "A locality-based approach for coded computation", "abstract": "Modern distributed computation infrastructures are often plagued by unavailabilities such as failing or slow servers. These unavailabilities adversely affect the tail latency of computation in distributed infrastructures. The simple solution of replicating computation entails significant resource overhead. Coded computation has emerged as a resource-efficient alternative, wherein multiple units of data are encoded to create parity units and the function to be computed is applied to each of these units on distinct servers. A decoder can use the available function outputs to decode the unavailable ones. Existing coded computation approaches are resource efficient only for simple variants of linear functions such as multilinear, with even the class of low degree polynomials requiring the same multiplicative overhead as replication for practically relevant straggler tolerance. \nIn this paper, we present a new approach to model coded computation via the lens of locality of codes. We introduce a generalized notion of locality, denoted computational locality, building upon the locality of an appropriately defined code. We show that computational locality is equivalent to the required number of workers for coded computation and leverage results from the well-studied locality of codes to design coded computation schemes. We show that recent results on coded computation of multivariate polynomials can be derived using local recovering schemes for Reed-Muller codes. We present coded computation schemes for multivariate polynomials that adaptively exploit locality properties of input data-- an inadmissible technique under existing frameworks. These schemes require fewer workers than the lower bound under existing coded computation frameworks, showing that the existing multiplicative overhead on the number of servers is not fundamental for coded computation of nonlinear functions.", "year": 2020, "ssId": "9cdf512f273083efa1ea01f7b31daa97a7bbe884", "arXivId": "2002.02440", "link": "https://arxiv.org/pdf/2002.02440.pdf", "openAccess": true, "authors": ["Michael Rudow", "K. V. Rashmi", "V. Guruswami"]}}
{"id": "e01aa6f8ce625469b6f161d7ab9e61a60ac33798", "content": {"title": "Online Versus Offline Rate in Streaming Codes for Variable-Size Messages", "abstract": "Providing high quality-of-service for live communication is a pervasive challenge which is plagued by packet losses during transmission. Streaming codes are a class of erasure codes specifically designed for such low-latency streaming communication settings. We consider the recently proposed setting of streaming codes under variable-size messages which reflects the requirements of applications such as live video streaming. In practice, streaming codes often need to operate in an \"online\" setting where the sizes of the future messages are unknown. Yet, previously studied upper bounds on the rate apply to \"offline\" coding schemes with access to all (including future) message sizes.In this paper, we evaluate whether the optimal offline rate is a feasible goal for online streaming codes when communicating over a burst-only packet loss channel. We identify two broad parameter regimes where, perhaps surprisingly, online streaming codes can, in fact, match the optimal offline rate. For both of these settings, we present rate-optimal online code constructions. For all remaining parameter settings, we establish that it is impossible for online schemes to attain the optimal offline rate.", "year": 2020, "ssId": "e01aa6f8ce625469b6f161d7ab9e61a60ac33798", "arXivId": "2006.03045", "link": "https://arxiv.org/pdf/2006.03045.pdf", "openAccess": true, "authors": ["Michael Rudow", "K. V. Rashmi"]}}
{"id": "4c94dc1b2391d78c9cfdd69955d20b56d7a16982", "content": {"title": "Access-optimal Linear MDS Convertible Codes for All Parameters", "abstract": "In large-scale distributed storage systems, erasure codes are used to achieve fault tolerance in the face of node failures. Tuning code redundancy to observed failure rates has been shown to significantly reduce storage cost. Such tuning of redundancy requires code conversion, i.e., a change in code dimension and length on already encoded data. Convertible codes [2] are a new class of codes designed to perform such conversions efficiently. The access cost of conversion is the number of nodes accessed during conversion.Existing literature has characterized the access cost of conversion of linear MDS convertible codes only for a specific and small subset of parameters. In this paper, we present lower bounds on the access cost of conversion of linear MDS codes for all valid parameters. Furthermore, we show that these lower bounds are tight by presenting an explicit construction for access-optimal linear MDS convertible codes for all valid parameters. En route, we show that, one of the degrees-of-freedom in the design of convertible codes that was inconsequential in the previously studied parameter regimes, turns out to be crucial when going beyond these regimes and adds to the challenge in the analysis and code construction.An extended version of this paper is accessible at: [1]", "year": 2020, "ssId": "4c94dc1b2391d78c9cfdd69955d20b56d7a16982", "arXivId": "2006.03042", "link": "https://arxiv.org/pdf/2006.03042.pdf", "openAccess": true, "authors": ["Francisco Maturana", "V. S. C. Mukka", "K. V. Rashmi"]}}
{"id": "26f427d2b27828f2893e95344342570699e9c589", "content": {"title": "Convertible Codes: New Class of Codes for Efficient Conversion of Coded Data in Distributed Storage", "abstract": "Erasure codes are typically used in large-scale distributed storage systems to provide durability of data in the face of failures. In this setting, a set of k blocks to be stored is encoded using an [n, k] code to generate n blocks that are then stored on different storage nodes. A recent work by Kadekodi et al. [Kadekodi et al., 2019] shows that the failure rate of storage devices vary significantly over time, and that changing the rate of the code (via a change in the parameters n and k) in response to such variations provides significant reduction in storage space requirement. However, the resource overhead of realizing such a change in the code rate on already encoded data in traditional codes is prohibitively high. \nMotivated by this application, in this work we first present a new framework to formalize the notion of code conversion - the process of converting data encoded with an [n^I, k^I] code into data encoded with an [n^F, k^F] code while maintaining desired decodability properties, such as the maximum-distance-separable (MDS) property. We then introduce convertible codes, a new class of code pairs that allow for code conversions in a resource-efficient manner. For an important parameter regime (which we call the merge regime) along with the widely used linearity and MDS decodability constraint, we prove tight bounds on the number of nodes accessed during code conversion. In particular, our achievability result is an explicit construction of MDS convertible codes that are optimal for all parameter values in the merge regime albeit with a high field size. We then present explicit low-field-size constructions of optimal MDS convertible codes for a broad range of parameters in the merge regime. Our results thus show that it is indeed possible to achieve code conversions with significantly lesser resources as compared to the default approach of re-encoding.", "year": 2020, "ssId": "26f427d2b27828f2893e95344342570699e9c589", "arXivId": null, "link": null, "openAccess": false, "authors": ["Francisco Maturana", "K. V. Rashmi"]}}
{"id": "d72a1579074a1a2bc500f257474144b1957d5166", "content": {"title": "Learning-Based Coded Computation", "abstract": "Recent advances have shown the potential for coded computation to impart resilience against slowdowns and failures that occur in distributed computing systems. However, existing coded computation approaches are either unable to support non-linear computations, or can only support a limited subset of non-linear computations while requiring high resource overhead. In this work, we propose a learning-based coded computation framework to overcome the challenges of performing coded computation for general non-linear functions. We show that careful use of machine learning within the coded computation framework can extend the reach of coded computation to imparting resilience to more general non-linear computations. We showcase the applicability of learning-based coded computation to neural network inference, a major workload in production services. Our evaluation results show that learning-based coded computation enables accurate reconstruction of unavailable results from widely deployed neural networks for a variety of inference tasks such as image classification, speech recognition, and object localization. We implement our proposed approach atop an open-source prediction serving system and show its promise in alleviating slowdowns that occur in neural network inference. These results indicate the potential for learning-based approaches to open new doors for the use of coded computation for broader, non-linear computations.", "year": 2020, "ssId": "d72a1579074a1a2bc500f257474144b1957d5166", "arXivId": null, "link": null, "openAccess": false, "authors": ["J. Kosaian", "K. V. Rashmi", "S. Venkataraman"]}}
{"id": "04b91791225a4f86b0715b41c6f56c00c197d810", "content": {"title": "Bandwidth Cost of Code Conversions in Distributed Storage: Fundamental Limits and Optimal Constructions", "abstract": "In distributed storage systems, an [$n, k$] code encodes $k$ message symbols into $n$ codeword symbols which are then stored on $n$ nodes in the system. Recent work has shown that significant savings in storage space can be obtained by tuning $n$ and $k$ to variations in device failure rates. Such tuning necessitates code conversion: the process of converting data encoded under an [$n^{I}, k^{I}$] code to its equivalent under an [$n^{F}, k^{F}$] code. The default approach for code conversion places significant burden on system resources. Convertible codes are a recently proposed class of codes for enabling resource-efficient conversions. Existing work on convertible codes has focused on minimizing access cost, i.e., the number of nodes accessed during conversion. Bandwidth, which corresponds to the amount of data read and transferred, is another important resource to optimize during conversions. In this paper, we initiate the study on the fundamental limits on conversion bandwidth and present constructions for conversion-bandwidth optimal convertible codes. First, we model the code conversion problem using information flow graphs with variable capacity edges. Second, focusing on MDS codes and an important subclass of convertible codes, we derive a lower bound on conversion bandwidth. The derived bound shows that conversion bandwidth can be significantly reduced even in regimes where access cost of conversion cannot be reduced. Third, we present an explicit construction for MDS convertible codes which match this lower bound and are thus conversion-bandwidth optimal.", "year": 2020, "ssId": "04b91791225a4f86b0715b41c6f56c00c197d810", "arXivId": "2008.12707", "link": "https://arxiv.org/pdf/2008.12707.pdf", "openAccess": true, "authors": ["Francisco Maturana", "K. V. Rashmi"]}}
{"id": "d89f4534d1a87005cdf470ec5d8154998d5abdc7", "content": {"title": "Parity Models: A General Framework for Coding-Based Resilience in ML Inference", "abstract": "Machine learning models are becoming the primary workhorses for many applications. Production services deploy models through prediction serving systems that take in queries and return predictions by performing inference on machine learning models. In order to scale to high query rates, prediction serving systems are run on many machines in cluster settings, and thus are prone to slowdowns and failures that inflate tail latency and cause violations of strict latency targets. Current approaches to reducing tail latency are inadequate for the latency targets of prediction serving, incur high resource overhead, or are inapplicable to the computations performed during inference. \nWe present ParM, a novel, general framework for making use of ideas from erasure coding and machine learning to achieve low-latency, resource-efficient resilience to slowdowns and failures in prediction serving systems. ParM encodes multiple queries together into a single parity query and performs inference on the parity query using a parity model. A decoder uses the output of a parity model to reconstruct approximations of unavailable predictions. ParM uses neural networks to learn parity models that enable simple, fast encoders and decoders to reconstruct unavailable predictions for a variety of inference tasks such as image classification, speech recognition, and object localization. We build ParM atop an open-source prediction serving system and through extensive evaluation show that ParM improves overall accuracy in the face of unavailability with low latency while using 2-4$\\times$ less additional resources than replication-based approaches. ParM reduces the gap between 99.9th percentile and median latency by up to $3.5\\times$ compared to approaches that use an equal amount of resources, while maintaining the same median.", "year": 2019, "ssId": "d89f4534d1a87005cdf470ec5d8154998d5abdc7", "arXivId": "1905.00863", "link": "https://arxiv.org/pdf/1905.00863.pdf", "openAccess": true, "authors": ["J. Kosaian", "K. V. Rashmi", "S. Venkataraman"]}}
{"id": "c5ed3d1a2ce418610a6fc9b5520a4f845279969a", "content": {"title": "Parity models: erasure-coded resilience for prediction serving systems", "abstract": "Machine learning models are becoming the primary work-horses for many applications. Services deploy models through prediction serving systems that take in queries and return predictions by performing inference on models. Prediction serving systems are commonly run on many machines in cluster settings, and thus are prone to slowdowns and failures that inflate tail latency. Erasure coding is a popular technique for achieving resource-efficient resilience to data unavailability in storage and communication systems. However, existing approaches for imparting erasure-coded resilience to distributed computation apply only to a severely limited class of functions, precluding their use for many serving workloads, such as neural network inference. We introduce parity models, a new approach for enabling erasure-coded resilience in prediction serving systems. A parity model is a neural network trained to transform erasure-coded queries into a form that enables a decoder to reconstruct slow or failed predictions. We implement parity models in ParM, a prediction serving system that makes use of erasure-coded resilience. ParM encodes multiple queries into a \"parity query,\" performs inference over parity queries using parity models, and decodes approximations of unavailable predictions by using the output of a parity model. We showcase the applicability of parity models to image classification, speech recognition, and object localization tasks. Using parity models, ParM reduces the gap between 99.9th percentile and median latency by up to 3.5X, while maintaining the same median. These results display the potential of parity models to unlock a new avenue to imparting resource-efficient resilience to prediction serving systems.", "year": 2019, "ssId": "c5ed3d1a2ce418610a6fc9b5520a4f845279969a", "arXivId": null, "link": null, "openAccess": false, "authors": ["J. Kosaian", "K. V. Rashmi", "S. Venkataraman"]}}
{"id": "a8ea980b63deaf1404cd9f539a575b4e7135466e", "content": {"title": "Queries Predictions Encoder Decoder Parity ModelModel Instance k Model Instance 1 Frontend Query 1 Query k Parity QueryPredictio", "abstract": "Machine learning models are becoming the primary workhorses formany applications. Services deploymodels through prediction serving systems that take in queries and return predictions by performing inference on models. Prediction serving systems are commonly run on many machines in cluster settings, and thus are prone to slowdowns and failures that inflate tail latency. Erasure coding is a popular technique for achieving resource-efficient resilience to data unavailability in storage and communication systems. However, existing approaches for imparting erasure-coded resilience to distributed computation apply only to a severely limited class of functions, precluding their use for many serving workloads, such as neural network inference. We introduce parity models, a new approach for enabling erasure-coded resilience in prediction serving systems. A parity model is a neural network trained to transform erasurecoded queries into a form that enables a decoder to reconstruct slow or failed predictions. We implement parity models in ParM, a prediction serving system that makes use of erasure-coded resilience. ParM encodes multiple queries into a \u201cparity query,\u201d performs inference over parity queries using parity models, and decodes approximations of unavailable predictions by using the output of a parity model. We showcase the applicability of parity models to image classification, speech recognition, and object localization tasks. Using parity models, ParM reduces the gap between 99.9th percentile and median latency by up to 3.5\u00d7, while maintaining the same median. These results display the potential of parity models to unlock a new avenue to imparting resourceefficient resilience to prediction serving systems. Jack Kosaian is supported by an SOSP 2019 student scholarship from the ACM Special Interest Group in Operating Systems. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP \u201919, October 27\u201330, 2019, Huntsville, ON, Canada \u00a9 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6873-5/19/10. . . $15.00 https://doi.org/10.1145/3341301.3359654 CCS Concepts \u2022 Computer systems organization \u2192 Redundancy; Reliability.", "year": 2019, "ssId": "a8ea980b63deaf1404cd9f539a575b4e7135466e", "arXivId": null, "link": null, "openAccess": false, "authors": ["J. Kosaian", "K. V. Rashmi", "S. Venkataraman"]}}
{"id": "3675958405f3ad1633d565efa36b4eb3004bcf59", "content": {"title": "Vantage: optimizing video upload for time-shifted viewing of social live streams", "abstract": "Social live video streaming (SLVS) applications are becoming increasingly popular with the rise of platforms such as Facebook-Live, YouTube-Live, Twitch and Periscope. A key characteristic that differentiates this new class of applications from traditional live streaming is that these live streams are watched by viewers at different delays; while some viewers watch a live stream in real-time, others view the content in a time-shifted manner at different delays. In the presence of variability in the upload bandwidth, which is typical in mobile environments, existing solutions silo viewers into either receiving low latency video at a lower quality or a higher quality video with a significant delay penalty, without accounting for the presence of diverse time-shifted viewers. In this paper, we present Vantage, a live-streaming upload solution that improves the overall quality of experience for diverse time-shifted viewers by using selective quality-enhancing retransmissions in addition to real-time frames, optimizing the encoding schedules to balance the allocation of the available bandwidth between the two. Our evaluation using real-world mobile network traces shows that Vantage can provide high quality simultaneously for both low-latency and delayed viewing. For delayed viewing, Vantage achieves an average improvement of 19.9% over real-time optimized video streaming techniques across all the network traces and test videos, with observed gains of up to 42.9%. These benefits come at the cost of an average drop in real-time quality of 3.3%, with a maximum drop of 7.1%. This represents a significant performance improvement over current techniques used for SLVS applications, which primarily optimize the video upload for real-time viewing.", "year": 2019, "ssId": "3675958405f3ad1633d565efa36b4eb3004bcf59", "arXivId": null, "link": null, "openAccess": false, "authors": ["Devdeep Ray", "J. Kosaian", "K. V. Rashmi", "S. Seshan"]}}
{"id": "163a67b5b0371035fa6e0f88b36ba97a32e735bc", "content": {"title": "Convertible Codes: Efficient Conversion of Coded Data in Distributed Storage", "abstract": "Large-scale distributed storage systems typically use erasure codes to provide durability of data in the face of failures. A set of $k$ blocks to be stored is encoded using an $[n, k]$ code to generate $n$ blocks that are then stored on different storage nodes. The redundancy configuration is chosen based on the failure rates of storage devices, and is typically kept constant. However, a recent work by Kadekodi et al. shows that the failure rate of storage devices vary significantly over time, and that adapting the redundancy configuration in response to such variations provides significant benefits. Converting the redundancy configuration of already encoded data by re-encoding requires significant overhead on resources such as accesses, device IO, network bandwidth, and compute cycles. \nIn this work, we first present a framework to formalize the notion of code conversion: the process of converting data encoded with an $[n^I, k^I]$ code into data encoded with an $[n^F, k^F]$ code while maintaining desired decodability properties, such as the maximum-distance-separable (MDS) property. We then introduce convertible codes, a new class of codes that allow for code conversions in a resource-efficient manner. For an important parameter regime (which we call the merge regime) along with the linearity and MDS decodability constraint, we prove tight bounds on the number of nodes accessed during code conversion. In particular, our achievability result is an explicit construction of MDS convertible codes that are optimal for all parameter values in the merge regime albeit with a high field size. We then present explicit low-field-size constructions of optimal MDS convertible codes for a broad range of parameters in the merge regime. Our results thus show that it is indeed possible to achieve code conversions with significantly lesser resources as compared to re-encoding.", "year": 2019, "ssId": "163a67b5b0371035fa6e0f88b36ba97a32e735bc", "arXivId": "1907.13119", "link": "https://arxiv.org/pdf/1907.13119.pdf", "openAccess": true, "authors": ["Francisco Maturana", "K. V. Rashmi"]}}
{"id": "aae3d5e24d02ae538030ef3995a86118c5323ae1", "content": {"title": "Cluster storage systems gotta have HeART: improving storage efficiency by exploiting disk-reliability heterogeneity", "abstract": "ions to Enhance Programmability, Portability,", "year": 2019, "ssId": "aae3d5e24d02ae538030ef3995a86118c5323ae1", "arXivId": null, "link": null, "openAccess": false, "authors": ["Saurabh Kadekodi", "K. V. Rashmi", "G. Ganger"]}}
