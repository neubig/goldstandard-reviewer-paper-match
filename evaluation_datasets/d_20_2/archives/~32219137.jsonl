{"id": "3c37b9ec2ff1828877575acc600b73c3bcde138f", "content": {"title": "Modeling Attrition in Recommender Systems with Departing Bandits", "abstract": "Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system in\ufb02uences the rewards accrued, but not the length of interac- tion. However, in real-world systems, dissatis\ufb01ed users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a \ufb01nite set of user types , and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward proba- bility. Each user\u2019s type is initially unknown and can only be inferred through their response to recommendations. More- over, if a user is dissatis\ufb01ed with their recommendation, they might depart the system. We \ufb01rst address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two types. While naive approaches cannot handle this setting, we provide an ef\ufb01cient learning algorithm that achieves \u02dc O ( \u221a T ) regret, where T is the number of users.", "year": 2022, "ssId": "3c37b9ec2ff1828877575acc600b73c3bcde138f", "arXivId": "2203.13423", "link": "https://arxiv.org/pdf/2203.13423.pdf", "openAccess": true, "authors": ["Omer Ben-Porat", "Lee Cohen", "Liu Leqi", "Zachary Chase Lipton", "Y. Mansour"]}}
{"id": "e6beab7c192d7fb04c8bfb0886464fd719cd3421", "content": {"title": "L EVERAGING U NLABELED D ATA TO P REDICT O UT - OF -D ISTRIBUTION P ERFORMANCE", "abstract": "Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model\u2019s confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance 2\u20134\u02c6 more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works.", "year": 2022, "ssId": "e6beab7c192d7fb04c8bfb0886464fd719cd3421", "arXivId": null, "link": null, "openAccess": false, "authors": ["S. Garg", "Sivaraman Balakrishnan", "Zachary Chase Lipton", "Behnam Neyshabur", "Hanie Sedghi"]}}
{"id": "4218563e1fe927440e00bf0abe5cb1e037deaf71", "content": {"title": "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance", "abstract": "Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model\u2019s confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance 2\u20134\u02c6 more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works.", "year": 2022, "ssId": "4218563e1fe927440e00bf0abe5cb1e037deaf71", "arXivId": "2201.04234", "link": "https://arxiv.org/pdf/2201.04234.pdf", "openAccess": true, "authors": ["S. Garg", "Sivaraman Balakrishnan", "Zachary Chase Lipton", "Behnam Neyshabur", "Hanie Sedghi"]}}
{"id": "559fdae33f0b7733b80a7dbcb902c79598a0d26e", "content": {"title": "Can Transformers be Strong Treatment Effect Estimators?", "abstract": "In this paper, we develop a general framework based on the Transformer architecture to address a variety of challenging treatment effect estimation (TEE) problems. Our methods are applicable both when covariates are tabular and when they consist of sequences (e.g., in text), and can handle discrete, continuous, structured, or dosage-associated treatments. While Transformers have already emerged as dominant methods for diverse domains, including natural language and computer vision, our experiments with Transformers as Treatment Effect Estimators (TransTEE) demonstrate that these inductive biases are also effective on the sorts of estimation problems and datasets that arise in research aimed at estimating causal effects. Moreover, we propose a propensity score network that is trained with TransTEE in an adversarial manner to promote independence between covariates and treatments to further address selection bias. Through extensive experiments, we show that TransTEE significantly outperforms competitive baselines with greater parameter efficiency over a wide range of benchmarks and settings.", "year": 2022, "ssId": "559fdae33f0b7733b80a7dbcb902c79598a0d26e", "arXivId": "2202.01336", "link": "https://arxiv.org/pdf/2202.01336.pdf", "openAccess": true, "authors": ["Yi-Fan Zhang", "Hanlin Zhang", "Zachary Chase Lipton", "Li Erran Li", "Eric P. Xing"]}}
{"id": "49775f20431c4a605c5dcc7111c9fe785bf00c62", "content": {"title": "On the Convergence and Optimality of Policy Gradient for Markov Coherent Risk", "abstract": "In order to model risk aversion in reinforcement learning, an emerging line of research adapts familiar algorithms to optimize coherent risk functionals, a class that includes conditional valueat-risk (CVaR). Because optimizing the coherent risk is difficult in Markov decision processes, recent work tends to focus on the Markov coherent risk (MCR), a time-consistent surrogate. While, policy gradient (PG) updates have been derived for this objective, it remains unclear (i) whether PG finds a global optimum for MCR; (ii) how to estimate the gradient in a tractable manner. In this paper, we demonstrate that, in general, MCR objectives (unlike the expected return) are not gradient dominated and that stationary points are not, in general, guaranteed to be globally optimal. Moreover, we present a tight upper bound on the suboptimality of the learned policy, characterizing its dependence on the nonlinearity of the objective and the degree of risk aversion. Addressing (ii), we propose a practical implementation of PG that uses state distribution reweighting to overcome previous limitations. Through experiments, we demonstrate that when the optimality gap is small, PG can learn risk-sensitive policies. However, we find that instances with large suboptimality gaps are abundant and easy to construct, outlining an important challenge for future research.", "year": 2021, "ssId": "49775f20431c4a605c5dcc7111c9fe785bf00c62", "arXivId": "2103.02827", "link": "https://arxiv.org/pdf/2103.02827.pdf", "openAccess": true, "authors": ["Audrey Huang", "Liu Leqi", "Zachary Chase Lipton", "K. Azizzadenesheli"]}}
{"id": "bd6c708a535af588d90025a0e6cf17407bf65434", "content": {"title": "Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations", "abstract": "In attempts to \u201cexplain\u201d predictions of machine learning models, researchers have proposed hundreds of techniques for attributing predictions to features that are deemed important. While these attributions are often claimed to hold the potential to improve human \u201cunderstanding\u201d of the models, surprisingly little work explicitly evaluates progress towards this aspiration. In this paper, we conduct a crowdsourcing study, where participants interact with deception detection models that have been trained to distinguish between genuine and fake hotel reviews. They are challenged both to simulate the model on fresh reviews, and to edit reviews with the goal of lowering the probability of the originally predicted class. Successful manipulations would lead to an adversarial example. During the training (but not the test) phase, input spans are highlighted to communicate salience. Through our evaluation, we observe that for a linear bag-of-words model, participants with access to the feature coefficients during training are able to cause a larger reduction in model confidence in the testing phase when compared to the no-explanation control. For the BERT-based classifier, popular local explanations do not improve their ability to reduce the model confidence over the no-explanation case. Remarkably, when the explanation for the BERT model is given by the (global) attributions of a linear model trained to imitate the BERT model, people can effectively manipulate the model.", "year": 2021, "ssId": "bd6c708a535af588d90025a0e6cf17407bf65434", "arXivId": "2112.09669", "link": "https://arxiv.org/pdf/2112.09669.pdf", "openAccess": true, "authors": ["Siddhant Arora", "Danish Pruthi", "N. Sadeh", "William W. Cohen", "Zachary Chase Lipton", "Graham Neubig"]}}
{"id": "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd", "content": {"title": "Parametric Complexity Bounds for Approximating PDEs with Neural Networks", "abstract": "Recent experiments have shown that deep networks can approximate solutions to high-dimensional PDEs, seemingly escaping the curse of dimensionality. However, questions regarding the theoretical basis for such approximations, including the required network size, remain open. In this paper, we investigate the representational power of neural networks for approximating solutions to linear elliptic PDEs with Dirichlet boundary conditions. We prove that when a PDE\u2019s coefficients are representable by small neural networks, the parameters required to approximate its solution scale polynomially with the input dimension d and proportionally to the parameter counts of the coefficient networks. To this we end, we develop a proof technique that simulates gradient descent (in an appropriate Hilbert space) by growing a neural network architecture whose iterates each participate as sub-networks in their (slightly larger) successors, and converge to the solution of the PDE. We bound the size of the solution, showing a polynomial dependence on d and no dependence on the volume of the domain.", "year": 2021, "ssId": "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd", "arXivId": "2103.02138", "link": "https://arxiv.org/pdf/2103.02138.pdf", "openAccess": true, "authors": ["Tanya Marwah", "Zachary Chase Lipton", "Andrej Risteski"]}}
{"id": "d864944df8e765d597484ace12dbc3ac99e950a9", "content": {"title": "On Proximal Policy Optimization's Heavy-tailed Gradients", "abstract": "Modern policy gradient algorithms such as Proximal Policy Optimization (PPO) rely on an arsenal of heuristics, including loss clipping and gradient clipping, to ensure successful learning. These heuristics are reminiscent of techniques from robust statistics, commonly used for estimation in outlier-rich (\u201cheavy-tailed\u201d) regimes. In this paper, we present a detailed empirical study to characterize the heavy-tailed nature of the gradients of the PPO surrogate reward function. We demonstrate that the gradients, especially for the actor network, exhibit pronounced heavy-tailedness and that it increases as the agent\u2019s policy diverges from the behavioral policy (i.e., as the agent goes further off policy). Further examination implicates the likelihood ratios and advantages in the surrogate reward as the main sources of the observed heavy-tailedness. We then highlight issues arising due to the heavy-tailed nature of the gradients. In this light, we study the effects of the standard PPO clipping heuristics, demonstrating that these tricks primarily serve to offset heavytailedness in gradients. Thus motivated, we propose incorporating GMOM, a high-dimensional robust estimator, into PPO as a substitute for three clipping tricks. Despite requiring less hyperparameter tuning, our method matches the performance of PPO (with all heuristics enabled) on a battery of MuJoCo continuous control tasks.", "year": 2021, "ssId": "d864944df8e765d597484ace12dbc3ac99e950a9", "arXivId": "2102.10264", "link": "https://arxiv.org/pdf/2102.10264.pdf", "openAccess": true, "authors": ["S. Garg", "Joshua Zhanson", "Emilio Parisotto", "A. Prasad", "J. Z. Kolter", "Sivaraman Balakrishnan", "Zachary Chase Lipton", "R. Salakhutdinov", "Pradeep Ravikumar"]}}
{"id": "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d", "content": {"title": "When curation becomes creation", "abstract": "Algorithms, microcontent, and the vanishing distinction between platforms and creators.", "year": 2021, "ssId": "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Liu Leqi", "Dylan Hadfield-Menell", "Zachary Chase Lipton"]}}
{"id": "c5950fa3ee124cf2dcb8783db6f582f49170fb45", "content": {"title": "RATT: Leveraging Unlabeled Data to Guarantee Generalization", "abstract": "To assess generalization, machine learning scientists typically either (i) bound the generalization gap and then (after training) plug in the empirical risk to obtain a bound on the true risk; or (ii) validate empirically on holdout data. However, (i) typically yields vacuous guarantees for overparameterized models; and (ii) shrinks the training set and its guarantee erodes with each reuse of the holdout set. In this paper, we leverage unlabeled data to produce generalization bounds. After augmenting our (labeled) training set with randomly labeled data, we train in the standard fashion. Whenever classifiers achieve low error on the clean data but high error on the random data, our bound ensures that the true risk is low. We prove that our bound is valid for 0-1 empirical risk minimization and with linear classifiers trained by gradient descent. Our approach is especially useful in conjunction with deep learning due to the early learning phenomenon whereby networks fit true labels before noisy labels but requires one intuitive assumption. Empirically, on canonical computer vision and NLP tasks, our bound provides non-vacuous generalization guarantees that track actual performance closely. This work enables practitioners to certify generalization even when (labeled) holdout data is unavailable and provides insights into the relationship between random label noise and generalization.", "year": 2021, "ssId": "c5950fa3ee124cf2dcb8783db6f582f49170fb45", "arXivId": "2105.00303", "link": "https://arxiv.org/pdf/2105.00303.pdf", "openAccess": true, "authors": ["S. Garg", "Sivaraman Balakrishnan", "J. Z. Kolter", "Zachary Chase Lipton"]}}
{"id": "b7731a9b9142a6deb132e99bc55ddbe458a537a6", "content": {"title": "Efficient Online Estimation of Causal Effects by Deciding What to Observe", "abstract": "Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we aim to estimate any functional of a probabilistic model (e.g., a causal effect) as efficiently as possible, by deciding, at each time, which data source to query. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by current estimates of the moments. We propose two selection strategies: (1) explore-then-commit (OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources may include subsets of mediators, confounders, and instrumental variables.", "year": 2021, "ssId": "b7731a9b9142a6deb132e99bc55ddbe458a537a6", "arXivId": "2108.09265", "link": "https://arxiv.org/pdf/2108.09265.pdf", "openAccess": true, "authors": ["Shantanu Gupta", "Zachary Chase Lipton", "David Benjamin Childers"]}}
{"id": "0fcdf20477f907aa50578876226f5fabf5e074ea", "content": {"title": "Correcting Exposure Bias for Link Recommendation", "abstract": "Link prediction methods are frequently applied in recommender systems, e.g., to suggest citations for academic papers or friends in social networks. However, exposure bias can arise when users are systematically underexposed to certain relevant items. For example, in citation networks, authors might be more likely to encounter papers from their own field and thus cite them preferentially. This bias can propagate through naively trained link predictors, leading to both biased evaluation and high generalization error (as assessed by true relevance). Moreover, this bias can be exacerbated by feedback loops. We propose estimators that leverage known exposure probabilities to mitigate this bias and consequent feedback loops. Next, we provide a loss function for learning the exposure probabilities from data. Finally, experiments on semi-synthetic data based on real-world citation networks, show that our methods reliably identify (truly) relevant citations. Additionally, our methods lead to greater diversity in the recommended papers\u2019 fields of study. The code is available at github.com/shantanu95/ exposure-bias-link-rec.", "year": 2021, "ssId": "0fcdf20477f907aa50578876226f5fabf5e074ea", "arXivId": "2106.07041", "link": "https://arxiv.org/pdf/2106.07041.pdf", "openAccess": true, "authors": ["Shantanu Gupta", "Hao Wang", "Zachary Chase Lipton", "Bernie Wang"]}}
{"id": "c5e4eafd85949e6aac9d8e98d5e03b2acf444046", "content": {"title": "On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study", "abstract": "In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC\u2019s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research.", "year": 2021, "ssId": "c5e4eafd85949e6aac9d8e98d5e03b2acf444046", "arXivId": "2106.00872", "link": "https://arxiv.org/pdf/2106.00872.pdf", "openAccess": true, "authors": ["Divyansh Kaushik", "Douwe Kiela", "Zachary Chase Lipton", "Wen-tau Yih"]}}
{"id": "49db57f300b270f16cbcb1891ca39e16981d42b5", "content": {"title": "An Open Repository of Real-Time COVID-19 Indicators", "abstract": "The COVID-19 pandemic presented enormous data challenges in the United States. Policy makers, epidemiological modelers, and health researchers all require up-to-date data on the pandemic and relevant public behavior, ideally at fine spatial and temporal resolution. The COVIDcast API is our attempt to fill this need: operational since April 2020, it provides open access to both traditional public health surveillance signals (cases, deaths, and hospitalizations) and many auxiliary indicators of COVID- 19 activity, such as signals extracted from de-identified medical claims data, massive online surveys, cell phone mobility data, and internet search trends. These are available at a fine geographic resolution (mostly at the county level) and are updated daily. The COVIDcast API also tracks all revisions to historical data, allowing modelers to account for the frequent revisions and backfill that are common for many public health data sources. All of the data is available in a common format through the API and accompanying R and Python software packages. This paper describes the data sources and signals, and provides examples demonstrating that the auxiliary signals in the COVIDcast API present information relevant to tracking COVID activity, augmenting traditional public health reporting and empowering research and decision-making.", "year": 2021, "ssId": "49db57f300b270f16cbcb1891ca39e16981d42b5", "arXivId": null, "link": null, "openAccess": false, "authors": ["A. Reinhart", "L. Brooks", "M. Jahja", "A. Rumack", "J. Tang", "W. Al Saeed", "T. Arnold", "A. Basu", "J. Bien", "A. Cabrera", "A. Chin", "E. J. Chua", "B. Clark", "N. DeFries", "J. Forlizzi", "S. Gratzl", "A. Green", "G. Haff", "R. Han", "A. Hu", "S. Hyun", "A. Joshi", "J. Kim", "A. Kuznetsov", "W. La Motte-Kerr", "Y. Lee", "K. Lee", "Zachary Chase Lipton", "M. Liu", "L. Mackey", "Kathryn Mazaitis", "D. McDonald", "B. Narasimhan", "N. L. Oliveira", "P. Patil", "Adam Perer", "C. Politsch", "S. Rajanala", "D. Rucker", "N. Shah", "V. Shankar", "J. Sharpnack", "D. Shemetov", "N. Simon", "Vikram Srivastava", "S. Tan", "R. Tibshirani", "Tuzhilin"]}}
{"id": "bd17620c6cb5ca97ef773499223d1509d123745f", "content": {"title": "Dive into Deep Learning", "abstract": "This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.", "year": 2021, "ssId": "bd17620c6cb5ca97ef773499223d1509d123745f", "arXivId": "2106.11342", "link": "https://arxiv.org/pdf/2106.11342.pdf", "openAccess": true, "authors": ["Aston Zhang", "Zachary Chase Lipton", "Mu Li", "Alex Smola"]}}
{"id": "f07c5c540233b22f0ca154c80c713e2aed3c9606", "content": {"title": "Symbolic Music Generation with Transformer-GANs", "abstract": "Autoregressive models using Transformers have emerged as the dominant approach for music generation with the goal of synthesizing minute-long compositions that exhibit largescale musical structure. These models are commonly trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. Unfortunately, the quality of samples from these models tends to degrade significantly for long sequences, a phenomenon attributed to exposure bias. Fortunately, we are able to detect these failures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our exploration of adversarial losses to complement the NLL objective. We use a pre-trained Span-BERT model for the discriminator of the GAN, which in our experiments helped with training stability. We use the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. This makes discrete sequences amenable to optimization in GANs. In addition, we break the sequences into smaller chunks to ensure that we stay within a given memory budget. We demonstrate via human evaluations and a new discriminative metric that the music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Music Transformer, and other GANs used for sequence generation. 57% of people prefer music generated via our approach while 43% prefer Music Transformer.", "year": 2021, "ssId": "f07c5c540233b22f0ca154c80c713e2aed3c9606", "arXivId": null, "link": null, "openAccess": false, "authors": ["Aashiq Muhamed", "Liang Li", "Xingjian Shi", "Suri Yaddanapudi", "Wayne Chi", "Dylan Jackson", "Rahul Suresh", "Zachary Chase Lipton", "Alex Smola"]}}
{"id": "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7", "content": {"title": "Off-Policy Risk Assessment in Contextual Bandits", "abstract": "Even when unable to run experiments, practitioners can evaluate prospective policies, using previously logged data. However, while the bandits literature has adopted a diverse set of objectives, most research on off-policy evaluation to date focuses on the expected reward. In this paper, we introduce Lipschitz risk functionals, a broad class of objectives that subsumes conditional value-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT risks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework that first estimates a target policy\u2019s CDF and then generates plugin estimates for any collection of Lipschitz risks, providing finite sample guarantees that hold simultaneously over the entire class. We instantiate OPRA with both importance sampling and doubly robust estimators. Our primary theoretical contributions are (i) the first uniform concentration inequalities for both CDF estimators in contextual bandits and (ii) error bounds on our Lipschitz risk estimates, which all converge at a rate of O(1/ p n).", "year": 2021, "ssId": "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7", "arXivId": "2104.08977", "link": "https://arxiv.org/pdf/2104.08977.pdf", "openAccess": true, "authors": ["Audrey Huang", "Liu Leqi", "Zachary Chase Lipton", "K. Azizzadenesheli"]}}
{"id": "04a94c15fec43e7563d58be697246a0dd6c57021", "content": {"title": "When Curation Becomes Creation: Algorithms, Microcontent, and the Vanishing Distinction between Platforms and Creators", "abstract": "Ever since social activity on the Internet began migrating from the wilds of the open web to the walled gardens erected by so-called platforms (think Myspace, Facebook, Twitter, YouTube, or TikTok), debates have raged about the responsibilities that these platforms ought to bear. And yet, despite intense scrutiny from the news media and grassroots movements of outraged users, platforms continue to operate, from a legal standpoint, on the friendliest terms. You might say that today\u2019s platforms enjoy a \u201chave your cake, eat it too, and here\u2019s a side of ice cream\u201d deal. They simultaneously benefit from: (1) broad discretion to organize (and censor) content however they choose; (2) powerful algorithms for curating a practically limitless supply of user-posted microcontent according to whatever ends they wish; and (3) absolution from almost any liability associated with that content. This favorable regulatory environment results from the current legal framework, which distinguishes between intermediaries (e.g., platforms) and content providers. This distinction is ill-adapted to the modern social media landscape, where platforms deploy powerful data-driven algorithms (so-called AI) to play an increasingly active role in shaping what people see and where users supply disconnected bits of raw content (tweets, photos, etc.) as fodder. Specifically, under Section 230 of the Telecommunications Act of 1996, \u201cinteractive computer services\u201d are shielded from liability for information produced by \u201cinformation content providers.\u201d While this provision was originally intended to protect telecommunications companies and Internet service providers from liability for content that merely passed through their plumbing [2], the designation now shelters services such as Facebook, Twitter, and YouTube, which actively shape user experiences. Excepting obligations to take down specific categories of content (e.g., child pornography and copyright violations), today\u2019s platforms have license to monetize whatever content they like, moderate if and when it aligns with their corporate objectives, and curate their content however they wish.", "year": 2021, "ssId": "04a94c15fec43e7563d58be697246a0dd6c57021", "arXivId": "2107.00441", "link": "https://arxiv.org/pdf/2107.00441.pdf", "openAccess": true, "authors": ["Liu Leqi", "Dylan Hadfield-Menell", "Zachary Chase Lipton"]}}
{"id": "bfb13c6889626e833bf449fdb361d186467919af", "content": {"title": "Practical Benefits of Feature Feedback Under Distribution Shift", "abstract": "In attempts to develop sample-efficient algorithms, researcher have explored myriad mechanisms for collecting and exploiting feature feedback, auxiliary annotations provided for training (but not test) instances that highlight salient evidence. Examples include bounding boxes around objects and salient spans in text. Despite its intuitive appeal, feature feedback has not delivered significant gains in practical problems as assessed on iid holdout sets. However, recent works on counterfactually augmented data suggest an alternative benefit of supplemental annotations: lessening sensitivity to spurious patterns and consequently delivering gains in out-of-domain evaluations. Inspired by these findings, we hypothesize that while the numerous existing methods for incorporating feature feedback have delivered negligible in-sample gains, they may nevertheless generalize better out-of-domain. In experiments addressing sentiment analysis, we show that feature feedback methods perform significantly better on various natural out-of-domain datasets even absent differences on in-domain evaluation. By contrast, on natural language inference tasks, performance remains comparable. Finally, we compare those tasks where feature feedback does (and does not) help.1", "year": 2021, "ssId": "bfb13c6889626e833bf449fdb361d186467919af", "arXivId": "2110.07566", "link": "https://arxiv.org/pdf/2110.07566.pdf", "openAccess": true, "authors": ["Anurag Katakkar", "Weiqin Wang", "Clay H. Yoo", "Zachary Chase Lipton", "Divyansh Kaushik"]}}
{"id": "a6d505a6e46c15ef0d213b9a4349ce2f852be894", "content": {"title": "Mixture Proportion Estimation and PU Learning: A Modern Approach", "abstract": "Given only positive examples and unlabeled examples (from both positive and negative classes), we might hope nevertheless to estimate an accurate positiveversus-negative classifier. Formally, this task is broken down into two subtasks: (i) Mixture Proportion Estimation (MPE)\u2014determining the fraction of positive examples in the unlabeled data; and (ii) PU-learning\u2014given such an estimate, learning the desired positive-versus-negative classifier. Unfortunately, classical methods for both problems break down in high-dimensional settings. Meanwhile, recently proposed heuristics lack theoretical coherence and depend precariously on hyperparameter tuning. In this paper, we propose two simple techniques: Best Bin Estimation (BBE) (for MPE); and Conditional Value Ignoring Risk (CVIR), a simple objective for PU-learning. Both methods dominate previous approaches empirically, and for BBE, we establish formal guarantees that hold whenever we can train a model to cleanly separate out a small subset of positive examples. Our final algorithm (TED), alternates between the two procedures, significantly improving both our mixture proportion estimator and classifier1.", "year": 2021, "ssId": "a6d505a6e46c15ef0d213b9a4349ce2f852be894", "arXivId": "2111.00980", "link": "https://arxiv.org/pdf/2111.00980.pdf", "openAccess": true, "authors": ["S. Garg", "Yifan Wu", "Alexander J. Smola", "Sivaraman Balakrishnan", "Zachary Chase Lipton"]}}
