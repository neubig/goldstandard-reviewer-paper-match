{"id": "a45c3120c077994409093771077a2d16f77674c5", "content": {"title": "PARAMETER-EFFICIENT TRANSFER LEARNING", "abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches finetune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.1", "year": 2022, "ssId": "a45c3120c077994409093771077a2d16f77674c5", "arXivId": null, "link": null, "openAccess": false, "authors": ["Junxian He", "Chunting Zhou", "Graham Neubig"]}}
{"id": "f1b52bf723d7f5c4b68c8551c4d168ed1224f016", "content": {"title": "Characterization and phylogenetic analysis of the complete mitochondrial genome of Sinocyclocheilus wenshanensis (Cypriniformes: Cyprinidae)", "abstract": "Abstract Sinocyclocheilus wenshanensis is a cyprinid fish species endemic to Southwestern China. In this study, we first sequenced and characterized the complete mitochondrial genome (mitogenome) of S. wenshanensis by next-generation sequencing method. The entire length of mitogenome is 16,595 base pairs (bp), containing 13 protein-coding genes, two ribosomal RNA genes, 22 transfer RNA genes, and a control region. Its gene arrangement pattern was identical to other previously reported Sinocyclocheilus fishes. The overall base composition is 31.12% A, 16.63% G, 25.45% T, and 26.80% C, with AT content of 56.57%. Phylogenetic analysis using mitogenome of 26 Cyprinidae fishes showed that S. wenshanensis are closely related to S. aluensis and S. oxycephalus. This work would provide molecular information fundamental to future phylogenetic analyses among Sinocyclocheilus species.", "year": 2022, "ssId": "f1b52bf723d7f5c4b68c8551c4d168ed1224f016", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chunqing Li", "Fang-Hua Hu", "Junxian He", "Xutong Li", "Hong-fu Yang", "Wei-xian Li", "Shanyuan Chen", "Heng Xiao"]}}
{"id": "4b9795493a937b9034be9c26afab23f6dc751f62", "content": {"title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval", "abstract": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RETOMATON \u2013 retrieval automaton \u2013 which approximates the datastore search, based on (1) clustering of entries into \u201cstates\u201d, and (2) state transitions from previous entries. This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RETOMATON can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity, or alternatively saves up to 83% of the nearest neighbor searches over kNN-LM (Khandelwal et al., 2020), without hurting perplexity.", "year": 2022, "ssId": "4b9795493a937b9034be9c26afab23f6dc751f62", "arXivId": "2201.12431", "link": "https://arxiv.org/pdf/2201.12431.pdf", "openAccess": true, "authors": ["Uri Alon", "Frank F. Xu", "Junxian He", "Sudipta Sengupta", "D. Roth", "Graham Neubig"]}}
{"id": "43a87867fe6bf4eb920f97fc753be4b727308923", "content": {"title": "Towards a Unified View of Parameter-Efficient Transfer Learning", "abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches finetune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.1", "year": 2021, "ssId": "43a87867fe6bf4eb920f97fc753be4b727308923", "arXivId": "2110.04366", "link": "https://arxiv.org/pdf/2110.04366.pdf", "openAccess": true, "authors": ["Junxian He", "Chunting Zhou", "Xuezhe Ma", "Taylor Berg-Kirkpatrick", "Graham Neubig"]}}
{"id": "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d", "content": {"title": "Efficient Nearest Neighbor Language Models", "abstract": "Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.", "year": 2021, "ssId": "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d", "arXivId": "2109.04212", "link": "https://arxiv.org/pdf/2109.04212.pdf", "openAccess": true, "authors": ["Junxian He", "Graham Neubig", "Taylor Berg-Kirkpatrick"]}}
{"id": "d5ec188a5a39e504788c1fe33457eeb816a99f31", "content": {"title": "Dependency Induction Through the Lens of Visual Perception", "abstract": "Most previous work on grammar induction focuses on learning phrasal or dependency structure purely from text. However, because the signal provided by text alone is limited, recently introduced visually grounded syntax models make use of multimodal information leading to improved performance in constituency grammar induction. However, as compared to dependency grammars, constituency grammars do not provide a straightforward way to incorporate visual information without enforcing language-specific heuristics. In this paper, we propose an unsupervised grammar induction model that leverages word concreteness and a structural vision-based heuristic to jointly learn constituency-structure and dependency-structure grammars. Our experiments find that concreteness is a strong indicator for learning dependency grammars, improving the direct attachment score (DAS) by over 50% as compared to state-of-the-art models trained on pure text. Next, we propose an extension of our model that leverages both word concreteness and visual semantic role labels in constituency and dependency parsing. Our experiments show that the proposed extension outperforms the current state-of-the-art visually grounded models in constituency parsing even with a smaller grammar size.", "year": 2021, "ssId": "d5ec188a5a39e504788c1fe33457eeb816a99f31", "arXivId": "2109.09790", "link": "https://arxiv.org/pdf/2109.09790.pdf", "openAccess": true, "authors": ["Ruisi Su", "Shruti Rijhwani", "Hao Zhu", "Junxian He", "Xinyu Wang", "Yonatan Bisk", "Graham Neubig"]}}
{"id": "57e4074c588c0e27e4c0bc89f12512ccdb900d79", "content": {"title": "A Probabilistic Formulation of Unsupervised Text Style Transfer", "abstract": "We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.", "year": 2020, "ssId": "57e4074c588c0e27e4c0bc89f12512ccdb900d79", "arXivId": "2002.03912", "link": "https://arxiv.org/pdf/2002.03912.pdf", "openAccess": true, "authors": ["Junxian He", "Xinyi Wang", "Graham Neubig", "Taylor Berg-Kirkpatrick"]}}
{"id": "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b", "content": {"title": "The Source-Target Domain Mismatch Problem in Machine Translation", "abstract": "While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with self-training and by increasing the amount of target side monolingual data.", "year": 2019, "ssId": "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b", "arXivId": "1909.13151", "link": "https://arxiv.org/pdf/1909.13151.pdf", "openAccess": true, "authors": ["Jiajun Shen", "Peng-Jen Chen", "Matt Le", "Junxian He", "Jiatao Gu", "Myle Ott", "Michael Auli", "Marc'Aurelio Ranzato"]}}
{"id": "12442420adf1c36887fafd108f4b7f4fc822ae60", "content": {"title": "Revisiting Self-Training for Neural Sequence Generation", "abstract": "Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a \"noisy\" version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin.", "year": 2019, "ssId": "12442420adf1c36887fafd108f4b7f4fc822ae60", "arXivId": "1909.13788", "link": "https://arxiv.org/pdf/1909.13788.pdf", "openAccess": true, "authors": ["Junxian He", "Jiatao Gu", "Jiajun Shen", "Marc'Aurelio Ranzato"]}}
{"id": "4f7b108830de2e7964b6e1a89bf1c2da60140a34", "content": {"title": "A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text", "abstract": "When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.", "year": 2019, "ssId": "4f7b108830de2e7964b6e1a89bf1c2da60140a34", "arXivId": "1909.00868", "link": "https://arxiv.org/pdf/1909.00868.pdf", "openAccess": true, "authors": ["Bohan Li", "Junxian He", "Graham Neubig", "Taylor Berg-Kirkpatrick", "Yiming Yang"]}}
{"id": "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff", "content": {"title": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation", "abstract": "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, summarization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model architecture, inference, and learning processes are properly decomposed. Modules at a high concept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https://www.texar.io.", "year": 2019, "ssId": "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff", "arXivId": null, "link": null, "openAccess": false, "authors": ["Zhiting Hu", "Haoran Shi", "Bowen Tan", "Wentao Wang", "Zichao Yang", "Tiancheng Zhao", "Junxian He", "Lianhui Qin", "Di Wang", "Xuezhe Ma", "Zhengzhong Liu", "Xiaodan Liang", "Wangrong Zhu", "Devendra Singh Sachan", "E. Xing"]}}
{"id": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328", "content": {"title": "Choosing Transfer Languages for Cross-Lingual Learning", "abstract": "Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.", "year": 2019, "ssId": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328", "arXivId": "1905.12688", "link": "https://arxiv.org/pdf/1905.12688.pdf", "openAccess": true, "authors": ["Yu-Hsiang Lin", "Chian-Yu Chen", "Jean Lee", "Zirui Li", "Yuyan Zhang", "M. Xia", "Shruti Rijhwani", "Junxian He", "Zhisong Zhang", "Xuezhe Ma", "Antonios Anastasopoulos", "Patrick Littell", "Graham Neubig"]}}
{"id": "e785441f5ccd6e4e29b3123e61121df5c65b88f7", "content": {"title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders", "abstract": "The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as \"posterior collapse\" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.", "year": 2019, "ssId": "e785441f5ccd6e4e29b3123e61121df5c65b88f7", "arXivId": "1901.05534", "link": "https://arxiv.org/pdf/1901.05534.pdf", "openAccess": true, "authors": ["Junxian He", "Daniel Spokoyny", "Graham Neubig", "Taylor Berg-Kirkpatrick"]}}
{"id": "6b7f2f30840b0d72484784a15b3be670868a9f95", "content": {"title": "Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections", "abstract": "Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of source model and target model are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use English as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this dataset that are distant from English, our method yields an average of 5.2% absolute improvement on POS tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models.", "year": 2019, "ssId": "6b7f2f30840b0d72484784a15b3be670868a9f95", "arXivId": "1906.02656", "link": "https://arxiv.org/pdf/1906.02656.pdf", "openAccess": true, "authors": ["Junxian He", "Zhisong Zhang", "Taylor Berg-Kirkpatrick", "Graham Neubig"]}}
{"id": "2068825cabd94c951a0282ed731a8b8f2da1721c", "content": {"title": "StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing", "abstract": "Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.", "year": 2018, "ssId": "2068825cabd94c951a0282ed731a8b8f2da1721c", "arXivId": "1806.07832", "link": "https://arxiv.org/pdf/1806.07832.pdf", "openAccess": true, "authors": ["Pengcheng Yin", "Chunting Zhou", "Junxian He", "Graham Neubig"]}}
{"id": "3b563c16e9a918631d63a20027dad735b625625a", "content": {"title": "Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation", "abstract": "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing and generalization across different text generation applications. The toolkit emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit.", "year": 2018, "ssId": "3b563c16e9a918631d63a20027dad735b625625a", "arXivId": "1809.00794", "link": "https://arxiv.org/pdf/1809.00794.pdf", "openAccess": true, "authors": ["Zhiting Hu", "Haoran Shi", "Zichao Yang", "Bowen Tan", "Tiancheng Zhao", "Junxian He", "Wentao Wang", "Xingjiang Yu", "Lianhui Qin", "Di Wang", "Xuezhe Ma", "Zhengzhong Liu", "Xiaodan Liang", "Wanrong Zhu", "Devendra Singh Sachan", "E. Xing"]}}
{"id": "a03675379685d88c727bc985a323cc71d06f2514", "content": {"title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "abstract": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "year": 2018, "ssId": "a03675379685d88c727bc985a323cc71d06f2514", "arXivId": "1808.09111", "link": "https://arxiv.org/pdf/1808.09111.pdf", "openAccess": true, "authors": ["Junxian He", "Graham Neubig", "Taylor Berg-Kirkpatrick"]}}
{"id": "49edf7f0dbad8b8c101af9ef95c72f62f545591e", "content": {"title": "Efficient Correlated Topic Modeling with Topic Embedding", "abstract": "Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.", "year": 2017, "ssId": "49edf7f0dbad8b8c101af9ef95c72f62f545591e", "arXivId": "1707.00206", "link": "https://arxiv.org/pdf/1707.00206.pdf", "openAccess": true, "authors": ["Junxian He", "Zhiting Hu", "Taylor Berg-Kirkpatrick", "Ying Huang", "E. Xing"]}}
{"id": "f05741b65a1d644f2fae4c654dae315a7451ee85", "content": {"title": "Text Network Exploration via Heterogeneous Web of Topics", "abstract": "A text network refers to a data type that each vertex is associated with a text document and the relationship between documents is represented by edges. The proliferation of text networks such as hyperlinked webpages and academic citation networks has led to an increasing demand for quickly developing a general sense of a new text network, namely text network exploration. In this paper, we address the problem of text network exploration through constructing a heterogeneous web of topics, which allows people to investigate a text network associating word level with document level. To achieve this, a probabilistic generative model for text and links is proposed, where three different relationships in the heterogeneous topic web are quantified. We also develop a prototype demo system named TopicAtlas to exhibit such heterogeneous topic web, and demonstrate how this system can facilitate the task of text network exploration. Extensive qualitative analyses are included to verify the effectiveness of this heterogeneous topic web. Besides, we validate our model on real-life text networks, showing that it preserves good performance on objective evaluation metrics.", "year": 2016, "ssId": "f05741b65a1d644f2fae4c654dae315a7451ee85", "arXivId": "1610.00219", "link": "https://arxiv.org/pdf/1610.00219.pdf", "openAccess": true, "authors": ["Junxian He", "Ying Huang", "Changfeng Liu", "Jiaming Shen", "Yuting Jia", "Xinbing Wang"]}}
{"id": "ce4db7a32724e0abc8afe27f74d33e32e099b8e6", "content": {"title": "MyoD Is a Novel Activator of Porcine FIT1 Gene by Interacting with the Canonical E-Box Element during Myogenesis", "abstract": "Fat-induced transcript 1 (FIT1/FITM1) gene is a member of the conserved gene family important for triglyceride-rich lipid droplet accumulation. FIT1 gene displays a similar muscle-specific expression across pigs, mice, and humans. Thus pigs can act as a useful model of many human diseases resulting from misexpression of FIT1 gene. Triglyceride content in skeletal muscle plays a key role in pork meat quality and flavors. An insertion/deletion mutation in porcine FIT1 coding region shows a high correlation with a series of fat traits. To gain better knowledge of the potential role of FIT1 gene in human diseases and the correlations with pork meat quality, our attention is given to the region upstream of the porcine FIT1 coding sequence. We cloned ~1 kb of the 5\u2032-flanking region of porcine FIT1 gene to define the role of this sequence in modulating the myogenic expression. A canonical E-box element that activated porcine FIT1 promoter activity during myogenesis was identified. Further analysis demonstrated that promoter activity was induced by overexpression of MyoD1, which bound to this canonical E-box during C2C12 differentiation. This is the first evidence that FIT1 as the direct novel target of MyoD is involved in muscle development.", "year": 2015, "ssId": "ce4db7a32724e0abc8afe27f74d33e32e099b8e6", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chi Yan", "Xiaoliang Xia", "Junxian He", "Z. Ren", "De-quan Xu", "Y. Xiong", "B. Zuo"]}}
