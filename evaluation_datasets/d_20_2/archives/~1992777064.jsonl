{"id": "c589c4ec7247980f38a6bd22f215fea8028a0f66", "content": {"title": "Re-Examining Human Annotations for Interpretable NLP", "abstract": "Explanation methods in Interpretable NLP often explain the model\u2019s decision by extracting evidence (rationale) from the input texts supporting the decision. Benchmark datasets for rationales have been released to evaluate how good the rationale is. The ground truth rationales in these datasets are of- ten human annotations obtained via crowd-sourced websites. Valuable as these datasets are, the details on how those human annotations are obtained are often not clearly speci\ufb01ed. We conduct comprehensive controlled experiments using crowd- sourced websites on two widely used datasets in Interpretable NLP to understand how those unsaid details can affect the annotation results. Speci\ufb01cally, we compare the annotation results obtained from recruiting workers satisfying different levels of quali\ufb01cation. We also provide high-quality workers with different instructions for completing the same under- lying tasks. Our results reveal that the annotation quality is highly subject to the workers\u2019 quali\ufb01cation, and workers can be guided to provide certain annotations by the instructions. We further show that speci\ufb01c explanation methods perform better when evaluated using the ground truth rationales obtained by particular instructions. Based on these observations, we highlight the importance of providing complete details of the annotation process and call for careful interpretation of any experiment results obtained using those annotations.", "year": 2022, "ssId": "c589c4ec7247980f38a6bd22f215fea8028a0f66", "arXivId": "2204.04580", "link": "https://arxiv.org/pdf/2204.04580.pdf", "openAccess": true, "authors": ["David C. Chiang", "Hung-yi Lee"]}}
{"id": "bd2f3822801a7e2f933d06c261b8783764d8ce18", "content": {"title": "Understanding, Detecting, and Separating Out-of-Distribution Samples and Adversarial Samples in Text Classification", "abstract": "In this paper, we study the differences and commonalities between statistically out-of-distribution (OOD) samples and adversarial (Adv) samples, both of which hurting a text classi\ufb01cation model\u2019s performance. We conduct analyses to compare the two types of anomalies (OOD and Adv samples) with the in-distribution (ID) ones from three aspects: the input features, the hidden representations in each layer of the model, and the output probability distributions of the classi\ufb01er. We \ufb01nd that OOD samples expose their aberration starting from the \ufb01rst layer, while the abnormalities of Adv samples do not emerge until the deeper layers of the model. We also illustrate that the models\u2019 output probabilities for Adv samples tend to be more uncon\ufb01dent. Based on our observations, we propose a simple method to separate ID, OOD, and Adv samples using the hidden representations and output probabilities of the model. On multiple combinations of ID, OOD datasets, and Adv attacks, our proposed method shows exceptional results on distinguishing ID, OOD, and Adv samples.", "year": 2022, "ssId": "bd2f3822801a7e2f933d06c261b8783764d8ce18", "arXivId": "2204.04458", "link": "https://arxiv.org/pdf/2204.04458.pdf", "openAccess": true, "authors": ["David C. Chiang", "Hung-yi Lee"]}}
{"id": "d3dd80269f2542cc173afb3a1df24b582a1e4af2", "content": {"title": "Overcoming a Theoretical Limitation of Self-Attention", "abstract": "Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer\u2019s classification decisions become less and less confident (that is, with crossentropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn\u2019s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.", "year": 2022, "ssId": "d3dd80269f2542cc173afb3a1df24b582a1e4af2", "arXivId": "2202.12172", "link": "https://arxiv.org/pdf/2202.12172.pdf", "openAccess": true, "authors": ["David C. Chiang", "Peter A. Cholak"]}}
{"id": "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8", "content": {"title": "On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets", "abstract": "Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what specific traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks. We propose to use artificially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have. By fine-tuning the pre-trained models on GLUE benchmark, we can learn how beneficial it is to transfer the knowledge from the model trained on the dataset possessing that specific trait. We define and discuss three different characteristics in the artificial dataset: 1) matching the token\u2019s uni-gram or bi-gram distribution between pre-training and downstream fine-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance. Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies. Based on our analysis, we find that models pretrained with artificial datasets are prone to learn spurious correlation in downstream tasks. Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.", "year": 2021, "ssId": "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8", "arXivId": "2109.03537", "link": "https://arxiv.org/pdf/2109.03537.pdf", "openAccess": true, "authors": ["David C. Chiang", "Hung-yi Lee"]}}
{"id": "b1d309073623d46548e55269fb73485a3b7f11a8", "content": {"title": "Pretrained Language Model Embryology: The Birth of ALBERT", "abstract": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL.", "year": 2020, "ssId": "b1d309073623d46548e55269fb73485a3b7f11a8", "arXivId": "2010.02480", "link": "https://arxiv.org/pdf/2010.02480.pdf", "openAccess": true, "authors": ["David C. Chiang", "S. Huang", "Hung-yi Lee"]}}
{"id": "88167f36dced91c279162d68af7225f2b4e2091c", "content": {"title": "Pre-Training a Language Model Without Human Language", "abstract": "In this paper, we study how the intrinsic nature of pre-training data contributes to the fine-tuned downstream performance. To this end, we pre-train different transformer-based masked language models on several corpora with certain features, and we fine-tune those language models on GLUE benchmarks. We find that models pre-trained on unstructured data beat those trained directly from scratch on downstream tasks. Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks. To our great astonishment, we uncover that pre-training on certain non-human language data gives GLUE performance close to performance pre-trained on another nonEnglish language.", "year": 2020, "ssId": "88167f36dced91c279162d68af7225f2b4e2091c", "arXivId": "2012.11995", "link": "https://arxiv.org/pdf/2012.11995.pdf", "openAccess": true, "authors": ["David C. Chiang", "Hung-yi Lee"]}}
