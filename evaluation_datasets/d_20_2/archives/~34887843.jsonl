{"id": "8a902a848c3710290f04f2d59030f5670d3433f8", "content": {"title": "Error Analysis and the Role of Morphology", "abstract": "We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of morphology increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the morphological features most predictive of error.", "year": 2021, "ssId": "8a902a848c3710290f04f2d59030f5670d3433f8", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Anders S\u00f8gaard"]}}
{"id": "464b47a6a395fa1338e230254965cf5f669e715c", "content": {"title": "Moses and the Character-Based Random Babbling Baseline: CoAStaL at AmericasNLP 2021 Shared Task", "abstract": "We evaluated a range of neural machine translation techniques developed specifically for low-resource scenarios. Unsuccessfully. In the end, we submitted two runs: (i) a standard phrase-based model, and (ii) a random babbling baseline using character trigrams. We found that it was surprisingly hard to beat (i), in spite of this model being, in theory, a bad fit for polysynthetic languages; and more interestingly, that (ii) was better than several of the submitted systems, highlighting how difficult low-resource machine translation for polysynthetic languages is.", "year": 2021, "ssId": "464b47a6a395fa1338e230254965cf5f669e715c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Rahul Aralikatte", "H\u00e9ctor Murrieta Bello", "Daniel Hershcovich", "Miryam de Lhoneux", "Anders S\u00f8gaard"]}}
{"id": "dc8ebb6d9908542ae474dc2b21bfb6a14216f678", "content": {"title": "How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task", "abstract": "This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.", "year": 2021, "ssId": "dc8ebb6d9908542ae474dc2b21bfb6a14216f678", "arXivId": null, "link": null, "openAccess": false, "authors": ["Rahul Aralikatte", "H\u00e9ctor Murrieta Bello", "Miryam de Lhoneux", "Daniel Hershcovich", "Marcel Bollmann", "Anders S\u00f8gaard"]}}
{"id": "7b96f6165ce5f686e46868c53b111b8e43b93de3", "content": {"title": "On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology", "abstract": "The field of natural language processing is experiencing a period of unprecedented growth, and with it a surge of published papers. This represents an opportunity for us to take stock of how we cite the work of other researchers, and whether this growth comes at the expense of \u201cforgetting\u201d about older literature. In this paper, we address this question through bibliographic analysis. By looking at the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019, we find that there is indeed a tendency for recent papers to cite more recent work, but the rate at which papers older than 15 years are cited has remained relatively stable.", "year": 2020, "ssId": "7b96f6165ce5f686e46868c53b111b8e43b93de3", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Desmond Elliott"]}}
{"id": "26a238217321008cd1daaa649683d461e16e7574", "content": {"title": "Historical Text Normalization with Delayed Rewards", "abstract": "Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words.", "year": 2019, "ssId": "26a238217321008cd1daaa649683d461e16e7574", "arXivId": null, "link": null, "openAccess": false, "authors": ["S. Flachs", "Marcel Bollmann", "Anders S\u00f8gaard"]}}
{"id": "bc632f81dab322ac610a8d11463cc1bba6130eda", "content": {"title": "Few-Shot and Zero-Shot Learning for Historical Text Normalization", "abstract": "Historical text normalization often relies on small training datasets. Recent work has shown that multi-task learning can lead to significant improvements by exploiting synergies with related datasets, but there has been no systematic study of different multi-task learning architectures. This paper evaluates 63 multi-task learning configurations for sequence-to-sequence-based historical text normalization across ten datasets from eight languages, using autoencoding, grapheme-to-phoneme mapping, and lemmatization as auxiliary tasks. We observe consistent, significant improvements across languages when training data for the target task is limited, but minimal or no improvements when training data is abundant. We also show that zero-shot learning outperforms the simple, but relatively strong, identity baseline.", "year": 2019, "ssId": "bc632f81dab322ac610a8d11463cc1bba6130eda", "arXivId": "1903.04870", "link": "https://arxiv.org/pdf/1903.04870.pdf", "openAccess": true, "authors": ["Marcel Bollmann", "N. Korchagina", "Anders S\u00f8gaard"]}}
{"id": "660119405bb48777cd71d85caa5ec2e90a336caf", "content": {"title": "A Large-Scale Comparison of Historical Text Normalization Systems", "abstract": "There is no consensus on the state-of-the-art approach to historical text normalization. Many techniques have been proposed, including rule-based methods, distance metrics, character-based statistical machine translation, and neural encoder\u2013decoder models, but studies have used different datasets, different evaluation methods, and have come to different conclusions. This paper presents the largest study of historical text normalization done so far. We critically survey the existing literature and report experiments on eight languages, comparing systems spanning all categories of proposed normalization techniques, analysing the effect of training data quantity, and using different evaluation methods. The datasets and scripts are made publicly available.", "year": 2019, "ssId": "660119405bb48777cd71d85caa5ec2e90a336caf", "arXivId": "1904.02036", "link": "https://arxiv.org/pdf/1904.02036.pdf", "openAccess": true, "authors": ["Marcel Bollmann"]}}
{"id": "8a880680b28dee5642ac88431b3ae1085b911f96", "content": {"title": "Naive Regularizers for Low-Resource Neural Machine Translation", "abstract": "Neural machine translation models have little inductive bias, which can be a disadvantage in low-resource scenarios. Neural models have to be trained on large amounts of data and have been shown to perform poorly when only limited data is available. We show that using naive regularization methods, based on sentence length, punctuation and word frequencies, to penalize translations that are very different from the input sentences, consistently improves the translation quality across multiple low-resource languages. We experiment with 12 language pairs, varying the training data size between 17k to 230k sentence pairs. Our best regularizer achieves an average increase of 1.5 BLEU score and 1.0 TER score across all the language pairs. For example, we achieve a BLEU score of 26.70 on the IWSLT15 English\u2013Vietnamese translation task simply by using relative differences in punctuation as a regularizer.", "year": 2019, "ssId": "8a880680b28dee5642ac88431b3ae1085b911f96", "arXivId": null, "link": null, "openAccess": false, "authors": ["Meriem Beloucif", "Ana Valeria Gonzalez", "Marcel Bollmann", "Anders S\u00f8gaard"]}}
{"id": "2a39a4f2d18e376ef8a6e2f45416e7b87957481e", "content": {"title": "Multi-task learning for historical text normalization: Size matters", "abstract": "Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that multi-task learning can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from multi-task learning. We explore the benefits of multi-task learning across 10 different datasets, representing different languages and periods. Our main finding\u2014contrary to what has been observed for other NLP tasks\u2014is that multi-task learning mainly works when target task data is very scarce.", "year": 2018, "ssId": "2a39a4f2d18e376ef8a6e2f45416e7b87957481e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Anders S\u00f8gaard", "Joachim Bingel"]}}
{"id": "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634", "content": {"title": "Normalization of historical texts with neural network models", "abstract": "Historische Dokumente werden zunehmend in digitalisierter Form verfugbar gemacht. Haufig sind sie jedoch durch eine Fulle von Schreibvarianten gekennzeichnet, welche die Anwendung computerlinguistischer Methoden (bzw. NLP-Tools) schwierig gestalten. Ein haufig verwendeter Ansatz ist die Normalisierung dieser Varianten auf moderne Schreibweisen. Die vorliegende Arbeit untersucht die Anwendung neuronaler Encoder-Decoder-Modelle fur die automatische Normalisierung historischer Sprachdaten. In einer umfassenden Auswertung auf historischen Korpora in acht verschiedenen Sprachen zeigt sich, dass das verwendete Modell \u2014 trotz zahlreicher Anpassungen und Verbesserungen wie z.B. Beam Search und Ensembling \u2014 meist eine schlechtere Normalisierungs-Genauigkeit hat als etablierte Methoden, die auf statistischer maschineller Ubersetzung beruhen.", "year": 2018, "ssId": "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann"]}}
{"id": "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8", "content": {"title": "The CLIN27 Shared Task: Translating Historical Text to Contemporary Language for Improving Automatic Linguistic Annotation", "abstract": "The CLIN27 shared task evaluates the effect of translating historical text to modern text with the goal of improving the quality of the output of contemporary natural language processing tools appl ...", "year": 2017, "ssId": "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8", "arXivId": null, "link": null, "openAccess": false, "authors": ["E. T. K. Sang", "Marcel Bollmann", "Remko Boschker", "F. Casacuberta", "F. Dietz", "Stefanie Dipper", "Miguel Domingo", "Rob van der Goot", "M. Koppen", "Nikola Ljubesic", "Robert \u00d6stling", "Florian Petran", "Eva Pettersson", "Yves Scherrer", "M. Schraagen", "L. Sevens", "J. Tiedemann", "Tom Vanallemeersch", "Kalliopi Zervanou"]}}
{"id": "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706", "content": {"title": "Learning attention for historical text normalization by learning to pronounce", "abstract": "Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works.", "year": 2017, "ssId": "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Joachim Bingel", "Anders S\u00f8gaard"]}}
{"id": "0132cb4384c3a6402353d8f349f8dd450d8ea4a2", "content": {"title": "Improving historical spelling normalization with bi-directional LSTMs and multi-task learning", "abstract": "Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model\u2019s performance further.", "year": 2016, "ssId": "0132cb4384c3a6402353d8f349f8dd450d8ea4a2", "arXivId": "1610.07844", "link": "https://arxiv.org/pdf/1610.07844.pdf", "openAccess": true, "authors": ["Marcel Bollmann", "Anders S\u00f8gaard"]}}
{"id": "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f", "content": {"title": "ReM: A reference corpus of Middle High German - corpus compilation, annotation, and access", "abstract": "This paper describes ReM and the results of the ReM project and its predecessors. All projects closely collaborate in developing common annotation standards to allow for diachronic investigations. ReA has already been published and made available via the corpus search tool ANNIS2 (Krause and Zeldes, 2016), while ReF and ReN are still in the annotation process. The ReM project builds on several earlier annotation efforts, such as the corpus of the new Middle High German Grammar (MiGraKo, Klein et al. (2009)), expanding them and adding further texts, to produce a reference corpus for Middle High German, which we will also call \u201cReM\u201d for short. The combined corpus, which consists of around two million tokens, provides a mostly complete collection of written records from Early Middle High German (1050\u20131200) as well as a selection of Middle High German texts from 1200 to 1350. Texts have been digitized and annotated with parts of speech and morphology (using the HiTS tagset, cf. Dipper et al. (2013)) as well as lemma information. Release 1.0 of ReM has been published in December 2016 and is also accessible via the ANNIS tool. The project website at https://www.linguistics.ruhr-uni-bochum. de/rem/ offers extensive documentation of the project and the corpus. The corpus", "year": 2016, "ssId": "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Florian Petran", "Marcel Bollmann", "Stefanie Dipper", "Thomas Klein"]}}
{"id": "3cc790174d138d7904189df997d5763f1793dedf", "content": {"title": "Evaluating Inter-Annotator Agreement on Historical Spelling Normalization", "abstract": "This paper deals with means of evaluating inter-annotator agreement for a normalization task. This task differs from common annotation tasks in two important aspects: (i) the class of labels (the normalized wordforms) is open, and (ii) annotations can match to different degrees. We propose a new method to measure inter-annotator agreement for the normalization task. It integrates common chancecorrected agreement measures, such as Fleiss\u2019s \u03ba or Krippendorff\u2019s \u03b1. The novelty of our proposed method lies in the way the annotated word forms are treated. First, they are evaluated character-wise; second, certain characters are mapped to more general categories.", "year": 2016, "ssId": "3cc790174d138d7904189df997d5763f1793dedf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Stefanie Dipper", "Florian Petran"]}}
{"id": "9dbd86f089c2132dc46d316750d9786d60d5d720", "content": {"title": "CorA: A web-based annotation tool for historical and other non-standard language data", "abstract": "We present CorA, a web-based annotation tool for manual annotation of historical and other non-standard language data. It allows for editing the primary data and modifying token boundaries during the annotation process. Further, it supports immediate retraining of taggers on newly annotated data.", "year": 2014, "ssId": "9dbd86f089c2132dc46d316750d9786d60d5d720", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Florian Petran", "Stefanie Dipper", "J. Krasselt"]}}
{"id": "39ffb5e9f2f36df42ef8ea010499e484c913e79e", "content": {"title": "Spelling Normalization of Historical German with Sparse Training Data", "abstract": "Recently, there has been a growing interest in historical language corpora. Projects to create such corpora exist for a variety of languages such as German (Scheible et al. 2011), Spanish (S\u00e1nchezMarco et al. 2010), or Slovene (Erjavec 2012). Annotation of these corpora is complicated by the fact that specialized tools for these language stages are typically not available. A common approach is to employ spelling normalization to map historical wordforms to modern ones (e.g., Adesam et al. 2012, Baron et al. 2009, Jurish 2010), so that existing tools for modern language (e.g., modern POS taggers) can be used on the normalized data. This paper presents an approach to spelling normalization that combines three different normalization algorithms and evaluates it on a diverse set of texts of historical German. The evaluation shows that this approach produces acceptable results even with comparatively small amounts of training data. The normalization methods were previously described in Bollmann (2012), though with a much more restricted evaluation.", "year": 2013, "ssId": "39ffb5e9f2f36df42ef8ea010499e484c913e79e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann"]}}
{"id": "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33", "content": {"title": "POS Tagging for Historical Texts with Sparse Training Data", "abstract": "This paper presents a method for part-ofspeech tagging of historical data and evaluates it on texts from different corpora of historical German (15th\u201318th century). Spelling normalization is used to preprocess the texts before applying a POS tagger trained on modern German corpora. Using only 250 manually normalized tokens as training data, the tagging accuracy of a manuscript from the 15th century can be raised from 28.65% to 74.89%.", "year": 2013, "ssId": "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann"]}}
{"id": "ed2cc779c7eb0004bd6dd50538a2cafca092c94f", "content": {"title": "Automatic Normalization for Linguistic Annotation of Historical Language Data", "abstract": "This paper deals with spelling normalization of historical texts with regard to further processing with modern part-of-speech taggers. Different methods for this task are presented and evaluated on a set of historical German texts from the 15th\u201318th century, and specific problems inherent to the processing of historical data are discussed. A chain combination using word-based and character-based techniques is shown to be best for normalization, while POS tagging of normalized data is shown to benefit from ignoring punctuation marks. Using these techniques, when 500 manually normalized tokens are used as training data for the normalization, the tagging accuracy of a manuscript from the 15th century can be raised from 28.65% to 76.27%.", "year": 2013, "ssId": "ed2cc779c7eb0004bd6dd50538a2cafca092c94f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Bochumer Linguistische"]}}
{"id": "adeed0816a2cab763e3bee769957ff1849985759", "content": {"title": "(Semi-)Automatic Normalization of Historical Texts using Distance Measures and the Norma tool", "abstract": "Historical texts typically show a high degree of variance in spelling. Normalization of variant word forms to their modern spellings can greatly benefit further processing of the data, e.g., POS tagging or lemmatization. This paper compares several approaches to normalization with a focus on methods based on string distance measures and evaluates them on two different types of historical texts. Furthermore, the Norma tool is introduced, an interactive normalization tool which is flexibly adaptable to different varieties of historical language data. It is shown that a combination of normalization methods produces the best results, achieving an accuracy between 74% and 94% depending on the type of text.", "year": 2012, "ssId": "adeed0816a2cab763e3bee769957ff1849985759", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann"]}}
