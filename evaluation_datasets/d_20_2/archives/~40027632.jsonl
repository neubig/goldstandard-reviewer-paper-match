{"id": "caabc3d0c5ece9d44fb2216a347362d4609934c1", "content": {"title": "A Systematic Evaluation of Large Language Models of Code", "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.", "year": 2022, "ssId": "caabc3d0c5ece9d44fb2216a347362d4609934c1", "arXivId": "2202.13169", "link": "https://arxiv.org/pdf/2202.13169.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"]}}
{"id": "c96363c42bc8c465902c22b8c33c8704233f519e", "content": {"title": "MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages", "abstract": "While there has been a recent burgeoning of applications at the intersection of natural and programming languages, such as code generation and code summarization, these applications are usually English-centric. This creates a barrier for program developers who are not proficient in English. To mitigate this gap in technology development across languages, we propose a multilingual dataset, MCoNaLa, to benchmark code generation from natural language commands extending beyond English. Modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896 NL-code pairs in three languages: Spanish, Japanese, and Russian. We present a quantitative evaluation of performance on the MCoNaLa dataset by testing with state-of-theart code generation systems. While the difficulties vary across these three languages, all systems lag significantly behind their English counterparts, revealing the challenges in adapting code generation to new languages. 1", "year": 2022, "ssId": "c96363c42bc8c465902c22b8c33c8704233f519e", "arXivId": "2203.08388", "link": "https://arxiv.org/pdf/2203.08388.pdf", "openAccess": true, "authors": ["Zhiruo Wang", "Grace Cuenca", "Shuyan Zhou", "Frank F. Xu", "Graham Neubig"]}}
{"id": "4b9795493a937b9034be9c26afab23f6dc751f62", "content": {"title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval", "abstract": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RETOMATON \u2013 retrieval automaton \u2013 which approximates the datastore search, based on (1) clustering of entries into \u201cstates\u201d, and (2) state transitions from previous entries. This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RETOMATON can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity, or alternatively saves up to 83% of the nearest neighbor searches over kNN-LM (Khandelwal et al., 2020), without hurting perplexity.", "year": 2022, "ssId": "4b9795493a937b9034be9c26afab23f6dc751f62", "arXivId": "2201.12431", "link": "https://arxiv.org/pdf/2201.12431.pdf", "openAccess": true, "authors": ["Uri Alon", "Frank F. Xu", "Junxian He", "Sudipta Sengupta", "D. Roth", "Graham Neubig"]}}
{"id": "1756376bf7cf0d0a7bec881d663b57907a361ecf", "content": {"title": "Learning Structural Edits via Incremental Tree Transformations", "abstract": "While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e. ''structural edits''). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g. deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.", "year": 2021, "ssId": "1756376bf7cf0d0a7bec881d663b57907a361ecf", "arXivId": "2101.12087", "link": "https://arxiv.org/pdf/2101.12087.pdf", "openAccess": true, "authors": ["Ziyu Yao", "Frank F. Xu", "Pengcheng Yin", "Huan Sun", "Graham Neubig"]}}
{"id": "4a160efbe80c38cd5eb2f92c7c095b49b113397d", "content": {"title": "In-IDE Code Generation from Natural Language: Promise and Challenges", "abstract": "\n A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning\n concept\n into\n code\n , especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from\n natural language queries\n , but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, \u201cAt the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?\u201d To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.\n", "year": 2021, "ssId": "4a160efbe80c38cd5eb2f92c7c095b49b113397d", "arXivId": "2101.11149", "link": "https://arxiv.org/pdf/2101.11149.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Bogdan Vasilescu", "Graham Neubig"]}}
{"id": "c6bb04f3d8000b7e800f6359082de39548c7da79", "content": {"title": "Capturing Structural Locality in Non-parametric Language Models", "abstract": "Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improved performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure.", "year": 2021, "ssId": "c6bb04f3d8000b7e800f6359082de39548c7da79", "arXivId": "2110.02870", "link": "https://arxiv.org/pdf/2110.02870.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Junxian He", "Graham Neubig", "V. Hellendoorn"]}}
{"id": "b6b76f529d273a35180d0dc65912db1538539067", "content": {"title": "Minimally Supervised Categorization of Text with Metadata", "abstract": "Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1)the presence of metadata : in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2)label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into the same semantic space to encode heterogeneous signals. Then, based on the same generative process, we synthesize training samples to address the bottleneck of label scarcity. We conduct a thorough evaluation on a wide range of datasets. Experimental results prove the effectiveness of MetaCat over many competitive baselines.", "year": 2020, "ssId": "b6b76f529d273a35180d0dc65912db1538539067", "arXivId": "2005.00624", "link": "https://arxiv.org/pdf/2005.00624.pdf", "openAccess": true, "authors": ["Yu Zhang", "Yu Meng", "Jiaxin Huang", "Frank F. Xu", "Xuan Wang", "Jiawei Han"]}}
{"id": "358d7d6333d3edd530e37efd8004cb9da8cfd5d4", "content": {"title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos", "abstract": "Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.", "year": 2020, "ssId": "358d7d6333d3edd530e37efd8004cb9da8cfd5d4", "arXivId": "2005.00706", "link": "https://arxiv.org/pdf/2005.00706.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Lei Ji", "Botian Shi", "Junyi Du", "Graham Neubig", "Yonatan Bisk", "Nan Duan"]}}
{"id": "4ffca5d623950e2396089e7fc1621b4a477436cb", "content": {"title": "Record-to-Text Generation with Style Imitation", "abstract": "Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional systems use templates to determine the realization of text. Yet manual or automatic construction of high-quality templates is difficult, and a template acting as hard constraints could harm content fidelity when it does not match the record perfectly. We study a new way of stylistic control by using existing sentences as \u201csoft\u201d templates. That is, a model learns to imitate the writing style of any given exemplar sentence, with automatic adaptions to faithfully describe the record. The problem is challenging due to the lack of parallel data. We develop a neural approach that includes a hybrid attention-copy mechanism, learns with weak supervisions, and is enhanced with a new content coverage constraint. We conduct experiments in restaurants and sports domains. Results show our approach achieves stronger performance than a range of comparison methods. Our approach balances well between content fidelity and style control given exemplars that match the records to varying degrees.", "year": 2020, "ssId": "4ffca5d623950e2396089e7fc1621b4a477436cb", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shuai Lin", "Wentao Wang", "Zichao Yang", "Xiaodan Liang", "Frank F. Xu", "E. Xing", "Zhiting Hu"]}}
{"id": "77910e51a40d17157fc798325d06edfa6cff18d6", "content": {"title": "Incorporating External Knowledge through Pre-training for Natural Language to Code Generation", "abstract": "Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at https://github.com/neulab/external-knowledge-codegen.", "year": 2020, "ssId": "77910e51a40d17157fc798325d06edfa6cff18d6", "arXivId": "2004.09015", "link": "https://arxiv.org/pdf/2004.09015.pdf", "openAccess": true, "authors": ["Frank F. Xu", "Zhengbao Jiang", "Pengcheng Yin", "Bogdan Vasilescu", "Graham Neubig"]}}
{"id": "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85", "content": {"title": "How Can We Know What Language Models Know?", "abstract": "Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as \u201cObama is a __ by profession\u201d. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as \u201cObama worked as a __ \u201d may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.", "year": 2019, "ssId": "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85", "arXivId": "1911.12543", "link": "https://arxiv.org/pdf/1911.12543.pdf", "openAccess": true, "authors": ["Zhengbao Jiang", "Frank F. Xu", "J. Araki", "Graham Neubig"]}}
{"id": "975551547fef77605fb85a551bbd7523b77746b7", "content": {"title": "HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories", "abstract": "GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneous information network embedding; keyword enrichment; topic modeling and pseudo document generation. Experimental results on two GitHub repository collections confirm that HiGitClass is superior to existing weakly-supervised and dataless hierarchical classification methods, especially in its ability to integrate both structured and unstructured data for repository classification. Code and datasets related to this paper are available at https://github.com/yuzhimanhua/HiGitClass.", "year": 2019, "ssId": "975551547fef77605fb85a551bbd7523b77746b7", "arXivId": "1910.07115", "link": "https://arxiv.org/pdf/1910.07115.pdf", "openAccess": true, "authors": ["Yu Zhang", "Frank F. Xu", "Sha Li", "Yu Meng", "Xuan Wang", "Qi Li", "Jiawei Han"]}}
{"id": "5693c74eb8ffde1490ba480fdc963f008243906a", "content": {"title": "AlpacaTag: An Active Learning-based Crowd Annotation Framework for Sequence Tagging", "abstract": "We introduce an open-source web-based data annotation framework (AlpacaTag) for sequence tagging tasks such as named-entity recognition (NER). The distinctive advantages of AlpacaTag are three-fold. 1) Active intelligent recommendation: dynamically suggesting annotations and sampling the most informative unlabeled instances with a back-end active learned model; 2) Automatic crowd consolidation: enhancing real-time interannotator agreement by merging inconsistent labels from multiple annotators; 3) Real-time model deployment: users can deploy their models in downstream systems while new annotations are being made. AlpacaTag is a comprehensive solution for sequence labeling tasks, ranging from rapid tagging with recommendations powered by active learning and auto-consolidation of crowd annotations to real-time model deployment.", "year": 2019, "ssId": "5693c74eb8ffde1490ba480fdc963f008243906a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Bill Y. Lin", "Dong-Ho Lee", "Frank F. Xu", "Ouyu Lan", "Xiang Ren"]}}
{"id": "0acbdcac9edf74cc2c1e98bd59e301c9300977d0", "content": {"title": "AlpacaTag: An Active Learning-based Crowd Annotation Framework for Sequence Tagging", "abstract": "We introduce an open-source web-based data annotation framework (AlpacaTag) for sequence tagging tasks such as named-entity recognition (NER). The distinctive advantages of AlpacaTag are three-fold. 1) Active intelligent recommendation: dynamically suggesting annotations and sampling the most informative unlabeled instances with a back-end active learned model; 2) Automatic crowd consolidation: enhancing real-time inter-annotator agreement by merging inconsistent labels from multiple annotators; 3) Real-time model deployment: users can deploy their models in downstream systems while new annotations are being made. AlpacaTag is a comprehensive solution for sequence labeling tasks, ranging from rapid tagging with recommendations powered by active learning and auto-consolidation of crowd annotations to real-time model deployment.", "year": 2019, "ssId": "0acbdcac9edf74cc2c1e98bd59e301c9300977d0", "arXivId": null, "link": null, "openAccess": false, "authors": ["Bill Yuchen Lin", "Dong-Ho Lee", "Frank F. Xu", "Ouyu Lan", "Xiang Ren"]}}
{"id": "d389d8c2e15f9e9269c17fe6f960f70559eee840", "content": {"title": "Parsimonious Morpheme Segmentation with an Application to Enriching Word Embeddings", "abstract": "Traditionally, many text-mining tasks treat individual word-tokens as the finest meaningful semantic granularity. However, in many languages and specialized corpora, words are composed by concatenating semantically meaningful subword structures. Word-level analysis cannot leverage the semantic information present in such subword structures. With regard to word embedding techniques, this leads to not only poor embeddings for infrequent words in long-tailed text corpora but also weak capabilities for handling out-of-vocabulary words. In this paper we propose MorphMine for unsupervised morpheme segmentation. MorphMine applies a parsimony criterion to hierarchically segment words into the fewest number of morphemes at each level of the hierarchy. This leads to longer shared morphemes at each level of segmentation. Experiments show that MorphMine segments words in a variety of languages into human-verified morphemes. Additionally, we experimentally demonstrate that utilizing MorphMine morphemes to enrich word embeddings consistently improves embedding quality on a variety of of embedding evaluations and a downstream language modeling task.", "year": 2019, "ssId": "d389d8c2e15f9e9269c17fe6f960f70559eee840", "arXivId": "1908.07832", "link": "https://arxiv.org/pdf/1908.07832.pdf", "openAccess": true, "authors": ["Ahmed El-Kishky", "Frank F. Xu", "Aston Zhang", "Jiawei Han"]}}
{"id": "82ae0d4b41046ccedb435ece08a61f198cf77bb9", "content": {"title": "Toward Unsupervised Text Content Manipulation", "abstract": "Controlled generation of text is of high practical use. Recent efforts have made impressive progress in generating or editing sentences with given textual attributes (e.g., sentiment). This work studies a new practical setting of text content manipulation. Given a structured record, such as `(PLAYER: Lebron, POINTS: 20, ASSISTS: 10)', and a reference sentence, such as `Kobe easily dropped 30 points', we aim to generate a sentence that accurately describes the full content in the record, with the same writing style (e.g., wording, transitions) of the reference. The problem is unsupervised due to lack of parallel data in practice, and is challenging to minimally yet effectively manipulate the text (by rewriting/adding/deleting text portions) to ensure fidelity to the structured content. We derive a dataset from a basketball game report corpus as our testbed, and develop a neural method with unsupervised competing objectives and explicit content coverage constraints. Automatic and human evaluations show superiority of our approach over competitive methods including a strong rule-based baseline and prior approaches designed for style transfer.", "year": 2019, "ssId": "82ae0d4b41046ccedb435ece08a61f198cf77bb9", "arXivId": "1901.09501", "link": "https://arxiv.org/pdf/1901.09501.pdf", "openAccess": true, "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Frank F. Xu", "E. Xing"]}}
{"id": "29437d98b9e6f45bef7029f3ce1237b8b284464f", "content": {"title": "Data-to-Text Generation with Style Imitation", "abstract": "Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., word choices, sentence structures). More traditional systems use templates to determine the realization of text. Yet manual or automatic construction of high-quality templates is difficult, and a template acting as hard constraints could harm content fidelity when it does not match the record perfectly. We study a new way of stylistic control by using existing sentences as soft templates. That is, the model learns to imitate the writing style of any given exemplar sentence, with automatic adaptions to faithfully describe the content record. The problem is challenging due to the lack of parallel data. We develop a neural approach that includes a hybrid attention-copy mechanism, learns with weak supervisions, and is enhanced with a new content coverage constraint. We conduct experiments in restaurants and sports domains. Results show our approach achieves stronger performance than a range of comparison methods. Our approach balances well between content fidelity and style control given exemplars that match the records to varying degrees.", "year": 2019, "ssId": "29437d98b9e6f45bef7029f3ce1237b8b284464f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shuai Lin", "Wentao Wang", "Zichao Yang", "Xiaodan Liang", "Frank F. Xu", "E. Xing", "Zhiting Hu"]}}
{"id": "933b03a81110676f4c61c449f1926ebd58bc47f7", "content": {"title": "StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible", "abstract": "Blind people frequently encounter inaccessible dynamic touchscreens in their everyday lives that are difficult, frustrating, and often impossible to use independently. Touchscreens are often the only way to control everything from coffee machines and payment terminals, to subway ticket machines and in-flight entertainment systems. Interacting with dynamic touchscreens is difficult non-visually because the visual user interfaces change, interactions often occur over multiple different screens, and it is easy to accidentally trigger interface actions while exploring the screen. To solve these problems, we introduce StateLens - a three-part reverse engineering solution that makes existing dynamic touchscreens accessible. First, StateLens reverse engineers the underlying state diagrams of existing interfaces using point-of-view videos found online or taken by users using a hybrid crowd-computer vision pipeline. Second, using the state diagrams, StateLens automatically generates conversational agents to guide blind users through specifying the tasks that the interface can perform, allowing the StateLens iOS application to provide interactive guidance and feedback so that blind users can access the interface. Finally, a set of 3D-printed accessories enable blind people to explore capacitive touchscreens without the risk of triggering accidental touches on the interface. Our technical evaluation shows that StateLens can accurately reconstruct interfaces from stationary, hand-held, and web videos; and, a user study of the complete system demonstrates that StateLens successfully enables blind users to access otherwise inaccessible dynamic touchscreens.", "year": 2019, "ssId": "933b03a81110676f4c61c449f1926ebd58bc47f7", "arXivId": "1908.07144", "link": "https://arxiv.org/pdf/1908.07144.pdf", "openAccess": true, "authors": ["Anhong Guo", "Junhan Kong", "Michael L. Rivera", "Frank F. Xu", "Jeffrey P. Bigham"]}}
{"id": "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0", "content": {"title": "Mining Cross-Cultural Differences and Similarities in Social Media", "abstract": "Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1) mining cross-cultural differences of named entities and 2) finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.", "year": 2018, "ssId": "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0", "arXivId": null, "link": null, "openAccess": false, "authors": ["Bill Yuchen Lin", "Frank F. Xu", "Kenny Q. Zhu", "Seung-won Hwang"]}}
{"id": "0718237a30408609554a0e2b90d35e37d54b1959", "content": {"title": "Entropy-Based Subword Mining with an Application to Word Embeddings", "abstract": "Recent literature has shown a wide variety of benefits to mapping traditional onehot representations of words and phrases to lower-dimensional real-valued vectors known as word embeddings. Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform embedding by learning distinct embedding vectors for each word. Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as prefixes, suffixes, and root-words as well as compound words. Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures. This not only leads to poor embeddings for text corpora that have longtail distributions, but also heuristic methods for handling out-of-vocabulary words. In this paper we propose SubwordMine, an entropybased subword mining algorithm that is fast, unsupervised, and fully data-driven. We show that this allows for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task.", "year": 2018, "ssId": "0718237a30408609554a0e2b90d35e37d54b1959", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ahmed El-Kishky", "Frank F. Xu", "Aston Zhang", "Stephen Macke", "Jiawei Han"]}}
