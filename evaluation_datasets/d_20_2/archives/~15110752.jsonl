{"id": "28e81f96eab94e99febcaaee00637825c8a3e664", "content": {"title": "Interpretable Machine Learning", "abstract": "The emergence of machine learning as a society-changing technology in the past decade has triggered concerns about people's inability to understand the reasoning of increasingly complex models. The field of IML (interpretable machine learning) grew out of these concerns, with the goal of empowering various stakeholders to tackle use cases, such as building trust in models, performing model debugging, and generally informing real human decision-making.", "year": 2021, "ssId": "28e81f96eab94e99febcaaee00637825c8a3e664", "arXivId": null, "link": null, "openAccess": false, "authors": ["Valerie Chen", "Jeffrey Li", "Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8", "content": {"title": "Interpretable Machine Learning: Moving From Mythos to Diagnostics", "abstract": "Despite years of progress in the field of Interpretable Machine Learning (IML), a significant gap persists between the technical objectives targeted by researchers\u2019 methods and the high-level goals stated as consumers\u2019 use cases. To address this gap, we argue for the IML community to embrace a diagnostic vision for the field. Instead of viewing IML methods as a panacea for a variety of overly broad use cases, we emphasize the need to systematically connect IML methods to narrower\u2013yet better defined\u2013target use cases. To formalize this vision, we propose a taxonomy including both methods and use cases, helping to conceptualize the current gaps between the two. Then, to connect these two sides, we describe a three-step workflow to enable researchers and consumers to define and validate IML methods as useful diagnostics. Eventually, by applying this workflow, a more complete version of the taxonomy will allow consumers to find relevant methods for their target use cases and researchers to identify motivating use cases for their methods.", "year": 2021, "ssId": "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8", "arXivId": "2103.06254", "link": "https://arxiv.org/pdf/2103.06254.pdf", "openAccess": true, "authors": ["Valerie Chen", "Jeffrey Li", "Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "ef59f05a30972742a714b8903848e4b5dfc5cdaf", "content": {"title": "Towards Connecting Use Cases and Methods in Interpretable Machine Learning", "abstract": "Despite increasing interest in the field of Interpretable Machine Learning (IML), a significant gap persists between the technical objectives targeted by researchers\u2019 methods and the high-level goals of consumers\u2019 use cases. In this work, we synthesize foundational work on IML methods and evaluation into an actionable taxonomy. This taxonomy serves as a tool to conceptualize the gap between researchers and consumers, illustrated by the lack of connections between its methods and use cases components. It also provides the foundation from which we describe a three-step workflow to better enable researchers and consumers to work together to discover what types of methods are useful for what use cases. Eventually, by building on the results generated from this workflow, a more complete version of the taxonomy will increasingly allow consumers to find relevant methods for their target use cases and researchers to identify applicable use cases for their proposed methods.", "year": 2021, "ssId": "ef59f05a30972742a714b8903848e4b5dfc5cdaf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Valerie Chen", "Jeffrey Li", "Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "0d6a4e45acde6f47d704ed0752f17f7ab52223af", "content": {"title": "Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning", "abstract": "Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions.", "year": 2020, "ssId": "0d6a4e45acde6f47d704ed0752f17f7ab52223af", "arXivId": "2011.00517", "link": "https://arxiv.org/pdf/2011.00517.pdf", "openAccess": true, "authors": ["Valerie Chen", "A. Gupta", "Kenneth Marino"]}}
{"id": "4cfbd97a5b42695697f70a9f28ee29711f6ca433", "content": {"title": "Task-Aware Novelty Detection for Visual-based Deep Learning in Autonomous Systems", "abstract": "Deep-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. This ability to determine the novelty of a new input with respect to a trained model is critical for such systems because novel inputs due to changes in the environment, adversarial attacks, or even unintentional noise can potentially lead to erroneous, perhaps life-threatening decisions. This paper proposes a learning framework that leverages information learned by the prediction model in a task-aware manner to detect novel scenarios. We use network saliency to provide the learning architecture with knowledge of the input areas that are most relevant to the decision-making and learn an association between the saliency map and the predicted output to determine the novelty of the input. We demonstrate the efficacy of this method through experiments on real-world driving datasets as well as through driving scenarios in our in-house indoor driving environment where the novel image can be sampled from another similar driving dataset with similar features or from adversarial attacked images from the training dataset. We find that our method is able to systematically detect novel inputs and quantify the deviation from the target prediction through this task-aware approach.", "year": 2020, "ssId": "4cfbd97a5b42695697f70a9f28ee29711f6ca433", "arXivId": null, "link": null, "openAccess": false, "authors": ["Valerie Chen", "Man-Ki Yoon", "Zhong Shao"]}}
{"id": "7a56aba1a4d4020c4933319588b9ed2b34d51125", "content": {"title": "Secure Computation for Machine Learning With SPDZ", "abstract": "Secure Multi-Party Computation (MPC) is an area of cryptography that enables computation on sensitive data from multiple sources while maintaining privacy guarantees. However, theoretical MPC protocols often do not scale efficiently to real-world data. This project investigates the efficiency of the SPDZ framework, which provides an implementation of an MPC protocol with malicious security, in the context of popular machine learning (ML) algorithms. In particular, we chose applications such as linear regression and logistic regression, which have been implemented and evaluated using semi-honest MPC techniques. We demonstrate that the SPDZ framework outperforms these previous implementations while providing stronger security.", "year": 2019, "ssId": "7a56aba1a4d4020c4933319588b9ed2b34d51125", "arXivId": "1901.00329", "link": "https://arxiv.org/pdf/1901.00329.pdf", "openAccess": true, "authors": ["Valerie Chen", "Valerio Pastro", "Mariana Raykova"]}}
{"id": "8873d1369590249113e1f0491ce49d1502395b9c", "content": {"title": "Video-Text Compliance: Activity Verification Based on Natural Language Instructions", "abstract": "We define a new multi-modal compliance problem, which is to determine if the human activity in a given video is in compliance with an associated text instruction. Solutions to the compliance problem could enable automatic compliance checking and efficient feedback in many real-world settings. To this end, we introduce the Video-Text Compliance (VTC) dataset, which contains videos of atomic activities, along with text instructions and compliance labels. The VTC dataset is constructed by an auto-augmentation technique, preserves privacy, and contains over 1.2 million frames. Finally, we present ComplianceNet, a novel end-to-end trainable compliance network that improves the baseline accuracy by 27.5% on average when trained on the VTC dataset. We plan to release the VTC dataset to the community for future research.", "year": 2019, "ssId": "8873d1369590249113e1f0491ce49d1502395b9c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Mayoore S. Jaiswal", "H. P. Hofstee", "Valerie Chen", "Suvadip Paul", "R. Feris", "Frank Liu", "A. Jagannathan", "A. Gattiker", "Inseok Hwang", "Jinho Lee", "Matthew H Tong", "Sahil Dureja", "Soham Shah"]}}
{"id": "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6", "content": {"title": "Novelty Detection via Network Saliency in Visual-Based Deep Learning", "abstract": "Machine-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. Often viewed as a black-box, it is non-obvious to determine when a model will make a safe decision and when it will make an erroneous, perhaps life-threatening one. Prior work on novelty detection deal with highly structured data and do not translate well to dynamic, real-world situations. This paper proposes a multi-step framework for the detection of novel scenarios in vision-based autonomous systems by leveraging information learned by the trained prediction model and a new image similarity metric. We demonstrate the efficacy of this method through experiments on a real-world driving dataset as well as on our in-house indoor racing environment.", "year": 2019, "ssId": "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6", "arXivId": "1906.03685", "link": "https://arxiv.org/pdf/1906.03685.pdf", "openAccess": true, "authors": ["Valerie Chen", "Man-Ki Yoon", "Zhong Shao"]}}
