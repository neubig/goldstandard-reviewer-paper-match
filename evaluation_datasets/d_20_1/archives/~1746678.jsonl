{"id": "d5634a21b3727258822b78f5c5ababf7261a5c79", "content": {"title": "Investigating Self-Supervised Learning for Speech Enhancement and Separation", "abstract": "Speech enhancement and separation are two fundamental tasks for robust speech processing. Speech enhancement suppresses background noise while speech separation extracts target speech from interfering speakers. Despite a great number of supervised learning-based enhancement and separation methods having been proposed and achieving good performance, studies on applying self-supervised learning (SSL) to enhancement and separation are limited. In this paper, we evaluate 13 SSL upstream methods on speech enhancement and separation downstream tasks. Our experimental results on Voicebank-DEMAND and Libri2Mix show that some SSL representations consistently outperform baseline features including the short-time Fourier transform (STFT) magnitude and log Mel filterbank (FBANK). Furthermore, we analyze the factors that make existing SSL frameworks difficult to apply to speech enhancement and separation and discuss the representation properties desired for both tasks. Our study is included as the official speech enhancement and separation downstreams for SUPERB.", "year": 2022, "ssId": "d5634a21b3727258822b78f5c5ababf7261a5c79", "arXivId": "2203.07960", "link": "https://arxiv.org/pdf/2203.07960.pdf", "openAccess": true, "authors": ["Zili Huang", "Shinji Watanabe", "Shu-wen Yang", "Paola Garc\u00eda", "S. Khudanpur"]}}
{"id": "730e5e83586dd5784051f933e7bb82571cec4c94", "content": {"title": "EEND-SS: Joint End-to-End Neural Speaker Diarization and Speech Separation for Flexible Number of Speakers", "abstract": "In this paper, we present a novel framework that jointly performs speaker diarization, speech separation, and speaker counting. Our proposed method combines end-to-end speaker diarization and speech separation methods, namely, End-to-End Neural Speaker Diarization with Encoder-Decoder-based Attractor calculation (EEND-EDA) and the Convolutional Time-domain Audio Separation Network (ConvTasNet) as multi-tasking joint model. We also propose the multiple 1\u00d71 convolutional layer architecture for estimating the separation masks corresponding to the number of speakers, and a post-processing technique for re\ufb01ning the separated speech signal with speech activity. Experiments using LibriMix dataset show that our proposed method outperforms the baselines in terms of diarization and separation performance for both \ufb01xed and \ufb02exible numbers of speakers, as well as speaker counting performance for \ufb02exible numbers of speakers. All materials will be open-sourced and reproducible in ESPnet toolkit 1 .", "year": 2022, "ssId": "730e5e83586dd5784051f933e7bb82571cec4c94", "arXivId": "2203.17068", "link": "https://arxiv.org/pdf/2203.17068.pdf", "openAccess": true, "authors": ["Yushi Ueda", "Soumi Maiti", "Shinji Watanabe", "Chunlei Zhang", "Meng Yu", "Shi-Xiong Zhang", "Yong Xu"]}}
{"id": "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320", "content": {"title": "Joint Speech Recognition and Audio Captioning", "abstract": "Speech samples recorded in both indoor and outdoor environments are often contaminated with secondary audio sources. Most endto-end monaural speech recognition systems either remove these background sounds using speech enhancement or train noise-robust models. For better model interpretability and holistic understanding, we aim to bring together the growing field of automated audio captioning (AAC) and the thoroughly studied automatic speech recognition (ASR). The goal of AAC is to generate natural language descriptions of contents in audio samples. We propose several approaches for end-to-end joint modeling of ASR and AAC tasks and demonstrate their advantages over traditional approaches, which model these tasks independently. A major hurdle in evaluating our proposed approach is the lack of labeled audio datasets with both speech transcriptions and audio captions. Therefore we also create a multi-task dataset by mixing the clean speech Wall Street Journal corpus with multiple levels of background noises chosen from the AudioCaps dataset. We also perform extensive experimental evaluation and show improvements of our proposed methods as compared to existing state-of-the-art ASR and AAC methods.", "year": 2022, "ssId": "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320", "arXivId": "2202.01405", "link": "https://arxiv.org/pdf/2202.01405.pdf", "openAccess": true, "authors": ["Chaitanya Narisetty", "E. Tsunoo", "Xuankai Chang", "Yosuke Kashiwagi", "Michael Hentschel", "Shinji Watanabe"]}}
{"id": "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b", "content": {"title": "Towards Low-distortion Multi-channel Speech Enhancement: The ESPNet-SE Submission to The L3DAS22 Challenge", "abstract": "This paper describes our submission to the L3DAS22 Challenge Task 1, which consists of speech enhancement with 3D Ambisonic microphones. The core of our approach combines Deep Neural Network (DNN) driven complex spectral mapping with linear beamformers such as the multi-frame multi-channel Wiener filter. Our proposed system has two DNNs and a linear beamformer in between. Both DNNs are trained to perform complex spectral mapping, using a combination of waveform and magnitude spectrum losses. The estimated signal from the first DNN is used to drive a linear beamformer, and the beamforming result, together with this enhanced signal, are used as extra inputs for the second DNN which refines the estimation. Then, from this new estimated signal, the linear beamformer and second DNN are run iteratively. The proposed method was ranked first in the challenge, achieving, on the evaluation set, a ranking metric of 0.984, versus 0.833 of the challenge baseline.", "year": 2022, "ssId": "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b", "arXivId": "2202.12298", "link": "https://arxiv.org/pdf/2202.12298.pdf", "openAccess": true, "authors": ["Yen-Ju Lu", "Samuele Cornell", "Xuankai Chang", "Wangyou Zhang", "Chenda Li", "Zhaoheng Ni", "Zhong-Qiu Wang", "Shinji Watanabe"]}}
{"id": "97db55b196cf0c768644a392a7e6c79d1c65207e", "content": {"title": "Improving Frame-Online Neural Speech Enhancement with Overlapped-Frame Prediction", "abstract": "\u2014Frame-online speech enhancement systems in the short-time Fourier transform (STFT) domain usually have an algorithmic latency equal to the window size due to the use of the overlap-add algorithm in the inverse STFT (iSTFT). This algorithmic latency allows the enhancement models to leverage future contextual information up to a length equal to the window size. However, current frame-online systems only partially leverage this future information. To fully exploit this information, this study proposes an overlapped-frame prediction technique for deep learning based frame-online speech enhancement, where at each frame our deep neural network (DNN) predicts the current and several past frames that are necessary for overlap-add, instead of only predicting the current frame. In addition, we propose a novel loss function to account for the scale difference between predicted and oracle target signals. Evaluations results on a noisy- reverberant speech enhancement task show the effectiveness of the proposed algorithms.", "year": 2022, "ssId": "97db55b196cf0c768644a392a7e6c79d1c65207e", "arXivId": "2204.07566", "link": "https://arxiv.org/pdf/2204.07566.pdf", "openAccess": true, "authors": ["Zhongqiu Wang", "Shinji Watanabe"]}}
{"id": "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e", "content": {"title": "Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR", "abstract": "Graph-based temporal classification (GTC), a generalized form of the connectionist temporal classification loss, was recently proposed to improve automatic speech recognition (ASR) systems using graph-based supervision. For example, GTC was first used to encode an N-best list of pseudo-label sequences into a graph for semi-supervised learning. In this paper, we propose an extension of GTC to model the posteriors of both labels and label transitions by a neural network, which can be applied to a wider range of tasks. As an example application, we use the extended GTC (GTC-e) for the multi-speaker speech recognition task. The transcriptions and speaker information of multi-speaker speech are represented by a graph , where the speaker information is associated with the transitions and ASR outputs with the nodes. Using GTC-e, multi-speaker ASR modelling becomes very similar to single-speaker ASR modeling, in that tokens by multiple speakers are recognized as a single merged sequence in chronological order. For evaluation, we perform experiments on a simulated multi-speaker speech dataset derived from LibriSpeech, obtaining promising results with performance close to classical benchmarks for the task.", "year": 2022, "ssId": "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e", "arXivId": "2203.00232", "link": "https://arxiv.org/pdf/2203.00232.pdf", "openAccess": true, "authors": ["Xuankai Chang", "Niko Moritz", "Takaaki Hori", "Shinji Watanabe", "Jonathan Le Roux"]}}
{"id": "a3e3a9d878999c7038c275e75f5cd8a232aa4999", "content": {"title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities", "abstract": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pretrained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focused on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pretrained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation. \u030aEqual contribution.", "year": 2022, "ssId": "a3e3a9d878999c7038c275e75f5cd8a232aa4999", "arXivId": "2203.06849", "link": "https://arxiv.org/pdf/2203.06849.pdf", "openAccess": true, "authors": ["Hsiang-Sheng Tsai", "Heng-Jui Chang", "Wen-Chin Huang", "Zili Huang", "Kushal Lakhotia", "Shu-wen Yang", "Shuyan Dong", "Andy T. Liu", "Cheng-I Lai", "Jiatong Shi", "Xuankai Chang", "Phil Hall", "Hsuan-Jui Chen", "Shang-Wen Li", "Shinji Watanabe", "Abdel-rahman Mohamed", "Hung-yi Lee"]}}
{"id": "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629", "content": {"title": "Defining Reasonably Foreseeable Parameter Ranges Using Real-World Traffic Data for Scenario-Based Safety Assessment of Automated Vehicles", "abstract": "Verification and validation of automated driving systems\u2019 safety are some of the biggest challenges for the introduction of automated vehicles into the market. Scenario-based safety assessment is an efficient and repeatable method to test the systems\u2019 safety before their deployment in the real world. However, even with limited traffic situations identified as critical to the system behavior, there is still an open range of parameters to describe each situation. Thus, defining specific parameter ranges is crucial to realize the scenario-based safety assessment approach. This study proposes a method to parameterize scenarios extracted from real-world traffic data, analyze their distribution and correlation, and incorporate them into the definition of reasonably foreseeable parameter ranges through the contextualization of resulting ranges with reasonable risk acceptance thresholds from different fields and international environments. Representative values can be selected from these specific parameter ranges to extract specific concrete scenarios applicable for the systems safety assessment. The applicability of the proposed method is demonstrated using parameter ranges obtained to define two sets of 960 cut-in and 6,442 deceleration scenarios extracted from a new set of traffic data collected from Japanese highways under the SAKURA initiative. The outcomes will enable comparisons with traffic data from other countries and inform automated driving system developers, standardization bodies, and policymakers to develop automated vehicle safety assessments applicable internationally.", "year": 2022, "ssId": "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629", "arXivId": null, "link": null, "openAccess": false, "authors": ["H. Nakamura", "H. Muslim", "R. Kato", "Shinji Watanabe", "H. Kaneko", "H. Imanaga", "J. Antona-Makoshi", "S. Kitajima", "N. Uchida", "E. Kitahara", "K. Ozawa", "S. Taniguchi"]}}
{"id": "5c333f11431d1f0d04ced62b712c8d05ebac0891", "content": {"title": "Conditional Diffusion Probabilistic Model for Speech Enhancement", "abstract": "Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.", "year": 2022, "ssId": "5c333f11431d1f0d04ced62b712c8d05ebac0891", "arXivId": "2202.05256", "link": "https://arxiv.org/pdf/2202.05256.pdf", "openAccess": true, "authors": ["Yen-Ju Lu", "Zhongqiu Wang", "Shinji Watanabe", "Alexander Richard", "Cheng Yu", "Yu Tsao"]}}
{"id": "128610c7df12bff1610949c551b6236cb350dcd9", "content": {"title": "Improving non-autoregressive end-to-end speech recognition with pre-trained acoustic and language models", "abstract": "While Transformers have achieved promising results in end-to-end (E2E) automatic speech recognition (ASR), their autoregressive (AR) structure becomes a bottleneck for speeding up the decoding process. For real-world deployment, ASR systems are desired to be highly accurate while achieving fast inference. Non-autoregressive (NAR) models have become a popular alternative due to their fast inference speed, but they still fall behind AR systems in recognition accuracy. To fulfill the two demands, in this paper, we propose a NAR CTC/attention model utilizing both pre-trained acoustic and language models: wav2vec2.0 and BERT. To bridge the modality gap between speech and text representations obtained from the pretrained models, we design a novel modality conversion mechanism, which is more suitable for logographic languages. During inference, we employ a CTC branch to generate a target length, which enables the BERT to predict tokens in parallel. We also design a cache-based CTC/attention joint decoding method to improve the recognition accuracy while keeping the decoding speed fast. Experimental results show that the proposed NAR model greatly outperforms our strong wav2vec2.0 CTC baseline (15.1% relative CER reduction on AISHELL-1). The proposed NAR model significantly surpasses previous NAR systems on the AISHELL-1 benchmark and shows a potential for English tasks.", "year": 2022, "ssId": "128610c7df12bff1610949c551b6236cb350dcd9", "arXivId": "2201.10103", "link": "https://arxiv.org/pdf/2201.10103.pdf", "openAccess": true, "authors": ["Keqi Deng", "Zehui Yang", "Shinji Watanabe", "Yosuke Higuchi", "Gaofeng Cheng", "Pengyuan Zhang"]}}
{"id": "b5002aa334f8d0c0e1a4dedad79580e10a928c30", "content": {"title": "Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation", "abstract": "Self-Supervised Learning (SSL) models have been successfully applied in various deep learning-based speech tasks, particu-larly those with a limited amount of data. However, the quality of SSL representations depends highly on the relatedness between the SSL training domain(s) and the target data domain. On the contrary, spectral feature (SF) extractors such as log Mel-\ufb01lterbanks are hand-crafted non-learnable components, and could be more robust to domain shifts. The present work examines the assumption that combining non-learnable SF extractors to SSL models is an effective approach to low resource speech tasks. We propose a learnable and interpretable framework to combine SF and SSL representations. The proposed framework outperforms signi\ufb01cantly both baseline and SSL models on Automatic Speech Recognition (ASR) and Speech Translation (ST) tasks on three low resource datasets. We addi-tionally design a mixture of experts based combination model. This last model reveals that the relative contribution of SSL models over conventional SF extractors is very small in case of domain mismatch between SSL training set and the target language data.", "year": 2022, "ssId": "b5002aa334f8d0c0e1a4dedad79580e10a928c30", "arXivId": "2204.02470", "link": "https://arxiv.org/pdf/2204.02470.pdf", "openAccess": true, "authors": ["Dan Berrebbi", "Jiatong Shi", "Brian Yan", "Osbel Lopez-Francisco", "Jonathan D. Amith", "Shinji Watanabe"]}}
{"id": "ede108538033ae00d1667685afbd488380020613", "content": {"title": "Different efficacies of neutralizing antibodies and antiviral drugs on SARS-CoV-2 Omicron subvariants, BA.1 and BA.2", "abstract": "The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) Omicron subvariant BA.2 has spread in many countries, replacing the earlier Omicron subvariant BA.1 and other variants. Here, using a cell culture infection assay, we quantified the intrinsic sensitivity of BA.2 and BA.1 compared with other variants of concern, Alpha, Gamma, and Delta, to five approved-neutralizing antibodies and antiviral drugs. Our assay revealed the diverse sensitivities of these variants to antibodies, including the loss of response of both BA.1 and BA.2 to casirivimab and of BA.1 to imdevimab. In contrast, EIDD-1931 and nirmatrelvir showed a more conserved activities to these variants. The viral response profile combined with mathematical analysis estimated differences in antiviral effects among variants in the clinical concentrations. These analyses provide essential evidence that gives insight into variant emergence\u2019s impact on choosing optimal drug treatment.", "year": 2022, "ssId": "ede108538033ae00d1667685afbd488380020613", "arXivId": null, "link": null, "openAccess": false, "authors": ["H. Ohashi", "Takayuki Hishiki", "D. Akazawa", "Kwang Su Kim", "Joohyeon Woo", "Kaho Shionoya", "K. Tsuchimoto", "S. Iwanami", "S. Moriyama", "H. Kinoshita", "Souichi Yamada", "Y. Kuroda", "Tsukasa Yamamoto", "N. Kishida", "Shinji Watanabe", "H. Hasegawa", "H. Ebihara", "Tadaki Suzuki", "Ken Maeda", "S. Fukushi", "Yoshimasa Takahashi", "S. Iwami", "K. Watashi"]}}
{"id": "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1", "content": {"title": "STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency", "abstract": "Deep learning based speech enhancement in the short-term Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window contains more samples and the frequency resolution can be higher for potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed based on the same 32 ms window size. To reduce this inherent latency, we adapt a conventional dual window size approach, where a regular input window size is used for STFT but a shorter output window is used for the overlap-add in the iSTFT, for STFTdomain deep learning based frame-online speech enhancement. Based on this STFT and iSTFT configuration, we employ singleor multi-microphone complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the RI components predicted by the DNN to conduct frameonline beamforming, the results of which are then used as extra features for a second DNN to perform frame-online post-filtering. The frequency-domain beamforming in between the two DNNs can be easily integrated with complex spectral mapping and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation results on a noisy-reverberant speech enhancement task demonstrate the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFTdomain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms.", "year": 2022, "ssId": "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1", "arXivId": "2204.09911", "link": "https://arxiv.org/pdf/2204.09911.pdf", "openAccess": true, "authors": ["Zhong-Qiu Wang", "G. Wichern", "Shinji Watanabe", "Jonathan Le Roux"]}}
{"id": "ab48fb72541653f40523caa9fcaac9cb84bf3373", "content": {"title": "End-to-End Multi-speaker ASR with Independent Vector Analysis", "abstract": "We develop an end-to-end system for multi-channel, multi-speaker automatic speech recognition. We propose a frontend for joint source separation and dereverberation based on the independent vector analysis (IVA) paradigm. It uses the fast and stable iterative source steering algorithm together with a neural source model. The parameters from the ASR module and the neural source model are optimized jointly from the ASR loss itself. We demonstrate competitive performance with previous systems using neural beamforming frontends. First, we explore the trade-offs when using various number of channels for training and testing. Second, we demonstrate that the proposed IVA frontend performs well on noisy data, even when trained on clean mixtures only. Furthermore, it extends without retraining to the separation of more speakers, which is demonstrated on mixtures of three and four speakers.", "year": 2022, "ssId": "ab48fb72541653f40523caa9fcaac9cb84bf3373", "arXivId": "2204.00218", "link": "https://arxiv.org/pdf/2204.00218.pdf", "openAccess": true, "authors": ["Robin Scheibler", "Wangyou Zhang", "Xuankai Chang", "Shinji Watanabe", "Y. Qian"]}}
{"id": "1e4e2aceed87febcc643f1473507c9535ba5c19a", "content": {"title": "Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation", "abstract": "Although Transformers have gained success in several speech processing tasks like spoken language understanding (SLU) and speech translation (ST), achieving online processing while keeping competitive performance is still essential for real-world interaction. In this paper, we take the first step on streaming SLU and simultaneous ST using a blockwise streaming Transformer, which is based on contextual block processing and blockwise synchronous beam search. Furthermore, we design an automatic speech recognition (ASR)-based intermediate loss regularization for the streaming SLU task to improve the classification performance further. As for the simultaneous ST task, we propose a cross-lingual encoding method, which employs a CTC branch optimized with target language translations. In addition, the CTC translation output is also used to refine the search space with CTC prefix score, achieving joint CTC/attention simultaneous translation for the first time. Experiments for SLU are conducted on FSC and SLURP corpora, while the ST task is evaluated on Fisher-CallHome Spanish and MuST-C En-De corpora. Experimental results show that the blockwise streaming Transformer achieves competitive results compared to offline models, especially with our proposed methods that further yield a 2.4% accuracy gain on the SLU task and a 4.3 BLEU gain on the ST task over streaming baselines.", "year": 2022, "ssId": "1e4e2aceed87febcc643f1473507c9535ba5c19a", "arXivId": "2204.08920", "link": "https://arxiv.org/pdf/2204.08920.pdf", "openAccess": true, "authors": ["Keqi Deng", "Shinji Watanabe", "Jiatong Shi", "Siddhant Arora"]}}
{"id": "76b36a059c0d8d66a1bf910de32b34dba19482fa", "content": {"title": "Run-and-back stitch search: novel block synchronous decoding for streaming encoder-decoder ASR", "abstract": "A streaming style inference of encoder\u2013decoder automatic speech recognition (ASR) systems is important for reducing latency, which is essential for interactive use cases. To this end, we propose a novel blockwise synchronous decoding algorithm with a hybrid approach that combines endpoint prediction and endpoint post-determination. In the endpoint prediction, we compute the expectation of the number of tokens that are yet to be emitted in the encoder features of the current blocks using the CTC posterior. Based on the expectation value, the decoder predicts the endpoint to realize continuous block synchronization, as a running stitch. Meanwhile, endpoint post-determination probabilistically detects backward jump of the source\u2013target attention, which is caused by the misprediction of endpoints. Then it resumes decoding by discarding those hypotheses, as back stitch. We combine these methods into a hybrid approach, namely run-and-back stitch search, which reduces the computational cost and latency. Evaluations of various ASR tasks show the efficiency of our proposed decoding algorithm, which achieves a latency reduction, for instance in the Librispeech test set from 1487 ms to 821 ms at the 90th percentile, while maintaining a high recognition accuracy.", "year": 2022, "ssId": "76b36a059c0d8d66a1bf910de32b34dba19482fa", "arXivId": "2201.10190", "link": "https://arxiv.org/pdf/2201.10190.pdf", "openAccess": true, "authors": ["E. Tsunoo", "Chaitanya Narisetty", "Michael Hentschel", "Yosuke Kashiwagi", "Shinji Watanabe"]}}
{"id": "0431f60546381a9e91fb156236c3c7056f57081f", "content": {"title": "SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy", "abstract": "Deep learning based singing voice synthesis (SVS) systems have been demonstrated to flexibly generate singing with better qualities, compared to conventional statistical parametric based methods. However, neural systems are generally datahungry and have difficulty to reach reasonable singing quality with limited public available training data. In this work, we explore different data augmentation methods to boost the training of SVS systems, including several strategies customized to SVS based on pitch augmentation and mix-up augmentation. To further stabilize the training, we introduce the cycle-consistent training strategy. Extensive experiments on two public singing databases demonstrate that our proposed augmentation methods and the stabilizing training strategy can significantly improve the performance on both objective and subjective evaluations.", "year": 2022, "ssId": "0431f60546381a9e91fb156236c3c7056f57081f", "arXivId": "2203.17001", "link": "https://arxiv.org/pdf/2203.17001.pdf", "openAccess": true, "authors": ["Shuai Guo", "Jiatong Shi", "Tao Qian", "Shinji Watanabe", "Qin Jin"]}}
{"id": "419e714f22c3fa2599abebd630cae5595c70bdef", "content": {"title": "End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation", "abstract": "This work presents our end-to-end (E2E) automatic speech recognition (ASR) model targetting at robust speech recognition, called Integraded speech Recognition with enhanced speech Input for Self-supervised learning representation (IRIS). Compared with conventional E2E ASR models, the proposed E2E model integrates two important modules including a speech enhancement (SE) module and a self-supervised learning representation (SSLR) module. The SE module enhances the noisy speech. Then the SSLR module extracts features from enhanced speech to be used for speech recognition (ASR). To train the proposed model, we establish an efficient learning scheme. Evaluation results on the monaural CHiME-4 task show that the IRIS model achieves the best performance reported in the literature for the single-channel CHiME-4 benchmark (2.0% for the real development and 3.9% for the real test) thanks to the powerful pre-trained SSLR module and the finetuned SE module.", "year": 2022, "ssId": "419e714f22c3fa2599abebd630cae5595c70bdef", "arXivId": "2204.00540", "link": "https://arxiv.org/pdf/2204.00540.pdf", "openAccess": true, "authors": ["Xuankai Chang", "Takashi Maekaku", "Yuya Fujita", "Shinji Watanabe"]}}
{"id": "499ada382b7ce8f1cbd890e8c21500d95e20f2fe", "content": {"title": "HEAR 2021: Holistic Evaluation of Audio Representations", "abstract": "What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without \ufb01ne-tuning? The aim of the HEAR 2021 NeurIPS challenge is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR 2021 evalu-ates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.", "year": 2022, "ssId": "499ada382b7ce8f1cbd890e8c21500d95e20f2fe", "arXivId": "2203.03022", "link": "https://arxiv.org/pdf/2203.03022.pdf", "openAccess": true, "authors": ["Joseph P. Turian", "Jordie Shier", "H. Khan", "B. Raj", "Bj\u00f6rn Schuller", "C. Steinmetz", "C. Malloy", "G. Tzanetakis", "Gissel Velarde", "K. McNally", "Max Henry", "Nicolas Pinto", "Camille Noufi", "Christian Clough", "Dorien Herremans", "Eduardo Fonseca", "Jesse Engel", "J. Salamon", "P. Esling", "Pranay Manocha", "Shinji Watanabe", "Zeyu Jin", "Yonatan Bisk"]}}
{"id": "1144cc3e86b1cc4160aedddb085d7861d4b528dc", "content": {"title": "Memory-Efficient Training of RNN-Transducer with Sampled Softmax", "abstract": "RNN-Transducer has been one of promising architectures for end-to-end automatic speech recognition. Although RNN-Transducer has many advantages including its strong accuracy and streaming-friendly property, its high memory consumption during training has been a critical problem for development. In this work, we propose to apply sampled softmax to RNN-Transducer, which requires only a small subset of vocabulary during training thus saves its memory consumption. We further extend sampled softmax to optimize memory consumption for a minibatch, and employ distributions of auxiliary CTC losses for sampling vocabulary to improve model accuracy. We present experimental results on LibriSpeech, AISHELL-1, and CSJ-APS, where sampled softmax greatly reduces memory consumption and still maintains the accuracy of the baseline model.", "year": 2022, "ssId": "1144cc3e86b1cc4160aedddb085d7861d4b528dc", "arXivId": "2203.16868", "link": "https://arxiv.org/pdf/2203.16868.pdf", "openAccess": true, "authors": ["Jaesong Lee", "Lukas Lee", "Shinji Watanabe"]}}
