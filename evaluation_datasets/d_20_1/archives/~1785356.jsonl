{"id": "73b22457a2f52a834d73d73a76b4124c1cb326be", "content": {"title": "Multiplayer Performative Prediction: Learning in Decision-Dependent Games", "abstract": "Learning problems commonly exhibit an interesting feedback mechanism wherein the population data reacts to competing decision makers\u2019 actions. This paper formulates a new game theoretic framework for this phenomenon, called multi-player performative prediction . We focus on two distinct solution concepts, namely ( i ) performatively stable equilibria and ( ii ) Nash equilibria of the game. The latter equilibria are arguably more informative, but can be found e\ufb03ciently only when the game is monotone. We show that under mild assumptions, the performatively stable equilibria can be found e\ufb03ciently by a variety of algorithms, including repeated retraining and the repeated (stochastic) gradient method. We then establish transparent su\ufb03cient conditions for strong monotonicity of the game and use them to develop algorithms for \ufb01nding Nash equilibria. We investigate derivative free methods and adaptive gradient algorithms wherein each player alternates between learning a parametric description of their distribution and gradient steps on the empirical risk. Synthetic and semi-synthetic numerical experiments illustrate the results. have and correlations equilibrium the results 10 nominal", "year": 2022, "ssId": "73b22457a2f52a834d73d73a76b4124c1cb326be", "arXivId": "2201.03398", "link": "https://arxiv.org/pdf/2201.03398.pdf", "openAccess": true, "authors": ["Adhyyan Narang", "Evan Faulkner", "D. Drusvyatskiy", "M. Fazel", "L. Ratliff"]}}
{"id": "740afdd1619d797145b056877865f941891e6a65", "content": {"title": "Decision-Dependent Risk Minimization in Geometrically Decaying Dynamic Environments", "abstract": "This paper studies the problem of expected loss minimization given a data distribution that is dependent on the decision-maker\u2019s action and evolves dynamically in time according to a geometric decay process. Novel algorithms for both the information setting in which the decision-maker has a \ufb01rst or- der gradient oracle and the setting in which they have simply a loss function oracle are are introduced. The algorithms operate on the same underlying principle: the decision-maker repeatedly deploys a \ufb01xed decision repeatedly over the length of an epoch, thereby allowing the dynamically changing environment to suf\ufb01ciently mix before updating the decision. The iteration complexity in each of the settings is shown to match existing rates for \ufb01rst and zero order stochastic gradient meth- ods up to logarithmic factors. The algorithms are evaluated on a \u201csemi-synthetic\u201d example using real world data from the SFpark dynamic pricing pilot study; it is shown that the announced prices result in an improvement for the institution\u2019s objective (target occupancy), while achieving an overall re-duction in parking rates.", "year": 2022, "ssId": "740afdd1619d797145b056877865f941891e6a65", "arXivId": "2204.08281", "link": "https://arxiv.org/pdf/2204.08281.pdf", "openAccess": true, "authors": ["Mitas Ray", "D. Drusvyatskiy", "M. Fazel", "L. Ratliff"]}}
{"id": "dbdefb498b619912a726fec7c85533594a1c6a1b", "content": {"title": "Minimax Optimization with Smooth Algorithmic Adversaries", "abstract": "This paper considers minimax optimization minx maxy f(x, y) in the challenging setting where f can be both nonconvex in x and nonconcave in y. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, many fundamental issues remain in theory, such as the absence of efficiently computable optimality notions, and cyclic or diverging behavior of existing algorithms. Our framework sprouts from the practical consideration that under a computational budget, the max-player can not fully maximize f(x, \u00b7) since nonconcave maximization is NP-hard in general. So, we propose a new algorithm for the min-player to play against smooth algorithms deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles), and to find an appropriate \u201cstationary point\u201d in a polynomial number of iterations. Our framework covers practical settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further provide complementing experiments that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice.", "year": 2021, "ssId": "dbdefb498b619912a726fec7c85533594a1c6a1b", "arXivId": "2106.01488", "link": "https://arxiv.org/pdf/2106.01488.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Chi Jin", "Praneeth Netrapalli", "L. Ratliff"]}}
{"id": "ba3322280992d0425bc9e2b4c59de24857e5f4e7", "content": {"title": "Approximate Regions of Attraction in Learning with Decision-Dependent Distributions", "abstract": "As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classification and decisiondependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classifier on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classifier to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient flows of performative risk minimization. We consider the case where there may be multiple local minimizers of performative risk, motivated by situations where the initial conditions may have significant impact on the long-term behavior of the system. We provide sufficient conditions to characterize the region of attraction for the various equilibria in this settings. Additionally, we introduce the notion of performative alignment, which provides a geometric condition on the convergence of repeated risk minimization to performative", "year": 2021, "ssId": "ba3322280992d0425bc9e2b4c59de24857e5f4e7", "arXivId": "2107.00055", "link": "https://arxiv.org/pdf/2107.00055.pdf", "openAccess": true, "authors": ["Roy Dong", "L. Ratliff"]}}
{"id": "e28b9bc26f5f7eb3b0532d823713400202372da2", "content": {"title": "Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms", "abstract": "The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation. We adopt this viewpoint and model the actor and critic interaction as a two-player general-sum game with a leader-follower structure known as a Stackelberg game. Given this abstraction, we propose a meta-framework for Stackelberg actor-critic algorithms where the leader player follows the total derivative of its objective instead of the usual individual gradient. From a theoretical standpoint, we develop a policy gradient theorem for the refined update and provide a local convergence guarantee for the Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an empirical standpoint, we demonstrate via simple examples that the learning dynamics we study mitigate cycling and accelerate convergence compared to the usual gradient dynamics given cost structures induced by actor-critic formulations. Finally, experiments on OpenAI gym environments show that Stackelberg actor-critic algorithms always perform at least as well and often significantly outperform the standard actor-critic algorithm counterparts.", "year": 2021, "ssId": "e28b9bc26f5f7eb3b0532d823713400202372da2", "arXivId": "2109.12286", "link": "https://arxiv.org/pdf/2109.12286.pdf", "openAccess": true, "authors": ["Liyuan Zheng", "Tanner Fiez", "Zane Alumbaugh", "Benjamin J. Chasnov", "L. Ratliff"]}}
{"id": "e9d26b9f5e6b619bbb759a67560cb949a9f034ba", "content": {"title": "Global Convergence to Local Minmax Equilibrium in Classes of Nonconvex Zero-Sum Games", "abstract": "We study gradient descent-ascent learning dynamics with timescale separation (\u03c4 -GDA) in unconstrained continuous action zero-sum games where the minimizing player faces a nonconvex optimization problem and the maximizing player optimizes a Polyak-\u0141ojasiewicz (P\u0141) or strongly-concave (SC) objective. In contrast to past work on gradient-based learning in nonconvex-P\u0141/SC zero-sum games, we assess convergence in relation to natural game-theoretic equilibria instead of only notions of stationarity. In pursuit of this goal, we prove that the only locally stable points of the \u03c4 -GDA continuous-time limiting system correspond to strict local minmax equilibria in each class of games. For these classes of games, we exploit timescale separation to construct a potential function that when combined with the stability characterization and an asymptotic saddle avoidance result gives a global asymptotic almost-sure convergence guarantee for the discrete-time gradient descent-ascent update to a set of the strict local minmax equilibrium. Moreover, we provide convergence rates for the gradient descent-ascent dynamics with timescale separation to approximate stationary points.", "year": 2021, "ssId": "e9d26b9f5e6b619bbb759a67560cb949a9f034ba", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "L. Ratliff", "Eric V. Mazumdar", "Evan Faulkner", "Adhyyan Narang"]}}
{"id": "b5a667bf189a0cfda22bac702d97b601ae6adb6f", "content": {"title": "Zeroth-Order Methods for Convex-Concave Minmax Problems: Applications to Decision-Dependent Risk Minimization", "abstract": "Min-max optimization is emerging as a key framework for analyzing problems of robustness to strategically and adversarially generated data. We propose a random reshuffling-based gradient free Optimistic Gradient Descent-Ascent algorithm for solving convex-concave min-max problems with finite sum structure. We prove that the algorithm enjoys the same convergence rate as that of zeroth-order algorithms for convex minimization problems. We further specialize the algorithm to solve distributionally robust, decision-dependent learning problems, where gradient information is not readily available. Through illustrative simulations, we observe that our proposed approach learns models that are simultaneously robust against adversarial distribution shifts and strategic decisions from the data sources, and outperforms existing methods from the strategic classification literature.", "year": 2021, "ssId": "b5a667bf189a0cfda22bac702d97b601ae6adb6f", "arXivId": "2106.09082", "link": "https://arxiv.org/pdf/2106.09082.pdf", "openAccess": true, "authors": ["C. Maheshwari", "Chih-Yuan Chiu", "Eric V. Mazumdar", "S. Sastry", "L. Ratliff"]}}
{"id": "73271677da83a3f55523148d1b43a0501f0a35dd", "content": {"title": "Online Learning in Periodic Zero-Sum Games", "abstract": "A seminal result in game theory is von Neumann\u2019s minmax theorem, which states that zero-sum games admit an essentially unique equilibrium solution. Classical learning results build on this theorem to show that online no-regret dynamics converge to an equilibrium in a time-average sense in zero-sum games. In the past several years, a key research direction has focused on characterizing the day-to-day behavior of such dynamics. General results in this direction show that broad classes of online learning dynamics are cyclic, and formally Poincar\u00e9 recurrent, in zero-sum games. We analyze the robustness of these online learning behaviors in the case of periodic zero-sum games with a time-invariant equilibrium. This model generalizes the usual repeated game formulation while also being a realistic and natural model of a repeated competition between players that depends on exogenous environmental variations such as time-of-day effects, week-to-week trends, and seasonality. Interestingly, time-average convergence may fail even in the simplest such settings, in spite of the equilibrium being fixed. In contrast, using novel analysis methods, we show that Poincar\u00e9 recurrence provably generalizes despite the complex, non-autonomous nature of these dynamical systems.", "year": 2021, "ssId": "73271677da83a3f55523148d1b43a0501f0a35dd", "arXivId": "2111.03377", "link": "https://arxiv.org/pdf/2111.03377.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Ryann Sim", "Stratis Skoulakis", "G. Piliouras", "L. Ratliff"]}}
{"id": "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7", "content": {"title": "Improved rates for derivative free play in convex games", "abstract": "The influential work of Bravo et al. (2018) shows that derivative free play in strongly monotone games has complexity O(d/\u03b5), where \u03b5 is the target accuracy on the expected squared distance to the solution. This note shows that the efficiency estimate is actually O(d/\u03b5), which reduces to the known efficiency guarantee for the method in unconstrained optimization. The argument we present is elementary, simply interpreting the method as stochastic gradient play on a slightly perturbed strongly monotone game.", "year": 2021, "ssId": "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7", "arXivId": null, "link": null, "openAccess": false, "authors": ["D. Drusvyatskiy", "L. Ratliff"]}}
{"id": "65f632cbac465633a13b1e3f8c8c410c2f3aec3d", "content": {"title": "Stackelberg MADDPG: Learning Emergent Behaviors via Information Asymmetry in Competitive Games", "abstract": "Using competitive multi-agent reinforcement learning (MARL) methods to solve physically grounded problems, such as robust control and interactive manipulation tasks, has become more popular in the robotics community. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low quality equilibrium. This paper proposes a novel game-theoretic MARL algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the \u2018leader\u2019 and the other as the \u2018follower\u2019 in a hierarchical interaction structure wherein the leader has an information advantage: the leader in ST-MADDPG updates using its total policy gradient, meaning it differentiates through the local best response of the follower. In a simple competitive robotics environment, we show that the leader learns a better policy by exploiting this information advantage and is able to either dominate the game or alleviate the native disadvantage from the game environment. In two practical robotic problems, ST-MADDPG allows the leader to learn more sophisticated and complex strategies that work well even against an unseen strong opponent.", "year": 2021, "ssId": "65f632cbac465633a13b1e3f8c8c410c2f3aec3d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Boling Yang", "Liyuan Zheng", "L. Ratliff", "Byron Boots"]}}
{"id": "ba602ea9aaecab5a3ad243211f110ae7db4cc66a", "content": {"title": "Which Echo Chamber? Regions of Attraction in Learning with Decision-Dependent Distributions", "abstract": "As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classification and decision-dependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classifier on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classifier to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient flows of performative risk minimization. We consider the case where there may be multiple local minimizers of performative risk, motivated by real world situations where the initial conditions may have significant impact on the long-term behavior of the system. As a motivating example, we consider a company whose current employee demographics affect the applicant pool they interview: the initial demographics of the company can affect the long-term hiring policies of the company. We provide sufficient conditions to characterize the region of attraction for the various equilibria in this settings. Additionally, we introduce the notion of performative alignment, which provides a geometric condition on the convergence of repeated risk minimization to performative risk minimizers.", "year": 2021, "ssId": "ba602ea9aaecab5a3ad243211f110ae7db4cc66a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Roy Dong", "L. Ratliff"]}}
{"id": "595a79ca667258ca2a4f5e7775e95a0fb0a0f024", "content": {"title": "Improved Rates for Derivative Free Gradient Play in Strongly Monotone Games", "abstract": "The in\ufb02uential work of Bravo et al. (2018) shows that derivative free gradient play in strongly monotone games has complexity O ( d 2 /\u03b5 3 ), where \u03b5 is the target accuracy on the expected squared distance to the solution. This paper shows that the e\ufb03ciency estimate is actually O ( d 2 /\u03b5 2 ), which reduces to the known e\ufb03ciency guarantee for the method in unconstrained optimization. The argument we present simply interprets the method as stochastic gradient play on a slightly perturbed strongly monotone game to achieve the improved rate.", "year": 2021, "ssId": "595a79ca667258ca2a4f5e7775e95a0fb0a0f024", "arXivId": "2111.09456", "link": "https://arxiv.org/pdf/2111.09456.pdf", "openAccess": true, "authors": ["D. Drusvyatskiy", "M. Fazel", "L. Ratliff"]}}
{"id": "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d", "content": {"title": "Modeling Curbside Parking as a Network of Finite Capacity Queues", "abstract": "Paid curbside parking can be advantageously modeled as a network of interdependent queues. To this end, we introduce methods for analyzing a special class of networks of finite capacity queues where drivers arrive from an exogenous source, join the queue if there is an available parking space, or continue to search at an adjacent queue for an available space. Furthermore, we apply this model to estimate the proportion of drivers cruising in the neighborhood of Belltown, Seattle, WA, USA. Using occupancy approximated by parking transaction data, we estimate the percentage of drivers cruising for curbside parking by comparing the rate of drivers unable to find parking to bulk through-traffic measurement data. We find percentages of up to 50% for a Belltown\u2019s 1st Ave. depending on the time, day, and direction of travel. We then calculate a per vehicle travel-time cost to social welfare incurred by this proportion: upward of a 10% increase in travel time to all drivers along 1st Ave. Last, we introduce a simulation tool and test assumptions made when estimating interesting performance metrics like the probability of a block-face being full. Our results suggest that while assuming exponential service time distributions is not justified, mean rate solutions under a Markovian relaxation of the problem is comparable to service times representative of parking transaction data in simulation.", "year": 2020, "ssId": "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chase P. Dowling", "L. Ratliff", "Baosen Zhang"]}}
{"id": "8f6763b339363216794f48895b9381d1a7caa88c", "content": {"title": "Disturbance Decoupling for Gradient-Based Multi-Agent Learning With Quadratic Costs", "abstract": "Motivated by applications of multi-agent learning in noisy environments, this letter studies the robustness of gradient-based learning dynamics with respect to disturbances. While disturbances injected along a coordinate corresponding to any individual player\u2019s actions can always affect the overall learning dynamics, a subset of players can be disturbance decoupled\u2014i.e., such players\u2019 actions are completely unaffected by the injected disturbance. We provide necessary and sufficient conditions to guarantee this property for games with quadratic cost functions, which encompass quadratic one-shot continuous games, finite-horizon linear quadratic (LQ) dynamic games, and bilinear games. Specifically, disturbance decoupling is characterized by both algebraic and graph-theoretic conditions on the learning dynamics, the latter is obtained by constructing a game graph based on gradients of players\u2019 costs. For LQ games, we show that disturbance decoupling imposes constraints on the controllable and unobservable subspaces of players. For two player bilinear games, we show that disturbance decoupling within a player\u2019s action coordinates imposes constraints on the payoff matrices. Illustrative numerical examples are provided.", "year": 2020, "ssId": "8f6763b339363216794f48895b9381d1a7caa88c", "arXivId": "2007.07228", "link": "https://arxiv.org/pdf/2007.07228.pdf", "openAccess": true, "authors": ["Sarah H. Q. Li", "L. Ratliff", "Beh\u00e7et A\u00e7ikmese"]}}
{"id": "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4", "content": {"title": "Safe Reinforcement Learning of Control-Affine Systems with Vertex Networks", "abstract": "This paper focuses on finding reinforcement learning policies for control systems with hard state and action constraints. Despite its success in many domains, reinforcement learning is challenging to apply to problems with hard constraints, especially if both the state variables and actions are constrained. Previous works seeking to ensure constraint satisfaction, or safety, have focused on adding a projection step to a learned policy. Yet, this approach requires solving an optimization problem at every policy execution step, which can lead to significant computational costs. \nTo tackle this problem, this paper proposes a new approach, termed Vertex Networks (VNs), with guarantees on safety during exploration and on learned control policies by incorporating the safety constraints into the policy network architecture. Leveraging the geometric property that all points within a convex set can be represented as the convex combination of its vertices, the proposed algorithm first learns the convex combination weights and then uses these weights along with the pre-calculated vertices to output an action. The output action is guaranteed to be safe by construction. Numerical examples illustrate that the proposed VN algorithm outperforms vanilla reinforcement learning in a variety of benchmark control tasks.", "year": 2020, "ssId": "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4", "arXivId": "2003.09488", "link": "https://arxiv.org/pdf/2003.09488.pdf", "openAccess": true, "authors": ["Liyuan Zheng", "Yuanyuan Shi", "L. Ratliff", "Baosen Zhang"]}}
{"id": "ab42ad9698386cc15a30a8c7885fa82b260f537b", "content": {"title": "Gaussian Mixture Models for Parking Demand Data", "abstract": "To mitigate congestion caused by drivers cruising in search of parking, performance-based pricing schemes have received a significant amount of attention. However, several recent studies suggest location, time-of-day, and awareness of policies are the primary factors that drive parking decisions. Harnessing data provided by the Seattle Department of Transportation and considering the aforementioned decision-making factors, we analyze the spatial and temporal properties of curbside parking demand and propose methods that can improve traditional policies with straightforward modifications by advancing the understanding of where and when to administer pricing policies. Specifically, we develop a Gaussian mixture model based technique to identify zones with similar parking demand as quantified by spatial autocorrelation. In support of this technique, we introduce a metric based on the repeatability of our Gaussian mixture model to investigate temporal consistency.", "year": 2020, "ssId": "ab42ad9698386cc15a30a8c7885fa82b260f537b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "L. Ratliff"]}}
{"id": "9c78481004b7dbb601b83cc081ec23c02e6f5270", "content": {"title": "Stability of Gradient Learning Dynamics in Continuous Games: Scalar Action Spaces", "abstract": "Learning processes in games explain how players grapple with one another in seeking an equilibrium. We study a natural model of learning based on individual gradients in two- player continuous games. In such games, the arguably natural notion of a local equilibrium is a differential Nash equilibrium. However, the set of locally exponentially stable equilibria of the learning dynamics do not necessarily coincide with the set of differential Nash equilibria of the corresponding game. To characterize this gap, we provide formal guarantees for the stability or instability of such fixed points by leveraging the spectrum of the linearized game dynamics. We provide a comprehensive understanding of scalar games and find that equilibria that are both stable and Nash are robust to variations in learning rates.", "year": 2020, "ssId": "9c78481004b7dbb601b83cc081ec23c02e6f5270", "arXivId": "2011.03650", "link": "https://arxiv.org/pdf/2011.03650.pdf", "openAccess": true, "authors": ["Benjamin J. Chasnov", "Daniel J. Calderone", "Beh\u00e7et A\u00e7ikmese", "Samuel A. Burden", "L. Ratliff"]}}
{"id": "568462ab0a0a59a2575b70db2cd9022572526f3f", "content": {"title": "Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study", "abstract": "Contemporary work on learning in continuous games has commonly overlooked the hierarchical decision-making structure present in machine learning problems formulated as games, instead treating them as simultaneous play games and adopting the Nash equilibrium solution concept. We deviate from this paradigm and provide a comprehensive study of learning in Stackelberg games. This work provides insights into the optimization landscape of zero-sum games by establishing connections between Nash and Stackelberg equilibria along with the limit points of simultaneous gradient descent. We derive novel gradient-based learning dynamics emulating the natural structure of a Stackelberg game using the implicit function theorem and provide convergence analysis for deterministic and stochastic updates for zero-sum and general-sum games. Notably, in zero-sum games using deterministic updates, we show the only critical points the dynamics converge to are Stackelberg equilibria and provide a local convergence rate. Empirically, our learning dynamics mitigate rotational behavior and exhibit benefits for training generative adversarial networks compared to simultaneous gradient descent.", "year": 2020, "ssId": "568462ab0a0a59a2575b70db2cd9022572526f3f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Tanner Fiez", "Benjamin J. Chasnov", "L. Ratliff"]}}
{"id": "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4", "content": {"title": "Constrained Upper Confidence Reinforcement Learning with Known Dynamics", "abstract": "Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. The algorithm, C-UCRL, is shown to have sub-linear regret (O(T 3 4 \u221a log(T/\u03b4))) with respect to the reward while satisfying the constraints even while learning with probability 1\u2212 \u03b4. An illustrative example is provided.", "year": 2020, "ssId": "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Liyuan Zheng", "L. Ratliff", "M. Zeilinger"]}}
{"id": "90b9d19af75c86f42865052c21305c70f884b5fe", "content": {"title": "Uncertainty in Multicommodity Routing Networks: When Does It Help?", "abstract": "We study the equilibrium behavior in a multicommodity selfish routing game with uncertain users, where each user over- or underestimates their congestion costs by a multiplicative factor. Surprisingly, we find that uncertainties in different directions have qualitatively distinct impacts on equilibria. Namely, contrary to the usual notion that uncertainty increases inefficiencies, network congestion decreases when users overestimate their costs. On the other hand, underestimation of costs leads to increased congestion. We apply these results to urban transportation networks, where drivers have different estimates about the cost of congestion. In light of the dynamic pricing policies aimed at tackling congestion, our results indicate that users' perception of these prices can significantly impact the policy's efficacy, and \u201ccaution in the face of uncertainty\u201d leads to favorable network conditions.", "year": 2020, "ssId": "90b9d19af75c86f42865052c21305c70f884b5fe", "arXivId": null, "link": null, "openAccess": false, "authors": ["S. Sekar", "Liyuan Zheng", "L. Ratliff", "Baosen Zhang"]}}
