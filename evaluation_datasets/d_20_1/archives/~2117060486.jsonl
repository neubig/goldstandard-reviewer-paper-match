{"id": "28e81f96eab94e99febcaaee00637825c8a3e664", "content": {"title": "Interpretable Machine Learning", "abstract": "The emergence of machine learning as a society-changing technology in the past decade has triggered concerns about people's inability to understand the reasoning of increasingly complex models. The field of IML (interpretable machine learning) grew out of these concerns, with the goal of empowering various stakeholders to tackle use cases, such as building trust in models, performing model debugging, and generally informing real human decision-making.", "year": 2021, "ssId": "28e81f96eab94e99febcaaee00637825c8a3e664", "arXivId": null, "link": null, "openAccess": false, "authors": ["Valerie Chen", "Jeffrey Li", "Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "09e4e0eee756da5658c6d572871130d53a89c72b", "content": {"title": "Bayesian Persuasion for Algorithmic Recourse", "abstract": "When subjected to automated decision-making, decision-subjects will strategically modify their observable features in ways they believe will maximize their chances of receiving a desirable outcome. In many situations, the underlying predictive model is deliberately kept secret to avoid gaming and maintain competitive advantage. This opacity forces the decision subjects to rely on incomplete information when making strategic feature modifications. We capture such settings as a game of Bayesian persuasion, in which the decision-maker sends a signal, e.g., an action recommendation, to a decision subject to incentivize them to take desirable actions. We formulate the decision-maker\u2019s problem of finding the optimal Bayesian incentive-compatible (BIC) action recommendation policy as an optimization problem and characterize the solution via a linear program. Through this characterization, we observe that while the problem of finding the optimal BIC recommendation policy can be simplified dramatically, the computational complexity of solving this linear program is closely tied to (1) the relative size of the decision-subjects\u2019 action space, and (2) the number of features utilized by the underlying predictive model. Finally, we provide bounds on the performance of the optimal BIC recommendation policy and show that it can lead to arbitrarily better outcomes compared to standard baselines.", "year": 2021, "ssId": "09e4e0eee756da5658c6d572871130d53a89c72b", "arXivId": "2112.06283", "link": "https://arxiv.org/pdf/2112.06283.pdf", "openAccess": true, "authors": ["Keegan Harris", "Valerie Chen", "Joon Sik Kim", "Ameet S. Talwalkar", "Hoda Heidari", "Zhiwei Steven Wu"]}}
{"id": "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8", "content": {"title": "Sanity Simulations for Saliency Methods", "abstract": "Saliency methods are a popular class of feature attribution tools that aim to capture a model\u2019s predictive reasoning by identifying \u201cimportant\u201d pixels in an input image. However, the development and adoption of saliency methods are currently hindered by the lack of access to underlying model reasoning, which prevents accurate method evaluation. In this work, we design a synthetic evaluation framework, SMERF, that allows us to perform ground-truth-based evaluation of saliency methods while controlling the underlying complexity of model reasoning. Experimental evaluations via SMERF reveal significant limitations in existing saliency methods, especially given the relative simplicity of SMERF\u2019s synthetic evaluation tasks. Moreover, the SMERF benchmarking suite represents a useful tool in the development of new saliency methods to potentially overcome these limitations.", "year": 2021, "ssId": "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8", "arXivId": "2105.06506", "link": "https://arxiv.org/pdf/2105.06506.pdf", "openAccess": true, "authors": ["Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "ef59f05a30972742a714b8903848e4b5dfc5cdaf", "content": {"title": "Towards Connecting Use Cases and Methods in Interpretable Machine Learning", "abstract": "Despite increasing interest in the field of Interpretable Machine Learning (IML), a significant gap persists between the technical objectives targeted by researchers\u2019 methods and the high-level goals of consumers\u2019 use cases. In this work, we synthesize foundational work on IML methods and evaluation into an actionable taxonomy. This taxonomy serves as a tool to conceptualize the gap between researchers and consumers, illustrated by the lack of connections between its methods and use cases components. It also provides the foundation from which we describe a three-step workflow to better enable researchers and consumers to work together to discover what types of methods are useful for what use cases. Eventually, by building on the results generated from this workflow, a more complete version of the taxonomy will increasingly allow consumers to find relevant methods for their target use cases and researchers to identify applicable use cases for their proposed methods.", "year": 2021, "ssId": "ef59f05a30972742a714b8903848e4b5dfc5cdaf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Valerie Chen", "Jeffrey Li", "Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8", "content": {"title": "Interpretable Machine Learning: Moving From Mythos to Diagnostics", "abstract": "Despite years of progress in the field of Interpretable Machine Learning (IML), a significant gap persists between the technical objectives targeted by researchers\u2019 methods and the high-level goals stated as consumers\u2019 use cases. To address this gap, we argue for the IML community to embrace a diagnostic vision for the field. Instead of viewing IML methods as a panacea for a variety of overly broad use cases, we emphasize the need to systematically connect IML methods to narrower\u2013yet better defined\u2013target use cases. To formalize this vision, we propose a taxonomy including both methods and use cases, helping to conceptualize the current gaps between the two. Then, to connect these two sides, we describe a three-step workflow to enable researchers and consumers to define and validate IML methods as useful diagnostics. Eventually, by applying this workflow, a more complete version of the taxonomy will allow consumers to find relevant methods for their target use cases and researchers to identify motivating use cases for their methods.", "year": 2021, "ssId": "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8", "arXivId": "2103.06254", "link": "https://arxiv.org/pdf/2103.06254.pdf", "openAccess": true, "authors": ["Valerie Chen", "Jeffrey Li", "Joon Sik Kim", "Gregory Plumb", "Ameet S. Talwalkar"]}}
{"id": "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c", "content": {"title": "PLLay: Efficient Topological Layer based on Persistent Landscapes", "abstract": "We propose PLLay, a novel topological layer for general deep learning models based on persistence landscapes, in which we can efficiently exploit the underlying topological features of the input data structure. In this work, we show differentiability with respect to layer inputs, for a general persistent homology with arbitrary filtration. Thus, our proposed layer can be placed anywhere in the network and feed critical information on the topological features of input data into subsequent layers to improve the learnability of the networks toward a given task. A task-optimal structure of PLLay is learned during training via backpropagation, without requiring any input featurization or data preprocessing. We provide a novel adaptation for the DTM function-based filtration, and show that the proposed layer is robust against noise and outliers through a stability analysis. We demonstrate the effectiveness of our approach by classification experiments on various datasets.", "year": 2020, "ssId": "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kwangho Kim", "Jisu Kim", "M. Zaheer", "Joon Sik Kim", "F. Chazal", "L. Wasserman"]}}
{"id": "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa", "content": {"title": "Automated Dependency Plots", "abstract": "In practical applications of machine learning, it is necessary to look beyond standard metrics such as test accuracy in order to validate various qualitative properties such as monotonicity with respect to a feature or combination of features, checking for undesirable changes or oscillations in the response, and differences in outcomes (e.g., discrimination) for a protected class. Partial dependence plots (PDP), including instance-specific PDPs (i.e., ICE plots), have been widely used as a visual way to understand or validate a model. In particular, PDPs visualize the model response as one feature is changed while holding other features fixed via an intuitive line graph. Yet, current PDPs suffer from two main drawbacks: (1) a user must manually sort or select interesting plots, and (2) PDPs are usually limited to plots along a single feature. To address these drawbacks, we formalize a method for automating the selection of interesting PDPs and extend PDPs beyond showing single features to show the model response along arbitrary directions, for example in raw feature spaces or a latent space arising from some generative model. We demonstrate the usefulness of our proposed PDP generalization across multiple use-cases and datasets including selecting between two models and understanding out-of-sample behavior.", "year": 2020, "ssId": "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa", "arXivId": null, "link": null, "openAccess": false, "authors": ["David I. Inouye", "Liu Leqi", "Joon Sik Kim", "Bryon Aragam", "Pradeep Ravikumar"]}}
{"id": "02a83a01d6236149e4ead01e202b2453f9590e9e", "content": {"title": "FACT: A Diagnostic for Group Fairness Trade-offs", "abstract": "Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conflict with one another, often with a necessary cost in loss of model's predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness-confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classifiers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness.", "year": 2020, "ssId": "02a83a01d6236149e4ead01e202b2453f9590e9e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Joon Sik Kim", "Jiahao Chen", "Ameet S. Talwalkar"]}}
{"id": "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f", "content": {"title": "Efficient Topological Layer based on Persistent Landscapes", "abstract": "We propose a novel topological layer for general deep learning models based on persistent landscapes, in which we can efficiently exploit underlying topological features of the input data structure. We use the robust DTM function and show differentiability with respect to layer inputs, for a general persistent homology with arbitrary filtration. Thus, our proposed layer can be placed anywhere in the network architecture and feed critical information on the topological features of input data into subsequent layers to improve the learnability of the networks toward a given task. A task-optimal structure of the topological layer is learned during training via backpropagation, without requiring any input featurization or data preprocessing. We provide a tight stability theorem, and show that the proposed layer is robust towards noise and outliers. We demonstrate the effectiveness of our approach by classification experiments on various datasets.", "year": 2020, "ssId": "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f", "arXivId": "2002.02778", "link": "https://arxiv.org/pdf/2002.02778.pdf", "openAccess": true, "authors": ["Kwangho Kim", "Jisu Kim", "Joon Sik Kim", "F. Chazal", "L. Wasserman"]}}
{"id": "4cc97c3858b558b4fa80ad73a894fcc7df841114", "content": {"title": "Model-Agnostic Characterization of Fairness Trade-offs", "abstract": "There exist several inherent trade-offs in designing a fair model, such as those between the model's predictive performance and fairness, or even among different notions of fairness. In practice, exploring these trade-offs requires significant human and computational resources. We propose a diagnostic that enables practitioners to explore these trade-offs without training a single model. Our work hinges on the observation that many widely-used fairness definitions can be expressed via the fairness-confusion tensor, an object obtained by splitting the traditional confusion matrix according to protected data attributes. Optimizing accuracy and fairness objectives directly over the elements in this tensor yields a data-dependent yet model-agnostic way of understanding several types of trade-offs. We further leverage this tensor-based perspective to generalize existing theoretical impossibility results to a wider range of fairness definitions. Finally, we demonstrate the usefulness of the proposed diagnostic on synthetic and real datasets.", "year": 2020, "ssId": "4cc97c3858b558b4fa80ad73a894fcc7df841114", "arXivId": "2004.03424", "link": "https://arxiv.org/pdf/2004.03424.pdf", "openAccess": true, "authors": ["Joon Sik Kim", "Jiahao Chen", "Ameet S. Talwalkar"]}}
{"id": "6eb974721719056ba8dc74a898c64ae1d081e0ae", "content": {"title": "Diagnostic Curves for Black Box Models", "abstract": "In safety-critical applications of machine learning, it is often necessary to look beyond standard metrics such as test accuracy in order to validate various qualitative properties such as monotonicity with respect to a feature or combination of features, checking for undesirable changes or oscillations in the response, and differences in outcomes (e.g. discrimination) for a protected class. To help answer this need, we propose a framework for approximately validating (or invalidating) various properties of a black box model by finding a univariate diagnostic curve in the input space whose output maximally violates a given property. These diagnostic curves show the exact value of the model along the curve and can be displayed with a simple and intuitive line graph. We demonstrate the usefulness of these diagnostic curves across multiple use-cases and datasets including selecting between two models and understanding out-of-sample behavior.", "year": 2019, "ssId": "6eb974721719056ba8dc74a898c64ae1d081e0ae", "arXivId": "1912.01108", "link": "https://arxiv.org/pdf/1912.01108.pdf", "openAccess": true, "authors": ["David I. Inouye", "Liu Leqi", "Joon Sik Kim", "Bryon Aragam", "Pradeep Ravikumar"]}}
{"id": "7a1a202e268ccc910e8044be556e56aa9eb5a94f", "content": {"title": "Automated Dependence Plots", "abstract": "In practical applications of machine learning, it is necessary to look beyond standard metrics such as test accuracy in order to validate various qualitative properties of a model. Partial dependence plots (PDP), including instance-specific PDPs (i.e., ICE plots), have been widely used as a visual tool to understand or validate a model. Yet, current PDPs suffer from two main drawbacks: (1) a user must manually sort or select interesting plots, and (2) PDPs are usually limited to plots along a single feature. To address these drawbacks, we formalize a method for automating the selection of interesting PDPs and extend PDPs beyond showing single features to show the model response along arbitrary directions, for example in raw feature space or a latent space arising from some generative model. We demonstrate the usefulness of our automated dependence plots (ADP) across multiple use-cases and datasets including model selection, bias detection, understanding out-of-sample behavior, and exploring the latent space of a generative model.", "year": 2019, "ssId": "7a1a202e268ccc910e8044be556e56aa9eb5a94f", "arXivId": null, "link": null, "openAccess": false, "authors": ["David I. Inouye", "Liu Leqi", "Joon Sik Kim", "Bryon Aragam", "Pradeep Ravikumar"]}}
{"id": "a34954d9e36ea6c57743f55124a6ae444b951c2c", "content": {"title": "Representer Point Selection for Explaining Deep Neural Networks", "abstract": "We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.", "year": 2018, "ssId": "a34954d9e36ea6c57743f55124a6ae444b951c2c", "arXivId": "1811.09720", "link": "https://arxiv.org/pdf/1811.09720.pdf", "openAccess": true, "authors": ["Chih-Kuan Yeh", "Joon Sik Kim", "I. E. Yen", "Pradeep Ravikumar"]}}
{"id": "bb80f7d2269308c3e91da8c47b290645e9d3d7d5", "content": {"title": "A Rotation Invariant Latent Factor Model for Moveme Discovery from Static Poses", "abstract": "We tackle the problem of learning a rotation invariant latent factor model when the training data is comprised of lower-dimensional projections of the original feature space. The main goal is the discovery of a set of 3-D bases poses that can characterize the manifold of primitive human motions, or movemes, from a training set of 2-D projected poses obtained from still images taken at various camera angles. The proposed technique for basis discovery is data-driven rather than hand-designed. The learned representation is rotation invariant, and can reconstruct any training instance from multiple viewing angles. We apply our method to modeling human poses in sports (via the Leeds Sports Dataset), and demonstrate the effectiveness of the learned bases in a range of applications such as activity classification, inference of dynamics from a single frame, and synthetic representation of movements.", "year": 2016, "ssId": "bb80f7d2269308c3e91da8c47b290645e9d3d7d5", "arXivId": "1609.07495", "link": "https://arxiv.org/pdf/1609.07495.pdf", "openAccess": true, "authors": ["M. R. Ronchi", "Joon Sik Kim", "Yisong Yue"]}}
