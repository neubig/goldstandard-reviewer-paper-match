{"id": "43d82bc8203c09edc7eb6b2bedcf4ab500690852", "content": {"title": "The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer", "abstract": "Large pre-trained multilingual models such as mBERT and XLM-R enabled effective crosslingual zero-shot transfer in many NLP tasks. A cross-lingual adjustment of these models using a small parallel corpus can potentially further improve results. This is a more data efficient method compared to training a machine-translation system or a multi-lingual model from scratch using only parallel data. In this study, we experiment with zero-shot transfer of English models to four typologically different languages (Spanish, Russian, Vietnamese, and Hindi) and three NLP tasks (QA, NLI, and NER). We carry out a cross-lingual adjustment of an off-the-shelf mBERT model. We confirm prior finding that this adjustment makes embeddings of semantically similar words from different languages closer to each other, while keeping unrelated words apart. However, from the paired-differences histograms introduced in our work we can see that the adjustment only modestly affects the relative distances between related and unrelated words. In contrast, fine-tuning of mBERT on English data (for a specific task such as NER) draws embeddings of both related and unrelated words closer to each other. The cross-lingual adjustment of mBERT improves NLI in four languages and NER in two languages, while QA performance never improves and sometimes degrades. When we fine-tune a cross-lingual adjusted mBERT for a specific task (e.g., NLI), the cross-lingual adjustment of mBERT may still improve the separation between related and related words, but this works consistently only for the XNLI task. Our study contributes to a better understanding of cross-lingual transfer capabilities of large multilingual language models and of effectiveness of their cross-lingual adjustment in various NLP tasks.", "year": 2022, "ssId": "43d82bc8203c09edc7eb6b2bedcf4ab500690852", "arXivId": "2204.06457", "link": "https://arxiv.org/pdf/2204.06457.pdf", "openAccess": true, "authors": ["Pavel Efimov", "Leonid Boytsov", "E. Arslanova", "Pavel Braslavski"]}}
{"id": "6ccac8a95bc77549b98d045db6d5e0de3d356ba4", "content": {"title": "Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency Benefits", "abstract": "We study the utility of the lexical translation model (IBM Model 1) for English text retrieval, in particular, its neural variants that are trained end-to-end. We use the neural Model1 as an aggregator layer applied to context-free or contextualized query/document embeddings. This new approach to design a neural ranking system has benefits for effectiveness, efficiency, and interpretability. Specifically, we show that adding an interpretable neural Model 1 layer on top of BERT-based contextualized embeddings (1) does not decrease accuracy and/or efficiency; and (2) may overcome the limitation on the maximum sequence length of existing BERT models. The context-free neural Model 1 is less effective than a BERT-based ranking model, but it can run efficiently on a CPU (without expensive index-time precomputation or query-time operations on large tensors). Using Model 1 we produced best neural and non-neural runs on the MS MARCO document ranking leaderboard in late 2020.", "year": 2021, "ssId": "6ccac8a95bc77549b98d045db6d5e0de3d356ba4", "arXivId": "2102.06815", "link": "https://arxiv.org/pdf/2102.06815.pdf", "openAccess": true, "authors": ["Leonid Boytsov", "Zico Kolter"]}}
{"id": "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5", "content": {"title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models", "abstract": "Due to high annotation costs making the best use of existing human-created training data is an important research direction. We, therefore, carry out a systematic evaluation of transferability of BERT-based neural ranking models across five English datasets. Previous studies focused primarily on zero-shot and few-shot transfer from a large dataset to a dataset with a small number of queries. In contrast, each of our collections has a substantial number of queries, which enables a full-shot evaluation mode and improves reliability of our results. Furthermore, since source datasets licences often prohibit commercial use, we compare transfer learning to training on pseudo-labels generated by a BM25 scorer. We find that training on pseudo-labels---possibly with subsequent fine-tuning using a modest number of annotated queries---can produce a competitive or better model compared to transfer learning. Yet, it is necessary to improve the stability and/or effectiveness of the few-shot training, which, sometimes, can degrade performance of a pretrained model.", "year": 2021, "ssId": "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5", "arXivId": "2103.03335", "link": "https://arxiv.org/pdf/2103.03335.pdf", "openAccess": true, "authors": ["Iurii Mokrii", "Leonid Boytsov", "Pavel Braslavski"]}}
{"id": "54316d2861eb3d575a8c7d071f4cf7c2fc30be01", "content": {"title": "Empirical robustification of pre-trained classifiers", "abstract": "Most pre-trained classifiers, though they may work extremely well on the domain they were trained upon, are not trained in a robust fashion, and therefore are susceptible to adversarial attacks. A recent technique, denoised-smoothing, demonstrated that it was possible to create certifiably robust classifiers from a pre-trained classifier (without any retraining) by pre-pending a denoising network and wrapping the entire pipeline within randomized smoothing. However, this is a costly procedure, which requires multiple queries due to the randomized smoothing element, and which ultimately is very dependent on the quality of the denoiser. In this paper, we demonstrate that a more conventional adversarial training approach also works when applied to this robustification process. Specifically, we show that by training an image-to-image translation model, prepended to a pre-trained classifier, with losses that optimize for both the fidelity of the image reconstruction and the adversarial performance of the end-to-end system, we can robustify pre-trained classifiers to a higher empirical degree of accuracy than denoised smoothing, while being more efficient at inference time. Furthermore, these robustifers are also transferable to some degree across multiple classifiers and even some architectures, illustrating that in some real sense they are removing the \u201cadversarial manifold\u201d from the input data, a task that has traditionally been very challenging for \u201cconventional\u201d preprocessing methods. This work is sponsored by DARPA grant HR11002020006 Bosch Center for Artificial Intelligence, Pittsburgh, USA Carnegie-Mellon University, Pittsburgh, USA. Correspondence to: Mohammad Norouzzadeh <arash.norouzzadeh@us.bosch.com>. Accepted by the ICML 2021 workshop on A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning. Copyright 2021 by the author(s).", "year": 2021, "ssId": "54316d2861eb3d575a8c7d071f4cf7c2fc30be01", "arXivId": null, "link": null, "openAccess": false, "authors": ["M. S. Norouzzadeh", "Wan-Yi Lin", "Leonid Boytsov", "Leslie Rice", "Huan Zhang", "Filipe Condessa", "J. Z. Kolter"]}}
{"id": "1ca247158522991ad54cccaac6c6938576a8bd26", "content": {"title": "Traditional IR rivals neural models on the MS~MARCO Document Ranking Leaderboard", "abstract": "This short document describes a traditional IR system that achieved MRR@100 equal to 0.298 on the MS MARCO Document Ranking leaderboard (on 2020-12-06). Although inferior to most BERT-based models, it outperformed several neural runs (as well as all non-neural ones), including two submissions that used a large pretrained Transformer model for re-ranking. We provide software and data to reproduce our results.", "year": 2020, "ssId": "1ca247158522991ad54cccaac6c6938576a8bd26", "arXivId": "2012.08020", "link": "https://arxiv.org/pdf/2012.08020.pdf", "openAccess": true, "authors": ["Leonid Boytsov"]}}
{"id": "58737fba500075136ee0f33f7801a5ac7f82ab68", "content": {"title": "Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants", "abstract": "When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.\u2019s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity \u201cmatter\u201d? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene\u2019s often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.", "year": 2020, "ssId": "58737fba500075136ee0f33f7801a5ac7f82ab68", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chris Kamphuis", "A. D. Vries", "Leonid Boytsov", "Jimmy J. Lin"]}}
{"id": "2c9158e20f58df04a6c5cd54dd3ee7d8df656421", "content": {"title": "Flexible retrieval with NMSLIB and FlexNeuART", "abstract": "Our objective is to introduce to the NLP community NMSLIB, describe a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations (with weights learned from training data), which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations (e.g., Lucene), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage.", "year": 2020, "ssId": "2c9158e20f58df04a6c5cd54dd3ee7d8df656421", "arXivId": "2010.14848", "link": "https://arxiv.org/pdf/2010.14848.pdf", "openAccess": true, "authors": ["Leonid Boytsov", "Eric Nyberg"]}}
{"id": "05b0c768ecd4a82e486923e83250ddd53bacbf67", "content": {"title": "Pruning Algorithms for Low-Dimensional Non-metric k-NN Search: A Case Study", "abstract": "We focus on low-dimensional non-metric search, where tree-based approaches permit efficient and accurate retrieval while having short indexing time. These methods rely on space partitioning and require a pruning rule to avoid visiting unpromising parts. We consider two known data-driven approaches to extend these rules to non-metric spaces: TriGen and a piece-wise linear approximation of the pruning rule. We propose and evaluate two adaptations of TriGen to non-symmetric similarities (TriGen does not support non-symmetric distances). We also evaluate a hybrid of TriGen and the piece-wise linear approximation pruning. We find that this hybrid approach is often more effective than either of the pruning rules. We make our software publicly available.", "year": 2019, "ssId": "05b0c768ecd4a82e486923e83250ddd53bacbf67", "arXivId": "1910.03539", "link": "https://arxiv.org/pdf/1910.03539.pdf", "openAccess": true, "authors": ["Leonid Boytsov", "Eric Nyberg"]}}
{"id": "2bd54adb3b5588281396a4b5dae7db09496b2c61", "content": {"title": "SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis", "abstract": "SberQuAD -- a large scale analog of Stanford SQuAD in the Russian language - is a valuable resource that has not been properly presented to the scientific community. We fill this gap by providing a description, a thorough analysis, and baseline experimental results.", "year": 2019, "ssId": "2bd54adb3b5588281396a4b5dae7db09496b2c61", "arXivId": "1912.09723", "link": "https://arxiv.org/pdf/1912.09723.pdf", "openAccess": true, "authors": ["Pavel Efimov", "Leonid Boytsov", "Pavel Braslavski"]}}
{"id": "ab5c6703fceb3dce6558be309cc65a4a8615c774", "content": {"title": "Accurate and Fast Retrieval for Complex Non-metric Data via Neighborhood Graphs", "abstract": "We demonstrate that a graph-based search algorithm\u2014relying on the construction of an approximate neighborhood graph\u2014can directly work with challenging non-metric and/or non-symmetric distances without resorting to metric-space mapping and/or distance symmetrization, which, in turn, lead to substantial performance degradation. Although the straightforward metrization and symmetrization is usually ineffective, we find that constructing an index using a modified, e.g., symmetrized, distance can improve performance. This observation paves a way to a new line of research of designing index-specific graph-construction distance functions.", "year": 2019, "ssId": "ab5c6703fceb3dce6558be309cc65a4a8615c774", "arXivId": "1910.03534", "link": "https://arxiv.org/pdf/1910.03534.pdf", "openAccess": true, "authors": ["Leonid Boytsov", "Eric Nyberg"]}}
{"id": "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35", "content": {"title": "A Simple Derivation of the Heap's Law from the Generalized Zipf's Law", "abstract": "I reproduce a rather simple formal derivation of the Heaps' law from the generalized Zipf's law, which I previously published in Russian.", "year": 2017, "ssId": "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35", "arXivId": "1711.03066", "link": "https://arxiv.org/pdf/1711.03066.pdf", "openAccess": true, "authors": ["Leonid Boytsov"]}}
{"id": "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf", "content": {"title": "Off the Beaten Path: Let's Replace Term-Based Retrieval with k-NN Search", "abstract": "Retrieval pipelines commonly rely on a term-based search to obtain candidate records, which are subsequently re-ranked. Some candidates are missed by this approach, e.g., due to a vocabulary mismatch. We address this issue by replacing the term-based search with a generic k-NN retrieval algorithm, where a similarity function can take into account subtle term associations. While an exact brute-force k-NN search using this similarity function is slow, we demonstrate that an approximate algorithm can be nearly two orders of magnitude faster at the expense of only a small loss in accuracy. A retrieval pipeline using an approximate k-NN search can be more effective and efficient than the term-based pipeline. This opens up new possibilities for designing effective retrieval pipelines. Our software (including data-generating code) and derivative data based on the Stack Overflow collection is available online.", "year": 2016, "ssId": "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf", "arXivId": "1610.10001", "link": "https://arxiv.org/pdf/1610.10001.pdf", "openAccess": true, "authors": ["Leonid Boytsov", "David Novak", "Yury Malkov", "Eric Nyberg"]}}
{"id": "387754dc8d4185fadd7c3c15e43956a4d085e8fe", "content": {"title": "Permutation Search Methods are Efficient, Yet Faster Search is Possible", "abstract": "We survey permutation-based methods for approximate k-nearest neighbor search. In these methods, every data point is represented by a ranked list of pivots sorted by the distance to this point. Such ranked lists are called permutations. The underpinning assumption is that, for both metric and non-metric spaces, the distance between permutations is a good proxy for the distance between original points. Thus, it should be possible to efficiently retrieve most true nearest neighbors by examining only a tiny subset of data points whose permutations are similar to the permutation of a query. We further test this assumption by carrying out an extensive experimental evaluation where permutation methods are pitted against state-of-the art benchmarks (the multi-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety of realistically large data set from the image and textual domain. The focus is on the high-accuracy retrieval methods for generic spaces. Additionally, we assume that both data and indices are stored in main memory. We find permutation methods to be reasonably efficient and describe a setup where these methods are most useful. To ease reproducibility, we make our software and data sets publicly available.", "year": 2015, "ssId": "387754dc8d4185fadd7c3c15e43956a4d085e8fe", "arXivId": "1506.03163", "link": "https://arxiv.org/pdf/1506.03163.pdf", "openAccess": true, "authors": ["Bilegsaikhan Naidan", "Leonid Boytsov", "Eric Nyberg"]}}
{"id": "2c2234548de4694b6455a19cd0d85a9d6c473456", "content": {"title": "Non-Metric Space Library Manual", "abstract": "This document describes a library for similarity searching. Even though the library contains a variety of metric-space access methods, our main focus is on search methods for non-metric spaces. Because there are fewer exact solutions for non-metric spaces, many of our methods give only approximate answers. Thus, the methods are evaluated in terms of efficiency-effectiveness trade-offs rather than merely in terms of their efficiency. Our goal is, therefore, to provide not only state-of-the-art approximate search methods for both non-metric and metric spaces, but also the tools to measure search quality. We concentrate on technical details, i.e., how to compile the code, run the benchmarks, evaluate results, and use our code in other applications. Additionally, we explain how to extend the code by adding new search methods and spaces.", "year": 2015, "ssId": "2c2234548de4694b6455a19cd0d85a9d6c473456", "arXivId": "1508.05470", "link": "https://arxiv.org/pdf/1508.05470.pdf", "openAccess": true, "authors": ["Bilegsaikhan Naidan", "Leonid Boytsov"]}}
{"id": "7dce2877758b0103d1f7a454c184dc641e123359", "content": {"title": "Structured retrieval using SOLR", "abstract": "This report presents SOLR Annographix: a software suite for indexing and querying annotation graphs generated by NLP pipelines. E ectiveness and e ciency of the software was evaluated in a document retrieval task for the purpose of factoid question answering (QA). To this end, we employed the AQUAINT text collection and TREC QA topics. The actual queries were generated from answer-bearing sentences using an approach of Bilotti [3]. Queries that included linguistically-motivated constraints outperformed unstructured queries: The di erences were substantial and statistically signi cant. In addition, we experimented with queries generated from randomly selected corpus sentences: It was possible to retrieve original documents in at least 90% of cases, while spending less than 0.1 sec per query (on average). These results are encouraging. Yet, the system needs to be improved by (1) implementing annotation compression, (2) extending the query language, (3) implementing better graph matching algorithms that would be less dependent on an early termination heuristic.", "year": 2015, "ssId": "7dce2877758b0103d1f7a454c184dc641e123359", "arXivId": null, "link": null, "openAccess": false, "authors": ["Leonid Boytsov"]}}
{"id": "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f", "content": {"title": "Metaphor Detection with Cross-Lingual Model Transfer", "abstract": "We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets in Spanish, Farsi, and Russian. The results support the hypothesis that metaphors are conceptual, rather than lexical, in nature.", "year": 2014, "ssId": "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Yulia Tsvetkov", "Leonid Boytsov", "A. Gershman", "Eric Nyberg", "Chris Dyer"]}}
{"id": "ca73cc17ca69fa0807e566c22c7c1711da916281", "content": {"title": "Comparative Analysis of Data Structures for Approximate Nearest Neighbor Search", "abstract": "Similarity searching has a vast range of applications in various fields of computer science. Many methods have been proposed for exact search, but they all suffer from the curse of dimensionality and are, thus, not applicable to high dimensional spaces. Approximate search methods are considerably more efficient in high dimensional spaces. Unfortunately, there are few theoretical results regarding the complexity of these methods and there are no comprehensive empirical evaluations, especially for non-metric spaces. To fill this gap, we present an empirical analysis of data structures for approximate nearest neighbor search in high dimensional spaces. We provide a comparison with recently published algorithms on several data sets. Our results show that small world approaches provide some of the best tradeoffs between efficiency and effectiveness in both metric and non-metric spaces. Keywords\u2013nearest neighbor search; metric space; non-metric search, approximate search; small world graphs", "year": 2014, "ssId": "ca73cc17ca69fa0807e566c22c7c1711da916281", "arXivId": null, "link": null, "openAccess": false, "authors": ["Alexander Ponomarenko", "N. Avrelin", "Bilegsaikhan Naidan", "Leonid Boytsov"]}}
{"id": "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203", "content": {"title": "CMU Multiple-choice Question Answering System at NTCIR-11 QA-Lab", "abstract": "We describe CMU\u2019s UIMA-based modular automatic question answering (QA) system. This system answers multiplechoice English questions for the world history entrance exam. Questions are preceded by short descriptions providing a historical context. Given the context and question-specic instructions, we generate veriable assertions for each answer choice. These assertions are evaluated using several evidencing modules, which assign a plausibility score to each assertion. These scores are then aggregated to produce the most plausible answer choice. In the NTCIR-11 QALab evaluations, our system achieved 51.6% accuracy on the training set, 47.2% on Phase 1 testing set, and 34.1% on Phase 2 testing set.", "year": 2014, "ssId": "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203", "arXivId": null, "link": null, "openAccess": false, "authors": ["Di Wang", "Leonid Boytsov", "J. Araki", "Alkesh Patel", "Jeffrey G. Gee", "Zhengzhong Liu", "Eric Nyberg", "T. Mitamura"]}}
{"id": "781e0e81834119c135091c8bdfcd1966c10b09ab", "content": {"title": "SIMD compression and the intersection of sorted integers", "abstract": "Sorted lists of integers are commonly used in inverted indexes and database systems. They are often compressed in memory. We can use the single\u2010instruction, multiple data (SIMD) instructions available in common processors to boost the speed of integer compression schemes. Our S4\u2010BP128\u2010D4 scheme uses as little as 0.7\u2009CPU cycles per decoded 32\u2010bit integer while still providing state\u2010of\u2010the\u2010art compression. However, if the subsequent processing of the integers is slow, the effort spent on optimizing decompression speed can be wasted. To show that it does not have to be so, we (1) vectorize and optimize the intersection of posting lists; (2) introduce the SIMD GALLOPING algorithm. We exploit the fact that one SIMD instruction can compare four pairs of 32\u2010bit integers at once. We experiment with two Text REtrieval Conference (TREC) text collections, GOV2 and ClueWeb09 (category B), using logs from the TREC million\u2010query track. We show that using only the SIMD instructions ubiquitous in all modern CPUs, our techniques for conjunctive queries can double the speed of a state\u2010of\u2010the\u2010art approach. Copyright \u00a9 2015 John Wiley & Sons, Ltd.", "year": 2014, "ssId": "781e0e81834119c135091c8bdfcd1966c10b09ab", "arXivId": "1401.6399", "link": "https://arxiv.org/pdf/1401.6399.pdf", "openAccess": true, "authors": ["D. Lemire", "Leonid Boytsov", "Nathan Kurz"]}}
{"id": "6eae6230ae277b6915706ec05241c8db6b9fab86", "content": {"title": "Engineering Efficient and Effective Non-metric Space Library", "abstract": "We present a new similarity search library and discuss a variety of design and performance issues related to its development. We adopt a position that engineering is equally important to design of the algorithms and pursue a goal of producing realistic benchmarks. To this end, we pay attention to various performance aspects and utilize modern hardware, which provides a high degree of parallelization. Since we focus on realistic measurements, performance of the methods should not be measured using merely the number of distance computations performed, because other costs, such as computation of a cheaper distance function, which approximates the original one, are oftentimes substantial. The paper includes preliminary experimental results, which support this point of view. Rather than looking for the best method, we want to ensure that the library implements competitive baselines, which can be useful for future work.", "year": 2013, "ssId": "6eae6230ae277b6915706ec05241c8db6b9fab86", "arXivId": null, "link": null, "openAccess": false, "authors": ["Leonid Boytsov", "Bilegsaikhan Naidan"]}}
