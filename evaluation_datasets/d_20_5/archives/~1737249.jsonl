{"id": "73a6e4574de038878be1bbb5985400998e420a5b", "content": {"title": "The Price of Strategyproofing Peer Assessment", "abstract": "Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of assignments, grant proposal review, conference peer review, and peer assessment of employees. Since an individual\u2019s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assignment quality required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.", "year": 2022, "ssId": "73a6e4574de038878be1bbb5985400998e420a5b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Komal Dhull", "Steven Jecmen", "Pravesh Kothari", "Nihar B. Shah"]}}
{"id": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "content": {"title": "Cite-seeing and Reviewing: A Study on Citation Bias in Peer Review", "abstract": "Citations play an important role in researchers\u2019 careers as a key factor in evaluation of scienti\ufb01c impact. Many anecdotes advice authors to exploit this fact and cite prospective reviewers to try obtaining a more positive evaluation for their submission. In this work, we investigate if such a citation bias actually exists: Does the citation of a reviewer\u2019s own work in a submission cause them to be positively biased towards the submission? In conjunction with the review process of two \ufb02agship conferences in machine learning and algorithmic economics, we execute an observational study to test for citation bias in peer review. In our analysis, we carefully account for various confounding factors such as paper quality and reviewer expertise, and apply di\ufb00erent modeling techniques to alleviate concerns regarding the model mismatch. Overall, our analysis involves 1,314 papers and 1,717 reviewers and detects citation bias in both venues we consider. In terms of the e\ufb00ect size, by citing a reviewer\u2019s work, a submission has a non-trivial chance of getting a higher score from the reviewer: an expected increase in the score is approximately 0 . 23 on a 5-point Likert item. For reference, a one-point increase of a score by a single reviewer improves the position of a submission by 11% on average.", "year": 2022, "ssId": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "arXivId": "2203.17239", "link": "https://arxiv.org/pdf/2203.17239.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Ryan Liu", "Shuchi Chawla", "F. Echenique", "Nihar B. Shah"]}}
{"id": "5e327c2285ddf2a76d08e5c00d16c7358bc5412c", "content": {"title": "Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators' Expertise", "abstract": "Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of homeworks, grant proposal review, conference peer review of scientific papers, and peer assessment of employees in organizations. Since an individual\u2019s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators\u2019 expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assigned evaluators\u2019 expertise required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.", "year": 2022, "ssId": "5e327c2285ddf2a76d08e5c00d16c7358bc5412c", "arXivId": "2201.10631", "link": "https://arxiv.org/pdf/2201.10631.pdf", "openAccess": true, "authors": ["Komal Dhull", "Steven Jecmen", "Pravesh Kothari", "Nihar B. Shah"]}}
{"id": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "content": {"title": "To ArXiv or not to ArXiv: A Study Quantifying Pros and Cons of Posting Preprints Online", "abstract": "Double-blind conferences have engaged in debates over whether to allow authors to post their papers online on arXiv or elsewhere during the review process. Independently, some authors of research papers face the dilemma of whether to put their papers on arXiv due to its pros and cons. We conduct a study to substantiate this debate and dilemma via quantitative measurements. Speci\ufb01cally, we conducted surveys of reviewers in two top-tier double-blind computer science conferences\u2014ICML 2021 (5361 submissions and 4699 reviewers) and EC 2021 (498 submissions and 190 reviewers). Our two main \ufb01ndings are as follows. First, more than a third of the reviewers self-report searching online for a paper they are assigned to review. Second, outside the review process, we \ufb01nd that preprints from better-ranked a\ufb03liations see a weakly higher visibility, with a correlation of 0.06 in ICML and 0.05 in EC. In particular, papers associated with the top-10-ranked a\ufb03liations had a visibility of approximately 11% in ICML and 22% in EC, whereas the remaining papers had a visibility of 7% and 18% respectively.", "year": 2022, "ssId": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "arXivId": "2203.17259", "link": "https://arxiv.org/pdf/2203.17259.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Ivan Stelmakh", "Xinwei Shen", "M. Meil\u0103", "F. Echenique", "Shuchi Chawla", "Nihar B. Shah"]}}
{"id": "2a82a16bdb793dc388391be57d6424f0d5090513", "content": {"title": "Integrating Rankings into Quantized Scores in Peer Review", "abstract": "In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a signi\ufb01cant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing work\ufb02ows, thereby leading to inef\ufb01ciencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and work\ufb02ows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and \ufb01nd that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.", "year": 2022, "ssId": "2a82a16bdb793dc388391be57d6424f0d5090513", "arXivId": "2204.03505", "link": "https://arxiv.org/pdf/2204.03505.pdf", "openAccess": true, "authors": ["Yusha Liu", "Yichong Xu", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "8b73e226815d57bf66fc94905ebd063e4957b449", "content": {"title": "Calibration with Privacy in Peer Review", "abstract": "Reviewers in peer review are often miscalibrated: they may be strict, lenient, extreme, moderate, etc. A number of algorithms have previously been proposed to calibrate reviews. Such attempts of calibration can however leak sensitive information about which reviewer reviewed which paper. In this paper, we identify this problem of calibration with privacy, and provide a foundational building block to address it. Specifically, we present a theoretical study of this problem under a simplified-yet-challenging model involving two reviewers, two papers, and an MAP-computing adversary. Our main results establish the Pareto frontier of the tradeoff between privacy (preventing the adversary from inferring reviewer identity) and utility (accepting better papers), and design explicit computationally-efficient algorithms that we prove are Pareto optimal.", "year": 2022, "ssId": "8b73e226815d57bf66fc94905ebd063e4957b449", "arXivId": "2201.11308", "link": "https://arxiv.org/pdf/2201.11308.pdf", "openAccess": true, "authors": ["Wenxin Ding", "Gautam Kamath", "Weina Wang", "Nihar B. Shah"]}}
{"id": "91184a2d40be8a0171b5c926b336666ed717ec6e", "content": {"title": "JCDL 2021 Tutorial on Systemic Challenges and Computational Solutions on Bias and Unfairness in Peer Review", "abstract": "Peer review is the backbone of scientific research and determines the composition of scientific digital libraries. Any systemic issues in peer review - such as biases or fraud - can systematically affect the resulting scientific digital library as well as any analyses on that library. They also affect billions of dollars in research grants made via peer review as well as entire careers of researchers. The tutorial will discuss various systemic issues in peer review via insightful experiments, several computational solutions proposed to address these issues, and a number of important open problems. A detailed writeup on the topics of this tutorial as well as a complete list of references is available in [1].", "year": 2021, "ssId": "91184a2d40be8a0171b5c926b336666ed717ec6e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "76f02d20e02c6baf39fee8f115cd94e4ceacf32b", "content": {"title": "Prior and Prejudice", "abstract": "Modern machine learning and computer science conferences are experiencing a surge in the number of submissions that challenges the quality of peer review as the number of competent reviewers is growing at a much slower rate. To curb this trend and reduce the burden on reviewers, several conferences have started encouraging or even requiring authors to declare the previous submission history of their papers. Such initiatives have been met with skepticism among authors, who raise the concern about a potential bias in reviewers' recommendations induced by this information. In this work, we investigate whether reviewers exhibit a bias caused by the knowledge that the submission under review was previously rejected at a similar venue, focusing on a population of novice reviewers who constitute a large fraction of the reviewer pool in leading machine learning and computer science conferences. We design and conduct a randomized controlled trial closely replicating the relevant components of the peer-review pipeline with $133$ reviewers (master's, junior PhD students, and recent graduates of top US universities) writing reviews for $19$ papers. The analysis reveals that reviewers indeed become negatively biased when they receive a signal about paper being a resubmission, giving almost 1 point lower overall score on a 10-point Likert item (\u0394 = -0.78, 95% CI = [-1.30, -0.24]) than reviewers who do not receive such a signal. Looking at specific criteria scores (originality, quality, clarity and significance), we observe that novice reviewers tend to underrate quality the most.", "year": 2021, "ssId": "76f02d20e02c6baf39fee8f115cd94e4ceacf32b", "arXivId": null, "link": null, "openAccess": false, "authors": ["I. Stelmakh", "Nihar B. Shah", "Aarti Singh", "Hal Daum\u00e9"]}}
{"id": "e2b097bce656db9215505659357263c43190194b", "content": {"title": "Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference Experiment Design", "abstract": "Many scientific conferences employ a two-phase paper review process, where some papers are assigned additional reviewers after the initial reviews are submitted. Many conferences also design and run experiments on their paper review process, where some papers are assigned reviewers who provide reviews under an experimental condition. In this paper, we consider the question: how should reviewers be divided between phases or conditions in order to maximize total assignment similarity? We make several contributions towards answering this question. First, we prove that when the set of papers requiring additional review is unknown, a simplified variant of this problem is NP-hard. Second, we empirically show that across several datasets pertaining to real conference data, dividing reviewers between phases/conditions uniformly at random allows an assignment that is nearly as good as the oracle optimal assignment. This uniformly random choice is practical for both the two-phase and conference experiment design settings. Third, we provide explanations of this phenomenon by providing theoretical bounds on the suboptimality of this random strategy under certain natural conditions. From these easily-interpretable conditions, we provide actionable insights to conference program chairs about whether a random reviewer split is suitable for their conference.", "year": 2021, "ssId": "e2b097bce656db9215505659357263c43190194b", "arXivId": "2108.06371", "link": "https://arxiv.org/pdf/2108.06371.pdf", "openAccess": true, "authors": ["Steven Jecmen", "Hanrui Zhang", "Ryan Liu", "Fei Fang", "V. Conitzer", "Nihar B. Shah"]}}
{"id": "4c0a915b9389e6489753a968085ee12833131d0a", "content": {"title": "KDD 2021 Tutorial on Systemic Challenges and Solutions on Bias and Unfairness in Peer Review", "abstract": "Introduction. Peer review is a cornerstone of academic practice [1]. The peer review process is highly regarded by the vast majority of researchers and considered by most to be essential to the communication of scholarly research [2\u20134]. However, there is also an overwhelming desire for improvement [2, 4, 5]. Problems in peer review have consequences much beyond the outcome for a specific paper or grant, particularly due to the widespread prevalence of the Matthew effect (\u201crich get richer\u201d) in academia [6]. As noted by [7] \u201can incompetent review may lead to the rejection of the submitted paper, or of the grant application, and the ultimate failure of the career of the author.\u201d (See also [8, 9].) The importance of peer review and the urgent need for improvements, behooves research on principled approaches towards addressing problems in peer review, particularly at scale. In this tutorial, we discuss a number of key challenges in peer review, outline several directions of research on this topic, and also highlight important open problems that we envisage to be exciting to the community. This document summarizes the contents of the tutorial and provides relevant references.", "year": 2021, "ssId": "4c0a915b9389e6489753a968085ee12833131d0a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "6d2d86cf5e80b58a03360559095ea3603548248f", "content": {"title": "A heuristic for statistical seriation", "abstract": "We study the statistical seriation problem, where the goal is to estimate a matrix whose rows satisfy the same shape constraint after a permutation of the columns. This is a important classical problem, with close connections to statistical literature in permutation-based models and also has wide applications ranging from archaeology to biology. Specifically, we consider the case where the rows are monotonically increasing after an unknown permutation of the columns. Past work has shown that the least-squares estimator is optimal up to logarithmic factors, but efficient algorithms for computing the least-squares estimator remain unknown to date. We approach this important problem from a heuristic perspective. Specifically, we replace the combinatorial permutation constraint by a continuous regularization term, and then use projected gradient descent to obtain a local minimum of the non-convex objective. We show that the attained local minimum is the global minimum in certain special cases under the noiseless setting, and preserves desirable properties under the noisy setting. Simulation results reveal that our proposed algorithm outperforms prior algorithms when (1) the underlying model is more complex than simplistic parametric assumptions such as low-rankedness, or (2) the signal-to-noise ratio is high. Under partial observations, the proposed algorithm requires an initialization, and different initializations may lead to different local minima. We empirically observe that the proposed algorithm yields consistent improvement over the initialization, even though different initializations start with different levels of quality.", "year": 2021, "ssId": "6d2d86cf5e80b58a03360559095ea3603548248f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Komal Dhull", "Jingyan Wang", "Nihar B. Shah", "Yuanzhi Li", "R. Ravi"]}}
{"id": "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264", "content": {"title": "WSDM 2021 Tutorial on Systematic Challenges and Computational Solutions on Bias and Unfairness in Peer Review", "abstract": "Peer review is the backbone of scientific research. Yet peer review is called \"biased,\" \"broken,\" and \"unscientific\" in many scientific disciplines. This problem is further compounded with the near-exponentially growing number of submissions in various computer science conferences. Due to the prevalence of \"Matthew effect'' of rich getting richer in academia, any source of unfairness in the peer review system, such as those discussed in this tutorial, can considerably affect the entire career trajectory of (young) researchers. This tutorial will discuss a number of systemic challenges in peer review such as biases, subjectivity, miscalibration, dishonest behavior, and noise. For each issue, the tutorial will first present insightful experiments to understand the issue. Then the tutorial will present computational techniques designed to address these challenges. Many open problems will be highlighted which are envisaged to be exciting to the WSDM audience, and will lead to significant impact if solved.", "year": 2021, "ssId": "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "cf7e8f47ad1c57738dc586109dcf28a22ab67b72", "content": {"title": "A SUPER* Algorithm to Optimize Paper Bidding in Peer Review", "abstract": "A number of applications involve sequential arrival of users, and require showing each user an ordering of items. A prime example (which forms the focus of this paper) is the bidding process in conference peer review where reviewers enter the system sequentially, each reviewer needs to be shown the list of submitted papers, and the reviewer then \"bids\" to review some papers. The order of the papers shown has a significant impact on the bids due to primacy effects. In deciding on the ordering of papers to show, there are two competing goals: (i) obtaining sufficiently many bids for each paper, and (ii) satisfying reviewers by showing them relevant items. In this paper, we begin by developing a framework to study this problem in a principled manner. We present an algorithm called SUPER*, inspired by the A* algorithm, for this goal. Theoretically, we show a local optimality guarantee of our algorithm and prove that popular baselines are considerably suboptimal. Moreover, under a community model for the similarities, we prove that SUPER* is near-optimal whereas the popular baselines are considerably suboptimal. In experiments on real data from ICLR 2018 and synthetic data, we find that SUPER* considerably outperforms baselines deployed in existing systems, consistently reducing the number of papers with fewer than requisite bids by 50-75% or more, and is also robust to various real world complexities.", "year": 2020, "ssId": "cf7e8f47ad1c57738dc586109dcf28a22ab67b72", "arXivId": "2007.07079", "link": "https://arxiv.org/pdf/2007.07079.pdf", "openAccess": true, "authors": ["Tanner Fiez", "Nihar B. Shah", "L. Ratliff"]}}
{"id": "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9", "content": {"title": "Two-Sample Testing on Pairwise Comparison Data and the Role of Modeling Assumptions", "abstract": "A number of applications require two-sample testing of pairwise comparison data. For instance, in crowdsourcing, there is a long-standing question of whether comparison data provided by people is distributed similar to ratings-converted-to-comparisons. Other examples include sports data analysis and peer grading. In this paper, we design a two-sample test for pairwise comparison data. We establish an upper bound on the sample complexity required to correctly distinguish between the distributions of the two sets of samples. Our test requires essentially no assumptions on the distributions. We then prove complementary information-theoretic lower bounds showing that our results are tight (in the minimax sense) up to constant factors. We also investigate the role of modeling assumptions by proving information-theoretic lower bounds for a range of pairwise comparison models (WST, MST, SST, parameter-based such as BTL and Thurstone).", "year": 2020, "ssId": "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9", "arXivId": null, "link": null, "openAccess": false, "authors": ["Charvi Rastogi", "Sivaraman Balakrishnan", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "803a0d2677a7d6b20c3964533595775fa5c7c750", "content": {"title": "Two-Sample Testing on Ranked Preference Data and the Role of Modeling Assumptions", "abstract": "A number of applications require two-sample testing on ranked preference data. For instance, in crowdsourcing, there is a long-standing question of whether pairwise comparison data provided by people is distributed similar to ratings-converted-to-comparisons. Other examples include sports data analysis and peer grading. In this paper, we design two-sample tests for pairwise comparison data and ranking data. For our two-sample test for pairwise comparison data, we establish an upper bound on the sample complexity required to correctly distinguish between the distributions of the two sets of samples. Our test requires essentially no assumptions on the distributions. We then prove complementary lower bounds showing that our results are tight (in the minimax sense) up to constant factors. We investigate the role of modeling assumptions by proving lower bounds for a range of pairwise comparison models (WST, MST,SST, parameter-based such as BTL and Thurstone). We also provide testing algorithms and associated sample complexity bounds for the problem of two-sample testing with partial (or total) ranking data.Furthermore, we empirically evaluate our results via extensive simulations as well as two real-world datasets consisting of pairwise comparisons. By applying our two-sample test on real-world pairwise comparison data, we conclude that ratings and rankings provided by people are indeed distributed differently. On the other hand, our test recognizes no significant difference in the relative performance of European football teams across two seasons. Finally, we apply our two-sample test on a real-world partial and total ranking dataset and find a statistically significant difference in Sushi preferences across demographic divisions based on gender, age and region of residence.", "year": 2020, "ssId": "803a0d2677a7d6b20c3964533595775fa5c7c750", "arXivId": "2006.11909", "link": "https://arxiv.org/pdf/2006.11909.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Sivaraman Balakrishnan", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "34c9e3152c9a14af711994230d8a3909daeaa7cf", "content": {"title": "Research Paper: Loss Functions, Axioms, and Peer Review", "abstract": "It is common to see a handful of reviewers reject a highly novel paper, because they view, say, extensive experiments as far more important than novelty, whereas the community as a whole would have embraced the paper. More generally, the disparate mapping of criteria scores to final recommendations by different reviewers is a major source of inconsistency in peer review. In this paper we present a framework inspired by empirical risk minimization (ERM) for learning the community\u2019s aggregate mapping. The key challenge that arises is the specification of a loss function for ERM. We consider the class of L(p, q) loss functions, which is a matrix-extension of the standard class of Lp losses on vectors; here the choice of the loss function amounts to choosing the hyperparameters p, q \u2208 [1,\u221e]. To deal with the absence of ground truth in our problem, we instead draw on computational social choice to identify desirable values of the hyperparameters p and q. Specifically, we characterize p = q = 1 as the only choice of these hyperparameters that satisfies three natural axiomatic properties. Finally, we implement and apply our approach to reviews from IJCAI 2017.", "year": 2020, "ssId": "34c9e3152c9a14af711994230d8a3909daeaa7cf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ritesh Noothigattu", "Nihar B. Shah", "Ariel D. Procaccia"]}}
{"id": "36d193c7a9523f55f9fe5ffd0730f248c241f5c7", "content": {"title": "AAAI 2020 Tutorial on Fairness and Bias in Peer Review and other Sociotechnical Intelligent Systems (Part II on Peer Review)", "abstract": "Peer review is the backbone of scholarly research, but it faces a number of challenges pertaining to bias and unfairness. There is an urgent need to improve peer review. This AAAI tutorial (part 2) discusses several problems, empirical studies, proposed solutions, and open problems in this domain. This document serves to provide a summary and references for the", "year": 2020, "ssId": "36d193c7a9523f55f9fe5ffd0730f248c241f5c7", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah", "Z. Lipton"]}}
{"id": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "content": {"title": "A Large Scale Randomized Controlled Trial on Herding in Peer-Review Discussions", "abstract": "Peer review is the backbone of academia and humans constitute a cornerstone of this process, being responsible for reviewing papers and making the final acceptance/rejection decisions. Given that human decision making is known to be susceptible to various cognitive biases, it is important to understand which (if any) biases are present in the peer-review process and design the pipeline such that the impact of these biases is minimized. In this work, we focus on the dynamics of between-reviewers discussions and investigate the presence of herding behaviour therein. In that, we aim to understand whether reviewers and more senior decision makers get disproportionately influenced by the first argument presented in the discussion when (in case of reviewers) they form an independent opinion about the paper before discussing it with others. Specifically, in conjunction with the review process of ICML 2020 -- a large, top tier machine learning conference -- we design and execute a randomized controlled trial with the goal of testing for the conditional causal effect of the discussion initiator's opinion on the outcome of a paper.", "year": 2020, "ssId": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "arXivId": "2011.15083", "link": "https://arxiv.org/pdf/2011.15083.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Nihar B. Shah", "Aarti Singh", "Hal Daum'e"]}}
{"id": "b0894f5c914cd90cc3b3e16b15bec11efe317b14", "content": {"title": "Catch Me if I Can: Detecting Strategic Behaviour in Peer Assessment", "abstract": "We consider the issue of strategic behaviour in various peer-assessment tasks, including peer grading of exams or homeworks and peer review in hiring or promotions. When a peer-assessment task is competitive (e.g., when students are graded on a curve), agents may be incentivized to misreport evaluations in order to improve their own final standing. Our focus is on designing methods for detection of such manipulations. Specifically, we consider a setting in which agents evaluate a subset of their peers and output rankings that are later aggregated to form a final ordering. In this paper, we investigate a statistical framework for this problem and design a principled test for detecting strategic behaviour. We prove that our test has strong false alarm guarantees and evaluate its detection ability in practical settings. For this, we design and execute an experiment that elicits strategic behaviour from subjects and release a dataset of patterns of strategic behaviour that may be of independent interest. We then use the collected data to conduct a series of real and semi-synthetic evaluations that demonstrate a strong detection power of our test.", "year": 2020, "ssId": "b0894f5c914cd90cc3b3e16b15bec11efe317b14", "arXivId": "2010.04041", "link": "https://arxiv.org/pdf/2010.04041.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2", "content": {"title": "Approval Voting and Incentives in Crowdsourcing", "abstract": "The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to %judiciously utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\"strictly proper\") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.", "year": 2020, "ssId": "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
