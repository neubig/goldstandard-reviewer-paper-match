{"id": "e03d9684a19c8f8e29ee97b347d4f1e280a88e44", "content": {"title": "Crossmodal Clustered Contrastive Learning: Grounding of Spoken Language to Gesture", "abstract": "Crossmodal grounding is a key technical challenge when generating relevant and well-timed gestures from spoken language. Often, the same gesture can accompany semantically different spoken language phrases which makes crossmodal grounding especially challenging. For example, a gesture (semi-circular with both hands) could co-occur with semantically different phrases \u201dentire bottom row\u201d (referring to a physical point) and \u201dmolecules expand and decay\u201d (referring to a scientific phenomena). In this paper, we introduce a self-supervised approach to learn representations better suited to such many-to-one grounding relationships between spoken language and gestures. As part of this approach, we propose a new contrastive loss function, Crossmodal Cluster NCE, that guides the model to learn spoken language representations which are consistent with the similarities in the gesture space. This gesture-aware space can help us generate more relevant gestures given language as input. We demonstrate the effectiveness of our approach on a publicly available dataset through quantitative and qualitative evaluations. Our proposed methodology significantly outperforms prior approaches for gestures-language grounding. Link to code: https://github.com/dondongwon/CC_NCE_GENEA.", "year": 2021, "ssId": "e03d9684a19c8f8e29ee97b347d4f1e280a88e44", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dongwon Lee", "Chaitanya Ahuja"]}}
{"id": "d5924c8cdef6270a955ba82c2b07a8282d869744", "content": {"title": "No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures", "abstract": "We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al. (2020) with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https://github.com/chahuja/aisle", "year": 2020, "ssId": "d5924c8cdef6270a955ba82c2b07a8282d869744", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chaitanya Ahuja", "Dong Won Lee", "Ryo Ishii", "Louis-Philippe Morency"]}}
{"id": "6e78e32481218e9391a88e6d0e30c0062ae71bec", "content": {"title": "Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach", "abstract": "How can we teach robots or virtual assistants to gesture naturally? Can we go further and adapt the gesturing style to follow a specific speaker? Gestures that are naturally timed with corresponding speech during human communication are called co-speech gestures. A key challenge, called gesture style transfer, is to learn a model that generates these gestures for a speaking agent 'A' in the gesturing style of a target speaker 'B'. A secondary goal is to simultaneously learn to generate co-speech gestures for multiple speakers while remembering what is unique about each speaker. We call this challenge style preservation. In this paper, we propose a new model, named Mix-StAGE, which trains a single model for multiple speakers while learning unique style embeddings for each speaker's gestures in an end-to-end manner. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. As Mix-StAGE disentangles style and content of gestures, gesturing styles for the same input speech can be altered by simply switching the style embeddings. Mix-StAGE also allows for style preservation when learning simultaneously from multiple speakers. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer. Our proposed Mix-StAGE model significantly outperforms the previous state-of-the-art approach for gesture generation and provides a path towards performing gesture style transfer across multiple speakers. Link to code, data, and videos: this http URL", "year": 2020, "ssId": "6e78e32481218e9391a88e6d0e30c0062ae71bec", "arXivId": "2007.12553", "link": "https://arxiv.org/pdf/2007.12553.pdf", "openAccess": true, "authors": ["Chaitanya Ahuja", "Dong Won Lee", "Y. Nakano", "Louis-Philippe Morency"]}}
{"id": "97bcea32979ed602fd404448a4e4cedad4171d79", "content": {"title": "Impact of Personality on Nonverbal Behavior Generation", "abstract": "To realize natural-looking virtual agents, one key technical challenge is to automatically generate nonverbal behaviors from spoken language. Since nonverbal behavior varies depending on personality, it is important to generate these nonverbal behaviors to match the expected personality of a virtual agent. In this work, we study how personality traits relate to the process of generating individual nonverbal behaviors from the whole body, including the head, eye gaze, arms, and posture. To study this, we first created a dialogue corpus including transcripts, a broad range of labelled nonverbal behaviors, and the Big Five personality scores of participants in dyad interactions. We constructed models that can predict each nonverbal behavior label given as an input language representation from the participants' spoken sentences. Our experimental results show that personality can help improve the prediction of nonverbal behaviors.", "year": 2020, "ssId": "97bcea32979ed602fd404448a4e4cedad4171d79", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ryo Ishii", "Chaitanya Ahuja", "Y. Nakano", "Louis-Philippe Morency"]}}
{"id": "ae77189921ffade5ee4c4d4a0e93e879d7280b80", "content": {"title": "Coalescing Narrative and Dialogue for Grounded Pose Forecasting", "abstract": "This research aims to create a data-driven end-to-end model for multimodal forecasting body pose and gestures of virtual avatars. A novel aspect of this research is to coalesce both narrative and dialogue for pose forecasting. In a narrative, language is used in a third person view to describe the avatar actions. In dialogue both first and second person views need to be integrated to accurately forecast avatar pose. Gestures and poses of a speaker are linked to other modalities: language and acoustics. We use these correlations to better predict the avatar\u2019s pose.", "year": 2019, "ssId": "ae77189921ffade5ee4c4d4a0e93e879d7280b80", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chaitanya Ahuja"]}}
{"id": "967b2d10b8b378f1da43fd4d9107826e540e1112", "content": {"title": "Language2Pose: Natural Language Grounded Pose Forecasting", "abstract": "Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations. In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language-to-Pose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-to-end using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and human-annotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.", "year": 2019, "ssId": "967b2d10b8b378f1da43fd4d9107826e540e1112", "arXivId": "1907.01108", "link": "https://arxiv.org/pdf/1907.01108.pdf", "openAccess": true, "authors": ["Chaitanya Ahuja", "Louis-Philippe Morency"]}}
{"id": "af92dd61340808f3008a84ae57803bb4aa57d03b", "content": {"title": "To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations", "abstract": "Non verbal behaviours such as gestures, facial expressions, body posture, and para-linguistic cues have been shown to complement or clarify verbal messages. Hence to improve telepresence, in form of an avatar, it is important to model these behaviours, especially in dyadic interactions. Creating such personalized avatars not only requires to model intrapersonal dynamics between a avatar\u2019s speech and their body pose, but it also needs to model interpersonal dynamics with the interlocutor present in the conversation. In this paper, we introduce a neural architecture named Dyadic Residual-Attention Model (DRAM), which integrates intrapersonal (monadic) and interpersonal (dyadic) dynamics using selective attention to generate sequences of body pose conditioned on audio and body pose of the interlocutor and audio of the human operating the avatar. We evaluate our proposed model on dyadic conversational data consisting of pose and audio of both participants, confirming the importance of adaptive attention between monadic and dyadic dynamics when predicting avatar pose. We also conduct a user study to analyze judgments of human observers. Our results confirm that the generated body pose is more natural, models intrapersonal dynamics and interpersonal dynamics better than non-adaptive monadic/dyadic models.", "year": 2019, "ssId": "af92dd61340808f3008a84ae57803bb4aa57d03b", "arXivId": "1910.02181", "link": "https://arxiv.org/pdf/1910.02181.pdf", "openAccess": true, "authors": ["Chaitanya Ahuja", "Shugao Ma", "Louis-Philippe Morency", "Yaser Sheikh"]}}
{"id": "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324", "content": {"title": "Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling", "abstract": "Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of information inside the units: a first flow along time dimension and a second flow along depth dimension. It also offers a symmetry in how information can flow horizontally and vertically. We analyze the effects of decoupling three different components of our LRU model: Reset Gate, Update Gate and Projected State. We evaluate this family on new LRU models on computational convergence rates and statistical efficiency. Our experiments are performed on four publicly-available datasets, comparing with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has better empirical computational convergence rates and statistical efficiency values, along with learning more accurate language models.", "year": 2017, "ssId": "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324", "arXivId": "1710.02254", "link": "https://arxiv.org/pdf/1710.02254.pdf", "openAccess": true, "authors": ["Chaitanya Ahuja", "Louis-Philippe Morency"]}}
{"id": "641af3bc3cc17993dc72098725d2eb9c0d98049d", "content": {"title": "Statistical Topological Data Analysis", "abstract": "Topological Data Analysis (TDA) refers to data analysis methods which study properties such as shape, topology and connectedness of the data. In this project we plan to study some of the key techniques used in TDA along with their established theoretical properties from a statistical perspective. TDA provides the data analyst with a set of tools to visualize and analyze a data sample. Below we give an overview of three such techniques summarized from [15], and then we take a deep dive into the density clustering and present some results from [6] which discusses how to construct confidence sets for density tree estimates. Assume that we observe a data sample X1, X2, . . . XN \u223c P , where Xi \u2208 R.", "year": 2017, "ssId": "641af3bc3cc17993dc72098725d2eb9c0d98049d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Bhuwan Dhingra", "Chaitanya Ahuja"]}}
{"id": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "content": {"title": "Multimodal Machine Learning: A Survey and Taxonomy", "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "year": 2017, "ssId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "arXivId": "1705.09406", "link": "https://arxiv.org/pdf/1705.09406.pdf", "openAccess": true, "authors": ["T. Baltru\u0161aitis", "Chaitanya Ahuja", "Louis-Philippe Morency"]}}
{"id": "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2", "content": {"title": "Fast modelling of pinna spectral notches from HRTFs using linear prediction residual cepstrum", "abstract": "Developing individualized head related transfer functions (HRTF) is an essential requirement for accurate virtualization of sound. However it is time consuming and complicated for both the subject and the developer. Obtaining the spectral notches which are the most prominent features of HRTF is very important to reconstruct the head related impulse response (HRIR) accurately. In this paper, a method suitable for fast computation of the frequencies of spectral notches is proposed. The linear prediction residual cepstrum is used to compute the spectral notches with a high degree of accuracy in this work. Subsequent use of Batteaus Reflection model to overlay the spectral notches on the pinna images indicate that the proposed method is able to provide finer contours. Experiments on reconstruction of the HRIR indicates that the method performs better than other methods.", "year": 2014, "ssId": "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chaitanya Ahuja", "R. Hegde"]}}
{"id": "de43afd166a79c24b3a7dd16c5695059d9f0aa71", "content": {"title": "Development of Concept of Transitivity in Pre-Operational Stage Children", "abstract": "Piaget\u2019s Theory gives a general overview on the cognitive development of a child from infancy to adulthood. There have been many arguments for and against the proposed timescale of cognitive learning versus the age of the child. We plan, via this work, to explore the learning capabilities of a child in pre-operational stage. The abstract concept of transitivity, which is not understood well by children of the age of 3-5 years of age, was induced in a group of children using physical analogy of a train system. It was then shown that the physical analogies (contrary to abstract ones) provide better insight and help children understand transitivity related concepts.", "year": 2014, "ssId": "de43afd166a79c24b3a7dd16c5695059d9f0aa71", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chaitanya Ahuja", "Pranjal Gupta", "A. Mukherjee"]}}
{"id": "436380dd75d8ff3f2debb29913bd2fe8dde0b684", "content": {"title": "A Complex Matrix Factorization Approach to Joint Modeling of Magnitude and Phase for Source Separation", "abstract": "Conventional NMF methods for source separation factorize the matrix of spectral magnitudes. Spectral Phase is not included in the decomposition process of these methods. However, phase of the speech mixture is generally used in reconstructing the target speech signal. This results in undesired traces of interfering sources in the target signal. In this paper the spectral phase is incorporated in the decomposition process itself. Additionally, the complex matrix factorization problem is reduced to an NMF problem using simple transformations. This results in effective separation of speech mixtures since both magnitude and phase are utilized jointly in the separation process. Improvement in source separation results are demonstrated using objective quality evaluations on the GRID corpus. As a side result, we have also investigated the intelligibility improvement aspect of target speaker in the presence of interfering speaker.", "year": 2014, "ssId": "436380dd75d8ff3f2debb29913bd2fe8dde0b684", "arXivId": "1411.6741", "link": "https://arxiv.org/pdf/1411.6741.pdf", "openAccess": true, "authors": ["Chaitanya Ahuja", "K. Nathwani", "R. Hegde"]}}
{"id": "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207", "content": {"title": "Extraction of pinna spectral notches in the median plane of a virtual spherical microphone array", "abstract": "In this paper, a fast method for the extraction of pinna spectral notches (PSN) in the median plane of a virtual spherical microphone array is discussed. In general, PSN can be extracted from the Head Related Impulse Response (HRIR) measured by a spherical array of microphones. However, the PSN extracted herein are computationally complex and also not accurate at lower elevation angles. This work proposes a novel approach to reconstruct the HRIR using microphones over the median plane of a virtual spherical array. The virtual spherical array itself is simulated using the Fourier Bessel series (FBS). Subsequently, these HRIRs are used to extract the PSN. This method is computationally efficient since it is done over the median plane rather than over the complete sphere. On the other hand, it is also accurate due to the utilization of the Fourier Bessel series in the extraction of the PSN. Experimental results obtained on the CIPIC database indicate a high degree of resemblance to the actual pinna walls, even at the lower elevation angles. The results are motivating enough for the method to be considered for resolving elevation ambiguity in 3D audio.", "year": 2014, "ssId": "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ankit Sohni", "Chaitanya Ahuja", "R. Hegde"]}}
