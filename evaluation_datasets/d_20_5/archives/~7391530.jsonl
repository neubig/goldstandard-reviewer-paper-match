{"id": "8b20173b98914f36302389e4c761c334fe867dcd", "content": {"title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts", "abstract": "Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L\u2019AMBRE \u2013 a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages.", "year": 2021, "ssId": "8b20173b98914f36302389e4c761c334fe867dcd", "arXivId": "2103.16590", "link": "https://arxiv.org/pdf/2103.16590.pdf", "openAccess": true, "authors": ["Adithya Pratapa", "Antonios Anastasopoulos", "Shruti Rijhwani", "Aditi Chaudhary", "David R. Mortensen", "Graham Neubig", "Yulia Tsvetkov"]}}
{"id": "b57da3ccf214e8dad49116c8db9590c2c89629f5", "content": {"title": "MasakhaNER: Named Entity Recognition for African Languages", "abstract": "Abstract We take a step towards addressing the under- representation of the African continent in NLP research by bringing together different stakeholders to create the first large, publicly available, high-quality dataset for named entity recognition (NER) in ten African languages. We detail the characteristics of these languages to help researchers and practitioners better understand the challenges they pose for NER tasks. We analyze our datasets and conduct an extensive empirical evaluation of state- of-the-art methods across both supervised and transfer learning settings. Finally, we release the data, code, and models to inspire future research on African NLP.1", "year": 2021, "ssId": "b57da3ccf214e8dad49116c8db9590c2c89629f5", "arXivId": "2103.11811", "link": "https://arxiv.org/pdf/2103.11811.pdf", "openAccess": true, "authors": ["David Ifeoluwa Adelani", "Jade Z. Abbott", "Graham Neubig", "Daniel D'souza", "Julia Kreutzer", "Constantine Lignos", "Chester Palen-Michel", "Happy Buzaaba", "Shruti Rijhwani", "Sebastian Ruder", "Stephen Mayhew", "Israel Abebe Azime", "Shamsuddeen Hassan Muhammad", "Chris C. Emezue", "J. Nakatumba-Nabende", "Perez Ogayo", "Anuoluwapo Aremu", "Catherine Gitau", "Derguene Mbaye", "Jesujoba Oluwadara Alabi", "Seid Muhie Yimam", "Tajuddeen R. Gwadabe", "I. Ezeani", "Rubungo Andre Niyongabo", "Jonathan Mukiibi", "V. Otiende", "Iroro Orife", "Davis David", "Samba Ngom", "Tosin P. Adewumi", "Paul Rayson", "Mofetoluwa Adeyemi", "Gerald Muriuki", "Emmanuel Anebi", "C. Chukwuneke", "N. Odu", "Eric Peter Wairagala", "S. Oyerinde", "Clemencia Siro", "Tobius Saul Bateesa", "Temilola Oloyede", "Yvonne Wambui", "Victor Akinode", "Deborah Nabagereka", "Maurice Katusiime", "A. Awokoya", "Mouhamadane Mboup", "D. Gebreyohannes", "Henok Tilaye", "Kelechi Nwaike", "Degaga Wolde", "A. Faye", "Blessing K. Sibanda", "Orevaoghene Ahia", "Bonaventure F. P. Dossou", "Kelechi Ogueji", "Thierno Ibrahima Diop", "A. Diallo", "Adewale Akinfaderin", "T. Marengereke", "Salomey Osei"]}}
{"id": "d5ec188a5a39e504788c1fe33457eeb816a99f31", "content": {"title": "Dependency Induction Through the Lens of Visual Perception", "abstract": "Most previous work on grammar induction focuses on learning phrasal or dependency structure purely from text. However, because the signal provided by text alone is limited, recently introduced visually grounded syntax models make use of multimodal information leading to improved performance in constituency grammar induction. However, as compared to dependency grammars, constituency grammars do not provide a straightforward way to incorporate visual information without enforcing language-specific heuristics. In this paper, we propose an unsupervised grammar induction model that leverages word concreteness and a structural vision-based heuristic to jointly learn constituency-structure and dependency-structure grammars. Our experiments find that concreteness is a strong indicator for learning dependency grammars, improving the direct attachment score (DAS) by over 50% as compared to state-of-the-art models trained on pure text. Next, we propose an extension of our model that leverages both word concreteness and visual semantic role labels in constituency and dependency parsing. Our experiments show that the proposed extension outperforms the current state-of-the-art visually grounded models in constituency parsing even with a smaller grammar size.", "year": 2021, "ssId": "d5ec188a5a39e504788c1fe33457eeb816a99f31", "arXivId": "2109.09790", "link": "https://arxiv.org/pdf/2109.09790.pdf", "openAccess": true, "authors": ["Ruisi Su", "Shruti Rijhwani", "Hao Zhu", "Junxian He", "Xinyu Wang", "Yonatan Bisk", "Graham Neubig"]}}
{"id": "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de", "content": {"title": "Soft Gazetteers for Low-Resource Named Entity Recognition", "abstract": "Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of \u201csoft gazetteers\u201d that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.", "year": 2020, "ssId": "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de", "arXivId": "2005.01866", "link": "https://arxiv.org/pdf/2005.01866.pdf", "openAccess": true, "authors": ["Shruti Rijhwani", "Shuyan Zhou", "Graham Neubig", "J. Carbonell"]}}
{"id": "4383e714f4535777ffb7b4f618d4ccede4b08bd3", "content": {"title": "AlloVera: A Multilingual Allophone Database", "abstract": "We introduce a new resource, AlloVera, which provides mappings from 218 allophones to phonemes for 14 languages. Phonemes are contrastive phonological units, and allophones are their various concrete realizations, which are predictable from phonological context. While phonemic representations are language specific, phonetic representations (stated in terms of (allo)phones) are much closer to a universal (language-independent) transcription. AlloVera allows the training of speech recognition models that output phonetic transcriptions in the International Phonetic Alphabet (IPA), regardless of the input language. We show that a \u201cuniversal\u201d allophone model, Allosaurus, built with AlloVera, outperforms \u201cuniversal\u201d phonemic models and language-specific models on a speech-transcription task. We explore the implications of this technology (and related technologies) for the documentation of endangered and minority languages. We further explore other applications for which AlloVera will be suitable as it grows, including phonological typology.", "year": 2020, "ssId": "4383e714f4535777ffb7b4f618d4ccede4b08bd3", "arXivId": "2004.08031", "link": "https://arxiv.org/pdf/2004.08031.pdf", "openAccess": true, "authors": ["David R. Mortensen", "Xinjian Li", "Patrick Littell", "A. Michaud", "Shruti Rijhwani", "Antonios Anastasopoulos", "A. Black", "Florian Metze", "Graham Neubig"]}}
{"id": "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71", "content": {"title": "OCR Post-Correction for Endangered Language Texts", "abstract": "There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.", "year": 2020, "ssId": "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71", "arXivId": "2011.05402", "link": "https://arxiv.org/pdf/2011.05402.pdf", "openAccess": true, "authors": ["Shruti Rijhwani", "Antonios Anastasopoulos", "Graham Neubig"]}}
{"id": "42605c1ee030721cb38a3c225992d63297a6ace0", "content": {"title": "A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization", "abstract": "Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh, PA, USA to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription, phone to orthography decoding, text-to-speech and text-speech forced alignment), 2) dictionary extraction and management, 3) search tools for corpora, and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw\u2019ida, Kwak\u2019wala, Ojibwe, San Juan Quiahije Chatino, and Seneca.", "year": 2020, "ssId": "42605c1ee030721cb38a3c225992d63297a6ace0", "arXivId": "2004.13203", "link": "https://arxiv.org/pdf/2004.13203.pdf", "openAccess": true, "authors": ["Graham Neubig", "Shruti Rijhwani", "Alexis Palmer", "Jordan MacKenzie", "Hilaria Cruz", "Xinjian Li", "Matthew Russell Lee", "Aditi Chaudhary", "Luke Gessler", "Steven P. Abney", "Shirley Anugrah Hayati", "Antonios Anastasopoulos", "Olga Zamaraeva", "Emily Prudhommeaux", "Jennette Child", "Sara Child", "R. Knowles", "Sarah Moeller", "J. Micher", "Yiyuan Li", "S. Zink", "M. Xia", "Roshan S. Sharma", "Patrick Littell"]}}
{"id": "c8171eaa3a3aac78c3b37351412101bc06e5f359", "content": {"title": "Practical Comparable Data Collection for Low-Resource Languages via Images", "abstract": "We propose a method of curating high-quality comparable training data for low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the source and target languages by getting captions for such images in both languages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs are acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected through our approach by experimenting on two downstream tasks - machine translation and dictionary extraction. All code and data are available at this https URL.", "year": 2020, "ssId": "c8171eaa3a3aac78c3b37351412101bc06e5f359", "arXivId": "2004.11954", "link": "https://arxiv.org/pdf/2004.11954.pdf", "openAccess": true, "authors": ["Aman Madaan", "Shruti Rijhwani", "Antonios Anastasopoulos", "Yiming Yang", "Graham Neubig"]}}
{"id": "6a9795853e5f39325deb0d916fe22d9e5a202a9f", "content": {"title": "Damaged Type and Areopagitica's Clandestine Printers", "abstract": "Milton Studies, Vol. 62, No. 1, 2020 Copyright \u00a9 2020 The Pennsylvania State University, University Park, PA abstract Milton\u2019s Areopagitica (1644) is one of the most significant texts in the history of the freedom of the press, and yet the pamphlet\u2019s clandestine printers have successfully eluded identification for over 375 years. By examining distinctive and damaged type pieces from 100 pamphlets from the 1640s, this article attributes the printing of Milton\u2019s Areopagitica to the London printers Matthew Simmons and Thomas Paine, with the possible involvement of Gregory Dexter. It further reveals a sophisticated ideological program of clandestine printing executed collaboratively by Paine and Simmons throughout 1644 and 1645 that includes not only Milton\u2019s Areopagitica but also Roger Williams\u2019s The Bloudy Tenent of Persecution, William Walwyn\u2019s The Compassionate Samaritane, Henry Robinson\u2019s Liberty of Conscience, Robinson\u2019s John the Baptist, and Milton\u2019s Of Education, Tetrachordon, and Colasterion.", "year": 2020, "ssId": "6a9795853e5f39325deb0d916fe22d9e5a202a9f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Christopher N. Warren", "Pierce Wiliams", "Shruti Rijhwani", "Max G'sell"]}}
{"id": "509b42fc150a057a64c4608f64e779ef04fdff47", "content": {"title": "Temporally-Informed Analysis of Named Entity Recognition", "abstract": "Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. To support these experiments, we introduce a novel data set of English tweets annotated with named entities. We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information. Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.", "year": 2020, "ssId": "509b42fc150a057a64c4608f64e779ef04fdff47", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shruti Rijhwani", "Daniel Preotiuc-Pietro"]}}
{"id": "67b29c3fe6f110125a8892e8ed128d20b23957ea", "content": {"title": "Towards Zero-resource Cross-lingual Entity Linking", "abstract": "Cross-lingual entity linking (XEL) grounds named entities in a source language to an English Knowledge Base (KB), such as Wikipedia. XEL is challenging for most languages because of limited availability of requisite resources. However, many works on XEL have been on simulated settings that actually use significant resources (e.g. source language Wikipedia, bilingual entity maps, multilingual embeddings) that are not available in truly low-resource languages. In this work, we first examine the effect of these resource assumptions and quantify how much the availability of these resource affects overall quality of existing XEL systems. We next propose three improvements to both entity candidate generation and disambiguation that make better use of the limited resources we do have in resource-scarce scenarios. With experiments on four extremely low-resource languages, we show that our model results in gains of 6-20% end-to-end linking accuracy.", "year": 2019, "ssId": "67b29c3fe6f110125a8892e8ed128d20b23957ea", "arXivId": "1909.13180", "link": "https://arxiv.org/pdf/1909.13180.pdf", "openAccess": true, "authors": ["Shuyan Zhou", "Shruti Rijhwani", "Graham Neubig"]}}
{"id": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328", "content": {"title": "Choosing Transfer Languages for Cross-Lingual Learning", "abstract": "Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.", "year": 2019, "ssId": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328", "arXivId": "1905.12688", "link": "https://arxiv.org/pdf/1905.12688.pdf", "openAccess": true, "authors": ["Yu-Hsiang Lin", "Chian-Yu Chen", "Jean Lee", "Zirui Li", "Yuyan Zhang", "M. Xia", "Shruti Rijhwani", "Junxian He", "Zhisong Zhang", "Xuezhe Ma", "Antonios Anastasopoulos", "Patrick Littell", "Graham Neubig"]}}
{"id": "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9", "content": {"title": "The ARIEL-CMU Systems for LoReHLT18", "abstract": "This paper describes the ARIEL-CMU submissions to the Low Resource Human Language Technologies (LoReHLT) 2018 evaluations for the tasks Machine Translation (MT), Entity Discovery and Linking (EDL), and detection of Situation Frames in Text and Speech (SF Text and Speech).", "year": 2019, "ssId": "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9", "arXivId": "1902.08899", "link": "https://arxiv.org/pdf/1902.08899.pdf", "openAccess": true, "authors": ["Aditi Chaudhary", "Siddharth Dalmia", "Junjie Hu", "Xinjian Li", "Austin Matthews", "Aldrian Obaja Muis", "Naoki Otani", "Shruti Rijhwani", "Zaid A. W. Sheikh", "Nidhi Vyas", "Xinyi Wang", "Jiateng Xie", "Ruochen Xu", "Chunting Zhou", "P. J. Jansen", "Yiming Yang", "Lori S. Levin", "Florian Metze", "T. Mitamura", "David R. Mortensen", "Graham Neubig", "E. Hovy", "A. Black", "J. Carbonell", "G. Horwood", "Shabnam Tafreshi", "Mona T. Diab", "Efsun Sarioglu Kayi", "N. Farra", "K. McKeown"]}}
{"id": "f7979c6690562c5f8bf700e3fd184c4d1df0a54c", "content": {"title": "Zero-shot Neural Transfer for Cross-lingual Entity Linking", "abstract": "Cross-lingual entity linking maps an entity mention in a source language to its corresponding entry in a structured knowledge base that is in a different (target) language. While previous work relies heavily on bilingual lexical resources to bridge the gap between the source and the target languages, these resources are scarce or unavailable for many low-resource languages. To address this problem, we investigate zero-shot cross-lingual entity linking, in which we assume no bilingual lexical resources are available in the source low-resource language. Specifically, we propose pivot-basedentity linking, which leverages information from a highresource \u201cpivot\u201d language to train character-level neural entity linking models that are transferred to the source lowresource language in a zero-shot manner. With experiments on 9 low-resource languages and transfer through a total of54 languages, we show that our proposed pivot-based framework improves entity linking accuracy 17% (absolute) on average over the baseline systems, for the zero-shot scenario.1 Further, we also investigate the use of language-universal phonological representations which improves average accuracy (absolute) by 36% when transferring between languages that use different scripts.", "year": 2018, "ssId": "f7979c6690562c5f8bf700e3fd184c4d1df0a54c", "arXivId": "1811.04154", "link": "https://arxiv.org/pdf/1811.04154.pdf", "openAccess": true, "authors": ["Shruti Rijhwani", "Jiateng Xie", "Graham Neubig", "J. Carbonell"]}}
{"id": "6dfecb5915e8b10841abe224c5361bbda7100637", "content": {"title": "Parser combinators for Tigrinya and Oromo morphology", "abstract": "We present rule-based morphological parsers in the Tigrinya and Oromo languages, based on a parser-combinator rather than finite-state paradigm. This paradigm allows rapid development and ease of integration with other systems, although at the cost of non-optimal theoretical efficiency. These parsers produce multiple output representations simultaneously, including lemmatization, morphological segmentation, and an English word-for-word gloss, and we evaluate these representations as input for entity detection and linking and humanitarian need detection.", "year": 2018, "ssId": "6dfecb5915e8b10841abe224c5361bbda7100637", "arXivId": null, "link": null, "openAccess": false, "authors": ["Patrick Littell", "R. Thomas McCoy", "Na-Rae Han", "Shruti Rijhwani", "Zaid A. W. Sheikh", "David R. Mortensen", "T. Mitamura", "Lori S. Levin"]}}
{"id": "549dae68d04eefad88885c64a4d946205e524b79", "content": {"title": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology-Based Representations", "abstract": "We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "year": 2017, "ssId": "549dae68d04eefad88885c64a4d946205e524b79", "arXivId": "1705.10900", "link": "https://arxiv.org/pdf/1705.10900.pdf", "openAccess": true, "authors": ["Paul Michel", "Abhilasha Ravichander", "Shruti Rijhwani"]}}
{"id": "e2c05b3abf77900ec82ffa8a95aa774308d2780f", "content": {"title": "Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique", "abstract": "Word-level language detection is necessary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large number of languages, which does not require any manually annotated training data. Our experiments with tweets in seven languages show a 74% relative error reduction in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quantitative analysis of code-switching patterns on Twitter, both global as well as region-specific, with 58M tweets.", "year": 2017, "ssId": "e2c05b3abf77900ec82ffa8a95aa774308d2780f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shruti Rijhwani", "R. Sequiera", "M. Choudhury", "Kalika Bali", "C. Maddila"]}}
{"id": "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde", "content": {"title": "Code-Switching as a Social Act: The Case of Arabic Wikipedia Talk Pages", "abstract": "Code-switching has been found to have social motivations in addition to syntactic constraints. In this work, we explore the social effect of code-switching in an online community. We present a task from the Arabic Wikipedia to capture language choice, in this case code-switching between Arabic and other languages, as a predictor of social influence in collaborative editing. We find that code-switching is positively associated with Wikipedia editor success, particularly borrowing technical language on pages with topics less directly related to Arabic-speaking regions.", "year": 2017, "ssId": "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde", "arXivId": null, "link": null, "openAccess": false, "authors": ["Michael Miller Yoder", "Shruti Rijhwani", "C. Ros\u00e9", "Lori S. Levin"]}}
{"id": "682e69be87f181edcf71800b54083595874d4ec6", "content": {"title": "Preserving Intermediate Objectives: One Simple Trick to Improve Learning for Hierarchical Models", "abstract": "Hierarchical models are utilized in a wide variety of problems which are characterized by task hierarchies, where predictions on smaller subtasks are useful for trying to predict a final task. Typically, neural networks are first trained for the subtasks, and the predictions of these networks are subsequently used as additional features when training a model and doing inference for a final task. In this work, we focus on improving learning for such hierarchical models and demonstrate our method on the task of speaker trait prediction. Speaker trait prediction aims to computationally identify which personality traits a speaker might be perceived to have, and has been of great interest to both the Artificial Intelligence and Social Science communities. Persuasiveness prediction in particular has been of interest, as persuasive speakers have a large amount of influence on our thoughts, opinions and beliefs. In this work, we examine how leveraging the relationship between related speaker traits in a hierarchical structure can help improve our ability to predict how persuasive a speaker is. We present a novel algorithm that allows us to backpropagate through this hierarchy. This hierarchical model achieves a 25% relative error reduction in classification accuracy over current state-of-the art methods on the publicly available POM dataset.", "year": 2017, "ssId": "682e69be87f181edcf71800b54083595874d4ec6", "arXivId": "1706.07867", "link": "https://arxiv.org/pdf/1706.07867.pdf", "openAccess": true, "authors": ["Abhilasha Ravichander", "Shruti Rijhwani", "Rajat Kulshreshtha", "Chirag Nagpal", "T. Baltru\u0161aitis", "Louis-Philippe Morency"]}}
{"id": "5ad44a9d6b850405da42f989711af431427425b5", "content": {"title": "Experiments with Cross-lingual Systems for Synthesis of Code-Mixed Text", "abstract": "Most Text to Speech (TTS) systems today assume that the input is in a single language written in its native script, which is the language that the TTS database is recorded in. However, due to the rise in conversational data available from social media, phenomena such as code-mixing, in which multiple languages are used together in the same conversation or sentence are now seen in text. TTS systems capable of synthesizing such text need to be able to handle multiple languages at the same time, and may also need to deal with noisy input. Previously, we proposed a framework to synthesize code-mixed text by using a TTS database in a single language, identifying the language that each word was from, normalizing spellings of a language written in a non-standardized script and mapping the phonetic space of mixed language to the language that the TTS database was recorded in. We extend this cross-lingual approach to more language pairs, and improve upon our language identification technique. We conduct listening tests to determine which of the two languages being mixed should be used as the target language. We perform experiments for code-mixed Hindi-English and German-English and conduct listening tests with bilingual speakers of these languages. From our subjective experiments we find that listeners have a strong preference for cross-lingual systems with Hindi as the target language for code-mixed Hindi and English text. We also find that listeners prefer cross-lingual systems in English that can synthesize German text for codemixed German and English text.", "year": 2016, "ssId": "5ad44a9d6b850405da42f989711af431427425b5", "arXivId": null, "link": null, "openAccess": false, "authors": ["Sunayana Sitaram", "Sai Krishna Rallabandi", "Shruti Rijhwani", "A. Black"]}}
