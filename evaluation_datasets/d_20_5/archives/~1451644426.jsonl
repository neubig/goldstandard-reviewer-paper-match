{"id": "cb0de2de79533d4faada3d745f43702eb89d1a60", "content": {"title": "Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards", "abstract": "Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates \u2013 the HuggingFace data card, a general purpose card for datasets in NLP, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing templates as our foundation, and iterative revisions based on feedback.", "year": 2021, "ssId": "cb0de2de79533d4faada3d745f43702eb89d1a60", "arXivId": "2108.07374", "link": "https://arxiv.org/pdf/2108.07374.pdf", "openAccess": true, "authors": ["Angelina McMillan-Major", "Salomey Osei", "Juan Diego Rodriguez", "Pawan Sasanka Ammanamanchi", "Sebastian Gehrmann", "Yacine Jernite"]}}
{"id": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "content": {"title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics", "abstract": "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.", "year": 2021, "ssId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "arXivId": "2102.01672", "link": "https://arxiv.org/pdf/2102.01672.pdf", "openAccess": true, "authors": ["Sebastian Gehrmann", "Tosin P. Adewumi", "Karmanya Aggarwal", "Pawan Sasanka Ammanamanchi", "Aremu Anuoluwapo", "Antoine Bosselut", "Khyathi Raghavi Chandu", "Miruna Clinciu", "Dipanjan Das", "Kaustubh D. Dhole", "Wanyu Du", "Esin Durmus", "Ondrej Dusek", "Chris C. Emezue", "Varun Gangal", "Cristina Garbacea", "Tatsunori B. Hashimoto", "Yufang Hou", "Yacine Jernite", "Harsh Jhamtani", "Yangfeng Ji", "Shailza Jolly", "Mihir Kale", "Dhruv Kumar", "Faisal Ladhak", "Aman Madaan", "Mounica Maddela", "Khyati Mahajan", "Saad Mahamood", "Bodhisattwa Prasad Majumder", "Pedro Henrique Martins", "Angelina McMillan-Major", "Simon Mille", "Emiel van Miltenburg", "Moin Nadeem", "Shashi Narayan", "Vitaly Nikolaev", "Rubungo Andre Niyongabo", "Salomey Osei", "Ankur P. Parikh", "Laura Perez-Beltrachini", "Niranjan Rao", "Vikas Raunak", "Juan Diego Rodriguez", "Sashank Santhanam", "Jo\u00e3o Sedoc", "Thibault Sellam", "Samira Shaikh", "Anastasia Shimorina", "Marco Antonio Sobrevilla Cabezudo", "Hendrik Strobelt", "Nishant Subramani", "W. Xu", "Diyi Yang", "Akhila Yerukola", "Jiawei Zhou"]}}
{"id": "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf", "content": {"title": "The Effect of Pretraining on Extractive Summarization for Scientific Documents", "abstract": "Large pretrained models have seen enormous success in extractive summarization tasks. In this work, we investigate the influence of pretraining on a BERT-based extractive summarization system for scientific documents. We derive significant performance improvements using an intermediate pretraining step that leverages existing summarization datasets and report state-of-the-art results on a recently released scientific summarization dataset, SciTLDR. We systematically analyze the intermediate pretraining step by varying the size and domain of the pretraining corpus, changing the length of the input sequence in the target task and varying target tasks. We also investigate how intermediate pretraining interacts with contextualized word embeddings trained on different domains.", "year": 2021, "ssId": "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Yash Gupta", "Pawan Sasanka Ammanamanchi", "Shikha Bordia", "Arjun Manoharan", "Deepak Mittal", "Ramakanth Pasunuru", "Manish Shrivastava", "M. Singh", "Mohit Bansal", "P. Jyothi"]}}
{"id": "40fc6e46f2921be346eacff86ce765ff5b28fbdd", "content": {"title": "BitMEX Funding Correlation with Bitcoin Exchange Rate", "abstract": "This paper examines the relationship between Inverse Perpetual Swap contracts, a Bitcoin derivative akin to futures and the margin funding interest rates levied on BitMEX. This paper proves the Heteroskedastic nature of funding rates and goes onto establish a causal relationship between the funding rates and the Bitcoin inverse Perpetual swap contracts based on Granger causality. The paper further dwells into developing a predictive model for funding rates using best-fitted GARCH models. Implications of the results are presented, and funding rates as a predictive tool for gauging the market trend is discussed.", "year": 2019, "ssId": "40fc6e46f2921be346eacff86ce765ff5b28fbdd", "arXivId": "1912.03270", "link": "https://arxiv.org/pdf/1912.03270.pdf", "openAccess": true, "authors": ["Sai Nimmagadda", "Pawan Sasanka Ammanamanchi"]}}
