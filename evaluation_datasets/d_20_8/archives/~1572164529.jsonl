{"id": "5e327c2285ddf2a76d08e5c00d16c7358bc5412c", "content": {"title": "Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators' Expertise", "abstract": "Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of homeworks, grant proposal review, conference peer review of scientific papers, and peer assessment of employees in organizations. Since an individual\u2019s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators\u2019 expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assigned evaluators\u2019 expertise required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.", "year": 2022, "ssId": "5e327c2285ddf2a76d08e5c00d16c7358bc5412c", "arXivId": "2201.10631", "link": "https://arxiv.org/pdf/2201.10631.pdf", "openAccess": true, "authors": ["Komal Dhull", "Steven Jecmen", "Pravesh Kothari", "Nihar B. Shah"]}}
{"id": "73a6e4574de038878be1bbb5985400998e420a5b", "content": {"title": "The Price of Strategyproofing Peer Assessment", "abstract": "Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of assignments, grant proposal review, conference peer review, and peer assessment of employees. Since an individual\u2019s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assignment quality required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.", "year": 2022, "ssId": "73a6e4574de038878be1bbb5985400998e420a5b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Komal Dhull", "Steven Jecmen", "Pravesh Kothari", "Nihar B. Shah"]}}
{"id": "e2b097bce656db9215505659357263c43190194b", "content": {"title": "Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference Experiment Design", "abstract": "Many scientific conferences employ a two-phase paper review process, where some papers are assigned additional reviewers after the initial reviews are submitted. Many conferences also design and run experiments on their paper review process, where some papers are assigned reviewers who provide reviews under an experimental condition. In this paper, we consider the question: how should reviewers be divided between phases or conditions in order to maximize total assignment similarity? We make several contributions towards answering this question. First, we prove that when the set of papers requiring additional review is unknown, a simplified variant of this problem is NP-hard. Second, we empirically show that across several datasets pertaining to real conference data, dividing reviewers between phases/conditions uniformly at random allows an assignment that is nearly as good as the oracle optimal assignment. This uniformly random choice is practical for both the two-phase and conference experiment design settings. Third, we provide explanations of this phenomenon by providing theoretical bounds on the suboptimality of this random strategy under certain natural conditions. From these easily-interpretable conditions, we provide actionable insights to conference program chairs about whether a random reviewer split is suitable for their conference.", "year": 2021, "ssId": "e2b097bce656db9215505659357263c43190194b", "arXivId": "2108.06371", "link": "https://arxiv.org/pdf/2108.06371.pdf", "openAccess": true, "authors": ["Steven Jecmen", "Hanrui Zhang", "Ryan Liu", "Fei Fang", "V. Conitzer", "Nihar B. Shah"]}}
{"id": "5d6f87e31d806a77d22e344106d0310be3342259", "content": {"title": "Mitigating Manipulation in Peer Review via Randomized Reviewer Assignments", "abstract": "We consider three important challenges in conference peer review: (i) reviewers maliciously attempting to get assigned to certain papers to provide positive reviews, possibly as part of quid-pro-quo arrangements with the authors; (ii) \"torpedo reviewing,\" where reviewers deliberately attempt to get assigned to certain papers that they dislike in order to reject them; (iii) reviewer de-anonymization on release of the similarities and the reviewer-assignment code. On the conceptual front, we identify connections between these three problems and present a framework that brings all these challenges under a common umbrella. We then present a (randomized) algorithm for reviewer assignment that can optimally solve the reviewer-assignment problem under any given constraints on the probability of assignment for any reviewer-paper pair. We further consider the problem of restricting the joint probability that certain suspect pairs of reviewers are assigned to certain papers, and show that this problem is NP-hard for arbitrary constraints on these joint probabilities but efficiently solvable for a practical special case. Finally, we experimentally evaluate our algorithms on datasets from past conferences, where we observe that they can limit the chance that any malicious reviewer gets assigned to their desired paper to 50% while producing assignments with over 90% of the total optimal similarity. Our algorithms still achieve this similarity while also preventing reviewers with close associations from being assigned to the same paper.", "year": 2020, "ssId": "5d6f87e31d806a77d22e344106d0310be3342259", "arXivId": "2006.16437", "link": "https://arxiv.org/pdf/2006.16437.pdf", "openAccess": true, "authors": ["Steven Jecmen", "Hanrui Zhang", "Ryan Liu", "Nihar B. Shah", "V. Conitzer", "Fei Fang"]}}
{"id": "30fa01df767339a6c8bd37c32160992fcb19ed18", "content": {"title": "Bounding Regret in Empirical Games", "abstract": "Empirical game-theoretic analysis refers to a set of models and techniques for solving large-scale games. However, there is a lack of a quantitative guarantee about the quality of output approximate Nash equilibria (NE). A natural quantitative guarantee for such an approximate NE is the regret in the game (i.e. the best deviation gain). We formulate this deviation gain computation as a multi-armed bandit problem, with a new optimization goal unlike those studied in prior work. We propose an efficient algorithm Super-Arm UCB (SAUCB) for the problem and a number of variants. We present sample complexity results as well as extensive experiments that show the better performance of SAUCB compared to several baselines.", "year": 2020, "ssId": "30fa01df767339a6c8bd37c32160992fcb19ed18", "arXivId": null, "link": null, "openAccess": false, "authors": ["Steven Jecmen", "Arunesh Sinha", "Zun Li", "Long Tran-Thanh"]}}
{"id": "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2", "content": {"title": "Bounding Regret in Simulated Games", "abstract": "We present a bandit-style problem arising from a specific problem in agent-based modeling of games. In this preliminary work, we provide some initial heuristic algorithms and compare against some baselines.", "year": 2018, "ssId": "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2", "arXivId": null, "link": null, "openAccess": false, "authors": ["Steven Jecmen", "Erik Brinkman", "Arunesh Sinha"]}}
