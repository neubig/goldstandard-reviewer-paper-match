{"id": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "content": {"title": "Cite-seeing and Reviewing: A Study on Citation Bias in Peer Review", "abstract": "Citations play an important role in researchers\u2019 careers as a key factor in evaluation of scienti\ufb01c impact. Many anecdotes advice authors to exploit this fact and cite prospective reviewers to try obtaining a more positive evaluation for their submission. In this work, we investigate if such a citation bias actually exists: Does the citation of a reviewer\u2019s own work in a submission cause them to be positively biased towards the submission? In conjunction with the review process of two \ufb02agship conferences in machine learning and algorithmic economics, we execute an observational study to test for citation bias in peer review. In our analysis, we carefully account for various confounding factors such as paper quality and reviewer expertise, and apply di\ufb00erent modeling techniques to alleviate concerns regarding the model mismatch. Overall, our analysis involves 1,314 papers and 1,717 reviewers and detects citation bias in both venues we consider. In terms of the e\ufb00ect size, by citing a reviewer\u2019s work, a submission has a non-trivial chance of getting a higher score from the reviewer: an expected increase in the score is approximately 0 . 23 on a 5-point Likert item. For reference, a one-point increase of a score by a single reviewer improves the position of a submission by 11% on average.", "year": 2022, "ssId": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "arXivId": "2203.17239", "link": "https://arxiv.org/pdf/2203.17239.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Ryan Liu", "Shuchi Chawla", "F. Echenique", "Nihar B. Shah"]}}
{"id": "73a6e4574de038878be1bbb5985400998e420a5b", "content": {"title": "The Price of Strategyproofing Peer Assessment", "abstract": "Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of assignments, grant proposal review, conference peer review, and peer assessment of employees. Since an individual\u2019s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assignment quality required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.", "year": 2022, "ssId": "73a6e4574de038878be1bbb5985400998e420a5b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Komal Dhull", "Steven Jecmen", "Pravesh Kothari", "Nihar B. Shah"]}}
{"id": "8b73e226815d57bf66fc94905ebd063e4957b449", "content": {"title": "Calibration with Privacy in Peer Review", "abstract": "Reviewers in peer review are often miscalibrated: they may be strict, lenient, extreme, moderate, etc. A number of algorithms have previously been proposed to calibrate reviews. Such attempts of calibration can however leak sensitive information about which reviewer reviewed which paper. In this paper, we identify this problem of calibration with privacy, and provide a foundational building block to address it. Specifically, we present a theoretical study of this problem under a simplified-yet-challenging model involving two reviewers, two papers, and an MAP-computing adversary. Our main results establish the Pareto frontier of the tradeoff between privacy (preventing the adversary from inferring reviewer identity) and utility (accepting better papers), and design explicit computationally-efficient algorithms that we prove are Pareto optimal.", "year": 2022, "ssId": "8b73e226815d57bf66fc94905ebd063e4957b449", "arXivId": "2201.11308", "link": "https://arxiv.org/pdf/2201.11308.pdf", "openAccess": true, "authors": ["Wenxin Ding", "Gautam Kamath", "Weina Wang", "Nihar B. Shah"]}}
{"id": "2a82a16bdb793dc388391be57d6424f0d5090513", "content": {"title": "Integrating Rankings into Quantized Scores in Peer Review", "abstract": "In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a signi\ufb01cant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing work\ufb02ows, thereby leading to inef\ufb01ciencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and work\ufb02ows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and \ufb01nd that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.", "year": 2022, "ssId": "2a82a16bdb793dc388391be57d6424f0d5090513", "arXivId": "2204.03505", "link": "https://arxiv.org/pdf/2204.03505.pdf", "openAccess": true, "authors": ["Yusha Liu", "Yichong Xu", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "content": {"title": "To ArXiv or not to ArXiv: A Study Quantifying Pros and Cons of Posting Preprints Online", "abstract": "Double-blind conferences have engaged in debates over whether to allow authors to post their papers online on arXiv or elsewhere during the review process. Independently, some authors of research papers face the dilemma of whether to put their papers on arXiv due to its pros and cons. We conduct a study to substantiate this debate and dilemma via quantitative measurements. Speci\ufb01cally, we conducted surveys of reviewers in two top-tier double-blind computer science conferences\u2014ICML 2021 (5361 submissions and 4699 reviewers) and EC 2021 (498 submissions and 190 reviewers). Our two main \ufb01ndings are as follows. First, more than a third of the reviewers self-report searching online for a paper they are assigned to review. Second, outside the review process, we \ufb01nd that preprints from better-ranked a\ufb03liations see a weakly higher visibility, with a correlation of 0.06 in ICML and 0.05 in EC. In particular, papers associated with the top-10-ranked a\ufb03liations had a visibility of approximately 11% in ICML and 22% in EC, whereas the remaining papers had a visibility of 7% and 18% respectively.", "year": 2022, "ssId": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "arXivId": "2203.17259", "link": "https://arxiv.org/pdf/2203.17259.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Ivan Stelmakh", "Xinwei Shen", "M. Meil\u0103", "F. Echenique", "Shuchi Chawla", "Nihar B. Shah"]}}
{"id": "5e327c2285ddf2a76d08e5c00d16c7358bc5412c", "content": {"title": "Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators' Expertise", "abstract": "Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of homeworks, grant proposal review, conference peer review of scientific papers, and peer assessment of employees in organizations. Since an individual\u2019s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators\u2019 expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assigned evaluators\u2019 expertise required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.", "year": 2022, "ssId": "5e327c2285ddf2a76d08e5c00d16c7358bc5412c", "arXivId": "2201.10631", "link": "https://arxiv.org/pdf/2201.10631.pdf", "openAccess": true, "authors": ["Komal Dhull", "Steven Jecmen", "Pravesh Kothari", "Nihar B. Shah"]}}
{"id": "91184a2d40be8a0171b5c926b336666ed717ec6e", "content": {"title": "JCDL 2021 Tutorial on Systemic Challenges and Computational Solutions on Bias and Unfairness in Peer Review", "abstract": "Peer review is the backbone of scientific research and determines the composition of scientific digital libraries. Any systemic issues in peer review - such as biases or fraud - can systematically affect the resulting scientific digital library as well as any analyses on that library. They also affect billions of dollars in research grants made via peer review as well as entire careers of researchers. The tutorial will discuss various systemic issues in peer review via insightful experiments, several computational solutions proposed to address these issues, and a number of important open problems. A detailed writeup on the topics of this tutorial as well as a complete list of references is available in [1].", "year": 2021, "ssId": "91184a2d40be8a0171b5c926b336666ed717ec6e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "76f02d20e02c6baf39fee8f115cd94e4ceacf32b", "content": {"title": "Prior and Prejudice", "abstract": "Modern machine learning and computer science conferences are experiencing a surge in the number of submissions that challenges the quality of peer review as the number of competent reviewers is growing at a much slower rate. To curb this trend and reduce the burden on reviewers, several conferences have started encouraging or even requiring authors to declare the previous submission history of their papers. Such initiatives have been met with skepticism among authors, who raise the concern about a potential bias in reviewers' recommendations induced by this information. In this work, we investigate whether reviewers exhibit a bias caused by the knowledge that the submission under review was previously rejected at a similar venue, focusing on a population of novice reviewers who constitute a large fraction of the reviewer pool in leading machine learning and computer science conferences. We design and conduct a randomized controlled trial closely replicating the relevant components of the peer-review pipeline with $133$ reviewers (master's, junior PhD students, and recent graduates of top US universities) writing reviews for $19$ papers. The analysis reveals that reviewers indeed become negatively biased when they receive a signal about paper being a resubmission, giving almost 1 point lower overall score on a 10-point Likert item (\u0394 = -0.78, 95% CI = [-1.30, -0.24]) than reviewers who do not receive such a signal. Looking at specific criteria scores (originality, quality, clarity and significance), we observe that novice reviewers tend to underrate quality the most.", "year": 2021, "ssId": "76f02d20e02c6baf39fee8f115cd94e4ceacf32b", "arXivId": null, "link": null, "openAccess": false, "authors": ["I. Stelmakh", "Nihar B. Shah", "Aarti Singh", "Hal Daum\u00e9"]}}
{"id": "6d2d86cf5e80b58a03360559095ea3603548248f", "content": {"title": "A heuristic for statistical seriation", "abstract": "We study the statistical seriation problem, where the goal is to estimate a matrix whose rows satisfy the same shape constraint after a permutation of the columns. This is a important classical problem, with close connections to statistical literature in permutation-based models and also has wide applications ranging from archaeology to biology. Specifically, we consider the case where the rows are monotonically increasing after an unknown permutation of the columns. Past work has shown that the least-squares estimator is optimal up to logarithmic factors, but efficient algorithms for computing the least-squares estimator remain unknown to date. We approach this important problem from a heuristic perspective. Specifically, we replace the combinatorial permutation constraint by a continuous regularization term, and then use projected gradient descent to obtain a local minimum of the non-convex objective. We show that the attained local minimum is the global minimum in certain special cases under the noiseless setting, and preserves desirable properties under the noisy setting. Simulation results reveal that our proposed algorithm outperforms prior algorithms when (1) the underlying model is more complex than simplistic parametric assumptions such as low-rankedness, or (2) the signal-to-noise ratio is high. Under partial observations, the proposed algorithm requires an initialization, and different initializations may lead to different local minima. We empirically observe that the proposed algorithm yields consistent improvement over the initialization, even though different initializations start with different levels of quality.", "year": 2021, "ssId": "6d2d86cf5e80b58a03360559095ea3603548248f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Komal Dhull", "Jingyan Wang", "Nihar B. Shah", "Yuanzhi Li", "R. Ravi"]}}
{"id": "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264", "content": {"title": "WSDM 2021 Tutorial on Systematic Challenges and Computational Solutions on Bias and Unfairness in Peer Review", "abstract": "Peer review is the backbone of scientific research. Yet peer review is called \"biased,\" \"broken,\" and \"unscientific\" in many scientific disciplines. This problem is further compounded with the near-exponentially growing number of submissions in various computer science conferences. Due to the prevalence of \"Matthew effect'' of rich getting richer in academia, any source of unfairness in the peer review system, such as those discussed in this tutorial, can considerably affect the entire career trajectory of (young) researchers. This tutorial will discuss a number of systemic challenges in peer review such as biases, subjectivity, miscalibration, dishonest behavior, and noise. For each issue, the tutorial will first present insightful experiments to understand the issue. Then the tutorial will present computational techniques designed to address these challenges. Many open problems will be highlighted which are envisaged to be exciting to the WSDM audience, and will lead to significant impact if solved.", "year": 2021, "ssId": "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "4c0a915b9389e6489753a968085ee12833131d0a", "content": {"title": "KDD 2021 Tutorial on Systemic Challenges and Solutions on Bias and Unfairness in Peer Review", "abstract": "Introduction. Peer review is a cornerstone of academic practice [1]. The peer review process is highly regarded by the vast majority of researchers and considered by most to be essential to the communication of scholarly research [2\u20134]. However, there is also an overwhelming desire for improvement [2, 4, 5]. Problems in peer review have consequences much beyond the outcome for a specific paper or grant, particularly due to the widespread prevalence of the Matthew effect (\u201crich get richer\u201d) in academia [6]. As noted by [7] \u201can incompetent review may lead to the rejection of the submitted paper, or of the grant application, and the ultimate failure of the career of the author.\u201d (See also [8, 9].) The importance of peer review and the urgent need for improvements, behooves research on principled approaches towards addressing problems in peer review, particularly at scale. In this tutorial, we discuss a number of key challenges in peer review, outline several directions of research on this topic, and also highlight important open problems that we envisage to be exciting to the community. This document summarizes the contents of the tutorial and provides relevant references.", "year": 2021, "ssId": "4c0a915b9389e6489753a968085ee12833131d0a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "e2b097bce656db9215505659357263c43190194b", "content": {"title": "Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference Experiment Design", "abstract": "Many scientific conferences employ a two-phase paper review process, where some papers are assigned additional reviewers after the initial reviews are submitted. Many conferences also design and run experiments on their paper review process, where some papers are assigned reviewers who provide reviews under an experimental condition. In this paper, we consider the question: how should reviewers be divided between phases or conditions in order to maximize total assignment similarity? We make several contributions towards answering this question. First, we prove that when the set of papers requiring additional review is unknown, a simplified variant of this problem is NP-hard. Second, we empirically show that across several datasets pertaining to real conference data, dividing reviewers between phases/conditions uniformly at random allows an assignment that is nearly as good as the oracle optimal assignment. This uniformly random choice is practical for both the two-phase and conference experiment design settings. Third, we provide explanations of this phenomenon by providing theoretical bounds on the suboptimality of this random strategy under certain natural conditions. From these easily-interpretable conditions, we provide actionable insights to conference program chairs about whether a random reviewer split is suitable for their conference.", "year": 2021, "ssId": "e2b097bce656db9215505659357263c43190194b", "arXivId": "2108.06371", "link": "https://arxiv.org/pdf/2108.06371.pdf", "openAccess": true, "authors": ["Steven Jecmen", "Hanrui Zhang", "Ryan Liu", "Fei Fang", "V. Conitzer", "Nihar B. Shah"]}}
{"id": "5d6f87e31d806a77d22e344106d0310be3342259", "content": {"title": "Mitigating Manipulation in Peer Review via Randomized Reviewer Assignments", "abstract": "We consider three important challenges in conference peer review: (i) reviewers maliciously attempting to get assigned to certain papers to provide positive reviews, possibly as part of quid-pro-quo arrangements with the authors; (ii) \"torpedo reviewing,\" where reviewers deliberately attempt to get assigned to certain papers that they dislike in order to reject them; (iii) reviewer de-anonymization on release of the similarities and the reviewer-assignment code. On the conceptual front, we identify connections between these three problems and present a framework that brings all these challenges under a common umbrella. We then present a (randomized) algorithm for reviewer assignment that can optimally solve the reviewer-assignment problem under any given constraints on the probability of assignment for any reviewer-paper pair. We further consider the problem of restricting the joint probability that certain suspect pairs of reviewers are assigned to certain papers, and show that this problem is NP-hard for arbitrary constraints on these joint probabilities but efficiently solvable for a practical special case. Finally, we experimentally evaluate our algorithms on datasets from past conferences, where we observe that they can limit the chance that any malicious reviewer gets assigned to their desired paper to 50% while producing assignments with over 90% of the total optimal similarity. Our algorithms still achieve this similarity while also preventing reviewers with close associations from being assigned to the same paper.", "year": 2020, "ssId": "5d6f87e31d806a77d22e344106d0310be3342259", "arXivId": "2006.16437", "link": "https://arxiv.org/pdf/2006.16437.pdf", "openAccess": true, "authors": ["Steven Jecmen", "Hanrui Zhang", "Ryan Liu", "Nihar B. Shah", "V. Conitzer", "Fei Fang"]}}
{"id": "36d193c7a9523f55f9fe5ffd0730f248c241f5c7", "content": {"title": "AAAI 2020 Tutorial on Fairness and Bias in Peer Review and other Sociotechnical Intelligent Systems (Part II on Peer Review)", "abstract": "Peer review is the backbone of scholarly research, but it faces a number of challenges pertaining to bias and unfairness. There is an urgent need to improve peer review. This AAAI tutorial (part 2) discusses several problems, empirical studies, proposed solutions, and open problems in this domain. This document serves to provide a summary and references for the", "year": 2020, "ssId": "36d193c7a9523f55f9fe5ffd0730f248c241f5c7", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah", "Z. Lipton"]}}
{"id": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "content": {"title": "A Large Scale Randomized Controlled Trial on Herding in Peer-Review Discussions", "abstract": "Peer review is the backbone of academia and humans constitute a cornerstone of this process, being responsible for reviewing papers and making the final acceptance/rejection decisions. Given that human decision making is known to be susceptible to various cognitive biases, it is important to understand which (if any) biases are present in the peer-review process and design the pipeline such that the impact of these biases is minimized. In this work, we focus on the dynamics of between-reviewers discussions and investigate the presence of herding behaviour therein. In that, we aim to understand whether reviewers and more senior decision makers get disproportionately influenced by the first argument presented in the discussion when (in case of reviewers) they form an independent opinion about the paper before discussing it with others. Specifically, in conjunction with the review process of ICML 2020 -- a large, top tier machine learning conference -- we design and execute a randomized controlled trial with the goal of testing for the conditional causal effect of the discussion initiator's opinion on the outcome of a paper.", "year": 2020, "ssId": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "arXivId": "2011.15083", "link": "https://arxiv.org/pdf/2011.15083.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Nihar B. Shah", "Aarti Singh", "Hal Daum'e"]}}
{"id": "cc19de8d0782917098029ed20261cbe0b0c62bf5", "content": {"title": "Uncovering Latent Biases in Text: Method and Application to Peer Review", "abstract": "Quantifying systematic disparities in numerical quantities such as employment rates and wages between population subgroups provides compelling evidence for the existence of societal biases. However, biases in the text written for members of different subgroups (such as in recommendation letters for male and non-male candidates), though widely reported anecdotally, remain challenging to quantify. In this work, we introduce a novel framework to quantify bias in text caused by the visibility of subgroup membership indicators. We develop a nonparametric estimation and inference procedure to estimate this bias. We then formalize an identification strategy to causally link the estimated bias to the visibility of subgroup membership indicators, provided observations from time periods both before and after an identity-hiding policy change. We identify an application wherein \"ground truth\" bias can be inferred to evaluate our framework, instead of relying on synthetic or secondary data. Specifically, we apply our framework to quantify biases in the text of peer reviews from a reputed machine learning conference before and after the conference adopted a double-blind reviewing policy. We show evidence of biases in the review ratings that serves as \"ground truth\", and show that our proposed framework accurately detects these biases from the review text without having access to the review ratings.", "year": 2020, "ssId": "cc19de8d0782917098029ed20261cbe0b0c62bf5", "arXivId": "2010.15300", "link": "https://arxiv.org/pdf/2010.15300.pdf", "openAccess": true, "authors": ["Emaad Manzoor", "Nihar B. Shah"]}}
{"id": "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2", "content": {"title": "Approval Voting and Incentives in Crowdsourcing", "abstract": "The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to %judiciously utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\"strictly proper\") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.", "year": 2020, "ssId": "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah"]}}
{"id": "01138945dc9de691cd559d09a46597cca7659efb", "content": {"title": "SIGMOD 2020 Tutorial on Fairness and Bias in Peer Review and Other Sociotechnical Intelligent Systems", "abstract": "Questions of fairness and bias abound in all socially-consequential decisions pertaining to collection and management of data. Whether designing protocols for peer review of research papers, setting hiring policies, or framing research question in genetics, any data-management decision with the potential to allocate benefits or confer harms raises concerns about who gains or loses that may fail to surface in naively-chosen performance measures. Data science interacts with these questions in two fundamentally different ways: (i) as the technology driving the very systems responsible for certain social impacts, posing new questions about what it means for such systems to accord with ethical norms and the law; and (ii) as a set of powerful tools for analyzing existing data management systems, e.g., for auditing existing systems for various biases. This tutorial will tackle both angles on the interaction between technology and society vis-a-vis concerns over fairness and bias, particularly focusing on the collection and management of data. Our presentation will cover a wide range of disciplinary perspectives with the first part focusing on the social impacts of technology and the formulations of fairness and bias defined via protected characteristics and the second part taking a deep into peer review and distributed human evaluations, to explore other forms of bias, such as that due to subjectivity, miscalibration, and dishonest behavior.", "year": 2020, "ssId": "01138945dc9de691cd559d09a46597cca7659efb", "arXivId": null, "link": null, "openAccess": false, "authors": ["Nihar B. Shah", "Zachary Chase Lipton"]}}
{"id": "18e8646001fc53465fdc8f8eb01523e24c134493", "content": {"title": "Ranking and Rating Rankings and Ratings", "abstract": "Cardinal scores collected from people are well known to suffer from miscalibrations. A popular approach to address this issue is to assume simplistic models of miscalibration (such as linear biases) to de-bias the scores. This approach, however, often fares poorly because people's miscalibrations are typically far more complex and not well understood. It is widely believed that in the absence of simplifying assumptions on the miscalibration, the only useful information in practice from the cardinal scores is the induced ranking. In this paper we address the fundamental question of whether this widespread folklore belief is actually true. We consider cardinal scores with arbitrary (or even adversarially chosen) miscalibrations that is only required to be consistent with the induced ranking. We design rating-based estimators and prove that despite making no assumptions on the ratings, they strictly and uniformly outperform all possible estimators that rely on only the ranking. These estimators can be used as a plug-in to show the superiority of cardinal scores over ordinal rankings for a variety of applications, including A/B testing and ranking. This work thus provides novel fundamental insights in the eternal debate between cardinal and ordinal data: It ranks the approach of using ratings higher than that of using rankings, and rates both approaches in terms of their estimation errors.", "year": 2020, "ssId": "18e8646001fc53465fdc8f8eb01523e24c134493", "arXivId": null, "link": null, "openAccess": false, "authors": ["Jingyan Wang", "Nihar B. Shah"]}}
{"id": "803a0d2677a7d6b20c3964533595775fa5c7c750", "content": {"title": "Two-Sample Testing on Ranked Preference Data and the Role of Modeling Assumptions", "abstract": "A number of applications require two-sample testing on ranked preference data. For instance, in crowdsourcing, there is a long-standing question of whether pairwise comparison data provided by people is distributed similar to ratings-converted-to-comparisons. Other examples include sports data analysis and peer grading. In this paper, we design two-sample tests for pairwise comparison data and ranking data. For our two-sample test for pairwise comparison data, we establish an upper bound on the sample complexity required to correctly distinguish between the distributions of the two sets of samples. Our test requires essentially no assumptions on the distributions. We then prove complementary lower bounds showing that our results are tight (in the minimax sense) up to constant factors. We investigate the role of modeling assumptions by proving lower bounds for a range of pairwise comparison models (WST, MST,SST, parameter-based such as BTL and Thurstone). We also provide testing algorithms and associated sample complexity bounds for the problem of two-sample testing with partial (or total) ranking data.Furthermore, we empirically evaluate our results via extensive simulations as well as two real-world datasets consisting of pairwise comparisons. By applying our two-sample test on real-world pairwise comparison data, we conclude that ratings and rankings provided by people are indeed distributed differently. On the other hand, our test recognizes no significant difference in the relative performance of European football teams across two seasons. Finally, we apply our two-sample test on a real-world partial and total ranking dataset and find a statistically significant difference in Sushi preferences across demographic divisions based on gender, age and region of residence.", "year": 2020, "ssId": "803a0d2677a7d6b20c3964533595775fa5c7c750", "arXivId": "2006.11909", "link": "https://arxiv.org/pdf/2006.11909.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Sivaraman Balakrishnan", "Nihar B. Shah", "Aarti Singh"]}}
