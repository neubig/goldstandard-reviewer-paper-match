{"id": "c96363c42bc8c465902c22b8c33c8704233f519e", "content": {"title": "MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages", "abstract": "While there has been a recent burgeoning of applications at the intersection of natural and programming languages, such as code generation and code summarization, these applications are usually English-centric. This creates a barrier for program developers who are not proficient in English. To mitigate this gap in technology development across languages, we propose a multilingual dataset, MCoNaLa, to benchmark code generation from natural language commands extending beyond English. Modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896 NL-code pairs in three languages: Spanish, Japanese, and Russian. We present a quantitative evaluation of performance on the MCoNaLa dataset by testing with state-of-theart code generation systems. While the difficulties vary across these three languages, all systems lag significantly behind their English counterparts, revealing the challenges in adapting code generation to new languages. 1", "year": 2022, "ssId": "c96363c42bc8c465902c22b8c33c8704233f519e", "arXivId": "2203.08388", "link": "https://arxiv.org/pdf/2203.08388.pdf", "openAccess": true, "authors": ["Zhiruo Wang", "Grace Cuenca", "Shuyan Zhou", "Frank F. Xu", "Graham Neubig"]}}
{"id": "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f", "content": {"title": "Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data", "abstract": "Procedures are inherently hierarchical. To make videos, one may need to purchase a camera, which in turn may require one to set a budget. While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation. In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to carry out a complex procedure. To this end, we develop a simple and efficient method that links steps (e.g., purchase a camera) in an article to other articles with similar goals (e.g., how to choose a camera), recursively constructing the KB. Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.1", "year": 2022, "ssId": "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f", "arXivId": "2203.07264", "link": "https://arxiv.org/pdf/2203.07264.pdf", "openAccess": true, "authors": ["Shuyan Zhou", "Li Zhang", "Yue Yang", "QING LYU", "Pengcheng Yin", "Chris Callison-Burch", "Graham Neubig"]}}
{"id": "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8", "content": {"title": "Hierarchical Control of Situated Agents through Natural Language", "abstract": "When humans conceive how to perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, in the literature on natural language (NL) command of situated agents, most works have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a powerful yet intuitive method of representing hierarchical procedural knowledge for agent command and control. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development.", "year": 2021, "ssId": "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8", "arXivId": "2109.08214", "link": "https://arxiv.org/pdf/2109.08214.pdf", "openAccess": true, "authors": ["Shuyan Zhou", "Pengcheng Yin", "Graham Neubig"]}}
{"id": "2583e7e279e2969493c3290c8f300ab32da40bf9", "content": {"title": "Improving Candidate Generation for Low-resource Cross-lingual Entity Linking", "abstract": "Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL.1", "year": 2020, "ssId": "2583e7e279e2969493c3290c8f300ab32da40bf9", "arXivId": "2003.01343", "link": "https://arxiv.org/pdf/2003.01343.pdf", "openAccess": true, "authors": ["Shuyan Zhou", "Shruti Rijhawani", "J. Wieting", "J. Carbonell", "Graham Neubig"]}}
{"id": "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de", "content": {"title": "Soft Gazetteers for Low-Resource Named Entity Recognition", "abstract": "Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of \u201csoft gazetteers\u201d that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.", "year": 2020, "ssId": "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de", "arXivId": "2005.01866", "link": "https://arxiv.org/pdf/2005.01866.pdf", "openAccess": true, "authors": ["Shruti Rijhwani", "Shuyan Zhou", "Graham Neubig", "J. Carbonell"]}}
{"id": "67b29c3fe6f110125a8892e8ed128d20b23957ea", "content": {"title": "Towards Zero-resource Cross-lingual Entity Linking", "abstract": "Cross-lingual entity linking (XEL) grounds named entities in a source language to an English Knowledge Base (KB), such as Wikipedia. XEL is challenging for most languages because of limited availability of requisite resources. However, many works on XEL have been on simulated settings that actually use significant resources (e.g. source language Wikipedia, bilingual entity maps, multilingual embeddings) that are not available in truly low-resource languages. In this work, we first examine the effect of these resource assumptions and quantify how much the availability of these resource affects overall quality of existing XEL systems. We next propose three improvements to both entity candidate generation and disambiguation that make better use of the limited resources we do have in resource-scarce scenarios. With experiments on four extremely low-resource languages, we show that our model results in gains of 6-20% end-to-end linking accuracy.", "year": 2019, "ssId": "67b29c3fe6f110125a8892e8ed128d20b23957ea", "arXivId": "1909.13180", "link": "https://arxiv.org/pdf/1909.13180.pdf", "openAccess": true, "authors": ["Shuyan Zhou", "Shruti Rijhwani", "Graham Neubig"]}}
{"id": "06f4de06fc37576e1e381cd76e375d57852047b9", "content": {"title": "Improving Robustness of Neural Machine Translation with Multi-task Learning", "abstract": "While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.", "year": 2019, "ssId": "06f4de06fc37576e1e381cd76e375d57852047b9", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shuyan Zhou", "Xiangkai Zeng", "Yingqi Zhou", "Antonios Anastasopoulos", "Graham Neubig"]}}
{"id": "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7", "content": {"title": "Aggregated Semantic Matching for Short Text Entity Linking", "abstract": "The task of entity linking aims to identify concepts mentioned in a text fragments and link them to a reference knowledge base. Entity linking in long text has been well studied in previous work. However, short text entity linking is more challenging since the text are noisy and less coherent. To better utilize the local information provided in short texts, we propose a novel neural network framework, Aggregated Semantic Matching (ASM), in which two different aspects of semantic information between the local context and the candidate entity are captured via representation-based and interaction-based neural semantic matching models, and then two matching signals work jointly for disambiguation with a rank aggregation mechanism. Our evaluation shows that the proposed model outperforms the state-of-the-arts on public tweet datasets.", "year": 2018, "ssId": "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7", "arXivId": null, "link": null, "openAccess": false, "authors": ["Feng Nie", "Shuyan Zhou", "Jing Liu", "Jinpeng Wang", "Chin-Yew Lin", "Rong Pan"]}}
