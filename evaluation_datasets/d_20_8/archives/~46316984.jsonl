{"id": "6695d3b92e7cd7f2359f698a09c7b3dc37996329", "content": {"title": "LAMP: Label Augmented Multimodal Pretraining", "abstract": "Multi-modal representation learning by pretraining has become an increasing interest due to its easy-to-use and potential benefit for various Visual-and-Language~(V-L) tasks. However its requirement of large volume and high-quality vision-language pairs highly hinders its values in practice. In this paper, we proposed a novel label-augmented V-L pretraining model, named LAMP, to address this problem. Specifically, we leveraged auto-generated labels of visual objects to enrich vision-language pairs with fine-grained alignment and correspondingly designed a novel pretraining task. Besides, we also found such label augmentation in second-stage pretraining would further universally benefit various downstream tasks. To evaluate LAMP, we compared it with some state-of-the-art models on four downstream tasks. The quantitative results and analysis have well proven the value of labels in V-L pretraining and the effectiveness of LAMP.", "year": 2020, "ssId": "6695d3b92e7cd7f2359f698a09c7b3dc37996329", "arXivId": "2012.04446", "link": "https://arxiv.org/pdf/2012.04446.pdf", "openAccess": true, "authors": ["Jia Guo", "Chen Zhu", "Yilun Zhao", "Heda Wang", "Yao Hu", "Xiaofei He", "Deng Cai"]}}
{"id": "059f515bf53bcddeca031fd4a4071c911999a3c6", "content": {"title": "Apparel-invariant Feature Learning for Apparel-changed Person Re-identification", "abstract": "With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons' appearance rarely changes. In real-world applications such as in a shopping mall, the same person's clothing may change, and different persons may wearing similar clothes. All these cases can result in an inconsistent ReID performance, revealing a critical problem that current ReID models heavily rely on person's apparels. Therefore, it is critical to learn an apparel-invariant person representation under cases like cloth changing or several persons wearing similar clothes. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth changing images according to the target cloth embedding. It's worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth changing images. We conduct extensive experiments on several datasets comparing with several baselines. Experimental results demonstrate that our proposal can improve the ReID performance of the baseline models.", "year": 2020, "ssId": "059f515bf53bcddeca031fd4a4071c911999a3c6", "arXivId": "2008.06181", "link": "https://arxiv.org/pdf/2008.06181.pdf", "openAccess": true, "authors": ["Zhengxu Yu", "Yilun Zhao", "Bin Hong", "Zhongming Jin", "Jianqiang Huang", "Deng Cai", "Xiansheng Hua"]}}
{"id": "a2221b03211408ac2db0559b9a54c1d72b5f560c", "content": {"title": "MusiCoder: A Universal Music-Acoustic Encoder Based on Transformers", "abstract": "Music annotation has always been one of the critical topics in the field of Music Information Retrieval (MIR). Traditional models use supervised learning for music annotation tasks. However, as supervised machine learning approaches increase in complexity, the increasing need for more annotated training data can often not be matched with available data. Moreover, over-reliance on labeled data when training supervised learning models can lead to unexpected results and open vulnerabilities for adversarial attacks. In this paper, a new self-supervised music acoustic representation learning approach named MusiCoder is proposed. Inspired by the success of BERT, MusiCoder builds upon the architecture of self-attention bidirectional transformers. Two pre-training objectives, including Contiguous Frames Masking (CFM) and Contiguous Channels Masking (CCM), are designed to adapt BERT-like masked reconstruction pre-training to continuous acoustic frame domain. The performance of MusiCoder is evaluated in two downstream music annotation tasks. The results show that MusiCoder outperforms the state-of-the-art models in both music genre classification and auto-tagging tasks. The effectiveness of MusiCoder indicates a great potential of a new self-supervised learning approach to understand music: first apply masked reconstruction tasks to pre-train a transformer-based model with massive unlabeled music acoustic data, and then finetune the model on specific downstream tasks with labeled data.", "year": 2020, "ssId": "a2221b03211408ac2db0559b9a54c1d72b5f560c", "arXivId": "2008.00781", "link": "https://arxiv.org/pdf/2008.00781.pdf", "openAccess": true, "authors": ["Yilun Zhao", "Xinda Wu", "Yuqing Ye", "Jia Guo", "Ke-jun Zhang"]}}
