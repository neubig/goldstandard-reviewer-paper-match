{"id": "62d17b6f6ad77fd71ef9954c7784700d5e316f1f", "content": {"title": "What Does it Mean for a Language Model to Preserve Privacy?", "abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.", "year": 2022, "ssId": "62d17b6f6ad77fd71ef9954c7784700d5e316f1f", "arXivId": "2202.05520", "link": "https://arxiv.org/pdf/2202.05520.pdf", "openAccess": true, "authors": ["Hannah Brown", "Katherine Lee", "FatemehSadat Mireshghallah", "R. Shokri", "Florian Tram\u00e8r"]}}
{"id": "425a4a9c0598e4101ca2f2b930f5c6986ce40a99", "content": {"title": "Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels", "abstract": "Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups.", "year": 2021, "ssId": "425a4a9c0598e4101ca2f2b930f5c6986ce40a99", "arXivId": "2103.07567", "link": "https://arxiv.org/pdf/2103.07567.pdf", "openAccess": true, "authors": ["FatemehSadat Mireshghallah", "Huseyin A. Inan", "Marcello Hasegawa", "Victor Ruhle", "Taylor Berg-Kirkpatrick", "Robert Sim"]}}
{"id": "0fcfa0ef253a81c103854e1dc123d90e7310a0e1", "content": {"title": "DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?", "abstract": "Recent advances in differentially private deep learning have demonstrated that application of differential privacy\u2013 speci\ufb01cally the DP-SGD algorithm\u2013 has a disparate impact on different sub-groups in the population, which leads to a signi\ufb01cantly high drop-in model utility for sub-populations that are under-represented (minori-ties), compared to well-represented ones. In this work, we aim to compare PATE, another mechanism for training deep learning models using differential privacy, with DP-SGD in terms of fairness. We show that PATE does have a disparate impact too, however, it is much less severe than DP-SGD. We draw insights from this observation on what might be promising directions in achieving better fairness-privacy trade-offs.", "year": 2021, "ssId": "0fcfa0ef253a81c103854e1dc123d90e7310a0e1", "arXivId": "2106.12576", "link": "https://arxiv.org/pdf/2106.12576.pdf", "openAccess": true, "authors": ["Archit Uniyal", "Rakshit Naidu", "Sasikanth Kotti", "Sahib Singh", "Patrik Joslin Kenfack", "FatemehSadat Mireshghallah", "Andrew Trask"]}}
{"id": "66f7d22d6373af5032074b25828331958b07e7f9", "content": {"title": "When Differential Privacy Meets Interpretability: A Case Study", "abstract": "Given the increase in the use of personal data for training Deep Neural Networks (DNNs) in tasks such as medical imaging and diagnosis, differentially private training of DNNs is surging in importance and there is a large body of work focusing on providing better privacy-utility trade-off. However, little attention is given to the interpretability of these models, and how the application of DP affects the quality of interpretations. We propose an extensive study into the effects of DP training on DNNs, especially on medical imaging applications, on the APTOS dataset.", "year": 2021, "ssId": "66f7d22d6373af5032074b25828331958b07e7f9", "arXivId": "2106.13203", "link": "https://arxiv.org/pdf/2106.13203.pdf", "openAccess": true, "authors": ["Rakshit Naidu", "Aman Priyanshu", "Aadith Kumar", "Sasikanth Kotti", "Haofan Wang", "FatemehSadat Mireshghallah"]}}
{"id": "66b83f0801d0c2d4194ff60c5ef9c754b51ce521", "content": {"title": "U-Noise: Learnable Noise Masks for Interpretable Image Segmentation", "abstract": "Deep Neural Networks (DNNs) are widely used for decision making in a myriad of critical applications, ranging from medical to societal and even judicial. Given the importance of these decisions, it is crucial for us to be able to interpret these models. We introduce a new method for interpreting image segmentation models by learning regions of images in which noise can be applied without hindering downstream model performance. We apply this method to segmentation of the pancreas in CT scans, and qualitatively compare the quality of the method to existing explainability techniques, such as Grad-CAM and occlusion sensitivity. Additionally we show that, unlike other methods, our interpretability model can be quantitatively evaluated based on the downstream performance over obscured images.", "year": 2021, "ssId": "66b83f0801d0c2d4194ff60c5ef9c754b51ce521", "arXivId": "2101.05791", "link": "https://arxiv.org/pdf/2101.05791.pdf", "openAccess": true, "authors": ["Teddy Koker", "FatemehSadat Mireshghallah", "Tom Titcombe", "G. Kaissis"]}}
{"id": "1b57ffe73ae95f339015c174ec574b59f99ea553", "content": {"title": "Not All Features Are Equal: Discovering Essential Features for Preserving Prediction Privacy", "abstract": "When receiving machine learning services from the cloud, the provider does not need to receive all features; in fact, only a subset of the features are necessary for the target prediction task. Discerning this subset is the key problem of this work. We formulate this problem as a gradient-based perturbation maximization method that discovers this subset in the input feature space with respect to the functionality of the prediction model used by the provider. After identifying the subset, our framework, Cloak, suppresses the rest of the features using utility-preserving constant values that are discovered through a separate gradient-based optimization process. We show that Cloak does not necessarily require collaboration from the service provider beyond its normal service, and can be applied in scenarios where we only have black-box access to the service provider\u2019s model. We theoretically guarantee that Cloak\u2019s optimizations reduce the upper bound of the Mutual Information (MI) between the data and the sifted representations that are sent out. Experimental results show that Cloak reduces the mutual information between the input and the sifted representations by 85.01% with only negligible reduction in utility (1.42%). In addition, we show that Cloak greatly diminishes adversaries\u2019 ability to learn and infer non-conducive features.", "year": 2021, "ssId": "1b57ffe73ae95f339015c174ec574b59f99ea553", "arXivId": null, "link": null, "openAccess": false, "authors": ["FatemehSadat Mireshghallah", "Mohammadkazem Taram", "A. Jalali", "Ahmed T. Elthakeb", "D. Tullsen", "H. Esmaeilzadeh"]}}
{"id": "63567f348231abed171c02f99d4c49c2892a2ade", "content": {"title": "Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy", "abstract": "Deployment of deep learning in different fields and industries is growing day by day due to its performance, which relies on the availability of data and compute. Data is often crowd-sourced and contains sensitive information about its contributors, which leaks into models that are trained on it. To achieve rigorous privacy guarantees, differentially private training mechanisms are used. However, it has recently been shown that differential privacy can exacerbate existing biases in the data and have disparate impacts on the accuracy of different subgroups of data. In this paper, we aim to study these effects within differentially private deep learning. Specifically, we aim to study how different levels of imbalance in the data affect the accuracy and the fairness of the decisions made by the model, given different levels of privacy. We demonstrate that even small imbalances and loose privacy guarantees can cause disparate impacts.", "year": 2020, "ssId": "63567f348231abed171c02f99d4c49c2892a2ade", "arXivId": "2009.06389", "link": "https://arxiv.org/pdf/2009.06389.pdf", "openAccess": true, "authors": ["Tom Farrand", "FatemehSadat Mireshghallah", "Sahib Singh", "Andrew Trask"]}}
{"id": "3261728694c0a53a2e8f95326f94147a28e03a83", "content": {"title": "Gradient-Based Deep Quantization of Neural Networks through Sinusoidal Adaptive Regularization", "abstract": "As deep neural networks make their ways into different domains, their compute efficiency is becoming a first-order constraint. Deep quantization, which reduces the bitwidth of the operations (below 8 bits), offers a unique opportunity as it can reduce both the storage and compute requirements of the network super-linearly. However, if not employed with diligence, this can lead to significant accuracy loss. Due to the strong inter-dependence between layers and exhibiting different characteristics across the same network, choosing an optimal bitwidth per layer granularity is not a straight forward. As such, deep quantization opens a large hyper-parameter space, the exploration of which is a major challenge. We propose a novel sinusoidal regularization, called SINAREQ, for deep quantized training. Leveraging the sinusoidal properties, we seek to learn multiple quantization parameterization in conjunction during gradient-based training process. Specifically, we learn (i) a per-layer quantization bitwidth along with (ii) a scale factor through learning the period of the sinusoidal function. At the same time, we exploit the periodicity, differentiability, and the local convexity profile in sinusoidal functions to automatically propel (iii) network weights towards values quantized at levels that are jointly determined. We show how SINAREQ balance compute efficiency and accuracy, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR-10, MobileNet, ResNet-18, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy. Furthermore, we carry out experimentation using fixed homogenous bitwidths with 3- to 5-bit assignment and show the versatility of SINAREQ in enhancing quantized training algorithms (DoReFa and WRPN) with about 4.8% accuracy improvements on average, and then outperforming multiple state-of-the-art techniques.", "year": 2020, "ssId": "3261728694c0a53a2e8f95326f94147a28e03a83", "arXivId": "2003.00146", "link": "https://arxiv.org/pdf/2003.00146.pdf", "openAccess": true, "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "FatemehSadat Mireshghallah", "T. Elgindi", "Charles-Alban Deledalle", "H. Esmaeilzadeh"]}}
{"id": "5d07db93e6fbd9e10713a2f372131c777077062d", "content": {"title": "ReLeQ : A Reinforcement Learning Approach for Automatic Deep Quantization of Neural Networks", "abstract": "Deep Quantization (below eight bits) can significantly reduce the DNN computation and storage by decreasing the bitwidth of network encodings. However, without arduous manual effort, this deep quantization can lead to significant accuracy loss, leaving it in a position of questionable utility. We propose a systematic approach to tackle this problem, by automating the process of discovering the bitwidths through an end-to-end deep reinforcement learning framework (ReLeQ). This framework utilizes the sample efficiency of proximal policy optimization to explore the exponentially large space of possible assignment of the bitwidths to the layers. We show how ReLeQ can balance speed and quality, and provide a heterogeneous bitwidth assignment for quantization of a large variety of deep networks with minimal accuracy loss ($\\leq$ \u2264 0.3% loss) while minimizing the computation and storage costs. With these DNNs, ReLeQ enables conventional hardware and custom DNN accelerator to achieve $~2.2\\times$ 2 . 2 \u00d7 speedup over 8-bit execution.", "year": 2020, "ssId": "5d07db93e6fbd9e10713a2f372131c777077062d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "FatemehSadat Mireshghallah", "A. Yazdanbakhsh", "H. Esmaeilzadeh"]}}
{"id": "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc", "content": {"title": "A Principled Approach to Learning Stochastic Representations for Privacy in Deep Neural Inference", "abstract": "INFerence-as-a-Service (INFaaS) in the cloud has enabled the prevalent use of Deep Neural Networks (DNNs) in home automation, targeted advertising, machine vision, etc. The cloud receives the inference request as a raw input, containing a rich set of private information, that can be misused or leaked, possibly inadvertently. This prevalent setting can compromise the privacy of users during the inference phase. This paper sets out to provide a principled approach, dubbed Cloak, that finds optimal stochastic perturbations to obfuscate the private data before it is sent to the cloud. To this end, Cloak reduces the information content of the transmitted data while conserving the essential pieces that enable the request to be serviced accurately. The key idea is formulating the discovery of this stochasticity as an offline gradient-based optimization problem that reformulates a pre-trained DNN (with optimized known weights) as an analytical function of the stochastic perturbations. Using Laplace distribution as a parametric model for the stochastic perturbations, Cloak learns the optimal parameters using gradient descent and Monte Carlo sampling. This set of optimized Laplace distributions further guarantee that the injected stochasticity satisfies the -differential privacy criterion. Experimental evaluations with real-world datasets show that, on average, the injected stochasticity can reduce the information content in the input data by 80.07%, while incurring 7.12% accuracy loss.", "year": 2020, "ssId": "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc", "arXivId": "2003.12154", "link": "https://arxiv.org/pdf/2003.12154.pdf", "openAccess": true, "authors": ["FatemehSadat Mireshghallah", "Mohammadkazem Taram", "A. Jalali", "Ahmed T. Elthakeb", "D. Tullsen", "H. Esmaeilzadeh"]}}
{"id": "777d7b4141c9ce163de99b747e94c8d1db12e11e", "content": {"title": "Interpretable Privacy for Deep Learning Inference", "abstract": "In order to receive machine learning services from a cloud-based service provider, consumers usually send their entire raw data (e.g. an entire image). However, this models reveals much more information to the service provider than what is actually necessary for the execution of the service. This work shows that, in many cases, only a small portion of the input is required for the service provider to offer an accurate prediction. Discovering this subset is one of the main objectives of this paper. We formulate this problem as a gradient-based perturbation maximization method that discovers this subset in the input feature space with respect to the decision making of the prediction model used by the provider. After identifying the essential subset, our framework, Cloak, suppresses the rest of the features in the consumer\u2019s input and only sends the essential ones to the cloud. As such, the service provider can use those features to return an accurate prediction and also to improve its service, while at the same time the privacy of the consumer is better protected. We also demonstrate in our experiments that by removing the extra features, the post-hoc fairness of the classifier is improved as well.", "year": 2020, "ssId": "777d7b4141c9ce163de99b747e94c8d1db12e11e", "arXivId": null, "link": null, "openAccess": false, "authors": ["FatemehSadat Mireshghallah", "Mohammadkazem Taram", "A. Jalali", "Ahmed T. Elthakeb", "D. Tullsen", "H. Esmaeilzadeh"]}}
{"id": "80a085a79ac6cee94f21d21ab8ca302458c4e131", "content": {"title": "Shredder: Learning Noise to Protect Privacy with Partial DNN Inference on the Edge", "abstract": "A wide variety of DNN applications increasingly rely on the cloud to perform their huge computation. This heavy trend toward cloud-hosted inference services raises serious privacy concerns. This model requires the sending of private and privileged data over the network to remote servers, exposing it to the service provider. Even if the provider is trusted, the data can still be vulnerable over communication channels or via side-channel attacks [1,2] at the provider. To that end, this paper aims to reduce the information content of the communicated data without compromising the cloud service's ability to provide a DNN inference with acceptably high accuracy. This paper presents an end-to-end framework, called Shredder, that, without altering the topology or the weights of a pre-trained network, learns an additive noise distribution that significantly reduces the information content of communicated data while maintaining the inference accuracy. Shredder learns the additive noise by casting it as a tensor of trainable parameters enabling us to devise a loss functions that strikes a balance between accuracy and information degradation. The loss function exposes a knob for a disciplined and controlled asymmetric trade-off between privacy and accuracy. While keeping the DNN intact, Shredder enables inference on noisy data without the need to update the model or the cloud. Experimentation with real-world DNNs shows that Shredder reduces the mutual information between the input and the communicated data to the cloud by 70.2% compared to the original execution while only sacrificing 1.46% loss in accuracy.", "year": 2019, "ssId": "80a085a79ac6cee94f21d21ab8ca302458c4e131", "arXivId": "1905.11814", "link": "https://arxiv.org/pdf/1905.11814.pdf", "openAccess": true, "authors": ["FatemehSadat Mireshghallah", "Mohammadkazem Taram", "Prakash Ramrakhyani", "D. Tullsen", "H. Esmaeilzadeh"]}}
{"id": "efe9fe804f34b18524708b18293508191bda78eb", "content": {"title": "Energy-Efficient Permanent Fault Tolerance in Hard Real-Time Systems", "abstract": "Triple Modular Redundancy (TMR) is a historical and long-time\u2013used approach for masking various kinds of faults. By employing redundancy and analyzing the results of three separate executions of the same program, TMR is able to attain excellent levels of reliability. While TMR provides a desirable level of reliability, it suffers from the high power consumption of the redundant hardware, a severe detriment to its broad adoption. The energy consumption of TMR can be mitigated if its operations are divided into two stages, and one stage is dropped in the absence of fault. Such an approach, which is evaluated in recent research, however, quickly fails in the presence of permanent faults, as we show in this paper. In this work, we introduce Reactive TMR, a novel energy-efficient approach for tolerating both transient and permanent faults. The key idea is to detect and deactivate faulty components and re-assign their tasks to functioning ones. Using a combination of static scheduling and dynamic task-management, our method decouples tasks from cores that are susceptible to result in a faulty execution; hence, it instinctively tolerates permanent faults and improves both reliability and energy-efficiency. Through a detailed evaluation, we show that our proposal reduces the energy consumption of baseline TMR by 30 percent while preserving its reliability. As compared to the state-of-the-art proposal for TMR, our method, while maintaining the energy consumption, augments hard-fault\u2013tolerance to the system.", "year": 2019, "ssId": "efe9fe804f34b18524708b18293508191bda78eb", "arXivId": null, "link": null, "openAccess": false, "authors": ["FatemehSadat Mireshghallah", "Mohammad Bakhshalipour", "Mohammad Sadrosadati", "H. Sarbazi-Azad"]}}
{"id": "23918ed366c60ae0ef85b0c80def63127f035e02", "content": {"title": "Shredder: Learning Noise Distributions to Protect Inference Privacy", "abstract": "A wide variety of deep neural applications increasingly rely on the cloud to perform their compute-heavy inference. This common practice requires sending private and privileged data over the network to remote servers, exposing it to the service provider and potentially compromising its privacy. Even if the provider is trusted, the data can still be vulnerable over communication channels or via side-channel attacks in the cloud. To that end, this paper aims to reduce the information content of the communicated data with as little as possible compromise on the inference accuracy by making the sent data noisy. An undisciplined addition of noise can significantly reduce the accuracy of inference, rendering the service unusable. To address this challenge, this paper devises Shredder, an end-to-end framework, that, without altering the topology or the weights of a pre-trained network, learns additive noise distributions that significantly reduce the information content of communicated data while maintaining the inference accuracy. The key idea is finding the additive noise distributions by casting it as a disjoint offline learning process with a loss function that strikes a balance between accuracy and information degradation. The loss function also exposes a knob for a disciplined and controlled asymmetric trade-off between privacy and accuracy. While keeping the DNN intact, Shredder divides inference between the cloud and the edge device, striking a balance between computation and communication. In the separate phase of inference, the edge device takes samples from the Laplace distributions that were collected during the proposed offline learning phase and populates a noise tensor with these sampled elements. Then, the edge device merely adds this populated noise tensor to the intermediate results to be sent to the cloud. As such, Shredder enables accurate inference on noisy intermediate data without the need to update the model or the cloud, or any training process during inference. We also formally show that Shredder maximizes privacy with minimal impact on DNN accuracy while the tradeoff between privacy and accuracy is controlled through a mathematical knob. Experimentation with six real-world DNNs from text processing and image classification shows that Shredder reduces the mutual information between the input and the communicated data to the cloud by 74.70% compared to the original execution while only sacrificing 1.58% loss in accuracy. On average, Shredder also offers a speedup of 1.79x over Wi-Fi and 2.17x over LTE compared to cloud-only execution when using an off-the-shelf mobile GPU (Tegra X2) on the edge.", "year": 2019, "ssId": "23918ed366c60ae0ef85b0c80def63127f035e02", "arXivId": null, "link": null, "openAccess": false, "authors": ["FatemehSadat Mireshghallah", "Mohammadkazem Taram", "Prakash Ramrakhyani", "D. Tullsen", "H. Esmaeilzadeh"]}}
{"id": "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f", "content": {"title": "ReLeQ: An Automatic Reinforcement Learning Approach for Deep Quantization of Neural Networks", "abstract": "Deep Neural Networks (DNNs) typically require massive amount of computation resource in inference tasks for computer vision applications. Quantization can significantly reduce DNN computation and storage by decreasing the bitwidth of network encodings. Recent research affirms that carefully selecting the quantization levels for each layer can preserve the accuracy while pushing the bitwidth below eight bits. However, without arduous manual effort, this deep quantization can lead to significant accuracy loss, leaving it in a position of questionable utility. As such, deep quantization opens a large hyper-parameter space (bitwidth of the layers), the exploration of which is a major challenge. We propose a systematic approach to tackle this problem, by automating the process of discovering the quantization levels through an end-to-end deep reinforcement learning framework (ReLeQ). We adapt policy optimization methods to the problem of quantization, and focus on finding the best design decisions in choosing the state and action spaces, network architecture and training framework, as well as the tuning of various hyperparamters. We show how ReLeQ can balance speed and quality, and provide an asymmetric general solution for quantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet, MobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy (=< 0.3% loss) while minimizing the computation and storage cost. With these DNNs, ReLeQ enables conventional hardware to achieve 2.2x speedup over 8-bit execution. Similarly, a custom DNN accelerator achieves 2.0x speedup and energy reduction compared to 8-bit runs. These encouraging results mark ReLeQ as the initial step towards automating the deep quantization of neural networks.", "year": 2018, "ssId": "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "FatemehSadat Mireshghallah", "A. Yazdanbakhsh", "H. Esmaeilzadeh"]}}
{"id": "3f311aee9d25b0284d21274cfc8706d6f0277f87", "content": {"title": "ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural Networks", "abstract": "Despite numerous state-of-the-art applications of Deep Neural Networks (DNNs) in a wide range of real-world tasks, two major challenges hinder further advances in DNNs: hyperparameter optimization and lack of computing power. Recent efforts show that quantizing the weights and activations of DNN layers to lower bitwidths takes a significant step toward reducing memory bandwidth and power consumption by using limited computing resources. This paper builds upon the algorithmic insight that the bitwidth of operations in DNNs can be reduced without compromising their classification accuracy. While the use of eight-bit weights and activations during inference maintains the accuracy in most cases, lower bitwidths can achieve the same accuracy while utilizing less power. However, deep quantization (quantizing bitwidths below eight) while maintaining accuracy requires a great deal of trial-and-error, fine-tuning as well as re-training. By formulating quantization bitwidth as a hyperparameter in the optimization problem of selecting the bitwidth, we tackle this issue by leveraging a state-of-the-art policy gradient based Reinforcement Learning (RL) algorithm called Proximal Policy Optimization [10] (PPO), to efficiently explore a large design space of DNN quantization. The proposed technique also opens up the possibility of performing heterogeneous quantization of the network (e.g., quantizing each layer to different bitwidth) as the RL agent learns the sensitivity of each layer with respect to accuracy in order to perform quantization of the entire network. We evaluated our method on several neural networks including MNIST, CIFAR10, SVHN and the RL agent quantizes these networks to average bitwidths of 2.25, 5 and 4 respectively with less than 0.3% accuracy loss in all cases.", "year": 2018, "ssId": "3f311aee9d25b0284d21274cfc8706d6f0277f87", "arXivId": "1811.01704", "link": "https://arxiv.org/pdf/1811.01704.pdf", "openAccess": true, "authors": ["Ahmed T. Elthakeb", "Prannoy Pilligundla", "A. Yazdanbakhsh", "Sean Kinzer", "FatemehSadat Mireshghallah", "H. Esmaeilzadeh"]}}
