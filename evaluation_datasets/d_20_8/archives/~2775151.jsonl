{"id": "d1c41eb99824e8f4752190da1b815378be23b4b9", "content": {"title": "Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation", "abstract": "Despite strong performance in many sequenceto-sequence tasks, autoregressive models trained with maximum likelihood estimation suffer from exposure bias, i.e. a discrepancy between the ground-truth prefixes used during training and the model-generated prefixes used at inference time. Scheduled sampling is a simple and often empirically successful approach which addresses this issue by incorporating model-generated prefixes into the training process. However, it has been argued that it is an inconsistent training objective leading to models ignoring the prefixes altogether. In this paper, we conduct systematic experiments and find that it ameliorates exposure bias by increasing model reliance on the input sequence. We also observe that as a side-effect, it worsens performance when the model-generated prefix is correct, a form of catastrophic forgetting. We propose using Elastic Weight Consolidation as trade-off between mitigating exposure bias and retaining output quality. Experiments on two IWSLT\u201914 translation tasks demonstrate that our approach alleviates catastrophic forgetting and significantly improves BLEU compared to standard scheduled sampling.", "year": 2021, "ssId": "d1c41eb99824e8f4752190da1b815378be23b4b9", "arXivId": "2109.06308", "link": "https://arxiv.org/pdf/2109.06308.pdf", "openAccess": true, "authors": ["Michalis Korakakis", "Andreas Vlachos"]}}
{"id": "1afa3ab80abda57920b8d456a6513e6f01cc82e7", "content": {"title": "Survival text regression for time-to-event prediction in conversations", "abstract": "Time-to-event prediction tasks are common in conversation modelling, for applications such as predicting the length of a conversation or when a user will stop contributing to a platform. Despite the fact that it is natural to frame such predictions as regression tasks, recent work has modelled them as classification tasks, determining whether the time-to-event is greater than a pre-determined cut-off point. While this allows for the application of classification models which are well studied in NLP, it imposes a formulation that is contrived, as well as less informative. In this paper, we explore how to handle time-to-event forecasting in conversations as regression tasks. We focus on a family of regression techniques known as survival regression, which are commonly used in the context of healthcare and reliability engineering. We adapt these models to time-to-event prediction in conversations, using linguistic markers as features. On three datasets, we demonstrate that they outperform commonly considered text regression methods and comparable classification models.", "year": 2021, "ssId": "1afa3ab80abda57920b8d456a6513e6f01cc82e7", "arXivId": null, "link": null, "openAccess": false, "authors": ["Christine de Kock", "Andreas Vlachos"]}}
{"id": "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda", "content": {"title": "DeliData: A dataset for deliberation in multi-party problem solving", "abstract": "Dialogue systems research is traditionally focused on dialogues between two interlocutors, largely ignoring group conversations. Moreover, most previous research is focused either on task-oriented dialogue (e.g. restaurant bookings) or user engagement (chatbots), while research on systems for collaborative dialogues is an under-explored area. To this end, we introduce the first publicly available dataset containing collaborative conversations on solving a cognitive task, consisting of 500 group dialogues and 14k utterances. Furthermore, we propose a novel annotation schema that captures deliberation cues and release 50 dialogues annotated with it. Finally, we demonstrate the usefulness of the annotated data in training classifiers to predict the constructiveness of a conversation. The data collection platform, dataset and annotated corpus are publicly available at https://delibot.", "year": 2021, "ssId": "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda", "arXivId": "2108.05271", "link": "https://arxiv.org/pdf/2108.05271.pdf", "openAccess": true, "authors": ["Georgi Karadzhov", "Tom Stafford", "Andreas Vlachos"]}}
{"id": "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5", "content": {"title": "Generating Token-Level Explanations for Natural Language Inference", "abstract": "The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the entailment relation. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods.", "year": 2019, "ssId": "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5", "arXivId": "1904.10717", "link": "https://arxiv.org/pdf/1904.10717.pdf", "openAccess": true, "authors": ["James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal"]}}
{"id": "d558c6b953e0267781ed5da90a35c122ba360f10", "content": {"title": "Strong Baselines for Complex Word Identification across Multiple Languages", "abstract": "Complex Word Identification (CWI) is the task of identifying which words or phrases in a sentence are difficult to understand by a target audience. The latest CWI Shared Task released data for two settings: monolingual (i.e. train and test in the same language) and cross-lingual (i.e. test in a language not seen during training). The best monolingual models relied on language-dependent features, which do not generalise in the cross-lingual setting, while the best cross-lingual model used neural networks with multi-task learning. In this paper, we present monolingual and cross-lingual CWI models that perform as well as (or better than) most models submitted to the latest CWI Shared Task. We show that carefully selected features and simple learning models can achieve state-of-the-art performance, and result in strong baselines for future development in this area. Finally, we discuss how inconsistencies in the annotation of the data can explain some of the results obtained.", "year": 2019, "ssId": "d558c6b953e0267781ed5da90a35c122ba360f10", "arXivId": "1904.05953", "link": "https://arxiv.org/pdf/1904.05953.pdf", "openAccess": true, "authors": ["Pierre Finnimore", "Elisabeth Fritzsch", "Daniel King", "Alison Sneyd", "Aneeq-ur Rehman", "Fernando Alva-Manchego", "Andreas Vlachos"]}}
{"id": "84bc74d875e748aa0f11ac0c5e3000b16484b053", "content": {"title": "Adversarial attacks against Fact Extraction and VERification", "abstract": "This paper describes a baseline for the second iteration of the Fact Extraction and VERification shared task (FEVER2.0) which explores the resilience of systems through adversarial evaluation. We present a collection of simple adversarial attacks against systems that participated in the first FEVER shared task. FEVER modeled the assessment of truthfulness of written claims as a joint information retrieval and natural language inference task using evidence from Wikipedia. A large number of participants made use of deep neural networks in their submissions to the shared task. The extent as to whether such models understand language has been the subject of a number of recent investigations and discussion in literature. In this paper, we present a simple method of generating entailment-preserving and entailment-altering perturbations of instances by common patterns within the training data. We find that a number of systems are greatly affected with absolute losses in classification accuracy of up to $29\\%$ on the newly perturbed instances. Using these newly generated instances, we construct a sample submission for the FEVER2.0 shared task. Addressing these types of attacks will aid in building more robust fact-checking models, as well as suggest directions to expand the datasets.", "year": 2019, "ssId": "84bc74d875e748aa0f11ac0c5e3000b16484b053", "arXivId": "1903.05543", "link": "https://arxiv.org/pdf/1903.05543.pdf", "openAccess": true, "authors": ["James Thorne", "Andreas Vlachos"]}}
{"id": "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19", "content": {"title": "HighRES: Highlight-based Reference-less Evaluation of Summarization", "abstract": "There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.", "year": 2019, "ssId": "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19", "arXivId": "1906.01361", "link": "https://arxiv.org/pdf/1906.01361.pdf", "openAccess": true, "authors": ["Hardy Hardy", "Shashi Narayan", "Andreas Vlachos"]}}
{"id": "d5810f15cfdd59da549ffa648c5a05d806d94eb7", "content": {"title": "Automated Fact Checking in the News Room", "abstract": "Fact checking is an essential task in journalism; its importance has been highlighted due to recently increased concerns and efforts in combating misinformation. In this paper, we present an automated fact checking platform which given a claim, it retrieves relevant textual evidence from a document collection, predicts whether each piece of evidence supports or refutes the claim, and returns a final verdict. We describe the architecture of the system and the user interface, focusing on the choices made to improve its user friendliness and transparency. We conduct a user study of the fact-checking platform in a journalistic setting: we integrated it with a collection of news articles and provide an evaluation of the platform using feedback from journalists in their workflow. We found that the predictions of our platform were correct 58% of the time, and 59% of the returned evidence was relevant.", "year": 2019, "ssId": "d5810f15cfdd59da549ffa648c5a05d806d94eb7", "arXivId": "1904.02037", "link": "https://arxiv.org/pdf/1904.02037.pdf", "openAccess": true, "authors": ["Sebasti\u00e3o Miranda", "David Nogueira", "A. Mendes", "Andreas Vlachos", "Andrew Secker", "Rebecca Garrett", "Jeff Mitchell", "Zita Marinho"]}}
{"id": "41a47363d261459c594525ef330e5fccaa8518a0", "content": {"title": "Topic or Style? Exploring the Most Useful Features for Authorship Attribution", "abstract": "Approaches to authorship attribution, the task of identifying the author of a document, are based on analysis of individuals\u2019 writing style and/or preferred topics. Although the problem has been widely explored, no previous studies have analysed the relationship between dataset characteristics and effectiveness of different types of features. This study carries out an analysis of four widely used datasets to explore how different types of features affect authorship attribution accuracy under varying conditions. The results of the analysis are applied to authorship attribution models based on both discrete and continuous representations. We apply the conclusions from our analysis to an extension of an existing approach to authorship attribution and outperform the prior state-of-the-art on two out of the four datasets used.", "year": 2018, "ssId": "41a47363d261459c594525ef330e5fccaa8518a0", "arXivId": null, "link": null, "openAccess": false, "authors": ["Yunita Sari", "Mark Stevenson", "Andreas Vlachos"]}}
{"id": "b1d24e8e08435b7c52335485a0d635abf9bc604c", "content": {"title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification", "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.", "year": 2018, "ssId": "b1d24e8e08435b7c52335485a0d635abf9bc604c", "arXivId": "1803.05355", "link": "https://arxiv.org/pdf/1803.05355.pdf", "openAccess": true, "authors": ["James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal"]}}
{"id": "312b12dd6aa558b92df3ddd9b1057aa80a0ad718", "content": {"title": "Zero-shot Relation Classification as Textual Entailment", "abstract": "We consider the task of relation classification, and pose this task as one of textual entailment. We show that this formulation leads to several advantages, including the ability to (i) perform zero-shot relation classification by exploiting relation descriptions, (ii) utilize existing textual entailment models, and (iii) leverage readily available textual entailment datasets, to enhance the performance of relation classification systems. Our experiments show that the proposed approach achieves 20.16% and 61.32% in F1 zero-shot classification performance on two datasets, which further improved to 22.80% and 64.78% respectively with the use of conditional encoding.", "year": 2018, "ssId": "312b12dd6aa558b92df3ddd9b1057aa80a0ad718", "arXivId": null, "link": null, "openAccess": false, "authors": ["A. Obamuyide", "Andreas Vlachos"]}}
{"id": "682660c7a014e806b924fdf1a2a3d999a9ac13cf", "content": {"title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation", "abstract": "Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset.", "year": 2018, "ssId": "682660c7a014e806b924fdf1a2a3d999a9ac13cf", "arXivId": "1808.09160", "link": "https://arxiv.org/pdf/1808.09160.pdf", "openAccess": true, "authors": ["Hardy Hardy", "Andreas Vlachos"]}}
{"id": "1f0524971c20a06d745ab784689eb8833435fde1", "content": {"title": "The Fact Extraction and VERification (FEVER) Shared Task", "abstract": "We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be Supported or Refuted using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems.", "year": 2018, "ssId": "1f0524971c20a06d745ab784689eb8833435fde1", "arXivId": "1811.10971", "link": "https://arxiv.org/pdf/1811.10971.pdf", "openAccess": true, "authors": ["James Thorne", "Andreas Vlachos", "O. Cocarascu", "Christos Christodoulopoulos", "Arpit Mittal"]}}
{"id": "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd", "content": {"title": "Report on the 2nd International Workshop on Recent Trends in News Information Retrieval (NewsIR'18)", "abstract": "The news industry has undergone a revolution in the past decade, with substantial changes continuing to this day. News consumption habits are changing due to the increase in the volume of news and the variety of sources. Readers need new mechanisms to cope with this vast volume of information in order to not only find a signal in the noise, but also to understand what is happening in the world given the multiple points of view describing events. These challenges in journalism relate to Information Retrieval (IR) and Natural Language Processing (NLP) fields such as: verification of a source's reliability; the integration of news with other sources of information; real-time processing of both news content and social streams; de-duplication of stories; and entity detection and disambiguation. Although IR and NLP have been applied to news for decades, the changing nature of the space requires fresh approaches and a closer collaboration with our colleagues from the journalism environment. Following the success of the previous version of the workshop (NewsIR'16), the goal of this workshop, held in conjunction with ECIR 2018, is to continue to stimulate such discussion between the communities and to share interesting approaches to solve real user problems. A total number of 19 submissions were received and reviewed, of which 12 were accepted for presentation. In addition to that, we had over 30 registered participants in the workshop who were pleased to attend the two keynote talks given by well-known experts in the field - Edgar Meij (from industry) and Peter Tolmie (from academia) and oral and poster presentations from the accepted papers. The workshop also included a breakout session to discuss ideas for a future data challenge in news IR and closed with a focused panel discussion to reflect on the day. In summary, several ideas were presented in the workshop on solving complex information needs in the news domain. In addition, the workshop concluded with suggestions of important challenges and shared tasks to work on as a community for News IR.", "year": 2018, "ssId": "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd", "arXivId": null, "link": null, "openAccess": false, "authors": ["M. Albakour", "D. Corney", "J. Gonzalo", "Miguel Martinez-Alvarez", "B\u00e1rbara Poblete", "Andreas Vlachos"]}}
{"id": "22616702da06431668022c649a017af9b333c530", "content": {"title": "Automated Fact Checking: Task Formulations, Methods and Future Directions", "abstract": "The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.", "year": 2018, "ssId": "22616702da06431668022c649a017af9b333c530", "arXivId": "1806.07687", "link": "https://arxiv.org/pdf/1806.07687.pdf", "openAccess": true, "authors": ["James Thorne", "Andreas Vlachos"]}}
{"id": "317d95f99ef62237f6c7d7834d1d19027166b392", "content": {"title": "Sheffield at E 2 E : structured prediction approaches to end-to-end language generation", "abstract": "We describe the two systems, and their variations, that were submitted by the University of Sheffield to the E2E NLG challenge. Our systems consist of different approaches to structured prediction for end-to-end language generation. Our first submitted system employs imitation learning for structured prediction to explore the large search space without explicitly enumerating it. Our second submitted system uses encoder-decoder architectures to generate sequences of words. Our submitted runs for each system achieved BLEU scores of 0.60 and 0.54 respectively. On human evaluation our imitation learning model were placed in the 2nd best quality and 3rd best naturalness clusters according to Trueskill scores, while our encoder-decoder model was the best performing system on naturalness but on quality it was placed in the 5th best cluster.", "year": 2018, "ssId": "317d95f99ef62237f6c7d7834d1d19027166b392", "arXivId": null, "link": null, "openAccess": false, "authors": ["Mingjie Chen", "Gerasimos Lampouras", "Andreas Vlachos"]}}
{"id": "efaf07d40b9c5837639bed129794efc00f02e4c3", "content": {"title": "Continuous N-gram Representations for Authorship Attribution", "abstract": "This paper presents work on using continuous representations for authorship attribution. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for n-gram features via a neural network jointly with the classification layer. Experimental results demonstrate that the proposed model outperforms the state-of-the-art on two datasets, while producing comparable results on the remaining two.", "year": 2017, "ssId": "efaf07d40b9c5837639bed129794efc00f02e4c3", "arXivId": null, "link": null, "openAccess": false, "authors": ["Mark Stevenson", "Andreas Vlachos", "Yunita Sari"]}}
{"id": "5665d864d0f1bce6672d6d2bf9f8d8646093cb37", "content": {"title": "An Extensible Framework for Verification of Numerical Claims", "abstract": "In this paper we present our automated fact checking system demonstration which we developed in order to participate in the Fast and Furious Fact Check challenge. We focused on simple numerical claims such as \u201cpopulation of Germany in 2015 was 80 million\u201d which comprised a quarter of the test instances in the challenge, achieving 68% accuracy. Our system extends previous work on semantic parsing and claim identification to handle temporal expressions and knowledge bases consisting of multiple tables, while relying solely on automatically generated training data. We demonstrate the extensible nature of our system by evaluating it on relations used in previous work. We make our system publicly available so that it can be used and extended by the community.", "year": 2017, "ssId": "5665d864d0f1bce6672d6d2bf9f8d8646093cb37", "arXivId": null, "link": null, "openAccess": false, "authors": ["James Thorne", "Andreas Vlachos"]}}
{"id": "43fe2d8781473360eeaae7a3284169a303200846", "content": {"title": "Fake news stance detection using stacked ensemble of classifiers", "abstract": "Fake news has become a hotly debated topic in journalism. In this paper, we present our entry to the 2017 Fake News Challenge which models the detection of fake news as a stance classification task that finished in 11th place on the leader board. Our entry is an ensemble system of classifiers developed by students in the context of their coursework. We show how we used the stacking ensemble method for this purpose and obtained improvements in classification accuracy exceeding each of the individual models\u2019 performance on the development data. Finally, we discuss aspects of the experimental setup of the challenge.", "year": 2017, "ssId": "43fe2d8781473360eeaae7a3284169a303200846", "arXivId": null, "link": null, "openAccess": false, "authors": ["James Thorne", "Mingjie Chen", "Giorgos Myrianthous", "Jiashu Pu", "Xiaoxuan Wang", "Andreas Vlachos"]}}
{"id": "281605579936538ee92bc4b0baad1b83c683c076", "content": {"title": "Sheffield at SemEval-2017 Task 9: Transition-based language generation from AMR", "abstract": "This paper describes the submission by the University of Sheffield to the SemEval 2017 Abstract Meaning Representation Parsing and Generation task (SemEval 2017 Task 9, Subtask 2). We cast language generation from AMR as a sequence of actions (e.g., insert/remove/rename edges and nodes) that progressively transform the AMR graph into a dependency parse tree. This transition-based approach relies on the fact that an AMR graph can be considered structurally similar to a dependency tree, with a focus on content rather than function words. An added benefit to this approach is the greater amount of data we can take advantage of to train the parse-to-text linearizer. Our submitted run on the test data achieved a BLEU score of 3.32 and a Trueskill score of -22.04 on automatic and human evaluation respectively.", "year": 2017, "ssId": "281605579936538ee92bc4b0baad1b83c683c076", "arXivId": null, "link": null, "openAccess": false, "authors": ["Gerasimos Lampouras", "Andreas Vlachos"]}}
