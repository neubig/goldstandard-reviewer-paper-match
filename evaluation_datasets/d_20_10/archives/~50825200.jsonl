{"id": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "content": {"title": "To ArXiv or not to ArXiv: A Study Quantifying Pros and Cons of Posting Preprints Online", "abstract": "Double-blind conferences have engaged in debates over whether to allow authors to post their papers online on arXiv or elsewhere during the review process. Independently, some authors of research papers face the dilemma of whether to put their papers on arXiv due to its pros and cons. We conduct a study to substantiate this debate and dilemma via quantitative measurements. Speci\ufb01cally, we conducted surveys of reviewers in two top-tier double-blind computer science conferences\u2014ICML 2021 (5361 submissions and 4699 reviewers) and EC 2021 (498 submissions and 190 reviewers). Our two main \ufb01ndings are as follows. First, more than a third of the reviewers self-report searching online for a paper they are assigned to review. Second, outside the review process, we \ufb01nd that preprints from better-ranked a\ufb03liations see a weakly higher visibility, with a correlation of 0.06 in ICML and 0.05 in EC. In particular, papers associated with the top-10-ranked a\ufb03liations had a visibility of approximately 11% in ICML and 22% in EC, whereas the remaining papers had a visibility of 7% and 18% respectively.", "year": 2022, "ssId": "fd0aa185be4e1f1fe3975779aec179348ec19ea8", "arXivId": "2203.17259", "link": "https://arxiv.org/pdf/2203.17259.pdf", "openAccess": true, "authors": ["Charvi Rastogi", "Ivan Stelmakh", "Xinwei Shen", "M. Meil\u0103", "F. Echenique", "Shuchi Chawla", "Nihar B. Shah"]}}
{"id": "203636315f7c9526189d88c541bedf623d63ea7c", "content": {"title": "ASQA: Factoid Questions Meet Long-Form Answers", "abstract": "An abundance of datasets and availability of reliable evaluation metrics have resulted in strong progress in factoid question answering (QA). This progress, however, does not easily transfer to the task of long-form QA , where the goal is to answer questions that require in-depth explanations. The hurdles include (i) a lack of high-quality data, and (ii) the absence of a well-de\ufb01ned notion of the answer\u2019s quality. In this work, we address these problems by (i) releasing a novel dataset and a task that we call ASQA (Answer Summaries for Questions which are Ambiguous); and (ii) proposing a reliable metric for measuring performance on ASQA. Our task focuses on factoid questions that are ambiguous, that is, have different correct answers depending on interpretation. Answers to ambiguous questions should synthesize factual information from multiple sources into a long-form summary that resolves the ambiguity. In contrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear notion of correctness: a user faced with a good summary should be able to answer different interpretations of the original ambiguous question. We use this notion of correctness to de\ufb01ne an automated metric of performance for ASQA. Our analysis demonstrates an agreement between this metric and human judgments, and reveals a considerable gap between human performance and strong baselines.", "year": 2022, "ssId": "203636315f7c9526189d88c541bedf623d63ea7c", "arXivId": "2204.06092", "link": "https://arxiv.org/pdf/2204.06092.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Yi Luan", "Bhuwan Dhingra", "Ming-Wei Chang"]}}
{"id": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "content": {"title": "Cite-seeing and Reviewing: A Study on Citation Bias in Peer Review", "abstract": "Citations play an important role in researchers\u2019 careers as a key factor in evaluation of scienti\ufb01c impact. Many anecdotes advice authors to exploit this fact and cite prospective reviewers to try obtaining a more positive evaluation for their submission. In this work, we investigate if such a citation bias actually exists: Does the citation of a reviewer\u2019s own work in a submission cause them to be positively biased towards the submission? In conjunction with the review process of two \ufb02agship conferences in machine learning and algorithmic economics, we execute an observational study to test for citation bias in peer review. In our analysis, we carefully account for various confounding factors such as paper quality and reviewer expertise, and apply di\ufb00erent modeling techniques to alleviate concerns regarding the model mismatch. Overall, our analysis involves 1,314 papers and 1,717 reviewers and detects citation bias in both venues we consider. In terms of the e\ufb00ect size, by citing a reviewer\u2019s work, a submission has a non-trivial chance of getting a higher score from the reviewer: an expected increase in the score is approximately 0 . 23 on a 5-point Likert item. For reference, a one-point increase of a score by a single reviewer improves the position of a submission by 11% on average.", "year": 2022, "ssId": "3dc4580a154df87f3a56aa3d16b00c5a935ebe15", "arXivId": "2203.17239", "link": "https://arxiv.org/pdf/2203.17239.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Ryan Liu", "Shuchi Chawla", "F. Echenique", "Nihar B. Shah"]}}
{"id": "1ccf412212873ae1b020762b8b86291e1fb11f65", "content": {"title": "CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription", "abstract": "Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CROWDSPEECH \u2014 the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VOXDIY \u2014 a counterpart of CROWDSPEECH for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.1", "year": 2021, "ssId": "1ccf412212873ae1b020762b8b86291e1fb11f65", "arXivId": "2107.01091", "link": "https://arxiv.org/pdf/2107.01091.pdf", "openAccess": true, "authors": ["Nikita Pavlichenko", "Ivan Stelmakh", "Dmitry Ustalov"]}}
{"id": "e318e554098224c9475dfc80765cbbb82fa4a409", "content": {"title": "Towards Fair, Equitable, and Efficient Peer Review", "abstract": "Peer review is the backbone of academia. The rapid growth of the number of submissions to leading publication venues has identified a need for automation of some parts of the peerreview pipeline and nowadays human referees are required to interact with various interfaces and technologies in this process. However, there exists evidence that if such interactions are not carefully designed, they can exacerbate various problems related to fairness and efficiency of the process. In my research, I aim to design a Human-AI collaboration pipeline in peer review to mitigate these issues and ensure that science progresses in a fair, equitable, and efficient manner. Despite peer review being the primary mechanism of science dissemination for decades, the rapid growth of the number of submissions to leading AI and ML conferences has challenged its sustainability in two ways: \u2022 It has brought up a call for automated tools to assist human decision-makers. \u2022 It has amplified the shortcomings of the peer-review procedure, making them more visible to the community and stressing the importance of research on peer review. These issues motivate my thesis research and I am passionate about working at the intersection of machine learning, operations research, social choice theory, and humancomputer interaction, to understand and develop a principled approach towards scientific peer review. Specifically, I believe that a carefully designed Human-AI collaboration is crucial for sustainability of peer review and in my work I aim at designing tools to support this collaboration. My research touches both algorithmic and human sides of the Human-AI collaboration and in the sequel I first describe my projects on supporting each of these sides. I then outline a direction for future work on bringing these sides to a closer interaction with a goal of improving the peer-review process. On a higher level, my work comprises novel theoretical and empirical contributions: I aim to design practical algorithms that are supported by strong theoretical guarantees and are evaluated in a carefully designed real-world experiments. The preliminary results I discuss below have already had a considerable impact in practice with some tools deployed in ICML 2020, and this inspires me to continue my work towards fair, equitable and efficient peer review. Copyright c \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Algorithmic Side. Past research in social science indicates that unfairness of the peer-review process may have farreaching consequences both on a development of research areas and on career trajectories of individual researchers. Therefore, my work on the algorithmic side is twofold: first, I aim to ensure that the algorithms used to automate peer review are themselves fair. Second, I aim at designing algorithms that help conference organizers to promote fairness. Fairness for Algorithms: The most automated part of the review process is the assignment of submissions to referees and most of the of the top AI and ML conferences rely on a simple and efficient matching algorithm developed by Charlin and Zemel (2013). Simultaneously, assignment is of the utmost importance: one cannot expect good reviews for papers that are assigned to unsuitable reviewers. In our past work (Stelmakh, Shah, and Singh 2018) we demonstrate that the state-of-the-art algorithm used by NeurIPS and ICML does not necessarily lead to a fair assignment, discriminating against some papers. More importantly, we design a novel assignment algorithm with provable guarantees on the fairness of the assignment that ensures that no paper is discriminated against to improve the assignment of more lucky counterparts. In addition to strong fairness guarantees, our algorithm is also optimal in terms of the accuracy of final decisions under a popular statistical model, that is, our algorithm theoretically outperforms the state-of-the-art algorithm both in terms of fairness and statistical accuracy. These guarantees are corroborated by an extensive empirical evaluation: in particular, our algorithm was tested and eventually deployed in the assignment of the ICML 2020 conference, improving the fairness by 15-30% while not trading off the conventional measure of the assignment quality. Algorithms for Fairness: While we can prove that algorithms employed to automate peer review satisfy the requirement of fairness, ensuring fairness of decisions made by humans is a more challenging task. An important direction that I am interested in is a use of algorithms to perform statistical testing for fairness and impartiality of final decisions. In our work (Stelmakh, Shah, and Singh 2019), we made the progress on this problem by contributing to the long-standing debate on the fairness of the decisions in single-blind peer review. In that, we design a novel semirandomized experimental procedure that allows to test for The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)", "year": 2021, "ssId": "e318e554098224c9475dfc80765cbbb82fa4a409", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ivan Stelmakh"]}}
{"id": "7488429131b8970425a66f3410920d98ff6e9c36", "content": {"title": "Debiasing Evaluations That are Biased by Evaluations", "abstract": "It is common to evaluate a set of items by soliciting people to rate them. For example, universities ask students to rate the teaching quality of their instructors, and conference organizers ask authors of submissions to evaluate the quality of the reviews. However, in these applications, students often give a higher rating to a course if they receive higher grades in a course, and authors often give a higher rating to the reviews if their papers are accepted to the conference. In this work, we call these external factors the \"outcome\" experienced by people, and consider the problem of mitigating these outcome-induced biases in the given ratings when some information about the outcome is available. We formulate the information about the outcome as a known partial ordering on the bias. We propose a debiasing method by solving a regularized optimization problem under this ordering constraint, and also provide a carefully designed cross-validation method that adaptively chooses the appropriate amount of regularization. We provide theoretical guarantees on the performance of our algorithm, as well as experimental evaluations.", "year": 2020, "ssId": "7488429131b8970425a66f3410920d98ff6e9c36", "arXivId": "2012.00714", "link": "https://arxiv.org/pdf/2012.00714.pdf", "openAccess": true, "authors": ["Jingyan Wang", "Ivan Stelmakh", "Yuting Wei"]}}
{"id": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "content": {"title": "A Large Scale Randomized Controlled Trial on Herding in Peer-Review Discussions", "abstract": "Peer review is the backbone of academia and humans constitute a cornerstone of this process, being responsible for reviewing papers and making the final acceptance/rejection decisions. Given that human decision making is known to be susceptible to various cognitive biases, it is important to understand which (if any) biases are present in the peer-review process and design the pipeline such that the impact of these biases is minimized. In this work, we focus on the dynamics of between-reviewers discussions and investigate the presence of herding behaviour therein. In that, we aim to understand whether reviewers and more senior decision makers get disproportionately influenced by the first argument presented in the discussion when (in case of reviewers) they form an independent opinion about the paper before discussing it with others. Specifically, in conjunction with the review process of ICML 2020 -- a large, top tier machine learning conference -- we design and execute a randomized controlled trial with the goal of testing for the conditional causal effect of the discussion initiator's opinion on the outcome of a paper.", "year": 2020, "ssId": "076b2ba158c35bd2941769864ce7455cf76ecd8e", "arXivId": "2011.15083", "link": "https://arxiv.org/pdf/2011.15083.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Charvi Rastogi", "Nihar B. Shah", "Aarti Singh", "Hal Daum'e"]}}
{"id": "2b3ab7e9c66bffc7af9e4413036e7bba686a7734", "content": {"title": "Prior and Prejudice: The Novice Reviewers' Bias against Resubmissions in Conference Peer Review", "abstract": "Modern machine learning and computer science conferences are experiencing a surge in the number of submissions that challenges the quality of peer review as the number of competent reviewers is growing at a much slower rate. To curb this trend and reduce the burden on reviewers, several conferences have started encouraging or even requiring authors to declare the previous submission history of their papers. Such initiatives have been met with skepticism among authors, who raise the concern about a potential bias in reviewers' recommendations induced by this information. In this work, we investigate whether reviewers exhibit a bias caused by the knowledge that the submission under review was previously rejected at a similar venue, focusing on a population of novice reviewers who constitute a large fraction of the reviewer pool in leading machine learning and computer science conferences. We design and conduct a randomized controlled trial closely replicating the relevant components of the peer-review pipeline with $133$ reviewers (master's, junior PhD students, and recent graduates of top US universities) writing reviews for $19$ papers. The analysis reveals that reviewers indeed become negatively biased when they receive a signal about paper being a resubmission, giving almost 1 point lower overall score on a 10-point Likert item ($\\Delta = -0.78, \\ 95\\% \\ \\text{CI} = [-1.30, -0.24]$) than reviewers who do not receive such a signal. Looking at specific criteria scores (originality, quality, clarity and significance), we observe that novice reviewers tend to underrate quality the most.", "year": 2020, "ssId": "2b3ab7e9c66bffc7af9e4413036e7bba686a7734", "arXivId": "2011.14646", "link": "https://arxiv.org/pdf/2011.14646.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Nihar B. Shah", "Aarti Singh", "Hal Daum'e"]}}
{"id": "f136a0fdc2065485c83396ae41d431395de51af4", "content": {"title": "A Novice-Reviewer Experiment to Address Scarcity of Qualified Reviewers in Large Conferences", "abstract": "Conference peer review constitutes a human-computation process whose importance cannot be overstated: not only it identifies the best submissions for acceptance, but, ultimately, it impacts the future of the whole research area by promoting some ideas and restraining others. A surge in the number of submissions received by leading AI conferences has challenged the sustainability of the review process by increasing the burden on the pool of qualified reviewers which is growing at a much slower rate. In this work, we consider the problem of reviewer recruiting with a focus on the scarcity of qualified reviewers in large conferences. Specifically, we design a procedure for (i) recruiting reviewers from the population not typically covered by major conferences and (ii) guiding them through the reviewing pipeline. In conjunction with the ICML 2020 \u2014 a large, top-tier machine learning conference \u2014 we recruit a small set of reviewers through our procedure and compare their performance with the general population of ICML reviewers. Our experiment reveals that a combination of the recruiting and guiding mechanisms allows for a principled enhancement of the reviewer pool and results in reviews of superior quality compared to the conventional pool of reviews as evaluated by senior members of the program committee (meta-reviewers).", "year": 2020, "ssId": "f136a0fdc2065485c83396ae41d431395de51af4", "arXivId": "2011.15050", "link": "https://arxiv.org/pdf/2011.15050.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Nihar B. Shah", "Aarti Singh", "Hal Daum'e"]}}
{"id": "b0894f5c914cd90cc3b3e16b15bec11efe317b14", "content": {"title": "Catch Me if I Can: Detecting Strategic Behaviour in Peer Assessment", "abstract": "We consider the issue of strategic behaviour in various peer-assessment tasks, including peer grading of exams or homeworks and peer review in hiring or promotions. When a peer-assessment task is competitive (e.g., when students are graded on a curve), agents may be incentivized to misreport evaluations in order to improve their own final standing. Our focus is on designing methods for detection of such manipulations. Specifically, we consider a setting in which agents evaluate a subset of their peers and output rankings that are later aggregated to form a final ordering. In this paper, we investigate a statistical framework for this problem and design a principled test for detecting strategic behaviour. We prove that our test has strong false alarm guarantees and evaluate its detection ability in practical settings. For this, we design and execute an experiment that elicits strategic behaviour from subjects and release a dataset of patterns of strategic behaviour that may be of independent interest. We then use the collected data to conduct a series of real and semi-synthetic evaluations that demonstrate a strong detection power of our test.", "year": 2020, "ssId": "b0894f5c914cd90cc3b3e16b15bec11efe317b14", "arXivId": "2010.04041", "link": "https://arxiv.org/pdf/2010.04041.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "004ddf5a39a735d0f8ec7547629c2bee65eb1f93", "content": {"title": "On Testing for Biases in Peer Review", "abstract": "We consider the issue of biases in scholarly research, specifically, in peer review. There is a long standing debate on whether exposing author identities to reviewers induces biases against certain groups, and our focus is on designing tests to detect the presence of such biases. Our starting point is a remarkable recent work by Tomkins, Zhang and Heavlin which conducted a controlled, large-scale experiment to investigate existence of biases in the peer reviewing of the WSDM conference. We present two sets of results in this paper. The first set of results is negative, and pertains to the statistical tests and the experimental setup used in the work of Tomkins et al. We show that the test employed therein does not guarantee control over false alarm probability and under correlations between relevant variables, coupled with any of the following conditions, with high probability can declare a presence of bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c) reviewer calibration. Moreover, we show that the setup of their experiment may itself inflate false alarm probability if (d) bidding is performed in non-blind manner or (e) popular reviewer assignment procedure is employed. Our second set of results is positive, in that we present a general framework for testing for biases in (single vs. double blind) peer review. We then present a hypothesis test with guaranteed control over false alarm probability and non-trivial power even under conditions (a)--(c). Conditions (d) and (e) are more fundamental problems that are tied to the experimental setup and not necessarily related to the test.", "year": 2019, "ssId": "004ddf5a39a735d0f8ec7547629c2bee65eb1f93", "arXivId": "1912.13188", "link": "https://arxiv.org/pdf/1912.13188.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "f32c67daa6a93281bd8645fc2fa423dca67aea00", "content": {"title": "PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review", "abstract": "We consider the problem of automated assignment of papers to reviewers in conference peer review, with a focus on fairness and statistical accuracy. Our fairness objective is to maximize the review quality of the most disadvantaged paper, in contrast to the commonly used objective of maximizing the total quality over all papers. We design an assignment algorithm based on an incremental max-flow procedure that we prove is near-optimally fair. Our statistical accuracy objective is to ensure correct recovery of the papers that should be accepted. We provide a sharp minimax analysis of the accuracy of the peer-review process for a popular objective-score model as well as for a novel subjective-score model that we propose in the paper. Our analysis proves that our proposed assignment algorithm also leads to a near-optimal statistical accuracy. Finally, we design a novel experiment that allows for an objective comparison of various assignment algorithms, and overcomes the inherent difficulty posed by the absence of a ground truth in experiments on peer-review. The results of this experiment as well as of other experiments on synthetic and real data corroborate the theoretical guarantees of our algorithm.", "year": 2018, "ssId": "f32c67daa6a93281bd8645fc2fa423dca67aea00", "arXivId": "1806.06237", "link": "https://arxiv.org/pdf/1806.06237.pdf", "openAccess": true, "authors": ["Ivan Stelmakh", "Nihar B. Shah", "Aarti Singh"]}}
{"id": "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6", "content": {"title": "Adaptive Algorithm of Tracking the Best Experts Trajectory", "abstract": "The problem of decision theoretic online learning is discussed. There is the set of methods, experts, and algorithms capable of making solutions (or predictions) and suffering losses due to the inaccuracy of their solutions. An adaptive algorithm whereby expert solutions are aggregated and sustained losses not exceeding (to a certain quantity called a regret) those of the best combination of experts distributed over the prediction interval is proposed. The algorithm is constructed using the Fixed-Share method combined with the Ada-Hedge algorithm used to exponentially weight expert solutions. The regret of the proposed algorithm is estimated. In the context of the given approach, there are no any stochastic assumptions about an initial data source and the boundedness of losses. The results of numerical experiments concerning the mixing of expert solutions with the help of the proposed algorithm are presented. The strategies of games on financial markets, which were suggested in our previous papers, play the role of expert strategies.", "year": 2017, "ssId": "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6", "arXivId": null, "link": null, "openAccess": false, "authors": ["V. V'yugin", "Ivan Stelmakh", "V. Trunov"]}}
{"id": "2bbb33ab8124e5078ec39e821a25c24c20a31b9b", "content": {"title": "Preface", "abstract": "The second workshop on Crowd Science is organized in conjunction with the 47th International Conference on Very Large Data Bases (VLDB 2021). This workshop is the second in a series of events that has the goal of helping crowdsourcing \u201ctransition\u201d from art to science, and tackles the research challenges that we face to make crowdsourcing a technology that users can easily access and leverage, and produces results that researchers and businesses can rely on. In addition to regular paper submissions, this year we have organized a crowdsourced audio transcription shared task that attracted 18 participants around the world. Eight submissions have beaten a strong baseline method, thus advancing the state-of-the-art in this challenging task. This workshop features three invited talks, seven paper presentations, an overview of this shared task, as well as a panel discussion. We received 11 submissions, out of which 7 were accepted as the talks at the workshop after peer review. Besides the accepted papers, the volume contains a shared task overview by its organizers. We thank all the authors for their contributions and the effort they put into it, and we are very grateful for the excellent work of our reviewers and program committee members. Last but not least, we would like to thank VLDB organizers for their assistance that made the workshop organization process a pleasure. The 2021 edition of the workshop took place in a hybrid format, and we look forward to the forthcoming (in-person) editions of the Crowd Science workshop.", "year": 2016, "ssId": "2bbb33ab8124e5078ec39e821a25c24c20a31b9b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dmitry Ustalov", "Fabio Casati", "A. Drutsa", "Ivan Stelmakh", "Nikita Pavlichenko", "Daria Baidakova"]}}
