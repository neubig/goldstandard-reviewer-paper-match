{"id": "796f29cee975603c7a1469df1eb21ed5142ecff5", "content": {"title": "RELIC: Retrieving Evidence for Literary Claims", "abstract": "Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work. Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching. We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines; however, experiments and analysis by human domain experts indicate that there is substantial room for improvement over our dense retriever.", "year": 2022, "ssId": "796f29cee975603c7a1469df1eb21ed5142ecff5", "arXivId": "2203.10053", "link": "https://arxiv.org/pdf/2203.10053.pdf", "openAccess": true, "authors": ["Katherine Thai", "Ya-yin Chang", "Kalpesh Krishna", "Mohit Iyyer"]}}
{"id": "b9a701c90f3d3df27366f5b29a97f798eb940ac7", "content": {"title": "ChapterBreak: A Challenge Dataset for Long-Range Language Models", "abstract": "While numerous architectures for long-range language models (LRLMs) have recently been proposed, a meaningful evaluation of their discourse-level language understanding capabilities has not yet followed. To this end, we introduce CHAPTERBREAK, a challenge dataset that provides an LRLM with a long segment from a narrative that ends at a chapter boundary and asks it to distinguish the beginning of the ground-truth next chapter from a set of negative segments sampled from the same narrative. A fine-grained human annotation reveals that our dataset contains many complex types of chapter transitions (e.g., parallel narratives, cliffhanger endings) that require processing global context to comprehend. Experiments on CHAPTERBREAK show that existing LRLMs fail to effectively leverage long-range context, substantially underperforming a segment-level model trained directly for this task. We publicly release our CHAPTERBREAK dataset to spur more principled future research into LRLMs.1", "year": 2022, "ssId": "b9a701c90f3d3df27366f5b29a97f798eb940ac7", "arXivId": "2204.10878", "link": "https://arxiv.org/pdf/2204.10878.pdf", "openAccess": true, "authors": ["Simeng Sun", "Katherine Thai", "Mohit Iyyer"]}}
{"id": "e1d35deec12d18e53ca97a3cf4071526ad47968d", "content": {"title": "How Much Do Modifications to Transformer Language Models Affect Their Ability to Learn Linguistic Knowledge?", "abstract": "Recent progress in large pretrained language models (LMs) has led to a growth of analyses examining what kinds of linguistic knowledge are encoded by these models. Due to computational constraints, existing analyses are mostly conducted on publicly-released LM checkpoints, which makes it difficult to study how various factors during training affect the models\u2019 acquisition of linguistic knowledge. In this paper, we train a suite of small-scale Transformer LMs that differ from each other with respect to architectural decisions (e.g., self-attention configuration) or training objectives (e.g., multi-tasking, focal loss). We evaluate these LMs on BLiMP, a targeted evaluation benchmark of multiple English linguistic phenomena. Our experiments show that while none of these modifications yields significant improvements on aggregate, changes to the loss function result in promising improvements on several subcategories (e.g., detecting adjunct islands, correctly scoping negative polarity items). We hope our work offers useful insights for future research into designing Transformer LMs that more effectively learn linguistic knowledge.", "year": 2022, "ssId": "e1d35deec12d18e53ca97a3cf4071526ad47968d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Simeng Sun", "Brian Dillon", "Mohit Iyyer"]}}
{"id": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "content": {"title": "Do Long-Range Language Models Actually Use Long-Range Context?", "abstract": "Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).", "year": 2021, "ssId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "arXivId": "2109.09115", "link": "https://arxiv.org/pdf/2109.09115.pdf", "openAccess": true, "authors": ["Simeng Sun", "Kalpesh Krishna", "Andrew Mattarella-Micke", "Mohit Iyyer"]}}
{"id": "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733", "content": {"title": "Predicting in-hospital mortality by combining clinical notes with time-series data", "abstract": "In intensive care units (ICUs), patient health is monitored through (1) continuous vital signals from various medical devices, and (2) clinical notes consisting of opinions and summaries from doctors which are recorded in electronic health records (EHR). It is difficult to jointly model these two sources of information because clinical notes, unlike vital signals, are collected at irregular intervals and their contents are relatively unstructured. In this paper, we present a model that combines both sources of information about ICU patients to make accurate in-hospital mortality predictions. We apply a fine-tuned BERT model to each of the patient\u2019s clinical notes. The resulting embeddings are then combined to obtain the overall embedding for the entire text part of the data. This is then combined with the output of an LSTM model that encodes patients\u2019 vital signals. Our model improves upon the state of the art for mortality prediction, attaining an AUC score of 0.9, compared to the previous 0.87, setting a new standard for mortality prediction on the MIMIC III benchmark.1", "year": 2021, "ssId": "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733", "arXivId": null, "link": null, "openAccess": false, "authors": ["I. Deznabi", "Mohit Iyyer", "M. Fiterau"]}}
{"id": "d706645fbbc6edfad5fb642b1dfc3019fcabbd99", "content": {"title": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation", "abstract": "Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.", "year": 2021, "ssId": "d706645fbbc6edfad5fb642b1dfc3019fcabbd99", "arXivId": "2109.06835", "link": "https://arxiv.org/pdf/2109.06835.pdf", "openAccess": true, "authors": ["Marzena Karpinska", "Nader Akoury", "Mohit Iyyer"]}}
{"id": "b593be8ff3c09c6994657678fcde0c5adf43328e", "content": {"title": "Improved Latent Tree Induction with Distant Supervision via Span Constraints", "abstract": "For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of span constraints (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from Wikipedia, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset.", "year": 2021, "ssId": "b593be8ff3c09c6994657678fcde0c5adf43328e", "arXivId": "2109.05112", "link": "https://arxiv.org/pdf/2109.05112.pdf", "openAccess": true, "authors": ["Zhiyang Xu", "Andrew Drozdov", "Jay Yoon Lee", "Timothy J. O'Gorman", "Subendhu Rongali", "Dylan Finkbeiner", "S. Suresh", "Mohit Iyyer", "A. McCallum"]}}
{"id": "7354b87a1b4c99ccd9cf25b7314927ced8b156f7", "content": {"title": "IGA: An Intent-Guided Authoring Assistant", "abstract": "While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confirm the quality of IGA\u2019s generated outputs, while a small-scale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing.", "year": 2021, "ssId": "7354b87a1b4c99ccd9cf25b7314927ced8b156f7", "arXivId": "2104.07000", "link": "https://arxiv.org/pdf/2104.07000.pdf", "openAccess": true, "authors": ["Simeng Sun", "Wenlong Zhao", "Varun Manjunatha", "R. Jain", "Vlad I. Morariu", "Franck Dernoncourt", "Balaji Vasan Srinivasan", "Mohit Iyyer"]}}
{"id": "3122a2d7799ba585b993e432b3deb47659b3f3c1", "content": {"title": "Hurdles to Progress in Long-form Question Answering", "abstract": "The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system\u2019s generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.", "year": 2021, "ssId": "3122a2d7799ba585b993e432b3deb47659b3f3c1", "arXivId": "2103.06332", "link": "https://arxiv.org/pdf/2103.06332.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Aurko Roy", "Mohit Iyyer"]}}
{"id": "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3", "content": {"title": "STraTA: Self-Training with Task Augmentation for Better Few-shot Learning", "abstract": "Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeled texts. Second, STraTA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STraTA can substantially improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the SST-2 sentiment dataset, STraTA, with only 8 training examples per class, achieves comparable results to standard fine-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective.", "year": 2021, "ssId": "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3", "arXivId": "2109.06270", "link": "https://arxiv.org/pdf/2109.06270.pdf", "openAccess": true, "authors": ["Tu Vu", "Minh-Thang Luong", "Quoc V. Le", "Grady Simon", "Mohit Iyyer"]}}
{"id": "981dbdf6f87f13f3f3047a925c519fc39a35202b", "content": {"title": "Revisiting Simple Neural Probabilistic Language Models", "abstract": "Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM\u2019s local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets.", "year": 2021, "ssId": "981dbdf6f87f13f3f3047a925c519fc39a35202b", "arXivId": "2104.03474", "link": "https://arxiv.org/pdf/2104.03474.pdf", "openAccess": true, "authors": ["Simeng Sun", "Mohit Iyyer"]}}
{"id": "386bfd0e411dee4f512a8737c55dd84846981182", "content": {"title": "TABBIE: Pretrained Representations of Tabular Data", "abstract": "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model\u2019s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.", "year": 2021, "ssId": "386bfd0e411dee4f512a8737c55dd84846981182", "arXivId": "2105.02584", "link": "https://arxiv.org/pdf/2105.02584.pdf", "openAccess": true, "authors": ["H. Iida", "Dung Ngoc Thai", "Varun Manjunatha", "Mohit Iyyer"]}}
{"id": "46ed42e4318e1363a0ec3dde195422cdfecf2017", "content": {"title": "Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration", "abstract": "Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.", "year": 2021, "ssId": "46ed42e4318e1363a0ec3dde195422cdfecf2017", "arXivId": "2109.06304", "link": "https://arxiv.org/pdf/2109.06304.pdf", "openAccess": true, "authors": ["Shufan Wang", "Laure Thompson", "Mohit Iyyer"]}}
{"id": "facefd2fc4b718c6a0d8096b4eb02866028a04c2", "content": {"title": "Weakly-Supervised Open-Retrieval Conversational Question Answering", "abstract": "Recent studies on Question Answering (QA) and Conversational QA (ConvQA) emphasize the role of retrieval: a system first retrieves evidence from a large collection and then extracts answers. This open-retrieval ConvQA setting typically assumes that each question is answerable by a single span of text within a particular passage (a span answer). The supervision signal is thus derived from whether or not the system can recover an exact match of this ground-truth answer span from the retrieved passages. This method is referred to as spanmatch weak supervision. However, information-seeking conversations are challenging for this span-match method since long answers, especially freeform answers, are not necessarily strict spans of any passage. Therefore, we introduce a learned weak supervision approach that can identify a paraphrased span of the known answer in a passage. Our experiments on QuAC and CoQA datasets show that the span-match weak supervisor can only handle conversations with span answers, and has less satisfactory results for freeform answers generated by people. Our method is more flexible as it can handle both span answers and freeform answers. Moreover, our method can be more powerful when combined with the span-match method which shows it is complementary to the span-match method. We also conduct in-depth analyses to show more insights on open-retrieval ConvQA under a weak supervision setting.", "year": 2021, "ssId": "facefd2fc4b718c6a0d8096b4eb02866028a04c2", "arXivId": "2103.02537", "link": "https://arxiv.org/pdf/2103.02537.pdf", "openAccess": true, "authors": ["Chen Qu", "Liu Yang", "Cen-Chieh Chen", "W. Bruce Croft", "Kalpesh Krishna", "Mohit Iyyer"]}}
{"id": "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e", "content": {"title": "Changing the Mind of Transformers for Topically-Controllable Language Generation", "abstract": "Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and (2) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers.", "year": 2021, "ssId": "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e", "arXivId": "2103.15335", "link": "https://arxiv.org/pdf/2103.15335.pdf", "openAccess": true, "authors": ["Haw-Shiuan Chang", "Jiaming Yuan", "Mohit Iyyer", "A. McCallum"]}}
{"id": "d1206ccabd1980848f14472d6548251c2fab7963", "content": {"title": "Exploring and Predicting Transferability across NLP Tasks", "abstract": "Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.", "year": 2020, "ssId": "d1206ccabd1980848f14472d6548251c2fab7963", "arXivId": "2005.00770", "link": "https://arxiv.org/pdf/2005.00770.pdf", "openAccess": true, "authors": ["Tu Vu", "Tong Wang", "Tsendsuren Munkhdalai", "Alessandro Sordoni", "Adam Trischler", "Andrew Mattarella-Micke", "Subhransu Maji", "Mohit Iyyer"]}}
{"id": "4bc9d6596069c9277b57a7ee1e1127d231f28663", "content": {"title": "Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders", "abstract": "The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.", "year": 2020, "ssId": "4bc9d6596069c9277b57a7ee1e1127d231f28663", "arXivId": null, "link": null, "openAccess": false, "authors": ["Andrew Drozdov", "Subendhu Rongali", "Yi-Pei Chen", "Timothy J. O'Gorman", "Mohit Iyyer", "A. McCallum"]}}
{"id": "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60", "content": {"title": "STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation", "abstract": "Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.", "year": 2020, "ssId": "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60", "arXivId": "2010.01717", "link": "https://arxiv.org/pdf/2010.01717.pdf", "openAccess": true, "authors": ["Nader Akoury", "Shufan Wang", "Josh Whiting", "Stephen Hood", "Nanyun Peng", "Mohit Iyyer"]}}
{"id": "476ff888fe3917f92b221c522ffb7bfaa4e1861b", "content": {"title": "Open-Retrieval Conversational Question Answering", "abstract": "Conversational search is one of the ultimate goals of information retrieval. Recent research approaches conversational search by simplified settings of response ranking and conversational question answering, where an answer is either selected from a given candidate set or extracted from a given passage. These simplifications neglect the fundamental role of retrieval in conversational search. To address this limitation, we introduce an open-retrieval conversational question answering (ORConvQA) setting, where we learn to retrieve evidence from a large collection before extracting answers, as a further step towards building functional conversational search systems. We create a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an end-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader that are all based on Transformers. Our extensive experiments on OR-QuAC demonstrate that a learnable retriever is crucial for ORConvQA. We further show that our system can make a substantial improvement when we enable history modeling in all system components. Moreover, we show that the reranker component contributes to the model performance by providing a regularization effect. Finally, further in-depth analyses are performed to provide new insights into ORConvQA.", "year": 2020, "ssId": "476ff888fe3917f92b221c522ffb7bfaa4e1861b", "arXivId": "2005.11364", "link": "https://arxiv.org/pdf/2005.11364.pdf", "openAccess": true, "authors": ["Chen Qu", "Liu Yang", "Cen-Chieh Chen", "Minghui Qiu", "W. Bruce Croft", "Mohit Iyyer"]}}
{"id": "07a9f47885cae97efb7b4aa109392128532433da", "content": {"title": "Hard-Coded Gaussian Attention for Neural Machine Translation", "abstract": "Recent work has questioned the importance of the Transformer\u2019s multi-headed attention for achieving high translation quality. We push further in this direction by developing a \u201chard-coded\u201d attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.", "year": 2020, "ssId": "07a9f47885cae97efb7b4aa109392128532433da", "arXivId": "2005.00742", "link": "https://arxiv.org/pdf/2005.00742.pdf", "openAccess": true, "authors": ["Weiqiu You", "Simeng Sun", "Mohit Iyyer"]}}
