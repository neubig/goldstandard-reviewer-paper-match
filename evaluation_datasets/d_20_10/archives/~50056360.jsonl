{"id": "ff7b5379641875be7357766af0b1e2bd55c74cc8", "content": {"title": "Transformer Memory as a Differentiable Search Index", "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.", "year": 2022, "ssId": "ff7b5379641875be7357766af0b1e2bd55c74cc8", "arXivId": "2202.06991", "link": "https://arxiv.org/pdf/2202.06991.pdf", "openAccess": true, "authors": ["Yi Tay", "V. Tran", "M. Dehghani", "Jianmo Ni", "Dara Bahri", "Harsh Mehta", "Zhen Qin", "Kai Hui", "Zhe Zhao", "Jai Gupta", "Tal Schuster", "William W. Cohen", "Donald Metzler"]}}
{"id": "33ce3cd897a3473973f338c154f3fe5c1175643c", "content": {"title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations", "abstract": "We present the Open Predicate Query Language (OPQL); a method for constructing a virtual KB (VKB) trained entirely from text. Large Knowledge Bases (KBs) are indispensable for a widerange of industry applications such as question answering and recommendation. Typically, KBs encode world knowledge in a structured, readily accessible form derived from laborious human annotation efforts. Unfortunately, while they are extremely high precision, KBs are inevitably highly incomplete and automated methods for enriching them are far too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set of relation mentions in a way that naturally enables reasoning and can be trained without any structured supervision. We demonstrate that OPQL outperforms prior VKB methods on two different KB reasoning tasks and, additionally, can be used as an external memory integrated into a language model (OPQL-LM) leading to improvements on two open-domain question answering tasks.", "year": 2021, "ssId": "33ce3cd897a3473973f338c154f3fe5c1175643c", "arXivId": "2102.07043", "link": "https://arxiv.org/pdf/2102.07043.pdf", "openAccess": true, "authors": ["Haitian Sun", "Pat Verga", "Bhuwan Dhingra", "R. Salakhutdinov", "William W. Cohen"]}}
{"id": "bd6c708a535af588d90025a0e6cf17407bf65434", "content": {"title": "Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations", "abstract": "In attempts to \u201cexplain\u201d predictions of machine learning models, researchers have proposed hundreds of techniques for attributing predictions to features that are deemed important. While these attributions are often claimed to hold the potential to improve human \u201cunderstanding\u201d of the models, surprisingly little work explicitly evaluates progress towards this aspiration. In this paper, we conduct a crowdsourcing study, where participants interact with deception detection models that have been trained to distinguish between genuine and fake hotel reviews. They are challenged both to simulate the model on fresh reviews, and to edit reviews with the goal of lowering the probability of the originally predicted class. Successful manipulations would lead to an adversarial example. During the training (but not the test) phase, input spans are highlighted to communicate salience. Through our evaluation, we observe that for a linear bag-of-words model, participants with access to the feature coefficients during training are able to cause a larger reduction in model confidence in the testing phase when compared to the no-explanation control. For the BERT-based classifier, popular local explanations do not improve their ability to reduce the model confidence over the no-explanation case. Remarkably, when the explanation for the BERT model is given by the (global) attributions of a linear model trained to imitate the BERT model, people can effectively manipulate the model.", "year": 2021, "ssId": "bd6c708a535af588d90025a0e6cf17407bf65434", "arXivId": "2112.09669", "link": "https://arxiv.org/pdf/2112.09669.pdf", "openAccess": true, "authors": ["Siddhant Arora", "Danish Pruthi", "N. Sadeh", "William W. Cohen", "Zachary Chase Lipton", "Graham Neubig"]}}
{"id": "ac8d33e4c0a45e227a47353f3f26fbb231482dc1", "content": {"title": "Time-Aware Language Models as Temporal Knowledge Bases", "abstract": "Abstract Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum\u2014those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently \u201crefreshed\u201d as new data arrives, without the need for retraining from scratch.", "year": 2021, "ssId": "ac8d33e4c0a45e227a47353f3f26fbb231482dc1", "arXivId": "2106.15110", "link": "https://arxiv.org/pdf/2106.15110.pdf", "openAccess": true, "authors": ["Bhuwan Dhingra", "Jeremy R. Cole", "Julian Martin Eisenschlos", "D. Gillick", "Jacob Eisenstein", "William W. Cohen"]}}
{"id": "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0", "content": {"title": "Chapter 5. Answering Natural-Language Questions with Neuro-Symbolic Knowledge Bases", "abstract": "Symbolic reasoning systems based on first-order logics are computationally powerful, and feedforward neural networks are computationally efficient, so unless P=NP, neural networks cannot, in general, emulate symbolic logics. Hence bridging the gap between neural and symbolic methods requires achieving a delicate balance: one needs to incorporate just enough of symbolic reasoning to be useful for a task, but not so much as to cause computational intractability. In this chapter we first present results that make this claim precise, and then use these formal results to inform the choice of a neuro-symbolic knowledge-based reasoning system, based on a set-based dataflow query language. We then present experimental results with a number of variants of this neuro-symbolic reasoner, and also show that this neuro-symbolic reasoner can be closely integrated into modern neural language models.", "year": 2021, "ssId": "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0", "arXivId": null, "link": null, "openAccess": false, "authors": ["Haitian Sun", "Pat Verga", "William W. Cohen"]}}
{"id": "2f7c03f0d3c6f51728e925a874c49a25559cc6b3", "content": {"title": "End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents", "abstract": "Answering complex questions from long documents requires aggregating multiple pieces of evidence and then predicting the answers. In this paper, we propose a multihop retrieval method, DOCHOPPER, to answer compositional questions over long documents. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step. In contrast to many other retrieval-based methods (e.g., RAG or REALM) the query is not augmented with a token sequence: instead, it is augmented by \u201cnumerically\u201d combining it with another neural representation. This means that model is end-to-end differentiable. We demonstrate that utilizing document structure in this was can largely improve question-answering and retrieval performance on long documents. We experimented with DOCHOPPER on three different QA tasks that require reading long documents to answer compositional questions: discourse entailment reasoning, factual QA with table and text, and information seeking QA from academic papers. DOCHOPPER outperforms all baseline models and achieves state-of-the-art results on all datasets. Additionally, DOCHOPPER is efficient at inference time, being 3\u223c10 times faster than the baselines.", "year": 2021, "ssId": "2f7c03f0d3c6f51728e925a874c49a25559cc6b3", "arXivId": null, "link": null, "openAccess": false, "authors": ["Haitian Sun", "William W. Cohen", "R. Salakhutdinov"]}}
{"id": "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b", "content": {"title": "Iterative Hierarchical Attention for Answering Complex Questions over Long Documents", "abstract": "We propose a new model, DOCHOPPER, that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. Similar to multi-hop question-answering (QA) systems, at each step, DOCHOPPER uses a query q to attend to information from a document, combines this \u201cretrieved\u201d information with q to produce the next query. However, in contrast to most previous multi-hop QA systems, DOCHOPPER is able to \u201cretrieve\u201d either short passages or long sections of the document, thus emulating a multi-step process of \u201cnavigating\u201d through a long document to answer a question. To enable this novel behavior, DOCHOPPER does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact neural representation of a hierarchical part of the document, which can potentially be quite large. We experiment with DOCHOPPER on four different QA tasks that require reading long and complex documents to answer multi-hop questions, and show that DOCHOPPER achieves state-of-the-art results on three of the datasets. Additionally, DOCHOPPER is efficient at inference time, being 3\u201310 times faster than the baselines.1", "year": 2021, "ssId": "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b", "arXivId": "2106.00200", "link": "https://arxiv.org/pdf/2106.00200.pdf", "openAccess": true, "authors": ["Haitian Sun", "William W. Cohen", "R. Salakhutdinov"]}}
{"id": "af38829cdb55ee7b71d49399f71397d975e40a95", "content": {"title": "ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers", "abstract": "We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features: (1) long context documents with information that is related in logically complex ways; (2) multi-hop questions that require compositional logical reasoning; (3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions; (4) questions asked without knowing the answers. We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions. We believe that this dataset will motivate further research in answering complex questions over long documents. Data and leaderboard are publicly available1.", "year": 2021, "ssId": "af38829cdb55ee7b71d49399f71397d975e40a95", "arXivId": "2110.06884", "link": "https://arxiv.org/pdf/2110.06884.pdf", "openAccess": true, "authors": ["Haitian Sun", "William W. Cohen", "R. Salakhutdinov"]}}
{"id": "395aae6e7a79e5760457ca38e868acc970016230", "content": {"title": "MATE: Multi-view Attention for Table Transformer Efficiency", "abstract": "This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.", "year": 2021, "ssId": "395aae6e7a79e5760457ca38e868acc970016230", "arXivId": "2109.04312", "link": "https://arxiv.org/pdf/2109.04312.pdf", "openAccess": true, "authors": ["Julian Martin Eisenschlos", "Maharshi Gor", "Thomas M\u00fcller", "William W. Cohen"]}}
{"id": "8b652c4d7a8d5836925ce0fe28a91dc661778524", "content": {"title": "What's the best place for an AI conference, Vancouver or ______: Why completing comparative questions is difficult", "abstract": "Although large neural language models (LMs) like BERT can be \ufb01netuned to yield state-of-the-art results on many NLP tasks, it is often unclear what these models actually learn. Here we study using such LMs to \ufb01ll in entities in human-authored comparative questions, like \u201cWhich country is older, India or ?\u201d\u2014i.e., we study the ability of neural LMs to ask (not answer ) reasonable questions. We show that accu-racy in this \ufb01ll-in-the-blank task is well-correlated with hu- man judgements of whether a question is reasonable, and that these models can be trained to achieve nearly human-level performance in completing comparative questions in three different subdomains. However, analysis shows that what they learn fails to model any sort of broad notion of which entities are semantically comparable or similar\u2014instead the trained models are very domain-speci\ufb01c, and performance is highly correlated with co-occurrences between speci\ufb01c enti- ties observed in the training set. This is true both for models that are pretrained on general text corpora, as well as mod- els trained on a large corpus of comparison questions. Our study thus reinforces recent results on the dif\ufb01culty of mak- ing claims about a deep model\u2019s world knowledge or linguistic competence based on performance on speci\ufb01c benchmark problems. We make our evaluation datasets publicly available to foster future research on complex understanding and rea- soning in such models at standards of human interaction.", "year": 2021, "ssId": "8b652c4d7a8d5836925ce0fe28a91dc661778524", "arXivId": "2104.01940", "link": "https://arxiv.org/pdf/2104.01940.pdf", "openAccess": true, "authors": ["Avishai Zagoury", "Einat Minkov", "Idan Szpektor", "William W. Cohen"]}}
{"id": "7f0dbd30dc839fd95ea953a9229c879396ca11c0", "content": {"title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "year": 2020, "ssId": "7f0dbd30dc839fd95ea953a9229c879396ca11c0", "arXivId": "2002.06115", "link": "https://arxiv.org/pdf/2002.06115.pdf", "openAccess": true, "authors": ["William W. Cohen", "Haitian Sun", "R. A. Hofer", "M. Siegler"]}}
{"id": "8c38bffc058d558e7c734032ba63942865e05ae4", "content": {"title": "Faithful Embeddings for Knowledge Base Queries", "abstract": "The deductive closure of an ideal knowledge base (KB) contains exactly the logical queries that the KB can answer. However, in practice KBs are both incomplete and over-specified, failing to answer some queries that have real-world answers. \\emph{Query embedding} (QE) techniques have been recently proposed where KB entities and KB queries are represented jointly in an embedding space, supporting relaxation and generalization in KB inference. However, experiments in this paper show that QE systems may disagree with deductive reasoning on answers that do not require generalization or relaxation. We address this problem with a novel QE method that is more faithful to deductive reasoning, and show that this leads to better performance on complex queries to incomplete KBs. Finally we show that inserting this new QE module into a neural question-answering system leads to substantial improvements over the state-of-the-art.", "year": 2020, "ssId": "8c38bffc058d558e7c734032ba63942865e05ae4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Haitian Sun", "Andrew O. Arnold", "Tania Bedrax-Weiss", "Fernando Pereira", "William W. Cohen"]}}
{"id": "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f", "content": {"title": "Guessing What's Plausible But Remembering What's True: Accurate Neural Reasoning for Question-Answering", "abstract": "Neural approaches to natural language processing (NLP) often fail at the logical reasoning needed for deeper language understanding. In particular, neural approaches to reasoning that rely on embedded generalizations of a knowledge base (KB) implicitly model which facts that are plausible, but may not model which facts are true, according to the KB. While generalizing the facts in a KB is useful for KB completion, the inability to distinguish between plausible inferences and logically entailed conclusions can be problematic in settings like as KB question answering (KBQA). We propose here a novel KB embedding scheme that supports generalization, but also allows accurate logical reasoning with a KB. Our approach introduces two new mechanisms for KB reasoning: neural retrieval over a set of embedded triples, and \u201cmemorization\u201d of highly specific information with a compact sketch structure. Experimentally, this leads to substantial improvements over the state-of-theart on two KBQA benchmarks.", "year": 2020, "ssId": "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f", "arXivId": "2004.03658", "link": "https://arxiv.org/pdf/2004.03658.pdf", "openAccess": true, "authors": ["Haitian Sun", "Andrew O. Arnold", "Tania Bedrax-Weiss", "Fernando Pereira", "William W. Cohen"]}}
{"id": "93d3e45395117e21214d404c8753b578c29266d1", "content": {"title": "Open Question Answering over Tables and Text", "abstract": "In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open QA systems have considered only retrieving information from unstructured text. Here we consider for the first time open QA over both tabular and textual data and present a new large-scale dataset Open Table-and-Text Question Answering (OTT-QA) to evaluate performance on this task. Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging -- our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use \"early fusion\" to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27%.", "year": 2020, "ssId": "93d3e45395117e21214d404c8753b578c29266d1", "arXivId": "2010.10439", "link": "https://arxiv.org/pdf/2010.10439.pdf", "openAccess": true, "authors": ["Wenhu Chen", "Ming-Wei Chang", "Eva Schlinger", "W. Wang", "William W. Cohen"]}}
{"id": "f48792e8a24e369c80e39a2a2b7451d108f02941", "content": {"title": "Towards Explainable Question Answering (XQA)", "abstract": "The increasing rate of information pollution on the Web requires novel solutions to tackle that. Question Answering (QA) interfaces are simplified and user-friendly interfaces to access information on the Web. However, similar to other AI applications, they are black boxes which do not manifest the details of the learning or reasoning steps for augmenting an answer. The Explainable Question Answering (XQA) system can alleviate the pain of information pollution where it provides transparency to the underlying computational model and exposes an interface enabling the end-user to access and validate provenance, validity, context, circulation, interpretation, and feedbacks of information. This position paper sheds light on the core concepts, expectations, and challenges in favor of the following questions (i) What is an XQA system?, (ii) Why do we need XQA?, (iii) When do we need XQA? (iv) How to represent the explanations? (iv) How to evaluate XQA systems?", "year": 2020, "ssId": "f48792e8a24e369c80e39a2a2b7451d108f02941", "arXivId": null, "link": null, "openAccess": false, "authors": ["Zhilin Yang", "Peng Qi", "William W. Cohen", "Christopher D. Manning"]}}
{"id": "bcbac71ac64cd6a6aaae41e37ebe960f508ab741", "content": {"title": "Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge", "abstract": "Massive language models are the core of modern NLP modeling and have been shown to encode impressive amounts of commonsense and factual information. However, that knowledge exists only within the latent parameters of the model, inaccessible to inspection and interpretation, and even worse, factual information memorized from the training corpora is likely to become stale as the world changes. Knowledge stored as parameters will also inevitably exhibit all of the biases inherent in the source materials. To address these problems, we develop a neural language model that includes an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge. We show that this model dramatically improves performance on two knowledge-intensive question-answering tasks. More interestingly, the model can be updated without re-training by manipulating its symbolic representations. In particular this model allows us to add new facts and overwrite existing ones in ways that are not possible for earlier models.", "year": 2020, "ssId": "bcbac71ac64cd6a6aaae41e37ebe960f508ab741", "arXivId": "2007.00849", "link": "https://arxiv.org/pdf/2007.00849.pdf", "openAccess": true, "authors": ["Pat Verga", "Haitian Sun", "Livio Baldini Soares", "William W. Cohen"]}}
{"id": "6027ef3b4e5585b45db0b9d333956425d3972351", "content": {"title": "Differentiable Open-Ended Commonsense Reasoning", "abstract": "Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic, we propose to study open-ended commonsense reasoning (OpenCSR) \u2014 the task of answering a commonsense question without any pre-defined choices \u2014 using as a resource only a corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt several popular commonsense reasoning benchmarks, and collect multiple new answers for each test question via crowd-sourcing. Experiments show that DrFact outperforms strong baseline methods by a large margin.", "year": 2020, "ssId": "6027ef3b4e5585b45db0b9d333956425d3972351", "arXivId": "2010.14439", "link": "https://arxiv.org/pdf/2010.14439.pdf", "openAccess": true, "authors": ["Bill Yuchen Lin", "Haitian Sun", "Bhuwan Dhingra", "M. Zaheer", "Xiang Ren", "William W. Cohen"]}}
{"id": "2232808cf3161ca4c434126e35f47ee33c0c8219", "content": {"title": "Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?", "abstract": "Abstract While many methods purport to explain predictions by highlighting salient features, what aims these explanations serve and how they ought to be evaluated often go unstated. In this work, we introduce a framework to quantify the value of explanations via the accuracy gains that they confer on a student model trained to simulate a teacher model. Crucially, the explanations are available to the student during training, but are not available at test time. Compared with prior proposals, our approach is less easily gamed, enabling principled, automatic, model-agnostic evaluation of attributions. Using our framework, we compare numerous attribution methods for text classification and question answering, and observe quantitative differences that are consistent (to a moderate to high degree) across different student model architectures and learning strategies.1", "year": 2020, "ssId": "2232808cf3161ca4c434126e35f47ee33c0c8219", "arXivId": "2012.00893", "link": "https://arxiv.org/pdf/2012.00893.pdf", "openAccess": true, "authors": ["Danish Pruthi", "Bhuwan Dhingra", "Livio Baldini Soares", "Michael Collins", "Z. Lipton", "Graham Neubig", "William W. Cohen"]}}
{"id": "a9e6222e71dd101d444b7192b3a0636c71edb0a4", "content": {"title": "VIRTUAL KNOWLEDGE BASE", "abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. On HotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking approach to retrieving the relevant passages required to answer a question. DrKIT is also very efficient, processing 10-100x more queries per second than existing multi-hop systems.1", "year": 2020, "ssId": "a9e6222e71dd101d444b7192b3a0636c71edb0a4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Bhuwan Dhingra", "M. Zaheer", "Vidhisha Balachandran", "Graham Neubig", "R. Salakhutdinov", "William W. Cohen"]}}
{"id": "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63", "content": {"title": "Differentiable Reasoning over a Virtual Knowledge Base", "abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a virtual KB, softly following paths of relations between mentions of entities in the corpus. At each step the operation uses a combination of sparse-matrix TFIDF indices and maximum inner product search (MIPS) on a special index of contextual representations. This module is differentiable, so the full system can be trained completely end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the index mention encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. DrKIT is also very efficient, processing upto 10x more queries per second than existing state-of-the-art QA systems.", "year": 2020, "ssId": "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63", "arXivId": "2002.10640", "link": "https://arxiv.org/pdf/2002.10640.pdf", "openAccess": true, "authors": ["Bhuwan Dhingra", "M. Zaheer", "Vidhisha Balachandran", "Graham Neubig", "R. Salakhutdinov", "William W. Cohen"]}}
