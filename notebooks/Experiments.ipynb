{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers as hlp\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scoring import compute_resolution, compute_main_metric\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path to the CSV file with evaluations\n",
    "DATA_PATH = '../data/evaluations.csv'\n",
    "\n",
    "# Path to the directory with predictions of similarities\n",
    "PRED_PATH = '../predictions'\n",
    "\n",
    "# Reading evaluations\n",
    "df = pd.read_csv(DATA_PATH, sep='\\t')\n",
    "# Transforming evaluations into the dict of the form {reviewer: {paper1: expertise1, paper2: expertise2, ...}}\n",
    "references, _ = hlp.to_dicts(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def score_performance(pred_file, references, valid_papers, bootstraps=None):\n",
    "    \"\"\"Compute the main metric for predicted similarity together with bootstrapped values for confidence intervals\n",
    "\n",
    "    :param pred_file: Name of the file where predicted similarities are stored (file must be in the PRED_PATH dir)\n",
    "    :param references: Ground truth values of expertise\n",
    "    :param valid_papers: Papers to include in evaluation\n",
    "    :param bootstraps: Subsampled reviewer pools for bootstrap computations\n",
    "    :return: Score of the predictions + data to compute confidence intervals (if `bootstraps` is not None)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(os.path.join(PRED_PATH, pred_file), 'r') as handler:\n",
    "        predictions = json.load(handler)\n",
    "\n",
    "    valid_reviewers = list(predictions.keys())\n",
    "\n",
    "    score = compute_main_metric(predictions, references, valid_papers, valid_reviewers)\n",
    "\n",
    "    if bootstraps is None:\n",
    "        return score\n",
    "\n",
    "    variations = [compute_main_metric(predictions, references, valid_papers, vr) for vr in bootstraps]\n",
    "\n",
    "    return score, variations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def score_resolution(pred_file, references, valid_papers, bootstraps=None):\n",
    "    \"\"\"Compute resolution ability of the algorithms (accuracy on hard/easy triples) together with bootstrapped\n",
    "    values for confidence intervals\n",
    "\n",
    "    :param pred_file: Name of the file where predicted similarities are stored (file must be in the PRED_PATH dir)\n",
    "    :param references: Ground truth values of expertise\n",
    "    :param valid_papers: Papers to include in evaluation\n",
    "    :param bootstraps: Subsampled reviewer pools for bootstrap computations\n",
    "    :return: Score of predictions on easy/hard triples + data to compute confidence intervals (if `bootstraps` is not None)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(os.path.join(PRED_PATH, pred_file), 'r') as handler:\n",
    "        predictions = json.load(handler)\n",
    "\n",
    "    valid_reviewers = list(predictions.keys())\n",
    "\n",
    "    score_easy = compute_resolution(predictions, references, valid_papers, valid_reviewers, regime='easy')\n",
    "    score_hard = compute_resolution(predictions, references, valid_papers, valid_reviewers, regime='hard')\n",
    "\n",
    "    if bootstraps is None:\n",
    "        return score_easy, score_hard\n",
    "\n",
    "    variations_easy = [compute_resolution(predictions, references, valid_papers, vr, regime='easy')['score']\n",
    "                                for vr in bootstraps]\n",
    "    variations_hard = [compute_resolution(predictions, references, valid_papers, vr, regime='hard')['score']\n",
    "                                for vr in bootstraps]\n",
    "\n",
    "    return score_easy, score_hard, variations_easy, variations_hard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Auxiliary functions to streamline computation of statistics.\n",
    "# These functions account for the procedure we use to average out the noise in profile creation.\n",
    "\n",
    "def get_mean(vals):\n",
    "    \"\"\" Average out the noise in pointwise estimates of performance \"\"\"\n",
    "    return round(np.mean(vals), 2)\n",
    "\n",
    "def get_ci(vals):\n",
    "    \"\"\" Get 95% confidence intervals for the pointwise performance \"\"\"\n",
    "\n",
    "    # Average out the noise due to randomness in the profile creation\n",
    "    boot = np.matrix(vals).mean(axis=0).tolist()[0]\n",
    "\n",
    "    # Get confidence interval\n",
    "    return f\"[{round(np.percentile(boot, 2.5), 2)}; {round(np.percentile(boot, 97.5), 2)}]\"\n",
    "\n",
    "def get_diff(vals_main, vals_contrast):\n",
    "    \"\"\" Difference in performance between two algorithms (main and contrast) \"\"\"\n",
    "    return round(np.mean(vals_main) - np.mean(vals_contrast), 2)\n",
    "\n",
    "def get_diff_ci(vals_main, vals_contrast):\n",
    "    \"\"\" 95% confidence interval for the difference in performance \"\"\"\n",
    "\n",
    "    boot = (np.matrix(vals_main) - np.matrix(vals_contrast)).mean(axis=0).tolist()[0]\n",
    "    return f\"[{round(np.percentile(boot, 2.5), 2)}; {round(np.percentile(boot, 97.5), 2)}]\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list of all reviewers (58) who participated in our study\n",
    "# list of all papers included in the dataset (463)\n",
    "all_reviewers = list(references.keys())\n",
    "\n",
    "all_papers = set()\n",
    "for rev in references:\n",
    "    all_papers = all_papers.union(references[rev].keys())\n",
    "\n",
    "# list of reviewers whose Semantic Scholar profiles include papers available on arXiv (57)\n",
    "# list of papers included in the dataset that have publicly available PDFs (457)\n",
    "reviewers_with_pdf = list(set(all_reviewers) - {'1789029797'})\n",
    "\n",
    "papers_with_pdf = list(set(all_papers) - {'203da29a37a983c487ce75a894b0d70698077bf5', '2eea63f896deed47cc0c0000e1482ec5c860fd0b',\n",
    "                                          '35750f1908f405bb38b0708972f33fe07b378b64', '3c8853d4ae3ad2633c47e840a48951d62b64a5b4',\n",
    "                                          'c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b', 'no_ss'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Main Eval (Section 6.1)\n",
    "\n",
    "In this section we evaluate algorithms on the datasets `d_20_{x}.json, x \\in {1, 2, ... 10}`. Specifically,\n",
    "1. We evaluate algorithms in the `title+abstract` regime\n",
    "2. We include `20` papers in the reviewers' profiles.\n",
    "3. To average out the noise associated with the randomness in constructing reviewer profiles (due to ties between papers published in the same year), we consider `10` versions of the dataset (parametrized by `x`) and report values averaged across these versions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results for Table 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of algorithms to compare in Table 3\n",
    "algorithms = ['tpms', 'elmo', 'specter', 'specter_mfr', 'acl']\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# Reviewers that we include in the score computation (all 58 reviewers are included)\n",
    "valid_reviewers = all_reviewers\n",
    "\n",
    "# Papers that we include in the score computation (all 463 papers are included)\n",
    "valid_papers = all_papers\n",
    "\n",
    "# Subsampling reviewer pools for bootstrapping\n",
    "bootstraps = [np.random.choice(valid_reviewers, len(valid_reviewers), replace=True) for x in range(n_bootstrap)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Results dict\n",
    "#\n",
    "# 'alg': Algorithm we evaluate\n",
    "# 'Pointwise': values of losses (main metric) on the original reviewer pool\n",
    "# 'Variations': values of losses (main metric) on the bootstrapped reviewer pools\n",
    "#\n",
    "# Recall that we average out the noise in the profile creation by evaluating algorithms\n",
    "# on 10 versions of the dataset that have slightly different reviewer profiles (due to randomness in\n",
    "# breaking ties between papers published in the same year). To handle this, we first compute pointwise\n",
    "# and bootstrapped values of loss on each of the 10 datasets and then average the values across the\n",
    "# datasets\n",
    "\n",
    "t3_res = {alg: {'pointwise': [], 'variations': []} for alg in algorithms}\n",
    "profile_length = 20\n",
    "rgm = 'ta' #we evaluate algorithms in the title+abstract regime\n",
    "\n",
    "# Populating results dict\n",
    "for algo in algorithms:\n",
    "\n",
    "    # Inner iterations for averaging out the randomness in the profile creation\n",
    "    for iteration in range(1, 11):\n",
    "\n",
    "        tmp = score_performance(f'{algo}_d_{profile_length}_{iteration}_{rgm}.json', references, valid_papers, bootstraps)\n",
    "\n",
    "        t3_res[algo]['pointwise'].append(tmp[0])\n",
    "        t3_res[algo]['variations'].append(tmp[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing the data of Table 3\n",
    "for algo in algorithms:\n",
    "\n",
    "    mn = get_mean(t3_res[algo]['pointwise'])\n",
    "    ci = get_ci(t3_res[algo]['variations'])\n",
    "\n",
    "    if algo == 'tpms':\n",
    "        print(f\"{algo:>11}: {mn:>4} \\t {ci:>12}\")\n",
    "    else:\n",
    "        mn_diff = get_diff(t3_res[algo]['pointwise'], t3_res['tpms']['pointwise'])\n",
    "        ci_diff = get_diff_ci(t3_res[algo]['variations'], t3_res['tpms']['variations'])\n",
    "        print(f\"{algo:>11}: {mn:>4} \\t {ci:>12} \\t {mn_diff:>5} \\t {ci_diff:>13}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. TPMS Evaluation (Section 6.2)\n",
    "\n",
    "In this section, we evaluate algorithms on the datasets `d_full_{y}_{x}.json, y \\in {1, 2, ..., 20}, x \\in {1, 2, ..., 5}`. Specifically:\n",
    "1. `full` indicates that we only use papers with pdfs publicly available (which excludes 6 of the papers reported by participants and one reviewer whose past papers are not available on arXiv). In return, all papers in these datasets have PDFs available and we can  employ full texts of papers in similarity computation\n",
    "2. We experiment with the profile length `y`, varying it from 1 to 20\n",
    "3. We work with `5` copies of each dataset to average out the noise in the profile creation (parametrized by `x`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results for Figure 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Regimes in which we evaluate the TPMS algorithm (title only, title+abstract, full text of papers)\n",
    "regimes = ['t', 'ta', 'f']\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# We take only those reviewers whose profiles contain papers available on arxiv (57 reviewers).\n",
    "valid_reviewers = reviewers_with_pdf\n",
    "\n",
    "# Similarly, we predict similarities against papers that we were able to find PDFs for (457 papers).\n",
    "valid_papers = papers_with_pdf\n",
    "\n",
    "bootstraps = [np.random.choice(valid_reviewers, len(valid_reviewers), replace=True) for x in range(n_bootstrap)]\n",
    "\n",
    "# In this section we evaluate the tpms algorithm only\n",
    "algo = 'tpms'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Results dict\n",
    "f2_res = {profile_length: {'t':[], 'ta': [], 'f': []} for profile_length in range(1, 21)}\n",
    "\n",
    "# Different values of profile length\n",
    "for profile_length in f2_res:\n",
    "    # Different regimes ('t', 'ta', 'f')\n",
    "    for regime in f2_res[profile_length]:\n",
    "        # Iterations to average out the noise in profile creation\n",
    "        for iteration in range(1, 6):\n",
    "\n",
    "            f2_res[profile_length][regime].append(\n",
    "                score_performance(f'{algo}_d_full_{profile_length}_{iteration}_{regime}.json', references, valid_papers)\n",
    "            )\n",
    "\n",
    "    for regime in f2_res[profile_length]:\n",
    "        f2_res[profile_length][regime] = np.mean(f2_res[profile_length][regime])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting Figure 2\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "num_paps = list(range(1, 21))\n",
    "\n",
    "ax.plot(num_paps, [f2_res[x]['t'] for x in range(1, 21)], label='title', linestyle='-', marker='o', markersize=10)\n",
    "ax.plot(num_paps, [f2_res[x]['ta'] for x in range(1, 21)], label='title+abstract', linestyle='--', marker='*', markersize=12)\n",
    "ax.plot(num_paps, [f2_res[x]['f'] for x in range(1, 21)], label='full text', linestyle='-.', marker='v', markersize=10)\n",
    "\n",
    "ax.set_ylim(0, 0.55)\n",
    "ax.set_xticks([1, 5, 10, 15, 20])\n",
    "ax.tick_params(labelsize=13)\n",
    "ax.set_xlabel(\"Number of papers in reviewer profile\", fontsize=20)\n",
    "ax.set_ylabel(\"Loss\", fontsize=20)\n",
    "legend = ax.legend(fontsize=16, loc=(0.67, 0.07), title='Paper content')\n",
    "_ = plt.setp(legend.get_title(),fontsize=18)\n",
    "\n",
    "plt.savefig(\"../figures/tmps_analysis.pdf\", bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results for Table 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Results dict\n",
    "# We set the profile length to 20 and compute confidence intervals for each choice of the paper content representation\n",
    "# ('t', 'ta', 'f')\n",
    "t4_res = {rgm: {'pointwise': [], 'variations': []} for rgm in regimes}\n",
    "profile_length = 20\n",
    "\n",
    "for rgm in regimes:\n",
    "    # Iterations to average out the noise in profile creation\n",
    "    for iteration in range(1, 6):\n",
    "\n",
    "        tmp = score_performance(f'{algo}_d_full_{profile_length}_{iteration}_{rgm}.json', references, valid_papers, bootstraps)\n",
    "\n",
    "        t4_res[rgm]['pointwise'].append(tmp[0])\n",
    "        t4_res[rgm]['variations'].append(tmp[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing the data of Table 4\n",
    "for rgm in regimes:\n",
    "\n",
    "    mn = get_mean(t4_res[rgm]['pointwise'])\n",
    "    ci = get_ci(t4_res[rgm]['variations'])\n",
    "\n",
    "    if rgm == 'ta':\n",
    "        print(f\"{rgm:>2}: {mn:>4} \\t {ci:>12}\")\n",
    "    else:\n",
    "        mn_diff = get_diff(t4_res[rgm]['pointwise'], t4_res['ta']['pointwise'])\n",
    "        ci_diff = get_diff_ci(t4_res[rgm]['variations'], t4_res['ta']['variations'])\n",
    "        print(f\"{rgm:>2}: {mn:>4} \\t {ci:>12} \\t {mn_diff:>3} \\t {ci_diff:>13}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Easy/Hard pairs (Section 7)\n",
    "\n",
    "Finally, we evaluate the resolution abilities of the algorithms in the title+abstract regime with profile length of 20. For this, we work we the same data we used in Part 1 (Main Eval).\n",
    "\n",
    "Additionally, we evaluate TPMS in the full text regime and for this evaluation we use the `d_full_20_*` datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results for Table 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Algorithms we evaluate in this section (title+abstract regime with 20 papers in the reviewers' bibliographies)\n",
    "algorithms = ['tpms', 'elmo', 'specter', 'specter_mfr', 'acl']\n",
    "n_bootstrap = 1000\n",
    "\n",
    "valid_reviewers = all_reviewers\n",
    "valid_papers = all_papers\n",
    "\n",
    "bootstraps = [np.random.choice(valid_reviewers, len(valid_reviewers), replace=True) for x in range(n_bootstrap)]\n",
    "\n",
    "# Counting the number of hard/easy triples\n",
    "info = score_resolution('constant_d_20_1_ta.json', references, valid_papers)\n",
    "print(f\"There are {info[0]['total']} easy triples and {info[1]['total']} hard triples\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Result dicts for easy/hard triples\n",
    "t5_res_easy = {algo: {'pointwise': [], 'variations': []} for algo in algorithms}\n",
    "t5_res_hard = {algo: {'pointwise': [], 'variations': []} for algo in algorithms}\n",
    "\n",
    "profile_length = 20\n",
    "rgm = 'ta' #title+abstract\n",
    "\n",
    "for algo in algorithms:\n",
    "    for iteration in range(1, 11):\n",
    "\n",
    "        tmp = score_resolution(f\"{algo}_d_{profile_length}_{iteration}_{rgm}.json\", references, valid_papers, bootstraps)\n",
    "\n",
    "        t5_res_easy[algo]['pointwise'].append(tmp[0]['score'])\n",
    "        t5_res_easy[algo]['variations'].append(tmp[2])\n",
    "\n",
    "        t5_res_hard[algo]['pointwise'].append(tmp[1]['score'])\n",
    "        t5_res_hard[algo]['variations'].append(tmp[3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We also evaluate the TPMS algorithm in the full text regime. For this, we use datasets 'd_full_20_*' that only contains\n",
    "# papers with pdfs publicly available\n",
    "algo = 'tpms'\n",
    "\n",
    "t5_res_easy['tpms_f'] = {'pointwise': [], 'variations': []}\n",
    "t5_res_hard['tpms_f'] = {'pointwise': [], 'variations': []}\n",
    "\n",
    "# 57 reviewers whose profiles have some papers with links to arXiv\n",
    "valid_reviewers = reviewers_with_pdf\n",
    "\n",
    "# 457 papers from the dataset with PDFs publicly available\n",
    "valid_papers = papers_with_pdf\n",
    "\n",
    "bootstraps = [np.random.choice(valid_reviewers, len(valid_reviewers), replace=True) for x in range(n_bootstrap)]\n",
    "\n",
    "profile_length = 20\n",
    "rgm = 'f' #full texts of papers\n",
    "\n",
    "for iteration in range(1, 6):\n",
    "\n",
    "    tmp_full = score_resolution(f\"{algo}_d_full_{profile_length}_{iteration}_{rgm}.json\", references, valid_papers, bootstraps)\n",
    "\n",
    "    t5_res_easy['tpms_f']['pointwise'].append(tmp_full[0]['score'])\n",
    "    t5_res_easy['tpms_f']['variations'].append(tmp_full[2])\n",
    "\n",
    "    t5_res_hard['tpms_f']['pointwise'].append(tmp_full[1]['score'])\n",
    "    t5_res_hard['tpms_f']['variations'].append(tmp_full[3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing data for Table 5 (easy triples)\n",
    "\n",
    "for algo in algorithms + ['tpms_f']:\n",
    "        print(f\"{algo:>11}: {get_mean(t5_res_easy[algo]['pointwise']):>4} \\t {get_ci(t5_res_easy[algo]['variations']):>12}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing data for Table 5 (hard triples)\n",
    "\n",
    "for algo in algorithms + ['tpms_f']:\n",
    "        print(f\"{algo:>11}: {get_mean(t5_res_hard[algo]['pointwise']):>4} \\t {get_ci(t5_res_hard[algo]['variations']):>12}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
